{
  "repo_name": "mindsdb_mindsdb",
  "issue_id": "1848",
  "issue_description": "# Couchbase and InfluxDB support\n\n- I use Couchbase as my MongoDB\r\n- InfluxDB is for my historical market data,",
  "issue_comments": [
    {
      "id": 1014585581,
      "user": "ZoranPandovski",
      "body": "@ceddybi In case you are interested in contributing connector we are happy to guide you :raised_hands: "
    },
    {
      "id": 1072675108,
      "user": "ktyborowski",
      "body": "I can take this one. Will start with InfluxDB as I've seen it mentioned multiple times ðŸ˜„ "
    },
    {
      "id": 1073814819,
      "user": "ZoranPandovski",
      "body": "@pixpack FYI we will move the datasources to the mindsdb repository. If you have already started on this please check https://github.com/mindsdb/mindsdb/pull/2060(still WIP) but you will know what structure to follow. "
    },
    {
      "id": 1074177141,
      "user": "ktyborowski",
      "body": "@ZoranPandovski yes I remember you mentioning this. I actually noticed that there already is an InfluxDB datasource but it uses an old influxDB client library that works only for InfluxDB 1.7 and earlier. I will make the update and create the PR in the core repository."
    },
    {
      "id": 1207947962,
      "user": "MinuraPunchihewa",
      "body": "@ZoranPandovski Can I take up the InfluxDB integration?"
    },
    {
      "id": 1214242622,
      "user": "AvinashGupta",
      "body": "@ZoranPandovski Can I work on integrating InfluxDB?"
    },
    {
      "id": 1214980582,
      "user": "ZoranPandovski",
      "body": "Sure @AvinashGupta "
    },
    {
      "id": 1215604094,
      "user": "ZoranPandovski",
      "body": "Hi @AvinashGupta did you start on this?"
    },
    {
      "id": 1215606719,
      "user": "AvinashGupta",
      "body": "Hi @ZoranPandovski, yes have started on it"
    },
    {
      "id": 1301897237,
      "user": "ianu82",
      "body": "Hey @AvinashGupta, did you finish InfluxDB support please? \r\n\r\n@ZoranPandovski: checking on status."
    }
  ],
  "text_context": "# Couchbase and InfluxDB support\n\n- I use Couchbase as my MongoDB\r\n- InfluxDB is for my historical market data,\n\n@ceddybi In case you are interested in contributing connector we are happy to guide you :raised_hands: \n\nI can take this one. Will start with InfluxDB as I've seen it mentioned multiple times ðŸ˜„ \n\n@pixpack FYI we will move the datasources to the mindsdb repository. If you have already started on this please check https://github.com/mindsdb/mindsdb/pull/2060(still WIP) but you will know what structure to follow. \n\n@ZoranPandovski yes I remember you mentioning this. I actually noticed that there already is an InfluxDB datasource but it uses an old influxDB client library that works only for InfluxDB 1.7 and earlier. I will make the update and create the PR in the core repository.\n\n@ZoranPandovski Can I take up the InfluxDB integration?\n\n@ZoranPandovski Can I work on integrating InfluxDB?\n\nSure @AvinashGupta \n\nHi @AvinashGupta did you start on this?\n\nHi @ZoranPandovski, yes have started on it\n\nHey @AvinashGupta, did you finish InfluxDB support please? \r\n\r\n@ZoranPandovski: checking on status.",
  "pr_link": "https://github.com/mindsdb/mindsdb/pull/2060",
  "code_context": [
    {
      "filename": "mindsdb/api/mysql/mysql_proxy/datahub/datanodes/mindsdb_datanode.py",
      "content": "import json\nimport copy\nfrom datetime import datetime\n\nfrom lightwood.api import dtype\nimport pandas as pd\nimport numpy as np\n\nfrom mindsdb.api.mysql.mysql_proxy.datahub.datanodes.datanode import DataNode\nfrom mindsdb.api.mysql.mysql_proxy.utilities.sql import query_df\nfrom mindsdb.api.mysql.mysql_proxy.utilities.functions import get_column_in_case\nfrom mindsdb.integrations.clickhouse.clickhouse import Clickhouse\nfrom mindsdb.integrations.mssql.mssql import MSSQL\nfrom mindsdb.utilities.functions import cast_row_types\nfrom mindsdb.utilities.config import Config\n\n\nclass NumpyJSONEncoder(json.JSONEncoder):\n    \"\"\"\n    Use this encoder to avoid\n    \"TypeError: Object of type float32 is not JSON serializable\"\n\n    Example:\n    x = np.float32(5)\n    json.dumps(x, cls=NumpyJSONEncoder)\n    \"\"\"\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, (np.float, np.float32, np.float64)):\n            return float(obj)\n        else:\n            return super().default(obj)\n\n\nclass MindsDBDataNode(DataNode):\n    type = 'mindsdb'\n\n    def __init__(self, model_interface, data_store, integration_controller):\n        self.config = Config()\n        self.model_interface = model_interface\n        self.data_store = data_store\n        self.integration_controller = integration_controller\n\n    def get_tables(self):\n        models = self.model_interface.get_models()\n        models = [x['name'] for x in models if x['status'] == 'complete']\n        models += ['predictors', 'commands', 'datasources']\n\n        return models\n\n    def has_table(self, table):\n        return table in self.get_tables()\n\n    def _get_model_columns(self, table_name):\n        model = self.model_interface.get_model_data(name=table_name)\n        dtype_dict = model.get('dtype_dict')\n        if isinstance(dtype_dict, dict) is False:\n            return []\n        columns = []\n        columns += list(dtype_dict.keys())\n        predict = model['predict']\n        if not isinstance(predict, list):\n            predict = [predict]\n        columns += [f'{x}_original' for x in predict]\n        for col in predict:\n            if dtype_dict[col] in (dtype.integer, dtype.float):\n                columns += [f\"{col}_min\", f\"{col}_max\"]\n            columns += [f\"{col}_confidence\"]\n            columns += [f\"{col}_explain\"]\n        return columns\n\n    def get_table_columns(self, table):\n        if table == 'predictors':\n            return ['name', 'status', 'accuracy', 'predict', 'update_status',\n                    'mindsdb_version', 'error', 'select_data_query',\n                    'training_options']\n        if table == 'commands':\n            return ['command']\n        if table == 'datasources':\n            return ['name', 'database_type', 'host', 'port', 'user']\n\n        columns = []\n\n        if table in [x['name'] for x in self.model_interface.get_models()]:\n            columns = self._get_model_columns(table)\n            columns += ['when_data', 'select_data_query']\n\n        return columns\n\n    def _select_predictors(self):\n        models = self.model_interface.get_models()\n        columns = ['name', 'status', 'accuracy', 'predict', 'update_status',\n                   'mindsdb_version', 'error', 'select_data_query',\n                   'training_options']\n        return pd.DataFrame([[\n            x['name'],\n            x['status'],\n            str(x['accuracy']) if x['accuracy'] is not None else None,\n            ', '.join(x['predict']) if isinstance(x['predict'], list) else x['predict'],\n            x['update'],\n            x['mindsdb_version'],\n            x['error'],\n            '',\n            ''   # TODO\n        ] for x in models], columns=columns)\n\n    def _select_integrations(self):\n        integrations = self.integration_controller.get_all()\n        result = [\n            [ds_name, ds_meta.get('type'), ds_meta.get('host'), ds_meta.get('port'), ds_meta.get('user')]\n            for ds_name, ds_meta in integrations.items()\n        ]\n        return pd.DataFrame(\n            result,\n            columns=['name', 'database_type', 'host', 'port', 'user']\n        )\n\n    def delete_predictor(self, name):\n        self.model_interface.delete_model(name)\n\n    def get_predictors(self, mindsdb_sql_query):\n        predictors_df = self._select_predictors()\n\n        try:\n            result_df = query_df(predictors_df, mindsdb_sql_query)\n        except Exception as e:\n            print(f'Exception! {e}')\n            return [], []\n\n        # FIXME https://github.com/mindsdb/dfsql/issues/38\n        # TODO remove it whem wll be sure query_df do properly casting\n        # result_df = result_df.where(pd.notnull(result_df), '')\n\n        return result_df.to_dict(orient='records'), list(result_df.columns)\n\n    def get_integrations(self, mindsdb_sql_query):\n        datasources_df = self._select_integrations()\n        try:\n            result_df = query_df(datasources_df, mindsdb_sql_query)\n        except Exception as e:\n            print(f'Exception! {e}')\n            return [], []\n        return result_df.to_dict(orient='records'), list(result_df.columns)\n\n    def select(self, table, columns=None, where=None, where_data=None, order_by=None, group_by=None, integration_name=None, integration_type=None):\n        ''' NOTE WHERE statements can be just $eq joined with 'and'\n        '''\n        if table == 'predictors':\n            return self._select_predictors()\n        if table == 'commands':\n            return []\n        if table == 'datasources':\n            return self._select_datasources()\n\n        original_when_data = None\n        if 'when_data' in where_data:\n            if len(where_data) > 1:\n                raise ValueError(\"Should not be used any other keys in 'where', if 'when_data' used\")\n            try:\n                original_when_data = where_data['when_data']\n                where_data = json.loads(where_data['when_data'])\n                if isinstance(where_data, list) is False:\n                    where_data = [where_data]\n            except Exception:\n                raise ValueError(f'''Error while parse 'when_data'=\"{where_data}\"''')\n\n        select_data_query = None\n        if integration_name is not None and 'select_data_query' in where_data:\n            select_data_query = where_data['select_data_query']\n            del where_data['select_data_query']\n\n            integration_data = self.integration_controller.get(integration_name)\n            if integration_type == 'clickhouse':\n                ch = Clickhouse(self.config, integration_name, integration_data)\n                res = ch._query(select_data_query.strip(' ;\\n') + ' FORMAT JSON')\n                data = res.json()['data']\n            elif integration_type == 'mssql':\n                mssql = MSSQL(self.config, integration_name, integration_data)\n                data = mssql._query(select_data_query, fetch=True)\n            else:\n                raise Exception(f'Unknown database type: {integration_type}')\n\n            where_data = data\n\n        new_where = {}\n        if where_data is None:\n            for key, value in where_data.items():\n                if isinstance(value, dict) is False or len(value.keys()) != 1 or list(value.keys())[0] != '$eq':\n                    # TODO value should be just string or number\n                    raise Exception()\n                new_where[key] = value['$eq']\n\n            if len(new_where) == 0:\n                return []\n\n            where_data = [new_where]\n\n        if isinstance(where_data, dict):\n            where_data = [where_data]\n\n        model = self.model_interface.get_model_data(name=table)\n        columns = list(model['dtype_dict'].keys())\n\n        # cast where_data column to case of original predicto column\n        if len(where_data) > 0:\n            row = where_data[0]\n            col_name_map = {}\n            for i, col_name in enumerate(row):\n                if col_name in ('__mindsdb_row_id', '__mdb_make_predictions'):\n                    continue\n                new_col_name = get_column_in_case(columns, col_name)\n                if new_col_name is not None and col_name != new_col_name:\n                    col_name_map[col_name] = new_col_name\n            if len(col_name_map) > 0:\n                for row in where_data:\n                    for old_col_name, new_col_name in col_name_map.items():\n                        row[new_col_name] = row[old_col_name]\n                        del row[old_col_name]\n\n        predicted_columns = model['predict']\n        if not isinstance(predicted_columns, list):\n            predicted_columns = [predicted_columns]\n\n        original_target_values = {}\n        for col in predicted_columns:\n            if where_data is not None:\n                if col in where_data:\n                    original_target_values[col + '_original'] = list(where_data[col])\n                else:\n                    original_target_values[col + '_original'] = [None] * len(where_data)\n            else:\n                original_target_values[col + '_original'] = [None]\n\n        pred_dicts, explanations = self.model_interface.predict(table, where_data, 'dict&explain')\n\n        # transform predictions to more convenient view\n        new_pred_dicts = []\n        for row in pred_dicts:\n            new_row = {}\n            for key in row:\n                new_row.update(row[key])\n                new_row[key] = new_row['predicted_value']\n            del new_row['predicted_value']\n            new_pred_dicts.append(new_row)\n        pred_dicts = new_pred_dicts\n\n        timeseries_settings = model['problem_definition']['timeseries_settings']\n\n        if timeseries_settings['is_timeseries'] is True:\n            __mdb_make_predictions = set([row.get('__mdb_make_predictions', True) for row in where_data]) == {True}\n\n            predict = model['predict']\n            group_by = timeseries_settings['group_by'] or []\n            order_by_column = timeseries_settings['order_by'][0]\n            horizon = timeseries_settings['horizon']\n\n            groups = set()\n            for row in pred_dicts:\n                groups.add(\n                    tuple([row[x] for x in group_by])\n                )\n\n            # split rows by groups\n            rows_by_groups = {}\n            for group in groups:\n                rows_by_groups[group] = {\n                    'rows': [],\n                    'explanations': []\n                }\n                for row_index, row in enumerate(pred_dicts):\n                    is_wrong_group = False\n                    for i, group_by_key in enumerate(group_by):\n                        if row[group_by_key] != group[i]:\n                            is_wrong_group = True\n                            break\n                    if not is_wrong_group:\n                        rows_by_groups[group]['rows'].append(row)\n                        rows_by_groups[group]['explanations'].append(explanations[row_index])\n\n            for group, data in rows_by_groups.items():\n                rows = data['rows']\n                explanations = data['explanations']\n\n                if len(rows) == 0:\n                    break\n\n                for row in rows:\n                    predictions = row[predict]\n                    if isinstance(predictions, list) is False:\n                        predictions = [predictions]\n\n                    date_values = row[order_by_column]\n                    if isinstance(date_values, list) is False:\n                        date_values = [date_values]\n\n                for i in range(len(rows) - 1):\n                    if horizon > 1:\n                        rows[i][predict] = rows[i][predict][0]\n                        rows[i][order_by_column] = rows[i][order_by_column][0]\n                    for col in ('predicted_value', 'confidence', 'confidence_lower_bound', 'confidence_upper_bound'):\n                        if horizon > 1:\n                            explanations[i][predict][col] = explanations[i][predict][col][0]\n\n                last_row = rows.pop()\n                last_explanation = explanations.pop()\n                for i in range(horizon):\n                    new_row = copy.deepcopy(last_row)\n                    if horizon > 1:\n                        new_row[predict] = new_row[predict][i]\n                        new_row[order_by_column] = new_row[order_by_column][i]\n                    if '__mindsdb_row_id' in new_row and (i > 0 or __mdb_make_predictions is False):\n                        new_row['__mindsdb_row_id'] = None\n                    rows.append(new_row)\n\n                    new_explanation = copy.deepcopy(last_explanation)\n                    for col in ('predicted_value', 'confidence', 'confidence_lower_bound', 'confidence_upper_bound'):\n                        if horizon > 1:\n                            new_explanation[predict][col] = new_explanation[predict][col][i]\n                    if i != 0:\n                        new_explanation[predict]['anomaly'] = None\n                        new_explanation[predict]['truth'] = None\n                    explanations.append(new_explanation)\n\n            pred_dicts = []\n            explanations = []\n            for group, data in rows_by_groups.items():\n                pred_dicts.extend(data['rows'])\n                explanations.extend(data['explanations'])\n\n            original_target_values[f'{predict}_original'] = []\n            for i in range(len(pred_dicts)):\n                original_target_values[f'{predict}_original'].append(explanations[i][predict].get('truth', None))\n\n            if model['dtypes'][order_by_column] == dtype.date:\n                for row in pred_dicts:\n                    if isinstance(row[order_by_column], (int, float)):\n                        row[order_by_column] = str(datetime.fromtimestamp(row[order_by_column]).date())\n            elif model['dtypes'][order_by_column] == dtype.datetime:\n                for row in pred_dicts:\n                    if isinstance(row[order_by_column], (int, float)):\n                        row[order_by_column] = str(datetime.fromtimestamp(row[order_by_column]))\n\n        keys = [x for x in pred_dicts[0] if x in columns]\n        min_max_keys = []\n        for col in predicted_columns:\n            if model['dtype_dict'][col] in (dtype.integer, dtype.float):\n                min_max_keys.append(col)\n\n        data = []\n        explains = []\n        keys_to_save = [*keys, '__mindsdb_row_id', 'select_data_query', 'when_data']\n        for i, el in enumerate(pred_dicts):\n            data.append({key: el.get(key) for key in keys_to_save})\n            explains.append(explanations[i])\n\n        for i, row in enumerate(data):\n            cast_row_types(row, model['dtype_dict'])\n\n            row['select_data_query'] = select_data_query\n            row['when_data'] = original_when_data\n\n            for k in original_target_values:\n                try:\n                    row[k] = original_target_values[k][i]\n                except Exception:\n                    row[k] = None\n\n            for column_name in columns:\n                if column_name not in row:\n                    row[column_name] = None\n\n            explanation = explains[i]\n            for key in predicted_columns:\n                row[key + '_confidence'] = explanation[key]['confidence']\n                row[key + '_explain'] = json.dumps(explanation[key], cls=NumpyJSONEncoder, ensure_ascii=False)\n                if 'anomaly' in explanation[key]:\n                    row[key + '_anomaly'] = explanation[key]['anomaly']\n            for key in min_max_keys:\n                if 'confidence_lower_bound' in explanation[key]:\n                    row[key + '_min'] = explanation[key]['confidence_lower_bound']\n                if 'confidence_upper_bound' in explanation[key]:\n                    row[key + '_max'] = explanation[key]['confidence_upper_bound']\n\n        return data\n"
    },
    {
      "filename": "mindsdb/integrations/__init__.py",
      "content": "from .clickhouse.clickhouse import ClickhouseConnectionChecker\nfrom .mariadb.mariadb import MariaDS\nfrom .mongodb.mongodb import MongoConnectionChecker\nfrom .mssql.mssql import MSSQLConnectionChecker\nfrom .mysql.mysql import MySqlDS\nfrom .postgres.postgres import PostgresDS\nfrom .redis.redisdb import RedisConnectionChecker\nfrom .kafka.kafkadb import KafkaConnectionChecker\nfrom .snowflake.snowflake import SnowflakeConnectionChecker\nfrom .trinodb.trinodb import TrinodbConnectionChecker\n\ntry:\n    from .scylladb.scylladb import ScyllaDBConnectionChecker\nexcept ImportError:\n    ScyllaDBConnectionChecker = None\ntry:\n    from .cassandra.cassandra import CassandraConnectionChecker\nexcept ImportError:\n    CassandraConnectionChecker = None\n\n\nCHECKERS = {\n    \"clickhouse\": ClickhouseConnectionChecker,\n    \"mariadb\": MariaDS,\n    \"mongodb\": MongoConnectionChecker,\n    \"mssql\": MSSQLConnectionChecker,\n    \"mysql\": MySqlDS,\n    \"singlestore\": MySqlDS,\n    \"postgres\": PostgresDS,\n    \"cockroachdb\": PostgresDS,\n    \"redis\": RedisConnectionChecker,\n    \"kafka\": KafkaConnectionChecker,\n    \"snowflake\": SnowflakeConnectionChecker,\n    \"trinodb\": TrinodbConnectionChecker\n}\n\n\nif ScyllaDBConnectionChecker is not None:\n    CHECKERS['scylladb'] = ScyllaDBConnectionChecker\n\nif CassandraConnectionChecker is not None:\n    CHECKERS['cassandra'] = CassandraConnectionChecker\n"
    },
    {
      "filename": "mindsdb/integrations/data_source.py",
      "content": "import json\nimport ast\nfrom copy import deepcopy\n\nimport pandas as pd\nimport numpy as np\nimport moz_sql_parser\nfrom moz_sql_parser.keywords import binary_ops\nimport traceback\n\n\ndef try_convert_to_dict(val):\n    if isinstance(val, dict):\n        return val\n    if pd.notnull(val):\n        try:\n            obj = json.loads(val)\n            if isinstance(obj, dict):\n                return obj\n            else:\n                raise Exception('Not a json dictionary (could be an int because json.loads is weird)!')\n        except Exception as e:\n            obj = ast.literal_eval(val)\n            if isinstance(obj, dict):\n                return obj\n            else:\n                raise Exception('Expression failed to evaluate to a dictionary')\n            return obj\n    else:\n        return {}\n\n\ndef unnest_df(df):\n    original_columns = df.columns\n    unnested = 0\n    for col in original_columns:\n        try:\n            json_col = df[col].apply(try_convert_to_dict)\n            if np.sum(len(x) for x in json_col) == 0:\n                raise Exception('Empty column !')\n        except Exception as e:\n            continue\n\n        unnested += 1\n        unnested_df = pd.json_normalize(json_col)\n        unnested_df.columns = [col + '.' + str(subcol) for subcol in unnested_df.columns]\n        df = df.drop(columns=[col])\n\n        for unnested_col in unnested_df.columns:\n            df[unnested_col] = unnested_df[unnested_col]\n\n    return df, unnested\n\n\ndef unnest(df, col_map):\n    df, unnested = unnest_df(df)\n    if unnested > 0:\n        col_map = {}\n        for col in df.columns:\n            col_map[col] = col\n    return df, col_map\n\nclass DataSource:\n    def __init__(self, df=None, query=None):\n        if type(self) is DataSource and df is None:\n            raise Exception('When you\\'re instantiating DataSource, you must provide :param df:')\n\n        self._query = query\n\n        if df is not None:\n            self._internal_df = df\n            self._internal_col_map = self._make_colmap(df)\n            self._internal_df, self._internal_col_map = unnest(self._internal_df, self._internal_col_map)\n        else:\n            self._internal_df = None\n            self._internal_col_map = None\n\n        self.data_types = {}\n        self.data_subtypes = {}\n\n    def __len__(self):\n        return len(self.df)\n\n    def _make_colmap(self, df):\n        col_map = {}\n        for col in df.columns:\n            col_map[col] = col\n        return col_map\n\n    def _extract_and_map(self):\n        if self._internal_df is None:\n            self._internal_df, self._internal_col_map = self.query(self._query)\n            self._internal_df, self._internal_col_map = unnest(self._internal_df, self._internal_col_map)\n\n    @property\n    def df(self):\n        self._extract_and_map()\n        return self._internal_df\n\n    @df.setter\n    def df(self, df):\n        self._internal_df = df\n\n    @property\n    def _col_map(self):\n        self._extract_and_map()\n        return self._internal_col_map\n\n    @_col_map.setter\n    def col_map(self, _col_map):\n        self._internal_col_map = _col_map\n\n    def _set_df(self, df, col_map):\n        self._internal_df = df\n        self._col_map = col_map\n\n    def drop_columns(self, column_list):\n        \"\"\"\n        Drop columns by original names\n\n        :param column_list: a list of columns that you want to drop\n        \"\"\"\n        columns_to_drop = []\n\n        for col in column_list:\n            if col not in self._col_map:\n                columns_to_drop.append(col)\n            else:\n                columns_to_drop.append(self._col_map[col])\n\n        self._internal_df.drop(columns=columns_to_drop, inplace=True)\n\n    def query(self, q=None):\n        \"\"\"\n        :param q: a query specific to type of datasource\n        Datasources must override this method to return pandas.DataFrame\n        based on :param q:\n        e.g. for MySqlDS :param q: must be a SQL query\n             for MongoDS :param q: must be a dictionary\n\n        :return: tuple(pandas.DataFrame, dict)\n        \"\"\"\n\n        # If it's not a subclass of DataSource, then dataframe was provided\n        # in the constructor and we just return it\n        if type(self) is DataSource:\n            return self._internal_df, self._internal_col_map\n\n        # If it is a subclass of DataSource, then this method must be overriden\n        else:\n            raise NotImplementedError('You must override DataSource.query')\n\n    def _filter_df(self, raw_condition, df):\n        \"\"\"Convert filter conditions to a paticular\n        DataFrame instance\"\"\"\n        col, cond, val = raw_condition\n        cond = cond.lower()\n        df = df[df[col].notnull()]\n\n        if cond == '>':\n            df = df[pd.to_numeric(df[col], errors='coerce') > val]\n        if cond == '<':\n            df = df[pd.to_numeric(df[col], errors='coerce') < val]\n        if cond == 'like':\n            df = df[df[col].apply(str).str.contains(str(val).replace(\"%\", \"\"))]\n        if cond == '=':\n            df = df[( df[col] == val ) | ( df[col] == str(val) )]\n        if cond == '!=':\n            df = df[( df[col] != val ) & ( df[col] != str(val) )]\n\n        return df\n\n    def filter(self, where=None, limit=None, get_col_map=False):\n        df = self.df\n        if where:\n            for cond in where:\n                df = self._filter_df(cond, df)\n\n        return df.head(limit) if limit else df\n\n    def __getstate__(self):\n        return self.__dict__\n\n    def __setstate__(self, d):\n        self.__dict__.update(d)\n\n    def __getattr__(self, attr):\n        \"\"\"\n        Map all other functions to the DataFrame\n        \"\"\"\n        try:\n            return super().__getattribute__(attr)\n        except AttributeError:\n            return getattr(self._internal_df, attr)\n\n    def __getitem__(self, key):\n        \"\"\"\n        Map all other items to the DataFrame\n        \"\"\"\n        return self.df.__getitem__(key)\n\n    def __setitem__(self, key, value):\n        \"\"\"\n        Support item assignment, mapped to DataFrame\n        \"\"\"\n        self.df.__setitem__(key, value)\n\n    def name(self):\n        return 'DataFrame'\n\n\nclass SQLDataSource(DataSource):\n    def __init__(self, query):\n        super().__init__(query=query)\n\n    def filter(self, where=None, limit=None, get_col_map=False):\n        try:\n            parsed_query = moz_sql_parser.parse(self._query.replace('FORMAT JSON', ''))\n\n            modified_columns = []\n            for col, op, value in where or []:\n                past_where_clause = parsed_query.get('where', {})\n\n                op = op.lower()\n                op_json = binary_ops.get(op, None)\n\n                if op_json is None:\n                    print(f\"Operator: {op} not found in the sql parser operator list\\n Using it anyway.\")\n                    op_json = op\n\n                if op == 'like':\n                    value = '%' + value.strip('%') + '%'\n                    if 'clickhouse' in self.name().lower():\n                        col = f'toString({col})'\n                    elif 'postgres' in self.name().lower():\n                        col = f'{col}::text'\n                    elif 'mariadb' in self.name().lower() or 'mysql' in self.name().lower() or 'mssql' in self.name().lower():\n                        col = f'CAST({col} AS TEXT)'\n\n                modified_columns.append(col)\n\n                where_clause = {op_json: [col, value]}\n\n                if len(past_where_clause) > 0:\n                    where_clause = {'and': [where_clause, past_where_clause]}\n\n                parsed_query['where'] = where_clause\n\n            if limit is not None:\n                parsed_query['limit'] = limit\n\n            query = moz_sql_parser.format(parsed_query)\n            query = query.replace('\"', \"'\")\n            query = query.replace(\"'.'\",\".\")\n\n            for col in modified_columns:\n                if f\"'{col}'\" in query:\n                    query = query.replace(f\"'{col}'\", col)\n\n            df, col_map = self.query(query)\n            df, col_map = unnest(df, col_map)\n            if get_col_map:\n                return df, col_map\n            else:\n                return df\n\n        except Exception as e:\n            print(traceback.format_exc())\n            print('Failed to filter using SQL: ', e)\n            return super().filter(where=where, limit=limit, get_col_map=get_col_map)\n\n    @property\n    def _col_map(self):\n        if self._internal_col_map is None:\n            _, self._internal_col_map = self.filter(where=[], limit=200, get_col_map=True)\n        return self._internal_col_map\n\n    def name(self):\n        raise NotImplementedError\n\n    def get_columns(self):\n        original_query = self._query.strip(' ;\\t\\n')\n        _result, columns = self.query(f\"select * from ({original_query}) as a limit 1\")\n        columns = list(columns.keys())\n        return columns\n\n    def get_row_count(self):\n        original_query = self._query.strip(' ;\\t\\n')\n        result, _columns = self.query(f\"select count(1) as count from ({original_query}) as a\")\n        return int(result['count'][0])"
    },
    {
      "filename": "mindsdb/integrations/mariadb/mariadb.py",
      "content": "import pandas as pd\nfrom pandas.io.sql import DatabaseError \nimport mysql.connector as mysqlc\n\nfrom mindsdb.integrations.data_source import SQLDataSource\nfrom lightwood.api import dtype\nfrom mindsdb.utilities.log import log\n\n\nclass MariaDS(SQLDataSource):\n    \"\"\"\n    SQL Datasource class used for connections to MariaDB\n    \"\"\"\n\n    def __init__(self, query=None, database='mysql', host='localhost',\n                 port=3306, user='root', password='', ssl=None, \n                 ssl_ca=None, ssl_cert=None, ssl_key=None, publish=None, \n                 type='mariadb', integrations_name=None):\n        super().__init__(query)\n        self.database = database\n        self.host = host\n        self.port = int(port)\n        self.user = user\n        self.password = password\n        self.ssl = ssl\n        self.ssl_ca = ssl_ca\n        self.ssl_cert = ssl_cert\n        self.ssl_key = ssl_key\n        \n    def _get_connection(self):\n        \"\"\"\n        TODO: re use the same connection instead of open/close per query\n        \"\"\"\n        config = {\n            \"host\": self.host,\n            \"port\": self.port,\n            \"user\": self.user,\n            \"password\": self.password,\n            \"database\": self.database\n        }\n        if self.ssl is True:\n            config['client_flags'] = [mysqlc.constants.ClientFlag.SSL]\n            if self.ssl_ca is not None:\n                config[\"ssl_ca\"] = self.ssl_ca\n            if self.ssl_cert is not None:\n                config[\"ssl_cert\"] = self.ssl_cert\n            if self.ssl_key is not None:\n                config[\"ssl_key\"] = self.ssl_key\n        return mysqlc.connect(**config)\n\n    def query(self, q):\n        try:\n            connection = self._get_connection()\n            with connection as cnn:\n                df = pd.read_sql(q, con=cnn)\n        except mysqlc.Error as err:\n            log.error(f'Something went wrong: {err}')\n        return df, self._make_colmap(df)\n\n    def check_connection(self):\n        try:\n            connection = self._get_connection()\n            with connection as cnn:\n                connected = cnn.is_connected()\n        except mysqlc.Error as err:\n            log.error(f'Something went wrong: {err}')\n        return connected\n\n    def name(self):\n        return 'MariaDB - {}'.format(self._query)\n\n"
    },
    {
      "filename": "mindsdb/integrations/mysql/mysql.py",
      "content": "from contextlib import closing\nimport mysql.connector as mysqlc\nfrom mindsdb.integrations.data_source import SQLDataSource\n\nfrom lightwood.api import dtype\nfrom mindsdb.utilities.log import log\n\n\nclass MySqlDS(SQLDataSource):\n    \"\"\"\n    SQL Datasource class used for connections to MySQL\n    TODO: Combine MySqlDS and MariaDS\n    \"\"\"\n    def __init__(self, **kwargs):\n        self.host = kwargs.get('host')\n        self.port = kwargs.get('port')\n        self.user = kwargs.get('user')\n        self.password = kwargs.get('password')\n        self.ssl = kwargs.get('ssl')\n        self.ssl_ca = kwargs.get('ssl_ca')\n        self.ssl_cert = kwargs.get('ssl_cert')\n        self.ssl_key = kwargs.get('ssl_key')\n\n    def _get_connection(self):\n        \"\"\"\n        TODO: re use the same connection instead of open/close per query\n        \"\"\"\n        config = {\n            \"host\": self.host,\n            \"port\": self.port,\n            \"user\": self.user,\n            \"password\": self.password\n        }\n        if self.ssl is True:\n            config['client_flags'] = [mysqlc.constants.ClientFlag.SSL]\n            if self.ssl_ca is not None:\n                config[\"ssl_ca\"] = self.ssl_ca\n            if self.ssl_cert is not None:\n                config[\"ssl_cert\"] = self.ssl_cert\n            if self.ssl_key is not None:\n                config[\"ssl_key\"] = self.ssl_key\n        return mysqlc.connect(**config)\n\n    def query(self, q):\n        try:\n            connection = self._get_connection()\n            with connection as cnn:\n                df = pd.read_sql(q, con=cnn)\n        except mysqlc.Error as err:\n            log.error(f'Something went wrong: {err}')\n        return df, self._make_colmap(df)\n\n    def check_connection(self):\n        try:\n            connection = self._get_connection()\n            with connection as cnn:\n                connected = cnn.is_connected()\n        except mysqlc.Error as err:\n            log.error(f'Something went wrong: {err}')\n        return connected\n\n"
    },
    {
      "filename": "mindsdb/integrations/postgres/postgres.py",
      "content": "import pandas as pd\nfrom psycopg_pool import ConnectionPool\nfrom mindsdb_datasources.datasources.data_source import SQLDataSource\nfrom mindsdb.utilities.log import log\n\n\nclass PostgresDS(SQLDataSource):\n    def __init__(self, **kwargs):\n        self.host = kwargs.get('host')\n        self.port = kwargs.get('port')\n        self.user = kwargs.get('user')\n        self.password = kwargs.get('password')\n        self.database = kwargs.get('database')\n\n    def _get_connection(self):\n        conn_pool = ConnectionPool(f'host={self.host} port={self.port} dbname={self.database} user={self.user} password={self.password}')\n        return conn_pool\n\n    def check_connection(self):\n        try:\n            conection = self._get_connection()\n            with conection.getconn() as con:\n                cur = con.cursor()\n                cur.execute('select 1;')\n            connected = True\n        except Exception as e:\n            connected = False\n        return connected\n\n    def query(self, q):\n        con = self._get_connection()\n        with conection.getconn() as con:\n            df = pd.read_sql(q, con=con)\n            df.columns = [x if isinstance(x, str) else x.decode('utf-8') for x in df.columns]\n            for col_name in df.columns:\n                try:\n                    df[col_name] = df[col_name].apply(lambda x: x if isinstance(x, str) else x.decode('utf-8'))\n                except Exception:\n                    pass\n            return df, self._make_colmap(df)\n\n    def name(self):\n        return 'Postgres - {}'.format(self._query)"
    },
    {
      "filename": "mindsdb/interfaces/database/database.py",
      "content": "from mindsdb.integrations.clickhouse.clickhouse import Clickhouse\nfrom mindsdb.integrations.mssql.mssql import MSSQL\nfrom mindsdb.integrations.mongodb.mongodb import MongoDB\nfrom mindsdb.integrations.redis.redisdb import Redis\nfrom mindsdb.integrations.kafka.kafkadb import Kafka\nfrom mindsdb.utilities.log import log as logger\nfrom mindsdb.utilities.config import Config\nfrom mindsdb.interfaces.database.integrations import IntegrationController\nfrom mindsdb.utilities.with_kwargs_wrapper import WithKWArgsWrapper\n\n\nclass DatabaseWrapper():\n    known_dbs = {'clickhouse': Clickhouse,\n                 'mssql': MSSQL,\n                 'mongodb': MongoDB,\n                 'redis': Redis,\n                 'kafka': Kafka}\n\n    def __init__(self, company_id):\n        self.config = Config()\n        self.company_id = company_id\n        self.integration_controller = WithKWArgsWrapper(\n            IntegrationController(), company_id=company_id\n        )\n\n    def setup_integration(self, db_alias):\n        try:\n            # If this is the name of an integration\n            integration = self._get_integration(db_alias)\n            if integration is False:\n                raise Exception(f'Unkonw database integration type for: {db_alias}')\n            if integration is not True:\n                integration.setup()\n        except Exception as e:\n            logger.warning('Failed to integrate with database ' + db_alias + f', error: {e}')\n\n    def _get_integration(self, db_alias):\n        integration = self.integration_controller.get(db_alias)\n        if integration:\n            db_type = integration['type']\n            if db_type in self.known_dbs:\n                return self.known_dbs[db_type](self.config, db_alias, integration)\n            logger.warning(f'Uknown integration type: {db_type} for database called: {db_alias}')\n            return False\n        return True\n\n    def _get_integrations(self, publish=False):\n        all_integrations = self.integration_controller.get_all()\n        if publish is True:\n            all_integrations = [\n                x for x, y in self.integration_controller.get_all().items()\n                if y.get('publish') is True\n            ]\n        else:\n            all_integrations = [x for x in self.integration_controller.get_all()]\n        integrations = [self._get_integration(x) for x in all_integrations]\n        integrations = [x for x in integrations if x is not True and x is not False]\n        return integrations\n\n    def register_predictors(self, model_data_arr, integration_name=None):\n        if integration_name is None:\n            integrations = self._get_integrations(publish=True)\n        else:\n            integration = self._get_integration(integration_name)\n            integrations = [] if isinstance(integration, bool) else [integration]\n\n        for integration in integrations:\n            if integration.check_connection():\n                try:\n                    integration.register_predictors(model_data_arr)\n                except Exception as e:\n                    logger.warning(f\"Error {e} when trying to register predictor to {integration.name}. Predictor wouldn't be registred.\")\n            else:\n                logger.warning(f\"There is no connection to {integration.name}. Predictor wouldn't be registred.\")\n\n    def unregister_predictor(self, name):\n        for integration in self._get_integrations(publish=True):\n            # FIXME\n            # !!! Integrations from config.json add to db on each start!!!!\n            if '@@@@@' in name:\n                sn = name.split('@@@@@')\n                assert len(sn) < 3  # security\n                name = sn[1]\n            if integration.check_connection():\n                integration.unregister_predictor(name)\n            else:\n                logger.warning(f\"There is no connection to {integration.name}. predictor wouldn't be unregistred\")\n\n    def check_connections(self):\n        connections = {}\n        for integration in self._get_integrations():\n            connections[integration.name] = integration.check_connection()\n\n        return connections\n"
    },
    {
      "filename": "mindsdb/interfaces/datastore/datastore.py",
      "content": "import json\nimport shutil\nimport os\nfrom pathlib import Path\n\nimport pandas as pd\nfrom mindsdb_sql import parse_sql\n\nimport mindsdb_datasources\nfrom mindsdb.__about__ import __version__ as mindsdb_version\nfrom mindsdb.interfaces.model.model_interface import ModelInterface\nfrom mindsdb.integrations.mariadb.mariadb import MariaDS\nfrom mindsdb_datasources import (\n    FileDS, ClickhouseDS, MySqlDS, PostgresDS, MSSQLDS, MongoDS,\n    SnowflakeDS, AthenaDS, CassandraDS, ScyllaDS, TrinoDS\n)\nfrom mindsdb.utilities.config import Config\nfrom mindsdb.utilities.log import log\nfrom mindsdb.utilities.json_encoder import CustomJSONEncoder\nfrom mindsdb.utilities.with_kwargs_wrapper import WithKWArgsWrapper\nfrom mindsdb.interfaces.storage.db import session, Dataset, Semaphor, Predictor, Analysis, File\nfrom mindsdb.interfaces.storage.fs import FsStore\nfrom mindsdb.interfaces.database.integrations import IntegrationController\nfrom mindsdb.interfaces.database.views import ViewController\nfrom mindsdb.api.mysql.mysql_proxy.utilities.sql import query_df\n\n\nclass QueryDS:\n    def __init__(self, query, source, source_type, company_id):\n        self.query = query\n        self.source = source\n        self.source_type = source_type\n        self.company_id = company_id\n\n    def query(self, q):\n        pass\n\n    @property\n    def df(self):\n        view_interface = WithKWArgsWrapper(\n            ViewController(),\n            company_id=self.company_id\n        )\n\n        integration_controller = WithKWArgsWrapper(\n            IntegrationController(),\n            company_id=self.company_id\n        )\n\n        data_store = WithKWArgsWrapper(\n            DataStore(),\n            company_id=self.company_id\n        )\n\n        query = self.query\n        if self.source_type == 'view_query':\n            if isinstance(query, str):\n                query = parse_sql(query, dialect='mysql')\n            query_str = str(query)\n\n            table = query.from_table.parts[-1]\n            view_metadata = view_interface.get(name=table)\n\n            integration = integration_controller.get_by_id(view_metadata['integration_id'])\n            integration_name = integration['name']\n\n            dataset_name = data_store.get_vacant_name(table)\n            data_store.save_datasource(dataset_name, integration_name, {'query': view_metadata['query']})\n            try:\n                dataset_object = data_store.get_datasource_obj(dataset_name)\n                data_df = dataset_object.df\n            finally:\n                data_store.delete_datasource(dataset_name)\n        else:\n            raise Exception(f'Unknown source_type: {self.source_type}')\n        return data_df\n\n\nclass DataStore():\n    def __init__(self):\n        self.config = Config()\n        self.fs_store = FsStore()\n        self.dir = self.config['paths']['datasources']\n        self.model_interface = ModelInterface()\n\n    def get_analysis(self, name, company_id=None):\n        dataset_record = session.query(Dataset).filter_by(company_id=company_id, name=name).first()\n        if dataset_record.analysis_id is None:\n            return None\n        analysis_record = session.query(Analysis).get(dataset_record.analysis_id)\n        if analysis_record is None:\n            return None\n        analysis = json.loads(analysis_record.analysis)\n        return analysis\n\n    def start_analysis(self, name, company_id=None):\n        dataset_record = session.query(Dataset).filter_by(company_id=company_id, name=name).first()\n        if dataset_record.analysis_id is not None:\n            return None\n\n        semaphor_record = session.query(Semaphor).filter_by(\n            company_id=company_id,\n            entity_id=dataset_record.id,\n            entity_type='dataset'\n        ).first()\n\n        if semaphor_record is None:\n            semaphor_record = Semaphor(\n                company_id=company_id,\n                entity_id=dataset_record.id,\n                entity_type='dataset',\n                action='write'\n            )\n            session.add(semaphor_record)\n            session.commit()\n        else:\n            return\n\n        try:\n            analysis = self.model_interface.analyse_dataset(\n                ds=self.get_datasource_obj(name, raw=True, company_id=company_id),\n                company_id=company_id\n            )\n            dataset_record = session.query(Dataset).filter_by(company_id=company_id, name=name).first()\n            analysis_record = Analysis(analysis=json.dumps(analysis, cls=CustomJSONEncoder))\n            session.add(analysis_record)\n            session.flush()\n            dataset_record.analysis_id = analysis_record.id\n            session.commit()\n        except Exception as e:\n            log.error(e)\n        finally:\n            semaphor_record = session.query(Semaphor).filter_by(company_id=company_id, entity_id=dataset_record.id, entity_type='dataset').first()\n            session.delete(semaphor_record)\n            session.commit()\n\n    def get_datasets(self, name=None, company_id=None):\n        dataset_arr = []\n        if name is not None:\n            dataset_record_arr = session.query(Dataset).filter_by(company_id=company_id, name=name)\n        else:\n            dataset_record_arr = session.query(Dataset).filter_by(company_id=company_id)\n        for dataset_record in dataset_record_arr:\n            try:\n                if dataset_record.data is None:\n                    continue\n                dataset = json.loads(dataset_record.data)\n                dataset['created_at'] = dataset_record.created_at\n                dataset['updated_at'] = dataset_record.updated_at\n                dataset['name'] = dataset_record.name\n                dataset['id'] = dataset_record.id\n                dataset_arr.append(dataset)\n            except Exception as e:\n                log.error(e)\n        return dataset_arr\n\n    def get_files_names(self, company_id=None):\n        \"\"\" return list of files names\n        \"\"\"\n        return [x[0] for x in session.query(File.name).filter_by(company_id=company_id)]\n\n    def get_file_meta(self, name, company_id=None):\n        file_record = session.query(File).filter_by(company_id=company_id, name=name).first()\n        if file_record is None:\n            return None\n        return {\n            'name': file_record.name,\n            'columns': file_record.columns,\n            'row_count': file_record.row_count\n        }\n\n    def get_data(self, name, where=None, limit=None, offset=None, company_id=None):\n        offset = 0 if offset is None else offset\n        ds = self.get_datasource_obj(name, company_id=company_id)\n\n        if limit is not None:\n            # @TODO Add `offset` to the `filter` method of the datasource and get rid of `offset`\n            filtered_ds = ds.filter(where=where, limit=limit + offset).iloc[offset:]\n        else:\n            filtered_ds = ds.filter(where=where)\n\n        filtered_ds = filtered_ds.where(pd.notnull(filtered_ds), None)\n        data = filtered_ds.to_dict(orient='records')\n        return {\n            'data': data,\n            'rowcount': len(ds),\n            'columns_names': list(data[0].keys())\n        }\n\n    def get_datasource(self, name, company_id=None):\n        dataset_arr = self.get_datasets(name, company_id=company_id)\n        if len(dataset_arr) == 1:\n            return dataset_arr[0]\n        # @TODO: Remove when db swithc is more stable, this should never happen, but good santiy check while this is kinda buggy\n        elif len(dataset_arr) > 1:\n            log.error('Two or more dataset with the same name, (', len(dataset_arr), ') | Full list: ', dataset_arr)\n            raise Exception('Two or more dataset with the same name')\n        return None\n\n    def delete_datasource(self, name, company_id=None):\n        dataset_record = Dataset.query.filter_by(company_id=company_id, name=name).first()\n        if not Config()[\"force_dataset_removing\"]:\n            linked_models = Predictor.query.filter_by(company_id=company_id, dataset_id=dataset_record.id).all()\n            if linked_models:\n                raise Exception(\"Can't delete {} dataset because there are next models linked to it: {}\".format(name, [model.name for model in linked_models]))\n        session.query(Semaphor).filter_by(\n            company_id=company_id, entity_id=dataset_record.id, entity_type='dataset'\n        ).delete()\n        session.delete(dataset_record)\n        session.commit()\n        self.fs_store.delete(f'datasource_{company_id}_{dataset_record.id}')  # TODO del in future\n        try:\n            shutil.rmtree(os.path.join(self.dir, f'{company_id}@@@@@{name}'))\n        except Exception:\n            pass\n\n    def get_vacant_name(self, base=None, company_id=None):\n        ''' returns name of dataset, which starts from 'base' and ds with that name is not exists yet\n        '''\n        if base is None:\n            base = 'dataset'\n        datasets = session.query(Dataset.name).filter_by(company_id=company_id).all()\n        datasets_names = [x[0] for x in datasets]\n        if base not in datasets_names:\n            return base\n        for i in range(1, 1000):\n            candidate = f'{base}_{i}'\n            if candidate not in datasets_names:\n                return candidate\n        raise Exception(f\"Can not find appropriate name for dataset '{base}'\")\n\n    def create_datasource(self, source_type, source, file_path=None, company_id=None):\n        integration_controller = IntegrationController()\n        if source_type == 'view_query':\n            dsClass = QueryDS\n            creation_info = {\n                'class': dsClass.__name__,\n                'args': [],\n                'kwargs': {\n                    'query': source['query'],\n                    'source': source['source'],   # view\n                    'source_type': source_type,\n                    'company_id': company_id\n                }\n            }\n\n            ds = dsClass(**creation_info['kwargs'])\n        elif source_type == 'file':\n            file_name = source.get('mindsdb_file_name')\n            file_record = session.query(File).filter_by(company_id=company_id, name=file_name).first()\n            if file_record is None:\n                raise Exception(f\"Cant find file '{file_name}'\")\n            self.fs_store.get(f'{company_id}@@@@@{file_name}', f'file_{company_id}_{file_record.id}', self.dir)\n            kwargs = {}\n            query = source.get('query')\n            if query is not None:\n                kwargs['query'] = query\n\n            path = Path(self.dir).joinpath(f'{company_id}@@@@@{file_name}').joinpath(file_record.source_file_path)\n\n            creation_info = {\n                'class': 'FileDS',\n                'args': [str(path)],\n                'kwargs': kwargs\n            }\n            ds = FileDS(str(path), **kwargs)\n\n        elif integration_controller.get(source_type, company_id) is not None:\n            integration = integration_controller.get(source_type, company_id)\n\n            ds_class_map = {\n                'clickhouse': ClickhouseDS,\n                'mariadb': MariaDS,\n                'mysql': MySqlDS,\n                'singlestore': MySqlDS,\n                'postgres': PostgresDS,\n                'cockroachdb': PostgresDS,\n                'mssql': MSSQLDS,\n                'mongodb': MongoDS,\n                'snowflake': SnowflakeDS,\n                'athena': AthenaDS,\n                'cassandra': CassandraDS,\n                'scylladb': ScyllaDS,\n                'trinodb': TrinoDS\n            }\n\n            try:\n                dsClass = ds_class_map[integration['type']]\n            except KeyError:\n                raise KeyError(f\"Unknown DS type: {source_type}, type is {integration['type']}\")\n\n            if dsClass is None:\n                raise Exception(f\"Unsupported dataset: {source_type}, type is {integration['type']}, please install required dependencies!\")\n\n            if integration['type'] in ['clickhouse']:\n                creation_info = {\n                    'class': dsClass.__name__,\n                    'args': [],\n                    'kwargs': {\n                        'query': source['query'],\n                        'user': integration['user'],\n                        'password': integration['password'],\n                        'host': integration['host'],\n                        'port': integration['port']\n                    }\n                }\n                ds = dsClass(**creation_info['kwargs'])\n\n            elif integration['type'] in ['mssql', 'postgres', 'cockroachdb', 'mariadb', 'mysql', 'singlestore', 'cassandra', 'scylladb']:\n                creation_info = {\n                    'class': dsClass.__name__,\n                    'args': [],\n                    'kwargs': {\n                        'query': source['query'],\n                        'user': integration['user'],\n                        'password': integration['password'],\n                        'host': integration['host'],\n                        'port': integration['port']\n                    }\n                }\n                kwargs = creation_info['kwargs']\n\n                integration_folder_name = f'integration_files_{company_id}_{integration[\"id\"]}'\n                if integration['type'] in ('mysql', 'mariadb'):\n                    kwargs['ssl'] = integration.get('ssl')\n                    kwargs['ssl_ca'] = integration.get('ssl_ca')\n                    kwargs['ssl_cert'] = integration.get('ssl_cert')\n                    kwargs['ssl_key'] = integration.get('ssl_key')\n                    for key in ['ssl_ca', 'ssl_cert', 'ssl_key']:\n                        if isinstance(kwargs[key], str) and len(kwargs[key]) > 0:\n                            kwargs[key] = os.path.join(\n                                self.integrations_dir,\n                                integration_folder_name,\n                                kwargs[key]\n                            )\n                elif integration['type'] in ('cassandra', 'scylla'):\n                    kwargs['secure_connect_bundle'] = integration.get('secure_connect_bundle')\n                    if (\n                        isinstance(kwargs['secure_connect_bundle'], str)\n                        and len(kwargs['secure_connect_bundle']) > 0\n                    ):\n                        kwargs['secure_connect_bundle'] = os.path.join(\n                            self.integrations_dir,\n                            integration_folder_name,\n                            kwargs['secure_connect_bundle']\n                        )\n\n                if 'database' in integration:\n                    kwargs['database'] = integration['database']\n\n                if 'database' in source:\n                    kwargs['database'] = source['database']\n\n                ds = dsClass(**kwargs)\n\n            elif integration['type'] == 'snowflake':\n                creation_info = {\n                    'class': dsClass.__name__,\n                    'args': [],\n                    'kwargs': {\n                        'query': source['query'],\n                        'schema': source.get('schema', integration['schema']),\n                        'warehouse': source.get('warehouse', integration['warehouse']),\n                        'database': source.get('database', integration['database']),\n                        'host': integration['host'],\n                        'password': integration['password'],\n                        'user': integration['user'],\n                        'account': integration['account']\n                    }\n                }\n\n                ds = dsClass(**creation_info['kwargs'])\n\n            elif integration['type'] == 'mongodb':\n                if isinstance(source['find'], str):\n                    source['find'] = json.loads(source['find'])\n                creation_info = {\n                    'class': dsClass.__name__,\n                    'args': [],\n                    'kwargs': {\n                        'database': source['database'],\n                        'collection': source['collection'],\n                        'query': source['find'],\n                        'user': integration['user'],\n                        'password': integration['password'],\n                        'host': integration['host'],\n                        'port': integration['port']\n                    }\n                }\n\n                ds = dsClass(**creation_info['kwargs'])\n\n            elif integration['type'] == 'athena':\n                creation_info = {\n                    'class': dsClass.__name__,\n                    'args': [],\n                    'kwargs': {\n                        'query': source['query'],\n                        'staging_dir': source['staging_dir'],\n                        'database': source['database'],\n                        'access_key': source['access_key'],\n                        'secret_key': source['secret_key'],\n                        'region_name': source['region_name']\n                    }\n                }\n\n                ds = dsClass(**creation_info['kwargs'])\n\n            elif integration['type'] == 'trinodb':\n                creation_info = {\n                    'class': dsClass.__name__,\n                    'args': [],\n                    'kwargs': {\n                        'query': source['query'],\n                        'user': integration['user'],\n                        'password': integration['password'],\n                        'host': integration['host'],\n                        'port': integration['port'],\n                        'schema': integration['schema'],\n                        'catalog': integration['catalog']\n                    }\n                }\n\n                ds = dsClass(**creation_info['kwargs'])\n        else:\n            # This probably only happens for urls\n            ds = FileDS(source)\n            creation_info = {\n                'class': 'FileDS',\n                'args': [source],\n                'kwargs': {}\n            }\n        return ds, creation_info\n\n    def save_file(self, name, file_path, file_name=None, company_id=None):\n        \"\"\" Save the file to our store\n\n            Args:\n                name (str): with that name file will be available in sql api\n                file_name (str): file name\n                file_path (str): path to the file\n                company_id (int): company id\n\n            Returns:\n                int: id of 'file' record in db\n        \"\"\"\n        if file_name is None:\n            file_name = Path(file_path).name\n\n        try:\n            ds_meta_dir = Path(self.dir).joinpath(f'{company_id}@@@@@{name}')\n            ds_meta_dir.mkdir()\n\n            source = ds_meta_dir.joinpath(file_name)\n            shutil.move(file_path, str(source))\n\n            ds = FileDS(str(source))\n            ds_meta = self._get_ds_meta(ds)\n\n            column_names = ds_meta['column_names']\n            if ds_meta['column_names'] is not None:\n                column_names = json.dumps([dict(name=x) for x in ds_meta['column_names']])\n            file_record = File(\n                name=name,\n                company_id=company_id,\n                source_file_path=file_name,\n                file_path=str(source),\n                row_count=ds_meta['row_count'],\n                columns=column_names\n            )\n            session.add(file_record)\n            session.commit()\n            self.fs_store.put(f'{company_id}@@@@@{name}', f'file_{company_id}_{file_record.id}', self.dir)\n        except Exception as e:\n            log.error(e)\n            shutil.rmtree(ds_meta_dir)\n            raise\n\n        return file_record.id\n\n    def delete_file(self, name, company_id):\n        file_record = session.query(File).filter_by(company_id=company_id, name=name).first()\n        if file_record is None:\n            return None\n        file_id = file_record.id\n        session.delete(file_record)\n        session.commit()\n        self.fs_store.delete(f'file_{company_id}_{file_id}')\n        return True\n\n    def _get_ds_meta(self, ds):\n        if hasattr(ds, 'get_columns') and hasattr(ds, 'get_row_count'):\n            try:\n                column_names = ds.get_columns()\n                row_count = ds.get_row_count()\n            except Exception:\n                df = ds.df\n                column_names = list(df.keys())\n                row_count = len(df)\n        else:\n            df = ds.df\n            column_names = list(df.keys())\n            row_count = len(df)\n        return {\n            'column_names': column_names,\n            'row_count': row_count\n        }\n\n    def save_datasource(self, name, source_type, source=None, file_path=None, company_id=None):\n        dataset_record = session.query(Dataset).filter_by(company_id=company_id, name=name).first()\n        while dataset_record is not None:\n            raise Exception(f'Dataset with name {name} already exists')\n\n        if source_type == 'views':\n            source_type = 'view_query'\n        elif source_type == 'files':\n            source_type = 'file'\n\n        try:\n            dataset_record = Dataset(\n                company_id=company_id,\n                name=name,\n                datasources_version=mindsdb_datasources.__version__,\n                mindsdb_version=mindsdb_version\n            )\n            session.add(dataset_record)\n            session.commit()\n\n            ds, creation_info = self.create_datasource(source_type, source, file_path, company_id)\n\n            ds_meta = self._get_ds_meta(ds)\n            column_names = ds_meta['column_names']\n            row_count = ds_meta['row_count']\n\n            dataset_record.ds_class = creation_info['class']\n            dataset_record.creation_info = json.dumps(creation_info)\n            dataset_record.data = json.dumps({\n                'source_type': source_type,\n                'source': source,\n                'row_count': row_count,\n                'columns': [dict(name=x) for x in column_names]\n            })\n\n            session.commit()\n\n        except Exception as e:\n            log.error(f'Error creating dataset {name}, exception: {e}')\n            try:\n                self.delete_datasource(name, company_id=company_id)\n            except Exception:\n                pass\n            raise e\n\n        return self.get_datasource_obj(name, raw=True, company_id=company_id)\n\n    def get_datasource_obj(self, name=None, id=None, raw=False, company_id=None):\n        try:\n            if name is not None:\n                dataset_record = session.query(Dataset).filter_by(company_id=company_id, name=name).first()\n            else:\n                dataset_record = session.query(Dataset).filter_by(company_id=company_id, id=id).first()\n\n            creation_info = json.loads(dataset_record.creation_info)\n            if raw:\n                return creation_info\n            else:\n                if dataset_record.ds_class == 'FileDS':\n                    file_record = session.query(File).filter_by(company_id=company_id, name=name).first()\n                    if file_record is not None:\n                        self.fs_store.get(f'{company_id}@@@@@{dataset_record.name}', f'file_{company_id}_{file_record.id}', self.dir)\n                return eval(creation_info['class'])(*creation_info['args'], **creation_info['kwargs'])\n        except Exception as e:\n            log.error(f'Error getting dataset {name}, exception: {e}')\n            return None\n"
    },
    {
      "filename": "tests/integration_tests/flows/test_mysql.py",
      "content": "import unittest\nimport inspect\nfrom pathlib import Path\nimport json\n\nimport mysql.connector\n\nfrom common import (\n    MINDSDB_DATABASE,\n    CONFIG_PATH,\n    run_environment,\n    get_all_pridict_fields,\n    check_prediction_values\n)\n\n\nTO_PREDICT = 'GDP_per_capita_USD'\n\nCONDITION = {\n    'Population': 2044147,\n    'Pop_Density': 13.9\n}\n\nTEST_DATA_TABLE = 'hdi'\nTEST_PREDICTOR_NAME = f'{TEST_DATA_TABLE}_predictor'\n\nINTEGRATION_NAME = 'default_mysql'\n\nconfig = {}\n\n\ndef query(q, as_dict=False, fetch=False):\n    con = mysql.connector.connect(\n        host=config['integrations'][INTEGRATION_NAME]['host'],\n        port=config['integrations'][INTEGRATION_NAME]['port'],\n        user=config['integrations'][INTEGRATION_NAME]['user'],\n        passwd=config['integrations'][INTEGRATION_NAME]['password'],\n        db=MINDSDB_DATABASE\n    )\n\n    cur = con.cursor(dictionary=as_dict)\n    cur.execute(q)\n    res = True\n    if fetch:\n        res = cur.fetchall()\n    con.commit()\n    con.close()\n    return res\n\n\ndef fetch(q, as_dict=True):\n    return query(q, as_dict, fetch=True)\n\n\nclass MySQLDBTest(unittest.TestCase):\n    def get_tables_in(self, schema):\n        test_tables = fetch(f'show tables from {schema}', as_dict=False)\n        return [x[0] for x in test_tables]\n\n    @classmethod\n    def setUpClass(cls):\n        run_environment(\n            apis=['mysql', 'http'],\n            override_config={\n                'integrations': {\n                    INTEGRATION_NAME: {\n                        'publish': True\n                    }\n                }\n            }\n        )\n\n        config.update(\n            json.loads(\n                Path(CONFIG_PATH).read_text()\n            )\n        )\n\n    def test_1_initial_state(self):\n        print(f'\\nExecuting {inspect.stack()[0].function}')\n\n        self.assertTrue(TEST_DATA_TABLE in self.get_tables_in('test_data'))\n\n        mindsdb_tables = self.get_tables_in(MINDSDB_DATABASE)\n\n        self.assertTrue(len(mindsdb_tables) >= 2)\n        self.assertTrue('predictors' in mindsdb_tables)\n        self.assertTrue('commands' in mindsdb_tables)\n\n        data = fetch(f'select * from {MINDSDB_DATABASE}.predictors;')\n        self.assertTrue(len(data) == 0)\n\n    def test_2_create_predictor(self):\n        print(f'\\nExecuting {inspect.stack()[0].function}')\n        print(f\"CREATE PREDICTOR {TEST_PREDICTOR_NAME} FROM {INTEGRATION_NAME} (SELECT * FROM test_data.{TEST_DATA_TABLE} LIMIT 50) PREDICT `{TO_PREDICT}`;\")\n        query(f\"CREATE PREDICTOR {TEST_PREDICTOR_NAME} FROM {INTEGRATION_NAME} (SELECT * FROM test_data.{TEST_DATA_TABLE} LIMIT 50) PREDICT `{TO_PREDICT}`;\")\n\n        print('predictor record in mindsdb.predictors')\n        res = fetch(f\"select status from {MINDSDB_DATABASE}.predictors where name = '{TEST_PREDICTOR_NAME}'\", as_dict=True)\n        self.assertTrue(len(res) == 1)\n        self.assertTrue(res[0]['status'] == 'complete')\n\n        print('predictor table in mindsdb db')\n        self.assertTrue(TEST_PREDICTOR_NAME in self.get_tables_in(MINDSDB_DATABASE))\n\n    def test_3_query_predictor(self):\n        print(f'\\nExecuting {inspect.stack()[0].function}')\n\n        fields = get_all_pridict_fields(TO_PREDICT)\n        conditions = json.dumps(CONDITION)\n        res = fetch(f\"\"\"\n            select\n                {','.join(fields)}\n            from\n                {MINDSDB_DATABASE}.{TEST_PREDICTOR_NAME}\n            where\n                when_data='{conditions}';\n        \"\"\", as_dict=True)\n\n        print('check result')\n        self.assertTrue(len(res) == 1)\n        self.assertTrue(check_prediction_values(res[0], TO_PREDICT))\n\n    def test_4_range_query(self):\n        print(f'\\nExecuting {inspect.stack()[0].function}')\n\n        res = fetch(f\"\"\"\n            select\n                *\n            from\n                {MINDSDB_DATABASE}.{TEST_PREDICTOR_NAME}\n            where\n                select_data_query='select * from test_data.{TEST_DATA_TABLE} limit 3';\n        \"\"\", as_dict=True)\n\n        print('check result')\n        self.assertTrue(len(res) == 3)\n        for r in res:\n            self.assertTrue(check_prediction_values(r, TO_PREDICT))\n\n    def test_5_delete_predictor_by_command(self):\n        print(f'\\nExecuting {inspect.stack()[0].function}')\n\n        query(f\"\"\"\n            insert into {MINDSDB_DATABASE}.commands values ('delete predictor {TEST_PREDICTOR_NAME}');\n        \"\"\")\n\n        self.assertTrue(TEST_PREDICTOR_NAME not in self.get_tables_in(MINDSDB_DATABASE))\n\n\nif __name__ == \"__main__\":\n    try:\n        unittest.main(failfast=True)\n        print('Tests passed!')\n    except Exception as e:\n        print(f'Tests Failed!\\n{e}')\n"
    }
  ]
}