{
  "repo_name": "scipy_scipy",
  "issue_id": "14471",
  "issue_description": "# methods 'revised simplex' and 'interior-point' are extremely slow?\n\nI'm currently working on a LP and want to compare different implementations of the revised simplex and IP methods to solve it.\r\n\r\nThe LP has a size of 200*200.\r\nThe problem itself could be interpreted as a transportation problem.\r\nDue to the problem structure I do have 40.000 constraints.\r\nI'm using numpy matrices as input.\r\n\r\nThe solving time of the revised simplex for this is 16 minutes.\r\nThe solving time of the interior point method is about 11 minutes.\r\n\r\nAre those old methods really that slow or am I doing something wrong?\r\n\r\nThanks in advance.\r\n\r\nI'm working on a windows machine with 16gb ram and i7.",
  "issue_comments": [
    {
      "id": 886079912,
      "user": "mdhaber",
      "body": "They might be that slow. If the problems are sparse, you can exploit that with `interior-point`. But use the new [HiGHS](https://docs.scipy.org/doc/scipy/reference/optimize.linprog-highs.html#optimize-linprog-highs) solvers, instead. They should be much faster.\r\n\r\nBy \"size of 200*200\" - and the fact that there are 40,000 constraints, it sounds like you have 40,000 variables and 40,000 constraints. Is that correct? Then the time would not be so surprising to me if the input is not sparse."
    },
    {
      "id": 886086908,
      "user": "N3UN3R",
      "body": "@mdhaber thanks for your fast response! yeah exactly.\r\nwell my constraint matrix is actually really sparse as it is a transportation problem. \r\nsparse matrices are just suppported by interior-point right? There's no documentation on which sparse matrices could be used as input though?\r\n\r\nI've just tested the highs-solvers:\r\n Solving time ipm : 0.731 s\r\n solving time ds: 0.794 s\r\n \r\n While the old methods 'interior-point' and 'revised' show that there is a redundant constraint the highs-solvers don't.\r\n Is there any reason why?\r\n \r\n Also highs-methods are implemented in just C++ right?\r\n What is about 'interior point' and 'revised' ? \r\n Is there any documentation on the algorithms time and space complexitys based on the code?\r\n \r\n \r\n"
    },
    {
      "id": 886090550,
      "user": "mdhaber",
      "body": "> sparse matrices are just supported by interior-point right? There's no documentation on which sparse matrices could be used as input though?\r\n\r\nWe should probably make it more explicit in the method-specific documentation that `A_ub` and `A_eq` can be sparse matrices. But there is a note about it in the [main `linprog` documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html).\r\n![image](https://user-images.githubusercontent.com/6570539/126877201-838f190a-42ce-4adc-a222-64a6b2458f8e.png)\r\n\r\nAnd in the [`interior-point`-specific](https://docs.scipy.org/doc/scipy/reference/optimize.linprog-interior-point.html#optimize-linprog-interior-point) documentation, it notes that in the `options` dictionary, you can specify `'sparse': True`.\r\n![image](https://user-images.githubusercontent.com/6570539/126877244-5c6ebb0d-6502-44fc-ae9d-67430cbb05ca.png)\r\n\r\nJust curious - if you'd be willing to check how long interior-point takes with sparse input, I'd be interested. \r\n\r\n> While the old methods 'interior-point' and 'revised' show that there is a redundant constraint the highs-solvers don't. Is there any reason why?\r\n\r\nThe HiGHS solvers skip the presolve procedure that is common to the older, Python linear programming solvers. HiGHS has its own presolve routines, and perhaps it doesn't check that or give a message about it.  (Removal of redundant constraints can be more expensive than if the redundant constraints were left in the problem, but it needed to be added because `'interior-point'` was really sensitive to redundancy in the problems. The HiGHS solvers might not be bothered by them.)\r\n\r\n> Also highs-methods are implemented in just C++ right?\r\n\r\nYes. See [here](https://www.maths.ed.ac.uk/hall/HiGHS/) for more information.\r\n\r\n> What is about 'interior point' and 'revised' ?\r\n\r\nPython. \r\n\r\n> Is there any documentation on the algorithms time and space complexitys based on the code?\r\n\r\nThe documentation points to references for the algorithms. Interior-point follows the reference pretty closely. Revised simplex is a little more generic, implemented based on the ideas in a textbook. I'm not sure if there will be useful information about time complexity. If I remember correctly, the worst-case asymptotic time complexity of all simplex-like methods is exponential but the actual time complexity is problem dependent and they perform quite well in practice. For interior-point, the paper might give some information about the complexity, which is theoretically polynomial - but again, actual time for a given problem depends a lot on the problem structure. \r\n\r\nI don't remember the space complexity being worse than a scalar multiple of the input for either algorithm, but perhaps I'm forgetting."
    },
    {
      "id": 886091681,
      "user": "mdhaber",
      "body": "I'm leaving this open and marking it as a good first issue: the documentation should mention that sparse arrays are accepted for `A_ub` and `A_eq`:\r\n\r\n![image](https://user-images.githubusercontent.com/6570539/126877584-93f6360f-b810-4d35-aab9-773ddd5f08ff.png)\r\n\r\nI think it would be best to just accept `2-D array or sparse matrix` for all the solvers. For `simplex` and `revised simplex`, we should convert to dense, and mention that in the documentation. This allows us to get the word out that `linprog` accepts sparse input, and at this point there's no good reason to use the old methods that don't really work with sparse matrices anyway. \r\n"
    },
    {
      "id": 886093023,
      "user": "N3UN3R",
      "body": "I've closed by accident..\r\n\r\n> We should probably make it more explicit in the method-specific documentation that A_ub and A_eq can be sparse matrices.\r\n\r\nSo do all solvers accept sparse input or just the interior-point-method?\r\n\r\n> And in the interior-point-specific documentation, it notes that in the options dictionary, you can specify 'sparse': True\r\n\r\nI will test this and report the results\r\n\r\n> Python\r\n\r\nAre 'interior point', 'revised simplex' and 'simplex' all 100 % pure python? While the documentation makes it clear that highs solvers are implemented in C++ it's unclear in which language the old methods were implemented..\r\n\r\n> The documentation points to references for the algorithms. Interior-point follows the reference pretty closely. Revised simplex is a little more generic, implemented based on the ideas in a textbook. I'm not sure if there will be useful information about time complexity. If I remember correctly, the worst-case asymptotic time complexity of all simplex-like methods is exponential but the actual time complexity is problem dependent and they perform quite well in practice. For interior-point, the paper might give some information about the complexity, which is theoretically polynomial - but again, actual time for a given problem depends a lot on the problem structure.\r\n\r\nThanks! I will have a closer look into the referenced papers..\r\n\r\n\r\n"
    },
    {
      "id": 886099503,
      "user": "mdhaber",
      "body": "> So do all solvers accept sparse input or just the interior-point-method?\r\n\r\nCurrently, the `'interior-point'` and the HiGHS solvers accept sparse input; `'revised simplex'` and `'simplex'` don't. To keep things simple in the documentation and most performant on average, I'm suggesting that all solvers _should_ accept sparse input, that we document that all solvers accept sparse input, and that we convert to dense for `'simplex'` and `'revised simplex'` (and document that this is the behavior).  \r\n\r\n> Are 'interior point', 'revised simplex' and 'simplex' all 100 % pure python?\r\n\r\nIf you consider NumPy and SciPy's LAPACK interfaces \"pure python\", then `'interior-point'` and `'simplex'` are pure python. `'revised simplex'` uses a class written in Cython to represent the inverse of the basis matrix, but is otherwise pure python. The HiGHS methods are implemented in C++, and we wrote Python wrappers."
    },
    {
      "id": 887104978,
      "user": "dcb2124",
      "body": "First time contributor here. So, if I were take this issue, I would just update the docs so that it's explicit that `linprog(method='interior-point')` takes sparse matrics for `A_ub` and `A_eq`? \r\n\r\nAnd adding the acceptance of sparse input to the other solvers would be a separate issue/feature request? "
    },
    {
      "id": 887153777,
      "user": "mdhaber",
      "body": "@dcb2124 Glad to hear you're interested!\r\n\r\n> So, if I were take this issue, I would just update the docs so that it's explicit that `linprog(method='interior-point')` takes sparse matrics for `A_ub` and `A_eq`? \r\n\r\n`linprog(method='interior-point')`, `linprog(method='highs')`, `linprog(method='highs-ds')`, and `linprog(method='highs-ipm')`, at least. \r\n\r\n> And adding the acceptance of sparse input to the other solvers would be a separate issue/feature request?\r\n\r\nIt could be. Just mentioning sparse input in the method-specific documentation for those four methods would be an improvement. But I thought it deserves to be mentioned in the top-level linprog documentation, too, since the default method `interior-point` and the new HiGHS methods all accept sparse input; only the old `simplex` and the (not often used) `revised simplex` method do not accept sparse input currently. However, there's not a good system for documenting method-specific argument types, which is why I thought it would make sense to make sure all the solvers accept sparse input. So if it were me, I would do all this in one PR."
    },
    {
      "id": 887168620,
      "user": "dcb2124",
      "body": "I'd like to take it.  How do I have this issue assigned to me?\r\n"
    },
    {
      "id": 887202913,
      "user": "mdhaber",
      "body": "We don't typically assign officially. If someone else were to see this issue, etiquette would suggest that they reach out to you before working on it themselves. But I officially assigned it to you : )"
    },
    {
      "id": 887680936,
      "user": "dcb2124",
      "body": "Thanks. I will get to this after work today."
    },
    {
      "id": 887886302,
      "user": "mdhaber",
      "body": "@mckib2 Really quick question - do you agree with the direction of [this comment](https://github.com/scipy/scipy/issues/14471#issuecomment-886099503) - `linprog` should always accept sparse input to keep documentation simple, but convert to dense before using `simplex` and `revised simplex` methods?"
    },
    {
      "id": 888806837,
      "user": "mckib2",
      "body": "> @mckib2 Really quick question - do you agree with the direction of [this comment](https://github.com/scipy/scipy/issues/14471#issuecomment-886099503) - `linprog` should always accept sparse input to keep documentation simple, but convert to dense before using `simplex` and `revised simplex` methods?\r\n\r\nYes, I like this direction.  There should probably be a warning about not using `simplex`/`revised simplex` methods with very large sparse matrices as you run the risk of running out of memory when converting to dense"
    },
    {
      "id": 890533353,
      "user": "N3UN3R",
      "body": "@mdhaber \r\nSo from my tests comparing the ip-options {sparse:True/False} I would say that the method interior-point runs about 1% faster using sparse matrices... I'd say that the speed up using sparse opition is quite low especially as I'm solving a really sparse problem.\r\n\r\nalso I was wondering if there exists a way to \"warmstart\" any of the sciPy-methods?"
    },
    {
      "id": 890557589,
      "user": "mdhaber",
      "body": "> I'd say that the speed up using sparse opition is quite low especially as I'm solving a really sparse problem.\r\n\r\nIt's possible that there's been a regression because I've noticed that on some machines. (Not that it really matters because HiGHS is the way to go.) But on other machines sparse is much faster - especially with CHOLMOD/sciki-sparse. Out of curiosity, what platform are you on? \r\n\r\nNo warmstart right now. It is already on a list of desired enhancements, although now I think we just have to wait and see if HiGHS adds it.\r\n\r\n_Update: by the way, the [original `'interior-point'` PR](https://github.com/scipy/scipy/pull/7123#issue-108883282) shows a substantial advantage using the sparse option._"
    },
    {
      "id": 890957882,
      "user": "N3UN3R",
      "body": "@mdhaber I'm working on a windows 10 Notebook, 16 GB memory, i7-8750H and SSD.\r\n\r\nThe HiGHS are difficult to compare solving times to other python implemented algorithms like for instance from the networkX-library as it's in C++ though...."
    },
    {
      "id": 891076091,
      "user": "mdhaber",
      "body": "Windows was my guess, as that's the platform I've noticed  sparse being slow (or dense being really fast) on. I'll investigate, as it could be an issue with the underlying solver (SuperLU).\r\n\r\n> The HiGHS are difficult to compare solving times to other python implemented algorithms...\r\n\r\nWall clock time is not good enough? (You are interested in something other than overall program execution time?)\r\n"
    },
    {
      "id": 891259633,
      "user": "N3UN3R",
      "body": "@mdhaber \r\n> Windows was my guess, as that's the platform I've noticed sparse being slow (or dense being really fast) on. I'll investigate, as it could be an issue with the underlying solver (SuperLU).\r\n\r\nInteresting... do you have any idea why this might be the case? \r\n\r\n> Wall clock time is not good enough? (You are interested in something other than overall program execution time?)\r\n\r\nWell I want to compare different algorithms for solving a specific problem... by using different programming languages clock time wouldn't be 'fair' as C++  for instance is a lot faster than python... or what are your thoughts on that ?"
    },
    {
      "id": 891284063,
      "user": "mdhaber",
      "body": "I think it's going to be hard to make fair comparisons of algorithms based on existing libraries because both language and implementation details (that are not specified by the algorithm) matter. (Is this an academic interest? I get the impression that most users are just interested in solving a problem as quickly as possible, and for them the wall clock time is what matters.) Also, besides the core algorithm, there are a lot of other things that run with `linprog` and other LP software that can dramatically affect the execution time of the core algorithm (presolve, basis crashing, etc.).  I'm not sure how fair of a comparison you can get, even if the underlying languages are the same.\r\n\r\n> Interesting... do you have any idea why this might be the case?\r\n\r\nWould you try passing different options for `permc_spec`? The default is \"MMD_AT_PLUS_A\" but that might be slowing things down terribly. Please try \"COLAMD\" and \"MMD_ATA\" e.g. `options={'permc_spec': 'COLAMD'}`. Also, please try turning off redundancy removal `options={'rr': False}`. \r\n\r\nI say that because I was just comparing the methods of solving a sparse, symmetric positive definite linear system that are used by the interior-point method. As a test matrix, I used [this](https://math.nist.gov/MatrixMarket/data/misc/cylshell/s2rmt3m1.html), which does not have the structure that we would get in `linprog`, but this structure is much simpler so it should only help the sparse solver. I used a random, dense, right hand side.\r\n\r\nUsing `splu(A_sparse, permc_spec='MMD_AT_PLUS_A').solve(b)`, which is essentially what happens in `linprog` when using the sparse option, it took over _30 seconds_. Using `linalg.cho_solve(linalg.cho_factor(A_dense), b)`, which is what happens when the matrix is treated as dense, it took 365 ms to solve. Even `linalg.solve(A_dense, b)` is much faster than `splu` with this `permc_spec`, at 826 ms. On the other hand, `splu(A_sparse, permc_spec=\"COLAMD\").solve(b)` takes only 111ms.\r\n\r\nThis might not be the whole story, as I'm running the fast benchmark suite with `'permc_spec':'COLAMD'` and that doesn't seem to speed it up. I'm currently running with the full suite.\r\n\r\nIn any case, if you want a more fair look at `interior-point`, SuiteSparse and the `scikit-sparse` package need to be available. Falling back on SuperLU puts the algorithm a considerably disadvantage because it can't take advantage of the matrix being symmetric positive definite. "
    },
    {
      "id": 891370812,
      "user": "mdhaber",
      "body": "More complete results are in, and for most benchmark problems that take much time at all, `sparse` is faster, and often quite a bit faster. I think that the graphs in the [original PR](https://github.com/scipy/scipy/pull/7123#issue-108883282) are still representative. "
    },
    {
      "id": 892473172,
      "user": "N3UN3R",
      "body": "> \r\n> \r\n> I think it's going to be hard to make fair comparisons of algorithms based on existing libraries because both language and implementation details (that are not specified by the algorithm) matter. (Is this an academic interest? I get the impression that most users are just interested in solving a problem as quickly as possible, and for them the wall clock time is what matters.) Also, besides the core algorithm, there are a lot of other things that run with `linprog` and other LP software that can dramatically affect the execution time of the core algorithm (presolve, basis crashing, etc.). I'm not sure how fair of a comparison you can get, even if the underlying languages are the same.\r\n\r\nyeah exactly its part of my bachelor thesis... But don't you think using algorithms that ar at least written in the same programming language makes them more comparable? \r\n\r\n> > Interesting... do you have any idea why this might be the case?\r\n> \r\n> Would you try passing different options for `permc_spec`? The default is \"MMD_AT_PLUS_A\" but that might be slowing things down terribly. Please try \"COLAMD\" and \"MMD_ATA\" e.g. `options={'permc_spec': 'COLAMD'}`. Also, please try turning off redundancy removal `options={'rr': False}`.\r\n\r\nyeah sure, I will report  the test results\r\n> \r\n> I say that because I was just comparing the methods of solving a sparse, symmetric positive definite linear system that are used by the interior-point method. As a test matrix, I used [this](https://math.nist.gov/MatrixMarket/data/misc/cylshell/s2rmt3m1.html), which does not have the structure that we would get in `linprog`, but this structure is much simpler so it should only help the sparse solver. I used a random, dense, right hand side.\r\n> \r\n> Using `splu(A_sparse, permc_spec='MMD_AT_PLUS_A').solve(b)`, which is essentially what happens in `linprog` when using the sparse option, it took over _30 seconds_. Using `linalg.cho_solve(linalg.cho_factor(A_dense), b)`, which is what happens when the matrix is treated as dense, it took 365 ms to solve. Even `linalg.solve(A_dense, b)` is much faster than `splu` with this `permc_spec`, at 826 ms. On the other hand, `splu(A_sparse, permc_spec=\"COLAMD\").solve(b)` takes only 111ms.\r\n\r\nIf I got this right you haven't tested this in the interior point method but in the underlying functions?\r\n\r\n> \r\n> In any case, if you want a more fair look at `interior-point`, SuiteSparse and the `scikit-sparse` package need to be available. Falling back on SuperLU puts the algorithm a considerably disadvantage because it can't take advantage of the matrix being symmetric positive definite.\r\n\r\n"
    },
    {
      "id": 892915196,
      "user": "mdhaber",
      "body": "> If I got this right you haven't tested this in the interior point method but in the underlying functions?\r\n\r\nI did both. I ran the `linprog` [benchmarks](http://scipy.github.io/devdocs/dev/contributor/benchmarking.html#benchmarking-with-asv) (after editing them to use `interior-point` with sparse and dense options) and found that they typically get the same (substantial) speedup for sparse matrices as they always have.  I also tested SuperLU independently a little and identified at least one case in which a very large, sparse matrix took _far_ longer to factor with SuperLU than with LAPACK, which could potentially explain `linprog` being slow for some sparse problems.\r\n\r\n> But don't you think using algorithms that ar at least written in the same programming language makes them more comparable?\r\n\r\nYes, but I'm still not sure that you can draw definitive conclusions about the speed of the underlying algorithm based on the wall clock run time of an implementation - even if the implementations are in the same language. You would be able to say more if you analyzed how the run time scaled with problem size (and fortunately, that should be mostly independent of programming language)."
    },
    {
      "id": 893390335,
      "user": "N3UN3R",
      "body": "> \r\n> \r\n> > If I got this right you haven't tested this in the interior point method but in the underlying functions?\r\n> \r\n> I did both. I ran the `linprog` [benchmarks](http://scipy.github.io/devdocs/dev/contributor/benchmarking.html#benchmarking-with-asv) (after editing them to use `interior-point` with sparse and dense options) and found that they typically get the same (substantial) speedup for sparse matrices as they always have. I also tested SuperLU independently a little and identified at least one case in which a very large, sparse matrix took _far_ longer to factor with SuperLU than with LAPACK, which could potentially explain `linprog` being slow for some sparse problems.\r\n\r\nalright.\r\nI was able to get huge speed up removing a redundant restriction. Atm I'm still testing but I will report results.\r\n\r\n\r\n\r\n> Yes, but I'm still not sure that you can draw definitive conclusions about the speed of the underlying algorithm based on the wall clock run time of an implementation - even if the implementations are in the same language. You would be able to say more if you analyzed how the run time scaled with problem size (and fortunately, that should be mostly independent of programming language).\r\n\r\nI've compared clock times as well as the runtimes for different problem sizes and did a curve fitting on the means. I'm with you that the this still doesn't give definitive insights in the algorithms themselves but I'd still say that this is the 'fairest' way to compare them. Even though implementation details could play a significant role.\r\n\r\n"
    },
    {
      "id": 893563176,
      "user": "mdhaber",
      "body": "Ok. Yes, i agree it is more fair if they are the same language. Let me know what you find out!"
    },
    {
      "id": 896124682,
      "user": "N3UN3R",
      "body": "@mdhaber \r\n\r\nSo I've tested 'ip', 'simplex' and 'revised' on scalability.\r\nAs expected 'ip' performs the best.\r\n'ip' performs about 6-7 times faster than 'simplex'.\r\n'ip' performs about 20 times faster than 'revised'.\r\n\r\nI find it really irritating that 'revised' runs slower than the 'simplex' method?\r\nAs from theory revised should be faster than the tableau method. Which is the case for 'smaller problems'.\r\n\r\nHave you observed faster running times of the tableau method before? "
    },
    {
      "id": 899342003,
      "user": "N3UN3R",
      "body": "@mdhaber \r\n\r\nSo in the 'revised' method\r\n\r\ndecomp_lu.py (lu_factor)\r\nand\r\nnumpy 'dot' function\r\n\r\nare the bottlenecks of the revised simplex which makes it up to 4 times slower than the tablau method\r\n"
    },
    {
      "id": 899519910,
      "user": "mdhaber",
      "body": "Can you be more specific? "
    },
    {
      "id": 2149697074,
      "user": "bbogo",
      "body": "May I just underline that even though \"simplex\" or \"interior-point\" are not efficient for large problems, they are still very useful from an academic perspective when learning about linear programming for the first time. I am a bit disappointed to learn that they will be removed in future versions. It would be nice to keep them there for learning purposes. "
    },
    {
      "id": 2151588008,
      "user": "ev-br",
      "body": "@bbogo as a drive-by comment, I wonder if you're interested in maintaining standalone versions of these methods? \r\nWe can probably help / advise on separating them into a separate package, and I think we can add links to that packages to the scipy docs. "
    }
  ],
  "text_context": "# methods 'revised simplex' and 'interior-point' are extremely slow?\n\nI'm currently working on a LP and want to compare different implementations of the revised simplex and IP methods to solve it.\r\n\r\nThe LP has a size of 200*200.\r\nThe problem itself could be interpreted as a transportation problem.\r\nDue to the problem structure I do have 40.000 constraints.\r\nI'm using numpy matrices as input.\r\n\r\nThe solving time of the revised simplex for this is 16 minutes.\r\nThe solving time of the interior point method is about 11 minutes.\r\n\r\nAre those old methods really that slow or am I doing something wrong?\r\n\r\nThanks in advance.\r\n\r\nI'm working on a windows machine with 16gb ram and i7.\n\nThey might be that slow. If the problems are sparse, you can exploit that with `interior-point`. But use the new [HiGHS](https://docs.scipy.org/doc/scipy/reference/optimize.linprog-highs.html#optimize-linprog-highs) solvers, instead. They should be much faster.\r\n\r\nBy \"size of 200*200\" - and the fact that there are 40,000 constraints, it sounds like you have 40,000 variables and 40,000 constraints. Is that correct? Then the time would not be so surprising to me if the input is not sparse.\n\n@mdhaber thanks for your fast response! yeah exactly.\r\nwell my constraint matrix is actually really sparse as it is a transportation problem. \r\nsparse matrices are just suppported by interior-point right? There's no documentation on which sparse matrices could be used as input though?\r\n\r\nI've just tested the highs-solvers:\r\n Solving time ipm : 0.731 s\r\n solving time ds: 0.794 s\r\n \r\n While the old methods 'interior-point' and 'revised' show that there is a redundant constraint the highs-solvers don't.\r\n Is there any reason why?\r\n \r\n Also highs-methods are implemented in just C++ right?\r\n What is about 'interior point' and 'revised' ? \r\n Is there any documentation on the algorithms time and space complexitys based on the code?\r\n \r\n \r\n\n\n> sparse matrices are just supported by interior-point right? There's no documentation on which sparse matrices could be used as input though?\r\n\r\nWe should probably make it more explicit in the method-specific documentation that `A_ub` and `A_eq` can be sparse matrices. But there is a note about it in the [main `linprog` documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html).\r\n![image](https://user-images.githubusercontent.com/6570539/126877201-838f190a-42ce-4adc-a222-64a6b2458f8e.png)\r\n\r\nAnd in the [`interior-point`-specific](https://docs.scipy.org/doc/scipy/reference/optimize.linprog-interior-point.html#optimize-linprog-interior-point) documentation, it notes that in the `options` dictionary, you can specify `'sparse': True`.\r\n![image](https://user-images.githubusercontent.com/6570539/126877244-5c6ebb0d-6502-44fc-ae9d-67430cbb05ca.png)\r\n\r\nJust curious - if you'd be willing to check how long interior-point takes with sparse input, I'd be interested. \r\n\r\n> While the old methods 'interior-point' and 'revised' show that there is a redundant constraint the highs-solvers don't. Is there any reason why?\r\n\r\nThe HiGHS solvers skip the presolve procedure that is common to the older, Python linear programming solvers. HiGHS has its own presolve routines, and perhaps it doesn't check that or give a message about it.  (Removal of redundant constraints can be more expensive than if the redundant constraints were left in the problem, but it needed to be added because `'interior-point'` was really sensitive to redundancy in the problems. The HiGHS solvers might not be bothered by them.)\r\n\r\n> Also highs-methods are implemented in just C++ right?\r\n\r\nYes. See [here](https://www.maths.ed.ac.uk/hall/HiGHS/) for more information.\r\n\r\n> What is about 'interior point' and 'revised' ?\r\n\r\nPython. \r\n\r\n> Is there any documentation on the algorithms time and space complexitys based on the code?\r\n\r\nThe documentation points to references for the algorithms. Interior-point follows the reference pretty closely. Revised simplex is a little more generic, implemented based on the ideas in a textbook. I'm not sure if there will be useful information about time complexity. If I remember correctly, the worst-case asymptotic time complexity of all simplex-like methods is exponential but the actual time complexity is problem dependent and they perform quite well in practice. For interior-point, the paper might give some information about the complexity, which is theoretically polynomial - but again, actual time for a given problem depends a lot on the problem structure. \r\n\r\nI don't remember the space complexity being worse than a scalar multiple of the input for either algorithm, but perhaps I'm forgetting.\n\nI'm leaving this open and marking it as a good first issue: the documentation should mention that sparse arrays are accepted for `A_ub` and `A_eq`:\r\n\r\n![image](https://user-images.githubusercontent.com/6570539/126877584-93f6360f-b810-4d35-aab9-773ddd5f08ff.png)\r\n\r\nI think it would be best to just accept `2-D array or sparse matrix` for all the solvers. For `simplex` and `revised simplex`, we should convert to dense, and mention that in the documentation. This allows us to get the word out that `linprog` accepts sparse input, and at this point there's no good reason to use the old methods that don't really work with sparse matrices anyway. \r\n\n\nI've closed by accident..\r\n\r\n> We should probably make it more explicit in the method-specific documentation that A_ub and A_eq can be sparse matrices.\r\n\r\nSo do all solvers accept sparse input or just the interior-point-method?\r\n\r\n> And in the interior-point-specific documentation, it notes that in the options dictionary, you can specify 'sparse': True\r\n\r\nI will test this and report the results\r\n\r\n> Python\r\n\r\nAre 'interior point', 'revised simplex' and 'simplex' all 100 % pure python? While the documentation makes it clear that highs solvers are implemented in C++ it's unclear in which language the old methods were implemented..\r\n\r\n> The documentation points to references for the algorithms. Interior-point follows the reference pretty closely. Revised simplex is a little more generic, implemented based on the ideas in a textbook. I'm not sure if there will be useful information about time complexity. If I remember correctly, the worst-case asymptotic time complexity of all simplex-like methods is exponential but the actual time complexity is problem dependent and they perform quite well in practice. For interior-point, the paper might give some information about the complexity, which is theoretically polynomial - but again, actual time for a given problem depends a lot on the problem structure.\r\n\r\nThanks! I will have a closer look into the referenced papers..\r\n\r\n\r\n\n\n> So do all solvers accept sparse input or just the interior-point-method?\r\n\r\nCurrently, the `'interior-point'` and the HiGHS solvers accept sparse input; `'revised simplex'` and `'simplex'` don't. To keep things simple in the documentation and most performant on average, I'm suggesting that all solvers _should_ accept sparse input, that we document that all solvers accept sparse input, and that we convert to dense for `'simplex'` and `'revised simplex'` (and document that this is the behavior).  \r\n\r\n> Are 'interior point', 'revised simplex' and 'simplex' all 100 % pure python?\r\n\r\nIf you consider NumPy and SciPy's LAPACK interfaces \"pure python\", then `'interior-point'` and `'simplex'` are pure python. `'revised simplex'` uses a class written in Cython to represent the inverse of the basis matrix, but is otherwise pure python. The HiGHS methods are implemented in C++, and we wrote Python wrappers.\n\nFirst time contributor here. So, if I were take this issue, I would just update the docs so that it's explicit that `linprog(method='interior-point')` takes sparse matrics for `A_ub` and `A_eq`? \r\n\r\nAnd adding the acceptance of sparse input to the other solvers would be a separate issue/feature request? \n\n@dcb2124 Glad to hear you're interested!\r\n\r\n> So, if I were take this issue, I would just update the docs so that it's explicit that `linprog(method='interior-point')` takes sparse matrics for `A_ub` and `A_eq`? \r\n\r\n`linprog(method='interior-point')`, `linprog(method='highs')`, `linprog(method='highs-ds')`, and `linprog(method='highs-ipm')`, at least. \r\n\r\n> And adding the acceptance of sparse input to the other solvers would be a separate issue/feature request?\r\n\r\nIt could be. Just mentioning sparse input in the method-specific documentation for those four methods would be an improvement. But I thought it deserves to be mentioned in the top-level linprog documentation, too, since the default method `interior-point` and the new HiGHS methods all accept sparse input; only the old `simplex` and the (not often used) `revised simplex` method do not accept sparse input currently. However, there's not a good system for documenting method-specific argument types, which is why I thought it would make sense to make sure all the solvers accept sparse input. So if it were me, I would do all this in one PR.\n\nI'd like to take it.  How do I have this issue assigned to me?\r\n\n\nWe don't typically assign officially. If someone else were to see this issue, etiquette would suggest that they reach out to you before working on it themselves. But I officially assigned it to you : )\n\nThanks. I will get to this after work today.\n\n@mckib2 Really quick question - do you agree with the direction of [this comment](https://github.com/scipy/scipy/issues/14471#issuecomment-886099503) - `linprog` should always accept sparse input to keep documentation simple, but convert to dense before using `simplex` and `revised simplex` methods?\n\n> @mckib2 Really quick question - do you agree with the direction of [this comment](https://github.com/scipy/scipy/issues/14471#issuecomment-886099503) - `linprog` should always accept sparse input to keep documentation simple, but convert to dense before using `simplex` and `revised simplex` methods?\r\n\r\nYes, I like this direction.  There should probably be a warning about not using `simplex`/`revised simplex` methods with very large sparse matrices as you run the risk of running out of memory when converting to dense\n\n@mdhaber \r\nSo from my tests comparing the ip-options {sparse:True/False} I would say that the method interior-point runs about 1% faster using sparse matrices... I'd say that the speed up using sparse opition is quite low especially as I'm solving a really sparse problem.\r\n\r\nalso I was wondering if there exists a way to \"warmstart\" any of the sciPy-methods?\n\n> I'd say that the speed up using sparse opition is quite low especially as I'm solving a really sparse problem.\r\n\r\nIt's possible that there's been a regression because I've noticed that on some machines. (Not that it really matters because HiGHS is the way to go.) But on other machines sparse is much faster - especially with CHOLMOD/sciki-sparse. Out of curiosity, what platform are you on? \r\n\r\nNo warmstart right now. It is already on a list of desired enhancements, although now I think we just have to wait and see if HiGHS adds it.\r\n\r\n_Update: by the way, the [original `'interior-point'` PR](https://github.com/scipy/scipy/pull/7123#issue-108883282) shows a substantial advantage using the sparse option._\n\n@mdhaber I'm working on a windows 10 Notebook, 16 GB memory, i7-8750H and SSD.\r\n\r\nThe HiGHS are difficult to compare solving times to other python implemented algorithms like for instance from the networkX-library as it's in C++ though....\n\nWindows was my guess, as that's the platform I've noticed  sparse being slow (or dense being really fast) on. I'll investigate, as it could be an issue with the underlying solver (SuperLU).\r\n\r\n> The HiGHS are difficult to compare solving times to other python implemented algorithms...\r\n\r\nWall clock time is not good enough? (You are interested in something other than overall program execution time?)\r\n\n\n@mdhaber \r\n> Windows was my guess, as that's the platform I've noticed sparse being slow (or dense being really fast) on. I'll investigate, as it could be an issue with the underlying solver (SuperLU).\r\n\r\nInteresting... do you have any idea why this might be the case? \r\n\r\n> Wall clock time is not good enough? (You are interested in something other than overall program execution time?)\r\n\r\nWell I want to compare different algorithms for solving a specific problem... by using different programming languages clock time wouldn't be 'fair' as C++  for instance is a lot faster than python... or what are your thoughts on that ?\n\nI think it's going to be hard to make fair comparisons of algorithms based on existing libraries because both language and implementation details (that are not specified by the algorithm) matter. (Is this an academic interest? I get the impression that most users are just interested in solving a problem as quickly as possible, and for them the wall clock time is what matters.) Also, besides the core algorithm, there are a lot of other things that run with `linprog` and other LP software that can dramatically affect the execution time of the core algorithm (presolve, basis crashing, etc.).  I'm not sure how fair of a comparison you can get, even if the underlying languages are the same.\r\n\r\n> Interesting... do you have any idea why this might be the case?\r\n\r\nWould you try passing different options for `permc_spec`? The default is \"MMD_AT_PLUS_A\" but that might be slowing things down terribly. Please try \"COLAMD\" and \"MMD_ATA\" e.g. `options={'permc_spec': 'COLAMD'}`. Also, please try turning off redundancy removal `options={'rr': False}`. \r\n\r\nI say that because I was just comparing the methods of solving a sparse, symmetric positive definite linear system that are used by the interior-point method. As a test matrix, I used [this](https://math.nist.gov/MatrixMarket/data/misc/cylshell/s2rmt3m1.html), which does not have the structure that we would get in `linprog`, but this structure is much simpler so it should only help the sparse solver. I used a random, dense, right hand side.\r\n\r\nUsing `splu(A_sparse, permc_spec='MMD_AT_PLUS_A').solve(b)`, which is essentially what happens in `linprog` when using the sparse option, it took over _30 seconds_. Using `linalg.cho_solve(linalg.cho_factor(A_dense), b)`, which is what happens when the matrix is treated as dense, it took 365 ms to solve. Even `linalg.solve(A_dense, b)` is much faster than `splu` with this `permc_spec`, at 826 ms. On the other hand, `splu(A_sparse, permc_spec=\"COLAMD\").solve(b)` takes only 111ms.\r\n\r\nThis might not be the whole story, as I'm running the fast benchmark suite with `'permc_spec':'COLAMD'` and that doesn't seem to speed it up. I'm currently running with the full suite.\r\n\r\nIn any case, if you want a more fair look at `interior-point`, SuiteSparse and the `scikit-sparse` package need to be available. Falling back on SuperLU puts the algorithm a considerably disadvantage because it can't take advantage of the matrix being symmetric positive definite. \n\nMore complete results are in, and for most benchmark problems that take much time at all, `sparse` is faster, and often quite a bit faster. I think that the graphs in the [original PR](https://github.com/scipy/scipy/pull/7123#issue-108883282) are still representative. \n\n> \r\n> \r\n> I think it's going to be hard to make fair comparisons of algorithms based on existing libraries because both language and implementation details (that are not specified by the algorithm) matter. (Is this an academic interest? I get the impression that most users are just interested in solving a problem as quickly as possible, and for them the wall clock time is what matters.) Also, besides the core algorithm, there are a lot of other things that run with `linprog` and other LP software that can dramatically affect the execution time of the core algorithm (presolve, basis crashing, etc.). I'm not sure how fair of a comparison you can get, even if the underlying languages are the same.\r\n\r\nyeah exactly its part of my bachelor thesis... But don't you think using algorithms that ar at least written in the same programming language makes them more comparable? \r\n\r\n> > Interesting... do you have any idea why this might be the case?\r\n> \r\n> Would you try passing different options for `permc_spec`? The default is \"MMD_AT_PLUS_A\" but that might be slowing things down terribly. Please try \"COLAMD\" and \"MMD_ATA\" e.g. `options={'permc_spec': 'COLAMD'}`. Also, please try turning off redundancy removal `options={'rr': False}`.\r\n\r\nyeah sure, I will report  the test results\r\n> \r\n> I say that because I was just comparing the methods of solving a sparse, symmetric positive definite linear system that are used by the interior-point method. As a test matrix, I used [this](https://math.nist.gov/MatrixMarket/data/misc/cylshell/s2rmt3m1.html), which does not have the structure that we would get in `linprog`, but this structure is much simpler so it should only help the sparse solver. I used a random, dense, right hand side.\r\n> \r\n> Using `splu(A_sparse, permc_spec='MMD_AT_PLUS_A').solve(b)`, which is essentially what happens in `linprog` when using the sparse option, it took over _30 seconds_. Using `linalg.cho_solve(linalg.cho_factor(A_dense), b)`, which is what happens when the matrix is treated as dense, it took 365 ms to solve. Even `linalg.solve(A_dense, b)` is much faster than `splu` with this `permc_spec`, at 826 ms. On the other hand, `splu(A_sparse, permc_spec=\"COLAMD\").solve(b)` takes only 111ms.\r\n\r\nIf I got this right you haven't tested this in the interior point method but in the underlying functions?\r\n\r\n> \r\n> In any case, if you want a more fair look at `interior-point`, SuiteSparse and the `scikit-sparse` package need to be available. Falling back on SuperLU puts the algorithm a considerably disadvantage because it can't take advantage of the matrix being symmetric positive definite.\r\n\r\n\n\n> If I got this right you haven't tested this in the interior point method but in the underlying functions?\r\n\r\nI did both. I ran the `linprog` [benchmarks](http://scipy.github.io/devdocs/dev/contributor/benchmarking.html#benchmarking-with-asv) (after editing them to use `interior-point` with sparse and dense options) and found that they typically get the same (substantial) speedup for sparse matrices as they always have.  I also tested SuperLU independently a little and identified at least one case in which a very large, sparse matrix took _far_ longer to factor with SuperLU than with LAPACK, which could potentially explain `linprog` being slow for some sparse problems.\r\n\r\n> But don't you think using algorithms that ar at least written in the same programming language makes them more comparable?\r\n\r\nYes, but I'm still not sure that you can draw definitive conclusions about the speed of the underlying algorithm based on the wall clock run time of an implementation - even if the implementations are in the same language. You would be able to say more if you analyzed how the run time scaled with problem size (and fortunately, that should be mostly independent of programming language).\n\n> \r\n> \r\n> > If I got this right you haven't tested this in the interior point method but in the underlying functions?\r\n> \r\n> I did both. I ran the `linprog` [benchmarks](http://scipy.github.io/devdocs/dev/contributor/benchmarking.html#benchmarking-with-asv) (after editing them to use `interior-point` with sparse and dense options) and found that they typically get the same (substantial) speedup for sparse matrices as they always have. I also tested SuperLU independently a little and identified at least one case in which a very large, sparse matrix took _far_ longer to factor with SuperLU than with LAPACK, which could potentially explain `linprog` being slow for some sparse problems.\r\n\r\nalright.\r\nI was able to get huge speed up removing a redundant restriction. Atm I'm still testing but I will report results.\r\n\r\n\r\n\r\n> Yes, but I'm still not sure that you can draw definitive conclusions about the speed of the underlying algorithm based on the wall clock run time of an implementation - even if the implementations are in the same language. You would be able to say more if you analyzed how the run time scaled with problem size (and fortunately, that should be mostly independent of programming language).\r\n\r\nI've compared clock times as well as the runtimes for different problem sizes and did a curve fitting on the means. I'm with you that the this still doesn't give definitive insights in the algorithms themselves but I'd still say that this is the 'fairest' way to compare them. Even though implementation details could play a significant role.\r\n\r\n\n\nOk. Yes, i agree it is more fair if they are the same language. Let me know what you find out!\n\n@mdhaber \r\n\r\nSo I've tested 'ip', 'simplex' and 'revised' on scalability.\r\nAs expected 'ip' performs the best.\r\n'ip' performs about 6-7 times faster than 'simplex'.\r\n'ip' performs about 20 times faster than 'revised'.\r\n\r\nI find it really irritating that 'revised' runs slower than the 'simplex' method?\r\nAs from theory revised should be faster than the tableau method. Which is the case for 'smaller problems'.\r\n\r\nHave you observed faster running times of the tableau method before? \n\n@mdhaber \r\n\r\nSo in the 'revised' method\r\n\r\ndecomp_lu.py (lu_factor)\r\nand\r\nnumpy 'dot' function\r\n\r\nare the bottlenecks of the revised simplex which makes it up to 4 times slower than the tablau method\r\n\n\nCan you be more specific? \n\nMay I just underline that even though \"simplex\" or \"interior-point\" are not efficient for large problems, they are still very useful from an academic perspective when learning about linear programming for the first time. I am a bit disappointed to learn that they will be removed in future versions. It would be nice to keep them there for learning purposes. \n\n@bbogo as a drive-by comment, I wonder if you're interested in maintaining standalone versions of these methods? \r\nWe can probably help / advise on separating them into a separate package, and I think we can add links to that packages to the scipy docs. ",
  "pr_link": "https://github.com/scipy/scipy/pull/7123",
  "code_context": []
}