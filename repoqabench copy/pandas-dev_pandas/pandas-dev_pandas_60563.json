{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "60563",
  "issue_description": "# BUG: value_counts() returns error/wrong result with PyArrow categorical columns with nulls\n\n### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\n# First case: just one column. It gives the error below\r\npd.DataFrame( { 'A': [ 'a1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) ).value_counts( dropna = False )\r\n\r\n# Second case: more than one column. It gives the wrong result below\r\npd.concat( [\r\n    pd.DataFrame( { 'A': [ 'a1', 'a2' ], 'B': [ 'b1', pd.NA ] }, dtype = pd.ArrowDtype( pa.string() ) ),\r\n    pd.DataFrame( { 'C': [ 'c1', 'c2' ], 'D': [ 'd1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) )\r\n], axis = 1 ).value_counts( dropna = False )\n```\n\n\n### Issue Description\n\n### First Case\r\nIt gives the following error:\r\n\r\n```python-traceback\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[6], line 1\r\n----> 1 pd.DataFrame( { 'A': [ 'a1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) ).value_counts( dropna = False )\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\frame.py:7519, in DataFrame.value_counts(self, subset, normalize, sort, ascending, dropna)\r\n   7517 # Force MultiIndex for a list_like subset with a single column\r\n   7518 if is_list_like(subset) and len(subset) == 1:  # type: ignore[arg-type]\r\n-> 7519     counts.index = MultiIndex.from_arrays(\r\n   7520         [counts.index], names=[counts.index.name]\r\n   7521     )\r\n   7523 return counts\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:533, in MultiIndex.from_arrays(cls, arrays, sortorder, names)\r\n    530     if len(arrays[i]) != len(arrays[i - 1]):\r\n    531         raise ValueError(\"all arrays must be same length\")\r\n--> 533 codes, levels = factorize_from_iterables(arrays)\r\n    534 if names is lib.no_default:\r\n    535     names = [getattr(arr, \"name\", None) for arr in arrays]\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:3069, in factorize_from_iterables(iterables)\r\n   3065 if len(iterables) == 0:\r\n   3066     # For consistency, it should return two empty lists.\r\n   3067     return [], []\r\n-> 3069 codes, categories = zip(*(factorize_from_iterable(it) for it in iterables))\r\n   3070 return list(codes), list(categories)\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:3069, in <genexpr>(.0)\r\n   3065 if len(iterables) == 0:\r\n   3066     # For consistency, it should return two empty lists.\r\n   3067     return [], []\r\n-> 3069 codes, categories = zip(*(factorize_from_iterable(it) for it in iterables))\r\n   3070 return list(codes), list(categories)\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:3042, in factorize_from_iterable(values)\r\n   3037     codes = values.codes\r\n   3038 else:\r\n   3039     # The value of ordered is irrelevant since we don't use cat as such,\r\n   3040     # but only the resulting categories, the order of which is independent\r\n   3041     # from ordered. Set ordered to False as default. See GH #15457\r\n-> 3042     cat = Categorical(values, ordered=False)\r\n   3043     categories = cat.categories\r\n   3044     codes = cat.codes\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:451, in Categorical.__init__(self, values, categories, ordered, dtype, fastpath, copy)\r\n    447 if dtype.categories is None:\r\n    448     if isinstance(values.dtype, ArrowDtype) and issubclass(\r\n    449         values.dtype.type, CategoricalDtypeType\r\n    450     ):\r\n--> 451         arr = values._pa_array.combine_chunks()\r\n    452         categories = arr.dictionary.to_pandas(types_mapper=ArrowDtype)\r\n    453         codes = arr.indices.to_numpy()\r\n\r\nAttributeError: 'Index' object has no attribute '_pa_array'\r\n```\r\nIndeed, the same error is returned also if no `pd.NA` is present.\r\n\r\n### Second case\r\nIt gives the following result:\r\n\r\n```python\r\nA   B     C   D \r\na1  b1    c1  d1    1\r\na2  <NA>  c2  d1    1\r\nName: count, dtype: int64\r\n```\r\n\r\n**Note that in second line D is d1 and not `<NA>`.**\r\n\r\nA more complete example in this JupyterLab notebook: [value_counts() Bug.pdf](https://github.com/user-attachments/files/18133225/value_counts.Bug.pdf)\r\n\n\n### Expected Behavior\n\nThe expected behavior is analogous to the result obtained with the NumPy backend.\r\n\r\n### First case\r\n\r\n```python\r\nA  \r\na1     1\r\n<NA>    1\r\nName: count, dtype: int64\r\n```\r\n\r\n### Second case\r\n\r\n```python\r\nA   B     C   D \r\na1  b1    c1  d1    1\r\na2  <NA>  c2  <NA>    1\r\nName: count, dtype: int64\r\n```\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 0691c5cf90477d3503834d983f69350f250a6ff7\r\npython                : 3.12.8\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 2019Server\r\nVersion               : 10.0.17763\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 165 Stepping 2, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : English_United States.1252\r\n\r\npandas                : 2.2.3\r\nnumpy                 : 2.1.2\r\npytz                  : 2024.2\r\ndateutil              : 2.9.0.post0\r\npip                   : 24.3.1\r\nCython                : None\r\nsphinx                : None\r\nIPython               : 8.29.0\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nblosc                 : None\r\nbottleneck            : 1.4.2\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\nhtml5lib              : None\r\nhypothesis            : None\r\ngcsfs                 : None\r\njinja2                : 3.1.4\r\nlxml.etree            : 5.3.0\r\nmatplotlib            : 3.9.2\r\nnumba                 : None\r\nnumexpr               : 2.10.1\r\nodfpy                 : None\r\nopenpyxl              : 3.1.5\r\npandas_gbq            : None\r\npsycopg2              : None\r\npymysql               : None\r\npyarrow               : 18.1.0\r\npyreadstat            : None\r\npytest                : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nxlsxwriter            : None\r\nzstandard             : 0.23.0\r\ntzdata                : 2024.2\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n</details>\r\n",
  "issue_comments": [
    {
      "id": 2543176361,
      "user": "rhshadrach",
      "body": "Thanks for the report, confirmed on main. It appears that changing\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/9501650e22767f8502a1e3edecfaf17c5769f150/pandas/core/arrays/categorical.py#L450\r\n\r\nto be \r\n\r\n```python\r\n                if isinstance(values, Index):\r\n                    arr = values._data._pa_array.combine_chunks()\r\n                else:\r\n                    arr = values._pa_array.combine_chunks()\r\n```\r\n\r\nresolves. Further investigations and PRs to fix are welcome!\r\n\r\nEven with the above fix, we still do not see NA values because of a bug in groupby. I've opened https://github.com/pandas-dev/pandas/issues/60567 for this."
    },
    {
      "id": 2543177937,
      "user": "hardeybisey",
      "body": "take"
    },
    {
      "id": 2543338107,
      "user": "NOBODIDI",
      "body": "take"
    },
    {
      "id": 2543370551,
      "user": "NOBODIDI",
      "body": "> Thanks for the report, confirmed on main. It appears that changing\r\n> \r\n> https://github.com/pandas-dev/pandas/blob/9501650e22767f8502a1e3edecfaf17c5769f150/pandas/core/arrays/categorical.py#L450\r\n> \r\n> to be\r\n> \r\n> ```python\r\n>                 if isinstance(values, Index):\r\n>                     arr = values._data._pa_array.combine_chunks()\r\n>                 else:\r\n>                     arr = values._pa_array.combine_chunks()\r\n> ```\r\n> \r\n> resolves. Further investigations and PRs to fix are welcome!\r\n> \r\n> Even with the above fix, we still do not see NA values because of a bug in groupby. I've opened #60567 for this.\r\n\r\nI used this method but instead I did: \r\n```python \r\nif values.__class__.__name__ == 'Index':\r\n```\r\nso that Index does not need to be imported, this version fixed the issue. I am open to feedback."
    },
    {
      "id": 2543545847,
      "user": "hardeybisey",
      "body": "Hi @NOBODIDI , I noticed you’ve already submitted a PR for this issue. I started working on it and was planning to submit mine by morning my time. In the future, it would be great if we could sync up to avoid overlaps by confirming whether the issue is still being actively worked on, especially when it’s been recently assigned."
    },
    {
      "id": 2561744590,
      "user": "yanweiSu",
      "body": "take"
    },
    {
      "id": 2561761769,
      "user": "ghost",
      "body": "Gio phai làm sao. Mai ra vcbank đang kí lại tk va sinh trac\r\n"
    },
    {
      "id": 2561902966,
      "user": "ghost",
      "body": "Vào Th 4, 25 thg 12, 2024 lúc 16:53 Thịnh Lương ***@***.***>\r\nđã viết:\r\n\r\n> _xác nhập/thông tin. @TandT&All. -you like. #phone:1:9:4+:2:8:2/0ne/. And.\r\n> Family<one:(-)\r\n"
    },
    {
      "id": 2663406379,
      "user": "chilin0525",
      "body": "The bug can still be reproduced on the current main branch. I will continue the work from https://github.com/pandas-dev/pandas/pull/60569/files to attempt a fix."
    },
    {
      "id": 2663406767,
      "user": "chilin0525",
      "body": "take"
    }
  ],
  "text_context": "# BUG: value_counts() returns error/wrong result with PyArrow categorical columns with nulls\n\n### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\n# First case: just one column. It gives the error below\r\npd.DataFrame( { 'A': [ 'a1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) ).value_counts( dropna = False )\r\n\r\n# Second case: more than one column. It gives the wrong result below\r\npd.concat( [\r\n    pd.DataFrame( { 'A': [ 'a1', 'a2' ], 'B': [ 'b1', pd.NA ] }, dtype = pd.ArrowDtype( pa.string() ) ),\r\n    pd.DataFrame( { 'C': [ 'c1', 'c2' ], 'D': [ 'd1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) )\r\n], axis = 1 ).value_counts( dropna = False )\n```\n\n\n### Issue Description\n\n### First Case\r\nIt gives the following error:\r\n\r\n```python-traceback\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[6], line 1\r\n----> 1 pd.DataFrame( { 'A': [ 'a1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) ).value_counts( dropna = False )\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\frame.py:7519, in DataFrame.value_counts(self, subset, normalize, sort, ascending, dropna)\r\n   7517 # Force MultiIndex for a list_like subset with a single column\r\n   7518 if is_list_like(subset) and len(subset) == 1:  # type: ignore[arg-type]\r\n-> 7519     counts.index = MultiIndex.from_arrays(\r\n   7520         [counts.index], names=[counts.index.name]\r\n   7521     )\r\n   7523 return counts\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:533, in MultiIndex.from_arrays(cls, arrays, sortorder, names)\r\n    530     if len(arrays[i]) != len(arrays[i - 1]):\r\n    531         raise ValueError(\"all arrays must be same length\")\r\n--> 533 codes, levels = factorize_from_iterables(arrays)\r\n    534 if names is lib.no_default:\r\n    535     names = [getattr(arr, \"name\", None) for arr in arrays]\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:3069, in factorize_from_iterables(iterables)\r\n   3065 if len(iterables) == 0:\r\n   3066     # For consistency, it should return two empty lists.\r\n   3067     return [], []\r\n-> 3069 codes, categories = zip(*(factorize_from_iterable(it) for it in iterables))\r\n   3070 return list(codes), list(categories)\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:3069, in <genexpr>(.0)\r\n   3065 if len(iterables) == 0:\r\n   3066     # For consistency, it should return two empty lists.\r\n   3067     return [], []\r\n-> 3069 codes, categories = zip(*(factorize_from_iterable(it) for it in iterables))\r\n   3070 return list(codes), list(categories)\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:3042, in factorize_from_iterable(values)\r\n   3037     codes = values.codes\r\n   3038 else:\r\n   3039     # The value of ordered is irrelevant since we don't use cat as such,\r\n   3040     # but only the resulting categories, the order of which is independent\r\n   3041     # from ordered. Set ordered to False as default. See GH #15457\r\n-> 3042     cat = Categorical(values, ordered=False)\r\n   3043     categories = cat.categories\r\n   3044     codes = cat.codes\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:451, in Categorical.__init__(self, values, categories, ordered, dtype, fastpath, copy)\r\n    447 if dtype.categories is None:\r\n    448     if isinstance(values.dtype, ArrowDtype) and issubclass(\r\n    449         values.dtype.type, CategoricalDtypeType\r\n    450     ):\r\n--> 451         arr = values._pa_array.combine_chunks()\r\n    452         categories = arr.dictionary.to_pandas(types_mapper=ArrowDtype)\r\n    453         codes = arr.indices.to_numpy()\r\n\r\nAttributeError: 'Index' object has no attribute '_pa_array'\r\n```\r\nIndeed, the same error is returned also if no `pd.NA` is present.\r\n\r\n### Second case\r\nIt gives the following result:\r\n\r\n```python\r\nA   B     C   D \r\na1  b1    c1  d1    1\r\na2  <NA>  c2  d1    1\r\nName: count, dtype: int64\r\n```\r\n\r\n**Note that in second line D is d1 and not `<NA>`.**\r\n\r\nA more complete example in this JupyterLab notebook: [value_counts() Bug.pdf](https://github.com/user-attachments/files/18133225/value_counts.Bug.pdf)\r\n\n\n### Expected Behavior\n\nThe expected behavior is analogous to the result obtained with the NumPy backend.\r\n\r\n### First case\r\n\r\n```python\r\nA  \r\na1     1\r\n<NA>    1\r\nName: count, dtype: int64\r\n```\r\n\r\n### Second case\r\n\r\n```python\r\nA   B     C   D \r\na1  b1    c1  d1    1\r\na2  <NA>  c2  <NA>    1\r\nName: count, dtype: int64\r\n```\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 0691c5cf90477d3503834d983f69350f250a6ff7\r\npython                : 3.12.8\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 2019Server\r\nVersion               : 10.0.17763\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 165 Stepping 2, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : English_United States.1252\r\n\r\npandas                : 2.2.3\r\nnumpy                 : 2.1.2\r\npytz                  : 2024.2\r\ndateutil              : 2.9.0.post0\r\npip                   : 24.3.1\r\nCython                : None\r\nsphinx                : None\r\nIPython               : 8.29.0\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nblosc                 : None\r\nbottleneck            : 1.4.2\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\nhtml5lib              : None\r\nhypothesis            : None\r\ngcsfs                 : None\r\njinja2                : 3.1.4\r\nlxml.etree            : 5.3.0\r\nmatplotlib            : 3.9.2\r\nnumba                 : None\r\nnumexpr               : 2.10.1\r\nodfpy                 : None\r\nopenpyxl              : 3.1.5\r\npandas_gbq            : None\r\npsycopg2              : None\r\npymysql               : None\r\npyarrow               : 18.1.0\r\npyreadstat            : None\r\npytest                : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nxlsxwriter            : None\r\nzstandard             : 0.23.0\r\ntzdata                : 2024.2\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n</details>\r\n\n\nThanks for the report, confirmed on main. It appears that changing\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/9501650e22767f8502a1e3edecfaf17c5769f150/pandas/core/arrays/categorical.py#L450\r\n\r\nto be \r\n\r\n```python\r\n                if isinstance(values, Index):\r\n                    arr = values._data._pa_array.combine_chunks()\r\n                else:\r\n                    arr = values._pa_array.combine_chunks()\r\n```\r\n\r\nresolves. Further investigations and PRs to fix are welcome!\r\n\r\nEven with the above fix, we still do not see NA values because of a bug in groupby. I've opened https://github.com/pandas-dev/pandas/issues/60567 for this.\n\ntake\n\ntake\n\n> Thanks for the report, confirmed on main. It appears that changing\r\n> \r\n> https://github.com/pandas-dev/pandas/blob/9501650e22767f8502a1e3edecfaf17c5769f150/pandas/core/arrays/categorical.py#L450\r\n> \r\n> to be\r\n> \r\n> ```python\r\n>                 if isinstance(values, Index):\r\n>                     arr = values._data._pa_array.combine_chunks()\r\n>                 else:\r\n>                     arr = values._pa_array.combine_chunks()\r\n> ```\r\n> \r\n> resolves. Further investigations and PRs to fix are welcome!\r\n> \r\n> Even with the above fix, we still do not see NA values because of a bug in groupby. I've opened #60567 for this.\r\n\r\nI used this method but instead I did: \r\n```python \r\nif values.__class__.__name__ == 'Index':\r\n```\r\nso that Index does not need to be imported, this version fixed the issue. I am open to feedback.\n\nHi @NOBODIDI , I noticed you’ve already submitted a PR for this issue. I started working on it and was planning to submit mine by morning my time. In the future, it would be great if we could sync up to avoid overlaps by confirming whether the issue is still being actively worked on, especially when it’s been recently assigned.\n\ntake\n\nGio phai làm sao. Mai ra vcbank đang kí lại tk va sinh trac\r\n\n\nVào Th 4, 25 thg 12, 2024 lúc 16:53 Thịnh Lương ***@***.***>\r\nđã viết:\r\n\r\n> _xác nhập/thông tin. @TandT&All. -you like. #phone:1:9:4+:2:8:2/0ne/. And.\r\n> Family<one:(-)\r\n\n\nThe bug can still be reproduced on the current main branch. I will continue the work from https://github.com/pandas-dev/pandas/pull/60569/files to attempt a fix.\n\ntake",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/60569",
  "code_context": [
    {
      "filename": "pandas/core/arrays/categorical.py",
      "content": "from __future__ import annotations\n\nfrom csv import QUOTE_NONNUMERIC\nfrom functools import partial\nimport operator\nfrom shutil import get_terminal_size\nfrom typing import (\n    TYPE_CHECKING,\n    Literal,\n    cast,\n    overload,\n)\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import (\n    NaT,\n    algos as libalgos,\n    lib,\n)\nfrom pandas._libs.arrays import NDArrayBacked\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._validators import validate_bool_kwarg\n\nfrom pandas.core.dtypes.cast import (\n    coerce_indexer_dtype,\n    find_common_type,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_int64,\n    ensure_platform_int,\n    is_any_real_numeric_dtype,\n    is_bool_dtype,\n    is_dict_like,\n    is_hashable,\n    is_integer_dtype,\n    is_list_like,\n    is_scalar,\n    needs_i8_conversion,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import (\n    ArrowDtype,\n    CategoricalDtype,\n    CategoricalDtypeType,\n    ExtensionDtype,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCIndex,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.missing import (\n    is_valid_na_for_dtype,\n    isna,\n)\n\nfrom pandas.core import (\n    algorithms,\n    arraylike,\n    ops,\n)\nfrom pandas.core.accessor import (\n    PandasDelegate,\n    delegate_names,\n)\nfrom pandas.core.algorithms import (\n    factorize,\n    take_nd,\n)\nfrom pandas.core.arrays._mixins import (\n    NDArrayBackedExtensionArray,\n    ravel_compat,\n)\nfrom pandas.core.base import (\n    ExtensionArray,\n    NoNewAttributesMixin,\n    PandasObject,\n)\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    extract_array,\n    sanitize_array,\n)\nfrom pandas.core.ops.common import unpack_zerodim_and_defer\nfrom pandas.core.sorting import nargsort\nfrom pandas.core.strings.object_array import ObjectStringArrayMixin\n\nfrom pandas.io.formats import console\n\nif TYPE_CHECKING:\n    from collections.abc import (\n        Callable,\n        Hashable,\n        Iterator,\n        Sequence,\n    )\n\n    from pandas._typing import (\n        ArrayLike,\n        AstypeArg,\n        AxisInt,\n        Dtype,\n        DtypeObj,\n        NpDtype,\n        Ordered,\n        Self,\n        Shape,\n        SortKind,\n        npt,\n    )\n\n    from pandas import (\n        DataFrame,\n        Index,\n        Series,\n    )\n\n\ndef _cat_compare_op(op):\n    opname = f\"__{op.__name__}__\"\n    fill_value = op is operator.ne\n\n    @unpack_zerodim_and_defer(opname)\n    def func(self, other):\n        hashable = is_hashable(other)\n        if is_list_like(other) and len(other) != len(self) and not hashable:\n            # in hashable case we may have a tuple that is itself a category\n            raise ValueError(\"Lengths must match.\")\n\n        if not self.ordered:\n            if opname in [\"__lt__\", \"__gt__\", \"__le__\", \"__ge__\"]:\n                raise TypeError(\n                    \"Unordered Categoricals can only compare equality or not\"\n                )\n        if isinstance(other, Categorical):\n            # Two Categoricals can only be compared if the categories are\n            # the same (maybe up to ordering, depending on ordered)\n\n            msg = \"Categoricals can only be compared if 'categories' are the same.\"\n            if not self._categories_match_up_to_permutation(other):\n                raise TypeError(msg)\n\n            if not self.ordered and not self.categories.equals(other.categories):\n                # both unordered and different order\n                other_codes = recode_for_categories(\n                    other.codes, other.categories, self.categories, copy=False\n                )\n            else:\n                other_codes = other._codes\n\n            ret = op(self._codes, other_codes)\n            mask = (self._codes == -1) | (other_codes == -1)\n            if mask.any():\n                ret[mask] = fill_value\n            return ret\n\n        if hashable:\n            if other in self.categories:\n                i = self._unbox_scalar(other)\n                ret = op(self._codes, i)\n\n                if opname not in {\"__eq__\", \"__ge__\", \"__gt__\"}:\n                    # GH#29820 performance trick; get_loc will always give i>=0,\n                    #  so in the cases (__ne__, __le__, __lt__) the setting\n                    #  here is a no-op, so can be skipped.\n                    mask = self._codes == -1\n                    ret[mask] = fill_value\n                return ret\n            else:\n                return ops.invalid_comparison(self, other, op)\n        else:\n            # allow categorical vs object dtype array comparisons for equality\n            # these are only positional comparisons\n            if opname not in [\"__eq__\", \"__ne__\"]:\n                raise TypeError(\n                    f\"Cannot compare a Categorical for op {opname} with \"\n                    f\"type {type(other)}.\\nIf you want to compare values, \"\n                    \"use 'np.asarray(cat) <op> other'.\"\n                )\n\n            if isinstance(other, ExtensionArray) and needs_i8_conversion(other.dtype):\n                # We would return NotImplemented here, but that messes up\n                #  ExtensionIndex's wrapped methods\n                return op(other, self)\n            return getattr(np.array(self), opname)(np.array(other))\n\n    func.__name__ = opname\n\n    return func\n\n\ndef contains(cat, key, container) -> bool:\n    \"\"\"\n    Helper for membership check for ``key`` in ``cat``.\n\n    This is a helper method for :method:`__contains__`\n    and :class:`CategoricalIndex.__contains__`.\n\n    Returns True if ``key`` is in ``cat.categories`` and the\n    location of ``key`` in ``categories`` is in ``container``.\n\n    Parameters\n    ----------\n    cat : :class:`Categorical`or :class:`categoricalIndex`\n    key : a hashable object\n        The key to check membership for.\n    container : Container (e.g. list-like or mapping)\n        The container to check for membership in.\n\n    Returns\n    -------\n    is_in : bool\n        True if ``key`` is in ``self.categories`` and location of\n        ``key`` in ``categories`` is in ``container``, else False.\n\n    Notes\n    -----\n    This method does not check for NaN values. Do that separately\n    before calling this method.\n    \"\"\"\n    hash(key)\n\n    # get location of key in categories.\n    # If a KeyError, the key isn't in categories, so logically\n    #  can't be in container either.\n    try:\n        loc = cat.categories.get_loc(key)\n    except (KeyError, TypeError):\n        return False\n\n    # loc is the location of key in categories, but also the *value*\n    # for key in container. So, `key` may be in categories,\n    # but still not in `container`. Example ('b' in categories,\n    # but not in values):\n    # 'b' in Categorical(['a'], categories=['a', 'b'])  # False\n    if is_scalar(loc):\n        return loc in container\n    else:\n        # if categories is an IntervalIndex, loc is an array.\n        return any(loc_ in container for loc_ in loc)\n\n\n# error: Definition of \"delete/ravel/T/repeat/copy\" in base class \"NDArrayBacked\"\n# is incompatible with definition in base class \"ExtensionArray\"\nclass Categorical(NDArrayBackedExtensionArray, PandasObject, ObjectStringArrayMixin):  # type: ignore[misc]\n    \"\"\"\n    Represent a categorical variable in classic R / S-plus fashion.\n\n    `Categoricals` can only take on a limited, and usually fixed, number\n    of possible values (`categories`). In contrast to statistical categorical\n    variables, a `Categorical` might have an order, but numerical operations\n    (additions, divisions, ...) are not possible.\n\n    All values of the `Categorical` are either in `categories` or `np.nan`.\n    Assigning values outside of `categories` will raise a `ValueError`. Order\n    is defined by the order of the `categories`, not lexical order of the\n    values.\n\n    Parameters\n    ----------\n    values : list-like\n        The values of the categorical. If categories are given, values not in\n        categories will be replaced with NaN.\n    categories : Index-like (unique), optional\n        The unique categories for this categorical. If not given, the\n        categories are assumed to be the unique values of `values` (sorted, if\n        possible, otherwise in the order in which they appear).\n    ordered : bool, default False\n        Whether or not this categorical is treated as a ordered categorical.\n        If True, the resulting categorical will be ordered.\n        An ordered categorical respects, when sorted, the order of its\n        `categories` attribute (which in turn is the `categories` argument, if\n        provided).\n    dtype : CategoricalDtype\n        An instance of ``CategoricalDtype`` to use for this categorical.\n    copy : bool, default True\n        Whether to copy if the codes are unchanged.\n\n    Attributes\n    ----------\n    categories : Index\n        The categories of this categorical.\n    codes : ndarray\n        The codes (integer positions, which point to the categories) of this\n        categorical, read only.\n    ordered : bool\n        Whether or not this Categorical is ordered.\n    dtype : CategoricalDtype\n        The instance of ``CategoricalDtype`` storing the ``categories``\n        and ``ordered``.\n\n    Methods\n    -------\n    from_codes\n    as_ordered\n    as_unordered\n    set_categories\n    rename_categories\n    reorder_categories\n    add_categories\n    remove_categories\n    remove_unused_categories\n    map\n    __array__\n\n    Raises\n    ------\n    ValueError\n        If the categories do not validate.\n    TypeError\n        If an explicit ``ordered=True`` is given but no `categories` and the\n        `values` are not sortable.\n\n    See Also\n    --------\n    CategoricalDtype : Type for categorical data.\n    CategoricalIndex : An Index with an underlying ``Categorical``.\n\n    Notes\n    -----\n    See the `user guide\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html>`__\n    for more.\n\n    Examples\n    --------\n    >>> pd.Categorical([1, 2, 3, 1, 2, 3])\n    [1, 2, 3, 1, 2, 3]\n    Categories (3, int64): [1, 2, 3]\n\n    >>> pd.Categorical([\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"])\n    ['a', 'b', 'c', 'a', 'b', 'c']\n    Categories (3, object): ['a', 'b', 'c']\n\n    Missing values are not included as a category.\n\n    >>> c = pd.Categorical([1, 2, 3, 1, 2, 3, np.nan])\n    >>> c\n    [1, 2, 3, 1, 2, 3, NaN]\n    Categories (3, int64): [1, 2, 3]\n\n    However, their presence is indicated in the `codes` attribute\n    by code `-1`.\n\n    >>> c.codes\n    array([ 0,  1,  2,  0,  1,  2, -1], dtype=int8)\n\n    Ordered `Categoricals` can be sorted according to the custom order\n    of the categories and can have a min and max value.\n\n    >>> c = pd.Categorical(\n    ...     [\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"], ordered=True, categories=[\"c\", \"b\", \"a\"]\n    ... )\n    >>> c\n    ['a', 'b', 'c', 'a', 'b', 'c']\n    Categories (3, object): ['c' < 'b' < 'a']\n    >>> c.min()\n    'c'\n    \"\"\"\n\n    # For comparisons, so that numpy uses our implementation if the compare\n    # ops, which raise\n    __array_priority__ = 1000\n    # tolist is not actually deprecated, just suppressed in the __dir__\n    _hidden_attrs = PandasObject._hidden_attrs | frozenset([\"tolist\"])\n    _typ = \"categorical\"\n\n    _dtype: CategoricalDtype\n\n    @classmethod\n    # error: Argument 2 of \"_simple_new\" is incompatible with supertype\n    # \"NDArrayBacked\"; supertype defines the argument type as\n    # \"Union[dtype[Any], ExtensionDtype]\"\n    def _simple_new(  # type: ignore[override]\n        cls, codes: np.ndarray, dtype: CategoricalDtype\n    ) -> Self:\n        # NB: This is not _quite_ as simple as the \"usual\" _simple_new\n        codes = coerce_indexer_dtype(codes, dtype.categories)\n        dtype = CategoricalDtype(ordered=False).update_dtype(dtype)\n        return super()._simple_new(codes, dtype)\n\n    def __init__(\n        self,\n        values,\n        categories=None,\n        ordered=None,\n        dtype: Dtype | None = None,\n        copy: bool = True,\n    ) -> None:\n        dtype = CategoricalDtype._from_values_or_dtype(\n            values, categories, ordered, dtype\n        )\n        # At this point, dtype is always a CategoricalDtype, but\n        # we may have dtype.categories be None, and we need to\n        # infer categories in a factorization step further below\n\n        if not is_list_like(values):\n            # GH#38433\n            raise TypeError(\"Categorical input must be list-like\")\n\n        # null_mask indicates missing values we want to exclude from inference.\n        # This means: only missing values in list-likes (not arrays/ndframes).\n        null_mask = np.array(False)\n\n        # sanitize input\n        vdtype = getattr(values, \"dtype\", None)\n        if isinstance(vdtype, CategoricalDtype):\n            if dtype.categories is None:\n                dtype = CategoricalDtype(values.categories, dtype.ordered)\n        elif isinstance(values, range):\n            from pandas.core.indexes.range import RangeIndex\n\n            values = RangeIndex(values)\n        elif not isinstance(values, (ABCIndex, ABCSeries, ExtensionArray)):\n            values = com.convert_to_list_like(values)\n            if isinstance(values, list) and len(values) == 0:\n                # By convention, empty lists result in object dtype:\n                values = np.array([], dtype=object)\n            elif isinstance(values, np.ndarray):\n                if values.ndim > 1:\n                    # preempt sanitize_array from raising ValueError\n                    raise NotImplementedError(\n                        \"> 1 ndim Categorical are not supported at this time\"\n                    )\n                values = sanitize_array(values, None)\n            else:\n                # i.e. must be a list\n                arr = sanitize_array(values, None)\n                null_mask = isna(arr)\n                if null_mask.any():\n                    # We remove null values here, then below will re-insert\n                    #  them, grep \"full_codes\"\n                    arr_list = [values[idx] for idx in np.where(~null_mask)[0]]\n\n                    # GH#44900 Do not cast to float if we have only missing values\n                    if arr_list or arr.dtype == \"object\":\n                        sanitize_dtype = None\n                    else:\n                        sanitize_dtype = arr.dtype\n\n                    arr = sanitize_array(arr_list, None, dtype=sanitize_dtype)\n                values = arr\n\n        if dtype.categories is None:\n            if isinstance(values.dtype, ArrowDtype) and issubclass(\n                values.dtype.type, CategoricalDtypeType\n            ):\n                if values.__class__.__name__ == 'Index':\n                    arr = values._data._pa_array.combine_chunks()\n                else:\n                    arr = values._pa_array.combine_chunks()\n                categories = arr.dictionary.to_pandas(types_mapper=ArrowDtype)\n                codes = arr.indices.to_numpy()\n                dtype = CategoricalDtype(categories, values.dtype.pyarrow_dtype.ordered)\n            else:\n                if not isinstance(values, ABCIndex):\n                    # in particular RangeIndex xref test_index_equal_range_categories\n                    values = sanitize_array(values, None)\n                try:\n                    codes, categories = factorize(values, sort=True)\n                except TypeError as err:\n                    codes, categories = factorize(values, sort=False)\n                    if dtype.ordered:\n                        # raise, as we don't have a sortable data structure and so\n                        # the user should give us one by specifying categories\n                        raise TypeError(\n                            \"'values' is not ordered, please \"\n                            \"explicitly specify the categories order \"\n                            \"by passing in a categories argument.\"\n                        ) from err\n\n                # we're inferring from values\n                dtype = CategoricalDtype(categories, dtype.ordered)\n\n        elif isinstance(values.dtype, CategoricalDtype):\n            old_codes = extract_array(values)._codes\n            codes = recode_for_categories(\n                old_codes, values.dtype.categories, dtype.categories, copy=copy\n            )\n\n        else:\n            codes = _get_codes_for_values(values, dtype.categories)\n\n        if null_mask.any():\n            # Reinsert -1 placeholders for previously removed missing values\n            full_codes = -np.ones(null_mask.shape, dtype=codes.dtype)\n            full_codes[~null_mask] = codes\n            codes = full_codes\n\n        dtype = CategoricalDtype(ordered=False).update_dtype(dtype)\n        arr = coerce_indexer_dtype(codes, dtype.categories)\n        super().__init__(arr, dtype)\n\n    @property\n    def dtype(self) -> CategoricalDtype:\n        \"\"\"\n        The :class:`~pandas.api.types.CategoricalDtype` for this instance.\n\n        See Also\n        --------\n        astype : Cast argument to a specified dtype.\n        CategoricalDtype : Type for categorical data.\n\n        Examples\n        --------\n        >>> cat = pd.Categorical([\"a\", \"b\"], ordered=True)\n        >>> cat\n        ['a', 'b']\n        Categories (2, object): ['a' < 'b']\n        >>> cat.dtype\n        CategoricalDtype(categories=['a', 'b'], ordered=True, categories_dtype=object)\n        \"\"\"\n        return self._dtype\n\n    @property\n    def _internal_fill_value(self) -> int:\n        # using the specific numpy integer instead of python int to get\n        #  the correct dtype back from _quantile in the all-NA case\n        dtype = self._ndarray.dtype\n        return dtype.type(-1)\n\n    @classmethod\n    def _from_sequence(\n        cls, scalars, *, dtype: Dtype | None = None, copy: bool = False\n    ) -> Self:\n        return cls(scalars, dtype=dtype, copy=copy)\n\n    @classmethod\n    def _from_scalars(cls, scalars, *, dtype: DtypeObj) -> Self:\n        if dtype is None:\n            # The _from_scalars strictness doesn't make much sense in this case.\n            raise NotImplementedError\n\n        res = cls._from_sequence(scalars, dtype=dtype)\n\n        # if there are any non-category elements in scalars, these will be\n        #  converted to NAs in res.\n        mask = isna(scalars)\n        if not (mask == res.isna()).all():\n            # Some non-category element in scalars got converted to NA in res.\n            raise ValueError\n        return res\n\n    @overload\n    def astype(self, dtype: npt.DTypeLike, copy: bool = ...) -> np.ndarray: ...\n\n    @overload\n    def astype(self, dtype: ExtensionDtype, copy: bool = ...) -> ExtensionArray: ...\n\n    @overload\n    def astype(self, dtype: AstypeArg, copy: bool = ...) -> ArrayLike: ...\n\n    def astype(self, dtype: AstypeArg, copy: bool = True) -> ArrayLike:\n        \"\"\"\n        Coerce this type to another dtype\n\n        Parameters\n        ----------\n        dtype : numpy dtype or pandas type\n        copy : bool, default True\n            By default, astype always returns a newly allocated object.\n            If copy is set to False and dtype is categorical, the original\n            object is returned.\n        \"\"\"\n        dtype = pandas_dtype(dtype)\n        result: Categorical | np.ndarray\n        if self.dtype is dtype:\n            result = self.copy() if copy else self\n\n        elif isinstance(dtype, CategoricalDtype):\n            # GH 10696/18593/18630\n            dtype = self.dtype.update_dtype(dtype)\n            self = self.copy() if copy else self\n            result = self._set_dtype(dtype)\n\n        elif isinstance(dtype, ExtensionDtype):\n            return super().astype(dtype, copy=copy)\n\n        elif dtype.kind in \"iu\" and self.isna().any():\n            raise ValueError(\"Cannot convert float NaN to integer\")\n\n        elif len(self.codes) == 0 or len(self.categories) == 0:\n            # For NumPy 1.x compatibility we cannot use copy=None.  And\n            # `copy=False` has the meaning of `copy=None` here:\n            if not copy:\n                result = np.asarray(self, dtype=dtype)\n            else:\n                result = np.array(self, dtype=dtype)\n\n        else:\n            # GH8628 (PERF): astype category codes instead of astyping array\n            new_cats = self.categories._values\n\n            try:\n                new_cats = new_cats.astype(dtype=dtype, copy=copy)\n                fill_value = self.categories._na_value\n                if not is_valid_na_for_dtype(fill_value, dtype):\n                    fill_value = lib.item_from_zerodim(\n                        np.array(self.categories._na_value).astype(dtype)\n                    )\n            except (\n                TypeError,  # downstream error msg for CategoricalIndex is misleading\n                ValueError,\n            ) as err:\n                msg = f\"Cannot cast {self.categories.dtype} dtype to {dtype}\"\n                raise ValueError(msg) from err\n\n            result = take_nd(\n                new_cats, ensure_platform_int(self._codes), fill_value=fill_value\n            )\n\n        return result\n\n    @classmethod\n    def _from_inferred_categories(\n        cls, inferred_categories, inferred_codes, dtype, true_values=None\n    ) -> Self:\n        \"\"\"\n        Construct a Categorical from inferred values.\n\n        For inferred categories (`dtype` is None) the categories are sorted.\n        For explicit `dtype`, the `inferred_categories` are cast to the\n        appropriate type.\n\n        Parameters\n        ----------\n        inferred_categories : Index\n        inferred_codes : Index\n        dtype : CategoricalDtype or 'category'\n        true_values : list, optional\n            If none are provided, the default ones are\n            \"True\", \"TRUE\", and \"true.\"\n\n        Returns\n        -------\n        Categorical\n        \"\"\"\n        from pandas import (\n            Index,\n            to_datetime,\n            to_numeric,\n            to_timedelta,\n        )\n\n        cats = Index(inferred_categories)\n        known_categories = (\n            isinstance(dtype, CategoricalDtype) and dtype.categories is not None\n        )\n\n        if known_categories:\n            # Convert to a specialized type with `dtype` if specified.\n            if is_any_real_numeric_dtype(dtype.categories.dtype):\n                cats = to_numeric(inferred_categories, errors=\"coerce\")\n            elif lib.is_np_dtype(dtype.categories.dtype, \"M\"):\n                cats = to_datetime(inferred_categories, errors=\"coerce\")\n            elif lib.is_np_dtype(dtype.categories.dtype, \"m\"):\n                cats = to_timedelta(inferred_categories, errors=\"coerce\")\n            elif is_bool_dtype(dtype.categories.dtype):\n                if true_values is None:\n                    true_values = [\"True\", \"TRUE\", \"true\"]\n\n                # error: Incompatible types in assignment (expression has type\n                # \"ndarray\", variable has type \"Index\")\n                cats = cats.isin(true_values)  # type: ignore[assignment]\n\n        if known_categories:\n            # Recode from observation order to dtype.categories order.\n            categories = dtype.categories\n            codes = recode_for_categories(inferred_codes, cats, categories)\n        elif not cats.is_monotonic_increasing:\n            # Sort categories and recode for unknown categories.\n            unsorted = cats.copy()\n            categories = cats.sort_values()\n\n            codes = recode_for_categories(inferred_codes, unsorted, categories)\n            dtype = CategoricalDtype(categories, ordered=False)\n        else:\n            dtype = CategoricalDtype(cats, ordered=False)\n            codes = inferred_codes\n\n        return cls._simple_new(codes, dtype=dtype)\n\n    @classmethod\n    def from_codes(\n        cls,\n        codes,\n        categories=None,\n        ordered=None,\n        dtype: Dtype | None = None,\n        validate: bool = True,\n    ) -> Self:\n        \"\"\"\n        Make a Categorical type from codes and categories or dtype.\n\n        This constructor is useful if you already have codes and\n        categories/dtype and so do not need the (computation intensive)\n        factorization step, which is usually done on the constructor.\n\n        If your data does not follow this convention, please use the normal\n        constructor.\n\n        Parameters\n        ----------\n        codes : array-like of int\n            An integer array, where each integer points to a category in\n            categories or dtype.categories, or else is -1 for NaN.\n        categories : index-like, optional\n            The categories for the categorical. Items need to be unique.\n            If the categories are not given here, then they must be provided\n            in `dtype`.\n        ordered : bool, optional\n            Whether or not this categorical is treated as an ordered\n            categorical. If not given here or in `dtype`, the resulting\n            categorical will be unordered.\n        dtype : CategoricalDtype or \"category\", optional\n            If :class:`CategoricalDtype`, cannot be used together with\n            `categories` or `ordered`.\n        validate : bool, default True\n            If True, validate that the codes are valid for the dtype.\n            If False, don't validate that the codes are valid. Be careful about skipping\n            validation, as invalid codes can lead to severe problems, such as segfaults.\n\n            .. versionadded:: 2.1.0\n\n        Returns\n        -------\n        Categorical\n\n        See Also\n        --------\n        codes : The category codes of the categorical.\n        CategoricalIndex : An Index with an underlying ``Categorical``.\n\n        Examples\n        --------\n        >>> dtype = pd.CategoricalDtype([\"a\", \"b\"], ordered=True)\n        >>> pd.Categorical.from_codes(codes=[0, 1, 0, 1], dtype=dtype)\n        ['a', 'b', 'a', 'b']\n        Categories (2, object): ['a' < 'b']\n        \"\"\"\n        dtype = CategoricalDtype._from_values_or_dtype(\n            categories=categories, ordered=ordered, dtype=dtype\n        )\n        if dtype.categories is None:\n            msg = (\n                \"The categories must be provided in 'categories' or \"\n                \"'dtype'. Both were None.\"\n            )\n            raise ValueError(msg)\n\n        if validate:\n            # beware: non-valid codes may segfault\n            codes = cls._validate_codes_for_dtype(codes, dtype=dtype)\n\n        return cls._simple_new(codes, dtype=dtype)\n\n    # ------------------------------------------------------------------\n    # Categories/Codes/Ordered\n\n    @property\n    def categories(self) -> Index:\n        \"\"\"\n        The categories of this categorical.\n\n        Setting assigns new values to each category (effectively a rename of\n        each individual category).\n\n        The assigned value has to be a list-like object. All items must be\n        unique and the number of items in the new categories must be the same\n        as the number of items in the old categories.\n\n        Raises\n        ------\n        ValueError\n            If the new categories do not validate as categories or if the\n            number of new categories is unequal the number of old categories\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n\n        Examples\n        --------\n        For :class:`pandas.Series`:\n\n        >>> ser = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\n        >>> ser.cat.categories\n        Index(['a', 'b', 'c'], dtype='object')\n\n        >>> raw_cat = pd.Categorical([\"a\", \"b\", \"c\", \"a\"], categories=[\"b\", \"c\", \"d\"])\n        >>> ser = pd.Series(raw_cat)\n        >>> ser.cat.categories\n        Index(['b', 'c', 'd'], dtype='object')\n\n        For :class:`pandas.Categorical`:\n\n        >>> cat = pd.Categorical([\"a\", \"b\"], ordered=True)\n        >>> cat.categories\n        Index(['a', 'b'], dtype='object')\n\n        For :class:`pandas.CategoricalIndex`:\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"c\", \"b\", \"a\", \"c\", \"b\"])\n        >>> ci.categories\n        Index(['a', 'b', 'c'], dtype='object')\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"c\"], categories=[\"c\", \"b\", \"a\"])\n        >>> ci.categories\n        Index(['c', 'b', 'a'], dtype='object')\n        \"\"\"\n        return self.dtype.categories\n\n    @property\n    def ordered(self) -> Ordered:\n        \"\"\"\n        Whether the categories have an ordered relationship.\n\n        See Also\n        --------\n        set_ordered : Set the ordered attribute.\n        as_ordered : Set the Categorical to be ordered.\n        as_unordered : Set the Categorical to be unordered.\n\n        Examples\n        --------\n        For :class:`pandas.Series`:\n\n        >>> ser = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\n        >>> ser.cat.ordered\n        False\n\n        >>> raw_cat = pd.Categorical([\"a\", \"b\", \"c\", \"a\"], ordered=True)\n        >>> ser = pd.Series(raw_cat)\n        >>> ser.cat.ordered\n        True\n\n        For :class:`pandas.Categorical`:\n\n        >>> cat = pd.Categorical([\"a\", \"b\"], ordered=True)\n        >>> cat.ordered\n        True\n\n        >>> cat = pd.Categorical([\"a\", \"b\"], ordered=False)\n        >>> cat.ordered\n        False\n\n        For :class:`pandas.CategoricalIndex`:\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"b\"], ordered=True)\n        >>> ci.ordered\n        True\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"b\"], ordered=False)\n        >>> ci.ordered\n        False\n        \"\"\"\n        return self.dtype.ordered\n\n    @property\n    def codes(self) -> np.ndarray:\n        \"\"\"\n        The category codes of this categorical index.\n\n        Codes are an array of integers which are the positions of the actual\n        values in the categories array.\n\n        There is no setter, use the other categorical methods and the normal item\n        setter to change values in the categorical.\n\n        Returns\n        -------\n        ndarray[int]\n            A non-writable view of the ``codes`` array.\n\n        See Also\n        --------\n        Categorical.from_codes : Make a Categorical from codes.\n        CategoricalIndex : An Index with an underlying ``Categorical``.\n\n        Examples\n        --------\n        For :class:`pandas.Categorical`:\n\n        >>> cat = pd.Categorical([\"a\", \"b\"], ordered=True)\n        >>> cat.codes\n        array([0, 1], dtype=int8)\n\n        For :class:`pandas.CategoricalIndex`:\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"])\n        >>> ci.codes\n        array([0, 1, 2, 0, 1, 2], dtype=int8)\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"c\"], categories=[\"c\", \"b\", \"a\"])\n        >>> ci.codes\n        array([2, 0], dtype=int8)\n        \"\"\"\n        v = self._codes.view()\n        v.flags.writeable = False\n        return v\n\n    def _set_categories(self, categories, fastpath: bool = False) -> None:\n        \"\"\"\n        Sets new categories inplace\n\n        Parameters\n        ----------\n        fastpath : bool, default False\n           Don't perform validation of the categories for uniqueness or nulls\n\n        Examples\n        --------\n        >>> c = pd.Categorical([\"a\", \"b\"])\n        >>> c\n        ['a', 'b']\n        Categories (2, object): ['a', 'b']\n\n        >>> c._set_categories(pd.Index([\"a\", \"c\"]))\n        >>> c\n        ['a', 'c']\n        Categories (2, object): ['a', 'c']\n        \"\"\"\n        if fastpath:\n            new_dtype = CategoricalDtype._from_fastpath(categories, self.ordered)\n        else:\n            new_dtype = CategoricalDtype(categories, ordered=self.ordered)\n        if (\n            not fastpath\n            and self.dtype.categories is not None\n            and len(new_dtype.categories) != len(self.dtype.categories)\n        ):\n            raise ValueError(\n                \"new categories need to have the same number of \"\n                \"items as the old categories!\"\n            )\n\n        super().__init__(self._ndarray, new_dtype)\n\n    def _set_dtype(self, dtype: CategoricalDtype) -> Self:\n        \"\"\"\n        Internal method for directly updating the CategoricalDtype\n\n        Parameters\n        ----------\n        dtype : CategoricalDtype\n\n        Notes\n        -----\n        We don't do any validation here. It's assumed that the dtype is\n        a (valid) instance of `CategoricalDtype`.\n        \"\"\"\n        codes = recode_for_categories(self.codes, self.categories, dtype.categories)\n        return type(self)._simple_new(codes, dtype=dtype)\n\n    def set_ordered(self, value: bool) -> Self:\n        \"\"\"\n        Set the ordered attribute to the boolean value.\n\n        Parameters\n        ----------\n        value : bool\n           Set whether this categorical is ordered (True) or not (False).\n        \"\"\"\n        new_dtype = CategoricalDtype(self.categories, ordered=value)\n        cat = self.copy()\n        NDArrayBacked.__init__(cat, cat._ndarray, new_dtype)\n        return cat\n\n    def as_ordered(self) -> Self:\n        \"\"\"\n        Set the Categorical to be ordered.\n\n        Returns\n        -------\n        Categorical\n            Ordered Categorical.\n\n        See Also\n        --------\n        as_unordered : Set the Categorical to be unordered.\n\n        Examples\n        --------\n        For :class:`pandas.Series`:\n\n        >>> ser = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\n        >>> ser.cat.ordered\n        False\n        >>> ser = ser.cat.as_ordered()\n        >>> ser.cat.ordered\n        True\n\n        For :class:`pandas.CategoricalIndex`:\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"b\", \"c\", \"a\"])\n        >>> ci.ordered\n        False\n        >>> ci = ci.as_ordered()\n        >>> ci.ordered\n        True\n        \"\"\"\n        return self.set_ordered(True)\n\n    def as_unordered(self) -> Self:\n        \"\"\"\n        Set the Categorical to be unordered.\n\n        Returns\n        -------\n        Categorical\n            Unordered Categorical.\n\n        See Also\n        --------\n        as_ordered : Set the Categorical to be ordered.\n\n        Examples\n        --------\n        For :class:`pandas.Series`:\n\n        >>> raw_cat = pd.Categorical([\"a\", \"b\", \"c\", \"a\"], ordered=True)\n        >>> ser = pd.Series(raw_cat)\n        >>> ser.cat.ordered\n        True\n        >>> ser = ser.cat.as_unordered()\n        >>> ser.cat.ordered\n        False\n\n        For :class:`pandas.CategoricalIndex`:\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"b\", \"c\", \"a\"], ordered=True)\n        >>> ci.ordered\n        True\n        >>> ci = ci.as_unordered()\n        >>> ci.ordered\n        False\n        \"\"\"\n        return self.set_ordered(False)\n\n    def set_categories(\n        self, new_categories, ordered=None, rename: bool = False\n    ) -> Self:\n        \"\"\"\n        Set the categories to the specified new categories.\n\n        ``new_categories`` can include new categories (which will result in\n        unused categories) or remove old categories (which results in values\n        set to ``NaN``). If ``rename=True``, the categories will simply be renamed\n        (less or more items than in old categories will result in values set to\n        ``NaN`` or in unused categories respectively).\n\n        This method can be used to perform more than one action of adding,\n        removing, and reordering simultaneously and is therefore faster than\n        performing the individual steps via the more specialised methods.\n\n        On the other hand this methods does not do checks (e.g., whether the\n        old categories are included in the new categories on a reorder), which\n        can result in surprising changes, for example when using special string\n        dtypes, which do not consider a S1 string equal to a single char\n        python string.\n\n        Parameters\n        ----------\n        new_categories : Index-like\n           The categories in new order.\n        ordered : bool, default None\n           Whether or not the categorical is treated as a ordered categorical.\n           If not given, do not change the ordered information.\n        rename : bool, default False\n           Whether or not the new_categories should be considered as a rename\n           of the old categories or as reordered categories.\n\n        Returns\n        -------\n        Categorical\n            New categories to be used, with optional ordering changes.\n\n        Raises\n        ------\n        ValueError\n            If new_categories does not validate as categories\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n\n        Examples\n        --------\n        For :class:`pandas.Series`:\n\n        >>> raw_cat = pd.Categorical(\n        ...     [\"a\", \"b\", \"c\", \"A\"], categories=[\"a\", \"b\", \"c\"], ordered=True\n        ... )\n        >>> ser = pd.Series(raw_cat)\n        >>> ser\n        0   a\n        1   b\n        2   c\n        3   NaN\n        dtype: category\n        Categories (3, object): ['a' < 'b' < 'c']\n\n        >>> ser.cat.set_categories([\"A\", \"B\", \"C\"], rename=True)\n        0   A\n        1   B\n        2   C\n        3   NaN\n        dtype: category\n        Categories (3, object): ['A' < 'B' < 'C']\n\n        For :class:`pandas.CategoricalIndex`:\n\n        >>> ci = pd.CategoricalIndex(\n        ...     [\"a\", \"b\", \"c\", \"A\"], categories=[\"a\", \"b\", \"c\"], ordered=True\n        ... )\n        >>> ci\n        CategoricalIndex(['a', 'b', 'c', nan], categories=['a', 'b', 'c'],\n                         ordered=True, dtype='category')\n\n        >>> ci.set_categories([\"A\", \"b\", \"c\"])\n        CategoricalIndex([nan, 'b', 'c', nan], categories=['A', 'b', 'c'],\n                         ordered=True, dtype='category')\n        >>> ci.set_categories([\"A\", \"b\", \"c\"], rename=True)\n        CategoricalIndex(['A', 'b', 'c', nan], categories=['A', 'b', 'c'],\n                         ordered=True, dtype='category')\n        \"\"\"\n\n        if ordered is None:\n            ordered = self.dtype.ordered\n        new_dtype = CategoricalDtype(new_categories, ordered=ordered)\n\n        cat = self.copy()\n        if rename:\n            if cat.dtype.categories is not None and len(new_dtype.categories) < len(\n                cat.dtype.categories\n            ):\n                # remove all _codes which are larger and set to -1/NaN\n                cat._codes[cat._codes >= len(new_dtype.categories)] = -1\n            codes = cat._codes\n        else:\n            codes = recode_for_categories(\n                cat.codes, cat.categories, new_dtype.categories\n            )\n        NDArrayBacked.__init__(cat, codes, new_dtype)\n        return cat\n\n    def rename_categories(self, new_categories) -> Self:\n        \"\"\"\n        Rename categories.\n\n        This method is commonly used to re-label or adjust the\n        category names in categorical data without changing the\n        underlying data. It is useful in situations where you want\n        to modify the labels used for clarity, consistency,\n        or readability.\n\n        Parameters\n        ----------\n        new_categories : list-like, dict-like or callable\n\n            New categories which will replace old categories.\n\n            * list-like: all items must be unique and the number of items in\n              the new categories must match the existing number of categories.\n\n            * dict-like: specifies a mapping from\n              old categories to new. Categories not contained in the mapping\n              are passed through and extra categories in the mapping are\n              ignored.\n\n            * callable : a callable that is called on all items in the old\n              categories and whose return values comprise the new categories.\n\n        Returns\n        -------\n        Categorical\n            Categorical with renamed categories.\n\n        Raises\n        ------\n        ValueError\n            If new categories are list-like and do not have the same number of\n            items than the current categories or do not validate as categories\n\n        See Also\n        --------\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n\n        Examples\n        --------\n        >>> c = pd.Categorical([\"a\", \"a\", \"b\"])\n        >>> c.rename_categories([0, 1])\n        [0, 0, 1]\n        Categories (2, int64): [0, 1]\n\n        For dict-like ``new_categories``, extra keys are ignored and\n        categories not in the dictionary are passed through\n\n        >>> c.rename_categories({\"a\": \"A\", \"c\": \"C\"})\n        ['A', 'A', 'b']\n        Categories (2, object): ['A', 'b']\n\n        You may also provide a callable to create the new categories\n\n        >>> c.rename_categories(lambda x: x.upper())\n        ['A', 'A', 'B']\n        Categories (2, object): ['A', 'B']\n        \"\"\"\n\n        if is_dict_like(new_categories):\n            new_categories = [\n                new_categories.get(item, item) for item in self.categories\n            ]\n        elif callable(new_categories):\n            new_categories = [new_categories(item) for item in self.categories]\n\n        cat = self.copy()\n        cat._set_categories(new_categories)\n        return cat\n\n    def reorder_categories(self, new_categories, ordered=None) -> Self:\n        \"\"\"\n        Reorder categories as specified in new_categories.\n\n        ``new_categories`` need to include all old categories and no new category\n        items.\n\n        Parameters\n        ----------\n        new_categories : Index-like\n           The categories in new order.\n        ordered : bool, optional\n           Whether or not the categorical is treated as a ordered categorical.\n           If not given, do not change the ordered information.\n\n        Returns\n        -------\n        Categorical\n            Categorical with reordered categories.\n\n        Raises\n        ------\n        ValueError\n            If the new categories do not contain all old category items or any\n            new ones\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n\n        Examples\n        --------\n        For :class:`pandas.Series`:\n\n        >>> ser = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\n        >>> ser = ser.cat.reorder_categories([\"c\", \"b\", \"a\"], ordered=True)\n        >>> ser\n        0   a\n        1   b\n        2   c\n        3   a\n        dtype: category\n        Categories (3, object): ['c' < 'b' < 'a']\n\n        >>> ser.sort_values()\n        2   c\n        1   b\n        0   a\n        3   a\n        dtype: category\n        Categories (3, object): ['c' < 'b' < 'a']\n\n        For :class:`pandas.CategoricalIndex`:\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"b\", \"c\", \"a\"])\n        >>> ci\n        CategoricalIndex(['a', 'b', 'c', 'a'], categories=['a', 'b', 'c'],\n                         ordered=False, dtype='category')\n        >>> ci.reorder_categories([\"c\", \"b\", \"a\"], ordered=True)\n        CategoricalIndex(['a', 'b', 'c', 'a'], categories=['c', 'b', 'a'],\n                         ordered=True, dtype='category')\n        \"\"\"\n        if (\n            len(self.categories) != len(new_categories)\n            or not self.categories.difference(new_categories).empty\n        ):\n            raise ValueError(\n                \"items in new_categories are not the same as in old categories\"\n            )\n        return self.set_categories(new_categories, ordered=ordered)\n\n    def add_categories(self, new_categories) -> Self:\n        \"\"\"\n        Add new categories.\n\n        `new_categories` will be included at the last/highest place in the\n        categories and will be unused directly after this call.\n\n        Parameters\n        ----------\n        new_categories : category or list-like of category\n            The new categories to be included.\n\n        Returns\n        -------\n        Categorical\n            Categorical with new categories added.\n\n        Raises\n        ------\n        ValueError\n            If the new categories include old categories or do not validate as\n            categories\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n\n        Examples\n        --------\n        >>> c = pd.Categorical([\"c\", \"b\", \"c\"])\n        >>> c\n        ['c', 'b', 'c']\n        Categories (2, object): ['b', 'c']\n\n        >>> c.add_categories([\"d\", \"a\"])\n        ['c', 'b', 'c']\n        Categories (4, object): ['b', 'c', 'd', 'a']\n        \"\"\"\n\n        if not is_list_like(new_categories):\n            new_categories = [new_categories]\n        already_included = set(new_categories) & set(self.dtype.categories)\n        if len(already_included) != 0:\n            raise ValueError(\n                f\"new categories must not include old categories: {already_included}\"\n            )\n\n        if hasattr(new_categories, \"dtype\"):\n            from pandas import Series\n\n            dtype = find_common_type(\n                [self.dtype.categories.dtype, new_categories.dtype]\n            )\n            new_categories = Series(\n                list(self.dtype.categories) + list(new_categories), dtype=dtype\n            )\n        else:\n            new_categories = list(self.dtype.categories) + list(new_categories)\n\n        new_dtype = CategoricalDtype(new_categories, self.ordered)\n        cat = self.copy()\n        codes = coerce_indexer_dtype(cat._ndarray, new_dtype.categories)\n        NDArrayBacked.__init__(cat, codes, new_dtype)\n        return cat\n\n    def remove_categories(self, removals) -> Self:\n        \"\"\"\n        Remove the specified categories.\n\n        The ``removals`` argument must be a subset of the current categories.\n        Any values that were part of the removed categories will be set to NaN.\n\n        Parameters\n        ----------\n        removals : category or list of categories\n           The categories which should be removed.\n\n        Returns\n        -------\n        Categorical\n            Categorical with removed categories.\n\n        Raises\n        ------\n        ValueError\n            If the removals are not contained in the categories\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n\n        Examples\n        --------\n        >>> c = pd.Categorical([\"a\", \"c\", \"b\", \"c\", \"d\"])\n        >>> c\n        ['a', 'c', 'b', 'c', 'd']\n        Categories (4, object): ['a', 'b', 'c', 'd']\n\n        >>> c.remove_categories([\"d\", \"a\"])\n        [NaN, 'c', 'b', 'c', NaN]\n        Categories (2, object): ['b', 'c']\n        \"\"\"\n        from pandas import Index\n\n        if not is_list_like(removals):\n            removals = [removals]\n\n        removals = Index(removals).unique().dropna()\n        new_categories = (\n            self.dtype.categories.difference(removals, sort=False)\n            if self.dtype.ordered is True\n            else self.dtype.categories.difference(removals)\n        )\n        not_included = removals.difference(self.dtype.categories)\n\n        if len(not_included) != 0:\n            not_included = set(not_included)\n            raise ValueError(f\"removals must all be in old categories: {not_included}\")\n\n        return self.set_categories(new_categories, ordered=self.ordered, rename=False)\n\n    def remove_unused_categories(self) -> Self:\n        \"\"\"\n        Remove categories which are not used.\n\n        This method is useful when working with datasets\n        that undergo dynamic changes where categories may no longer be\n        relevant, allowing to maintain a clean, efficient data structure.\n\n        Returns\n        -------\n        Categorical\n            Categorical with unused categories dropped.\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        set_categories : Set the categories to the specified ones.\n\n        Examples\n        --------\n        >>> c = pd.Categorical([\"a\", \"c\", \"b\", \"c\", \"d\"])\n        >>> c\n        ['a', 'c', 'b', 'c', 'd']\n        Categories (4, object): ['a', 'b', 'c', 'd']\n\n        >>> c[2] = \"a\"\n        >>> c[4] = \"c\"\n        >>> c\n        ['a', 'c', 'a', 'c', 'c']\n        Categories (4, object): ['a', 'b', 'c', 'd']\n\n        >>> c.remove_unused_categories()\n        ['a', 'c', 'a', 'c', 'c']\n        Categories (2, object): ['a', 'c']\n        \"\"\"\n        idx, inv = np.unique(self._codes, return_inverse=True)\n\n        if idx.size != 0 and idx[0] == -1:  # na sentinel\n            idx, inv = idx[1:], inv - 1\n\n        new_categories = self.dtype.categories.take(idx)\n        new_dtype = CategoricalDtype._from_fastpath(\n            new_categories, ordered=self.ordered\n        )\n        new_codes = coerce_indexer_dtype(inv, new_dtype.categories)\n\n        cat = self.copy()\n        NDArrayBacked.__init__(cat, new_codes, new_dtype)\n        return cat\n\n    # ------------------------------------------------------------------\n\n    def map(\n        self,\n        mapper,\n        na_action: Literal[\"ignore\"] | None = None,\n    ):\n        \"\"\"\n        Map categories using an input mapping or function.\n\n        Maps the categories to new categories. If the mapping correspondence is\n        one-to-one the result is a :class:`~pandas.Categorical` which has the\n        same order property as the original, otherwise a :class:`~pandas.Index`\n        is returned. NaN values are unaffected.\n\n        If a `dict` or :class:`~pandas.Series` is used any unmapped category is\n        mapped to `NaN`. Note that if this happens an :class:`~pandas.Index`\n        will be returned.\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NaN values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        pandas.Categorical or pandas.Index\n            Mapped categorical.\n\n        See Also\n        --------\n        CategoricalIndex.map : Apply a mapping correspondence on a\n            :class:`~pandas.CategoricalIndex`.\n        Index.map : Apply a mapping correspondence on an\n            :class:`~pandas.Index`.\n        Series.map : Apply a mapping correspondence on a\n            :class:`~pandas.Series`.\n        Series.apply : Apply more complex functions on a\n            :class:`~pandas.Series`.\n\n        Examples\n        --------\n        >>> cat = pd.Categorical([\"a\", \"b\", \"c\"])\n        >>> cat\n        ['a', 'b', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> cat.map(lambda x: x.upper(), na_action=None)\n        ['A', 'B', 'C']\n        Categories (3, object): ['A', 'B', 'C']\n        >>> cat.map({\"a\": \"first\", \"b\": \"second\", \"c\": \"third\"}, na_action=None)\n        ['first', 'second', 'third']\n        Categories (3, object): ['first', 'second', 'third']\n\n        If the mapping is one-to-one the ordering of the categories is\n        preserved:\n\n        >>> cat = pd.Categorical([\"a\", \"b\", \"c\"], ordered=True)\n        >>> cat\n        ['a', 'b', 'c']\n        Categories (3, object): ['a' < 'b' < 'c']\n        >>> cat.map({\"a\": 3, \"b\": 2, \"c\": 1}, na_action=None)\n        [3, 2, 1]\n        Categories (3, int64): [3 < 2 < 1]\n\n        If the mapping is not one-to-one an :class:`~pandas.Index` is returned:\n\n        >>> cat.map({\"a\": \"first\", \"b\": \"second\", \"c\": \"first\"}, na_action=None)\n        Index(['first', 'second', 'first'], dtype='object')\n\n        If a `dict` is used, all unmapped categories are mapped to `NaN` and\n        the result is an :class:`~pandas.Index`:\n\n        >>> cat.map({\"a\": \"first\", \"b\": \"second\"}, na_action=None)\n        Index(['first', 'second', nan], dtype='object')\n        \"\"\"\n        assert callable(mapper) or is_dict_like(mapper)\n\n        new_categories = self.categories.map(mapper)\n\n        has_nans = np.any(self._codes == -1)\n\n        na_val = np.nan\n        if na_action is None and has_nans:\n            na_val = mapper(np.nan) if callable(mapper) else mapper.get(np.nan, np.nan)\n\n        if new_categories.is_unique and not new_categories.hasnans and na_val is np.nan:\n            new_dtype = CategoricalDtype(new_categories, ordered=self.ordered)\n            return self.from_codes(self._codes.copy(), dtype=new_dtype, validate=False)\n\n        if has_nans:\n            new_categories = new_categories.insert(len(new_categories), na_val)\n\n        return np.take(new_categories, self._codes)\n\n    __eq__ = _cat_compare_op(operator.eq)\n    __ne__ = _cat_compare_op(operator.ne)\n    __lt__ = _cat_compare_op(operator.lt)\n    __gt__ = _cat_compare_op(operator.gt)\n    __le__ = _cat_compare_op(operator.le)\n    __ge__ = _cat_compare_op(operator.ge)\n\n    # -------------------------------------------------------------\n    # Validators; ideally these can be de-duplicated\n\n    def _validate_setitem_value(self, value):\n        if not is_hashable(value):\n            # wrap scalars and hashable-listlikes in list\n            return self._validate_listlike(value)\n        else:\n            return self._validate_scalar(value)\n\n    def _validate_scalar(self, fill_value):\n        \"\"\"\n        Convert a user-facing fill_value to a representation to use with our\n        underlying ndarray, raising TypeError if this is not possible.\n\n        Parameters\n        ----------\n        fill_value : object\n\n        Returns\n        -------\n        fill_value : int\n\n        Raises\n        ------\n        TypeError\n        \"\"\"\n\n        if is_valid_na_for_dtype(fill_value, self.categories.dtype):\n            fill_value = -1\n        elif fill_value in self.categories:\n            fill_value = self._unbox_scalar(fill_value)\n        else:\n            raise TypeError(\n                \"Cannot setitem on a Categorical with a new \"\n                f\"category ({fill_value}), set the categories first\"\n            ) from None\n        return fill_value\n\n    @classmethod\n    def _validate_codes_for_dtype(cls, codes, *, dtype: CategoricalDtype) -> np.ndarray:\n        if isinstance(codes, ExtensionArray) and is_integer_dtype(codes.dtype):\n            # Avoid the implicit conversion of Int to object\n            if isna(codes).any():\n                raise ValueError(\"codes cannot contain NA values\")\n            codes = codes.to_numpy(dtype=np.int64)\n        else:\n            codes = np.asarray(codes)\n        if len(codes) and codes.dtype.kind not in \"iu\":\n            raise ValueError(\"codes need to be array-like integers\")\n\n        if len(codes) and (codes.max() >= len(dtype.categories) or codes.min() < -1):\n            raise ValueError(\"codes need to be between -1 and len(categories)-1\")\n        return codes\n\n    # -------------------------------------------------------------\n\n    @ravel_compat\n    def __array__(\n        self, dtype: NpDtype | None = None, copy: bool | None = None\n    ) -> np.ndarray:\n        \"\"\"\n        The numpy array interface.\n\n        Users should not call this directly. Rather, it is invoked by\n        :func:`numpy.array` and :func:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : np.dtype or None\n            Specifies the the dtype for the array.\n\n        copy : bool or None, optional\n            See :func:`numpy.asarray`.\n\n        Returns\n        -------\n        numpy.array\n            A numpy array of either the specified dtype or,\n            if dtype==None (default), the same dtype as\n            categorical.categories.dtype.\n\n        See Also\n        --------\n        numpy.asarray : Convert input to numpy.ndarray.\n\n        Examples\n        --------\n\n        >>> cat = pd.Categorical([\"a\", \"b\"], ordered=True)\n\n        The following calls ``cat.__array__``\n\n        >>> np.asarray(cat)\n        array(['a', 'b'], dtype=object)\n        \"\"\"\n        if copy is False:\n            raise ValueError(\n                \"Unable to avoid copy while creating an array as requested.\"\n            )\n\n        ret = take_nd(self.categories._values, self._codes)\n        # When we're a Categorical[ExtensionArray], like Interval,\n        # we need to ensure __array__ gets all the way to an\n        # ndarray.\n\n        # `take_nd` should already make a copy, so don't force again.\n        return np.asarray(ret, dtype=dtype)\n\n    def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n        # for binary ops, use our custom dunder methods\n        result = arraylike.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        if \"out\" in kwargs:\n            # e.g. test_numpy_ufuncs_out\n            return arraylike.dispatch_ufunc_with_out(\n                self, ufunc, method, *inputs, **kwargs\n            )\n\n        if method == \"reduce\":\n            # e.g. TestCategoricalAnalytics::test_min_max_ordered\n            result = arraylike.dispatch_reduction_ufunc(\n                self, ufunc, method, *inputs, **kwargs\n            )\n            if result is not NotImplemented:\n                return result\n\n        # for all other cases, raise for now (similarly as what happens in\n        # Series.__array_prepare__)\n        raise TypeError(\n            f\"Object with dtype {self.dtype} cannot perform \"\n            f\"the numpy op {ufunc.__name__}\"\n        )\n\n    def __setstate__(self, state) -> None:\n        \"\"\"Necessary for making this object picklable\"\"\"\n        if not isinstance(state, dict):\n            return super().__setstate__(state)\n\n        if \"_dtype\" not in state:\n            state[\"_dtype\"] = CategoricalDtype(state[\"_categories\"], state[\"_ordered\"])\n\n        if \"_codes\" in state and \"_ndarray\" not in state:\n            # backward compat, changed what is property vs attribute\n            state[\"_ndarray\"] = state.pop(\"_codes\")\n\n        super().__setstate__(state)\n\n    @property\n    def nbytes(self) -> int:\n        return self._codes.nbytes + self.dtype.categories.values.nbytes\n\n    def memory_usage(self, deep: bool = False) -> int:\n        \"\"\"\n        Memory usage of my values\n\n        Parameters\n        ----------\n        deep : bool\n            Introspect the data deeply, interrogate\n            `object` dtypes for system-level memory consumption\n\n        Returns\n        -------\n        bytes used\n\n        Notes\n        -----\n        Memory usage does not include memory consumed by elements that\n        are not components of the array if deep=False\n\n        See Also\n        --------\n        numpy.ndarray.nbytes\n        \"\"\"\n        return self._codes.nbytes + self.dtype.categories.memory_usage(deep=deep)\n\n    def isna(self) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Detect missing values\n\n        Missing values (-1 in .codes) are detected.\n\n        Returns\n        -------\n        np.ndarray[bool] of whether my values are null\n\n        See Also\n        --------\n        isna : Top-level isna.\n        isnull : Alias of isna.\n        Categorical.notna : Boolean inverse of Categorical.isna.\n\n        \"\"\"\n        return self._codes == -1\n\n    isnull = isna\n\n    def notna(self) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Inverse of isna\n\n        Both missing values (-1 in .codes) and NA as a category are detected as\n        null.\n\n        Returns\n        -------\n        np.ndarray[bool] of whether my values are not null\n\n        See Also\n        --------\n        notna : Top-level notna.\n        notnull : Alias of notna.\n        Categorical.isna : Boolean inverse of Categorical.notna.\n\n        \"\"\"\n        return ~self.isna()\n\n    notnull = notna\n\n    def value_counts(self, dropna: bool = True) -> Series:\n        \"\"\"\n        Return a Series containing counts of each category.\n\n        Every category will have an entry, even those with a count of 0.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't include counts of NaN.\n\n        Returns\n        -------\n        counts : Series\n\n        See Also\n        --------\n        Series.value_counts\n        \"\"\"\n        from pandas import (\n            CategoricalIndex,\n            Series,\n        )\n\n        code, cat = self._codes, self.categories\n        ncat, mask = (len(cat), code >= 0)\n        ix, clean = np.arange(ncat), mask.all()\n\n        if dropna or clean:\n            obs = code if clean else code[mask]\n            count = np.bincount(obs, minlength=ncat or 0)\n        else:\n            count = np.bincount(np.where(mask, code, ncat))\n            ix = np.append(ix, -1)\n\n        ix = coerce_indexer_dtype(ix, self.dtype.categories)\n        ix_categorical = self._from_backing_data(ix)\n\n        return Series(\n            count,\n            index=CategoricalIndex(ix_categorical),\n            dtype=\"int64\",\n            name=\"count\",\n            copy=False,\n        )\n\n    # error: Argument 2 of \"_empty\" is incompatible with supertype\n    # \"NDArrayBackedExtensionArray\"; supertype defines the argument type as\n    # \"ExtensionDtype\"\n    @classmethod\n    def _empty(  # type: ignore[override]\n        cls, shape: Shape, dtype: CategoricalDtype\n    ) -> Self:\n        \"\"\"\n        Analogous to np.empty(shape, dtype=dtype)\n\n        Parameters\n        ----------\n        shape : tuple[int]\n        dtype : CategoricalDtype\n        \"\"\"\n        arr = cls._from_sequence([], dtype=dtype)\n\n        # We have to use np.zeros instead of np.empty otherwise the resulting\n        #  ndarray may contain codes not supported by this dtype, in which\n        #  case repr(result) could segfault.\n        backing = np.zeros(shape, dtype=arr._ndarray.dtype)\n\n        return arr._from_backing_data(backing)\n\n    def _internal_get_values(self) -> ArrayLike:\n        \"\"\"\n        Return the values.\n\n        For internal compatibility with pandas formatting.\n\n        Returns\n        -------\n        np.ndarray or ExtensionArray\n            A numpy array or ExtensionArray of the same dtype as\n            categorical.categories.dtype.\n        \"\"\"\n        # if we are a datetime and period index, return Index to keep metadata\n        if needs_i8_conversion(self.categories.dtype):\n            return self.categories.take(self._codes, fill_value=NaT)._values\n        elif is_integer_dtype(self.categories.dtype) and -1 in self._codes:\n            return (\n                self.categories.astype(\"object\")\n                .take(self._codes, fill_value=np.nan)\n                ._values\n            )\n        return np.array(self)\n\n    def check_for_ordered(self, op) -> None:\n        \"\"\"assert that we are ordered\"\"\"\n        if not self.ordered:\n            raise TypeError(\n                f\"Categorical is not ordered for operation {op}\\n\"\n                \"you can use .as_ordered() to change the \"\n                \"Categorical to an ordered one\\n\"\n            )\n\n    def argsort(\n        self, *, ascending: bool = True, kind: SortKind = \"quicksort\", **kwargs\n    ) -> npt.NDArray[np.intp]:\n        \"\"\"\n        Return the indices that would sort the Categorical.\n\n        Missing values are sorted at the end.\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            Whether the indices should result in an ascending\n            or descending sort.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n            Sorting algorithm.\n        **kwargs:\n            passed through to :func:`numpy.argsort`.\n\n        Returns\n        -------\n        np.ndarray[np.intp]\n\n        See Also\n        --------\n        numpy.ndarray.argsort\n\n        Notes\n        -----\n        While an ordering is applied to the category values, arg-sorting\n        in this context refers more to organizing and grouping together\n        based on matching category values. Thus, this function can be\n        called on an unordered Categorical instance unlike the functions\n        'Categorical.min' and 'Categorical.max'.\n\n        Examples\n        --------\n        >>> pd.Categorical([\"b\", \"b\", \"a\", \"c\"]).argsort()\n        array([2, 0, 1, 3])\n\n        >>> cat = pd.Categorical(\n        ...     [\"b\", \"b\", \"a\", \"c\"], categories=[\"c\", \"b\", \"a\"], ordered=True\n        ... )\n        >>> cat.argsort()\n        array([3, 0, 1, 2])\n\n        Missing values are placed at the end\n\n        >>> cat = pd.Categorical([2, None, 1])\n        >>> cat.argsort()\n        array([2, 0, 1])\n        \"\"\"\n        return super().argsort(ascending=ascending, kind=kind, **kwargs)\n\n    @overload\n    def sort_values(\n        self,\n        *,\n        inplace: Literal[False] = ...,\n        ascending: bool = ...,\n        na_position: str = ...,\n    ) -> Self: ...\n\n    @overload\n    def sort_values(\n        self, *, inplace: Literal[True], ascending: bool = ..., na_position: str = ...\n    ) -> None: ...\n\n    def sort_values(\n        self,\n        *,\n        inplace: bool = False,\n        ascending: bool = True,\n        na_position: str = \"last\",\n    ) -> Self | None:\n        \"\"\"\n        Sort the Categorical by category value returning a new\n        Categorical by default.\n\n        While an ordering is applied to the category values, sorting in this\n        context refers more to organizing and grouping together based on\n        matching category values. Thus, this function can be called on an\n        unordered Categorical instance unlike the functions 'Categorical.min'\n        and 'Categorical.max'.\n\n        Parameters\n        ----------\n        inplace : bool, default False\n            Do operation in place.\n        ascending : bool, default True\n            Order ascending. Passing False orders descending. The\n            ordering parameter provides the method by which the\n            category values are organized.\n        na_position : {'first', 'last'} (optional, default='last')\n            'first' puts NaNs at the beginning\n            'last' puts NaNs at the end\n\n        Returns\n        -------\n        Categorical or None\n\n        See Also\n        --------\n        Categorical.sort\n        Series.sort_values\n\n        Examples\n        --------\n        >>> c = pd.Categorical([1, 2, 2, 1, 5])\n        >>> c\n        [1, 2, 2, 1, 5]\n        Categories (3, int64): [1, 2, 5]\n        >>> c.sort_values()\n        [1, 1, 2, 2, 5]\n        Categories (3, int64): [1, 2, 5]\n        >>> c.sort_values(ascending=False)\n        [5, 2, 2, 1, 1]\n        Categories (3, int64): [1, 2, 5]\n\n        >>> c = pd.Categorical([1, 2, 2, 1, 5])\n\n        'sort_values' behaviour with NaNs. Note that 'na_position'\n        is independent of the 'ascending' parameter:\n\n        >>> c = pd.Categorical([np.nan, 2, 2, np.nan, 5])\n        >>> c\n        [NaN, 2, 2, NaN, 5]\n        Categories (2, int64): [2, 5]\n        >>> c.sort_values()\n        [2, 2, 5, NaN, NaN]\n        Categories (2, int64): [2, 5]\n        >>> c.sort_values(ascending=False)\n        [5, 2, 2, NaN, NaN]\n        Categories (2, int64): [2, 5]\n        >>> c.sort_values(na_position=\"first\")\n        [NaN, NaN, 2, 2, 5]\n        Categories (2, int64): [2, 5]\n        >>> c.sort_values(ascending=False, na_position=\"first\")\n        [NaN, NaN, 5, 2, 2]\n        Categories (2, int64): [2, 5]\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if na_position not in [\"last\", \"first\"]:\n            raise ValueError(f\"invalid na_position: {na_position!r}\")\n\n        sorted_idx = nargsort(self, ascending=ascending, na_position=na_position)\n\n        if not inplace:\n            codes = self._codes[sorted_idx]\n            return self._from_backing_data(codes)\n        self._codes[:] = self._codes[sorted_idx]\n        return None\n\n    def _rank(\n        self,\n        *,\n        axis: AxisInt = 0,\n        method: str = \"average\",\n        na_option: str = \"keep\",\n        ascending: bool = True,\n        pct: bool = False,\n    ):\n        \"\"\"\n        See Series.rank.__doc__.\n        \"\"\"\n        if axis != 0:\n            raise NotImplementedError\n        vff = self._values_for_rank()\n        return algorithms.rank(\n            vff,\n            axis=axis,\n            method=method,\n            na_option=na_option,\n            ascending=ascending,\n            pct=pct,\n        )\n\n    def _values_for_rank(self) -> np.ndarray:\n        \"\"\"\n        For correctly ranking ordered categorical data. See GH#15420\n\n        Ordered categorical data should be ranked on the basis of\n        codes with -1 translated to NaN.\n\n        Returns\n        -------\n        numpy.array\n\n        \"\"\"\n        from pandas import Series\n\n        if self.ordered:\n            values = self.codes\n            mask = values == -1\n            if mask.any():\n                values = values.astype(\"float64\")\n                values[mask] = np.nan\n        elif is_any_real_numeric_dtype(self.categories.dtype):\n            values = np.array(self)\n        else:\n            #  reorder the categories (so rank can use the float codes)\n            #  instead of passing an object array to rank\n            values = np.array(\n                self.rename_categories(\n                    Series(self.categories, copy=False).rank().values\n                )\n            )\n        return values\n\n    def _hash_pandas_object(\n        self, *, encoding: str, hash_key: str, categorize: bool\n    ) -> npt.NDArray[np.uint64]:\n        \"\"\"\n        Hash a Categorical by hashing its categories, and then mapping the codes\n        to the hashes.\n\n        Parameters\n        ----------\n        encoding : str\n        hash_key : str\n        categorize : bool\n            Ignored for Categorical.\n\n        Returns\n        -------\n        np.ndarray[uint64]\n        \"\"\"\n        # Note we ignore categorize, as we are already Categorical.\n        from pandas.core.util.hashing import hash_array\n\n        # Convert ExtensionArrays to ndarrays\n        values = np.asarray(self.categories._values)\n        hashed = hash_array(values, encoding, hash_key, categorize=False)\n\n        # we have uint64, as we don't directly support missing values\n        # we don't want to use take_nd which will coerce to float\n        # instead, directly construct the result with a\n        # max(np.uint64) as the missing value indicator\n        #\n        # TODO: GH#15362\n\n        mask = self.isna()\n        if len(hashed):\n            result = hashed.take(self._codes)\n        else:\n            result = np.zeros(len(mask), dtype=\"uint64\")\n\n        if mask.any():\n            result[mask] = lib.u8max\n\n        return result\n\n    # ------------------------------------------------------------------\n    # NDArrayBackedExtensionArray compat\n\n    @property\n    def _codes(self) -> np.ndarray:\n        return self._ndarray\n\n    def _box_func(self, i: int):\n        if i == -1:\n            return np.nan\n        return self.categories[i]\n\n    def _unbox_scalar(self, key) -> int:\n        # searchsorted is very performance sensitive. By converting codes\n        # to same dtype as self.codes, we get much faster performance.\n        code = self.categories.get_loc(key)\n        code = self._ndarray.dtype.type(code)\n        return code\n\n    # ------------------------------------------------------------------\n\n    def __iter__(self) -> Iterator:\n        \"\"\"\n        Returns an Iterator over the values of this Categorical.\n        \"\"\"\n        if self.ndim == 1:\n            return iter(self._internal_get_values().tolist())\n        else:\n            return (self[n] for n in range(len(self)))\n\n    def __contains__(self, key) -> bool:\n        \"\"\"\n        Returns True if `key` is in this Categorical.\n        \"\"\"\n        # if key is a NaN, check if any NaN is in self.\n        if is_valid_na_for_dtype(key, self.categories.dtype):\n            return bool(self.isna().any())\n\n        return contains(self, key, container=self._codes)\n\n    # ------------------------------------------------------------------\n    # Rendering Methods\n\n    # error: Return type \"None\" of \"_formatter\" incompatible with return\n    # type \"Callable[[Any], str | None]\" in supertype \"ExtensionArray\"\n    def _formatter(self, boxed: bool = False) -> None:  # type: ignore[override]\n        # Returning None here will cause format_array to do inference.\n        return None\n\n    def _repr_categories(self) -> list[str]:\n        \"\"\"\n        return the base repr for the categories\n        \"\"\"\n        max_categories = (\n            10\n            if get_option(\"display.max_categories\") == 0\n            else get_option(\"display.max_categories\")\n        )\n        from pandas.io.formats import format as fmt\n\n        format_array = partial(\n            fmt.format_array, formatter=None, quoting=QUOTE_NONNUMERIC\n        )\n        if len(self.categories) > max_categories:\n            num = max_categories // 2\n            head = format_array(self.categories[:num]._values)\n            tail = format_array(self.categories[-num:]._values)\n            category_strs = head + [\"...\"] + tail\n        else:\n            category_strs = format_array(self.categories._values)\n\n        # Strip all leading spaces, which format_array adds for columns...\n        category_strs = [x.strip() for x in category_strs]\n        return category_strs\n\n    def _get_repr_footer(self) -> str:\n        \"\"\"\n        Returns a string representation of the footer.\n        \"\"\"\n        category_strs = self._repr_categories()\n        dtype = str(self.categories.dtype)\n        levheader = f\"Categories ({len(self.categories)}, {dtype}): \"\n        width, _ = get_terminal_size()\n        max_width = get_option(\"display.width\") or width\n        if console.in_ipython_frontend():\n            # 0 = no breaks\n            max_width = 0\n        levstring = \"\"\n        start = True\n        cur_col_len = len(levheader)  # header\n        sep_len, sep = (3, \" < \") if self.ordered else (2, \", \")\n        linesep = f\"{sep.rstrip()}\\n\"  # remove whitespace\n        for val in category_strs:\n            if max_width != 0 and cur_col_len + sep_len + len(val) > max_width:\n                levstring += linesep + (\" \" * (len(levheader) + 1))\n                cur_col_len = len(levheader) + 1  # header + a whitespace\n            elif not start:\n                levstring += sep\n                cur_col_len += len(val)\n            levstring += val\n            start = False\n        # replace to simple save space by\n        return f\"{levheader}[{levstring.replace(' < ... < ', ' ... ')}]\"\n\n    def _get_values_repr(self) -> str:\n        from pandas.io.formats import format as fmt\n\n        assert len(self) > 0\n\n        vals = self._internal_get_values()\n        fmt_values = fmt.format_array(\n            vals,\n            None,\n            float_format=None,\n            na_rep=\"NaN\",\n            quoting=QUOTE_NONNUMERIC,\n        )\n\n        fmt_values = [i.strip() for i in fmt_values]\n        joined = \", \".join(fmt_values)\n        result = \"[\" + joined + \"]\"\n        return result\n\n    def __repr__(self) -> str:\n        \"\"\"\n        String representation.\n        \"\"\"\n        footer = self._get_repr_footer()\n        length = len(self)\n        max_len = 10\n        if length > max_len:\n            # In long cases we do not display all entries, so we add Length\n            #  information to the __repr__.\n            num = max_len // 2\n            head = self[:num]._get_values_repr()\n            tail = self[-(max_len - num) :]._get_values_repr()\n            body = f\"{head[:-1]}, ..., {tail[1:]}\"\n            length_info = f\"Length: {len(self)}\"\n            result = f\"{body}\\n{length_info}\\n{footer}\"\n        elif length > 0:\n            body = self._get_values_repr()\n            result = f\"{body}\\n{footer}\"\n        else:\n            # In the empty case we use a comma instead of newline to get\n            #  a more compact __repr__\n            body = \"[]\"\n            result = f\"{body}, {footer}\"\n\n        return result\n\n    # ------------------------------------------------------------------\n\n    def _validate_listlike(self, value):\n        # NB: here we assume scalar-like tuples have already been excluded\n        value = extract_array(value, extract_numpy=True)\n\n        # require identical categories set\n        if isinstance(value, Categorical):\n            if self.dtype != value.dtype:\n                raise TypeError(\n                    \"Cannot set a Categorical with another, \"\n                    \"without identical categories\"\n                )\n            # dtype equality implies categories_match_up_to_permutation\n            value = self._encode_with_my_categories(value)\n            return value._codes\n\n        from pandas import Index\n\n        # tupleize_cols=False for e.g. test_fillna_iterable_category GH#41914\n        to_add = Index._with_infer(value, tupleize_cols=False).difference(\n            self.categories\n        )\n\n        # no assignments of values not in categories, but it's always ok to set\n        # something to np.nan\n        if len(to_add) and not isna(to_add).all():\n            raise TypeError(\n                \"Cannot setitem on a Categorical with a new \"\n                \"category, set the categories first\"\n            )\n\n        codes = self.categories.get_indexer(value)\n        return codes.astype(self._ndarray.dtype, copy=False)\n\n    def _reverse_indexer(self) -> dict[Hashable, npt.NDArray[np.intp]]:\n        \"\"\"\n        Compute the inverse of a categorical, returning\n        a dict of categories -> indexers.\n\n        *This is an internal function*\n\n        Returns\n        -------\n        Dict[Hashable, np.ndarray[np.intp]]\n            dict of categories -> indexers\n\n        Examples\n        --------\n        >>> c = pd.Categorical(list(\"aabca\"))\n        >>> c\n        ['a', 'a', 'b', 'c', 'a']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> c.categories\n        Index(['a', 'b', 'c'], dtype='object')\n        >>> c.codes\n        array([0, 0, 1, 2, 0], dtype=int8)\n        >>> c._reverse_indexer()\n        {'a': array([0, 1, 4]), 'b': array([2]), 'c': array([3])}\n\n        \"\"\"\n        categories = self.categories\n        r, counts = libalgos.groupsort_indexer(\n            ensure_platform_int(self.codes), categories.size\n        )\n        counts = ensure_int64(counts).cumsum()\n        _result = (r[start:end] for start, end in zip(counts, counts[1:]))\n        return dict(zip(categories, _result))\n\n    # ------------------------------------------------------------------\n    # Reductions\n\n    def _reduce(\n        self, name: str, *, skipna: bool = True, keepdims: bool = False, **kwargs\n    ):\n        result = super()._reduce(name, skipna=skipna, keepdims=keepdims, **kwargs)\n        if name in [\"argmax\", \"argmin\"]:\n            # don't wrap in Categorical!\n            return result\n        if keepdims:\n            return type(self)(result, dtype=self.dtype)\n        else:\n            return result\n\n    def min(self, *, skipna: bool = True, **kwargs):\n        \"\"\"\n        The minimum value of the object.\n\n        Only ordered `Categoricals` have a minimum!\n\n        Raises\n        ------\n        TypeError\n            If the `Categorical` is not `ordered`.\n\n        Returns\n        -------\n        min : the minimum of this `Categorical`, NA value if empty\n        \"\"\"\n        nv.validate_minmax_axis(kwargs.get(\"axis\", 0))\n        nv.validate_min((), kwargs)\n        self.check_for_ordered(\"min\")\n\n        if not len(self._codes):\n            return self.dtype.na_value\n\n        good = self._codes != -1\n        if not good.all():\n            if skipna and good.any():\n                pointer = self._codes[good].min()\n            else:\n                return np.nan\n        else:\n            pointer = self._codes.min()\n        return self._wrap_reduction_result(None, pointer)\n\n    def max(self, *, skipna: bool = True, **kwargs):\n        \"\"\"\n        The maximum value of the object.\n\n        Only ordered `Categoricals` have a maximum!\n\n        Raises\n        ------\n        TypeError\n            If the `Categorical` is not `ordered`.\n\n        Returns\n        -------\n        max : the maximum of this `Categorical`, NA if array is empty\n        \"\"\"\n        nv.validate_minmax_axis(kwargs.get(\"axis\", 0))\n        nv.validate_max((), kwargs)\n        self.check_for_ordered(\"max\")\n\n        if not len(self._codes):\n            return self.dtype.na_value\n\n        good = self._codes != -1\n        if not good.all():\n            if skipna and good.any():\n                pointer = self._codes[good].max()\n            else:\n                return np.nan\n        else:\n            pointer = self._codes.max()\n        return self._wrap_reduction_result(None, pointer)\n\n    def _mode(self, dropna: bool = True) -> Categorical:\n        codes = self._codes\n        mask = None\n        if dropna:\n            mask = self.isna()\n\n        res_codes = algorithms.mode(codes, mask=mask)\n        res_codes = cast(np.ndarray, res_codes)\n        assert res_codes.dtype == codes.dtype\n        res = self._from_backing_data(res_codes)\n        return res\n\n    # ------------------------------------------------------------------\n    # ExtensionArray Interface\n\n    def unique(self) -> Self:\n        \"\"\"\n        Return the ``Categorical`` which ``categories`` and ``codes`` are\n        unique.\n\n        .. versionchanged:: 1.3.0\n\n            Previously, unused categories were dropped from the new categories.\n\n        Returns\n        -------\n        Categorical\n\n        See Also\n        --------\n        pandas.unique\n        CategoricalIndex.unique\n        Series.unique : Return unique values of Series object.\n\n        Examples\n        --------\n        >>> pd.Categorical(list(\"baabc\")).unique()\n        ['b', 'a', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> pd.Categorical(list(\"baab\"), categories=list(\"abc\"), ordered=True).unique()\n        ['b', 'a']\n        Categories (3, object): ['a' < 'b' < 'c']\n        \"\"\"\n        return super().unique()\n\n    def equals(self, other: object) -> bool:\n        \"\"\"\n        Returns True if categorical arrays are equal.\n\n        Parameters\n        ----------\n        other : `Categorical`\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        if not isinstance(other, Categorical):\n            return False\n        elif self._categories_match_up_to_permutation(other):\n            other = self._encode_with_my_categories(other)\n            return np.array_equal(self._codes, other._codes)\n        return False\n\n    def _accumulate(self, name: str, skipna: bool = True, **kwargs) -> Self:\n        func: Callable\n        if name == \"cummin\":\n            func = np.minimum.accumulate\n        elif name == \"cummax\":\n            func = np.maximum.accumulate\n        else:\n            raise TypeError(f\"Accumulation {name} not supported for {type(self)}\")\n        self.check_for_ordered(name)\n\n        codes = self.codes.copy()\n        mask = self.isna()\n        if func == np.minimum.accumulate:\n            codes[mask] = np.iinfo(codes.dtype.type).max\n        # no need to change codes for maximum because codes[mask] is already -1\n        if not skipna:\n            mask = np.maximum.accumulate(mask)\n\n        codes = func(codes)\n        codes[mask] = -1\n        return self._simple_new(codes, dtype=self._dtype)\n\n    @classmethod\n    def _concat_same_type(cls, to_concat: Sequence[Self], axis: AxisInt = 0) -> Self:\n        from pandas.core.dtypes.concat import union_categoricals\n\n        first = to_concat[0]\n        if axis >= first.ndim:\n            raise ValueError(\n                f\"axis {axis} is out of bounds for array of dimension {first.ndim}\"\n            )\n\n        if axis == 1:\n            # Flatten, concatenate then reshape\n            if not all(x.ndim == 2 for x in to_concat):\n                raise ValueError\n\n            # pass correctly-shaped to union_categoricals\n            tc_flat = []\n            for obj in to_concat:\n                tc_flat.extend([obj[:, i] for i in range(obj.shape[1])])\n\n            res_flat = cls._concat_same_type(tc_flat, axis=0)\n\n            result = res_flat.reshape(len(first), -1, order=\"F\")\n            return result\n\n        # error: Incompatible types in assignment (expression has type \"Categorical\",\n        # variable has type \"Self\")\n        result = union_categoricals(to_concat)  # type: ignore[assignment]\n        return result\n\n    # ------------------------------------------------------------------\n\n    def _encode_with_my_categories(self, other: Categorical) -> Categorical:\n        \"\"\"\n        Re-encode another categorical using this Categorical's categories.\n\n        Notes\n        -----\n        This assumes we have already checked\n        self._categories_match_up_to_permutation(other).\n        \"\"\"\n        # Indexing on codes is more efficient if categories are the same,\n        #  so we can apply some optimizations based on the degree of\n        #  dtype-matching.\n        codes = recode_for_categories(\n            other.codes, other.categories, self.categories, copy=False\n        )\n        return self._from_backing_data(codes)\n\n    def _categories_match_up_to_permutation(self, other: Categorical) -> bool:\n        \"\"\"\n        Returns True if categoricals are the same dtype\n          same categories, and same ordered\n\n        Parameters\n        ----------\n        other : Categorical\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return hash(self.dtype) == hash(other.dtype)\n\n    def describe(self) -> DataFrame:\n        \"\"\"\n        Describes this Categorical\n\n        Returns\n        -------\n        description: `DataFrame`\n            A dataframe with frequency and counts by category.\n        \"\"\"\n        counts = self.value_counts(dropna=False)\n        freqs = counts / counts.sum()\n\n        from pandas import Index\n        from pandas.core.reshape.concat import concat\n\n        result = concat([counts, freqs], ignore_index=True, axis=1)\n        result.columns = Index([\"counts\", \"freqs\"])\n        result.index.name = \"categories\"\n\n        return result\n\n    def isin(self, values: ArrayLike) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Check whether `values` are contained in Categorical.\n\n        Return a boolean NumPy Array showing whether each element in\n        the Categorical matches an element in the passed sequence of\n        `values` exactly.\n\n        Parameters\n        ----------\n        values : np.ndarray or ExtensionArray\n            The sequence of values to test. Passing in a single string will\n            raise a ``TypeError``. Instead, turn a single string into a\n            list of one element.\n\n        Returns\n        -------\n        np.ndarray[bool]\n\n        Raises\n        ------\n        TypeError\n          * If `values` is not a set or list-like\n\n        See Also\n        --------\n        pandas.Series.isin : Equivalent method on Series.\n\n        Examples\n        --------\n        >>> s = pd.Categorical([\"llama\", \"cow\", \"llama\", \"beetle\", \"llama\", \"hippo\"])\n        >>> s.isin([\"cow\", \"llama\"])\n        array([ True,  True,  True, False,  True, False])\n\n        Passing a single string as ``s.isin('llama')`` will raise an error. Use\n        a list of one element instead:\n\n        >>> s.isin([\"llama\"])\n        array([ True, False,  True, False,  True, False])\n        \"\"\"\n        null_mask = np.asarray(isna(values))\n        code_values = self.categories.get_indexer_for(values)\n        code_values = code_values[null_mask | (code_values >= 0)]\n        return algorithms.isin(self.codes, code_values)\n\n    # ------------------------------------------------------------------------\n    # String methods interface\n    def _str_map(\n        self, f, na_value=lib.no_default, dtype=np.dtype(\"object\"), convert: bool = True\n    ):\n        # Optimization to apply the callable `f` to the categories once\n        # and rebuild the result by `take`ing from the result with the codes.\n        # Returns the same type as the object-dtype implementation though.\n        categories = self.categories\n        codes = self.codes\n        if categories.dtype == \"string\":\n            result = categories.array._str_map(f, na_value, dtype)  # type: ignore[attr-defined]\n            if (\n                categories.dtype.na_value is np.nan  # type: ignore[union-attr]\n                and is_bool_dtype(dtype)\n                and (na_value is lib.no_default or isna(na_value))\n            ):\n                # NaN propagates as False for functions with boolean return type\n                na_value = False\n        else:\n            from pandas.core.arrays import NumpyExtensionArray\n\n            result = NumpyExtensionArray(categories.to_numpy())._str_map(\n                f, na_value, dtype\n            )\n        return take_nd(result, codes, fill_value=na_value)\n\n    def _str_get_dummies(self, sep: str = \"|\", dtype: NpDtype | None = None):\n        # sep may not be in categories. Just bail on this.\n        from pandas.core.arrays import NumpyExtensionArray\n\n        return NumpyExtensionArray(self.to_numpy(str, na_value=\"NaN\"))._str_get_dummies(\n            sep, dtype\n        )\n\n    # ------------------------------------------------------------------------\n    # GroupBy Methods\n\n    def _groupby_op(\n        self,\n        *,\n        how: str,\n        has_dropped_na: bool,\n        min_count: int,\n        ngroups: int,\n        ids: npt.NDArray[np.intp],\n        **kwargs,\n    ):\n        from pandas.core.groupby.ops import WrappedCythonOp\n\n        kind = WrappedCythonOp.get_kind_from_how(how)\n        op = WrappedCythonOp(how=how, kind=kind, has_dropped_na=has_dropped_na)\n\n        dtype = self.dtype\n        if how in [\"sum\", \"prod\", \"cumsum\", \"cumprod\", \"skew\"]:\n            raise TypeError(f\"{dtype} type does not support {how} operations\")\n        if how in [\"min\", \"max\", \"rank\", \"idxmin\", \"idxmax\"] and not dtype.ordered:\n            # raise TypeError instead of NotImplementedError to ensure we\n            #  don't go down a group-by-group path, since in the empty-groups\n            #  case that would fail to raise\n            raise TypeError(f\"Cannot perform {how} with non-ordered Categorical\")\n        if how not in [\n            \"rank\",\n            \"any\",\n            \"all\",\n            \"first\",\n            \"last\",\n            \"min\",\n            \"max\",\n            \"idxmin\",\n            \"idxmax\",\n        ]:\n            if kind == \"transform\":\n                raise TypeError(f\"{dtype} type does not support {how} operations\")\n            raise TypeError(f\"{dtype} dtype does not support aggregation '{how}'\")\n\n        result_mask = None\n        mask = self.isna()\n        if how == \"rank\":\n            assert self.ordered  # checked earlier\n            npvalues = self._ndarray\n        elif how in [\"first\", \"last\", \"min\", \"max\", \"idxmin\", \"idxmax\"]:\n            npvalues = self._ndarray\n            result_mask = np.zeros(ngroups, dtype=bool)\n        else:\n            # any/all\n            npvalues = self.astype(bool)\n\n        res_values = op._cython_op_ndim_compat(\n            npvalues,\n            min_count=min_count,\n            ngroups=ngroups,\n            comp_ids=ids,\n            mask=mask,\n            result_mask=result_mask,\n            **kwargs,\n        )\n\n        if how in op.cast_blocklist:\n            return res_values\n        elif how in [\"first\", \"last\", \"min\", \"max\"]:\n            res_values[result_mask == 1] = -1\n        return self._from_backing_data(res_values)\n\n\n# The Series.cat accessor\n\n\n@delegate_names(\n    delegate=Categorical, accessors=[\"categories\", \"ordered\"], typ=\"property\"\n)\n@delegate_names(\n    delegate=Categorical,\n    accessors=[\n        \"rename_categories\",\n        \"reorder_categories\",\n        \"add_categories\",\n        \"remove_categories\",\n        \"remove_unused_categories\",\n        \"set_categories\",\n        \"as_ordered\",\n        \"as_unordered\",\n    ],\n    typ=\"method\",\n)\nclass CategoricalAccessor(PandasDelegate, PandasObject, NoNewAttributesMixin):\n    \"\"\"\n    Accessor object for categorical properties of the Series values.\n\n    Parameters\n    ----------\n    data : Series or CategoricalIndex\n        The object to which the categorical accessor is attached.\n\n    See Also\n    --------\n    Series.dt : Accessor object for datetimelike properties of the Series values.\n    Series.sparse : Accessor for sparse matrix data types.\n\n    Examples\n    --------\n    >>> s = pd.Series(list(\"abbccc\")).astype(\"category\")\n    >>> s\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n\n    >>> s.cat.categories\n    Index(['a', 'b', 'c'], dtype='object')\n\n    >>> s.cat.rename_categories(list(\"cba\"))\n    0    c\n    1    b\n    2    b\n    3    a\n    4    a\n    5    a\n    dtype: category\n    Categories (3, object): ['c', 'b', 'a']\n\n    >>> s.cat.reorder_categories(list(\"cba\"))\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['c', 'b', 'a']\n\n    >>> s.cat.add_categories([\"d\", \"e\"])\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (5, object): ['a', 'b', 'c', 'd', 'e']\n\n    >>> s.cat.remove_categories([\"a\", \"c\"])\n    0    NaN\n    1      b\n    2      b\n    3    NaN\n    4    NaN\n    5    NaN\n    dtype: category\n    Categories (1, object): ['b']\n\n    >>> s1 = s.cat.add_categories([\"d\", \"e\"])\n    >>> s1.cat.remove_unused_categories()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n\n    >>> s.cat.set_categories(list(\"abcde\"))\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (5, object): ['a', 'b', 'c', 'd', 'e']\n\n    >>> s.cat.as_ordered()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a' < 'b' < 'c']\n\n    >>> s.cat.as_unordered()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n    \"\"\"\n\n    def __init__(self, data) -> None:\n        self._validate(data)\n        self._parent = data.values\n        self._index = data.index\n        self._name = data.name\n        self._freeze()\n\n    @staticmethod\n    def _validate(data) -> None:\n        if not isinstance(data.dtype, CategoricalDtype):\n            raise AttributeError(\"Can only use .cat accessor with a 'category' dtype\")\n\n    def _delegate_property_get(self, name: str):\n        return getattr(self._parent, name)\n\n    # error: Signature of \"_delegate_property_set\" incompatible with supertype\n    # \"PandasDelegate\"\n    def _delegate_property_set(self, name: str, new_values) -> None:  # type: ignore[override]\n        setattr(self._parent, name, new_values)\n\n    @property\n    def codes(self) -> Series:\n        \"\"\"\n        Return Series of codes as well as the index.\n\n        See Also\n        --------\n        Series.cat.categories : Return the categories of this categorical.\n        Series.cat.as_ordered : Set the Categorical to be ordered.\n        Series.cat.as_unordered : Set the Categorical to be unordered.\n\n        Examples\n        --------\n        >>> raw_cate = pd.Categorical([\"a\", \"b\", \"c\", \"a\"], categories=[\"a\", \"b\"])\n        >>> ser = pd.Series(raw_cate)\n        >>> ser.cat.codes\n        0   0\n        1   1\n        2  -1\n        3   0\n        dtype: int8\n        \"\"\"\n        from pandas import Series\n\n        return Series(self._parent.codes, index=self._index)\n\n    def _delegate_method(self, name: str, *args, **kwargs):\n        from pandas import Series\n\n        method = getattr(self._parent, name)\n        res = method(*args, **kwargs)\n        if res is not None:\n            return Series(res, index=self._index, name=self._name)\n\n\n# utility routines\n\n\ndef _get_codes_for_values(\n    values: Index | Series | ExtensionArray | np.ndarray,\n    categories: Index,\n) -> np.ndarray:\n    \"\"\"\n    utility routine to turn values into codes given the specified categories\n\n    If `values` is known to be a Categorical, use recode_for_categories instead.\n    \"\"\"\n    codes = categories.get_indexer_for(values)\n    return coerce_indexer_dtype(codes, categories)\n\n\ndef recode_for_categories(\n    codes: np.ndarray, old_categories, new_categories, copy: bool = True\n) -> np.ndarray:\n    \"\"\"\n    Convert a set of codes for to a new set of categories\n\n    Parameters\n    ----------\n    codes : np.ndarray\n    old_categories, new_categories : Index\n    copy: bool, default True\n        Whether to copy if the codes are unchanged.\n\n    Returns\n    -------\n    new_codes : np.ndarray[np.int64]\n\n    Examples\n    --------\n    >>> old_cat = pd.Index([\"b\", \"a\", \"c\"])\n    >>> new_cat = pd.Index([\"a\", \"b\"])\n    >>> codes = np.array([0, 1, 1, 2])\n    >>> recode_for_categories(codes, old_cat, new_cat)\n    array([ 1,  0,  0, -1], dtype=int8)\n    \"\"\"\n    if len(old_categories) == 0:\n        # All null anyway, so just retain the nulls\n        if copy:\n            return codes.copy()\n        return codes\n    elif new_categories.equals(old_categories):\n        # Same categories, so no need to actually recode\n        if copy:\n            return codes.copy()\n        return codes\n\n    indexer = coerce_indexer_dtype(\n        new_categories.get_indexer_for(old_categories), new_categories\n    )\n    new_codes = take_nd(indexer, codes, fill_value=-1)\n    return new_codes\n\n\ndef factorize_from_iterable(values) -> tuple[np.ndarray, Index]:\n    \"\"\"\n    Factorize an input `values` into `categories` and `codes`. Preserves\n    categorical dtype in `categories`.\n\n    Parameters\n    ----------\n    values : list-like\n\n    Returns\n    -------\n    codes : ndarray\n    categories : Index\n        If `values` has a categorical dtype, then `categories` is\n        a CategoricalIndex keeping the categories and order of `values`.\n    \"\"\"\n    from pandas import CategoricalIndex\n\n    if not is_list_like(values):\n        raise TypeError(\"Input must be list-like\")\n\n    categories: Index\n\n    vdtype = getattr(values, \"dtype\", None)\n    if isinstance(vdtype, CategoricalDtype):\n        values = extract_array(values)\n        # The Categorical we want to build has the same categories\n        # as values but its codes are by def [0, ..., len(n_categories) - 1]\n        cat_codes = np.arange(len(values.categories), dtype=values.codes.dtype)\n        cat = Categorical.from_codes(cat_codes, dtype=values.dtype, validate=False)\n\n        categories = CategoricalIndex(cat)\n        codes = values.codes\n    else:\n        # The value of ordered is irrelevant since we don't use cat as such,\n        # but only the resulting categories, the order of which is independent\n        # from ordered. Set ordered to False as default. See GH #15457\n        cat = Categorical(values, ordered=False)\n        categories = cat.categories\n        codes = cat.codes\n    return codes, categories\n\n\ndef factorize_from_iterables(iterables) -> tuple[list[np.ndarray], list[Index]]:\n    \"\"\"\n    A higher-level wrapper over `factorize_from_iterable`.\n\n    Parameters\n    ----------\n    iterables : list-like of list-likes\n\n    Returns\n    -------\n    codes : list of ndarrays\n    categories : list of Indexes\n\n    Notes\n    -----\n    See `factorize_from_iterable` for more info.\n    \"\"\"\n    if len(iterables) == 0:\n        # For consistency, it should return two empty lists.\n        return [], []\n\n    codes, categories = zip(*(factorize_from_iterable(it) for it in iterables))\n    return list(codes), list(categories)\n"
    },
    {
      "filename": "pandas/tests/arrays/categorical/test_dtypes.py",
      "content": "import numpy as np\nimport pyarrow as pa\nimport pytest\n\nfrom pandas.core.dtypes.dtypes import (\n    ArrowDtype,\n    CategoricalDtype,\n)\n\nfrom pandas import (\n    Categorical,\n    CategoricalIndex,\n    Index,\n    IntervalIndex,\n    Series,\n    Timestamp,\n)\nimport pandas._testing as tm\n\n\nclass TestCategoricalDtypes:\n    def test_categories_match_up_to_permutation(self):\n        # test dtype comparisons between cats\n\n        c1 = Categorical(list(\"aabca\"), categories=list(\"abc\"), ordered=False)\n        c2 = Categorical(list(\"aabca\"), categories=list(\"cab\"), ordered=False)\n        c3 = Categorical(list(\"aabca\"), categories=list(\"cab\"), ordered=True)\n        assert c1._categories_match_up_to_permutation(c1)\n        assert c2._categories_match_up_to_permutation(c2)\n        assert c3._categories_match_up_to_permutation(c3)\n        assert c1._categories_match_up_to_permutation(c2)\n        assert not c1._categories_match_up_to_permutation(c3)\n        assert not c1._categories_match_up_to_permutation(Index(list(\"aabca\")))\n        assert not c1._categories_match_up_to_permutation(c1.astype(object))\n        assert c1._categories_match_up_to_permutation(CategoricalIndex(c1))\n        assert c1._categories_match_up_to_permutation(\n            CategoricalIndex(c1, categories=list(\"cab\"))\n        )\n        assert not c1._categories_match_up_to_permutation(\n            CategoricalIndex(c1, ordered=True)\n        )\n\n        # GH 16659\n        s1 = Series(c1)\n        s2 = Series(c2)\n        s3 = Series(c3)\n        assert c1._categories_match_up_to_permutation(s1)\n        assert c2._categories_match_up_to_permutation(s2)\n        assert c3._categories_match_up_to_permutation(s3)\n        assert c1._categories_match_up_to_permutation(s2)\n        assert not c1._categories_match_up_to_permutation(s3)\n        assert not c1._categories_match_up_to_permutation(s1.astype(object))\n\n    def test_set_dtype_same(self):\n        c = Categorical([\"a\", \"b\", \"c\"])\n        result = c._set_dtype(CategoricalDtype([\"a\", \"b\", \"c\"]))\n        tm.assert_categorical_equal(result, c)\n\n    def test_set_dtype_new_categories(self):\n        c = Categorical([\"a\", \"b\", \"c\"])\n        result = c._set_dtype(CategoricalDtype(list(\"abcd\")))\n        tm.assert_numpy_array_equal(result.codes, c.codes)\n        tm.assert_index_equal(result.dtype.categories, Index(list(\"abcd\")))\n\n    @pytest.mark.parametrize(\n        \"values, categories, new_categories\",\n        [\n            # No NaNs, same cats, same order\n            ([\"a\", \"b\", \"a\"], [\"a\", \"b\"], [\"a\", \"b\"]),\n            # No NaNs, same cats, different order\n            ([\"a\", \"b\", \"a\"], [\"a\", \"b\"], [\"b\", \"a\"]),\n            # Same, unsorted\n            ([\"b\", \"a\", \"a\"], [\"a\", \"b\"], [\"a\", \"b\"]),\n            # No NaNs, same cats, different order\n            ([\"b\", \"a\", \"a\"], [\"a\", \"b\"], [\"b\", \"a\"]),\n            # NaNs\n            ([\"a\", \"b\", \"c\"], [\"a\", \"b\"], [\"a\", \"b\"]),\n            ([\"a\", \"b\", \"c\"], [\"a\", \"b\"], [\"b\", \"a\"]),\n            ([\"b\", \"a\", \"c\"], [\"a\", \"b\"], [\"a\", \"b\"]),\n            ([\"b\", \"a\", \"c\"], [\"a\", \"b\"], [\"b\", \"a\"]),\n            # Introduce NaNs\n            ([\"a\", \"b\", \"c\"], [\"a\", \"b\"], [\"a\"]),\n            ([\"a\", \"b\", \"c\"], [\"a\", \"b\"], [\"b\"]),\n            ([\"b\", \"a\", \"c\"], [\"a\", \"b\"], [\"a\"]),\n            ([\"b\", \"a\", \"c\"], [\"a\", \"b\"], [\"b\"]),\n            # No overlap\n            ([\"a\", \"b\", \"c\"], [\"a\", \"b\"], [\"d\", \"e\"]),\n        ],\n    )\n    def test_set_dtype_many(self, values, categories, new_categories, ordered):\n        c = Categorical(values, categories)\n        expected = Categorical(values, new_categories, ordered)\n        result = c._set_dtype(expected.dtype)\n        tm.assert_categorical_equal(result, expected)\n\n    def test_set_dtype_no_overlap(self):\n        c = Categorical([\"a\", \"b\", \"c\"], [\"d\", \"e\"])\n        result = c._set_dtype(CategoricalDtype([\"a\", \"b\"]))\n        expected = Categorical([None, None, None], categories=[\"a\", \"b\"])\n        tm.assert_categorical_equal(result, expected)\n\n    def test_codes_dtypes(self):\n        # GH 8453\n        result = Categorical([\"foo\", \"bar\", \"baz\"])\n        assert result.codes.dtype == \"int8\"\n\n        result = Categorical([f\"foo{i:05d}\" for i in range(400)])\n        assert result.codes.dtype == \"int16\"\n\n        result = Categorical([f\"foo{i:05d}\" for i in range(40000)])\n        assert result.codes.dtype == \"int32\"\n\n        # adding cats\n        result = Categorical([\"foo\", \"bar\", \"baz\"])\n        assert result.codes.dtype == \"int8\"\n        result = result.add_categories([f\"foo{i:05d}\" for i in range(400)])\n        assert result.codes.dtype == \"int16\"\n\n        # removing cats\n        result = result.remove_categories([f\"foo{i:05d}\" for i in range(300)])\n        assert result.codes.dtype == \"int8\"\n\n    def test_iter_python_types(self):\n        # GH-19909\n        cat = Categorical([1, 2])\n        assert isinstance(next(iter(cat)), int)\n        assert isinstance(cat.tolist()[0], int)\n\n    def test_iter_python_types_datetime(self):\n        cat = Categorical([Timestamp(\"2017-01-01\"), Timestamp(\"2017-01-02\")])\n        assert isinstance(next(iter(cat)), Timestamp)\n        assert isinstance(cat.tolist()[0], Timestamp)\n\n    def test_interval_index_category(self):\n        # GH 38316\n        index = IntervalIndex.from_breaks(np.arange(3, dtype=\"uint64\"))\n\n        result = CategoricalIndex(index).dtype.categories\n        expected = IntervalIndex.from_arrays(\n            [0, 1], [1, 2], dtype=\"interval[uint64, right]\"\n        )\n        tm.assert_index_equal(result, expected)\n\n    def test_values_is_index():\n        # GH 60563\n        values = Index([\"a1\", \"a2\"], dtype=ArrowDtype(pa.string()))\n        arr = values._data._pa_array.combine_chunks()\n\n        assert arr.equals(values._data._pa_array.combine_chunks())\n\n    def test_values_is_not_index():\n        # GH 60563\n        values = Series([\"a1\", \"a2\"], dtype=ArrowDtype(pa.string()))\n        arr = values._pa_array.combine_chunks()\n\n        assert arr.equals(values._pa_array.combine_chunks())\n"
    }
  ]
}