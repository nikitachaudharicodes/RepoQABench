{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "49656",
  "issue_description": "# STYLE enable pylint's redefined-outer-name\n\nThis warning could potentially prevent bugs, so I'm pretty keen on putting in the effort to enable it.\r\n\r\nMany people can work on this one together. To work on it, please:\r\n1. choose 2-3 files\r\n2. make sure you have `pylint` version 2.15.5 installed (`pip install pylint==2.15.5`) - check this is the same as the number in the `.pre-commit-config.yaml` file\r\n3. for each file, run `pylint --disable=all --enable=redefined-outer-name <file>`\r\n4. fix up the file until it passes\r\n5. stage, commit, push, open pull request\r\n\r\nFor example, suppose you choose the file `pandas/io/json/_table_schema.py`. Running `pylint` on it, you get:\r\n```\r\n$ pylint --disable=all --enable=redefined-outer-name pandas/io/json/_table_schema.py\r\n************* Module pandas.io.json._table_schema\r\npandas/io/json/_table_schema.py:314:23: W0621: Redefining name 'json' from outer scope (line 15) (redefined-outer-name)\r\n\r\n-----------------------------------\r\nYour code has been rated at 9.92/10\r\n```\r\n\r\nIf we look at that file, we see that there's a function\r\n```python\r\ndef parse_table_schema(json, precise_float):\r\n```\r\nwhich takes `json` as an argument, but the file also has\r\n```python\r\nfrom pandas._libs import json\r\n```\r\nat the top.\r\n\r\nThe simplest fix is probably to just modify the import to be\r\n```python\r\nfrom pandas._libs.json import loads\r\n```\r\nand then to remove\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/f9ff3796329e4bedb4a5477739f5eb8d2e40761d/pandas/io/json/_table_schema.py#L44\r\n\r\nas `loads` is the only function used from `pandas._libs.json`.\r\n\r\n---\r\n\r\nOther notes:\r\n1. multiple people can work on this issue at the same time, please don't comment 'take'\r\n2. please choose no more than 3 files to work on in the same PR. Comment which files you'd like to take so we don't duplicate work\r\n\r\nIt's probably worth running `pylint` on a directory first to see what files need working on, e.g.\r\n```\r\npylint --disable=all --enable=redefined-outer-name pandas/io/\r\n```\r\n\r\n\r\n---\r\n\r\nFiles that still need fixing:\r\n\r\n- pandas/_version.py\r\n- pandas/conftest.py\r\n- pandas/core/generic.py\r\n- pandas/core/internals/concat.py\r\n- pandas/core/reshape/merge.py\r\n- pandas/core/tools/datetimes.py\r\n- pandas/io/formats/format.py\r\n- pandas/io/formats/style.py\r\n- pandas/io/json/_json.py\r\n- pandas/io/xml.py\r\n- pandas/util/_decorators.py\r\n- pandas/util/_doctools.py\r\n\r\nLet's ignore test files for now",
  "issue_comments": [
    {
      "id": 1312378943,
      "user": "grtcoder",
      "body": "Hey, I would like to work on this"
    },
    {
      "id": 1312381946,
      "user": "MarcoGorelli",
      "body": "go ahead - as mentioned in the issue:\r\n\r\n> please choose no more than 3 files to work on in the same PR. Comment which files you'd like to take so we don't duplicate work"
    },
    {
      "id": 1312388146,
      "user": "ramvikrams",
      "body": "I'll take `pandas\\core\\generic.py`  "
    },
    {
      "id": 1312404574,
      "user": "grtcoder",
      "body": "I'll take \r\n\r\n1. `pandas/core/arrays/datetimelike.py` \r\n2. `pandas/core/base.py`\r\n3. `pandas/core/computation/pytables.py`"
    },
    {
      "id": 1312506794,
      "user": "Dhrubajyoti-Giri",
      "body": "I'll take `pandas/core/internals/concat.py`, `pandas/core/reshape/merge.py`, `pandas/io/json/_json.py`."
    },
    {
      "id": 1312522836,
      "user": "bhaveshrp",
      "body": "I'll take\r\n\r\n1. `pandas\\core\\frame.py`\r\n2. `pandas\\core\\dtypes\\inference.py`"
    },
    {
      "id": 1312525199,
      "user": "alphacrack",
      "body": "I'll take:\r\n1. `pandas/core/dtypes/cast.py`\r\n2. `pandas/core/dtypes/dtypes.py`\r\n3. `pandas/core/indexes/base.py`"
    },
    {
      "id": 1312582486,
      "user": "aditya-anulekh",
      "body": "Added a pull request fixing the following files\r\n\r\n* `pandas/core/indexes/datetimes.py`\r\n* `pandas/io/formats/xml.py`\r\n* `pandas/io/json/_table_schema.py`"
    },
    {
      "id": 1312711563,
      "user": "nandinimalik16",
      "body": "I will take\r\n\r\n1. `pandas/_version.py`\r\n2. `pandas/conftest.py`\r\n3. `pandas/io/formats/format.py`"
    },
    {
      "id": 1312836173,
      "user": "isaac-chung",
      "body": "I'll take:\r\n\r\n1. `pandas/plotting/_matplotlib/converter.py`\r\n2. `pandas/plotting/_matplotlib/core.py`\r\n3. `pandas/plotting/_matplotlib/tools.py`"
    },
    {
      "id": 1314709725,
      "user": "seanjedi",
      "body": "Hi, I'd like to work on: \r\n1. `pandas/io/formats/style.py`\r\n2. `pandas/io/json/_json.py`\r\n3. `pandas/io/xml.py`"
    },
    {
      "id": 1314934810,
      "user": "zemnly",
      "body": "Are there any files which haven't been taken so I can work on them?"
    },
    {
      "id": 1314949845,
      "user": "MarcoGorelli",
      "body": "Yeah there's still some, I've updated the issue with an up-to-date list\r\n\r\nNote that https://github.com/pandas-dev/pandas/pull/49668/files is already tackling some, but the rest should be up for grabs\r\n\r\nThanks all for your help with this one ðŸ™ "
    },
    {
      "id": 1314959341,
      "user": "zemnly",
      "body": "Thanks, I'd like to take :\r\n1. `pandas/core/algorithms.py`\r\n2. `pandas/core/generic.py`\r\n3. `pandas/core/tools/datetimes.py`"
    },
    {
      "id": 1314968898,
      "user": "calhockemeyer",
      "body": "I will take: `pandas/core/resample.py`. I think that's the only file that hasn't been taken"
    },
    {
      "id": 1315030301,
      "user": "bang128",
      "body": "Can I also contribute to `pandas/core/algorithms.py`?\r\n"
    },
    {
      "id": 1315034645,
      "user": "ramvikrams",
      "body": "> Can I also contribute to `pandas/core/algorithms.py`?\r\n\r\nYes you can i'll leave that file"
    },
    {
      "id": 1315036118,
      "user": "bang128",
      "body": "> > Can I also contribute to `pandas/core/algorithms.py`?\r\n> \r\n> Yes you can i'll leave that file\r\n\r\nThank you so much!"
    },
    {
      "id": 1317428222,
      "user": "zmwaris1",
      "body": "Hey, I'd like to work on these files if anyone else is not already working on it:\r\n`pandas/core/internals/concat.py`\r\n`pandas/core/reshape/merge.py`\r\n`pandas/io/json/_json.py`\r\n\r\nThanks."
    },
    {
      "id": 1319051806,
      "user": "MarcoGorelli",
      "body": "@zmwaris1 please do - I've updated the list of files which still need doing"
    },
    {
      "id": 1319652725,
      "user": "seanjedi",
      "body": "@zmwaris1 I apologize for the lateness, been having a hectic past two days. \r\nI saw your post above, and I wanted you to know that I have been working on `pandas/io/json/_json.py` on my fork recently here: https://github.com/seanjedi/pandas/tree/seanjedi-49724_pylint's_redefined_outer_name\r\nI just wanted you to know so that we don't duplicate work effort. \r\nFeel free to take a look at it and let me know if I made a mistake or if you have any suggestions on making it better."
    },
    {
      "id": 1321294358,
      "user": "wchung42",
      "body": "Hi, are there any files that still needs to be worked on?"
    },
    {
      "id": 1321583904,
      "user": "MarcoGorelli",
      "body": "yup, check the issue description"
    },
    {
      "id": 1322516155,
      "user": "JasmandeepKaur",
      "body": "Hi, I'll be working on these files:\r\n```\r\npandas/util/_decorators.py\r\npandas/util/_doctools.py\r\npandas/util/_test_decorators.py\r\n```"
    },
    {
      "id": 1322520658,
      "user": "MarcoGorelli",
      "body": "let's leave out `pandas/util/_test_decorators.py`, but the other two are fine"
    },
    {
      "id": 1322557345,
      "user": "JasmandeepKaur",
      "body": "> let's leave out `pandas/util/_test_decorators.py`, but the other two are fine\r\n\r\nWill do."
    },
    {
      "id": 1322967845,
      "user": "angularOcean",
      "body": "Cross referencing the files that need fixing list with what people have commented on taking it, it looks like everything has/is being  handled, is there any other files that need working on?"
    },
    {
      "id": 1323290241,
      "user": "MarcoGorelli",
      "body": "if people have commented but they haven't opened a PR, then feel free to take over"
    },
    {
      "id": 1324189973,
      "user": "angularOcean",
      "body": "Okay it doesn't look like a PR has been opened for `pandas/core/tools/datetimes.py` or `pandas/io/formats/format.py` so I will try taking on those two. "
    },
    {
      "id": 1326776159,
      "user": "MarcoGorelli",
      "body": "We're almost done, there's just:\r\n\r\n- pandas/io/formats/format.py\r\n- pandas/core/generic.py\r\n\r\nleft\r\n\r\nIf anyone wants to take them, and then (in the same PR) remove these lines from `.pre-commit-config.yaml`\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/0429b648709de4af526c8a65313b60a8f7419f1d/.pre-commit-config.yaml#L81-L83\r\n\r\n, then we can close the issue"
    }
  ],
  "text_context": "# STYLE enable pylint's redefined-outer-name\n\nThis warning could potentially prevent bugs, so I'm pretty keen on putting in the effort to enable it.\r\n\r\nMany people can work on this one together. To work on it, please:\r\n1. choose 2-3 files\r\n2. make sure you have `pylint` version 2.15.5 installed (`pip install pylint==2.15.5`) - check this is the same as the number in the `.pre-commit-config.yaml` file\r\n3. for each file, run `pylint --disable=all --enable=redefined-outer-name <file>`\r\n4. fix up the file until it passes\r\n5. stage, commit, push, open pull request\r\n\r\nFor example, suppose you choose the file `pandas/io/json/_table_schema.py`. Running `pylint` on it, you get:\r\n```\r\n$ pylint --disable=all --enable=redefined-outer-name pandas/io/json/_table_schema.py\r\n************* Module pandas.io.json._table_schema\r\npandas/io/json/_table_schema.py:314:23: W0621: Redefining name 'json' from outer scope (line 15) (redefined-outer-name)\r\n\r\n-----------------------------------\r\nYour code has been rated at 9.92/10\r\n```\r\n\r\nIf we look at that file, we see that there's a function\r\n```python\r\ndef parse_table_schema(json, precise_float):\r\n```\r\nwhich takes `json` as an argument, but the file also has\r\n```python\r\nfrom pandas._libs import json\r\n```\r\nat the top.\r\n\r\nThe simplest fix is probably to just modify the import to be\r\n```python\r\nfrom pandas._libs.json import loads\r\n```\r\nand then to remove\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/f9ff3796329e4bedb4a5477739f5eb8d2e40761d/pandas/io/json/_table_schema.py#L44\r\n\r\nas `loads` is the only function used from `pandas._libs.json`.\r\n\r\n---\r\n\r\nOther notes:\r\n1. multiple people can work on this issue at the same time, please don't comment 'take'\r\n2. please choose no more than 3 files to work on in the same PR. Comment which files you'd like to take so we don't duplicate work\r\n\r\nIt's probably worth running `pylint` on a directory first to see what files need working on, e.g.\r\n```\r\npylint --disable=all --enable=redefined-outer-name pandas/io/\r\n```\r\n\r\n\r\n---\r\n\r\nFiles that still need fixing:\r\n\r\n- pandas/_version.py\r\n- pandas/conftest.py\r\n- pandas/core/generic.py\r\n- pandas/core/internals/concat.py\r\n- pandas/core/reshape/merge.py\r\n- pandas/core/tools/datetimes.py\r\n- pandas/io/formats/format.py\r\n- pandas/io/formats/style.py\r\n- pandas/io/json/_json.py\r\n- pandas/io/xml.py\r\n- pandas/util/_decorators.py\r\n- pandas/util/_doctools.py\r\n\r\nLet's ignore test files for now\n\nHey, I would like to work on this\n\ngo ahead - as mentioned in the issue:\r\n\r\n> please choose no more than 3 files to work on in the same PR. Comment which files you'd like to take so we don't duplicate work\n\nI'll take `pandas\\core\\generic.py`  \n\nI'll take \r\n\r\n1. `pandas/core/arrays/datetimelike.py` \r\n2. `pandas/core/base.py`\r\n3. `pandas/core/computation/pytables.py`\n\nI'll take `pandas/core/internals/concat.py`, `pandas/core/reshape/merge.py`, `pandas/io/json/_json.py`.\n\nI'll take\r\n\r\n1. `pandas\\core\\frame.py`\r\n2. `pandas\\core\\dtypes\\inference.py`\n\nI'll take:\r\n1. `pandas/core/dtypes/cast.py`\r\n2. `pandas/core/dtypes/dtypes.py`\r\n3. `pandas/core/indexes/base.py`\n\nAdded a pull request fixing the following files\r\n\r\n* `pandas/core/indexes/datetimes.py`\r\n* `pandas/io/formats/xml.py`\r\n* `pandas/io/json/_table_schema.py`\n\nI will take\r\n\r\n1. `pandas/_version.py`\r\n2. `pandas/conftest.py`\r\n3. `pandas/io/formats/format.py`\n\nI'll take:\r\n\r\n1. `pandas/plotting/_matplotlib/converter.py`\r\n2. `pandas/plotting/_matplotlib/core.py`\r\n3. `pandas/plotting/_matplotlib/tools.py`\n\nHi, I'd like to work on: \r\n1. `pandas/io/formats/style.py`\r\n2. `pandas/io/json/_json.py`\r\n3. `pandas/io/xml.py`\n\nAre there any files which haven't been taken so I can work on them?\n\nYeah there's still some, I've updated the issue with an up-to-date list\r\n\r\nNote that https://github.com/pandas-dev/pandas/pull/49668/files is already tackling some, but the rest should be up for grabs\r\n\r\nThanks all for your help with this one ðŸ™ \n\nThanks, I'd like to take :\r\n1. `pandas/core/algorithms.py`\r\n2. `pandas/core/generic.py`\r\n3. `pandas/core/tools/datetimes.py`\n\nI will take: `pandas/core/resample.py`. I think that's the only file that hasn't been taken\n\nCan I also contribute to `pandas/core/algorithms.py`?\r\n\n\n> Can I also contribute to `pandas/core/algorithms.py`?\r\n\r\nYes you can i'll leave that file\n\n> > Can I also contribute to `pandas/core/algorithms.py`?\r\n> \r\n> Yes you can i'll leave that file\r\n\r\nThank you so much!\n\nHey, I'd like to work on these files if anyone else is not already working on it:\r\n`pandas/core/internals/concat.py`\r\n`pandas/core/reshape/merge.py`\r\n`pandas/io/json/_json.py`\r\n\r\nThanks.\n\n@zmwaris1 please do - I've updated the list of files which still need doing\n\n@zmwaris1 I apologize for the lateness, been having a hectic past two days. \r\nI saw your post above, and I wanted you to know that I have been working on `pandas/io/json/_json.py` on my fork recently here: https://github.com/seanjedi/pandas/tree/seanjedi-49724_pylint's_redefined_outer_name\r\nI just wanted you to know so that we don't duplicate work effort. \r\nFeel free to take a look at it and let me know if I made a mistake or if you have any suggestions on making it better.\n\nHi, are there any files that still needs to be worked on?\n\nyup, check the issue description\n\nHi, I'll be working on these files:\r\n```\r\npandas/util/_decorators.py\r\npandas/util/_doctools.py\r\npandas/util/_test_decorators.py\r\n```\n\nlet's leave out `pandas/util/_test_decorators.py`, but the other two are fine\n\n> let's leave out `pandas/util/_test_decorators.py`, but the other two are fine\r\n\r\nWill do.\n\nCross referencing the files that need fixing list with what people have commented on taking it, it looks like everything has/is being  handled, is there any other files that need working on?\n\nif people have commented but they haven't opened a PR, then feel free to take over\n\nOkay it doesn't look like a PR has been opened for `pandas/core/tools/datetimes.py` or `pandas/io/formats/format.py` so I will try taking on those two. \n\nWe're almost done, there's just:\r\n\r\n- pandas/io/formats/format.py\r\n- pandas/core/generic.py\r\n\r\nleft\r\n\r\nIf anyone wants to take them, and then (in the same PR) remove these lines from `.pre-commit-config.yaml`\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/0429b648709de4af526c8a65313b60a8f7419f1d/.pre-commit-config.yaml#L81-L83\r\n\r\n, then we can close the issue",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/49668",
  "code_context": [
    {
      "filename": "pandas/core/internals/concat.py",
      "content": "from __future__ import annotations\n\nimport copy as stdlib_copy\nimport itertools\nfrom typing import (\n    TYPE_CHECKING,\n    Sequence,\n    cast,\n)\n\nimport numpy as np\n\nfrom pandas._libs import (\n    NaT,\n    internals as libinternals,\n)\nfrom pandas._libs.missing import NA\nfrom pandas._typing import (\n    ArrayLike,\n    AxisInt,\n    DtypeObj,\n    Manager,\n    Shape,\n)\nfrom pandas.util._decorators import cache_readonly\n\nfrom pandas.core.dtypes.astype import astype_array\nfrom pandas.core.dtypes.cast import (\n    ensure_dtype_can_hold_na,\n    find_common_type,\n)\nfrom pandas.core.dtypes.common import (\n    is_1d_only_ea_dtype,\n    is_dtype_equal,\n    is_scalar,\n    needs_i8_conversion,\n)\nfrom pandas.core.dtypes.concat import concat_compat\nfrom pandas.core.dtypes.dtypes import (\n    DatetimeTZDtype,\n    ExtensionDtype,\n)\nfrom pandas.core.dtypes.missing import (\n    is_valid_na_for_dtype,\n    isna,\n    isna_all,\n)\n\nimport pandas.core.algorithms as algos\nfrom pandas.core.arrays import (\n    DatetimeArray,\n    ExtensionArray,\n)\nfrom pandas.core.arrays.sparse import SparseDtype\nfrom pandas.core.construction import ensure_wrapped_if_datetimelike\nfrom pandas.core.internals.array_manager import (\n    ArrayManager,\n    NullArrayProxy,\n)\nfrom pandas.core.internals.blocks import (\n    ensure_block_shape,\n    new_block_2d,\n)\nfrom pandas.core.internals.managers import BlockManager\n\nif TYPE_CHECKING:\n    from pandas import Index\n    from pandas.core.internals.blocks import Block\n\n\ndef _concatenate_array_managers(\n    mgrs_indexers, axes: list[Index], concat_axis: AxisInt, copy: bool\n) -> Manager:\n    \"\"\"\n    Concatenate array managers into one.\n\n    Parameters\n    ----------\n    mgrs_indexers : list of (ArrayManager, {axis: indexer,...}) tuples\n    axes : list of Index\n    concat_axis : int\n    copy : bool\n\n    Returns\n    -------\n    ArrayManager\n    \"\"\"\n    # reindex all arrays\n    mgrs = []\n    for mgr, indexers in mgrs_indexers:\n        axis1_made_copy = False\n        for ax, indexer in indexers.items():\n            mgr = mgr.reindex_indexer(\n                axes[ax], indexer, axis=ax, allow_dups=True, use_na_proxy=True\n            )\n            if ax == 1 and indexer is not None:\n                axis1_made_copy = True\n        if copy and concat_axis == 0 and not axis1_made_copy:\n            # for concat_axis 1 we will always get a copy through concat_arrays\n            mgr = mgr.copy()\n        mgrs.append(mgr)\n\n    if concat_axis == 1:\n        # concatting along the rows -> concat the reindexed arrays\n        # TODO(ArrayManager) doesn't yet preserve the correct dtype\n        arrays = [\n            concat_arrays([mgrs[i].arrays[j] for i in range(len(mgrs))])\n            for j in range(len(mgrs[0].arrays))\n        ]\n    else:\n        # concatting along the columns -> combine reindexed arrays in a single manager\n        assert concat_axis == 0\n        arrays = list(itertools.chain.from_iterable([mgr.arrays for mgr in mgrs]))\n\n    new_mgr = ArrayManager(arrays, [axes[1], axes[0]], verify_integrity=False)\n    return new_mgr\n\n\ndef concat_arrays(to_concat: list) -> ArrayLike:\n    \"\"\"\n    Alternative for concat_compat but specialized for use in the ArrayManager.\n\n    Differences: only deals with 1D arrays (no axis keyword), assumes\n    ensure_wrapped_if_datetimelike and does not skip empty arrays to determine\n    the dtype.\n    In addition ensures that all NullArrayProxies get replaced with actual\n    arrays.\n\n    Parameters\n    ----------\n    to_concat : list of arrays\n\n    Returns\n    -------\n    np.ndarray or ExtensionArray\n    \"\"\"\n    # ignore the all-NA proxies to determine the resulting dtype\n    to_concat_no_proxy = [x for x in to_concat if not isinstance(x, NullArrayProxy)]\n\n    dtypes = {x.dtype for x in to_concat_no_proxy}\n    single_dtype = len(dtypes) == 1\n\n    if single_dtype:\n        target_dtype = to_concat_no_proxy[0].dtype\n    elif all(x.kind in [\"i\", \"u\", \"b\"] and isinstance(x, np.dtype) for x in dtypes):\n        # GH#42092\n        target_dtype = np.find_common_type(list(dtypes), [])\n    else:\n        target_dtype = find_common_type([arr.dtype for arr in to_concat_no_proxy])\n\n    to_concat = [\n        arr.to_array(target_dtype)\n        if isinstance(arr, NullArrayProxy)\n        else astype_array(arr, target_dtype, copy=False)\n        for arr in to_concat\n    ]\n\n    if isinstance(to_concat[0], ExtensionArray):\n        cls = type(to_concat[0])\n        return cls._concat_same_type(to_concat)\n\n    result = np.concatenate(to_concat)\n\n    # TODO decide on exact behaviour (we shouldn't do this only for empty result)\n    # see https://github.com/pandas-dev/pandas/issues/39817\n    if len(result) == 0:\n        # all empties -> check for bool to not coerce to float\n        kinds = {obj.dtype.kind for obj in to_concat_no_proxy}\n        if len(kinds) != 1:\n            if \"b\" in kinds:\n                result = result.astype(object)\n    return result\n\n\ndef concatenate_managers(\n    mgrs_indexers, axes: list[Index], concat_axis: AxisInt, copy: bool\n) -> Manager:\n    \"\"\"\n    Concatenate block managers into one.\n\n    Parameters\n    ----------\n    mgrs_indexers : list of (BlockManager, {axis: indexer,...}) tuples\n    axes : list of Index\n    concat_axis : int\n    copy : bool\n\n    Returns\n    -------\n    BlockManager\n    \"\"\"\n    # TODO(ArrayManager) this assumes that all managers are of the same type\n    if isinstance(mgrs_indexers[0][0], ArrayManager):\n        return _concatenate_array_managers(mgrs_indexers, axes, concat_axis, copy)\n\n    mgrs_indexers = _maybe_reindex_columns_na_proxy(axes, mgrs_indexers)\n\n    concat_plans = [\n        _get_mgr_concatenation_plan(mgr, indexers) for mgr, indexers in mgrs_indexers\n    ]\n    concat_plan = _combine_concat_plans(concat_plans, concat_axis)\n    blocks = []\n\n    for placement, join_units in concat_plan:\n        unit = join_units[0]\n        blk = unit.block\n\n        if len(join_units) == 1 and not join_units[0].indexers:\n            values = blk.values\n            if copy:\n                values = values.copy()\n            else:\n                values = values.view()\n            fastpath = True\n        elif _is_uniform_join_units(join_units):\n            vals = [ju.block.values for ju in join_units]\n\n            if not blk.is_extension:\n                # _is_uniform_join_units ensures a single dtype, so\n                #  we can use np.concatenate, which is more performant\n                #  than concat_compat\n                values = np.concatenate(vals, axis=1)\n            else:\n                # TODO(EA2D): special-casing not needed with 2D EAs\n                values = concat_compat(vals, axis=1)\n                values = ensure_block_shape(values, ndim=2)\n\n            values = ensure_wrapped_if_datetimelike(values)\n\n            fastpath = blk.values.dtype == values.dtype\n        else:\n            values = _concatenate_join_units(join_units, concat_axis, copy=copy)\n            fastpath = False\n\n        if fastpath:\n            b = blk.make_block_same_class(values, placement=placement)\n        else:\n            b = new_block_2d(values, placement=placement)\n\n        blocks.append(b)\n\n    return BlockManager(tuple(blocks), axes)\n\n\ndef _maybe_reindex_columns_na_proxy(\n    axes: list[Index], mgrs_indexers: list[tuple[BlockManager, dict[int, np.ndarray]]]\n) -> list[tuple[BlockManager, dict[int, np.ndarray]]]:\n    \"\"\"\n    Reindex along columns so that all of the BlockManagers being concatenated\n    have matching columns.\n\n    Columns added in this reindexing have dtype=np.void, indicating they\n    should be ignored when choosing a column's final dtype.\n    \"\"\"\n    new_mgrs_indexers = []\n    for mgr, indexers in mgrs_indexers:\n        # We only reindex for axis=0 (i.e. columns), as this can be done cheaply\n        if 0 in indexers:\n            new_mgr = mgr.reindex_indexer(\n                axes[0],\n                indexers[0],\n                axis=0,\n                copy=False,\n                only_slice=True,\n                allow_dups=True,\n                use_na_proxy=True,\n            )\n            new_indexers = indexers.copy()\n            del new_indexers[0]\n            new_mgrs_indexers.append((new_mgr, new_indexers))\n        else:\n            new_mgrs_indexers.append((mgr, indexers))\n\n    return new_mgrs_indexers\n\n\ndef _get_mgr_concatenation_plan(mgr: BlockManager, indexers: dict[int, np.ndarray]):\n    \"\"\"\n    Construct concatenation plan for given block manager and indexers.\n\n    Parameters\n    ----------\n    mgr : BlockManager\n    indexers : dict of {axis: indexer}\n\n    Returns\n    -------\n    plan : list of (BlockPlacement, JoinUnit) tuples\n\n    \"\"\"\n    # Calculate post-reindex shape , save for item axis which will be separate\n    # for each block anyway.\n    mgr_shape_list = list(mgr.shape)\n    for ax, indexer in indexers.items():\n        mgr_shape_list[ax] = len(indexer)\n    mgr_shape = tuple(mgr_shape_list)\n\n    assert 0 not in indexers\n\n    if mgr.is_single_block:\n        blk = mgr.blocks[0]\n        return [(blk.mgr_locs, JoinUnit(blk, mgr_shape, indexers))]\n\n    blknos = mgr.blknos\n    blklocs = mgr.blklocs\n\n    plan = []\n    for blkno, placements in libinternals.get_blkno_placements(blknos, group=False):\n\n        assert placements.is_slice_like\n        assert blkno != -1\n\n        join_unit_indexers = indexers.copy()\n\n        shape_list = list(mgr_shape)\n        shape_list[0] = len(placements)\n        shape = tuple(shape_list)\n\n        blk = mgr.blocks[blkno]\n        ax0_blk_indexer = blklocs[placements.indexer]\n\n        unit_no_ax0_reindexing = (\n            len(placements) == len(blk.mgr_locs)\n            and\n            # Fastpath detection of join unit not\n            # needing to reindex its block: no ax0\n            # reindexing took place and block\n            # placement was sequential before.\n            (\n                (blk.mgr_locs.is_slice_like and blk.mgr_locs.as_slice.step == 1)\n                or\n                # Slow-ish detection: all indexer locs\n                # are sequential (and length match is\n                # checked above).\n                (np.diff(ax0_blk_indexer) == 1).all()\n            )\n        )\n\n        # Omit indexer if no item reindexing is required.\n        if unit_no_ax0_reindexing:\n            join_unit_indexers.pop(0, None)\n        else:\n            join_unit_indexers[0] = ax0_blk_indexer\n\n        unit = JoinUnit(blk, shape, join_unit_indexers)\n\n        plan.append((placements, unit))\n\n    return plan\n\n\nclass JoinUnit:\n    def __init__(self, block: Block, shape: Shape, indexers=None) -> None:\n        # Passing shape explicitly is required for cases when block is None.\n        # Note: block is None implies indexers is None, but not vice-versa\n        if indexers is None:\n            indexers = {}\n        self.block = block\n        self.indexers = indexers\n        self.shape = shape\n\n    def __repr__(self) -> str:\n        return f\"{type(self).__name__}({repr(self.block)}, {self.indexers})\"\n\n    @cache_readonly\n    def needs_filling(self) -> bool:\n        for indexer in self.indexers.values():\n            # FIXME: cache results of indexer == -1 checks.\n            if (indexer == -1).any():\n                return True\n\n        return False\n\n    @cache_readonly\n    def dtype(self) -> DtypeObj:\n        blk = self.block\n        if blk.values.dtype.kind == \"V\":\n            raise AssertionError(\"Block is None, no dtype\")\n\n        if not self.needs_filling:\n            return blk.dtype\n        return ensure_dtype_can_hold_na(blk.dtype)\n\n    def _is_valid_na_for(self, dtype: DtypeObj) -> bool:\n        \"\"\"\n        Check that we are all-NA of a type/dtype that is compatible with this dtype.\n        Augments `self.is_na` with an additional check of the type of NA values.\n        \"\"\"\n        if not self.is_na:\n            return False\n        if self.block.dtype.kind == \"V\":\n            return True\n\n        if self.dtype == object:\n            values = self.block.values\n            return all(is_valid_na_for_dtype(x, dtype) for x in values.ravel(order=\"K\"))\n\n        na_value = self.block.fill_value\n        if na_value is NaT and not is_dtype_equal(self.dtype, dtype):\n            # e.g. we are dt64 and other is td64\n            # fill_values match but we should not cast self.block.values to dtype\n            # TODO: this will need updating if we ever have non-nano dt64/td64\n            return False\n\n        if na_value is NA and needs_i8_conversion(dtype):\n            # FIXME: kludge; test_append_empty_frame_with_timedelta64ns_nat\n            #  e.g. self.dtype == \"Int64\" and dtype is td64, we dont want\n            #  to consider these as matching\n            return False\n\n        # TODO: better to use can_hold_element?\n        return is_valid_na_for_dtype(na_value, dtype)\n\n    @cache_readonly\n    def is_na(self) -> bool:\n        blk = self.block\n        if blk.dtype.kind == \"V\":\n            return True\n\n        if not blk._can_hold_na:\n            return False\n\n        values = blk.values\n        if values.size == 0:\n            return True\n        if isinstance(values.dtype, SparseDtype):\n            return False\n\n        if values.ndim == 1:\n            # TODO(EA2D): no need for special case with 2D EAs\n            val = values[0]\n            if not is_scalar(val) or not isna(val):\n                # ideally isna_all would do this short-circuiting\n                return False\n            return isna_all(values)\n        else:\n            val = values[0][0]\n            if not is_scalar(val) or not isna(val):\n                # ideally isna_all would do this short-circuiting\n                return False\n            return all(isna_all(row) for row in values)\n\n    def get_reindexed_values(self, empty_dtype: DtypeObj, upcasted_na) -> ArrayLike:\n        values: ArrayLike\n\n        if upcasted_na is None and self.block.dtype.kind != \"V\":\n            # No upcasting is necessary\n            fill_value = self.block.fill_value\n            values = self.block.values\n        else:\n            fill_value = upcasted_na\n\n            if self._is_valid_na_for(empty_dtype):\n                # note: always holds when self.block.dtype.kind == \"V\"\n                blk_dtype = self.block.dtype\n\n                if blk_dtype == np.dtype(\"object\"):\n                    # we want to avoid filling with np.nan if we are\n                    # using None; we already know that we are all\n                    # nulls\n                    values = self.block.values.ravel(order=\"K\")\n                    if len(values) and values[0] is None:\n                        fill_value = None\n\n                if isinstance(empty_dtype, DatetimeTZDtype):\n                    # NB: exclude e.g. pyarrow[dt64tz] dtypes\n                    i8values = np.full(self.shape, fill_value.value)\n                    return DatetimeArray(i8values, dtype=empty_dtype)\n\n                elif is_1d_only_ea_dtype(empty_dtype):\n                    if is_dtype_equal(blk_dtype, empty_dtype) and self.indexers:\n                        # avoid creating new empty array if we already have an array\n                        # with correct dtype that can be reindexed\n                        pass\n                    else:\n                        empty_dtype = cast(ExtensionDtype, empty_dtype)\n                        cls = empty_dtype.construct_array_type()\n\n                        missing_arr = cls._from_sequence([], dtype=empty_dtype)\n                        ncols, nrows = self.shape\n                        assert ncols == 1, ncols\n                        empty_arr = -1 * np.ones((nrows,), dtype=np.intp)\n                        return missing_arr.take(\n                            empty_arr, allow_fill=True, fill_value=fill_value\n                        )\n                elif isinstance(empty_dtype, ExtensionDtype):\n                    # TODO: no tests get here, a handful would if we disabled\n                    #  the dt64tz special-case above (which is faster)\n                    cls = empty_dtype.construct_array_type()\n                    missing_arr = cls._empty(shape=self.shape, dtype=empty_dtype)\n                    missing_arr[:] = fill_value\n                    return missing_arr\n                else:\n                    # NB: we should never get here with empty_dtype integer or bool;\n                    #  if we did, the missing_arr.fill would cast to gibberish\n                    missing_arr = np.empty(self.shape, dtype=empty_dtype)\n                    missing_arr.fill(fill_value)\n                    return missing_arr\n\n            if (not self.indexers) and (not self.block._can_consolidate):\n                # preserve these for validation in concat_compat\n                return self.block.values\n\n            if self.block.is_bool:\n                # External code requested filling/upcasting, bool values must\n                # be upcasted to object to avoid being upcasted to numeric.\n                values = self.block.astype(np.dtype(\"object\")).values\n            else:\n                # No dtype upcasting is done here, it will be performed during\n                # concatenation itself.\n                values = self.block.values\n\n        if not self.indexers:\n            # If there's no indexing to be done, we want to signal outside\n            # code that this array must be copied explicitly.  This is done\n            # by returning a view and checking `retval.base`.\n            values = values.view()\n\n        else:\n            for ax, indexer in self.indexers.items():\n                values = algos.take_nd(values, indexer, axis=ax)\n\n        return values\n\n\ndef _concatenate_join_units(\n    join_units: list[JoinUnit], concat_axis: AxisInt, copy: bool\n) -> ArrayLike:\n    \"\"\"\n    Concatenate values from several join units along selected axis.\n    \"\"\"\n    if concat_axis == 0 and len(join_units) > 1:\n        # Concatenating join units along ax0 is handled in _merge_blocks.\n        raise AssertionError(\"Concatenating join units along axis0\")\n\n    empty_dtype = _get_empty_dtype(join_units)\n\n    has_none_blocks = any(unit.block.dtype.kind == \"V\" for unit in join_units)\n    upcasted_na = _dtype_to_na_value(empty_dtype, has_none_blocks)\n\n    to_concat = [\n        ju.get_reindexed_values(empty_dtype=empty_dtype, upcasted_na=upcasted_na)\n        for ju in join_units\n    ]\n\n    if len(to_concat) == 1:\n        # Only one block, nothing to concatenate.\n        concat_values = to_concat[0]\n        if copy:\n            if isinstance(concat_values, np.ndarray):\n                # non-reindexed (=not yet copied) arrays are made into a view\n                # in JoinUnit.get_reindexed_values\n                if concat_values.base is not None:\n                    concat_values = concat_values.copy()\n            else:\n                concat_values = concat_values.copy()\n\n    elif any(is_1d_only_ea_dtype(t.dtype) for t in to_concat):\n        # TODO(EA2D): special case not needed if all EAs used HybridBlocks\n        # NB: we are still assuming here that Hybrid blocks have shape (1, N)\n        # concatting with at least one EA means we are concatting a single column\n        # the non-EA values are 2D arrays with shape (1, n)\n\n        # error: No overload variant of \"__getitem__\" of \"ExtensionArray\" matches\n        # argument type \"Tuple[int, slice]\"\n        to_concat = [\n            t\n            if is_1d_only_ea_dtype(t.dtype)\n            else t[0, :]  # type: ignore[call-overload]\n            for t in to_concat\n        ]\n        concat_values = concat_compat(to_concat, axis=0, ea_compat_axis=True)\n        concat_values = ensure_block_shape(concat_values, 2)\n\n    else:\n        concat_values = concat_compat(to_concat, axis=concat_axis)\n\n    return concat_values\n\n\ndef _dtype_to_na_value(dtype: DtypeObj, has_none_blocks: bool):\n    \"\"\"\n    Find the NA value to go with this dtype.\n    \"\"\"\n    if isinstance(dtype, ExtensionDtype):\n        return dtype.na_value\n    elif dtype.kind in [\"m\", \"M\"]:\n        return dtype.type(\"NaT\")\n    elif dtype.kind in [\"f\", \"c\"]:\n        return dtype.type(\"NaN\")\n    elif dtype.kind == \"b\":\n        # different from missing.na_value_for_dtype\n        return None\n    elif dtype.kind in [\"i\", \"u\"]:\n        if not has_none_blocks:\n            # different from missing.na_value_for_dtype\n            return None\n        return np.nan\n    elif dtype.kind == \"O\":\n        return np.nan\n    raise NotImplementedError\n\n\ndef _get_empty_dtype(join_units: Sequence[JoinUnit]) -> DtypeObj:\n    \"\"\"\n    Return dtype and N/A values to use when concatenating specified units.\n\n    Returned N/A value may be None which means there was no casting involved.\n\n    Returns\n    -------\n    dtype\n    \"\"\"\n    if len(join_units) == 1:\n        blk = join_units[0].block\n        return blk.dtype\n\n    if _is_uniform_reindex(join_units):\n        empty_dtype = join_units[0].block.dtype\n        return empty_dtype\n\n    has_none_blocks = any(unit.block.dtype.kind == \"V\" for unit in join_units)\n\n    dtypes = [unit.dtype for unit in join_units if not unit.is_na]\n    if not len(dtypes):\n        dtypes = [unit.dtype for unit in join_units if unit.block.dtype.kind != \"V\"]\n\n    dtype = find_common_type(dtypes)\n    if has_none_blocks:\n        dtype = ensure_dtype_can_hold_na(dtype)\n    return dtype\n\n\ndef _is_uniform_join_units(join_units: list[JoinUnit]) -> bool:\n    \"\"\"\n    Check if the join units consist of blocks of uniform type that can\n    be concatenated using Block.concat_same_type instead of the generic\n    _concatenate_join_units (which uses `concat_compat`).\n\n    \"\"\"\n    first = join_units[0].block\n    if first.dtype.kind == \"V\":\n        return False\n    return (\n        # exclude cases where a) ju.block is None or b) we have e.g. Int64+int64\n        all(type(ju.block) is type(first) for ju in join_units)\n        and\n        # e.g. DatetimeLikeBlock can be dt64 or td64, but these are not uniform\n        all(\n            is_dtype_equal(ju.block.dtype, first.dtype)\n            # GH#42092 we only want the dtype_equal check for non-numeric blocks\n            #  (for now, may change but that would need a deprecation)\n            or ju.block.dtype.kind in [\"b\", \"i\", \"u\"]\n            for ju in join_units\n        )\n        and\n        # no blocks that would get missing values (can lead to type upcasts)\n        # unless we're an extension dtype.\n        all(not ju.is_na or ju.block.is_extension for ju in join_units)\n        and\n        # no blocks with indexers (as then the dimensions do not fit)\n        all(not ju.indexers for ju in join_units)\n        and\n        # only use this path when there is something to concatenate\n        len(join_units) > 1\n    )\n\n\ndef _is_uniform_reindex(join_units) -> bool:\n    return (\n        # TODO: should this be ju.block._can_hold_na?\n        all(ju.block.is_extension for ju in join_units)\n        and len({ju.block.dtype.name for ju in join_units}) == 1\n    )\n\n\ndef _trim_join_unit(join_unit: JoinUnit, length: int) -> JoinUnit:\n    \"\"\"\n    Reduce join_unit's shape along item axis to length.\n\n    Extra items that didn't fit are returned as a separate block.\n    \"\"\"\n    if 0 not in join_unit.indexers:\n        extra_indexers = join_unit.indexers\n\n        if join_unit.block is None:\n            extra_block = None\n        else:\n            extra_block = join_unit.block.getitem_block(slice(length, None))\n            join_unit.block = join_unit.block.getitem_block(slice(length))\n    else:\n        extra_block = join_unit.block\n\n        extra_indexers = stdlib_copy.copy(join_unit.indexers)\n        extra_indexers[0] = extra_indexers[0][length:]\n        join_unit.indexers[0] = join_unit.indexers[0][:length]\n\n    extra_shape = (join_unit.shape[0] - length,) + join_unit.shape[1:]\n    join_unit.shape = (length,) + join_unit.shape[1:]\n\n    return JoinUnit(block=extra_block, indexers=extra_indexers, shape=extra_shape)\n\n\ndef _combine_concat_plans(plans, concat_axis: AxisInt):\n    \"\"\"\n    Combine multiple concatenation plans into one.\n\n    existing_plan is updated in-place.\n    \"\"\"\n    if len(plans) == 1:\n        for p in plans[0]:\n            yield p[0], [p[1]]\n\n    elif concat_axis == 0:\n        offset = 0\n        for plan in plans:\n            last_plc = None\n\n            for plc, unit in plan:\n                yield plc.add(offset), [unit]\n                last_plc = plc\n\n            if last_plc is not None:\n                offset += last_plc.as_slice.stop\n\n    else:\n        # singleton list so we can modify it as a side-effect within _next_or_none\n        num_ended = [0]\n\n        def _next_or_none(seq):\n            retval = next(seq, None)\n            if retval is None:\n                num_ended[0] += 1\n            return retval\n\n        plans = list(map(iter, plans))\n        next_items = list(map(_next_or_none, plans))\n\n        while num_ended[0] != len(next_items):\n            if num_ended[0] > 0:\n                raise ValueError(\"Plan shapes are not aligned\")\n\n            placements, units = zip(*next_items)\n\n            lengths = list(map(len, placements))\n            min_len, max_len = min(lengths), max(lengths)\n\n            if min_len == max_len:\n                yield placements[0], units\n                next_items[:] = map(_next_or_none, plans)\n            else:\n                yielded_placement = None\n                yielded_units = [None] * len(next_items)\n                for i, (plc, unit) in enumerate(next_items):\n                    yielded_units[i] = unit\n                    if len(plc) > min_len:\n                        # _trim_join_unit updates unit in place, so only\n                        # placement needs to be sliced to skip min_len.\n                        next_items[i] = (plc[min_len:], _trim_join_unit(unit, min_len))\n                    else:\n                        yielded_placement = plc\n                        next_items[i] = _next_or_none(plans[i])\n\n                yield yielded_placement, yielded_units\n"
    },
    {
      "filename": "pandas/core/reshape/merge.py",
      "content": "\"\"\"\nSQL-style merge routines\n\"\"\"\nfrom __future__ import annotations\n\nimport copy as stdlib_copy\nimport datetime\nfrom functools import partial\nimport string\nfrom typing import (\n    TYPE_CHECKING,\n    Hashable,\n    Sequence,\n    cast,\n)\nimport uuid\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import (\n    Timedelta,\n    hashtable as libhashtable,\n    join as libjoin,\n    lib,\n)\nfrom pandas._typing import (\n    AnyArrayLike,\n    ArrayLike,\n    AxisInt,\n    DtypeObj,\n    IndexLabel,\n    Shape,\n    Suffixes,\n    npt,\n)\nfrom pandas.errors import MergeError\nfrom pandas.util._decorators import (\n    Appender,\n    Substitution,\n    cache_readonly,\n)\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.core.dtypes.cast import find_common_type\nfrom pandas.core.dtypes.common import (\n    ensure_float64,\n    ensure_int64,\n    ensure_object,\n    is_array_like,\n    is_bool,\n    is_bool_dtype,\n    is_categorical_dtype,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float_dtype,\n    is_integer,\n    is_integer_dtype,\n    is_list_like,\n    is_number,\n    is_numeric_dtype,\n    is_object_dtype,\n    needs_i8_conversion,\n)\nfrom pandas.core.dtypes.dtypes import DatetimeTZDtype\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.missing import (\n    isna,\n    na_value_for_dtype,\n)\n\nfrom pandas import (\n    Categorical,\n    Index,\n    MultiIndex,\n    Series,\n)\nimport pandas.core.algorithms as algos\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.arrays._mixins import NDArrayBackedExtensionArray\nimport pandas.core.common as com\nfrom pandas.core.construction import extract_array\nfrom pandas.core.frame import _merge_doc\nfrom pandas.core.indexes.api import default_index\nfrom pandas.core.sorting import is_int64_overflow_possible\n\nif TYPE_CHECKING:\n    from pandas import DataFrame\n    from pandas.core import groupby\n    from pandas.core.arrays import DatetimeArray\n\n\n@Substitution(\"\\nleft : DataFrame or named Series\")\n@Appender(_merge_doc, indents=0)\ndef merge(\n    left: DataFrame | Series,\n    right: DataFrame | Series,\n    how: str = \"inner\",\n    on: IndexLabel | None = None,\n    left_on: IndexLabel | None = None,\n    right_on: IndexLabel | None = None,\n    left_index: bool = False,\n    right_index: bool = False,\n    sort: bool = False,\n    suffixes: Suffixes = (\"_x\", \"_y\"),\n    copy: bool = True,\n    indicator: str | bool = False,\n    validate: str | None = None,\n) -> DataFrame:\n    op = _MergeOperation(\n        left,\n        right,\n        how=how,\n        on=on,\n        left_on=left_on,\n        right_on=right_on,\n        left_index=left_index,\n        right_index=right_index,\n        sort=sort,\n        suffixes=suffixes,\n        indicator=indicator,\n        validate=validate,\n    )\n    return op.get_result(copy=copy)\n\n\nif __debug__:\n    merge.__doc__ = _merge_doc % \"\\nleft : DataFrame\"\n\n\ndef _groupby_and_merge(by, left: DataFrame, right: DataFrame, merge_pieces):\n    \"\"\"\n    groupby & merge; we are always performing a left-by type operation\n\n    Parameters\n    ----------\n    by: field to group\n    left: DataFrame\n    right: DataFrame\n    merge_pieces: function for merging\n    \"\"\"\n    pieces = []\n    if not isinstance(by, (list, tuple)):\n        by = [by]\n\n    lby = left.groupby(by, sort=False)\n    rby: groupby.DataFrameGroupBy | None = None\n\n    # if we can groupby the rhs\n    # then we can get vastly better perf\n    if all(item in right.columns for item in by):\n        rby = right.groupby(by, sort=False)\n\n    for key, lhs in lby.grouper.get_iterator(lby._selected_obj, axis=lby.axis):\n\n        if rby is None:\n            rhs = right\n        else:\n            try:\n                rhs = right.take(rby.indices[key])\n            except KeyError:\n                # key doesn't exist in left\n                lcols = lhs.columns.tolist()\n                cols = lcols + [r for r in right.columns if r not in set(lcols)]\n                merged = lhs.reindex(columns=cols)\n                merged.index = range(len(merged))\n                pieces.append(merged)\n                continue\n\n        merged = merge_pieces(lhs, rhs)\n\n        # make sure join keys are in the merged\n        # TODO, should merge_pieces do this?\n        merged[by] = key\n\n        pieces.append(merged)\n\n    # preserve the original order\n    # if we have a missing piece this can be reset\n    from pandas.core.reshape.concat import concat\n\n    result = concat(pieces, ignore_index=True)\n    result = result.reindex(columns=pieces[0].columns, copy=False)\n    return result, lby\n\n\ndef merge_ordered(\n    left: DataFrame,\n    right: DataFrame,\n    on: IndexLabel | None = None,\n    left_on: IndexLabel | None = None,\n    right_on: IndexLabel | None = None,\n    left_by=None,\n    right_by=None,\n    fill_method: str | None = None,\n    suffixes: Suffixes = (\"_x\", \"_y\"),\n    how: str = \"outer\",\n) -> DataFrame:\n    \"\"\"\n    Perform a merge for ordered data with optional filling/interpolation.\n\n    Designed for ordered data like time series data. Optionally\n    perform group-wise merge (see examples).\n\n    Parameters\n    ----------\n    left : DataFrame\n    right : DataFrame\n    on : label or list\n        Field names to join on. Must be found in both DataFrames.\n    left_on : label or list, or array-like\n        Field names to join on in left DataFrame. Can be a vector or list of\n        vectors of the length of the DataFrame to use a particular vector as\n        the join key instead of columns.\n    right_on : label or list, or array-like\n        Field names to join on in right DataFrame or vector/list of vectors per\n        left_on docs.\n    left_by : column name or list of column names\n        Group left DataFrame by group columns and merge piece by piece with\n        right DataFrame.\n    right_by : column name or list of column names\n        Group right DataFrame by group columns and merge piece by piece with\n        left DataFrame.\n    fill_method : {'ffill', None}, default None\n        Interpolation method for data.\n    suffixes : list-like, default is (\"_x\", \"_y\")\n        A length-2 sequence where each element is optionally a string\n        indicating the suffix to add to overlapping column names in\n        `left` and `right` respectively. Pass a value of `None` instead\n        of a string to indicate that the column name from `left` or\n        `right` should be left as-is, with no suffix. At least one of the\n        values must not be None.\n\n        .. versionchanged:: 0.25.0\n    how : {'left', 'right', 'outer', 'inner'}, default 'outer'\n        * left: use only keys from left frame (SQL: left outer join)\n        * right: use only keys from right frame (SQL: right outer join)\n        * outer: use union of keys from both frames (SQL: full outer join)\n        * inner: use intersection of keys from both frames (SQL: inner join).\n\n    Returns\n    -------\n    DataFrame\n        The merged DataFrame output type will be the same as\n        'left', if it is a subclass of DataFrame.\n\n    See Also\n    --------\n    merge : Merge with a database-style join.\n    merge_asof : Merge on nearest keys.\n\n    Examples\n    --------\n    >>> df1 = pd.DataFrame(\n    ...     {\n    ...         \"key\": [\"a\", \"c\", \"e\", \"a\", \"c\", \"e\"],\n    ...         \"lvalue\": [1, 2, 3, 1, 2, 3],\n    ...         \"group\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"]\n    ...     }\n    ... )\n    >>> df1\n          key  lvalue group\n    0   a       1     a\n    1   c       2     a\n    2   e       3     a\n    3   a       1     b\n    4   c       2     b\n    5   e       3     b\n\n    >>> df2 = pd.DataFrame({\"key\": [\"b\", \"c\", \"d\"], \"rvalue\": [1, 2, 3]})\n    >>> df2\n          key  rvalue\n    0   b       1\n    1   c       2\n    2   d       3\n\n    >>> merge_ordered(df1, df2, fill_method=\"ffill\", left_by=\"group\")\n      key  lvalue group  rvalue\n    0   a       1     a     NaN\n    1   b       1     a     1.0\n    2   c       2     a     2.0\n    3   d       2     a     3.0\n    4   e       3     a     3.0\n    5   a       1     b     NaN\n    6   b       1     b     1.0\n    7   c       2     b     2.0\n    8   d       2     b     3.0\n    9   e       3     b     3.0\n    \"\"\"\n\n    def _merger(x, y) -> DataFrame:\n        # perform the ordered merge operation\n        op = _OrderedMerge(\n            x,\n            y,\n            on=on,\n            left_on=left_on,\n            right_on=right_on,\n            suffixes=suffixes,\n            fill_method=fill_method,\n            how=how,\n        )\n        return op.get_result()\n\n    if left_by is not None and right_by is not None:\n        raise ValueError(\"Can only group either left or right frames\")\n    if left_by is not None:\n        if isinstance(left_by, str):\n            left_by = [left_by]\n        check = set(left_by).difference(left.columns)\n        if len(check) != 0:\n            raise KeyError(f\"{check} not found in left columns\")\n        result, _ = _groupby_and_merge(left_by, left, right, lambda x, y: _merger(x, y))\n    elif right_by is not None:\n        if isinstance(right_by, str):\n            right_by = [right_by]\n        check = set(right_by).difference(right.columns)\n        if len(check) != 0:\n            raise KeyError(f\"{check} not found in right columns\")\n        result, _ = _groupby_and_merge(\n            right_by, right, left, lambda x, y: _merger(y, x)\n        )\n    else:\n        result = _merger(left, right)\n    return result\n\n\ndef merge_asof(\n    left: DataFrame | Series,\n    right: DataFrame | Series,\n    on: IndexLabel | None = None,\n    left_on: IndexLabel | None = None,\n    right_on: IndexLabel | None = None,\n    left_index: bool = False,\n    right_index: bool = False,\n    by=None,\n    left_by=None,\n    right_by=None,\n    suffixes: Suffixes = (\"_x\", \"_y\"),\n    tolerance=None,\n    allow_exact_matches: bool = True,\n    direction: str = \"backward\",\n) -> DataFrame:\n    \"\"\"\n    Perform a merge by key distance.\n\n    This is similar to a left-join except that we match on nearest\n    key rather than equal keys. Both DataFrames must be sorted by the key.\n\n    For each row in the left DataFrame:\n\n      - A \"backward\" search selects the last row in the right DataFrame whose\n        'on' key is less than or equal to the left's key.\n\n      - A \"forward\" search selects the first row in the right DataFrame whose\n        'on' key is greater than or equal to the left's key.\n\n      - A \"nearest\" search selects the row in the right DataFrame whose 'on'\n        key is closest in absolute distance to the left's key.\n\n    The default is \"backward\" and is compatible in versions below 0.20.0.\n    The direction parameter was added in version 0.20.0 and introduces\n    \"forward\" and \"nearest\".\n\n    Optionally match on equivalent keys with 'by' before searching with 'on'.\n\n    Parameters\n    ----------\n    left : DataFrame or named Series\n    right : DataFrame or named Series\n    on : label\n        Field name to join on. Must be found in both DataFrames.\n        The data MUST be ordered. Furthermore this must be a numeric column,\n        such as datetimelike, integer, or float. On or left_on/right_on\n        must be given.\n    left_on : label\n        Field name to join on in left DataFrame.\n    right_on : label\n        Field name to join on in right DataFrame.\n    left_index : bool\n        Use the index of the left DataFrame as the join key.\n    right_index : bool\n        Use the index of the right DataFrame as the join key.\n    by : column name or list of column names\n        Match on these columns before performing merge operation.\n    left_by : column name\n        Field names to match on in the left DataFrame.\n    right_by : column name\n        Field names to match on in the right DataFrame.\n    suffixes : 2-length sequence (tuple, list, ...)\n        Suffix to apply to overlapping column names in the left and right\n        side, respectively.\n    tolerance : int or Timedelta, optional, default None\n        Select asof tolerance within this range; must be compatible\n        with the merge index.\n    allow_exact_matches : bool, default True\n\n        - If True, allow matching with the same 'on' value\n          (i.e. less-than-or-equal-to / greater-than-or-equal-to)\n        - If False, don't match the same 'on' value\n          (i.e., strictly less-than / strictly greater-than).\n\n    direction : 'backward' (default), 'forward', or 'nearest'\n        Whether to search for prior, subsequent, or closest matches.\n\n    Returns\n    -------\n    merged : DataFrame\n\n    See Also\n    --------\n    merge : Merge with a database-style join.\n    merge_ordered : Merge with optional filling/interpolation.\n\n    Examples\n    --------\n    >>> left = pd.DataFrame({\"a\": [1, 5, 10], \"left_val\": [\"a\", \"b\", \"c\"]})\n    >>> left\n        a left_val\n    0   1        a\n    1   5        b\n    2  10        c\n\n    >>> right = pd.DataFrame({\"a\": [1, 2, 3, 6, 7], \"right_val\": [1, 2, 3, 6, 7]})\n    >>> right\n       a  right_val\n    0  1          1\n    1  2          2\n    2  3          3\n    3  6          6\n    4  7          7\n\n    >>> pd.merge_asof(left, right, on=\"a\")\n        a left_val  right_val\n    0   1        a          1\n    1   5        b          3\n    2  10        c          7\n\n    >>> pd.merge_asof(left, right, on=\"a\", allow_exact_matches=False)\n        a left_val  right_val\n    0   1        a        NaN\n    1   5        b        3.0\n    2  10        c        7.0\n\n    >>> pd.merge_asof(left, right, on=\"a\", direction=\"forward\")\n        a left_val  right_val\n    0   1        a        1.0\n    1   5        b        6.0\n    2  10        c        NaN\n\n    >>> pd.merge_asof(left, right, on=\"a\", direction=\"nearest\")\n        a left_val  right_val\n    0   1        a          1\n    1   5        b          6\n    2  10        c          7\n\n    We can use indexed DataFrames as well.\n\n    >>> left = pd.DataFrame({\"left_val\": [\"a\", \"b\", \"c\"]}, index=[1, 5, 10])\n    >>> left\n       left_val\n    1         a\n    5         b\n    10        c\n\n    >>> right = pd.DataFrame({\"right_val\": [1, 2, 3, 6, 7]}, index=[1, 2, 3, 6, 7])\n    >>> right\n       right_val\n    1          1\n    2          2\n    3          3\n    6          6\n    7          7\n\n    >>> pd.merge_asof(left, right, left_index=True, right_index=True)\n       left_val  right_val\n    1         a          1\n    5         b          3\n    10        c          7\n\n    Here is a real-world times-series example\n\n    >>> quotes = pd.DataFrame(\n    ...     {\n    ...         \"time\": [\n    ...             pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n    ...             pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n    ...             pd.Timestamp(\"2016-05-25 13:30:00.030\"),\n    ...             pd.Timestamp(\"2016-05-25 13:30:00.041\"),\n    ...             pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n    ...             pd.Timestamp(\"2016-05-25 13:30:00.049\"),\n    ...             pd.Timestamp(\"2016-05-25 13:30:00.072\"),\n    ...             pd.Timestamp(\"2016-05-25 13:30:00.075\")\n    ...         ],\n    ...         \"ticker\": [\n    ...                \"GOOG\",\n    ...                \"MSFT\",\n    ...                \"MSFT\",\n    ...                \"MSFT\",\n    ...                \"GOOG\",\n    ...                \"AAPL\",\n    ...                \"GOOG\",\n    ...                \"MSFT\"\n    ...            ],\n    ...            \"bid\": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],\n    ...            \"ask\": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03]\n    ...     }\n    ... )\n    >>> quotes\n                         time ticker     bid     ask\n    0 2016-05-25 13:30:00.023   GOOG  720.50  720.93\n    1 2016-05-25 13:30:00.023   MSFT   51.95   51.96\n    2 2016-05-25 13:30:00.030   MSFT   51.97   51.98\n    3 2016-05-25 13:30:00.041   MSFT   51.99   52.00\n    4 2016-05-25 13:30:00.048   GOOG  720.50  720.93\n    5 2016-05-25 13:30:00.049   AAPL   97.99   98.01\n    6 2016-05-25 13:30:00.072   GOOG  720.50  720.88\n    7 2016-05-25 13:30:00.075   MSFT   52.01   52.03\n\n    >>> trades = pd.DataFrame(\n    ...        {\n    ...            \"time\": [\n    ...                pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n    ...                pd.Timestamp(\"2016-05-25 13:30:00.038\"),\n    ...                pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n    ...                pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n    ...                pd.Timestamp(\"2016-05-25 13:30:00.048\")\n    ...            ],\n    ...            \"ticker\": [\"MSFT\", \"MSFT\", \"GOOG\", \"GOOG\", \"AAPL\"],\n    ...            \"price\": [51.95, 51.95, 720.77, 720.92, 98.0],\n    ...            \"quantity\": [75, 155, 100, 100, 100]\n    ...        }\n    ...    )\n    >>> trades\n                         time ticker   price  quantity\n    0 2016-05-25 13:30:00.023   MSFT   51.95        75\n    1 2016-05-25 13:30:00.038   MSFT   51.95       155\n    2 2016-05-25 13:30:00.048   GOOG  720.77       100\n    3 2016-05-25 13:30:00.048   GOOG  720.92       100\n    4 2016-05-25 13:30:00.048   AAPL   98.00       100\n\n    By default we are taking the asof of the quotes\n\n    >>> pd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\")\n                         time ticker   price  quantity     bid     ask\n    0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n    1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n    2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n    3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n    4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n\n    We only asof within 2ms between the quote time and the trade time\n\n    >>> pd.merge_asof(\n    ...     trades, quotes, on=\"time\", by=\"ticker\", tolerance=pd.Timedelta(\"2ms\")\n    ... )\n                         time ticker   price  quantity     bid     ask\n    0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n    1 2016-05-25 13:30:00.038   MSFT   51.95       155     NaN     NaN\n    2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n    3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n    4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n\n    We only asof within 10ms between the quote time and the trade time\n    and we exclude exact matches on time. However *prior* data will\n    propagate forward\n\n    >>> pd.merge_asof(\n    ...     trades,\n    ...     quotes,\n    ...     on=\"time\",\n    ...     by=\"ticker\",\n    ...     tolerance=pd.Timedelta(\"10ms\"),\n    ...     allow_exact_matches=False\n    ... )\n                         time ticker   price  quantity     bid     ask\n    0 2016-05-25 13:30:00.023   MSFT   51.95        75     NaN     NaN\n    1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n    2 2016-05-25 13:30:00.048   GOOG  720.77       100     NaN     NaN\n    3 2016-05-25 13:30:00.048   GOOG  720.92       100     NaN     NaN\n    4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n    \"\"\"\n    op = _AsOfMerge(\n        left,\n        right,\n        on=on,\n        left_on=left_on,\n        right_on=right_on,\n        left_index=left_index,\n        right_index=right_index,\n        by=by,\n        left_by=left_by,\n        right_by=right_by,\n        suffixes=suffixes,\n        how=\"asof\",\n        tolerance=tolerance,\n        allow_exact_matches=allow_exact_matches,\n        direction=direction,\n    )\n    return op.get_result()\n\n\n# TODO: transformations??\n# TODO: only copy DataFrames when modification necessary\nclass _MergeOperation:\n    \"\"\"\n    Perform a database (SQL) merge operation between two DataFrame or Series\n    objects using either columns as keys or their row indexes\n    \"\"\"\n\n    _merge_type = \"merge\"\n    how: str\n    on: IndexLabel | None\n    # left_on/right_on may be None when passed, but in validate_specification\n    #  get replaced with non-None.\n    left_on: Sequence[Hashable | AnyArrayLike]\n    right_on: Sequence[Hashable | AnyArrayLike]\n    left_index: bool\n    right_index: bool\n    axis: AxisInt\n    bm_axis: AxisInt\n    sort: bool\n    suffixes: Suffixes\n    copy: bool\n    indicator: str | bool\n    validate: str | None\n    join_names: list[Hashable]\n    right_join_keys: list[AnyArrayLike]\n    left_join_keys: list[AnyArrayLike]\n\n    def __init__(\n        self,\n        left: DataFrame | Series,\n        right: DataFrame | Series,\n        how: str = \"inner\",\n        on: IndexLabel | None = None,\n        left_on: IndexLabel | None = None,\n        right_on: IndexLabel | None = None,\n        axis: AxisInt = 1,\n        left_index: bool = False,\n        right_index: bool = False,\n        sort: bool = True,\n        suffixes: Suffixes = (\"_x\", \"_y\"),\n        indicator: str | bool = False,\n        validate: str | None = None,\n    ) -> None:\n        _left = _validate_operand(left)\n        _right = _validate_operand(right)\n        self.left = self.orig_left = _left\n        self.right = self.orig_right = _right\n        self.how = how\n\n        # bm_axis -> the axis on the BlockManager\n        self.bm_axis = axis\n        # axis --> the axis on the Series/DataFrame\n        self.axis = 1 - axis if self.left.ndim == 2 else 0\n\n        self.on = com.maybe_make_list(on)\n\n        self.suffixes = suffixes\n        self.sort = sort\n\n        self.left_index = left_index\n        self.right_index = right_index\n\n        self.indicator = indicator\n\n        if not is_bool(left_index):\n            raise ValueError(\n                f\"left_index parameter must be of type bool, not {type(left_index)}\"\n            )\n        if not is_bool(right_index):\n            raise ValueError(\n                f\"right_index parameter must be of type bool, not {type(right_index)}\"\n            )\n\n        # GH 40993: raise when merging between different levels; enforced in 2.0\n        if _left.columns.nlevels != _right.columns.nlevels:\n            msg = (\n                \"Not allowed to merge between different levels. \"\n                f\"({_left.columns.nlevels} levels on the left, \"\n                f\"{_right.columns.nlevels} on the right)\"\n            )\n            raise MergeError(msg)\n\n        self.left_on, self.right_on = self._validate_left_right_on(left_on, right_on)\n\n        cross_col = None\n        if self.how == \"cross\":\n            (\n                self.left,\n                self.right,\n                self.how,\n                cross_col,\n            ) = self._create_cross_configuration(self.left, self.right)\n            self.left_on = self.right_on = [cross_col]\n        self._cross = cross_col\n\n        # note this function has side effects\n        (\n            self.left_join_keys,\n            self.right_join_keys,\n            self.join_names,\n        ) = self._get_merge_keys()\n\n        # validate the merge keys dtypes. We may need to coerce\n        # to avoid incompatible dtypes\n        self._maybe_coerce_merge_keys()\n\n        # If argument passed to validate,\n        # check if columns specified as unique\n        # are in fact unique.\n        if validate is not None:\n            self._validate(validate)\n\n    def _reindex_and_concat(\n        self,\n        join_index: Index,\n        left_indexer: npt.NDArray[np.intp] | None,\n        right_indexer: npt.NDArray[np.intp] | None,\n        copy: bool,\n    ) -> DataFrame:\n        \"\"\"\n        reindex along index and concat along columns.\n        \"\"\"\n        # Take views so we do not alter the originals\n        left = self.left[:]\n        right = self.right[:]\n\n        llabels, rlabels = _items_overlap_with_suffix(\n            self.left._info_axis, self.right._info_axis, self.suffixes\n        )\n\n        if left_indexer is not None:\n            # Pinning the index here (and in the right code just below) is not\n            #  necessary, but makes the `.take` more performant if we have e.g.\n            #  a MultiIndex for left.index.\n            lmgr = left._mgr.reindex_indexer(\n                join_index,\n                left_indexer,\n                axis=1,\n                copy=False,\n                only_slice=True,\n                allow_dups=True,\n                use_na_proxy=True,\n            )\n            left = left._constructor(lmgr)\n        left.index = join_index\n\n        if right_indexer is not None:\n            rmgr = right._mgr.reindex_indexer(\n                join_index,\n                right_indexer,\n                axis=1,\n                copy=False,\n                only_slice=True,\n                allow_dups=True,\n                use_na_proxy=True,\n            )\n            right = right._constructor(rmgr)\n        right.index = join_index\n\n        from pandas import concat\n\n        left.columns = llabels\n        right.columns = rlabels\n        result = concat([left, right], axis=1, copy=copy)\n        return result\n\n    def get_result(self, copy: bool = True) -> DataFrame:\n        if self.indicator:\n            self.left, self.right = self._indicator_pre_merge(self.left, self.right)\n\n        join_index, left_indexer, right_indexer = self._get_join_info()\n\n        result = self._reindex_and_concat(\n            join_index, left_indexer, right_indexer, copy=copy\n        )\n        result = result.__finalize__(self, method=self._merge_type)\n\n        if self.indicator:\n            result = self._indicator_post_merge(result)\n\n        self._maybe_add_join_keys(result, left_indexer, right_indexer)\n\n        self._maybe_restore_index_levels(result)\n\n        self._maybe_drop_cross_column(result, self._cross)\n\n        return result.__finalize__(self, method=\"merge\")\n\n    def _maybe_drop_cross_column(\n        self, result: DataFrame, cross_col: str | None\n    ) -> None:\n        if cross_col is not None:\n            del result[cross_col]\n\n    @cache_readonly\n    def _indicator_name(self) -> str | None:\n        if isinstance(self.indicator, str):\n            return self.indicator\n        elif isinstance(self.indicator, bool):\n            return \"_merge\" if self.indicator else None\n        else:\n            raise ValueError(\n                \"indicator option can only accept boolean or string arguments\"\n            )\n\n    def _indicator_pre_merge(\n        self, left: DataFrame, right: DataFrame\n    ) -> tuple[DataFrame, DataFrame]:\n\n        columns = left.columns.union(right.columns)\n\n        for i in [\"_left_indicator\", \"_right_indicator\"]:\n            if i in columns:\n                raise ValueError(\n                    \"Cannot use `indicator=True` option when \"\n                    f\"data contains a column named {i}\"\n                )\n        if self._indicator_name in columns:\n            raise ValueError(\n                \"Cannot use name of an existing column for indicator column\"\n            )\n\n        left = left.copy()\n        right = right.copy()\n\n        left[\"_left_indicator\"] = 1\n        left[\"_left_indicator\"] = left[\"_left_indicator\"].astype(\"int8\")\n\n        right[\"_right_indicator\"] = 2\n        right[\"_right_indicator\"] = right[\"_right_indicator\"].astype(\"int8\")\n\n        return left, right\n\n    def _indicator_post_merge(self, result: DataFrame) -> DataFrame:\n\n        result[\"_left_indicator\"] = result[\"_left_indicator\"].fillna(0)\n        result[\"_right_indicator\"] = result[\"_right_indicator\"].fillna(0)\n\n        result[self._indicator_name] = Categorical(\n            (result[\"_left_indicator\"] + result[\"_right_indicator\"]),\n            categories=[1, 2, 3],\n        )\n        result[self._indicator_name] = result[\n            self._indicator_name\n        ].cat.rename_categories([\"left_only\", \"right_only\", \"both\"])\n\n        result = result.drop(labels=[\"_left_indicator\", \"_right_indicator\"], axis=1)\n        return result\n\n    def _maybe_restore_index_levels(self, result: DataFrame) -> None:\n        \"\"\"\n        Restore index levels specified as `on` parameters\n\n        Here we check for cases where `self.left_on` and `self.right_on` pairs\n        each reference an index level in their respective DataFrames. The\n        joined columns corresponding to these pairs are then restored to the\n        index of `result`.\n\n        **Note:** This method has side effects. It modifies `result` in-place\n\n        Parameters\n        ----------\n        result: DataFrame\n            merge result\n\n        Returns\n        -------\n        None\n        \"\"\"\n        names_to_restore = []\n        for name, left_key, right_key in zip(\n            self.join_names, self.left_on, self.right_on\n        ):\n            if (\n                # Argument 1 to \"_is_level_reference\" of \"NDFrame\" has incompatible\n                # type \"Union[Hashable, ExtensionArray, Index, Series]\"; expected\n                # \"Hashable\"\n                self.orig_left._is_level_reference(left_key)  # type: ignore[arg-type]\n                # Argument 1 to \"_is_level_reference\" of \"NDFrame\" has incompatible\n                # type \"Union[Hashable, ExtensionArray, Index, Series]\"; expected\n                # \"Hashable\"\n                and self.orig_right._is_level_reference(\n                    right_key  # type: ignore[arg-type]\n                )\n                and left_key == right_key\n                and name not in result.index.names\n            ):\n\n                names_to_restore.append(name)\n\n        if names_to_restore:\n            result.set_index(names_to_restore, inplace=True)\n\n    def _maybe_add_join_keys(\n        self,\n        result: DataFrame,\n        left_indexer: np.ndarray | None,\n        right_indexer: np.ndarray | None,\n    ) -> None:\n\n        left_has_missing = None\n        right_has_missing = None\n\n        assert all(is_array_like(x) for x in self.left_join_keys)\n\n        keys = zip(self.join_names, self.left_on, self.right_on)\n        for i, (name, lname, rname) in enumerate(keys):\n            if not _should_fill(lname, rname):\n                continue\n\n            take_left, take_right = None, None\n\n            if name in result:\n\n                if left_indexer is not None and right_indexer is not None:\n                    if name in self.left:\n\n                        if left_has_missing is None:\n                            left_has_missing = (left_indexer == -1).any()\n\n                        if left_has_missing:\n                            take_right = self.right_join_keys[i]\n\n                            if not is_dtype_equal(\n                                result[name].dtype, self.left[name].dtype\n                            ):\n                                take_left = self.left[name]._values\n\n                    elif name in self.right:\n\n                        if right_has_missing is None:\n                            right_has_missing = (right_indexer == -1).any()\n\n                        if right_has_missing:\n                            take_left = self.left_join_keys[i]\n\n                            if not is_dtype_equal(\n                                result[name].dtype, self.right[name].dtype\n                            ):\n                                take_right = self.right[name]._values\n\n            elif left_indexer is not None:\n                take_left = self.left_join_keys[i]\n                take_right = self.right_join_keys[i]\n\n            if take_left is not None or take_right is not None:\n\n                if take_left is None:\n                    lvals = result[name]._values\n                else:\n                    # TODO: can we pin down take_left's type earlier?\n                    take_left = extract_array(take_left, extract_numpy=True)\n                    lfill = na_value_for_dtype(take_left.dtype)\n                    lvals = algos.take_nd(take_left, left_indexer, fill_value=lfill)\n\n                if take_right is None:\n                    rvals = result[name]._values\n                else:\n                    # TODO: can we pin down take_right's type earlier?\n                    taker = extract_array(take_right, extract_numpy=True)\n                    rfill = na_value_for_dtype(taker.dtype)\n                    rvals = algos.take_nd(taker, right_indexer, fill_value=rfill)\n\n                # if we have an all missing left_indexer\n                # make sure to just use the right values or vice-versa\n                mask_left = left_indexer == -1\n                # error: Item \"bool\" of \"Union[Any, bool]\" has no attribute \"all\"\n                if mask_left.all():  # type: ignore[union-attr]\n                    key_col = Index(rvals)\n                    result_dtype = rvals.dtype\n                elif right_indexer is not None and (right_indexer == -1).all():\n                    key_col = Index(lvals)\n                    result_dtype = lvals.dtype\n                else:\n                    key_col = Index(lvals).where(~mask_left, rvals)\n                    result_dtype = find_common_type([lvals.dtype, rvals.dtype])\n\n                if result._is_label_reference(name):\n                    result[name] = Series(\n                        key_col, dtype=result_dtype, index=result.index\n                    )\n                elif result._is_level_reference(name):\n                    if isinstance(result.index, MultiIndex):\n                        key_col.name = name\n                        idx_list = [\n                            result.index.get_level_values(level_name)\n                            if level_name != name\n                            else key_col\n                            for level_name in result.index.names\n                        ]\n\n                        result.set_index(idx_list, inplace=True)\n                    else:\n                        result.index = Index(key_col, name=name)\n                else:\n                    result.insert(i, name or f\"key_{i}\", key_col)\n\n    def _get_join_indexers(self) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n        \"\"\"return the join indexers\"\"\"\n        return get_join_indexers(\n            self.left_join_keys, self.right_join_keys, sort=self.sort, how=self.how\n        )\n\n    def _get_join_info(\n        self,\n    ) -> tuple[Index, npt.NDArray[np.intp] | None, npt.NDArray[np.intp] | None]:\n\n        left_ax = self.left.axes[self.axis]\n        right_ax = self.right.axes[self.axis]\n\n        if self.left_index and self.right_index and self.how != \"asof\":\n            join_index, left_indexer, right_indexer = left_ax.join(\n                right_ax, how=self.how, return_indexers=True, sort=self.sort\n            )\n\n        elif self.right_index and self.how == \"left\":\n            join_index, left_indexer, right_indexer = _left_join_on_index(\n                left_ax, right_ax, self.left_join_keys, sort=self.sort\n            )\n\n        elif self.left_index and self.how == \"right\":\n            join_index, right_indexer, left_indexer = _left_join_on_index(\n                right_ax, left_ax, self.right_join_keys, sort=self.sort\n            )\n        else:\n            (left_indexer, right_indexer) = self._get_join_indexers()\n\n            if self.right_index:\n                if len(self.left) > 0:\n                    join_index = self._create_join_index(\n                        self.left.index,\n                        self.right.index,\n                        left_indexer,\n                        how=\"right\",\n                    )\n                else:\n                    join_index = self.right.index.take(right_indexer)\n            elif self.left_index:\n                if self.how == \"asof\":\n                    # GH#33463 asof should always behave like a left merge\n                    join_index = self._create_join_index(\n                        self.left.index,\n                        self.right.index,\n                        left_indexer,\n                        how=\"left\",\n                    )\n\n                elif len(self.right) > 0:\n                    join_index = self._create_join_index(\n                        self.right.index,\n                        self.left.index,\n                        right_indexer,\n                        how=\"left\",\n                    )\n                else:\n                    join_index = self.left.index.take(left_indexer)\n            else:\n                join_index = default_index(len(left_indexer))\n\n        if len(join_index) == 0:\n            join_index = join_index.astype(object)\n        return join_index, left_indexer, right_indexer\n\n    def _create_join_index(\n        self,\n        index: Index,\n        other_index: Index,\n        indexer: npt.NDArray[np.intp],\n        how: str = \"left\",\n    ) -> Index:\n        \"\"\"\n        Create a join index by rearranging one index to match another\n\n        Parameters\n        ----------\n        index : Index being rearranged\n        other_index : Index used to supply values not found in index\n        indexer : np.ndarray[np.intp] how to rearrange index\n        how : str\n            Replacement is only necessary if indexer based on other_index.\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        if self.how in (how, \"outer\") and not isinstance(other_index, MultiIndex):\n            # if final index requires values in other_index but not target\n            # index, indexer may hold missing (-1) values, causing Index.take\n            # to take the final value in target index. So, we set the last\n            # element to be the desired fill value. We do not use allow_fill\n            # and fill_value because it throws a ValueError on integer indices\n            mask = indexer == -1\n            if np.any(mask):\n                fill_value = na_value_for_dtype(index.dtype, compat=False)\n                index = index.append(Index([fill_value]))\n        return index.take(indexer)\n\n    def _get_merge_keys(\n        self,\n    ) -> tuple[list[AnyArrayLike], list[AnyArrayLike], list[Hashable]]:\n        \"\"\"\n        Note: has side effects (copy/delete key columns)\n\n        Parameters\n        ----------\n        left\n        right\n        on\n\n        Returns\n        -------\n        left_keys, right_keys, join_names\n        \"\"\"\n        # left_keys, right_keys entries can actually be anything listlike\n        #  with a 'dtype' attr\n        left_keys: list[AnyArrayLike] = []\n        right_keys: list[AnyArrayLike] = []\n        join_names: list[Hashable] = []\n        right_drop: list[Hashable] = []\n        left_drop: list[Hashable] = []\n\n        left, right = self.left, self.right\n\n        is_lkey = lambda x: is_array_like(x) and len(x) == len(left)\n        is_rkey = lambda x: is_array_like(x) and len(x) == len(right)\n\n        # Note that pd.merge_asof() has separate 'on' and 'by' parameters. A\n        # user could, for example, request 'left_index' and 'left_by'. In a\n        # regular pd.merge(), users cannot specify both 'left_index' and\n        # 'left_on'. (Instead, users have a MultiIndex). That means the\n        # self.left_on in this function is always empty in a pd.merge(), but\n        # a pd.merge_asof(left_index=True, left_by=...) will result in a\n        # self.left_on array with a None in the middle of it. This requires\n        # a work-around as designated in the code below.\n        # See _validate_left_right_on() for where this happens.\n\n        # ugh, spaghetti re #733\n        if _any(self.left_on) and _any(self.right_on):\n            for lk, rk in zip(self.left_on, self.right_on):\n                if is_lkey(lk):\n                    lk = cast(AnyArrayLike, lk)\n                    left_keys.append(lk)\n                    if is_rkey(rk):\n                        rk = cast(AnyArrayLike, rk)\n                        right_keys.append(rk)\n                        join_names.append(None)  # what to do?\n                    else:\n                        # Then we're either Hashable or a wrong-length arraylike,\n                        #  the latter of which will raise\n                        rk = cast(Hashable, rk)\n                        if rk is not None:\n                            right_keys.append(right._get_label_or_level_values(rk))\n                            join_names.append(rk)\n                        else:\n                            # work-around for merge_asof(right_index=True)\n                            right_keys.append(right.index)\n                            join_names.append(right.index.name)\n                else:\n                    if not is_rkey(rk):\n                        # Then we're either Hashable or a wrong-length arraylike,\n                        #  the latter of which will raise\n                        rk = cast(Hashable, rk)\n                        if rk is not None:\n                            right_keys.append(right._get_label_or_level_values(rk))\n                        else:\n                            # work-around for merge_asof(right_index=True)\n                            right_keys.append(right.index)\n                        if lk is not None and lk == rk:  # FIXME: what about other NAs?\n                            # avoid key upcast in corner case (length-0)\n                            lk = cast(Hashable, lk)\n                            if len(left) > 0:\n                                right_drop.append(rk)\n                            else:\n                                left_drop.append(lk)\n                    else:\n                        rk = cast(AnyArrayLike, rk)\n                        right_keys.append(rk)\n                    if lk is not None:\n                        # Then we're either Hashable or a wrong-length arraylike,\n                        #  the latter of which will raise\n                        lk = cast(Hashable, lk)\n                        left_keys.append(left._get_label_or_level_values(lk))\n                        join_names.append(lk)\n                    else:\n                        # work-around for merge_asof(left_index=True)\n                        left_keys.append(left.index)\n                        join_names.append(left.index.name)\n        elif _any(self.left_on):\n            for k in self.left_on:\n                if is_lkey(k):\n                    k = cast(AnyArrayLike, k)\n                    left_keys.append(k)\n                    join_names.append(None)\n                else:\n                    # Then we're either Hashable or a wrong-length arraylike,\n                    #  the latter of which will raise\n                    k = cast(Hashable, k)\n                    left_keys.append(left._get_label_or_level_values(k))\n                    join_names.append(k)\n            if isinstance(self.right.index, MultiIndex):\n                right_keys = [\n                    lev._values.take(lev_codes)\n                    for lev, lev_codes in zip(\n                        self.right.index.levels, self.right.index.codes\n                    )\n                ]\n            else:\n                right_keys = [self.right.index._values]\n        elif _any(self.right_on):\n            for k in self.right_on:\n                if is_rkey(k):\n                    k = cast(AnyArrayLike, k)\n                    right_keys.append(k)\n                    join_names.append(None)\n                else:\n                    # Then we're either Hashable or a wrong-length arraylike,\n                    #  the latter of which will raise\n                    k = cast(Hashable, k)\n                    right_keys.append(right._get_label_or_level_values(k))\n                    join_names.append(k)\n            if isinstance(self.left.index, MultiIndex):\n                left_keys = [\n                    lev._values.take(lev_codes)\n                    for lev, lev_codes in zip(\n                        self.left.index.levels, self.left.index.codes\n                    )\n                ]\n            else:\n                left_keys = [self.left.index._values]\n\n        if left_drop:\n            self.left = self.left._drop_labels_or_levels(left_drop)\n\n        if right_drop:\n            self.right = self.right._drop_labels_or_levels(right_drop)\n\n        return left_keys, right_keys, join_names\n\n    def _maybe_coerce_merge_keys(self) -> None:\n        # we have valid merges but we may have to further\n        # coerce these if they are originally incompatible types\n        #\n        # for example if these are categorical, but are not dtype_equal\n        # or if we have object and integer dtypes\n\n        for lk, rk, name in zip(\n            self.left_join_keys, self.right_join_keys, self.join_names\n        ):\n            if (len(lk) and not len(rk)) or (not len(lk) and len(rk)):\n                continue\n\n            lk = extract_array(lk, extract_numpy=True)\n            rk = extract_array(rk, extract_numpy=True)\n\n            lk_is_cat = is_categorical_dtype(lk.dtype)\n            rk_is_cat = is_categorical_dtype(rk.dtype)\n            lk_is_object = is_object_dtype(lk.dtype)\n            rk_is_object = is_object_dtype(rk.dtype)\n\n            # if either left or right is a categorical\n            # then the must match exactly in categories & ordered\n            if lk_is_cat and rk_is_cat:\n                lk = cast(Categorical, lk)\n                rk = cast(Categorical, rk)\n                if lk._categories_match_up_to_permutation(rk):\n                    continue\n\n            elif lk_is_cat or rk_is_cat:\n                pass\n\n            elif is_dtype_equal(lk.dtype, rk.dtype):\n                continue\n\n            msg = (\n                f\"You are trying to merge on {lk.dtype} and \"\n                f\"{rk.dtype} columns. If you wish to proceed you should use pd.concat\"\n            )\n\n            # if we are numeric, then allow differing\n            # kinds to proceed, eg. int64 and int8, int and float\n            # further if we are object, but we infer to\n            # the same, then proceed\n            if is_numeric_dtype(lk.dtype) and is_numeric_dtype(rk.dtype):\n                if lk.dtype.kind == rk.dtype.kind:\n                    continue\n\n                # check whether ints and floats\n                if is_integer_dtype(rk.dtype) and is_float_dtype(lk.dtype):\n                    # GH 47391 numpy > 1.24 will raise a RuntimeError for nan -> int\n                    with np.errstate(invalid=\"ignore\"):\n                        # error: Argument 1 to \"astype\" of \"ndarray\" has incompatible\n                        # type \"Union[ExtensionDtype, Any, dtype[Any]]\"; expected\n                        # \"Union[dtype[Any], Type[Any], _SupportsDType[dtype[Any]]]\"\n                        casted = lk.astype(rk.dtype)  # type: ignore[arg-type]\n\n                        mask = ~np.isnan(lk)\n                        match = lk == casted\n                        # error: Item \"ExtensionArray\" of \"Union[ExtensionArray,\n                        # ndarray[Any, Any], Any]\" has no attribute \"all\"\n                        if not match[mask].all():  # type: ignore[union-attr]\n                            warnings.warn(\n                                \"You are merging on int and float \"\n                                \"columns where the float values \"\n                                \"are not equal to their int representation.\",\n                                UserWarning,\n                                stacklevel=find_stack_level(),\n                            )\n                    continue\n\n                if is_float_dtype(rk.dtype) and is_integer_dtype(lk.dtype):\n                    # GH 47391 numpy > 1.24 will raise a RuntimeError for nan -> int\n                    with np.errstate(invalid=\"ignore\"):\n                        # error: Argument 1 to \"astype\" of \"ndarray\" has incompatible\n                        # type \"Union[ExtensionDtype, Any, dtype[Any]]\"; expected\n                        # \"Union[dtype[Any], Type[Any], _SupportsDType[dtype[Any]]]\"\n                        casted = rk.astype(lk.dtype)  # type: ignore[arg-type]\n\n                        mask = ~np.isnan(rk)\n                        match = rk == casted\n                        # error: Item \"ExtensionArray\" of \"Union[ExtensionArray,\n                        # ndarray[Any, Any], Any]\" has no attribute \"all\"\n                        if not match[mask].all():  # type: ignore[union-attr]\n                            warnings.warn(\n                                \"You are merging on int and float \"\n                                \"columns where the float values \"\n                                \"are not equal to their int representation.\",\n                                UserWarning,\n                                stacklevel=find_stack_level(),\n                            )\n                    continue\n\n                # let's infer and see if we are ok\n                if lib.infer_dtype(lk, skipna=False) == lib.infer_dtype(\n                    rk, skipna=False\n                ):\n                    continue\n\n            # Check if we are trying to merge on obviously\n            # incompatible dtypes GH 9780, GH 15800\n\n            # bool values are coerced to object\n            elif (lk_is_object and is_bool_dtype(rk.dtype)) or (\n                is_bool_dtype(lk.dtype) and rk_is_object\n            ):\n                pass\n\n            # object values are allowed to be merged\n            elif (lk_is_object and is_numeric_dtype(rk.dtype)) or (\n                is_numeric_dtype(lk.dtype) and rk_is_object\n            ):\n                inferred_left = lib.infer_dtype(lk, skipna=False)\n                inferred_right = lib.infer_dtype(rk, skipna=False)\n                bool_types = [\"integer\", \"mixed-integer\", \"boolean\", \"empty\"]\n                string_types = [\"string\", \"unicode\", \"mixed\", \"bytes\", \"empty\"]\n\n                # inferred bool\n                if inferred_left in bool_types and inferred_right in bool_types:\n                    pass\n\n                # unless we are merging non-string-like with string-like\n                elif (\n                    inferred_left in string_types and inferred_right not in string_types\n                ) or (\n                    inferred_right in string_types and inferred_left not in string_types\n                ):\n                    raise ValueError(msg)\n\n            # datetimelikes must match exactly\n            elif needs_i8_conversion(lk.dtype) and not needs_i8_conversion(rk.dtype):\n                raise ValueError(msg)\n            elif not needs_i8_conversion(lk.dtype) and needs_i8_conversion(rk.dtype):\n                raise ValueError(msg)\n            elif isinstance(lk.dtype, DatetimeTZDtype) and not isinstance(\n                rk.dtype, DatetimeTZDtype\n            ):\n                raise ValueError(msg)\n            elif not isinstance(lk.dtype, DatetimeTZDtype) and isinstance(\n                rk.dtype, DatetimeTZDtype\n            ):\n                raise ValueError(msg)\n\n            elif lk_is_object and rk_is_object:\n                continue\n\n            # Houston, we have a problem!\n            # let's coerce to object if the dtypes aren't\n            # categorical, otherwise coerce to the category\n            # dtype. If we coerced categories to object,\n            # then we would lose type information on some\n            # columns, and end up trying to merge\n            # incompatible dtypes. See GH 16900.\n            if name in self.left.columns:\n                typ = cast(Categorical, lk).categories.dtype if lk_is_cat else object\n                self.left = self.left.copy()\n                self.left[name] = self.left[name].astype(typ)\n            if name in self.right.columns:\n                typ = cast(Categorical, rk).categories.dtype if rk_is_cat else object\n                self.right = self.right.copy()\n                self.right[name] = self.right[name].astype(typ)\n\n    def _create_cross_configuration(\n        self, left: DataFrame, right: DataFrame\n    ) -> tuple[DataFrame, DataFrame, str, str]:\n        \"\"\"\n        Creates the configuration to dispatch the cross operation to inner join,\n        e.g. adding a join column and resetting parameters. Join column is added\n        to a new object, no inplace modification\n\n        Parameters\n        ----------\n        left : DataFrame\n        right : DataFrame\n\n        Returns\n        -------\n            a tuple (left, right, how, cross_col) representing the adjusted\n            DataFrames with cross_col, the merge operation set to inner and the column\n            to join over.\n        \"\"\"\n        cross_col = f\"_cross_{uuid.uuid4()}\"\n        how = \"inner\"\n        return (\n            left.assign(**{cross_col: 1}),\n            right.assign(**{cross_col: 1}),\n            how,\n            cross_col,\n        )\n\n    def _validate_left_right_on(self, left_on, right_on):\n        left_on = com.maybe_make_list(left_on)\n        right_on = com.maybe_make_list(right_on)\n\n        if self.how == \"cross\":\n            if (\n                self.left_index\n                or self.right_index\n                or right_on is not None\n                or left_on is not None\n                or self.on is not None\n            ):\n                raise MergeError(\n                    \"Can not pass on, right_on, left_on or set right_index=True or \"\n                    \"left_index=True\"\n                )\n        # Hm, any way to make this logic less complicated??\n        elif self.on is None and left_on is None and right_on is None:\n\n            if self.left_index and self.right_index:\n                left_on, right_on = (), ()\n            elif self.left_index:\n                raise MergeError(\"Must pass right_on or right_index=True\")\n            elif self.right_index:\n                raise MergeError(\"Must pass left_on or left_index=True\")\n            else:\n                # use the common columns\n                left_cols = self.left.columns\n                right_cols = self.right.columns\n                common_cols = left_cols.intersection(right_cols)\n                if len(common_cols) == 0:\n                    raise MergeError(\n                        \"No common columns to perform merge on. \"\n                        f\"Merge options: left_on={left_on}, \"\n                        f\"right_on={right_on}, \"\n                        f\"left_index={self.left_index}, \"\n                        f\"right_index={self.right_index}\"\n                    )\n                if (\n                    not left_cols.join(common_cols, how=\"inner\").is_unique\n                    or not right_cols.join(common_cols, how=\"inner\").is_unique\n                ):\n                    raise MergeError(f\"Data columns not unique: {repr(common_cols)}\")\n                left_on = right_on = common_cols\n        elif self.on is not None:\n            if left_on is not None or right_on is not None:\n                raise MergeError(\n                    'Can only pass argument \"on\" OR \"left_on\" '\n                    'and \"right_on\", not a combination of both.'\n                )\n            if self.left_index or self.right_index:\n                raise MergeError(\n                    'Can only pass argument \"on\" OR \"left_index\" '\n                    'and \"right_index\", not a combination of both.'\n                )\n            left_on = right_on = self.on\n        elif left_on is not None:\n            if self.left_index:\n                raise MergeError(\n                    'Can only pass argument \"left_on\" OR \"left_index\" not both.'\n                )\n            if not self.right_index and right_on is None:\n                raise MergeError('Must pass \"right_on\" OR \"right_index\".')\n            n = len(left_on)\n            if self.right_index:\n                if len(left_on) != self.right.index.nlevels:\n                    raise ValueError(\n                        \"len(left_on) must equal the number \"\n                        'of levels in the index of \"right\"'\n                    )\n                right_on = [None] * n\n        elif right_on is not None:\n            if self.right_index:\n                raise MergeError(\n                    'Can only pass argument \"right_on\" OR \"right_index\" not both.'\n                )\n            if not self.left_index and left_on is None:\n                raise MergeError('Must pass \"left_on\" OR \"left_index\".')\n            n = len(right_on)\n            if self.left_index:\n                if len(right_on) != self.left.index.nlevels:\n                    raise ValueError(\n                        \"len(right_on) must equal the number \"\n                        'of levels in the index of \"left\"'\n                    )\n                left_on = [None] * n\n        if self.how != \"cross\" and len(right_on) != len(left_on):\n            raise ValueError(\"len(right_on) must equal len(left_on)\")\n\n        return left_on, right_on\n\n    def _validate(self, validate: str) -> None:\n\n        # Check uniqueness of each\n        if self.left_index:\n            left_unique = self.orig_left.index.is_unique\n        else:\n            left_unique = MultiIndex.from_arrays(self.left_join_keys).is_unique\n\n        if self.right_index:\n            right_unique = self.orig_right.index.is_unique\n        else:\n            right_unique = MultiIndex.from_arrays(self.right_join_keys).is_unique\n\n        # Check data integrity\n        if validate in [\"one_to_one\", \"1:1\"]:\n            if not left_unique and not right_unique:\n                raise MergeError(\n                    \"Merge keys are not unique in either left \"\n                    \"or right dataset; not a one-to-one merge\"\n                )\n            if not left_unique:\n                raise MergeError(\n                    \"Merge keys are not unique in left dataset; not a one-to-one merge\"\n                )\n            if not right_unique:\n                raise MergeError(\n                    \"Merge keys are not unique in right dataset; not a one-to-one merge\"\n                )\n\n        elif validate in [\"one_to_many\", \"1:m\"]:\n            if not left_unique:\n                raise MergeError(\n                    \"Merge keys are not unique in left dataset; not a one-to-many merge\"\n                )\n\n        elif validate in [\"many_to_one\", \"m:1\"]:\n            if not right_unique:\n                raise MergeError(\n                    \"Merge keys are not unique in right dataset; \"\n                    \"not a many-to-one merge\"\n                )\n\n        elif validate in [\"many_to_many\", \"m:m\"]:\n            pass\n\n        else:\n            raise ValueError(\n                f'\"{validate}\" is not a valid argument. '\n                \"Valid arguments are:\\n\"\n                '- \"1:1\"\\n'\n                '- \"1:m\"\\n'\n                '- \"m:1\"\\n'\n                '- \"m:m\"\\n'\n                '- \"one_to_one\"\\n'\n                '- \"one_to_many\"\\n'\n                '- \"many_to_one\"\\n'\n                '- \"many_to_many\"'\n            )\n\n\ndef get_join_indexers(\n    left_keys, right_keys, sort: bool = False, how: str = \"inner\", **kwargs\n) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n    \"\"\"\n\n    Parameters\n    ----------\n    left_keys : ndarray, Index, Series\n    right_keys : ndarray, Index, Series\n    sort : bool, default False\n    how : {'inner', 'outer', 'left', 'right'}, default 'inner'\n\n    Returns\n    -------\n    np.ndarray[np.intp]\n        Indexer into the left_keys.\n    np.ndarray[np.intp]\n        Indexer into the right_keys.\n    \"\"\"\n    assert len(left_keys) == len(\n        right_keys\n    ), \"left_key and right_keys must be the same length\"\n\n    # fast-path for empty left/right\n    left_n = len(left_keys[0])\n    right_n = len(right_keys[0])\n    if left_n == 0:\n        if how in [\"left\", \"inner\", \"cross\"]:\n            return _get_empty_indexer()\n        elif not sort and how in [\"right\", \"outer\"]:\n            return _get_no_sort_one_missing_indexer(right_n, True)\n    elif right_n == 0:\n        if how in [\"right\", \"inner\", \"cross\"]:\n            return _get_empty_indexer()\n        elif not sort and how in [\"left\", \"outer\"]:\n            return _get_no_sort_one_missing_indexer(left_n, False)\n\n    # get left & right join labels and num. of levels at each location\n    mapped = (\n        _factorize_keys(left_keys[n], right_keys[n], sort=sort, how=how)\n        for n in range(len(left_keys))\n    )\n    zipped = zip(*mapped)\n    llab, rlab, shape = (list(x) for x in zipped)\n\n    # get flat i8 keys from label lists\n    lkey, rkey = _get_join_keys(llab, rlab, tuple(shape), sort)\n\n    # factorize keys to a dense i8 space\n    # `count` is the num. of unique keys\n    # set(lkey) | set(rkey) == range(count)\n\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)\n    # preserve left frame order if how == 'left' and sort == False\n    kwargs = stdlib_copy.copy(kwargs)\n    if how in (\"left\", \"right\"):\n        kwargs[\"sort\"] = sort\n    join_func = {\n        \"inner\": libjoin.inner_join,\n        \"left\": libjoin.left_outer_join,\n        \"right\": lambda x, y, count, **kwargs: libjoin.left_outer_join(\n            y, x, count, **kwargs\n        )[::-1],\n        \"outer\": libjoin.full_outer_join,\n    }[how]\n\n    # error: Cannot call function of unknown type\n    return join_func(lkey, rkey, count, **kwargs)  # type: ignore[operator]\n\n\ndef restore_dropped_levels_multijoin(\n    left: MultiIndex,\n    right: MultiIndex,\n    dropped_level_names,\n    join_index: Index,\n    lindexer: npt.NDArray[np.intp],\n    rindexer: npt.NDArray[np.intp],\n) -> tuple[list[Index], npt.NDArray[np.intp], list[Hashable]]:\n    \"\"\"\n    *this is an internal non-public method*\n\n    Returns the levels, labels and names of a multi-index to multi-index join.\n    Depending on the type of join, this method restores the appropriate\n    dropped levels of the joined multi-index.\n    The method relies on lindexer, rindexer which hold the index positions of\n    left and right, where a join was feasible\n\n    Parameters\n    ----------\n    left : MultiIndex\n        left index\n    right : MultiIndex\n        right index\n    dropped_level_names : str array\n        list of non-common level names\n    join_index : Index\n        the index of the join between the\n        common levels of left and right\n    lindexer : np.ndarray[np.intp]\n        left indexer\n    rindexer : np.ndarray[np.intp]\n        right indexer\n\n    Returns\n    -------\n    levels : list of Index\n        levels of combined multiindexes\n    labels : np.ndarray[np.intp]\n        labels of combined multiindexes\n    names : List[Hashable]\n        names of combined multiindex levels\n\n    \"\"\"\n\n    def _convert_to_multiindex(index: Index) -> MultiIndex:\n        if isinstance(index, MultiIndex):\n            return index\n        else:\n            return MultiIndex.from_arrays([index._values], names=[index.name])\n\n    # For multi-multi joins with one overlapping level,\n    # the returned index if of type Index\n    # Assure that join_index is of type MultiIndex\n    # so that dropped levels can be appended\n    join_index = _convert_to_multiindex(join_index)\n\n    join_levels = join_index.levels\n    join_codes = join_index.codes\n    join_names = join_index.names\n\n    # Iterate through the levels that must be restored\n    for dropped_level_name in dropped_level_names:\n        if dropped_level_name in left.names:\n            idx = left\n            indexer = lindexer\n        else:\n            idx = right\n            indexer = rindexer\n\n        # The index of the level name to be restored\n        name_idx = idx.names.index(dropped_level_name)\n\n        restore_levels = idx.levels[name_idx]\n        # Inject -1 in the codes list where a join was not possible\n        # IOW indexer[i]=-1\n        codes = idx.codes[name_idx]\n        if indexer is None:\n            restore_codes = codes\n        else:\n            restore_codes = algos.take_nd(codes, indexer, fill_value=-1)\n\n        # error: Cannot determine type of \"__add__\"\n        join_levels = join_levels + [restore_levels]  # type: ignore[has-type]\n        join_codes = join_codes + [restore_codes]\n        join_names = join_names + [dropped_level_name]\n\n    return join_levels, join_codes, join_names\n\n\nclass _OrderedMerge(_MergeOperation):\n    _merge_type = \"ordered_merge\"\n\n    def __init__(\n        self,\n        left: DataFrame | Series,\n        right: DataFrame | Series,\n        on: IndexLabel | None = None,\n        left_on: IndexLabel | None = None,\n        right_on: IndexLabel | None = None,\n        left_index: bool = False,\n        right_index: bool = False,\n        axis: AxisInt = 1,\n        suffixes: Suffixes = (\"_x\", \"_y\"),\n        fill_method: str | None = None,\n        how: str = \"outer\",\n    ) -> None:\n\n        self.fill_method = fill_method\n        _MergeOperation.__init__(\n            self,\n            left,\n            right,\n            on=on,\n            left_on=left_on,\n            left_index=left_index,\n            right_index=right_index,\n            right_on=right_on,\n            axis=axis,\n            how=how,\n            suffixes=suffixes,\n            sort=True,  # factorize sorts\n        )\n\n    def get_result(self, copy: bool = True) -> DataFrame:\n        join_index, left_indexer, right_indexer = self._get_join_info()\n\n        llabels, rlabels = _items_overlap_with_suffix(\n            self.left._info_axis, self.right._info_axis, self.suffixes\n        )\n\n        left_join_indexer: np.ndarray | None\n        right_join_indexer: np.ndarray | None\n\n        if self.fill_method == \"ffill\":\n            if left_indexer is None:\n                raise TypeError(\"left_indexer cannot be None\")\n            left_indexer, right_indexer = cast(np.ndarray, left_indexer), cast(\n                np.ndarray, right_indexer\n            )\n            left_join_indexer = libjoin.ffill_indexer(left_indexer)\n            right_join_indexer = libjoin.ffill_indexer(right_indexer)\n        else:\n            left_join_indexer = left_indexer\n            right_join_indexer = right_indexer\n\n        result = self._reindex_and_concat(\n            join_index, left_join_indexer, right_join_indexer, copy=copy\n        )\n        self._maybe_add_join_keys(result, left_indexer, right_indexer)\n\n        return result\n\n\ndef _asof_by_function(direction: str):\n    name = f\"asof_join_{direction}_on_X_by_Y\"\n    return getattr(libjoin, name, None)\n\n\n_type_casters = {\n    \"int64_t\": ensure_int64,\n    \"double\": ensure_float64,\n    \"object\": ensure_object,\n}\n\n\ndef _get_cython_type_upcast(dtype: DtypeObj) -> str:\n    \"\"\"Upcast a dtype to 'int64_t', 'double', or 'object'\"\"\"\n    if is_integer_dtype(dtype):\n        return \"int64_t\"\n    elif is_float_dtype(dtype):\n        return \"double\"\n    else:\n        return \"object\"\n\n\nclass _AsOfMerge(_OrderedMerge):\n    _merge_type = \"asof_merge\"\n\n    def __init__(\n        self,\n        left: DataFrame | Series,\n        right: DataFrame | Series,\n        on: IndexLabel | None = None,\n        left_on: IndexLabel | None = None,\n        right_on: IndexLabel | None = None,\n        left_index: bool = False,\n        right_index: bool = False,\n        by=None,\n        left_by=None,\n        right_by=None,\n        axis: AxisInt = 1,\n        suffixes: Suffixes = (\"_x\", \"_y\"),\n        copy: bool = True,\n        fill_method: str | None = None,\n        how: str = \"asof\",\n        tolerance=None,\n        allow_exact_matches: bool = True,\n        direction: str = \"backward\",\n    ) -> None:\n\n        self.by = by\n        self.left_by = left_by\n        self.right_by = right_by\n        self.tolerance = tolerance\n        self.allow_exact_matches = allow_exact_matches\n        self.direction = direction\n\n        _OrderedMerge.__init__(\n            self,\n            left,\n            right,\n            on=on,\n            left_on=left_on,\n            right_on=right_on,\n            left_index=left_index,\n            right_index=right_index,\n            axis=axis,\n            how=how,\n            suffixes=suffixes,\n            fill_method=fill_method,\n        )\n\n    def _validate_left_right_on(self, left_on, right_on):\n        left_on, right_on = super()._validate_left_right_on(left_on, right_on)\n\n        # we only allow on to be a single item for on\n        if len(left_on) != 1 and not self.left_index:\n            raise MergeError(\"can only asof on a key for left\")\n\n        if len(right_on) != 1 and not self.right_index:\n            raise MergeError(\"can only asof on a key for right\")\n\n        if self.left_index and isinstance(self.left.index, MultiIndex):\n            raise MergeError(\"left can only have one index\")\n\n        if self.right_index and isinstance(self.right.index, MultiIndex):\n            raise MergeError(\"right can only have one index\")\n\n        # set 'by' columns\n        if self.by is not None:\n            if self.left_by is not None or self.right_by is not None:\n                raise MergeError(\"Can only pass by OR left_by and right_by\")\n            self.left_by = self.right_by = self.by\n        if self.left_by is None and self.right_by is not None:\n            raise MergeError(\"missing left_by\")\n        if self.left_by is not None and self.right_by is None:\n            raise MergeError(\"missing right_by\")\n\n        # GH#29130 Check that merge keys do not have dtype object\n        if not self.left_index:\n            left_on_0 = left_on[0]\n            if is_array_like(left_on_0):\n                lo_dtype = left_on_0.dtype\n            else:\n                lo_dtype = (\n                    self.left[left_on_0].dtype\n                    if left_on_0 in self.left.columns\n                    else self.left.index.get_level_values(left_on_0)\n                )\n        else:\n            lo_dtype = self.left.index.dtype\n\n        if not self.right_index:\n            right_on_0 = right_on[0]\n            if is_array_like(right_on_0):\n                ro_dtype = right_on_0.dtype\n            else:\n                ro_dtype = (\n                    self.right[right_on_0].dtype\n                    if right_on_0 in self.right.columns\n                    else self.right.index.get_level_values(right_on_0)\n                )\n        else:\n            ro_dtype = self.right.index.dtype\n\n        if is_object_dtype(lo_dtype) or is_object_dtype(ro_dtype):\n            raise MergeError(\n                f\"Incompatible merge dtype, {repr(ro_dtype)} and \"\n                f\"{repr(lo_dtype)}, both sides must have numeric dtype\"\n            )\n\n        # add 'by' to our key-list so we can have it in the\n        # output as a key\n        if self.left_by is not None:\n            if not is_list_like(self.left_by):\n                self.left_by = [self.left_by]\n            if not is_list_like(self.right_by):\n                self.right_by = [self.right_by]\n\n            if len(self.left_by) != len(self.right_by):\n                raise MergeError(\"left_by and right_by must be same length\")\n\n            left_on = self.left_by + list(left_on)\n            right_on = self.right_by + list(right_on)\n\n        # check 'direction' is valid\n        if self.direction not in [\"backward\", \"forward\", \"nearest\"]:\n            raise MergeError(f\"direction invalid: {self.direction}\")\n\n        return left_on, right_on\n\n    def _get_merge_keys(\n        self,\n    ) -> tuple[list[AnyArrayLike], list[AnyArrayLike], list[Hashable]]:\n\n        # note this function has side effects\n        (left_join_keys, right_join_keys, join_names) = super()._get_merge_keys()\n\n        # validate index types are the same\n        for i, (lk, rk) in enumerate(zip(left_join_keys, right_join_keys)):\n            if not is_dtype_equal(lk.dtype, rk.dtype):\n                if is_categorical_dtype(lk.dtype) and is_categorical_dtype(rk.dtype):\n                    # The generic error message is confusing for categoricals.\n                    #\n                    # In this function, the join keys include both the original\n                    # ones of the merge_asof() call, and also the keys passed\n                    # to its by= argument. Unordered but equal categories\n                    # are not supported for the former, but will fail\n                    # later with a ValueError, so we don't *need* to check\n                    # for them here.\n                    msg = (\n                        f\"incompatible merge keys [{i}] {repr(lk.dtype)} and \"\n                        f\"{repr(rk.dtype)}, both sides category, but not equal ones\"\n                    )\n                else:\n                    msg = (\n                        f\"incompatible merge keys [{i}] {repr(lk.dtype)} and \"\n                        f\"{repr(rk.dtype)}, must be the same type\"\n                    )\n                raise MergeError(msg)\n\n        # validate tolerance; datetime.timedelta or Timedelta if we have a DTI\n        if self.tolerance is not None:\n\n            if self.left_index:\n                # Actually more specifically an Index\n                lt = cast(AnyArrayLike, self.left.index)\n            else:\n                lt = left_join_keys[-1]\n\n            msg = (\n                f\"incompatible tolerance {self.tolerance}, must be compat \"\n                f\"with type {repr(lt.dtype)}\"\n            )\n\n            if needs_i8_conversion(lt):\n                if not isinstance(self.tolerance, datetime.timedelta):\n                    raise MergeError(msg)\n                if self.tolerance < Timedelta(0):\n                    raise MergeError(\"tolerance must be positive\")\n\n            elif is_integer_dtype(lt):\n                if not is_integer(self.tolerance):\n                    raise MergeError(msg)\n                if self.tolerance < 0:\n                    raise MergeError(\"tolerance must be positive\")\n\n            elif is_float_dtype(lt):\n                if not is_number(self.tolerance):\n                    raise MergeError(msg)\n                if self.tolerance < 0:\n                    raise MergeError(\"tolerance must be positive\")\n\n            else:\n                raise MergeError(\"key must be integer, timestamp or float\")\n\n        # validate allow_exact_matches\n        if not is_bool(self.allow_exact_matches):\n            msg = (\n                \"allow_exact_matches must be boolean, \"\n                f\"passed {self.allow_exact_matches}\"\n            )\n            raise MergeError(msg)\n\n        return left_join_keys, right_join_keys, join_names\n\n    def _get_join_indexers(self) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n        \"\"\"return the join indexers\"\"\"\n\n        def flip(xs) -> np.ndarray:\n            \"\"\"unlike np.transpose, this returns an array of tuples\"\"\"\n\n            def injection(obj):\n                if not is_extension_array_dtype(obj):\n                    # ndarray\n                    return obj\n                obj = extract_array(obj)\n                if isinstance(obj, NDArrayBackedExtensionArray):\n                    # fastpath for e.g. dt64tz, categorical\n                    return obj._ndarray\n                # FIXME: returning obj._values_for_argsort() here doesn't\n                #  break in any existing test cases, but i (@jbrockmendel)\n                #  am pretty sure it should!\n                #  e.g.\n                #  arr = pd.array([0, pd.NA, 255], dtype=\"UInt8\")\n                #  will have values_for_argsort (before GH#45434)\n                #  np.array([0, 255, 255], dtype=np.uint8)\n                #  and the non-injectivity should make a difference somehow\n                #  shouldn't it?\n                return np.asarray(obj)\n\n            xs = [injection(x) for x in xs]\n            labels = list(string.ascii_lowercase[: len(xs)])\n            dtypes = [x.dtype for x in xs]\n            labeled_dtypes = list(zip(labels, dtypes))\n            return np.array(list(zip(*xs)), labeled_dtypes)\n\n        # values to compare\n        left_values = (\n            self.left.index._values if self.left_index else self.left_join_keys[-1]\n        )\n        right_values = (\n            self.right.index._values if self.right_index else self.right_join_keys[-1]\n        )\n        tolerance = self.tolerance\n\n        # we require sortedness and non-null values in the join keys\n        if not Index(left_values).is_monotonic_increasing:\n            side = \"left\"\n            if isna(left_values).any():\n                raise ValueError(f\"Merge keys contain null values on {side} side\")\n            raise ValueError(f\"{side} keys must be sorted\")\n\n        if not Index(right_values).is_monotonic_increasing:\n            side = \"right\"\n            if isna(right_values).any():\n                raise ValueError(f\"Merge keys contain null values on {side} side\")\n            raise ValueError(f\"{side} keys must be sorted\")\n\n        # initial type conversion as needed\n        if needs_i8_conversion(left_values):\n            left_values = left_values.view(\"i8\")\n            right_values = right_values.view(\"i8\")\n            if tolerance is not None:\n                tolerance = Timedelta(tolerance)\n                tolerance = tolerance.value\n\n        # a \"by\" parameter requires special handling\n        if self.left_by is not None:\n            # remove 'on' parameter from values if one existed\n            if self.left_index and self.right_index:\n                left_by_values = self.left_join_keys\n                right_by_values = self.right_join_keys\n            else:\n                left_by_values = self.left_join_keys[0:-1]\n                right_by_values = self.right_join_keys[0:-1]\n\n            # get tuple representation of values if more than one\n            if len(left_by_values) == 1:\n                lbv = left_by_values[0]\n                rbv = right_by_values[0]\n            else:\n                # We get here with non-ndarrays in test_merge_by_col_tz_aware\n                #  and test_merge_groupby_multiple_column_with_categorical_column\n                lbv = flip(left_by_values)\n                rbv = flip(right_by_values)\n\n            # upcast 'by' parameter because HashTable is limited\n            by_type = _get_cython_type_upcast(lbv.dtype)\n            by_type_caster = _type_casters[by_type]\n            # error: Cannot call function of unknown type\n            left_by_values = by_type_caster(lbv)  # type: ignore[operator]\n            # error: Cannot call function of unknown type\n            right_by_values = by_type_caster(rbv)  # type: ignore[operator]\n\n            # choose appropriate function by type\n            func = _asof_by_function(self.direction)\n            return func(\n                left_values,\n                right_values,\n                left_by_values,\n                right_by_values,\n                self.allow_exact_matches,\n                tolerance,\n            )\n        else:\n            # choose appropriate function by type\n            func = _asof_by_function(self.direction)\n            return func(\n                left_values,\n                right_values,\n                None,\n                None,\n                self.allow_exact_matches,\n                tolerance,\n                False,\n            )\n\n\ndef _get_multiindex_indexer(\n    join_keys, index: MultiIndex, sort: bool\n) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n\n    # left & right join labels and num. of levels at each location\n    mapped = (\n        _factorize_keys(index.levels[n], join_keys[n], sort=sort)\n        for n in range(index.nlevels)\n    )\n    zipped = zip(*mapped)\n    rcodes, lcodes, shape = (list(x) for x in zipped)\n    if sort:\n        rcodes = list(map(np.take, rcodes, index.codes))\n    else:\n        i8copy = lambda a: a.astype(\"i8\", subok=False, copy=True)\n        rcodes = list(map(i8copy, index.codes))\n\n    # fix right labels if there were any nulls\n    for i, join_key in enumerate(join_keys):\n        mask = index.codes[i] == -1\n        if mask.any():\n            # check if there already was any nulls at this location\n            # if there was, it is factorized to `shape[i] - 1`\n            a = join_key[lcodes[i] == shape[i] - 1]\n            if a.size == 0 or not a[0] != a[0]:\n                shape[i] += 1\n\n            rcodes[i][mask] = shape[i] - 1\n\n    # get flat i8 join keys\n    lkey, rkey = _get_join_keys(lcodes, rcodes, tuple(shape), sort)\n\n    # factorize keys to a dense i8 space\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)\n\n    return libjoin.left_outer_join(lkey, rkey, count, sort=sort)\n\n\ndef _get_single_indexer(\n    join_key, index: Index, sort: bool = False\n) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n    left_key, right_key, count = _factorize_keys(join_key, index._values, sort=sort)\n\n    return libjoin.left_outer_join(left_key, right_key, count, sort=sort)\n\n\ndef _get_empty_indexer() -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n    \"\"\"Return empty join indexers.\"\"\"\n    return (\n        np.array([], dtype=np.intp),\n        np.array([], dtype=np.intp),\n    )\n\n\ndef _get_no_sort_one_missing_indexer(\n    n: int, left_missing: bool\n) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n    \"\"\"\n    Return join indexers where all of one side is selected without sorting\n    and none of the other side is selected.\n\n    Parameters\n    ----------\n    n : int\n        Length of indexers to create.\n    left_missing : bool\n        If True, the left indexer will contain only -1's.\n        If False, the right indexer will contain only -1's.\n\n    Returns\n    -------\n    np.ndarray[np.intp]\n        Left indexer\n    np.ndarray[np.intp]\n        Right indexer\n    \"\"\"\n    idx = np.arange(n, dtype=np.intp)\n    idx_missing = np.full(shape=n, fill_value=-1, dtype=np.intp)\n    if left_missing:\n        return idx_missing, idx\n    return idx, idx_missing\n\n\ndef _left_join_on_index(\n    left_ax: Index, right_ax: Index, join_keys, sort: bool = False\n) -> tuple[Index, npt.NDArray[np.intp] | None, npt.NDArray[np.intp]]:\n    if len(join_keys) > 1:\n        if not (\n            isinstance(right_ax, MultiIndex) and len(join_keys) == right_ax.nlevels\n        ):\n            raise AssertionError(\n                \"If more than one join key is given then \"\n                \"'right_ax' must be a MultiIndex and the \"\n                \"number of join keys must be the number of levels in right_ax\"\n            )\n\n        left_indexer, right_indexer = _get_multiindex_indexer(\n            join_keys, right_ax, sort=sort\n        )\n    else:\n        jkey = join_keys[0]\n\n        left_indexer, right_indexer = _get_single_indexer(jkey, right_ax, sort=sort)\n\n    if sort or len(left_ax) != len(left_indexer):\n        # if asked to sort or there are 1-to-many matches\n        join_index = left_ax.take(left_indexer)\n        return join_index, left_indexer, right_indexer\n\n    # left frame preserves order & length of its index\n    return left_ax, None, right_indexer\n\n\ndef _factorize_keys(\n    lk: ArrayLike, rk: ArrayLike, sort: bool = True, how: str = \"inner\"\n) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp], int]:\n    \"\"\"\n    Encode left and right keys as enumerated types.\n\n    This is used to get the join indexers to be used when merging DataFrames.\n\n    Parameters\n    ----------\n    lk : array-like\n        Left key.\n    rk : array-like\n        Right key.\n    sort : bool, defaults to True\n        If True, the encoding is done such that the unique elements in the\n        keys are sorted.\n    how : {â€˜leftâ€™, â€˜rightâ€™, â€˜outerâ€™, â€˜innerâ€™}, default â€˜innerâ€™\n        Type of merge.\n\n    Returns\n    -------\n    np.ndarray[np.intp]\n        Left (resp. right if called with `key='right'`) labels, as enumerated type.\n    np.ndarray[np.intp]\n        Right (resp. left if called with `key='right'`) labels, as enumerated type.\n    int\n        Number of unique elements in union of left and right labels.\n\n    See Also\n    --------\n    merge : Merge DataFrame or named Series objects\n        with a database-style join.\n    algorithms.factorize : Encode the object as an enumerated type\n        or categorical variable.\n\n    Examples\n    --------\n    >>> lk = np.array([\"a\", \"c\", \"b\"])\n    >>> rk = np.array([\"a\", \"c\"])\n\n    Here, the unique values are `'a', 'b', 'c'`. With the default\n    `sort=True`, the encoding will be `{0: 'a', 1: 'b', 2: 'c'}`:\n\n    >>> pd.core.reshape.merge._factorize_keys(lk, rk)\n    (array([0, 2, 1]), array([0, 2]), 3)\n\n    With the `sort=False`, the encoding will correspond to the order\n    in which the unique elements first appear: `{0: 'a', 1: 'c', 2: 'b'}`:\n\n    >>> pd.core.reshape.merge._factorize_keys(lk, rk, sort=False)\n    (array([0, 1, 2]), array([0, 1]), 3)\n    \"\"\"\n    # Some pre-processing for non-ndarray lk / rk\n    lk = extract_array(lk, extract_numpy=True, extract_range=True)\n    rk = extract_array(rk, extract_numpy=True, extract_range=True)\n    # TODO: if either is a RangeIndex, we can likely factorize more efficiently?\n\n    if isinstance(lk.dtype, DatetimeTZDtype) and isinstance(rk.dtype, DatetimeTZDtype):\n        # Extract the ndarray (UTC-localized) values\n        # Note: we dont need the dtypes to match, as these can still be compared\n        # TODO(non-nano): need to make sure resolutions match\n        lk = cast(\"DatetimeArray\", lk)._ndarray\n        rk = cast(\"DatetimeArray\", rk)._ndarray\n\n    elif (\n        is_categorical_dtype(lk.dtype)\n        and is_categorical_dtype(rk.dtype)\n        and is_dtype_equal(lk.dtype, rk.dtype)\n    ):\n        assert isinstance(lk, Categorical)\n        assert isinstance(rk, Categorical)\n        # Cast rk to encoding so we can compare codes with lk\n\n        rk = lk._encode_with_my_categories(rk)\n\n        lk = ensure_int64(lk.codes)\n        rk = ensure_int64(rk.codes)\n\n    elif isinstance(lk, ExtensionArray) and is_dtype_equal(lk.dtype, rk.dtype):\n        lk, _ = lk._values_for_factorize()\n\n        # error: Item \"ndarray\" of \"Union[Any, ndarray]\" has no attribute\n        # \"_values_for_factorize\"\n        rk, _ = rk._values_for_factorize()  # type: ignore[union-attr]\n\n    klass: type[libhashtable.Factorizer] | type[libhashtable.Int64Factorizer]\n    if is_integer_dtype(lk.dtype) and is_integer_dtype(rk.dtype):\n        # GH#23917 TODO: needs tests for case where lk is integer-dtype\n        #  and rk is datetime-dtype\n        klass = libhashtable.Int64Factorizer\n        lk = ensure_int64(np.asarray(lk))\n        rk = ensure_int64(np.asarray(rk))\n\n    elif needs_i8_conversion(lk.dtype) and is_dtype_equal(lk.dtype, rk.dtype):\n        # GH#23917 TODO: Needs tests for non-matching dtypes\n        klass = libhashtable.Int64Factorizer\n        lk = ensure_int64(np.asarray(lk, dtype=np.int64))\n        rk = ensure_int64(np.asarray(rk, dtype=np.int64))\n\n    else:\n        klass = libhashtable.ObjectFactorizer\n        lk = ensure_object(lk)\n        rk = ensure_object(rk)\n\n    rizer = klass(max(len(lk), len(rk)))\n\n    # Argument 1 to \"factorize\" of \"ObjectFactorizer\" has incompatible type\n    # \"Union[ndarray[Any, dtype[signedinteger[_64Bit]]],\n    # ndarray[Any, dtype[object_]]]\"; expected \"ndarray[Any, dtype[object_]]\"\n    llab = rizer.factorize(lk)  # type: ignore[arg-type]\n    # Argument 1 to \"factorize\" of \"ObjectFactorizer\" has incompatible type\n    # \"Union[ndarray[Any, dtype[signedinteger[_64Bit]]],\n    # ndarray[Any, dtype[object_]]]\"; expected \"ndarray[Any, dtype[object_]]\"\n    rlab = rizer.factorize(rk)  # type: ignore[arg-type]\n    assert llab.dtype == np.dtype(np.intp), llab.dtype\n    assert rlab.dtype == np.dtype(np.intp), rlab.dtype\n\n    count = rizer.get_count()\n\n    if sort:\n        uniques = rizer.uniques.to_array()\n        llab, rlab = _sort_labels(uniques, llab, rlab)\n\n    # NA group\n    lmask = llab == -1\n    lany = lmask.any()\n    rmask = rlab == -1\n    rany = rmask.any()\n\n    if lany or rany:\n        if lany:\n            np.putmask(llab, lmask, count)\n        if rany:\n            np.putmask(rlab, rmask, count)\n        count += 1\n\n    if how == \"right\":\n        return rlab, llab, count\n    return llab, rlab, count\n\n\ndef _sort_labels(\n    uniques: np.ndarray, left: npt.NDArray[np.intp], right: npt.NDArray[np.intp]\n) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n\n    llength = len(left)\n    labels = np.concatenate([left, right])\n\n    _, new_labels = algos.safe_sort(uniques, labels, use_na_sentinel=True)\n    new_left, new_right = new_labels[:llength], new_labels[llength:]\n\n    return new_left, new_right\n\n\ndef _get_join_keys(\n    llab: list[npt.NDArray[np.int64 | np.intp]],\n    rlab: list[npt.NDArray[np.int64 | np.intp]],\n    shape: Shape,\n    sort: bool,\n) -> tuple[npt.NDArray[np.int64], npt.NDArray[np.int64]]:\n\n    # how many levels can be done without overflow\n    nlev = next(\n        lev\n        for lev in range(len(shape), 0, -1)\n        if not is_int64_overflow_possible(shape[:lev])\n    )\n\n    # get keys for the first `nlev` levels\n    stride = np.prod(shape[1:nlev], dtype=\"i8\")\n    lkey = stride * llab[0].astype(\"i8\", subok=False, copy=False)\n    rkey = stride * rlab[0].astype(\"i8\", subok=False, copy=False)\n\n    for i in range(1, nlev):\n        with np.errstate(divide=\"ignore\"):\n            stride //= shape[i]\n        lkey += llab[i] * stride\n        rkey += rlab[i] * stride\n\n    if nlev == len(shape):  # all done!\n        return lkey, rkey\n\n    # densify current keys to avoid overflow\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)\n\n    llab = [lkey] + llab[nlev:]\n    rlab = [rkey] + rlab[nlev:]\n    shape = (count,) + shape[nlev:]\n\n    return _get_join_keys(llab, rlab, shape, sort)\n\n\ndef _should_fill(lname, rname) -> bool:\n    if not isinstance(lname, str) or not isinstance(rname, str):\n        return True\n    return lname == rname\n\n\ndef _any(x) -> bool:\n    return x is not None and com.any_not_none(*x)\n\n\ndef _validate_operand(obj: DataFrame | Series) -> DataFrame:\n    if isinstance(obj, ABCDataFrame):\n        return obj\n    elif isinstance(obj, ABCSeries):\n        if obj.name is None:\n            raise ValueError(\"Cannot merge a Series without a name\")\n        return obj.to_frame()\n    else:\n        raise TypeError(\n            f\"Can only merge Series or DataFrame objects, a {type(obj)} was passed\"\n        )\n\n\ndef _items_overlap_with_suffix(\n    left: Index, right: Index, suffixes: Suffixes\n) -> tuple[Index, Index]:\n    \"\"\"\n    Suffixes type validation.\n\n    If two indices overlap, add suffixes to overlapping entries.\n\n    If corresponding suffix is empty, the entry is simply converted to string.\n\n    \"\"\"\n    if not is_list_like(suffixes, allow_sets=False) or isinstance(suffixes, dict):\n        raise TypeError(\n            f\"Passing 'suffixes' as a {type(suffixes)}, is not supported. \"\n            \"Provide 'suffixes' as a tuple instead.\"\n        )\n\n    to_rename = left.intersection(right)\n    if len(to_rename) == 0:\n        return left, right\n\n    lsuffix, rsuffix = suffixes\n\n    if not lsuffix and not rsuffix:\n        raise ValueError(f\"columns overlap but no suffix specified: {to_rename}\")\n\n    def renamer(x, suffix):\n        \"\"\"\n        Rename the left and right indices.\n\n        If there is overlap, and suffix is not None, add\n        suffix, otherwise, leave it as-is.\n\n        Parameters\n        ----------\n        x : original column name\n        suffix : str or None\n\n        Returns\n        -------\n        x : renamed column name\n        \"\"\"\n        if x in to_rename and suffix is not None:\n            return f\"{x}{suffix}\"\n        return x\n\n    lrenamer = partial(renamer, suffix=lsuffix)\n    rrenamer = partial(renamer, suffix=rsuffix)\n\n    llabels = left._transform_index(lrenamer)\n    rlabels = right._transform_index(rrenamer)\n\n    dups = []\n    if not llabels.is_unique:\n        # Only warn when duplicates are caused because of suffixes, already duplicated\n        # columns in origin should not warn\n        dups = llabels[(llabels.duplicated()) & (~left.duplicated())].tolist()\n    if not rlabels.is_unique:\n        dups.extend(rlabels[(rlabels.duplicated()) & (~right.duplicated())].tolist())\n    if dups:\n        raise MergeError(\n            f\"Passing 'suffixes' which cause duplicate columns {set(dups)} is \"\n            f\"not allowed.\",\n        )\n\n    return llabels, rlabels\n"
    },
    {
      "filename": "pandas/io/json/_json.py",
      "content": "from __future__ import annotations\n\nfrom abc import (\n    ABC,\n    abstractmethod,\n)\nfrom collections import abc\nfrom io import StringIO\nfrom itertools import islice\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Generic,\n    Literal,\n    Mapping,\n    TypeVar,\n    overload,\n)\n\nimport numpy as np\n\nfrom pandas._libs.json import(\n    loads,\n    dumps,\n)\nfrom pandas._libs.tslibs import iNaT\nfrom pandas._typing import (\n    CompressionOptions,\n    DtypeArg,\n    FilePath,\n    IndexLabel,\n    JSONSerializable,\n    ReadBuffer,\n    StorageOptions,\n    WriteBuffer,\n)\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import doc\n\nfrom pandas.core.dtypes.common import (\n    ensure_str,\n    is_period_dtype,\n)\n\nfrom pandas import (\n    DataFrame,\n    MultiIndex,\n    Series,\n    isna,\n    notna,\n    to_datetime,\n)\nfrom pandas.core.reshape.concat import concat\nfrom pandas.core.shared_docs import _shared_docs\n\nfrom pandas.io.common import (\n    IOHandles,\n    _extension_to_compression,\n    file_exists,\n    get_handle,\n    is_fsspec_url,\n    is_url,\n    stringify_path,\n)\nfrom pandas.io.json._normalize import convert_to_line_delimits\nfrom pandas.io.json._table_schema import (\n    build_table_schema,\n    parse_table_schema,\n)\nfrom pandas.io.parsers.readers import validate_integer\n\nif TYPE_CHECKING:\n    from pandas.core.generic import NDFrame\n\nFrameSeriesStrT = TypeVar(\"FrameSeriesStrT\", bound=Literal[\"frame\", \"series\"])\n\n# interface to/from\n@overload\ndef to_json(\n    path_or_buf: FilePath | WriteBuffer[str] | WriteBuffer[bytes],\n    obj: NDFrame,\n    orient: str | None = ...,\n    date_format: str = ...,\n    double_precision: int = ...,\n    force_ascii: bool = ...,\n    date_unit: str = ...,\n    default_handler: Callable[[Any], JSONSerializable] | None = ...,\n    lines: bool = ...,\n    compression: CompressionOptions = ...,\n    index: bool = ...,\n    indent: int = ...,\n    storage_options: StorageOptions = ...,\n    mode: Literal[\"a\", \"w\"] = ...,\n) -> None:\n    ...\n\n\n@overload\ndef to_json(\n    path_or_buf: None,\n    obj: NDFrame,\n    orient: str | None = ...,\n    date_format: str = ...,\n    double_precision: int = ...,\n    force_ascii: bool = ...,\n    date_unit: str = ...,\n    default_handler: Callable[[Any], JSONSerializable] | None = ...,\n    lines: bool = ...,\n    compression: CompressionOptions = ...,\n    index: bool = ...,\n    indent: int = ...,\n    storage_options: StorageOptions = ...,\n    mode: Literal[\"a\", \"w\"] = ...,\n) -> str:\n    ...\n\n\ndef to_json(\n    path_or_buf: FilePath | WriteBuffer[str] | WriteBuffer[bytes] | None,\n    obj: NDFrame,\n    orient: str | None = None,\n    date_format: str = \"epoch\",\n    double_precision: int = 10,\n    force_ascii: bool = True,\n    date_unit: str = \"ms\",\n    default_handler: Callable[[Any], JSONSerializable] | None = None,\n    lines: bool = False,\n    compression: CompressionOptions = \"infer\",\n    index: bool = True,\n    indent: int = 0,\n    storage_options: StorageOptions = None,\n    mode: Literal[\"a\", \"w\"] = \"w\",\n) -> str | None:\n\n    if not index and orient not in [\"split\", \"table\"]:\n        raise ValueError(\n            \"'index=False' is only valid when 'orient' is 'split' or 'table'\"\n        )\n\n    if lines and orient != \"records\":\n        raise ValueError(\"'lines' keyword only valid when 'orient' is records\")\n\n    if mode not in [\"a\", \"w\"]:\n        msg = (\n            f\"mode={mode} is not a valid option.\"\n            \"Only 'w' and 'a' are currently supported.\"\n        )\n        raise ValueError(msg)\n\n    if mode == \"a\" and (not lines or orient != \"records\"):\n        msg = (\n            \"mode='a' (append) is only supported when\"\n            \"lines is True and orient is 'records'\"\n        )\n        raise ValueError(msg)\n\n    if orient == \"table\" and isinstance(obj, Series):\n        obj = obj.to_frame(name=obj.name or \"values\")\n\n    writer: type[Writer]\n    if orient == \"table\" and isinstance(obj, DataFrame):\n        writer = JSONTableWriter\n    elif isinstance(obj, Series):\n        writer = SeriesWriter\n    elif isinstance(obj, DataFrame):\n        writer = FrameWriter\n    else:\n        raise NotImplementedError(\"'obj' should be a Series or a DataFrame\")\n\n    s = writer(\n        obj,\n        orient=orient,\n        date_format=date_format,\n        double_precision=double_precision,\n        ensure_ascii=force_ascii,\n        date_unit=date_unit,\n        default_handler=default_handler,\n        index=index,\n        indent=indent,\n    ).write()\n\n    if lines:\n        s = convert_to_line_delimits(s)\n\n    if path_or_buf is not None:\n        # apply compression and byte/text conversion\n        with get_handle(\n            path_or_buf, mode, compression=compression, storage_options=storage_options\n        ) as handles:\n            handles.handle.write(s)\n    else:\n        return s\n    return None\n\n\nclass Writer(ABC):\n    _default_orient: str\n\n    def __init__(\n        self,\n        obj: NDFrame,\n        orient: str | None,\n        date_format: str,\n        double_precision: int,\n        ensure_ascii: bool,\n        date_unit: str,\n        index: bool,\n        default_handler: Callable[[Any], JSONSerializable] | None = None,\n        indent: int = 0,\n    ) -> None:\n        self.obj = obj\n\n        if orient is None:\n            orient = self._default_orient\n\n        self.orient = orient\n        self.date_format = date_format\n        self.double_precision = double_precision\n        self.ensure_ascii = ensure_ascii\n        self.date_unit = date_unit\n        self.default_handler = default_handler\n        self.index = index\n        self.indent = indent\n\n        self.is_copy = None\n        self._format_axes()\n\n    def _format_axes(self):\n        raise AbstractMethodError(self)\n\n    def write(self) -> str:\n        iso_dates = self.date_format == \"iso\"\n        return dumps(\n            self.obj_to_write,\n            orient=self.orient,\n            double_precision=self.double_precision,\n            ensure_ascii=self.ensure_ascii,\n            date_unit=self.date_unit,\n            iso_dates=iso_dates,\n            default_handler=self.default_handler,\n            indent=self.indent,\n        )\n\n    @property\n    @abstractmethod\n    def obj_to_write(self) -> NDFrame | Mapping[IndexLabel, Any]:\n        \"\"\"Object to write in JSON format.\"\"\"\n\n\nclass SeriesWriter(Writer):\n    _default_orient = \"index\"\n\n    @property\n    def obj_to_write(self) -> NDFrame | Mapping[IndexLabel, Any]:\n        if not self.index and self.orient == \"split\":\n            return {\"name\": self.obj.name, \"data\": self.obj.values}\n        else:\n            return self.obj\n\n    def _format_axes(self):\n        if not self.obj.index.is_unique and self.orient == \"index\":\n            raise ValueError(f\"Series index must be unique for orient='{self.orient}'\")\n\n\nclass FrameWriter(Writer):\n    _default_orient = \"columns\"\n\n    @property\n    def obj_to_write(self) -> NDFrame | Mapping[IndexLabel, Any]:\n        if not self.index and self.orient == \"split\":\n            obj_to_write = self.obj.to_dict(orient=\"split\")\n            del obj_to_write[\"index\"]\n        else:\n            obj_to_write = self.obj\n        return obj_to_write\n\n    def _format_axes(self):\n        \"\"\"\n        Try to format axes if they are datelike.\n        \"\"\"\n        if not self.obj.index.is_unique and self.orient in (\"index\", \"columns\"):\n            raise ValueError(\n                f\"DataFrame index must be unique for orient='{self.orient}'.\"\n            )\n        if not self.obj.columns.is_unique and self.orient in (\n            \"index\",\n            \"columns\",\n            \"records\",\n        ):\n            raise ValueError(\n                f\"DataFrame columns must be unique for orient='{self.orient}'.\"\n            )\n\n\nclass JSONTableWriter(FrameWriter):\n    _default_orient = \"records\"\n\n    def __init__(\n        self,\n        obj,\n        orient: str | None,\n        date_format: str,\n        double_precision: int,\n        ensure_ascii: bool,\n        date_unit: str,\n        index: bool,\n        default_handler: Callable[[Any], JSONSerializable] | None = None,\n        indent: int = 0,\n    ) -> None:\n        \"\"\"\n        Adds a `schema` attribute with the Table Schema, resets\n        the index (can't do in caller, because the schema inference needs\n        to know what the index is, forces orient to records, and forces\n        date_format to 'iso'.\n        \"\"\"\n        super().__init__(\n            obj,\n            orient,\n            date_format,\n            double_precision,\n            ensure_ascii,\n            date_unit,\n            index,\n            default_handler=default_handler,\n            indent=indent,\n        )\n\n        if date_format != \"iso\":\n            msg = (\n                \"Trying to write with `orient='table'` and \"\n                f\"`date_format='{date_format}'`. Table Schema requires dates \"\n                \"to be formatted with `date_format='iso'`\"\n            )\n            raise ValueError(msg)\n\n        self.schema = build_table_schema(obj, index=self.index)\n\n        # NotImplemented on a column MultiIndex\n        if obj.ndim == 2 and isinstance(obj.columns, MultiIndex):\n            raise NotImplementedError(\n                \"orient='table' is not supported for MultiIndex columns\"\n            )\n\n        # TODO: Do this timedelta properly in objToJSON.c See GH #15137\n        if (\n            (obj.ndim == 1)\n            and (obj.name in set(obj.index.names))\n            or len(obj.columns.intersection(obj.index.names))\n        ):\n            msg = \"Overlapping names between the index and columns\"\n            raise ValueError(msg)\n\n        obj = obj.copy()\n        timedeltas = obj.select_dtypes(include=[\"timedelta\"]).columns\n        if len(timedeltas):\n            obj[timedeltas] = obj[timedeltas].applymap(lambda x: x.isoformat())\n        # Convert PeriodIndex to datetimes before serializing\n        if is_period_dtype(obj.index.dtype):\n            obj.index = obj.index.to_timestamp()\n\n        # exclude index from obj if index=False\n        if not self.index:\n            self.obj = obj.reset_index(drop=True)\n        else:\n            self.obj = obj.reset_index(drop=False)\n        self.date_format = \"iso\"\n        self.orient = \"records\"\n        self.index = index\n\n    @property\n    def obj_to_write(self) -> NDFrame | Mapping[IndexLabel, Any]:\n        return {\"schema\": self.schema, \"data\": self.obj}\n\n\n@overload\ndef read_json(\n    path_or_buf: FilePath | ReadBuffer[str] | ReadBuffer[bytes],\n    *,\n    orient: str | None = ...,\n    typ: Literal[\"frame\"] = ...,\n    dtype: DtypeArg | None = ...,\n    convert_axes=...,\n    convert_dates: bool | list[str] = ...,\n    keep_default_dates: bool = ...,\n    precise_float: bool = ...,\n    date_unit: str | None = ...,\n    encoding: str | None = ...,\n    encoding_errors: str | None = ...,\n    lines: bool = ...,\n    chunksize: int,\n    compression: CompressionOptions = ...,\n    nrows: int | None = ...,\n    storage_options: StorageOptions = ...,\n) -> JsonReader[Literal[\"frame\"]]:\n    ...\n\n\n@overload\ndef read_json(\n    path_or_buf: FilePath | ReadBuffer[str] | ReadBuffer[bytes],\n    *,\n    orient: str | None = ...,\n    typ: Literal[\"series\"],\n    dtype: DtypeArg | None = ...,\n    convert_axes=...,\n    convert_dates: bool | list[str] = ...,\n    keep_default_dates: bool = ...,\n    precise_float: bool = ...,\n    date_unit: str | None = ...,\n    encoding: str | None = ...,\n    encoding_errors: str | None = ...,\n    lines: bool = ...,\n    chunksize: int,\n    compression: CompressionOptions = ...,\n    nrows: int | None = ...,\n    storage_options: StorageOptions = ...,\n) -> JsonReader[Literal[\"series\"]]:\n    ...\n\n\n@overload\ndef read_json(\n    path_or_buf: FilePath | ReadBuffer[str] | ReadBuffer[bytes],\n    *,\n    orient: str | None = ...,\n    typ: Literal[\"series\"],\n    dtype: DtypeArg | None = ...,\n    convert_axes=...,\n    convert_dates: bool | list[str] = ...,\n    keep_default_dates: bool = ...,\n    precise_float: bool = ...,\n    date_unit: str | None = ...,\n    encoding: str | None = ...,\n    encoding_errors: str | None = ...,\n    lines: bool = ...,\n    chunksize: None = ...,\n    compression: CompressionOptions = ...,\n    nrows: int | None = ...,\n    storage_options: StorageOptions = ...,\n) -> Series:\n    ...\n\n\n@overload\ndef read_json(\n    path_or_buf: FilePath | ReadBuffer[str] | ReadBuffer[bytes],\n    *,\n    orient: str | None = ...,\n    typ: Literal[\"frame\"] = ...,\n    dtype: DtypeArg | None = ...,\n    convert_axes=...,\n    convert_dates: bool | list[str] = ...,\n    keep_default_dates: bool = ...,\n    precise_float: bool = ...,\n    date_unit: str | None = ...,\n    encoding: str | None = ...,\n    encoding_errors: str | None = ...,\n    lines: bool = ...,\n    chunksize: None = ...,\n    compression: CompressionOptions = ...,\n    nrows: int | None = ...,\n    storage_options: StorageOptions = ...,\n) -> DataFrame:\n    ...\n\n\n@doc(\n    storage_options=_shared_docs[\"storage_options\"],\n    decompression_options=_shared_docs[\"decompression_options\"] % \"path_or_buf\",\n)\ndef read_json(\n    path_or_buf: FilePath | ReadBuffer[str] | ReadBuffer[bytes],\n    *,\n    orient: str | None = None,\n    typ: Literal[\"frame\", \"series\"] = \"frame\",\n    dtype: DtypeArg | None = None,\n    convert_axes=None,\n    convert_dates: bool | list[str] = True,\n    keep_default_dates: bool = True,\n    precise_float: bool = False,\n    date_unit: str | None = None,\n    encoding: str | None = None,\n    encoding_errors: str | None = \"strict\",\n    lines: bool = False,\n    chunksize: int | None = None,\n    compression: CompressionOptions = \"infer\",\n    nrows: int | None = None,\n    storage_options: StorageOptions = None,\n) -> DataFrame | Series | JsonReader:\n    \"\"\"\n    Convert a JSON string to pandas object.\n\n    Parameters\n    ----------\n    path_or_buf : a valid JSON str, path object or file-like object\n        Any valid string path is acceptable. The string could be a URL. Valid\n        URL schemes include http, ftp, s3, and file. For file URLs, a host is\n        expected. A local file could be:\n        ``file://localhost/path/to/table.json``.\n\n        If you want to pass in a path object, pandas accepts any\n        ``os.PathLike``.\n\n        By file-like object, we refer to objects with a ``read()`` method,\n        such as a file handle (e.g. via builtin ``open`` function)\n        or ``StringIO``.\n    orient : str, optional\n        Indication of expected JSON string format.\n        Compatible JSON strings can be produced by ``to_json()`` with a\n        corresponding orient value.\n        The set of possible orients is:\n\n        - ``'split'`` : dict like\n          ``{{index -> [index], columns -> [columns], data -> [values]}}``\n        - ``'records'`` : list like\n          ``[{{column -> value}}, ... , {{column -> value}}]``\n        - ``'index'`` : dict like ``{{index -> {{column -> value}}}}``\n        - ``'columns'`` : dict like ``{{column -> {{index -> value}}}}``\n        - ``'values'`` : just the values array\n\n        The allowed and default values depend on the value\n        of the `typ` parameter.\n\n        * when ``typ == 'series'``,\n\n          - allowed orients are ``{{'split','records','index'}}``\n          - default is ``'index'``\n          - The Series index must be unique for orient ``'index'``.\n\n        * when ``typ == 'frame'``,\n\n          - allowed orients are ``{{'split','records','index',\n            'columns','values', 'table'}}``\n          - default is ``'columns'``\n          - The DataFrame index must be unique for orients ``'index'`` and\n            ``'columns'``.\n          - The DataFrame columns must be unique for orients ``'index'``,\n            ``'columns'``, and ``'records'``.\n\n    typ : {{'frame', 'series'}}, default 'frame'\n        The type of object to recover.\n\n    dtype : bool or dict, default None\n        If True, infer dtypes; if a dict of column to dtype, then use those;\n        if False, then don't infer dtypes at all, applies only to the data.\n\n        For all ``orient`` values except ``'table'``, default is True.\n\n        .. versionchanged:: 0.25.0\n\n           Not applicable for ``orient='table'``.\n\n    convert_axes : bool, default None\n        Try to convert the axes to the proper dtypes.\n\n        For all ``orient`` values except ``'table'``, default is True.\n\n        .. versionchanged:: 0.25.0\n\n           Not applicable for ``orient='table'``.\n\n    convert_dates : bool or list of str, default True\n        If True then default datelike columns may be converted (depending on\n        keep_default_dates).\n        If False, no dates will be converted.\n        If a list of column names, then those columns will be converted and\n        default datelike columns may also be converted (depending on\n        keep_default_dates).\n\n    keep_default_dates : bool, default True\n        If parsing dates (convert_dates is not False), then try to parse the\n        default datelike columns.\n        A column label is datelike if\n\n        * it ends with ``'_at'``,\n\n        * it ends with ``'_time'``,\n\n        * it begins with ``'timestamp'``,\n\n        * it is ``'modified'``, or\n\n        * it is ``'date'``.\n\n    precise_float : bool, default False\n        Set to enable usage of higher precision (strtod) function when\n        decoding string to double values. Default (False) is to use fast but\n        less precise builtin functionality.\n\n    date_unit : str, default None\n        The timestamp unit to detect if converting dates. The default behaviour\n        is to try and detect the correct precision, but if this is not desired\n        then pass one of 's', 'ms', 'us' or 'ns' to force parsing only seconds,\n        milliseconds, microseconds or nanoseconds respectively.\n\n    encoding : str, default is 'utf-8'\n        The encoding to use to decode py3 bytes.\n\n    encoding_errors : str, optional, default \"strict\"\n        How encoding errors are treated. `List of possible values\n        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n        .. versionadded:: 1.3.0\n\n    lines : bool, default False\n        Read the file as a json object per line.\n\n    chunksize : int, optional\n        Return JsonReader object for iteration.\n        See the `line-delimited json docs\n        <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#line-delimited-json>`_\n        for more information on ``chunksize``.\n        This can only be passed if `lines=True`.\n        If this is None, the file will be read into memory all at once.\n\n        .. versionchanged:: 1.2\n\n           ``JsonReader`` is a context manager.\n\n    {decompression_options}\n\n        .. versionchanged:: 1.4.0 Zstandard support.\n\n    nrows : int, optional\n        The number of lines from the line-delimited jsonfile that has to be read.\n        This can only be passed if `lines=True`.\n        If this is None, all the rows will be returned.\n\n        .. versionadded:: 1.1\n\n    {storage_options}\n\n        .. versionadded:: 1.2.0\n\n    Returns\n    -------\n    Series or DataFrame\n        The type returned depends on the value of `typ`.\n\n    See Also\n    --------\n    DataFrame.to_json : Convert a DataFrame to a JSON string.\n    Series.to_json : Convert a Series to a JSON string.\n    json_normalize : Normalize semi-structured JSON data into a flat table.\n\n    Notes\n    -----\n    Specific to ``orient='table'``, if a :class:`DataFrame` with a literal\n    :class:`Index` name of `index` gets written with :func:`to_json`, the\n    subsequent read operation will incorrectly set the :class:`Index` name to\n    ``None``. This is because `index` is also used by :func:`DataFrame.to_json`\n    to denote a missing :class:`Index` name, and the subsequent\n    :func:`read_json` operation cannot distinguish between the two. The same\n    limitation is encountered with a :class:`MultiIndex` and any names\n    beginning with ``'level_'``.\n\n    Examples\n    --------\n    >>> df = pd.DataFrame([['a', 'b'], ['c', 'd']],\n    ...                   index=['row 1', 'row 2'],\n    ...                   columns=['col 1', 'col 2'])\n\n    Encoding/decoding a Dataframe using ``'split'`` formatted JSON:\n\n    >>> df.to_json(orient='split')\n        '\\\n{{\\\n\"columns\":[\"col 1\",\"col 2\"],\\\n\"index\":[\"row 1\",\"row 2\"],\\\n\"data\":[[\"a\",\"b\"],[\"c\",\"d\"]]\\\n}}\\\n'\n    >>> pd.read_json(_, orient='split')\n          col 1 col 2\n    row 1     a     b\n    row 2     c     d\n\n    Encoding/decoding a Dataframe using ``'index'`` formatted JSON:\n\n    >>> df.to_json(orient='index')\n    '{{\"row 1\":{{\"col 1\":\"a\",\"col 2\":\"b\"}},\"row 2\":{{\"col 1\":\"c\",\"col 2\":\"d\"}}}}'\n\n    >>> pd.read_json(_, orient='index')\n          col 1 col 2\n    row 1     a     b\n    row 2     c     d\n\n    Encoding/decoding a Dataframe using ``'records'`` formatted JSON.\n    Note that index labels are not preserved with this encoding.\n\n    >>> df.to_json(orient='records')\n    '[{{\"col 1\":\"a\",\"col 2\":\"b\"}},{{\"col 1\":\"c\",\"col 2\":\"d\"}}]'\n    >>> pd.read_json(_, orient='records')\n      col 1 col 2\n    0     a     b\n    1     c     d\n\n    Encoding with Table Schema\n\n    >>> df.to_json(orient='table')\n        '\\\n{{\"schema\":{{\"fields\":[\\\n{{\"name\":\"index\",\"type\":\"string\"}},\\\n{{\"name\":\"col 1\",\"type\":\"string\"}},\\\n{{\"name\":\"col 2\",\"type\":\"string\"}}],\\\n\"primaryKey\":[\"index\"],\\\n\"pandas_version\":\"1.4.0\"}},\\\n\"data\":[\\\n{{\"index\":\"row 1\",\"col 1\":\"a\",\"col 2\":\"b\"}},\\\n{{\"index\":\"row 2\",\"col 1\":\"c\",\"col 2\":\"d\"}}]\\\n}}\\\n'\n    \"\"\"\n    if orient == \"table\" and dtype:\n        raise ValueError(\"cannot pass both dtype and orient='table'\")\n    if orient == \"table\" and convert_axes:\n        raise ValueError(\"cannot pass both convert_axes and orient='table'\")\n\n    if dtype is None and orient != \"table\":\n        # error: Incompatible types in assignment (expression has type \"bool\", variable\n        # has type \"Union[ExtensionDtype, str, dtype[Any], Type[str], Type[float],\n        # Type[int], Type[complex], Type[bool], Type[object], Dict[Hashable,\n        # Union[ExtensionDtype, Union[str, dtype[Any]], Type[str], Type[float],\n        # Type[int], Type[complex], Type[bool], Type[object]]], None]\")\n        dtype = True  # type: ignore[assignment]\n    if convert_axes is None and orient != \"table\":\n        convert_axes = True\n\n    json_reader = JsonReader(\n        path_or_buf,\n        orient=orient,\n        typ=typ,\n        dtype=dtype,\n        convert_axes=convert_axes,\n        convert_dates=convert_dates,\n        keep_default_dates=keep_default_dates,\n        precise_float=precise_float,\n        date_unit=date_unit,\n        encoding=encoding,\n        lines=lines,\n        chunksize=chunksize,\n        compression=compression,\n        nrows=nrows,\n        storage_options=storage_options,\n        encoding_errors=encoding_errors,\n    )\n\n    if chunksize:\n        return json_reader\n\n    with json_reader:\n        return json_reader.read()\n\n\nclass JsonReader(abc.Iterator, Generic[FrameSeriesStrT]):\n    \"\"\"\n    JsonReader provides an interface for reading in a JSON file.\n\n    If initialized with ``lines=True`` and ``chunksize``, can be iterated over\n    ``chunksize`` lines at a time. Otherwise, calling ``read`` reads in the\n    whole document.\n    \"\"\"\n\n    def __init__(\n        self,\n        filepath_or_buffer,\n        orient,\n        typ: FrameSeriesStrT,\n        dtype,\n        convert_axes,\n        convert_dates,\n        keep_default_dates: bool,\n        precise_float: bool,\n        date_unit,\n        encoding,\n        lines: bool,\n        chunksize: int | None,\n        compression: CompressionOptions,\n        nrows: int | None,\n        storage_options: StorageOptions = None,\n        encoding_errors: str | None = \"strict\",\n    ) -> None:\n\n        self.orient = orient\n        self.typ = typ\n        self.dtype = dtype\n        self.convert_axes = convert_axes\n        self.convert_dates = convert_dates\n        self.keep_default_dates = keep_default_dates\n        self.precise_float = precise_float\n        self.date_unit = date_unit\n        self.encoding = encoding\n        self.compression = compression\n        self.storage_options = storage_options\n        self.lines = lines\n        self.chunksize = chunksize\n        self.nrows_seen = 0\n        self.nrows = nrows\n        self.encoding_errors = encoding_errors\n        self.handles: IOHandles[str] | None = None\n\n        if self.chunksize is not None:\n            self.chunksize = validate_integer(\"chunksize\", self.chunksize, 1)\n            if not self.lines:\n                raise ValueError(\"chunksize can only be passed if lines=True\")\n        if self.nrows is not None:\n            self.nrows = validate_integer(\"nrows\", self.nrows, 0)\n            if not self.lines:\n                raise ValueError(\"nrows can only be passed if lines=True\")\n\n        data = self._get_data_from_filepath(filepath_or_buffer)\n        self.data = self._preprocess_data(data)\n\n    def _preprocess_data(self, data):\n        \"\"\"\n        At this point, the data either has a `read` attribute (e.g. a file\n        object or a StringIO) or is a string that is a JSON document.\n\n        If self.chunksize, we prepare the data for the `__next__` method.\n        Otherwise, we read it into memory for the `read` method.\n        \"\"\"\n        if hasattr(data, \"read\") and not (self.chunksize or self.nrows):\n            with self:\n                data = data.read()\n        if not hasattr(data, \"read\") and (self.chunksize or self.nrows):\n            data = StringIO(data)\n\n        return data\n\n    def _get_data_from_filepath(self, filepath_or_buffer):\n        \"\"\"\n        The function read_json accepts three input types:\n            1. filepath (string-like)\n            2. file-like object (e.g. open file object, StringIO)\n            3. JSON string\n\n        This method turns (1) into (2) to simplify the rest of the processing.\n        It returns input types (2) and (3) unchanged.\n\n        It raises FileNotFoundError if the input is a string ending in\n        one of .json, .json.gz, .json.bz2, etc. but no such file exists.\n        \"\"\"\n        # if it is a string but the file does not exist, it might be a JSON string\n        filepath_or_buffer = stringify_path(filepath_or_buffer)\n        if (\n            not isinstance(filepath_or_buffer, str)\n            or is_url(filepath_or_buffer)\n            or is_fsspec_url(filepath_or_buffer)\n            or file_exists(filepath_or_buffer)\n        ):\n            self.handles = get_handle(\n                filepath_or_buffer,\n                \"r\",\n                encoding=self.encoding,\n                compression=self.compression,\n                storage_options=self.storage_options,\n                errors=self.encoding_errors,\n            )\n            filepath_or_buffer = self.handles.handle\n        elif (\n            isinstance(filepath_or_buffer, str)\n            and filepath_or_buffer.lower().endswith(\n                (\".json\",) + tuple(f\".json{c}\" for c in _extension_to_compression)\n            )\n            and not file_exists(filepath_or_buffer)\n        ):\n            raise FileNotFoundError(f\"File {filepath_or_buffer} does not exist\")\n\n        return filepath_or_buffer\n\n    def _combine_lines(self, lines) -> str:\n        \"\"\"\n        Combines a list of JSON objects into one JSON object.\n        \"\"\"\n        return (\n            f'[{\",\".join([line for line in (line.strip() for line in lines) if line])}]'\n        )\n\n    @overload\n    def read(self: JsonReader[Literal[\"frame\"]]) -> DataFrame:\n        ...\n\n    @overload\n    def read(self: JsonReader[Literal[\"series\"]]) -> Series:\n        ...\n\n    @overload\n    def read(self: JsonReader[Literal[\"frame\", \"series\"]]) -> DataFrame | Series:\n        ...\n\n    def read(self) -> DataFrame | Series:\n        \"\"\"\n        Read the whole JSON input into a pandas object.\n        \"\"\"\n        obj: DataFrame | Series\n        if self.lines:\n            if self.chunksize:\n                obj = concat(self)\n            elif self.nrows:\n                lines = list(islice(self.data, self.nrows))\n                lines_json = self._combine_lines(lines)\n                obj = self._get_object_parser(lines_json)\n            else:\n                data = ensure_str(self.data)\n                data_lines = data.split(\"\\n\")\n                obj = self._get_object_parser(self._combine_lines(data_lines))\n        else:\n            obj = self._get_object_parser(self.data)\n        self.close()\n        return obj\n\n    def _get_object_parser(self, json) -> DataFrame | Series:\n        \"\"\"\n        Parses a json document into a pandas object.\n        \"\"\"\n        typ = self.typ\n        dtype = self.dtype\n        kwargs = {\n            \"orient\": self.orient,\n            \"dtype\": self.dtype,\n            \"convert_axes\": self.convert_axes,\n            \"convert_dates\": self.convert_dates,\n            \"keep_default_dates\": self.keep_default_dates,\n            \"precise_float\": self.precise_float,\n            \"date_unit\": self.date_unit,\n        }\n        obj = None\n        if typ == \"frame\":\n            obj = FrameParser(json, **kwargs).parse()\n\n        if typ == \"series\" or obj is None:\n            if not isinstance(dtype, bool):\n                kwargs[\"dtype\"] = dtype\n            obj = SeriesParser(json, **kwargs).parse()\n\n        return obj\n\n    def close(self) -> None:\n        \"\"\"\n        If we opened a stream earlier, in _get_data_from_filepath, we should\n        close it.\n\n        If an open stream or file was passed, we leave it open.\n        \"\"\"\n        if self.handles is not None:\n            self.handles.close()\n\n    def __iter__(self: JsonReader[FrameSeriesStrT]) -> JsonReader[FrameSeriesStrT]:\n        return self\n\n    @overload\n    def __next__(self: JsonReader[Literal[\"frame\"]]) -> DataFrame:\n        ...\n\n    @overload\n    def __next__(self: JsonReader[Literal[\"series\"]]) -> Series:\n        ...\n\n    @overload\n    def __next__(self: JsonReader[Literal[\"frame\", \"series\"]]) -> DataFrame | Series:\n        ...\n\n    def __next__(self) -> DataFrame | Series:\n        if self.nrows:\n            if self.nrows_seen >= self.nrows:\n                self.close()\n                raise StopIteration\n\n        lines = list(islice(self.data, self.chunksize))\n        if lines:\n            lines_json = self._combine_lines(lines)\n            obj = self._get_object_parser(lines_json)\n\n            # Make sure that the returned objects have the right index.\n            obj.index = range(self.nrows_seen, self.nrows_seen + len(obj))\n            self.nrows_seen += len(obj)\n\n            return obj\n\n        self.close()\n        raise StopIteration\n\n    def __enter__(self) -> JsonReader[FrameSeriesStrT]:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_value: BaseException | None,\n        traceback: TracebackType | None,\n    ) -> None:\n        self.close()\n\n\nclass Parser:\n    _split_keys: tuple[str, ...]\n    _default_orient: str\n\n    _STAMP_UNITS = (\"s\", \"ms\", \"us\", \"ns\")\n    _MIN_STAMPS = {\n        \"s\": 31536000,\n        \"ms\": 31536000000,\n        \"us\": 31536000000000,\n        \"ns\": 31536000000000000,\n    }\n\n    def __init__(\n        self,\n        json,\n        orient,\n        dtype: DtypeArg | None = None,\n        convert_axes: bool = True,\n        convert_dates: bool | list[str] = True,\n        keep_default_dates: bool = False,\n        precise_float: bool = False,\n        date_unit=None,\n    ) -> None:\n        self.json = json\n\n        if orient is None:\n            orient = self._default_orient\n\n        self.orient = orient\n\n        self.dtype = dtype\n\n        if date_unit is not None:\n            date_unit = date_unit.lower()\n            if date_unit not in self._STAMP_UNITS:\n                raise ValueError(f\"date_unit must be one of {self._STAMP_UNITS}\")\n            self.min_stamp = self._MIN_STAMPS[date_unit]\n        else:\n            self.min_stamp = self._MIN_STAMPS[\"s\"]\n\n        self.precise_float = precise_float\n        self.convert_axes = convert_axes\n        self.convert_dates = convert_dates\n        self.date_unit = date_unit\n        self.keep_default_dates = keep_default_dates\n        self.obj: DataFrame | Series | None = None\n\n    def check_keys_split(self, decoded) -> None:\n        \"\"\"\n        Checks that dict has only the appropriate keys for orient='split'.\n        \"\"\"\n        bad_keys = set(decoded.keys()).difference(set(self._split_keys))\n        if bad_keys:\n            bad_keys_joined = \", \".join(bad_keys)\n            raise ValueError(f\"JSON data had unexpected key(s): {bad_keys_joined}\")\n\n    def parse(self):\n        self._parse()\n\n        if self.obj is None:\n            return None\n        if self.convert_axes:\n            self._convert_axes()\n        self._try_convert_types()\n        return self.obj\n\n    def _parse(self):\n        raise AbstractMethodError(self)\n\n    def _convert_axes(self) -> None:\n        \"\"\"\n        Try to convert axes.\n        \"\"\"\n        obj = self.obj\n        assert obj is not None  # for mypy\n        for axis_name in obj._AXIS_ORDERS:\n            new_axis, result = self._try_convert_data(\n                name=axis_name,\n                data=obj._get_axis(axis_name),\n                use_dtypes=False,\n                convert_dates=True,\n            )\n            if result:\n                setattr(self.obj, axis_name, new_axis)\n\n    def _try_convert_types(self):\n        raise AbstractMethodError(self)\n\n    def _try_convert_data(\n        self,\n        name,\n        data,\n        use_dtypes: bool = True,\n        convert_dates: bool | list[str] = True,\n    ):\n        \"\"\"\n        Try to parse a ndarray like into a column by inferring dtype.\n        \"\"\"\n        # don't try to coerce, unless a force conversion\n        if use_dtypes:\n            if not self.dtype:\n                if all(notna(data)):\n                    return data, False\n                return data.fillna(np.nan), True\n\n            # error: Non-overlapping identity check (left operand type:\n            # \"Union[ExtensionDtype, str, dtype[Any], Type[object],\n            # Dict[Hashable, Union[ExtensionDtype, Union[str, dtype[Any]],\n            # Type[str], Type[float], Type[int], Type[complex], Type[bool],\n            # Type[object]]]]\", right operand type: \"Literal[True]\")\n            elif self.dtype is True:  # type: ignore[comparison-overlap]\n                pass\n            else:\n                # dtype to force\n                dtype = (\n                    self.dtype.get(name) if isinstance(self.dtype, dict) else self.dtype\n                )\n                if dtype is not None:\n                    try:\n                        return data.astype(dtype), True\n                    except (TypeError, ValueError):\n                        return data, False\n\n        if convert_dates:\n            new_data, result = self._try_convert_to_date(data)\n            if result:\n                return new_data, True\n\n        if data.dtype == \"object\":\n\n            # try float\n            try:\n                data = data.astype(\"float64\")\n            except (TypeError, ValueError):\n                pass\n\n        if data.dtype.kind == \"f\":\n\n            if data.dtype != \"float64\":\n\n                # coerce floats to 64\n                try:\n                    data = data.astype(\"float64\")\n                except (TypeError, ValueError):\n                    pass\n\n        # don't coerce 0-len data\n        if len(data) and data.dtype in (\"float\", \"object\"):\n\n            # coerce ints if we can\n            try:\n                new_data = data.astype(\"int64\")\n                if (new_data == data).all():\n                    data = new_data\n            except (TypeError, ValueError, OverflowError):\n                pass\n\n        # coerce ints to 64\n        if data.dtype == \"int\":\n\n            # coerce floats to 64\n            try:\n                data = data.astype(\"int64\")\n            except (TypeError, ValueError):\n                pass\n\n        # if we have an index, we want to preserve dtypes\n        if name == \"index\" and len(data):\n            if self.orient == \"split\":\n                return data, False\n\n        return data, True\n\n    def _try_convert_to_date(self, data):\n        \"\"\"\n        Try to parse a ndarray like into a date column.\n\n        Try to coerce object in epoch/iso formats and integer/float in epoch\n        formats. Return a boolean if parsing was successful.\n        \"\"\"\n        # no conversion on empty\n        if not len(data):\n            return data, False\n\n        new_data = data\n        if new_data.dtype == \"object\":\n            try:\n                new_data = data.astype(\"int64\")\n            except (TypeError, ValueError, OverflowError):\n                pass\n\n        # ignore numbers that are out of range\n        if issubclass(new_data.dtype.type, np.number):\n            in_range = (\n                isna(new_data._values)\n                | (new_data > self.min_stamp)\n                | (new_data._values == iNaT)\n            )\n            if not in_range.all():\n                return data, False\n\n        date_units = (self.date_unit,) if self.date_unit else self._STAMP_UNITS\n        for date_unit in date_units:\n            try:\n                new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError, TypeError):\n                continue\n            return new_data, True\n        return data, False\n\n    def _try_convert_dates(self):\n        raise AbstractMethodError(self)\n\n\nclass SeriesParser(Parser):\n    _default_orient = \"index\"\n    _split_keys = (\"name\", \"index\", \"data\")\n\n    def _parse(self) -> None:\n        data = loads(self.json, precise_float=self.precise_float)\n\n        if self.orient == \"split\":\n            decoded = {str(k): v for k, v in data.items()}\n            self.check_keys_split(decoded)\n            self.obj = Series(**decoded)\n        else:\n            self.obj = Series(data)\n\n    def _try_convert_types(self) -> None:\n        if self.obj is None:\n            return\n        obj, result = self._try_convert_data(\n            \"data\", self.obj, convert_dates=self.convert_dates\n        )\n        if result:\n            self.obj = obj\n\n\nclass FrameParser(Parser):\n    _default_orient = \"columns\"\n    _split_keys = (\"columns\", \"index\", \"data\")\n\n    def _parse(self) -> None:\n\n        json = self.json\n        orient = self.orient\n\n        if orient == \"columns\":\n            self.obj = DataFrame(\n                loads(json, precise_float=self.precise_float), dtype=None\n            )\n        elif orient == \"split\":\n            decoded = {\n                str(k): v\n                for k, v in loads(json, precise_float=self.precise_float).items()\n            }\n            self.check_keys_split(decoded)\n            self.obj = DataFrame(dtype=None, **decoded)\n        elif orient == \"index\":\n            self.obj = DataFrame.from_dict(\n                loads(json, precise_float=self.precise_float),\n                dtype=None,\n                orient=\"index\",\n            )\n        elif orient == \"table\":\n            self.obj = parse_table_schema(json, precise_float=self.precise_float)\n        else:\n            self.obj = DataFrame(\n                loads(json, precise_float=self.precise_float), dtype=None\n            )\n\n    def _process_converter(self, f, filt=None) -> None:\n        \"\"\"\n        Take a conversion function and possibly recreate the frame.\n        \"\"\"\n        if filt is None:\n            filt = lambda col, c: True\n\n        obj = self.obj\n        assert obj is not None  # for mypy\n\n        needs_new_obj = False\n        new_obj = {}\n        for i, (col, c) in enumerate(obj.items()):\n            if filt(col, c):\n                new_data, result = f(col, c)\n                if result:\n                    c = new_data\n                    needs_new_obj = True\n            new_obj[i] = c\n\n        if needs_new_obj:\n\n            # possibly handle dup columns\n            new_frame = DataFrame(new_obj, index=obj.index)\n            new_frame.columns = obj.columns\n            self.obj = new_frame\n\n    def _try_convert_types(self) -> None:\n        if self.obj is None:\n            return\n        if self.convert_dates:\n            self._try_convert_dates()\n\n        self._process_converter(\n            lambda col, c: self._try_convert_data(col, c, convert_dates=False)\n        )\n\n    def _try_convert_dates(self) -> None:\n        if self.obj is None:\n            return\n\n        # our columns to parse\n        convert_dates_list_bool = self.convert_dates\n        if isinstance(convert_dates_list_bool, bool):\n            convert_dates_list_bool = []\n        convert_dates = set(convert_dates_list_bool)\n\n        def is_ok(col) -> bool:\n            \"\"\"\n            Return if this col is ok to try for a date parse.\n            \"\"\"\n            if not isinstance(col, str):\n                return False\n\n            col_lower = col.lower()\n            if (\n                col_lower.endswith(\"_at\")\n                or col_lower.endswith(\"_time\")\n                or col_lower == \"modified\"\n                or col_lower == \"date\"\n                or col_lower == \"datetime\"\n                or col_lower.startswith(\"timestamp\")\n            ):\n                return True\n            return False\n\n        self._process_converter(\n            lambda col, c: self._try_convert_to_date(c),\n            lambda col, c: (\n                (self.keep_default_dates and is_ok(col)) or col in convert_dates\n            ),\n        )\n"
    }
  ]
}