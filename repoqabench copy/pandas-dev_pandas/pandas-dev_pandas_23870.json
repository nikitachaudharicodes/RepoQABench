{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "23870",
  "issue_description": "# DOC: Use only one blank line to separate sections or paragraphs\n\nWhen checking for errors with  `./scripts/validate_docstrings.py`, There is an error that comes up:\r\n\r\n**GL03**\r\n**Use only one blank line to separate sections or paragraphs**\r\n\r\nI started working on this and will create a pull request for this issue shortly to go into more detail on what I fixed.\r\n\r\n@datapythonista ",
  "issue_comments": [
    {
      "id": 441331187,
      "user": "datapythonista",
      "body": "Reopening. Most of the issues have been fixed, but still some left, see: https://github.com/pandas-dev/pandas/pull/23871#issuecomment-441274426"
    },
    {
      "id": 468856134,
      "user": "abkosar",
      "body": "Hey @datapythonista, I am new to open source and would like to work on this! Just curious if someone is working on it? If not I can give it a shot!\r\n\r\nThanks!"
    },
    {
      "id": 469256206,
      "user": "datapythonista",
      "body": "I think this is trickier than it looks, but surely a good first issue, and you can definitely work on it.\r\n\r\nExecuting `./scripts/validate_docstrings.py --errors=GL03` should give you the errors, and should return nothing once you fixed this issue.\r\n\r\nThanks @abkosar "
    },
    {
      "id": 469270991,
      "user": "abkosar",
      "body": "Thanks!"
    },
    {
      "id": 469338654,
      "user": "abkosar",
      "body": "So started working on the issue. When I run `./scripts/validate_docstrings.py --errors=GL03` there are 98 errors:\r\n\r\n- pandas.Series.to_numpy:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.agg: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.aggregate: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.cummax: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.cummin: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.cumprod: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.cumsum: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.kurt: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.mad: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.mean: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.median: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.skew: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.kurtosis: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.compound: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.dayofweek: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.weekday: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.is_month_start: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.is_month_end: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.is_quarter_start: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.is_quarter_end: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.is_year_start: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.is_year_end: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.is_leap_year: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Timedelta: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Timedelta.max: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Timedelta.min: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.SparseArray: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.read_feather: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.core.resample.Resampler.apply:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.resample.Resampler.aggregate:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.resample.Resampler.pipe:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.resample.Resampler.mean:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Panel.cummax: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.cummin: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.cumprod: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.cumsum: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.mean: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.median: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.skew: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.swaplevel: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.plotting.andrews_curves: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.describe_option: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.reset_option: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.get_option: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.set_option: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.melt: Double line break found;   please use only one blank line to separate sections or paragraphs, and do not   leave blank lines at the end of docstrings\r\n- pandas.Index.reindex: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.dayofweek: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.weekday: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.is_month_start:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.is_month_end: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.is_quarter_start:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.is_quarter_end:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.is_year_start:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.is_year_end: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.is_leap_year: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.window.Rolling.aggregate:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.window.Expanding.aggregate:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.GroupBy.head: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.GroupBy.mean: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.GroupBy.nth: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.GroupBy.tail: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.corr:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.cov:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.diff:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.fillna:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.hist:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.idxmax:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.idxmin:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.mad:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.skew:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.take:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.tshift:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.SeriesGroupBy.nlargest:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.SeriesGroupBy.nsmallest:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.SeriesGroupBy.unique:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.SeriesGroupBy.is_monotonic_increasing:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.SeriesGroupBy.is_monotonic_decreasing:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.corrwith:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.agg: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.aggregate: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.compound: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.cummax: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.cummin: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.cumprod: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.cumsum: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.kurt: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.kurtosis: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.mad: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.mean: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.median: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.skew: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.to_html: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.ptp: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.compound: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.kurt: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.kurtosis: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.mad: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n"
    },
    {
      "id": 499784768,
      "user": "NatholBMX",
      "body": "Hi @abkosar \r\nHave you mady any progress on this?"
    },
    {
      "id": 500048735,
      "user": "abkosar",
      "body": "Hi @NatholBMX \r\n\r\nI recently started working on this again. I would still like to work on it and sorry for not updating it for a long time."
    },
    {
      "id": 502939912,
      "user": "abkosar",
      "body": "Hi @NatholBMX \r\n\r\nWhen I run `./scripts/validate_docstrings.py --errors=GL03` I get this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./scripts/validate_docstrings.py\", line 989, in <module>\r\n    args.ignore_deprecated))\r\n  File \"./scripts/validate_docstrings.py\", line 889, in main\r\n    result = validate_all(prefix, ignore_deprecated)\r\n  File \"./scripts/validate_docstrings.py\", line 843, in validate_all\r\n    doc_info = validate_one(func_name)\r\n  File \"./scripts/validate_docstrings.py\", line 799, in validate_one\r\n    errs, wrns, examples_errs = get_validation_data(doc)\r\n  File \"./scripts/validate_docstrings.py\", line 747, in get_validation_data\r\n    if not doc.see_also:\r\n  File \"./scripts/validate_docstrings.py\", line 476, in see_also\r\n    for funcs, desc in self.doc['See Also']:\r\nValueError: too many values to unpack (expected 2)\r\n```\r\n\r\nI synced my fork before doing this.\r\n\r\nIs there anything that I should do differently?"
    },
    {
      "id": 502978624,
      "user": "datapythonista",
      "body": "This is working for me with master. Can you make sure you run `conda env update` in the root of the pandas repo to use the correct versions of everything. Or may be you've got changes in the docstrings that can be causing this?"
    },
    {
      "id": 504213005,
      "user": "abkosar",
      "body": "Hey @datapythonista, thanks for the reply.\r\n\r\nSo I ran `conda env update` and I don't get the error now; however I think there is still something going on. Now when I ran it runs but I get this output instead of the output I posted at the very beginning:\r\n\r\n```\r\n./scripts/validate_docstrings.py:799: PerformanceWarning: indexing past lexsort depth may impact performance.\r\n  errs, wrns, examples_errs = get_validation_data(doc)\r\n./scripts/validate_docstrings.py:1: FutureWarning: Series.to_sparse is deprecated and will be removed in a future version\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:1: FutureWarning: Series.to_sparse is deprecated and will be removed in a future version\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:559: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\r\n  runner.run(test, out=f.write)\r\n./scripts/validate_docstrings.py:1: FutureWarning: the 'box' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'box'\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:1: FutureWarning: DataFrame.ftypes is deprecated and will be removed in a future version. Use DataFrame.dtypes instead.\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:799: PerformanceWarning: indexing past lexsort depth may impact performance.\r\n  errs, wrns, examples_errs = get_validation_data(doc)\r\n/Users/ardkosar/Documents/Personal_Projects/open_source/pandas/pandas/plotting/_matplotlib/converter.py:96: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\r\n\r\nTo register the converters:\r\n        >>> from pandas.plotting import register_matplotlib_converters\r\n        >>> register_matplotlib_converters()\r\n  warnings.warn(msg, FutureWarning)\r\n./scripts/validate_docstrings.py:1: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\r\n  #!/usr/bin/env python\r\n```\r\nI haven't made any changes yet so I don't think that's causing any issues.\r\n\r\nI think I'll start working on the output I got the first time I ran this but I'm just curious why it doesn't give the same output again."
    },
    {
      "id": 504214381,
      "user": "datapythonista",
      "body": "Those are warnings, they should be fixed eventually, but not relevant for what you're doing.\r\n\r\nIf that's all what you get, I think there are no GL03 errors. You can also run:\r\n```\r\n./scripts/validate_docstrings.py --errors=GL03 && echo \"NO ERRORS FOUND\"\r\n```\r\n\r\nIf the message gets printed means that no errors are found indeed (validate_docstrings.py returns an exit code 0 if no errors are found, and the && will make the echo just be executed if that's the case)"
    },
    {
      "id": 504214987,
      "user": "abkosar",
      "body": "Yep that's exactly what happened: \r\n```\r\n./scripts/validate_docstrings.py:799: PerformanceWarning: indexing past lexsort depth may impact performance.\r\n  errs, wrns, examples_errs = get_validation_data(doc)\r\n./scripts/validate_docstrings.py:1: FutureWarning: Series.to_sparse is deprecated and will be removed in a future version\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:1: FutureWarning: Series.to_sparse is deprecated and will be removed in a future version\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:559: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\r\n  runner.run(test, out=f.write)\r\n./scripts/validate_docstrings.py:1: FutureWarning: the 'box' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'box'\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:1: FutureWarning: DataFrame.ftypes is deprecated and will be removed in a future version. Use DataFrame.dtypes instead.\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:799: PerformanceWarning: indexing past lexsort depth may impact performance.\r\n  errs, wrns, examples_errs = get_validation_data(doc)\r\n/Users/ardkosar/Documents/Personal_Projects/open_source/pandas/pandas/plotting/_matplotlib/converter.py:96: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\r\n\r\nTo register the converters:\r\n        >>> from pandas.plotting import register_matplotlib_converters\r\n        >>> register_matplotlib_converters()\r\n  warnings.warn(msg, FutureWarning)\r\n./scripts/validate_docstrings.py:1: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\r\n  #!/usr/bin/env python\r\nNO ERRORS FOUND\r\n```\r\n"
    },
    {
      "id": 504216526,
      "user": "datapythonista",
      "body": "I just saw now that this issue is for `GL03` and that we're already validating those in `ci/code_checks.sh`, so it's normal that you don't get any error.\r\n\r\nClosing this issue, but you can work on GL01 and GL02 (I'd take care of them together, since they are related). There are several errors, if you manage to fix them all (feel free to open a PR just for a subset), then please add the codes in `ci/code_checks.sh` so they start being validated in the CI.\r\n\r\nAny question let me know."
    },
    {
      "id": 504216946,
      "user": "datapythonista",
      "body": "You can get the errors with: `./scripts/validate_docstrings.py --errors=GL01,GL02`, the idea is that:\r\n```python\r\ndef foo():\r\n    \"\"\"This is incorrect.\"\"\"\r\n```\r\nit should be\r\n```python\r\ndef foo():\r\n    \"\"\"\r\n    This is correct.\r\n    \"\"\"\r\n```"
    },
    {
      "id": 504220077,
      "user": "abkosar",
      "body": "Sure, I'd be happy to work on those, thanks!"
    },
    {
      "id": 504319377,
      "user": "datapythonista",
      "body": "@abkosar just realized that #26526 wasn't merged until now, which fixes some of those. You should update your branch (i.e. `git fetch upstream && git merge upstream/master`) before start working on those. Good news is that you'll have many less to fix. :)"
    },
    {
      "id": 504378273,
      "user": "datapythonista",
      "body": "@abkosar I created #26982 for part of the issues, you may want to work in some of the others"
    },
    {
      "id": 504482095,
      "user": "abkosar",
      "body": "@datapythonista Sounds good to me. Should I also create seperate issues for each category like you or an issue for everything I fixed?"
    },
    {
      "id": 504483126,
      "user": "datapythonista",
      "body": "Feel free to do it, but probably it's simpler that you just fix as many cases as you want not in that issue, and open a PR. The reason for creating the issue is that I think there are two many cases for a single PR (and probably a single person), so we can have someone else working on that."
    },
    {
      "id": 504483649,
      "user": "abkosar",
      "body": "Ok sounds good, thanks!"
    },
    {
      "id": 509735605,
      "user": "abkosar",
      "body": "Hey @datapythonista -- So I want to start working on GL01 and GL02 however when I tried to do `./scripts/validate_docstrings.py --errors=GL01,GL02` again I am getting the following error:\r\n\r\n```\r\n(pandas-dev) WKMUS3584619:pandas ardkosar$ ./scripts/validate_docstrings.py --errors=GL01,GL02\r\nTraceback (most recent call last):\r\n  File \"./scripts/validate_docstrings.py\", line 1047, in <module>\r\n    args.ignore_deprecated,\r\n  File \"./scripts/validate_docstrings.py\", line 931, in main\r\n    result = validate_all(prefix, ignore_deprecated)\r\n  File \"./scripts/validate_docstrings.py\", line 877, in validate_all\r\n    api_items += list(get_api_items(f))\r\n  File \"./scripts/validate_docstrings.py\", line 229, in get_api_items\r\n    func = getattr(func, part)\r\nAttributeError: type object 'pandas._libs.interval.Interval' has no attribute 'is_empty'\r\n```\r\nI did the following steps before running the script:\r\n\r\n- `conda env update`\r\n- `git fetch upstream && git merge upstream/master`\r\n\r\nI didn't want to create another issue just for this so wanted to ask here.\r\n\r\nAm I doing something wrong?\r\n\r\nThanks!\r\n"
    },
    {
      "id": 509735922,
      "user": "WillAyd",
      "body": "You need to rebuild the C extensions, so `python setup.py build_ext --inplace`"
    },
    {
      "id": 509742770,
      "user": "abkosar",
      "body": "@WillAyd Thanks, it worked!"
    },
    {
      "id": 511140318,
      "user": "abkosar",
      "body": "Hey @datapythonista -- I have another question.\r\n\r\nSo for example the `validate_docstrings.py` script gives me:\r\n\r\n```\r\npandas.PeriodIndex.day: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.day: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.dayofweek: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.dayofweek: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.dayofyear: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.dayofyear: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.days_in_month: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.days_in_month: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.daysinmonth: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.daysinmonth: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.hour: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.hour: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.minute: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.minute: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.month: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.month: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.quarter: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.quarter: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.second: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.second: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.week: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.week: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.weekday: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.weekday: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.weekofyear: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.weekofyear: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.year: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.year: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\n```\r\n\r\nI thought that I'd find these methods at `pandas.core.indexes.period` in the `PeriodIndex` class. However in that class I can't find these attributes. \r\n\r\nAm I looking at the wrong place?"
    }
  ],
  "text_context": "# DOC: Use only one blank line to separate sections or paragraphs\n\nWhen checking for errors with  `./scripts/validate_docstrings.py`, There is an error that comes up:\r\n\r\n**GL03**\r\n**Use only one blank line to separate sections or paragraphs**\r\n\r\nI started working on this and will create a pull request for this issue shortly to go into more detail on what I fixed.\r\n\r\n@datapythonista \n\nReopening. Most of the issues have been fixed, but still some left, see: https://github.com/pandas-dev/pandas/pull/23871#issuecomment-441274426\n\nHey @datapythonista, I am new to open source and would like to work on this! Just curious if someone is working on it? If not I can give it a shot!\r\n\r\nThanks!\n\nI think this is trickier than it looks, but surely a good first issue, and you can definitely work on it.\r\n\r\nExecuting `./scripts/validate_docstrings.py --errors=GL03` should give you the errors, and should return nothing once you fixed this issue.\r\n\r\nThanks @abkosar \n\nThanks!\n\nSo started working on the issue. When I run `./scripts/validate_docstrings.py --errors=GL03` there are 98 errors:\r\n\r\n- pandas.Series.to_numpy:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.agg: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.aggregate: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.cummax: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.cummin: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.cumprod: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.cumsum: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.kurt: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.mad: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.mean: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.median: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.skew: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.kurtosis: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.compound: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.dayofweek: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.weekday: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.is_month_start: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.is_month_end: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.is_quarter_start: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.is_quarter_end: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.is_year_start: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.is_year_end: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.dt.is_leap_year: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Timedelta: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Timedelta.max: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Timedelta.min: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.SparseArray: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.read_feather: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.core.resample.Resampler.apply:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.resample.Resampler.aggregate:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.resample.Resampler.pipe:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.resample.Resampler.mean:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Panel.cummax: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.cummin: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.cumprod: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.cumsum: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.mean: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.median: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.skew: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.swaplevel: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.plotting.andrews_curves: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.describe_option: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.reset_option: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.get_option: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.set_option: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.melt: Double line break found;   please use only one blank line to separate sections or paragraphs, and do not   leave blank lines at the end of docstrings\r\n- pandas.Index.reindex: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.dayofweek: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.weekday: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.is_month_start:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.is_month_end: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.is_quarter_start:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.is_quarter_end:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.is_year_start:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.is_year_end: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DatetimeIndex.is_leap_year: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.window.Rolling.aggregate:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.window.Expanding.aggregate:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.GroupBy.head: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.GroupBy.mean: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.GroupBy.nth: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.GroupBy.tail: Double   line break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.corr:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.cov:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.diff:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.fillna:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.hist:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.idxmax:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.idxmin:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.mad:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.skew:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.take:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.tshift:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.SeriesGroupBy.nlargest:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.SeriesGroupBy.nsmallest:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.SeriesGroupBy.unique:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.SeriesGroupBy.is_monotonic_increasing:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.SeriesGroupBy.is_monotonic_decreasing:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.core.groupby.DataFrameGroupBy.corrwith:   Double line break found; please use only one blank line to separate sections   or paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.agg: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.aggregate: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.compound: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.cummax: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.cummin: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.cumprod: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.cumsum: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.kurt: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.kurtosis: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.mad: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.mean: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.median: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.skew: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.DataFrame.to_html: Double line   break found; please use only one blank line to separate sections or   paragraphs, and do not leave blank lines at the end of docstrings\r\n- pandas.Series.ptp: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.compound: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.kurt: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.kurtosis: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n- pandas.Panel.mad: Double line break   found; please use only one blank line to separate sections or paragraphs, and   do not leave blank lines at the end of docstrings\r\n\n\nHi @abkosar \r\nHave you mady any progress on this?\n\nHi @NatholBMX \r\n\r\nI recently started working on this again. I would still like to work on it and sorry for not updating it for a long time.\n\nHi @NatholBMX \r\n\r\nWhen I run `./scripts/validate_docstrings.py --errors=GL03` I get this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./scripts/validate_docstrings.py\", line 989, in <module>\r\n    args.ignore_deprecated))\r\n  File \"./scripts/validate_docstrings.py\", line 889, in main\r\n    result = validate_all(prefix, ignore_deprecated)\r\n  File \"./scripts/validate_docstrings.py\", line 843, in validate_all\r\n    doc_info = validate_one(func_name)\r\n  File \"./scripts/validate_docstrings.py\", line 799, in validate_one\r\n    errs, wrns, examples_errs = get_validation_data(doc)\r\n  File \"./scripts/validate_docstrings.py\", line 747, in get_validation_data\r\n    if not doc.see_also:\r\n  File \"./scripts/validate_docstrings.py\", line 476, in see_also\r\n    for funcs, desc in self.doc['See Also']:\r\nValueError: too many values to unpack (expected 2)\r\n```\r\n\r\nI synced my fork before doing this.\r\n\r\nIs there anything that I should do differently?\n\nThis is working for me with master. Can you make sure you run `conda env update` in the root of the pandas repo to use the correct versions of everything. Or may be you've got changes in the docstrings that can be causing this?\n\nHey @datapythonista, thanks for the reply.\r\n\r\nSo I ran `conda env update` and I don't get the error now; however I think there is still something going on. Now when I ran it runs but I get this output instead of the output I posted at the very beginning:\r\n\r\n```\r\n./scripts/validate_docstrings.py:799: PerformanceWarning: indexing past lexsort depth may impact performance.\r\n  errs, wrns, examples_errs = get_validation_data(doc)\r\n./scripts/validate_docstrings.py:1: FutureWarning: Series.to_sparse is deprecated and will be removed in a future version\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:1: FutureWarning: Series.to_sparse is deprecated and will be removed in a future version\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:559: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\r\n  runner.run(test, out=f.write)\r\n./scripts/validate_docstrings.py:1: FutureWarning: the 'box' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'box'\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:1: FutureWarning: DataFrame.ftypes is deprecated and will be removed in a future version. Use DataFrame.dtypes instead.\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:799: PerformanceWarning: indexing past lexsort depth may impact performance.\r\n  errs, wrns, examples_errs = get_validation_data(doc)\r\n/Users/ardkosar/Documents/Personal_Projects/open_source/pandas/pandas/plotting/_matplotlib/converter.py:96: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\r\n\r\nTo register the converters:\r\n        >>> from pandas.plotting import register_matplotlib_converters\r\n        >>> register_matplotlib_converters()\r\n  warnings.warn(msg, FutureWarning)\r\n./scripts/validate_docstrings.py:1: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\r\n  #!/usr/bin/env python\r\n```\r\nI haven't made any changes yet so I don't think that's causing any issues.\r\n\r\nI think I'll start working on the output I got the first time I ran this but I'm just curious why it doesn't give the same output again.\n\nThose are warnings, they should be fixed eventually, but not relevant for what you're doing.\r\n\r\nIf that's all what you get, I think there are no GL03 errors. You can also run:\r\n```\r\n./scripts/validate_docstrings.py --errors=GL03 && echo \"NO ERRORS FOUND\"\r\n```\r\n\r\nIf the message gets printed means that no errors are found indeed (validate_docstrings.py returns an exit code 0 if no errors are found, and the && will make the echo just be executed if that's the case)\n\nYep that's exactly what happened: \r\n```\r\n./scripts/validate_docstrings.py:799: PerformanceWarning: indexing past lexsort depth may impact performance.\r\n  errs, wrns, examples_errs = get_validation_data(doc)\r\n./scripts/validate_docstrings.py:1: FutureWarning: Series.to_sparse is deprecated and will be removed in a future version\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:1: FutureWarning: Series.to_sparse is deprecated and will be removed in a future version\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:559: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\r\n  runner.run(test, out=f.write)\r\n./scripts/validate_docstrings.py:1: FutureWarning: the 'box' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'box'\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:1: FutureWarning: DataFrame.ftypes is deprecated and will be removed in a future version. Use DataFrame.dtypes instead.\r\n  #!/usr/bin/env python\r\n./scripts/validate_docstrings.py:799: PerformanceWarning: indexing past lexsort depth may impact performance.\r\n  errs, wrns, examples_errs = get_validation_data(doc)\r\n/Users/ardkosar/Documents/Personal_Projects/open_source/pandas/pandas/plotting/_matplotlib/converter.py:96: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\r\n\r\nTo register the converters:\r\n        >>> from pandas.plotting import register_matplotlib_converters\r\n        >>> register_matplotlib_converters()\r\n  warnings.warn(msg, FutureWarning)\r\n./scripts/validate_docstrings.py:1: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\r\n  #!/usr/bin/env python\r\nNO ERRORS FOUND\r\n```\r\n\n\nI just saw now that this issue is for `GL03` and that we're already validating those in `ci/code_checks.sh`, so it's normal that you don't get any error.\r\n\r\nClosing this issue, but you can work on GL01 and GL02 (I'd take care of them together, since they are related). There are several errors, if you manage to fix them all (feel free to open a PR just for a subset), then please add the codes in `ci/code_checks.sh` so they start being validated in the CI.\r\n\r\nAny question let me know.\n\nYou can get the errors with: `./scripts/validate_docstrings.py --errors=GL01,GL02`, the idea is that:\r\n```python\r\ndef foo():\r\n    \"\"\"This is incorrect.\"\"\"\r\n```\r\nit should be\r\n```python\r\ndef foo():\r\n    \"\"\"\r\n    This is correct.\r\n    \"\"\"\r\n```\n\nSure, I'd be happy to work on those, thanks!\n\n@abkosar just realized that #26526 wasn't merged until now, which fixes some of those. You should update your branch (i.e. `git fetch upstream && git merge upstream/master`) before start working on those. Good news is that you'll have many less to fix. :)\n\n@abkosar I created #26982 for part of the issues, you may want to work in some of the others\n\n@datapythonista Sounds good to me. Should I also create seperate issues for each category like you or an issue for everything I fixed?\n\nFeel free to do it, but probably it's simpler that you just fix as many cases as you want not in that issue, and open a PR. The reason for creating the issue is that I think there are two many cases for a single PR (and probably a single person), so we can have someone else working on that.\n\nOk sounds good, thanks!\n\nHey @datapythonista -- So I want to start working on GL01 and GL02 however when I tried to do `./scripts/validate_docstrings.py --errors=GL01,GL02` again I am getting the following error:\r\n\r\n```\r\n(pandas-dev) WKMUS3584619:pandas ardkosar$ ./scripts/validate_docstrings.py --errors=GL01,GL02\r\nTraceback (most recent call last):\r\n  File \"./scripts/validate_docstrings.py\", line 1047, in <module>\r\n    args.ignore_deprecated,\r\n  File \"./scripts/validate_docstrings.py\", line 931, in main\r\n    result = validate_all(prefix, ignore_deprecated)\r\n  File \"./scripts/validate_docstrings.py\", line 877, in validate_all\r\n    api_items += list(get_api_items(f))\r\n  File \"./scripts/validate_docstrings.py\", line 229, in get_api_items\r\n    func = getattr(func, part)\r\nAttributeError: type object 'pandas._libs.interval.Interval' has no attribute 'is_empty'\r\n```\r\nI did the following steps before running the script:\r\n\r\n- `conda env update`\r\n- `git fetch upstream && git merge upstream/master`\r\n\r\nI didn't want to create another issue just for this so wanted to ask here.\r\n\r\nAm I doing something wrong?\r\n\r\nThanks!\r\n\n\nYou need to rebuild the C extensions, so `python setup.py build_ext --inplace`\n\n@WillAyd Thanks, it worked!\n\nHey @datapythonista -- I have another question.\r\n\r\nSo for example the `validate_docstrings.py` script gives me:\r\n\r\n```\r\npandas.PeriodIndex.day: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.day: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.dayofweek: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.dayofweek: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.dayofyear: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.dayofyear: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.days_in_month: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.days_in_month: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.daysinmonth: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.daysinmonth: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.hour: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.hour: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.minute: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.minute: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.month: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.month: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.quarter: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.quarter: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.second: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.second: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.week: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.week: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.weekday: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.weekday: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.weekofyear: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.weekofyear: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\npandas.PeriodIndex.year: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)\r\npandas.PeriodIndex.year: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)\r\n```\r\n\r\nI thought that I'd find these methods at `pandas.core.indexes.period` in the `PeriodIndex` class. However in that class I can't find these attributes. \r\n\r\nAm I looking at the wrong place?",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/23871",
  "code_context": [
    {
      "filename": "pandas/core/algorithms.py",
      "content": "\"\"\"\nGeneric data algorithms. This module is experimental at the moment and not\nintended for public consumption\n\"\"\"\nfrom __future__ import division\n\nfrom textwrap import dedent\nfrom warnings import catch_warnings, simplefilter, warn\n\nimport numpy as np\n\nfrom pandas._libs import algos, hashtable as htable, lib\nfrom pandas._libs.tslib import iNaT\nfrom pandas.util._decorators import Appender, Substitution, deprecate_kwarg\n\nfrom pandas.core.dtypes.cast import (\n    construct_1d_object_array_from_listlike, maybe_promote)\nfrom pandas.core.dtypes.common import (\n    ensure_float64, ensure_int64, ensure_object, ensure_platform_int,\n    ensure_uint64, is_array_like, is_bool_dtype, is_categorical_dtype,\n    is_complex_dtype, is_datetime64_any_dtype, is_datetime64tz_dtype,\n    is_datetimelike, is_datetimetz, is_extension_array_dtype, is_float_dtype,\n    is_integer_dtype, is_interval_dtype, is_list_like, is_numeric_dtype,\n    is_object_dtype, is_period_dtype, is_scalar, is_signed_integer_dtype,\n    is_sparse, is_timedelta64_dtype, is_unsigned_integer_dtype,\n    needs_i8_conversion)\nfrom pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries\nfrom pandas.core.dtypes.missing import isna, na_value_for_dtype\n\nfrom pandas.core import common as com\n\n_shared_docs = {}\n\n\n# --------------- #\n# dtype access    #\n# --------------- #\ndef _ensure_data(values, dtype=None):\n    \"\"\"\n    routine to ensure that our data is of the correct\n    input dtype for lower-level routines\n\n    This will coerce:\n    - ints -> int64\n    - uint -> uint64\n    - bool -> uint64 (TODO this should be uint8)\n    - datetimelike -> i8\n    - datetime64tz -> i8 (in local tz)\n    - categorical -> codes\n\n    Parameters\n    ----------\n    values : array-like\n    dtype : pandas_dtype, optional\n        coerce to this dtype\n\n    Returns\n    -------\n    (ndarray, pandas_dtype, algo dtype as a string)\n\n    \"\"\"\n\n    # we check some simple dtypes first\n    try:\n        if is_object_dtype(dtype):\n            return ensure_object(np.asarray(values)), 'object', 'object'\n        if is_bool_dtype(values) or is_bool_dtype(dtype):\n            # we are actually coercing to uint64\n            # until our algos support uint8 directly (see TODO)\n            return np.asarray(values).astype('uint64'), 'bool', 'uint64'\n        elif is_signed_integer_dtype(values) or is_signed_integer_dtype(dtype):\n            return ensure_int64(values), 'int64', 'int64'\n        elif (is_unsigned_integer_dtype(values) or\n              is_unsigned_integer_dtype(dtype)):\n            return ensure_uint64(values), 'uint64', 'uint64'\n        elif is_float_dtype(values) or is_float_dtype(dtype):\n            return ensure_float64(values), 'float64', 'float64'\n        elif is_object_dtype(values) and dtype is None:\n            return ensure_object(np.asarray(values)), 'object', 'object'\n        elif is_complex_dtype(values) or is_complex_dtype(dtype):\n\n            # ignore the fact that we are casting to float\n            # which discards complex parts\n            with catch_warnings():\n                simplefilter(\"ignore\", np.ComplexWarning)\n                values = ensure_float64(values)\n            return values, 'float64', 'float64'\n\n    except (TypeError, ValueError, OverflowError):\n        # if we are trying to coerce to a dtype\n        # and it is incompat this will fall thru to here\n        return ensure_object(values), 'object', 'object'\n\n    # datetimelike\n    if (needs_i8_conversion(values) or\n            is_period_dtype(dtype) or\n            is_datetime64_any_dtype(dtype) or\n            is_timedelta64_dtype(dtype)):\n        if is_period_dtype(values) or is_period_dtype(dtype):\n            from pandas import PeriodIndex\n            values = PeriodIndex(values)\n            dtype = values.dtype\n        elif is_timedelta64_dtype(values) or is_timedelta64_dtype(dtype):\n            from pandas import TimedeltaIndex\n            values = TimedeltaIndex(values)\n            dtype = values.dtype\n        else:\n            # Datetime\n            from pandas import DatetimeIndex\n            values = DatetimeIndex(values)\n            dtype = values.dtype\n\n        return values.asi8, dtype, 'int64'\n\n    elif (is_categorical_dtype(values) and\n          (is_categorical_dtype(dtype) or dtype is None)):\n        values = getattr(values, 'values', values)\n        values = values.codes\n        dtype = 'category'\n\n        # we are actually coercing to int64\n        # until our algos support int* directly (not all do)\n        values = ensure_int64(values)\n\n        return values, dtype, 'int64'\n\n    # we have failed, return object\n    values = np.asarray(values, dtype=np.object)\n    return ensure_object(values), 'object', 'object'\n\n\ndef _reconstruct_data(values, dtype, original):\n    \"\"\"\n    reverse of _ensure_data\n\n    Parameters\n    ----------\n    values : ndarray\n    dtype : pandas_dtype\n    original : ndarray-like\n\n    Returns\n    -------\n    Index for extension types, otherwise ndarray casted to dtype\n    \"\"\"\n    from pandas import Index\n    if is_extension_array_dtype(dtype):\n        values = dtype.construct_array_type()._from_sequence(values)\n    elif is_datetime64tz_dtype(dtype) or is_period_dtype(dtype):\n        values = Index(original)._shallow_copy(values, name=None)\n    elif is_bool_dtype(dtype):\n        values = values.astype(dtype)\n\n        # we only support object dtypes bool Index\n        if isinstance(original, Index):\n            values = values.astype(object)\n    elif dtype is not None:\n        values = values.astype(dtype)\n\n    return values\n\n\ndef _ensure_arraylike(values):\n    \"\"\"\n    ensure that we are arraylike if not already\n    \"\"\"\n    if not is_array_like(values):\n        inferred = lib.infer_dtype(values)\n        if inferred in ['mixed', 'string', 'unicode']:\n            if isinstance(values, tuple):\n                values = list(values)\n            values = construct_1d_object_array_from_listlike(values)\n        else:\n            values = np.asarray(values)\n    return values\n\n\n_hashtables = {\n    'float64': (htable.Float64HashTable, htable.Float64Vector),\n    'uint64': (htable.UInt64HashTable, htable.UInt64Vector),\n    'int64': (htable.Int64HashTable, htable.Int64Vector),\n    'string': (htable.StringHashTable, htable.ObjectVector),\n    'object': (htable.PyObjectHashTable, htable.ObjectVector)\n}\n\n\ndef _get_hashtable_algo(values):\n    \"\"\"\n    Parameters\n    ----------\n    values : arraylike\n\n    Returns\n    -------\n    tuples(hashtable class,\n           vector class,\n           values,\n           dtype,\n           ndtype)\n    \"\"\"\n    values, dtype, ndtype = _ensure_data(values)\n\n    if ndtype == 'object':\n\n        # its cheaper to use a String Hash Table than Object\n        if lib.infer_dtype(values) in ['string']:\n            ndtype = 'string'\n        else:\n            ndtype = 'object'\n\n    htable, table = _hashtables[ndtype]\n    return (htable, table, values, dtype, ndtype)\n\n\ndef _get_data_algo(values, func_map):\n\n    if is_categorical_dtype(values):\n        values = values._values_for_rank()\n\n    values, dtype, ndtype = _ensure_data(values)\n    if ndtype == 'object':\n\n        # its cheaper to use a String Hash Table than Object\n        if lib.infer_dtype(values) in ['string']:\n            ndtype = 'string'\n\n    f = func_map.get(ndtype, func_map['object'])\n\n    return f, values\n\n\n# --------------- #\n# top-level algos #\n# --------------- #\n\ndef match(to_match, values, na_sentinel=-1):\n    \"\"\"\n    Compute locations of to_match into values\n\n    Parameters\n    ----------\n    to_match : array-like\n        values to find positions of\n    values : array-like\n        Unique set of values\n    na_sentinel : int, default -1\n        Value to mark \"not found\"\n\n    Examples\n    --------\n\n    Returns\n    -------\n    match : ndarray of integers\n    \"\"\"\n    values = com.asarray_tuplesafe(values)\n    htable, _, values, dtype, ndtype = _get_hashtable_algo(values)\n    to_match, _, _ = _ensure_data(to_match, dtype)\n    table = htable(min(len(to_match), 1000000))\n    table.map_locations(values)\n    result = table.lookup(to_match)\n\n    if na_sentinel != -1:\n\n        # replace but return a numpy array\n        # use a Series because it handles dtype conversions properly\n        from pandas import Series\n        result = Series(result.ravel()).replace(-1, na_sentinel)\n        result = result.values.reshape(result.shape)\n\n    return result\n\n\ndef unique(values):\n    \"\"\"\n    Hash table-based unique. Uniques are returned in order\n    of appearance. This does NOT sort.\n\n    Significantly faster than numpy.unique. Includes NA values.\n\n    Parameters\n    ----------\n    values : 1d array-like\n\n    Returns\n    -------\n    unique values.\n      - If the input is an Index, the return is an Index\n      - If the input is a Categorical dtype, the return is a Categorical\n      - If the input is a Series/ndarray, the return will be an ndarray\n\n    Examples\n    --------\n    >>> pd.unique(pd.Series([2, 1, 3, 3]))\n    array([2, 1, 3])\n\n    >>> pd.unique(pd.Series([2] + [1] * 5))\n    array([2, 1])\n\n    >>> pd.unique(pd.Series([pd.Timestamp('20160101'),\n    ...                     pd.Timestamp('20160101')]))\n    array(['2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\n\n    >>> pd.unique(pd.Series([pd.Timestamp('20160101', tz='US/Eastern'),\n    ...                      pd.Timestamp('20160101', tz='US/Eastern')]))\n    array([Timestamp('2016-01-01 00:00:00-0500', tz='US/Eastern')],\n          dtype=object)\n\n    >>> pd.unique(pd.Index([pd.Timestamp('20160101', tz='US/Eastern'),\n    ...                     pd.Timestamp('20160101', tz='US/Eastern')]))\n    DatetimeIndex(['2016-01-01 00:00:00-05:00'],\n    ...           dtype='datetime64[ns, US/Eastern]', freq=None)\n\n    >>> pd.unique(list('baabc'))\n    array(['b', 'a', 'c'], dtype=object)\n\n    An unordered Categorical will return categories in the\n    order of appearance.\n\n    >>> pd.unique(pd.Series(pd.Categorical(list('baabc'))))\n    [b, a, c]\n    Categories (3, object): [b, a, c]\n\n    >>> pd.unique(pd.Series(pd.Categorical(list('baabc'),\n    ...                                    categories=list('abc'))))\n    [b, a, c]\n    Categories (3, object): [b, a, c]\n\n    An ordered Categorical preserves the category ordering.\n\n    >>> pd.unique(pd.Series(pd.Categorical(list('baabc'),\n    ...                                    categories=list('abc'),\n    ...                                    ordered=True)))\n    [b, a, c]\n    Categories (3, object): [a < b < c]\n\n    An array of tuples\n\n    >>> pd.unique([('a', 'b'), ('b', 'a'), ('a', 'c'), ('b', 'a')])\n    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\n\n    See Also\n    --------\n    pandas.Index.unique\n    pandas.Series.unique\n    \"\"\"\n\n    values = _ensure_arraylike(values)\n\n    if is_extension_array_dtype(values):\n        # Dispatch to extension dtype's unique.\n        return values.unique()\n\n    original = values\n    htable, _, values, dtype, ndtype = _get_hashtable_algo(values)\n\n    table = htable(len(values))\n    uniques = table.unique(values)\n    uniques = _reconstruct_data(uniques, dtype, original)\n\n    if isinstance(original, ABCSeries) and is_datetime64tz_dtype(dtype):\n        # we are special casing datetime64tz_dtype\n        # to return an object array of tz-aware Timestamps\n\n        # TODO: it must return DatetimeArray with tz in pandas 2.0\n        uniques = uniques.astype(object).values\n\n    return uniques\n\n\nunique1d = unique\n\n\ndef isin(comps, values):\n    \"\"\"\n    Compute the isin boolean array\n\n    Parameters\n    ----------\n    comps : array-like\n    values : array-like\n\n    Returns\n    -------\n    boolean array same length as comps\n    \"\"\"\n\n    if not is_list_like(comps):\n        raise TypeError(\"only list-like objects are allowed to be passed\"\n                        \" to isin(), you passed a [{comps_type}]\"\n                        .format(comps_type=type(comps).__name__))\n    if not is_list_like(values):\n        raise TypeError(\"only list-like objects are allowed to be passed\"\n                        \" to isin(), you passed a [{values_type}]\"\n                        .format(values_type=type(values).__name__))\n\n    if not isinstance(values, (ABCIndex, ABCSeries, np.ndarray)):\n        values = construct_1d_object_array_from_listlike(list(values))\n\n    if is_categorical_dtype(comps):\n        # TODO(extension)\n        # handle categoricals\n        return comps._values.isin(values)\n\n    comps = com.values_from_object(comps)\n\n    comps, dtype, _ = _ensure_data(comps)\n    values, _, _ = _ensure_data(values, dtype=dtype)\n\n    # faster for larger cases to use np.in1d\n    f = lambda x, y: htable.ismember_object(x, values)\n\n    # GH16012\n    # Ensure np.in1d doesn't get object types or it *may* throw an exception\n    if len(comps) > 1000000 and not is_object_dtype(comps):\n        f = lambda x, y: np.in1d(x, y)\n    elif is_integer_dtype(comps):\n        try:\n            values = values.astype('int64', copy=False)\n            comps = comps.astype('int64', copy=False)\n            f = lambda x, y: htable.ismember_int64(x, y)\n        except (TypeError, ValueError, OverflowError):\n            values = values.astype(object)\n            comps = comps.astype(object)\n\n    elif is_float_dtype(comps):\n        try:\n            values = values.astype('float64', copy=False)\n            comps = comps.astype('float64', copy=False)\n            f = lambda x, y: htable.ismember_float64(x, y)\n        except (TypeError, ValueError):\n            values = values.astype(object)\n            comps = comps.astype(object)\n\n    return f(comps, values)\n\n\ndef _factorize_array(values, na_sentinel=-1, size_hint=None,\n                     na_value=None):\n    \"\"\"Factorize an array-like to labels and uniques.\n\n    This doesn't do any coercion of types or unboxing before factorization.\n\n    Parameters\n    ----------\n    values : ndarray\n    na_sentinel : int, default -1\n    size_hint : int, optional\n        Passsed through to the hashtable's 'get_labels' method\n    na_value : object, optional\n        A value in `values` to consider missing. Note: only use this\n        parameter when you know that you don't have any values pandas would\n        consider missing in the array (NaN for float data, iNaT for\n        datetimes, etc.).\n\n    Returns\n    -------\n    labels, uniques : ndarray\n    \"\"\"\n    (hash_klass, _), values = _get_data_algo(values, _hashtables)\n\n    table = hash_klass(size_hint or len(values))\n    labels, uniques = table.factorize(values, na_sentinel=na_sentinel,\n                                      na_value=na_value)\n\n    labels = ensure_platform_int(labels)\n    return labels, uniques\n\n\n_shared_docs['factorize'] = \"\"\"\n    Encode the object as an enumerated type or categorical variable.\n\n    This method is useful for obtaining a numeric representation of an\n    array when all that matters is identifying distinct values. `factorize`\n    is available as both a top-level function :func:`pandas.factorize`,\n    and as a method :meth:`Series.factorize` and :meth:`Index.factorize`.\n\n    Parameters\n    ----------\n    %(values)s%(sort)s%(order)s\n    na_sentinel : int, default -1\n        Value to mark \"not found\".\n    %(size_hint)s\\\n\n    Returns\n    -------\n    labels : ndarray\n        An integer ndarray that's an indexer into `uniques`.\n        ``uniques.take(labels)`` will have the same values as `values`.\n    uniques : ndarray, Index, or Categorical\n        The unique valid values. When `values` is Categorical, `uniques`\n        is a Categorical. When `values` is some other pandas object, an\n        `Index` is returned. Otherwise, a 1-D ndarray is returned.\n\n        .. note ::\n\n           Even if there's a missing value in `values`, `uniques` will\n           *not* contain an entry for it.\n\n    See Also\n    --------\n    cut : Discretize continuous-valued array.\n    unique : Find the unique value in an array.\n\n    Examples\n    --------\n    These examples all show factorize as a top-level method like\n    ``pd.factorize(values)``. The results are identical for methods like\n    :meth:`Series.factorize`.\n\n    >>> labels, uniques = pd.factorize(['b', 'b', 'a', 'c', 'b'])\n    >>> labels\n    array([0, 0, 1, 2, 0])\n    >>> uniques\n    array(['b', 'a', 'c'], dtype=object)\n\n    With ``sort=True``, the `uniques` will be sorted, and `labels` will be\n    shuffled so that the relationship is the maintained.\n\n    >>> labels, uniques = pd.factorize(['b', 'b', 'a', 'c', 'b'], sort=True)\n    >>> labels\n    array([1, 1, 0, 2, 1])\n    >>> uniques\n    array(['a', 'b', 'c'], dtype=object)\n\n    Missing values are indicated in `labels` with `na_sentinel`\n    (``-1`` by default). Note that missing values are never\n    included in `uniques`.\n\n    >>> labels, uniques = pd.factorize(['b', None, 'a', 'c', 'b'])\n    >>> labels\n    array([ 0, -1,  1,  2,  0])\n    >>> uniques\n    array(['b', 'a', 'c'], dtype=object)\n\n    Thus far, we've only factorized lists (which are internally coerced to\n    NumPy arrays). When factorizing pandas objects, the type of `uniques`\n    will differ. For Categoricals, a `Categorical` is returned.\n\n    >>> cat = pd.Categorical(['a', 'a', 'c'], categories=['a', 'b', 'c'])\n    >>> labels, uniques = pd.factorize(cat)\n    >>> labels\n    array([0, 0, 1])\n    >>> uniques\n    [a, c]\n    Categories (3, object): [a, b, c]\n\n    Notice that ``'b'`` is in ``uniques.categories``, despite not being\n    present in ``cat.values``.\n\n    For all other pandas objects, an Index of the appropriate type is\n    returned.\n\n    >>> cat = pd.Series(['a', 'a', 'c'])\n    >>> labels, uniques = pd.factorize(cat)\n    >>> labels\n    array([0, 0, 1])\n    >>> uniques\n    Index(['a', 'c'], dtype='object')\n    \"\"\"\n\n\n@Substitution(\n    values=dedent(\"\"\"\\\n    values : sequence\n        A 1-D sequence. Sequences that aren't pandas objects are\n        coerced to ndarrays before factorization.\n    \"\"\"),\n    order=dedent(\"\"\"\\\n    order\n        .. deprecated:: 0.23.0\n\n           This parameter has no effect and is deprecated.\n    \"\"\"),\n    sort=dedent(\"\"\"\\\n    sort : bool, default False\n        Sort `uniques` and shuffle `labels` to maintain the\n        relationship.\n    \"\"\"),\n    size_hint=dedent(\"\"\"\\\n    size_hint : int, optional\n        Hint to the hashtable sizer.\n    \"\"\"),\n)\n@Appender(_shared_docs['factorize'])\n@deprecate_kwarg(old_arg_name='order', new_arg_name=None)\ndef factorize(values, sort=False, order=None, na_sentinel=-1, size_hint=None):\n    # Implementation notes: This method is responsible for 3 things\n    # 1.) coercing data to array-like (ndarray, Index, extension array)\n    # 2.) factorizing labels and uniques\n    # 3.) Maybe boxing the output in an Index\n    #\n    # Step 2 is dispatched to extension types (like Categorical). They are\n    # responsible only for factorization. All data coercion, sorting and boxing\n    # should happen here.\n\n    values = _ensure_arraylike(values)\n    original = values\n\n    if is_extension_array_dtype(values):\n        values = getattr(values, '_values', values)\n        labels, uniques = values.factorize(na_sentinel=na_sentinel)\n        dtype = original.dtype\n    else:\n        values, dtype, _ = _ensure_data(values)\n\n        if (is_datetime64_any_dtype(original) or\n                is_timedelta64_dtype(original) or\n                is_period_dtype(original)):\n            na_value = na_value_for_dtype(original.dtype)\n        else:\n            na_value = None\n\n        labels, uniques = _factorize_array(values,\n                                           na_sentinel=na_sentinel,\n                                           size_hint=size_hint,\n                                           na_value=na_value)\n\n    if sort and len(uniques) > 0:\n        from pandas.core.sorting import safe_sort\n        try:\n            order = uniques.argsort()\n            order2 = order.argsort()\n            labels = take_1d(order2, labels, fill_value=na_sentinel)\n            uniques = uniques.take(order)\n        except TypeError:\n            # Mixed types, where uniques.argsort fails.\n            uniques, labels = safe_sort(uniques, labels,\n                                        na_sentinel=na_sentinel,\n                                        assume_unique=True)\n\n    uniques = _reconstruct_data(uniques, dtype, original)\n\n    # return original tenor\n    if isinstance(original, ABCIndexClass):\n        uniques = original._shallow_copy(uniques, name=None)\n    elif isinstance(original, ABCSeries):\n        from pandas import Index\n        uniques = Index(uniques)\n\n    return labels, uniques\n\n\ndef value_counts(values, sort=True, ascending=False, normalize=False,\n                 bins=None, dropna=True):\n    \"\"\"\n    Compute a histogram of the counts of non-null values.\n\n    Parameters\n    ----------\n    values : ndarray (1-d)\n    sort : boolean, default True\n        Sort by values\n    ascending : boolean, default False\n        Sort in ascending order\n    normalize: boolean, default False\n        If True then compute a relative histogram\n    bins : integer, optional\n        Rather than count values, group them into half-open bins,\n        convenience for pd.cut, only works with numeric data\n    dropna : boolean, default True\n        Don't include counts of NaN\n\n    Returns\n    -------\n    value_counts : Series\n\n    \"\"\"\n    from pandas.core.series import Series, Index\n    name = getattr(values, 'name', None)\n\n    if bins is not None:\n        try:\n            from pandas.core.reshape.tile import cut\n            values = Series(values)\n            ii = cut(values, bins, include_lowest=True)\n        except TypeError:\n            raise TypeError(\"bins argument only works with numeric data.\")\n\n        # count, remove nulls (from the index), and but the bins\n        result = ii.value_counts(dropna=dropna)\n        result = result[result.index.notna()]\n        result.index = result.index.astype('interval')\n        result = result.sort_index()\n\n        # if we are dropna and we have NO values\n        if dropna and (result.values == 0).all():\n            result = result.iloc[0:0]\n\n        # normalizing is by len of all (regardless of dropna)\n        counts = np.array([len(ii)])\n\n    else:\n\n        if is_extension_array_dtype(values) or is_sparse(values):\n\n            # handle Categorical and sparse,\n            result = Series(values)._values.value_counts(dropna=dropna)\n            result.name = name\n            counts = result.values\n\n        else:\n            keys, counts = _value_counts_arraylike(values, dropna)\n\n            if not isinstance(keys, Index):\n                keys = Index(keys)\n            result = Series(counts, index=keys, name=name)\n\n    if sort:\n        result = result.sort_values(ascending=ascending)\n\n    if normalize:\n        result = result / float(counts.sum())\n\n    return result\n\n\ndef _value_counts_arraylike(values, dropna):\n    \"\"\"\n    Parameters\n    ----------\n    values : arraylike\n    dropna : boolean\n\n    Returns\n    -------\n    (uniques, counts)\n\n    \"\"\"\n    values = _ensure_arraylike(values)\n    original = values\n    values, dtype, ndtype = _ensure_data(values)\n\n    if needs_i8_conversion(dtype):\n        # i8\n\n        keys, counts = htable.value_count_int64(values, dropna)\n\n        if dropna:\n            msk = keys != iNaT\n            keys, counts = keys[msk], counts[msk]\n\n    else:\n        # ndarray like\n\n        # TODO: handle uint8\n        f = getattr(htable, \"value_count_{dtype}\".format(dtype=ndtype))\n        keys, counts = f(values, dropna)\n\n        mask = isna(values)\n        if not dropna and mask.any():\n            if not isna(keys).any():\n                keys = np.insert(keys, 0, np.NaN)\n                counts = np.insert(counts, 0, mask.sum())\n\n    keys = _reconstruct_data(keys, original.dtype, original)\n\n    return keys, counts\n\n\ndef duplicated(values, keep='first'):\n    \"\"\"\n    Return boolean ndarray denoting duplicate values.\n\n    .. versionadded:: 0.19.0\n\n    Parameters\n    ----------\n    values : ndarray-like\n        Array over which to check for duplicate values.\n    keep : {'first', 'last', False}, default 'first'\n        - ``first`` : Mark duplicates as ``True`` except for the first\n          occurrence.\n        - ``last`` : Mark duplicates as ``True`` except for the last\n          occurrence.\n        - False : Mark all duplicates as ``True``.\n\n    Returns\n    -------\n    duplicated : ndarray\n    \"\"\"\n\n    values, dtype, ndtype = _ensure_data(values)\n    f = getattr(htable, \"duplicated_{dtype}\".format(dtype=ndtype))\n    return f(values, keep=keep)\n\n\ndef mode(values, dropna=True):\n    \"\"\"\n    Returns the mode(s) of an array.\n\n    Parameters\n    ----------\n    values : array-like\n        Array over which to check for duplicate values.\n    dropna : boolean, default True\n        Don't consider counts of NaN/NaT.\n\n        .. versionadded:: 0.24.0\n\n    Returns\n    -------\n    mode : Series\n    \"\"\"\n    from pandas import Series\n\n    values = _ensure_arraylike(values)\n    original = values\n\n    # categorical is a fast-path\n    if is_categorical_dtype(values):\n        if isinstance(values, Series):\n            return Series(values.values.mode(dropna=dropna), name=values.name)\n        return values.mode(dropna=dropna)\n\n    if dropna and is_datetimelike(values):\n        mask = values.isnull()\n        values = values[~mask]\n\n    values, dtype, ndtype = _ensure_data(values)\n\n    f = getattr(htable, \"mode_{dtype}\".format(dtype=ndtype))\n    result = f(values, dropna=dropna)\n    try:\n        result = np.sort(result)\n    except TypeError as e:\n        warn(\"Unable to sort modes: {error}\".format(error=e))\n\n    result = _reconstruct_data(result, original.dtype, original)\n    return Series(result)\n\n\ndef rank(values, axis=0, method='average', na_option='keep',\n         ascending=True, pct=False):\n    \"\"\"\n    Rank the values along a given axis.\n\n    Parameters\n    ----------\n    values : array-like\n        Array whose values will be ranked. The number of dimensions in this\n        array must not exceed 2.\n    axis : int, default 0\n        Axis over which to perform rankings.\n    method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n        The method by which tiebreaks are broken during the ranking.\n    na_option : {'keep', 'top'}, default 'keep'\n        The method by which NaNs are placed in the ranking.\n        - ``keep``: rank each NaN value with a NaN ranking\n        - ``top``: replace each NaN with either +/- inf so that they\n                   there are ranked at the top\n    ascending : boolean, default True\n        Whether or not the elements should be ranked in ascending order.\n    pct : boolean, default False\n        Whether or not to the display the returned rankings in integer form\n        (e.g. 1, 2, 3) or in percentile form (e.g. 0.333..., 0.666..., 1).\n    \"\"\"\n    if values.ndim == 1:\n        f, values = _get_data_algo(values, _rank1d_functions)\n        ranks = f(values, ties_method=method, ascending=ascending,\n                  na_option=na_option, pct=pct)\n    elif values.ndim == 2:\n        f, values = _get_data_algo(values, _rank2d_functions)\n        ranks = f(values, axis=axis, ties_method=method,\n                  ascending=ascending, na_option=na_option, pct=pct)\n    else:\n        raise TypeError(\"Array with ndim > 2 are not supported.\")\n\n    return ranks\n\n\ndef checked_add_with_arr(arr, b, arr_mask=None, b_mask=None):\n    \"\"\"\n    Perform array addition that checks for underflow and overflow.\n\n    Performs the addition of an int64 array and an int64 integer (or array)\n    but checks that they do not result in overflow first. For elements that\n    are indicated to be NaN, whether or not there is overflow for that element\n    is automatically ignored.\n\n    Parameters\n    ----------\n    arr : array addend.\n    b : array or scalar addend.\n    arr_mask : boolean array or None\n        array indicating which elements to exclude from checking\n    b_mask : boolean array or boolean or None\n        array or scalar indicating which element(s) to exclude from checking\n\n    Returns\n    -------\n    sum : An array for elements x + b for each element x in arr if b is\n          a scalar or an array for elements x + y for each element pair\n          (x, y) in (arr, b).\n\n    Raises\n    ------\n    OverflowError if any x + y exceeds the maximum or minimum int64 value.\n    \"\"\"\n    # For performance reasons, we broadcast 'b' to the new array 'b2'\n    # so that it has the same size as 'arr'.\n    b2 = np.broadcast_to(b, arr.shape)\n    if b_mask is not None:\n        # We do the same broadcasting for b_mask as well.\n        b2_mask = np.broadcast_to(b_mask, arr.shape)\n    else:\n        b2_mask = None\n\n    # For elements that are NaN, regardless of their value, we should\n    # ignore whether they overflow or not when doing the checked add.\n    if arr_mask is not None and b2_mask is not None:\n        not_nan = np.logical_not(arr_mask | b2_mask)\n    elif arr_mask is not None:\n        not_nan = np.logical_not(arr_mask)\n    elif b_mask is not None:\n        not_nan = np.logical_not(b2_mask)\n    else:\n        not_nan = np.empty(arr.shape, dtype=bool)\n        not_nan.fill(True)\n\n    # gh-14324: For each element in 'arr' and its corresponding element\n    # in 'b2', we check the sign of the element in 'b2'. If it is positive,\n    # we then check whether its sum with the element in 'arr' exceeds\n    # np.iinfo(np.int64).max. If so, we have an overflow error. If it\n    # it is negative, we then check whether its sum with the element in\n    # 'arr' exceeds np.iinfo(np.int64).min. If so, we have an overflow\n    # error as well.\n    mask1 = b2 > 0\n    mask2 = b2 < 0\n\n    if not mask1.any():\n        to_raise = ((np.iinfo(np.int64).min - b2 > arr) & not_nan).any()\n    elif not mask2.any():\n        to_raise = ((np.iinfo(np.int64).max - b2 < arr) & not_nan).any()\n    else:\n        to_raise = (((np.iinfo(np.int64).max -\n                      b2[mask1] < arr[mask1]) & not_nan[mask1]).any() or\n                    ((np.iinfo(np.int64).min -\n                      b2[mask2] > arr[mask2]) & not_nan[mask2]).any())\n\n    if to_raise:\n        raise OverflowError(\"Overflow in int64 addition\")\n    return arr + b\n\n\n_rank1d_functions = {\n    'float64': algos.rank_1d_float64,\n    'int64': algos.rank_1d_int64,\n    'uint64': algos.rank_1d_uint64,\n    'object': algos.rank_1d_object\n}\n\n_rank2d_functions = {\n    'float64': algos.rank_2d_float64,\n    'int64': algos.rank_2d_int64,\n    'uint64': algos.rank_2d_uint64,\n    'object': algos.rank_2d_object\n}\n\n\ndef quantile(x, q, interpolation_method='fraction'):\n    \"\"\"\n    Compute sample quantile or quantiles of the input array. For example, q=0.5\n    computes the median.\n\n    The `interpolation_method` parameter supports three values, namely\n    `fraction` (default), `lower` and `higher`. Interpolation is done only,\n    if the desired quantile lies between two data points `i` and `j`. For\n    `fraction`, the result is an interpolated value between `i` and `j`;\n    for `lower`, the result is `i`, for `higher` the result is `j`.\n\n    Parameters\n    ----------\n    x : ndarray\n        Values from which to extract score.\n    q : scalar or array\n        Percentile at which to extract score.\n    interpolation_method : {'fraction', 'lower', 'higher'}, optional\n        This optional parameter specifies the interpolation method to use,\n        when the desired quantile lies between two data points `i` and `j`:\n\n        - fraction: `i + (j - i)*fraction`, where `fraction` is the\n                    fractional part of the index surrounded by `i` and `j`.\n        -lower: `i`.\n        - higher: `j`.\n\n    Returns\n    -------\n    score : float\n        Score at percentile.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> a = np.arange(100)\n    >>> stats.scoreatpercentile(a, 50)\n    49.5\n\n    \"\"\"\n    x = np.asarray(x)\n    mask = isna(x)\n\n    x = x[~mask]\n\n    values = np.sort(x)\n\n    def _interpolate(a, b, fraction):\n        \"\"\"Returns the point at the given fraction between a and b, where\n        'fraction' must be between 0 and 1.\n        \"\"\"\n        return a + (b - a) * fraction\n\n    def _get_score(at):\n        if len(values) == 0:\n            return np.nan\n\n        idx = at * (len(values) - 1)\n        if idx % 1 == 0:\n            score = values[int(idx)]\n        else:\n            if interpolation_method == 'fraction':\n                score = _interpolate(values[int(idx)], values[int(idx) + 1],\n                                     idx % 1)\n            elif interpolation_method == 'lower':\n                score = values[np.floor(idx)]\n            elif interpolation_method == 'higher':\n                score = values[np.ceil(idx)]\n            else:\n                raise ValueError(\"interpolation_method can only be 'fraction' \"\n                                 \", 'lower' or 'higher'\")\n\n        return score\n\n    if is_scalar(q):\n        return _get_score(q)\n    else:\n        q = np.asarray(q, np.float64)\n        return algos.arrmap_float64(q, _get_score)\n\n\n# --------------- #\n# select n        #\n# --------------- #\n\nclass SelectN(object):\n\n    def __init__(self, obj, n, keep):\n        self.obj = obj\n        self.n = n\n        self.keep = keep\n\n        if self.keep not in ('first', 'last', 'all'):\n            raise ValueError('keep must be either \"first\", \"last\" or \"all\"')\n\n    def nlargest(self):\n        return self.compute('nlargest')\n\n    def nsmallest(self):\n        return self.compute('nsmallest')\n\n    @staticmethod\n    def is_valid_dtype_n_method(dtype):\n        \"\"\"\n        Helper function to determine if dtype is valid for\n        nsmallest/nlargest methods\n        \"\"\"\n        return ((is_numeric_dtype(dtype) and not is_complex_dtype(dtype)) or\n                needs_i8_conversion(dtype))\n\n\nclass SelectNSeries(SelectN):\n    \"\"\"\n    Implement n largest/smallest for Series\n\n    Parameters\n    ----------\n    obj : Series\n    n : int\n    keep : {'first', 'last'}, default 'first'\n\n    Returns\n    -------\n    nordered : Series\n    \"\"\"\n\n    def compute(self, method):\n\n        n = self.n\n        dtype = self.obj.dtype\n        if not self.is_valid_dtype_n_method(dtype):\n            raise TypeError(\"Cannot use method '{method}' with \"\n                            \"dtype {dtype}\".format(method=method,\n                                                   dtype=dtype))\n\n        if n <= 0:\n            return self.obj[[]]\n\n        dropped = self.obj.dropna()\n\n        # slow method\n        if n >= len(self.obj):\n\n            reverse_it = (self.keep == 'last' or method == 'nlargest')\n            ascending = method == 'nsmallest'\n            slc = np.s_[::-1] if reverse_it else np.s_[:]\n            return dropped[slc].sort_values(ascending=ascending).head(n)\n\n        # fast method\n        arr, pandas_dtype, _ = _ensure_data(dropped.values)\n        if method == 'nlargest':\n            arr = -arr\n            if is_integer_dtype(pandas_dtype):\n                # GH 21426: ensure reverse ordering at boundaries\n                arr -= 1\n\n        if self.keep == 'last':\n            arr = arr[::-1]\n\n        narr = len(arr)\n        n = min(n, narr)\n\n        kth_val = algos.kth_smallest(arr.copy(), n - 1)\n        ns, = np.nonzero(arr <= kth_val)\n        inds = ns[arr[ns].argsort(kind='mergesort')]\n\n        if self.keep != 'all':\n            inds = inds[:n]\n\n        if self.keep == 'last':\n            # reverse indices\n            inds = narr - 1 - inds\n\n        return dropped.iloc[inds]\n\n\nclass SelectNFrame(SelectN):\n    \"\"\"\n    Implement n largest/smallest for DataFrame\n\n    Parameters\n    ----------\n    obj : DataFrame\n    n : int\n    keep : {'first', 'last'}, default 'first'\n    columns : list or str\n\n    Returns\n    -------\n    nordered : DataFrame\n    \"\"\"\n\n    def __init__(self, obj, n, keep, columns):\n        super(SelectNFrame, self).__init__(obj, n, keep)\n        if not is_list_like(columns) or isinstance(columns, tuple):\n            columns = [columns]\n        columns = list(columns)\n        self.columns = columns\n\n    def compute(self, method):\n\n        from pandas import Int64Index\n        n = self.n\n        frame = self.obj\n        columns = self.columns\n\n        for column in columns:\n            dtype = frame[column].dtype\n            if not self.is_valid_dtype_n_method(dtype):\n                raise TypeError((\n                    \"Column {column!r} has dtype {dtype}, cannot use method \"\n                    \"{method!r} with this dtype\"\n                ).format(column=column, dtype=dtype, method=method))\n\n        def get_indexer(current_indexer, other_indexer):\n            \"\"\"Helper function to concat `current_indexer` and `other_indexer`\n            depending on `method`\n            \"\"\"\n            if method == 'nsmallest':\n                return current_indexer.append(other_indexer)\n            else:\n                return other_indexer.append(current_indexer)\n\n        # Below we save and reset the index in case index contains duplicates\n        original_index = frame.index\n        cur_frame = frame = frame.reset_index(drop=True)\n        cur_n = n\n        indexer = Int64Index([])\n\n        for i, column in enumerate(columns):\n            # For each column we apply method to cur_frame[column].\n            # If it's the last column or if we have the number of\n            # results desired we are done.\n            # Otherwise there are duplicates of the largest/smallest\n            # value and we need to look at the rest of the columns\n            # to determine which of the rows with the largest/smallest\n            # value in the column to keep.\n            series = cur_frame[column]\n            is_last_column = len(columns) - 1 == i\n            values = getattr(series, method)(\n                cur_n,\n                keep=self.keep if is_last_column else 'all')\n\n            if is_last_column or len(values) <= cur_n:\n                indexer = get_indexer(indexer, values.index)\n                break\n\n            # Now find all values which are equal to\n            # the (nsmallest: largest)/(nlarrgest: smallest)\n            # from our series.\n            border_value = values == values[values.index[-1]]\n\n            # Some of these values are among the top-n\n            # some aren't.\n            unsafe_values = values[border_value]\n\n            # These values are definitely among the top-n\n            safe_values = values[~border_value]\n            indexer = get_indexer(indexer, safe_values.index)\n\n            # Go on and separate the unsafe_values on the remaining\n            # columns.\n            cur_frame = cur_frame.loc[unsafe_values.index]\n            cur_n = n - len(indexer)\n\n        frame = frame.take(indexer)\n\n        # Restore the index on frame\n        frame.index = original_index.take(indexer)\n\n        # If there is only one column, the frame is already sorted.\n        if len(columns) == 1:\n            return frame\n\n        ascending = method == 'nsmallest'\n\n        return frame.sort_values(\n            columns,\n            ascending=ascending,\n            kind='mergesort')\n\n\n# ------- ## ---- #\n# take #\n# ---- #\n\n\ndef _view_wrapper(f, arr_dtype=None, out_dtype=None, fill_wrap=None):\n    def wrapper(arr, indexer, out, fill_value=np.nan):\n        if arr_dtype is not None:\n            arr = arr.view(arr_dtype)\n        if out_dtype is not None:\n            out = out.view(out_dtype)\n        if fill_wrap is not None:\n            fill_value = fill_wrap(fill_value)\n        f(arr, indexer, out, fill_value=fill_value)\n\n    return wrapper\n\n\ndef _convert_wrapper(f, conv_dtype):\n    def wrapper(arr, indexer, out, fill_value=np.nan):\n        arr = arr.astype(conv_dtype)\n        f(arr, indexer, out, fill_value=fill_value)\n\n    return wrapper\n\n\ndef _take_2d_multi_object(arr, indexer, out, fill_value, mask_info):\n    # this is not ideal, performance-wise, but it's better than raising\n    # an exception (best to optimize in Cython to avoid getting here)\n    row_idx, col_idx = indexer\n    if mask_info is not None:\n        (row_mask, col_mask), (row_needs, col_needs) = mask_info\n    else:\n        row_mask = row_idx == -1\n        col_mask = col_idx == -1\n        row_needs = row_mask.any()\n        col_needs = col_mask.any()\n    if fill_value is not None:\n        if row_needs:\n            out[row_mask, :] = fill_value\n        if col_needs:\n            out[:, col_mask] = fill_value\n    for i in range(len(row_idx)):\n        u_ = row_idx[i]\n        for j in range(len(col_idx)):\n            v = col_idx[j]\n            out[i, j] = arr[u_, v]\n\n\ndef _take_nd_object(arr, indexer, out, axis, fill_value, mask_info):\n    if mask_info is not None:\n        mask, needs_masking = mask_info\n    else:\n        mask = indexer == -1\n        needs_masking = mask.any()\n    if arr.dtype != out.dtype:\n        arr = arr.astype(out.dtype)\n    if arr.shape[axis] > 0:\n        arr.take(ensure_platform_int(indexer), axis=axis, out=out)\n    if needs_masking:\n        outindexer = [slice(None)] * arr.ndim\n        outindexer[axis] = mask\n        out[tuple(outindexer)] = fill_value\n\n\n_take_1d_dict = {\n    ('int8', 'int8'): algos.take_1d_int8_int8,\n    ('int8', 'int32'): algos.take_1d_int8_int32,\n    ('int8', 'int64'): algos.take_1d_int8_int64,\n    ('int8', 'float64'): algos.take_1d_int8_float64,\n    ('int16', 'int16'): algos.take_1d_int16_int16,\n    ('int16', 'int32'): algos.take_1d_int16_int32,\n    ('int16', 'int64'): algos.take_1d_int16_int64,\n    ('int16', 'float64'): algos.take_1d_int16_float64,\n    ('int32', 'int32'): algos.take_1d_int32_int32,\n    ('int32', 'int64'): algos.take_1d_int32_int64,\n    ('int32', 'float64'): algos.take_1d_int32_float64,\n    ('int64', 'int64'): algos.take_1d_int64_int64,\n    ('int64', 'float64'): algos.take_1d_int64_float64,\n    ('float32', 'float32'): algos.take_1d_float32_float32,\n    ('float32', 'float64'): algos.take_1d_float32_float64,\n    ('float64', 'float64'): algos.take_1d_float64_float64,\n    ('object', 'object'): algos.take_1d_object_object,\n    ('bool', 'bool'): _view_wrapper(algos.take_1d_bool_bool, np.uint8,\n                                    np.uint8),\n    ('bool', 'object'): _view_wrapper(algos.take_1d_bool_object, np.uint8,\n                                      None),\n    ('datetime64[ns]', 'datetime64[ns]'): _view_wrapper(\n        algos.take_1d_int64_int64, np.int64, np.int64, np.int64)\n}\n\n_take_2d_axis0_dict = {\n    ('int8', 'int8'): algos.take_2d_axis0_int8_int8,\n    ('int8', 'int32'): algos.take_2d_axis0_int8_int32,\n    ('int8', 'int64'): algos.take_2d_axis0_int8_int64,\n    ('int8', 'float64'): algos.take_2d_axis0_int8_float64,\n    ('int16', 'int16'): algos.take_2d_axis0_int16_int16,\n    ('int16', 'int32'): algos.take_2d_axis0_int16_int32,\n    ('int16', 'int64'): algos.take_2d_axis0_int16_int64,\n    ('int16', 'float64'): algos.take_2d_axis0_int16_float64,\n    ('int32', 'int32'): algos.take_2d_axis0_int32_int32,\n    ('int32', 'int64'): algos.take_2d_axis0_int32_int64,\n    ('int32', 'float64'): algos.take_2d_axis0_int32_float64,\n    ('int64', 'int64'): algos.take_2d_axis0_int64_int64,\n    ('int64', 'float64'): algos.take_2d_axis0_int64_float64,\n    ('float32', 'float32'): algos.take_2d_axis0_float32_float32,\n    ('float32', 'float64'): algos.take_2d_axis0_float32_float64,\n    ('float64', 'float64'): algos.take_2d_axis0_float64_float64,\n    ('object', 'object'): algos.take_2d_axis0_object_object,\n    ('bool', 'bool'): _view_wrapper(algos.take_2d_axis0_bool_bool, np.uint8,\n                                    np.uint8),\n    ('bool', 'object'): _view_wrapper(algos.take_2d_axis0_bool_object,\n                                      np.uint8, None),\n    ('datetime64[ns]', 'datetime64[ns]'):\n    _view_wrapper(algos.take_2d_axis0_int64_int64, np.int64, np.int64,\n                  fill_wrap=np.int64)\n}\n\n_take_2d_axis1_dict = {\n    ('int8', 'int8'): algos.take_2d_axis1_int8_int8,\n    ('int8', 'int32'): algos.take_2d_axis1_int8_int32,\n    ('int8', 'int64'): algos.take_2d_axis1_int8_int64,\n    ('int8', 'float64'): algos.take_2d_axis1_int8_float64,\n    ('int16', 'int16'): algos.take_2d_axis1_int16_int16,\n    ('int16', 'int32'): algos.take_2d_axis1_int16_int32,\n    ('int16', 'int64'): algos.take_2d_axis1_int16_int64,\n    ('int16', 'float64'): algos.take_2d_axis1_int16_float64,\n    ('int32', 'int32'): algos.take_2d_axis1_int32_int32,\n    ('int32', 'int64'): algos.take_2d_axis1_int32_int64,\n    ('int32', 'float64'): algos.take_2d_axis1_int32_float64,\n    ('int64', 'int64'): algos.take_2d_axis1_int64_int64,\n    ('int64', 'float64'): algos.take_2d_axis1_int64_float64,\n    ('float32', 'float32'): algos.take_2d_axis1_float32_float32,\n    ('float32', 'float64'): algos.take_2d_axis1_float32_float64,\n    ('float64', 'float64'): algos.take_2d_axis1_float64_float64,\n    ('object', 'object'): algos.take_2d_axis1_object_object,\n    ('bool', 'bool'): _view_wrapper(algos.take_2d_axis1_bool_bool, np.uint8,\n                                    np.uint8),\n    ('bool', 'object'): _view_wrapper(algos.take_2d_axis1_bool_object,\n                                      np.uint8, None),\n    ('datetime64[ns]', 'datetime64[ns]'):\n    _view_wrapper(algos.take_2d_axis1_int64_int64, np.int64, np.int64,\n                  fill_wrap=np.int64)\n}\n\n_take_2d_multi_dict = {\n    ('int8', 'int8'): algos.take_2d_multi_int8_int8,\n    ('int8', 'int32'): algos.take_2d_multi_int8_int32,\n    ('int8', 'int64'): algos.take_2d_multi_int8_int64,\n    ('int8', 'float64'): algos.take_2d_multi_int8_float64,\n    ('int16', 'int16'): algos.take_2d_multi_int16_int16,\n    ('int16', 'int32'): algos.take_2d_multi_int16_int32,\n    ('int16', 'int64'): algos.take_2d_multi_int16_int64,\n    ('int16', 'float64'): algos.take_2d_multi_int16_float64,\n    ('int32', 'int32'): algos.take_2d_multi_int32_int32,\n    ('int32', 'int64'): algos.take_2d_multi_int32_int64,\n    ('int32', 'float64'): algos.take_2d_multi_int32_float64,\n    ('int64', 'int64'): algos.take_2d_multi_int64_int64,\n    ('int64', 'float64'): algos.take_2d_multi_int64_float64,\n    ('float32', 'float32'): algos.take_2d_multi_float32_float32,\n    ('float32', 'float64'): algos.take_2d_multi_float32_float64,\n    ('float64', 'float64'): algos.take_2d_multi_float64_float64,\n    ('object', 'object'): algos.take_2d_multi_object_object,\n    ('bool', 'bool'): _view_wrapper(algos.take_2d_multi_bool_bool, np.uint8,\n                                    np.uint8),\n    ('bool', 'object'): _view_wrapper(algos.take_2d_multi_bool_object,\n                                      np.uint8, None),\n    ('datetime64[ns]', 'datetime64[ns]'):\n    _view_wrapper(algos.take_2d_multi_int64_int64, np.int64, np.int64,\n                  fill_wrap=np.int64)\n}\n\n\ndef _get_take_nd_function(ndim, arr_dtype, out_dtype, axis=0, mask_info=None):\n    if ndim <= 2:\n        tup = (arr_dtype.name, out_dtype.name)\n        if ndim == 1:\n            func = _take_1d_dict.get(tup, None)\n        elif ndim == 2:\n            if axis == 0:\n                func = _take_2d_axis0_dict.get(tup, None)\n            else:\n                func = _take_2d_axis1_dict.get(tup, None)\n        if func is not None:\n            return func\n\n        tup = (out_dtype.name, out_dtype.name)\n        if ndim == 1:\n            func = _take_1d_dict.get(tup, None)\n        elif ndim == 2:\n            if axis == 0:\n                func = _take_2d_axis0_dict.get(tup, None)\n            else:\n                func = _take_2d_axis1_dict.get(tup, None)\n        if func is not None:\n            func = _convert_wrapper(func, out_dtype)\n            return func\n\n    def func(arr, indexer, out, fill_value=np.nan):\n        indexer = ensure_int64(indexer)\n        _take_nd_object(arr, indexer, out, axis=axis, fill_value=fill_value,\n                        mask_info=mask_info)\n\n    return func\n\n\ndef take(arr, indices, axis=0, allow_fill=False, fill_value=None):\n    \"\"\"\n    Take elements from an array.\n\n    .. versionadded:: 0.23.0\n\n    Parameters\n    ----------\n    arr : sequence\n        Non array-likes (sequences without a dtype) are coerced\n        to an ndarray.\n    indices : sequence of integers\n        Indices to be taken.\n    axis : int, default 0\n        The axis over which to select values.\n    allow_fill : bool, default False\n        How to handle negative values in `indices`.\n\n        * False: negative values in `indices` indicate positional indices\n          from the right (the default). This is similar to :func:`numpy.take`.\n\n        * True: negative values in `indices` indicate\n          missing values. These values are set to `fill_value`. Any other\n          other negative values raise a ``ValueError``.\n\n    fill_value : any, optional\n        Fill value to use for NA-indices when `allow_fill` is True.\n        This may be ``None``, in which case the default NA value for\n        the type (``self.dtype.na_value``) is used.\n\n        For multi-dimensional `arr`, each *element* is filled with\n        `fill_value`.\n\n    Returns\n    -------\n    ndarray or ExtensionArray\n        Same type as the input.\n\n    Raises\n    ------\n    IndexError\n        When `indices` is out of bounds for the array.\n    ValueError\n        When the indexer contains negative values other than ``-1``\n        and `allow_fill` is True.\n\n    Notes\n    -----\n    When `allow_fill` is False, `indices` may be whatever dimensionality\n    is accepted by NumPy for `arr`.\n\n    When `allow_fill` is True, `indices` should be 1-D.\n\n    See Also\n    --------\n    numpy.take\n\n    Examples\n    --------\n    >>> from pandas.api.extensions import take\n\n    With the default ``allow_fill=False``, negative numbers indicate\n    positional indices from the right.\n\n    >>> take(np.array([10, 20, 30]), [0, 0, -1])\n    array([10, 10, 30])\n\n    Setting ``allow_fill=True`` will place `fill_value` in those positions.\n\n    >>> take(np.array([10, 20, 30]), [0, 0, -1], allow_fill=True)\n    array([10., 10., nan])\n\n    >>> take(np.array([10, 20, 30]), [0, 0, -1], allow_fill=True,\n    ...      fill_value=-10)\n    array([ 10,  10, -10])\n    \"\"\"\n    from pandas.core.indexing import validate_indices\n\n    if not is_array_like(arr):\n        arr = np.asarray(arr)\n\n    indices = np.asarray(indices, dtype=np.intp)\n\n    if allow_fill:\n        # Pandas style, -1 means NA\n        validate_indices(indices, len(arr))\n        result = take_1d(arr, indices, axis=axis, allow_fill=True,\n                         fill_value=fill_value)\n    else:\n        # NumPy style\n        result = arr.take(indices, axis=axis)\n    return result\n\n\ndef take_nd(arr, indexer, axis=0, out=None, fill_value=np.nan, mask_info=None,\n            allow_fill=True):\n    \"\"\"\n    Specialized Cython take which sets NaN values in one pass\n\n    This dispatches to ``take`` defined on ExtensionArrays. It does not\n    currently dispatch to ``SparseArray.take`` for sparse ``arr``.\n\n    Parameters\n    ----------\n    arr : array-like\n        Input array.\n    indexer : ndarray\n        1-D array of indices to take, subarrays corresponding to -1 value\n        indices are filed with fill_value\n    axis : int, default 0\n        Axis to take from\n    out : ndarray or None, default None\n        Optional output array, must be appropriate type to hold input and\n        fill_value together, if indexer has any -1 value entries; call\n        _maybe_promote to determine this type for any fill_value\n    fill_value : any, default np.nan\n        Fill value to replace -1 values with\n    mask_info : tuple of (ndarray, boolean)\n        If provided, value should correspond to:\n            (indexer != -1, (indexer != -1).any())\n        If not provided, it will be computed internally if necessary\n    allow_fill : boolean, default True\n        If False, indexer is assumed to contain no -1 values so no filling\n        will be done.  This short-circuits computation of a mask.  Result is\n        undefined if allow_fill == False and -1 is present in indexer.\n\n    Returns\n    -------\n    subarray : array-like\n        May be the same type as the input, or cast to an ndarray.\n    \"\"\"\n\n    # TODO(EA): Remove these if / elifs as datetimeTZ, interval, become EAs\n    # dispatch to internal type takes\n    if is_extension_array_dtype(arr):\n        return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n    elif is_datetimetz(arr):\n        return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n    elif is_interval_dtype(arr):\n        return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\n    if is_sparse(arr):\n        arr = arr.get_values()\n    elif isinstance(arr, (ABCIndexClass, ABCSeries)):\n        arr = arr.values\n\n    arr = np.asarray(arr)\n\n    if indexer is None:\n        indexer = np.arange(arr.shape[axis], dtype=np.int64)\n        dtype, fill_value = arr.dtype, arr.dtype.type()\n    else:\n        indexer = ensure_int64(indexer, copy=False)\n        if not allow_fill:\n            dtype, fill_value = arr.dtype, arr.dtype.type()\n            mask_info = None, False\n        else:\n            # check for promotion based on types only (do this first because\n            # it's faster than computing a mask)\n            dtype, fill_value = maybe_promote(arr.dtype, fill_value)\n            if dtype != arr.dtype and (out is None or out.dtype != dtype):\n                # check if promotion is actually required based on indexer\n                if mask_info is not None:\n                    mask, needs_masking = mask_info\n                else:\n                    mask = indexer == -1\n                    needs_masking = mask.any()\n                    mask_info = mask, needs_masking\n                if needs_masking:\n                    if out is not None and out.dtype != dtype:\n                        raise TypeError('Incompatible type for fill_value')\n                else:\n                    # if not, then depromote, set fill_value to dummy\n                    # (it won't be used but we don't want the cython code\n                    # to crash when trying to cast it to dtype)\n                    dtype, fill_value = arr.dtype, arr.dtype.type()\n\n    flip_order = False\n    if arr.ndim == 2:\n        if arr.flags.f_contiguous:\n            flip_order = True\n\n    if flip_order:\n        arr = arr.T\n        axis = arr.ndim - axis - 1\n        if out is not None:\n            out = out.T\n\n    # at this point, it's guaranteed that dtype can hold both the arr values\n    # and the fill_value\n    if out is None:\n        out_shape = list(arr.shape)\n        out_shape[axis] = len(indexer)\n        out_shape = tuple(out_shape)\n        if arr.flags.f_contiguous and axis == arr.ndim - 1:\n            # minor tweak that can make an order-of-magnitude difference\n            # for dataframes initialized directly from 2-d ndarrays\n            # (s.t. df.values is c-contiguous and df._data.blocks[0] is its\n            # f-contiguous transpose)\n            out = np.empty(out_shape, dtype=dtype, order='F')\n        else:\n            out = np.empty(out_shape, dtype=dtype)\n\n    func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\n                                 mask_info=mask_info)\n    func(arr, indexer, out, fill_value)\n\n    if flip_order:\n        out = out.T\n    return out\n\n\ntake_1d = take_nd\n\n\ndef take_2d_multi(arr, indexer, out=None, fill_value=np.nan, mask_info=None,\n                  allow_fill=True):\n    \"\"\"\n    Specialized Cython take which sets NaN values in one pass\n    \"\"\"\n    if indexer is None or (indexer[0] is None and indexer[1] is None):\n        row_idx = np.arange(arr.shape[0], dtype=np.int64)\n        col_idx = np.arange(arr.shape[1], dtype=np.int64)\n        indexer = row_idx, col_idx\n        dtype, fill_value = arr.dtype, arr.dtype.type()\n    else:\n        row_idx, col_idx = indexer\n        if row_idx is None:\n            row_idx = np.arange(arr.shape[0], dtype=np.int64)\n        else:\n            row_idx = ensure_int64(row_idx)\n        if col_idx is None:\n            col_idx = np.arange(arr.shape[1], dtype=np.int64)\n        else:\n            col_idx = ensure_int64(col_idx)\n        indexer = row_idx, col_idx\n        if not allow_fill:\n            dtype, fill_value = arr.dtype, arr.dtype.type()\n            mask_info = None, False\n        else:\n            # check for promotion based on types only (do this first because\n            # it's faster than computing a mask)\n            dtype, fill_value = maybe_promote(arr.dtype, fill_value)\n            if dtype != arr.dtype and (out is None or out.dtype != dtype):\n                # check if promotion is actually required based on indexer\n                if mask_info is not None:\n                    (row_mask, col_mask), (row_needs, col_needs) = mask_info\n                else:\n                    row_mask = row_idx == -1\n                    col_mask = col_idx == -1\n                    row_needs = row_mask.any()\n                    col_needs = col_mask.any()\n                    mask_info = (row_mask, col_mask), (row_needs, col_needs)\n                if row_needs or col_needs:\n                    if out is not None and out.dtype != dtype:\n                        raise TypeError('Incompatible type for fill_value')\n                else:\n                    # if not, then depromote, set fill_value to dummy\n                    # (it won't be used but we don't want the cython code\n                    # to crash when trying to cast it to dtype)\n                    dtype, fill_value = arr.dtype, arr.dtype.type()\n\n    # at this point, it's guaranteed that dtype can hold both the arr values\n    # and the fill_value\n    if out is None:\n        out_shape = len(row_idx), len(col_idx)\n        out = np.empty(out_shape, dtype=dtype)\n\n    func = _take_2d_multi_dict.get((arr.dtype.name, out.dtype.name), None)\n    if func is None and arr.dtype != out.dtype:\n        func = _take_2d_multi_dict.get((out.dtype.name, out.dtype.name), None)\n        if func is not None:\n            func = _convert_wrapper(func, out.dtype)\n    if func is None:\n\n        def func(arr, indexer, out, fill_value=np.nan):\n            _take_2d_multi_object(arr, indexer, out, fill_value=fill_value,\n                                  mask_info=mask_info)\n\n    func(arr, indexer, out=out, fill_value=fill_value)\n    return out\n\n\n# ---- #\n# diff #\n# ---- #\n\n_diff_special = {\n    'float64': algos.diff_2d_float64,\n    'float32': algos.diff_2d_float32,\n    'int64': algos.diff_2d_int64,\n    'int32': algos.diff_2d_int32,\n    'int16': algos.diff_2d_int16,\n    'int8': algos.diff_2d_int8,\n}\n\n\ndef diff(arr, n, axis=0):\n    \"\"\"\n    difference of n between self,\n    analogous to s-s.shift(n)\n\n    Parameters\n    ----------\n    arr : ndarray\n    n : int\n        number of periods\n    axis : int\n        axis to shift on\n\n    Returns\n    -------\n    shifted\n\n    \"\"\"\n\n    n = int(n)\n    na = np.nan\n    dtype = arr.dtype\n\n    is_timedelta = False\n    if needs_i8_conversion(arr):\n        dtype = np.float64\n        arr = arr.view('i8')\n        na = iNaT\n        is_timedelta = True\n\n    elif is_bool_dtype(dtype):\n        dtype = np.object_\n\n    elif is_integer_dtype(dtype):\n        dtype = np.float64\n\n    dtype = np.dtype(dtype)\n    out_arr = np.empty(arr.shape, dtype=dtype)\n\n    na_indexer = [slice(None)] * arr.ndim\n    na_indexer[axis] = slice(None, n) if n >= 0 else slice(n, None)\n    out_arr[tuple(na_indexer)] = na\n\n    if arr.ndim == 2 and arr.dtype.name in _diff_special:\n        f = _diff_special[arr.dtype.name]\n        f(arr, out_arr, n, axis)\n    else:\n        res_indexer = [slice(None)] * arr.ndim\n        res_indexer[axis] = slice(n, None) if n >= 0 else slice(None, n)\n        res_indexer = tuple(res_indexer)\n\n        lag_indexer = [slice(None)] * arr.ndim\n        lag_indexer[axis] = slice(None, -n) if n > 0 else slice(-n, None)\n        lag_indexer = tuple(lag_indexer)\n\n        # need to make sure that we account for na for datelike/timedelta\n        # we don't actually want to subtract these i8 numbers\n        if is_timedelta:\n            res = arr[res_indexer]\n            lag = arr[lag_indexer]\n\n            mask = (arr[res_indexer] == na) | (arr[lag_indexer] == na)\n            if mask.any():\n                res = res.copy()\n                res[mask] = 0\n                lag = lag.copy()\n                lag[mask] = 0\n\n            result = res - lag\n            result[mask] = na\n            out_arr[res_indexer] = result\n        else:\n            out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\n\n    if is_timedelta:\n        from pandas import TimedeltaIndex\n        out_arr = TimedeltaIndex(out_arr.ravel().astype('int64')).asi8.reshape(\n            out_arr.shape).astype('timedelta64[ns]')\n\n    return out_arr\n"
    },
    {
      "filename": "pandas/core/config.py",
      "content": "\"\"\"\nThe config module holds package-wide configurables and provides\na uniform API for working with them.\n\nOverview\n========\n\nThis module supports the following requirements:\n- options are referenced using keys in dot.notation, e.g. \"x.y.option - z\".\n- keys are case-insensitive.\n- functions should accept partial/regex keys, when unambiguous.\n- options can be registered by modules at import time.\n- options can be registered at init-time (via core.config_init)\n- options have a default value, and (optionally) a description and\n  validation function associated with them.\n- options can be deprecated, in which case referencing them\n  should produce a warning.\n- deprecated options can optionally be rerouted to a replacement\n  so that accessing a deprecated option reroutes to a differently\n  named option.\n- options can be reset to their default value.\n- all option can be reset to their default value at once.\n- all options in a certain sub - namespace can be reset at once.\n- the user can set / get / reset or ask for the description of an option.\n- a developer can register and mark an option as deprecated.\n- you can register a callback to be invoked when the option value\n  is set or reset. Changing the stored value is considered misuse, but\n  is not verboten.\n\nImplementation\n==============\n\n- Data is stored using nested dictionaries, and should be accessed\n  through the provided API.\n\n- \"Registered options\" and \"Deprecated options\" have metadata associated\n  with them, which are stored in auxiliary dictionaries keyed on the\n  fully-qualified key, e.g. \"x.y.z.option\".\n\n- the config_init module is imported by the package's __init__.py file.\n  placing any register_option() calls there will ensure those options\n  are available as soon as pandas is loaded. If you use register_option\n  in a module, it will only be available after that module is imported,\n  which you should be aware of.\n\n- `config_prefix` is a context_manager (for use with the `with` keyword)\n  which can save developers some typing, see the docstring.\n\n\"\"\"\n\nfrom collections import namedtuple\nfrom contextlib import contextmanager\nimport re\nimport warnings\n\nimport pandas.compat as compat\nfrom pandas.compat import lmap, map, u\n\nDeprecatedOption = namedtuple('DeprecatedOption', 'key msg rkey removal_ver')\nRegisteredOption = namedtuple('RegisteredOption',\n                              'key defval doc validator cb')\n\n_deprecated_options = {}  # holds deprecated option metdata\n_registered_options = {}  # holds registered option metdata\n_global_config = {}  # holds the current values for registered options\n_reserved_keys = ['all']  # keys which have a special meaning\n\n\nclass OptionError(AttributeError, KeyError):\n    \"\"\"Exception for pandas.options, backwards compatible with KeyError\n    checks\n    \"\"\"\n\n#\n# User API\n\n\ndef _get_single_key(pat, silent):\n    keys = _select_options(pat)\n    if len(keys) == 0:\n        if not silent:\n            _warn_if_deprecated(pat)\n        raise OptionError('No such keys(s): {pat!r}'.format(pat=pat))\n    if len(keys) > 1:\n        raise OptionError('Pattern matched multiple keys')\n    key = keys[0]\n\n    if not silent:\n        _warn_if_deprecated(key)\n\n    key = _translate_key(key)\n\n    return key\n\n\ndef _get_option(pat, silent=False):\n    key = _get_single_key(pat, silent)\n\n    # walk the nested dict\n    root, k = _get_root(key)\n    return root[k]\n\n\ndef _set_option(*args, **kwargs):\n    # must at least 1 arg deal with constraints later\n    nargs = len(args)\n    if not nargs or nargs % 2 != 0:\n        raise ValueError(\"Must provide an even number of non-keyword \"\n                         \"arguments\")\n\n    # default to false\n    silent = kwargs.pop('silent', False)\n\n    if kwargs:\n        msg = '_set_option() got an unexpected keyword argument \"{kwarg}\"'\n        raise TypeError(msg.format(list(kwargs.keys())[0]))\n\n    for k, v in zip(args[::2], args[1::2]):\n        key = _get_single_key(k, silent)\n\n        o = _get_registered_option(key)\n        if o and o.validator:\n            o.validator(v)\n\n        # walk the nested dict\n        root, k = _get_root(key)\n        root[k] = v\n\n        if o.cb:\n            if silent:\n                with warnings.catch_warnings(record=True):\n                    o.cb(key)\n            else:\n                o.cb(key)\n\n\ndef _describe_option(pat='', _print_desc=True):\n\n    keys = _select_options(pat)\n    if len(keys) == 0:\n        raise OptionError('No such keys(s)')\n\n    s = u('')\n    for k in keys:  # filter by pat\n        s += _build_option_description(k)\n\n    if _print_desc:\n        print(s)\n    else:\n        return s\n\n\ndef _reset_option(pat, silent=False):\n\n    keys = _select_options(pat)\n\n    if len(keys) == 0:\n        raise OptionError('No such keys(s)')\n\n    if len(keys) > 1 and len(pat) < 4 and pat != 'all':\n        raise ValueError('You must specify at least 4 characters when '\n                         'resetting multiple keys, use the special keyword '\n                         '\"all\" to reset all the options to their default '\n                         'value')\n\n    for k in keys:\n        _set_option(k, _registered_options[k].defval, silent=silent)\n\n\ndef get_default_val(pat):\n    key = _get_single_key(pat, silent=True)\n    return _get_registered_option(key).defval\n\n\nclass DictWrapper(object):\n    \"\"\" provide attribute-style access to a nested dict\"\"\"\n\n    def __init__(self, d, prefix=\"\"):\n        object.__setattr__(self, \"d\", d)\n        object.__setattr__(self, \"prefix\", prefix)\n\n    def __setattr__(self, key, val):\n        prefix = object.__getattribute__(self, \"prefix\")\n        if prefix:\n            prefix += \".\"\n        prefix += key\n        # you can't set new keys\n        # can you can't overwrite subtrees\n        if key in self.d and not isinstance(self.d[key], dict):\n            _set_option(prefix, val)\n        else:\n            raise OptionError(\"You can only set the value of existing options\")\n\n    def __getattr__(self, key):\n        prefix = object.__getattribute__(self, \"prefix\")\n        if prefix:\n            prefix += \".\"\n        prefix += key\n        try:\n            v = object.__getattribute__(self, \"d\")[key]\n        except KeyError:\n            raise OptionError(\"No such option\")\n        if isinstance(v, dict):\n            return DictWrapper(v, prefix)\n        else:\n            return _get_option(prefix)\n\n    def __dir__(self):\n        return list(self.d.keys())\n\n# For user convenience,  we'd like to have the available options described\n# in the docstring. For dev convenience we'd like to generate the docstrings\n# dynamically instead of maintaining them by hand. To this, we use the\n# class below which wraps functions inside a callable, and converts\n# __doc__ into a property function. The doctsrings below are templates\n# using the py2.6+ advanced formatting syntax to plug in a concise list\n# of options, and option descriptions.\n\n\nclass CallableDynamicDoc(object):\n\n    def __init__(self, func, doc_tmpl):\n        self.__doc_tmpl__ = doc_tmpl\n        self.__func__ = func\n\n    def __call__(self, *args, **kwds):\n        return self.__func__(*args, **kwds)\n\n    @property\n    def __doc__(self):\n        opts_desc = _describe_option('all', _print_desc=False)\n        opts_list = pp_options_list(list(_registered_options.keys()))\n        return self.__doc_tmpl__.format(opts_desc=opts_desc,\n                                        opts_list=opts_list)\n\n\n_get_option_tmpl = \"\"\"\nget_option(pat)\n\nRetrieves the value of the specified option.\n\nAvailable options:\n\n{opts_list}\n\nParameters\n----------\npat : str\n    Regexp which should match a single option.\n    Note: partial matches are supported for convenience, but unless you use the\n    full option name (e.g. x.y.z.option_name), your code may break in future\n    versions if new options with similar names are introduced.\n\nReturns\n-------\nresult : the value of the option\n\nRaises\n------\nOptionError : if no such option exists\n\nNotes\n-----\nThe available options with its descriptions:\n\n{opts_desc}\n\"\"\"\n\n_set_option_tmpl = \"\"\"\nset_option(pat, value)\n\nSets the value of the specified option.\n\nAvailable options:\n\n{opts_list}\n\nParameters\n----------\npat : str\n    Regexp which should match a single option.\n    Note: partial matches are supported for convenience, but unless you use the\n    full option name (e.g. x.y.z.option_name), your code may break in future\n    versions if new options with similar names are introduced.\nvalue :\n    new value of option.\n\nReturns\n-------\nNone\n\nRaises\n------\nOptionError if no such option exists\n\nNotes\n-----\nThe available options with its descriptions:\n\n{opts_desc}\n\"\"\"\n\n_describe_option_tmpl = \"\"\"\ndescribe_option(pat, _print_desc=False)\n\nPrints the description for one or more registered options.\n\nCall with not arguments to get a listing for all registered options.\n\nAvailable options:\n\n{opts_list}\n\nParameters\n----------\npat : str\n    Regexp pattern. All matching keys will have their description displayed.\n_print_desc : bool, default True\n    If True (default) the description(s) will be printed to stdout.\n    Otherwise, the description(s) will be returned as a unicode string\n    (for testing).\n\nReturns\n-------\nNone by default, the description(s) as a unicode string if _print_desc\nis False\n\nNotes\n-----\nThe available options with its descriptions:\n\n{opts_desc}\n\"\"\"\n\n_reset_option_tmpl = \"\"\"\nreset_option(pat)\n\nReset one or more options to their default value.\n\nPass \"all\" as argument to reset all options.\n\nAvailable options:\n\n{opts_list}\n\nParameters\n----------\npat : str/regex\n    If specified only options matching `prefix*` will be reset.\n    Note: partial matches are supported for convenience, but unless you\n    use the full option name (e.g. x.y.z.option_name), your code may break\n    in future versions if new options with similar names are introduced.\n\nReturns\n-------\nNone\n\nNotes\n-----\nThe available options with its descriptions:\n\n{opts_desc}\n\"\"\"\n\n# bind the functions with their docstrings into a Callable\n# and use that as the functions exposed in pd.api\nget_option = CallableDynamicDoc(_get_option, _get_option_tmpl)\nset_option = CallableDynamicDoc(_set_option, _set_option_tmpl)\nreset_option = CallableDynamicDoc(_reset_option, _reset_option_tmpl)\ndescribe_option = CallableDynamicDoc(_describe_option, _describe_option_tmpl)\noptions = DictWrapper(_global_config)\n\n#\n# Functions for use by pandas developers, in addition to User - api\n\n\nclass option_context(object):\n    \"\"\"\n    Context manager to temporarily set options in the `with` statement context.\n\n    You need to invoke as ``option_context(pat, val, [(pat, val), ...])``.\n\n    Examples\n    --------\n\n    >>> with option_context('display.max_rows', 10, 'display.max_columns', 5):\n    ...     ...\n    \"\"\"\n\n    def __init__(self, *args):\n        if not (len(args) % 2 == 0 and len(args) >= 2):\n            raise ValueError('Need to invoke as'\n                             ' option_context(pat, val, [(pat, val), ...]).')\n\n        self.ops = list(zip(args[::2], args[1::2]))\n\n    def __enter__(self):\n        undo = []\n        for pat, val in self.ops:\n            undo.append((pat, _get_option(pat, silent=True)))\n\n        self.undo = undo\n\n        for pat, val in self.ops:\n            _set_option(pat, val, silent=True)\n\n    def __exit__(self, *args):\n        if self.undo:\n            for pat, val in self.undo:\n                _set_option(pat, val, silent=True)\n\n\ndef register_option(key, defval, doc='', validator=None, cb=None):\n    \"\"\"Register an option in the package-wide pandas config object\n\n    Parameters\n    ----------\n    key       - a fully-qualified key, e.g. \"x.y.option - z\".\n    defval    - the default value of the option\n    doc       - a string description of the option\n    validator - a function of a single argument, should raise `ValueError` if\n                called with a value which is not a legal value for the option.\n    cb        - a function of a single argument \"key\", which is called\n                immediately after an option value is set/reset. key is\n                the full name of the option.\n\n    Returns\n    -------\n    Nothing.\n\n    Raises\n    ------\n    ValueError if `validator` is specified and `defval` is not a valid value.\n\n    \"\"\"\n    import tokenize\n    import keyword\n    key = key.lower()\n\n    if key in _registered_options:\n        msg = \"Option '{key}' has already been registered\"\n        raise OptionError(msg.format(key=key))\n    if key in _reserved_keys:\n        msg = \"Option '{key}' is a reserved key\"\n        raise OptionError(msg.format(key=key))\n\n    # the default value should be legal\n    if validator:\n        validator(defval)\n\n    # walk the nested dict, creating dicts as needed along the path\n    path = key.split('.')\n\n    for k in path:\n        if not bool(re.match('^' + tokenize.Name + '$', k)):\n            raise ValueError(\"{k} is not a valid identifier\".format(k=k))\n        if keyword.iskeyword(k):\n            raise ValueError(\"{k} is a python keyword\".format(k=k))\n\n    cursor = _global_config\n    msg = \"Path prefix to option '{option}' is already an option\"\n    for i, p in enumerate(path[:-1]):\n        if not isinstance(cursor, dict):\n            raise OptionError(msg.format(option='.'.join(path[:i])))\n        if p not in cursor:\n            cursor[p] = {}\n        cursor = cursor[p]\n\n    if not isinstance(cursor, dict):\n        raise OptionError(msg.format(option='.'.join(path[:-1])))\n\n    cursor[path[-1]] = defval  # initialize\n\n    # save the option metadata\n    _registered_options[key] = RegisteredOption(key=key, defval=defval,\n                                                doc=doc, validator=validator,\n                                                cb=cb)\n\n\ndef deprecate_option(key, msg=None, rkey=None, removal_ver=None):\n    \"\"\"\n    Mark option `key` as deprecated, if code attempts to access this option,\n    a warning will be produced, using `msg` if given, or a default message\n    if not.\n    if `rkey` is given, any access to the key will be re-routed to `rkey`.\n\n    Neither the existence of `key` nor that if `rkey` is checked. If they\n    do not exist, any subsequence access will fail as usual, after the\n    deprecation warning is given.\n\n    Parameters\n    ----------\n    key - the name of the option to be deprecated. must be a fully-qualified\n          option name (e.g \"x.y.z.rkey\").\n\n    msg - (Optional) a warning message to output when the key is referenced.\n          if no message is given a default message will be emitted.\n\n    rkey - (Optional) the name of an option to reroute access to.\n           If specified, any referenced `key` will be re-routed to `rkey`\n           including set/get/reset.\n           rkey must be a fully-qualified option name (e.g \"x.y.z.rkey\").\n           used by the default message if no `msg` is specified.\n\n    removal_ver - (Optional) specifies the version in which this option will\n                  be removed. used by the default message if no `msg`\n                  is specified.\n\n    Returns\n    -------\n    Nothing\n\n    Raises\n    ------\n    OptionError - if key has already been deprecated.\n\n    \"\"\"\n\n    key = key.lower()\n\n    if key in _deprecated_options:\n        msg = \"Option '{key}' has already been defined as deprecated.\"\n        raise OptionError(msg.format(key=key))\n\n    _deprecated_options[key] = DeprecatedOption(key, msg, rkey, removal_ver)\n\n#\n# functions internal to the module\n\n\ndef _select_options(pat):\n    \"\"\"returns a list of keys matching `pat`\n\n    if pat==\"all\", returns all registered options\n    \"\"\"\n\n    # short-circuit for exact key\n    if pat in _registered_options:\n        return [pat]\n\n    # else look through all of them\n    keys = sorted(_registered_options.keys())\n    if pat == 'all':  # reserved key\n        return keys\n\n    return [k for k in keys if re.search(pat, k, re.I)]\n\n\ndef _get_root(key):\n    path = key.split('.')\n    cursor = _global_config\n    for p in path[:-1]:\n        cursor = cursor[p]\n    return cursor, path[-1]\n\n\ndef _is_deprecated(key):\n    \"\"\" Returns True if the given option has been deprecated \"\"\"\n\n    key = key.lower()\n    return key in _deprecated_options\n\n\ndef _get_deprecated_option(key):\n    \"\"\"\n    Retrieves the metadata for a deprecated option, if `key` is deprecated.\n\n    Returns\n    -------\n    DeprecatedOption (namedtuple) if key is deprecated, None otherwise\n    \"\"\"\n\n    try:\n        d = _deprecated_options[key]\n    except KeyError:\n        return None\n    else:\n        return d\n\n\ndef _get_registered_option(key):\n    \"\"\"\n    Retrieves the option metadata if `key` is a registered option.\n\n    Returns\n    -------\n    RegisteredOption (namedtuple) if key is deprecated, None otherwise\n    \"\"\"\n    return _registered_options.get(key)\n\n\ndef _translate_key(key):\n    \"\"\"\n    if key id deprecated and a replacement key defined, will return the\n    replacement key, otherwise returns `key` as - is\n    \"\"\"\n\n    d = _get_deprecated_option(key)\n    if d:\n        return d.rkey or key\n    else:\n        return key\n\n\ndef _warn_if_deprecated(key):\n    \"\"\"\n    Checks if `key` is a deprecated option and if so, prints a warning.\n\n    Returns\n    -------\n    bool - True if `key` is deprecated, False otherwise.\n    \"\"\"\n\n    d = _get_deprecated_option(key)\n    if d:\n        if d.msg:\n            print(d.msg)\n            warnings.warn(d.msg, FutureWarning)\n        else:\n            msg = \"'{key}' is deprecated\".format(key=key)\n            if d.removal_ver:\n                msg += (' and will be removed in {version}'\n                        .format(version=d.removal_ver))\n            if d.rkey:\n                msg += \", please use '{rkey}' instead.\".format(rkey=d.rkey)\n            else:\n                msg += ', please refrain from using it.'\n\n            warnings.warn(msg, FutureWarning)\n        return True\n    return False\n\n\ndef _build_option_description(k):\n    \"\"\" Builds a formatted description of a registered option and prints it \"\"\"\n\n    o = _get_registered_option(k)\n    d = _get_deprecated_option(k)\n\n    s = u('{k} ').format(k=k)\n\n    if o.doc:\n        s += '\\n'.join(o.doc.strip().split('\\n'))\n    else:\n        s += 'No description available.'\n\n    if o:\n        s += (u('\\n    [default: {default}] [currently: {current}]')\n              .format(default=o.defval, current=_get_option(k, True)))\n\n    if d:\n        s += u('\\n    (Deprecated')\n        s += (u(', use `{rkey}` instead.')\n              .format(rkey=d.rkey if d.rkey else ''))\n        s += u(')')\n\n    s += '\\n\\n'\n    return s\n\n\ndef pp_options_list(keys, width=80, _print=False):\n    \"\"\" Builds a concise listing of available options, grouped by prefix \"\"\"\n\n    from textwrap import wrap\n    from itertools import groupby\n\n    def pp(name, ks):\n        pfx = ('- ' + name + '.[' if name else '')\n        ls = wrap(', '.join(ks), width, initial_indent=pfx,\n                  subsequent_indent='  ', break_long_words=False)\n        if ls and ls[-1] and name:\n            ls[-1] = ls[-1] + ']'\n        return ls\n\n    ls = []\n    singles = [x for x in sorted(keys) if x.find('.') < 0]\n    if singles:\n        ls += pp('', singles)\n    keys = [x for x in keys if x.find('.') >= 0]\n\n    for k, g in groupby(sorted(keys), lambda x: x[:x.rfind('.')]):\n        ks = [x[len(k) + 1:] for x in list(g)]\n        ls += pp(k, ks)\n    s = '\\n'.join(ls)\n    if _print:\n        print(s)\n    else:\n        return s\n\n#\n# helpers\n\n\n@contextmanager\ndef config_prefix(prefix):\n    \"\"\"contextmanager for multiple invocations of API with a common prefix\n\n    supported API functions: (register / get / set )__option\n\n    Warning: This is not thread - safe, and won't work properly if you import\n    the API functions into your module using the \"from x import y\" construct.\n\n    Example:\n\n    import pandas.core.config as cf\n    with cf.config_prefix(\"display.font\"):\n        cf.register_option(\"color\", \"red\")\n        cf.register_option(\"size\", \" 5 pt\")\n        cf.set_option(size, \" 6 pt\")\n        cf.get_option(size)\n        ...\n\n        etc'\n\n    will register options \"display.font.color\", \"display.font.size\", set the\n    value of \"display.font.size\"... and so on.\n    \"\"\"\n\n    # Note: reset_option relies on set_option, and on key directly\n    # it does not fit in to this monkey-patching scheme\n\n    global register_option, get_option, set_option, reset_option\n\n    def wrap(func):\n        def inner(key, *args, **kwds):\n            pkey = '{prefix}.{key}'.format(prefix=prefix, key=key)\n            return func(pkey, *args, **kwds)\n\n        return inner\n\n    _register_option = register_option\n    _get_option = get_option\n    _set_option = set_option\n    set_option = wrap(set_option)\n    get_option = wrap(get_option)\n    register_option = wrap(register_option)\n    yield None\n    set_option = _set_option\n    get_option = _get_option\n    register_option = _register_option\n\n# These factories and methods are handy for use as the validator\n# arg in register_option\n\n\ndef is_type_factory(_type):\n    \"\"\"\n\n    Parameters\n    ----------\n    `_type` - a type to be compared against (e.g. type(x) == `_type`)\n\n    Returns\n    -------\n    validator - a function of a single argument x , which raises\n                ValueError if type(x) is not equal to `_type`\n\n    \"\"\"\n\n    def inner(x):\n        if type(x) != _type:\n            msg = \"Value must have type '{typ!s}'\"\n            raise ValueError(msg.format(typ=_type))\n\n    return inner\n\n\ndef is_instance_factory(_type):\n    \"\"\"\n\n    Parameters\n    ----------\n    `_type` - the type to be checked against\n\n    Returns\n    -------\n    validator - a function of a single argument x , which raises\n                ValueError if x is not an instance of `_type`\n\n    \"\"\"\n    if isinstance(_type, (tuple, list)):\n        _type = tuple(_type)\n        from pandas.io.formats.printing import pprint_thing\n        type_repr = \"|\".join(map(pprint_thing, _type))\n    else:\n        type_repr = \"'{typ}'\".format(typ=_type)\n\n    def inner(x):\n        if not isinstance(x, _type):\n            msg = \"Value must be an instance of {type_repr}\"\n            raise ValueError(msg.format(type_repr=type_repr))\n\n    return inner\n\n\ndef is_one_of_factory(legal_values):\n\n    callables = [c for c in legal_values if callable(c)]\n    legal_values = [c for c in legal_values if not callable(c)]\n\n    def inner(x):\n        from pandas.io.formats.printing import pprint_thing as pp\n        if x not in legal_values:\n\n            if not any(c(x) for c in callables):\n                pp_values = pp(\"|\".join(lmap(pp, legal_values)))\n                msg = \"Value must be one of {pp_values}\"\n                if len(callables):\n                    msg += \" or a callable\"\n                raise ValueError(msg.format(pp_values=pp_values))\n\n    return inner\n\n\n# common type validators, for convenience\n# usage: register_option(... , validator = is_int)\nis_int = is_type_factory(int)\nis_bool = is_type_factory(bool)\nis_float = is_type_factory(float)\nis_str = is_type_factory(str)\nis_unicode = is_type_factory(compat.text_type)\nis_text = is_instance_factory((str, bytes))\n\n\ndef is_callable(obj):\n    \"\"\"\n\n    Parameters\n    ----------\n    `obj` - the object to be checked\n\n    Returns\n    -------\n    validator - returns True if object is callable\n        raises ValueError otherwise.\n\n    \"\"\"\n    if not callable(obj):\n        raise ValueError(\"Value must be a callable\")\n    return True\n"
    },
    {
      "filename": "pandas/core/dtypes/common.py",
      "content": "\"\"\" common type operations \"\"\"\nimport numpy as np\n\nfrom pandas._libs import algos, lib\nfrom pandas._libs.interval import Interval\nfrom pandas._libs.tslibs import Period, Timestamp, conversion\nfrom pandas.compat import PY3, PY36, binary_type, string_types, text_type\n\nfrom pandas.core.dtypes.dtypes import (\n    CategoricalDtype, CategoricalDtypeType, DatetimeTZDtype, ExtensionDtype,\n    IntervalDtype, PandasExtensionDtype, PeriodDtype, _pandas_registry,\n    registry)\nfrom pandas.core.dtypes.generic import (\n    ABCCategorical, ABCCategoricalIndex, ABCDateOffset, ABCDatetimeIndex,\n    ABCIndexClass, ABCPeriodArray, ABCPeriodIndex, ABCSeries, ABCSparseArray,\n    ABCSparseSeries)\nfrom pandas.core.dtypes.inference import (  # noqa:F401\n    is_array_like, is_bool, is_complex, is_decimal, is_dict_like, is_file_like,\n    is_float, is_hashable, is_integer, is_interval, is_iterator, is_list_like,\n    is_named_tuple, is_nested_list_like, is_number, is_re, is_re_compilable,\n    is_scalar, is_sequence, is_string_like)\n\n_POSSIBLY_CAST_DTYPES = {np.dtype(t).name\n                         for t in ['O', 'int8', 'uint8', 'int16', 'uint16',\n                                   'int32', 'uint32', 'int64', 'uint64']}\n\n_NS_DTYPE = conversion.NS_DTYPE\n_TD_DTYPE = conversion.TD_DTYPE\n_INT64_DTYPE = np.dtype(np.int64)\n\n# oh the troubles to reduce import time\n_is_scipy_sparse = None\n\nensure_float64 = algos.ensure_float64\nensure_float32 = algos.ensure_float32\n\n_ensure_datetime64ns = conversion.ensure_datetime64ns\n_ensure_timedelta64ns = conversion.ensure_timedelta64ns\n\n\ndef ensure_float(arr):\n    \"\"\"\n    Ensure that an array object has a float dtype if possible.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array whose data type we want to enforce as float.\n\n    Returns\n    -------\n    float_arr : The original array cast to the float dtype if\n                possible. Otherwise, the original array is returned.\n    \"\"\"\n\n    if issubclass(arr.dtype.type, (np.integer, np.bool_)):\n        arr = arr.astype(float)\n    return arr\n\n\nensure_uint64 = algos.ensure_uint64\nensure_int64 = algos.ensure_int64\nensure_int32 = algos.ensure_int32\nensure_int16 = algos.ensure_int16\nensure_int8 = algos.ensure_int8\nensure_platform_int = algos.ensure_platform_int\nensure_object = algos.ensure_object\n\n\ndef ensure_categorical(arr):\n    \"\"\"\n    Ensure that an array-like object is a Categorical (if not already).\n\n    Parameters\n    ----------\n    arr : array-like\n        The array that we want to convert into a Categorical.\n\n    Returns\n    -------\n    cat_arr : The original array cast as a Categorical. If it already\n              is a Categorical, we return as is.\n    \"\"\"\n\n    if not is_categorical(arr):\n        from pandas import Categorical\n        arr = Categorical(arr)\n    return arr\n\n\ndef ensure_int64_or_float64(arr, copy=False):\n    \"\"\"\n    Ensure that an dtype array of some integer dtype\n    has an int64 dtype if possible\n    If it's not possible, potentially because of overflow,\n    convert the array to float64 instead.\n\n    Parameters\n    ----------\n    arr : array-like\n          The array whose data type we want to enforce.\n    copy: boolean\n          Whether to copy the original array or reuse\n          it in place, if possible.\n\n    Returns\n    -------\n    out_arr : The input array cast as int64 if\n              possible without overflow.\n              Otherwise the input array cast to float64.\n    \"\"\"\n    try:\n        return arr.astype('int64', copy=copy, casting='safe')\n    except TypeError:\n        return arr.astype('float64', copy=copy)\n\n\ndef is_object_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether an array-like or dtype is of the object dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array-like or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like or dtype is of the object dtype.\n\n    Examples\n    --------\n    >>> is_object_dtype(object)\n    True\n    >>> is_object_dtype(int)\n    False\n    >>> is_object_dtype(np.array([], dtype=object))\n    True\n    >>> is_object_dtype(np.array([], dtype=int))\n    False\n    >>> is_object_dtype([1, 2, 3])\n    False\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    tipo = _get_dtype_type(arr_or_dtype)\n    return issubclass(tipo, np.object_)\n\n\ndef is_sparse(arr):\n    \"\"\"\n    Check whether an array-like is a 1-D pandas sparse array.\n\n    Check that the one-dimensional array-like is a pandas sparse array.\n    Returns True if it is a pandas sparse array, not another type of\n    sparse array.\n\n    Parameters\n    ----------\n    arr : array-like\n        Array-like to check.\n\n    Returns\n    -------\n    bool\n        Whether or not the array-like is a pandas sparse array.\n\n    See Also\n    --------\n    DataFrame.to_sparse : Convert DataFrame to a SparseDataFrame.\n    Series.to_sparse : Convert Series to SparseSeries.\n    Series.to_dense : Return dense representation of a Series.\n\n    Examples\n    --------\n    Returns `True` if the parameter is a 1-D pandas sparse array.\n\n    >>> is_sparse(pd.SparseArray([0, 0, 1, 0]))\n    True\n    >>> is_sparse(pd.SparseSeries([0, 0, 1, 0]))\n    True\n\n    Returns `False` if the parameter is not sparse.\n\n    >>> is_sparse(np.array([0, 0, 1, 0]))\n    False\n    >>> is_sparse(pd.Series([0, 1, 0, 0]))\n    False\n\n    Returns `False` if the parameter is not a pandas sparse array.\n\n    >>> from scipy.sparse import bsr_matrix\n    >>> is_sparse(bsr_matrix([0, 1, 0, 0]))\n    False\n\n    Returns `False` if the parameter has more than one dimension.\n\n    >>> df = pd.SparseDataFrame([389., 24., 80.5, np.nan],\n                                columns=['max_speed'],\n                                index=['falcon', 'parrot', 'lion', 'monkey'])\n    >>> is_sparse(df)\n    False\n    >>> is_sparse(df.max_speed)\n    True\n    \"\"\"\n    from pandas.core.arrays.sparse import SparseDtype\n\n    dtype = getattr(arr, 'dtype', arr)\n    return isinstance(dtype, SparseDtype)\n\n\ndef is_scipy_sparse(arr):\n    \"\"\"\n    Check whether an array-like is a scipy.sparse.spmatrix instance.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array-like to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like is a\n              scipy.sparse.spmatrix instance.\n\n    Notes\n    -----\n    If scipy is not installed, this function will always return False.\n\n    Examples\n    --------\n    >>> from scipy.sparse import bsr_matrix\n    >>> is_scipy_sparse(bsr_matrix([1, 2, 3]))\n    True\n    >>> is_scipy_sparse(pd.SparseArray([1, 2, 3]))\n    False\n    >>> is_scipy_sparse(pd.SparseSeries([1, 2, 3]))\n    False\n    \"\"\"\n\n    global _is_scipy_sparse\n\n    if _is_scipy_sparse is None:\n        try:\n            from scipy.sparse import issparse as _is_scipy_sparse\n        except ImportError:\n            _is_scipy_sparse = lambda _: False\n\n    return _is_scipy_sparse(arr)\n\n\ndef is_categorical(arr):\n    \"\"\"\n    Check whether an array-like is a Categorical instance.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array-like to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like is of a Categorical instance.\n\n    Examples\n    --------\n    >>> is_categorical([1, 2, 3])\n    False\n\n    Categoricals, Series Categoricals, and CategoricalIndex will return True.\n\n    >>> cat = pd.Categorical([1, 2, 3])\n    >>> is_categorical(cat)\n    True\n    >>> is_categorical(pd.Series(cat))\n    True\n    >>> is_categorical(pd.CategoricalIndex([1, 2, 3]))\n    True\n    \"\"\"\n\n    return isinstance(arr, ABCCategorical) or is_categorical_dtype(arr)\n\n\ndef is_datetimetz(arr):\n    \"\"\"\n    Check whether an array-like is a datetime array-like with a timezone\n    component in its dtype.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array-like to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like is a datetime array-like with\n              a timezone component in its dtype.\n\n    Examples\n    --------\n    >>> is_datetimetz([1, 2, 3])\n    False\n\n    Although the following examples are both DatetimeIndex objects,\n    the first one returns False because it has no timezone component\n    unlike the second one, which returns True.\n\n    >>> is_datetimetz(pd.DatetimeIndex([1, 2, 3]))\n    False\n    >>> is_datetimetz(pd.DatetimeIndex([1, 2, 3], tz=\"US/Eastern\"))\n    True\n\n    The object need not be a DatetimeIndex object. It just needs to have\n    a dtype which has a timezone component.\n\n    >>> dtype = DatetimeTZDtype(\"ns\", tz=\"US/Eastern\")\n    >>> s = pd.Series([], dtype=dtype)\n    >>> is_datetimetz(s)\n    True\n    \"\"\"\n\n    # TODO: do we need this function?\n    # It seems like a repeat of is_datetime64tz_dtype.\n\n    return ((isinstance(arr, ABCDatetimeIndex) and\n             getattr(arr, 'tz', None) is not None) or\n            is_datetime64tz_dtype(arr))\n\n\ndef is_offsetlike(arr_or_obj):\n    \"\"\"\n    Check if obj or all elements of list-like is DateOffset\n\n    Parameters\n    ----------\n    arr_or_obj : object\n\n    Returns\n    -------\n    boolean : Whether the object is a DateOffset or listlike of DatetOffsets\n\n    Examples\n    --------\n    >>> is_offsetlike(pd.DateOffset(days=1))\n    True\n    >>> is_offsetlike('offset')\n    False\n    >>> is_offsetlike([pd.offsets.Minute(4), pd.offsets.MonthEnd()])\n    True\n    >>> is_offsetlike(np.array([pd.DateOffset(months=3), pd.Timestamp.now()]))\n    False\n    \"\"\"\n    if isinstance(arr_or_obj, ABCDateOffset):\n        return True\n    elif (is_list_like(arr_or_obj) and len(arr_or_obj) and\n          is_object_dtype(arr_or_obj)):\n        return all(isinstance(x, ABCDateOffset) for x in arr_or_obj)\n    return False\n\n\ndef is_period(arr):\n    \"\"\"\n    Check whether an array-like is a periodical index.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array-like to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like is a periodical index.\n\n    Examples\n    --------\n    >>> is_period([1, 2, 3])\n    False\n    >>> is_period(pd.Index([1, 2, 3]))\n    False\n    >>> is_period(pd.PeriodIndex([\"2017-01-01\"], freq=\"D\"))\n    True\n    \"\"\"\n\n    # TODO: do we need this function?\n    # It seems like a repeat of is_period_arraylike.\n    return isinstance(arr, ABCPeriodIndex) or is_period_arraylike(arr)\n\n\ndef is_datetime64_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether an array-like or dtype is of the datetime64 dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array-like or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like or dtype is of\n              the datetime64 dtype.\n\n    Examples\n    --------\n    >>> is_datetime64_dtype(object)\n    False\n    >>> is_datetime64_dtype(np.datetime64)\n    True\n    >>> is_datetime64_dtype(np.array([], dtype=int))\n    False\n    >>> is_datetime64_dtype(np.array([], dtype=np.datetime64))\n    True\n    >>> is_datetime64_dtype([1, 2, 3])\n    False\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    try:\n        tipo = _get_dtype_type(arr_or_dtype)\n    except (TypeError, UnicodeEncodeError):\n        return False\n    return issubclass(tipo, np.datetime64)\n\n\ndef is_datetime64tz_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether an array-like or dtype is of a DatetimeTZDtype dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array-like or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like or dtype is of\n              a DatetimeTZDtype dtype.\n\n    Examples\n    --------\n    >>> is_datetime64tz_dtype(object)\n    False\n    >>> is_datetime64tz_dtype([1, 2, 3])\n    False\n    >>> is_datetime64tz_dtype(pd.DatetimeIndex([1, 2, 3]))  # tz-naive\n    False\n    >>> is_datetime64tz_dtype(pd.DatetimeIndex([1, 2, 3], tz=\"US/Eastern\"))\n    True\n\n    >>> dtype = DatetimeTZDtype(\"ns\", tz=\"US/Eastern\")\n    >>> s = pd.Series([], dtype=dtype)\n    >>> is_datetime64tz_dtype(dtype)\n    True\n    >>> is_datetime64tz_dtype(s)\n    True\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    return DatetimeTZDtype.is_dtype(arr_or_dtype)\n\n\ndef is_timedelta64_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether an array-like or dtype is of the timedelta64 dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array-like or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like or dtype is\n              of the timedelta64 dtype.\n\n    Examples\n    --------\n    >>> is_timedelta64_dtype(object)\n    False\n    >>> is_timedelta64_dtype(np.timedelta64)\n    True\n    >>> is_timedelta64_dtype([1, 2, 3])\n    False\n    >>> is_timedelta64_dtype(pd.Series([], dtype=\"timedelta64[ns]\"))\n    True\n    >>> is_timedelta64_dtype('0 days')\n    False\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    try:\n        tipo = _get_dtype_type(arr_or_dtype)\n    except (TypeError, ValueError, SyntaxError):\n        return False\n    return issubclass(tipo, np.timedelta64)\n\n\ndef is_period_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether an array-like or dtype is of the Period dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array-like or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like or dtype is of the Period dtype.\n\n    Examples\n    --------\n    >>> is_period_dtype(object)\n    False\n    >>> is_period_dtype(PeriodDtype(freq=\"D\"))\n    True\n    >>> is_period_dtype([1, 2, 3])\n    False\n    >>> is_period_dtype(pd.Period(\"2017-01-01\"))\n    False\n    >>> is_period_dtype(pd.PeriodIndex([], freq=\"A\"))\n    True\n    \"\"\"\n\n    # TODO: Consider making Period an instance of PeriodDtype\n    if arr_or_dtype is None:\n        return False\n    return PeriodDtype.is_dtype(arr_or_dtype)\n\n\ndef is_interval_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether an array-like or dtype is of the Interval dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array-like or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like or dtype is\n              of the Interval dtype.\n\n    Examples\n    --------\n    >>> is_interval_dtype(object)\n    False\n    >>> is_interval_dtype(IntervalDtype())\n    True\n    >>> is_interval_dtype([1, 2, 3])\n    False\n    >>>\n    >>> interval = pd.Interval(1, 2, closed=\"right\")\n    >>> is_interval_dtype(interval)\n    False\n    >>> is_interval_dtype(pd.IntervalIndex([interval]))\n    True\n    \"\"\"\n\n    # TODO: Consider making Interval an instance of IntervalDtype\n    if arr_or_dtype is None:\n        return False\n    return IntervalDtype.is_dtype(arr_or_dtype)\n\n\ndef is_categorical_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether an array-like or dtype is of the Categorical dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array-like or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like or dtype is\n              of the Categorical dtype.\n\n    Examples\n    --------\n    >>> is_categorical_dtype(object)\n    False\n    >>> is_categorical_dtype(CategoricalDtype())\n    True\n    >>> is_categorical_dtype([1, 2, 3])\n    False\n    >>> is_categorical_dtype(pd.Categorical([1, 2, 3]))\n    True\n    >>> is_categorical_dtype(pd.CategoricalIndex([1, 2, 3]))\n    True\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    return CategoricalDtype.is_dtype(arr_or_dtype)\n\n\ndef is_string_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of the string dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of the string dtype.\n\n    Examples\n    --------\n    >>> is_string_dtype(str)\n    True\n    >>> is_string_dtype(object)\n    True\n    >>> is_string_dtype(int)\n    False\n    >>>\n    >>> is_string_dtype(np.array(['a', 'b']))\n    True\n    >>> is_string_dtype(pd.Series([1, 2]))\n    False\n    \"\"\"\n\n    # TODO: gh-15585: consider making the checks stricter.\n\n    if arr_or_dtype is None:\n        return False\n    try:\n        dtype = _get_dtype(arr_or_dtype)\n        return dtype.kind in ('O', 'S', 'U') and not is_period_dtype(dtype)\n    except TypeError:\n        return False\n\n\ndef is_period_arraylike(arr):\n    \"\"\"\n    Check whether an array-like is a periodical array-like or PeriodIndex.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array-like to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like is a periodical\n              array-like or PeriodIndex instance.\n\n    Examples\n    --------\n    >>> is_period_arraylike([1, 2, 3])\n    False\n    >>> is_period_arraylike(pd.Index([1, 2, 3]))\n    False\n    >>> is_period_arraylike(pd.PeriodIndex([\"2017-01-01\"], freq=\"D\"))\n    True\n    \"\"\"\n\n    if isinstance(arr, (ABCPeriodIndex, ABCPeriodArray)):\n        return True\n    elif isinstance(arr, (np.ndarray, ABCSeries)):\n        return is_period_dtype(arr.dtype)\n    return getattr(arr, 'inferred_type', None) == 'period'\n\n\ndef is_datetime_arraylike(arr):\n    \"\"\"\n    Check whether an array-like is a datetime array-like or DatetimeIndex.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array-like to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like is a datetime\n              array-like or DatetimeIndex.\n\n    Examples\n    --------\n    >>> is_datetime_arraylike([1, 2, 3])\n    False\n    >>> is_datetime_arraylike(pd.Index([1, 2, 3]))\n    False\n    >>> is_datetime_arraylike(pd.DatetimeIndex([1, 2, 3]))\n    True\n    \"\"\"\n\n    if isinstance(arr, ABCDatetimeIndex):\n        return True\n    elif isinstance(arr, (np.ndarray, ABCSeries)):\n        return arr.dtype == object and lib.infer_dtype(arr) == 'datetime'\n    return getattr(arr, 'inferred_type', None) == 'datetime'\n\n\ndef is_datetimelike(arr):\n    \"\"\"\n    Check whether an array-like is a datetime-like array-like.\n\n    Acceptable datetime-like objects are (but not limited to) datetime\n    indices, periodic indices, and timedelta indices.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array-like to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like is a datetime-like array-like.\n\n    Examples\n    --------\n    >>> is_datetimelike([1, 2, 3])\n    False\n    >>> is_datetimelike(pd.Index([1, 2, 3]))\n    False\n    >>> is_datetimelike(pd.DatetimeIndex([1, 2, 3]))\n    True\n    >>> is_datetimelike(pd.DatetimeIndex([1, 2, 3], tz=\"US/Eastern\"))\n    True\n    >>> is_datetimelike(pd.PeriodIndex([], freq=\"A\"))\n    True\n    >>> is_datetimelike(np.array([], dtype=np.datetime64))\n    True\n    >>> is_datetimelike(pd.Series([], dtype=\"timedelta64[ns]\"))\n    True\n    >>>\n    >>> dtype = DatetimeTZDtype(\"ns\", tz=\"US/Eastern\")\n    >>> s = pd.Series([], dtype=dtype)\n    >>> is_datetimelike(s)\n    True\n    \"\"\"\n\n    return (is_datetime64_dtype(arr) or is_datetime64tz_dtype(arr) or\n            is_timedelta64_dtype(arr) or\n            isinstance(arr, ABCPeriodIndex) or\n            is_datetimetz(arr))\n\n\ndef is_dtype_equal(source, target):\n    \"\"\"\n    Check if two dtypes are equal.\n\n    Parameters\n    ----------\n    source : The first dtype to compare\n    target : The second dtype to compare\n\n    Returns\n    ----------\n    boolean : Whether or not the two dtypes are equal.\n\n    Examples\n    --------\n    >>> is_dtype_equal(int, float)\n    False\n    >>> is_dtype_equal(\"int\", int)\n    True\n    >>> is_dtype_equal(object, \"category\")\n    False\n    >>> is_dtype_equal(CategoricalDtype(), \"category\")\n    True\n    >>> is_dtype_equal(DatetimeTZDtype(), \"datetime64\")\n    False\n    \"\"\"\n\n    try:\n        source = _get_dtype(source)\n        target = _get_dtype(target)\n        return source == target\n    except (TypeError, AttributeError):\n\n        # invalid comparison\n        # object == category will hit this\n        return False\n\n\ndef is_dtype_union_equal(source, target):\n    \"\"\"\n    Check whether two arrays have compatible dtypes to do a union.\n    numpy types are checked with ``is_dtype_equal``. Extension types are\n    checked separately.\n\n    Parameters\n    ----------\n    source : The first dtype to compare\n    target : The second dtype to compare\n\n    Returns\n    ----------\n    boolean : Whether or not the two dtypes are equal.\n\n    >>> is_dtype_equal(\"int\", int)\n    True\n\n    >>> is_dtype_equal(CategoricalDtype(['a', 'b'],\n    ...                CategoricalDtype(['b', 'c']))\n    True\n\n    >>> is_dtype_equal(CategoricalDtype(['a', 'b'],\n    ...                CategoricalDtype(['b', 'c'], ordered=True))\n    False\n    \"\"\"\n    source = _get_dtype(source)\n    target = _get_dtype(target)\n    if is_categorical_dtype(source) and is_categorical_dtype(target):\n        # ordered False for both\n        return source.ordered is target.ordered\n    return is_dtype_equal(source, target)\n\n\ndef is_any_int_dtype(arr_or_dtype):\n    \"\"\"Check whether the provided array or dtype is of an integer dtype.\n\n    In this function, timedelta64 instances are also considered \"any-integer\"\n    type objects and will return True.\n\n    This function is internal and should not be exposed in the public API.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of an integer dtype.\n\n    Examples\n    --------\n    >>> is_any_int_dtype(str)\n    False\n    >>> is_any_int_dtype(int)\n    True\n    >>> is_any_int_dtype(float)\n    False\n    >>> is_any_int_dtype(np.uint64)\n    True\n    >>> is_any_int_dtype(np.datetime64)\n    False\n    >>> is_any_int_dtype(np.timedelta64)\n    True\n    >>> is_any_int_dtype(np.array(['a', 'b']))\n    False\n    >>> is_any_int_dtype(pd.Series([1, 2]))\n    True\n    >>> is_any_int_dtype(np.array([], dtype=np.timedelta64))\n    True\n    >>> is_any_int_dtype(pd.Index([1, 2.]))  # float\n    False\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    tipo = _get_dtype_type(arr_or_dtype)\n    return issubclass(tipo, np.integer)\n\n\ndef is_integer_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of an integer dtype.\n\n    Unlike in `in_any_int_dtype`, timedelta64 instances will return False.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of an integer dtype\n              and not an instance of timedelta64.\n\n    Examples\n    --------\n    >>> is_integer_dtype(str)\n    False\n    >>> is_integer_dtype(int)\n    True\n    >>> is_integer_dtype(float)\n    False\n    >>> is_integer_dtype(np.uint64)\n    True\n    >>> is_integer_dtype(np.datetime64)\n    False\n    >>> is_integer_dtype(np.timedelta64)\n    False\n    >>> is_integer_dtype(np.array(['a', 'b']))\n    False\n    >>> is_integer_dtype(pd.Series([1, 2]))\n    True\n    >>> is_integer_dtype(np.array([], dtype=np.timedelta64))\n    False\n    >>> is_integer_dtype(pd.Index([1, 2.]))  # float\n    False\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    tipo = _get_dtype_type(arr_or_dtype)\n    return (issubclass(tipo, np.integer) and\n            not issubclass(tipo, (np.datetime64, np.timedelta64)))\n\n\ndef is_signed_integer_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of a signed integer dtype.\n\n    Unlike in `in_any_int_dtype`, timedelta64 instances will return False.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of a signed integer dtype\n              and not an instance of timedelta64.\n\n    Examples\n    --------\n    >>> is_signed_integer_dtype(str)\n    False\n    >>> is_signed_integer_dtype(int)\n    True\n    >>> is_signed_integer_dtype(float)\n    False\n    >>> is_signed_integer_dtype(np.uint64)  # unsigned\n    False\n    >>> is_signed_integer_dtype(np.datetime64)\n    False\n    >>> is_signed_integer_dtype(np.timedelta64)\n    False\n    >>> is_signed_integer_dtype(np.array(['a', 'b']))\n    False\n    >>> is_signed_integer_dtype(pd.Series([1, 2]))\n    True\n    >>> is_signed_integer_dtype(np.array([], dtype=np.timedelta64))\n    False\n    >>> is_signed_integer_dtype(pd.Index([1, 2.]))  # float\n    False\n    >>> is_signed_integer_dtype(np.array([1, 2], dtype=np.uint32))  # unsigned\n    False\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    tipo = _get_dtype_type(arr_or_dtype)\n    return (issubclass(tipo, np.signedinteger) and\n            not issubclass(tipo, (np.datetime64, np.timedelta64)))\n\n\ndef is_unsigned_integer_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of an unsigned integer dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of an\n              unsigned integer dtype.\n\n    Examples\n    --------\n    >>> is_unsigned_integer_dtype(str)\n    False\n    >>> is_unsigned_integer_dtype(int)  # signed\n    False\n    >>> is_unsigned_integer_dtype(float)\n    False\n    >>> is_unsigned_integer_dtype(np.uint64)\n    True\n    >>> is_unsigned_integer_dtype(np.array(['a', 'b']))\n    False\n    >>> is_unsigned_integer_dtype(pd.Series([1, 2]))  # signed\n    False\n    >>> is_unsigned_integer_dtype(pd.Index([1, 2.]))  # float\n    False\n    >>> is_unsigned_integer_dtype(np.array([1, 2], dtype=np.uint32))\n    True\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    tipo = _get_dtype_type(arr_or_dtype)\n    return (issubclass(tipo, np.unsignedinteger) and\n            not issubclass(tipo, (np.datetime64, np.timedelta64)))\n\n\ndef is_int64_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of the int64 dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of the int64 dtype.\n\n    Notes\n    -----\n    Depending on system architecture, the return value of `is_int64_dtype(\n    int)` will be True if the OS uses 64-bit integers and False if the OS\n    uses 32-bit integers.\n\n    Examples\n    --------\n    >>> is_int64_dtype(str)\n    False\n    >>> is_int64_dtype(np.int32)\n    False\n    >>> is_int64_dtype(np.int64)\n    True\n    >>> is_int64_dtype(float)\n    False\n    >>> is_int64_dtype(np.uint64)  # unsigned\n    False\n    >>> is_int64_dtype(np.array(['a', 'b']))\n    False\n    >>> is_int64_dtype(np.array([1, 2], dtype=np.int64))\n    True\n    >>> is_int64_dtype(pd.Index([1, 2.]))  # float\n    False\n    >>> is_int64_dtype(np.array([1, 2], dtype=np.uint32))  # unsigned\n    False\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    tipo = _get_dtype_type(arr_or_dtype)\n    return issubclass(tipo, np.int64)\n\n\ndef is_int_or_datetime_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of an\n    integer, timedelta64, or datetime64 dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of an\n              integer, timedelta64, or datetime64 dtype.\n\n    Examples\n    --------\n    >>> is_int_or_datetime_dtype(str)\n    False\n    >>> is_int_or_datetime_dtype(int)\n    True\n    >>> is_int_or_datetime_dtype(float)\n    False\n    >>> is_int_or_datetime_dtype(np.uint64)\n    True\n    >>> is_int_or_datetime_dtype(np.datetime64)\n    True\n    >>> is_int_or_datetime_dtype(np.timedelta64)\n    True\n    >>> is_int_or_datetime_dtype(np.array(['a', 'b']))\n    False\n    >>> is_int_or_datetime_dtype(pd.Series([1, 2]))\n    True\n    >>> is_int_or_datetime_dtype(np.array([], dtype=np.timedelta64))\n    True\n    >>> is_int_or_datetime_dtype(np.array([], dtype=np.datetime64))\n    True\n    >>> is_int_or_datetime_dtype(pd.Index([1, 2.]))  # float\n    False\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    tipo = _get_dtype_type(arr_or_dtype)\n    return (issubclass(tipo, np.integer) or\n            issubclass(tipo, (np.datetime64, np.timedelta64)))\n\n\ndef is_datetime64_any_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of the datetime64 dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of the datetime64 dtype.\n\n    Examples\n    --------\n    >>> is_datetime64_any_dtype(str)\n    False\n    >>> is_datetime64_any_dtype(int)\n    False\n    >>> is_datetime64_any_dtype(np.datetime64)  # can be tz-naive\n    True\n    >>> is_datetime64_any_dtype(DatetimeTZDtype(\"ns\", \"US/Eastern\"))\n    True\n    >>> is_datetime64_any_dtype(np.array(['a', 'b']))\n    False\n    >>> is_datetime64_any_dtype(np.array([1, 2]))\n    False\n    >>> is_datetime64_any_dtype(np.array([], dtype=np.datetime64))\n    True\n    >>> is_datetime64_any_dtype(pd.DatetimeIndex([1, 2, 3],\n                                dtype=np.datetime64))\n    True\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    return (is_datetime64_dtype(arr_or_dtype) or\n            is_datetime64tz_dtype(arr_or_dtype))\n\n\ndef is_datetime64_ns_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of the datetime64[ns] dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of the datetime64[ns] dtype.\n\n    Examples\n    --------\n    >>> is_datetime64_ns_dtype(str)\n    False\n    >>> is_datetime64_ns_dtype(int)\n    False\n    >>> is_datetime64_ns_dtype(np.datetime64)  # no unit\n    False\n    >>> is_datetime64_ns_dtype(DatetimeTZDtype(\"ns\", \"US/Eastern\"))\n    True\n    >>> is_datetime64_ns_dtype(np.array(['a', 'b']))\n    False\n    >>> is_datetime64_ns_dtype(np.array([1, 2]))\n    False\n    >>> is_datetime64_ns_dtype(np.array([], dtype=np.datetime64))  # no unit\n    False\n    >>> is_datetime64_ns_dtype(np.array([],\n                               dtype=\"datetime64[ps]\"))  # wrong unit\n    False\n    >>> is_datetime64_ns_dtype(pd.DatetimeIndex([1, 2, 3],\n                               dtype=np.datetime64))  # has 'ns' unit\n    True\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    try:\n        tipo = _get_dtype(arr_or_dtype)\n    except TypeError:\n        if is_datetime64tz_dtype(arr_or_dtype):\n            tipo = _get_dtype(arr_or_dtype.dtype)\n        else:\n            return False\n    return tipo == _NS_DTYPE or getattr(tipo, 'base', None) == _NS_DTYPE\n\n\ndef is_timedelta64_ns_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of the timedelta64[ns] dtype.\n\n    This is a very specific dtype, so generic ones like `np.timedelta64`\n    will return False if passed into this function.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of the\n              timedelta64[ns] dtype.\n\n    Examples\n    --------\n    >>> is_timedelta64_ns_dtype(np.dtype('m8[ns]'))\n    True\n    >>> is_timedelta64_ns_dtype(np.dtype('m8[ps]'))  # Wrong frequency\n    False\n    >>> is_timedelta64_ns_dtype(np.array([1, 2], dtype='m8[ns]'))\n    True\n    >>> is_timedelta64_ns_dtype(np.array([1, 2], dtype=np.timedelta64))\n    False\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    try:\n        tipo = _get_dtype(arr_or_dtype)\n        return tipo == _TD_DTYPE\n    except TypeError:\n        return False\n\n\ndef is_datetime_or_timedelta_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of\n    a timedelta64 or datetime64 dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of a\n              timedelta64, or datetime64 dtype.\n\n    Examples\n    --------\n    >>> is_datetime_or_timedelta_dtype(str)\n    False\n    >>> is_datetime_or_timedelta_dtype(int)\n    False\n    >>> is_datetime_or_timedelta_dtype(np.datetime64)\n    True\n    >>> is_datetime_or_timedelta_dtype(np.timedelta64)\n    True\n    >>> is_datetime_or_timedelta_dtype(np.array(['a', 'b']))\n    False\n    >>> is_datetime_or_timedelta_dtype(pd.Series([1, 2]))\n    False\n    >>> is_datetime_or_timedelta_dtype(np.array([], dtype=np.timedelta64))\n    True\n    >>> is_datetime_or_timedelta_dtype(np.array([], dtype=np.datetime64))\n    True\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    tipo = _get_dtype_type(arr_or_dtype)\n    return issubclass(tipo, (np.datetime64, np.timedelta64))\n\n\ndef _is_unorderable_exception(e):\n    \"\"\"\n    Check if the exception raised is an unorderable exception.\n\n    The error message differs for 3 <= PY <= 3.5 and PY >= 3.6, so\n    we need to condition based on Python version.\n\n    Parameters\n    ----------\n    e : Exception or sub-class\n        The exception object to check.\n\n    Returns\n    -------\n    boolean : Whether or not the exception raised is an unorderable exception.\n    \"\"\"\n\n    if PY36:\n        return \"'>' not supported between instances of\" in str(e)\n\n    elif PY3:\n        return 'unorderable' in str(e)\n    return False\n\n\ndef is_numeric_v_string_like(a, b):\n    \"\"\"\n    Check if we are comparing a string-like object to a numeric ndarray.\n\n    NumPy doesn't like to compare such objects, especially numeric arrays\n    and scalar string-likes.\n\n    Parameters\n    ----------\n    a : array-like, scalar\n        The first object to check.\n    b : array-like, scalar\n        The second object to check.\n\n    Returns\n    -------\n    boolean : Whether we return a comparing a string-like\n              object to a numeric array.\n\n    Examples\n    --------\n    >>> is_numeric_v_string_like(1, 1)\n    False\n    >>> is_numeric_v_string_like(\"foo\", \"foo\")\n    False\n    >>> is_numeric_v_string_like(1, \"foo\")  # non-array numeric\n    False\n    >>> is_numeric_v_string_like(np.array([1]), \"foo\")\n    True\n    >>> is_numeric_v_string_like(\"foo\", np.array([1]))  # symmetric check\n    True\n    >>> is_numeric_v_string_like(np.array([1, 2]), np.array([\"foo\"]))\n    True\n    >>> is_numeric_v_string_like(np.array([\"foo\"]), np.array([1, 2]))\n    True\n    >>> is_numeric_v_string_like(np.array([1]), np.array([2]))\n    False\n    >>> is_numeric_v_string_like(np.array([\"foo\"]), np.array([\"foo\"]))\n    False\n    \"\"\"\n\n    is_a_array = isinstance(a, np.ndarray)\n    is_b_array = isinstance(b, np.ndarray)\n\n    is_a_numeric_array = is_a_array and is_numeric_dtype(a)\n    is_b_numeric_array = is_b_array and is_numeric_dtype(b)\n    is_a_string_array = is_a_array and is_string_like_dtype(a)\n    is_b_string_array = is_b_array and is_string_like_dtype(b)\n\n    is_a_scalar_string_like = not is_a_array and is_string_like(a)\n    is_b_scalar_string_like = not is_b_array and is_string_like(b)\n\n    return ((is_a_numeric_array and is_b_scalar_string_like) or\n            (is_b_numeric_array and is_a_scalar_string_like) or\n            (is_a_numeric_array and is_b_string_array) or\n            (is_b_numeric_array and is_a_string_array))\n\n\ndef is_datetimelike_v_numeric(a, b):\n    \"\"\"\n    Check if we are comparing a datetime-like object to a numeric object.\n\n    By \"numeric,\" we mean an object that is either of an int or float dtype.\n\n    Parameters\n    ----------\n    a : array-like, scalar\n        The first object to check.\n    b : array-like, scalar\n        The second object to check.\n\n    Returns\n    -------\n    boolean : Whether we return a comparing a datetime-like\n              to a numeric object.\n\n    Examples\n    --------\n    >>> dt = np.datetime64(pd.datetime(2017, 1, 1))\n    >>>\n    >>> is_datetimelike_v_numeric(1, 1)\n    False\n    >>> is_datetimelike_v_numeric(dt, dt)\n    False\n    >>> is_datetimelike_v_numeric(1, dt)\n    True\n    >>> is_datetimelike_v_numeric(dt, 1)  # symmetric check\n    True\n    >>> is_datetimelike_v_numeric(np.array([dt]), 1)\n    True\n    >>> is_datetimelike_v_numeric(np.array([1]), dt)\n    True\n    >>> is_datetimelike_v_numeric(np.array([dt]), np.array([1]))\n    True\n    >>> is_datetimelike_v_numeric(np.array([1]), np.array([2]))\n    False\n    >>> is_datetimelike_v_numeric(np.array([dt]), np.array([dt]))\n    False\n    \"\"\"\n\n    if not hasattr(a, 'dtype'):\n        a = np.asarray(a)\n    if not hasattr(b, 'dtype'):\n        b = np.asarray(b)\n\n    def is_numeric(x):\n        \"\"\"\n        Check if an object has a numeric dtype (i.e. integer or float).\n        \"\"\"\n        return is_integer_dtype(x) or is_float_dtype(x)\n\n    is_datetimelike = needs_i8_conversion\n    return ((is_datetimelike(a) and is_numeric(b)) or\n            (is_datetimelike(b) and is_numeric(a)))\n\n\ndef is_datetimelike_v_object(a, b):\n    \"\"\"\n    Check if we are comparing a datetime-like object to an object instance.\n\n    Parameters\n    ----------\n    a : array-like, scalar\n        The first object to check.\n    b : array-like, scalar\n        The second object to check.\n\n    Returns\n    -------\n    boolean : Whether we return a comparing a datetime-like\n              to an object instance.\n\n    Examples\n    --------\n    >>> obj = object()\n    >>> dt = np.datetime64(pd.datetime(2017, 1, 1))\n    >>>\n    >>> is_datetimelike_v_object(obj, obj)\n    False\n    >>> is_datetimelike_v_object(dt, dt)\n    False\n    >>> is_datetimelike_v_object(obj, dt)\n    True\n    >>> is_datetimelike_v_object(dt, obj)  # symmetric check\n    True\n    >>> is_datetimelike_v_object(np.array([dt]), obj)\n    True\n    >>> is_datetimelike_v_object(np.array([obj]), dt)\n    True\n    >>> is_datetimelike_v_object(np.array([dt]), np.array([obj]))\n    True\n    >>> is_datetimelike_v_object(np.array([obj]), np.array([obj]))\n    False\n    >>> is_datetimelike_v_object(np.array([dt]), np.array([1]))\n    False\n    >>> is_datetimelike_v_object(np.array([dt]), np.array([dt]))\n    False\n    \"\"\"\n\n    if not hasattr(a, 'dtype'):\n        a = np.asarray(a)\n    if not hasattr(b, 'dtype'):\n        b = np.asarray(b)\n\n    is_datetimelike = needs_i8_conversion\n    return ((is_datetimelike(a) and is_object_dtype(b)) or\n            (is_datetimelike(b) and is_object_dtype(a)))\n\n\ndef needs_i8_conversion(arr_or_dtype):\n    \"\"\"\n    Check whether the array or dtype should be converted to int64.\n\n    An array-like or dtype \"needs\" such a conversion if the array-like\n    or dtype is of a datetime-like dtype\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype should be converted to int64.\n\n    Examples\n    --------\n    >>> needs_i8_conversion(str)\n    False\n    >>> needs_i8_conversion(np.int64)\n    False\n    >>> needs_i8_conversion(np.datetime64)\n    True\n    >>> needs_i8_conversion(np.array(['a', 'b']))\n    False\n    >>> needs_i8_conversion(pd.Series([1, 2]))\n    False\n    >>> needs_i8_conversion(pd.Series([], dtype=\"timedelta64[ns]\"))\n    True\n    >>> needs_i8_conversion(pd.DatetimeIndex([1, 2, 3], tz=\"US/Eastern\"))\n    True\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    return (is_datetime_or_timedelta_dtype(arr_or_dtype) or\n            is_datetime64tz_dtype(arr_or_dtype) or\n            is_period_dtype(arr_or_dtype))\n\n\ndef is_numeric_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of a numeric dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of a numeric dtype.\n\n    Examples\n    --------\n    >>> is_numeric_dtype(str)\n    False\n    >>> is_numeric_dtype(int)\n    True\n    >>> is_numeric_dtype(float)\n    True\n    >>> is_numeric_dtype(np.uint64)\n    True\n    >>> is_numeric_dtype(np.datetime64)\n    False\n    >>> is_numeric_dtype(np.timedelta64)\n    False\n    >>> is_numeric_dtype(np.array(['a', 'b']))\n    False\n    >>> is_numeric_dtype(pd.Series([1, 2]))\n    True\n    >>> is_numeric_dtype(pd.Index([1, 2.]))\n    True\n    >>> is_numeric_dtype(np.array([], dtype=np.timedelta64))\n    False\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    tipo = _get_dtype_type(arr_or_dtype)\n    return (issubclass(tipo, (np.number, np.bool_)) and\n            not issubclass(tipo, (np.datetime64, np.timedelta64)))\n\n\ndef is_string_like_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of a string-like dtype.\n\n    Unlike `is_string_dtype`, the object dtype is excluded because it\n    is a mixed dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of the string dtype.\n\n    Examples\n    --------\n    >>> is_string_like_dtype(str)\n    True\n    >>> is_string_like_dtype(object)\n    False\n    >>> is_string_like_dtype(np.array(['a', 'b']))\n    True\n    >>> is_string_like_dtype(pd.Series([1, 2]))\n    False\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    try:\n        dtype = _get_dtype(arr_or_dtype)\n        return dtype.kind in ('S', 'U')\n    except TypeError:\n        return False\n\n\ndef is_float_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of a float dtype.\n\n    This function is internal and should not be exposed in the public API.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of a float dtype.\n\n    Examples\n    --------\n    >>> is_float_dtype(str)\n    False\n    >>> is_float_dtype(int)\n    False\n    >>> is_float_dtype(float)\n    True\n    >>> is_float_dtype(np.array(['a', 'b']))\n    False\n    >>> is_float_dtype(pd.Series([1, 2]))\n    False\n    >>> is_float_dtype(pd.Index([1, 2.]))\n    True\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    tipo = _get_dtype_type(arr_or_dtype)\n    return issubclass(tipo, np.floating)\n\n\ndef is_floating_dtype(arr_or_dtype):\n    \"\"\"Check whether the provided array or dtype is an instance of\n    numpy's float dtype.\n\n    .. deprecated:: 0.20.0\n\n    Unlike, `is_float_dtype`, this check is a lot stricter, as it requires\n    `isinstance` of `np.floating` and not `issubclass`.\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    tipo = _get_dtype_type(arr_or_dtype)\n    return isinstance(tipo, np.floating)\n\n\ndef is_bool_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of a boolean dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of a boolean dtype.\n\n    Notes\n    -----\n    An ExtensionArray is considered boolean when the ``_is_boolean``\n    attribute is set to True.\n\n    Examples\n    --------\n    >>> is_bool_dtype(str)\n    False\n    >>> is_bool_dtype(int)\n    False\n    >>> is_bool_dtype(bool)\n    True\n    >>> is_bool_dtype(np.bool)\n    True\n    >>> is_bool_dtype(np.array(['a', 'b']))\n    False\n    >>> is_bool_dtype(pd.Series([1, 2]))\n    False\n    >>> is_bool_dtype(np.array([True, False]))\n    True\n    >>> is_bool_dtype(pd.Categorical([True, False]))\n    True\n    >>> is_bool_dtype(pd.SparseArray([True, False]))\n    True\n    \"\"\"\n    if arr_or_dtype is None:\n        return False\n    try:\n        tipo = _get_dtype_type(arr_or_dtype)\n    except ValueError:\n        # this isn't even a dtype\n        return False\n\n    if isinstance(arr_or_dtype, (ABCCategorical, ABCCategoricalIndex)):\n        arr_or_dtype = arr_or_dtype.dtype\n\n    if isinstance(arr_or_dtype, CategoricalDtype):\n        arr_or_dtype = arr_or_dtype.categories\n        # now we use the special definition for Index\n\n    if isinstance(arr_or_dtype, ABCIndexClass):\n\n        # TODO(jreback)\n        # we don't have a boolean Index class\n        # so its object, we need to infer to\n        # guess this\n        return (arr_or_dtype.is_object and\n                arr_or_dtype.inferred_type == 'boolean')\n    elif is_extension_array_dtype(arr_or_dtype):\n        dtype = getattr(arr_or_dtype, 'dtype', arr_or_dtype)\n        return dtype._is_boolean\n\n    return issubclass(tipo, np.bool_)\n\n\ndef is_extension_type(arr):\n    \"\"\"\n    Check whether an array-like is of a pandas extension class instance.\n\n    Extension classes include categoricals, pandas sparse objects (i.e.\n    classes represented within the pandas library and not ones external\n    to it like scipy sparse matrices), and datetime-like arrays.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array-like to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array-like is of a pandas\n              extension class instance.\n\n    Examples\n    --------\n    >>> is_extension_type([1, 2, 3])\n    False\n    >>> is_extension_type(np.array([1, 2, 3]))\n    False\n    >>>\n    >>> cat = pd.Categorical([1, 2, 3])\n    >>>\n    >>> is_extension_type(cat)\n    True\n    >>> is_extension_type(pd.Series(cat))\n    True\n    >>> is_extension_type(pd.SparseArray([1, 2, 3]))\n    True\n    >>> is_extension_type(pd.SparseSeries([1, 2, 3]))\n    True\n    >>>\n    >>> from scipy.sparse import bsr_matrix\n    >>> is_extension_type(bsr_matrix([1, 2, 3]))\n    False\n    >>> is_extension_type(pd.DatetimeIndex([1, 2, 3]))\n    False\n    >>> is_extension_type(pd.DatetimeIndex([1, 2, 3], tz=\"US/Eastern\"))\n    True\n    >>>\n    >>> dtype = DatetimeTZDtype(\"ns\", tz=\"US/Eastern\")\n    >>> s = pd.Series([], dtype=dtype)\n    >>> is_extension_type(s)\n    True\n    \"\"\"\n\n    if is_categorical(arr):\n        return True\n    elif is_sparse(arr):\n        return True\n    elif is_datetimetz(arr):\n        return True\n    return False\n\n\ndef is_extension_array_dtype(arr_or_dtype):\n    \"\"\"Check if an object is a pandas extension array type.\n\n    Parameters\n    ----------\n    arr_or_dtype : object\n\n    Returns\n    -------\n    bool\n\n    Notes\n    -----\n    This checks whether an object implements the pandas extension\n    array interface. In pandas, this includes:\n\n    * Categorical\n    * Sparse\n    * Interval\n\n    Third-party libraries may implement arrays or types satisfying\n    this interface as well.\n    \"\"\"\n    dtype = getattr(arr_or_dtype, 'dtype', arr_or_dtype)\n    return (isinstance(dtype, ExtensionDtype) or\n            registry.find(dtype) is not None)\n\n\ndef is_complex_dtype(arr_or_dtype):\n    \"\"\"\n    Check whether the provided array or dtype is of a complex dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean : Whether or not the array or dtype is of a compex dtype.\n\n    Examples\n    --------\n    >>> is_complex_dtype(str)\n    False\n    >>> is_complex_dtype(int)\n    False\n    >>> is_complex_dtype(np.complex)\n    True\n    >>> is_complex_dtype(np.array(['a', 'b']))\n    False\n    >>> is_complex_dtype(pd.Series([1, 2]))\n    False\n    >>> is_complex_dtype(np.array([1 + 1j, 5]))\n    True\n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    tipo = _get_dtype_type(arr_or_dtype)\n    return issubclass(tipo, np.complexfloating)\n\n\ndef _coerce_to_dtype(dtype):\n    \"\"\"\n    Coerce a string or np.dtype to a pandas or numpy\n    dtype if possible.\n\n    If we cannot convert to a pandas dtype initially,\n    we convert to a numpy dtype.\n\n    Parameters\n    ----------\n    dtype : The dtype that we want to coerce.\n\n    Returns\n    -------\n    pd_or_np_dtype : The coerced dtype.\n    \"\"\"\n\n    if is_categorical_dtype(dtype):\n        categories = getattr(dtype, 'categories', None)\n        ordered = getattr(dtype, 'ordered', False)\n        dtype = CategoricalDtype(categories=categories, ordered=ordered)\n    elif is_datetime64tz_dtype(dtype):\n        dtype = DatetimeTZDtype(dtype)\n    elif is_period_dtype(dtype):\n        dtype = PeriodDtype(dtype)\n    elif is_interval_dtype(dtype):\n        dtype = IntervalDtype(dtype)\n    else:\n        dtype = np.dtype(dtype)\n    return dtype\n\n\ndef _get_dtype(arr_or_dtype):\n    \"\"\"\n    Get the dtype instance associated with an array\n    or dtype object.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array-like or dtype object whose dtype we want to extract.\n\n    Returns\n    -------\n    obj_dtype : The extract dtype instance from the\n                passed in array or dtype object.\n\n    Raises\n    ------\n    TypeError : The passed in object is None.\n    \"\"\"\n\n    # TODO(extension)\n    # replace with pandas_dtype\n\n    if arr_or_dtype is None:\n        raise TypeError(\"Cannot deduce dtype from null object\")\n    if isinstance(arr_or_dtype, np.dtype):\n        return arr_or_dtype\n    elif isinstance(arr_or_dtype, type):\n        return np.dtype(arr_or_dtype)\n    elif isinstance(arr_or_dtype, ExtensionDtype):\n        return arr_or_dtype\n    elif isinstance(arr_or_dtype, DatetimeTZDtype):\n        return arr_or_dtype\n    elif isinstance(arr_or_dtype, PeriodDtype):\n        return arr_or_dtype\n    elif isinstance(arr_or_dtype, IntervalDtype):\n        return arr_or_dtype\n    elif isinstance(arr_or_dtype, string_types):\n        if is_categorical_dtype(arr_or_dtype):\n            return CategoricalDtype.construct_from_string(arr_or_dtype)\n        elif is_datetime64tz_dtype(arr_or_dtype):\n            return DatetimeTZDtype.construct_from_string(arr_or_dtype)\n        elif is_period_dtype(arr_or_dtype):\n            return PeriodDtype.construct_from_string(arr_or_dtype)\n        elif is_interval_dtype(arr_or_dtype):\n            return IntervalDtype.construct_from_string(arr_or_dtype)\n    elif isinstance(arr_or_dtype, (ABCCategorical, ABCCategoricalIndex,\n                                   ABCSparseArray, ABCSparseSeries)):\n        return arr_or_dtype.dtype\n\n    if hasattr(arr_or_dtype, 'dtype'):\n        arr_or_dtype = arr_or_dtype.dtype\n    return np.dtype(arr_or_dtype)\n\n\ndef _get_dtype_type(arr_or_dtype):\n    \"\"\"\n    Get the type (NOT dtype) instance associated with\n    an array or dtype object.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array-like or dtype object whose type we want to extract.\n\n    Returns\n    -------\n    obj_type : The extract type instance from the\n               passed in array or dtype object.\n    \"\"\"\n\n    # TODO(extension)\n    # replace with pandas_dtype\n    if isinstance(arr_or_dtype, np.dtype):\n        return arr_or_dtype.type\n    elif isinstance(arr_or_dtype, type):\n        return np.dtype(arr_or_dtype).type\n    elif isinstance(arr_or_dtype, CategoricalDtype):\n        return CategoricalDtypeType\n    elif isinstance(arr_or_dtype, DatetimeTZDtype):\n        return Timestamp\n    elif isinstance(arr_or_dtype, IntervalDtype):\n        return Interval\n    elif isinstance(arr_or_dtype, PeriodDtype):\n        return Period\n    elif isinstance(arr_or_dtype, string_types):\n        if is_categorical_dtype(arr_or_dtype):\n            return CategoricalDtypeType\n        elif is_datetime64tz_dtype(arr_or_dtype):\n            return Timestamp\n        elif is_period_dtype(arr_or_dtype):\n            return Period\n        elif is_interval_dtype(arr_or_dtype):\n            return Interval\n        return _get_dtype_type(np.dtype(arr_or_dtype))\n    else:\n        from pandas.core.arrays.sparse import SparseDtype\n        if isinstance(arr_or_dtype, (ABCSparseSeries,\n                                     ABCSparseArray,\n                                     SparseDtype)):\n            dtype = getattr(arr_or_dtype, 'dtype', arr_or_dtype)\n            return dtype.type\n    try:\n        return arr_or_dtype.dtype.type\n    except AttributeError:\n        return type(None)\n\n\ndef _get_dtype_from_object(dtype):\n    \"\"\"\n    Get a numpy dtype.type-style object for a dtype object.\n\n    This methods also includes handling of the datetime64[ns] and\n    datetime64[ns, TZ] objects.\n\n    If no dtype can be found, we return ``object``.\n\n    Parameters\n    ----------\n    dtype : dtype, type\n        The dtype object whose numpy dtype.type-style\n        object we want to extract.\n\n    Returns\n    -------\n    dtype_object : The extracted numpy dtype.type-style object.\n    \"\"\"\n\n    if isinstance(dtype, type) and issubclass(dtype, np.generic):\n        # Type object from a dtype\n        return dtype\n    elif is_categorical(dtype):\n        return CategoricalDtype().type\n    elif is_datetimetz(dtype):\n        return DatetimeTZDtype(dtype).type\n    elif isinstance(dtype, np.dtype):  # dtype object\n        try:\n            _validate_date_like_dtype(dtype)\n        except TypeError:\n            # Should still pass if we don't have a date-like\n            pass\n        return dtype.type\n    elif isinstance(dtype, string_types):\n        if dtype in ['datetimetz', 'datetime64tz']:\n            return DatetimeTZDtype.type\n        elif dtype in ['period']:\n            raise NotImplementedError\n\n        if dtype == 'datetime' or dtype == 'timedelta':\n            dtype += '64'\n\n        try:\n            return _get_dtype_from_object(getattr(np, dtype))\n        except (AttributeError, TypeError):\n            # Handles cases like _get_dtype(int) i.e.,\n            # Python objects that are valid dtypes\n            # (unlike user-defined types, in general)\n            #\n            # TypeError handles the float16 type code of 'e'\n            # further handle internal types\n            pass\n\n    return _get_dtype_from_object(np.dtype(dtype))\n\n\ndef _validate_date_like_dtype(dtype):\n    \"\"\"\n    Check whether the dtype is a date-like dtype. Raises an error if invalid.\n\n    Parameters\n    ----------\n    dtype : dtype, type\n        The dtype to check.\n\n    Raises\n    ------\n    TypeError : The dtype could not be casted to a date-like dtype.\n    ValueError : The dtype is an illegal date-like dtype (e.g. the\n                 the frequency provided is too specific)\n    \"\"\"\n\n    try:\n        typ = np.datetime_data(dtype)[0]\n    except ValueError as e:\n        raise TypeError('{error}'.format(error=e))\n    if typ != 'generic' and typ != 'ns':\n        msg = '{name!r} is too specific of a frequency, try passing {type!r}'\n        raise ValueError(msg.format(name=dtype.name, type=dtype.type.__name__))\n\n\n_string_dtypes = frozenset(map(_get_dtype_from_object, (binary_type,\n                                                        text_type)))\n\n\ndef pandas_dtype(dtype):\n    \"\"\"\n    Converts input into a pandas only dtype object or a numpy dtype object.\n\n    Parameters\n    ----------\n    dtype : object to be converted\n\n    Returns\n    -------\n    np.dtype or a pandas dtype\n\n    Raises\n    ------\n    TypeError if not a dtype\n    \"\"\"\n    # short-circuit\n    if isinstance(dtype, np.ndarray):\n        return dtype.dtype\n    elif isinstance(dtype, np.dtype):\n        return dtype\n\n    # registered extension types\n    result = _pandas_registry.find(dtype) or registry.find(dtype)\n    if result is not None:\n        return result\n\n    # un-registered extension types\n    elif isinstance(dtype, (PandasExtensionDtype, ExtensionDtype)):\n        return dtype\n\n    # try a numpy dtype\n    # raise a consistent TypeError if failed\n    try:\n        npdtype = np.dtype(dtype)\n    except Exception:\n        # we don't want to force a repr of the non-string\n        if not isinstance(dtype, string_types):\n            raise TypeError(\"data type not understood\")\n        raise TypeError(\"data type '{}' not understood\".format(\n            dtype))\n\n    # Any invalid dtype (such as pd.Timestamp) should raise an error.\n    # np.dtype(invalid_type).kind = 0 for such objects. However, this will\n    # also catch some valid dtypes such as object, np.object_ and 'object'\n    # which we safeguard against by catching them earlier and returning\n    # np.dtype(valid_dtype) before this condition is evaluated.\n    if is_hashable(dtype) and dtype in [object, np.object_, 'object', 'O']:\n        # check hashability to avoid errors/DeprecationWarning when we get\n        # here and `dtype` is an array\n        return npdtype\n    elif npdtype.kind == 'O':\n        raise TypeError(\"dtype '{}' not understood\".format(dtype))\n\n    return npdtype\n"
    },
    {
      "filename": "pandas/core/frame.py",
      "content": "# pylint: disable=E1101\n# pylint: disable=W0212,W0703,W0622\n\"\"\"\nDataFrame\n---------\nAn efficient 2D container for potentially mixed-type time series or other\nlabeled data series.\n\nSimilar to its R counterpart, data.frame, except providing automatic data\nalignment and a host of useful data manipulation methods having to do with the\nlabeling information\n\"\"\"\nfrom __future__ import division\n\nimport collections\nimport functools\nimport itertools\nimport sys\nimport warnings\nfrom textwrap import dedent\n\nimport numpy as np\nimport numpy.ma as ma\n\nfrom pandas._libs import lib, algos as libalgos\n\nfrom pandas.util._decorators import (Appender, Substitution,\n                                     rewrite_axis_style_signature,\n                                     deprecate_kwarg)\nfrom pandas.util._validators import (validate_bool_kwarg,\n                                     validate_axis_style_args)\n\nfrom pandas import compat\nfrom pandas.compat import (range, map, zip, lrange, lmap, lzip, StringIO, u,\n                           OrderedDict, PY36, raise_with_traceback,\n                           string_and_binary_types)\nfrom pandas.compat.numpy import function as nv\n\nfrom pandas.core.dtypes.cast import (\n    maybe_upcast,\n    cast_scalar_to_array,\n    construct_1d_arraylike_from_scalar,\n    infer_dtype_from_scalar,\n    maybe_cast_to_datetime,\n    maybe_infer_to_datetimelike,\n    maybe_convert_platform,\n    maybe_downcast_to_dtype,\n    invalidate_string_dtypes,\n    coerce_to_dtypes,\n    maybe_upcast_putmask,\n    find_common_type)\nfrom pandas.core.dtypes.common import (\n    is_categorical_dtype,\n    is_object_dtype,\n    is_extension_type,\n    is_extension_array_dtype,\n    is_datetimetz,\n    is_datetime64_any_dtype,\n    is_bool_dtype,\n    is_integer_dtype,\n    is_float_dtype,\n    is_integer,\n    is_scalar,\n    is_dtype_equal,\n    needs_i8_conversion,\n    _get_dtype_from_object,\n    ensure_float64,\n    ensure_int64,\n    ensure_platform_int,\n    is_list_like,\n    is_nested_list_like,\n    is_iterator,\n    is_sequence,\n    is_named_tuple)\nfrom pandas.core.dtypes.generic import ABCSeries, ABCIndexClass, ABCMultiIndex\nfrom pandas.core.dtypes.missing import isna, notna\n\nfrom pandas.core import algorithms\nfrom pandas.core import common as com\nfrom pandas.core import nanops\nfrom pandas.core import ops\nfrom pandas.core.accessor import CachedAccessor\nfrom pandas.core.arrays import Categorical, ExtensionArray\nfrom pandas.core.config import get_option\nfrom pandas.core.generic import NDFrame, _shared_docs\nfrom pandas.core.index import (Index, MultiIndex, ensure_index,\n                               ensure_index_from_sequences)\nfrom pandas.core.indexes import base as ibase\nfrom pandas.core.indexes.datetimes import DatetimeIndex\nfrom pandas.core.indexes.period import PeriodIndex\nfrom pandas.core.indexes.timedeltas import TimedeltaIndex\nfrom pandas.core.indexing import (maybe_droplevels, convert_to_index_sliceable,\n                                  check_bool_indexer)\nfrom pandas.core.internals import (BlockManager,\n                                   create_block_manager_from_arrays,\n                                   create_block_manager_from_blocks)\nfrom pandas.core.series import Series\n\nfrom pandas.io.formats import console\nfrom pandas.io.formats import format as fmt\nfrom pandas.io.formats.printing import pprint_thing\n\nimport pandas.plotting._core as gfx\n\n# ---------------------------------------------------------------------\n# Docstring templates\n\n_shared_doc_kwargs = dict(\n    axes='index, columns', klass='DataFrame',\n    axes_single_arg=\"{0 or 'index', 1 or 'columns'}\",\n    axis=\"\"\"axis : {0 or 'index', 1 or 'columns'}, default 0\n        If 0 or 'index': apply function to each column.\n        If 1 or 'columns': apply function to each row.\"\"\",\n    optional_by=\"\"\"\n        by : str or list of str\n            Name or list of names to sort by.\n\n            - if `axis` is 0 or `'index'` then `by` may contain index\n              levels and/or column labels\n            - if `axis` is 1 or `'columns'` then `by` may contain column\n              levels and/or index labels\n\n            .. versionchanged:: 0.23.0\n               Allow specifying index or column level names.\"\"\",\n    versionadded_to_excel='',\n    optional_labels=\"\"\"labels : array-like, optional\n            New labels / index to conform the axis specified by 'axis' to.\"\"\",\n    optional_axis=\"\"\"axis : int or str, optional\n            Axis to target. Can be either the axis name ('index', 'columns')\n            or number (0, 1).\"\"\",\n)\n\n_numeric_only_doc = \"\"\"numeric_only : boolean, default None\n    Include only float, int, boolean data. If None, will attempt to use\n    everything, then use only numeric data\n\"\"\"\n\n_merge_doc = \"\"\"\nMerge DataFrame or named Series objects with a database-style join.\n\nThe join is done on columns or indexes. If joining columns on\ncolumns, the DataFrame indexes *will be ignored*. Otherwise if joining indexes\non indexes or indexes on a column or columns, the index will be passed on.\n\nParameters\n----------%s\nright : DataFrame or named Series\n    Object to merge with.\nhow : {'left', 'right', 'outer', 'inner'}, default 'inner'\n    Type of merge to be performed.\n\n    * left: use only keys from left frame, similar to a SQL left outer join;\n      preserve key order.\n    * right: use only keys from right frame, similar to a SQL right outer join;\n      preserve key order.\n    * outer: use union of keys from both frames, similar to a SQL full outer\n      join; sort keys lexicographically.\n    * inner: use intersection of keys from both frames, similar to a SQL inner\n      join; preserve the order of the left keys.\non : label or list\n    Column or index level names to join on. These must be found in both\n    DataFrames. If `on` is None and not merging on indexes then this defaults\n    to the intersection of the columns in both DataFrames.\nleft_on : label or list, or array-like\n    Column or index level names to join on in the left DataFrame. Can also\n    be an array or list of arrays of the length of the left DataFrame.\n    These arrays are treated as if they are columns.\nright_on : label or list, or array-like\n    Column or index level names to join on in the right DataFrame. Can also\n    be an array or list of arrays of the length of the right DataFrame.\n    These arrays are treated as if they are columns.\nleft_index : bool, default False\n    Use the index from the left DataFrame as the join key(s). If it is a\n    MultiIndex, the number of keys in the other DataFrame (either the index\n    or a number of columns) must match the number of levels.\nright_index : bool, default False\n    Use the index from the right DataFrame as the join key. Same caveats as\n    left_index.\nsort : bool, default False\n    Sort the join keys lexicographically in the result DataFrame. If False,\n    the order of the join keys depends on the join type (how keyword).\nsuffixes : tuple of (str, str), default ('_x', '_y')\n    Suffix to apply to overlapping column names in the left and right\n    side, respectively. To raise an exception on overlapping columns use\n    (False, False).\ncopy : bool, default True\n    If False, avoid copy if possible.\nindicator : bool or str, default False\n    If True, adds a column to output DataFrame called \"_merge\" with\n    information on the source of each row.\n    If string, column with information on source of each row will be added to\n    output DataFrame, and column will be named value of string.\n    Information column is Categorical-type and takes on a value of \"left_only\"\n    for observations whose merge key only appears in 'left' DataFrame,\n    \"right_only\" for observations whose merge key only appears in 'right'\n    DataFrame, and \"both\" if the observation's merge key is found in both.\n\nvalidate : str, optional\n    If specified, checks if merge is of specified type.\n\n    * \"one_to_one\" or \"1:1\": check if merge keys are unique in both\n      left and right datasets.\n    * \"one_to_many\" or \"1:m\": check if merge keys are unique in left\n      dataset.\n    * \"many_to_one\" or \"m:1\": check if merge keys are unique in right\n      dataset.\n    * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.\n\n    .. versionadded:: 0.21.0\n\nReturns\n-------\nDataFrame\n    A DataFrame of the two merged objects.\n\nNotes\n-----\nSupport for specifying index levels as the `on`, `left_on`, and\n`right_on` parameters was added in version 0.23.0\nSupport for merging named Series objects was added in version 0.24.0\n\nSee Also\n--------\nmerge_ordered : Merge with optional filling/interpolation.\nmerge_asof : Merge on nearest keys.\nDataFrame.join : Similar method using indices.\n\nExamples\n--------\n\n>>> df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [1, 2, 3, 5]})\n>>> df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [5, 6, 7, 8]})\n>>> df1\n    lkey value\n0   foo      1\n1   bar      2\n2   baz      3\n3   foo      5\n>>> df2\n    rkey value\n0   foo      5\n1   bar      6\n2   baz      7\n3   foo      8\n\nMerge df1 and df2 on the lkey and rkey columns. The value columns have\nthe default suffixes, _x and _y, appended.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey')\n  lkey  value_x rkey  value_y\n0  foo        1  foo        5\n1  foo        1  foo        8\n2  foo        5  foo        5\n3  foo        5  foo        8\n4  bar        2  bar        6\n5  baz        3  baz        7\n\nMerge DataFrames df1 and df2 with specified left and right suffixes\nappended to any overlapping columns.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey',\n...           suffixes=('_left', '_right'))\n  lkey  value_left rkey  value_right\n0  foo           1  foo            5\n1  foo           1  foo            8\n2  foo           5  foo            5\n3  foo           5  foo            8\n4  bar           2  bar            6\n5  baz           3  baz            7\n\nMerge DataFrames df1 and df2, but raise an exception if the DataFrames have\nany overlapping columns.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))\nTraceback (most recent call last):\n...\nValueError: columns overlap but no suffix specified:\n    Index(['value'], dtype='object')\n\"\"\"\n\n# -----------------------------------------------------------------------\n# DataFrame class\n\n\nclass DataFrame(NDFrame):\n    \"\"\" Two-dimensional size-mutable, potentially heterogeneous tabular data\n    structure with labeled axes (rows and columns). Arithmetic operations\n    align on both row and column labels. Can be thought of as a dict-like\n    container for Series objects. The primary pandas data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n        Dict can contain Series, arrays, constants, or list-like objects\n\n        .. versionchanged :: 0.23.0\n           If data is a dict, argument order is maintained for Python 3.6\n           and later.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing information part of input data and no index provided\n    columns : Index or array-like\n        Column labels to use for resulting frame. Will default to\n        RangeIndex (0, 1, 2, ..., n) if no column labels are provided\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is allowed. If None, infer\n    copy : boolean, default False\n        Copy data from inputs. Only affects DataFrame / 2d ndarray input\n\n    Examples\n    --------\n    Constructing DataFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> df = pd.DataFrame(data=d)\n    >>> df\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> df.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> df = pd.DataFrame(data=d, dtype=np.int8)\n    >>> df.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing DataFrame from numpy ndarray:\n\n    >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> df2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n\n    See Also\n    --------\n    DataFrame.from_records : Constructor from tuples, also record arrays.\n    DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n    DataFrame.from_items : From sequence of (key, value) pairs\n        pandas.read_csv, pandas.read_table, pandas.read_clipboard.\n    \"\"\"\n\n    @property\n    def _constructor(self):\n        return DataFrame\n\n    _constructor_sliced = Series\n    _deprecations = NDFrame._deprecations | frozenset(\n        ['get_value', 'set_value', 'from_csv', 'from_items'])\n    _accessors = set()\n\n    @property\n    def _constructor_expanddim(self):\n        from pandas.core.panel import Panel\n        return Panel\n\n    def __init__(self, data=None, index=None, columns=None, dtype=None,\n                 copy=False):\n        if data is None:\n            data = {}\n        if dtype is not None:\n            dtype = self._validate_dtype(dtype)\n\n        if isinstance(data, DataFrame):\n            data = data._data\n\n        if isinstance(data, BlockManager):\n            mgr = self._init_mgr(data, axes=dict(index=index, columns=columns),\n                                 dtype=dtype, copy=copy)\n        elif isinstance(data, dict):\n            mgr = self._init_dict(data, index, columns, dtype=dtype)\n        elif isinstance(data, ma.MaskedArray):\n            import numpy.ma.mrecords as mrecords\n            # masked recarray\n            if isinstance(data, mrecords.MaskedRecords):\n                mgr = _masked_rec_array_to_mgr(data, index, columns, dtype,\n                                               copy)\n\n            # a masked array\n            else:\n                mask = ma.getmaskarray(data)\n                if mask.any():\n                    data, fill_value = maybe_upcast(data, copy=True)\n                    data[mask] = fill_value\n                else:\n                    data = data.copy()\n                mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n                                         copy=copy)\n\n        elif isinstance(data, (np.ndarray, Series, Index)):\n            if data.dtype.names:\n                data_columns = list(data.dtype.names)\n                data = {k: data[k] for k in data_columns}\n                if columns is None:\n                    columns = data_columns\n                mgr = self._init_dict(data, index, columns, dtype=dtype)\n            elif getattr(data, 'name', None) is not None:\n                mgr = self._init_dict({data.name: data}, index, columns,\n                                      dtype=dtype)\n            else:\n                mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n                                         copy=copy)\n\n        # For data is list-like, or Iterable (will consume into list)\n        elif (isinstance(data, compat.Iterable)\n              and not isinstance(data, string_and_binary_types)):\n            if not isinstance(data, compat.Sequence):\n                data = list(data)\n            if len(data) > 0:\n                if is_list_like(data[0]) and getattr(data[0], 'ndim', 1) == 1:\n                    if is_named_tuple(data[0]) and columns is None:\n                        columns = data[0]._fields\n                    arrays, columns = _to_arrays(data, columns, dtype=dtype)\n                    columns = ensure_index(columns)\n\n                    # set the index\n                    if index is None:\n                        if isinstance(data[0], Series):\n                            index = _get_names_from_index(data)\n                        elif isinstance(data[0], Categorical):\n                            index = ibase.default_index(len(data[0]))\n                        else:\n                            index = ibase.default_index(len(data))\n\n                    mgr = _arrays_to_mgr(arrays, columns, index, columns,\n                                         dtype=dtype)\n                else:\n                    mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n                                             copy=copy)\n            else:\n                mgr = self._init_dict({}, index, columns, dtype=dtype)\n        else:\n            try:\n                arr = np.array(data, dtype=dtype, copy=copy)\n            except (ValueError, TypeError) as e:\n                exc = TypeError('DataFrame constructor called with '\n                                'incompatible data and dtype: {e}'.format(e=e))\n                raise_with_traceback(exc)\n\n            if arr.ndim == 0 and index is not None and columns is not None:\n                values = cast_scalar_to_array((len(index), len(columns)),\n                                              data, dtype=dtype)\n                mgr = self._init_ndarray(values, index, columns,\n                                         dtype=values.dtype, copy=False)\n            else:\n                raise ValueError('DataFrame constructor not properly called!')\n\n        NDFrame.__init__(self, mgr, fastpath=True)\n\n    def _init_dict(self, data, index, columns, dtype=None):\n        \"\"\"\n        Segregate Series based on type and coerce into matrices.\n        Needs to handle a lot of exceptional cases.\n        \"\"\"\n        if columns is not None:\n            arrays = Series(data, index=columns, dtype=object)\n            data_names = arrays.index\n\n            missing = arrays.isnull()\n            if index is None:\n                # GH10856\n                # raise ValueError if only scalars in dict\n                index = extract_index(arrays[~missing])\n            else:\n                index = ensure_index(index)\n\n            # no obvious \"empty\" int column\n            if missing.any() and not is_integer_dtype(dtype):\n                if dtype is None or np.issubdtype(dtype, np.flexible):\n                    # 1783\n                    nan_dtype = object\n                else:\n                    nan_dtype = dtype\n                v = construct_1d_arraylike_from_scalar(np.nan, len(index),\n                                                       nan_dtype)\n                arrays.loc[missing] = [v] * missing.sum()\n\n        else:\n            keys = com.dict_keys_to_ordered_list(data)\n            columns = data_names = Index(keys)\n            arrays = [data[k] for k in keys]\n\n        return _arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)\n\n    def _init_ndarray(self, values, index, columns, dtype=None, copy=False):\n        # input must be a ndarray, list, Series, index\n\n        if isinstance(values, Series):\n            if columns is None:\n                if values.name is not None:\n                    columns = [values.name]\n            if index is None:\n                index = values.index\n            else:\n                values = values.reindex(index)\n\n            # zero len case (GH #2234)\n            if not len(values) and columns is not None and len(columns):\n                values = np.empty((0, 1), dtype=object)\n\n        # helper to create the axes as indexes\n        def _get_axes(N, K, index=index, columns=columns):\n            # return axes or defaults\n\n            if index is None:\n                index = ibase.default_index(N)\n            else:\n                index = ensure_index(index)\n\n            if columns is None:\n                columns = ibase.default_index(K)\n            else:\n                columns = ensure_index(columns)\n            return index, columns\n\n        # we could have a categorical type passed or coerced to 'category'\n        # recast this to an _arrays_to_mgr\n        if (is_categorical_dtype(getattr(values, 'dtype', None)) or\n                is_categorical_dtype(dtype)):\n\n            if not hasattr(values, 'dtype'):\n                values = _prep_ndarray(values, copy=copy)\n                values = values.ravel()\n            elif copy:\n                values = values.copy()\n\n            index, columns = _get_axes(len(values), 1)\n            return _arrays_to_mgr([values], columns, index, columns,\n                                  dtype=dtype)\n        elif (is_datetimetz(values) or is_extension_array_dtype(values)):\n            # GH19157\n            if columns is None:\n                columns = [0]\n            return _arrays_to_mgr([values], columns, index, columns,\n                                  dtype=dtype)\n\n        # by definition an array here\n        # the dtypes will be coerced to a single dtype\n        values = _prep_ndarray(values, copy=copy)\n\n        if dtype is not None:\n            if not is_dtype_equal(values.dtype, dtype):\n                try:\n                    values = values.astype(dtype)\n                except Exception as orig:\n                    e = ValueError(\"failed to cast to '{dtype}' (Exception \"\n                                   \"was: {orig})\".format(dtype=dtype,\n                                                         orig=orig))\n                    raise_with_traceback(e)\n\n        index, columns = _get_axes(*values.shape)\n        values = values.T\n\n        # if we don't have a dtype specified, then try to convert objects\n        # on the entire block; this is to convert if we have datetimelike's\n        # embedded in an object type\n        if dtype is None and is_object_dtype(values):\n            values = maybe_infer_to_datetimelike(values)\n\n        return create_block_manager_from_blocks([values], [columns, index])\n\n    @property\n    def axes(self):\n        \"\"\"\n        Return a list representing the axes of the DataFrame.\n\n        It has the row axis labels and column axis labels as the only members.\n        They are returned in that order.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.axes\n        [RangeIndex(start=0, stop=2, step=1), Index(['coll', 'col2'],\n        dtype='object')]\n        \"\"\"\n        return [self.index, self.columns]\n\n    @property\n    def shape(self):\n        \"\"\"\n        Return a tuple representing the dimensionality of the DataFrame.\n\n        See Also\n        --------\n        ndarray.shape\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.shape\n        (2, 2)\n\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4],\n        ...                    'col3': [5, 6]})\n        >>> df.shape\n        (2, 3)\n        \"\"\"\n        return len(self.index), len(self.columns)\n\n    @property\n    def _is_homogeneous_type(self):\n        \"\"\"\n        Whether all the columns in a DataFrame have the same type.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3, 4]})._is_homogeneous_type\n        True\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.0]})._is_homogeneous_type\n        False\n\n        Items with the same type but different sizes are considered\n        different types.\n\n        >>> DataFrame({\n        ...    \"A\": np.array([1, 2], dtype=np.int32),\n        ...    \"B\": np.array([1, 2], dtype=np.int64)})._is_homogeneous_type\n        False\n        \"\"\"\n        if self._data.any_extension_types:\n            return len({block.dtype for block in self._data.blocks}) == 1\n        else:\n            return not self._data.is_mixed_type\n\n    def _repr_fits_vertical_(self):\n        \"\"\"\n        Check length against max_rows.\n        \"\"\"\n        max_rows = get_option(\"display.max_rows\")\n        return len(self) <= max_rows\n\n    def _repr_fits_horizontal_(self, ignore_width=False):\n        \"\"\"\n        Check if full repr fits in horizontal boundaries imposed by the display\n        options width and max_columns. In case off non-interactive session, no\n        boundaries apply.\n\n        ignore_width is here so ipnb+HTML output can behave the way\n        users expect. display.max_columns remains in effect.\n        GH3541, GH3573\n        \"\"\"\n\n        width, height = console.get_console_size()\n        max_columns = get_option(\"display.max_columns\")\n        nb_columns = len(self.columns)\n\n        # exceed max columns\n        if ((max_columns and nb_columns > max_columns) or\n                ((not ignore_width) and width and nb_columns > (width // 2))):\n            return False\n\n        # used by repr_html under IPython notebook or scripts ignore terminal\n        # dims\n        if ignore_width or not console.in_interactive_session():\n            return True\n\n        if (get_option('display.width') is not None or\n                console.in_ipython_frontend()):\n            # check at least the column row for excessive width\n            max_rows = 1\n        else:\n            max_rows = get_option(\"display.max_rows\")\n\n        # when auto-detecting, so width=None and not in ipython front end\n        # check whether repr fits horizontal by actually checking\n        # the width of the rendered repr\n        buf = StringIO()\n\n        # only care about the stuff we'll actually print out\n        # and to_string on entire frame may be expensive\n        d = self\n\n        if not (max_rows is None):  # unlimited rows\n            # min of two, where one may be None\n            d = d.iloc[:min(max_rows, len(d))]\n        else:\n            return True\n\n        d.to_string(buf=buf)\n        value = buf.getvalue()\n        repr_width = max(len(l) for l in value.split('\\n'))\n\n        return repr_width < width\n\n    def _info_repr(self):\n        \"\"\"True if the repr should show the info view.\"\"\"\n        info_repr_option = (get_option(\"display.large_repr\") == \"info\")\n        return info_repr_option and not (self._repr_fits_horizontal_() and\n                                         self._repr_fits_vertical_())\n\n    def __unicode__(self):\n        \"\"\"\n        Return a string representation for a particular DataFrame\n\n        Invoked by unicode(df) in py2 only. Yields a Unicode String in both\n        py2/py3.\n        \"\"\"\n        buf = StringIO(u(\"\"))\n        if self._info_repr():\n            self.info(buf=buf)\n            return buf.getvalue()\n\n        max_rows = get_option(\"display.max_rows\")\n        max_cols = get_option(\"display.max_columns\")\n        show_dimensions = get_option(\"display.show_dimensions\")\n        if get_option(\"display.expand_frame_repr\"):\n            width, _ = console.get_console_size()\n        else:\n            width = None\n        self.to_string(buf=buf, max_rows=max_rows, max_cols=max_cols,\n                       line_width=width, show_dimensions=show_dimensions)\n\n        return buf.getvalue()\n\n    def _repr_html_(self):\n        \"\"\"\n        Return a html representation for a particular DataFrame.\n        Mainly for IPython notebook.\n        \"\"\"\n        # qtconsole doesn't report its line width, and also\n        # behaves badly when outputting an HTML table\n        # that doesn't fit the window, so disable it.\n        # XXX: In IPython 3.x and above, the Qt console will not attempt to\n        # display HTML, so this check can be removed when support for\n        # IPython 2.x is no longer needed.\n        if console.in_qtconsole():\n            # 'HTML output is disabled in QtConsole'\n            return None\n\n        if self._info_repr():\n            buf = StringIO(u(\"\"))\n            self.info(buf=buf)\n            # need to escape the <class>, should be the first line.\n            val = buf.getvalue().replace('<', r'&lt;', 1)\n            val = val.replace('>', r'&gt;', 1)\n            return '<pre>' + val + '</pre>'\n\n        if get_option(\"display.notebook_repr_html\"):\n            max_rows = get_option(\"display.max_rows\")\n            max_cols = get_option(\"display.max_columns\")\n            show_dimensions = get_option(\"display.show_dimensions\")\n\n            return self.to_html(max_rows=max_rows, max_cols=max_cols,\n                                show_dimensions=show_dimensions, notebook=True)\n        else:\n            return None\n\n    @property\n    def style(self):\n        \"\"\"\n        Property returning a Styler object containing methods for\n        building a styled HTML representation fo the DataFrame.\n\n        See Also\n        --------\n        pandas.io.formats.style.Styler\n        \"\"\"\n        from pandas.io.formats.style import Styler\n        return Styler(self)\n\n    def iteritems(self):\n        r\"\"\"\n        Iterator over (column name, Series) pairs.\n\n        Iterates over the DataFrame columns, returning a tuple with\n        the column name and the content as a Series.\n\n        Yields\n        ------\n        label : object\n            The column names for the DataFrame being iterated over.\n        content : Series\n            The column entries belonging to each label, as a Series.\n\n        See Also\n        --------\n        DataFrame.iterrows : Iterate over DataFrame rows as\n            (index, Series) pairs.\n        DataFrame.itertuples : Iterate over DataFrame rows as namedtuples\n            of the values.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],\n        ...                   'population': [1864, 22000, 80000]},\n        ...                   index=['panda', 'polar', 'koala'])\n        >>> df\n                species   population\n        panda \tbear \t  1864\n        polar \tbear \t  22000\n        koala \tmarsupial 80000\n        >>> for label, content in df.iteritems():\n        ...     print('label:', label)\n        ...     print('content:', content, sep='\\n')\n        ...\n        label: species\n        content:\n        panda         bear\n        polar         bear\n        koala    marsupial\n        Name: species, dtype: object\n        label: population\n        content:\n        panda     1864\n        polar    22000\n        koala    80000\n        Name: population, dtype: int64\n        \"\"\"\n        if self.columns.is_unique and hasattr(self, '_item_cache'):\n            for k in self.columns:\n                yield k, self._get_item_cache(k)\n        else:\n            for i, k in enumerate(self.columns):\n                yield k, self._ixs(i, axis=1)\n\n    def iterrows(self):\n        \"\"\"\n        Iterate over DataFrame rows as (index, Series) pairs.\n\n        Notes\n        -----\n\n        1. Because ``iterrows`` returns a Series for each row,\n           it does **not** preserve dtypes across the rows (dtypes are\n           preserved across columns for DataFrames). For example,\n\n           >>> df = pd.DataFrame([[1, 1.5]], columns=['int', 'float'])\n           >>> row = next(df.iterrows())[1]\n           >>> row\n           int      1.0\n           float    1.5\n           Name: 0, dtype: float64\n           >>> print(row['int'].dtype)\n           float64\n           >>> print(df['int'].dtype)\n           int64\n\n           To preserve dtypes while iterating over the rows, it is better\n           to use :meth:`itertuples` which returns namedtuples of the values\n           and which is generally faster than ``iterrows``.\n\n        2. You should **never modify** something you are iterating over.\n           This is not guaranteed to work in all cases. Depending on the\n           data types, the iterator returns a copy and not a view, and writing\n           to it will have no effect.\n\n        Yields\n        ------\n        index : label or tuple of label\n            The index of the row. A tuple for a `MultiIndex`.\n        data : Series\n            The data of the row as a Series.\n\n        it : generator\n            A generator that iterates over the rows of the frame.\n\n        See Also\n        --------\n        itertuples : Iterate over DataFrame rows as namedtuples of the values.\n        iteritems : Iterate over (column name, Series) pairs.\n        \"\"\"\n        columns = self.columns\n        klass = self._constructor_sliced\n        for k, v in zip(self.index, self.values):\n            s = klass(v, index=columns, name=k)\n            yield k, s\n\n    def itertuples(self, index=True, name=\"Pandas\"):\n        \"\"\"\n        Iterate over DataFrame rows as namedtuples.\n\n        Parameters\n        ----------\n        index : bool, default True\n            If True, return the index as the first element of the tuple.\n        name : str, default \"Pandas\"\n            The name of the returned namedtuples or None to return regular\n            tuples.\n\n        Yields\n        -------\n        collections.namedtuple\n            Yields a namedtuple for each row in the DataFrame with the first\n            field possibly being the index and following fields being the\n            column values.\n\n        Notes\n        -----\n        The column names will be renamed to positional names if they are\n        invalid Python identifiers, repeated, or start with an underscore.\n        With a large number of columns (>255), regular tuples are returned.\n\n        See Also\n        --------\n        DataFrame.iterrows : Iterate over DataFrame rows as (index, Series)\n            pairs.\n        DataFrame.iteritems : Iterate over (column name, Series) pairs.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]},\n        ...                   index=['dog', 'hawk'])\n        >>> df\n              num_legs  num_wings\n        dog          4          0\n        hawk         2          2\n        >>> for row in df.itertuples():\n        ...     print(row)\n        ...\n        Pandas(Index='dog', num_legs=4, num_wings=0)\n        Pandas(Index='hawk', num_legs=2, num_wings=2)\n\n        By setting the `index` parameter to False we can remove the index\n        as the first element of the tuple:\n\n        >>> for row in df.itertuples(index=False):\n        ...     print(row)\n        ...\n        Pandas(num_legs=4, num_wings=0)\n        Pandas(num_legs=2, num_wings=2)\n\n        With the `name` parameter set we set a custom name for the yielded\n        namedtuples:\n\n        >>> for row in df.itertuples(name='Animal'):\n        ...     print(row)\n        ...\n        Animal(Index='dog', num_legs=4, num_wings=0)\n        Animal(Index='hawk', num_legs=2, num_wings=2)\n        \"\"\"\n        arrays = []\n        fields = []\n        if index:\n            arrays.append(self.index)\n            fields.append(\"Index\")\n\n        # use integer indexing because of possible duplicate column names\n        arrays.extend(self.iloc[:, k] for k in range(len(self.columns)))\n\n        # Python 3 supports at most 255 arguments to constructor, and\n        # things get slow with this many fields in Python 2\n        if name is not None and len(self.columns) + index < 256:\n            # `rename` is unsupported in Python 2.6\n            try:\n                itertuple = collections.namedtuple(name,\n                                                   fields + list(self.columns),\n                                                   rename=True)\n                return map(itertuple._make, zip(*arrays))\n            except Exception:\n                pass\n\n        # fallback to regular tuples\n        return zip(*arrays)\n\n    items = iteritems\n\n    def __len__(self):\n        \"\"\"Returns length of info axis, but here we use the index \"\"\"\n        return len(self.index)\n\n    def dot(self, other):\n        \"\"\"\n        Matrix multiplication with DataFrame or Series objects.  Can also be\n        called using `self @ other` in Python >= 3.5.\n\n        Parameters\n        ----------\n        other : DataFrame or Series\n\n        Returns\n        -------\n        dot_product : DataFrame or Series\n        \"\"\"\n        if isinstance(other, (Series, DataFrame)):\n            common = self.columns.union(other.index)\n            if (len(common) > len(self.columns) or\n                    len(common) > len(other.index)):\n                raise ValueError('matrices are not aligned')\n\n            left = self.reindex(columns=common, copy=False)\n            right = other.reindex(index=common, copy=False)\n            lvals = left.values\n            rvals = right.values\n        else:\n            left = self\n            lvals = self.values\n            rvals = np.asarray(other)\n            if lvals.shape[1] != rvals.shape[0]:\n                raise ValueError('Dot product shape mismatch, '\n                                 '{s} vs {r}'.format(s=lvals.shape,\n                                                     r=rvals.shape))\n\n        if isinstance(other, DataFrame):\n            return self._constructor(np.dot(lvals, rvals), index=left.index,\n                                     columns=other.columns)\n        elif isinstance(other, Series):\n            return Series(np.dot(lvals, rvals), index=left.index)\n        elif isinstance(rvals, (np.ndarray, Index)):\n            result = np.dot(lvals, rvals)\n            if result.ndim == 2:\n                return self._constructor(result, index=left.index)\n            else:\n                return Series(result, index=left.index)\n        else:  # pragma: no cover\n            raise TypeError('unsupported type: {oth}'.format(oth=type(other)))\n\n    def __matmul__(self, other):\n        \"\"\" Matrix multiplication using binary `@` operator in Python>=3.5 \"\"\"\n        return self.dot(other)\n\n    def __rmatmul__(self, other):\n        \"\"\" Matrix multiplication using binary `@` operator in Python>=3.5 \"\"\"\n        return self.T.dot(np.transpose(other)).T\n\n    # ----------------------------------------------------------------------\n    # IO methods (to / from other formats)\n\n    @classmethod\n    def from_dict(cls, data, orient='columns', dtype=None, columns=None):\n        \"\"\"\n        Construct DataFrame from dict of array-like or dicts.\n\n        Creates DataFrame object from dictionary by columns or by index\n        allowing dtype specification.\n\n        Parameters\n        ----------\n        data : dict\n            Of the form {field : array-like} or {field : dict}.\n        orient : {'columns', 'index'}, default 'columns'\n            The \"orientation\" of the data. If the keys of the passed dict\n            should be the columns of the resulting DataFrame, pass 'columns'\n            (default). Otherwise if the keys should be rows, pass 'index'.\n        dtype : dtype, default None\n            Data type to force, otherwise infer.\n        columns : list, default None\n            Column labels to use when ``orient='index'``. Raises a ValueError\n            if used with ``orient='columns'``.\n\n            .. versionadded:: 0.23.0\n\n        Returns\n        -------\n        pandas.DataFrame\n\n        See Also\n        --------\n        DataFrame.from_records : DataFrame from ndarray (structured\n            dtype), list of tuples, dict, or DataFrame.\n        DataFrame : DataFrame object creation using constructor.\n\n        Examples\n        --------\n        By default the keys of the dict become the DataFrame columns:\n\n        >>> data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}\n        >>> pd.DataFrame.from_dict(data)\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n\n        Specify ``orient='index'`` to create the DataFrame using dictionary\n        keys as rows:\n\n        >>> data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']}\n        >>> pd.DataFrame.from_dict(data, orient='index')\n               0  1  2  3\n        row_1  3  2  1  0\n        row_2  a  b  c  d\n\n        When using the 'index' orientation, the column names can be\n        specified manually:\n\n        >>> pd.DataFrame.from_dict(data, orient='index',\n        ...                        columns=['A', 'B', 'C', 'D'])\n               A  B  C  D\n        row_1  3  2  1  0\n        row_2  a  b  c  d\n        \"\"\"\n        index = None\n        orient = orient.lower()\n        if orient == 'index':\n            if len(data) > 0:\n                # TODO speed up Series case\n                if isinstance(list(data.values())[0], (Series, dict)):\n                    data = _from_nested_dict(data)\n                else:\n                    data, index = list(data.values()), list(data.keys())\n        elif orient == 'columns':\n            if columns is not None:\n                raise ValueError(\"cannot use columns parameter with \"\n                                 \"orient='columns'\")\n        else:  # pragma: no cover\n            raise ValueError('only recognize index or columns for orient')\n\n        return cls(data, index=index, columns=columns, dtype=dtype)\n\n    def to_dict(self, orient='dict', into=dict):\n        \"\"\"\n        Convert the DataFrame to a dictionary.\n\n        The type of the key-value pairs can be customized with the parameters\n        (see below).\n\n        Parameters\n        ----------\n        orient : str {'dict', 'list', 'series', 'split', 'records', 'index'}\n            Determines the type of the values of the dictionary.\n\n            - 'dict' (default) : dict like {column -> {index -> value}}\n            - 'list' : dict like {column -> [values]}\n            - 'series' : dict like {column -> Series(values)}\n            - 'split' : dict like\n              {'index' -> [index], 'columns' -> [columns], 'data' -> [values]}\n            - 'records' : list like\n              [{column -> value}, ... , {column -> value}]\n            - 'index' : dict like {index -> {column -> value}}\n\n            Abbreviations are allowed. `s` indicates `series` and `sp`\n            indicates `split`.\n\n        into : class, default dict\n            The collections.Mapping subclass used for all Mappings\n            in the return value.  Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n            .. versionadded:: 0.21.0\n\n        Returns\n        -------\n        dict, list or collections.Mapping\n            Return a collections.Mapping object representing the DataFrame.\n            The resulting transformation depends on the `orient` parameter.\n\n        See Also\n        --------\n        DataFrame.from_dict: Create a DataFrame from a dictionary.\n        DataFrame.to_json: Convert a DataFrame to JSON format.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2],\n        ...                    'col2': [0.5, 0.75]},\n        ...                   index=['row1', 'row2'])\n        >>> df\n              col1  col2\n        row1     1  0.50\n        row2     2  0.75\n        >>> df.to_dict()\n        {'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}}\n\n        You can specify the return orientation.\n\n        >>> df.to_dict('series')\n        {'col1': row1    1\n                 row2    2\n        Name: col1, dtype: int64,\n        'col2': row1    0.50\n                row2    0.75\n        Name: col2, dtype: float64}\n\n        >>> df.to_dict('split')\n        {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],\n         'data': [[1.0, 0.5], [2.0, 0.75]]}\n\n        >>> df.to_dict('records')\n        [{'col1': 1.0, 'col2': 0.5}, {'col1': 2.0, 'col2': 0.75}]\n\n        >>> df.to_dict('index')\n        {'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}}\n\n        You can also specify the mapping type.\n\n        >>> from collections import OrderedDict, defaultdict\n        >>> df.to_dict(into=OrderedDict)\n        OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])),\n                     ('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))])\n\n        If you want a `defaultdict`, you need to initialize it:\n\n        >>> dd = defaultdict(list)\n        >>> df.to_dict('records', into=dd)\n        [defaultdict(<class 'list'>, {'col1': 1.0, 'col2': 0.5}),\n         defaultdict(<class 'list'>, {'col1': 2.0, 'col2': 0.75})]\n        \"\"\"\n        if not self.columns.is_unique:\n            warnings.warn(\"DataFrame columns are not unique, some \"\n                          \"columns will be omitted.\", UserWarning,\n                          stacklevel=2)\n        # GH16122\n        into_c = com.standardize_mapping(into)\n        if orient.lower().startswith('d'):\n            return into_c(\n                (k, v.to_dict(into)) for k, v in compat.iteritems(self))\n        elif orient.lower().startswith('l'):\n            return into_c((k, v.tolist()) for k, v in compat.iteritems(self))\n        elif orient.lower().startswith('sp'):\n            return into_c((('index', self.index.tolist()),\n                           ('columns', self.columns.tolist()),\n                           ('data', lib.map_infer(self.values.ravel(),\n                                                  com.maybe_box_datetimelike)\n                            .reshape(self.values.shape).tolist())))\n        elif orient.lower().startswith('s'):\n            return into_c((k, com.maybe_box_datetimelike(v))\n                          for k, v in compat.iteritems(self))\n        elif orient.lower().startswith('r'):\n            return [into_c((k, com.maybe_box_datetimelike(v))\n                           for k, v in zip(self.columns, np.atleast_1d(row)))\n                    for row in self.values]\n        elif orient.lower().startswith('i'):\n            if not self.index.is_unique:\n                raise ValueError(\n                    \"DataFrame index must be unique for orient='index'.\"\n                )\n            return into_c((t[0], dict(zip(self.columns, t[1:])))\n                          for t in self.itertuples())\n        else:\n            raise ValueError(\"orient '{o}' not understood\".format(o=orient))\n\n    def to_gbq(self, destination_table, project_id=None, chunksize=None,\n               reauth=False, if_exists='fail', auth_local_webserver=False,\n               table_schema=None, location=None, progress_bar=True,\n               credentials=None, verbose=None, private_key=None):\n        \"\"\"\n        Write a DataFrame to a Google BigQuery table.\n\n        This function requires the `pandas-gbq package\n        <https://pandas-gbq.readthedocs.io>`__.\n\n        See the `How to authenticate with Google BigQuery\n        <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html>`__\n        guide for authentication instructions.\n\n        Parameters\n        ----------\n        destination_table : str\n            Name of table to be written, in the form ``dataset.tablename``.\n        project_id : str, optional\n            Google BigQuery Account project ID. Optional when available from\n            the environment.\n        chunksize : int, optional\n            Number of rows to be inserted in each chunk from the dataframe.\n            Set to ``None`` to load the whole dataframe at once.\n        reauth : bool, default False\n            Force Google BigQuery to re-authenticate the user. This is useful\n            if multiple accounts are used.\n        if_exists : str, default 'fail'\n            Behavior when the destination table exists. Value can be one of:\n\n            ``'fail'``\n                If table exists, do nothing.\n            ``'replace'``\n                If table exists, drop it, recreate it, and insert data.\n            ``'append'``\n                If table exists, insert data. Create if does not exist.\n        private_key : str, optional\n            Service account private key in JSON format. Can be file path\n            or string contents. This is useful for remote server\n            authentication (eg. Jupyter/IPython notebook on remote host).\n        auth_local_webserver : bool, default False\n            Use the `local webserver flow`_ instead of the `console flow`_\n            when getting user credentials.\n\n            .. _local webserver flow:\n                http://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server\n            .. _console flow:\n                http://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console\n\n            *New in version 0.2.0 of pandas-gbq*.\n        table_schema : list of dicts, optional\n            List of BigQuery table fields to which according DataFrame\n            columns conform to, e.g. ``[{'name': 'col1', 'type':\n            'STRING'},...]``. If schema is not provided, it will be\n            generated according to dtypes of DataFrame columns. See\n            BigQuery API documentation on available names of a field.\n\n            *New in version 0.3.1 of pandas-gbq*.\n        location : str, optional\n            Location where the load job should run. See the `BigQuery locations\n            documentation\n            <https://cloud.google.com/bigquery/docs/dataset-locations>`__ for a\n            list of available locations. The location must match that of the\n            target dataset.\n\n            *New in version 0.5.0 of pandas-gbq*.\n        progress_bar : bool, default True\n            Use the library `tqdm` to show the progress bar for the upload,\n            chunk by chunk.\n\n            *New in version 0.5.0 of pandas-gbq*.\n        credentials : google.auth.credentials.Credentials, optional\n            Credentials for accessing Google APIs. Use this parameter to\n            override default credentials, such as to use Compute Engine\n            :class:`google.auth.compute_engine.Credentials` or Service\n            Account :class:`google.oauth2.service_account.Credentials`\n            directly.\n\n            *New in version 0.8.0 of pandas-gbq*.\n\n            .. versionadded:: 0.24.0\n        verbose : bool, deprecated\n            Deprecated in pandas-gbq version 0.4.0. Use the `logging module\n            to adjust verbosity instead\n            <https://pandas-gbq.readthedocs.io/en/latest/intro.html#logging>`__.\n        private_key : str, deprecated\n            Deprecated in pandas-gbq version 0.8.0. Use the ``credentials``\n            parameter and\n            :func:`google.oauth2.service_account.Credentials.from_service_account_info`\n            or\n            :func:`google.oauth2.service_account.Credentials.from_service_account_file`\n            instead.\n\n            Service account private key in JSON format. Can be file path\n            or string contents. This is useful for remote server\n            authentication (eg. Jupyter/IPython notebook on remote host).\n\n        See Also\n        --------\n        pandas_gbq.to_gbq : This function in the pandas-gbq library.\n        pandas.read_gbq : Read a DataFrame from Google BigQuery.\n        \"\"\"\n        from pandas.io import gbq\n        return gbq.to_gbq(\n            self, destination_table, project_id=project_id,\n            chunksize=chunksize, reauth=reauth, if_exists=if_exists,\n            auth_local_webserver=auth_local_webserver,\n            table_schema=table_schema, location=location,\n            progress_bar=progress_bar, credentials=credentials,\n            verbose=verbose, private_key=private_key)\n\n    @classmethod\n    def from_records(cls, data, index=None, exclude=None, columns=None,\n                     coerce_float=False, nrows=None):\n        \"\"\"\n        Convert structured or record ndarray to DataFrame\n\n        Parameters\n        ----------\n        data : ndarray (structured dtype), list of tuples, dict, or DataFrame\n        index : string, list of fields, array-like\n            Field of array to use as the index, alternately a specific set of\n            input labels to use\n        exclude : sequence, default None\n            Columns or fields to exclude\n        columns : sequence, default None\n            Column names to use. If the passed data do not have names\n            associated with them, this argument provides names for the\n            columns. Otherwise this argument indicates the order of the columns\n            in the result (any names not found in the data will become all-NA\n            columns)\n        coerce_float : boolean, default False\n            Attempt to convert values of non-string, non-numeric objects (like\n            decimal.Decimal) to floating point, useful for SQL result sets\n        nrows : int, default None\n            Number of rows to read if data is an iterator\n\n        Returns\n        -------\n        df : DataFrame\n        \"\"\"\n\n        # Make a copy of the input columns so we can modify it\n        if columns is not None:\n            columns = ensure_index(columns)\n\n        if is_iterator(data):\n            if nrows == 0:\n                return cls()\n\n            try:\n                first_row = next(data)\n            except StopIteration:\n                return cls(index=index, columns=columns)\n\n            dtype = None\n            if hasattr(first_row, 'dtype') and first_row.dtype.names:\n                dtype = first_row.dtype\n\n            values = [first_row]\n\n            if nrows is None:\n                values += data\n            else:\n                values.extend(itertools.islice(data, nrows - 1))\n\n            if dtype is not None:\n                data = np.array(values, dtype=dtype)\n            else:\n                data = values\n\n        if isinstance(data, dict):\n            if columns is None:\n                columns = arr_columns = ensure_index(sorted(data))\n                arrays = [data[k] for k in columns]\n            else:\n                arrays = []\n                arr_columns = []\n                for k, v in compat.iteritems(data):\n                    if k in columns:\n                        arr_columns.append(k)\n                        arrays.append(v)\n\n                arrays, arr_columns = _reorder_arrays(arrays, arr_columns,\n                                                      columns)\n\n        elif isinstance(data, (np.ndarray, DataFrame)):\n            arrays, columns = _to_arrays(data, columns)\n            if columns is not None:\n                columns = ensure_index(columns)\n            arr_columns = columns\n        else:\n            arrays, arr_columns = _to_arrays(data, columns,\n                                             coerce_float=coerce_float)\n\n            arr_columns = ensure_index(arr_columns)\n            if columns is not None:\n                columns = ensure_index(columns)\n            else:\n                columns = arr_columns\n\n        if exclude is None:\n            exclude = set()\n        else:\n            exclude = set(exclude)\n\n        result_index = None\n        if index is not None:\n            if (isinstance(index, compat.string_types) or\n                    not hasattr(index, \"__iter__\")):\n                i = columns.get_loc(index)\n                exclude.add(index)\n                if len(arrays) > 0:\n                    result_index = Index(arrays[i], name=index)\n                else:\n                    result_index = Index([], name=index)\n            else:\n                try:\n                    to_remove = [arr_columns.get_loc(field) for field in index]\n                    index_data = [arrays[i] for i in to_remove]\n                    result_index = ensure_index_from_sequences(index_data,\n                                                               names=index)\n\n                    exclude.update(index)\n                except Exception:\n                    result_index = index\n\n        if any(exclude):\n            arr_exclude = [x for x in exclude if x in arr_columns]\n            to_remove = [arr_columns.get_loc(col) for col in arr_exclude]\n            arrays = [v for i, v in enumerate(arrays) if i not in to_remove]\n\n            arr_columns = arr_columns.drop(arr_exclude)\n            columns = columns.drop(exclude)\n\n        mgr = _arrays_to_mgr(arrays, arr_columns, result_index, columns)\n\n        return cls(mgr)\n\n    def to_records(self, index=True, convert_datetime64=None):\n        \"\"\"\n        Convert DataFrame to a NumPy record array.\n\n        Index will be included as the first field of the record array if\n        requested.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Include index in resulting record array, stored in 'index'\n            field or using the index label, if set.\n        convert_datetime64 : bool, default None\n            .. deprecated:: 0.23.0\n\n            Whether to convert the index to datetime.datetime if it is a\n            DatetimeIndex.\n\n        Returns\n        -------\n        numpy.recarray\n            NumPy ndarray with the DataFrame labels as fields and each row\n            of the DataFrame as entries.\n\n        See Also\n        --------\n        DataFrame.from_records: Convert structured or record ndarray\n            to DataFrame.\n        numpy.recarray: An ndarray that allows field access using\n            attributes, analogous to typed columns in a\n            spreadsheet.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]},\n        ...                   index=['a', 'b'])\n        >>> df\n           A     B\n        a  1  0.50\n        b  2  0.75\n        >>> df.to_records()\n        rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n                  dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')])\n\n        If the DataFrame index has no label then the recarray field name\n        is set to 'index'. If the index has a label then this is used as the\n        field name:\n\n        >>> df.index = df.index.rename(\"I\")\n        >>> df.to_records()\n        rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n                  dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')])\n\n        The index can be excluded from the record array:\n\n        >>> df.to_records(index=False)\n        rec.array([(1, 0.5 ), (2, 0.75)],\n                  dtype=[('A', '<i8'), ('B', '<f8')])\n        \"\"\"\n\n        if convert_datetime64 is not None:\n            warnings.warn(\"The 'convert_datetime64' parameter is \"\n                          \"deprecated and will be removed in a future \"\n                          \"version\",\n                          FutureWarning, stacklevel=2)\n\n        if index:\n            if is_datetime64_any_dtype(self.index) and convert_datetime64:\n                ix_vals = [self.index.to_pydatetime()]\n            else:\n                if isinstance(self.index, MultiIndex):\n                    # array of tuples to numpy cols. copy copy copy\n                    ix_vals = lmap(np.array, zip(*self.index.values))\n                else:\n                    ix_vals = [self.index.values]\n\n            arrays = ix_vals + [self[c].get_values() for c in self.columns]\n\n            count = 0\n            index_names = list(self.index.names)\n            if isinstance(self.index, MultiIndex):\n                for i, n in enumerate(index_names):\n                    if n is None:\n                        index_names[i] = 'level_%d' % count\n                        count += 1\n            elif index_names[0] is None:\n                index_names = ['index']\n            names = (lmap(compat.text_type, index_names) +\n                     lmap(compat.text_type, self.columns))\n        else:\n            arrays = [self[c].get_values() for c in self.columns]\n            names = lmap(compat.text_type, self.columns)\n\n        formats = [v.dtype for v in arrays]\n        return np.rec.fromarrays(\n            arrays,\n            dtype={'names': names, 'formats': formats}\n        )\n\n    @classmethod\n    def from_items(cls, items, columns=None, orient='columns'):\n        \"\"\"Construct a dataframe from a list of tuples\n\n        .. deprecated:: 0.23.0\n          `from_items` is deprecated and will be removed in a future version.\n          Use :meth:`DataFrame.from_dict(dict(items)) <DataFrame.from_dict>`\n          instead.\n          :meth:`DataFrame.from_dict(OrderedDict(items)) <DataFrame.from_dict>`\n          may be used to preserve the key order.\n\n        Convert (key, value) pairs to DataFrame. The keys will be the axis\n        index (usually the columns, but depends on the specified\n        orientation). The values should be arrays or Series.\n\n        Parameters\n        ----------\n        items : sequence of (key, value) pairs\n            Values should be arrays or Series.\n        columns : sequence of column labels, optional\n            Must be passed if orient='index'.\n        orient : {'columns', 'index'}, default 'columns'\n            The \"orientation\" of the data. If the keys of the\n            input correspond to column labels, pass 'columns'\n            (default). Otherwise if the keys correspond to the index,\n            pass 'index'.\n\n        Returns\n        -------\n        frame : DataFrame\n        \"\"\"\n\n        warnings.warn(\"from_items is deprecated. Please use \"\n                      \"DataFrame.from_dict(dict(items), ...) instead. \"\n                      \"DataFrame.from_dict(OrderedDict(items)) may be used to \"\n                      \"preserve the key order.\",\n                      FutureWarning, stacklevel=2)\n\n        keys, values = lzip(*items)\n\n        if orient == 'columns':\n            if columns is not None:\n                columns = ensure_index(columns)\n\n                idict = dict(items)\n                if len(idict) < len(items):\n                    if not columns.equals(ensure_index(keys)):\n                        raise ValueError('With non-unique item names, passed '\n                                         'columns must be identical')\n                    arrays = values\n                else:\n                    arrays = [idict[k] for k in columns if k in idict]\n            else:\n                columns = ensure_index(keys)\n                arrays = values\n\n            # GH 17312\n            # Provide more informative error msg when scalar values passed\n            try:\n                return cls._from_arrays(arrays, columns, None)\n\n            except ValueError:\n                if not is_nested_list_like(values):\n                    raise ValueError('The value in each (key, value) pair '\n                                     'must be an array, Series, or dict')\n\n        elif orient == 'index':\n            if columns is None:\n                raise TypeError(\"Must pass columns with orient='index'\")\n\n            keys = ensure_index(keys)\n\n            # GH 17312\n            # Provide more informative error msg when scalar values passed\n            try:\n                arr = np.array(values, dtype=object).T\n                data = [lib.maybe_convert_objects(v) for v in arr]\n                return cls._from_arrays(data, columns, keys)\n\n            except TypeError:\n                if not is_nested_list_like(values):\n                    raise ValueError('The value in each (key, value) pair '\n                                     'must be an array, Series, or dict')\n\n        else:  # pragma: no cover\n            raise ValueError(\"'orient' must be either 'columns' or 'index'\")\n\n    @classmethod\n    def _from_arrays(cls, arrays, columns, index, dtype=None):\n        mgr = _arrays_to_mgr(arrays, columns, index, columns, dtype=dtype)\n        return cls(mgr)\n\n    @classmethod\n    def from_csv(cls, path, header=0, sep=',', index_col=0, parse_dates=True,\n                 encoding=None, tupleize_cols=None,\n                 infer_datetime_format=False):\n        \"\"\"Read CSV file.\n\n        .. deprecated:: 0.21.0\n            Use :func:`pandas.read_csv` instead.\n\n        It is preferable to use the more powerful :func:`pandas.read_csv`\n        for most general purposes, but ``from_csv`` makes for an easy\n        roundtrip to and from a file (the exact counterpart of\n        ``to_csv``), especially with a DataFrame of time series data.\n\n        This method only differs from the preferred :func:`pandas.read_csv`\n        in some defaults:\n\n        - `index_col` is ``0`` instead of ``None`` (take first column as index\n          by default)\n        - `parse_dates` is ``True`` instead of ``False`` (try parsing the index\n          as datetime by default)\n\n        So a ``pd.DataFrame.from_csv(path)`` can be replaced by\n        ``pd.read_csv(path, index_col=0, parse_dates=True)``.\n\n        Parameters\n        ----------\n        path : string file path or file handle / StringIO\n        header : int, default 0\n            Row to use as header (skip prior rows)\n        sep : string, default ','\n            Field delimiter\n        index_col : int or sequence, default 0\n            Column to use for index. If a sequence is given, a MultiIndex\n            is used. Different default from read_table\n        parse_dates : boolean, default True\n            Parse dates. Different default from read_table\n        tupleize_cols : boolean, default False\n            write multi_index columns as a list of tuples (if True)\n            or new (expanded format) if False)\n        infer_datetime_format : boolean, default False\n            If True and `parse_dates` is True for a column, try to infer the\n            datetime format based on the first datetime string. If the format\n            can be inferred, there often will be a large parsing speed-up.\n\n        See Also\n        --------\n        pandas.read_csv\n\n        Returns\n        -------\n        y : DataFrame\n        \"\"\"\n\n        warnings.warn(\"from_csv is deprecated. Please use read_csv(...) \"\n                      \"instead. Note that some of the default arguments are \"\n                      \"different, so please refer to the documentation \"\n                      \"for from_csv when changing your function calls\",\n                      FutureWarning, stacklevel=2)\n\n        from pandas.io.parsers import read_csv\n        return read_csv(path, header=header, sep=sep,\n                        parse_dates=parse_dates, index_col=index_col,\n                        encoding=encoding, tupleize_cols=tupleize_cols,\n                        infer_datetime_format=infer_datetime_format)\n\n    def to_sparse(self, fill_value=None, kind='block'):\n        \"\"\"\n        Convert to SparseDataFrame.\n\n        Implement the sparse version of the DataFrame meaning that any data\n        matching a specific value it's omitted in the representation.\n        The sparse DataFrame allows for a more efficient storage.\n\n        Parameters\n        ----------\n        fill_value : float, default None\n            The specific value that should be omitted in the representation.\n        kind : {'block', 'integer'}, default 'block'\n            The kind of the SparseIndex tracking where data is not equal to\n            the fill value:\n\n            - 'block' tracks only the locations and sizes of blocks of data.\n            - 'integer' keeps an array with all the locations of the data.\n\n            In most cases 'block' is recommended, since it's more memory\n            efficient.\n\n        Returns\n        -------\n        SparseDataFrame\n            The sparse representation of the DataFrame.\n\n        See Also\n        --------\n        DataFrame.to_dense :\n            Converts the DataFrame back to the its dense form.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([(np.nan, np.nan),\n        ...                    (1., np.nan),\n        ...                    (np.nan, 1.)])\n        >>> df\n             0    1\n        0  NaN  NaN\n        1  1.0  NaN\n        2  NaN  1.0\n        >>> type(df)\n        <class 'pandas.core.frame.DataFrame'>\n\n        >>> sdf = df.to_sparse()\n        >>> sdf\n             0    1\n        0  NaN  NaN\n        1  1.0  NaN\n        2  NaN  1.0\n        >>> type(sdf)\n        <class 'pandas.core.sparse.frame.SparseDataFrame'>\n        \"\"\"\n        from pandas.core.sparse.api import SparseDataFrame\n        return SparseDataFrame(self._series, index=self.index,\n                               columns=self.columns, default_kind=kind,\n                               default_fill_value=fill_value)\n\n    def to_panel(self):\n        \"\"\"\n        Transform long (stacked) format (DataFrame) into wide (3D, Panel)\n        format.\n\n        .. deprecated:: 0.20.0\n\n        Currently the index of the DataFrame must be a 2-level MultiIndex. This\n        may be generalized later\n\n        Returns\n        -------\n        panel : Panel\n        \"\"\"\n        # only support this kind for now\n        if (not isinstance(self.index, MultiIndex) or  # pragma: no cover\n                len(self.index.levels) != 2):\n            raise NotImplementedError('Only 2-level MultiIndex are supported.')\n\n        if not self.index.is_unique:\n            raise ValueError(\"Can't convert non-uniquely indexed \"\n                             \"DataFrame to Panel\")\n\n        self._consolidate_inplace()\n\n        # minor axis must be sorted\n        if self.index.lexsort_depth < 2:\n            selfsorted = self.sort_index(level=0)\n        else:\n            selfsorted = self\n\n        major_axis, minor_axis = selfsorted.index.levels\n        major_labels, minor_labels = selfsorted.index.labels\n        shape = len(major_axis), len(minor_axis)\n\n        # preserve names, if any\n        major_axis = major_axis.copy()\n        major_axis.name = self.index.names[0]\n\n        minor_axis = minor_axis.copy()\n        minor_axis.name = self.index.names[1]\n\n        # create new axes\n        new_axes = [selfsorted.columns, major_axis, minor_axis]\n\n        # create new manager\n        new_mgr = selfsorted._data.reshape_nd(axes=new_axes,\n                                              labels=[major_labels,\n                                                      minor_labels],\n                                              shape=shape,\n                                              ref_items=selfsorted.columns)\n\n        return self._constructor_expanddim(new_mgr)\n\n    @deprecate_kwarg(old_arg_name='encoding', new_arg_name=None)\n    def to_stata(self, fname, convert_dates=None, write_index=True,\n                 encoding=\"latin-1\", byteorder=None, time_stamp=None,\n                 data_label=None, variable_labels=None, version=114,\n                 convert_strl=None):\n        \"\"\"\n        Export DataFrame object to Stata dta format.\n\n        Writes the DataFrame to a Stata dataset file.\n        \"dta\" files contain a Stata dataset.\n\n        Parameters\n        ----------\n        fname : str, buffer or path object\n            String, path object (pathlib.Path or py._path.local.LocalPath) or\n            object implementing a binary write() function. If using a buffer\n            then the buffer will not be automatically closed after the file\n            data has been written.\n        convert_dates : dict\n            Dictionary mapping columns containing datetime types to stata\n            internal format to use when writing the dates. Options are 'tc',\n            'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer\n            or a name. Datetime columns that do not have a conversion type\n            specified will be converted to 'tc'. Raises NotImplementedError if\n            a datetime column has timezone information.\n        write_index : bool\n            Write the index to Stata dataset.\n        encoding : str\n            Default is latin-1. Unicode is not supported.\n        byteorder : str\n            Can be \">\", \"<\", \"little\", or \"big\". default is `sys.byteorder`.\n        time_stamp : datetime\n            A datetime to use as file creation date.  Default is the current\n            time.\n        data_label : str, optional\n            A label for the data set.  Must be 80 characters or smaller.\n        variable_labels : dict\n            Dictionary containing columns as keys and variable labels as\n            values. Each label must be 80 characters or smaller.\n\n            .. versionadded:: 0.19.0\n\n        version : {114, 117}, default 114\n            Version to use in the output dta file.  Version 114 can be used\n            read by Stata 10 and later.  Version 117 can be read by Stata 13\n            or later. Version 114 limits string variables to 244 characters or\n            fewer while 117 allows strings with lengths up to 2,000,000\n            characters.\n\n            .. versionadded:: 0.23.0\n\n        convert_strl : list, optional\n            List of column names to convert to string columns to Stata StrL\n            format. Only available if version is 117.  Storing strings in the\n            StrL format can produce smaller dta files if strings have more than\n            8 characters and values are repeated.\n\n            .. versionadded:: 0.23.0\n\n        Raises\n        ------\n        NotImplementedError\n            * If datetimes contain timezone information\n            * Column dtype is not representable in Stata\n        ValueError\n            * Columns listed in convert_dates are neither datetime64[ns]\n              or datetime.datetime\n            * Column listed in convert_dates is not in DataFrame\n            * Categorical label contains more than 32,000 characters\n\n            .. versionadded:: 0.19.0\n\n        See Also\n        --------\n        read_stata : Import Stata data files.\n        io.stata.StataWriter : Low-level writer for Stata data files.\n        io.stata.StataWriter117 : Low-level writer for version 117 files.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'animal': ['falcon', 'parrot', 'falcon',\n        ...                               'parrot'],\n        ...                    'speed': [350, 18, 361, 15]})\n        >>> df.to_stata('animals.dta')  # doctest: +SKIP\n        \"\"\"\n        kwargs = {}\n        if version not in (114, 117):\n            raise ValueError('Only formats 114 and 117 supported.')\n        if version == 114:\n            if convert_strl is not None:\n                raise ValueError('strl support is only available when using '\n                                 'format 117')\n            from pandas.io.stata import StataWriter as statawriter\n        else:\n            from pandas.io.stata import StataWriter117 as statawriter\n            kwargs['convert_strl'] = convert_strl\n\n        writer = statawriter(fname, self, convert_dates=convert_dates,\n                             byteorder=byteorder, time_stamp=time_stamp,\n                             data_label=data_label, write_index=write_index,\n                             variable_labels=variable_labels, **kwargs)\n        writer.write_file()\n\n    def to_feather(self, fname):\n        \"\"\"\n        write out the binary feather-format for DataFrames\n\n        .. versionadded:: 0.20.0\n\n        Parameters\n        ----------\n        fname : str\n            string file path\n        \"\"\"\n        from pandas.io.feather_format import to_feather\n        to_feather(self, fname)\n\n    def to_parquet(self, fname, engine='auto', compression='snappy',\n                   index=None, partition_cols=None, **kwargs):\n        \"\"\"\n        Write a DataFrame to the binary parquet format.\n\n        .. versionadded:: 0.21.0\n\n        This function writes the dataframe as a `parquet file\n        <https://parquet.apache.org/>`_. You can choose different parquet\n        backends, and have the option of compression. See\n        :ref:`the user guide <io.parquet>` for more details.\n\n        Parameters\n        ----------\n        fname : str\n            File path or Root Directory path. Will be used as Root Directory\n            path while writing a partitioned dataset.\n\n            .. versionchanged:: 0.24.0\n\n        engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n            Parquet library to use. If 'auto', then the option\n            ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n            behavior is to try 'pyarrow', falling back to 'fastparquet' if\n            'pyarrow' is unavailable.\n        compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'\n            Name of the compression to use. Use ``None`` for no compression.\n        index : bool, default None\n            If ``True``, include the dataframe's index(es) in the file output.\n            If ``False``, they will not be written to the file. If ``None``,\n            the behavior depends on the chosen engine.\n\n            .. versionadded:: 0.24.0\n\n        partition_cols : list, optional, default None\n            Column names by which to partition the dataset\n            Columns are partitioned in the order they are given\n\n            .. versionadded:: 0.24.0\n\n        **kwargs\n            Additional arguments passed to the parquet library. See\n            :ref:`pandas io <io.parquet>` for more details.\n\n        See Also\n        --------\n        read_parquet : Read a parquet file.\n        DataFrame.to_csv : Write a csv file.\n        DataFrame.to_sql : Write to a sql table.\n        DataFrame.to_hdf : Write to hdf.\n\n        Notes\n        -----\n        This function requires either the `fastparquet\n        <https://pypi.org/project/fastparquet>`_ or `pyarrow\n        <https://arrow.apache.org/docs/python/>`_ library.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.to_parquet('df.parquet.gzip',\n        ...               compression='gzip')  # doctest: +SKIP\n        >>> pd.read_parquet('df.parquet.gzip')  # doctest: +SKIP\n           col1  col2\n        0     1     3\n        1     2     4\n        \"\"\"\n        from pandas.io.parquet import to_parquet\n        to_parquet(self, fname, engine,\n                   compression=compression, index=index,\n                   partition_cols=partition_cols, **kwargs)\n\n    @Substitution(header='Write out the column names. If a list of strings '\n                         'is given, it is assumed to be aliases for the '\n                         'column names')\n    @Substitution(shared_params=fmt.common_docstring,\n                  returns=fmt.return_docstring)\n    def to_string(self, buf=None, columns=None, col_space=None, header=True,\n                  index=True, na_rep='NaN', formatters=None, float_format=None,\n                  sparsify=None, index_names=True, justify=None,\n                  max_rows=None, max_cols=None, show_dimensions=False,\n                  decimal='.', line_width=None):\n        \"\"\"\n        Render a DataFrame to a console-friendly tabular output.\n        %(shared_params)s\n        line_width : int, optional\n            Width to wrap a line in characters.\n        %(returns)s\n        See Also\n        --------\n        to_html : Convert DataFrame to HTML.\n\n        Examples\n        --------\n        >>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n        >>> df = pd.DataFrame(d)\n        >>> print(df.to_string())\n           col1  col2\n        0     1     4\n        1     2     5\n        2     3     6\n        \"\"\"\n\n        formatter = fmt.DataFrameFormatter(self, buf=buf, columns=columns,\n                                           col_space=col_space, na_rep=na_rep,\n                                           formatters=formatters,\n                                           float_format=float_format,\n                                           sparsify=sparsify, justify=justify,\n                                           index_names=index_names,\n                                           header=header, index=index,\n                                           max_rows=max_rows,\n                                           max_cols=max_cols,\n                                           show_dimensions=show_dimensions,\n                                           decimal=decimal,\n                                           line_width=line_width)\n        formatter.to_string()\n\n        if buf is None:\n            result = formatter.buf.getvalue()\n            return result\n\n    @Substitution(header='Whether to print column labels, default True')\n    @Substitution(shared_params=fmt.common_docstring,\n                  returns=fmt.return_docstring)\n    def to_html(self, buf=None, columns=None, col_space=None, header=True,\n                index=True, na_rep='NaN', formatters=None, float_format=None,\n                sparsify=None, index_names=True, justify=None, max_rows=None,\n                max_cols=None, show_dimensions=False, decimal='.',\n                bold_rows=True, classes=None, escape=True,\n                notebook=False, border=None, table_id=None):\n        \"\"\"\n        Render a DataFrame as an HTML table.\n        %(shared_params)s\n        bold_rows : bool, default True\n            Make the row labels bold in the output.\n        classes : str or list or tuple, default None\n            CSS class(es) to apply to the resulting html table.\n        escape : bool, default True\n            Convert the characters <, >, and & to HTML-safe sequences.\n        notebook : {True, False}, default False\n            Whether the generated HTML is for IPython Notebook.\n        border : int\n            A ``border=border`` attribute is included in the opening\n            `<table>` tag. Default ``pd.options.html.border``.\n\n            .. versionadded:: 0.19.0\n\n        table_id : str, optional\n            A css id is included in the opening `<table>` tag if specified.\n\n            .. versionadded:: 0.23.0\n        %(returns)s\n        See Also\n        --------\n        to_string : Convert DataFrame to a string.\n        \"\"\"\n\n        if (justify is not None and\n                justify not in fmt._VALID_JUSTIFY_PARAMETERS):\n            raise ValueError(\"Invalid value for justify parameter\")\n\n        formatter = fmt.DataFrameFormatter(self, buf=buf, columns=columns,\n                                           col_space=col_space, na_rep=na_rep,\n                                           formatters=formatters,\n                                           float_format=float_format,\n                                           sparsify=sparsify, justify=justify,\n                                           index_names=index_names,\n                                           header=header, index=index,\n                                           bold_rows=bold_rows, escape=escape,\n                                           max_rows=max_rows,\n                                           max_cols=max_cols,\n                                           show_dimensions=show_dimensions,\n                                           decimal=decimal, table_id=table_id)\n        # TODO: a generic formatter wld b in DataFrameFormatter\n        formatter.to_html(classes=classes, notebook=notebook, border=border)\n\n        if buf is None:\n            return formatter.buf.getvalue()\n\n    def info(self, verbose=None, buf=None, max_cols=None, memory_usage=None,\n             null_counts=None):\n        \"\"\"\n        Print a concise summary of a DataFrame.\n\n        This method prints information about a DataFrame including\n        the index dtype and column dtypes, non-null values and memory usage.\n\n        Parameters\n        ----------\n        verbose : bool, optional\n            Whether to print the full summary. By default, the setting in\n            ``pandas.options.display.max_info_columns`` is followed.\n        buf : writable buffer, defaults to sys.stdout\n            Where to send the output. By default, the output is printed to\n            sys.stdout. Pass a writable buffer if you need to further process\n            the output.\n        max_cols : int, optional\n            When to switch from the verbose to the truncated output. If the\n            DataFrame has more than `max_cols` columns, the truncated output\n            is used. By default, the setting in\n            ``pandas.options.display.max_info_columns`` is used.\n        memory_usage : bool, str, optional\n            Specifies whether total memory usage of the DataFrame\n            elements (including the index) should be displayed. By default,\n            this follows the ``pandas.options.display.memory_usage`` setting.\n\n            True always show memory usage. False never shows memory usage.\n            A value of 'deep' is equivalent to \"True with deep introspection\".\n            Memory usage is shown in human-readable units (base-2\n            representation). Without deep introspection a memory estimation is\n            made based in column dtype and number of rows assuming values\n            consume the same memory amount for corresponding dtypes. With deep\n            memory introspection, a real memory usage calculation is performed\n            at the cost of computational resources.\n        null_counts : bool, optional\n            Whether to show the non-null counts. By default, this is shown\n            only if the frame is smaller than\n            ``pandas.options.display.max_info_rows`` and\n            ``pandas.options.display.max_info_columns``. A value of True always\n            shows the counts, and False never shows the counts.\n\n        Returns\n        -------\n        None\n            This method prints a summary of a DataFrame and returns None.\n\n        See Also\n        --------\n        DataFrame.describe: Generate descriptive statistics of DataFrame\n            columns.\n        DataFrame.memory_usage: Memory usage of DataFrame columns.\n\n        Examples\n        --------\n        >>> int_values = [1, 2, 3, 4, 5]\n        >>> text_values = ['alpha', 'beta', 'gamma', 'delta', 'epsilon']\n        >>> float_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n        >>> df = pd.DataFrame({\"int_col\": int_values, \"text_col\": text_values,\n        ...                   \"float_col\": float_values})\n        >>> df\n           int_col text_col  float_col\n        0        1    alpha       0.00\n        1        2     beta       0.25\n        2        3    gamma       0.50\n        3        4    delta       0.75\n        4        5  epsilon       1.00\n\n        Prints information of all columns:\n\n        >>> df.info(verbose=True)\n        <class 'pandas.core.frame.DataFrame'>\n        RangeIndex: 5 entries, 0 to 4\n        Data columns (total 3 columns):\n        int_col      5 non-null int64\n        text_col     5 non-null object\n        float_col    5 non-null float64\n        dtypes: float64(1), int64(1), object(1)\n        memory usage: 200.0+ bytes\n\n        Prints a summary of columns count and its dtypes but not per column\n        information:\n\n        >>> df.info(verbose=False)\n        <class 'pandas.core.frame.DataFrame'>\n        RangeIndex: 5 entries, 0 to 4\n        Columns: 3 entries, int_col to float_col\n        dtypes: float64(1), int64(1), object(1)\n        memory usage: 200.0+ bytes\n\n        Pipe output of DataFrame.info to buffer instead of sys.stdout, get\n        buffer content and writes to a text file:\n\n        >>> import io\n        >>> buffer = io.StringIO()\n        >>> df.info(buf=buffer)\n        >>> s = buffer.getvalue()\n        >>> with open(\"df_info.txt\", \"w\",\n        ...           encoding=\"utf-8\") as f:  # doctest: +SKIP\n        ...     f.write(s)\n        260\n\n        The `memory_usage` parameter allows deep introspection mode, specially\n        useful for big DataFrames and fine-tune memory optimization:\n\n        >>> random_strings_array = np.random.choice(['a', 'b', 'c'], 10 ** 6)\n        >>> df = pd.DataFrame({\n        ...     'column_1': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n        ...     'column_2': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n        ...     'column_3': np.random.choice(['a', 'b', 'c'], 10 ** 6)\n        ... })\n        >>> df.info()\n        <class 'pandas.core.frame.DataFrame'>\n        RangeIndex: 1000000 entries, 0 to 999999\n        Data columns (total 3 columns):\n        column_1    1000000 non-null object\n        column_2    1000000 non-null object\n        column_3    1000000 non-null object\n        dtypes: object(3)\n        memory usage: 22.9+ MB\n\n        >>> df.info(memory_usage='deep')\n        <class 'pandas.core.frame.DataFrame'>\n        RangeIndex: 1000000 entries, 0 to 999999\n        Data columns (total 3 columns):\n        column_1    1000000 non-null object\n        column_2    1000000 non-null object\n        column_3    1000000 non-null object\n        dtypes: object(3)\n        memory usage: 188.8 MB\n        \"\"\"\n\n        if buf is None:  # pragma: no cover\n            buf = sys.stdout\n\n        lines = []\n\n        lines.append(str(type(self)))\n        lines.append(self.index._summary())\n\n        if len(self.columns) == 0:\n            lines.append('Empty {name}'.format(name=type(self).__name__))\n            fmt.buffer_put_lines(buf, lines)\n            return\n\n        cols = self.columns\n\n        # hack\n        if max_cols is None:\n            max_cols = get_option('display.max_info_columns',\n                                  len(self.columns) + 1)\n\n        max_rows = get_option('display.max_info_rows', len(self) + 1)\n\n        if null_counts is None:\n            show_counts = ((len(self.columns) <= max_cols) and\n                           (len(self) < max_rows))\n        else:\n            show_counts = null_counts\n        exceeds_info_cols = len(self.columns) > max_cols\n\n        def _verbose_repr():\n            lines.append('Data columns (total %d columns):' %\n                         len(self.columns))\n            space = max(len(pprint_thing(k)) for k in self.columns) + 4\n            counts = None\n\n            tmpl = \"{count}{dtype}\"\n            if show_counts:\n                counts = self.count()\n                if len(cols) != len(counts):  # pragma: no cover\n                    raise AssertionError(\n                        'Columns must equal counts '\n                        '({cols:d} != {counts:d})'.format(\n                            cols=len(cols), counts=len(counts)))\n                tmpl = \"{count} non-null {dtype}\"\n\n            dtypes = self.dtypes\n            for i, col in enumerate(self.columns):\n                dtype = dtypes.iloc[i]\n                col = pprint_thing(col)\n\n                count = \"\"\n                if show_counts:\n                    count = counts.iloc[i]\n\n                lines.append(_put_str(col, space) + tmpl.format(count=count,\n                                                                dtype=dtype))\n\n        def _non_verbose_repr():\n            lines.append(self.columns._summary(name='Columns'))\n\n        def _sizeof_fmt(num, size_qualifier):\n            # returns size in human readable format\n            for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n                if num < 1024.0:\n                    return (\"{num:3.1f}{size_q} \"\n                            \"{x}\".format(num=num, size_q=size_qualifier, x=x))\n                num /= 1024.0\n            return \"{num:3.1f}{size_q} {pb}\".format(num=num,\n                                                    size_q=size_qualifier,\n                                                    pb='PB')\n\n        if verbose:\n            _verbose_repr()\n        elif verbose is False:  # specifically set to False, not nesc None\n            _non_verbose_repr()\n        else:\n            if exceeds_info_cols:\n                _non_verbose_repr()\n            else:\n                _verbose_repr()\n\n        counts = self.get_dtype_counts()\n        dtypes = ['{k}({kk:d})'.format(k=k[0], kk=k[1]) for k\n                  in sorted(compat.iteritems(counts))]\n        lines.append('dtypes: {types}'.format(types=', '.join(dtypes)))\n\n        if memory_usage is None:\n            memory_usage = get_option('display.memory_usage')\n        if memory_usage:\n            # append memory usage of df to display\n            size_qualifier = ''\n            if memory_usage == 'deep':\n                deep = True\n            else:\n                # size_qualifier is just a best effort; not guaranteed to catch\n                # all cases (e.g., it misses categorical data even with object\n                # categories)\n                deep = False\n                if ('object' in counts or\n                        self.index._is_memory_usage_qualified()):\n                    size_qualifier = '+'\n            mem_usage = self.memory_usage(index=True, deep=deep).sum()\n            lines.append(\"memory usage: {mem}\\n\".format(\n                mem=_sizeof_fmt(mem_usage, size_qualifier)))\n\n        fmt.buffer_put_lines(buf, lines)\n\n    def memory_usage(self, index=True, deep=False):\n        \"\"\"\n        Return the memory usage of each column in bytes.\n\n        The memory usage can optionally include the contribution of\n        the index and elements of `object` dtype.\n\n        This value is displayed in `DataFrame.info` by default. This can be\n        suppressed by setting ``pandas.options.display.memory_usage`` to False.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the DataFrame's\n            index in returned Series. If ``index=True`` the memory usage of the\n            index the first item in the output.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned values.\n\n        Returns\n        -------\n        sizes : Series\n            A Series whose index is the original column names and whose values\n            is the memory usage of each column in bytes.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of an\n            ndarray.\n        Series.memory_usage : Bytes consumed by a Series.\n        pandas.Categorical : Memory-efficient array for string values with\n            many repeated values.\n        DataFrame.info : Concise summary of a DataFrame.\n\n        Examples\n        --------\n        >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool']\n        >>> data = dict([(t, np.ones(shape=5000).astype(t))\n        ...              for t in dtypes])\n        >>> df = pd.DataFrame(data)\n        >>> df.head()\n           int64  float64  complex128 object  bool\n        0      1      1.0      (1+0j)      1  True\n        1      1      1.0      (1+0j)      1  True\n        2      1      1.0      (1+0j)      1  True\n        3      1      1.0      (1+0j)      1  True\n        4      1      1.0      (1+0j)      1  True\n\n        >>> df.memory_usage()\n        Index            80\n        int64         40000\n        float64       40000\n        complex128    80000\n        object        40000\n        bool           5000\n        dtype: int64\n\n        >>> df.memory_usage(index=False)\n        int64         40000\n        float64       40000\n        complex128    80000\n        object        40000\n        bool           5000\n        dtype: int64\n\n        The memory footprint of `object` dtype columns is ignored by default:\n\n        >>> df.memory_usage(deep=True)\n        Index             80\n        int64          40000\n        float64        40000\n        complex128     80000\n        object        160000\n        bool            5000\n        dtype: int64\n\n        Use a Categorical for efficient storage of an object-dtype column with\n        many repeated values.\n\n        >>> df['object'].astype('category').memory_usage(deep=True)\n        5168\n        \"\"\"\n        result = Series([c.memory_usage(index=False, deep=deep)\n                         for col, c in self.iteritems()], index=self.columns)\n        if index:\n            result = Series(self.index.memory_usage(deep=deep),\n                            index=['Index']).append(result)\n        return result\n\n    def transpose(self, *args, **kwargs):\n        \"\"\"\n        Transpose index and columns.\n\n        Reflect the DataFrame over its main diagonal by writing rows as columns\n        and vice-versa. The property :attr:`.T` is an accessor to the method\n        :meth:`transpose`.\n\n        Parameters\n        ----------\n        copy : bool, default False\n            If True, the underlying data is copied. Otherwise (default), no\n            copy is made if possible.\n        *args, **kwargs\n            Additional keywords have no effect but might be accepted for\n            compatibility with numpy.\n\n        Returns\n        -------\n        DataFrame\n            The transposed DataFrame.\n\n        See Also\n        --------\n        numpy.transpose : Permute the dimensions of a given array.\n\n        Notes\n        -----\n        Transposing a DataFrame with mixed dtypes will result in a homogeneous\n        DataFrame with the `object` dtype. In such a case, a copy of the data\n        is always made.\n\n        Examples\n        --------\n        **Square DataFrame with homogeneous dtype**\n\n        >>> d1 = {'col1': [1, 2], 'col2': [3, 4]}\n        >>> df1 = pd.DataFrame(data=d1)\n        >>> df1\n           col1  col2\n        0     1     3\n        1     2     4\n\n        >>> df1_transposed = df1.T # or df1.transpose()\n        >>> df1_transposed\n              0  1\n        col1  1  2\n        col2  3  4\n\n        When the dtype is homogeneous in the original DataFrame, we get a\n        transposed DataFrame with the same dtype:\n\n        >>> df1.dtypes\n        col1    int64\n        col2    int64\n        dtype: object\n        >>> df1_transposed.dtypes\n        0    int64\n        1    int64\n        dtype: object\n\n        **Non-square DataFrame with mixed dtypes**\n\n        >>> d2 = {'name': ['Alice', 'Bob'],\n        ...       'score': [9.5, 8],\n        ...       'employed': [False, True],\n        ...       'kids': [0, 0]}\n        >>> df2 = pd.DataFrame(data=d2)\n        >>> df2\n            name  score  employed  kids\n        0  Alice    9.5     False     0\n        1    Bob    8.0      True     0\n\n        >>> df2_transposed = df2.T # or df2.transpose()\n        >>> df2_transposed\n                      0     1\n        name      Alice   Bob\n        score       9.5     8\n        employed  False  True\n        kids          0     0\n\n        When the DataFrame has mixed dtypes, we get a transposed DataFrame with\n        the `object` dtype:\n\n        >>> df2.dtypes\n        name         object\n        score       float64\n        employed       bool\n        kids          int64\n        dtype: object\n        >>> df2_transposed.dtypes\n        0    object\n        1    object\n        dtype: object\n        \"\"\"\n        nv.validate_transpose(args, dict())\n        return super(DataFrame, self).transpose(1, 0, **kwargs)\n\n    T = property(transpose)\n\n    # ----------------------------------------------------------------------\n    # Picklability\n\n    # legacy pickle formats\n    def _unpickle_frame_compat(self, state):  # pragma: no cover\n        if len(state) == 2:  # pragma: no cover\n            series, idx = state\n            columns = sorted(series)\n        else:\n            series, cols, idx = state\n            columns = com._unpickle_array(cols)\n\n        index = com._unpickle_array(idx)\n        self._data = self._init_dict(series, index, columns, None)\n\n    def _unpickle_matrix_compat(self, state):  # pragma: no cover\n        # old unpickling\n        (vals, idx, cols), object_state = state\n\n        index = com._unpickle_array(idx)\n        dm = DataFrame(vals, index=index, columns=com._unpickle_array(cols),\n                       copy=False)\n\n        if object_state is not None:\n            ovals, _, ocols = object_state\n            objects = DataFrame(ovals, index=index,\n                                columns=com._unpickle_array(ocols), copy=False)\n\n            dm = dm.join(objects)\n\n        self._data = dm._data\n\n    # ----------------------------------------------------------------------\n    # Getting and setting elements\n\n    def get_value(self, index, col, takeable=False):\n        \"\"\"Quickly retrieve single value at passed column and index\n\n        .. deprecated:: 0.21.0\n            Use .at[] or .iat[] accessors instead.\n\n        Parameters\n        ----------\n        index : row label\n        col : column label\n        takeable : interpret the index/col as indexers, default False\n\n        Returns\n        -------\n        value : scalar value\n        \"\"\"\n\n        warnings.warn(\"get_value is deprecated and will be removed \"\n                      \"in a future release. Please use \"\n                      \".at[] or .iat[] accessors instead\", FutureWarning,\n                      stacklevel=2)\n        return self._get_value(index, col, takeable=takeable)\n\n    def _get_value(self, index, col, takeable=False):\n\n        if takeable:\n            series = self._iget_item_cache(col)\n            return com.maybe_box_datetimelike(series._values[index])\n\n        series = self._get_item_cache(col)\n        engine = self.index._engine\n\n        try:\n            return engine.get_value(series._values, index)\n        except (TypeError, ValueError):\n\n            # we cannot handle direct indexing\n            # use positional\n            col = self.columns.get_loc(col)\n            index = self.index.get_loc(index)\n            return self._get_value(index, col, takeable=True)\n    _get_value.__doc__ = get_value.__doc__\n\n    def set_value(self, index, col, value, takeable=False):\n        \"\"\"Put single value at passed column and index\n\n        .. deprecated:: 0.21.0\n            Use .at[] or .iat[] accessors instead.\n\n        Parameters\n        ----------\n        index : row label\n        col : column label\n        value : scalar value\n        takeable : interpret the index/col as indexers, default False\n\n        Returns\n        -------\n        frame : DataFrame\n            If label pair is contained, will be reference to calling DataFrame,\n            otherwise a new object\n        \"\"\"\n        warnings.warn(\"set_value is deprecated and will be removed \"\n                      \"in a future release. Please use \"\n                      \".at[] or .iat[] accessors instead\", FutureWarning,\n                      stacklevel=2)\n        return self._set_value(index, col, value, takeable=takeable)\n\n    def _set_value(self, index, col, value, takeable=False):\n        try:\n            if takeable is True:\n                series = self._iget_item_cache(col)\n                return series._set_value(index, value, takeable=True)\n\n            series = self._get_item_cache(col)\n            engine = self.index._engine\n            engine.set_value(series._values, index, value)\n            return self\n        except (KeyError, TypeError):\n\n            # set using a non-recursive method & reset the cache\n            self.loc[index, col] = value\n            self._item_cache.pop(col, None)\n\n            return self\n    _set_value.__doc__ = set_value.__doc__\n\n    def _ixs(self, i, axis=0):\n        \"\"\"\n        i : int, slice, or sequence of integers\n        axis : int\n        \"\"\"\n\n        # irow\n        if axis == 0:\n            \"\"\"\n            Notes\n            -----\n            If slice passed, the resulting data will be a view\n            \"\"\"\n\n            if isinstance(i, slice):\n                return self[i]\n            else:\n                label = self.index[i]\n                if isinstance(label, Index):\n                    # a location index by definition\n                    result = self.take(i, axis=axis)\n                    copy = True\n                else:\n                    new_values = self._data.fast_xs(i)\n                    if is_scalar(new_values):\n                        return new_values\n\n                    # if we are a copy, mark as such\n                    copy = (isinstance(new_values, np.ndarray) and\n                            new_values.base is None)\n                    result = self._constructor_sliced(new_values,\n                                                      index=self.columns,\n                                                      name=self.index[i],\n                                                      dtype=new_values.dtype)\n                result._set_is_copy(self, copy=copy)\n                return result\n\n        # icol\n        else:\n            \"\"\"\n            Notes\n            -----\n            If slice passed, the resulting data will be a view\n            \"\"\"\n\n            label = self.columns[i]\n            if isinstance(i, slice):\n                # need to return view\n                lab_slice = slice(label[0], label[-1])\n                return self.loc[:, lab_slice]\n            else:\n                if isinstance(label, Index):\n                    return self._take(i, axis=1)\n\n                index_len = len(self.index)\n\n                # if the values returned are not the same length\n                # as the index (iow a not found value), iget returns\n                # a 0-len ndarray. This is effectively catching\n                # a numpy error (as numpy should really raise)\n                values = self._data.iget(i)\n\n                if index_len and not len(values):\n                    values = np.array([np.nan] * index_len, dtype=object)\n                result = self._box_col_values(values, label)\n\n                # this is a cached value, mark it so\n                result._set_as_cached(label, self)\n\n                return result\n\n    def __getitem__(self, key):\n        key = com.apply_if_callable(key, self)\n\n        # shortcut if the key is in columns\n        try:\n            if self.columns.is_unique and key in self.columns:\n                if self.columns.nlevels > 1:\n                    return self._getitem_multilevel(key)\n                return self._get_item_cache(key)\n        except (TypeError, ValueError):\n            # The TypeError correctly catches non hashable \"key\" (e.g. list)\n            # The ValueError can be removed once GH #21729 is fixed\n            pass\n\n        # Do we have a slicer (on rows)?\n        indexer = convert_to_index_sliceable(self, key)\n        if indexer is not None:\n            return self._slice(indexer, axis=0)\n\n        # Do we have a (boolean) DataFrame?\n        if isinstance(key, DataFrame):\n            return self._getitem_frame(key)\n\n        # Do we have a (boolean) 1d indexer?\n        if com.is_bool_indexer(key):\n            return self._getitem_bool_array(key)\n\n        # We are left with two options: a single key, and a collection of keys,\n        # We interpret tuples as collections only for non-MultiIndex\n        is_single_key = isinstance(key, tuple) or not is_list_like(key)\n\n        if is_single_key:\n            if self.columns.nlevels > 1:\n                return self._getitem_multilevel(key)\n            indexer = self.columns.get_loc(key)\n            if is_integer(indexer):\n                indexer = [indexer]\n        else:\n            if is_iterator(key):\n                key = list(key)\n            indexer = self.loc._convert_to_indexer(key, axis=1,\n                                                   raise_missing=True)\n\n        # take() does not accept boolean indexers\n        if getattr(indexer, \"dtype\", None) == bool:\n            indexer = np.where(indexer)[0]\n\n        data = self._take(indexer, axis=1)\n\n        if is_single_key:\n            # What does looking for a single key in a non-unique index return?\n            # The behavior is inconsistent. It returns a Series, except when\n            # - the key itself is repeated (test on data.shape, #9519), or\n            # - we have a MultiIndex on columns (test on self.columns, #21309)\n            if data.shape[1] == 1 and not isinstance(self.columns, MultiIndex):\n                data = data[key]\n\n        return data\n\n    def _getitem_bool_array(self, key):\n        # also raises Exception if object array with NA values\n        # warning here just in case -- previously __setitem__ was\n        # reindexing but __getitem__ was not; it seems more reasonable to\n        # go with the __setitem__ behavior since that is more consistent\n        # with all other indexing behavior\n        if isinstance(key, Series) and not key.index.equals(self.index):\n            warnings.warn(\"Boolean Series key will be reindexed to match \"\n                          \"DataFrame index.\", UserWarning, stacklevel=3)\n        elif len(key) != len(self.index):\n            raise ValueError('Item wrong length %d instead of %d.' %\n                             (len(key), len(self.index)))\n\n        # check_bool_indexer will throw exception if Series key cannot\n        # be reindexed to match DataFrame rows\n        key = check_bool_indexer(self.index, key)\n        indexer = key.nonzero()[0]\n        return self._take(indexer, axis=0)\n\n    def _getitem_multilevel(self, key):\n        loc = self.columns.get_loc(key)\n        if isinstance(loc, (slice, Series, np.ndarray, Index)):\n            new_columns = self.columns[loc]\n            result_columns = maybe_droplevels(new_columns, key)\n            if self._is_mixed_type:\n                result = self.reindex(columns=new_columns)\n                result.columns = result_columns\n            else:\n                new_values = self.values[:, loc]\n                result = self._constructor(new_values, index=self.index,\n                                           columns=result_columns)\n                result = result.__finalize__(self)\n\n            # If there is only one column being returned, and its name is\n            # either an empty string, or a tuple with an empty string as its\n            # first element, then treat the empty string as a placeholder\n            # and return the column as if the user had provided that empty\n            # string in the key. If the result is a Series, exclude the\n            # implied empty string from its name.\n            if len(result.columns) == 1:\n                top = result.columns[0]\n                if isinstance(top, tuple):\n                    top = top[0]\n                if top == '':\n                    result = result['']\n                    if isinstance(result, Series):\n                        result = self._constructor_sliced(result,\n                                                          index=self.index,\n                                                          name=key)\n\n            result._set_is_copy(self)\n            return result\n        else:\n            return self._get_item_cache(key)\n\n    def _getitem_frame(self, key):\n        if key.values.size and not is_bool_dtype(key.values):\n            raise ValueError('Must pass DataFrame with boolean values only')\n        return self.where(key)\n\n    def query(self, expr, inplace=False, **kwargs):\n        \"\"\"Query the columns of a frame with a boolean expression.\n\n        Parameters\n        ----------\n        expr : string\n            The query string to evaluate.  You can refer to variables\n            in the environment by prefixing them with an '@' character like\n            ``@a + b``.\n        inplace : bool\n            Whether the query should modify the data in place or return\n            a modified copy\n\n            .. versionadded:: 0.18.0\n\n        kwargs : dict\n            See the documentation for :func:`pandas.eval` for complete details\n            on the keyword arguments accepted by :meth:`DataFrame.query`.\n\n        Returns\n        -------\n        q : DataFrame\n\n        Notes\n        -----\n        The result of the evaluation of this expression is first passed to\n        :attr:`DataFrame.loc` and if that fails because of a\n        multidimensional key (e.g., a DataFrame) then the result will be passed\n        to :meth:`DataFrame.__getitem__`.\n\n        This method uses the top-level :func:`pandas.eval` function to\n        evaluate the passed query.\n\n        The :meth:`~pandas.DataFrame.query` method uses a slightly\n        modified Python syntax by default. For example, the ``&`` and ``|``\n        (bitwise) operators have the precedence of their boolean cousins,\n        :keyword:`and` and :keyword:`or`. This *is* syntactically valid Python,\n        however the semantics are different.\n\n        You can change the semantics of the expression by passing the keyword\n        argument ``parser='python'``. This enforces the same semantics as\n        evaluation in Python space. Likewise, you can pass ``engine='python'``\n        to evaluate an expression using Python itself as a backend. This is not\n        recommended as it is inefficient compared to using ``numexpr`` as the\n        engine.\n\n        The :attr:`DataFrame.index` and\n        :attr:`DataFrame.columns` attributes of the\n        :class:`~pandas.DataFrame` instance are placed in the query namespace\n        by default, which allows you to treat both the index and columns of the\n        frame as a column in the frame.\n        The identifier ``index`` is used for the frame index; you can also\n        use the name of the index to identify it in a query. Please note that\n        Python keywords may not be used as identifiers.\n\n        For further details and examples see the ``query`` documentation in\n        :ref:`indexing <indexing.query>`.\n\n        See Also\n        --------\n        pandas.eval\n        DataFrame.eval\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.random.randn(10, 2), columns=list('ab'))\n        >>> df.query('a > b')\n        >>> df[df.a > df.b]  # same result as the previous expression\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        if not isinstance(expr, compat.string_types):\n            msg = \"expr must be a string to be evaluated, {0} given\"\n            raise ValueError(msg.format(type(expr)))\n        kwargs['level'] = kwargs.pop('level', 0) + 1\n        kwargs['target'] = None\n        res = self.eval(expr, **kwargs)\n\n        try:\n            new_data = self.loc[res]\n        except ValueError:\n            # when res is multi-dimensional loc raises, but this is sometimes a\n            # valid query\n            new_data = self[res]\n\n        if inplace:\n            self._update_inplace(new_data)\n        else:\n            return new_data\n\n    def eval(self, expr, inplace=False, **kwargs):\n        \"\"\"\n        Evaluate a string describing operations on DataFrame columns.\n\n        Operates on columns only, not specific rows or elements.  This allows\n        `eval` to run arbitrary code, which can make you vulnerable to code\n        injection if you pass user input to this function.\n\n        Parameters\n        ----------\n        expr : str\n            The expression string to evaluate.\n        inplace : bool, default False\n            If the expression contains an assignment, whether to perform the\n            operation inplace and mutate the existing DataFrame. Otherwise,\n            a new DataFrame is returned.\n\n            .. versionadded:: 0.18.0.\n        kwargs : dict\n            See the documentation for :func:`~pandas.eval` for complete details\n            on the keyword arguments accepted by\n            :meth:`~pandas.DataFrame.query`.\n\n        Returns\n        -------\n        ndarray, scalar, or pandas object\n            The result of the evaluation.\n\n        See Also\n        --------\n        DataFrame.query : Evaluates a boolean expression to query the columns\n            of a frame.\n        DataFrame.assign : Can evaluate an expression or function to create new\n            values for a column.\n        pandas.eval : Evaluate a Python expression as a string using various\n            backends.\n\n        Notes\n        -----\n        For more details see the API documentation for :func:`~pandas.eval`.\n        For detailed examples see :ref:`enhancing performance with eval\n        <enhancingperf.eval>`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': range(1, 6), 'B': range(10, 0, -2)})\n        >>> df\n           A   B\n        0  1  10\n        1  2   8\n        2  3   6\n        3  4   4\n        4  5   2\n        >>> df.eval('A + B')\n        0    11\n        1    10\n        2     9\n        3     8\n        4     7\n        dtype: int64\n\n        Assignment is allowed though by default the original DataFrame is not\n        modified.\n\n        >>> df.eval('C = A + B')\n           A   B   C\n        0  1  10  11\n        1  2   8  10\n        2  3   6   9\n        3  4   4   8\n        4  5   2   7\n        >>> df\n           A   B\n        0  1  10\n        1  2   8\n        2  3   6\n        3  4   4\n        4  5   2\n\n        Use ``inplace=True`` to modify the original DataFrame.\n\n        >>> df.eval('C = A + B', inplace=True)\n        >>> df\n           A   B   C\n        0  1  10  11\n        1  2   8  10\n        2  3   6   9\n        3  4   4   8\n        4  5   2   7\n        \"\"\"\n        from pandas.core.computation.eval import eval as _eval\n\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        resolvers = kwargs.pop('resolvers', None)\n        kwargs['level'] = kwargs.pop('level', 0) + 1\n        if resolvers is None:\n            index_resolvers = self._get_index_resolvers()\n            resolvers = dict(self.iteritems()), index_resolvers\n        if 'target' not in kwargs:\n            kwargs['target'] = self\n        kwargs['resolvers'] = kwargs.get('resolvers', ()) + tuple(resolvers)\n        return _eval(expr, inplace=inplace, **kwargs)\n\n    def select_dtypes(self, include=None, exclude=None):\n        \"\"\"\n        Return a subset of the DataFrame's columns based on the column dtypes.\n\n        Parameters\n        ----------\n        include, exclude : scalar or list-like\n            A selection of dtypes or strings to be included/excluded. At least\n            one of these parameters must be supplied.\n\n        Raises\n        ------\n        ValueError\n            * If both of ``include`` and ``exclude`` are empty\n            * If ``include`` and ``exclude`` have overlapping elements\n            * If any kind of string dtype is passed in.\n\n        Returns\n        -------\n        subset : DataFrame\n            The subset of the frame including the dtypes in ``include`` and\n            excluding the dtypes in ``exclude``.\n\n        Notes\n        -----\n        * To select all *numeric* types, use ``np.number`` or ``'number'``\n        * To select strings you must use the ``object`` dtype, but note that\n          this will return *all* object dtype columns\n        * See the `numpy dtype hierarchy\n          <http://docs.scipy.org/doc/numpy/reference/arrays.scalars.html>`__\n        * To select datetimes, use ``np.datetime64``, ``'datetime'`` or\n          ``'datetime64'``\n        * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or\n          ``'timedelta64'``\n        * To select Pandas categorical dtypes, use ``'category'``\n        * To select Pandas datetimetz dtypes, use ``'datetimetz'`` (new in\n          0.20.0) or ``'datetime64[ns, tz]'``\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'a': [1, 2] * 3,\n        ...                    'b': [True, False] * 3,\n        ...                    'c': [1.0, 2.0] * 3})\n        >>> df\n                a      b  c\n        0       1   True  1.0\n        1       2  False  2.0\n        2       1   True  1.0\n        3       2  False  2.0\n        4       1   True  1.0\n        5       2  False  2.0\n\n        >>> df.select_dtypes(include='bool')\n           b\n        0  True\n        1  False\n        2  True\n        3  False\n        4  True\n        5  False\n\n        >>> df.select_dtypes(include=['float64'])\n           c\n        0  1.0\n        1  2.0\n        2  1.0\n        3  2.0\n        4  1.0\n        5  2.0\n\n        >>> df.select_dtypes(exclude=['int'])\n               b    c\n        0   True  1.0\n        1  False  2.0\n        2   True  1.0\n        3  False  2.0\n        4   True  1.0\n        5  False  2.0\n        \"\"\"\n        def _get_info_slice(obj, indexer):\n            \"\"\"Slice the info axis of `obj` with `indexer`.\"\"\"\n            if not hasattr(obj, '_info_axis_number'):\n                msg = 'object of type {typ!r} has no info axis'\n                raise TypeError(msg.format(typ=type(obj).__name__))\n            slices = [slice(None)] * obj.ndim\n            slices[obj._info_axis_number] = indexer\n            return tuple(slices)\n\n        if not is_list_like(include):\n            include = (include,) if include is not None else ()\n        if not is_list_like(exclude):\n            exclude = (exclude,) if exclude is not None else ()\n\n        selection = tuple(map(frozenset, (include, exclude)))\n\n        if not any(selection):\n            raise ValueError('at least one of include or exclude must be '\n                             'nonempty')\n\n        # convert the myriad valid dtypes object to a single representation\n        include, exclude = map(\n            lambda x: frozenset(map(_get_dtype_from_object, x)), selection)\n        for dtypes in (include, exclude):\n            invalidate_string_dtypes(dtypes)\n\n        # can't both include AND exclude!\n        if not include.isdisjoint(exclude):\n            raise ValueError('include and exclude overlap on {inc_ex}'.format(\n                inc_ex=(include & exclude)))\n\n        # empty include/exclude -> defaults to True\n        # three cases (we've already raised if both are empty)\n        # case 1: empty include, nonempty exclude\n        # we have True, True, ... True for include, same for exclude\n        # in the loop below we get the excluded\n        # and when we call '&' below we get only the excluded\n        # case 2: nonempty include, empty exclude\n        # same as case 1, but with include\n        # case 3: both nonempty\n        # the \"union\" of the logic of case 1 and case 2:\n        # we get the included and excluded, and return their logical and\n        include_these = Series(not bool(include), index=self.columns)\n        exclude_these = Series(not bool(exclude), index=self.columns)\n\n        def is_dtype_instance_mapper(idx, dtype):\n            return idx, functools.partial(issubclass, dtype.type)\n\n        for idx, f in itertools.starmap(is_dtype_instance_mapper,\n                                        enumerate(self.dtypes)):\n            if include:  # checks for the case of empty include or exclude\n                include_these.iloc[idx] = any(map(f, include))\n            if exclude:\n                exclude_these.iloc[idx] = not any(map(f, exclude))\n\n        dtype_indexer = include_these & exclude_these\n        return self.loc[_get_info_slice(self, dtype_indexer)]\n\n    def _box_item_values(self, key, values):\n        items = self.columns[self.columns.get_loc(key)]\n        if values.ndim == 2:\n            return self._constructor(values.T, columns=items, index=self.index)\n        else:\n            return self._box_col_values(values, items)\n\n    def _box_col_values(self, values, items):\n        \"\"\" provide boxed values for a column \"\"\"\n        klass = self._constructor_sliced\n        return klass(values, index=self.index, name=items, fastpath=True)\n\n    def __setitem__(self, key, value):\n        key = com.apply_if_callable(key, self)\n\n        # see if we can slice the rows\n        indexer = convert_to_index_sliceable(self, key)\n        if indexer is not None:\n            return self._setitem_slice(indexer, value)\n\n        if isinstance(key, DataFrame) or getattr(key, 'ndim', None) == 2:\n            self._setitem_frame(key, value)\n        elif isinstance(key, (Series, np.ndarray, list, Index)):\n            self._setitem_array(key, value)\n        else:\n            # set column\n            self._set_item(key, value)\n\n    def _setitem_slice(self, key, value):\n        self._check_setitem_copy()\n        self.loc._setitem_with_indexer(key, value)\n\n    def _setitem_array(self, key, value):\n        # also raises Exception if object array with NA values\n        if com.is_bool_indexer(key):\n            if len(key) != len(self.index):\n                raise ValueError('Item wrong length %d instead of %d!' %\n                                 (len(key), len(self.index)))\n            key = check_bool_indexer(self.index, key)\n            indexer = key.nonzero()[0]\n            self._check_setitem_copy()\n            self.loc._setitem_with_indexer(indexer, value)\n        else:\n            if isinstance(value, DataFrame):\n                if len(value.columns) != len(key):\n                    raise ValueError('Columns must be same length as key')\n                for k1, k2 in zip(key, value.columns):\n                    self[k1] = value[k2]\n            else:\n                indexer = self.loc._convert_to_indexer(key, axis=1)\n                self._check_setitem_copy()\n                self.loc._setitem_with_indexer((slice(None), indexer), value)\n\n    def _setitem_frame(self, key, value):\n        # support boolean setting with DataFrame input, e.g.\n        # df[df > df2] = 0\n        if isinstance(key, np.ndarray):\n            if key.shape != self.shape:\n                raise ValueError(\n                    'Array conditional must be same shape as self'\n                )\n            key = self._constructor(key, **self._construct_axes_dict())\n\n        if key.values.size and not is_bool_dtype(key.values):\n            raise TypeError(\n                'Must pass DataFrame or 2-d ndarray with boolean values only'\n            )\n\n        self._check_inplace_setting(value)\n        self._check_setitem_copy()\n        self._where(-key, value, inplace=True)\n\n    def _ensure_valid_index(self, value):\n        \"\"\"\n        ensure that if we don't have an index, that we can create one from the\n        passed value\n        \"\"\"\n        # GH5632, make sure that we are a Series convertible\n        if not len(self.index) and is_list_like(value):\n            try:\n                value = Series(value)\n            except (ValueError, NotImplementedError, TypeError):\n                raise ValueError('Cannot set a frame with no defined index '\n                                 'and a value that cannot be converted to a '\n                                 'Series')\n\n            self._data = self._data.reindex_axis(value.index.copy(), axis=1,\n                                                 fill_value=np.nan)\n\n    def _set_item(self, key, value):\n        \"\"\"\n        Add series to DataFrame in specified column.\n\n        If series is a numpy-array (not a Series/TimeSeries), it must be the\n        same length as the DataFrames index or an error will be thrown.\n\n        Series/TimeSeries will be conformed to the DataFrames index to\n        ensure homogeneity.\n        \"\"\"\n\n        self._ensure_valid_index(value)\n        value = self._sanitize_column(key, value)\n        NDFrame._set_item(self, key, value)\n\n        # check if we are modifying a copy\n        # try to set first as we want an invalid\n        # value exception to occur first\n        if len(self):\n            self._check_setitem_copy()\n\n    def insert(self, loc, column, value, allow_duplicates=False):\n        \"\"\"\n        Insert column into DataFrame at specified location.\n\n        Raises a ValueError if `column` is already contained in the DataFrame,\n        unless `allow_duplicates` is set to True.\n\n        Parameters\n        ----------\n        loc : int\n            Insertion index. Must verify 0 <= loc <= len(columns)\n        column : string, number, or hashable object\n            label of the inserted column\n        value : int, Series, or array-like\n        allow_duplicates : bool, optional\n        \"\"\"\n        self._ensure_valid_index(value)\n        value = self._sanitize_column(column, value, broadcast=False)\n        self._data.insert(loc, column, value,\n                          allow_duplicates=allow_duplicates)\n\n    def assign(self, **kwargs):\n        r\"\"\"\n        Assign new columns to a DataFrame.\n\n        Returns a new object with all original columns in addition to new ones.\n        Existing columns that are re-assigned will be overwritten.\n\n        Parameters\n        ----------\n        **kwargs : dict of {str: callable or Series}\n            The column names are keywords. If the values are\n            callable, they are computed on the DataFrame and\n            assigned to the new columns. The callable must not\n            change input DataFrame (though pandas doesn't check it).\n            If the values are not callable, (e.g. a Series, scalar, or array),\n            they are simply assigned.\n\n        Returns\n        -------\n        DataFrame\n            A new DataFrame with the new columns in addition to\n            all the existing columns.\n\n        Notes\n        -----\n        Assigning multiple columns within the same ``assign`` is possible.\n        For Python 3.6 and above, later items in '\\*\\*kwargs' may refer to\n        newly created or modified columns in 'df'; items are computed and\n        assigned into 'df' in order.  For Python 3.5 and below, the order of\n        keyword arguments is not specified, you cannot refer to newly created\n        or modified columns. All items are computed first, and then assigned\n        in alphabetical order.\n\n        .. versionchanged :: 0.23.0\n\n           Keyword argument order is maintained for Python 3.6 and later.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'temp_c': [17.0, 25.0]},\n        ...                   index=['Portland', 'Berkeley'])\n        >>> df\n                  temp_c\n        Portland    17.0\n        Berkeley    25.0\n\n        Where the value is a callable, evaluated on `df`:\n\n        >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)\n                  temp_c  temp_f\n        Portland    17.0    62.6\n        Berkeley    25.0    77.0\n\n        Alternatively, the same behavior can be achieved by directly\n        referencing an existing Series or sequence:\n\n        >>> df.assign(temp_f=df['temp_c'] * 9 / 5 + 32)\n                  temp_c  temp_f\n        Portland    17.0    62.6\n        Berkeley    25.0    77.0\n\n        In Python 3.6+, you can create multiple columns within the same assign\n        where one of the columns depends on another one defined within the same\n        assign:\n\n        >>> df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32,\n        ...           temp_k=lambda x: (x['temp_f'] +  459.67) * 5 / 9)\n                  temp_c  temp_f  temp_k\n        Portland    17.0    62.6  290.15\n        Berkeley    25.0    77.0  298.15\n        \"\"\"\n        data = self.copy()\n\n        # >= 3.6 preserve order of kwargs\n        if PY36:\n            for k, v in kwargs.items():\n                data[k] = com.apply_if_callable(v, data)\n        else:\n            # <= 3.5: do all calculations first...\n            results = OrderedDict()\n            for k, v in kwargs.items():\n                results[k] = com.apply_if_callable(v, data)\n\n            # <= 3.5 and earlier\n            results = sorted(results.items())\n            # ... and then assign\n            for k, v in results:\n                data[k] = v\n        return data\n\n    def _sanitize_column(self, key, value, broadcast=True):\n        \"\"\"\n        Ensures new columns (which go into the BlockManager as new blocks) are\n        always copied and converted into an array.\n\n        Parameters\n        ----------\n        key : object\n        value : scalar, Series, or array-like\n        broadcast : bool, default True\n            If ``key`` matches multiple duplicate column names in the\n            DataFrame, this parameter indicates whether ``value`` should be\n            tiled so that the returned array contains a (duplicated) column for\n            each occurrence of the key. If False, ``value`` will not be tiled.\n\n        Returns\n        -------\n        sanitized_column : numpy-array\n        \"\"\"\n\n        def reindexer(value):\n            # reindex if necessary\n\n            if value.index.equals(self.index) or not len(self.index):\n                value = value._values.copy()\n            else:\n\n                # GH 4107\n                try:\n                    value = value.reindex(self.index)._values\n                except Exception as e:\n\n                    # duplicate axis\n                    if not value.index.is_unique:\n                        raise e\n\n                    # other\n                    raise TypeError('incompatible index of inserted column '\n                                    'with frame index')\n            return value\n\n        if isinstance(value, Series):\n            value = reindexer(value)\n\n        elif isinstance(value, DataFrame):\n            # align right-hand-side columns if self.columns\n            # is multi-index and self[key] is a sub-frame\n            if isinstance(self.columns, MultiIndex) and key in self.columns:\n                loc = self.columns.get_loc(key)\n                if isinstance(loc, (slice, Series, np.ndarray, Index)):\n                    cols = maybe_droplevels(self.columns[loc], key)\n                    if len(cols) and not cols.equals(value.columns):\n                        value = value.reindex(cols, axis=1)\n            # now align rows\n            value = reindexer(value).T\n\n        elif isinstance(value, ExtensionArray):\n            from pandas.core.series import _sanitize_index\n            # Explicitly copy here, instead of in _sanitize_index,\n            # as sanitize_index won't copy an EA, even with copy=True\n            value = value.copy()\n            value = _sanitize_index(value, self.index, copy=False)\n\n        elif isinstance(value, Index) or is_sequence(value):\n            from pandas.core.series import _sanitize_index\n\n            # turn me into an ndarray\n            value = _sanitize_index(value, self.index, copy=False)\n            if not isinstance(value, (np.ndarray, Index)):\n                if isinstance(value, list) and len(value) > 0:\n                    value = maybe_convert_platform(value)\n                else:\n                    value = com.asarray_tuplesafe(value)\n            elif value.ndim == 2:\n                value = value.copy().T\n            elif isinstance(value, Index):\n                value = value.copy(deep=True)\n            else:\n                value = value.copy()\n\n            # possibly infer to datetimelike\n            if is_object_dtype(value.dtype):\n                value = maybe_infer_to_datetimelike(value)\n\n        else:\n            # cast ignores pandas dtypes. so save the dtype first\n            infer_dtype, _ = infer_dtype_from_scalar(\n                value, pandas_dtype=True)\n\n            # upcast\n            value = cast_scalar_to_array(len(self.index), value)\n            value = maybe_cast_to_datetime(value, infer_dtype)\n\n        # return internal types directly\n        if is_extension_type(value) or is_extension_array_dtype(value):\n            return value\n\n        # broadcast across multiple columns if necessary\n        if broadcast and key in self.columns and value.ndim == 1:\n            if (not self.columns.is_unique or\n                    isinstance(self.columns, MultiIndex)):\n                existing_piece = self[key]\n                if isinstance(existing_piece, DataFrame):\n                    value = np.tile(value, (len(existing_piece.columns), 1))\n\n        return np.atleast_2d(np.asarray(value))\n\n    @property\n    def _series(self):\n        result = {}\n        for idx, item in enumerate(self.columns):\n            result[item] = Series(self._data.iget(idx), index=self.index,\n                                  name=item)\n        return result\n\n    def lookup(self, row_labels, col_labels):\n        \"\"\"Label-based \"fancy indexing\" function for DataFrame.\n        Given equal-length arrays of row and column labels, return an\n        array of the values corresponding to each (row, col) pair.\n\n        Parameters\n        ----------\n        row_labels : sequence\n            The row labels to use for lookup\n        col_labels : sequence\n            The column labels to use for lookup\n\n        Notes\n        -----\n        Akin to::\n\n            result = []\n            for row, col in zip(row_labels, col_labels):\n                result.append(df.get_value(row, col))\n\n        Examples\n        --------\n        values : ndarray\n            The found values\n        \"\"\"\n        n = len(row_labels)\n        if n != len(col_labels):\n            raise ValueError('Row labels must have same size as column labels')\n\n        thresh = 1000\n        if not self._is_mixed_type or n > thresh:\n            values = self.values\n            ridx = self.index.get_indexer(row_labels)\n            cidx = self.columns.get_indexer(col_labels)\n            if (ridx == -1).any():\n                raise KeyError('One or more row labels was not found')\n            if (cidx == -1).any():\n                raise KeyError('One or more column labels was not found')\n            flat_index = ridx * len(self.columns) + cidx\n            result = values.flat[flat_index]\n        else:\n            result = np.empty(n, dtype='O')\n            for i, (r, c) in enumerate(zip(row_labels, col_labels)):\n                result[i] = self._get_value(r, c)\n\n        if is_object_dtype(result):\n            result = lib.maybe_convert_objects(result)\n\n        return result\n\n    # ----------------------------------------------------------------------\n    # Reindexing and alignment\n\n    def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,\n                      copy):\n        frame = self\n\n        columns = axes['columns']\n        if columns is not None:\n            frame = frame._reindex_columns(columns, method, copy, level,\n                                           fill_value, limit, tolerance)\n\n        index = axes['index']\n        if index is not None:\n            frame = frame._reindex_index(index, method, copy, level,\n                                         fill_value, limit, tolerance)\n\n        return frame\n\n    def _reindex_index(self, new_index, method, copy, level, fill_value=np.nan,\n                       limit=None, tolerance=None):\n        new_index, indexer = self.index.reindex(new_index, method=method,\n                                                level=level, limit=limit,\n                                                tolerance=tolerance)\n        return self._reindex_with_indexers({0: [new_index, indexer]},\n                                           copy=copy, fill_value=fill_value,\n                                           allow_dups=False)\n\n    def _reindex_columns(self, new_columns, method, copy, level,\n                         fill_value=None, limit=None, tolerance=None):\n        new_columns, indexer = self.columns.reindex(new_columns, method=method,\n                                                    level=level, limit=limit,\n                                                    tolerance=tolerance)\n        return self._reindex_with_indexers({1: [new_columns, indexer]},\n                                           copy=copy, fill_value=fill_value,\n                                           allow_dups=False)\n\n    def _reindex_multi(self, axes, copy, fill_value):\n        \"\"\" we are guaranteed non-Nones in the axes! \"\"\"\n\n        new_index, row_indexer = self.index.reindex(axes['index'])\n        new_columns, col_indexer = self.columns.reindex(axes['columns'])\n\n        if row_indexer is not None and col_indexer is not None:\n            indexer = row_indexer, col_indexer\n            new_values = algorithms.take_2d_multi(self.values, indexer,\n                                                  fill_value=fill_value)\n            return self._constructor(new_values, index=new_index,\n                                     columns=new_columns)\n        else:\n            return self._reindex_with_indexers({0: [new_index, row_indexer],\n                                                1: [new_columns, col_indexer]},\n                                               copy=copy,\n                                               fill_value=fill_value)\n\n    @Appender(_shared_docs['align'] % _shared_doc_kwargs)\n    def align(self, other, join='outer', axis=None, level=None, copy=True,\n              fill_value=None, method=None, limit=None, fill_axis=0,\n              broadcast_axis=None):\n        return super(DataFrame, self).align(other, join=join, axis=axis,\n                                            level=level, copy=copy,\n                                            fill_value=fill_value,\n                                            method=method, limit=limit,\n                                            fill_axis=fill_axis,\n                                            broadcast_axis=broadcast_axis)\n\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(NDFrame.reindex.__doc__)\n    @rewrite_axis_style_signature('labels', [('method', None),\n                                             ('copy', True),\n                                             ('level', None),\n                                             ('fill_value', np.nan),\n                                             ('limit', None),\n                                             ('tolerance', None)])\n    def reindex(self, *args, **kwargs):\n        axes = validate_axis_style_args(self, args, kwargs, 'labels',\n                                        'reindex')\n        kwargs.update(axes)\n        # Pop these, since the values are in `kwargs` under different names\n        kwargs.pop('axis', None)\n        kwargs.pop('labels', None)\n        return super(DataFrame, self).reindex(**kwargs)\n\n    @Appender(_shared_docs['reindex_axis'] % _shared_doc_kwargs)\n    def reindex_axis(self, labels, axis=0, method=None, level=None, copy=True,\n                     limit=None, fill_value=np.nan):\n        return super(DataFrame,\n                     self).reindex_axis(labels=labels, axis=axis,\n                                        method=method, level=level, copy=copy,\n                                        limit=limit, fill_value=fill_value)\n\n    def drop(self, labels=None, axis=0, index=None, columns=None,\n             level=None, inplace=False, errors='raise'):\n        \"\"\"\n        Drop specified labels from rows or columns.\n\n        Remove rows or columns by specifying label names and corresponding\n        axis, or by specifying directly index or column names. When using a\n        multi-index, labels on different levels can be removed by specifying\n        the level.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index or column labels to drop.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Whether to drop labels from the index (0 or 'index') or\n            columns (1 or 'columns').\n        index, columns : single label or list-like\n            Alternative to specifying axis (``labels, axis=1``\n            is equivalent to ``columns=labels``).\n\n            .. versionadded:: 0.21.0\n        level : int or level name, optional\n            For MultiIndex, level from which the labels will be removed.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and only existing labels are\n            dropped.\n\n        Returns\n        -------\n        dropped : pandas.DataFrame\n\n        See Also\n        --------\n        DataFrame.loc : Label-location based indexer for selection by label.\n        DataFrame.dropna : Return DataFrame with labels on given axis omitted\n            where (all or any) data are missing.\n        DataFrame.drop_duplicates : Return DataFrame with duplicate rows\n            removed, optionally only considering certain columns.\n        Series.drop : Return Series with specified index labels removed.\n\n        Raises\n        ------\n        KeyError\n            If none of the labels are found in the selected axis\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.arange(12).reshape(3,4),\n        ...                   columns=['A', 'B', 'C', 'D'])\n        >>> df\n           A  B   C   D\n        0  0  1   2   3\n        1  4  5   6   7\n        2  8  9  10  11\n\n        Drop columns\n\n        >>> df.drop(['B', 'C'], axis=1)\n           A   D\n        0  0   3\n        1  4   7\n        2  8  11\n\n        >>> df.drop(columns=['B', 'C'])\n           A   D\n        0  0   3\n        1  4   7\n        2  8  11\n\n        Drop a row by index\n\n        >>> df.drop([0, 1])\n           A  B   C   D\n        2  8  9  10  11\n\n        Drop columns and/or rows of MultiIndex DataFrame\n\n        >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n        ...                              ['speed', 'weight', 'length']],\n        ...                      labels=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                              [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n        ...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n        ...                         [250, 150], [1.5, 0.8], [320, 250],\n        ...                         [1, 0.8], [0.3,0.2]])\n        >>> df\n                        big     small\n        lama    speed   45.0    30.0\n                weight  200.0   100.0\n                length  1.5     1.0\n        cow     speed   30.0    20.0\n                weight  250.0   150.0\n                length  1.5     0.8\n        falcon  speed   320.0   250.0\n                weight  1.0     0.8\n                length  0.3     0.2\n\n        >>> df.drop(index='cow', columns='small')\n                        big\n        lama    speed   45.0\n                weight  200.0\n                length  1.5\n        falcon  speed   320.0\n                weight  1.0\n                length  0.3\n\n        >>> df.drop(index='length', level=1)\n                        big     small\n        lama    speed   45.0    30.0\n                weight  200.0   100.0\n        cow     speed   30.0    20.0\n                weight  250.0   150.0\n        falcon  speed   320.0   250.0\n                weight  1.0     0.8\n        \"\"\"\n        return super(DataFrame, self).drop(labels=labels, axis=axis,\n                                           index=index, columns=columns,\n                                           level=level, inplace=inplace,\n                                           errors=errors)\n\n    @rewrite_axis_style_signature('mapper', [('copy', True),\n                                             ('inplace', False),\n                                             ('level', None)])\n    def rename(self, *args, **kwargs):\n        \"\"\"Alter axes labels.\n\n        Function / dict values must be unique (1-to-1). Labels not contained in\n        a dict / Series will be left as-is. Extra labels listed don't throw an\n        error.\n\n        See the :ref:`user guide <basics.rename>` for more.\n\n        Parameters\n        ----------\n        mapper, index, columns : dict-like or function, optional\n            dict-like or functions transformations to apply to\n            that axis' values. Use either ``mapper`` and ``axis`` to\n            specify the axis to target with ``mapper``, or ``index`` and\n            ``columns``.\n        axis : int or str, optional\n            Axis to target with ``mapper``. Can be either the axis name\n            ('index', 'columns') or number (0, 1). The default is 'index'.\n        copy : boolean, default True\n            Also copy underlying data\n        inplace : boolean, default False\n            Whether to return a new DataFrame. If True then value of copy is\n            ignored.\n        level : int or level name, default None\n            In case of a MultiIndex, only rename labels in the specified\n            level.\n\n        Returns\n        -------\n        renamed : DataFrame\n\n        See Also\n        --------\n        pandas.DataFrame.rename_axis\n\n        Examples\n        --------\n\n        ``DataFrame.rename`` supports two calling conventions\n\n        * ``(index=index_mapper, columns=columns_mapper, ...)``\n        * ``(mapper, axis={'index', 'columns'}, ...)``\n\n        We *highly* recommend using keyword arguments to clarify your\n        intent.\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n        >>> df.rename(index=str, columns={\"A\": \"a\", \"B\": \"c\"})\n           a  c\n        0  1  4\n        1  2  5\n        2  3  6\n\n        >>> df.rename(index=str, columns={\"A\": \"a\", \"C\": \"c\"})\n           a  B\n        0  1  4\n        1  2  5\n        2  3  6\n\n        Using axis-style parameters\n\n        >>> df.rename(str.lower, axis='columns')\n           a  b\n        0  1  4\n        1  2  5\n        2  3  6\n\n        >>> df.rename({1: 2, 2: 4}, axis='index')\n           A  B\n        0  1  4\n        2  2  5\n        4  3  6\n        \"\"\"\n        axes = validate_axis_style_args(self, args, kwargs, 'mapper', 'rename')\n        kwargs.update(axes)\n        # Pop these, since the values are in `kwargs` under different names\n        kwargs.pop('axis', None)\n        kwargs.pop('mapper', None)\n        return super(DataFrame, self).rename(**kwargs)\n\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(NDFrame.fillna.__doc__)\n    def fillna(self, value=None, method=None, axis=None, inplace=False,\n               limit=None, downcast=None, **kwargs):\n        return super(DataFrame,\n                     self).fillna(value=value, method=method, axis=axis,\n                                  inplace=inplace, limit=limit,\n                                  downcast=downcast, **kwargs)\n\n    @Appender(_shared_docs['replace'] % _shared_doc_kwargs)\n    def replace(self, to_replace=None, value=None, inplace=False, limit=None,\n                regex=False, method='pad'):\n        return super(DataFrame, self).replace(to_replace=to_replace,\n                                              value=value, inplace=inplace,\n                                              limit=limit, regex=regex,\n                                              method=method)\n\n    @Appender(_shared_docs['shift'] % _shared_doc_kwargs)\n    def shift(self, periods=1, freq=None, axis=0):\n        return super(DataFrame, self).shift(periods=periods, freq=freq,\n                                            axis=axis)\n\n    def set_index(self, keys, drop=True, append=False, inplace=False,\n                  verify_integrity=False):\n        \"\"\"\n        Set the DataFrame index (row labels) using one or more existing\n        columns. By default yields a new object.\n\n        Parameters\n        ----------\n        keys : column label or list of column labels / arrays\n        drop : boolean, default True\n            Delete columns to be used as the new index\n        append : boolean, default False\n            Whether to append columns to existing index\n        inplace : boolean, default False\n            Modify the DataFrame in place (do not create a new object)\n        verify_integrity : boolean, default False\n            Check the new index for duplicates. Otherwise defer the check until\n            necessary. Setting to False will improve the performance of this\n            method\n\n        Returns\n        -------\n        DataFrame\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'month': [1, 4, 7, 10],\n        ...                    'year': [2012, 2014, 2013, 2014],\n        ...                    'sale':[55, 40, 84, 31]})\n           month  sale  year\n        0  1      55    2012\n        1  4      40    2014\n        2  7      84    2013\n        3  10     31    2014\n\n        Set the index to become the 'month' column:\n\n        >>> df.set_index('month')\n               sale  year\n        month\n        1      55    2012\n        4      40    2014\n        7      84    2013\n        10     31    2014\n\n        Create a multi-index using columns 'year' and 'month':\n\n        >>> df.set_index(['year', 'month'])\n                    sale\n        year  month\n        2012  1     55\n        2014  4     40\n        2013  7     84\n        2014  10    31\n\n        Create a multi-index using a set of values and a column:\n\n        >>> df.set_index([[1, 2, 3, 4], 'year'])\n                 month  sale\n           year\n        1  2012  1      55\n        2  2014  4      40\n        3  2013  7      84\n        4  2014  10     31\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        if not isinstance(keys, list):\n            keys = [keys]\n\n        missing = []\n        for col in keys:\n            if (is_scalar(col) or isinstance(col, tuple)) and col in self:\n                # tuples can be both column keys or list-likes\n                # if they are valid column keys, everything is fine\n                continue\n            elif is_scalar(col) and col not in self:\n                # tuples that are not column keys are considered list-like,\n                # not considered missing\n                missing.append(col)\n            elif (not is_list_like(col, allow_sets=False)\n                  or getattr(col, 'ndim', 1) > 1):\n                raise TypeError('The parameter \"keys\" may only contain a '\n                                'combination of valid column keys and '\n                                'one-dimensional list-likes')\n\n        if missing:\n            raise KeyError('{}'.format(missing))\n\n        if inplace:\n            frame = self\n        else:\n            frame = self.copy()\n\n        arrays = []\n        names = []\n        if append:\n            names = [x for x in self.index.names]\n            if isinstance(self.index, ABCMultiIndex):\n                for i in range(self.index.nlevels):\n                    arrays.append(self.index._get_level_values(i))\n            else:\n                arrays.append(self.index)\n\n        to_remove = []\n        for col in keys:\n            if isinstance(col, ABCMultiIndex):\n                for n in range(col.nlevels):\n                    arrays.append(col._get_level_values(n))\n                names.extend(col.names)\n            elif isinstance(col, (ABCIndexClass, ABCSeries)):\n                # if Index then not MultiIndex (treated above)\n                arrays.append(col)\n                names.append(col.name)\n            elif isinstance(col, (list, np.ndarray)):\n                arrays.append(col)\n                names.append(None)\n            elif (is_list_like(col)\n                  and not (isinstance(col, tuple) and col in self)):\n                # all other list-likes (but avoid valid column keys)\n                col = list(col)  # ensure iterator do not get read twice etc.\n                arrays.append(col)\n                names.append(None)\n            # from here, col can only be a column label\n            else:\n                arrays.append(frame[col]._values)\n                names.append(col)\n                if drop:\n                    to_remove.append(col)\n\n        index = ensure_index_from_sequences(arrays, names)\n\n        if verify_integrity and not index.is_unique:\n            duplicates = index[index.duplicated()].unique()\n            raise ValueError('Index has duplicate keys: {dup}'.format(\n                dup=duplicates))\n\n        # use set to handle duplicate column names gracefully in case of drop\n        for c in set(to_remove):\n            del frame[c]\n\n        # clear up memory usage\n        index._cleanup()\n\n        frame.index = index\n\n        if not inplace:\n            return frame\n\n    def reset_index(self, level=None, drop=False, inplace=False, col_level=0,\n                    col_fill=''):\n        \"\"\"\n        For DataFrame with multi-level index, return new DataFrame with\n        labeling information in the columns under the index names, defaulting\n        to 'level_0', 'level_1', etc. if any are None. For a standard index,\n        the index name will be used (if set), otherwise a default 'index' or\n        'level_0' (if 'index' is already taken) will be used.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default None\n            Only remove the given levels from the index. Removes all levels by\n            default\n        drop : boolean, default False\n            Do not try to insert index into dataframe columns. This resets\n            the index to the default integer index.\n        inplace : boolean, default False\n            Modify the DataFrame in place (do not create a new object)\n        col_level : int or str, default 0\n            If the columns have multiple levels, determines which level the\n            labels are inserted into. By default it is inserted into the first\n            level.\n        col_fill : object, default ''\n            If the columns have multiple levels, determines how the other\n            levels are named. If None then the index name is repeated.\n\n        Returns\n        -------\n        resetted : DataFrame\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('bird',    389.0),\n        ...                    ('bird',     24.0),\n        ...                    ('mammal',   80.5),\n        ...                    ('mammal', np.nan)],\n        ...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n        ...                   columns=('class', 'max_speed'))\n        >>> df\n                 class  max_speed\n        falcon    bird      389.0\n        parrot    bird       24.0\n        lion    mammal       80.5\n        monkey  mammal        NaN\n\n        When we reset the index, the old index is added as a column, and a\n        new sequential index is used:\n\n        >>> df.reset_index()\n            index   class  max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        We can use the `drop` parameter to avoid the old index being added as\n        a column:\n\n        >>> df.reset_index(drop=True)\n            class  max_speed\n        0    bird      389.0\n        1    bird       24.0\n        2  mammal       80.5\n        3  mammal        NaN\n\n        You can also use `reset_index` with `MultiIndex`.\n\n        >>> index = pd.MultiIndex.from_tuples([('bird', 'falcon'),\n        ...                                    ('bird', 'parrot'),\n        ...                                    ('mammal', 'lion'),\n        ...                                    ('mammal', 'monkey')],\n        ...                                   names=['class', 'name'])\n        >>> columns = pd.MultiIndex.from_tuples([('speed', 'max'),\n        ...                                      ('species', 'type')])\n        >>> df = pd.DataFrame([(389.0, 'fly'),\n        ...                    ( 24.0, 'fly'),\n        ...                    ( 80.5, 'run'),\n        ...                    (np.nan, 'jump')],\n        ...                   index=index,\n        ...                   columns=columns)\n        >>> df\n                       speed species\n                         max    type\n        class  name\n        bird   falcon  389.0     fly\n               parrot   24.0     fly\n        mammal lion     80.5     run\n               monkey    NaN    jump\n\n        If the index has multiple levels, we can reset a subset of them:\n\n        >>> df.reset_index(level='class')\n                 class  speed species\n                          max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        If we are not dropping the index, by default, it is placed in the top\n        level. We can place it in another level:\n\n        >>> df.reset_index(level='class', col_level=1)\n                        speed species\n                 class    max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        When the index is inserted under another level, we can specify under\n        which one with the parameter `col_fill`:\n\n        >>> df.reset_index(level='class', col_level=1, col_fill='species')\n                      species  speed species\n                        class    max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n\n        If we specify a nonexistent level for `col_fill`, it is created:\n\n        >>> df.reset_index(level='class', col_level=1, col_fill='genus')\n                        genus  speed species\n                        class    max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        if inplace:\n            new_obj = self\n        else:\n            new_obj = self.copy()\n\n        def _maybe_casted_values(index, labels=None):\n            values = index._values\n            if not isinstance(index, (PeriodIndex, DatetimeIndex)):\n                if values.dtype == np.object_:\n                    values = lib.maybe_convert_objects(values)\n\n            # if we have the labels, extract the values with a mask\n            if labels is not None:\n                mask = labels == -1\n\n                # we can have situations where the whole mask is -1,\n                # meaning there is nothing found in labels, so make all nan's\n                if mask.all():\n                    values = np.empty(len(mask))\n                    values.fill(np.nan)\n                else:\n                    values = values.take(labels)\n                    if mask.any():\n                        values, changed = maybe_upcast_putmask(\n                            values, mask, np.nan)\n            return values\n\n        new_index = ibase.default_index(len(new_obj))\n        if level is not None:\n            if not isinstance(level, (tuple, list)):\n                level = [level]\n            level = [self.index._get_level_number(lev) for lev in level]\n            if len(level) < self.index.nlevels:\n                new_index = self.index.droplevel(level)\n\n        if not drop:\n            if isinstance(self.index, MultiIndex):\n                names = [n if n is not None else ('level_%d' % i)\n                         for (i, n) in enumerate(self.index.names)]\n                to_insert = lzip(self.index.levels, self.index.labels)\n            else:\n                default = 'index' if 'index' not in self else 'level_0'\n                names = ([default] if self.index.name is None\n                         else [self.index.name])\n                to_insert = ((self.index, None),)\n\n            multi_col = isinstance(self.columns, MultiIndex)\n            for i, (lev, lab) in reversed(list(enumerate(to_insert))):\n                if not (level is None or i in level):\n                    continue\n                name = names[i]\n                if multi_col:\n                    col_name = (list(name) if isinstance(name, tuple)\n                                else [name])\n                    if col_fill is None:\n                        if len(col_name) not in (1, self.columns.nlevels):\n                            raise ValueError(\"col_fill=None is incompatible \"\n                                             \"with incomplete column name \"\n                                             \"{}\".format(name))\n                        col_fill = col_name[0]\n\n                    lev_num = self.columns._get_level_number(col_level)\n                    name_lst = [col_fill] * lev_num + col_name\n                    missing = self.columns.nlevels - len(name_lst)\n                    name_lst += [col_fill] * missing\n                    name = tuple(name_lst)\n                # to ndarray and maybe infer different dtype\n                level_values = _maybe_casted_values(lev, lab)\n                new_obj.insert(0, name, level_values)\n\n        new_obj.index = new_index\n        if not inplace:\n            return new_obj\n\n    # ----------------------------------------------------------------------\n    # Reindex-based selection methods\n\n    @Appender(_shared_docs['isna'] % _shared_doc_kwargs)\n    def isna(self):\n        return super(DataFrame, self).isna()\n\n    @Appender(_shared_docs['isna'] % _shared_doc_kwargs)\n    def isnull(self):\n        return super(DataFrame, self).isnull()\n\n    @Appender(_shared_docs['notna'] % _shared_doc_kwargs)\n    def notna(self):\n        return super(DataFrame, self).notna()\n\n    @Appender(_shared_docs['notna'] % _shared_doc_kwargs)\n    def notnull(self):\n        return super(DataFrame, self).notnull()\n\n    def dropna(self, axis=0, how='any', thresh=None, subset=None,\n               inplace=False):\n        \"\"\"\n        Remove missing values.\n\n        See the :ref:`User Guide <missing_data>` for more on which values are\n        considered missing, and how to work with missing data.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Determine if rows or columns which contain missing values are\n            removed.\n\n            * 0, or 'index' : Drop rows which contain missing values.\n            * 1, or 'columns' : Drop columns which contain missing value.\n\n            .. deprecated:: 0.23.0\n\n               Pass tuple or list to drop on multiple axes.\n               Only a single axis is allowed.\n\n        how : {'any', 'all'}, default 'any'\n            Determine if row or column is removed from DataFrame, when we have\n            at least one NA or all NA.\n\n            * 'any' : If any NA values are present, drop that row or column.\n            * 'all' : If all values are NA, drop that row or column.\n\n        thresh : int, optional\n            Require that many non-NA values.\n        subset : array-like, optional\n            Labels along other axis to consider, e.g. if you are dropping rows\n            these would be a list of columns to include.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n\n        Returns\n        -------\n        DataFrame\n            DataFrame with NA entries dropped from it.\n\n        See Also\n        --------\n        DataFrame.isna: Indicate missing values.\n        DataFrame.notna : Indicate existing (non-missing) values.\n        DataFrame.fillna : Replace missing values.\n        Series.dropna : Drop missing values.\n        Index.dropna : Drop missing indices.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'],\n        ...                    \"toy\": [np.nan, 'Batmobile', 'Bullwhip'],\n        ...                    \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"),\n        ...                             pd.NaT]})\n        >>> df\n               name        toy       born\n        0    Alfred        NaN        NaT\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Drop the rows where at least one element is missing.\n\n        >>> df.dropna()\n             name        toy       born\n        1  Batman  Batmobile 1940-04-25\n\n        Drop the columns where at least one element is missing.\n\n        >>> df.dropna(axis='columns')\n               name\n        0    Alfred\n        1    Batman\n        2  Catwoman\n\n        Drop the rows where all elements are missing.\n\n        >>> df.dropna(how='all')\n               name        toy       born\n        0    Alfred        NaN        NaT\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Keep only the rows with at least 2 non-NA values.\n\n        >>> df.dropna(thresh=2)\n               name        toy       born\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Define in which columns to look for missing values.\n\n        >>> df.dropna(subset=['name', 'born'])\n               name        toy       born\n        1    Batman  Batmobile 1940-04-25\n\n        Keep the DataFrame with valid entries in the same variable.\n\n        >>> df.dropna(inplace=True)\n        >>> df\n             name        toy       born\n        1  Batman  Batmobile 1940-04-25\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        if isinstance(axis, (tuple, list)):\n            # GH20987\n            msg = (\"supplying multiple axes to axis is deprecated and \"\n                   \"will be removed in a future version.\")\n            warnings.warn(msg, FutureWarning, stacklevel=2)\n\n            result = self\n            for ax in axis:\n                result = result.dropna(how=how, thresh=thresh, subset=subset,\n                                       axis=ax)\n        else:\n            axis = self._get_axis_number(axis)\n            agg_axis = 1 - axis\n\n            agg_obj = self\n            if subset is not None:\n                ax = self._get_axis(agg_axis)\n                indices = ax.get_indexer_for(subset)\n                check = indices == -1\n                if check.any():\n                    raise KeyError(list(np.compress(check, subset)))\n                agg_obj = self.take(indices, axis=agg_axis)\n\n            count = agg_obj.count(axis=agg_axis)\n\n            if thresh is not None:\n                mask = count >= thresh\n            elif how == 'any':\n                mask = count == len(agg_obj._get_axis(agg_axis))\n            elif how == 'all':\n                mask = count > 0\n            else:\n                if how is not None:\n                    raise ValueError('invalid how option: {h}'.format(h=how))\n                else:\n                    raise TypeError('must specify how or thresh')\n\n            result = self._take(mask.nonzero()[0], axis=axis)\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result\n\n    def drop_duplicates(self, subset=None, keep='first', inplace=False):\n        \"\"\"\n        Return DataFrame with duplicate rows removed, optionally only\n        considering certain columns\n\n        Parameters\n        ----------\n        subset : column label or sequence of labels, optional\n            Only consider certain columns for identifying duplicates, by\n            default use all of the columns\n        keep : {'first', 'last', False}, default 'first'\n            - ``first`` : Drop duplicates except for the first occurrence.\n            - ``last`` : Drop duplicates except for the last occurrence.\n            - False : Drop all duplicates.\n        inplace : boolean, default False\n            Whether to drop duplicates in place or to return a copy\n\n        Returns\n        -------\n        deduplicated : DataFrame\n        \"\"\"\n        if self.empty:\n            return self.copy()\n\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        duplicated = self.duplicated(subset, keep=keep)\n\n        if inplace:\n            inds, = (-duplicated).nonzero()\n            new_data = self._data.take(inds)\n            self._update_inplace(new_data)\n        else:\n            return self[-duplicated]\n\n    def duplicated(self, subset=None, keep='first'):\n        \"\"\"\n        Return boolean Series denoting duplicate rows, optionally only\n        considering certain columns\n\n        Parameters\n        ----------\n        subset : column label or sequence of labels, optional\n            Only consider certain columns for identifying duplicates, by\n            default use all of the columns\n        keep : {'first', 'last', False}, default 'first'\n            - ``first`` : Mark duplicates as ``True`` except for the\n              first occurrence.\n            - ``last`` : Mark duplicates as ``True`` except for the\n              last occurrence.\n            - False : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        duplicated : Series\n        \"\"\"\n        from pandas.core.sorting import get_group_index\n        from pandas._libs.hashtable import duplicated_int64, _SIZE_HINT_LIMIT\n\n        if self.empty:\n            return Series()\n\n        def f(vals):\n            labels, shape = algorithms.factorize(\n                vals, size_hint=min(len(self), _SIZE_HINT_LIMIT))\n            return labels.astype('i8', copy=False), len(shape)\n\n        if subset is None:\n            subset = self.columns\n        elif (not np.iterable(subset) or\n              isinstance(subset, compat.string_types) or\n              isinstance(subset, tuple) and subset in self.columns):\n            subset = subset,\n\n        # Verify all columns in subset exist in the queried dataframe\n        # Otherwise, raise a KeyError, same as if you try to __getitem__ with a\n        # key that doesn't exist.\n        diff = Index(subset).difference(self.columns)\n        if not diff.empty:\n            raise KeyError(diff)\n\n        vals = (col.values for name, col in self.iteritems()\n                if name in subset)\n        labels, shape = map(list, zip(*map(f, vals)))\n\n        ids = get_group_index(labels, shape, sort=False, xnull=False)\n        return Series(duplicated_int64(ids, keep), index=self.index)\n\n    # ----------------------------------------------------------------------\n    # Sorting\n\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(NDFrame.sort_values.__doc__)\n    def sort_values(self, by, axis=0, ascending=True, inplace=False,\n                    kind='quicksort', na_position='last'):\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        axis = self._get_axis_number(axis)\n\n        if not isinstance(by, list):\n            by = [by]\n        if is_sequence(ascending) and len(by) != len(ascending):\n            raise ValueError('Length of ascending (%d) != length of by (%d)' %\n                             (len(ascending), len(by)))\n        if len(by) > 1:\n            from pandas.core.sorting import lexsort_indexer\n\n            keys = []\n            for x in by:\n                k = self._get_label_or_level_values(x, axis=axis)\n                keys.append(k)\n            indexer = lexsort_indexer(keys, orders=ascending,\n                                      na_position=na_position)\n            indexer = ensure_platform_int(indexer)\n        else:\n            from pandas.core.sorting import nargsort\n\n            by = by[0]\n            k = self._get_label_or_level_values(by, axis=axis)\n\n            if isinstance(ascending, (tuple, list)):\n                ascending = ascending[0]\n\n            indexer = nargsort(k, kind=kind, ascending=ascending,\n                               na_position=na_position)\n\n        new_data = self._data.take(indexer,\n                                   axis=self._get_block_manager_axis(axis),\n                                   verify=False)\n\n        if inplace:\n            return self._update_inplace(new_data)\n        else:\n            return self._constructor(new_data).__finalize__(self)\n\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(NDFrame.sort_index.__doc__)\n    def sort_index(self, axis=0, level=None, ascending=True, inplace=False,\n                   kind='quicksort', na_position='last', sort_remaining=True,\n                   by=None):\n\n        # TODO: this can be combined with Series.sort_index impl as\n        # almost identical\n\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        # 10726\n        if by is not None:\n            warnings.warn(\"by argument to sort_index is deprecated, \"\n                          \"please use .sort_values(by=...)\",\n                          FutureWarning, stacklevel=2)\n            if level is not None:\n                raise ValueError(\"unable to simultaneously sort by and level\")\n            return self.sort_values(by, axis=axis, ascending=ascending,\n                                    inplace=inplace)\n\n        axis = self._get_axis_number(axis)\n        labels = self._get_axis(axis)\n\n        # make sure that the axis is lexsorted to start\n        # if not we need to reconstruct to get the correct indexer\n        labels = labels._sort_levels_monotonic()\n        if level is not None:\n\n            new_axis, indexer = labels.sortlevel(level, ascending=ascending,\n                                                 sort_remaining=sort_remaining)\n\n        elif isinstance(labels, MultiIndex):\n            from pandas.core.sorting import lexsort_indexer\n\n            indexer = lexsort_indexer(labels._get_labels_for_sorting(),\n                                      orders=ascending,\n                                      na_position=na_position)\n        else:\n            from pandas.core.sorting import nargsort\n\n            # Check monotonic-ness before sort an index\n            # GH11080\n            if ((ascending and labels.is_monotonic_increasing) or\n                    (not ascending and labels.is_monotonic_decreasing)):\n                if inplace:\n                    return\n                else:\n                    return self.copy()\n\n            indexer = nargsort(labels, kind=kind, ascending=ascending,\n                               na_position=na_position)\n\n        baxis = self._get_block_manager_axis(axis)\n        new_data = self._data.take(indexer,\n                                   axis=baxis,\n                                   verify=False)\n\n        # reconstruct axis if needed\n        new_data.axes[baxis] = new_data.axes[baxis]._sort_levels_monotonic()\n\n        if inplace:\n            return self._update_inplace(new_data)\n        else:\n            return self._constructor(new_data).__finalize__(self)\n\n    def nlargest(self, n, columns, keep='first'):\n        \"\"\"\n        Return the first `n` rows ordered by `columns` in descending order.\n\n        Return the first `n` rows with the largest values in `columns`, in\n        descending order. The columns that are not specified are returned as\n        well, but not used for ordering.\n\n        This method is equivalent to\n        ``df.sort_values(columns, ascending=False).head(n)``, but more\n        performant.\n\n        Parameters\n        ----------\n        n : int\n            Number of rows to return.\n        columns : label or list of labels\n            Column label(s) to order by.\n        keep : {'first', 'last', 'all'}, default 'first'\n            Where there are duplicate values:\n\n            - `first` : prioritize the first occurrence(s)\n            - `last` : prioritize the last occurrence(s)\n            - ``all`` : do not drop any duplicates, even it means\n                        selecting more than `n` items.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        DataFrame\n            The first `n` rows ordered by the given columns in descending\n            order.\n\n        See Also\n        --------\n        DataFrame.nsmallest : Return the first `n` rows ordered by `columns` in\n            ascending order.\n        DataFrame.sort_values : Sort DataFrame by the values.\n        DataFrame.head : Return the first `n` rows without re-ordering.\n\n        Notes\n        -----\n        This function cannot be used with all column types. For example, when\n        specifying columns with `object` or `category` dtypes, ``TypeError`` is\n        raised.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n        ...                                   434000, 434000, 337000, 11300,\n        ...                                   11300, 11300],\n        ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n        ...                            17036, 182, 38, 311],\n        ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n        ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n        ...                   index=[\"Italy\", \"France\", \"Malta\",\n        ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n        ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n        >>> df\n                  population      GDP alpha-2\n        Italy       59000000  1937894      IT\n        France      65000000  2583560      FR\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n        Iceland       337000    17036      IS\n        Nauru          11300      182      NR\n        Tuvalu         11300       38      TV\n        Anguilla       11300      311      AI\n\n        In the following example, we will use ``nlargest`` to select the three\n        rows having the largest values in column \"population\".\n\n        >>> df.nlargest(3, 'population')\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Malta       434000    12011      MT\n\n        When using ``keep='last'``, ties are resolved in reverse order:\n\n        >>> df.nlargest(3, 'population', keep='last')\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Brunei      434000    12128      BN\n\n        When using ``keep='all'``, all duplicate items are maintained:\n\n        >>> df.nlargest(3, 'population', keep='all')\n                  population      GDP alpha-2\n        France      65000000  2583560      FR\n        Italy       59000000  1937894      IT\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n\n        To order by the largest values in column \"population\" and then \"GDP\",\n        we can specify multiple columns like in the next example.\n\n        >>> df.nlargest(3, ['population', 'GDP'])\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Brunei      434000    12128      BN\n        \"\"\"\n        return algorithms.SelectNFrame(self,\n                                       n=n,\n                                       keep=keep,\n                                       columns=columns).nlargest()\n\n    def nsmallest(self, n, columns, keep='first'):\n        \"\"\"\n        Return the first `n` rows ordered by `columns` in ascending order.\n\n        Return the first `n` rows with the smallest values in `columns`, in\n        ascending order. The columns that are not specified are returned as\n        well, but not used for ordering.\n\n        This method is equivalent to\n        ``df.sort_values(columns, ascending=True).head(n)``, but more\n        performant.\n\n        Parameters\n        ----------\n        n : int\n            Number of items to retrieve.\n        columns : list or str\n            Column name or names to order by.\n        keep : {'first', 'last', 'all'}, default 'first'\n            Where there are duplicate values:\n\n            - ``first`` : take the first occurrence.\n            - ``last`` : take the last occurrence.\n            - ``all`` : do not drop any duplicates, even it means\n              selecting more than `n` items.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.nlargest : Return the first `n` rows ordered by `columns` in\n            descending order.\n        DataFrame.sort_values : Sort DataFrame by the values.\n        DataFrame.head : Return the first `n` rows without re-ordering.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n        ...                                   434000, 434000, 337000, 11300,\n        ...                                   11300, 11300],\n        ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n        ...                            17036, 182, 38, 311],\n        ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n        ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n        ...                   index=[\"Italy\", \"France\", \"Malta\",\n        ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n        ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n        >>> df\n                  population      GDP alpha-2\n        Italy       59000000  1937894      IT\n        France      65000000  2583560      FR\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n        Iceland       337000    17036      IS\n        Nauru          11300      182      NR\n        Tuvalu         11300       38      TV\n        Anguilla       11300      311      AI\n\n        In the following example, we will use ``nsmallest`` to select the\n        three rows having the smallest values in column \"a\".\n\n        >>> df.nsmallest(3, 'population')\n                  population  GDP alpha-2\n        Nauru          11300  182      NR\n        Tuvalu         11300   38      TV\n        Anguilla       11300  311      AI\n\n        When using ``keep='last'``, ties are resolved in reverse order:\n\n        >>> df.nsmallest(3, 'population', keep='last')\n                  population  GDP alpha-2\n        Anguilla       11300  311      AI\n        Tuvalu         11300   38      TV\n        Nauru          11300  182      NR\n\n        When using ``keep='all'``, all duplicate items are maintained:\n\n        >>> df.nsmallest(3, 'population', keep='all')\n                  population  GDP alpha-2\n        Nauru          11300  182      NR\n        Tuvalu         11300   38      TV\n        Anguilla       11300  311      AI\n\n        To order by the largest values in column \"a\" and then \"c\", we can\n        specify multiple columns like in the next example.\n\n        >>> df.nsmallest(3, ['population', 'GDP'])\n                  population  GDP alpha-2\n        Tuvalu         11300   38      TV\n        Nauru          11300  182      NR\n        Anguilla       11300  311      AI\n        \"\"\"\n        return algorithms.SelectNFrame(self,\n                                       n=n,\n                                       keep=keep,\n                                       columns=columns).nsmallest()\n\n    def swaplevel(self, i=-2, j=-1, axis=0):\n        \"\"\"\n        Swap levels i and j in a MultiIndex on a particular axis\n\n        Parameters\n        ----------\n        i, j : int, string (can be mixed)\n            Level of index to be swapped. Can pass level name as string.\n\n        Returns\n        -------\n        swapped : same type as caller (new object)\n\n        .. versionchanged:: 0.18.1\n\n           The indexes ``i`` and ``j`` are now optional, and default to\n           the two innermost levels of the index.\n        \"\"\"\n        result = self.copy()\n\n        axis = self._get_axis_number(axis)\n        if axis == 0:\n            result.index = result.index.swaplevel(i, j)\n        else:\n            result.columns = result.columns.swaplevel(i, j)\n        return result\n\n    def reorder_levels(self, order, axis=0):\n        \"\"\"\n        Rearrange index levels using input order.\n        May not drop or duplicate levels\n\n        Parameters\n        ----------\n        order : list of int or list of str\n            List representing new level order. Reference level by number\n            (position) or by key (label).\n        axis : int\n            Where to reorder levels.\n\n        Returns\n        -------\n        type of caller (new object)\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        if not isinstance(self._get_axis(axis),\n                          MultiIndex):  # pragma: no cover\n            raise TypeError('Can only reorder levels on a hierarchical axis.')\n\n        result = self.copy()\n\n        if axis == 0:\n            result.index = result.index.reorder_levels(order)\n        else:\n            result.columns = result.columns.reorder_levels(order)\n        return result\n\n    # ----------------------------------------------------------------------\n    # Arithmetic / combination related\n\n    def _combine_frame(self, other, func, fill_value=None, level=None):\n        this, other = self.align(other, join='outer', level=level, copy=False)\n        new_index, new_columns = this.index, this.columns\n\n        def _arith_op(left, right):\n            # for the mixed_type case where we iterate over columns,\n            # _arith_op(left, right) is equivalent to\n            # left._binop(right, func, fill_value=fill_value)\n            left, right = ops.fill_binop(left, right, fill_value)\n            return func(left, right)\n\n        if ops.should_series_dispatch(this, other, func):\n            # iterate over columns\n            return ops.dispatch_to_series(this, other, _arith_op)\n        else:\n            result = _arith_op(this.values, other.values)\n            return self._constructor(result,\n                                     index=new_index, columns=new_columns,\n                                     copy=False)\n\n    def _combine_match_index(self, other, func, level=None):\n        left, right = self.align(other, join='outer', axis=0, level=level,\n                                 copy=False)\n        assert left.index.equals(right.index)\n\n        if left._is_mixed_type or right._is_mixed_type:\n            # operate column-wise; avoid costly object-casting in `.values`\n            return ops.dispatch_to_series(left, right, func)\n        else:\n            # fastpath --> operate directly on values\n            with np.errstate(all=\"ignore\"):\n                new_data = func(left.values.T, right.values).T\n            return self._constructor(new_data,\n                                     index=left.index, columns=self.columns,\n                                     copy=False)\n\n    def _combine_match_columns(self, other, func, level=None):\n        assert isinstance(other, Series)\n        left, right = self.align(other, join='outer', axis=1, level=level,\n                                 copy=False)\n        assert left.columns.equals(right.index)\n        return ops.dispatch_to_series(left, right, func, axis=\"columns\")\n\n    def _combine_const(self, other, func):\n        assert lib.is_scalar(other) or np.ndim(other) == 0\n        return ops.dispatch_to_series(self, other, func)\n\n    def combine(self, other, func, fill_value=None, overwrite=True):\n        \"\"\"\n        Perform column-wise combine with another DataFrame based on a\n        passed function.\n\n        Combines a DataFrame with `other` DataFrame using `func`\n        to element-wise combine columns. The row and column indexes of the\n        resulting DataFrame will be the union of the two.\n\n        Parameters\n        ----------\n        other : DataFrame\n            The DataFrame to merge column-wise.\n        func : function\n            Function that takes two series as inputs and return a Series or a\n            scalar. Used to merge the two dataframes column by columns.\n        fill_value : scalar value, default None\n            The value to fill NaNs with prior to passing any column to the\n            merge func.\n        overwrite : boolean, default True\n            If True, columns in `self` that do not exist in `other` will be\n            overwritten with NaNs.\n\n        Returns\n        -------\n        result : DataFrame\n\n        Examples\n        --------\n        Combine using a simple function that chooses the smaller column.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2\n        >>> df1.combine(df2, take_smaller)\n           A  B\n        0  0  3\n        1  0  3\n\n        Example using a true element-wise combine function.\n\n        >>> df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine(df2, np.minimum)\n           A  B\n        0  1  2\n        1  0  3\n\n        Using `fill_value` fills Nones prior to passing the column to the\n        merge function.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine(df2, take_smaller, fill_value=-5)\n           A    B\n        0  0 -5.0\n        1  0  4.0\n\n        However, if the same element in both dataframes is None, that None\n        is preserved\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]})\n        >>> df1.combine(df2, take_smaller, fill_value=-5)\n           A    B\n        0  0  NaN\n        1  0  3.0\n\n        Example that demonstrates the use of `overwrite` and behavior when\n        the axis differ between the dataframes.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1],}, index=[1, 2])\n        >>> df1.combine(df2, take_smaller)\n             A    B     C\n        0  NaN  NaN   NaN\n        1  NaN  3.0 -10.0\n        2  NaN  3.0   1.0\n\n        >>> df1.combine(df2, take_smaller, overwrite=False)\n             A    B     C\n        0  0.0  NaN   NaN\n        1  0.0  3.0 -10.0\n        2  NaN  3.0   1.0\n\n        Demonstrating the preference of the passed in dataframe.\n\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1],}, index=[1, 2])\n        >>> df2.combine(df1, take_smaller)\n           A    B   C\n        0  0.0  NaN NaN\n        1  0.0  3.0 NaN\n        2  NaN  3.0 NaN\n\n        >>> df2.combine(df1, take_smaller, overwrite=False)\n             A    B   C\n        0  0.0  NaN NaN\n        1  0.0  3.0 1.0\n        2  NaN  3.0 1.0\n\n        See Also\n        --------\n        DataFrame.combine_first : Combine two DataFrame objects and default to\n            non-null values in frame calling the method.\n        \"\"\"\n        other_idxlen = len(other.index)  # save for compare\n\n        this, other = self.align(other, copy=False)\n        new_index = this.index\n\n        if other.empty and len(new_index) == len(self.index):\n            return self.copy()\n\n        if self.empty and len(other) == other_idxlen:\n            return other.copy()\n\n        # sorts if possible\n        new_columns = this.columns.union(other.columns)\n        do_fill = fill_value is not None\n        result = {}\n        for col in new_columns:\n            series = this[col]\n            otherSeries = other[col]\n\n            this_dtype = series.dtype\n            other_dtype = otherSeries.dtype\n\n            this_mask = isna(series)\n            other_mask = isna(otherSeries)\n\n            # don't overwrite columns unecessarily\n            # DO propagate if this column is not in the intersection\n            if not overwrite and other_mask.all():\n                result[col] = this[col].copy()\n                continue\n\n            if do_fill:\n                series = series.copy()\n                otherSeries = otherSeries.copy()\n                series[this_mask] = fill_value\n                otherSeries[other_mask] = fill_value\n\n            if col not in self.columns:\n                # If self DataFrame does not have col in other DataFrame,\n                # try to promote series, which is all NaN, as other_dtype.\n                new_dtype = other_dtype\n                try:\n                    series = series.astype(new_dtype, copy=False)\n                except ValueError:\n                    # e.g. new_dtype is integer types\n                    pass\n            else:\n                # if we have different dtypes, possibly promote\n                new_dtype = find_common_type([this_dtype, other_dtype])\n                if not is_dtype_equal(this_dtype, new_dtype):\n                    series = series.astype(new_dtype)\n                if not is_dtype_equal(other_dtype, new_dtype):\n                    otherSeries = otherSeries.astype(new_dtype)\n\n            arr = func(series, otherSeries)\n            arr = maybe_downcast_to_dtype(arr, this_dtype)\n\n            result[col] = arr\n\n        # convert_objects just in case\n        return self._constructor(result, index=new_index,\n                                 columns=new_columns)\n\n    def combine_first(self, other):\n        \"\"\"\n        Update null elements with value in the same location in `other`.\n\n        Combine two DataFrame objects by filling null values in one DataFrame\n        with non-null values from other DataFrame. The row and column indexes\n        of the resulting DataFrame will be the union of the two.\n\n        Parameters\n        ----------\n        other : DataFrame\n            Provided DataFrame to use to fill null values.\n\n        Returns\n        -------\n        combined : DataFrame\n\n        Examples\n        --------\n\n        >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine_first(df2)\n             A    B\n        0  1.0  3.0\n        1  0.0  4.0\n\n        Null values still persist if the location of that null value\n        does not exist in `other`\n\n        >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]})\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2])\n        >>> df1.combine_first(df2)\n             A    B    C\n        0  NaN  4.0  NaN\n        1  0.0  3.0  1.0\n        2  NaN  3.0  1.0\n\n        See Also\n        --------\n        DataFrame.combine : Perform series-wise operation on two DataFrames\n            using a given function.\n        \"\"\"\n        import pandas.core.computation.expressions as expressions\n\n        def extract_values(arr):\n            # Does two things:\n            # 1. maybe gets the values from the Series / Index\n            # 2. convert datelike to i8\n            if isinstance(arr, (ABCIndexClass, ABCSeries)):\n                arr = arr._values\n\n            if needs_i8_conversion(arr):\n                # TODO(DatetimelikeArray): just use .asi8\n                if is_extension_array_dtype(arr.dtype):\n                    arr = arr.asi8\n                else:\n                    arr = arr.view('i8')\n            return arr\n\n        def combiner(x, y):\n            mask = isna(x)\n            if isinstance(mask, (ABCIndexClass, ABCSeries)):\n                mask = mask._values\n\n            x_values = extract_values(x)\n            y_values = extract_values(y)\n\n            # If the column y in other DataFrame is not in first DataFrame,\n            # just return y_values.\n            if y.name not in self.columns:\n                return y_values\n\n            return expressions.where(mask, y_values, x_values)\n\n        return self.combine(other, combiner, overwrite=False)\n\n    @deprecate_kwarg(old_arg_name='raise_conflict', new_arg_name='errors',\n                     mapping={False: 'ignore', True: 'raise'})\n    def update(self, other, join='left', overwrite=True, filter_func=None,\n               errors='ignore'):\n        \"\"\"\n        Modify in place using non-NA values from another DataFrame.\n\n        Aligns on indices. There is no return value.\n\n        Parameters\n        ----------\n        other : DataFrame, or object coercible into a DataFrame\n            Should have at least one matching index/column label\n            with the original DataFrame. If a Series is passed,\n            its name attribute must be set, and that will be\n            used as the column name to align with the original DataFrame.\n        join : {'left'}, default 'left'\n            Only left join is implemented, keeping the index and columns of the\n            original object.\n        overwrite : bool, default True\n            How to handle non-NA values for overlapping keys:\n\n            * True: overwrite original DataFrame's values\n              with values from `other`.\n            * False: only update values that are NA in\n              the original DataFrame.\n\n        filter_func : callable(1d-array) -> bool 1d-array, optional\n            Can choose to replace values other than NA. Return True for values\n            that should be updated.\n        errors : {'raise', 'ignore'}, default 'ignore'\n            If 'raise', will raise a ValueError if the DataFrame and `other`\n            both contain non-NA data in the same place.\n\n            .. versionchanged :: 0.24.0\n               Changed from `raise_conflict=False|True`\n               to `errors='ignore'|'raise'`.\n\n        Returns\n        -------\n        None : method directly changes calling object\n\n        Raises\n        ------\n        ValueError\n            * When `errors='raise'` and there's overlapping non-NA data.\n            * When `errors` is not either `'ignore'` or `'raise'`\n        NotImplementedError\n            * If `join != 'left'`\n\n        See Also\n        --------\n        dict.update : Similar method for dictionaries.\n        DataFrame.merge : For column(s)-on-columns(s) operations.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2, 3],\n        ...                    'B': [400, 500, 600]})\n        >>> new_df = pd.DataFrame({'B': [4, 5, 6],\n        ...                        'C': [7, 8, 9]})\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  1  4\n        1  2  5\n        2  3  6\n\n        The DataFrame's length does not increase as a result of the update,\n        only values at matching index/column labels are updated.\n\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_df = pd.DataFrame({'B': ['d', 'e', 'f', 'g', 'h', 'i']})\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  a  d\n        1  b  e\n        2  c  f\n\n        For Series, it's name attribute must be set.\n\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_column = pd.Series(['d', 'e'], name='B', index=[0, 2])\n        >>> df.update(new_column)\n        >>> df\n           A  B\n        0  a  d\n        1  b  y\n        2  c  e\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_df = pd.DataFrame({'B': ['d', 'e']}, index=[1, 2])\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  a  x\n        1  b  d\n        2  c  e\n\n        If `other` contains NaNs the corresponding values are not updated\n        in the original dataframe.\n\n        >>> df = pd.DataFrame({'A': [1, 2, 3],\n        ...                    'B': [400, 500, 600]})\n        >>> new_df = pd.DataFrame({'B': [4, np.nan, 6]})\n        >>> df.update(new_df)\n        >>> df\n           A      B\n        0  1    4.0\n        1  2  500.0\n        2  3    6.0\n        \"\"\"\n        import pandas.core.computation.expressions as expressions\n        # TODO: Support other joins\n        if join != 'left':  # pragma: no cover\n            raise NotImplementedError(\"Only left join is supported\")\n        if errors not in ['ignore', 'raise']:\n            raise ValueError(\"The parameter errors must be either \"\n                             \"'ignore' or 'raise'\")\n\n        if not isinstance(other, DataFrame):\n            other = DataFrame(other)\n\n        other = other.reindex_like(self)\n\n        for col in self.columns:\n            this = self[col].values\n            that = other[col].values\n            if filter_func is not None:\n                with np.errstate(all='ignore'):\n                    mask = ~filter_func(this) | isna(that)\n            else:\n                if errors == 'raise':\n                    mask_this = notna(that)\n                    mask_that = notna(this)\n                    if any(mask_this & mask_that):\n                        raise ValueError(\"Data overlaps.\")\n\n                if overwrite:\n                    mask = isna(that)\n                else:\n                    mask = notna(this)\n\n            # don't overwrite columns unecessarily\n            if mask.all():\n                continue\n\n            self[col] = expressions.where(mask, this, that)\n\n    # ----------------------------------------------------------------------\n    # Data reshaping\n\n    _shared_docs['pivot'] = \"\"\"\n        Return reshaped DataFrame organized by given index / column values.\n\n        Reshape data (produce a \"pivot\" table) based on column values. Uses\n        unique values from specified `index` / `columns` to form axes of the\n        resulting DataFrame. This function does not support data\n        aggregation, multiple values will result in a MultiIndex in the\n        columns. See the :ref:`User Guide <reshaping>` for more on reshaping.\n\n        Parameters\n        ----------%s\n        index : string or object, optional\n            Column to use to make new frame's index. If None, uses\n            existing index.\n        columns : string or object\n            Column to use to make new frame's columns.\n        values : string, object or a list of the previous, optional\n            Column(s) to use for populating new frame's values. If not\n            specified, all remaining columns will be used and the result will\n            have hierarchically indexed columns.\n\n            .. versionchanged :: 0.23.0\n               Also accept list of column names.\n\n        Returns\n        -------\n        DataFrame\n            Returns reshaped DataFrame.\n\n        Raises\n        ------\n        ValueError:\n            When there are any `index`, `columns` combinations with multiple\n            values. `DataFrame.pivot_table` when you need to aggregate.\n\n        See Also\n        --------\n        DataFrame.pivot_table : Generalization of pivot that can handle\n            duplicate values for one index/column pair.\n        DataFrame.unstack : Pivot based on the index values instead of a\n            column.\n\n        Notes\n        -----\n        For finer-tuned control, see hierarchical indexing documentation along\n        with the related stack/unstack methods.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n        ...                            'two'],\n        ...                    'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n        ...                    'baz': [1, 2, 3, 4, 5, 6],\n        ...                    'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\n        >>> df\n            foo   bar  baz  zoo\n        0   one   A    1    x\n        1   one   B    2    y\n        2   one   C    3    z\n        3   two   A    4    q\n        4   two   B    5    w\n        5   two   C    6    t\n\n        >>> df.pivot(index='foo', columns='bar', values='baz')\n        bar  A   B   C\n        foo\n        one  1   2   3\n        two  4   5   6\n\n        >>> df.pivot(index='foo', columns='bar')['baz']\n        bar  A   B   C\n        foo\n        one  1   2   3\n        two  4   5   6\n\n        >>> df.pivot(index='foo', columns='bar', values=['baz', 'zoo'])\n              baz       zoo\n        bar   A  B  C   A  B  C\n        foo\n        one   1  2  3   x  y  z\n        two   4  5  6   q  w  t\n\n        A ValueError is raised if there are any duplicates.\n\n        >>> df = pd.DataFrame({\"foo\": ['one', 'one', 'two', 'two'],\n        ...                    \"bar\": ['A', 'A', 'B', 'C'],\n        ...                    \"baz\": [1, 2, 3, 4]})\n        >>> df\n           foo bar  baz\n        0  one   A    1\n        1  one   A    2\n        2  two   B    3\n        3  two   C    4\n\n        Notice that the first two rows are the same for our `index`\n        and `columns` arguments.\n\n        >>> df.pivot(index='foo', columns='bar', values='baz')\n        Traceback (most recent call last):\n           ...\n        ValueError: Index contains duplicate entries, cannot reshape\n        \"\"\"\n\n    @Substitution('')\n    @Appender(_shared_docs['pivot'])\n    def pivot(self, index=None, columns=None, values=None):\n        from pandas.core.reshape.pivot import pivot\n        return pivot(self, index=index, columns=columns, values=values)\n\n    _shared_docs['pivot_table'] = \"\"\"\n        Create a spreadsheet-style pivot table as a DataFrame. The levels in\n        the pivot table will be stored in MultiIndex objects (hierarchical\n        indexes) on the index and columns of the result DataFrame\n\n        Parameters\n        ----------%s\n        values : column to aggregate, optional\n        index : column, Grouper, array, or list of the previous\n            If an array is passed, it must be the same length as the data. The\n            list can contain any of the other types (except list).\n            Keys to group by on the pivot table index.  If an array is passed,\n            it is being used as the same manner as column values.\n        columns : column, Grouper, array, or list of the previous\n            If an array is passed, it must be the same length as the data. The\n            list can contain any of the other types (except list).\n            Keys to group by on the pivot table column.  If an array is passed,\n            it is being used as the same manner as column values.\n        aggfunc : function, list of functions, dict, default numpy.mean\n            If list of functions passed, the resulting pivot table will have\n            hierarchical columns whose top level are the function names\n            (inferred from the function objects themselves)\n            If dict is passed, the key is column to aggregate and value\n            is function or list of functions\n        fill_value : scalar, default None\n            Value to replace missing values with\n        margins : boolean, default False\n            Add all row / columns (e.g. for subtotal / grand totals)\n        dropna : boolean, default True\n            Do not include columns whose entries are all NaN\n        margins_name : string, default 'All'\n            Name of the row / column that will contain the totals\n            when margins is True.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\",\n        ...                          \"bar\", \"bar\", \"bar\", \"bar\"],\n        ...                    \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\",\n        ...                          \"one\", \"one\", \"two\", \"two\"],\n        ...                    \"C\": [\"small\", \"large\", \"large\", \"small\",\n        ...                          \"small\", \"large\", \"small\", \"small\",\n        ...                          \"large\"],\n        ...                    \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7],\n        ...                    \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]})\n        >>> df\n             A    B      C  D  E\n        0  foo  one  small  1  2\n        1  foo  one  large  2  4\n        2  foo  one  large  2  5\n        3  foo  two  small  3  5\n        4  foo  two  small  3  6\n        5  bar  one  large  4  6\n        6  bar  one  small  5  8\n        7  bar  two  small  6  9\n        8  bar  two  large  7  9\n\n        This first example aggregates values by taking the sum.\n\n        >>> table = pivot_table(df, values='D', index=['A', 'B'],\n        ...                     columns=['C'], aggfunc=np.sum)\n        >>> table\n        C        large  small\n        A   B\n        bar one      4      5\n            two      7      6\n        foo one      4      1\n            two    NaN      6\n\n        We can also fill missing values using the `fill_value` parameter.\n\n        >>> table = pivot_table(df, values='D', index=['A', 'B'],\n        ...                     columns=['C'], aggfunc=np.sum, fill_value=0)\n        >>> table\n        C        large  small\n        A   B\n        bar one      4      5\n            two      7      6\n        foo one      4      1\n            two      0      6\n\n        The next example aggregates by taking the mean across multiple columns.\n\n        >>> table = pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n        ...                     aggfunc={'D': np.mean,\n        ...                              'E': np.mean})\n        >>> table\n                          D         E\n                       mean      mean\n        A   C\n        bar large  5.500000  7.500000\n            small  5.500000  8.500000\n        foo large  2.000000  4.500000\n            small  2.333333  4.333333\n\n        We can also calculate multiple types of aggregations for any given\n        value column.\n\n        >>> table = pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n        ...                     aggfunc={'D': np.mean,\n        ...                              'E': [min, max, np.mean]})\n        >>> table\n                          D   E\n                       mean max      mean min\n        A   C\n        bar large  5.500000  9   7.500000   6\n            small  5.500000  9   8.500000   8\n        foo large  2.000000  5   4.500000   4\n            small  2.333333  6   4.333333   2\n\n        Returns\n        -------\n        table : DataFrame\n\n        See Also\n        --------\n        DataFrame.pivot : Pivot without aggregation that can handle\n            non-numeric data.\n        \"\"\"\n\n    @Substitution('')\n    @Appender(_shared_docs['pivot_table'])\n    def pivot_table(self, values=None, index=None, columns=None,\n                    aggfunc='mean', fill_value=None, margins=False,\n                    dropna=True, margins_name='All'):\n        from pandas.core.reshape.pivot import pivot_table\n        return pivot_table(self, values=values, index=index, columns=columns,\n                           aggfunc=aggfunc, fill_value=fill_value,\n                           margins=margins, dropna=dropna,\n                           margins_name=margins_name)\n\n    def stack(self, level=-1, dropna=True):\n        \"\"\"\n        Stack the prescribed level(s) from columns to index.\n\n        Return a reshaped DataFrame or Series having a multi-level\n        index with one or more new inner-most levels compared to the current\n        DataFrame. The new inner-most levels are created by pivoting the\n        columns of the current dataframe:\n\n          - if the columns have a single level, the output is a Series;\n          - if the columns have multiple levels, the new index\n            level(s) is (are) taken from the prescribed level(s) and\n            the output is a DataFrame.\n\n        The new index levels are sorted.\n\n        Parameters\n        ----------\n        level : int, str, list, default -1\n            Level(s) to stack from the column axis onto the index\n            axis, defined as one index or label, or a list of indices\n            or labels.\n        dropna : bool, default True\n            Whether to drop rows in the resulting Frame/Series with\n            missing values. Stacking a column level onto the index\n            axis can create combinations of index and column values\n            that are missing from the original dataframe. See Examples\n            section.\n\n        Returns\n        -------\n        DataFrame or Series\n            Stacked dataframe or series.\n\n        See Also\n        --------\n        DataFrame.unstack : Unstack prescribed level(s) from index axis\n             onto column axis.\n        DataFrame.pivot : Reshape dataframe from long format to wide\n             format.\n        DataFrame.pivot_table : Create a spreadsheet-style pivot table\n             as a DataFrame.\n\n        Notes\n        -----\n        The function is named by analogy with a collection of books\n        being re-organised from being side by side on a horizontal\n        position (the columns of the dataframe) to being stacked\n        vertically on top of of each other (in the index of the\n        dataframe).\n\n        Examples\n        --------\n        **Single level columns**\n\n        >>> df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=['weight', 'height'])\n\n        Stacking a dataframe with a single level column axis returns a Series:\n\n        >>> df_single_level_cols\n             weight height\n        cat       0      1\n        dog       2      3\n        >>> df_single_level_cols.stack()\n        cat  weight    0\n             height    1\n        dog  weight    2\n             height    3\n        dtype: int64\n\n        **Multi level columns: simple case**\n\n        >>> multicol1 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n        ...                                        ('weight', 'pounds')])\n        >>> df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=multicol1)\n\n        Stacking a dataframe with a multi-level column axis:\n\n        >>> df_multi_level_cols1\n             weight\n                 kg    pounds\n        cat       1        2\n        dog       2        4\n        >>> df_multi_level_cols1.stack()\n                    weight\n        cat kg           1\n            pounds       2\n        dog kg           2\n            pounds       4\n\n        **Missing values**\n\n        >>> multicol2 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n        ...                                        ('height', 'm')])\n        >>> df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=multicol2)\n\n        It is common to have missing values when stacking a dataframe\n        with multi-level columns, as the stacked dataframe typically\n        has more values than the original dataframe. Missing values\n        are filled with NaNs:\n\n        >>> df_multi_level_cols2\n            weight height\n                kg      m\n        cat    1.0    2.0\n        dog    3.0    4.0\n        >>> df_multi_level_cols2.stack()\n                height  weight\n        cat kg     NaN     1.0\n            m      2.0     NaN\n        dog kg     NaN     3.0\n            m      4.0     NaN\n\n        **Prescribing the level(s) to be stacked**\n\n        The first parameter controls which level or levels are stacked:\n\n        >>> df_multi_level_cols2.stack(0)\n                     kg    m\n        cat height  NaN  2.0\n            weight  1.0  NaN\n        dog height  NaN  4.0\n            weight  3.0  NaN\n        >>> df_multi_level_cols2.stack([0, 1])\n        cat  height  m     2.0\n             weight  kg    1.0\n        dog  height  m     4.0\n             weight  kg    3.0\n        dtype: float64\n\n        **Dropping missing values**\n\n        >>> df_multi_level_cols3 = pd.DataFrame([[None, 1.0], [2.0, 3.0]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=multicol2)\n\n        Note that rows where all values are missing are dropped by\n        default but this behaviour can be controlled via the dropna\n        keyword parameter:\n\n        >>> df_multi_level_cols3\n            weight height\n                kg      m\n        cat    NaN    1.0\n        dog    2.0    3.0\n        >>> df_multi_level_cols3.stack(dropna=False)\n                height  weight\n        cat kg     NaN     NaN\n            m      1.0     NaN\n        dog kg     NaN     2.0\n            m      3.0     NaN\n        >>> df_multi_level_cols3.stack(dropna=True)\n                height  weight\n        cat m      1.0     NaN\n        dog kg     NaN     2.0\n            m      3.0     NaN\n        \"\"\"\n        from pandas.core.reshape.reshape import stack, stack_multiple\n\n        if isinstance(level, (tuple, list)):\n            return stack_multiple(self, level, dropna=dropna)\n        else:\n            return stack(self, level, dropna=dropna)\n\n    def unstack(self, level=-1, fill_value=None):\n        \"\"\"\n        Pivot a level of the (necessarily hierarchical) index labels, returning\n        a DataFrame having a new level of column labels whose inner-most level\n        consists of the pivoted index labels. If the index is not a MultiIndex,\n        the output will be a Series (the analogue of stack when the columns are\n        not a MultiIndex).\n        The level involved will automatically get sorted.\n\n        Parameters\n        ----------\n        level : int, string, or list of these, default -1 (last level)\n            Level(s) of index to unstack, can pass level name\n        fill_value : replace NaN with this value if the unstack produces\n            missing values\n\n            .. versionadded:: 0.18.0\n\n        See Also\n        --------\n        DataFrame.pivot : Pivot a table based on column values.\n        DataFrame.stack : Pivot a level of the column labels (inverse operation\n            from `unstack`).\n\n        Examples\n        --------\n        >>> index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),\n        ...                                    ('two', 'a'), ('two', 'b')])\n        >>> s = pd.Series(np.arange(1.0, 5.0), index=index)\n        >>> s\n        one  a   1.0\n             b   2.0\n        two  a   3.0\n             b   4.0\n        dtype: float64\n\n        >>> s.unstack(level=-1)\n             a   b\n        one  1.0  2.0\n        two  3.0  4.0\n\n        >>> s.unstack(level=0)\n           one  two\n        a  1.0   3.0\n        b  2.0   4.0\n\n        >>> df = s.unstack(level=0)\n        >>> df.unstack()\n        one  a  1.0\n             b  2.0\n        two  a  3.0\n             b  4.0\n        dtype: float64\n\n        Returns\n        -------\n        unstacked : DataFrame or Series\n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n        return unstack(self, level, fill_value)\n\n    _shared_docs['melt'] = (\"\"\"\n    \"Unpivots\" a DataFrame from wide format to long format, optionally\n    leaving identifier variables set.\n\n    This function is useful to massage a DataFrame into a format where one\n    or more columns are identifier variables (`id_vars`), while all other\n    columns, considered measured variables (`value_vars`), are \"unpivoted\" to\n    the row axis, leaving just two non-identifier columns, 'variable' and\n    'value'.\n\n    %(versionadded)s\n    Parameters\n    ----------\n    frame : DataFrame\n    id_vars : tuple, list, or ndarray, optional\n        Column(s) to use as identifier variables.\n    value_vars : tuple, list, or ndarray, optional\n        Column(s) to unpivot. If not specified, uses all columns that\n        are not set as `id_vars`.\n    var_name : scalar\n        Name to use for the 'variable' column. If None it uses\n        ``frame.columns.name`` or 'variable'.\n    value_name : scalar, default 'value'\n        Name to use for the 'value' column.\n    col_level : int or string, optional\n        If columns are a MultiIndex then use this level to melt.\n\n    See Also\n    --------\n    %(other)s\n    pivot_table\n    DataFrame.pivot\n\n    Examples\n    --------\n    >>> df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n    ...                    'B': {0: 1, 1: 3, 2: 5},\n    ...                    'C': {0: 2, 1: 4, 2: 6}})\n    >>> df\n       A  B  C\n    0  a  1  2\n    1  b  3  4\n    2  c  5  6\n\n    >>> %(caller)sid_vars=['A'], value_vars=['B'])\n       A variable  value\n    0  a        B      1\n    1  b        B      3\n    2  c        B      5\n\n    >>> %(caller)sid_vars=['A'], value_vars=['B', 'C'])\n       A variable  value\n    0  a        B      1\n    1  b        B      3\n    2  c        B      5\n    3  a        C      2\n    4  b        C      4\n    5  c        C      6\n\n    The names of 'variable' and 'value' columns can be customized:\n\n    >>> %(caller)sid_vars=['A'], value_vars=['B'],\n    ...         var_name='myVarname', value_name='myValname')\n       A myVarname  myValname\n    0  a         B          1\n    1  b         B          3\n    2  c         B          5\n\n    If you have multi-index columns:\n\n    >>> df.columns = [list('ABC'), list('DEF')]\n    >>> df\n       A  B  C\n       D  E  F\n    0  a  1  2\n    1  b  3  4\n    2  c  5  6\n\n    >>> %(caller)scol_level=0, id_vars=['A'], value_vars=['B'])\n       A variable  value\n    0  a        B      1\n    1  b        B      3\n    2  c        B      5\n\n    >>> %(caller)sid_vars=[('A', 'D')], value_vars=[('B', 'E')])\n      (A, D) variable_0 variable_1  value\n    0      a          B          E      1\n    1      b          B          E      3\n    2      c          B          E      5\n\n    \"\"\")\n\n    @Appender(_shared_docs['melt'] %\n              dict(caller='df.melt(',\n                   versionadded='.. versionadded:: 0.20.0\\n',\n                   other='melt'))\n    def melt(self, id_vars=None, value_vars=None, var_name=None,\n             value_name='value', col_level=None):\n        from pandas.core.reshape.melt import melt\n        return melt(self, id_vars=id_vars, value_vars=value_vars,\n                    var_name=var_name, value_name=value_name,\n                    col_level=col_level)\n\n    # ----------------------------------------------------------------------\n    # Time series-related\n\n    def diff(self, periods=1, axis=0):\n        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of a DataFrame element compared with another\n        element in the DataFrame (default is the element in the same column\n        of the previous row).\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative\n            values.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Take difference over rows (0) or columns (1).\n\n            .. versionadded:: 0.16.1.\n\n        Returns\n        -------\n        diffed : DataFrame\n\n        See Also\n        --------\n        Series.diff: First discrete difference for a Series.\n        DataFrame.pct_change: Percent change over given number of periods.\n        DataFrame.shift: Shift index by desired number of periods with an\n            optional time freq.\n\n        Examples\n        --------\n        Difference with previous row\n\n        >>> df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6],\n        ...                    'b': [1, 1, 2, 3, 5, 8],\n        ...                    'c': [1, 4, 9, 16, 25, 36]})\n        >>> df\n           a  b   c\n        0  1  1   1\n        1  2  1   4\n        2  3  2   9\n        3  4  3  16\n        4  5  5  25\n        5  6  8  36\n\n        >>> df.diff()\n             a    b     c\n        0  NaN  NaN   NaN\n        1  1.0  0.0   3.0\n        2  1.0  1.0   5.0\n        3  1.0  1.0   7.0\n        4  1.0  2.0   9.0\n        5  1.0  3.0  11.0\n\n        Difference with previous column\n\n        >>> df.diff(axis=1)\n            a    b     c\n        0 NaN  0.0   0.0\n        1 NaN -1.0   3.0\n        2 NaN -1.0   7.0\n        3 NaN -1.0  13.0\n        4 NaN  0.0  20.0\n        5 NaN  2.0  28.0\n\n        Difference with 3rd previous row\n\n        >>> df.diff(periods=3)\n             a    b     c\n        0  NaN  NaN   NaN\n        1  NaN  NaN   NaN\n        2  NaN  NaN   NaN\n        3  3.0  2.0  15.0\n        4  3.0  4.0  21.0\n        5  3.0  6.0  27.0\n\n        Difference with following row\n\n        >>> df.diff(periods=-1)\n             a    b     c\n        0 -1.0  0.0  -3.0\n        1 -1.0 -1.0  -5.0\n        2 -1.0 -1.0  -7.0\n        3 -1.0 -2.0  -9.0\n        4 -1.0 -3.0 -11.0\n        5  NaN  NaN   NaN\n        \"\"\"\n        bm_axis = self._get_block_manager_axis(axis)\n        new_data = self._data.diff(n=periods, axis=bm_axis)\n        return self._constructor(new_data)\n\n    # ----------------------------------------------------------------------\n    # Function application\n\n    def _gotitem(self,\n                 key,           # type: Union[str, List[str]]\n                 ndim,          # type: int\n                 subset=None    # type: Union[Series, DataFrame, None]\n                 ):\n        # type: (...) -> Union[Series, DataFrame]\n        \"\"\"\n        sub-classes to define\n        return a sliced object\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : 1,2\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n        if subset is None:\n            subset = self\n        elif subset.ndim == 1:  # is Series\n            return subset\n\n        # TODO: _shallow_copy(subset)?\n        return subset[key]\n\n    _agg_doc = dedent(\"\"\"\n    The aggregation operations are always performed over an axis, either the\n    index (default) or the column axis. This behavior is different from\n    `numpy` aggregation functions (`mean`, `median`, `prod`, `sum`, `std`,\n    `var`), where the default is to compute the aggregation of the flattened\n    array, e.g., ``numpy.mean(arr_2d)`` as opposed to ``numpy.mean(arr_2d,\n    axis=0)``.\n\n    `agg` is an alias for `aggregate`. Use the alias.\n\n    Examples\n    --------\n    >>> df = pd.DataFrame([[1, 2, 3],\n    ...                    [4, 5, 6],\n    ...                    [7, 8, 9],\n    ...                    [np.nan, np.nan, np.nan]],\n    ...                   columns=['A', 'B', 'C'])\n\n    Aggregate these functions over the rows.\n\n    >>> df.agg(['sum', 'min'])\n            A     B     C\n    sum  12.0  15.0  18.0\n    min   1.0   2.0   3.0\n\n    Different aggregations per column.\n\n    >>> df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})\n            A    B\n    max   NaN  8.0\n    min   1.0  2.0\n    sum  12.0  NaN\n\n    Aggregate over the columns.\n\n    >>> df.agg(\"mean\", axis=\"columns\")\n    0    2.0\n    1    5.0\n    2    8.0\n    3    NaN\n    dtype: float64\n\n    See Also\n    --------\n    DataFrame.apply : Perform any type of operations.\n    DataFrame.transform : Perform transformation type operations.\n    pandas.core.groupby.GroupBy : Perform operations over groups.\n    pandas.core.resample.Resampler : Perform operations over resampled bins.\n    pandas.core.window.Rolling : Perform operations over rolling window.\n    pandas.core.window.Expanding : Perform operations over expanding window.\n    pandas.core.window.EWM : Perform operation over exponential weighted\n        window.\n    \"\"\")\n\n    @Appender(_agg_doc)\n    @Appender(_shared_docs['aggregate'] % dict(\n        versionadded='.. versionadded:: 0.20.0',\n        **_shared_doc_kwargs))\n    def aggregate(self, func, axis=0, *args, **kwargs):\n        axis = self._get_axis_number(axis)\n\n        result = None\n        try:\n            result, how = self._aggregate(func, axis=axis, *args, **kwargs)\n        except TypeError:\n            pass\n        if result is None:\n            return self.apply(func, axis=axis, args=args, **kwargs)\n        return result\n\n    def _aggregate(self, arg, axis=0, *args, **kwargs):\n        if axis == 1:\n            # NDFrame.aggregate returns a tuple, and we need to transpose\n            # only result\n            result, how = (super(DataFrame, self.T)\n                           ._aggregate(arg, *args, **kwargs))\n            result = result.T if result is not None else result\n            return result, how\n        return super(DataFrame, self)._aggregate(arg, *args, **kwargs)\n\n    agg = aggregate\n\n    @Appender(_shared_docs['transform'] % _shared_doc_kwargs)\n    def transform(self, func, axis=0, *args, **kwargs):\n        axis = self._get_axis_number(axis)\n        if axis == 1:\n            return super(DataFrame, self.T).transform(func, *args, **kwargs).T\n        return super(DataFrame, self).transform(func, *args, **kwargs)\n\n    def apply(self, func, axis=0, broadcast=None, raw=False, reduce=None,\n              result_type=None, args=(), **kwds):\n        \"\"\"\n        Apply a function along an axis of the DataFrame.\n\n        Objects passed to the function are Series objects whose index is\n        either the DataFrame's index (``axis=0``) or the DataFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to apply to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': apply function to each column.\n            * 1 or 'columns': apply function to each row.\n        broadcast : bool, optional\n            Only relevant for aggregation functions:\n\n            * ``False`` or ``None`` : returns a Series whose length is the\n              length of the index or the number of columns (based on the\n              `axis` parameter)\n            * ``True`` : results will be broadcast to the original shape\n              of the frame, the original index and columns will be retained.\n\n            .. deprecated:: 0.23.0\n               This argument will be removed in a future version, replaced\n               by result_type='broadcast'.\n\n        raw : bool, default False\n            * ``False`` : passes each row or column as a Series to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just applying a NumPy reduction function this will\n              achieve much better performance.\n        reduce : bool or None, default None\n            Try to apply reduction procedures. If the DataFrame is empty,\n            `apply` will use `reduce` to determine whether the result\n            should be a Series or a DataFrame. If ``reduce=None`` (the\n            default), `apply`'s return value will be guessed by calling\n            `func` on an empty Series\n            (note: while guessing, exceptions raised by `func` will be\n            ignored).\n            If ``reduce=True`` a Series will always be returned, and if\n            ``reduce=False`` a DataFrame will always be returned.\n\n            .. deprecated:: 0.23.0\n               This argument will be removed in a future version, replaced\n               by ``result_type='reduce'``.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Series if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the DataFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Series\n            of those. However if the apply function returns a Series these\n            are expanded to columns.\n\n            .. versionadded:: 0.23.0\n\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/series.\n        **kwds\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Notes\n        -----\n        In the current implementation apply calls `func` twice on the\n        first column/row to decide whether it can take a fast or slow\n        code path. This can lead to unexpected behavior if `func` has\n        side-effects, as they will take effect twice for the first\n        column/row.\n\n        See Also\n        --------\n        DataFrame.applymap: For elementwise operations.\n        DataFrame.aggregate: Only perform aggregating type operations.\n        DataFrame.transform: Only perform transforming type operations.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([[4, 9],] * 3, columns=['A', 'B'])\n        >>> df\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(df)``):\n\n        >>> df.apply(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> df.apply(np.sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> df.apply(np.sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Retuning a list-like will result in a Series\n\n        >>> df.apply(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing result_type='expand' will expand list-like results\n        to columns of a Dataframe\n\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Series inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Series index.\n\n        >>> df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returns\n        -------\n        applied : Series or DataFrame\n        \"\"\"\n        from pandas.core.apply import frame_apply\n        op = frame_apply(self,\n                         func=func,\n                         axis=axis,\n                         broadcast=broadcast,\n                         raw=raw,\n                         reduce=reduce,\n                         result_type=result_type,\n                         args=args,\n                         kwds=kwds)\n        return op.get_result()\n\n    def applymap(self, func):\n        \"\"\"\n        Apply a function to a Dataframe elementwise.\n\n        This method applies a function that accepts and returns a scalar\n        to every element of a DataFrame.\n\n        Parameters\n        ----------\n        func : callable\n            Python function, returns a single value from a single value.\n\n        Returns\n        -------\n        DataFrame\n            Transformed DataFrame.\n\n        See Also\n        --------\n        DataFrame.apply : Apply a function along input axis of DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])\n        >>> df\n               0      1\n        0  1.000  2.120\n        1  3.356  4.567\n\n        >>> df.applymap(lambda x: len(str(x)))\n           0  1\n        0  3  4\n        1  5  5\n\n        Note that a vectorized version of `func` often exists, which will\n        be much faster. You could square each number elementwise.\n\n        >>> df.applymap(lambda x: x**2)\n                   0          1\n        0   1.000000   4.494400\n        1  11.262736  20.857489\n\n        But it's better to avoid applymap in that case.\n\n        >>> df ** 2\n                   0          1\n        0   1.000000   4.494400\n        1  11.262736  20.857489\n        \"\"\"\n\n        # if we have a dtype == 'M8[ns]', provide boxed values\n        def infer(x):\n            if x.empty:\n                return lib.map_infer(x, func)\n            return lib.map_infer(x.astype(object).values, func)\n\n        return self.apply(infer)\n\n    # ----------------------------------------------------------------------\n    # Merging / joining methods\n\n    def append(self, other, ignore_index=False,\n               verify_integrity=False, sort=None):\n        \"\"\"\n        Append rows of `other` to the end of caller, returning a new object.\n\n        Columns in `other` that are not in the caller are added as new columns.\n\n        Parameters\n        ----------\n        other : DataFrame or Series/dict-like object, or list of these\n            The data to append.\n        ignore_index : boolean, default False\n            If True, do not use the index labels.\n        verify_integrity : boolean, default False\n            If True, raise ValueError on creating index with duplicates.\n        sort : boolean, default None\n            Sort columns if the columns of `self` and `other` are not aligned.\n            The default sorting is deprecated and will change to not-sorting\n            in a future version of pandas. Explicitly pass ``sort=True`` to\n            silence the warning and sort. Explicitly pass ``sort=False`` to\n            silence the warning and not sort.\n\n            .. versionadded:: 0.23.0\n\n        Returns\n        -------\n        appended : DataFrame\n\n        Notes\n        -----\n        If a list of dict/series is passed and the keys are all contained in\n        the DataFrame's index, the order of the columns in the resulting\n        DataFrame will be unchanged.\n\n        Iteratively appending rows to a DataFrame can be more computationally\n        intensive than a single concatenate. A better solution is to append\n        those rows to a list and then concatenate the list with the original\n        DataFrame all at once.\n\n        See Also\n        --------\n        pandas.concat : General function to concatenate DataFrame, Series\n            or Panel objects.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n        >>> df\n           A  B\n        0  1  2\n        1  3  4\n        >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n        >>> df.append(df2)\n           A  B\n        0  1  2\n        1  3  4\n        0  5  6\n        1  7  8\n\n        With `ignore_index` set to True:\n\n        >>> df.append(df2, ignore_index=True)\n           A  B\n        0  1  2\n        1  3  4\n        2  5  6\n        3  7  8\n\n        The following, while not recommended methods for generating DataFrames,\n        show two ways to generate a DataFrame from multiple data sources.\n\n        Less efficient:\n\n        >>> df = pd.DataFrame(columns=['A'])\n        >>> for i in range(5):\n        ...     df = df.append({'A': i}, ignore_index=True)\n        >>> df\n           A\n        0  0\n        1  1\n        2  2\n        3  3\n        4  4\n\n        More efficient:\n\n        >>> pd.concat([pd.DataFrame([i], columns=['A']) for i in range(5)],\n        ...           ignore_index=True)\n           A\n        0  0\n        1  1\n        2  2\n        3  3\n        4  4\n        \"\"\"\n        if isinstance(other, (Series, dict)):\n            if isinstance(other, dict):\n                other = Series(other)\n            if other.name is None and not ignore_index:\n                raise TypeError('Can only append a Series if ignore_index=True'\n                                ' or if the Series has a name')\n\n            if other.name is None:\n                index = None\n            else:\n                # other must have the same index name as self, otherwise\n                # index name will be reset\n                index = Index([other.name], name=self.index.name)\n\n            idx_diff = other.index.difference(self.columns)\n            try:\n                combined_columns = self.columns.append(idx_diff)\n            except TypeError:\n                combined_columns = self.columns.astype(object).append(idx_diff)\n            other = other.reindex(combined_columns, copy=False)\n            other = DataFrame(other.values.reshape((1, len(other))),\n                              index=index,\n                              columns=combined_columns)\n            other = other._convert(datetime=True, timedelta=True)\n            if not self.columns.equals(combined_columns):\n                self = self.reindex(columns=combined_columns)\n        elif isinstance(other, list) and not isinstance(other[0], DataFrame):\n            other = DataFrame(other)\n            if (self.columns.get_indexer(other.columns) >= 0).all():\n                other = other.loc[:, self.columns]\n\n        from pandas.core.reshape.concat import concat\n        if isinstance(other, (list, tuple)):\n            to_concat = [self] + other\n        else:\n            to_concat = [self, other]\n        return concat(to_concat, ignore_index=ignore_index,\n                      verify_integrity=verify_integrity,\n                      sort=sort)\n\n    def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n             sort=False):\n        \"\"\"\n        Join columns of another DataFrame.\n\n        Join columns with `other` DataFrame either on index or on a key\n        column. Efficiently join multiple DataFrame objects by index at once by\n        passing a list.\n\n        Parameters\n        ----------\n        other : DataFrame, Series, or list of DataFrame\n            Index should be similar to one of the columns in this one. If a\n            Series is passed, its name attribute must be set, and that will be\n            used as the column name in the resulting joined DataFrame.\n        on : str, list of str, or array-like, optional\n            Column or index level name(s) in the caller to join on the index\n            in `other`, otherwise joins index-on-index. If multiple\n            values given, the `other` DataFrame must have a MultiIndex. Can\n            pass an array as the join key if it is not already contained in\n            the calling DataFrame. Like an Excel VLOOKUP operation.\n        how : {'left', 'right', 'outer', 'inner'}, default 'left'\n            How to handle the operation of the two objects.\n\n            * left: use calling frame's index (or column if on is specified)\n            * right: use `other`'s index.\n            * outer: form union of calling frame's index (or column if on is\n              specified) with `other`'s index, and sort it.\n              lexicographically.\n            * inner: form intersection of calling frame's index (or column if\n              on is specified) with `other`'s index, preserving the order\n              of the calling's one.\n        lsuffix : str, default ''\n            Suffix to use from left frame's overlapping columns.\n        rsuffix : str, default ''\n            Suffix to use from right frame's overlapping columns.\n        sort : bool, default False\n            Order result DataFrame lexicographically by the join key. If False,\n            the order of the join key depends on the join type (how keyword).\n\n        Returns\n        -------\n        DataFrame\n            A dataframe containing columns from both the caller and `other`.\n\n        Notes\n        -----\n        Parameters `on`, `lsuffix`, and `rsuffix` are not supported when\n        passing a list of `DataFrame` objects.\n\n        Support for specifying index levels as the `on` parameter was added\n        in version 0.23.0.\n\n        See Also\n        --------\n        DataFrame.merge : For column(s)-on-columns(s) operations.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],\n        ...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n\n        >>> df\n          key   A\n        0  K0  A0\n        1  K1  A1\n        2  K2  A2\n        3  K3  A3\n        4  K4  A4\n        5  K5  A5\n\n        >>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],\n        ...                       'B': ['B0', 'B1', 'B2']})\n\n        >>> other\n          key   B\n        0  K0  B0\n        1  K1  B1\n        2  K2  B2\n\n        Join DataFrames using their indexes.\n\n        >>> df.join(other, lsuffix='_caller', rsuffix='_other')\n          key_caller   A key_other    B\n        0         K0  A0        K0   B0\n        1         K1  A1        K1   B1\n        2         K2  A2        K2   B2\n        3         K3  A3       NaN  NaN\n        4         K4  A4       NaN  NaN\n        5         K5  A5       NaN  NaN\n\n        If we want to join using the key columns, we need to set key to be\n        the index in both `df` and `other`. The joined DataFrame will have\n        key as its index.\n\n        >>> df.set_index('key').join(other.set_index('key'))\n              A    B\n        key\n        K0   A0   B0\n        K1   A1   B1\n        K2   A2   B2\n        K3   A3  NaN\n        K4   A4  NaN\n        K5   A5  NaN\n\n        Another option to join using the key columns is to use the `on`\n        parameter. DataFrame.join always uses `other`'s index but we can use\n        any column in `df`. This method preserves the original DataFrame's\n        index in the result.\n\n        >>> df.join(other.set_index('key'), on='key')\n          key   A    B\n        0  K0  A0   B0\n        1  K1  A1   B1\n        2  K2  A2   B2\n        3  K3  A3  NaN\n        4  K4  A4  NaN\n        5  K5  A5  NaN\n        \"\"\"\n        # For SparseDataFrame's benefit\n        return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n                                 rsuffix=rsuffix, sort=sort)\n\n    def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n                     sort=False):\n        from pandas.core.reshape.merge import merge\n        from pandas.core.reshape.concat import concat\n\n        if isinstance(other, Series):\n            if other.name is None:\n                raise ValueError('Other Series must have a name')\n            other = DataFrame({other.name: other})\n\n        if isinstance(other, DataFrame):\n            return merge(self, other, left_on=on, how=how,\n                         left_index=on is None, right_index=True,\n                         suffixes=(lsuffix, rsuffix), sort=sort)\n        else:\n            if on is not None:\n                raise ValueError('Joining multiple DataFrames only supported'\n                                 ' for joining on index')\n\n            frames = [self] + list(other)\n\n            can_concat = all(df.index.is_unique for df in frames)\n\n            # join indexes only using concat\n            if can_concat:\n                if how == 'left':\n                    how = 'outer'\n                    join_axes = [self.index]\n                else:\n                    join_axes = None\n                return concat(frames, axis=1, join=how, join_axes=join_axes,\n                              verify_integrity=True)\n\n            joined = frames[0]\n\n            for frame in frames[1:]:\n                joined = merge(joined, frame, how=how, left_index=True,\n                               right_index=True)\n\n            return joined\n\n    @Substitution('')\n    @Appender(_merge_doc, indents=2)\n    def merge(self, right, how='inner', on=None, left_on=None, right_on=None,\n              left_index=False, right_index=False, sort=False,\n              suffixes=('_x', '_y'), copy=True, indicator=False,\n              validate=None):\n        from pandas.core.reshape.merge import merge\n        return merge(self, right, how=how, on=on, left_on=left_on,\n                     right_on=right_on, left_index=left_index,\n                     right_index=right_index, sort=sort, suffixes=suffixes,\n                     copy=copy, indicator=indicator, validate=validate)\n\n    def round(self, decimals=0, *args, **kwargs):\n        \"\"\"\n        Round a DataFrame to a variable number of decimal places.\n\n        Parameters\n        ----------\n        decimals : int, dict, Series\n            Number of decimal places to round each column to. If an int is\n            given, round each column to the same number of places.\n            Otherwise dict and Series round to variable numbers of places.\n            Column names should be in the keys if `decimals` is a\n            dict-like, or in the index if `decimals` is a Series. Any\n            columns not included in `decimals` will be left as is. Elements\n            of `decimals` which are not columns of the input will be\n            ignored.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        numpy.around\n        Series.round\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.random.random([3, 3]),\n        ...     columns=['A', 'B', 'C'], index=['first', 'second', 'third'])\n        >>> df\n                       A         B         C\n        first   0.028208  0.992815  0.173891\n        second  0.038683  0.645646  0.577595\n        third   0.877076  0.149370  0.491027\n        >>> df.round(2)\n                   A     B     C\n        first   0.03  0.99  0.17\n        second  0.04  0.65  0.58\n        third   0.88  0.15  0.49\n        >>> df.round({'A': 1, 'C': 2})\n                  A         B     C\n        first   0.0  0.992815  0.17\n        second  0.0  0.645646  0.58\n        third   0.9  0.149370  0.49\n        >>> decimals = pd.Series([1, 0, 2], index=['A', 'B', 'C'])\n        >>> df.round(decimals)\n                  A  B     C\n        first   0.0  1  0.17\n        second  0.0  1  0.58\n        third   0.9  0  0.49\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        def _dict_round(df, decimals):\n            for col, vals in df.iteritems():\n                try:\n                    yield _series_round(vals, decimals[col])\n                except KeyError:\n                    yield vals\n\n        def _series_round(s, decimals):\n            if is_integer_dtype(s) or is_float_dtype(s):\n                return s.round(decimals)\n            return s\n\n        nv.validate_round(args, kwargs)\n\n        if isinstance(decimals, (dict, Series)):\n            if isinstance(decimals, Series):\n                if not decimals.index.is_unique:\n                    raise ValueError(\"Index of decimals must be unique\")\n            new_cols = [col for col in _dict_round(self, decimals)]\n        elif is_integer(decimals):\n            # Dispatch to Series.round\n            new_cols = [_series_round(v, decimals)\n                        for _, v in self.iteritems()]\n        else:\n            raise TypeError(\"decimals must be an integer, a dict-like or a \"\n                            \"Series\")\n\n        if len(new_cols) > 0:\n            return self._constructor(concat(new_cols, axis=1),\n                                     index=self.index,\n                                     columns=self.columns)\n        else:\n            return self\n\n    # ----------------------------------------------------------------------\n    # Statistical methods, etc.\n\n    def corr(self, method='pearson', min_periods=1):\n        \"\"\"\n        Compute pairwise correlation of columns, excluding NA/null values\n\n        Parameters\n        ----------\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            * pearson : standard correlation coefficient\n            * kendall : Kendall Tau correlation coefficient\n            * spearman : Spearman rank correlation\n            * callable: callable with input two 1d ndarrays\n                and returning a float\n                .. versionadded:: 0.24.0\n\n        min_periods : int, optional\n            Minimum number of observations required per pair of columns\n            to have a valid result. Currently only available for pearson\n            and spearman correlation\n\n        Returns\n        -------\n        y : DataFrame\n\n        Examples\n        --------\n        >>> histogram_intersection = lambda a, b: np.minimum(a, b\n        ... ).sum().round(decimals=1)\n        >>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df.corr(method=histogram_intersection)\n              dogs cats\n        dogs   1.0  0.3\n        cats   0.3  1.0\n        \"\"\"\n        numeric_df = self._get_numeric_data()\n        cols = numeric_df.columns\n        idx = cols.copy()\n        mat = numeric_df.values\n\n        if method == 'pearson':\n            correl = libalgos.nancorr(ensure_float64(mat), minp=min_periods)\n        elif method == 'spearman':\n            correl = libalgos.nancorr_spearman(ensure_float64(mat),\n                                               minp=min_periods)\n        elif method == 'kendall' or callable(method):\n            if min_periods is None:\n                min_periods = 1\n            mat = ensure_float64(mat).T\n            corrf = nanops.get_corr_func(method)\n            K = len(cols)\n            correl = np.empty((K, K), dtype=float)\n            mask = np.isfinite(mat)\n            for i, ac in enumerate(mat):\n                for j, bc in enumerate(mat):\n                    if i > j:\n                        continue\n\n                    valid = mask[i] & mask[j]\n                    if valid.sum() < min_periods:\n                        c = np.nan\n                    elif i == j:\n                        c = 1.\n                    elif not valid.all():\n                        c = corrf(ac[valid], bc[valid])\n                    else:\n                        c = corrf(ac, bc)\n                    correl[i, j] = c\n                    correl[j, i] = c\n        else:\n            raise ValueError(\"method must be either 'pearson', \"\n                             \"'spearman', or 'kendall', '{method}' \"\n                             \"was supplied\".format(method=method))\n\n        return self._constructor(correl, index=idx, columns=cols)\n\n    def cov(self, min_periods=None):\n        \"\"\"\n        Compute pairwise covariance of columns, excluding NA/null values.\n\n        Compute the pairwise covariance among the series of a DataFrame.\n        The returned data frame is the `covariance matrix\n        <https://en.wikipedia.org/wiki/Covariance_matrix>`__ of the columns\n        of the DataFrame.\n\n        Both NA and null values are automatically excluded from the\n        calculation. (See the note below about bias from missing values.)\n        A threshold can be set for the minimum number of\n        observations for each value created. Comparisons with observations\n        below this threshold will be returned as ``NaN``.\n\n        This method is generally used for the analysis of time series data to\n        understand the relationship between different measures\n        across time.\n\n        Parameters\n        ----------\n        min_periods : int, optional\n            Minimum number of observations required per pair of columns\n            to have a valid result.\n\n        Returns\n        -------\n        DataFrame\n            The covariance matrix of the series of the DataFrame.\n\n        See Also\n        --------\n        pandas.Series.cov : Compute covariance with another Series.\n        pandas.core.window.EWM.cov: Exponential weighted sample covariance.\n        pandas.core.window.Expanding.cov : Expanding sample covariance.\n        pandas.core.window.Rolling.cov : Rolling sample covariance.\n\n        Notes\n        -----\n        Returns the covariance matrix of the DataFrame's time series.\n        The covariance is normalized by N-1.\n\n        For DataFrames that have Series that are missing data (assuming that\n        data is `missing at random\n        <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random>`__)\n        the returned covariance matrix will be an unbiased estimate\n        of the variance and covariance between the member Series.\n\n        However, for many applications this estimate may not be acceptable\n        because the estimate covariance matrix is not guaranteed to be positive\n        semi-definite. This could lead to estimate correlations having\n        absolute values which are greater than one, and/or a non-invertible\n        covariance matrix. See `Estimation of covariance matrices\n        <http://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_\n        matrices>`__ for more details.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df.cov()\n                  dogs      cats\n        dogs  0.666667 -1.000000\n        cats -1.000000  1.666667\n\n        >>> np.random.seed(42)\n        >>> df = pd.DataFrame(np.random.randn(1000, 5),\n        ...                   columns=['a', 'b', 'c', 'd', 'e'])\n        >>> df.cov()\n                  a         b         c         d         e\n        a  0.998438 -0.020161  0.059277 -0.008943  0.014144\n        b -0.020161  1.059352 -0.008543 -0.024738  0.009826\n        c  0.059277 -0.008543  1.010670 -0.001486 -0.000271\n        d -0.008943 -0.024738 -0.001486  0.921297 -0.013692\n        e  0.014144  0.009826 -0.000271 -0.013692  0.977795\n\n        **Minimum number of periods**\n\n        This method also supports an optional ``min_periods`` keyword\n        that specifies the required minimum number of non-NA observations for\n        each column pair in order to have a valid result:\n\n        >>> np.random.seed(42)\n        >>> df = pd.DataFrame(np.random.randn(20, 3),\n        ...                   columns=['a', 'b', 'c'])\n        >>> df.loc[df.index[:5], 'a'] = np.nan\n        >>> df.loc[df.index[5:10], 'b'] = np.nan\n        >>> df.cov(min_periods=12)\n                  a         b         c\n        a  0.316741       NaN -0.150812\n        b       NaN  1.248003  0.191417\n        c -0.150812  0.191417  0.895202\n        \"\"\"\n        numeric_df = self._get_numeric_data()\n        cols = numeric_df.columns\n        idx = cols.copy()\n        mat = numeric_df.values\n\n        if notna(mat).all():\n            if min_periods is not None and min_periods > len(mat):\n                baseCov = np.empty((mat.shape[1], mat.shape[1]))\n                baseCov.fill(np.nan)\n            else:\n                baseCov = np.cov(mat.T)\n            baseCov = baseCov.reshape((len(cols), len(cols)))\n        else:\n            baseCov = libalgos.nancorr(ensure_float64(mat), cov=True,\n                                       minp=min_periods)\n\n        return self._constructor(baseCov, index=idx, columns=cols)\n\n    def corrwith(self, other, axis=0, drop=False):\n        \"\"\"\n        Compute pairwise correlation between rows or columns of two DataFrame\n        objects.\n\n        Parameters\n        ----------\n        other : DataFrame, Series\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            0 or 'index' to compute column-wise, 1 or 'columns' for row-wise\n        drop : boolean, default False\n            Drop missing indices from result, default returns union of all\n\n        Returns\n        -------\n        correls : Series\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        this = self._get_numeric_data()\n\n        if isinstance(other, Series):\n            return this.apply(other.corr, axis=axis)\n\n        other = other._get_numeric_data()\n\n        left, right = this.align(other, join='inner', copy=False)\n\n        # mask missing values\n        left = left + right * 0\n        right = right + left * 0\n\n        if axis == 1:\n            left = left.T\n            right = right.T\n\n        # demeaned data\n        ldem = left - left.mean()\n        rdem = right - right.mean()\n\n        num = (ldem * rdem).sum()\n        dom = (left.count() - 1) * left.std() * right.std()\n\n        correl = num / dom\n\n        if not drop:\n            raxis = 1 if axis == 0 else 0\n            result_index = this._get_axis(raxis).union(other._get_axis(raxis))\n            correl = correl.reindex(result_index)\n\n        return correl\n\n    # ----------------------------------------------------------------------\n    # ndarray-like stats methods\n\n    def count(self, axis=0, level=None, numeric_only=False):\n        \"\"\"\n        Count non-NA cells for each column or row.\n\n        The values `None`, `NaN`, `NaT`, and optionally `numpy.inf` (depending\n        on `pandas.options.mode.use_inf_as_na`) are considered NA.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            If 0 or 'index' counts are generated for each column.\n            If 1 or 'columns' counts are generated for each **row**.\n        level : int or str, optional\n            If the axis is a `MultiIndex` (hierarchical), count along a\n            particular `level`, collapsing into a `DataFrame`.\n            A `str` specifies the level name.\n        numeric_only : boolean, default False\n            Include only `float`, `int` or `boolean` data.\n\n        Returns\n        -------\n        Series or DataFrame\n            For each column/row the number of non-NA/null entries.\n            If `level` is specified returns a `DataFrame`.\n\n        See Also\n        --------\n        Series.count: Number of non-NA elements in a Series.\n        DataFrame.shape: Number of DataFrame rows and columns (including NA\n            elements).\n        DataFrame.isna: Boolean same-sized DataFrame showing places of NA\n            elements.\n\n        Examples\n        --------\n        Constructing DataFrame from a dictionary:\n\n        >>> df = pd.DataFrame({\"Person\":\n        ...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n        ...                    \"Age\": [24., np.nan, 21., 33, 26],\n        ...                    \"Single\": [False, True, True, True, False]})\n        >>> df\n           Person   Age  Single\n        0    John  24.0   False\n        1    Myla   NaN    True\n        2   Lewis  21.0    True\n        3    John  33.0    True\n        4    Myla  26.0   False\n\n        Notice the uncounted NA values:\n\n        >>> df.count()\n        Person    5\n        Age       4\n        Single    5\n        dtype: int64\n\n        Counts for each **row**:\n\n        >>> df.count(axis='columns')\n        0    3\n        1    2\n        2    3\n        3    3\n        4    3\n        dtype: int64\n\n        Counts for one level of a `MultiIndex`:\n\n        >>> df.set_index([\"Person\", \"Single\"]).count(level=\"Person\")\n                Age\n        Person\n        John      2\n        Lewis     1\n        Myla      1\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        if level is not None:\n            return self._count_level(level, axis=axis,\n                                     numeric_only=numeric_only)\n\n        if numeric_only:\n            frame = self._get_numeric_data()\n        else:\n            frame = self\n\n        # GH #423\n        if len(frame._get_axis(axis)) == 0:\n            result = Series(0, index=frame._get_agg_axis(axis))\n        else:\n            if frame._is_mixed_type or frame._data.any_extension_types:\n                # the or any_extension_types is really only hit for single-\n                # column frames with an extension array\n                result = notna(frame).sum(axis=axis)\n            else:\n                # GH13407\n                series_counts = notna(frame).sum(axis=axis)\n                counts = series_counts.values\n                result = Series(counts, index=frame._get_agg_axis(axis))\n\n        return result.astype('int64')\n\n    def _count_level(self, level, axis=0, numeric_only=False):\n        if numeric_only:\n            frame = self._get_numeric_data()\n        else:\n            frame = self\n\n        count_axis = frame._get_axis(axis)\n        agg_axis = frame._get_agg_axis(axis)\n\n        if not isinstance(count_axis, MultiIndex):\n            raise TypeError(\"Can only count levels on hierarchical \"\n                            \"{ax}.\".format(ax=self._get_axis_name(axis)))\n\n        if frame._is_mixed_type:\n            # Since we have mixed types, calling notna(frame.values) might\n            # upcast everything to object\n            mask = notna(frame).values\n        else:\n            # But use the speedup when we have homogeneous dtypes\n            mask = notna(frame.values)\n\n        if axis == 1:\n            # We're transposing the mask rather than frame to avoid potential\n            # upcasts to object, which induces a ~20x slowdown\n            mask = mask.T\n\n        if isinstance(level, compat.string_types):\n            level = count_axis._get_level_number(level)\n\n        level_index = count_axis.levels[level]\n        labels = ensure_int64(count_axis.labels[level])\n        counts = lib.count_level_2d(mask, labels, len(level_index), axis=0)\n\n        result = DataFrame(counts, index=level_index, columns=agg_axis)\n\n        if axis == 1:\n            # Undo our earlier transpose\n            return result.T\n        else:\n            return result\n\n    def _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,\n                filter_type=None, **kwds):\n        if axis is None and filter_type == 'bool':\n            labels = None\n            constructor = None\n        else:\n            # TODO: Make other agg func handle axis=None properly\n            axis = self._get_axis_number(axis)\n            labels = self._get_agg_axis(axis)\n            constructor = self._constructor\n\n        def f(x):\n            return op(x, axis=axis, skipna=skipna, **kwds)\n\n        # exclude timedelta/datetime unless we are uniform types\n        if axis == 1 and self._is_mixed_type and self._is_datelike_mixed_type:\n            numeric_only = True\n\n        if numeric_only is None:\n            try:\n                values = self.values\n                result = f(values)\n\n                if (filter_type == 'bool' and is_object_dtype(values) and\n                        axis is None):\n                    # work around https://github.com/numpy/numpy/issues/10489\n                    # TODO: combine with hasattr(result, 'dtype') further down\n                    # hard since we don't have `values` down there.\n                    result = np.bool_(result)\n            except Exception as e:\n\n                # try by-column first\n                if filter_type is None and axis == 0:\n                    try:\n\n                        # this can end up with a non-reduction\n                        # but not always. if the types are mixed\n                        # with datelike then need to make sure a series\n\n                        # we only end up here if we have not specified\n                        # numeric_only and yet we have tried a\n                        # column-by-column reduction, where we have mixed type.\n                        # So let's just do what we can\n                        from pandas.core.apply import frame_apply\n                        opa = frame_apply(self,\n                                          func=f,\n                                          result_type='expand',\n                                          ignore_failures=True)\n                        result = opa.get_result()\n                        if result.ndim == self.ndim:\n                            result = result.iloc[0]\n                        return result\n                    except Exception:\n                        pass\n\n                if filter_type is None or filter_type == 'numeric':\n                    data = self._get_numeric_data()\n                elif filter_type == 'bool':\n                    data = self._get_bool_data()\n                else:  # pragma: no cover\n                    e = NotImplementedError(\n                        \"Handling exception with filter_type {f} not\"\n                        \"implemented.\".format(f=filter_type))\n                    raise_with_traceback(e)\n                with np.errstate(all='ignore'):\n                    result = f(data.values)\n                labels = data._get_agg_axis(axis)\n        else:\n            if numeric_only:\n                if filter_type is None or filter_type == 'numeric':\n                    data = self._get_numeric_data()\n                elif filter_type == 'bool':\n                    data = self._get_bool_data()\n                else:  # pragma: no cover\n                    msg = (\"Generating numeric_only data with filter_type {f}\"\n                           \"not supported.\".format(f=filter_type))\n                    raise NotImplementedError(msg)\n                values = data.values\n                labels = data._get_agg_axis(axis)\n            else:\n                values = self.values\n            result = f(values)\n\n        if hasattr(result, 'dtype') and is_object_dtype(result.dtype):\n            try:\n                if filter_type is None or filter_type == 'numeric':\n                    result = result.astype(np.float64)\n                elif filter_type == 'bool' and notna(result).all():\n                    result = result.astype(np.bool_)\n            except (ValueError, TypeError):\n\n                # try to coerce to the original dtypes item by item if we can\n                if axis == 0:\n                    result = coerce_to_dtypes(result, self.dtypes)\n\n        if constructor is not None:\n            result = Series(result, index=labels)\n        return result\n\n    def nunique(self, axis=0, dropna=True):\n        \"\"\"\n        Return Series with number of distinct observations over requested\n        axis.\n\n        .. versionadded:: 0.20.0\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n        dropna : boolean, default True\n            Don't include NaN in the counts.\n\n        Returns\n        -------\n        nunique : Series\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [1, 1, 1]})\n        >>> df.nunique()\n        A    3\n        B    1\n\n        >>> df.nunique(axis=1)\n        0    1\n        1    2\n        2    2\n        \"\"\"\n        return self.apply(Series.nunique, axis=axis, dropna=dropna)\n\n    def idxmin(self, axis=0, skipna=True):\n        \"\"\"\n        Return index of first occurrence of minimum over requested axis.\n        NA/null values are excluded.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            0 or 'index' for row-wise, 1 or 'columns' for column-wise\n        skipna : boolean, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n\n        Raises\n        ------\n        ValueError\n            * If the row/column is empty\n\n        Returns\n        -------\n        idxmin : Series\n\n        Notes\n        -----\n        This method is the DataFrame version of ``ndarray.argmin``.\n\n        See Also\n        --------\n        Series.idxmin\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        indices = nanops.nanargmin(self.values, axis=axis, skipna=skipna)\n        index = self._get_axis(axis)\n        result = [index[i] if i >= 0 else np.nan for i in indices]\n        return Series(result, index=self._get_agg_axis(axis))\n\n    def idxmax(self, axis=0, skipna=True):\n        \"\"\"\n        Return index of first occurrence of maximum over requested axis.\n        NA/null values are excluded.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            0 or 'index' for row-wise, 1 or 'columns' for column-wise\n        skipna : boolean, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n\n        Raises\n        ------\n        ValueError\n            * If the row/column is empty\n\n        Returns\n        -------\n        idxmax : Series\n\n        Notes\n        -----\n        This method is the DataFrame version of ``ndarray.argmax``.\n\n        See Also\n        --------\n        Series.idxmax\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        indices = nanops.nanargmax(self.values, axis=axis, skipna=skipna)\n        index = self._get_axis(axis)\n        result = [index[i] if i >= 0 else np.nan for i in indices]\n        return Series(result, index=self._get_agg_axis(axis))\n\n    def _get_agg_axis(self, axis_num):\n        \"\"\" let's be explicit about this \"\"\"\n        if axis_num == 0:\n            return self.columns\n        elif axis_num == 1:\n            return self.index\n        else:\n            raise ValueError('Axis must be 0 or 1 (got %r)' % axis_num)\n\n    def mode(self, axis=0, numeric_only=False, dropna=True):\n        \"\"\"\n        Get the mode(s) of each element along the selected axis.\n\n        The mode of a set of values is the value that appears most often.\n        It can be multiple values.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to iterate over while searching for the mode:\n\n            * 0 or 'index' : get mode of each column\n            * 1 or 'columns' : get mode of each row\n        numeric_only : bool, default False\n            If True, only apply to numeric columns.\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        DataFrame\n            The modes of each column or row.\n\n        See Also\n        --------\n        Series.mode : Return the highest frequency value in a Series.\n        Series.value_counts : Return the counts of values in a Series.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('bird', 2, 2),\n        ...                    ('mammal', 4, np.nan),\n        ...                    ('arthropod', 8, 0),\n        ...                    ('bird', 2, np.nan)],\n        ...                   index=('falcon', 'horse', 'spider', 'ostrich'),\n        ...                   columns=('species', 'legs', 'wings'))\n        >>> df\n                   species  legs  wings\n        falcon        bird     2    2.0\n        horse       mammal     4    NaN\n        spider   arthropod     8    0.0\n        ostrich       bird     2    NaN\n\n        By default, missing values are not considered, and the mode of wings\n        are both 0 and 2. The second row of species and legs contains ``NaN``,\n        because they have only one mode, but the DataFrame has two rows.\n\n        >>> df.mode()\n          species  legs  wings\n        0    bird   2.0    0.0\n        1     NaN   NaN    2.0\n\n        Setting ``dropna=False`` ``NaN`` values are considered and they can be\n        the mode (like for wings).\n\n        >>> df.mode(dropna=False)\n          species  legs  wings\n        0    bird     2    NaN\n\n        Setting ``numeric_only=True``, only the mode of numeric columns is\n        computed, and columns of other types are ignored.\n\n        >>> df.mode(numeric_only=True)\n           legs  wings\n        0   2.0    0.0\n        1   NaN    2.0\n\n        To compute the mode over columns and not rows, use the axis parameter:\n\n        >>> df.mode(axis='columns', numeric_only=True)\n                   0    1\n        falcon   2.0  NaN\n        horse    4.0  NaN\n        spider   0.0  8.0\n        ostrich  2.0  NaN\n        \"\"\"\n        data = self if not numeric_only else self._get_numeric_data()\n\n        def f(s):\n            return s.mode(dropna=dropna)\n\n        return data.apply(f, axis=axis)\n\n    def quantile(self, q=0.5, axis=0, numeric_only=True,\n                 interpolation='linear'):\n        \"\"\"\n        Return values at the given quantile over requested axis.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            0 <= q <= 1, the quantile(s) to compute\n        axis : {0, 1, 'index', 'columns'} (default 0)\n            0 or 'index' for row-wise, 1 or 'columns' for column-wise\n        numeric_only : boolean, default True\n            If False, the quantile of datetime and timedelta data will be\n            computed as well\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            .. versionadded:: 0.18.0\n\n            This optional parameter specifies the interpolation method to use,\n            when the desired quantile lies between two data points `i` and `j`:\n\n            * linear: `i + (j - i) * fraction`, where `fraction` is the\n              fractional part of the index surrounded by `i` and `j`.\n            * lower: `i`.\n            * higher: `j`.\n            * nearest: `i` or `j` whichever is nearest.\n            * midpoint: (`i` + `j`) / 2.\n\n        Returns\n        -------\n        quantiles : Series or DataFrame\n\n            - If ``q`` is an array, a DataFrame will be returned where the\n              index is ``q``, the columns are the columns of self, and the\n              values are the quantiles.\n            - If ``q`` is a float, a Series will be returned where the\n              index is the columns of self and the values are the quantiles.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]),\n                              columns=['a', 'b'])\n        >>> df.quantile(.1)\n        a    1.3\n        b    3.7\n        dtype: float64\n        >>> df.quantile([.1, .5])\n               a     b\n        0.1  1.3   3.7\n        0.5  2.5  55.0\n\n        Specifying `numeric_only=False` will also compute the quantile of\n        datetime and timedelta data.\n\n        >>> df = pd.DataFrame({'A': [1, 2],\n                               'B': [pd.Timestamp('2010'),\n                                     pd.Timestamp('2011')],\n                               'C': [pd.Timedelta('1 days'),\n                                     pd.Timedelta('2 days')]})\n        >>> df.quantile(0.5, numeric_only=False)\n        A                    1.5\n        B    2010-07-02 12:00:00\n        C        1 days 12:00:00\n        Name: 0.5, dtype: object\n\n        See Also\n        --------\n        pandas.core.window.Rolling.quantile\n        numpy.percentile\n        \"\"\"\n        self._check_percentile(q)\n\n        data = self._get_numeric_data() if numeric_only else self\n        axis = self._get_axis_number(axis)\n        is_transposed = axis == 1\n\n        if is_transposed:\n            data = data.T\n\n        result = data._data.quantile(qs=q,\n                                     axis=1,\n                                     interpolation=interpolation,\n                                     transposed=is_transposed)\n\n        if result.ndim == 2:\n            result = self._constructor(result)\n        else:\n            result = self._constructor_sliced(result, name=q)\n\n        if is_transposed:\n            result = result.T\n\n        return result\n\n    def to_timestamp(self, freq=None, how='start', axis=0, copy=True):\n        \"\"\"\n        Cast to DatetimeIndex of timestamps, at *beginning* of period\n\n        Parameters\n        ----------\n        freq : string, default frequency of PeriodIndex\n            Desired frequency\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to convert (the index by default)\n        copy : boolean, default True\n            If false then underlying input data is not copied\n\n        Returns\n        -------\n        df : DataFrame with DatetimeIndex\n        \"\"\"\n        new_data = self._data\n        if copy:\n            new_data = new_data.copy()\n\n        axis = self._get_axis_number(axis)\n        if axis == 0:\n            new_data.set_axis(1, self.index.to_timestamp(freq=freq, how=how))\n        elif axis == 1:\n            new_data.set_axis(0, self.columns.to_timestamp(freq=freq, how=how))\n        else:  # pragma: no cover\n            raise AssertionError('Axis must be 0 or 1. Got {ax!s}'.format(\n                ax=axis))\n\n        return self._constructor(new_data)\n\n    def to_period(self, freq=None, axis=0, copy=True):\n        \"\"\"\n        Convert DataFrame from DatetimeIndex to PeriodIndex with desired\n        frequency (inferred from index if not passed)\n\n        Parameters\n        ----------\n        freq : string, default\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to convert (the index by default)\n        copy : boolean, default True\n            If False then underlying input data is not copied\n\n        Returns\n        -------\n        ts : TimeSeries with PeriodIndex\n        \"\"\"\n        new_data = self._data\n        if copy:\n            new_data = new_data.copy()\n\n        axis = self._get_axis_number(axis)\n        if axis == 0:\n            new_data.set_axis(1, self.index.to_period(freq=freq))\n        elif axis == 1:\n            new_data.set_axis(0, self.columns.to_period(freq=freq))\n        else:  # pragma: no cover\n            raise AssertionError('Axis must be 0 or 1. Got {ax!s}'.format(\n                ax=axis))\n\n        return self._constructor(new_data)\n\n    def isin(self, values):\n        \"\"\"\n        Whether each element in the DataFrame is contained in values.\n\n        Parameters\n        ----------\n        values : iterable, Series, DataFrame or dict\n            The result will only be true at a location if all the\n            labels match. If `values` is a Series, that's the index. If\n            `values` is a dict, the keys must be the column names,\n            which must match. If `values` is a DataFrame,\n            then both the index and column labels must match.\n\n        Returns\n        -------\n        DataFrame\n            DataFrame of booleans showing whether each element in the DataFrame\n            is contained in values.\n\n        See Also\n        --------\n        DataFrame.eq: Equality test for DataFrame.\n        Series.isin: Equivalent method on Series.\n        Series.str.contains: Test if pattern or regex is contained within a\n            string of a Series or Index.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]},\n        ...                   index=['falcon', 'dog'])\n        >>> df\n                num_legs  num_wings\n        falcon         2          2\n        dog            4          0\n\n        When ``values`` is a list check whether every value in the DataFrame\n        is present in the list (which animals have 0 or 2 legs or wings)\n\n        >>> df.isin([0, 2])\n                num_legs  num_wings\n        falcon      True       True\n        dog        False       True\n\n        When ``values`` is a dict, we can pass values to check for each\n        column separately:\n\n        >>> df.isin({'num_wings': [0, 3]})\n                num_legs  num_wings\n        falcon     False      False\n        dog        False       True\n\n        When ``values`` is a Series or DataFrame the index and column must\n        match. Note that 'falcon' does not match based on the number of legs\n        in df2.\n\n        >>> other = pd.DataFrame({'num_legs': [8, 2],'num_wings': [0, 2]},\n        ...                      index=['spider', 'falcon'])\n        >>> df.isin(other)\n                num_legs  num_wings\n        falcon      True       True\n        dog        False      False\n        \"\"\"\n        if isinstance(values, dict):\n            from pandas.core.reshape.concat import concat\n            values = collections.defaultdict(list, values)\n            return concat((self.iloc[:, [i]].isin(values[col])\n                           for i, col in enumerate(self.columns)), axis=1)\n        elif isinstance(values, Series):\n            if not values.index.is_unique:\n                raise ValueError(\"cannot compute isin with \"\n                                 \"a duplicate axis.\")\n            return self.eq(values.reindex_like(self), axis='index')\n        elif isinstance(values, DataFrame):\n            if not (values.columns.is_unique and values.index.is_unique):\n                raise ValueError(\"cannot compute isin with \"\n                                 \"a duplicate axis.\")\n            return self.eq(values.reindex_like(self))\n        else:\n            if not is_list_like(values):\n                raise TypeError(\"only list-like or dict-like objects are \"\n                                \"allowed to be passed to DataFrame.isin(), \"\n                                \"you passed a \"\n                                \"{0!r}\".format(type(values).__name__))\n            return DataFrame(\n                algorithms.isin(self.values.ravel(),\n                                values).reshape(self.shape), self.index,\n                self.columns)\n\n    # ----------------------------------------------------------------------\n    # Add plotting methods to DataFrame\n    plot = CachedAccessor(\"plot\", gfx.FramePlotMethods)\n    hist = gfx.hist_frame\n    boxplot = gfx.boxplot_frame\n\n\nDataFrame._setup_axes(['index', 'columns'], info_axis=1, stat_axis=0,\n                      axes_are_reversed=True, aliases={'rows': 0},\n                      docs={\n                          'index': 'The index (row labels) of the DataFrame.',\n                          'columns': 'The column labels of the DataFrame.'})\nDataFrame._add_numeric_operations()\nDataFrame._add_series_or_dataframe_operations()\n\nops.add_flex_arithmetic_methods(DataFrame)\nops.add_special_arithmetic_methods(DataFrame)\n\n\ndef _arrays_to_mgr(arrays, arr_names, index, columns, dtype=None):\n    \"\"\"\n    Segregate Series based on type and coerce into matrices.\n    Needs to handle a lot of exceptional cases.\n    \"\"\"\n    # figure out the index, if necessary\n    if index is None:\n        index = extract_index(arrays)\n    else:\n        index = ensure_index(index)\n\n    # don't force copy because getting jammed in an ndarray anyway\n    arrays = _homogenize(arrays, index, dtype)\n\n    # from BlockManager perspective\n    axes = [ensure_index(columns), index]\n\n    return create_block_manager_from_arrays(arrays, arr_names, axes)\n\n\ndef extract_index(data):\n    from pandas.core.index import _union_indexes\n\n    index = None\n    if len(data) == 0:\n        index = Index([])\n    elif len(data) > 0:\n        raw_lengths = []\n        indexes = []\n\n        have_raw_arrays = False\n        have_series = False\n        have_dicts = False\n\n        for v in data:\n            if isinstance(v, Series):\n                have_series = True\n                indexes.append(v.index)\n            elif isinstance(v, dict):\n                have_dicts = True\n                indexes.append(list(v.keys()))\n            elif is_list_like(v) and getattr(v, 'ndim', 1) == 1:\n                have_raw_arrays = True\n                raw_lengths.append(len(v))\n\n        if not indexes and not raw_lengths:\n            raise ValueError('If using all scalar values, you must pass'\n                             ' an index')\n\n        if have_series or have_dicts:\n            index = _union_indexes(indexes)\n\n        if have_raw_arrays:\n            lengths = list(set(raw_lengths))\n            if len(lengths) > 1:\n                raise ValueError('arrays must all be same length')\n\n            if have_dicts:\n                raise ValueError('Mixing dicts with non-Series may lead to '\n                                 'ambiguous ordering.')\n\n            if have_series:\n                if lengths[0] != len(index):\n                    msg = ('array length %d does not match index length %d' %\n                           (lengths[0], len(index)))\n                    raise ValueError(msg)\n            else:\n                index = ibase.default_index(lengths[0])\n\n    return ensure_index(index)\n\n\ndef _prep_ndarray(values, copy=True):\n    if not isinstance(values, (np.ndarray, Series, Index)):\n        if len(values) == 0:\n            return np.empty((0, 0), dtype=object)\n\n        def convert(v):\n            return maybe_convert_platform(v)\n\n        # we could have a 1-dim or 2-dim list here\n        # this is equiv of np.asarray, but does object conversion\n        # and platform dtype preservation\n        try:\n            if is_list_like(values[0]) or hasattr(values[0], 'len'):\n                values = np.array([convert(v) for v in values])\n            elif isinstance(values[0], np.ndarray) and values[0].ndim == 0:\n                # GH#21861\n                values = np.array([convert(v) for v in values])\n            else:\n                values = convert(values)\n        except (ValueError, TypeError):\n            values = convert(values)\n\n    else:\n\n        # drop subclass info, do not copy data\n        values = np.asarray(values)\n        if copy:\n            values = values.copy()\n\n    if values.ndim == 1:\n        values = values.reshape((values.shape[0], 1))\n    elif values.ndim != 2:\n        raise ValueError('Must pass 2-d input')\n\n    return values\n\n\ndef _to_arrays(data, columns, coerce_float=False, dtype=None):\n    \"\"\"\n    Return list of arrays, columns\n    \"\"\"\n    if isinstance(data, DataFrame):\n        if columns is not None:\n            arrays = [data._ixs(i, axis=1).values\n                      for i, col in enumerate(data.columns) if col in columns]\n        else:\n            columns = data.columns\n            arrays = [data._ixs(i, axis=1).values for i in range(len(columns))]\n\n        return arrays, columns\n\n    if not len(data):\n        if isinstance(data, np.ndarray):\n            columns = data.dtype.names\n            if columns is not None:\n                return [[]] * len(columns), columns\n        return [], []  # columns if columns is not None else []\n    if isinstance(data[0], (list, tuple)):\n        return _list_to_arrays(data, columns, coerce_float=coerce_float,\n                               dtype=dtype)\n    elif isinstance(data[0], compat.Mapping):\n        return _list_of_dict_to_arrays(data, columns,\n                                       coerce_float=coerce_float, dtype=dtype)\n    elif isinstance(data[0], Series):\n        return _list_of_series_to_arrays(data, columns,\n                                         coerce_float=coerce_float,\n                                         dtype=dtype)\n    elif isinstance(data[0], Categorical):\n        if columns is None:\n            columns = ibase.default_index(len(data))\n        return data, columns\n    elif (isinstance(data, (np.ndarray, Series, Index)) and\n          data.dtype.names is not None):\n\n        columns = list(data.dtype.names)\n        arrays = [data[k] for k in columns]\n        return arrays, columns\n    else:\n        # last ditch effort\n        data = lmap(tuple, data)\n        return _list_to_arrays(data, columns, coerce_float=coerce_float,\n                               dtype=dtype)\n\n\ndef _masked_rec_array_to_mgr(data, index, columns, dtype, copy):\n    \"\"\" extract from a masked rec array and create the manager \"\"\"\n\n    # essentially process a record array then fill it\n    fill_value = data.fill_value\n    fdata = ma.getdata(data)\n    if index is None:\n        index = _get_names_from_index(fdata)\n        if index is None:\n            index = ibase.default_index(len(data))\n    index = ensure_index(index)\n\n    if columns is not None:\n        columns = ensure_index(columns)\n    arrays, arr_columns = _to_arrays(fdata, columns)\n\n    # fill if needed\n    new_arrays = []\n    for fv, arr, col in zip(fill_value, arrays, arr_columns):\n        mask = ma.getmaskarray(data[col])\n        if mask.any():\n            arr, fv = maybe_upcast(arr, fill_value=fv, copy=True)\n            arr[mask] = fv\n        new_arrays.append(arr)\n\n    # create the manager\n    arrays, arr_columns = _reorder_arrays(new_arrays, arr_columns, columns)\n    if columns is None:\n        columns = arr_columns\n\n    mgr = _arrays_to_mgr(arrays, arr_columns, index, columns)\n\n    if copy:\n        mgr = mgr.copy()\n    return mgr\n\n\ndef _reorder_arrays(arrays, arr_columns, columns):\n    # reorder according to the columns\n    if (columns is not None and len(columns) and arr_columns is not None and\n            len(arr_columns)):\n        indexer = ensure_index(arr_columns).get_indexer(columns)\n        arr_columns = ensure_index([arr_columns[i] for i in indexer])\n        arrays = [arrays[i] for i in indexer]\n    return arrays, arr_columns\n\n\ndef _list_to_arrays(data, columns, coerce_float=False, dtype=None):\n    if len(data) > 0 and isinstance(data[0], tuple):\n        content = list(lib.to_object_array_tuples(data).T)\n    else:\n        # list of lists\n        content = list(lib.to_object_array(data).T)\n    return _convert_object_array(content, columns, dtype=dtype,\n                                 coerce_float=coerce_float)\n\n\ndef _list_of_series_to_arrays(data, columns, coerce_float=False, dtype=None):\n    from pandas.core.index import _get_objs_combined_axis\n\n    if columns is None:\n        columns = _get_objs_combined_axis(data, sort=False)\n\n    indexer_cache = {}\n\n    aligned_values = []\n    for s in data:\n        index = getattr(s, 'index', None)\n        if index is None:\n            index = ibase.default_index(len(s))\n\n        if id(index) in indexer_cache:\n            indexer = indexer_cache[id(index)]\n        else:\n            indexer = indexer_cache[id(index)] = index.get_indexer(columns)\n\n        values = com.values_from_object(s)\n        aligned_values.append(algorithms.take_1d(values, indexer))\n\n    values = np.vstack(aligned_values)\n\n    if values.dtype == np.object_:\n        content = list(values.T)\n        return _convert_object_array(content, columns, dtype=dtype,\n                                     coerce_float=coerce_float)\n    else:\n        return values.T, columns\n\n\ndef _list_of_dict_to_arrays(data, columns, coerce_float=False, dtype=None):\n    if columns is None:\n        gen = (list(x.keys()) for x in data)\n        sort = not any(isinstance(d, OrderedDict) for d in data)\n        columns = lib.fast_unique_multiple_list_gen(gen, sort=sort)\n\n    # assure that they are of the base dict class and not of derived\n    # classes\n    data = [(type(d) is dict) and d or dict(d) for d in data]\n\n    content = list(lib.dicts_to_array(data, list(columns)).T)\n    return _convert_object_array(content, columns, dtype=dtype,\n                                 coerce_float=coerce_float)\n\n\ndef _convert_object_array(content, columns, coerce_float=False, dtype=None):\n    if columns is None:\n        columns = ibase.default_index(len(content))\n    else:\n        if len(columns) != len(content):  # pragma: no cover\n            # caller's responsibility to check for this...\n            raise AssertionError('{col:d} columns passed, passed data had '\n                                 '{con} columns'.format(col=len(columns),\n                                                        con=len(content)))\n\n    # provide soft conversion of object dtypes\n    def convert(arr):\n        if dtype != object and dtype != np.object:\n            arr = lib.maybe_convert_objects(arr, try_float=coerce_float)\n            arr = maybe_cast_to_datetime(arr, dtype)\n        return arr\n\n    arrays = [convert(arr) for arr in content]\n\n    return arrays, columns\n\n\ndef _get_names_from_index(data):\n    has_some_name = any(getattr(s, 'name', None) is not None for s in data)\n    if not has_some_name:\n        return ibase.default_index(len(data))\n\n    index = lrange(len(data))\n    count = 0\n    for i, s in enumerate(data):\n        n = getattr(s, 'name', None)\n        if n is not None:\n            index[i] = n\n        else:\n            index[i] = 'Unnamed %d' % count\n            count += 1\n\n    return index\n\n\ndef _homogenize(data, index, dtype=None):\n    from pandas.core.series import _sanitize_array\n\n    oindex = None\n    homogenized = []\n\n    for v in data:\n        if isinstance(v, Series):\n            if dtype is not None:\n                v = v.astype(dtype)\n            if v.index is not index:\n                # Forces alignment. No need to copy data since we\n                # are putting it into an ndarray later\n                v = v.reindex(index, copy=False)\n        else:\n            if isinstance(v, dict):\n                if oindex is None:\n                    oindex = index.astype('O')\n\n                if isinstance(index, (DatetimeIndex, TimedeltaIndex)):\n                    v = com.dict_compat(v)\n                else:\n                    v = dict(v)\n                v = lib.fast_multiget(v, oindex.values, default=np.nan)\n            v = _sanitize_array(v, index, dtype=dtype, copy=False,\n                                raise_cast_failure=False)\n\n        homogenized.append(v)\n\n    return homogenized\n\n\ndef _from_nested_dict(data):\n    # TODO: this should be seriously cythonized\n    new_data = OrderedDict()\n    for index, s in compat.iteritems(data):\n        for col, v in compat.iteritems(s):\n            new_data[col] = new_data.get(col, OrderedDict())\n            new_data[col][index] = v\n    return new_data\n\n\ndef _put_str(s, space):\n    return u'{s}'.format(s=s)[:space].ljust(space)\n"
    },
    {
      "filename": "pandas/core/groupby/generic.py",
      "content": "\"\"\"\nDefine the SeriesGroupBy, DataFrameGroupBy, and PanelGroupBy\nclasses that hold the groupby interfaces (and some implementations).\n\nThese are user facing as the result of the ``df.groupby(...)`` operations,\nwhich here returns a DataFrameGroupBy object.\n\"\"\"\n\nimport collections\nimport copy\nfrom functools import partial\nfrom textwrap import dedent\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import Timestamp, lib\nimport pandas.compat as compat\nfrom pandas.compat import lzip, map\nfrom pandas.compat.numpy import _np_version_under1p13\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import Appender, Substitution\n\nfrom pandas.core.dtypes.cast import maybe_downcast_to_dtype\nfrom pandas.core.dtypes.common import (\n    ensure_int64, ensure_platform_int, is_bool, is_datetimelike,\n    is_integer_dtype, is_interval_dtype, is_numeric_dtype, is_scalar)\nfrom pandas.core.dtypes.missing import isna, notna\n\nimport pandas.core.algorithms as algorithms\nfrom pandas.core.arrays import Categorical\nfrom pandas.core.base import DataError, SpecificationError\nimport pandas.core.common as com\nfrom pandas.core.frame import DataFrame\nfrom pandas.core.generic import NDFrame, _shared_docs\nfrom pandas.core.groupby import base\nfrom pandas.core.groupby.groupby import (\n    GroupBy, _apply_docs, _transform_template)\nfrom pandas.core.index import CategoricalIndex, Index, MultiIndex\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.internals import BlockManager, make_block\nfrom pandas.core.panel import Panel\nfrom pandas.core.series import Series\n\nfrom pandas.plotting._core import boxplot_frame_groupby\n\n\nclass NDFrameGroupBy(GroupBy):\n\n    def _iterate_slices(self):\n        if self.axis == 0:\n            # kludge\n            if self._selection is None:\n                slice_axis = self.obj.columns\n            else:\n                slice_axis = self._selection_list\n            slicer = lambda x: self.obj[x]\n        else:\n            slice_axis = self.obj.index\n            slicer = self.obj.xs\n\n        for val in slice_axis:\n            if val in self.exclusions:\n                continue\n            yield val, slicer(val)\n\n    def _cython_agg_general(self, how, alt=None, numeric_only=True,\n                            min_count=-1):\n        new_items, new_blocks = self._cython_agg_blocks(\n            how, alt=alt, numeric_only=numeric_only, min_count=min_count)\n        return self._wrap_agged_blocks(new_items, new_blocks)\n\n    def _wrap_agged_blocks(self, items, blocks):\n        obj = self._obj_with_exclusions\n\n        new_axes = list(obj._data.axes)\n\n        # more kludge\n        if self.axis == 0:\n            new_axes[0], new_axes[1] = new_axes[1], self.grouper.result_index\n        else:\n            new_axes[self.axis] = self.grouper.result_index\n\n        # Make sure block manager integrity check passes.\n        assert new_axes[0].equals(items)\n        new_axes[0] = items\n\n        mgr = BlockManager(blocks, new_axes)\n\n        new_obj = type(obj)(mgr)\n\n        return self._post_process_cython_aggregate(new_obj)\n\n    _block_agg_axis = 0\n\n    def _cython_agg_blocks(self, how, alt=None, numeric_only=True,\n                           min_count=-1):\n        # TODO: the actual managing of mgr_locs is a PITA\n        # here, it should happen via BlockManager.combine\n\n        data, agg_axis = self._get_data_to_aggregate()\n\n        if numeric_only:\n            data = data.get_numeric_data(copy=False)\n\n        new_blocks = []\n        new_items = []\n        deleted_items = []\n        for block in data.blocks:\n\n            locs = block.mgr_locs.as_array\n            try:\n                result, _ = self.grouper.aggregate(\n                    block.values, how, axis=agg_axis, min_count=min_count)\n            except NotImplementedError:\n                # generally if we have numeric_only=False\n                # and non-applicable functions\n                # try to python agg\n\n                if alt is None:\n                    # we cannot perform the operation\n                    # in an alternate way, exclude the block\n                    deleted_items.append(locs)\n                    continue\n\n                # call our grouper again with only this block\n                from pandas.core.groupby.groupby import groupby\n\n                obj = self.obj[data.items[locs]]\n                s = groupby(obj, self.grouper)\n                result = s.aggregate(lambda x: alt(x, axis=self.axis))\n\n            finally:\n\n                # see if we can cast the block back to the original dtype\n                result = block._try_coerce_and_cast_result(result)\n                newb = block.make_block(result)\n\n            new_items.append(locs)\n            new_blocks.append(newb)\n\n        if len(new_blocks) == 0:\n            raise DataError('No numeric types to aggregate')\n\n        # reset the locs in the blocks to correspond to our\n        # current ordering\n        indexer = np.concatenate(new_items)\n        new_items = data.items.take(np.sort(indexer))\n\n        if len(deleted_items):\n\n            # we need to adjust the indexer to account for the\n            # items we have removed\n            # really should be done in internals :<\n\n            deleted = np.concatenate(deleted_items)\n            ai = np.arange(len(data))\n            mask = np.zeros(len(data))\n            mask[deleted] = 1\n            indexer = (ai - mask.cumsum())[indexer]\n\n        offset = 0\n        for b in new_blocks:\n            loc = len(b.mgr_locs)\n            b.mgr_locs = indexer[offset:(offset + loc)]\n            offset += loc\n\n        return new_items, new_blocks\n\n    def _get_data_to_aggregate(self):\n        obj = self._obj_with_exclusions\n        if self.axis == 0:\n            return obj.swapaxes(0, 1)._data, 1\n        else:\n            return obj._data, self.axis\n\n    def _post_process_cython_aggregate(self, obj):\n        # undoing kludge from below\n        if self.axis == 0:\n            obj = obj.swapaxes(0, 1)\n        return obj\n\n    def aggregate(self, arg, *args, **kwargs):\n\n        _level = kwargs.pop('_level', None)\n        result, how = self._aggregate(arg, _level=_level, *args, **kwargs)\n        if how is None:\n            return result\n\n        if result is None:\n\n            # grouper specific aggregations\n            if self.grouper.nkeys > 1:\n                return self._python_agg_general(arg, *args, **kwargs)\n            else:\n\n                # try to treat as if we are passing a list\n                try:\n                    assert not args and not kwargs\n                    result = self._aggregate_multiple_funcs(\n                        [arg], _level=_level, _axis=self.axis)\n                    result.columns = Index(\n                        result.columns.levels[0],\n                        name=self._selected_obj.columns.name)\n                except Exception:\n                    result = self._aggregate_generic(arg, *args, **kwargs)\n\n        if not self.as_index:\n            self._insert_inaxis_grouper_inplace(result)\n            result.index = np.arange(len(result))\n\n        return result._convert(datetime=True)\n\n    agg = aggregate\n\n    def _aggregate_generic(self, func, *args, **kwargs):\n        if self.grouper.nkeys != 1:\n            raise AssertionError('Number of keys must be 1')\n\n        axis = self.axis\n        obj = self._obj_with_exclusions\n\n        result = {}\n        if axis != obj._info_axis_number:\n            try:\n                for name, data in self:\n                    result[name] = self._try_cast(func(data, *args, **kwargs),\n                                                  data)\n            except Exception:\n                return self._aggregate_item_by_item(func, *args, **kwargs)\n        else:\n            for name in self.indices:\n                try:\n                    data = self.get_group(name, obj=obj)\n                    result[name] = self._try_cast(func(data, *args, **kwargs),\n                                                  data)\n                except Exception:\n                    wrapper = lambda x: func(x, *args, **kwargs)\n                    result[name] = data.apply(wrapper, axis=axis)\n\n        return self._wrap_generic_output(result, obj)\n\n    def _wrap_aggregated_output(self, output, names=None):\n        raise AbstractMethodError(self)\n\n    def _aggregate_item_by_item(self, func, *args, **kwargs):\n        # only for axis==0\n\n        obj = self._obj_with_exclusions\n        result = {}\n        cannot_agg = []\n        errors = None\n        for item in obj:\n            try:\n                data = obj[item]\n                colg = SeriesGroupBy(data, selection=item,\n                                     grouper=self.grouper)\n                result[item] = self._try_cast(\n                    colg.aggregate(func, *args, **kwargs), data)\n            except ValueError:\n                cannot_agg.append(item)\n                continue\n            except TypeError as e:\n                cannot_agg.append(item)\n                errors = e\n                continue\n\n        result_columns = obj.columns\n        if cannot_agg:\n            result_columns = result_columns.drop(cannot_agg)\n\n            # GH6337\n            if not len(result_columns) and errors is not None:\n                raise errors\n\n        return DataFrame(result, columns=result_columns)\n\n    def _decide_output_index(self, output, labels):\n        if len(output) == len(labels):\n            output_keys = labels\n        else:\n            output_keys = sorted(output)\n            try:\n                output_keys.sort()\n            except Exception:  # pragma: no cover\n                pass\n\n            if isinstance(labels, MultiIndex):\n                output_keys = MultiIndex.from_tuples(output_keys,\n                                                     names=labels.names)\n\n        return output_keys\n\n    def _wrap_applied_output(self, keys, values, not_indexed_same=False):\n        from pandas.core.index import _all_indexes_same\n        from pandas.core.tools.numeric import to_numeric\n\n        if len(keys) == 0:\n            return DataFrame(index=keys)\n\n        key_names = self.grouper.names\n\n        # GH12824.\n        def first_not_none(values):\n            try:\n                return next(com._not_none(*values))\n            except StopIteration:\n                return None\n\n        v = first_not_none(values)\n\n        if v is None:\n            # GH9684. If all values are None, then this will throw an error.\n            # We'd prefer it return an empty dataframe.\n            return DataFrame()\n        elif isinstance(v, DataFrame):\n            return self._concat_objects(keys, values,\n                                        not_indexed_same=not_indexed_same)\n        elif self.grouper.groupings is not None:\n            if len(self.grouper.groupings) > 1:\n                key_index = self.grouper.result_index\n\n            else:\n                ping = self.grouper.groupings[0]\n                if len(keys) == ping.ngroups:\n                    key_index = ping.group_index\n                    key_index.name = key_names[0]\n\n                    key_lookup = Index(keys)\n                    indexer = key_lookup.get_indexer(key_index)\n\n                    # reorder the values\n                    values = [values[i] for i in indexer]\n                else:\n\n                    key_index = Index(keys, name=key_names[0])\n\n                # don't use the key indexer\n                if not self.as_index:\n                    key_index = None\n\n            # make Nones an empty object\n            v = first_not_none(values)\n            if v is None:\n                return DataFrame()\n            elif isinstance(v, NDFrame):\n                values = [\n                    x if x is not None else\n                    v._constructor(**v._construct_axes_dict())\n                    for x in values\n                ]\n\n            v = values[0]\n\n            if isinstance(v, (np.ndarray, Index, Series)):\n                if isinstance(v, Series):\n                    applied_index = self._selected_obj._get_axis(self.axis)\n                    all_indexed_same = _all_indexes_same([\n                        x.index for x in values\n                    ])\n                    singular_series = (len(values) == 1 and\n                                       applied_index.nlevels == 1)\n\n                    # GH3596\n                    # provide a reduction (Frame -> Series) if groups are\n                    # unique\n                    if self.squeeze:\n\n                        # assign the name to this series\n                        if singular_series:\n                            values[0].name = keys[0]\n\n                            # GH2893\n                            # we have series in the values array, we want to\n                            # produce a series:\n                            # if any of the sub-series are not indexed the same\n                            # OR we don't have a multi-index and we have only a\n                            # single values\n                            return self._concat_objects(\n                                keys, values, not_indexed_same=not_indexed_same\n                            )\n\n                        # still a series\n                        # path added as of GH 5545\n                        elif all_indexed_same:\n                            from pandas.core.reshape.concat import concat\n                            return concat(values)\n\n                    if not all_indexed_same:\n                        # GH 8467\n                        return self._concat_objects(\n                            keys, values, not_indexed_same=True,\n                        )\n\n                try:\n                    if self.axis == 0:\n                        # GH6124 if the list of Series have a consistent name,\n                        # then propagate that name to the result.\n                        index = v.index.copy()\n                        if index.name is None:\n                            # Only propagate the series name to the result\n                            # if all series have a consistent name.  If the\n                            # series do not have a consistent name, do\n                            # nothing.\n                            names = {v.name for v in values}\n                            if len(names) == 1:\n                                index.name = list(names)[0]\n\n                        # normally use vstack as its faster than concat\n                        # and if we have mi-columns\n                        if (isinstance(v.index, MultiIndex) or\n                                key_index is None or\n                                isinstance(key_index, MultiIndex)):\n                            stacked_values = np.vstack([\n                                np.asarray(v) for v in values\n                            ])\n                            result = DataFrame(stacked_values, index=key_index,\n                                               columns=index)\n                        else:\n                            # GH5788 instead of stacking; concat gets the\n                            # dtypes correct\n                            from pandas.core.reshape.concat import concat\n                            result = concat(values, keys=key_index,\n                                            names=key_index.names,\n                                            axis=self.axis).unstack()\n                            result.columns = index\n                    else:\n                        stacked_values = np.vstack([np.asarray(v)\n                                                    for v in values])\n                        result = DataFrame(stacked_values.T, index=v.index,\n                                           columns=key_index)\n\n                except (ValueError, AttributeError):\n                    # GH1738: values is list of arrays of unequal lengths fall\n                    # through to the outer else caluse\n                    return Series(values, index=key_index,\n                                  name=self._selection_name)\n\n                # if we have date/time like in the original, then coerce dates\n                # as we are stacking can easily have object dtypes here\n                so = self._selected_obj\n                if (so.ndim == 2 and so.dtypes.apply(is_datetimelike).any()):\n                    result = result.apply(\n                        lambda x: to_numeric(x, errors='ignore'))\n                    date_cols = self._selected_obj.select_dtypes(\n                        include=['datetime', 'timedelta']).columns\n                    date_cols = date_cols.intersection(result.columns)\n                    result[date_cols] = (result[date_cols]\n                                         ._convert(datetime=True,\n                                                   coerce=True))\n                else:\n                    result = result._convert(datetime=True)\n\n                return self._reindex_output(result)\n\n            # values are not series or array-like but scalars\n            else:\n                # only coerce dates if we find at least 1 datetime\n                coerce = any(isinstance(x, Timestamp) for x in values)\n                # self._selection_name not passed through to Series as the\n                # result should not take the name of original selection\n                # of columns\n                return (Series(values, index=key_index)\n                        ._convert(datetime=True,\n                                  coerce=coerce))\n\n        else:\n            # Handle cases like BinGrouper\n            return self._concat_objects(keys, values,\n                                        not_indexed_same=not_indexed_same)\n\n    def _transform_general(self, func, *args, **kwargs):\n        from pandas.core.reshape.concat import concat\n\n        applied = []\n        obj = self._obj_with_exclusions\n        gen = self.grouper.get_iterator(obj, axis=self.axis)\n        fast_path, slow_path = self._define_paths(func, *args, **kwargs)\n\n        path = None\n        for name, group in gen:\n            object.__setattr__(group, 'name', name)\n\n            if path is None:\n                # Try slow path and fast path.\n                try:\n                    path, res = self._choose_path(fast_path, slow_path, group)\n                except TypeError:\n                    return self._transform_item_by_item(obj, fast_path)\n                except ValueError:\n                    msg = 'transform must return a scalar value for each group'\n                    raise ValueError(msg)\n            else:\n                res = path(group)\n\n            if isinstance(res, Series):\n\n                # we need to broadcast across the\n                # other dimension; this will preserve dtypes\n                # GH14457\n                if not np.prod(group.shape):\n                    continue\n                elif res.index.is_(obj.index):\n                    r = concat([res] * len(group.columns), axis=1)\n                    r.columns = group.columns\n                    r.index = group.index\n                else:\n                    r = DataFrame(\n                        np.concatenate([res.values] * len(group.index)\n                                       ).reshape(group.shape),\n                        columns=group.columns, index=group.index)\n\n                applied.append(r)\n            else:\n                applied.append(res)\n\n        concat_index = obj.columns if self.axis == 0 else obj.index\n        concatenated = concat(applied, join_axes=[concat_index],\n                              axis=self.axis, verify_integrity=False)\n        return self._set_result_index_ordered(concatenated)\n\n    @Substitution(klass='DataFrame', selected='')\n    @Appender(_transform_template)\n    def transform(self, func, *args, **kwargs):\n\n        # optimized transforms\n        func = self._is_cython_func(func) or func\n        if isinstance(func, compat.string_types):\n            if func in base.cython_transforms:\n                # cythonized transform\n                return getattr(self, func)(*args, **kwargs)\n            else:\n                # cythonized aggregation and merge\n                result = getattr(self, func)(*args, **kwargs)\n        else:\n            return self._transform_general(func, *args, **kwargs)\n\n        # a reduction transform\n        if not isinstance(result, DataFrame):\n            return self._transform_general(func, *args, **kwargs)\n\n        obj = self._obj_with_exclusions\n\n        # nuiscance columns\n        if not result.columns.equals(obj.columns):\n            return self._transform_general(func, *args, **kwargs)\n\n        return self._transform_fast(result, obj, func)\n\n    def _transform_fast(self, result, obj, func_nm):\n        \"\"\"\n        Fast transform path for aggregations\n        \"\"\"\n        # if there were groups with no observations (Categorical only?)\n        # try casting data to original dtype\n        cast = self._transform_should_cast(func_nm)\n\n        # for each col, reshape to to size of original frame\n        # by take operation\n        ids, _, ngroup = self.grouper.group_info\n        output = []\n        for i, _ in enumerate(result.columns):\n            res = algorithms.take_1d(result.iloc[:, i].values, ids)\n            if cast:\n                res = self._try_cast(res, obj.iloc[:, i])\n            output.append(res)\n\n        return DataFrame._from_arrays(output, columns=result.columns,\n                                      index=obj.index)\n\n    def _define_paths(self, func, *args, **kwargs):\n        if isinstance(func, compat.string_types):\n            fast_path = lambda group: getattr(group, func)(*args, **kwargs)\n            slow_path = lambda group: group.apply(\n                lambda x: getattr(x, func)(*args, **kwargs), axis=self.axis)\n        else:\n            fast_path = lambda group: func(group, *args, **kwargs)\n            slow_path = lambda group: group.apply(\n                lambda x: func(x, *args, **kwargs), axis=self.axis)\n        return fast_path, slow_path\n\n    def _choose_path(self, fast_path, slow_path, group):\n        path = slow_path\n        res = slow_path(group)\n\n        # if we make it here, test if we can use the fast path\n        try:\n            res_fast = fast_path(group)\n\n            # verify fast path does not change columns (and names), otherwise\n            # its results cannot be joined with those of the slow path\n            if res_fast.columns != group.columns:\n                return path, res\n            # verify numerical equality with the slow path\n            if res.shape == res_fast.shape:\n                res_r = res.values.ravel()\n                res_fast_r = res_fast.values.ravel()\n                mask = notna(res_r)\n                if (res_r[mask] == res_fast_r[mask]).all():\n                    path = fast_path\n        except Exception:\n            pass\n        return path, res\n\n    def _transform_item_by_item(self, obj, wrapper):\n        # iterate through columns\n        output = {}\n        inds = []\n        for i, col in enumerate(obj):\n            try:\n                output[col] = self[col].transform(wrapper)\n                inds.append(i)\n            except Exception:\n                pass\n\n        if len(output) == 0:  # pragma: no cover\n            raise TypeError('Transform function invalid for data types')\n\n        columns = obj.columns\n        if len(output) < len(obj.columns):\n            columns = columns.take(inds)\n\n        return DataFrame(output, index=obj.index, columns=columns)\n\n    def filter(self, func, dropna=True, *args, **kwargs):  # noqa\n        \"\"\"\n        Return a copy of a DataFrame excluding elements from groups that\n        do not satisfy the boolean criterion specified by func.\n\n        Parameters\n        ----------\n        f : function\n            Function to apply to each subframe. Should return True or False.\n        dropna : Drop groups that do not pass the filter. True by default;\n            if False, groups that evaluate False are filled with NaNs.\n\n        Notes\n        -----\n        Each subframe is endowed the attribute 'name' in case you need to know\n        which group you are working on.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n        ...                           'foo', 'bar'],\n        ...                    'B' : [1, 2, 3, 4, 5, 6],\n        ...                    'C' : [2.0, 5., 8., 1., 2., 9.]})\n        >>> grouped = df.groupby('A')\n        >>> grouped.filter(lambda x: x['B'].mean() > 3.)\n             A  B    C\n        1  bar  2  5.0\n        3  bar  4  1.0\n        5  bar  6  9.0\n\n        Returns\n        -------\n        filtered : DataFrame\n        \"\"\"\n\n        indices = []\n\n        obj = self._selected_obj\n        gen = self.grouper.get_iterator(obj, axis=self.axis)\n\n        for name, group in gen:\n            object.__setattr__(group, 'name', name)\n\n            res = func(group, *args, **kwargs)\n\n            try:\n                res = res.squeeze()\n            except AttributeError:  # allow e.g., scalars and frames to pass\n                pass\n\n            # interpret the result of the filter\n            if is_bool(res) or (is_scalar(res) and isna(res)):\n                if res and notna(res):\n                    indices.append(self._get_index(name))\n            else:\n                # non scalars aren't allowed\n                raise TypeError(\"filter function returned a %s, \"\n                                \"but expected a scalar bool\" %\n                                type(res).__name__)\n\n        return self._apply_filter(indices, dropna)\n\n\nclass SeriesGroupBy(GroupBy):\n    #\n    # Make class defs of attributes on SeriesGroupBy whitelist\n\n    _apply_whitelist = base.series_apply_whitelist\n    for _def_str in base.whitelist_method_generator(\n            GroupBy, Series, _apply_whitelist):\n        exec(_def_str)\n\n    @property\n    def _selection_name(self):\n        \"\"\"\n        since we are a series, we by definition only have\n        a single name, but may be the result of a selection or\n        the name of our object\n        \"\"\"\n        if self._selection is None:\n            return self.obj.name\n        else:\n            return self._selection\n\n    _agg_doc = dedent(\"\"\"\n    Examples\n    --------\n\n    >>> s = pd.Series([1, 2, 3, 4])\n\n    >>> s\n    0    1\n    1    2\n    2    3\n    3    4\n    dtype: int64\n\n    >>> s.groupby([1, 1, 2, 2]).min()\n    1    1\n    2    3\n    dtype: int64\n\n    >>> s.groupby([1, 1, 2, 2]).agg('min')\n    1    1\n    2    3\n    dtype: int64\n\n    >>> s.groupby([1, 1, 2, 2]).agg(['min', 'max'])\n       min  max\n    1    1    2\n    2    3    4\n\n    See Also\n    --------\n    pandas.Series.groupby.apply\n    pandas.Series.groupby.transform\n    pandas.Series.aggregate\n\n    \"\"\")\n\n    @Appender(_apply_docs['template']\n              .format(input='series',\n                      examples=_apply_docs['series_examples']))\n    def apply(self, func, *args, **kwargs):\n        return super(SeriesGroupBy, self).apply(func, *args, **kwargs)\n\n    @Appender(_agg_doc)\n    @Appender(_shared_docs['aggregate'] % dict(\n        klass='Series',\n        versionadded='',\n        axis=''))\n    def aggregate(self, func_or_funcs, *args, **kwargs):\n        _level = kwargs.pop('_level', None)\n        if isinstance(func_or_funcs, compat.string_types):\n            return getattr(self, func_or_funcs)(*args, **kwargs)\n\n        if isinstance(func_or_funcs, compat.Iterable):\n            # Catch instances of lists / tuples\n            # but not the class list / tuple itself.\n            ret = self._aggregate_multiple_funcs(func_or_funcs,\n                                                 (_level or 0) + 1)\n        else:\n            cyfunc = self._is_cython_func(func_or_funcs)\n            if cyfunc and not args and not kwargs:\n                return getattr(self, cyfunc)()\n\n            if self.grouper.nkeys > 1:\n                return self._python_agg_general(func_or_funcs, *args, **kwargs)\n\n            try:\n                return self._python_agg_general(func_or_funcs, *args, **kwargs)\n            except Exception:\n                result = self._aggregate_named(func_or_funcs, *args, **kwargs)\n\n            index = Index(sorted(result), name=self.grouper.names[0])\n            ret = Series(result, index=index)\n\n        if not self.as_index:  # pragma: no cover\n            print('Warning, ignoring as_index=True')\n\n        # _level handled at higher\n        if not _level and isinstance(ret, dict):\n            from pandas import concat\n            ret = concat(ret, axis=1)\n        return ret\n\n    agg = aggregate\n\n    def _aggregate_multiple_funcs(self, arg, _level):\n        if isinstance(arg, dict):\n\n            # show the deprecation, but only if we\n            # have not shown a higher level one\n            # GH 15931\n            if isinstance(self._selected_obj, Series) and _level <= 1:\n                warnings.warn(\n                    (\"using a dict on a Series for aggregation\\n\"\n                     \"is deprecated and will be removed in a future \"\n                     \"version\"),\n                    FutureWarning, stacklevel=3)\n\n            columns = list(arg.keys())\n            arg = list(arg.items())\n        elif any(isinstance(x, (tuple, list)) for x in arg):\n            arg = [(x, x) if not isinstance(x, (tuple, list)) else x\n                   for x in arg]\n\n            # indicated column order\n            columns = lzip(*arg)[0]\n        else:\n            # list of functions / function names\n            columns = []\n            for f in arg:\n                if isinstance(f, compat.string_types):\n                    columns.append(f)\n                else:\n                    # protect against callables without names\n                    columns.append(com.get_callable_name(f))\n            arg = lzip(columns, arg)\n\n        results = {}\n        for name, func in arg:\n            obj = self\n            if name in results:\n                raise SpecificationError('Function names must be unique, '\n                                         'found multiple named %s' % name)\n\n            # reset the cache so that we\n            # only include the named selection\n            if name in self._selected_obj:\n                obj = copy.copy(obj)\n                obj._reset_cache()\n                obj._selection = name\n            results[name] = obj.aggregate(func)\n\n        if any(isinstance(x, DataFrame) for x in compat.itervalues(results)):\n            # let higher level handle\n            if _level:\n                return results\n\n        return DataFrame(results, columns=columns)\n\n    def _wrap_output(self, output, index, names=None):\n        \"\"\" common agg/transform wrapping logic \"\"\"\n        output = output[self._selection_name]\n\n        if names is not None:\n            return DataFrame(output, index=index, columns=names)\n        else:\n            name = self._selection_name\n            if name is None:\n                name = self._selected_obj.name\n            return Series(output, index=index, name=name)\n\n    def _wrap_aggregated_output(self, output, names=None):\n        return self._wrap_output(output=output,\n                                 index=self.grouper.result_index,\n                                 names=names)\n\n    def _wrap_transformed_output(self, output, names=None):\n        return self._wrap_output(output=output,\n                                 index=self.obj.index,\n                                 names=names)\n\n    def _wrap_applied_output(self, keys, values, not_indexed_same=False):\n        if len(keys) == 0:\n            # GH #6265\n            return Series([], name=self._selection_name, index=keys)\n\n        def _get_index():\n            if self.grouper.nkeys > 1:\n                index = MultiIndex.from_tuples(keys, names=self.grouper.names)\n            else:\n                index = Index(keys, name=self.grouper.names[0])\n            return index\n\n        if isinstance(values[0], dict):\n            # GH #823\n            index = _get_index()\n            result = DataFrame(values, index=index).stack()\n            result.name = self._selection_name\n            return result\n\n        if isinstance(values[0], (Series, dict)):\n            return self._concat_objects(keys, values,\n                                        not_indexed_same=not_indexed_same)\n        elif isinstance(values[0], DataFrame):\n            # possible that Series -> DataFrame by applied function\n            return self._concat_objects(keys, values,\n                                        not_indexed_same=not_indexed_same)\n        else:\n            # GH #6265\n            return Series(values, index=_get_index(),\n                          name=self._selection_name)\n\n    def _aggregate_named(self, func, *args, **kwargs):\n        result = {}\n\n        for name, group in self:\n            group.name = name\n            output = func(group, *args, **kwargs)\n            if isinstance(output, (Series, Index, np.ndarray)):\n                raise Exception('Must produce aggregated value')\n            result[name] = self._try_cast(output, group)\n\n        return result\n\n    @Substitution(klass='Series', selected='A.')\n    @Appender(_transform_template)\n    def transform(self, func, *args, **kwargs):\n        func = self._is_cython_func(func) or func\n\n        # if string function\n        if isinstance(func, compat.string_types):\n            if func in base.cython_transforms:\n                # cythonized transform\n                return getattr(self, func)(*args, **kwargs)\n            else:\n                # cythonized aggregation and merge\n                return self._transform_fast(\n                    lambda: getattr(self, func)(*args, **kwargs), func)\n\n        # reg transform\n        klass = self._selected_obj.__class__\n        results = []\n        wrapper = lambda x: func(x, *args, **kwargs)\n        for name, group in self:\n            object.__setattr__(group, 'name', name)\n            res = wrapper(group)\n\n            if hasattr(res, 'values'):\n                res = res.values\n\n            indexer = self._get_index(name)\n            s = klass(res, indexer)\n            results.append(s)\n\n        from pandas.core.reshape.concat import concat\n        result = concat(results).sort_index()\n\n        # we will only try to coerce the result type if\n        # we have a numeric dtype, as these are *always* udfs\n        # the cython take a different path (and casting)\n        dtype = self._selected_obj.dtype\n        if is_numeric_dtype(dtype):\n            result = maybe_downcast_to_dtype(result, dtype)\n\n        result.name = self._selected_obj.name\n        result.index = self._selected_obj.index\n        return result\n\n    def _transform_fast(self, func, func_nm):\n        \"\"\"\n        fast version of transform, only applicable to\n        builtin/cythonizable functions\n        \"\"\"\n        if isinstance(func, compat.string_types):\n            func = getattr(self, func)\n\n        ids, _, ngroup = self.grouper.group_info\n        cast = self._transform_should_cast(func_nm)\n        out = algorithms.take_1d(func().values, ids)\n        if cast:\n            out = self._try_cast(out, self.obj)\n        return Series(out, index=self.obj.index, name=self.obj.name)\n\n    def filter(self, func, dropna=True, *args, **kwargs):  # noqa\n        \"\"\"\n        Return a copy of a Series excluding elements from groups that\n        do not satisfy the boolean criterion specified by func.\n\n        Parameters\n        ----------\n        func : function\n            To apply to each group. Should return True or False.\n        dropna : Drop groups that do not pass the filter. True by default;\n            if False, groups that evaluate False are filled with NaNs.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n        ...                           'foo', 'bar'],\n        ...                    'B' : [1, 2, 3, 4, 5, 6],\n        ...                    'C' : [2.0, 5., 8., 1., 2., 9.]})\n        >>> grouped = df.groupby('A')\n        >>> df.groupby('A').B.filter(lambda x: x.mean() > 3.)\n        1    2\n        3    4\n        5    6\n        Name: B, dtype: int64\n\n        Returns\n        -------\n        filtered : Series\n        \"\"\"\n        if isinstance(func, compat.string_types):\n            wrapper = lambda x: getattr(x, func)(*args, **kwargs)\n        else:\n            wrapper = lambda x: func(x, *args, **kwargs)\n\n        # Interpret np.nan as False.\n        def true_and_notna(x, *args, **kwargs):\n            b = wrapper(x, *args, **kwargs)\n            return b and notna(b)\n\n        try:\n            indices = [self._get_index(name) for name, group in self\n                       if true_and_notna(group)]\n        except ValueError:\n            raise TypeError(\"the filter must return a boolean result\")\n        except TypeError:\n            raise TypeError(\"the filter must return a boolean result\")\n\n        filtered = self._apply_filter(indices, dropna)\n        return filtered\n\n    def nunique(self, dropna=True):\n        \"\"\" Returns number of unique elements in the group \"\"\"\n        ids, _, _ = self.grouper.group_info\n\n        val = self.obj.get_values()\n\n        try:\n            sorter = np.lexsort((val, ids))\n        except TypeError:  # catches object dtypes\n            msg = ('val.dtype must be object, got {dtype}'\n                   .format(dtype=val.dtype))\n            assert val.dtype == object, msg\n            val, _ = algorithms.factorize(val, sort=False)\n            sorter = np.lexsort((val, ids))\n            _isna = lambda a: a == -1\n        else:\n            _isna = isna\n\n        ids, val = ids[sorter], val[sorter]\n\n        # group boundaries are where group ids change\n        # unique observations are where sorted values change\n        idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, val[1:] != val[:-1]]\n\n        # 1st item of each group is a new unique observation\n        mask = _isna(val)\n        if dropna:\n            inc[idx] = 1\n            inc[mask] = 0\n        else:\n            inc[mask & np.r_[False, mask[:-1]]] = 0\n            inc[idx] = 1\n\n        out = np.add.reduceat(inc, idx).astype('int64', copy=False)\n        if len(ids):\n            # NaN/NaT group exists if the head of ids is -1,\n            # so remove it from res and exclude its index from idx\n            if ids[0] == -1:\n                res = out[1:]\n                idx = idx[np.flatnonzero(idx)]\n            else:\n                res = out\n        else:\n            res = out[1:]\n        ri = self.grouper.result_index\n\n        # we might have duplications among the bins\n        if len(res) != len(ri):\n            res, out = np.zeros(len(ri), dtype=out.dtype), res\n            res[ids[idx]] = out\n\n        return Series(res,\n                      index=ri,\n                      name=self._selection_name)\n\n    @Appender(Series.describe.__doc__)\n    def describe(self, **kwargs):\n        result = self.apply(lambda x: x.describe(**kwargs))\n        if self.axis == 1:\n            return result.T\n        return result.unstack()\n\n    def value_counts(self, normalize=False, sort=True, ascending=False,\n                     bins=None, dropna=True):\n\n        from pandas.core.reshape.tile import cut\n        from pandas.core.reshape.merge import _get_join_indexers\n\n        if bins is not None and not np.iterable(bins):\n            # scalar bins cannot be done at top level\n            # in a backward compatible way\n            return self.apply(Series.value_counts,\n                              normalize=normalize,\n                              sort=sort,\n                              ascending=ascending,\n                              bins=bins)\n\n        ids, _, _ = self.grouper.group_info\n        val = self.obj.get_values()\n\n        # groupby removes null keys from groupings\n        mask = ids != -1\n        ids, val = ids[mask], val[mask]\n\n        if bins is None:\n            lab, lev = algorithms.factorize(val, sort=True)\n            llab = lambda lab, inc: lab[inc]\n        else:\n\n            # lab is a Categorical with categories an IntervalIndex\n            lab = cut(Series(val), bins, include_lowest=True)\n            lev = lab.cat.categories\n            lab = lev.take(lab.cat.codes)\n            llab = lambda lab, inc: lab[inc]._multiindex.labels[-1]\n\n        if is_interval_dtype(lab):\n            # TODO: should we do this inside II?\n            sorter = np.lexsort((lab.left, lab.right, ids))\n        else:\n            sorter = np.lexsort((lab, ids))\n\n        ids, lab = ids[sorter], lab[sorter]\n\n        # group boundaries are where group ids change\n        idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n\n        # new values are where sorted labels change\n        lchanges = llab(lab, slice(1, None)) != llab(lab, slice(None, -1))\n        inc = np.r_[True, lchanges]\n        inc[idx] = True  # group boundaries are also new values\n        out = np.diff(np.nonzero(np.r_[inc, True])[0])  # value counts\n\n        # num. of times each group should be repeated\n        rep = partial(np.repeat, repeats=np.add.reduceat(inc, idx))\n\n        # multi-index components\n        labels = list(map(rep, self.grouper.recons_labels)) + [llab(lab, inc)]\n        levels = [ping.group_index for ping in self.grouper.groupings] + [lev]\n        names = self.grouper.names + [self._selection_name]\n\n        if dropna:\n            mask = labels[-1] != -1\n            if mask.all():\n                dropna = False\n            else:\n                out, labels = out[mask], [label[mask] for label in labels]\n\n        if normalize:\n            out = out.astype('float')\n            d = np.diff(np.r_[idx, len(ids)])\n            if dropna:\n                m = ids[lab == -1]\n                np.add.at(d, m, -1)\n                acc = rep(d)[mask]\n            else:\n                acc = rep(d)\n            out /= acc\n\n        if sort and bins is None:\n            cat = ids[inc][mask] if dropna else ids[inc]\n            sorter = np.lexsort((out if ascending else -out, cat))\n            out, labels[-1] = out[sorter], labels[-1][sorter]\n\n        if bins is None:\n            mi = MultiIndex(levels=levels, labels=labels, names=names,\n                            verify_integrity=False)\n\n            if is_integer_dtype(out):\n                out = ensure_int64(out)\n            return Series(out, index=mi, name=self._selection_name)\n\n        # for compat. with libgroupby.value_counts need to ensure every\n        # bin is present at every index level, null filled with zeros\n        diff = np.zeros(len(out), dtype='bool')\n        for lab in labels[:-1]:\n            diff |= np.r_[True, lab[1:] != lab[:-1]]\n\n        ncat, nbin = diff.sum(), len(levels[-1])\n\n        left = [np.repeat(np.arange(ncat), nbin),\n                np.tile(np.arange(nbin), ncat)]\n\n        right = [diff.cumsum() - 1, labels[-1]]\n\n        _, idx = _get_join_indexers(left, right, sort=False, how='left')\n        out = np.where(idx != -1, out[idx], 0)\n\n        if sort:\n            sorter = np.lexsort((out if ascending else -out, left[0]))\n            out, left[-1] = out[sorter], left[-1][sorter]\n\n        # build the multi-index w/ full levels\n        labels = list(map(lambda lab: np.repeat(lab[diff], nbin), labels[:-1]))\n        labels.append(left[-1])\n\n        mi = MultiIndex(levels=levels, labels=labels, names=names,\n                        verify_integrity=False)\n\n        if is_integer_dtype(out):\n            out = ensure_int64(out)\n        return Series(out, index=mi, name=self._selection_name)\n\n    def count(self):\n        \"\"\" Compute count of group, excluding missing values \"\"\"\n        ids, _, ngroups = self.grouper.group_info\n        val = self.obj.get_values()\n\n        mask = (ids != -1) & ~isna(val)\n        ids = ensure_platform_int(ids)\n        minlength = ngroups or (None if _np_version_under1p13 else 0)\n        out = np.bincount(ids[mask], minlength=minlength)\n\n        return Series(out,\n                      index=self.grouper.result_index,\n                      name=self._selection_name,\n                      dtype='int64')\n\n    def _apply_to_column_groupbys(self, func):\n        \"\"\" return a pass thru \"\"\"\n        return func(self)\n\n    def pct_change(self, periods=1, fill_method='pad', limit=None, freq=None):\n        \"\"\"Calculate percent change of each value to previous entry in group\"\"\"\n        filled = getattr(self, fill_method)(limit=limit)\n        shifted = filled.shift(periods=periods, freq=freq)\n\n        return (filled / shifted) - 1\n\n\nclass DataFrameGroupBy(NDFrameGroupBy):\n\n    _apply_whitelist = base.dataframe_apply_whitelist\n\n    #\n    # Make class defs of attributes on DataFrameGroupBy whitelist.\n    for _def_str in base.whitelist_method_generator(\n            GroupBy, DataFrame, _apply_whitelist):\n        exec(_def_str)\n\n    _block_agg_axis = 1\n\n    _agg_doc = dedent(\"\"\"\n    Examples\n    --------\n\n    >>> df = pd.DataFrame({'A': [1, 1, 2, 2],\n    ...                    'B': [1, 2, 3, 4],\n    ...                    'C': np.random.randn(4)})\n\n    >>> df\n       A  B         C\n    0  1  1  0.362838\n    1  1  2  0.227877\n    2  2  3  1.267767\n    3  2  4 -0.562860\n\n    The aggregation is for each column.\n\n    >>> df.groupby('A').agg('min')\n       B         C\n    A\n    1  1  0.227877\n    2  3 -0.562860\n\n    Multiple aggregations\n\n    >>> df.groupby('A').agg(['min', 'max'])\n        B             C\n      min max       min       max\n    A\n    1   1   2  0.227877  0.362838\n    2   3   4 -0.562860  1.267767\n\n    Select a column for aggregation\n\n    >>> df.groupby('A').B.agg(['min', 'max'])\n       min  max\n    A\n    1    1    2\n    2    3    4\n\n    Different aggregations per column\n\n    >>> df.groupby('A').agg({'B': ['min', 'max'], 'C': 'sum'})\n        B             C\n      min max       sum\n    A\n    1   1   2  0.590716\n    2   3   4  0.704907\n\n    See Also\n    --------\n    pandas.DataFrame.groupby.apply\n    pandas.DataFrame.groupby.transform\n    pandas.DataFrame.aggregate\n    \"\"\")\n\n    @Appender(_agg_doc)\n    @Appender(_shared_docs['aggregate'] % dict(\n        klass='DataFrame',\n        versionadded='',\n        axis=''))\n    def aggregate(self, arg, *args, **kwargs):\n        return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n\n    agg = aggregate\n\n    def _gotitem(self, key, ndim, subset=None):\n        \"\"\"\n        sub-classes to define\n        return a sliced object\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : 1,2\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n\n        if ndim == 2:\n            if subset is None:\n                subset = self.obj\n            return DataFrameGroupBy(subset, self.grouper, selection=key,\n                                    grouper=self.grouper,\n                                    exclusions=self.exclusions,\n                                    as_index=self.as_index)\n        elif ndim == 1:\n            if subset is None:\n                subset = self.obj[key]\n            return SeriesGroupBy(subset, selection=key,\n                                 grouper=self.grouper)\n\n        raise AssertionError(\"invalid ndim for _gotitem\")\n\n    def _wrap_generic_output(self, result, obj):\n        result_index = self.grouper.levels[0]\n\n        if self.axis == 0:\n            return DataFrame(result, index=obj.columns,\n                             columns=result_index).T\n        else:\n            return DataFrame(result, index=obj.index,\n                             columns=result_index)\n\n    def _get_data_to_aggregate(self):\n        obj = self._obj_with_exclusions\n        if self.axis == 1:\n            return obj.T._data, 1\n        else:\n            return obj._data, 1\n\n    def _insert_inaxis_grouper_inplace(self, result):\n        # zip in reverse so we can always insert at loc 0\n        izip = zip(* map(reversed, (\n            self.grouper.names,\n            self.grouper.get_group_levels(),\n            [grp.in_axis for grp in self.grouper.groupings])))\n\n        for name, lev, in_axis in izip:\n            if in_axis:\n                result.insert(0, name, lev)\n\n    def _wrap_aggregated_output(self, output, names=None):\n        agg_axis = 0 if self.axis == 1 else 1\n        agg_labels = self._obj_with_exclusions._get_axis(agg_axis)\n\n        output_keys = self._decide_output_index(output, agg_labels)\n\n        if not self.as_index:\n            result = DataFrame(output, columns=output_keys)\n            self._insert_inaxis_grouper_inplace(result)\n            result = result._consolidate()\n        else:\n            index = self.grouper.result_index\n            result = DataFrame(output, index=index, columns=output_keys)\n\n        if self.axis == 1:\n            result = result.T\n\n        return self._reindex_output(result)._convert(datetime=True)\n\n    def _wrap_transformed_output(self, output, names=None):\n        return DataFrame(output, index=self.obj.index)\n\n    def _wrap_agged_blocks(self, items, blocks):\n        if not self.as_index:\n            index = np.arange(blocks[0].values.shape[-1])\n            mgr = BlockManager(blocks, [items, index])\n            result = DataFrame(mgr)\n\n            self._insert_inaxis_grouper_inplace(result)\n            result = result._consolidate()\n        else:\n            index = self.grouper.result_index\n            mgr = BlockManager(blocks, [items, index])\n            result = DataFrame(mgr)\n\n        if self.axis == 1:\n            result = result.T\n\n        return self._reindex_output(result)._convert(datetime=True)\n\n    def _reindex_output(self, result):\n        \"\"\"\n        If we have categorical groupers, then we want to make sure that\n        we have a fully reindex-output to the levels. These may have not\n        participated in the groupings (e.g. may have all been\n        nan groups);\n\n        This can re-expand the output space\n        \"\"\"\n\n        # we need to re-expand the output space to accomodate all values\n        # whether observed or not in the cartesian product of our groupes\n        groupings = self.grouper.groupings\n        if groupings is None:\n            return result\n        elif len(groupings) == 1:\n            return result\n\n        # if we only care about the observed values\n        # we are done\n        elif self.observed:\n            return result\n\n        # reindexing only applies to a Categorical grouper\n        elif not any(isinstance(ping.grouper, (Categorical, CategoricalIndex))\n                     for ping in groupings):\n            return result\n\n        levels_list = [ping.group_index for ping in groupings]\n        index, _ = MultiIndex.from_product(\n            levels_list, names=self.grouper.names).sortlevel()\n\n        if self.as_index:\n            d = {self.obj._get_axis_name(self.axis): index, 'copy': False}\n            return result.reindex(**d)\n\n        # GH 13204\n        # Here, the categorical in-axis groupers, which need to be fully\n        # expanded, are columns in `result`. An idea is to do:\n        # result = result.set_index(self.grouper.names)\n        #                .reindex(index).reset_index()\n        # but special care has to be taken because of possible not-in-axis\n        # groupers.\n        # So, we manually select and drop the in-axis grouper columns,\n        # reindex `result`, and then reset the in-axis grouper columns.\n\n        # Select in-axis groupers\n        in_axis_grps = [(i, ping.name) for (i, ping)\n                        in enumerate(groupings) if ping.in_axis]\n        g_nums, g_names = zip(*in_axis_grps)\n\n        result = result.drop(labels=list(g_names), axis=1)\n\n        # Set a temp index and reindex (possibly expanding)\n        result = result.set_index(self.grouper.result_index\n                                  ).reindex(index, copy=False)\n\n        # Reset in-axis grouper columns\n        # (using level numbers `g_nums` because level names may not be unique)\n        result = result.reset_index(level=g_nums)\n\n        return result.reset_index(drop=True)\n\n    def _iterate_column_groupbys(self):\n        for i, colname in enumerate(self._selected_obj.columns):\n            yield colname, SeriesGroupBy(self._selected_obj.iloc[:, i],\n                                         selection=colname,\n                                         grouper=self.grouper,\n                                         exclusions=self.exclusions)\n\n    def _apply_to_column_groupbys(self, func):\n        from pandas.core.reshape.concat import concat\n        return concat(\n            (func(col_groupby) for _, col_groupby\n             in self._iterate_column_groupbys()),\n            keys=self._selected_obj.columns, axis=1)\n\n    def _fill(self, direction, limit=None):\n        \"\"\"Overridden method to join grouped columns in output\"\"\"\n        res = super(DataFrameGroupBy, self)._fill(direction, limit=limit)\n        output = collections.OrderedDict(\n            (grp.name, grp.grouper) for grp in self.grouper.groupings)\n\n        from pandas import concat\n        return concat((self._wrap_transformed_output(output), res), axis=1)\n\n    def count(self):\n        \"\"\" Compute count of group, excluding missing values \"\"\"\n        from pandas.core.dtypes.missing import _isna_ndarraylike as _isna\n\n        data, _ = self._get_data_to_aggregate()\n        ids, _, ngroups = self.grouper.group_info\n        mask = ids != -1\n\n        val = ((mask & ~_isna(np.atleast_2d(blk.get_values())))\n               for blk in data.blocks)\n        loc = (blk.mgr_locs for blk in data.blocks)\n\n        counter = partial(\n            lib.count_level_2d, labels=ids, max_bin=ngroups, axis=1)\n        blk = map(make_block, map(counter, val), loc)\n\n        return self._wrap_agged_blocks(data.items, list(blk))\n\n    def nunique(self, dropna=True):\n        \"\"\"\n        Return DataFrame with number of distinct observations per group for\n        each column.\n\n        .. versionadded:: 0.20.0\n\n        Parameters\n        ----------\n        dropna : boolean, default True\n            Don't include NaN in the counts.\n\n        Returns\n        -------\n        nunique: DataFrame\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',\n        ...                           'ham', 'ham'],\n        ...                    'value1': [1, 5, 5, 2, 5, 5],\n        ...                    'value2': list('abbaxy')})\n        >>> df\n             id  value1 value2\n        0  spam       1      a\n        1   egg       5      b\n        2   egg       5      b\n        3  spam       2      a\n        4   ham       5      x\n        5   ham       5      y\n\n        >>> df.groupby('id').nunique()\n            id  value1  value2\n        id\n        egg    1       1       1\n        ham    1       1       2\n        spam   1       2       1\n\n        # check for rows with the same id but conflicting values\n        >>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())\n             id  value1 value2\n        0  spam       1      a\n        3  spam       2      a\n        4   ham       5      x\n        5   ham       5      y\n        \"\"\"\n\n        obj = self._selected_obj\n\n        def groupby_series(obj, col=None):\n            return SeriesGroupBy(obj,\n                                 selection=col,\n                                 grouper=self.grouper).nunique(dropna=dropna)\n\n        if isinstance(obj, Series):\n            results = groupby_series(obj)\n        else:\n            from pandas.core.reshape.concat import concat\n            results = [groupby_series(obj[col], col) for col in obj.columns]\n            results = concat(results, axis=1)\n\n        if not self.as_index:\n            results.index = ibase.default_index(len(results))\n        return results\n\n    boxplot = boxplot_frame_groupby\n\n\nclass PanelGroupBy(NDFrameGroupBy):\n\n    def aggregate(self, arg, *args, **kwargs):\n        return super(PanelGroupBy, self).aggregate(arg, *args, **kwargs)\n\n    agg = aggregate\n\n    def _iterate_slices(self):\n        if self.axis == 0:\n            # kludge\n            if self._selection is None:\n                slice_axis = self._selected_obj.items\n            else:\n                slice_axis = self._selection_list\n            slicer = lambda x: self._selected_obj[x]\n        else:\n            raise NotImplementedError(\"axis other than 0 is not supported\")\n\n        for val in slice_axis:\n            if val in self.exclusions:\n                continue\n\n            yield val, slicer(val)\n\n    def aggregate(self, arg, *args, **kwargs):\n        \"\"\"\n        Aggregate using input function or dict of {column -> function}\n\n        Parameters\n        ----------\n        arg : function or dict\n            Function to use for aggregating groups. If a function, must either\n            work when passed a Panel or when passed to Panel.apply. If\n            pass a dict, the keys must be DataFrame column names\n\n        Returns\n        -------\n        aggregated : Panel\n        \"\"\"\n        if isinstance(arg, compat.string_types):\n            return getattr(self, arg)(*args, **kwargs)\n\n        return self._aggregate_generic(arg, *args, **kwargs)\n\n    def _wrap_generic_output(self, result, obj):\n        if self.axis == 0:\n            new_axes = list(obj.axes)\n            new_axes[0] = self.grouper.result_index\n        elif self.axis == 1:\n            x, y, z = obj.axes\n            new_axes = [self.grouper.result_index, z, x]\n        else:\n            x, y, z = obj.axes\n            new_axes = [self.grouper.result_index, y, x]\n\n        result = Panel._from_axes(result, new_axes)\n\n        if self.axis == 1:\n            result = result.swapaxes(0, 1).swapaxes(0, 2)\n        elif self.axis == 2:\n            result = result.swapaxes(0, 2)\n\n        return result\n\n    def _aggregate_item_by_item(self, func, *args, **kwargs):\n        obj = self._obj_with_exclusions\n        result = {}\n\n        if self.axis > 0:\n            for item in obj:\n                try:\n                    itemg = DataFrameGroupBy(obj[item],\n                                             axis=self.axis - 1,\n                                             grouper=self.grouper)\n                    result[item] = itemg.aggregate(func, *args, **kwargs)\n                except (ValueError, TypeError):\n                    raise\n            new_axes = list(obj.axes)\n            new_axes[self.axis] = self.grouper.result_index\n            return Panel._from_axes(result, new_axes)\n        else:\n            raise ValueError(\"axis value must be greater than 0\")\n\n    def _wrap_aggregated_output(self, output, names=None):\n        raise AbstractMethodError(self)\n"
    },
    {
      "filename": "pandas/core/groupby/groupby.py",
      "content": "\"\"\"\nProvide the groupby split-apply-combine paradigm. Define the GroupBy\nclass providing the base-class of operations.\n\nThe SeriesGroupBy and DataFrameGroupBy sub-class\n(defined in pandas.core.groupby.generic)\nexpose these user-facing objects to provide specific functionailty.\n\"\"\"\n\nimport collections\nfrom contextlib import contextmanager\nimport datetime\nfrom functools import partial, wraps\nimport types\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import Timestamp, groupby as libgroupby\nimport pandas.compat as compat\nfrom pandas.compat import callable, range, set_function_name, zip\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import Appender, Substitution, cache_readonly\nfrom pandas.util._validators import validate_kwargs\n\nfrom pandas.core.dtypes.cast import maybe_downcast_to_dtype\nfrom pandas.core.dtypes.common import (\n    ensure_float, is_extension_array_dtype, is_numeric_dtype, is_scalar)\nfrom pandas.core.dtypes.missing import isna, notna\n\nimport pandas.core.algorithms as algorithms\nfrom pandas.core.base import (\n    DataError, GroupByError, PandasObject, SelectionMixin, SpecificationError)\nimport pandas.core.common as com\nfrom pandas.core.config import option_context\nfrom pandas.core.frame import DataFrame\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.groupby import base\nfrom pandas.core.index import Index, MultiIndex\nfrom pandas.core.series import Series\nfrom pandas.core.sorting import get_group_index_sorter\n\n_doc_template = \"\"\"\n        See Also\n        --------\n        pandas.Series.%(name)s\n        pandas.DataFrame.%(name)s\n        pandas.Panel.%(name)s\n\"\"\"\n\n_apply_docs = dict(\n    template=\"\"\"\n    Apply function `func`  group-wise and combine the results together.\n\n    The function passed to `apply` must take a {input} as its first\n    argument and return a DataFrame, Series or scalar. `apply` will\n    then take care of combining the results back together into a single\n    dataframe or series. `apply` is therefore a highly flexible\n    grouping method.\n\n    While `apply` is a very flexible method, its downside is that\n    using it can be quite a bit slower than using more specific methods\n    like `agg` or `transform`. Pandas offers a wide range of method that will\n    be much faster than using `apply` for their specific purposes, so try to\n    use them before reaching for `apply`.\n\n    Parameters\n    ----------\n    func : callable\n        A callable that takes a {input} as its first argument, and\n        returns a dataframe, a series or a scalar. In addition the\n        callable may take positional and keyword arguments.\n    args, kwargs : tuple and dict\n        Optional positional and keyword arguments to pass to `func`.\n\n    Returns\n    -------\n    applied : Series or DataFrame\n\n    Notes\n    -----\n    In the current implementation `apply` calls `func` twice on the\n    first group to decide whether it can take a fast or slow code\n    path. This can lead to unexpected behavior if `func` has\n    side-effects, as they will take effect twice for the first\n    group.\n\n    Examples\n    --------\n    {examples}\n\n    See Also\n    --------\n    pipe : Apply function to the full GroupBy object instead of to each\n        group.\n    aggregate : Apply aggregate function to the GroupBy object.\n    transform : Apply function column-by-column to the GroupBy object.\n    Series.apply : Apply a function to a Series.\n    DataFrame.apply : Apply a function to each row or column of a DataFrame.\n    \"\"\",\n    dataframe_examples=\"\"\"\n    >>> df = pd.DataFrame({'A': 'a a b'.split(),\n                           'B': [1,2,3],\n                           'C': [4,6, 5]})\n    >>> g = df.groupby('A')\n\n    Notice that ``g`` has two groups, ``a`` and ``b``.\n    Calling `apply` in various ways, we can get different grouping results:\n\n    Example 1: below the function passed to `apply` takes a DataFrame as\n    its argument and returns a DataFrame. `apply` combines the result for\n    each group together into a new DataFrame:\n\n    >>> g[['B', 'C']].apply(lambda x: x / x.sum())\n              B    C\n    0  0.333333  0.4\n    1  0.666667  0.6\n    2  1.000000  1.0\n\n    Example 2: The function passed to `apply` takes a DataFrame as\n    its argument and returns a Series.  `apply` combines the result for\n    each group together into a new DataFrame:\n\n    >>> g[['B', 'C']].apply(lambda x: x.max() - x.min())\n       B  C\n    A\n    a  1  2\n    b  0  0\n\n    Example 3: The function passed to `apply` takes a DataFrame as\n    its argument and returns a scalar. `apply` combines the result for\n    each group together into a Series, including setting the index as\n    appropriate:\n\n    >>> g.apply(lambda x: x.C.max() - x.B.min())\n    A\n    a    5\n    b    2\n    dtype: int64\n    \"\"\",\n    series_examples=\"\"\"\n    >>> s = pd.Series([0, 1, 2], index='a a b'.split())\n    >>> g = s.groupby(s.index)\n\n    From ``s`` above we can see that ``g`` has two groups, ``a`` and ``b``.\n    Calling `apply` in various ways, we can get different grouping results:\n\n    Example 1: The function passed to `apply` takes a Series as\n    its argument and returns a Series.  `apply` combines the result for\n    each group together into a new Series:\n\n    >>> g.apply(lambda x:  x*2 if x.name == 'b' else x/2)\n    0    0.0\n    1    0.5\n    2    4.0\n    dtype: float64\n\n    Example 2: The function passed to `apply` takes a Series as\n    its argument and returns a scalar. `apply` combines the result for\n    each group together into a Series, including setting the index as\n    appropriate:\n\n    >>> g.apply(lambda x: x.max() - x.min())\n    a    1\n    b    0\n    dtype: int64\n    \"\"\")\n\n_pipe_template = \"\"\"\\\nApply a function `func` with arguments to this %(klass)s object and return\nthe function's result.\n\n%(versionadded)s\n\nUse `.pipe` when you want to improve readability by chaining together\nfunctions that expect Series, DataFrames, GroupBy or Resampler objects.\nInstead of writing\n\n>>> h(g(f(df.groupby('group')), arg1=a), arg2=b, arg3=c)\n\nYou can write\n\n>>> (df.groupby('group')\n...    .pipe(f)\n...    .pipe(g, arg1=a)\n...    .pipe(h, arg2=b, arg3=c))\n\nwhich is much more readable.\n\nParameters\n----------\nfunc : callable or tuple of (callable, string)\n    Function to apply to this %(klass)s object or, alternatively,\n    a `(callable, data_keyword)` tuple where `data_keyword` is a\n    string indicating the keyword of `callable` that expects the\n    %(klass)s object.\nargs : iterable, optional\n       positional arguments passed into `func`.\nkwargs : dict, optional\n         a dictionary of keyword arguments passed into `func`.\n\nReturns\n-------\nobject : the return type of `func`.\n\nNotes\n-----\nSee more `here\n<http://pandas.pydata.org/pandas-docs/stable/groupby.html#piping-function-calls>`_\n\nExamples\n--------\n%(examples)s\n\nSee Also\n--------\npandas.Series.pipe : Apply a function with arguments to a series.\npandas.DataFrame.pipe: Apply a function with arguments to a dataframe.\napply : Apply function to each group instead of to the\n    full %(klass)s object.\n\"\"\"\n\n_transform_template = \"\"\"\nCall function producing a like-indexed %(klass)s on each group and\nreturn a %(klass)s having the same indexes as the original object\nfilled with the transformed values\n\nParameters\n----------\nf : function\n    Function to apply to each group\n\nNotes\n-----\nEach group is endowed the attribute 'name' in case you need to know\nwhich group you are working on.\n\nThe current implementation imposes three requirements on f:\n\n* f must return a value that either has the same shape as the input\n  subframe or can be broadcast to the shape of the input subframe.\n  For example, f returns a scalar it will be broadcast to have the\n  same shape as the input subframe.\n* if this is a DataFrame, f must support application column-by-column\n  in the subframe. If f also supports application to the entire subframe,\n  then a fast path is used starting from the second chunk.\n* f must not mutate groups. Mutation is not supported and may\n  produce unexpected results.\n\nReturns\n-------\n%(klass)s\n\nSee Also\n--------\naggregate, transform\n\nExamples\n--------\n\n# Same shape\n>>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n...                           'foo', 'bar'],\n...                    'B' : ['one', 'one', 'two', 'three',\n...                          'two', 'two'],\n...                    'C' : [1, 5, 5, 2, 5, 5],\n...                    'D' : [2.0, 5., 8., 1., 2., 9.]})\n>>> grouped = df.groupby('A')\n>>> grouped.transform(lambda x: (x - x.mean()) / x.std())\n          C         D\n0 -1.154701 -0.577350\n1  0.577350  0.000000\n2  0.577350  1.154701\n3 -1.154701 -1.000000\n4  0.577350 -0.577350\n5  0.577350  1.000000\n\n# Broadcastable\n>>> grouped.transform(lambda x: x.max() - x.min())\n   C    D\n0  4  6.0\n1  3  8.0\n2  4  6.0\n3  3  8.0\n4  4  6.0\n5  3  8.0\n\n\"\"\"\n\n\nclass GroupByPlot(PandasObject):\n    \"\"\"\n    Class implementing the .plot attribute for groupby objects\n    \"\"\"\n\n    def __init__(self, groupby):\n        self._groupby = groupby\n\n    def __call__(self, *args, **kwargs):\n        def f(self):\n            return self.plot(*args, **kwargs)\n        f.__name__ = 'plot'\n        return self._groupby.apply(f)\n\n    def __getattr__(self, name):\n        def attr(*args, **kwargs):\n            def f(self):\n                return getattr(self.plot, name)(*args, **kwargs)\n            return self._groupby.apply(f)\n        return attr\n\n\n@contextmanager\ndef _group_selection_context(groupby):\n    \"\"\"\n    set / reset the _group_selection_context\n    \"\"\"\n    groupby._set_group_selection()\n    yield groupby\n    groupby._reset_group_selection()\n\n\nclass _GroupBy(PandasObject, SelectionMixin):\n    _group_selection = None\n    _apply_whitelist = frozenset()\n\n    def __init__(self, obj, keys=None, axis=0, level=None,\n                 grouper=None, exclusions=None, selection=None, as_index=True,\n                 sort=True, group_keys=True, squeeze=False,\n                 observed=False, **kwargs):\n\n        self._selection = selection\n\n        if isinstance(obj, NDFrame):\n            obj._consolidate_inplace()\n\n        self.level = level\n\n        if not as_index:\n            if not isinstance(obj, DataFrame):\n                raise TypeError('as_index=False only valid with DataFrame')\n            if axis != 0:\n                raise ValueError('as_index=False only valid for axis=0')\n\n        self.as_index = as_index\n        self.keys = keys\n        self.sort = sort\n        self.group_keys = group_keys\n        self.squeeze = squeeze\n        self.observed = observed\n        self.mutated = kwargs.pop('mutated', False)\n\n        if grouper is None:\n            from pandas.core.groupby.grouper import _get_grouper\n            grouper, exclusions, obj = _get_grouper(obj, keys,\n                                                    axis=axis,\n                                                    level=level,\n                                                    sort=sort,\n                                                    observed=observed,\n                                                    mutated=self.mutated)\n\n        self.obj = obj\n        self.axis = obj._get_axis_number(axis)\n        self.grouper = grouper\n        self.exclusions = set(exclusions) if exclusions else set()\n\n        # we accept no other args\n        validate_kwargs('group', kwargs, {})\n\n    def __len__(self):\n        return len(self.groups)\n\n    def __unicode__(self):\n        # TODO: Better unicode/repr for GroupBy object\n        return object.__repr__(self)\n\n    def _assure_grouper(self):\n        \"\"\"\n        we create the grouper on instantiation\n        sub-classes may have a different policy\n        \"\"\"\n        pass\n\n    @property\n    def groups(self):\n        \"\"\" dict {group name -> group labels} \"\"\"\n        self._assure_grouper()\n        return self.grouper.groups\n\n    @property\n    def ngroups(self):\n        self._assure_grouper()\n        return self.grouper.ngroups\n\n    @property\n    def indices(self):\n        \"\"\" dict {group name -> group indices} \"\"\"\n        self._assure_grouper()\n        return self.grouper.indices\n\n    def _get_indices(self, names):\n        \"\"\"\n        safe get multiple indices, translate keys for\n        datelike to underlying repr\n        \"\"\"\n\n        def get_converter(s):\n            # possibly convert to the actual key types\n            # in the indices, could be a Timestamp or a np.datetime64\n            if isinstance(s, (Timestamp, datetime.datetime)):\n                return lambda key: Timestamp(key)\n            elif isinstance(s, np.datetime64):\n                return lambda key: Timestamp(key).asm8\n            else:\n                return lambda key: key\n\n        if len(names) == 0:\n            return []\n\n        if len(self.indices) > 0:\n            index_sample = next(iter(self.indices))\n        else:\n            index_sample = None     # Dummy sample\n\n        name_sample = names[0]\n        if isinstance(index_sample, tuple):\n            if not isinstance(name_sample, tuple):\n                msg = (\"must supply a tuple to get_group with multiple\"\n                       \" grouping keys\")\n                raise ValueError(msg)\n            if not len(name_sample) == len(index_sample):\n                try:\n                    # If the original grouper was a tuple\n                    return [self.indices[name] for name in names]\n                except KeyError:\n                    # turns out it wasn't a tuple\n                    msg = (\"must supply a a same-length tuple to get_group\"\n                           \" with multiple grouping keys\")\n                    raise ValueError(msg)\n\n            converters = [get_converter(s) for s in index_sample]\n            names = [tuple(f(n) for f, n in zip(converters, name))\n                     for name in names]\n\n        else:\n            converter = get_converter(index_sample)\n            names = [converter(name) for name in names]\n\n        return [self.indices.get(name, []) for name in names]\n\n    def _get_index(self, name):\n        \"\"\" safe get index, translate keys for datelike to underlying repr \"\"\"\n        return self._get_indices([name])[0]\n\n    @cache_readonly\n    def _selected_obj(self):\n\n        if self._selection is None or isinstance(self.obj, Series):\n            if self._group_selection is not None:\n                return self.obj[self._group_selection]\n            return self.obj\n        else:\n            return self.obj[self._selection]\n\n    def _reset_group_selection(self):\n        \"\"\"\n        Clear group based selection. Used for methods needing to return info on\n        each group regardless of whether a group selection was previously set.\n        \"\"\"\n        if self._group_selection is not None:\n            # GH12839 clear cached selection too when changing group selection\n            self._group_selection = None\n            self._reset_cache('_selected_obj')\n\n    def _set_group_selection(self):\n        \"\"\"\n        Create group based selection. Used when selection is not passed\n        directly but instead via a grouper.\n\n        NOTE: this should be paired with a call to _reset_group_selection\n        \"\"\"\n        grp = self.grouper\n        if not (self.as_index and\n                getattr(grp, 'groupings', None) is not None and\n                self.obj.ndim > 1 and\n                self._group_selection is None):\n            return\n\n        ax = self.obj._info_axis\n        groupers = [g.name for g in grp.groupings\n                    if g.level is None and g.in_axis]\n\n        if len(groupers):\n            # GH12839 clear selected obj cache when group selection changes\n            self._group_selection = ax.difference(Index(groupers),\n                                                  sort=False).tolist()\n            self._reset_cache('_selected_obj')\n\n    def _set_result_index_ordered(self, result):\n        # set the result index on the passed values object and\n        # return the new object, xref 8046\n\n        # the values/counts are repeated according to the group index\n        # shortcut if we have an already ordered grouper\n        if not self.grouper.is_monotonic:\n            index = Index(np.concatenate(\n                self._get_indices(self.grouper.result_index)))\n            result.set_axis(index, axis=self.axis, inplace=True)\n            result = result.sort_index(axis=self.axis)\n\n        result.set_axis(self.obj._get_axis(self.axis), axis=self.axis,\n                        inplace=True)\n        return result\n\n    def _dir_additions(self):\n        return self.obj._dir_additions() | self._apply_whitelist\n\n    def __getattr__(self, attr):\n        if attr in self._internal_names_set:\n            return object.__getattribute__(self, attr)\n        if attr in self.obj:\n            return self[attr]\n        if hasattr(self.obj, attr):\n            return self._make_wrapper(attr)\n\n        raise AttributeError(\"%r object has no attribute %r\" %\n                             (type(self).__name__, attr))\n\n    @Substitution(klass='GroupBy',\n                  versionadded='.. versionadded:: 0.21.0',\n                  examples=\"\"\"\\\n>>> df = pd.DataFrame({'A': 'a b a b'.split(), 'B': [1, 2, 3, 4]})\n>>> df\n   A  B\n0  a  1\n1  b  2\n2  a  3\n3  b  4\n\nTo get the difference between each groups maximum and minimum value in one\npass, you can do\n\n>>> df.groupby('A').pipe(lambda x: x.max() - x.min())\n   B\nA\na  2\nb  2\"\"\")\n    @Appender(_pipe_template)\n    def pipe(self, func, *args, **kwargs):\n        return com._pipe(self, func, *args, **kwargs)\n\n    plot = property(GroupByPlot)\n\n    def _make_wrapper(self, name):\n        if name not in self._apply_whitelist:\n            is_callable = callable(getattr(self._selected_obj, name, None))\n            kind = ' callable ' if is_callable else ' '\n            msg = (\"Cannot access{0}attribute {1!r} of {2!r} objects, try \"\n                   \"using the 'apply' method\".format(kind, name,\n                                                     type(self).__name__))\n            raise AttributeError(msg)\n\n        self._set_group_selection()\n\n        # need to setup the selection\n        # as are not passed directly but in the grouper\n        f = getattr(self._selected_obj, name)\n        if not isinstance(f, types.MethodType):\n            return self.apply(lambda self: getattr(self, name))\n\n        f = getattr(type(self._selected_obj), name)\n\n        def wrapper(*args, **kwargs):\n            # a little trickery for aggregation functions that need an axis\n            # argument\n            kwargs_with_axis = kwargs.copy()\n            if ('axis' not in kwargs_with_axis or\n                    kwargs_with_axis['axis'] is None):\n                kwargs_with_axis['axis'] = self.axis\n\n            def curried_with_axis(x):\n                return f(x, *args, **kwargs_with_axis)\n\n            def curried(x):\n                return f(x, *args, **kwargs)\n\n            # preserve the name so we can detect it when calling plot methods,\n            # to avoid duplicates\n            curried.__name__ = curried_with_axis.__name__ = name\n\n            # special case otherwise extra plots are created when catching the\n            # exception below\n            if name in base.plotting_methods:\n                return self.apply(curried)\n\n            try:\n                return self.apply(curried_with_axis)\n            except Exception:\n                try:\n                    return self.apply(curried)\n                except Exception:\n\n                    # related to : GH3688\n                    # try item-by-item\n                    # this can be called recursively, so need to raise\n                    # ValueError\n                    # if we don't have this method to indicated to aggregate to\n                    # mark this column as an error\n                    try:\n                        return self._aggregate_item_by_item(name,\n                                                            *args, **kwargs)\n                    except (AttributeError):\n                        raise ValueError\n\n        return wrapper\n\n    def get_group(self, name, obj=None):\n        \"\"\"\n        Constructs NDFrame from group with provided name\n\n        Parameters\n        ----------\n        name : object\n            the name of the group to get as a DataFrame\n        obj : NDFrame, default None\n            the NDFrame to take the DataFrame out of.  If\n            it is None, the object groupby was called on will\n            be used\n\n        Returns\n        -------\n        group : same type as obj\n        \"\"\"\n        if obj is None:\n            obj = self._selected_obj\n\n        inds = self._get_index(name)\n        if not len(inds):\n            raise KeyError(name)\n\n        return obj._take(inds, axis=self.axis)\n\n    def __iter__(self):\n        \"\"\"\n        Groupby iterator\n\n        Returns\n        -------\n        Generator yielding sequence of (name, subsetted object)\n        for each group\n        \"\"\"\n        return self.grouper.get_iterator(self.obj, axis=self.axis)\n\n    @Appender(_apply_docs['template']\n              .format(input=\"dataframe\",\n                      examples=_apply_docs['dataframe_examples']))\n    def apply(self, func, *args, **kwargs):\n\n        func = self._is_builtin_func(func)\n\n        # this is needed so we don't try and wrap strings. If we could\n        # resolve functions to their callable functions prior, this\n        # wouldn't be needed\n        if args or kwargs:\n            if callable(func):\n\n                @wraps(func)\n                def f(g):\n                    with np.errstate(all='ignore'):\n                        return func(g, *args, **kwargs)\n            else:\n                raise ValueError('func must be a callable if args or '\n                                 'kwargs are supplied')\n        else:\n            f = func\n\n        # ignore SettingWithCopy here in case the user mutates\n        with option_context('mode.chained_assignment', None):\n            try:\n                result = self._python_apply_general(f)\n            except Exception:\n\n                # gh-20949\n                # try again, with .apply acting as a filtering\n                # operation, by excluding the grouping column\n                # This would normally not be triggered\n                # except if the udf is trying an operation that\n                # fails on *some* columns, e.g. a numeric operation\n                # on a string grouper column\n\n                with _group_selection_context(self):\n                    return self._python_apply_general(f)\n\n        return result\n\n    def _python_apply_general(self, f):\n        keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n                                                   self.axis)\n\n        return self._wrap_applied_output(\n            keys,\n            values,\n            not_indexed_same=mutated or self.mutated)\n\n    def _iterate_slices(self):\n        yield self._selection_name, self._selected_obj\n\n    def transform(self, func, *args, **kwargs):\n        raise AbstractMethodError(self)\n\n    def _cumcount_array(self, ascending=True):\n        \"\"\"\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Notes\n        -----\n        this is currently implementing sort=False\n        (though the default is sort=True) for groupby in general\n        \"\"\"\n        ids, _, ngroups = self.grouper.group_info\n        sorter = get_group_index_sorter(ids, ngroups)\n        ids, count = ids[sorter], len(ids)\n\n        if count == 0:\n            return np.empty(0, dtype=np.int64)\n\n        run = np.r_[True, ids[:-1] != ids[1:]]\n        rep = np.diff(np.r_[np.nonzero(run)[0], count])\n        out = (~run).cumsum()\n\n        if ascending:\n            out -= np.repeat(out[run], rep)\n        else:\n            out = np.repeat(out[np.r_[run[1:], True]], rep) - out\n\n        rev = np.empty(count, dtype=np.intp)\n        rev[sorter] = np.arange(count, dtype=np.intp)\n        return out[rev].astype(np.int64, copy=False)\n\n    def _try_cast(self, result, obj, numeric_only=False):\n        \"\"\"\n        try to cast the result to our obj original type,\n        we may have roundtripped thru object in the mean-time\n\n        if numeric_only is True, then only try to cast numerics\n        and not datetimelikes\n\n        \"\"\"\n        if obj.ndim > 1:\n            dtype = obj.values.dtype\n        else:\n            dtype = obj.dtype\n\n        if not is_scalar(result):\n            if is_extension_array_dtype(dtype):\n                # The function can return something of any type, so check\n                # if the type is compatible with the calling EA.\n                try:\n                    result = obj.values._from_sequence(result)\n                except Exception:\n                    # https://github.com/pandas-dev/pandas/issues/22850\n                    # pandas has no control over what 3rd-party ExtensionArrays\n                    # do in _values_from_sequence. We still want ops to work\n                    # though, so we catch any regular Exception.\n                    pass\n            elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:\n                result = maybe_downcast_to_dtype(result, dtype)\n\n        return result\n\n    def _transform_should_cast(self, func_nm):\n        \"\"\"\n        Parameters:\n        -----------\n        func_nm: str\n            The name of the aggregation function being performed\n\n        Returns:\n        --------\n        bool\n            Whether transform should attempt to cast the result of aggregation\n        \"\"\"\n        return (self.size().fillna(0) > 0).any() and (\n            func_nm not in base.cython_cast_blacklist)\n\n    def _cython_transform(self, how, numeric_only=True, **kwargs):\n        output = collections.OrderedDict()\n        for name, obj in self._iterate_slices():\n            is_numeric = is_numeric_dtype(obj.dtype)\n            if numeric_only and not is_numeric:\n                continue\n\n            try:\n                result, names = self.grouper.transform(obj.values, how,\n                                                       **kwargs)\n            except NotImplementedError:\n                continue\n            except AssertionError as e:\n                raise GroupByError(str(e))\n            if self._transform_should_cast(how):\n                output[name] = self._try_cast(result, obj)\n            else:\n                output[name] = result\n\n        if len(output) == 0:\n            raise DataError('No numeric types to aggregate')\n\n        return self._wrap_transformed_output(output, names)\n\n    def _cython_agg_general(self, how, alt=None, numeric_only=True,\n                            min_count=-1):\n        output = {}\n        for name, obj in self._iterate_slices():\n            is_numeric = is_numeric_dtype(obj.dtype)\n            if numeric_only and not is_numeric:\n                continue\n\n            try:\n                result, names = self.grouper.aggregate(obj.values, how,\n                                                       min_count=min_count)\n            except AssertionError as e:\n                raise GroupByError(str(e))\n            output[name] = self._try_cast(result, obj)\n\n        if len(output) == 0:\n            raise DataError('No numeric types to aggregate')\n\n        return self._wrap_aggregated_output(output, names)\n\n    def _python_agg_general(self, func, *args, **kwargs):\n        func = self._is_builtin_func(func)\n        f = lambda x: func(x, *args, **kwargs)\n\n        # iterate through \"columns\" ex exclusions to populate output dict\n        output = {}\n        for name, obj in self._iterate_slices():\n            try:\n                result, counts = self.grouper.agg_series(obj, f)\n                output[name] = self._try_cast(result, obj, numeric_only=True)\n            except TypeError:\n                continue\n\n        if len(output) == 0:\n            return self._python_apply_general(f)\n\n        if self.grouper._filter_empty_groups:\n\n            mask = counts.ravel() > 0\n            for name, result in compat.iteritems(output):\n\n                # since we are masking, make sure that we have a float object\n                values = result\n                if is_numeric_dtype(values.dtype):\n                    values = ensure_float(values)\n\n                output[name] = self._try_cast(values[mask], result)\n\n        return self._wrap_aggregated_output(output)\n\n    def _wrap_applied_output(self, *args, **kwargs):\n        raise AbstractMethodError(self)\n\n    def _concat_objects(self, keys, values, not_indexed_same=False):\n        from pandas.core.reshape.concat import concat\n\n        def reset_identity(values):\n            # reset the identities of the components\n            # of the values to prevent aliasing\n            for v in com._not_none(*values):\n                ax = v._get_axis(self.axis)\n                ax._reset_identity()\n            return values\n\n        if not not_indexed_same:\n            result = concat(values, axis=self.axis)\n            ax = self._selected_obj._get_axis(self.axis)\n\n            if isinstance(result, Series):\n                result = result.reindex(ax)\n            else:\n\n                # this is a very unfortunate situation\n                # we have a multi-index that is NOT lexsorted\n                # and we have a result which is duplicated\n                # we can't reindex, so we resort to this\n                # GH 14776\n                if isinstance(ax, MultiIndex) and not ax.is_unique:\n                    indexer = algorithms.unique1d(\n                        result.index.get_indexer_for(ax.values))\n                    result = result.take(indexer, axis=self.axis)\n                else:\n                    result = result.reindex(ax, axis=self.axis)\n\n        elif self.group_keys:\n\n            values = reset_identity(values)\n            if self.as_index:\n\n                # possible MI return case\n                group_keys = keys\n                group_levels = self.grouper.levels\n                group_names = self.grouper.names\n\n                result = concat(values, axis=self.axis, keys=group_keys,\n                                levels=group_levels, names=group_names,\n                                sort=False)\n            else:\n\n                # GH5610, returns a MI, with the first level being a\n                # range index\n                keys = list(range(len(values)))\n                result = concat(values, axis=self.axis, keys=keys)\n        else:\n            values = reset_identity(values)\n            result = concat(values, axis=self.axis)\n\n        if (isinstance(result, Series) and\n                getattr(self, '_selection_name', None) is not None):\n\n            result.name = self._selection_name\n\n        return result\n\n    def _apply_filter(self, indices, dropna):\n        if len(indices) == 0:\n            indices = np.array([], dtype='int64')\n        else:\n            indices = np.sort(np.concatenate(indices))\n        if dropna:\n            filtered = self._selected_obj.take(indices, axis=self.axis)\n        else:\n            mask = np.empty(len(self._selected_obj.index), dtype=bool)\n            mask.fill(False)\n            mask[indices.astype(int)] = True\n            # mask fails to broadcast when passed to where; broadcast manually.\n            mask = np.tile(mask, list(self._selected_obj.shape[1:]) + [1]).T\n            filtered = self._selected_obj.where(mask)  # Fill with NaNs.\n        return filtered\n\n\nclass GroupBy(_GroupBy):\n\n    \"\"\"\n    Class for grouping and aggregating relational data. See aggregate,\n    transform, and apply functions on this object.\n\n    It's easiest to use obj.groupby(...) to use GroupBy, but you can also do:\n\n    ::\n\n        grouped = groupby(obj, ...)\n\n    Parameters\n    ----------\n    obj : pandas object\n    axis : int, default 0\n    level : int, default None\n        Level of MultiIndex\n    groupings : list of Grouping objects\n        Most users should ignore this\n    exclusions : array-like, optional\n        List of columns to exclude\n    name : string\n        Most users should ignore this\n\n    Notes\n    -----\n    After grouping, see aggregate, apply, and transform functions. Here are\n    some other brief notes about usage. When grouping by multiple groups, the\n    result index will be a MultiIndex (hierarchical) by default.\n\n    Iteration produces (key, group) tuples, i.e. chunking the data by group. So\n    you can write code like:\n\n    ::\n\n        grouped = obj.groupby(keys, axis=axis)\n        for key, group in grouped:\n            # do something with the data\n\n    Function calls on GroupBy, if not specially implemented, \"dispatch\" to the\n    grouped data. So if you group a DataFrame and wish to invoke the std()\n    method on each group, you can simply do:\n\n    ::\n\n        df.groupby(mapper).std()\n\n    rather than\n\n    ::\n\n        df.groupby(mapper).aggregate(np.std)\n\n    You can pass arguments to these \"wrapped\" functions, too.\n\n    See the online documentation for full exposition on these topics and much\n    more\n\n    Returns\n    -------\n    **Attributes**\n    groups : dict\n        {group name -> group labels}\n    len(grouped) : int\n        Number of groups\n    \"\"\"\n    def _bool_agg(self, val_test, skipna):\n        \"\"\"Shared func to call any / all Cython GroupBy implementations\"\"\"\n\n        def objs_to_bool(vals):\n            try:\n                vals = vals.astype(np.bool)\n            except ValueError:  # for objects\n                vals = np.array([bool(x) for x in vals])\n\n            return vals.view(np.uint8)\n\n        def result_to_bool(result):\n            return result.astype(np.bool, copy=False)\n\n        return self._get_cythonized_result('group_any_all', self.grouper,\n                                           aggregate=True,\n                                           cython_dtype=np.uint8,\n                                           needs_values=True,\n                                           needs_mask=True,\n                                           pre_processing=objs_to_bool,\n                                           post_processing=result_to_bool,\n                                           val_test=val_test, skipna=skipna)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def any(self, skipna=True):\n        \"\"\"\n        Returns True if any value in the group is truthful, else False\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Flag to ignore nan values during truth testing\n        \"\"\"\n        return self._bool_agg('any', skipna)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def all(self, skipna=True):\n        \"\"\"\n        Returns True if all values in the group are truthful, else False\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Flag to ignore nan values during truth testing\n        \"\"\"\n        return self._bool_agg('all', skipna)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def count(self):\n        \"\"\"Compute count of group, excluding missing values\"\"\"\n\n        # defined here for API doc\n        raise NotImplementedError\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def mean(self, *args, **kwargs):\n        \"\"\"\n        Compute mean of groups, excluding missing values.\n\n        Returns\n        -------\n        pandas.Series or pandas.DataFrame\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5],\n        ...                    'C': [1, 2, 1, 1, 2]}, columns=['A', 'B', 'C'])\n\n        Groupby one column and return the mean of the remaining columns in\n        each group.\n\n        >>> df.groupby('A').mean()\n        >>>\n             B         C\n        A\n        1  3.0  1.333333\n        2  4.0  1.500000\n\n        Groupby two columns and return the mean of the remaining column.\n\n        >>> df.groupby(['A', 'B']).mean()\n        >>>\n               C\n        A B\n        1 2.0  2\n          4.0  1\n        2 3.0  1\n          5.0  2\n\n        Groupby one column and return the mean of only particular column in\n        the group.\n\n        >>> df.groupby('A')['B'].mean()\n        >>>\n        A\n        1    3.0\n        2    4.0\n        Name: B, dtype: float64\n        \"\"\"\n        nv.validate_groupby_func('mean', args, kwargs, ['numeric_only'])\n        try:\n            return self._cython_agg_general('mean', **kwargs)\n        except GroupByError:\n            raise\n        except Exception:  # pragma: no cover\n            with _group_selection_context(self):\n                f = lambda x: x.mean(axis=self.axis, **kwargs)\n                return self._python_agg_general(f)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def median(self, **kwargs):\n        \"\"\"\n        Compute median of groups, excluding missing values\n\n        For multiple groupings, the result index will be a MultiIndex\n        \"\"\"\n        try:\n            return self._cython_agg_general('median', **kwargs)\n        except GroupByError:\n            raise\n        except Exception:  # pragma: no cover\n\n            def f(x):\n                if isinstance(x, np.ndarray):\n                    x = Series(x)\n                return x.median(axis=self.axis, **kwargs)\n            with _group_selection_context(self):\n                return self._python_agg_general(f)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def std(self, ddof=1, *args, **kwargs):\n        \"\"\"\n        Compute standard deviation of groups, excluding missing values\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Parameters\n        ----------\n        ddof : integer, default 1\n            degrees of freedom\n        \"\"\"\n\n        # TODO: implement at Cython level?\n        nv.validate_groupby_func('std', args, kwargs)\n        return np.sqrt(self.var(ddof=ddof, **kwargs))\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def var(self, ddof=1, *args, **kwargs):\n        \"\"\"\n        Compute variance of groups, excluding missing values\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Parameters\n        ----------\n        ddof : integer, default 1\n            degrees of freedom\n        \"\"\"\n        nv.validate_groupby_func('var', args, kwargs)\n        if ddof == 1:\n            try:\n                return self._cython_agg_general('var', **kwargs)\n            except Exception:\n                f = lambda x: x.var(ddof=ddof, **kwargs)\n                with _group_selection_context(self):\n                    return self._python_agg_general(f)\n        else:\n            f = lambda x: x.var(ddof=ddof, **kwargs)\n            with _group_selection_context(self):\n                return self._python_agg_general(f)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def sem(self, ddof=1):\n        \"\"\"\n        Compute standard error of the mean of groups, excluding missing values\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Parameters\n        ----------\n        ddof : integer, default 1\n            degrees of freedom\n        \"\"\"\n\n        return self.std(ddof=ddof) / np.sqrt(self.count())\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def size(self):\n        \"\"\"Compute group sizes\"\"\"\n        result = self.grouper.size()\n\n        if isinstance(self.obj, Series):\n            result.name = getattr(self.obj, 'name', None)\n        return result\n\n    @classmethod\n    def _add_numeric_operations(cls):\n        \"\"\" add numeric operations to the GroupBy generically \"\"\"\n\n        def groupby_function(name, alias, npfunc,\n                             numeric_only=True, _convert=False,\n                             min_count=-1):\n\n            _local_template = \"Compute %(f)s of group values\"\n\n            @Substitution(name='groupby', f=name)\n            @Appender(_doc_template)\n            @Appender(_local_template)\n            def f(self, **kwargs):\n                if 'numeric_only' not in kwargs:\n                    kwargs['numeric_only'] = numeric_only\n                if 'min_count' not in kwargs:\n                    kwargs['min_count'] = min_count\n\n                self._set_group_selection()\n                try:\n                    return self._cython_agg_general(\n                        alias, alt=npfunc, **kwargs)\n                except AssertionError as e:\n                    raise SpecificationError(str(e))\n                except Exception:\n                    result = self.aggregate(\n                        lambda x: npfunc(x, axis=self.axis))\n                    if _convert:\n                        result = result._convert(datetime=True)\n                    return result\n\n            set_function_name(f, name, cls)\n\n            return f\n\n        def first_compat(x, axis=0):\n\n            def first(x):\n\n                x = np.asarray(x)\n                x = x[notna(x)]\n                if len(x) == 0:\n                    return np.nan\n                return x[0]\n\n            if isinstance(x, DataFrame):\n                return x.apply(first, axis=axis)\n            else:\n                return first(x)\n\n        def last_compat(x, axis=0):\n\n            def last(x):\n\n                x = np.asarray(x)\n                x = x[notna(x)]\n                if len(x) == 0:\n                    return np.nan\n                return x[-1]\n\n            if isinstance(x, DataFrame):\n                return x.apply(last, axis=axis)\n            else:\n                return last(x)\n\n        cls.sum = groupby_function('sum', 'add', np.sum, min_count=0)\n        cls.prod = groupby_function('prod', 'prod', np.prod, min_count=0)\n        cls.min = groupby_function('min', 'min', np.min, numeric_only=False)\n        cls.max = groupby_function('max', 'max', np.max, numeric_only=False)\n        cls.first = groupby_function('first', 'first', first_compat,\n                                     numeric_only=False)\n        cls.last = groupby_function('last', 'last', last_compat,\n                                    numeric_only=False)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def ohlc(self):\n        \"\"\"\n        Compute sum of values, excluding missing values\n        For multiple groupings, the result index will be a MultiIndex\n        \"\"\"\n\n        return self._apply_to_column_groupbys(\n            lambda x: x._cython_agg_general('ohlc'))\n\n    @Appender(DataFrame.describe.__doc__)\n    def describe(self, **kwargs):\n        with _group_selection_context(self):\n            result = self.apply(lambda x: x.describe(**kwargs))\n            if self.axis == 1:\n                return result.T\n            return result.unstack()\n\n    def resample(self, rule, *args, **kwargs):\n        \"\"\"\n        Provide resampling when using a TimeGrouper.\n\n        Given a grouper, the function resamples it according to a string\n        \"string\" -> \"frequency\".\n\n        See the :ref:`frequency aliases <timeseries.offset-aliases>`\n        documentation for more details.\n\n        Parameters\n        ----------\n        rule : str or DateOffset\n            The offset string or object representing target grouper conversion.\n        *args, **kwargs\n            Possible arguments are `how`, `fill_method`, `limit`, `kind` and\n            `on`, and other arguments of `TimeGrouper`.\n\n        Returns\n        -------\n        Grouper\n            Return a new grouper with our resampler appended.\n\n        See Also\n        --------\n        pandas.Grouper : Specify a frequency to resample with when\n            grouping by a key.\n        DatetimeIndex.resample : Frequency conversion and resampling of\n            time series.\n\n        Examples\n        --------\n        >>> idx = pd.date_range('1/1/2000', periods=4, freq='T')\n        >>> df = pd.DataFrame(data=4 * [range(2)],\n        ...                   index=idx,\n        ...                   columns=['a', 'b'])\n        >>> df.iloc[2, 0] = 5\n        >>> df\n                            a  b\n        2000-01-01 00:00:00  0  1\n        2000-01-01 00:01:00  0  1\n        2000-01-01 00:02:00  5  1\n        2000-01-01 00:03:00  0  1\n\n        Downsample the DataFrame into 3 minute bins and sum the values of\n        the timestamps falling into a bin.\n\n        >>> df.groupby('a').resample('3T').sum()\n                                 a  b\n        a\n        0   2000-01-01 00:00:00  0  2\n            2000-01-01 00:03:00  0  1\n        5   2000-01-01 00:00:00  5  1\n\n        Upsample the series into 30 second bins.\n\n        >>> df.groupby('a').resample('30S').sum()\n                            a  b\n        a\n        0   2000-01-01 00:00:00  0  1\n            2000-01-01 00:00:30  0  0\n            2000-01-01 00:01:00  0  1\n            2000-01-01 00:01:30  0  0\n            2000-01-01 00:02:00  0  0\n            2000-01-01 00:02:30  0  0\n            2000-01-01 00:03:00  0  1\n        5   2000-01-01 00:02:00  5  1\n\n        Resample by month. Values are assigned to the month of the period.\n\n        >>> df.groupby('a').resample('M').sum()\n                    a  b\n        a\n        0   2000-01-31  0  3\n        5   2000-01-31  5  1\n\n        Downsample the series into 3 minute bins as above, but close the right\n        side of the bin interval.\n\n        >>> df.groupby('a').resample('3T', closed='right').sum()\n                                 a  b\n        a\n        0   1999-12-31 23:57:00  0  1\n            2000-01-01 00:00:00  0  2\n        5   2000-01-01 00:00:00  5  1\n\n        Downsample the series into 3 minute bins and close the right side of\n        the bin interval, but label each bin using the right edge instead of\n        the left.\n\n        >>> df.groupby('a').resample('3T', closed='right', label='right').sum()\n                                 a  b\n        a\n        0   2000-01-01 00:00:00  0  1\n            2000-01-01 00:03:00  0  2\n        5   2000-01-01 00:03:00  5  1\n\n        Add an offset of twenty seconds.\n\n        >>> df.groupby('a').resample('3T', loffset='20s').sum()\n                               a  b\n        a\n        0   2000-01-01 00:00:20  0  2\n            2000-01-01 00:03:20  0  1\n        5   2000-01-01 00:00:20  5  1\n        \"\"\"\n        from pandas.core.resample import get_resampler_for_grouping\n        return get_resampler_for_grouping(self, rule, *args, **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def rolling(self, *args, **kwargs):\n        \"\"\"\n        Return a rolling grouper, providing rolling\n        functionality per group\n\n        \"\"\"\n        from pandas.core.window import RollingGroupby\n        return RollingGroupby(self, *args, **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def expanding(self, *args, **kwargs):\n        \"\"\"\n        Return an expanding grouper, providing expanding\n        functionality per group\n\n        \"\"\"\n        from pandas.core.window import ExpandingGroupby\n        return ExpandingGroupby(self, *args, **kwargs)\n\n    def _fill(self, direction, limit=None):\n        \"\"\"Shared function for `pad` and `backfill` to call Cython method\n\n        Parameters\n        ----------\n        direction : {'ffill', 'bfill'}\n            Direction passed to underlying Cython function. `bfill` will cause\n            values to be filled backwards. `ffill` and any other values will\n            default to a forward fill\n        limit : int, default None\n            Maximum number of consecutive values to fill. If `None`, this\n            method will convert to -1 prior to passing to Cython\n\n        Returns\n        -------\n        `Series` or `DataFrame` with filled values\n\n        See Also\n        --------\n        pad\n        backfill\n        \"\"\"\n        # Need int value for Cython\n        if limit is None:\n            limit = -1\n\n        return self._get_cythonized_result('group_fillna_indexer',\n                                           self.grouper, needs_mask=True,\n                                           cython_dtype=np.int64,\n                                           result_is_index=True,\n                                           direction=direction, limit=limit)\n\n    @Substitution(name='groupby')\n    def pad(self, limit=None):\n        \"\"\"\n        Forward fill the values\n\n        Parameters\n        ----------\n        limit : integer, optional\n            limit of how many values to fill\n\n        See Also\n        --------\n        Series.pad\n        DataFrame.pad\n        Series.fillna\n        DataFrame.fillna\n        \"\"\"\n        return self._fill('ffill', limit=limit)\n    ffill = pad\n\n    @Substitution(name='groupby')\n    def backfill(self, limit=None):\n        \"\"\"\n        Backward fill the values\n\n        Parameters\n        ----------\n        limit : integer, optional\n            limit of how many values to fill\n\n        See Also\n        --------\n        Series.backfill\n        DataFrame.backfill\n        Series.fillna\n        DataFrame.fillna\n        \"\"\"\n        return self._fill('bfill', limit=limit)\n    bfill = backfill\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def nth(self, n, dropna=None):\n        \"\"\"\n        Take the nth row from each group if n is an int, or a subset of rows\n        if n is a list of ints.\n\n        If dropna, will take the nth non-null row, dropna is either\n        Truthy (if a Series) or 'all', 'any' (if a DataFrame);\n        this is equivalent to calling dropna(how=dropna) before the\n        groupby.\n\n        Parameters\n        ----------\n        n : int or list of ints\n            a single nth value for the row or a list of nth values\n        dropna : None or str, optional\n            apply the specified dropna operation before counting which row is\n            the nth row. Needs to be None, 'any' or 'all'\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5]}, columns=['A', 'B'])\n        >>> g = df.groupby('A')\n        >>> g.nth(0)\n             B\n        A\n        1  NaN\n        2  3.0\n        >>> g.nth(1)\n             B\n        A\n        1  2.0\n        2  5.0\n        >>> g.nth(-1)\n             B\n        A\n        1  4.0\n        2  5.0\n        >>> g.nth([0, 1])\n             B\n        A\n        1  NaN\n        1  2.0\n        2  3.0\n        2  5.0\n\n        Specifying `dropna` allows count ignoring ``NaN``\n\n        >>> g.nth(0, dropna='any')\n             B\n        A\n        1  2.0\n        2  3.0\n\n        NaNs denote group exhausted when using dropna\n\n        >>> g.nth(3, dropna='any')\n            B\n        A\n        1 NaN\n        2 NaN\n\n        Specifying `as_index=False` in `groupby` keeps the original index.\n\n        >>> df.groupby('A', as_index=False).nth(1)\n           A    B\n        1  1  2.0\n        4  2  5.0\n        \"\"\"\n\n        if isinstance(n, int):\n            nth_values = [n]\n        elif isinstance(n, (set, list, tuple)):\n            nth_values = list(set(n))\n            if dropna is not None:\n                raise ValueError(\n                    \"dropna option with a list of nth values is not supported\")\n        else:\n            raise TypeError(\"n needs to be an int or a list/set/tuple of ints\")\n\n        nth_values = np.array(nth_values, dtype=np.intp)\n        self._set_group_selection()\n\n        if not dropna:\n            mask_left = np.in1d(self._cumcount_array(), nth_values)\n            mask_right = np.in1d(self._cumcount_array(ascending=False) + 1,\n                                 -nth_values)\n            mask = mask_left | mask_right\n\n            out = self._selected_obj[mask]\n            if not self.as_index:\n                return out\n\n            ids, _, _ = self.grouper.group_info\n            out.index = self.grouper.result_index[ids[mask]]\n\n            return out.sort_index() if self.sort else out\n\n        if dropna not in ['any', 'all']:\n            if isinstance(self._selected_obj, Series) and dropna is True:\n                warnings.warn(\"the dropna={dropna} keyword is deprecated,\"\n                              \"use dropna='all' instead. \"\n                              \"For a Series groupby, dropna must be \"\n                              \"either None, 'any' or 'all'.\".format(\n                                  dropna=dropna),\n                              FutureWarning,\n                              stacklevel=2)\n                dropna = 'all'\n            else:\n                # Note: when agg-ing picker doesn't raise this,\n                # just returns NaN\n                raise ValueError(\"For a DataFrame groupby, dropna must be \"\n                                 \"either None, 'any' or 'all', \"\n                                 \"(was passed %s).\" % (dropna),)\n\n        # old behaviour, but with all and any support for DataFrames.\n        # modified in GH 7559 to have better perf\n        max_len = n if n >= 0 else - 1 - n\n        dropped = self.obj.dropna(how=dropna, axis=self.axis)\n\n        # get a new grouper for our dropped obj\n        if self.keys is None and self.level is None:\n\n            # we don't have the grouper info available\n            # (e.g. we have selected out\n            # a column that is not in the current object)\n            axis = self.grouper.axis\n            grouper = axis[axis.isin(dropped.index)]\n\n        else:\n\n            # create a grouper with the original parameters, but on the dropped\n            # object\n            from pandas.core.groupby.grouper import _get_grouper\n            grouper, _, _ = _get_grouper(dropped, key=self.keys,\n                                         axis=self.axis, level=self.level,\n                                         sort=self.sort,\n                                         mutated=self.mutated)\n\n        grb = dropped.groupby(grouper, as_index=self.as_index, sort=self.sort)\n        sizes, result = grb.size(), grb.nth(n)\n        mask = (sizes < max_len).values\n\n        # set the results which don't meet the criteria\n        if len(result) and mask.any():\n            result.loc[mask] = np.nan\n\n        # reset/reindex to the original groups\n        if (len(self.obj) == len(dropped) or\n                len(result) == len(self.grouper.result_index)):\n            result.index = self.grouper.result_index\n        else:\n            result = result.reindex(self.grouper.result_index)\n\n        return result\n\n    @Substitution(name='groupby')\n    def ngroup(self, ascending=True):\n        \"\"\"\n        Number each group from 0 to the number of groups - 1.\n\n        This is the enumerative complement of cumcount.  Note that the\n        numbers given to the groups match the order in which the groups\n        would be seen when iterating over the groupby object, not the\n        order they are first observed.\n\n        .. versionadded:: 0.20.2\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from number of group - 1 to 0.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame({\"A\": list(\"aaabba\")})\n        >>> df\n           A\n        0  a\n        1  a\n        2  a\n        3  b\n        4  b\n        5  a\n        >>> df.groupby('A').ngroup()\n        0    0\n        1    0\n        2    0\n        3    1\n        4    1\n        5    0\n        dtype: int64\n        >>> df.groupby('A').ngroup(ascending=False)\n        0    1\n        1    1\n        2    1\n        3    0\n        4    0\n        5    1\n        dtype: int64\n        >>> df.groupby([\"A\", [1,1,2,3,2,1]]).ngroup()\n        0    0\n        1    0\n        2    1\n        3    3\n        4    2\n        5    0\n        dtype: int64\n\n        See Also\n        --------\n        .cumcount : Number the rows in each group.\n        \"\"\"\n\n        with _group_selection_context(self):\n            index = self._selected_obj.index\n            result = Series(self.grouper.group_info[0], index)\n            if not ascending:\n                result = self.ngroups - 1 - result\n            return result\n\n    @Substitution(name='groupby')\n    def cumcount(self, ascending=True):\n        \"\"\"\n        Number each item in each group from 0 to the length of that group - 1.\n\n        Essentially this is equivalent to\n\n        >>> self.apply(lambda x: pd.Series(np.arange(len(x)), x.index))\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([['a'], ['a'], ['a'], ['b'], ['b'], ['a']],\n        ...                   columns=['A'])\n        >>> df\n           A\n        0  a\n        1  a\n        2  a\n        3  b\n        4  b\n        5  a\n        >>> df.groupby('A').cumcount()\n        0    0\n        1    1\n        2    2\n        3    0\n        4    1\n        5    3\n        dtype: int64\n        >>> df.groupby('A').cumcount(ascending=False)\n        0    3\n        1    2\n        2    1\n        3    1\n        4    0\n        5    0\n        dtype: int64\n\n        See Also\n        --------\n        .ngroup : Number the groups themselves.\n        \"\"\"\n\n        with _group_selection_context(self):\n            index = self._selected_obj.index\n            cumcounts = self._cumcount_array(ascending=ascending)\n            return Series(cumcounts, index)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def rank(self, method='average', ascending=True, na_option='keep',\n             pct=False, axis=0):\n        \"\"\"\n        Provides the rank of values within each group.\n\n        Parameters\n        ----------\n        method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n            * average: average rank of group\n            * min: lowest rank in group\n            * max: highest rank in group\n            * first: ranks assigned in order they appear in the array\n            * dense: like 'min', but rank always increases by 1 between groups\n        ascending : boolean, default True\n            False for ranks by high (1) to low (N)\n        na_option :  {'keep', 'top', 'bottom'}, default 'keep'\n            * keep: leave NA values where they are\n            * top: smallest rank if ascending\n            * bottom: smallest rank if descending\n        pct : boolean, default False\n            Compute percentage rank of data within each group\n        axis : int, default 0\n            The axis of the object over which to compute the rank.\n\n        Returns\n        -----\n        DataFrame with ranking of values within each group\n        \"\"\"\n        if na_option not in {'keep', 'top', 'bottom'}:\n            msg = \"na_option must be one of 'keep', 'top', or 'bottom'\"\n            raise ValueError(msg)\n        return self._cython_transform('rank', numeric_only=False,\n                                      ties_method=method, ascending=ascending,\n                                      na_option=na_option, pct=pct, axis=axis)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def cumprod(self, axis=0, *args, **kwargs):\n        \"\"\"Cumulative product for each group\"\"\"\n        nv.validate_groupby_func('cumprod', args, kwargs,\n                                 ['numeric_only', 'skipna'])\n        if axis != 0:\n            return self.apply(lambda x: x.cumprod(axis=axis, **kwargs))\n\n        return self._cython_transform('cumprod', **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def cumsum(self, axis=0, *args, **kwargs):\n        \"\"\"Cumulative sum for each group\"\"\"\n        nv.validate_groupby_func('cumsum', args, kwargs,\n                                 ['numeric_only', 'skipna'])\n        if axis != 0:\n            return self.apply(lambda x: x.cumsum(axis=axis, **kwargs))\n\n        return self._cython_transform('cumsum', **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def cummin(self, axis=0, **kwargs):\n        \"\"\"Cumulative min for each group\"\"\"\n        if axis != 0:\n            return self.apply(lambda x: np.minimum.accumulate(x, axis))\n\n        return self._cython_transform('cummin', numeric_only=False)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def cummax(self, axis=0, **kwargs):\n        \"\"\"Cumulative max for each group\"\"\"\n        if axis != 0:\n            return self.apply(lambda x: np.maximum.accumulate(x, axis))\n\n        return self._cython_transform('cummax', numeric_only=False)\n\n    def _get_cythonized_result(self, how, grouper, aggregate=False,\n                               cython_dtype=None, needs_values=False,\n                               needs_mask=False, needs_ngroups=False,\n                               result_is_index=False,\n                               pre_processing=None, post_processing=None,\n                               **kwargs):\n        \"\"\"Get result for Cythonized functions\n\n        Parameters\n        ----------\n        how : str, Cythonized function name to be called\n        grouper : Grouper object containing pertinent group info\n        aggregate : bool, default False\n            Whether the result should be aggregated to match the number of\n            groups\n        cython_dtype : default None\n            Type of the array that will be modified by the Cython call. If\n            `None`, the type will be inferred from the values of each slice\n        needs_values : bool, default False\n            Whether the values should be a part of the Cython call\n            signature\n        needs_mask : bool, default False\n            Whether boolean mask needs to be part of the Cython call\n            signature\n        needs_ngroups : bool, default False\n            Whether number of groups is part of the Cython call signature\n        result_is_index : bool, default False\n            Whether the result of the Cython operation is an index of\n            values to be retrieved, instead of the actual values themselves\n        pre_processing : function, default None\n            Function to be applied to `values` prior to passing to Cython\n            Raises if `needs_values` is False\n        post_processing : function, default None\n            Function to be applied to result of Cython function\n        **kwargs : dict\n            Extra arguments to be passed back to Cython funcs\n\n        Returns\n        -------\n        `Series` or `DataFrame`  with filled values\n        \"\"\"\n        if result_is_index and aggregate:\n            raise ValueError(\"'result_is_index' and 'aggregate' cannot both \"\n                             \"be True!\")\n        if post_processing:\n            if not callable(pre_processing):\n                raise ValueError(\"'post_processing' must be a callable!\")\n        if pre_processing:\n            if not callable(pre_processing):\n                raise ValueError(\"'pre_processing' must be a callable!\")\n            if not needs_values:\n                raise ValueError(\"Cannot use 'pre_processing' without \"\n                                 \"specifying 'needs_values'!\")\n\n        labels, _, ngroups = grouper.group_info\n        output = collections.OrderedDict()\n        base_func = getattr(libgroupby, how)\n\n        for name, obj in self._iterate_slices():\n            if aggregate:\n                result_sz = ngroups\n            else:\n                result_sz = len(obj.values)\n\n            if not cython_dtype:\n                cython_dtype = obj.values.dtype\n\n            result = np.zeros(result_sz, dtype=cython_dtype)\n            func = partial(base_func, result, labels)\n            if needs_values:\n                vals = obj.values\n                if pre_processing:\n                    vals = pre_processing(vals)\n                func = partial(func, vals)\n\n            if needs_mask:\n                mask = isna(obj.values).view(np.uint8)\n                func = partial(func, mask)\n\n            if needs_ngroups:\n                func = partial(func, ngroups)\n\n            func(**kwargs)  # Call func to modify indexer values in place\n\n            if result_is_index:\n                result = algorithms.take_nd(obj.values, result)\n\n            if post_processing:\n                result = post_processing(result)\n\n            output[name] = result\n\n        if aggregate:\n            return self._wrap_aggregated_output(output)\n        else:\n            return self._wrap_transformed_output(output)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def shift(self, periods=1, freq=None, axis=0):\n        \"\"\"\n        Shift each group by periods observations\n\n        Parameters\n        ----------\n        periods : integer, default 1\n            number of periods to shift\n        freq : frequency string\n        axis : axis to shift, default 0\n        \"\"\"\n\n        if freq is not None or axis != 0:\n            return self.apply(lambda x: x.shift(periods, freq, axis))\n\n        return self._get_cythonized_result('group_shift_indexer',\n                                           self.grouper, cython_dtype=np.int64,\n                                           needs_ngroups=True,\n                                           result_is_index=True,\n                                           periods=periods)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def pct_change(self, periods=1, fill_method='pad', limit=None, freq=None,\n                   axis=0):\n        \"\"\"Calculate pct_change of each value to previous entry in group\"\"\"\n        if freq is not None or axis != 0:\n            return self.apply(lambda x: x.pct_change(periods=periods,\n                                                     fill_method=fill_method,\n                                                     limit=limit, freq=freq,\n                                                     axis=axis))\n\n        filled = getattr(self, fill_method)(limit=limit).drop(\n            self.grouper.names, axis=1)\n        shifted = filled.shift(periods=periods, freq=freq)\n\n        return (filled / shifted) - 1\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def head(self, n=5):\n        \"\"\"\n        Returns first n rows of each group.\n\n        Essentially equivalent to ``.apply(lambda x: x.head(n))``,\n        except ignores as_index flag.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([[1, 2], [1, 4], [5, 6]],\n                              columns=['A', 'B'])\n        >>> df.groupby('A', as_index=False).head(1)\n           A  B\n        0  1  2\n        2  5  6\n        >>> df.groupby('A').head(1)\n           A  B\n        0  1  2\n        2  5  6\n        \"\"\"\n        self._reset_group_selection()\n        mask = self._cumcount_array() < n\n        return self._selected_obj[mask]\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def tail(self, n=5):\n        \"\"\"\n        Returns last n rows of each group\n\n        Essentially equivalent to ``.apply(lambda x: x.tail(n))``,\n        except ignores as_index flag.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([['a', 1], ['a', 2], ['b', 1], ['b', 2]],\n                              columns=['A', 'B'])\n        >>> df.groupby('A').tail(1)\n           A  B\n        1  a  2\n        3  b  2\n        >>> df.groupby('A').head(1)\n           A  B\n        0  a  1\n        2  b  1\n        \"\"\"\n        self._reset_group_selection()\n        mask = self._cumcount_array(ascending=False) < n\n        return self._selected_obj[mask]\n\n\nGroupBy._add_numeric_operations()\n\n\n@Appender(GroupBy.__doc__)\ndef groupby(obj, by, **kwds):\n    if isinstance(obj, Series):\n        from pandas.core.groupby.generic import SeriesGroupBy\n        klass = SeriesGroupBy\n    elif isinstance(obj, DataFrame):\n        from pandas.core.groupby.generic import DataFrameGroupBy\n        klass = DataFrameGroupBy\n    else:  # pragma: no cover\n        raise TypeError('invalid type: %s' % type(obj))\n\n    return klass(obj, by, **kwds)\n"
    },
    {
      "filename": "pandas/core/indexes/base.py",
      "content": "from datetime import datetime, timedelta\nimport operator\nfrom textwrap import dedent\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import (\n    Timedelta, algos as libalgos, index as libindex, join as libjoin, lib,\n    tslibs)\nfrom pandas._libs.lib import is_datetime_array\nimport pandas.compat as compat\nfrom pandas.compat import range, set_function_name, u\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import Appender, Substitution, cache_readonly\n\nfrom pandas.core.dtypes.cast import maybe_cast_to_integer_array\nfrom pandas.core.dtypes.common import (\n    ensure_categorical, ensure_int64, ensure_object, ensure_platform_int,\n    is_bool, is_bool_dtype, is_categorical, is_categorical_dtype,\n    is_datetime64_any_dtype, is_datetime64tz_dtype, is_dtype_equal,\n    is_dtype_union_equal, is_extension_array_dtype, is_float, is_float_dtype,\n    is_hashable, is_integer, is_integer_dtype, is_interval_dtype, is_iterator,\n    is_list_like, is_object_dtype, is_period_dtype, is_scalar,\n    is_signed_integer_dtype, is_timedelta64_dtype, is_unsigned_integer_dtype)\nimport pandas.core.dtypes.concat as _concat\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame, ABCDateOffset, ABCDatetimeIndex, ABCIndexClass,\n    ABCMultiIndex, ABCPeriodIndex, ABCSeries, ABCTimedeltaArray,\n    ABCTimedeltaIndex)\nfrom pandas.core.dtypes.missing import array_equivalent, isna\n\nfrom pandas.core import ops\nfrom pandas.core.accessor import CachedAccessor\nimport pandas.core.algorithms as algos\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.base import IndexOpsMixin, PandasObject\nimport pandas.core.common as com\nfrom pandas.core.indexes.frozen import FrozenList\nimport pandas.core.missing as missing\nfrom pandas.core.ops import get_op_result_name, make_invalid_op\nimport pandas.core.sorting as sorting\nfrom pandas.core.strings import StringMethods\n\nfrom pandas.io.formats.printing import (\n    default_pprint, format_object_attrs, format_object_summary, pprint_thing)\n\n__all__ = ['Index']\n\n_unsortable_types = frozenset(('mixed', 'mixed-integer'))\n\n_index_doc_kwargs = dict(klass='Index', inplace='',\n                         target_klass='Index',\n                         unique='Index', duplicated='np.ndarray')\n_index_shared_docs = dict()\n\n\ndef _try_get_item(x):\n    try:\n        return x.item()\n    except AttributeError:\n        return x\n\n\ndef _make_comparison_op(op, cls):\n    def cmp_method(self, other):\n        if isinstance(other, (np.ndarray, Index, ABCSeries)):\n            if other.ndim > 0 and len(self) != len(other):\n                raise ValueError('Lengths must match to compare')\n\n        from .multi import MultiIndex\n        if is_object_dtype(self) and not isinstance(self, MultiIndex):\n            # don't pass MultiIndex\n            with np.errstate(all='ignore'):\n                result = ops._comp_method_OBJECT_ARRAY(op, self.values, other)\n\n        else:\n\n            # numpy will show a DeprecationWarning on invalid elementwise\n            # comparisons, this will raise in the future\n            with warnings.catch_warnings(record=True):\n                warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                with np.errstate(all='ignore'):\n                    result = op(self.values, np.asarray(other))\n\n        # technically we could support bool dtyped Index\n        # for now just return the indexing array directly\n        if is_bool_dtype(result):\n            return result\n        try:\n            return Index(result)\n        except TypeError:\n            return result\n\n    name = '__{name}__'.format(name=op.__name__)\n    # TODO: docstring?\n    return set_function_name(cmp_method, name, cls)\n\n\ndef _make_arithmetic_op(op, cls):\n    def index_arithmetic_method(self, other):\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n            return NotImplemented\n        elif isinstance(other, ABCTimedeltaIndex):\n            # Defer to subclass implementation\n            return NotImplemented\n        elif (isinstance(other, (np.ndarray, ABCTimedeltaArray)) and\n              is_timedelta64_dtype(other)):\n            # GH#22390; wrap in Series for op, this will in turn wrap in\n            # TimedeltaIndex, but will correctly raise TypeError instead of\n            # NullFrequencyError for add/sub ops\n            from pandas import Series\n            other = Series(other)\n            out = op(self, other)\n            return Index(out, name=self.name)\n\n        other = self._validate_for_numeric_binop(other, op)\n\n        # handle time-based others\n        if isinstance(other, (ABCDateOffset, np.timedelta64, timedelta)):\n            return self._evaluate_with_timedelta_like(other, op)\n        elif isinstance(other, (datetime, np.datetime64)):\n            return self._evaluate_with_datetime_like(other, op)\n\n        values = self.values\n        with np.errstate(all='ignore'):\n            result = op(values, other)\n\n        result = missing.dispatch_missing(op, values, other, result)\n\n        attrs = self._get_attributes_dict()\n        attrs = self._maybe_update_attributes(attrs)\n        if op is divmod:\n            result = (Index(result[0], **attrs), Index(result[1], **attrs))\n        else:\n            result = Index(result, **attrs)\n        return result\n\n    name = '__{name}__'.format(name=op.__name__)\n    # TODO: docstring?\n    return set_function_name(index_arithmetic_method, name, cls)\n\n\nclass InvalidIndexError(Exception):\n    pass\n\n\n_o_dtype = np.dtype(object)\n_Identity = object\n\n\ndef _new_Index(cls, d):\n    \"\"\" This is called upon unpickling, rather than the default which doesn't\n    have arguments and breaks __new__\n    \"\"\"\n    # required for backward compat, because PI can't be instantiated with\n    # ordinals through __new__ GH #13277\n    if issubclass(cls, ABCPeriodIndex):\n        from pandas.core.indexes.period import _new_PeriodIndex\n        return _new_PeriodIndex(cls, **d)\n    return cls.__new__(cls, **d)\n\n\nclass Index(IndexOpsMixin, PandasObject):\n    \"\"\"\n    Immutable ndarray implementing an ordered, sliceable set. The basic object\n    storing axis labels for all pandas objects\n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n    dtype : NumPy dtype (default: object)\n        If dtype is None, we find the dtype that best fits the data.\n        If an actual dtype is provided, we coerce to that dtype if it's safe.\n        Otherwise, an error will be raised.\n    copy : bool\n        Make a copy of input ndarray\n    name : object\n        Name to be stored in the index\n    tupleize_cols : bool (default: True)\n        When True, attempt to create a MultiIndex if possible\n\n    Notes\n    -----\n    An Index instance can **only** contain hashable objects\n\n    Examples\n    --------\n    >>> pd.Index([1, 2, 3])\n    Int64Index([1, 2, 3], dtype='int64')\n\n    >>> pd.Index(list('abc'))\n    Index(['a', 'b', 'c'], dtype='object')\n\n    See Also\n    ---------\n    RangeIndex : Index implementing a monotonic integer range.\n    CategoricalIndex : Index of :class:`Categorical` s.\n    MultiIndex : A multi-level, or hierarchical, Index.\n    IntervalIndex : An Index of :class:`Interval` s.\n    DatetimeIndex, TimedeltaIndex, PeriodIndex\n    Int64Index, UInt64Index,  Float64Index\n    \"\"\"\n    # To hand over control to subclasses\n    _join_precedence = 1\n\n    # Cython methods; see github.com/cython/cython/issues/2647\n    #  for why we need to wrap these instead of making them class attributes\n    # Moreover, cython will choose the appropriate-dtyped sub-function\n    #  given the dtypes of the passed arguments\n    def _left_indexer_unique(self, left, right):\n        return libjoin.left_join_indexer_unique(left, right)\n\n    def _left_indexer(self, left, right):\n        return libjoin.left_join_indexer(left, right)\n\n    def _inner_indexer(self, left, right):\n        return libjoin.inner_join_indexer(left, right)\n\n    def _outer_indexer(self, left, right):\n        return libjoin.outer_join_indexer(left, right)\n\n    _typ = 'index'\n    _data = None\n    _id = None\n    name = None\n    asi8 = None\n    _comparables = ['name']\n    _attributes = ['name']\n    _is_numeric_dtype = False\n    _can_hold_na = True\n\n    # would we like our indexing holder to defer to us\n    _defer_to_indexing = False\n\n    # prioritize current class for _shallow_copy_with_infer,\n    # used to infer integers as datetime-likes\n    _infer_as_myclass = False\n\n    _engine_type = libindex.ObjectEngine\n\n    _accessors = {'str'}\n\n    str = CachedAccessor(\"str\", StringMethods)\n\n    def __new__(cls, data=None, dtype=None, copy=False, name=None,\n                fastpath=None, tupleize_cols=True, **kwargs):\n\n        if name is None and hasattr(data, 'name'):\n            name = data.name\n\n        if fastpath is not None:\n            warnings.warn(\"The 'fastpath' keyword is deprecated, and will be \"\n                          \"removed in a future version.\",\n                          FutureWarning, stacklevel=2)\n            if fastpath:\n                return cls._simple_new(data, name)\n\n        from .range import RangeIndex\n\n        # range\n        if isinstance(data, RangeIndex):\n            return RangeIndex(start=data, copy=copy, dtype=dtype, name=name)\n        elif isinstance(data, range):\n            return RangeIndex.from_range(data, copy=copy, dtype=dtype,\n                                         name=name)\n\n        # categorical\n        elif is_categorical_dtype(data) or is_categorical_dtype(dtype):\n            from .category import CategoricalIndex\n            return CategoricalIndex(data, dtype=dtype, copy=copy, name=name,\n                                    **kwargs)\n\n        # interval\n        elif ((is_interval_dtype(data) or is_interval_dtype(dtype)) and\n              not is_object_dtype(dtype)):\n            from .interval import IntervalIndex\n            closed = kwargs.get('closed', None)\n            return IntervalIndex(data, dtype=dtype, name=name, copy=copy,\n                                 closed=closed)\n\n        elif (is_datetime64_any_dtype(data) or\n              (dtype is not None and is_datetime64_any_dtype(dtype)) or\n                'tz' in kwargs):\n            from pandas import DatetimeIndex\n\n            if dtype is not None and is_dtype_equal(_o_dtype, dtype):\n                # GH#23524 passing `dtype=object` to DatetimeIndex is invalid,\n                #  will raise in the where `data` is already tz-aware.  So\n                #  we leave it out of this step and cast to object-dtype after\n                #  the DatetimeIndex construction.\n                # Note we can pass copy=False because the .astype below\n                #  will always make a copy\n                result = DatetimeIndex(data, copy=False, name=name, **kwargs)\n                return result.astype(object)\n            else:\n                result = DatetimeIndex(data, copy=copy, name=name,\n                                       dtype=dtype, **kwargs)\n                return result\n\n        elif (is_timedelta64_dtype(data) or\n              (dtype is not None and is_timedelta64_dtype(dtype))):\n            from pandas import TimedeltaIndex\n            result = TimedeltaIndex(data, copy=copy, name=name, **kwargs)\n            if dtype is not None and _o_dtype == dtype:\n                return Index(result.to_pytimedelta(), dtype=_o_dtype)\n            else:\n                return result\n\n        elif is_period_dtype(data) and not is_object_dtype(dtype):\n            from pandas import PeriodIndex\n            result = PeriodIndex(data, copy=copy, name=name, **kwargs)\n            return result\n\n        # extension dtype\n        elif is_extension_array_dtype(data) or is_extension_array_dtype(dtype):\n            data = np.asarray(data)\n            if not (dtype is None or is_object_dtype(dtype)):\n\n                # coerce to the provided dtype\n                data = dtype.construct_array_type()._from_sequence(\n                    data, dtype=dtype, copy=False)\n\n            # coerce to the object dtype\n            data = data.astype(object)\n            return Index(data, dtype=object, copy=copy, name=name,\n                         **kwargs)\n\n        # index-like\n        elif isinstance(data, (np.ndarray, Index, ABCSeries)):\n            if dtype is not None:\n                try:\n\n                    # we need to avoid having numpy coerce\n                    # things that look like ints/floats to ints unless\n                    # they are actually ints, e.g. '0' and 0.0\n                    # should not be coerced\n                    # GH 11836\n                    if is_integer_dtype(dtype):\n                        inferred = lib.infer_dtype(data)\n                        if inferred == 'integer':\n                            data = maybe_cast_to_integer_array(data, dtype,\n                                                               copy=copy)\n                        elif inferred in ['floating', 'mixed-integer-float']:\n                            if isna(data).any():\n                                raise ValueError('cannot convert float '\n                                                 'NaN to integer')\n\n                            if inferred == \"mixed-integer-float\":\n                                data = maybe_cast_to_integer_array(data, dtype)\n\n                            # If we are actually all equal to integers,\n                            # then coerce to integer.\n                            try:\n                                return cls._try_convert_to_int_index(\n                                    data, copy, name, dtype)\n                            except ValueError:\n                                pass\n\n                            # Return an actual float index.\n                            from .numeric import Float64Index\n                            return Float64Index(data, copy=copy, dtype=dtype,\n                                                name=name)\n\n                        elif inferred == 'string':\n                            pass\n                        else:\n                            data = data.astype(dtype)\n                    elif is_float_dtype(dtype):\n                        inferred = lib.infer_dtype(data)\n                        if inferred == 'string':\n                            pass\n                        else:\n                            data = data.astype(dtype)\n                    else:\n                        data = np.array(data, dtype=dtype, copy=copy)\n\n                except (TypeError, ValueError) as e:\n                    msg = str(e)\n                    if (\"cannot convert float\" in msg or\n                            \"Trying to coerce float values to integer\" in msg):\n                        raise\n\n            # maybe coerce to a sub-class\n            from pandas.core.indexes.period import (\n                PeriodIndex, IncompatibleFrequency)\n\n            if is_signed_integer_dtype(data.dtype):\n                from .numeric import Int64Index\n                return Int64Index(data, copy=copy, dtype=dtype, name=name)\n            elif is_unsigned_integer_dtype(data.dtype):\n                from .numeric import UInt64Index\n                return UInt64Index(data, copy=copy, dtype=dtype, name=name)\n            elif is_float_dtype(data.dtype):\n                from .numeric import Float64Index\n                return Float64Index(data, copy=copy, dtype=dtype, name=name)\n            elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n                subarr = data.astype('object')\n            else:\n                subarr = com.asarray_tuplesafe(data, dtype=object)\n\n            # asarray_tuplesafe does not always copy underlying data,\n            # so need to make sure that this happens\n            if copy:\n                subarr = subarr.copy()\n\n            if dtype is None:\n                inferred = lib.infer_dtype(subarr)\n                if inferred == 'integer':\n                    try:\n                        return cls._try_convert_to_int_index(\n                            subarr, copy, name, dtype)\n                    except ValueError:\n                        pass\n\n                    return Index(subarr, copy=copy,\n                                 dtype=object, name=name)\n                elif inferred in ['floating', 'mixed-integer-float']:\n                    from .numeric import Float64Index\n                    return Float64Index(subarr, copy=copy, name=name)\n                elif inferred == 'interval':\n                    from .interval import IntervalIndex\n                    return IntervalIndex(subarr, name=name, copy=copy)\n                elif inferred == 'boolean':\n                    # don't support boolean explicitly ATM\n                    pass\n                elif inferred != 'string':\n                    if inferred.startswith('datetime'):\n                        if (lib.is_datetime_with_singletz_array(subarr) or\n                                'tz' in kwargs):\n                            # only when subarr has the same tz\n                            from pandas import DatetimeIndex\n                            try:\n                                return DatetimeIndex(subarr, copy=copy,\n                                                     name=name, **kwargs)\n                            except tslibs.OutOfBoundsDatetime:\n                                pass\n\n                    elif inferred.startswith('timedelta'):\n                        from pandas import TimedeltaIndex\n                        return TimedeltaIndex(subarr, copy=copy, name=name,\n                                              **kwargs)\n                    elif inferred == 'period':\n                        try:\n                            return PeriodIndex(subarr, name=name, **kwargs)\n                        except IncompatibleFrequency:\n                            pass\n            return cls._simple_new(subarr, name)\n\n        elif hasattr(data, '__array__'):\n            return Index(np.asarray(data), dtype=dtype, copy=copy, name=name,\n                         **kwargs)\n        elif data is None or is_scalar(data):\n            cls._scalar_data_error(data)\n        else:\n            if tupleize_cols and is_list_like(data):\n                # GH21470: convert iterable to list before determining if empty\n                if is_iterator(data):\n                    data = list(data)\n\n                if data and all(isinstance(e, tuple) for e in data):\n                    # we must be all tuples, otherwise don't construct\n                    # 10697\n                    from .multi import MultiIndex\n                    return MultiIndex.from_tuples(\n                        data, names=name or kwargs.get('names'))\n            # other iterable of some kind\n            subarr = com.asarray_tuplesafe(data, dtype=object)\n            return Index(subarr, dtype=dtype, copy=copy, name=name, **kwargs)\n\n    \"\"\"\n    NOTE for new Index creation:\n\n    - _simple_new: It returns new Index with the same type as the caller.\n      All metadata (such as name) must be provided by caller's responsibility.\n      Using _shallow_copy is recommended because it fills these metadata\n      otherwise specified.\n\n    - _shallow_copy: It returns new Index with the same type (using\n      _simple_new), but fills caller's metadata otherwise specified. Passed\n      kwargs will overwrite corresponding metadata.\n\n    - _shallow_copy_with_infer: It returns new Index inferring its type\n      from passed values. It fills caller's metadata otherwise specified as the\n      same as _shallow_copy.\n\n    See each method's docstring.\n    \"\"\"\n\n    @classmethod\n    def _simple_new(cls, values, name=None, dtype=None, **kwargs):\n        \"\"\"\n        we require the we have a dtype compat for the values\n        if we are passed a non-dtype compat, then coerce using the constructor\n\n        Must be careful not to recurse.\n        \"\"\"\n        if not hasattr(values, 'dtype'):\n            if (values is None or not len(values)) and dtype is not None:\n                values = np.empty(0, dtype=dtype)\n            else:\n                values = np.array(values, copy=False)\n                if is_object_dtype(values):\n                    values = cls(values, name=name, dtype=dtype,\n                                 **kwargs)._ndarray_values\n\n        if isinstance(values, (ABCSeries, ABCIndexClass)):\n            # Index._data must always be an ndarray.\n            # This is no-copy for when _values is an ndarray,\n            # which should be always at this point.\n            values = np.asarray(values._values)\n\n        result = object.__new__(cls)\n        result._data = values\n        result.name = name\n        for k, v in compat.iteritems(kwargs):\n            setattr(result, k, v)\n        return result._reset_identity()\n\n    _index_shared_docs['_shallow_copy'] = \"\"\"\n        create a new Index with the same class as the caller, don't copy the\n        data, use the same object attributes with passed in attributes taking\n        precedence\n\n        *this is an internal non-public method*\n\n        Parameters\n        ----------\n        values : the values to create the new Index, optional\n        kwargs : updates the default attributes for this Index\n        \"\"\"\n\n    @Appender(_index_shared_docs['_shallow_copy'])\n    def _shallow_copy(self, values=None, **kwargs):\n        if values is None:\n            values = self.values\n        attributes = self._get_attributes_dict()\n        attributes.update(kwargs)\n        if not len(values) and 'dtype' not in kwargs:\n            attributes['dtype'] = self.dtype\n\n        # _simple_new expects an ndarray\n        values = getattr(values, 'values', values)\n        if isinstance(values, ABCDatetimeIndex):\n            # `self.values` returns `self` for tz-aware, so we need to unwrap\n            #  more specifically\n            values = values.asi8\n\n        return self._simple_new(values, **attributes)\n\n    def _shallow_copy_with_infer(self, values, **kwargs):\n        \"\"\"\n        create a new Index inferring the class with passed value, don't copy\n        the data, use the same object attributes with passed in attributes\n        taking precedence\n\n        *this is an internal non-public method*\n\n        Parameters\n        ----------\n        values : the values to create the new Index, optional\n        kwargs : updates the default attributes for this Index\n        \"\"\"\n        attributes = self._get_attributes_dict()\n        attributes.update(kwargs)\n        attributes['copy'] = False\n        if not len(values) and 'dtype' not in kwargs:\n            attributes['dtype'] = self.dtype\n        if self._infer_as_myclass:\n            try:\n                return self._constructor(values, **attributes)\n            except (TypeError, ValueError):\n                pass\n        return Index(values, **attributes)\n\n    def _deepcopy_if_needed(self, orig, copy=False):\n        \"\"\"\n        .. versionadded:: 0.19.0\n\n        Make a copy of self if data coincides (in memory) with orig.\n        Subclasses should override this if self._base is not an ndarray.\n\n        Parameters\n        ----------\n        orig : ndarray\n            other ndarray to compare self._data against\n        copy : boolean, default False\n            when False, do not run any check, just return self\n\n        Returns\n        -------\n        A copy of self if needed, otherwise self : Index\n        \"\"\"\n        if copy:\n            # Retrieve the \"base objects\", i.e. the original memory allocations\n            if not isinstance(orig, np.ndarray):\n                # orig is a DatetimeIndex\n                orig = orig.values\n            orig = orig if orig.base is None else orig.base\n            new = self._data if self._data.base is None else self._data.base\n            if orig is new:\n                return self.copy(deep=True)\n\n        return self\n\n    def _update_inplace(self, result, **kwargs):\n        # guard when called from IndexOpsMixin\n        raise TypeError(\"Index can't be updated inplace\")\n\n    def _sort_levels_monotonic(self):\n        \"\"\" compat with MultiIndex \"\"\"\n        return self\n\n    _index_shared_docs['_get_grouper_for_level'] = \"\"\"\n        Get index grouper corresponding to an index level\n\n        Parameters\n        ----------\n        mapper: Group mapping function or None\n            Function mapping index values to groups\n        level : int or None\n            Index level\n\n        Returns\n        -------\n        grouper : Index\n            Index of values to group on\n        labels : ndarray of int or None\n            Array of locations in level_index\n        uniques : Index or None\n            Index of unique values for level\n        \"\"\"\n\n    @Appender(_index_shared_docs['_get_grouper_for_level'])\n    def _get_grouper_for_level(self, mapper, level=None):\n        assert level is None or level == 0\n        if mapper is None:\n            grouper = self\n        else:\n            grouper = self.map(mapper)\n\n        return grouper, None, None\n\n    def is_(self, other):\n        \"\"\"\n        More flexible, faster check like ``is`` but that works through views\n\n        Note: this is *not* the same as ``Index.identical()``, which checks\n        that metadata is also the same.\n\n        Parameters\n        ----------\n        other : object\n            other object to compare against.\n\n        Returns\n        -------\n        True if both have same underlying data, False otherwise : bool\n        \"\"\"\n        # use something other than None to be clearer\n        return self._id is getattr(\n            other, '_id', Ellipsis) and self._id is not None\n\n    def _reset_identity(self):\n        \"\"\"Initializes or resets ``_id`` attribute with new object\"\"\"\n        self._id = _Identity()\n        return self\n\n    # ndarray compat\n    def __len__(self):\n        \"\"\"\n        return the length of the Index\n        \"\"\"\n        return len(self._data)\n\n    def __array__(self, dtype=None):\n        \"\"\" the array interface, return my values \"\"\"\n        return self._data.view(np.ndarray)\n\n    def __array_wrap__(self, result, context=None):\n        \"\"\"\n        Gets called after a ufunc\n        \"\"\"\n        if is_bool_dtype(result):\n            return result\n\n        attrs = self._get_attributes_dict()\n        attrs = self._maybe_update_attributes(attrs)\n        return Index(result, **attrs)\n\n    @cache_readonly\n    def dtype(self):\n        \"\"\" return the dtype object of the underlying data \"\"\"\n        return self._data.dtype\n\n    @cache_readonly\n    def dtype_str(self):\n        \"\"\" return the dtype str of the underlying data \"\"\"\n        return str(self.dtype)\n\n    @property\n    def values(self):\n        \"\"\" return the underlying data as an ndarray \"\"\"\n        return self._data.view(np.ndarray)\n\n    @property\n    def _values(self):\n        # type: () -> Union[ExtensionArray, Index, np.ndarray]\n        # TODO(EA): remove index types as they become extension arrays\n        \"\"\"The best array representation.\n\n        This is an ndarray, ExtensionArray, or Index subclass. This differs\n        from ``_ndarray_values``, which always returns an ndarray.\n\n        Both ``_values`` and ``_ndarray_values`` are consistent between\n        ``Series`` and ``Index``.\n\n        It may differ from the public '.values' method.\n\n        index             | values          | _values       | _ndarray_values |\n        ----------------- | --------------- | ------------- | --------------- |\n        Index             | ndarray         | ndarray       | ndarray         |\n        CategoricalIndex  | Categorical     | Categorical   | ndarray[int]    |\n        DatetimeIndex     | ndarray[M8ns]   | ndarray[M8ns] | ndarray[M8ns]   |\n        DatetimeIndex[tz] | ndarray[M8ns]   | DTI[tz]       | ndarray[M8ns]   |\n        PeriodIndex       | ndarray[object] | PeriodArray   | ndarray[int]    |\n        IntervalIndex     | IntervalArray   | IntervalArray | ndarray[object] |\n\n        See Also\n        --------\n        values\n        _ndarray_values\n        \"\"\"\n        return self.values\n\n    def get_values(self):\n        \"\"\"\n        Return `Index` data as an `numpy.ndarray`.\n\n        Returns\n        -------\n        numpy.ndarray\n            A one-dimensional numpy array of the `Index` values.\n\n        See Also\n        --------\n        Index.values : The attribute that get_values wraps.\n\n        Examples\n        --------\n        Getting the `Index` values of a `DataFrame`:\n\n        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n        ...                    index=['a', 'b', 'c'], columns=['A', 'B', 'C'])\n        >>> df\n           A  B  C\n        a  1  2  3\n        b  4  5  6\n        c  7  8  9\n        >>> df.index.get_values()\n        array(['a', 'b', 'c'], dtype=object)\n\n        Standalone `Index` values:\n\n        >>> idx = pd.Index(['1', '2', '3'])\n        >>> idx.get_values()\n        array(['1', '2', '3'], dtype=object)\n\n        `MultiIndex` arrays also have only one dimension:\n\n        >>> midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],\n        ...                                  names=('number', 'letter'))\n        >>> midx.get_values()\n        array([(1, 'a'), (2, 'b'), (3, 'c')], dtype=object)\n        >>> midx.get_values().ndim\n        1\n        \"\"\"\n        return self.values\n\n    @Appender(IndexOpsMixin.memory_usage.__doc__)\n    def memory_usage(self, deep=False):\n        result = super(Index, self).memory_usage(deep=deep)\n\n        # include our engine hashtable\n        result += self._engine.sizeof(deep=deep)\n        return result\n\n    # ops compat\n    def repeat(self, repeats, *args, **kwargs):\n        \"\"\"\n        Repeat elements of an Index.\n\n        Returns a new index where each element of the current index\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int\n            The number of repetitions for each element.\n        **kwargs\n            Additional keywords have no effect but might be accepted for\n            compatibility with numpy.\n\n        Returns\n        -------\n        pandas.Index\n            Newly created Index with repeated elements.\n\n        See Also\n        --------\n        Series.repeat : Equivalent function for Series.\n        numpy.repeat : Underlying implementation.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3])\n        >>> idx\n        Int64Index([1, 2, 3], dtype='int64')\n        >>> idx.repeat(2)\n        Int64Index([1, 1, 2, 2, 3, 3], dtype='int64')\n        >>> idx.repeat(3)\n        Int64Index([1, 1, 1, 2, 2, 2, 3, 3, 3], dtype='int64')\n        \"\"\"\n        nv.validate_repeat(args, kwargs)\n        return self._shallow_copy(self._values.repeat(repeats))\n\n    _index_shared_docs['where'] = \"\"\"\n        .. versionadded:: 0.19.0\n\n        Return an Index of same shape as self and whose corresponding\n        entries are from self where cond is True and otherwise are from\n        other.\n\n        Parameters\n        ----------\n        cond : boolean array-like with the same length as self\n        other : scalar, or array-like\n        \"\"\"\n\n    @Appender(_index_shared_docs['where'])\n    def where(self, cond, other=None):\n        if other is None:\n            other = self._na_value\n\n        dtype = self.dtype\n        values = self.values\n\n        if is_bool(other) or is_bool_dtype(other):\n\n            # bools force casting\n            values = values.astype(object)\n            dtype = None\n\n        values = np.where(cond, values, other)\n\n        if self._is_numeric_dtype and np.any(isna(values)):\n            # We can't coerce to the numeric dtype of \"self\" (unless\n            # it's float) if there are NaN values in our output.\n            dtype = None\n\n        return self._shallow_copy_with_infer(values, dtype=dtype)\n\n    def ravel(self, order='C'):\n        \"\"\"\n        return an ndarray of the flattened values of the underlying data\n\n        See Also\n        --------\n        numpy.ndarray.ravel\n        \"\"\"\n        return self._ndarray_values.ravel(order=order)\n\n    # construction helpers\n    @classmethod\n    def _try_convert_to_int_index(cls, data, copy, name, dtype):\n        \"\"\"\n        Attempt to convert an array of data into an integer index.\n\n        Parameters\n        ----------\n        data : The data to convert.\n        copy : Whether to copy the data or not.\n        name : The name of the index returned.\n\n        Returns\n        -------\n        int_index : data converted to either an Int64Index or a\n                    UInt64Index\n\n        Raises\n        ------\n        ValueError if the conversion was not successful.\n        \"\"\"\n\n        from .numeric import Int64Index, UInt64Index\n        if not is_unsigned_integer_dtype(dtype):\n            # skip int64 conversion attempt if uint-like dtype is passed, as\n            # this could return Int64Index when UInt64Index is what's desrired\n            try:\n                res = data.astype('i8', copy=False)\n                if (res == data).all():\n                    return Int64Index(res, copy=copy, name=name)\n            except (OverflowError, TypeError, ValueError):\n                pass\n\n        # Conversion to int64 failed (possibly due to overflow) or was skipped,\n        # so let's try now with uint64.\n        try:\n            res = data.astype('u8', copy=False)\n            if (res == data).all():\n                return UInt64Index(res, copy=copy, name=name)\n        except (OverflowError, TypeError, ValueError):\n            pass\n\n        raise ValueError\n\n    @classmethod\n    def _scalar_data_error(cls, data):\n        raise TypeError('{0}(...) must be called with a collection of some '\n                        'kind, {1} was passed'.format(cls.__name__,\n                                                      repr(data)))\n\n    @classmethod\n    def _string_data_error(cls, data):\n        raise TypeError('String dtype not supported, you may need '\n                        'to explicitly cast to a numeric type')\n\n    @classmethod\n    def _coerce_to_ndarray(cls, data):\n        \"\"\"coerces data to ndarray, raises on scalar data. Converts other\n        iterables to list first and then to array. Does not touch ndarrays.\n        \"\"\"\n\n        if not isinstance(data, (np.ndarray, Index)):\n            if data is None or is_scalar(data):\n                cls._scalar_data_error(data)\n\n            # other iterable of some kind\n            if not isinstance(data, (ABCSeries, list, tuple)):\n                data = list(data)\n            data = np.asarray(data)\n        return data\n\n    def _get_attributes_dict(self):\n        \"\"\" return an attributes dict for my class \"\"\"\n        return {k: getattr(self, k, None) for k in self._attributes}\n\n    def view(self, cls=None):\n\n        # we need to see if we are subclassing an\n        # index type here\n        if cls is not None and not hasattr(cls, '_typ'):\n            result = self._data.view(cls)\n        else:\n            result = self._shallow_copy()\n        if isinstance(result, Index):\n            result._id = self._id\n        return result\n\n    def _coerce_scalar_to_index(self, item):\n        \"\"\"\n        we need to coerce a scalar to a compat for our index type\n\n        Parameters\n        ----------\n        item : scalar item to coerce\n        \"\"\"\n        dtype = self.dtype\n\n        if self._is_numeric_dtype and isna(item):\n            # We can't coerce to the numeric dtype of \"self\" (unless\n            # it's float) if there are NaN values in our output.\n            dtype = None\n\n        return Index([item], dtype=dtype, **self._get_attributes_dict())\n\n    _index_shared_docs['copy'] = \"\"\"\n        Make a copy of this object.  Name and dtype sets those attributes on\n        the new object.\n\n        Parameters\n        ----------\n        name : string, optional\n        deep : boolean, default False\n        dtype : numpy dtype or pandas type\n\n        Returns\n        -------\n        copy : Index\n\n        Notes\n        -----\n        In most cases, there should be no functional difference from using\n        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n        \"\"\"\n\n    @Appender(_index_shared_docs['copy'])\n    def copy(self, name=None, deep=False, dtype=None, **kwargs):\n        if deep:\n            new_index = self._shallow_copy(self._data.copy())\n        else:\n            new_index = self._shallow_copy()\n\n        names = kwargs.get('names')\n        names = self._validate_names(name=name, names=names, deep=deep)\n        new_index = new_index.set_names(names)\n\n        if dtype:\n            new_index = new_index.astype(dtype)\n        return new_index\n\n    def __copy__(self, **kwargs):\n        return self.copy(**kwargs)\n\n    def __deepcopy__(self, memo=None):\n        \"\"\"\n        Parameters\n        ----------\n        memo, default None\n            Standard signature. Unused\n        \"\"\"\n        if memo is None:\n            memo = {}\n        return self.copy(deep=True)\n\n    def _validate_names(self, name=None, names=None, deep=False):\n        \"\"\"\n        Handles the quirks of having a singular 'name' parameter for general\n        Index and plural 'names' parameter for MultiIndex.\n        \"\"\"\n        from copy import deepcopy\n        if names is not None and name is not None:\n            raise TypeError(\"Can only provide one of `names` and `name`\")\n        elif names is None and name is None:\n            return deepcopy(self.names) if deep else self.names\n        elif names is not None:\n            if not is_list_like(names):\n                raise TypeError(\"Must pass list-like as `names`.\")\n            return names\n        else:\n            if not is_list_like(name):\n                return [name]\n            return name\n\n    def __unicode__(self):\n        \"\"\"\n        Return a string representation for this object.\n\n        Invoked by unicode(df) in py2 only. Yields a Unicode String in both\n        py2/py3.\n        \"\"\"\n        klass = self.__class__.__name__\n        data = self._format_data()\n        attrs = self._format_attrs()\n        space = self._format_space()\n\n        prepr = (u(\",%s\") %\n                 space).join(u(\"%s=%s\") % (k, v) for k, v in attrs)\n\n        # no data provided, just attributes\n        if data is None:\n            data = ''\n\n        res = u(\"%s(%s%s)\") % (klass, data, prepr)\n\n        return res\n\n    def _format_space(self):\n\n        # using space here controls if the attributes\n        # are line separated or not (the default)\n\n        # max_seq_items = get_option('display.max_seq_items')\n        # if len(self) > max_seq_items:\n        #    space = \"\\n%s\" % (' ' * (len(klass) + 1))\n        return \" \"\n\n    @property\n    def _formatter_func(self):\n        \"\"\"\n        Return the formatter function\n        \"\"\"\n        return default_pprint\n\n    def _format_data(self, name=None):\n        \"\"\"\n        Return the formatted data as a unicode string\n        \"\"\"\n\n        # do we want to justify (only do so for non-objects)\n        is_justify = not (self.inferred_type in ('string', 'unicode') or\n                          (self.inferred_type == 'categorical' and\n                           is_object_dtype(self.categories)))\n\n        return format_object_summary(self, self._formatter_func,\n                                     is_justify=is_justify, name=name)\n\n    def _format_attrs(self):\n        \"\"\"\n        Return a list of tuples of the (attr,formatted_value)\n        \"\"\"\n        return format_object_attrs(self)\n\n    def to_flat_index(self):\n        \"\"\"\n        Identity method.\n\n        .. versionadded:: 0.24.0\n\n        This is implemented for compatability with subclass implementations\n        when chaining.\n\n        Returns\n        -------\n        pd.Index\n            Caller.\n\n        See Also\n        --------\n        MultiIndex.to_flat_index : Subclass implementation.\n        \"\"\"\n        return self\n\n    def to_series(self, index=None, name=None):\n        \"\"\"\n        Create a Series with both index and values equal to the index keys\n        useful with map for returning an indexer based on an index\n\n        Parameters\n        ----------\n        index : Index, optional\n            index of resulting Series. If None, defaults to original index\n        name : string, optional\n            name of resulting Series. If None, defaults to name of original\n            index\n\n        Returns\n        -------\n        Series : dtype will be based on the type of the Index values.\n        \"\"\"\n\n        from pandas import Series\n\n        if index is None:\n            index = self._shallow_copy()\n        if name is None:\n            name = self.name\n\n        return Series(self.values.copy(), index=index, name=name)\n\n    def to_frame(self, index=True, name=None):\n        \"\"\"\n        Create a DataFrame with a column containing the Index.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        index : boolean, default True\n            Set the index of the returned DataFrame as the original Index.\n\n        name : object, default None\n            The passed name should substitute for the index name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame containing the original Index data.\n\n        See Also\n        --------\n        Index.to_series : Convert an Index to a Series.\n        Series.to_frame : Convert Series to DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n        >>> idx.to_frame()\n               animal\n        animal\n        Ant       Ant\n        Bear     Bear\n        Cow       Cow\n\n        By default, the original Index is reused. To enforce a new Index:\n\n        >>> idx.to_frame(index=False)\n            animal\n        0   Ant\n        1  Bear\n        2   Cow\n\n        To override the name of the resulting column, specify `name`:\n\n        >>> idx.to_frame(index=False, name='zoo')\n            zoo\n        0   Ant\n        1  Bear\n        2   Cow\n        \"\"\"\n\n        from pandas import DataFrame\n        if name is None:\n            name = self.name or 0\n        result = DataFrame({name: self.values.copy()})\n\n        if index:\n            result.index = self\n        return result\n\n    _index_shared_docs['astype'] = \"\"\"\n        Create an Index with values cast to dtypes. The class of a new Index\n        is determined by dtype. When conversion is impossible, a ValueError\n        exception is raised.\n\n        Parameters\n        ----------\n        dtype : numpy dtype or pandas type\n        copy : bool, default True\n            By default, astype always returns a newly allocated object.\n            If copy is set to False and internal requirements on dtype are\n            satisfied, the original data is used to create a new Index\n            or the original Index is returned.\n\n            .. versionadded:: 0.19.0\n        \"\"\"\n\n    @Appender(_index_shared_docs['astype'])\n    def astype(self, dtype, copy=True):\n        if is_dtype_equal(self.dtype, dtype):\n            return self.copy() if copy else self\n\n        elif is_categorical_dtype(dtype):\n            from .category import CategoricalIndex\n            return CategoricalIndex(self.values, name=self.name, dtype=dtype,\n                                    copy=copy)\n\n        elif is_extension_array_dtype(dtype):\n            return Index(np.asarray(self), dtype=dtype, copy=copy)\n\n        try:\n            if is_datetime64tz_dtype(dtype):\n                from pandas import DatetimeIndex\n                return DatetimeIndex(self.values, name=self.name, dtype=dtype,\n                                     copy=copy)\n            return Index(self.values.astype(dtype, copy=copy), name=self.name,\n                         dtype=dtype)\n        except (TypeError, ValueError):\n            msg = 'Cannot cast {name} to dtype {dtype}'\n            raise TypeError(msg.format(name=type(self).__name__, dtype=dtype))\n\n    def _to_safe_for_reshape(self):\n        \"\"\" convert to object if we are a categorical \"\"\"\n        return self\n\n    def _assert_can_do_setop(self, other):\n        if not is_list_like(other):\n            raise TypeError('Input must be Index or array-like')\n        return True\n\n    def _convert_can_do_setop(self, other):\n        if not isinstance(other, Index):\n            other = Index(other, name=self.name)\n            result_name = self.name\n        else:\n            result_name = get_op_result_name(self, other)\n        return other, result_name\n\n    def _convert_for_op(self, value):\n        \"\"\" Convert value to be insertable to ndarray \"\"\"\n        return value\n\n    def _assert_can_do_op(self, value):\n        \"\"\" Check value is valid for scalar op \"\"\"\n        if not is_scalar(value):\n            msg = \"'value' must be a scalar, passed: {0}\"\n            raise TypeError(msg.format(type(value).__name__))\n\n    @property\n    def nlevels(self):\n        return 1\n\n    def _get_names(self):\n        return FrozenList((self.name, ))\n\n    def _set_names(self, values, level=None):\n        \"\"\"\n        Set new names on index. Each name has to be a hashable type.\n\n        Parameters\n        ----------\n        values : str or sequence\n            name(s) to set\n        level : int, level name, or sequence of int/level names (default None)\n            If the index is a MultiIndex (hierarchical), level(s) to set (None\n            for all levels).  Otherwise level must be None\n\n        Raises\n        ------\n        TypeError if each name is not hashable.\n        \"\"\"\n        if not is_list_like(values):\n            raise ValueError('Names must be a list-like')\n        if len(values) != 1:\n            raise ValueError('Length of new names must be 1, got %d' %\n                             len(values))\n\n        # GH 20527\n        # All items in 'name' need to be hashable:\n        for name in values:\n            if not is_hashable(name):\n                raise TypeError('{}.name must be a hashable type'\n                                .format(self.__class__.__name__))\n        self.name = values[0]\n\n    names = property(fset=_set_names, fget=_get_names)\n\n    def set_names(self, names, level=None, inplace=False):\n        \"\"\"\n        Set Index or MultiIndex name.\n\n        Able to set new names partially and by level.\n\n        Parameters\n        ----------\n        names : label or list of label\n            Name(s) to set.\n        level : int, label or list of int or label, optional\n            If the index is a MultiIndex, level(s) to set (None for all\n            levels). Otherwise level must be None.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index\n            The same type as the caller or None if inplace is True.\n\n        See Also\n        --------\n        Index.rename : Able to set new names without level.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx\n        Int64Index([1, 2, 3, 4], dtype='int64')\n        >>> idx.set_names('quarter')\n        Int64Index([1, 2, 3, 4], dtype='int64', name='quarter')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]])\n        >>> idx\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]])\n        >>> idx.set_names(['kind', 'year'], inplace=True)\n        >>> idx\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]],\n                   names=['kind', 'year'])\n        >>> idx.set_names('species', level=0)\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]],\n                   names=['species', 'year'])\n        \"\"\"\n\n        from .multi import MultiIndex\n        if level is not None and not isinstance(self, MultiIndex):\n            raise ValueError('Level must be None for non-MultiIndex')\n\n        if level is not None and not is_list_like(level) and is_list_like(\n                names):\n            msg = \"Names must be a string when a single level is provided.\"\n            raise TypeError(msg)\n\n        if not is_list_like(names) and level is None and self.nlevels > 1:\n            raise TypeError(\"Must pass list-like as `names`.\")\n\n        if not is_list_like(names):\n            names = [names]\n        if level is not None and not is_list_like(level):\n            level = [level]\n\n        if inplace:\n            idx = self\n        else:\n            idx = self._shallow_copy()\n        idx._set_names(names, level=level)\n        if not inplace:\n            return idx\n\n    def rename(self, name, inplace=False):\n        \"\"\"\n        Alter Index or MultiIndex name.\n\n        Able to set new names without level. Defaults to returning new index.\n        Length of names must match number of levels in MultiIndex.\n\n        Parameters\n        ----------\n        name : label or list of labels\n            Name(s) to set.\n        inplace : boolean, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index\n            The same type as the caller or None if inplace is True.\n\n        See Also\n        --------\n        Index.set_names : Able to set new names partially and by level.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['A', 'C', 'A', 'B'], name='score')\n        >>> idx.rename('grade')\n        Index(['A', 'C', 'A', 'B'], dtype='object', name='grade')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]],\n        ...                                   names=['kind', 'year'])\n        >>> idx\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]],\n                   names=['kind', 'year'])\n        >>> idx.rename(['species', 'year'])\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]],\n                   names=['species', 'year'])\n        >>> idx.rename('species')\n        Traceback (most recent call last):\n        TypeError: Must pass list-like as `names`.\n        \"\"\"\n        return self.set_names([name], inplace=inplace)\n\n    @property\n    def _has_complex_internals(self):\n        # to disable groupby tricks in MultiIndex\n        return False\n\n    def _summary(self, name=None):\n        \"\"\"\n        Return a summarized representation\n\n        Parameters\n        ----------\n        name : str\n            name to use in the summary representation\n\n        Returns\n        -------\n        String with a summarized representation of the index\n        \"\"\"\n        if len(self) > 0:\n            head = self[0]\n            if (hasattr(head, 'format') and\n                    not isinstance(head, compat.string_types)):\n                head = head.format()\n            tail = self[-1]\n            if (hasattr(tail, 'format') and\n                    not isinstance(tail, compat.string_types)):\n                tail = tail.format()\n            index_summary = ', %s to %s' % (pprint_thing(head),\n                                            pprint_thing(tail))\n        else:\n            index_summary = ''\n\n        if name is None:\n            name = type(self).__name__\n        return '%s: %s entries%s' % (name, len(self), index_summary)\n\n    def summary(self, name=None):\n        \"\"\"\n        Return a summarized representation\n        .. deprecated:: 0.23.0\n        \"\"\"\n        warnings.warn(\"'summary' is deprecated and will be removed in a \"\n                      \"future version.\", FutureWarning, stacklevel=2)\n        return self._summary(name)\n\n    def _mpl_repr(self):\n        # how to represent ourselves to matplotlib\n        return self.values\n\n    _na_value = np.nan\n    \"\"\"The expected NA value to use with this index.\"\"\"\n\n    # introspection\n    @property\n    def is_monotonic(self):\n        \"\"\" alias for is_monotonic_increasing (deprecated) \"\"\"\n        return self.is_monotonic_increasing\n\n    @property\n    def is_monotonic_increasing(self):\n        \"\"\"\n        return if the index is monotonic increasing (only equal or\n        increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3]).is_monotonic_increasing\n        True\n        >>> Index([1, 2, 2]).is_monotonic_increasing\n        True\n        >>> Index([1, 3, 2]).is_monotonic_increasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_increasing\n\n    @property\n    def is_monotonic_decreasing(self):\n        \"\"\"\n        return if the index is monotonic decreasing (only equal or\n        decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1]).is_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2]).is_monotonic_decreasing\n        True\n        >>> Index([3, 1, 2]).is_monotonic_decreasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_decreasing\n\n    @property\n    def _is_strictly_monotonic_increasing(self):\n        \"\"\"return if the index is strictly monotonic increasing\n        (only increasing) values\n\n        Examples\n        --------\n        >>> Index([1, 2, 3])._is_strictly_monotonic_increasing\n        True\n        >>> Index([1, 2, 2])._is_strictly_monotonic_increasing\n        False\n        >>> Index([1, 3, 2])._is_strictly_monotonic_increasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_increasing\n\n    @property\n    def _is_strictly_monotonic_decreasing(self):\n        \"\"\"return if the index is strictly monotonic decreasing\n        (only decreasing) values\n\n        Examples\n        --------\n        >>> Index([3, 2, 1])._is_strictly_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2])._is_strictly_monotonic_decreasing\n        False\n        >>> Index([3, 1, 2])._is_strictly_monotonic_decreasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_decreasing\n\n    def is_lexsorted_for_tuple(self, tup):\n        return True\n\n    @cache_readonly\n    def is_unique(self):\n        \"\"\" return if the index has unique values \"\"\"\n        return self._engine.is_unique\n\n    @property\n    def has_duplicates(self):\n        return not self.is_unique\n\n    def is_boolean(self):\n        return self.inferred_type in ['boolean']\n\n    def is_integer(self):\n        return self.inferred_type in ['integer']\n\n    def is_floating(self):\n        return self.inferred_type in ['floating', 'mixed-integer-float']\n\n    def is_numeric(self):\n        return self.inferred_type in ['integer', 'floating']\n\n    def is_object(self):\n        return is_object_dtype(self.dtype)\n\n    def is_categorical(self):\n        \"\"\"\n        Check if the Index holds categorical data.\n\n        Returns\n        -------\n        boolean\n            True if the Index is categorical.\n\n        See Also\n        --------\n        CategoricalIndex : Index for categorical data.\n\n        Examples\n        --------\n        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.is_categorical()\n        True\n\n        >>> idx = pd.Index([1, 3, 5, 7])\n        >>> idx.is_categorical()\n        False\n\n        >>> s = pd.Series([\"Peter\", \"Victor\", \"Elisabeth\", \"Mar\"])\n        >>> s\n        0        Peter\n        1       Victor\n        2    Elisabeth\n        3          Mar\n        dtype: object\n        >>> s.index.is_categorical()\n        False\n        \"\"\"\n        return self.inferred_type in ['categorical']\n\n    def is_interval(self):\n        return self.inferred_type in ['interval']\n\n    def is_mixed(self):\n        return self.inferred_type in ['mixed']\n\n    def holds_integer(self):\n        return self.inferred_type in ['integer', 'mixed-integer']\n\n    _index_shared_docs['_convert_scalar_indexer'] = \"\"\"\n        Convert a scalar indexer.\n\n        Parameters\n        ----------\n        key : label of the slice bound\n        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_scalar_indexer'])\n    def _convert_scalar_indexer(self, key, kind=None):\n        assert kind in ['ix', 'loc', 'getitem', 'iloc', None]\n\n        if kind == 'iloc':\n            return self._validate_indexer('positional', key, kind)\n\n        if len(self) and not isinstance(self, ABCMultiIndex,):\n\n            # we can raise here if we are definitive that this\n            # is positional indexing (eg. .ix on with a float)\n            # or label indexing if we are using a type able\n            # to be represented in the index\n\n            if kind in ['getitem', 'ix'] and is_float(key):\n                if not self.is_floating():\n                    return self._invalid_indexer('label', key)\n\n            elif kind in ['loc'] and is_float(key):\n\n                # we want to raise KeyError on string/mixed here\n                # technically we *could* raise a TypeError\n                # on anything but mixed though\n                if self.inferred_type not in ['floating',\n                                              'mixed-integer-float',\n                                              'string',\n                                              'unicode',\n                                              'mixed']:\n                    return self._invalid_indexer('label', key)\n\n            elif kind in ['loc'] and is_integer(key):\n                if not self.holds_integer():\n                    return self._invalid_indexer('label', key)\n\n        return key\n\n    _index_shared_docs['_convert_slice_indexer'] = \"\"\"\n        Convert a slice indexer.\n\n        By definition, these are labels unless 'iloc' is passed in.\n        Floats are not allowed as the start, step, or stop of the slice.\n\n        Parameters\n        ----------\n        key : label of the slice bound\n        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_slice_indexer'])\n    def _convert_slice_indexer(self, key, kind=None):\n        assert kind in ['ix', 'loc', 'getitem', 'iloc', None]\n\n        # if we are not a slice, then we are done\n        if not isinstance(key, slice):\n            return key\n\n        # validate iloc\n        if kind == 'iloc':\n            return slice(self._validate_indexer('slice', key.start, kind),\n                         self._validate_indexer('slice', key.stop, kind),\n                         self._validate_indexer('slice', key.step, kind))\n\n        # potentially cast the bounds to integers\n        start, stop, step = key.start, key.stop, key.step\n\n        # figure out if this is a positional indexer\n        def is_int(v):\n            return v is None or is_integer(v)\n\n        is_null_slicer = start is None and stop is None\n        is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not self.is_integer()\n\n        if kind == 'getitem':\n            \"\"\"\n            called from the getitem slicers, validate that we are in fact\n            integers\n            \"\"\"\n            if self.is_integer() or is_index_slice:\n                return slice(self._validate_indexer('slice', key.start, kind),\n                             self._validate_indexer('slice', key.stop, kind),\n                             self._validate_indexer('slice', key.step, kind))\n\n        # convert the slice to an indexer here\n\n        # if we are mixed and have integers\n        try:\n            if is_positional and self.is_mixed():\n                # Validate start & stop\n                if start is not None:\n                    self.get_loc(start)\n                if stop is not None:\n                    self.get_loc(stop)\n                is_positional = False\n        except KeyError:\n            if self.inferred_type == 'mixed-integer-float':\n                raise\n\n        if is_null_slicer:\n            indexer = key\n        elif is_positional:\n            indexer = key\n        else:\n            try:\n                indexer = self.slice_indexer(start, stop, step, kind=kind)\n            except Exception:\n                if is_index_slice:\n                    if self.is_integer():\n                        raise\n                    else:\n                        indexer = key\n                else:\n                    raise\n\n        return indexer\n\n    def _convert_listlike_indexer(self, keyarr, kind=None):\n        \"\"\"\n        Parameters\n        ----------\n        keyarr : list-like\n            Indexer to convert.\n\n        Returns\n        -------\n        tuple (indexer, keyarr)\n            indexer is an ndarray or None if cannot convert\n            keyarr are tuple-safe keys\n        \"\"\"\n        if isinstance(keyarr, Index):\n            keyarr = self._convert_index_indexer(keyarr)\n        else:\n            keyarr = self._convert_arr_indexer(keyarr)\n\n        indexer = self._convert_list_indexer(keyarr, kind=kind)\n        return indexer, keyarr\n\n    _index_shared_docs['_convert_arr_indexer'] = \"\"\"\n        Convert an array-like indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : array-like\n            Indexer to convert.\n\n        Returns\n        -------\n        converted_keyarr : array-like\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_arr_indexer'])\n    def _convert_arr_indexer(self, keyarr):\n        keyarr = com.asarray_tuplesafe(keyarr)\n        return keyarr\n\n    _index_shared_docs['_convert_index_indexer'] = \"\"\"\n        Convert an Index indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : Index (or sub-class)\n            Indexer to convert.\n\n        Returns\n        -------\n        converted_keyarr : Index (or sub-class)\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_index_indexer'])\n    def _convert_index_indexer(self, keyarr):\n        return keyarr\n\n    _index_shared_docs['_convert_list_indexer'] = \"\"\"\n        Convert a list-like indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : Index (or sub-class)\n            Indexer to convert.\n        kind : iloc, ix, loc, optional\n\n        Returns\n        -------\n        positional indexer or None\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_list_indexer'])\n    def _convert_list_indexer(self, keyarr, kind=None):\n        if (kind in [None, 'iloc', 'ix'] and\n                is_integer_dtype(keyarr) and not self.is_floating() and\n                not isinstance(keyarr, ABCPeriodIndex)):\n\n            if self.inferred_type == 'mixed-integer':\n                indexer = self.get_indexer(keyarr)\n                if (indexer >= 0).all():\n                    return indexer\n                # missing values are flagged as -1 by get_indexer and negative\n                # indices are already converted to positive indices in the\n                # above if-statement, so the negative flags are changed to\n                # values outside the range of indices so as to trigger an\n                # IndexError in maybe_convert_indices\n                indexer[indexer < 0] = len(self)\n                from pandas.core.indexing import maybe_convert_indices\n                return maybe_convert_indices(indexer, len(self))\n\n            elif not self.inferred_type == 'integer':\n                keyarr = np.where(keyarr < 0, len(self) + keyarr, keyarr)\n                return keyarr\n\n        return None\n\n    def _invalid_indexer(self, form, key):\n        \"\"\" consistent invalid indexer message \"\"\"\n        raise TypeError(\"cannot do {form} indexing on {klass} with these \"\n                        \"indexers [{key}] of {kind}\".format(\n                            form=form, klass=type(self), key=key,\n                            kind=type(key)))\n\n    def get_duplicates(self):\n        \"\"\"\n        Extract duplicated index elements.\n\n        Returns a sorted list of index elements which appear more than once in\n        the index.\n\n        .. deprecated:: 0.23.0\n            Use idx[idx.duplicated()].unique() instead\n\n        Returns\n        -------\n        array-like\n            List of duplicated indexes.\n\n        See Also\n        --------\n        Index.duplicated : Return boolean array denoting duplicates.\n        Index.drop_duplicates : Return Index with duplicates removed.\n\n        Examples\n        --------\n\n        Works on different Index of types.\n\n        >>> pd.Index([1, 2, 2, 3, 3, 3, 4]).get_duplicates()  # doctest: +SKIP\n        [2, 3]\n\n        Note that for a DatetimeIndex, it does not return a list but a new\n        DatetimeIndex:\n\n        >>> dates = pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-03',\n        ...                         '2018-01-03', '2018-01-04', '2018-01-04'],\n        ...                        format='%Y-%m-%d')\n        >>> pd.Index(dates).get_duplicates()  # doctest: +SKIP\n        DatetimeIndex(['2018-01-03', '2018-01-04'],\n                      dtype='datetime64[ns]', freq=None)\n\n        Sorts duplicated elements even when indexes are unordered.\n\n        >>> pd.Index([1, 2, 3, 2, 3, 4, 3]).get_duplicates()  # doctest: +SKIP\n        [2, 3]\n\n        Return empty array-like structure when all elements are unique.\n\n        >>> pd.Index([1, 2, 3, 4]).get_duplicates()  # doctest: +SKIP\n        []\n        >>> dates = pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-03'],\n        ...                        format='%Y-%m-%d')\n        >>> pd.Index(dates).get_duplicates()  # doctest: +SKIP\n        DatetimeIndex([], dtype='datetime64[ns]', freq=None)\n        \"\"\"\n        warnings.warn(\"'get_duplicates' is deprecated and will be removed in \"\n                      \"a future release. You can use \"\n                      \"idx[idx.duplicated()].unique() instead\",\n                      FutureWarning, stacklevel=2)\n\n        return self[self.duplicated()].unique()\n\n    def _cleanup(self):\n        self._engine.clear_mapping()\n\n    @cache_readonly\n    def _constructor(self):\n        return type(self)\n\n    @cache_readonly\n    def _engine(self):\n        # property, for now, slow to look up\n        return self._engine_type(lambda: self._ndarray_values, len(self))\n\n    def _validate_index_level(self, level):\n        \"\"\"\n        Validate index level.\n\n        For single-level Index getting level number is a no-op, but some\n        verification must be done like in MultiIndex.\n\n        \"\"\"\n        if isinstance(level, int):\n            if level < 0 and level != -1:\n                raise IndexError(\"Too many levels: Index has only 1 level,\"\n                                 \" %d is not a valid level number\" % (level, ))\n            elif level > 0:\n                raise IndexError(\"Too many levels:\"\n                                 \" Index has only 1 level, not %d\" %\n                                 (level + 1))\n        elif level != self.name:\n            raise KeyError('Level %s must be same as name (%s)' %\n                           (level, self.name))\n\n    def _get_level_number(self, level):\n        self._validate_index_level(level)\n        return 0\n\n    @cache_readonly\n    def inferred_type(self):\n        \"\"\" return a string of the type inferred from the values \"\"\"\n        return lib.infer_dtype(self)\n\n    def _is_memory_usage_qualified(self):\n        \"\"\" return a boolean if we need a qualified .info display \"\"\"\n        return self.is_object()\n\n    def is_type_compatible(self, kind):\n        return kind == self.inferred_type\n\n    @cache_readonly\n    def is_all_dates(self):\n        if self._data is None:\n            return False\n        return is_datetime_array(ensure_object(self.values))\n\n    def __reduce__(self):\n        d = dict(data=self._data)\n        d.update(self._get_attributes_dict())\n        return _new_Index, (self.__class__, d), None\n\n    def __setstate__(self, state):\n        \"\"\"Necessary for making this object picklable\"\"\"\n\n        if isinstance(state, dict):\n            self._data = state.pop('data')\n            for k, v in compat.iteritems(state):\n                setattr(self, k, v)\n\n        elif isinstance(state, tuple):\n\n            if len(state) == 2:\n                nd_state, own_state = state\n                data = np.empty(nd_state[1], dtype=nd_state[2])\n                np.ndarray.__setstate__(data, nd_state)\n                self.name = own_state[0]\n\n            else:  # pragma: no cover\n                data = np.empty(state)\n                np.ndarray.__setstate__(data, state)\n\n            self._data = data\n            self._reset_identity()\n        else:\n            raise Exception(\"invalid pickle state\")\n\n    _unpickle_compat = __setstate__\n\n    def __nonzero__(self):\n        raise ValueError(\"The truth value of a {0} is ambiguous. \"\n                         \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n                         .format(self.__class__.__name__))\n\n    __bool__ = __nonzero__\n\n    _index_shared_docs['__contains__'] = \"\"\"\n        return a boolean if this key is IN the index\n\n        Parameters\n        ----------\n        key : object\n\n        Returns\n        -------\n        boolean\n        \"\"\"\n\n    @Appender(_index_shared_docs['__contains__'] % _index_doc_kwargs)\n    def __contains__(self, key):\n        hash(key)\n        try:\n            return key in self._engine\n        except (OverflowError, TypeError, ValueError):\n            return False\n\n    _index_shared_docs['contains'] = \"\"\"\n        return a boolean if this key is IN the index\n\n        Parameters\n        ----------\n        key : object\n\n        Returns\n        -------\n        boolean\n        \"\"\"\n\n    @Appender(_index_shared_docs['contains'] % _index_doc_kwargs)\n    def contains(self, key):\n        hash(key)\n        try:\n            return key in self._engine\n        except (TypeError, ValueError):\n            return False\n\n    def __hash__(self):\n        raise TypeError(\"unhashable type: %r\" % type(self).__name__)\n\n    def __setitem__(self, key, value):\n        raise TypeError(\"Index does not support mutable operations\")\n\n    def __getitem__(self, key):\n        \"\"\"\n        Override numpy.ndarray's __getitem__ method to work as desired.\n\n        This function adds lists and Series as valid boolean indexers\n        (ndarrays only supports ndarray with dtype=bool).\n\n        If resulting ndim != 1, plain ndarray is returned instead of\n        corresponding `Index` subclass.\n\n        \"\"\"\n        # There's no custom logic to be implemented in __getslice__, so it's\n        # not overloaded intentionally.\n        getitem = self._data.__getitem__\n        promote = self._shallow_copy\n\n        if is_scalar(key):\n            key = com.cast_scalar_indexer(key)\n            return getitem(key)\n\n        if isinstance(key, slice):\n            # This case is separated from the conditional above to avoid\n            # pessimization of basic indexing.\n            return promote(getitem(key))\n\n        if com.is_bool_indexer(key):\n            key = np.asarray(key, dtype=bool)\n\n        key = com.values_from_object(key)\n        result = getitem(key)\n        if not is_scalar(result):\n            return promote(result)\n        else:\n            return result\n\n    def _can_hold_identifiers_and_holds_name(self, name):\n        \"\"\"\n        Faster check for ``name in self`` when we know `name` is a Python\n        identifier (e.g. in NDFrame.__getattr__, which hits this to support\n        . key lookup). For indexes that can't hold identifiers (everything\n        but object & categorical) we just return False.\n\n        https://github.com/pandas-dev/pandas/issues/19764\n        \"\"\"\n        if self.is_object() or self.is_categorical():\n            return name in self\n        return False\n\n    def append(self, other):\n        \"\"\"\n        Append a collection of Index options together\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        appended : Index\n        \"\"\"\n\n        to_concat = [self]\n\n        if isinstance(other, (list, tuple)):\n            to_concat = to_concat + list(other)\n        else:\n            to_concat.append(other)\n\n        for obj in to_concat:\n            if not isinstance(obj, Index):\n                raise TypeError('all inputs must be Index')\n\n        names = {obj.name for obj in to_concat}\n        name = None if len(names) > 1 else self.name\n\n        return self._concat(to_concat, name)\n\n    def _concat(self, to_concat, name):\n\n        typs = _concat.get_dtype_kinds(to_concat)\n\n        if len(typs) == 1:\n            return self._concat_same_dtype(to_concat, name=name)\n        return _concat._concat_index_asobject(to_concat, name=name)\n\n    def _concat_same_dtype(self, to_concat, name):\n        \"\"\"\n        Concatenate to_concat which has the same class\n        \"\"\"\n        # must be overridden in specific classes\n        return _concat._concat_index_asobject(to_concat, name)\n\n    _index_shared_docs['take'] = \"\"\"\n        return a new %(klass)s of the values selected by the indices\n\n        For internal compatibility with numpy arrays.\n\n        Parameters\n        ----------\n        indices : list\n            Indices to be taken\n        axis : int, optional\n            The axis over which to select values, always 0.\n        allow_fill : bool, default True\n        fill_value : bool, default None\n            If allow_fill=True and fill_value is not None, indices specified by\n            -1 is regarded as NA. If Index doesn't hold NA, raise ValueError\n\n        See Also\n        --------\n        numpy.ndarray.take\n        \"\"\"\n\n    @Appender(_index_shared_docs['take'] % _index_doc_kwargs)\n    def take(self, indices, axis=0, allow_fill=True,\n             fill_value=None, **kwargs):\n        if kwargs:\n            nv.validate_take(tuple(), kwargs)\n        indices = ensure_platform_int(indices)\n        if self._can_hold_na:\n            taken = self._assert_take_fillable(self.values, indices,\n                                               allow_fill=allow_fill,\n                                               fill_value=fill_value,\n                                               na_value=self._na_value)\n        else:\n            if allow_fill and fill_value is not None:\n                msg = 'Unable to fill values because {0} cannot contain NA'\n                raise ValueError(msg.format(self.__class__.__name__))\n            taken = self.values.take(indices)\n        return self._shallow_copy(taken)\n\n    def _assert_take_fillable(self, values, indices, allow_fill=True,\n                              fill_value=None, na_value=np.nan):\n        \"\"\" Internal method to handle NA filling of take \"\"\"\n        indices = ensure_platform_int(indices)\n\n        # only fill if we are passing a non-None fill_value\n        if allow_fill and fill_value is not None:\n            if (indices < -1).any():\n                msg = ('When allow_fill=True and fill_value is not None, '\n                       'all indices must be >= -1')\n                raise ValueError(msg)\n            taken = algos.take(values,\n                               indices,\n                               allow_fill=allow_fill,\n                               fill_value=na_value)\n        else:\n            taken = values.take(indices)\n        return taken\n\n    @cache_readonly\n    def _isnan(self):\n        \"\"\" return if each value is nan\"\"\"\n        if self._can_hold_na:\n            return isna(self)\n        else:\n            # shouldn't reach to this condition by checking hasnans beforehand\n            values = np.empty(len(self), dtype=np.bool_)\n            values.fill(False)\n            return values\n\n    @cache_readonly\n    def _nan_idxs(self):\n        if self._can_hold_na:\n            w, = self._isnan.nonzero()\n            return w\n        else:\n            return np.array([], dtype=np.int64)\n\n    @cache_readonly\n    def hasnans(self):\n        \"\"\"\n        Return if I have any nans; enables various perf speedups.\n        \"\"\"\n        if self._can_hold_na:\n            return bool(self._isnan.any())\n        else:\n            return False\n\n    def isna(self):\n        \"\"\"\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`pd.NaT`, get\n        mapped to ``True`` values.\n        Everything else get mapped to ``False`` values. Characters such as\n        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n\n        .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        numpy.ndarray\n            A boolean array of whether my values are NA\n\n        See Also\n        --------\n        pandas.Index.notna : Boolean inverse of isna.\n        pandas.Index.dropna : Omit entries with missing values.\n        pandas.isna : Top-level isna.\n        Series.isna : Detect missing values in Series object.\n\n        Examples\n        --------\n        Show which entries in a pandas.Index are NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.isna()\n        array([False, False,  True], dtype=bool)\n\n        Empty strings are not considered NA values. None is considered an NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.isna()\n        array([False, False, False,  True], dtype=bool)\n\n        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n\n        >>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n        ...                         pd.Timestamp(''), None, pd.NaT])\n        >>> idx\n        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n                      dtype='datetime64[ns]', freq=None)\n        >>> idx.isna()\n        array([False,  True,  True,  True], dtype=bool)\n        \"\"\"\n        return self._isnan\n    isnull = isna\n\n    def notna(self):\n        \"\"\"\n        Detect existing (non-missing) values.\n\n        Return a boolean same-sized object indicating if the values are not NA.\n        Non-missing values get mapped to ``True``. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n        NA values, such as None or :attr:`numpy.NaN`, get mapped to ``False``\n        values.\n\n        .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        numpy.ndarray\n            Boolean array to indicate which entries are not NA.\n\n        See Also\n        --------\n        Index.notnull : Alias of notna.\n        Index.isna: Inverse of notna.\n        pandas.notna : Top-level notna.\n\n        Examples\n        --------\n        Show which entries in an Index are not NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.notna()\n        array([ True,  True, False])\n\n        Empty strings are not considered NA values. None is considered a NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.notna()\n        array([ True,  True,  True, False])\n        \"\"\"\n        return ~self.isna()\n    notnull = notna\n\n    def putmask(self, mask, value):\n        \"\"\"\n        return a new Index of the values set with the mask\n\n        See Also\n        --------\n        numpy.ndarray.putmask\n        \"\"\"\n        values = self.values.copy()\n        try:\n            np.putmask(values, mask, self._convert_for_op(value))\n            return self._shallow_copy(values)\n        except (ValueError, TypeError) as err:\n            if is_object_dtype(self):\n                raise err\n\n            # coerces to object\n            return self.astype(object).putmask(mask, value)\n\n    def format(self, name=False, formatter=None, **kwargs):\n        \"\"\"\n        Render a string representation of the Index\n        \"\"\"\n        header = []\n        if name:\n            header.append(pprint_thing(self.name,\n                                       escape_chars=('\\t', '\\r', '\\n')) if\n                          self.name is not None else '')\n\n        if formatter is not None:\n            return header + list(self.map(formatter))\n\n        return self._format_with_header(header, **kwargs)\n\n    def _format_with_header(self, header, na_rep='NaN', **kwargs):\n        values = self.values\n\n        from pandas.io.formats.format import format_array\n\n        if is_categorical_dtype(values.dtype):\n            values = np.array(values)\n\n        elif is_object_dtype(values.dtype):\n            values = lib.maybe_convert_objects(values, safe=1)\n\n        if is_object_dtype(values.dtype):\n            result = [pprint_thing(x, escape_chars=('\\t', '\\r', '\\n'))\n                      for x in values]\n\n            # could have nans\n            mask = isna(values)\n            if mask.any():\n                result = np.array(result)\n                result[mask] = na_rep\n                result = result.tolist()\n\n        else:\n            result = _trim_front(format_array(values, None, justify='left'))\n        return header + result\n\n    def to_native_types(self, slicer=None, **kwargs):\n        \"\"\"\n        Format specified values of `self` and return them.\n\n        Parameters\n        ----------\n        slicer : int, array-like\n            An indexer into `self` that specifies which values\n            are used in the formatting process.\n        kwargs : dict\n            Options for specifying how the values should be formatted.\n            These options include the following:\n\n            1) na_rep : str\n                The value that serves as a placeholder for NULL values\n            2) quoting : bool or None\n                Whether or not there are quoted values in `self`\n            3) date_format : str\n                The format used to represent date-like values\n        \"\"\"\n\n        values = self\n        if slicer is not None:\n            values = values[slicer]\n        return values._format_native_types(**kwargs)\n\n    def _format_native_types(self, na_rep='', quoting=None, **kwargs):\n        \"\"\" actually format my specific types \"\"\"\n        mask = isna(self)\n        if not self.is_object() and not quoting:\n            values = np.asarray(self).astype(str)\n        else:\n            values = np.array(self, dtype=object, copy=True)\n\n        values[mask] = na_rep\n        return values\n\n    def equals(self, other):\n        \"\"\"\n        Determines if two Index objects contain the same elements.\n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n\n        if is_object_dtype(self) and not is_object_dtype(other):\n            # if other is not object, use other's logic for coercion\n            return other.equals(self)\n\n        try:\n            return array_equivalent(com.values_from_object(self),\n                                    com.values_from_object(other))\n        except Exception:\n            return False\n\n    def identical(self, other):\n        \"\"\"Similar to equals, but check that other comparable attributes are\n        also equal\n        \"\"\"\n        return (self.equals(other) and\n                all((getattr(self, c, None) == getattr(other, c, None)\n                     for c in self._comparables)) and\n                type(self) == type(other))\n\n    def asof(self, label):\n        \"\"\"\n        Return the label from the index, or, if not present, the previous one.\n\n        Assuming that the index is sorted, return the passed index label if it\n        is in the index, or return the previous index label if the passed one\n        is not in the index.\n\n        Parameters\n        ----------\n        label : object\n            The label up to which the method returns the latest index label.\n\n        Returns\n        -------\n        object\n            The passed label if it is in the index. The previous label if the\n            passed label is not in the sorted index or `NaN` if there is no\n            such label.\n\n        See Also\n        --------\n        Series.asof : Return the latest value in a Series up to the\n            passed index.\n        merge_asof : Perform an asof merge (similar to left join but it\n            matches on nearest key rather than equal key).\n        Index.get_loc : An `asof` is a thin wrapper around `get_loc`\n            with method='pad'.\n\n        Examples\n        --------\n        `Index.asof` returns the latest index label up to the passed label.\n\n        >>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])\n        >>> idx.asof('2014-01-01')\n        '2013-12-31'\n\n        If the label is in the index, the method returns the passed label.\n\n        >>> idx.asof('2014-01-02')\n        '2014-01-02'\n\n        If all of the labels in the index are later than the passed label,\n        NaN is returned.\n\n        >>> idx.asof('1999-01-02')\n        nan\n\n        If the index is not sorted, an error is raised.\n\n        >>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',\n        ...                            '2014-01-03'])\n        >>> idx_not_sorted.asof('2013-12-31')\n        Traceback (most recent call last):\n        ValueError: index must be monotonic increasing or decreasing\n        \"\"\"\n        try:\n            loc = self.get_loc(label, method='pad')\n        except KeyError:\n            return self._na_value\n        else:\n            if isinstance(loc, slice):\n                loc = loc.indices(len(self))[-1]\n            return self[loc]\n\n    def asof_locs(self, where, mask):\n        \"\"\"\n        where : array of timestamps\n        mask : array of booleans where data is not NA\n        \"\"\"\n        locs = self.values[mask].searchsorted(where.values, side='right')\n\n        locs = np.where(locs > 0, locs - 1, 0)\n        result = np.arange(len(self))[mask].take(locs)\n\n        first = mask.argmax()\n        result[(locs == 0) & (where.values < self.values[first])] = -1\n\n        return result\n\n    def sort_values(self, return_indexer=False, ascending=True):\n        \"\"\"\n        Return a sorted copy of the index.\n\n        Return a sorted copy of the index, and optionally return the indices\n        that sorted the index itself.\n\n        Parameters\n        ----------\n        return_indexer : bool, default False\n            Should the indices that would sort the index be returned.\n        ascending : bool, default True\n            Should the index values be sorted in an ascending order.\n\n        Returns\n        -------\n        sorted_index : pandas.Index\n            Sorted copy of the index.\n        indexer : numpy.ndarray, optional\n            The indices that the index itself was sorted by.\n\n        See Also\n        --------\n        pandas.Series.sort_values : Sort values of a Series.\n        pandas.DataFrame.sort_values : Sort values in a DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index([10, 100, 1, 1000])\n        >>> idx\n        Int64Index([10, 100, 1, 1000], dtype='int64')\n\n        Sort values in ascending order (default behavior).\n\n        >>> idx.sort_values()\n        Int64Index([1, 10, 100, 1000], dtype='int64')\n\n        Sort values in descending order, and also get the indices `idx` was\n        sorted by.\n\n        >>> idx.sort_values(ascending=False, return_indexer=True)\n        (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\n        \"\"\"\n        _as = self.argsort()\n        if not ascending:\n            _as = _as[::-1]\n\n        sorted_index = self.take(_as)\n\n        if return_indexer:\n            return sorted_index, _as\n        else:\n            return sorted_index\n\n    def sort(self, *args, **kwargs):\n        raise TypeError(\"cannot sort an Index object in-place, use \"\n                        \"sort_values instead\")\n\n    def sortlevel(self, level=None, ascending=True, sort_remaining=None):\n        \"\"\"\n\n        For internal compatibility with with the Index API\n\n        Sort the Index. This is for compat with MultiIndex\n\n        Parameters\n        ----------\n        ascending : boolean, default True\n            False to sort in descending order\n\n        level, sort_remaining are compat parameters\n\n        Returns\n        -------\n        sorted_index : Index\n        \"\"\"\n        return self.sort_values(return_indexer=True, ascending=ascending)\n\n    def shift(self, periods=1, freq=None):\n        \"\"\"\n        Shift index by desired number of time frequency increments.\n\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n        freq : pandas.DateOffset, pandas.Timedelta or string, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        pandas.Index\n            shifted index\n\n        See Also\n        --------\n        Series.shift : Shift values of Series.\n\n        Examples\n        --------\n        Put the first 5 month starts of 2011 into an index.\n\n        >>> month_starts = pd.date_range('1/1/2011', periods=5, freq='MS')\n        >>> month_starts\n        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n                       '2011-05-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Shift the index by 10 days.\n\n        >>> month_starts.shift(10, freq='D')\n        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n                       '2011-05-11'],\n                      dtype='datetime64[ns]', freq=None)\n\n        The default value of `freq` is the `freq` attribute of the index,\n        which is 'MS' (month start) in this example.\n\n        >>> month_starts.shift(10)\n        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n                       '2012-03-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Notes\n        -----\n        This method is only implemented for datetime-like index classes,\n        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n        \"\"\"\n        raise NotImplementedError(\"Not supported for type %s\" %\n                                  type(self).__name__)\n\n    def argsort(self, *args, **kwargs):\n        \"\"\"\n        Return the integer indices that would sort the index.\n\n        Parameters\n        ----------\n        *args\n            Passed to `numpy.ndarray.argsort`.\n        **kwargs\n            Passed to `numpy.ndarray.argsort`.\n\n        Returns\n        -------\n        numpy.ndarray\n            Integer indices that would sort the index if used as\n            an indexer.\n\n        See Also\n        --------\n        numpy.argsort : Similar method for NumPy arrays.\n        Index.sort_values : Return sorted copy of Index.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['b', 'a', 'd', 'c'])\n        >>> idx\n        Index(['b', 'a', 'd', 'c'], dtype='object')\n\n        >>> order = idx.argsort()\n        >>> order\n        array([1, 0, 3, 2])\n\n        >>> idx[order]\n        Index(['a', 'b', 'c', 'd'], dtype='object')\n        \"\"\"\n        result = self.asi8\n        if result is None:\n            result = np.array(self)\n        return result.argsort(*args, **kwargs)\n\n    def __add__(self, other):\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n            return NotImplemented\n        return Index(np.array(self) + other)\n\n    def __radd__(self, other):\n        return Index(other + np.array(self))\n\n    def __iadd__(self, other):\n        # alias for __add__\n        return self + other\n\n    def __sub__(self, other):\n        return Index(np.array(self) - other)\n\n    def __rsub__(self, other):\n        return Index(other - np.array(self))\n\n    def __and__(self, other):\n        return self.intersection(other)\n\n    def __or__(self, other):\n        return self.union(other)\n\n    def __xor__(self, other):\n        return self.symmetric_difference(other)\n\n    def _get_reconciled_name_object(self, other):\n        \"\"\"\n        If the result of a set operation will be self,\n        return self, unless the name changes, in which\n        case make a shallow copy of self.\n        \"\"\"\n        name = get_op_result_name(self, other)\n        if self.name != name:\n            return self._shallow_copy(name=name)\n        return self\n\n    def union(self, other):\n        \"\"\"\n        Form the union of two Index objects and sorts if possible.\n\n        Parameters\n        ----------\n        other : Index or array-like\n\n        Returns\n        -------\n        union : Index\n\n        Examples\n        --------\n\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.union(idx2)\n        Int64Index([1, 2, 3, 4, 5, 6], dtype='int64')\n        \"\"\"\n        self._assert_can_do_setop(other)\n        other = ensure_index(other)\n\n        if len(other) == 0 or self.equals(other):\n            return self._get_reconciled_name_object(other)\n\n        if len(self) == 0:\n            return other._get_reconciled_name_object(self)\n\n        # TODO: is_dtype_union_equal is a hack around\n        # 1. buggy set ops with duplicates (GH #13432)\n        # 2. CategoricalIndex lacking setops (GH #10186)\n        # Once those are fixed, this workaround can be removed\n        if not is_dtype_union_equal(self.dtype, other.dtype):\n            this = self.astype('O')\n            other = other.astype('O')\n            return this.union(other)\n\n        # TODO(EA): setops-refactor, clean all this up\n        if is_period_dtype(self) or is_datetime64tz_dtype(self):\n            lvals = self._ndarray_values\n        else:\n            lvals = self._values\n        if is_period_dtype(other) or is_datetime64tz_dtype(other):\n            rvals = other._ndarray_values\n        else:\n            rvals = other._values\n\n        if self.is_monotonic and other.is_monotonic:\n            try:\n                result = self._outer_indexer(lvals, rvals)[0]\n            except TypeError:\n                # incomparable objects\n                result = list(lvals)\n\n                # worth making this faster? a very unusual case\n                value_set = set(lvals)\n                result.extend([x for x in rvals if x not in value_set])\n        else:\n            indexer = self.get_indexer(other)\n            indexer, = (indexer == -1).nonzero()\n\n            if len(indexer) > 0:\n                other_diff = algos.take_nd(rvals, indexer,\n                                           allow_fill=False)\n                result = _concat._concat_compat((lvals, other_diff))\n\n                try:\n                    lvals[0] < other_diff[0]\n                except TypeError as e:\n                    warnings.warn(\"%s, sort order is undefined for \"\n                                  \"incomparable objects\" % e, RuntimeWarning,\n                                  stacklevel=3)\n                else:\n                    types = frozenset((self.inferred_type,\n                                       other.inferred_type))\n                    if not types & _unsortable_types:\n                        result.sort()\n\n            else:\n                result = lvals\n\n                try:\n                    result = np.sort(result)\n                except TypeError as e:\n                    warnings.warn(\"%s, sort order is undefined for \"\n                                  \"incomparable objects\" % e, RuntimeWarning,\n                                  stacklevel=3)\n\n        # for subclasses\n        return self._wrap_setop_result(other, result)\n\n    def _wrap_setop_result(self, other, result):\n        return self._constructor(result, name=get_op_result_name(self, other))\n\n    def intersection(self, other):\n        \"\"\"\n        Form the intersection of two Index objects.\n\n        This returns a new Index with elements common to the index and `other`,\n        preserving the order of the calling index.\n\n        Parameters\n        ----------\n        other : Index or array-like\n\n        Returns\n        -------\n        intersection : Index\n\n        Examples\n        --------\n\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.intersection(idx2)\n        Int64Index([3, 4], dtype='int64')\n        \"\"\"\n        self._assert_can_do_setop(other)\n        other = ensure_index(other)\n\n        if self.equals(other):\n            return self._get_reconciled_name_object(other)\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            this = self.astype('O')\n            other = other.astype('O')\n            return this.intersection(other)\n\n        # TODO(EA): setops-refactor, clean all this up\n        if is_period_dtype(self):\n            lvals = self._ndarray_values\n        else:\n            lvals = self._values\n        if is_period_dtype(other):\n            rvals = other._ndarray_values\n        else:\n            rvals = other._values\n\n        if self.is_monotonic and other.is_monotonic:\n            try:\n                result = self._inner_indexer(lvals, rvals)[0]\n                return self._wrap_setop_result(other, result)\n            except TypeError:\n                pass\n\n        try:\n            indexer = Index(rvals).get_indexer(lvals)\n            indexer = indexer.take((indexer != -1).nonzero()[0])\n        except Exception:\n            # duplicates\n            indexer = algos.unique1d(\n                Index(rvals).get_indexer_non_unique(lvals)[0])\n            indexer = indexer[indexer != -1]\n\n        taken = other.take(indexer)\n        if self.name != other.name:\n            taken.name = None\n        return taken\n\n    def difference(self, other, sort=True):\n        \"\"\"\n        Return a new Index with elements from the index that are not in\n        `other`.\n\n        This is the set difference of two Index objects.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : bool, default True\n            Sort the resulting index if possible\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        difference : Index\n\n        Examples\n        --------\n\n        >>> idx1 = pd.Index([2, 1, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.difference(idx2)\n        Int64Index([1, 2], dtype='int64')\n        >>> idx1.difference(idx2, sort=False)\n        Int64Index([2, 1], dtype='int64')\n        \"\"\"\n        self._assert_can_do_setop(other)\n\n        if self.equals(other):\n            # pass an empty np.ndarray with the appropriate dtype\n            return self._shallow_copy(self._data[:0])\n\n        other, result_name = self._convert_can_do_setop(other)\n\n        this = self._get_unique_index()\n\n        indexer = this.get_indexer(other)\n        indexer = indexer.take((indexer != -1).nonzero()[0])\n\n        label_diff = np.setdiff1d(np.arange(this.size), indexer,\n                                  assume_unique=True)\n        the_diff = this.values.take(label_diff)\n        if sort:\n            try:\n                the_diff = sorting.safe_sort(the_diff)\n            except TypeError:\n                pass\n\n        return this._shallow_copy(the_diff, name=result_name, freq=None)\n\n    def symmetric_difference(self, other, result_name=None):\n        \"\"\"\n        Compute the symmetric difference of two Index objects.\n        It's sorted if sorting is possible.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        result_name : str\n\n        Returns\n        -------\n        symmetric_difference : Index\n\n        Notes\n        -----\n        ``symmetric_difference`` contains elements that appear in either\n        ``idx1`` or ``idx2`` but not both. Equivalent to the Index created by\n        ``idx1.difference(idx2) | idx2.difference(idx1)`` with duplicates\n        dropped.\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([2, 3, 4, 5])\n        >>> idx1.symmetric_difference(idx2)\n        Int64Index([1, 5], dtype='int64')\n\n        You can also use the ``^`` operator:\n\n        >>> idx1 ^ idx2\n        Int64Index([1, 5], dtype='int64')\n        \"\"\"\n        self._assert_can_do_setop(other)\n        other, result_name_update = self._convert_can_do_setop(other)\n        if result_name is None:\n            result_name = result_name_update\n\n        this = self._get_unique_index()\n        other = other._get_unique_index()\n        indexer = this.get_indexer(other)\n\n        # {this} minus {other}\n        common_indexer = indexer.take((indexer != -1).nonzero()[0])\n        left_indexer = np.setdiff1d(np.arange(this.size), common_indexer,\n                                    assume_unique=True)\n        left_diff = this.values.take(left_indexer)\n\n        # {other} minus {this}\n        right_indexer = (indexer == -1).nonzero()[0]\n        right_diff = other.values.take(right_indexer)\n\n        the_diff = _concat._concat_compat([left_diff, right_diff])\n        try:\n            the_diff = sorting.safe_sort(the_diff)\n        except TypeError:\n            pass\n\n        attribs = self._get_attributes_dict()\n        attribs['name'] = result_name\n        if 'freq' in attribs:\n            attribs['freq'] = None\n        return self._shallow_copy_with_infer(the_diff, **attribs)\n\n    def _get_unique_index(self, dropna=False):\n        \"\"\"\n        Returns an index containing unique values.\n\n        Parameters\n        ----------\n        dropna : bool\n            If True, NaN values are dropped.\n\n        Returns\n        -------\n        uniques : index\n        \"\"\"\n        if self.is_unique and not dropna:\n            return self\n\n        values = self.values\n\n        if not self.is_unique:\n            values = self.unique()\n\n        if dropna:\n            try:\n                if self.hasnans:\n                    values = values[~isna(values)]\n            except NotImplementedError:\n                pass\n\n        return self._shallow_copy(values)\n\n    _index_shared_docs['get_loc'] = \"\"\"\n        Get integer location, slice or boolean mask for requested label.\n\n        Parameters\n        ----------\n        key : label\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        tolerance : optional\n            Maximum distance from index value for inexact matches. The value of\n            the index at the matching location most satisfy the equation\n            ``abs(index[loc] - key) <= tolerance``.\n\n            Tolerance may be a scalar\n            value, which applies the same tolerance to all values, or\n            list-like, which applies variable tolerance per element. List-like\n            includes list, tuple, array, Series, and must be the same size as\n            the index and its dtype must exactly match the index's type.\n\n            .. versionadded:: 0.21.0 (list-like tolerance)\n\n        Returns\n        -------\n        loc : int if unique index, slice if monotonic index, else mask\n\n        Examples\n        ---------\n        >>> unique_index = pd.Index(list('abc'))\n        >>> unique_index.get_loc('b')\n        1\n\n        >>> monotonic_index = pd.Index(list('abbc'))\n        >>> monotonic_index.get_loc('b')\n        slice(1, 3, None)\n\n        >>> non_monotonic_index = pd.Index(list('abcb'))\n        >>> non_monotonic_index.get_loc('b')\n        array([False,  True, False,  True], dtype=bool)\n        \"\"\"\n\n    @Appender(_index_shared_docs['get_loc'])\n    def get_loc(self, key, method=None, tolerance=None):\n        if method is None:\n            if tolerance is not None:\n                raise ValueError('tolerance argument only valid if using pad, '\n                                 'backfill or nearest lookups')\n            try:\n                return self._engine.get_loc(key)\n            except KeyError:\n                return self._engine.get_loc(self._maybe_cast_indexer(key))\n        indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n        if indexer.ndim > 1 or indexer.size > 1:\n            raise TypeError('get_loc requires scalar valued input')\n        loc = indexer.item()\n        if loc == -1:\n            raise KeyError(key)\n        return loc\n\n    def get_value(self, series, key):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray. Only use this if you\n        know what you're doing\n        \"\"\"\n\n        # if we have something that is Index-like, then\n        # use this, e.g. DatetimeIndex\n        s = getattr(series, '_values', None)\n        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):\n            # GH 20882, 21257\n            # Unify Index and ExtensionArray treatment\n            # First try to convert the key to a location\n            # If that fails, raise a KeyError if an integer\n            # index, otherwise, see if key is an integer, and\n            # try that\n            try:\n                iloc = self.get_loc(key)\n                return s[iloc]\n            except KeyError:\n                if (len(self) > 0 and\n                        (self.holds_integer() or self.is_boolean())):\n                    raise\n                elif is_integer(key):\n                    return s[key]\n\n        s = com.values_from_object(series)\n        k = com.values_from_object(key)\n\n        k = self._convert_scalar_indexer(k, kind='getitem')\n        try:\n            return self._engine.get_value(s, k,\n                                          tz=getattr(series.dtype, 'tz', None))\n        except KeyError as e1:\n            if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                raise\n\n            try:\n                return libindex.get_value_box(s, key)\n            except IndexError:\n                raise\n            except TypeError:\n                # generator/iterator-like\n                if is_iterator(key):\n                    raise InvalidIndexError(key)\n                else:\n                    raise e1\n            except Exception:  # pragma: no cover\n                raise e1\n        except TypeError:\n            # python 3\n            if is_scalar(key):  # pragma: no cover\n                raise IndexError(key)\n            raise InvalidIndexError(key)\n\n    def set_value(self, arr, key, value):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray. Only use this if you\n        know what you're doing\n        \"\"\"\n        self._engine.set_value(com.values_from_object(arr),\n                               com.values_from_object(key), value)\n\n    def _get_level_values(self, level):\n        \"\"\"\n        Return an Index of values for requested level.\n\n        This is primarily useful to get an individual level of values from a\n        MultiIndex, but is provided on Index as well for compatability.\n\n        Parameters\n        ----------\n        level : int or str\n            It is either the integer position or the name of the level.\n\n        Returns\n        -------\n        values : Index\n            Calling object, as there is only one level in the Index.\n\n        See Also\n        --------\n        MultiIndex.get_level_values : Get values for a level of a MultiIndex.\n\n        Notes\n        -----\n        For Index, level should be 0, since there are no multiple levels.\n\n        Examples\n        --------\n\n        >>> idx = pd.Index(list('abc'))\n        >>> idx\n        Index(['a', 'b', 'c'], dtype='object')\n\n        Get level values by supplying `level` as integer:\n\n        >>> idx.get_level_values(0)\n        Index(['a', 'b', 'c'], dtype='object')\n        \"\"\"\n        self._validate_index_level(level)\n        return self\n\n    get_level_values = _get_level_values\n\n    def droplevel(self, level=0):\n        \"\"\"\n        Return index with requested level(s) removed. If resulting index has\n        only 1 level left, the result will be of Index type, not MultiIndex.\n\n        .. versionadded:: 0.23.1 (support for non-MultiIndex)\n\n        Parameters\n        ----------\n        level : int, str, or list-like, default 0\n            If a string is given, must be the name of a level\n            If list-like, elements must be names or indexes of levels.\n\n        Returns\n        -------\n        index : Index or MultiIndex\n        \"\"\"\n        if not isinstance(level, (tuple, list)):\n            level = [level]\n\n        levnums = sorted(self._get_level_number(lev) for lev in level)[::-1]\n\n        if len(level) == 0:\n            return self\n        if len(level) >= self.nlevels:\n            raise ValueError(\"Cannot remove {} levels from an index with {} \"\n                             \"levels: at least one level must be \"\n                             \"left.\".format(len(level), self.nlevels))\n        # The two checks above guarantee that here self is a MultiIndex\n\n        new_levels = list(self.levels)\n        new_labels = list(self.labels)\n        new_names = list(self.names)\n\n        for i in levnums:\n            new_levels.pop(i)\n            new_labels.pop(i)\n            new_names.pop(i)\n\n        if len(new_levels) == 1:\n\n            # set nan if needed\n            mask = new_labels[0] == -1\n            result = new_levels[0].take(new_labels[0])\n            if mask.any():\n                result = result.putmask(mask, np.nan)\n\n            result.name = new_names[0]\n            return result\n        else:\n            from .multi import MultiIndex\n            return MultiIndex(levels=new_levels, labels=new_labels,\n                              names=new_names, verify_integrity=False)\n\n    _index_shared_docs['get_indexer'] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        limit : int, optional\n            Maximum number of consecutive labels in ``target`` to match for\n            inexact matches.\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations most\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n\n            Tolerance may be a scalar value, which applies the same tolerance\n            to all values, or list-like, which applies variable tolerance per\n            element. List-like includes list, tuple, array, Series, and must be\n            the same size as the index and its dtype must exactly match the\n            index's type.\n\n            .. versionadded:: 0.21.0 (list-like tolerance)\n\n        Returns\n        -------\n        indexer : ndarray of int\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n\n        Examples\n        --------\n        >>> index = pd.Index(['c', 'a', 'b'])\n        >>> index.get_indexer(['a', 'b', 'x'])\n        array([ 1,  2, -1])\n\n        Notice that the return value is an array of locations in ``index``\n        and ``x`` is marked by -1, as it is not in ``index``.\n        \"\"\"\n\n    @Appender(_index_shared_docs['get_indexer'] % _index_doc_kwargs)\n    def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        method = missing.clean_reindex_fill_method(method)\n        target = ensure_index(target)\n        if tolerance is not None:\n            tolerance = self._convert_tolerance(tolerance, target)\n\n        # Treat boolean labels passed to a numeric index as not found. Without\n        # this fix False and True would be treated as 0 and 1 respectively.\n        # (GH #16877)\n        if target.is_boolean() and self.is_numeric():\n            return ensure_platform_int(np.repeat(-1, target.size))\n\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer(ptarget, method=method, limit=limit,\n                                     tolerance=tolerance)\n\n        if not is_dtype_equal(self.dtype, target.dtype):\n            this = self.astype(object)\n            target = target.astype(object)\n            return this.get_indexer(target, method=method, limit=limit,\n                                    tolerance=tolerance)\n\n        if not self.is_unique:\n            raise InvalidIndexError('Reindexing only valid with uniquely'\n                                    ' valued Index objects')\n\n        if method == 'pad' or method == 'backfill':\n            indexer = self._get_fill_indexer(target, method, limit, tolerance)\n        elif method == 'nearest':\n            indexer = self._get_nearest_indexer(target, limit, tolerance)\n        else:\n            if tolerance is not None:\n                raise ValueError('tolerance argument only valid if doing pad, '\n                                 'backfill or nearest reindexing')\n            if limit is not None:\n                raise ValueError('limit argument only valid if doing pad, '\n                                 'backfill or nearest reindexing')\n\n            indexer = self._engine.get_indexer(target._ndarray_values)\n\n        return ensure_platform_int(indexer)\n\n    def _convert_tolerance(self, tolerance, target):\n        # override this method on subclasses\n        tolerance = np.asarray(tolerance)\n        if target.size != tolerance.size and tolerance.size > 1:\n            raise ValueError('list-like tolerance size must match '\n                             'target index size')\n        return tolerance\n\n    def _get_fill_indexer(self, target, method, limit=None, tolerance=None):\n        if self.is_monotonic_increasing and target.is_monotonic_increasing:\n            method = (self._engine.get_pad_indexer if method == 'pad' else\n                      self._engine.get_backfill_indexer)\n            indexer = method(target._ndarray_values, limit)\n        else:\n            indexer = self._get_fill_indexer_searchsorted(target, method,\n                                                          limit)\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target._ndarray_values,\n                                                     indexer,\n                                                     tolerance)\n        return indexer\n\n    def _get_fill_indexer_searchsorted(self, target, method, limit=None):\n        \"\"\"\n        Fallback pad/backfill get_indexer that works for monotonic decreasing\n        indexes and non-monotonic targets\n        \"\"\"\n        if limit is not None:\n            raise ValueError('limit argument for %r method only well-defined '\n                             'if index and target are monotonic' % method)\n\n        side = 'left' if method == 'pad' else 'right'\n\n        # find exact matches first (this simplifies the algorithm)\n        indexer = self.get_indexer(target)\n        nonexact = (indexer == -1)\n        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact],\n                                                         side)\n        if side == 'left':\n            # searchsorted returns \"indices into a sorted array such that,\n            # if the corresponding elements in v were inserted before the\n            # indices, the order of a would be preserved\".\n            # Thus, we need to subtract 1 to find values to the left.\n            indexer[nonexact] -= 1\n            # This also mapped not found values (values of 0 from\n            # np.searchsorted) to -1, which conveniently is also our\n            # sentinel for missing values\n        else:\n            # Mark indices to the right of the largest value as not found\n            indexer[indexer == len(self)] = -1\n        return indexer\n\n    def _get_nearest_indexer(self, target, limit, tolerance):\n        \"\"\"\n        Get the indexer for the nearest index labels; requires an index with\n        values that can be subtracted from each other (e.g., not strings or\n        tuples).\n        \"\"\"\n        left_indexer = self.get_indexer(target, 'pad', limit=limit)\n        right_indexer = self.get_indexer(target, 'backfill', limit=limit)\n\n        target = np.asarray(target)\n        left_distances = abs(self.values[left_indexer] - target)\n        right_distances = abs(self.values[right_indexer] - target)\n\n        op = operator.lt if self.is_monotonic_increasing else operator.le\n        indexer = np.where(op(left_distances, right_distances) |\n                           (right_indexer == -1), left_indexer, right_indexer)\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target, indexer,\n                                                     tolerance)\n        return indexer\n\n    def _filter_indexer_tolerance(self, target, indexer, tolerance):\n        distance = abs(self.values[indexer] - target)\n        indexer = np.where(distance <= tolerance, indexer, -1)\n        return indexer\n\n    _index_shared_docs['get_indexer_non_unique'] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n\n        Returns\n        -------\n        indexer : ndarray of int\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n        missing : ndarray of int\n            An indexer into the target of the values not found.\n            These correspond to the -1 in the indexer array\n        \"\"\"\n\n    @Appender(_index_shared_docs['get_indexer_non_unique'] % _index_doc_kwargs)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        if is_categorical(target):\n            target = target.astype(target.dtype.categories.dtype)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n\n        if self.is_all_dates:\n            self = Index(self.asi8)\n            tgt_values = target.asi8\n        else:\n            tgt_values = target._ndarray_values\n\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n\n    def get_indexer_for(self, target, **kwargs):\n        \"\"\"\n        guaranteed return of an indexer even when non-unique\n        This dispatches to get_indexer or get_indexer_nonunique as appropriate\n        \"\"\"\n        if self.is_unique:\n            return self.get_indexer(target, **kwargs)\n        indexer, _ = self.get_indexer_non_unique(target, **kwargs)\n        return indexer\n\n    def _maybe_promote(self, other):\n        # A hack, but it works\n        from pandas import DatetimeIndex\n        if self.inferred_type == 'date' and isinstance(other, DatetimeIndex):\n            return DatetimeIndex(self), other\n        elif self.inferred_type == 'boolean':\n            if not is_object_dtype(self.dtype):\n                return self.astype('object'), other.astype('object')\n        return self, other\n\n    def groupby(self, values):\n        \"\"\"\n        Group the index labels by a given array of values.\n\n        Parameters\n        ----------\n        values : array\n            Values used to determine the groups.\n\n        Returns\n        -------\n        groups : dict\n            {group name -> group labels}\n        \"\"\"\n\n        # TODO: if we are a MultiIndex, we can do better\n        # that converting to tuples\n        from .multi import MultiIndex\n        if isinstance(values, MultiIndex):\n            values = values.values\n        values = ensure_categorical(values)\n        result = values._reverse_indexer()\n\n        # map to the label\n        result = {k: self.take(v) for k, v in compat.iteritems(result)}\n\n        return result\n\n    def map(self, mapper, na_action=None):\n        \"\"\"\n        Map values using input correspondence (a dict, Series, or function).\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}\n            If 'ignore', propagate NA values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        applied : Union[Index, MultiIndex], inferred\n            The output of the mapping function applied to the index.\n            If the function returns a tuple with more than one element\n            a MultiIndex will be returned.\n        \"\"\"\n\n        from .multi import MultiIndex\n        new_values = super(Index, self)._map_values(\n            mapper, na_action=na_action)\n\n        attributes = self._get_attributes_dict()\n\n        # we can return a MultiIndex\n        if new_values.size and isinstance(new_values[0], tuple):\n            if isinstance(self, MultiIndex):\n                names = self.names\n            elif attributes.get('name'):\n                names = [attributes.get('name')] * len(new_values[0])\n            else:\n                names = None\n            return MultiIndex.from_tuples(new_values,\n                                          names=names)\n\n        attributes['copy'] = False\n        if not new_values.size:\n            # empty\n            attributes['dtype'] = self.dtype\n\n        return Index(new_values, **attributes)\n\n    def isin(self, values, level=None):\n        \"\"\"\n        Return a boolean array where the index values are in `values`.\n\n        Compute boolean array of whether each index value is found in the\n        passed set of values. The length of the returned boolean array matches\n        the length of the index.\n\n        Parameters\n        ----------\n        values : set or list-like\n            Sought values.\n\n            .. versionadded:: 0.18.1\n\n               Support for values as a set.\n\n        level : str or int, optional\n            Name or position of the index level to use (if the index is a\n            `MultiIndex`).\n\n        Returns\n        -------\n        is_contained : ndarray\n            NumPy array of boolean values.\n\n        See Also\n        --------\n        Series.isin : Same for Series.\n        DataFrame.isin : Same method for DataFrames.\n\n        Notes\n        -----\n        In the case of `MultiIndex` you must either specify `values` as a\n        list-like object containing tuples that are the same length as the\n        number of levels, or specify `level`. Otherwise it will raise a\n        ``ValueError``.\n\n        If `level` is specified:\n\n        - if it is the name of one *and only one* index level, use that level;\n        - otherwise it should be a number indicating level position.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1,2,3])\n        >>> idx\n        Int64Index([1, 2, 3], dtype='int64')\n\n        Check whether each index value in a list of values.\n        >>> idx.isin([1, 4])\n        array([ True, False, False])\n\n        >>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n        ...                                  ['red', 'blue', 'green']],\n        ...                                  names=('number', 'color'))\n        >>> midx\n        MultiIndex(levels=[[1, 2, 3], ['blue', 'green', 'red']],\n                   labels=[[0, 1, 2], [2, 0, 1]],\n                   names=['number', 'color'])\n\n        Check whether the strings in the 'color' level of the MultiIndex\n        are in a list of colors.\n\n        >>> midx.isin(['red', 'orange', 'yellow'], level='color')\n        array([ True, False, False])\n\n        To check across the levels of a MultiIndex, pass a list of tuples:\n\n        >>> midx.isin([(1, 'red'), (3, 'red')])\n        array([ True, False, False])\n\n        For a DatetimeIndex, string values in `values` are converted to\n        Timestamps.\n\n        >>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\n        >>> dti = pd.to_datetime(dates)\n        >>> dti\n        DatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\n        dtype='datetime64[ns]', freq=None)\n\n        >>> dti.isin(['2000-03-11'])\n        array([ True, False, False])\n        \"\"\"\n        if level is not None:\n            self._validate_index_level(level)\n        return algos.isin(self, values)\n\n    def _can_reindex(self, indexer):\n        \"\"\"\n        *this is an internal non-public method*\n\n        Check if we are allowing reindexing with this particular indexer\n\n        Parameters\n        ----------\n        indexer : an integer indexer\n\n        Raises\n        ------\n        ValueError if its a duplicate axis\n        \"\"\"\n\n        # trying to reindex on an axis with duplicates\n        if not self.is_unique and len(indexer):\n            raise ValueError(\"cannot reindex from a duplicate axis\")\n\n    def reindex(self, target, method=None, level=None, limit=None,\n                tolerance=None):\n        \"\"\"\n        Create index with target's values (move/add/delete values as necessary)\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index\n        indexer : np.ndarray or None\n            Indices of output values in original index\n\n        \"\"\"\n        # GH6552: preserve names when reindexing to non-named target\n        # (i.e. neither Index nor Series).\n        preserve_names = not hasattr(target, 'name')\n\n        # GH7774: preserve dtype/tz if target is empty and not an Index.\n        target = _ensure_has_len(target)  # target may be an iterator\n\n        if not isinstance(target, Index) and len(target) == 0:\n            attrs = self._get_attributes_dict()\n            attrs.pop('freq', None)  # don't preserve freq\n            values = self._data[:0]  # appropriately-dtyped empty array\n            target = self._simple_new(values, dtype=self.dtype, **attrs)\n        else:\n            target = ensure_index(target)\n\n        if level is not None:\n            if method is not None:\n                raise TypeError('Fill method not supported if level passed')\n            _, indexer, _ = self._join_level(target, level, how='right',\n                                             return_indexers=True)\n        else:\n            if self.equals(target):\n                indexer = None\n            else:\n\n                if self.is_unique:\n                    indexer = self.get_indexer(target, method=method,\n                                               limit=limit,\n                                               tolerance=tolerance)\n                else:\n                    if method is not None or limit is not None:\n                        raise ValueError(\"cannot reindex a non-unique index \"\n                                         \"with a method or limit\")\n                    indexer, missing = self.get_indexer_non_unique(target)\n\n        if preserve_names and target.nlevels == 1 and target.name != self.name:\n            target = target.copy()\n            target.name = self.name\n\n        return target, indexer\n\n    def _reindex_non_unique(self, target):\n        \"\"\"\n        *this is an internal non-public method*\n\n        Create a new index with target's values (move/add/delete values as\n        necessary) use with non-unique Index and a possibly non-unique target\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index\n        indexer : np.ndarray or None\n            Indices of output values in original index\n\n        \"\"\"\n\n        target = ensure_index(target)\n        indexer, missing = self.get_indexer_non_unique(target)\n        check = indexer != -1\n        new_labels = self.take(indexer[check])\n        new_indexer = None\n\n        if len(missing):\n            length = np.arange(len(indexer))\n\n            missing = ensure_platform_int(missing)\n            missing_labels = target.take(missing)\n            missing_indexer = ensure_int64(length[~check])\n            cur_labels = self.take(indexer[check]).values\n            cur_indexer = ensure_int64(length[check])\n\n            new_labels = np.empty(tuple([len(indexer)]), dtype=object)\n            new_labels[cur_indexer] = cur_labels\n            new_labels[missing_indexer] = missing_labels\n\n            # a unique indexer\n            if target.is_unique:\n\n                # see GH5553, make sure we use the right indexer\n                new_indexer = np.arange(len(indexer))\n                new_indexer[cur_indexer] = np.arange(len(cur_labels))\n                new_indexer[missing_indexer] = -1\n\n            # we have a non_unique selector, need to use the original\n            # indexer here\n            else:\n\n                # need to retake to have the same size as the indexer\n                indexer[~check] = -1\n\n                # reset the new indexer to account for the new size\n                new_indexer = np.arange(len(self.take(indexer)))\n                new_indexer[~check] = -1\n\n        new_index = self._shallow_copy_with_infer(new_labels, freq=None)\n        return new_index, indexer, new_indexer\n\n    _index_shared_docs['join'] = \"\"\"\n        *this is an internal non-public method*\n\n        Compute join_index and indexers to conform data\n        structures to the new index.\n\n        Parameters\n        ----------\n        other : Index\n        how : {'left', 'right', 'inner', 'outer'}\n        level : int or level name, default None\n        return_indexers : boolean, default False\n        sort : boolean, default False\n            Sort the join keys lexicographically in the result Index. If False,\n            the order of the join keys depends on the join type (how keyword)\n\n            .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        join_index, (left_indexer, right_indexer)\n        \"\"\"\n\n    @Appender(_index_shared_docs['join'])\n    def join(self, other, how='left', level=None, return_indexers=False,\n             sort=False):\n        from .multi import MultiIndex\n        self_is_mi = isinstance(self, MultiIndex)\n        other_is_mi = isinstance(other, MultiIndex)\n\n        # try to figure out the join level\n        # GH3662\n        if level is None and (self_is_mi or other_is_mi):\n\n            # have the same levels/names so a simple join\n            if self.names == other.names:\n                pass\n            else:\n                return self._join_multi(other, how=how,\n                                        return_indexers=return_indexers)\n\n        # join on the level\n        if level is not None and (self_is_mi or other_is_mi):\n            return self._join_level(other, level, how=how,\n                                    return_indexers=return_indexers)\n\n        other = ensure_index(other)\n\n        if len(other) == 0 and how in ('left', 'outer'):\n            join_index = self._shallow_copy()\n            if return_indexers:\n                rindexer = np.repeat(-1, len(join_index))\n                return join_index, None, rindexer\n            else:\n                return join_index\n\n        if len(self) == 0 and how in ('right', 'outer'):\n            join_index = other._shallow_copy()\n            if return_indexers:\n                lindexer = np.repeat(-1, len(join_index))\n                return join_index, lindexer, None\n            else:\n                return join_index\n\n        if self._join_precedence < other._join_precedence:\n            how = {'right': 'left', 'left': 'right'}.get(how, how)\n            result = other.join(self, how=how, level=level,\n                                return_indexers=return_indexers)\n            if return_indexers:\n                x, y, z = result\n                result = x, z, y\n            return result\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            this = self.astype('O')\n            other = other.astype('O')\n            return this.join(other, how=how, return_indexers=return_indexers)\n\n        _validate_join_method(how)\n\n        if not self.is_unique and not other.is_unique:\n            return self._join_non_unique(other, how=how,\n                                         return_indexers=return_indexers)\n        elif not self.is_unique or not other.is_unique:\n            if self.is_monotonic and other.is_monotonic:\n                return self._join_monotonic(other, how=how,\n                                            return_indexers=return_indexers)\n            else:\n                return self._join_non_unique(other, how=how,\n                                             return_indexers=return_indexers)\n        elif self.is_monotonic and other.is_monotonic:\n            try:\n                return self._join_monotonic(other, how=how,\n                                            return_indexers=return_indexers)\n            except TypeError:\n                pass\n\n        if how == 'left':\n            join_index = self\n        elif how == 'right':\n            join_index = other\n        elif how == 'inner':\n            join_index = self.intersection(other)\n        elif how == 'outer':\n            join_index = self.union(other)\n\n        if sort:\n            join_index = join_index.sort_values()\n\n        if return_indexers:\n            if join_index is self:\n                lindexer = None\n            else:\n                lindexer = self.get_indexer(join_index)\n            if join_index is other:\n                rindexer = None\n            else:\n                rindexer = other.get_indexer(join_index)\n            return join_index, lindexer, rindexer\n        else:\n            return join_index\n\n    def _join_multi(self, other, how, return_indexers=True):\n        from .multi import MultiIndex\n        from pandas.core.reshape.merge import _restore_dropped_levels_multijoin\n\n        # figure out join names\n        self_names = set(com._not_none(*self.names))\n        other_names = set(com._not_none(*other.names))\n        overlap = self_names & other_names\n\n        # need at least 1 in common\n        if not overlap:\n            raise ValueError(\"cannot join with no overlapping index names\")\n\n        self_is_mi = isinstance(self, MultiIndex)\n        other_is_mi = isinstance(other, MultiIndex)\n\n        if self_is_mi and other_is_mi:\n\n            # Drop the non-matching levels from left and right respectively\n            ldrop_names = list(self_names - overlap)\n            rdrop_names = list(other_names - overlap)\n\n            self_jnlevels = self.droplevel(ldrop_names)\n            other_jnlevels = other.droplevel(rdrop_names)\n\n            # Join left and right\n            # Join on same leveled multi-index frames is supported\n            join_idx, lidx, ridx = self_jnlevels.join(other_jnlevels, how,\n                                                      return_indexers=True)\n\n            # Restore the dropped levels\n            # Returned index level order is\n            # common levels, ldrop_names, rdrop_names\n            dropped_names = ldrop_names + rdrop_names\n\n            levels, labels, names = (\n                _restore_dropped_levels_multijoin(self, other,\n                                                  dropped_names,\n                                                  join_idx,\n                                                  lidx, ridx))\n\n            # Re-create the multi-index\n            multi_join_idx = MultiIndex(levels=levels, labels=labels,\n                                        names=names, verify_integrity=False)\n\n            multi_join_idx = multi_join_idx.remove_unused_levels()\n\n            return multi_join_idx, lidx, ridx\n\n        jl = list(overlap)[0]\n\n        # Case where only one index is multi\n        # make the indices into mi's that match\n        flip_order = False\n        if self_is_mi:\n            self, other = other, self\n            flip_order = True\n            # flip if join method is right or left\n            how = {'right': 'left', 'left': 'right'}.get(how, how)\n\n        level = other.names.index(jl)\n        result = self._join_level(other, level, how=how,\n                                  return_indexers=return_indexers)\n\n        if flip_order:\n            if isinstance(result, tuple):\n                return result[0], result[2], result[1]\n        return result\n\n    def _join_non_unique(self, other, how='left', return_indexers=False):\n        from pandas.core.reshape.merge import _get_join_indexers\n\n        left_idx, right_idx = _get_join_indexers([self._ndarray_values],\n                                                 [other._ndarray_values],\n                                                 how=how,\n                                                 sort=True)\n\n        left_idx = ensure_platform_int(left_idx)\n        right_idx = ensure_platform_int(right_idx)\n\n        join_index = np.asarray(self._ndarray_values.take(left_idx))\n        mask = left_idx == -1\n        np.putmask(join_index, mask, other._ndarray_values.take(right_idx))\n\n        join_index = self._wrap_joined_index(join_index, other)\n\n        if return_indexers:\n            return join_index, left_idx, right_idx\n        else:\n            return join_index\n\n    def _join_level(self, other, level, how='left', return_indexers=False,\n                    keep_order=True):\n        \"\"\"\n        The join method *only* affects the level of the resulting\n        MultiIndex. Otherwise it just exactly aligns the Index data to the\n        labels of the level in the MultiIndex. If `keep_order` == True, the\n        order of the data indexed by the MultiIndex will not be changed;\n        otherwise, it will tie out with `other`.\n        \"\"\"\n        from .multi import MultiIndex\n\n        def _get_leaf_sorter(labels):\n            \"\"\"\n            returns sorter for the inner most level while preserving the\n            order of higher levels\n            \"\"\"\n            if labels[0].size == 0:\n                return np.empty(0, dtype='int64')\n\n            if len(labels) == 1:\n                lab = ensure_int64(labels[0])\n                sorter, _ = libalgos.groupsort_indexer(lab, 1 + lab.max())\n                return sorter\n\n            # find indexers of beginning of each set of\n            # same-key labels w.r.t all but last level\n            tic = labels[0][:-1] != labels[0][1:]\n            for lab in labels[1:-1]:\n                tic |= lab[:-1] != lab[1:]\n\n            starts = np.hstack(([True], tic, [True])).nonzero()[0]\n            lab = ensure_int64(labels[-1])\n            return lib.get_level_sorter(lab, ensure_int64(starts))\n\n        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n            raise TypeError('Join on level between two MultiIndex objects '\n                            'is ambiguous')\n\n        left, right = self, other\n\n        flip_order = not isinstance(self, MultiIndex)\n        if flip_order:\n            left, right = right, left\n            how = {'right': 'left', 'left': 'right'}.get(how, how)\n\n        level = left._get_level_number(level)\n        old_level = left.levels[level]\n\n        if not right.is_unique:\n            raise NotImplementedError('Index._join_level on non-unique index '\n                                      'is not implemented')\n\n        new_level, left_lev_indexer, right_lev_indexer = \\\n            old_level.join(right, how=how, return_indexers=True)\n\n        if left_lev_indexer is None:\n            if keep_order or len(left) == 0:\n                left_indexer = None\n                join_index = left\n            else:  # sort the leaves\n                left_indexer = _get_leaf_sorter(left.labels[:level + 1])\n                join_index = left[left_indexer]\n\n        else:\n            left_lev_indexer = ensure_int64(left_lev_indexer)\n            rev_indexer = lib.get_reverse_indexer(left_lev_indexer,\n                                                  len(old_level))\n\n            new_lev_labels = algos.take_nd(rev_indexer, left.labels[level],\n                                           allow_fill=False)\n\n            new_labels = list(left.labels)\n            new_labels[level] = new_lev_labels\n\n            new_levels = list(left.levels)\n            new_levels[level] = new_level\n\n            if keep_order:  # just drop missing values. o.w. keep order\n                left_indexer = np.arange(len(left), dtype=np.intp)\n                mask = new_lev_labels != -1\n                if not mask.all():\n                    new_labels = [lab[mask] for lab in new_labels]\n                    left_indexer = left_indexer[mask]\n\n            else:  # tie out the order with other\n                if level == 0:  # outer most level, take the fast route\n                    ngroups = 1 + new_lev_labels.max()\n                    left_indexer, counts = libalgos.groupsort_indexer(\n                        new_lev_labels, ngroups)\n\n                    # missing values are placed first; drop them!\n                    left_indexer = left_indexer[counts[0]:]\n                    new_labels = [lab[left_indexer] for lab in new_labels]\n\n                else:  # sort the leaves\n                    mask = new_lev_labels != -1\n                    mask_all = mask.all()\n                    if not mask_all:\n                        new_labels = [lab[mask] for lab in new_labels]\n\n                    left_indexer = _get_leaf_sorter(new_labels[:level + 1])\n                    new_labels = [lab[left_indexer] for lab in new_labels]\n\n                    # left_indexers are w.r.t masked frame.\n                    # reverse to original frame!\n                    if not mask_all:\n                        left_indexer = mask.nonzero()[0][left_indexer]\n\n            join_index = MultiIndex(levels=new_levels, labels=new_labels,\n                                    names=left.names, verify_integrity=False)\n\n        if right_lev_indexer is not None:\n            right_indexer = algos.take_nd(right_lev_indexer,\n                                          join_index.labels[level],\n                                          allow_fill=False)\n        else:\n            right_indexer = join_index.labels[level]\n\n        if flip_order:\n            left_indexer, right_indexer = right_indexer, left_indexer\n\n        if return_indexers:\n            left_indexer = (None if left_indexer is None\n                            else ensure_platform_int(left_indexer))\n            right_indexer = (None if right_indexer is None\n                             else ensure_platform_int(right_indexer))\n            return join_index, left_indexer, right_indexer\n        else:\n            return join_index\n\n    def _join_monotonic(self, other, how='left', return_indexers=False):\n        if self.equals(other):\n            ret_index = other if how == 'right' else self\n            if return_indexers:\n                return ret_index, None, None\n            else:\n                return ret_index\n\n        sv = self._ndarray_values\n        ov = other._ndarray_values\n\n        if self.is_unique and other.is_unique:\n            # We can perform much better than the general case\n            if how == 'left':\n                join_index = self\n                lidx = None\n                ridx = self._left_indexer_unique(sv, ov)\n            elif how == 'right':\n                join_index = other\n                lidx = self._left_indexer_unique(ov, sv)\n                ridx = None\n            elif how == 'inner':\n                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n                join_index = self._wrap_joined_index(join_index, other)\n            elif how == 'outer':\n                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n                join_index = self._wrap_joined_index(join_index, other)\n        else:\n            if how == 'left':\n                join_index, lidx, ridx = self._left_indexer(sv, ov)\n            elif how == 'right':\n                join_index, ridx, lidx = self._left_indexer(ov, sv)\n            elif how == 'inner':\n                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n            elif how == 'outer':\n                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n            join_index = self._wrap_joined_index(join_index, other)\n\n        if return_indexers:\n            lidx = None if lidx is None else ensure_platform_int(lidx)\n            ridx = None if ridx is None else ensure_platform_int(ridx)\n            return join_index, lidx, ridx\n        else:\n            return join_index\n\n    def _wrap_joined_index(self, joined, other):\n        name = get_op_result_name(self, other)\n        return Index(joined, name=name)\n\n    def _get_string_slice(self, key, use_lhs=True, use_rhs=True):\n        # this is for partial string indexing,\n        # overridden in DatetimeIndex, TimedeltaIndex and PeriodIndex\n        raise NotImplementedError\n\n    def slice_indexer(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        For an ordered or unique index, compute the slice indexer for input\n        labels and step.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning\n        end : label, default None\n            If None, defaults to the end\n        step : int, default None\n        kind : string, default None\n\n        Returns\n        -------\n        indexer : slice\n\n        Raises\n        ------\n        KeyError : If key does not exist, or key is not unique and index is\n            not ordered.\n\n        Notes\n        -----\n        This function assumes that the data is sorted, so use at your own peril\n\n        Examples\n        ---------\n        This is a method on all index types. For example you can do:\n\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_indexer(start='b', end='c')\n        slice(1, 3)\n\n        >>> idx = pd.MultiIndex.from_arrays([list('abcd'), list('efgh')])\n        >>> idx.slice_indexer(start='b', end=('c', 'g'))\n        slice(1, 3)\n        \"\"\"\n        start_slice, end_slice = self.slice_locs(start, end, step=step,\n                                                 kind=kind)\n\n        # return a slice\n        if not is_scalar(start_slice):\n            raise AssertionError(\"Start slice bound is non-scalar\")\n        if not is_scalar(end_slice):\n            raise AssertionError(\"End slice bound is non-scalar\")\n\n        return slice(start_slice, end_slice, step)\n\n    def _maybe_cast_indexer(self, key):\n        \"\"\"\n        If we have a float key and are not a floating index\n        then try to cast to an int if equivalent\n        \"\"\"\n\n        if is_float(key) and not self.is_floating():\n            try:\n                ckey = int(key)\n                if ckey == key:\n                    key = ckey\n            except (OverflowError, ValueError, TypeError):\n                pass\n        return key\n\n    def _validate_indexer(self, form, key, kind):\n        \"\"\"\n        if we are positional indexer\n        validate that we have appropriate typed bounds\n        must be an integer\n        \"\"\"\n        assert kind in ['ix', 'loc', 'getitem', 'iloc']\n\n        if key is None:\n            pass\n        elif is_integer(key):\n            pass\n        elif kind in ['iloc', 'getitem']:\n            self._invalid_indexer(form, key)\n        return key\n\n    _index_shared_docs['_maybe_cast_slice_bound'] = \"\"\"\n        This function should be overloaded in subclasses that allow non-trivial\n        casting on label-slice bounds, e.g. datetime-like indices allowing\n        strings containing formatted datetimes.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'ix', 'loc', 'getitem'}\n\n        Returns\n        -------\n        label :  object\n\n        Notes\n        -----\n        Value of `side` parameter should be validated in caller.\n\n        \"\"\"\n\n    @Appender(_index_shared_docs['_maybe_cast_slice_bound'])\n    def _maybe_cast_slice_bound(self, label, side, kind):\n        assert kind in ['ix', 'loc', 'getitem', None]\n\n        # We are a plain index here (sub-class override this method if they\n        # wish to have special treatment for floats/ints, e.g. Float64Index and\n        # datetimelike Indexes\n        # reject them\n        if is_float(label):\n            if not (kind in ['ix'] and (self.holds_integer() or\n                                        self.is_floating())):\n                self._invalid_indexer('slice', label)\n\n        # we are trying to find integer bounds on a non-integer based index\n        # this is rejected (generally .loc gets you here)\n        elif is_integer(label):\n            self._invalid_indexer('slice', label)\n\n        return label\n\n    def _searchsorted_monotonic(self, label, side='left'):\n        if self.is_monotonic_increasing:\n            return self.searchsorted(label, side=side)\n        elif self.is_monotonic_decreasing:\n            # np.searchsorted expects ascending sort order, have to reverse\n            # everything for it to work (element ordering, search side and\n            # resulting value).\n            pos = self[::-1].searchsorted(label, side='right' if side == 'left'\n                                          else 'left')\n            return len(self) - pos\n\n        raise ValueError('index must be monotonic increasing or decreasing')\n\n    def _get_loc_only_exact_matches(self, key):\n        \"\"\"\n        This is overridden on subclasses (namely, IntervalIndex) to control\n        get_slice_bound.\n        \"\"\"\n        return self.get_loc(key)\n\n    def get_slice_bound(self, label, side, kind):\n        \"\"\"\n        Calculate slice bound that corresponds to given label.\n\n        Returns leftmost (one-past-the-rightmost if ``side=='right'``) position\n        of given label.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'ix', 'loc', 'getitem'}\n        \"\"\"\n        assert kind in ['ix', 'loc', 'getitem', None]\n\n        if side not in ('left', 'right'):\n            raise ValueError(\"Invalid value for side kwarg,\"\n                             \" must be either 'left' or 'right': %s\" %\n                             (side, ))\n\n        original_label = label\n\n        # For datetime indices label may be a string that has to be converted\n        # to datetime boundary according to its resolution.\n        label = self._maybe_cast_slice_bound(label, side, kind)\n\n        # we need to look up the label\n        try:\n            slc = self._get_loc_only_exact_matches(label)\n        except KeyError as err:\n            try:\n                return self._searchsorted_monotonic(label, side)\n            except ValueError:\n                # raise the original KeyError\n                raise err\n\n        if isinstance(slc, np.ndarray):\n            # get_loc may return a boolean array or an array of indices, which\n            # is OK as long as they are representable by a slice.\n            if is_bool_dtype(slc):\n                slc = lib.maybe_booleans_to_slice(slc.view('u1'))\n            else:\n                slc = lib.maybe_indices_to_slice(slc.astype('i8'), len(self))\n            if isinstance(slc, np.ndarray):\n                raise KeyError(\"Cannot get %s slice bound for non-unique \"\n                               \"label: %r\" % (side, original_label))\n\n        if isinstance(slc, slice):\n            if side == 'left':\n                return slc.start\n            else:\n                return slc.stop\n        else:\n            if side == 'right':\n                return slc + 1\n            else:\n                return slc\n\n    def slice_locs(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        Compute slice locations for input labels.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning\n        end : label, default None\n            If None, defaults to the end\n        step : int, defaults None\n            If None, defaults to 1\n        kind : {'ix', 'loc', 'getitem'} or None\n\n        Returns\n        -------\n        start, end : int\n\n        Notes\n        -----\n        This method only works if the index is monotonic or unique.\n\n        Examples\n        ---------\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_locs(start='b', end='c')\n        (1, 3)\n\n        See Also\n        --------\n        Index.get_loc : Get location for a single label.\n        \"\"\"\n        inc = (step is None or step >= 0)\n\n        if not inc:\n            # If it's a reverse slice, temporarily swap bounds.\n            start, end = end, start\n\n        start_slice = None\n        if start is not None:\n            start_slice = self.get_slice_bound(start, 'left', kind)\n        if start_slice is None:\n            start_slice = 0\n\n        end_slice = None\n        if end is not None:\n            end_slice = self.get_slice_bound(end, 'right', kind)\n        if end_slice is None:\n            end_slice = len(self)\n\n        if not inc:\n            # Bounds at this moment are swapped, swap them back and shift by 1.\n            #\n            # slice_locs('B', 'A', step=-1): s='B', e='A'\n            #\n            #              s='A'                 e='B'\n            # AFTER SWAP:    |                     |\n            #                v ------------------> V\n            #           -----------------------------------\n            #           | | |A|A|A|A| | | | | |B|B| | | | |\n            #           -----------------------------------\n            #              ^ <------------------ ^\n            # SHOULD BE:   |                     |\n            #           end=s-1              start=e-1\n            #\n            end_slice, start_slice = start_slice - 1, end_slice - 1\n\n            # i == -1 triggers ``len(self) + i`` selection that points to the\n            # last element, not before-the-first one, subtracting len(self)\n            # compensates that.\n            if end_slice == -1:\n                end_slice -= len(self)\n            if start_slice == -1:\n                start_slice -= len(self)\n\n        return start_slice, end_slice\n\n    def delete(self, loc):\n        \"\"\"\n        Make new Index with passed location(-s) deleted\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        return self._shallow_copy(np.delete(self._data, loc))\n\n    def insert(self, loc, item):\n        \"\"\"\n        Make new Index inserting new item at location. Follows\n        Python list.append semantics for negative values\n\n        Parameters\n        ----------\n        loc : int\n        item : object\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        _self = np.asarray(self)\n        item = self._coerce_scalar_to_index(item)._ndarray_values\n        idx = np.concatenate((_self[:loc], item, _self[loc:]))\n        return self._shallow_copy_with_infer(idx)\n\n    def drop(self, labels, errors='raise'):\n        \"\"\"\n        Make new Index with passed list of labels deleted\n\n        Parameters\n        ----------\n        labels : array-like\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and existing labels are dropped.\n\n        Returns\n        -------\n        dropped : Index\n\n        Raises\n        ------\n        KeyError\n            If not all of the labels are found in the selected axis\n        \"\"\"\n        arr_dtype = 'object' if self.dtype == 'object' else None\n        labels = com.index_labels_to_array(labels, dtype=arr_dtype)\n        indexer = self.get_indexer(labels)\n        mask = indexer == -1\n        if mask.any():\n            if errors != 'ignore':\n                raise KeyError(\n                    '{} not found in axis'.format(labels[mask]))\n            indexer = indexer[~mask]\n        return self.delete(indexer)\n\n    _index_shared_docs['index_unique'] = (\n        \"\"\"\n        Return unique values in the index. Uniques are returned in order\n        of appearance, this does NOT sort.\n\n        Parameters\n        ----------\n        level : int or str, optional, default None\n            Only return values from specified level (for MultiIndex)\n\n            .. versionadded:: 0.23.0\n\n        Returns\n        -------\n        Index without duplicates\n\n        See Also\n        --------\n        unique\n        Series.unique\n        \"\"\")\n\n    @Appender(_index_shared_docs['index_unique'] % _index_doc_kwargs)\n    def unique(self, level=None):\n        if level is not None:\n            self._validate_index_level(level)\n        result = super(Index, self).unique()\n        return self._shallow_copy(result)\n\n    def drop_duplicates(self, keep='first'):\n        \"\"\"\n        Return Index with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        Returns\n        -------\n        deduplicated : Index\n\n        See Also\n        --------\n        Series.drop_duplicates : Equivalent method on Series.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Index.duplicated : Related method on Index, indicating duplicate\n            Index values.\n\n        Examples\n        --------\n        Generate an pandas.Index with duplicate values.\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])\n\n        The `keep` parameter controls  which duplicate values are removed.\n        The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> idx.drop_duplicates(keep='first')\n        Index(['lama', 'cow', 'beetle', 'hippo'], dtype='object')\n\n        The value 'last' keeps the last occurrence for each set of duplicated\n        entries.\n\n        >>> idx.drop_duplicates(keep='last')\n        Index(['cow', 'beetle', 'lama', 'hippo'], dtype='object')\n\n        The value ``False`` discards all sets of duplicated entries.\n\n        >>> idx.drop_duplicates(keep=False)\n        Index(['cow', 'beetle', 'hippo'], dtype='object')\n        \"\"\"\n        return super(Index, self).drop_duplicates(keep=keep)\n\n    def duplicated(self, keep='first'):\n        \"\"\"\n        Indicate duplicate index values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        array. Either all duplicates, all except the first, or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            The value or values in a set of duplicates to mark as missing.\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set to False and all others to True:\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> idx.duplicated()\n        array([False, False,  True, False,  True])\n\n        which is equivalent to\n\n        >>> idx.duplicated(keep='first')\n        array([False, False,  True, False,  True])\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> idx.duplicated(keep='last')\n        array([ True, False,  True, False, False])\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> idx.duplicated(keep=False)\n        array([ True, False,  True, False,  True])\n\n        Returns\n        -------\n        numpy.ndarray\n\n        See Also\n        --------\n        pandas.Series.duplicated : Equivalent method on pandas.Series.\n        pandas.DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        pandas.Index.drop_duplicates : Remove duplicate values from Index.\n        \"\"\"\n        return super(Index, self).duplicated(keep=keep)\n\n    _index_shared_docs['fillna'] = \"\"\"\n        Fill NA/NaN values with the specified value\n\n        Parameters\n        ----------\n        value : scalar\n            Scalar value to use to fill holes (e.g. 0).\n            This value cannot be a list-likes.\n        downcast : dict, default is None\n            a dict of item->dtype of what to downcast if possible,\n            or the string 'infer' which will try to downcast to an appropriate\n            equal type (e.g. float64 to int64 if possible)\n\n        Returns\n        -------\n        filled : %(klass)s\n        \"\"\"\n\n    @Appender(_index_shared_docs['fillna'])\n    def fillna(self, value=None, downcast=None):\n        self._assert_can_do_op(value)\n        if self.hasnans:\n            result = self.putmask(self._isnan, value)\n            if downcast is None:\n                # no need to care metadata other than name\n                # because it can't have freq if\n                return Index(result, name=self.name)\n        return self._shallow_copy()\n\n    _index_shared_docs['dropna'] = \"\"\"\n        Return Index without NA/NaN values\n\n        Parameters\n        ----------\n        how :  {'any', 'all'}, default 'any'\n            If the Index is a MultiIndex, drop the value when any or all levels\n            are NaN.\n\n        Returns\n        -------\n        valid : Index\n        \"\"\"\n\n    @Appender(_index_shared_docs['dropna'])\n    def dropna(self, how='any'):\n        if how not in ('any', 'all'):\n            raise ValueError(\"invalid how option: {0}\".format(how))\n\n        if self.hasnans:\n            return self._shallow_copy(self.values[~self._isnan])\n        return self._shallow_copy()\n\n    def _evaluate_with_timedelta_like(self, other, op):\n        # Timedelta knows how to operate with np.array, so dispatch to that\n        # operation and then wrap the results\n        if self._is_numeric_dtype and op.__name__ in ['add', 'sub',\n                                                      'radd', 'rsub']:\n            raise TypeError(\"Operation {opname} between {cls} and {other} \"\n                            \"is invalid\".format(opname=op.__name__,\n                                                cls=self.dtype,\n                                                other=type(other).__name__))\n\n        other = Timedelta(other)\n        values = self.values\n\n        with np.errstate(all='ignore'):\n            result = op(values, other)\n\n        attrs = self._get_attributes_dict()\n        attrs = self._maybe_update_attributes(attrs)\n        if op == divmod:\n            return Index(result[0], **attrs), Index(result[1], **attrs)\n        return Index(result, **attrs)\n\n    def _evaluate_with_datetime_like(self, other, op):\n        raise TypeError(\"can only perform ops with datetime like values\")\n\n    @classmethod\n    def _add_comparison_methods(cls):\n        \"\"\" add in comparison methods \"\"\"\n        cls.__eq__ = _make_comparison_op(operator.eq, cls)\n        cls.__ne__ = _make_comparison_op(operator.ne, cls)\n        cls.__lt__ = _make_comparison_op(operator.lt, cls)\n        cls.__gt__ = _make_comparison_op(operator.gt, cls)\n        cls.__le__ = _make_comparison_op(operator.le, cls)\n        cls.__ge__ = _make_comparison_op(operator.ge, cls)\n\n    @classmethod\n    def _add_numeric_methods_add_sub_disabled(cls):\n        \"\"\" add in the numeric add/sub methods to disable \"\"\"\n        cls.__add__ = make_invalid_op('__add__')\n        cls.__radd__ = make_invalid_op('__radd__')\n        cls.__iadd__ = make_invalid_op('__iadd__')\n        cls.__sub__ = make_invalid_op('__sub__')\n        cls.__rsub__ = make_invalid_op('__rsub__')\n        cls.__isub__ = make_invalid_op('__isub__')\n\n    @classmethod\n    def _add_numeric_methods_disabled(cls):\n        \"\"\" add in numeric methods to disable other than add/sub \"\"\"\n        cls.__pow__ = make_invalid_op('__pow__')\n        cls.__rpow__ = make_invalid_op('__rpow__')\n        cls.__mul__ = make_invalid_op('__mul__')\n        cls.__rmul__ = make_invalid_op('__rmul__')\n        cls.__floordiv__ = make_invalid_op('__floordiv__')\n        cls.__rfloordiv__ = make_invalid_op('__rfloordiv__')\n        cls.__truediv__ = make_invalid_op('__truediv__')\n        cls.__rtruediv__ = make_invalid_op('__rtruediv__')\n        if not compat.PY3:\n            cls.__div__ = make_invalid_op('__div__')\n            cls.__rdiv__ = make_invalid_op('__rdiv__')\n        cls.__mod__ = make_invalid_op('__mod__')\n        cls.__divmod__ = make_invalid_op('__divmod__')\n        cls.__neg__ = make_invalid_op('__neg__')\n        cls.__pos__ = make_invalid_op('__pos__')\n        cls.__abs__ = make_invalid_op('__abs__')\n        cls.__inv__ = make_invalid_op('__inv__')\n\n    def _maybe_update_attributes(self, attrs):\n        \"\"\" Update Index attributes (e.g. freq) depending on op \"\"\"\n        return attrs\n\n    def _validate_for_numeric_unaryop(self, op, opstr):\n        \"\"\" validate if we can perform a numeric unary operation \"\"\"\n\n        if not self._is_numeric_dtype:\n            raise TypeError(\"cannot evaluate a numeric op \"\n                            \"{opstr} for type: {typ}\"\n                            .format(opstr=opstr, typ=type(self).__name__))\n\n    def _validate_for_numeric_binop(self, other, op):\n        \"\"\"\n        return valid other, evaluate or raise TypeError\n        if we are not of the appropriate type\n\n        internal method called by ops\n        \"\"\"\n        opstr = '__{opname}__'.format(opname=op.__name__)\n        # if we are an inheritor of numeric,\n        # but not actually numeric (e.g. DatetimeIndex/PeriodIndex)\n        if not self._is_numeric_dtype:\n            raise TypeError(\"cannot evaluate a numeric op {opstr} \"\n                            \"for type: {typ}\"\n                            .format(opstr=opstr, typ=type(self).__name__))\n\n        if isinstance(other, Index):\n            if not other._is_numeric_dtype:\n                raise TypeError(\"cannot evaluate a numeric op \"\n                                \"{opstr} with type: {typ}\"\n                                .format(opstr=opstr, typ=type(other)))\n        elif isinstance(other, np.ndarray) and not other.ndim:\n            other = other.item()\n\n        if isinstance(other, (Index, ABCSeries, np.ndarray)):\n            if len(self) != len(other):\n                raise ValueError(\"cannot evaluate a numeric op with \"\n                                 \"unequal lengths\")\n            other = com.values_from_object(other)\n            if other.dtype.kind not in ['f', 'i', 'u']:\n                raise TypeError(\"cannot evaluate a numeric op \"\n                                \"with a non-numeric dtype\")\n        elif isinstance(other, (ABCDateOffset, np.timedelta64, timedelta)):\n            # higher up to handle\n            pass\n        elif isinstance(other, (datetime, np.datetime64)):\n            # higher up to handle\n            pass\n        else:\n            if not (is_float(other) or is_integer(other)):\n                raise TypeError(\"can only perform ops with scalar values\")\n\n        return other\n\n    @classmethod\n    def _add_numeric_methods_binary(cls):\n        \"\"\" add in numeric methods \"\"\"\n        cls.__add__ = _make_arithmetic_op(operator.add, cls)\n        cls.__radd__ = _make_arithmetic_op(ops.radd, cls)\n        cls.__sub__ = _make_arithmetic_op(operator.sub, cls)\n        cls.__rsub__ = _make_arithmetic_op(ops.rsub, cls)\n        cls.__mul__ = _make_arithmetic_op(operator.mul, cls)\n        cls.__rmul__ = _make_arithmetic_op(ops.rmul, cls)\n        cls.__rpow__ = _make_arithmetic_op(ops.rpow, cls)\n        cls.__pow__ = _make_arithmetic_op(operator.pow, cls)\n        cls.__mod__ = _make_arithmetic_op(operator.mod, cls)\n        cls.__floordiv__ = _make_arithmetic_op(operator.floordiv, cls)\n        cls.__rfloordiv__ = _make_arithmetic_op(ops.rfloordiv, cls)\n        cls.__truediv__ = _make_arithmetic_op(operator.truediv, cls)\n        cls.__rtruediv__ = _make_arithmetic_op(ops.rtruediv, cls)\n        if not compat.PY3:\n            cls.__div__ = _make_arithmetic_op(operator.div, cls)\n            cls.__rdiv__ = _make_arithmetic_op(ops.rdiv, cls)\n\n        cls.__divmod__ = _make_arithmetic_op(divmod, cls)\n\n    @classmethod\n    def _add_numeric_methods_unary(cls):\n        \"\"\" add in numeric unary methods \"\"\"\n\n        def _make_evaluate_unary(op, opstr):\n\n            def _evaluate_numeric_unary(self):\n\n                self._validate_for_numeric_unaryop(op, opstr)\n                attrs = self._get_attributes_dict()\n                attrs = self._maybe_update_attributes(attrs)\n                return Index(op(self.values), **attrs)\n\n            return _evaluate_numeric_unary\n\n        cls.__neg__ = _make_evaluate_unary(operator.neg, '__neg__')\n        cls.__pos__ = _make_evaluate_unary(operator.pos, '__pos__')\n        cls.__abs__ = _make_evaluate_unary(np.abs, '__abs__')\n        cls.__inv__ = _make_evaluate_unary(lambda x: -x, '__inv__')\n\n    @classmethod\n    def _add_numeric_methods(cls):\n        cls._add_numeric_methods_unary()\n        cls._add_numeric_methods_binary()\n\n    @classmethod\n    def _add_logical_methods(cls):\n        \"\"\" add in logical methods \"\"\"\n\n        _doc = \"\"\"\n        %(desc)s\n\n        Parameters\n        ----------\n        *args\n            These parameters will be passed to numpy.%(outname)s.\n        **kwargs\n            These parameters will be passed to numpy.%(outname)s.\n\n        Returns\n        -------\n        %(outname)s : bool or array_like (if axis is specified)\n            A single element array_like may be converted to bool.\"\"\"\n\n        _index_shared_docs['index_all'] = dedent(\"\"\"\n\n        See Also\n        --------\n        pandas.Index.any : Return whether any element in an Index is True.\n        pandas.Series.any : Return whether any element in a Series is True.\n        pandas.Series.all : Return whether all elements in a Series are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        **all**\n\n        True, because nonzero integers are considered True.\n\n        >>> pd.Index([1, 2, 3]).all()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 1, 2]).all()\n        False\n\n        **any**\n\n        True, because ``1`` is considered True.\n\n        >>> pd.Index([0, 0, 1]).any()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 0, 0]).any()\n        False\n        \"\"\")\n\n        _index_shared_docs['index_any'] = dedent(\"\"\"\n\n        See Also\n        --------\n        pandas.Index.all : Return whether all elements are True.\n        pandas.Series.all : Return whether all elements are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        >>> index = pd.Index([0, 1, 2])\n        >>> index.any()\n        True\n\n        >>> index = pd.Index([0, 0, 0])\n        >>> index.any()\n        False\n        \"\"\")\n\n        def _make_logical_function(name, desc, f):\n            @Substitution(outname=name, desc=desc)\n            @Appender(_index_shared_docs['index_' + name])\n            @Appender(_doc)\n            def logical_func(self, *args, **kwargs):\n                result = f(self.values)\n                if (isinstance(result, (np.ndarray, ABCSeries, Index)) and\n                        result.ndim == 0):\n                    # return NumPy type\n                    return result.dtype.type(result.item())\n                else:  # pragma: no cover\n                    return result\n\n            logical_func.__name__ = name\n            return logical_func\n\n        cls.all = _make_logical_function('all', 'Return whether all elements '\n                                                'are True.',\n                                         np.all)\n        cls.any = _make_logical_function('any',\n                                         'Return whether any element is True.',\n                                         np.any)\n\n    @classmethod\n    def _add_logical_methods_disabled(cls):\n        \"\"\" add in logical methods to disable \"\"\"\n        cls.all = make_invalid_op('all')\n        cls.any = make_invalid_op('any')\n\n\nIndex._add_numeric_methods_disabled()\nIndex._add_logical_methods()\nIndex._add_comparison_methods()\n\n\ndef ensure_index_from_sequences(sequences, names=None):\n    \"\"\"Construct an index from sequences of data.\n\n    A single sequence returns an Index. Many sequences returns a\n    MultiIndex.\n\n    Parameters\n    ----------\n    sequences : sequence of sequences\n    names : sequence of str\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    Examples\n    --------\n    >>> ensure_index_from_sequences([[1, 2, 3]], names=['name'])\n    Int64Index([1, 2, 3], dtype='int64', name='name')\n\n    >>> ensure_index_from_sequences([['a', 'a'], ['a', 'b']],\n                                    names=['L1', 'L2'])\n    MultiIndex(levels=[['a'], ['a', 'b']],\n               labels=[[0, 0], [0, 1]],\n               names=['L1', 'L2'])\n\n    See Also\n    --------\n    ensure_index\n    \"\"\"\n    from .multi import MultiIndex\n\n    if len(sequences) == 1:\n        if names is not None:\n            names = names[0]\n        return Index(sequences[0], name=names)\n    else:\n        return MultiIndex.from_arrays(sequences, names=names)\n\n\ndef ensure_index(index_like, copy=False):\n    \"\"\"\n    Ensure that we have an index from some index-like object\n\n    Parameters\n    ----------\n    index : sequence\n        An Index or other sequence\n    copy : bool\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    Examples\n    --------\n    >>> ensure_index(['a', 'b'])\n    Index(['a', 'b'], dtype='object')\n\n    >>> ensure_index([('a', 'a'),  ('b', 'c')])\n    Index([('a', 'a'), ('b', 'c')], dtype='object')\n\n    >>> ensure_index([['a', 'a'], ['b', 'c']])\n    MultiIndex(levels=[['a'], ['b', 'c']],\n               labels=[[0, 0], [0, 1]])\n\n    See Also\n    --------\n    ensure_index_from_sequences\n    \"\"\"\n    if isinstance(index_like, Index):\n        if copy:\n            index_like = index_like.copy()\n        return index_like\n    if hasattr(index_like, 'name'):\n        return Index(index_like, name=index_like.name, copy=copy)\n\n    if is_iterator(index_like):\n        index_like = list(index_like)\n\n    # must check for exactly list here because of strict type\n    # check in clean_index_list\n    if isinstance(index_like, list):\n        if type(index_like) != list:\n            index_like = list(index_like)\n\n        converted, all_arrays = lib.clean_index_list(index_like)\n\n        if len(converted) > 0 and all_arrays:\n            from .multi import MultiIndex\n            return MultiIndex.from_arrays(converted)\n        else:\n            index_like = converted\n    else:\n        # clean_index_list does the equivalent of copying\n        # so only need to do this if not list instance\n        if copy:\n            from copy import copy\n            index_like = copy(index_like)\n\n    return Index(index_like)\n\n\ndef _ensure_has_len(seq):\n    \"\"\"If seq is an iterator, put its values into a list.\"\"\"\n    try:\n        len(seq)\n    except TypeError:\n        return list(seq)\n    else:\n        return seq\n\n\ndef _trim_front(strings):\n    \"\"\"\n    Trims zeros and decimal points\n    \"\"\"\n    trimmed = strings\n    while len(strings) > 0 and all(x[0] == ' ' for x in trimmed):\n        trimmed = [x[1:] for x in trimmed]\n    return trimmed\n\n\ndef _validate_join_method(method):\n    if method not in ['left', 'right', 'inner', 'outer']:\n        raise ValueError('do not recognize join method %s' % method)\n\n\ndef default_index(n):\n    from pandas.core.index import RangeIndex\n    return RangeIndex(0, n, name=None)\n"
    },
    {
      "filename": "pandas/core/indexes/category.py",
      "content": "import operator\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import index as libindex\nimport pandas.compat as compat\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import Appender, cache_readonly\n\nfrom pandas.core.dtypes.common import (\n    ensure_platform_int, is_categorical_dtype, is_interval_dtype, is_list_like,\n    is_scalar)\nfrom pandas.core.dtypes.dtypes import CategoricalDtype\nfrom pandas.core.dtypes.generic import ABCCategorical, ABCSeries\nfrom pandas.core.dtypes.missing import array_equivalent, isna\n\nfrom pandas.core import accessor\nfrom pandas.core.algorithms import take_1d\nfrom pandas.core.arrays.categorical import Categorical, contains\nimport pandas.core.common as com\nfrom pandas.core.config import get_option\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.base import Index, _index_shared_docs\nimport pandas.core.missing as missing\nfrom pandas.core.ops import get_op_result_name\n\n_index_doc_kwargs = dict(ibase._index_doc_kwargs)\n_index_doc_kwargs.update(dict(target_klass='CategoricalIndex'))\n\n\n@accessor.delegate_names(\n    delegate=Categorical,\n    accessors=[\"rename_categories\",\n               \"reorder_categories\",\n               \"add_categories\",\n               \"remove_categories\",\n               \"remove_unused_categories\",\n               \"set_categories\",\n               \"as_ordered\", \"as_unordered\",\n               \"min\", \"max\"],\n    typ='method', overwrite=True)\nclass CategoricalIndex(Index, accessor.PandasDelegate):\n    \"\"\"\n    Immutable Index implementing an ordered, sliceable set. CategoricalIndex\n    represents a sparsely populated Index with an underlying Categorical.\n\n    Parameters\n    ----------\n    data : array-like or Categorical, (1-dimensional)\n    categories : optional, array-like\n        categories for the CategoricalIndex\n    ordered : boolean,\n        designating if the categories are ordered\n    copy : bool\n        Make a copy of input ndarray\n    name : object\n        Name to be stored in the index\n\n    Attributes\n    ----------\n    codes\n    categories\n    ordered\n\n    Methods\n    -------\n    rename_categories\n    reorder_categories\n    add_categories\n    remove_categories\n    remove_unused_categories\n    set_categories\n    as_ordered\n    as_unordered\n    map\n\n    See Also\n    --------\n    Categorical, Index\n    \"\"\"\n\n    _typ = 'categoricalindex'\n\n    @property\n    def _engine_type(self):\n        # self.codes can have dtype int8, int16, int32 or int64, so we need\n        # to return the corresponding engine type (libindex.Int8Engine, etc.).\n        return {np.int8: libindex.Int8Engine,\n                np.int16: libindex.Int16Engine,\n                np.int32: libindex.Int32Engine,\n                np.int64: libindex.Int64Engine,\n                }[self.codes.dtype.type]\n\n    _attributes = ['name']\n\n    def __new__(cls, data=None, categories=None, ordered=None, dtype=None,\n                copy=False, name=None, fastpath=None):\n\n        if fastpath is not None:\n            warnings.warn(\"The 'fastpath' keyword is deprecated, and will be \"\n                          \"removed in a future version.\",\n                          FutureWarning, stacklevel=2)\n            if fastpath:\n                return cls._simple_new(data, name=name, dtype=dtype)\n\n        if name is None and hasattr(data, 'name'):\n            name = data.name\n\n        if isinstance(data, ABCCategorical):\n            data = cls._create_categorical(data, categories, ordered,\n                                           dtype)\n        elif isinstance(data, CategoricalIndex):\n            data = data._data\n            data = cls._create_categorical(data, categories, ordered,\n                                           dtype)\n        else:\n\n            # don't allow scalars\n            # if data is None, then categories must be provided\n            if is_scalar(data):\n                if data is not None or categories is None:\n                    cls._scalar_data_error(data)\n                data = []\n            data = cls._create_categorical(data, categories, ordered,\n                                           dtype)\n\n        if copy:\n            data = data.copy()\n\n        return cls._simple_new(data, name=name)\n\n    def _create_from_codes(self, codes, categories=None, ordered=None,\n                           name=None):\n        \"\"\"\n        *this is an internal non-public method*\n\n        create the correct categorical from codes\n\n        Parameters\n        ----------\n        codes : new codes\n        categories : optional categories, defaults to existing\n        ordered : optional ordered attribute, defaults to existing\n        name : optional name attribute, defaults to existing\n\n        Returns\n        -------\n        CategoricalIndex\n        \"\"\"\n\n        if categories is None:\n            categories = self.categories\n        if ordered is None:\n            ordered = self.ordered\n        if name is None:\n            name = self.name\n        cat = Categorical.from_codes(codes, categories=categories,\n                                     ordered=ordered)\n        return CategoricalIndex(cat, name=name)\n\n    @classmethod\n    def _create_categorical(cls, data, categories=None, ordered=None,\n                            dtype=None):\n        \"\"\"\n        *this is an internal non-public method*\n\n        create the correct categorical from data and the properties\n\n        Parameters\n        ----------\n        data : data for new Categorical\n        categories : optional categories, defaults to existing\n        ordered : optional ordered attribute, defaults to existing\n        dtype : CategoricalDtype, defaults to existing\n\n        Returns\n        -------\n        Categorical\n        \"\"\"\n        if (isinstance(data, (cls, ABCSeries)) and\n                is_categorical_dtype(data)):\n            data = data.values\n\n        if not isinstance(data, ABCCategorical):\n            if ordered is None and dtype is None:\n                ordered = False\n            data = Categorical(data, categories=categories, ordered=ordered,\n                               dtype=dtype)\n        else:\n            if categories is not None:\n                data = data.set_categories(categories, ordered=ordered)\n            elif ordered is not None and ordered != data.ordered:\n                data = data.set_ordered(ordered)\n            if isinstance(dtype, CategoricalDtype) and dtype != data.dtype:\n                # we want to silently ignore dtype='category'\n                data = data._set_dtype(dtype)\n        return data\n\n    @classmethod\n    def _simple_new(cls, values, name=None, categories=None, ordered=None,\n                    dtype=None, **kwargs):\n        result = object.__new__(cls)\n\n        values = cls._create_categorical(values, categories, ordered,\n                                         dtype=dtype)\n        result._data = values\n        result.name = name\n        for k, v in compat.iteritems(kwargs):\n            setattr(result, k, v)\n\n        result._reset_identity()\n        return result\n\n    @Appender(_index_shared_docs['_shallow_copy'])\n    def _shallow_copy(self, values=None, categories=None, ordered=None,\n                      dtype=None, **kwargs):\n        # categories and ordered can't be part of attributes,\n        # as these are properties\n        # we want to reuse self.dtype if possible, i.e. neither are\n        # overridden.\n        if dtype is not None and (categories is not None or\n                                  ordered is not None):\n            raise TypeError(\"Cannot specify both `dtype` and `categories` \"\n                            \"or `ordered`\")\n\n        if categories is None and ordered is None:\n            dtype = self.dtype if dtype is None else dtype\n            return super(CategoricalIndex, self)._shallow_copy(\n                values=values, dtype=dtype, **kwargs)\n        if categories is None:\n            categories = self.categories\n        if ordered is None:\n            ordered = self.ordered\n\n        return super(CategoricalIndex, self)._shallow_copy(\n            values=values, categories=categories,\n            ordered=ordered, **kwargs)\n\n    def _is_dtype_compat(self, other):\n        \"\"\"\n        *this is an internal non-public method*\n\n        provide a comparison between the dtype of self and other (coercing if\n        needed)\n\n        Raises\n        ------\n        TypeError if the dtypes are not compatible\n        \"\"\"\n        if is_categorical_dtype(other):\n            if isinstance(other, CategoricalIndex):\n                other = other._values\n            if not other.is_dtype_equal(self):\n                raise TypeError(\"categories must match existing categories \"\n                                \"when appending\")\n        else:\n            values = other\n            if not is_list_like(values):\n                values = [values]\n            other = CategoricalIndex(self._create_categorical(\n                other, dtype=self.dtype))\n            if not other.isin(values).all():\n                raise TypeError(\"cannot append a non-category item to a \"\n                                \"CategoricalIndex\")\n\n        return other\n\n    def equals(self, other):\n        \"\"\"\n        Determines if two CategorialIndex objects contain the same elements.\n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n\n        try:\n            other = self._is_dtype_compat(other)\n            return array_equivalent(self._data, other)\n        except (TypeError, ValueError):\n            pass\n\n        return False\n\n    @property\n    def _formatter_func(self):\n        return self.categories._formatter_func\n\n    def _format_attrs(self):\n        \"\"\"\n        Return a list of tuples of the (attr,formatted_value)\n        \"\"\"\n        max_categories = (10 if get_option(\"display.max_categories\") == 0 else\n                          get_option(\"display.max_categories\"))\n        attrs = [\n            ('categories',\n             ibase.default_pprint(self.categories,\n                                  max_seq_items=max_categories)),\n            ('ordered', self.ordered)]\n        if self.name is not None:\n            attrs.append(('name', ibase.default_pprint(self.name)))\n        attrs.append(('dtype', \"'%s'\" % self.dtype.name))\n        max_seq_items = get_option('display.max_seq_items') or len(self)\n        if len(self) > max_seq_items:\n            attrs.append(('length', len(self)))\n        return attrs\n\n    @property\n    def inferred_type(self):\n        return 'categorical'\n\n    @property\n    def values(self):\n        \"\"\" return the underlying data, which is a Categorical \"\"\"\n        return self._data\n\n    @property\n    def itemsize(self):\n        # Size of the items in categories, not codes.\n        return self.values.itemsize\n\n    def _wrap_setop_result(self, other, result):\n        name = get_op_result_name(self, other)\n        return self._shallow_copy(result, name=name)\n\n    def get_values(self):\n        \"\"\" return the underlying data as an ndarray \"\"\"\n        return self._data.get_values()\n\n    def tolist(self):\n        return self._data.tolist()\n\n    @property\n    def codes(self):\n        return self._data.codes\n\n    @property\n    def categories(self):\n        return self._data.categories\n\n    @property\n    def ordered(self):\n        return self._data.ordered\n\n    def _reverse_indexer(self):\n        return self._data._reverse_indexer()\n\n    @Appender(_index_shared_docs['__contains__'] % _index_doc_kwargs)\n    def __contains__(self, key):\n        # if key is a NaN, check if any NaN is in self.\n        if isna(key):\n            return self.hasnans\n\n        return contains(self, key, container=self._engine)\n\n    @Appender(_index_shared_docs['contains'] % _index_doc_kwargs)\n    def contains(self, key):\n        return key in self\n\n    def __array__(self, dtype=None):\n        \"\"\" the array interface, return my values \"\"\"\n        return np.array(self._data, dtype=dtype)\n\n    @Appender(_index_shared_docs['astype'])\n    def astype(self, dtype, copy=True):\n        if is_interval_dtype(dtype):\n            from pandas import IntervalIndex\n            return IntervalIndex(np.array(self))\n        elif is_categorical_dtype(dtype):\n            # GH 18630\n            dtype = self.dtype.update_dtype(dtype)\n            if dtype == self.dtype:\n                return self.copy() if copy else self\n\n        return super(CategoricalIndex, self).astype(dtype=dtype, copy=copy)\n\n    @cache_readonly\n    def _isnan(self):\n        \"\"\" return if each value is nan\"\"\"\n        return self._data.codes == -1\n\n    @Appender(ibase._index_shared_docs['fillna'])\n    def fillna(self, value, downcast=None):\n        self._assert_can_do_op(value)\n        return CategoricalIndex(self._data.fillna(value), name=self.name)\n\n    def argsort(self, *args, **kwargs):\n        return self.values.argsort(*args, **kwargs)\n\n    @cache_readonly\n    def _engine(self):\n\n        # we are going to look things up with the codes themselves\n        return self._engine_type(lambda: self.codes, len(self))\n\n    # introspection\n    @cache_readonly\n    def is_unique(self):\n        return self._engine.is_unique\n\n    @property\n    def is_monotonic_increasing(self):\n        return self._engine.is_monotonic_increasing\n\n    @property\n    def is_monotonic_decreasing(self):\n        return self._engine.is_monotonic_decreasing\n\n    @Appender(_index_shared_docs['index_unique'] % _index_doc_kwargs)\n    def unique(self, level=None):\n        if level is not None:\n            self._validate_index_level(level)\n        result = self.values.unique()\n        # CategoricalIndex._shallow_copy keeps original categories\n        # and ordered if not otherwise specified\n        return self._shallow_copy(result, categories=result.categories,\n                                  ordered=result.ordered)\n\n    @Appender(Index.duplicated.__doc__)\n    def duplicated(self, keep='first'):\n        from pandas._libs.hashtable import duplicated_int64\n        codes = self.codes.astype('i8')\n        return duplicated_int64(codes, keep)\n\n    def _to_safe_for_reshape(self):\n        \"\"\" convert to object if we are a categorical \"\"\"\n        return self.astype('object')\n\n    def get_loc(self, key, method=None):\n        \"\"\"\n        Get integer location, slice or boolean mask for requested label.\n\n        Parameters\n        ----------\n        key : label\n        method : {None}\n            * default: exact matches only.\n\n        Returns\n        -------\n        loc : int if unique index, slice if monotonic index, else mask\n\n        Raises\n        ------\n        KeyError : if the key is not in the index\n\n        Examples\n        ---------\n        >>> unique_index = pd.CategoricalIndex(list('abc'))\n        >>> unique_index.get_loc('b')\n        1\n\n        >>> monotonic_index = pd.CategoricalIndex(list('abbc'))\n        >>> monotonic_index.get_loc('b')\n        slice(1, 3, None)\n\n        >>> non_monotonic_index = pd.CategoricalIndex(list('abcb'))\n        >>> non_monotonic_index.get_loc('b')\n        array([False,  True, False,  True], dtype=bool)\n        \"\"\"\n        code = self.categories.get_loc(key)\n        code = self.codes.dtype.type(code)\n        try:\n            return self._engine.get_loc(code)\n        except KeyError:\n            raise KeyError(key)\n\n    def get_value(self, series, key):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray. Only use this if you\n        know what you're doing\n        \"\"\"\n        try:\n            k = com.values_from_object(key)\n            k = self._convert_scalar_indexer(k, kind='getitem')\n            indexer = self.get_loc(k)\n            return series.iloc[indexer]\n        except (KeyError, TypeError):\n            pass\n\n        # we might be a positional inexer\n        return super(CategoricalIndex, self).get_value(series, key)\n\n    def _can_reindex(self, indexer):\n        \"\"\" always allow reindexing \"\"\"\n        pass\n\n    @Appender(_index_shared_docs['where'])\n    def where(self, cond, other=None):\n        if other is None:\n            other = self._na_value\n        values = np.where(cond, self.values, other)\n\n        cat = Categorical(values, dtype=self.dtype)\n        return self._shallow_copy(cat, **self._get_attributes_dict())\n\n    def reindex(self, target, method=None, level=None, limit=None,\n                tolerance=None):\n        \"\"\"\n        Create index with target's values (move/add/delete values as necessary)\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index\n        indexer : np.ndarray or None\n            Indices of output values in original index\n\n        \"\"\"\n\n        if method is not None:\n            raise NotImplementedError(\"argument method is not implemented for \"\n                                      \"CategoricalIndex.reindex\")\n        if level is not None:\n            raise NotImplementedError(\"argument level is not implemented for \"\n                                      \"CategoricalIndex.reindex\")\n        if limit is not None:\n            raise NotImplementedError(\"argument limit is not implemented for \"\n                                      \"CategoricalIndex.reindex\")\n\n        target = ibase.ensure_index(target)\n\n        if not is_categorical_dtype(target) and not target.is_unique:\n            raise ValueError(\"cannot reindex with a non-unique indexer\")\n\n        indexer, missing = self.get_indexer_non_unique(np.array(target))\n\n        if len(self.codes):\n            new_target = self.take(indexer)\n        else:\n            new_target = target\n\n        # filling in missing if needed\n        if len(missing):\n            cats = self.categories.get_indexer(target)\n\n            if (cats == -1).any():\n                # coerce to a regular index here!\n                result = Index(np.array(self), name=self.name)\n                new_target, indexer, _ = result._reindex_non_unique(\n                    np.array(target))\n            else:\n\n                codes = new_target.codes.copy()\n                codes[indexer == -1] = cats[missing]\n                new_target = self._create_from_codes(codes)\n\n        # we always want to return an Index type here\n        # to be consistent with .reindex for other index types (e.g. they don't\n        # coerce based on the actual values, only on the dtype)\n        # unless we had an initial Categorical to begin with\n        # in which case we are going to conform to the passed Categorical\n        new_target = np.asarray(new_target)\n        if is_categorical_dtype(target):\n            new_target = target._shallow_copy(new_target, name=self.name)\n        else:\n            new_target = Index(new_target, name=self.name)\n\n        return new_target, indexer\n\n    def _reindex_non_unique(self, target):\n        \"\"\" reindex from a non-unique; which CategoricalIndex's are almost\n        always\n        \"\"\"\n        new_target, indexer = self.reindex(target)\n        new_indexer = None\n\n        check = indexer == -1\n        if check.any():\n            new_indexer = np.arange(len(self.take(indexer)))\n            new_indexer[check] = -1\n\n        cats = self.categories.get_indexer(target)\n        if not (cats == -1).any():\n            # .reindex returns normal Index. Revert to CategoricalIndex if\n            # all targets are included in my categories\n            new_target = self._shallow_copy(new_target)\n\n        return new_target, indexer, new_indexer\n\n    @Appender(_index_shared_docs['get_indexer'] % _index_doc_kwargs)\n    def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        from pandas.core.arrays.categorical import _recode_for_categories\n\n        method = missing.clean_reindex_fill_method(method)\n        target = ibase.ensure_index(target)\n\n        if self.is_unique and self.equals(target):\n            return np.arange(len(self), dtype='intp')\n\n        if method == 'pad' or method == 'backfill':\n            raise NotImplementedError(\"method='pad' and method='backfill' not \"\n                                      \"implemented yet for CategoricalIndex\")\n        elif method == 'nearest':\n            raise NotImplementedError(\"method='nearest' not implemented yet \"\n                                      'for CategoricalIndex')\n\n        if (isinstance(target, CategoricalIndex) and\n                self.values.is_dtype_equal(target)):\n            if self.values.equals(target.values):\n                # we have the same codes\n                codes = target.codes\n            else:\n                codes = _recode_for_categories(target.codes,\n                                               target.categories,\n                                               self.values.categories)\n        else:\n            if isinstance(target, CategoricalIndex):\n                code_indexer = self.categories.get_indexer(target.categories)\n                codes = take_1d(code_indexer, target.codes, fill_value=-1)\n            else:\n                codes = self.categories.get_indexer(target)\n\n        indexer, _ = self._engine.get_indexer_non_unique(codes)\n        return ensure_platform_int(indexer)\n\n    @Appender(_index_shared_docs['get_indexer_non_unique'] % _index_doc_kwargs)\n    def get_indexer_non_unique(self, target):\n        target = ibase.ensure_index(target)\n\n        if isinstance(target, CategoricalIndex):\n            # Indexing on codes is more efficient if categories are the same:\n            if target.categories is self.categories:\n                target = target.codes\n                indexer, missing = self._engine.get_indexer_non_unique(target)\n                return ensure_platform_int(indexer), missing\n            target = target.values\n\n        codes = self.categories.get_indexer(target)\n        indexer, missing = self._engine.get_indexer_non_unique(codes)\n        return ensure_platform_int(indexer), missing\n\n    @Appender(_index_shared_docs['_convert_scalar_indexer'])\n    def _convert_scalar_indexer(self, key, kind=None):\n        if self.categories._defer_to_indexing:\n            return self.categories._convert_scalar_indexer(key, kind=kind)\n\n        return super(CategoricalIndex, self)._convert_scalar_indexer(\n            key, kind=kind)\n\n    @Appender(_index_shared_docs['_convert_list_indexer'])\n    def _convert_list_indexer(self, keyarr, kind=None):\n        # Return our indexer or raise if all of the values are not included in\n        # the categories\n\n        if self.categories._defer_to_indexing:\n            indexer = self.categories._convert_list_indexer(keyarr, kind=kind)\n            return Index(self.codes).get_indexer_for(indexer)\n\n        indexer = self.categories.get_indexer(np.asarray(keyarr))\n        if (indexer == -1).any():\n            raise KeyError(\n                \"a list-indexer must only \"\n                \"include values that are \"\n                \"in the categories\")\n\n        return self.get_indexer(keyarr)\n\n    @Appender(_index_shared_docs['_convert_arr_indexer'])\n    def _convert_arr_indexer(self, keyarr):\n        keyarr = com.asarray_tuplesafe(keyarr)\n\n        if self.categories._defer_to_indexing:\n            return keyarr\n\n        return self._shallow_copy(keyarr)\n\n    @Appender(_index_shared_docs['_convert_index_indexer'])\n    def _convert_index_indexer(self, keyarr):\n        return self._shallow_copy(keyarr)\n\n    @Appender(_index_shared_docs['take'] % _index_doc_kwargs)\n    def take(self, indices, axis=0, allow_fill=True,\n             fill_value=None, **kwargs):\n        nv.validate_take(tuple(), kwargs)\n        indices = ensure_platform_int(indices)\n        taken = self._assert_take_fillable(self.codes, indices,\n                                           allow_fill=allow_fill,\n                                           fill_value=fill_value,\n                                           na_value=-1)\n        return self._create_from_codes(taken)\n\n    def is_dtype_equal(self, other):\n        return self._data.is_dtype_equal(other)\n\n    take_nd = take\n\n    def map(self, mapper):\n        \"\"\"\n        Map values using input correspondence (a dict, Series, or function).\n\n        Maps the values (their categories, not the codes) of the index to new\n        categories. If the mapping correspondence is one-to-one the result is a\n        :class:`~pandas.CategoricalIndex` which has the same order property as\n        the original, otherwise an :class:`~pandas.Index` is returned.\n\n        If a `dict` or :class:`~pandas.Series` is used any unmapped category is\n        mapped to `NaN`. Note that if this happens an :class:`~pandas.Index`\n        will be returned.\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            Mapping correspondence.\n\n        Returns\n        -------\n        pandas.CategoricalIndex or pandas.Index\n            Mapped index.\n\n        See Also\n        --------\n        Index.map : Apply a mapping correspondence on an\n            :class:`~pandas.Index`.\n        Series.map : Apply a mapping correspondence on a\n            :class:`~pandas.Series`.\n        Series.apply : Apply more complex functions on a\n            :class:`~pandas.Series`.\n\n        Examples\n        --------\n        >>> idx = pd.CategoricalIndex(['a', 'b', 'c'])\n        >>> idx\n        CategoricalIndex(['a', 'b', 'c'], categories=['a', 'b', 'c'],\n                         ordered=False, dtype='category')\n        >>> idx.map(lambda x: x.upper())\n        CategoricalIndex(['A', 'B', 'C'], categories=['A', 'B', 'C'],\n                         ordered=False, dtype='category')\n        >>> idx.map({'a': 'first', 'b': 'second', 'c': 'third'})\n        CategoricalIndex(['first', 'second', 'third'], categories=['first',\n                         'second', 'third'], ordered=False, dtype='category')\n\n        If the mapping is one-to-one the ordering of the categories is\n        preserved:\n\n        >>> idx = pd.CategoricalIndex(['a', 'b', 'c'], ordered=True)\n        >>> idx\n        CategoricalIndex(['a', 'b', 'c'], categories=['a', 'b', 'c'],\n                         ordered=True, dtype='category')\n        >>> idx.map({'a': 3, 'b': 2, 'c': 1})\n        CategoricalIndex([3, 2, 1], categories=[3, 2, 1], ordered=True,\n                         dtype='category')\n\n        If the mapping is not one-to-one an :class:`~pandas.Index` is returned:\n\n        >>> idx.map({'a': 'first', 'b': 'second', 'c': 'first'})\n        Index(['first', 'second', 'first'], dtype='object')\n\n        If a `dict` is used, all unmapped categories are mapped to `NaN` and\n        the result is an :class:`~pandas.Index`:\n\n        >>> idx.map({'a': 'first', 'b': 'second'})\n        Index(['first', 'second', nan], dtype='object')\n        \"\"\"\n        return self._shallow_copy_with_infer(self.values.map(mapper))\n\n    def delete(self, loc):\n        \"\"\"\n        Make new Index with passed location(-s) deleted\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        return self._create_from_codes(np.delete(self.codes, loc))\n\n    def insert(self, loc, item):\n        \"\"\"\n        Make new Index inserting new item at location. Follows\n        Python list.append semantics for negative values\n\n        Parameters\n        ----------\n        loc : int\n        item : object\n\n        Returns\n        -------\n        new_index : Index\n\n        Raises\n        ------\n        ValueError if the item is not in the categories\n\n        \"\"\"\n        code = self.categories.get_indexer([item])\n        if (code == -1) and not (is_scalar(item) and isna(item)):\n            raise TypeError(\"cannot insert an item into a CategoricalIndex \"\n                            \"that is not already an existing category\")\n\n        codes = self.codes\n        codes = np.concatenate((codes[:loc], code, codes[loc:]))\n        return self._create_from_codes(codes)\n\n    def _concat(self, to_concat, name):\n        # if calling index is category, don't check dtype of others\n        return CategoricalIndex._concat_same_dtype(self, to_concat, name)\n\n    def _concat_same_dtype(self, to_concat, name):\n        \"\"\"\n        Concatenate to_concat which has the same class\n        ValueError if other is not in the categories\n        \"\"\"\n        to_concat = [self._is_dtype_compat(c) for c in to_concat]\n        codes = np.concatenate([c.codes for c in to_concat])\n        result = self._create_from_codes(codes, name=name)\n        # if name is None, _create_from_codes sets self.name\n        result.name = name\n        return result\n\n    def _codes_for_groupby(self, sort, observed):\n        \"\"\" Return a Categorical adjusted for groupby \"\"\"\n        return self.values._codes_for_groupby(sort, observed)\n\n    @classmethod\n    def _add_comparison_methods(cls):\n        \"\"\" add in comparison methods \"\"\"\n\n        def _make_compare(op):\n            opname = '__{op}__'.format(op=op.__name__)\n\n            def _evaluate_compare(self, other):\n\n                # if we have a Categorical type, then must have the same\n                # categories\n                if isinstance(other, CategoricalIndex):\n                    other = other._values\n                elif isinstance(other, Index):\n                    other = self._create_categorical(\n                        other._values, dtype=self.dtype)\n\n                if isinstance(other, (ABCCategorical, np.ndarray,\n                                      ABCSeries)):\n                    if len(self.values) != len(other):\n                        raise ValueError(\"Lengths must match to compare\")\n\n                if isinstance(other, ABCCategorical):\n                    if not self.values.is_dtype_equal(other):\n                        raise TypeError(\"categorical index comparisons must \"\n                                        \"have the same categories and ordered \"\n                                        \"attributes\")\n\n                result = op(self.values, other)\n                if isinstance(result, ABCSeries):\n                    # Dispatch to pd.Categorical returned NotImplemented\n                    # and we got a Series back; down-cast to ndarray\n                    result = result.values\n                return result\n\n            return compat.set_function_name(_evaluate_compare, opname, cls)\n\n        cls.__eq__ = _make_compare(operator.eq)\n        cls.__ne__ = _make_compare(operator.ne)\n        cls.__lt__ = _make_compare(operator.lt)\n        cls.__gt__ = _make_compare(operator.gt)\n        cls.__le__ = _make_compare(operator.le)\n        cls.__ge__ = _make_compare(operator.ge)\n\n    def _delegate_method(self, name, *args, **kwargs):\n        \"\"\" method delegation to the ._values \"\"\"\n        method = getattr(self._values, name)\n        if 'inplace' in kwargs:\n            raise ValueError(\"cannot use inplace with CategoricalIndex\")\n        res = method(*args, **kwargs)\n        if is_scalar(res):\n            return res\n        return CategoricalIndex(res, name=self.name)\n\n\nCategoricalIndex._add_numeric_methods_add_sub_disabled()\nCategoricalIndex._add_numeric_methods_disabled()\nCategoricalIndex._add_logical_methods_disabled()\nCategoricalIndex._add_comparison_methods()\n"
    },
    {
      "filename": "pandas/core/indexes/datetimes.py",
      "content": "# pylint: disable=E1101\nfrom __future__ import division\n\nfrom datetime import datetime, time, timedelta\nimport operator\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import (\n    Timestamp, index as libindex, join as libjoin, lib, tslib as libts)\nfrom pandas._libs.tslibs import (\n    ccalendar, conversion, fields, parsing, timezones)\nimport pandas.compat as compat\nfrom pandas.util._decorators import Appender, Substitution, cache_readonly\n\nfrom pandas.core.dtypes.common import (\n    _INT64_DTYPE, _NS_DTYPE, ensure_int64, is_datetime64_dtype,\n    is_datetime64_ns_dtype, is_datetimetz, is_dtype_equal, is_float,\n    is_integer, is_integer_dtype, is_list_like, is_period_dtype, is_scalar,\n    is_string_like, pandas_dtype)\nimport pandas.core.dtypes.concat as _concat\nfrom pandas.core.dtypes.generic import ABCSeries\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core.arrays import datetimelike as dtl\nfrom pandas.core.arrays.datetimes import (\n    DatetimeArrayMixin as DatetimeArray, _to_m8)\nfrom pandas.core.base import _shared_docs\nimport pandas.core.common as com\nfrom pandas.core.indexes.base import Index, _index_shared_docs\nfrom pandas.core.indexes.datetimelike import (\n    DatelikeOps, DatetimeIndexOpsMixin, TimelikeOps, wrap_array_method,\n    wrap_field_accessor)\nfrom pandas.core.indexes.numeric import Int64Index\nfrom pandas.core.ops import get_op_result_name\nimport pandas.core.tools.datetimes as tools\n\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import Resolution, to_offset\nfrom pandas.tseries.offsets import CDay, prefix_mapping\n\n\ndef _new_DatetimeIndex(cls, d):\n    \"\"\" This is called upon unpickling, rather than the default which doesn't\n    have arguments and breaks __new__ \"\"\"\n\n    # data are already in UTC\n    # so need to localize\n    tz = d.pop('tz', None)\n\n    result = cls.__new__(cls, verify_integrity=False, **d)\n    if tz is not None:\n        result = result.tz_localize('UTC').tz_convert(tz)\n    return result\n\n\nclass DatetimeIndex(DatetimeArray, DatelikeOps, TimelikeOps,\n                    DatetimeIndexOpsMixin, Int64Index):\n    \"\"\"\n    Immutable ndarray of datetime64 data, represented internally as int64, and\n    which can be boxed to Timestamp objects that are subclasses of datetime and\n    carry metadata such as frequency information.\n\n    Parameters\n    ----------\n    data  : array-like (1-dimensional), optional\n        Optional datetime-like data to construct index with\n    copy  : bool\n        Make a copy of input ndarray\n    freq : string or pandas offset object, optional\n        One of pandas date offset strings or corresponding objects. The string\n        'infer' can be passed in order to set the frequency of the index as the\n        inferred frequency upon creation\n\n    start : starting value, datetime-like, optional\n        If data is None, start is used as the start point in generating regular\n        timestamp data.\n    periods  : int, optional, > 0\n        Number of periods to generate, if generating index. Takes precedence\n        over end argument\n    end   : end time, datetime-like, optional\n        If periods is none, generated index will extend to first conforming\n        time on or just past end argument\n    closed : string or None, default None\n        Make the interval closed with respect to the given frequency to\n        the 'left', 'right', or both sides (None)\n    tz : pytz.timezone or dateutil.tz.tzfile\n    ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'\n        When clocks moved backward due to DST, ambiguous times may arise.\n        For example in Central European Time (UTC+01), when going from 03:00\n        DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC\n        and at 01:30:00 UTC. In such a situation, the `ambiguous` parameter\n        dictates how ambiguous times should be handled.\n\n        - 'infer' will attempt to infer fall dst-transition hours based on\n          order\n        - bool-ndarray where True signifies a DST time, False signifies a\n          non-DST time (note that this flag is only applicable for ambiguous\n          times)\n        - 'NaT' will return NaT where there are ambiguous times\n        - 'raise' will raise an AmbiguousTimeError if there are ambiguous times\n    name : object\n        Name to be stored in the index\n    dayfirst : bool, default False\n        If True, parse dates in `data` with the day first order\n    yearfirst : bool, default False\n        If True parse dates in `data` with the year first order\n\n    Attributes\n    ----------\n    year\n    month\n    day\n    hour\n    minute\n    second\n    microsecond\n    nanosecond\n    date\n    time\n    timetz\n    dayofyear\n    weekofyear\n    week\n    dayofweek\n    weekday\n    quarter\n    tz\n    freq\n    freqstr\n    is_month_start\n    is_month_end\n    is_quarter_start\n    is_quarter_end\n    is_year_start\n    is_year_end\n    is_leap_year\n    inferred_freq\n\n    Methods\n    -------\n    normalize\n    strftime\n    snap\n    tz_convert\n    tz_localize\n    round\n    floor\n    ceil\n    to_period\n    to_perioddelta\n    to_pydatetime\n    to_series\n    to_frame\n    month_name\n    day_name\n\n    Notes\n    -----\n    To learn more about the frequency strings, please see `this link\n    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.\n\n    See Also\n    ---------\n    Index : The base pandas Index type.\n    TimedeltaIndex : Index of timedelta64 data.\n    PeriodIndex : Index of Period data.\n    pandas.to_datetime : Convert argument to datetime.\n    \"\"\"\n    _typ = 'datetimeindex'\n    _join_precedence = 10\n\n    def _join_i8_wrapper(joinf, **kwargs):\n        return DatetimeIndexOpsMixin._join_i8_wrapper(joinf, dtype='M8[ns]',\n                                                      **kwargs)\n\n    _inner_indexer = _join_i8_wrapper(libjoin.inner_join_indexer_int64)\n    _outer_indexer = _join_i8_wrapper(libjoin.outer_join_indexer_int64)\n    _left_indexer = _join_i8_wrapper(libjoin.left_join_indexer_int64)\n    _left_indexer_unique = _join_i8_wrapper(\n        libjoin.left_join_indexer_unique_int64, with_indexers=False)\n\n    _engine_type = libindex.DatetimeEngine\n\n    _tz = None\n    _freq = None\n    _comparables = ['name', 'freqstr', 'tz']\n    _attributes = ['name', 'freq', 'tz']\n\n    # dummy attribute so that datetime.__eq__(DatetimeArray) defers\n    # by returning NotImplemented\n    timetuple = None\n\n    # define my properties & methods for delegation\n    _bool_ops = ['is_month_start', 'is_month_end',\n                 'is_quarter_start', 'is_quarter_end', 'is_year_start',\n                 'is_year_end', 'is_leap_year']\n    _object_ops = ['weekday_name', 'freq', 'tz']\n    _field_ops = ['year', 'month', 'day', 'hour', 'minute', 'second',\n                  'weekofyear', 'week', 'weekday', 'dayofweek',\n                  'dayofyear', 'quarter', 'days_in_month',\n                  'daysinmonth', 'microsecond',\n                  'nanosecond']\n    _other_ops = ['date', 'time', 'timetz']\n    _datetimelike_ops = _field_ops + _object_ops + _bool_ops + _other_ops\n    _datetimelike_methods = ['to_period', 'tz_localize',\n                             'tz_convert',\n                             'normalize', 'strftime', 'round', 'floor',\n                             'ceil', 'month_name', 'day_name']\n\n    _is_numeric_dtype = False\n    _infer_as_myclass = True\n\n    # --------------------------------------------------------------------\n    # Constructors\n\n    def __new__(cls, data=None,\n                freq=None, start=None, end=None, periods=None, tz=None,\n                normalize=False, closed=None, ambiguous='raise',\n                dayfirst=False, yearfirst=False, dtype=None,\n                copy=False, name=None, verify_integrity=True):\n\n        if data is None:\n            # TODO: Remove this block and associated kwargs; GH#20535\n            result = cls._generate_range(start, end, periods,\n                                         freq=freq, tz=tz, normalize=normalize,\n                                         closed=closed, ambiguous=ambiguous)\n            result.name = name\n            return result\n\n        if is_scalar(data):\n            raise TypeError(\"{cls}() must be called with a \"\n                            \"collection of some kind, {data} was passed\"\n                            .format(cls=cls.__name__, data=repr(data)))\n\n        # - Cases checked above all return/raise before reaching here - #\n\n        # This allows to later ensure that the 'copy' parameter is honored:\n        if isinstance(data, Index):\n            ref_to_data = data._data\n        else:\n            ref_to_data = data\n\n        if name is None and hasattr(data, 'name'):\n            name = data.name\n\n        freq, freq_infer = dtl.maybe_infer_freq(freq)\n\n        # if dtype has an embedded tz, capture it\n        tz = dtl.validate_tz_from_dtype(dtype, tz)\n\n        if not isinstance(data, (np.ndarray, Index, ABCSeries, DatetimeArray)):\n            # other iterable of some kind\n            if not isinstance(data, (list, tuple)):\n                data = list(data)\n            data = np.asarray(data, dtype='O')\n        elif isinstance(data, ABCSeries):\n            data = data._values\n\n        # data must be Index or np.ndarray here\n        if not (is_datetime64_dtype(data) or is_datetimetz(data) or\n                is_integer_dtype(data) or lib.infer_dtype(data) == 'integer'):\n            data = tools.to_datetime(data, dayfirst=dayfirst,\n                                     yearfirst=yearfirst)\n\n        if isinstance(data, DatetimeArray):\n            if tz is None:\n                tz = data.tz\n            elif data.tz is None:\n                data = data.tz_localize(tz, ambiguous=ambiguous)\n            else:\n                # the tz's must match\n                if not timezones.tz_compare(tz, data.tz):\n                    msg = ('data is already tz-aware {0}, unable to '\n                           'set specified tz: {1}')\n                    raise TypeError(msg.format(data.tz, tz))\n\n            subarr = data._data\n\n            if freq is None:\n                freq = data.freq\n                verify_integrity = False\n        elif issubclass(data.dtype.type, np.datetime64):\n            if data.dtype != _NS_DTYPE:\n                data = conversion.ensure_datetime64ns(data)\n            if tz is not None:\n                # Convert tz-naive to UTC\n                tz = timezones.maybe_get_tz(tz)\n                data = conversion.tz_localize_to_utc(data.view('i8'), tz,\n                                                     ambiguous=ambiguous)\n            subarr = data.view(_NS_DTYPE)\n        else:\n            # must be integer dtype otherwise\n            # assume this data are epoch timestamps\n            if data.dtype != _INT64_DTYPE:\n                data = data.astype(np.int64, copy=False)\n            subarr = data.view(_NS_DTYPE)\n\n        assert isinstance(subarr, np.ndarray), type(subarr)\n        assert subarr.dtype == 'M8[ns]', subarr.dtype\n\n        subarr = cls._simple_new(subarr, name=name, freq=freq, tz=tz)\n        if dtype is not None:\n            if not is_dtype_equal(subarr.dtype, dtype):\n                # dtype must be coerced to DatetimeTZDtype above\n                if subarr.tz is not None:\n                    raise ValueError(\"cannot localize from non-UTC data\")\n\n        if verify_integrity and len(subarr) > 0:\n            if freq is not None and not freq_infer:\n                cls._validate_frequency(subarr, freq, ambiguous=ambiguous)\n\n        if freq_infer:\n            subarr.freq = to_offset(subarr.inferred_freq)\n\n        return subarr._deepcopy_if_needed(ref_to_data, copy)\n\n    @classmethod\n    def _simple_new(cls, values, name=None, freq=None, tz=None,\n                    dtype=None, **kwargs):\n        \"\"\"\n        we require the we have a dtype compat for the values\n        if we are passed a non-dtype compat, then coerce using the constructor\n        \"\"\"\n        # DatetimeArray._simple_new will accept either i8 or M8[ns] dtypes\n        assert isinstance(values, np.ndarray), type(values)\n\n        result = super(DatetimeIndex, cls)._simple_new(values, freq, tz,\n                                                       **kwargs)\n        result.name = name\n        result._reset_identity()\n        return result\n\n    # --------------------------------------------------------------------\n\n    @property\n    def _values(self):\n        # tz-naive -> ndarray\n        # tz-aware -> DatetimeIndex\n        if self.tz is not None:\n            return self\n        else:\n            return self.values\n\n    @property\n    def tz(self):\n        # GH 18595\n        return self._tz\n\n    @tz.setter\n    def tz(self, value):\n        # GH 3746: Prevent localizing or converting the index by setting tz\n        raise AttributeError(\"Cannot directly set timezone. Use tz_localize() \"\n                             \"or tz_convert() as appropriate\")\n\n    @property\n    def size(self):\n        # TODO: Remove this when we have a DatetimeTZArray\n        # Necessary to avoid recursion error since DTI._values is a DTI\n        # for TZ-aware\n        return self._ndarray_values.size\n\n    @property\n    def shape(self):\n        # TODO: Remove this when we have a DatetimeTZArray\n        # Necessary to avoid recursion error since DTI._values is a DTI\n        # for TZ-aware\n        return self._ndarray_values.shape\n\n    @property\n    def nbytes(self):\n        # TODO: Remove this when we have a DatetimeTZArray\n        # Necessary to avoid recursion error since DTI._values is a DTI\n        # for TZ-aware\n        return self._ndarray_values.nbytes\n\n    def _mpl_repr(self):\n        # how to represent ourselves to matplotlib\n        return libts.ints_to_pydatetime(self.asi8, self.tz)\n\n    @cache_readonly\n    def _is_dates_only(self):\n        \"\"\"Return a boolean if we are only dates (and don't have a timezone)\"\"\"\n        from pandas.io.formats.format import _is_dates_only\n        return _is_dates_only(self.values) and self.tz is None\n\n    @property\n    def _formatter_func(self):\n        from pandas.io.formats.format import _get_format_datetime64\n        formatter = _get_format_datetime64(is_dates_only=self._is_dates_only)\n        return lambda x: \"'%s'\" % formatter(x, tz=self.tz)\n\n    def __reduce__(self):\n\n        # we use a special reudce here because we need\n        # to simply set the .tz (and not reinterpret it)\n\n        d = dict(data=self._data)\n        d.update(self._get_attributes_dict())\n        return _new_DatetimeIndex, (self.__class__, d), None\n\n    def __setstate__(self, state):\n        \"\"\"Necessary for making this object picklable\"\"\"\n        if isinstance(state, dict):\n            super(DatetimeIndex, self).__setstate__(state)\n\n        elif isinstance(state, tuple):\n\n            # < 0.15 compat\n            if len(state) == 2:\n                nd_state, own_state = state\n                data = np.empty(nd_state[1], dtype=nd_state[2])\n                np.ndarray.__setstate__(data, nd_state)\n\n                self.name = own_state[0]\n                self._freq = own_state[1]\n                self._tz = timezones.tz_standardize(own_state[2])\n\n                # provide numpy < 1.7 compat\n                if nd_state[2] == 'M8[us]':\n                    new_state = np.ndarray.__reduce__(data.astype('M8[ns]'))\n                    np.ndarray.__setstate__(data, new_state[2])\n\n            else:  # pragma: no cover\n                data = np.empty(state)\n                np.ndarray.__setstate__(data, state)\n\n            self._data = data\n            self._reset_identity()\n\n        else:\n            raise Exception(\"invalid pickle state\")\n    _unpickle_compat = __setstate__\n\n    def _convert_for_op(self, value):\n        \"\"\" Convert value to be insertable to ndarray \"\"\"\n        if self._has_same_tz(value):\n            return _to_m8(value)\n        raise ValueError('Passed item and index have different timezone')\n\n    def _maybe_update_attributes(self, attrs):\n        \"\"\" Update Index attributes (e.g. freq) depending on op \"\"\"\n        freq = attrs.get('freq', None)\n        if freq is not None:\n            # no need to infer if freq is None\n            attrs['freq'] = 'infer'\n        return attrs\n\n    def _format_native_types(self, na_rep='NaT', date_format=None, **kwargs):\n        from pandas.io.formats.format import _get_format_datetime64_from_values\n        format = _get_format_datetime64_from_values(self, date_format)\n\n        return libts.format_array_from_datetime(self.asi8,\n                                                tz=self.tz,\n                                                format=format,\n                                                na_rep=na_rep)\n\n    @Appender(_index_shared_docs['astype'])\n    def astype(self, dtype, copy=True):\n        dtype = pandas_dtype(dtype)\n        if (is_datetime64_ns_dtype(dtype) and\n                not is_dtype_equal(dtype, self.dtype)):\n            # GH 18951: datetime64_ns dtype but not equal means different tz\n            new_tz = getattr(dtype, 'tz', None)\n            if getattr(self.dtype, 'tz', None) is None:\n                return self.tz_localize(new_tz)\n            return self.tz_convert(new_tz)\n        elif is_period_dtype(dtype):\n            return self.to_period(freq=dtype.freq)\n        return super(DatetimeIndex, self).astype(dtype, copy=copy)\n\n    def _get_time_micros(self):\n        values = self.asi8\n        if self.tz is not None and not timezones.is_utc(self.tz):\n            values = self._local_timestamps()\n        return fields.get_time_micros(values)\n\n    def to_series(self, keep_tz=None, index=None, name=None):\n        \"\"\"\n        Create a Series with both index and values equal to the index keys\n        useful with map for returning an indexer based on an index\n\n        Parameters\n        ----------\n        keep_tz : optional, defaults False\n            Return the data keeping the timezone.\n\n            If keep_tz is True:\n\n              If the timezone is not set, the resulting\n              Series will have a datetime64[ns] dtype.\n\n              Otherwise the Series will have an datetime64[ns, tz] dtype; the\n              tz will be preserved.\n\n            If keep_tz is False:\n\n              Series will have a datetime64[ns] dtype. TZ aware\n              objects will have the tz removed.\n\n            .. versionchanged:: 0.24\n                The default value will change to True in a future release.\n                You can set ``keep_tz=True`` to already obtain the future\n                behaviour and silence the warning.\n\n        index : Index, optional\n            index of resulting Series. If None, defaults to original index\n        name : string, optional\n            name of resulting Series. If None, defaults to name of original\n            index\n\n        Returns\n        -------\n        Series\n        \"\"\"\n        from pandas import Series\n\n        if index is None:\n            index = self._shallow_copy()\n        if name is None:\n            name = self.name\n\n        if keep_tz is None and self.tz is not None:\n            warnings.warn(\"The default of the 'keep_tz' keyword will change \"\n                          \"to True in a future release. You can set \"\n                          \"'keep_tz=True' to obtain the future behaviour and \"\n                          \"silence this warning.\", FutureWarning, stacklevel=2)\n            keep_tz = False\n        elif keep_tz is False:\n            warnings.warn(\"Specifying 'keep_tz=False' is deprecated and this \"\n                          \"option will be removed in a future release. If \"\n                          \"you want to remove the timezone information, you \"\n                          \"can do 'idx.tz_convert(None)' before calling \"\n                          \"'to_series'.\", FutureWarning, stacklevel=2)\n\n        if keep_tz and self.tz is not None:\n            # preserve the tz & copy\n            values = self.copy(deep=True)\n        else:\n            values = self.values.copy()\n\n        return Series(values, index=index, name=name)\n\n    def snap(self, freq='S'):\n        \"\"\"\n        Snap time stamps to nearest occurring frequency\n        \"\"\"\n        # Superdumb, punting on any optimizing\n        freq = to_offset(freq)\n\n        snapped = np.empty(len(self), dtype=_NS_DTYPE)\n\n        for i, v in enumerate(self):\n            s = v\n            if not freq.onOffset(s):\n                t0 = freq.rollback(s)\n                t1 = freq.rollforward(s)\n                if abs(s - t0) < abs(t1 - s):\n                    s = t0\n                else:\n                    s = t1\n            snapped[i] = s\n\n        # we know it conforms; skip check\n        return DatetimeIndex(snapped, freq=freq, verify_integrity=False)\n        # TODO: what about self.name?  if so, use shallow_copy?\n\n    def unique(self, level=None):\n        if level is not None:\n            self._validate_index_level(level)\n\n        # TODO(DatetimeArray): change dispatch once inheritance is removed\n        # call DatetimeArray method\n        result = DatetimeArray.unique(self)\n        return self._shallow_copy(result._data)\n\n    def union(self, other):\n        \"\"\"\n        Specialized union for DatetimeIndex objects. If combine\n        overlapping ranges with the same DateOffset, will be much\n        faster than Index.union\n\n        Parameters\n        ----------\n        other : DatetimeIndex or array-like\n\n        Returns\n        -------\n        y : Index or DatetimeIndex\n        \"\"\"\n        self._assert_can_do_setop(other)\n\n        if len(other) == 0 or self.equals(other) or len(self) == 0:\n            return super(DatetimeIndex, self).union(other)\n\n        if not isinstance(other, DatetimeIndex):\n            try:\n                other = DatetimeIndex(other)\n            except TypeError:\n                pass\n\n        this, other = self._maybe_utc_convert(other)\n\n        if this._can_fast_union(other):\n            return this._fast_union(other)\n        else:\n            result = Index.union(this, other)\n            if isinstance(result, DatetimeIndex):\n                result._tz = timezones.tz_standardize(this.tz)\n                if (result.freq is None and\n                        (this.freq is not None or other.freq is not None)):\n                    result.freq = to_offset(result.inferred_freq)\n            return result\n\n    def union_many(self, others):\n        \"\"\"\n        A bit of a hack to accelerate unioning a collection of indexes\n        \"\"\"\n        this = self\n\n        for other in others:\n            if not isinstance(this, DatetimeIndex):\n                this = Index.union(this, other)\n                continue\n\n            if not isinstance(other, DatetimeIndex):\n                try:\n                    other = DatetimeIndex(other)\n                except TypeError:\n                    pass\n\n            this, other = this._maybe_utc_convert(other)\n\n            if this._can_fast_union(other):\n                this = this._fast_union(other)\n            else:\n                tz = this.tz\n                this = Index.union(this, other)\n                if isinstance(this, DatetimeIndex):\n                    this._tz = timezones.tz_standardize(tz)\n\n        return this\n\n    def join(self, other, how='left', level=None, return_indexers=False,\n             sort=False):\n        \"\"\"\n        See Index.join\n        \"\"\"\n        if (not isinstance(other, DatetimeIndex) and len(other) > 0 and\n            other.inferred_type not in ('floating', 'integer', 'mixed-integer',\n                                        'mixed-integer-float', 'mixed')):\n            try:\n                other = DatetimeIndex(other)\n            except (TypeError, ValueError):\n                pass\n\n        this, other = self._maybe_utc_convert(other)\n        return Index.join(this, other, how=how, level=level,\n                          return_indexers=return_indexers, sort=sort)\n\n    def _maybe_utc_convert(self, other):\n        this = self\n        if isinstance(other, DatetimeIndex):\n            if self.tz is not None:\n                if other.tz is None:\n                    raise TypeError('Cannot join tz-naive with tz-aware '\n                                    'DatetimeIndex')\n            elif other.tz is not None:\n                raise TypeError('Cannot join tz-naive with tz-aware '\n                                'DatetimeIndex')\n\n            if not timezones.tz_compare(self.tz, other.tz):\n                this = self.tz_convert('UTC')\n                other = other.tz_convert('UTC')\n        return this, other\n\n    def _wrap_joined_index(self, joined, other):\n        name = get_op_result_name(self, other)\n        if (isinstance(other, DatetimeIndex) and\n                self.freq == other.freq and\n                self._can_fast_union(other)):\n            joined = self._shallow_copy(joined)\n            joined.name = name\n            return joined\n        else:\n            tz = getattr(other, 'tz', None)\n            return self._simple_new(joined, name, tz=tz)\n\n    def _can_fast_union(self, other):\n        if not isinstance(other, DatetimeIndex):\n            return False\n\n        freq = self.freq\n\n        if freq is None or freq != other.freq:\n            return False\n\n        if not self.is_monotonic or not other.is_monotonic:\n            return False\n\n        if len(self) == 0 or len(other) == 0:\n            return True\n\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        else:\n            left, right = other, self\n\n        right_start = right[0]\n        left_end = left[-1]\n\n        # Only need to \"adjoin\", not overlap\n        try:\n            return (right_start == left_end + freq) or right_start in left\n        except (ValueError):\n\n            # if we are comparing a freq that does not propagate timezones\n            # this will raise\n            return False\n\n    def _fast_union(self, other):\n        if len(other) == 0:\n            return self.view(type(self))\n\n        if len(self) == 0:\n            return other.view(type(self))\n\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        else:\n            left, right = other, self\n\n        left_end = left[-1]\n        right_end = right[-1]\n\n        # TODO: consider re-implementing freq._should_cache for fastpath\n\n        # concatenate dates\n        if left_end < right_end:\n            loc = right.searchsorted(left_end, side='right')\n            right_chunk = right.values[loc:]\n            dates = _concat._concat_compat((left.values, right_chunk))\n            return self._shallow_copy(dates)\n        else:\n            return left\n\n    def _wrap_setop_result(self, other, result):\n        name = get_op_result_name(self, other)\n        if not timezones.tz_compare(self.tz, other.tz):\n            raise ValueError('Passed item and index have different timezone')\n        return self._shallow_copy(result, name=name, freq=None, tz=self.tz)\n\n    def intersection(self, other):\n        \"\"\"\n        Specialized intersection for DatetimeIndex objects. May be much faster\n        than Index.intersection\n\n        Parameters\n        ----------\n        other : DatetimeIndex or array-like\n\n        Returns\n        -------\n        y : Index or DatetimeIndex\n        \"\"\"\n        self._assert_can_do_setop(other)\n\n        if self.equals(other):\n            return self._get_reconciled_name_object(other)\n\n        if not isinstance(other, DatetimeIndex):\n            try:\n                other = DatetimeIndex(other)\n            except (TypeError, ValueError):\n                pass\n            result = Index.intersection(self, other)\n            if isinstance(result, DatetimeIndex):\n                if result.freq is None:\n                    result.freq = to_offset(result.inferred_freq)\n            return result\n\n        elif (other.freq is None or self.freq is None or\n              other.freq != self.freq or\n              not other.freq.isAnchored() or\n              (not self.is_monotonic or not other.is_monotonic)):\n            result = Index.intersection(self, other)\n            result = self._shallow_copy(result._values, name=result.name,\n                                        tz=result.tz, freq=None)\n            if result.freq is None:\n                result.freq = to_offset(result.inferred_freq)\n            return result\n\n        if len(self) == 0:\n            return self\n        if len(other) == 0:\n            return other\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        else:\n            left, right = other, self\n\n        end = min(left[-1], right[-1])\n        start = right[0]\n\n        if end < start:\n            return type(self)(data=[])\n        else:\n            lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left.values[lslice]\n            return self._shallow_copy(left_chunk)\n\n    def _parsed_string_to_bounds(self, reso, parsed):\n        \"\"\"\n        Calculate datetime bounds for parsed time string and its resolution.\n\n        Parameters\n        ----------\n        reso : Resolution\n            Resolution provided by parsed string.\n        parsed : datetime\n            Datetime from parsed string.\n\n        Returns\n        -------\n        lower, upper: pd.Timestamp\n\n        \"\"\"\n        if reso == 'year':\n            return (Timestamp(datetime(parsed.year, 1, 1), tz=self.tz),\n                    Timestamp(datetime(parsed.year, 12, 31, 23,\n                                       59, 59, 999999), tz=self.tz))\n        elif reso == 'month':\n            d = ccalendar.get_days_in_month(parsed.year, parsed.month)\n            return (Timestamp(datetime(parsed.year, parsed.month, 1),\n                              tz=self.tz),\n                    Timestamp(datetime(parsed.year, parsed.month, d, 23,\n                                       59, 59, 999999), tz=self.tz))\n        elif reso == 'quarter':\n            qe = (((parsed.month - 1) + 2) % 12) + 1  # two months ahead\n            d = ccalendar.get_days_in_month(parsed.year, qe)  # at end of month\n            return (Timestamp(datetime(parsed.year, parsed.month, 1),\n                              tz=self.tz),\n                    Timestamp(datetime(parsed.year, qe, d, 23, 59,\n                                       59, 999999), tz=self.tz))\n        elif reso == 'day':\n            st = datetime(parsed.year, parsed.month, parsed.day)\n            return (Timestamp(st, tz=self.tz),\n                    Timestamp(Timestamp(st + offsets.Day(),\n                                        tz=self.tz).value - 1))\n        elif reso == 'hour':\n            st = datetime(parsed.year, parsed.month, parsed.day,\n                          hour=parsed.hour)\n            return (Timestamp(st, tz=self.tz),\n                    Timestamp(Timestamp(st + offsets.Hour(),\n                                        tz=self.tz).value - 1))\n        elif reso == 'minute':\n            st = datetime(parsed.year, parsed.month, parsed.day,\n                          hour=parsed.hour, minute=parsed.minute)\n            return (Timestamp(st, tz=self.tz),\n                    Timestamp(Timestamp(st + offsets.Minute(),\n                                        tz=self.tz).value - 1))\n        elif reso == 'second':\n            st = datetime(parsed.year, parsed.month, parsed.day,\n                          hour=parsed.hour, minute=parsed.minute,\n                          second=parsed.second)\n            return (Timestamp(st, tz=self.tz),\n                    Timestamp(Timestamp(st + offsets.Second(),\n                                        tz=self.tz).value - 1))\n        elif reso == 'microsecond':\n            st = datetime(parsed.year, parsed.month, parsed.day,\n                          parsed.hour, parsed.minute, parsed.second,\n                          parsed.microsecond)\n            return (Timestamp(st, tz=self.tz), Timestamp(st, tz=self.tz))\n        else:\n            raise KeyError\n\n    def _partial_date_slice(self, reso, parsed, use_lhs=True, use_rhs=True):\n        is_monotonic = self.is_monotonic\n        if (is_monotonic and reso in ['day', 'hour', 'minute', 'second'] and\n                self._resolution >= Resolution.get_reso(reso)):\n            # These resolution/monotonicity validations came from GH3931,\n            # GH3452 and GH2369.\n\n            # See also GH14826\n            raise KeyError\n\n        if reso == 'microsecond':\n            # _partial_date_slice doesn't allow microsecond resolution, but\n            # _parsed_string_to_bounds allows it.\n            raise KeyError\n\n        t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n        stamps = self.asi8\n\n        if is_monotonic:\n\n            # we are out of range\n            if (len(stamps) and ((use_lhs and t1.value < stamps[0] and\n                                  t2.value < stamps[0]) or\n                                 ((use_rhs and t1.value > stamps[-1] and\n                                   t2.value > stamps[-1])))):\n                raise KeyError\n\n            # a monotonic (sorted) series can be sliced\n            left = stamps.searchsorted(\n                t1.value, side='left') if use_lhs else None\n            right = stamps.searchsorted(\n                t2.value, side='right') if use_rhs else None\n\n            return slice(left, right)\n\n        lhs_mask = (stamps >= t1.value) if use_lhs else True\n        rhs_mask = (stamps <= t2.value) if use_rhs else True\n\n        # try to find a the dates\n        return (lhs_mask & rhs_mask).nonzero()[0]\n\n    def _maybe_promote(self, other):\n        if other.inferred_type == 'date':\n            other = DatetimeIndex(other)\n        return self, other\n\n    def get_value(self, series, key):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray. Only use this if you\n        know what you're doing\n        \"\"\"\n\n        if isinstance(key, datetime):\n\n            # needed to localize naive datetimes\n            if self.tz is not None:\n                if key.tzinfo is not None:\n                    key = Timestamp(key).tz_convert(self.tz)\n                else:\n                    key = Timestamp(key).tz_localize(self.tz)\n\n            return self.get_value_maybe_box(series, key)\n\n        if isinstance(key, time):\n            locs = self.indexer_at_time(key)\n            return series.take(locs)\n\n        try:\n            return com.maybe_box(self, Index.get_value(self, series, key),\n                                 series, key)\n        except KeyError:\n            try:\n                loc = self._get_string_slice(key)\n                return series[loc]\n            except (TypeError, ValueError, KeyError):\n                pass\n\n            try:\n                return self.get_value_maybe_box(series, key)\n            except (TypeError, ValueError, KeyError):\n                raise KeyError(key)\n\n    def get_value_maybe_box(self, series, key):\n        # needed to localize naive datetimes\n        if self.tz is not None:\n            key = Timestamp(key)\n            if key.tzinfo is not None:\n                key = key.tz_convert(self.tz)\n            else:\n                key = key.tz_localize(self.tz)\n        elif not isinstance(key, Timestamp):\n            key = Timestamp(key)\n        values = self._engine.get_value(com.values_from_object(series),\n                                        key, tz=self.tz)\n        return com.maybe_box(self, values, series, key)\n\n    def get_loc(self, key, method=None, tolerance=None):\n        \"\"\"\n        Get integer location for requested label\n\n        Returns\n        -------\n        loc : int\n        \"\"\"\n\n        if tolerance is not None:\n            # try converting tolerance now, so errors don't get swallowed by\n            # the try/except clauses below\n            tolerance = self._convert_tolerance(tolerance, np.asarray(key))\n\n        if isinstance(key, datetime):\n            # needed to localize naive datetimes\n            if key.tzinfo is None:\n                key = Timestamp(key, tz=self.tz)\n            else:\n                key = Timestamp(key).tz_convert(self.tz)\n            return Index.get_loc(self, key, method, tolerance)\n\n        elif isinstance(key, timedelta):\n            # GH#20464\n            raise TypeError(\"Cannot index {cls} with {other}\"\n                            .format(cls=type(self).__name__,\n                                    other=type(key).__name__))\n\n        if isinstance(key, time):\n            if method is not None:\n                raise NotImplementedError('cannot yet lookup inexact labels '\n                                          'when key is a time object')\n            return self.indexer_at_time(key)\n\n        try:\n            return Index.get_loc(self, key, method, tolerance)\n        except (KeyError, ValueError, TypeError):\n            try:\n                return self._get_string_slice(key)\n            except (TypeError, KeyError, ValueError):\n                pass\n\n            try:\n                stamp = Timestamp(key)\n                if stamp.tzinfo is not None and self.tz is not None:\n                    stamp = stamp.tz_convert(self.tz)\n                else:\n                    stamp = stamp.tz_localize(self.tz)\n                return Index.get_loc(self, stamp, method, tolerance)\n            except KeyError:\n                raise KeyError(key)\n            except ValueError as e:\n                # list-like tolerance size must match target index size\n                if 'list-like' in str(e):\n                    raise e\n                raise KeyError(key)\n\n    def _maybe_cast_slice_bound(self, label, side, kind):\n        \"\"\"\n        If label is a string, cast it to datetime according to resolution.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'ix', 'loc', 'getitem'}\n\n        Returns\n        -------\n        label :  object\n\n        Notes\n        -----\n        Value of `side` parameter should be validated in caller.\n\n        \"\"\"\n        assert kind in ['ix', 'loc', 'getitem', None]\n\n        if is_float(label) or isinstance(label, time) or is_integer(label):\n            self._invalid_indexer('slice', label)\n\n        if isinstance(label, compat.string_types):\n            freq = getattr(self, 'freqstr',\n                           getattr(self, 'inferred_freq', None))\n            _, parsed, reso = parsing.parse_time_string(label, freq)\n            lower, upper = self._parsed_string_to_bounds(reso, parsed)\n            # lower, upper form the half-open interval:\n            #   [parsed, parsed + 1 freq)\n            # because label may be passed to searchsorted\n            # the bounds need swapped if index is reverse sorted and has a\n            # length > 1 (is_monotonic_decreasing gives True for empty\n            # and length 1 index)\n            if self._is_strictly_monotonic_decreasing and len(self) > 1:\n                return upper if side == 'left' else lower\n            return lower if side == 'left' else upper\n        else:\n            return label\n\n    def _get_string_slice(self, key, use_lhs=True, use_rhs=True):\n        freq = getattr(self, 'freqstr',\n                       getattr(self, 'inferred_freq', None))\n        _, parsed, reso = parsing.parse_time_string(key, freq)\n        loc = self._partial_date_slice(reso, parsed, use_lhs=use_lhs,\n                                       use_rhs=use_rhs)\n        return loc\n\n    def slice_indexer(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        Return indexer for specified label slice.\n        Index.slice_indexer, customized to handle time slicing.\n\n        In addition to functionality provided by Index.slice_indexer, does the\n        following:\n\n        - if both `start` and `end` are instances of `datetime.time`, it\n          invokes `indexer_between_time`\n        - if `start` and `end` are both either string or None perform\n          value-based selection in non-monotonic cases.\n\n        \"\"\"\n        # For historical reasons DatetimeIndex supports slices between two\n        # instances of datetime.time as if it were applying a slice mask to\n        # an array of (self.hour, self.minute, self.seconds, self.microsecond).\n        if isinstance(start, time) and isinstance(end, time):\n            if step is not None and step != 1:\n                raise ValueError('Must have step size of 1 with time slices')\n            return self.indexer_between_time(start, end)\n\n        if isinstance(start, time) or isinstance(end, time):\n            raise KeyError('Cannot mix time and non-time slice keys')\n\n        try:\n            return Index.slice_indexer(self, start, end, step, kind=kind)\n        except KeyError:\n            # For historical reasons DatetimeIndex by default supports\n            # value-based partial (aka string) slices on non-monotonic arrays,\n            # let's try that.\n            if ((start is None or isinstance(start, compat.string_types)) and\n                    (end is None or isinstance(end, compat.string_types))):\n                mask = True\n                if start is not None:\n                    start_casted = self._maybe_cast_slice_bound(\n                        start, 'left', kind)\n                    mask = start_casted <= self\n\n                if end is not None:\n                    end_casted = self._maybe_cast_slice_bound(\n                        end, 'right', kind)\n                    mask = (self <= end_casted) & mask\n\n                indexer = mask.nonzero()[0][::step]\n                if len(indexer) == len(self):\n                    return slice(None)\n                else:\n                    return indexer\n            else:\n                raise\n\n    # --------------------------------------------------------------------\n    # Wrapping DatetimeArray\n\n    _timezone = cache_readonly(DatetimeArray._timezone.fget)\n    is_normalized = cache_readonly(DatetimeArray.is_normalized.fget)\n    _resolution = cache_readonly(DatetimeArray._resolution.fget)\n\n    year = wrap_field_accessor(DatetimeArray.year)\n    month = wrap_field_accessor(DatetimeArray.month)\n    day = wrap_field_accessor(DatetimeArray.day)\n    hour = wrap_field_accessor(DatetimeArray.hour)\n    minute = wrap_field_accessor(DatetimeArray.minute)\n    second = wrap_field_accessor(DatetimeArray.second)\n    microsecond = wrap_field_accessor(DatetimeArray.microsecond)\n    nanosecond = wrap_field_accessor(DatetimeArray.nanosecond)\n    weekofyear = wrap_field_accessor(DatetimeArray.weekofyear)\n    week = weekofyear\n    dayofweek = wrap_field_accessor(DatetimeArray.dayofweek)\n    weekday = dayofweek\n\n    weekday_name = wrap_field_accessor(DatetimeArray.weekday_name)\n\n    dayofyear = wrap_field_accessor(DatetimeArray.dayofyear)\n    quarter = wrap_field_accessor(DatetimeArray.quarter)\n    days_in_month = wrap_field_accessor(DatetimeArray.days_in_month)\n    daysinmonth = days_in_month\n    is_month_start = wrap_field_accessor(DatetimeArray.is_month_start)\n    is_month_end = wrap_field_accessor(DatetimeArray.is_month_end)\n    is_quarter_start = wrap_field_accessor(DatetimeArray.is_quarter_start)\n    is_quarter_end = wrap_field_accessor(DatetimeArray.is_quarter_end)\n    is_year_start = wrap_field_accessor(DatetimeArray.is_year_start)\n    is_year_end = wrap_field_accessor(DatetimeArray.is_year_end)\n    is_leap_year = wrap_field_accessor(DatetimeArray.is_leap_year)\n\n    tz_localize = wrap_array_method(DatetimeArray.tz_localize, True)\n    tz_convert = wrap_array_method(DatetimeArray.tz_convert, True)\n    to_perioddelta = wrap_array_method(DatetimeArray.to_perioddelta,\n                                       False)\n    to_period = wrap_array_method(DatetimeArray.to_period, True)\n    normalize = wrap_array_method(DatetimeArray.normalize, True)\n    to_julian_date = wrap_array_method(DatetimeArray.to_julian_date,\n                                       False)\n    month_name = wrap_array_method(DatetimeArray.month_name, True)\n    day_name = wrap_array_method(DatetimeArray.day_name, True)\n\n    # --------------------------------------------------------------------\n\n    @Substitution(klass='DatetimeIndex')\n    @Appender(_shared_docs['searchsorted'])\n    def searchsorted(self, value, side='left', sorter=None):\n        if isinstance(value, (np.ndarray, Index)):\n            value = np.array(value, dtype=_NS_DTYPE, copy=False)\n        else:\n            value = _to_m8(value, tz=self.tz)\n\n        return self.values.searchsorted(value, side=side)\n\n    def is_type_compatible(self, typ):\n        return typ == self.inferred_type or typ == 'datetime'\n\n    @property\n    def inferred_type(self):\n        # b/c datetime is represented as microseconds since the epoch, make\n        # sure we can't have ambiguous indexing\n        return 'datetime64'\n\n    @property\n    def is_all_dates(self):\n        return True\n\n    def insert(self, loc, item):\n        \"\"\"\n        Make new Index inserting new item at location\n\n        Parameters\n        ----------\n        loc : int\n        item : object\n            if not either a Python datetime or a numpy integer-like, returned\n            Index dtype will be object rather than datetime.\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        if is_scalar(item) and isna(item):\n            # GH 18295\n            item = self._na_value\n\n        freq = None\n\n        if isinstance(item, (datetime, np.datetime64)):\n            self._assert_can_do_op(item)\n            if not self._has_same_tz(item) and not isna(item):\n                raise ValueError(\n                    'Passed item and index have different timezone')\n            # check freq can be preserved on edge cases\n            if self.size and self.freq is not None:\n                if ((loc == 0 or loc == -len(self)) and\n                        item + self.freq == self[0]):\n                    freq = self.freq\n                elif (loc == len(self)) and item - self.freq == self[-1]:\n                    freq = self.freq\n            item = _to_m8(item, tz=self.tz)\n\n        try:\n            new_dates = np.concatenate((self[:loc].asi8, [item.view(np.int64)],\n                                        self[loc:].asi8))\n            return self._shallow_copy(new_dates, freq=freq)\n        except (AttributeError, TypeError):\n\n            # fall back to object index\n            if isinstance(item, compat.string_types):\n                return self.astype(object).insert(loc, item)\n            raise TypeError(\n                \"cannot insert DatetimeIndex with incompatible label\")\n\n    def delete(self, loc):\n        \"\"\"\n        Make a new DatetimeIndex with passed location(s) deleted.\n\n        Parameters\n        ----------\n        loc: int, slice or array of ints\n            Indicate which sub-arrays to remove.\n\n        Returns\n        -------\n        new_index : DatetimeIndex\n        \"\"\"\n        new_dates = np.delete(self.asi8, loc)\n\n        freq = None\n        if is_integer(loc):\n            if loc in (0, -len(self), -1, len(self) - 1):\n                freq = self.freq\n        else:\n            if is_list_like(loc):\n                loc = lib.maybe_indices_to_slice(\n                    ensure_int64(np.array(loc)), len(self))\n            if isinstance(loc, slice) and loc.step in (1, None):\n                if (loc.start in (0, None) or loc.stop in (len(self), None)):\n                    freq = self.freq\n\n        return self._shallow_copy(new_dates, freq=freq)\n\n    def indexer_at_time(self, time, asof=False):\n        \"\"\"\n        Returns index locations of index values at particular time of day\n        (e.g. 9:30AM).\n\n        Parameters\n        ----------\n        time : datetime.time or string\n            datetime.time or string in appropriate format (\"%H:%M\", \"%H%M\",\n            \"%I:%M%p\", \"%I%M%p\", \"%H:%M:%S\", \"%H%M%S\", \"%I:%M:%S%p\",\n            \"%I%M%S%p\").\n\n        Returns\n        -------\n        values_at_time : array of integers\n\n        See Also\n        --------\n        indexer_between_time, DataFrame.at_time\n        \"\"\"\n        from dateutil.parser import parse\n\n        if asof:\n            raise NotImplementedError(\"'asof' argument is not supported\")\n\n        if isinstance(time, compat.string_types):\n            time = parse(time).time()\n\n        if time.tzinfo:\n            # TODO\n            raise NotImplementedError(\"argument 'time' with timezone info is \"\n                                      \"not supported\")\n\n        time_micros = self._get_time_micros()\n        micros = _time_to_micros(time)\n        return (micros == time_micros).nonzero()[0]\n\n    def indexer_between_time(self, start_time, end_time, include_start=True,\n                             include_end=True):\n        \"\"\"\n        Return index locations of values between particular times of day\n        (e.g., 9:00-9:30AM).\n\n        Parameters\n        ----------\n        start_time, end_time : datetime.time, str\n            datetime.time or string in appropriate format (\"%H:%M\", \"%H%M\",\n            \"%I:%M%p\", \"%I%M%p\", \"%H:%M:%S\", \"%H%M%S\", \"%I:%M:%S%p\",\n            \"%I%M%S%p\").\n        include_start : boolean, default True\n        include_end : boolean, default True\n\n        Returns\n        -------\n        values_between_time : array of integers\n\n        See Also\n        --------\n        indexer_at_time, DataFrame.between_time\n        \"\"\"\n        start_time = tools.to_time(start_time)\n        end_time = tools.to_time(end_time)\n        time_micros = self._get_time_micros()\n        start_micros = _time_to_micros(start_time)\n        end_micros = _time_to_micros(end_time)\n\n        if include_start and include_end:\n            lop = rop = operator.le\n        elif include_start:\n            lop = operator.le\n            rop = operator.lt\n        elif include_end:\n            lop = operator.lt\n            rop = operator.le\n        else:\n            lop = rop = operator.lt\n\n        if start_time <= end_time:\n            join_op = operator.and_\n        else:\n            join_op = operator.or_\n\n        mask = join_op(lop(start_micros, time_micros),\n                       rop(time_micros, end_micros))\n\n        return mask.nonzero()[0]\n\n\nDatetimeIndex._add_comparison_ops()\nDatetimeIndex._add_numeric_methods_disabled()\nDatetimeIndex._add_logical_methods_disabled()\nDatetimeIndex._add_datetimelike_methods()\n\n\ndef date_range(start=None, end=None, periods=None, freq=None, tz=None,\n               normalize=False, name=None, closed=None, **kwargs):\n    \"\"\"\n    Return a fixed frequency DatetimeIndex.\n\n    Parameters\n    ----------\n    start : str or datetime-like, optional\n        Left bound for generating dates.\n    end : str or datetime-like, optional\n        Right bound for generating dates.\n    periods : integer, optional\n        Number of periods to generate.\n    freq : str or DateOffset, default 'D'\n        Frequency strings can have multiples, e.g. '5H'. See\n        :ref:`here <timeseries.offset_aliases>` for a list of\n        frequency aliases.\n    tz : str or tzinfo, optional\n        Time zone name for returning localized DatetimeIndex, for example\n        'Asia/Hong_Kong'. By default, the resulting DatetimeIndex is\n        timezone-naive.\n    normalize : bool, default False\n        Normalize start/end dates to midnight before generating date range.\n    name : str, default None\n        Name of the resulting DatetimeIndex.\n    closed : {None, 'left', 'right'}, optional\n        Make the interval closed with respect to the given frequency to\n        the 'left', 'right', or both sides (None, the default).\n    **kwargs\n        For compatibility. Has no effect on the result.\n\n    Returns\n    -------\n    rng : DatetimeIndex\n\n    See Also\n    --------\n    pandas.DatetimeIndex : An immutable container for datetimes.\n    pandas.timedelta_range : Return a fixed frequency TimedeltaIndex.\n    pandas.period_range : Return a fixed frequency PeriodIndex.\n    pandas.interval_range : Return a fixed frequency IntervalIndex.\n\n    Notes\n    -----\n    Of the four parameters ``start``, ``end``, ``periods``, and ``freq``,\n    exactly three must be specified. If ``freq`` is omitted, the resulting\n    ``DatetimeIndex`` will have ``periods`` linearly spaced elements between\n    ``start`` and ``end`` (closed on both sides).\n\n    To learn more about the frequency strings, please see `this link\n    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.\n\n    Examples\n    --------\n    **Specifying the values**\n\n    The next four examples generate the same `DatetimeIndex`, but vary\n    the combination of `start`, `end` and `periods`.\n\n    Specify `start` and `end`, with the default daily frequency.\n\n    >>> pd.date_range(start='1/1/2018', end='1/08/2018')\n    DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n                   '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n                  dtype='datetime64[ns]', freq='D')\n\n    Specify `start` and `periods`, the number of periods (days).\n\n    >>> pd.date_range(start='1/1/2018', periods=8)\n    DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n                   '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n                  dtype='datetime64[ns]', freq='D')\n\n    Specify `end` and `periods`, the number of periods (days).\n\n    >>> pd.date_range(end='1/1/2018', periods=8)\n    DatetimeIndex(['2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28',\n                   '2017-12-29', '2017-12-30', '2017-12-31', '2018-01-01'],\n                  dtype='datetime64[ns]', freq='D')\n\n    Specify `start`, `end`, and `periods`; the frequency is generated\n    automatically (linearly spaced).\n\n    >>> pd.date_range(start='2018-04-24', end='2018-04-27', periods=3)\n    DatetimeIndex(['2018-04-24 00:00:00', '2018-04-25 12:00:00',\n                   '2018-04-27 00:00:00'],\n                  dtype='datetime64[ns]', freq=None)\n\n    **Other Parameters**\n\n    Changed the `freq` (frequency) to ``'M'`` (month end frequency).\n\n    >>> pd.date_range(start='1/1/2018', periods=5, freq='M')\n    DatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n                   '2018-05-31'],\n                  dtype='datetime64[ns]', freq='M')\n\n    Multiples are allowed\n\n    >>> pd.date_range(start='1/1/2018', periods=5, freq='3M')\n    DatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n                   '2019-01-31'],\n                  dtype='datetime64[ns]', freq='3M')\n\n    `freq` can also be specified as an Offset object.\n\n    >>> pd.date_range(start='1/1/2018', periods=5, freq=pd.offsets.MonthEnd(3))\n    DatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n                   '2019-01-31'],\n                  dtype='datetime64[ns]', freq='3M')\n\n    Specify `tz` to set the timezone.\n\n    >>> pd.date_range(start='1/1/2018', periods=5, tz='Asia/Tokyo')\n    DatetimeIndex(['2018-01-01 00:00:00+09:00', '2018-01-02 00:00:00+09:00',\n                   '2018-01-03 00:00:00+09:00', '2018-01-04 00:00:00+09:00',\n                   '2018-01-05 00:00:00+09:00'],\n                  dtype='datetime64[ns, Asia/Tokyo]', freq='D')\n\n    `closed` controls whether to include `start` and `end` that are on the\n    boundary. The default includes boundary points on either end.\n\n    >>> pd.date_range(start='2017-01-01', end='2017-01-04', closed=None)\n    DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04'],\n                  dtype='datetime64[ns]', freq='D')\n\n    Use ``closed='left'`` to exclude `end` if it falls on the boundary.\n\n    >>> pd.date_range(start='2017-01-01', end='2017-01-04', closed='left')\n    DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03'],\n                  dtype='datetime64[ns]', freq='D')\n\n    Use ``closed='right'`` to exclude `start` if it falls on the boundary.\n\n    >>> pd.date_range(start='2017-01-01', end='2017-01-04', closed='right')\n    DatetimeIndex(['2017-01-02', '2017-01-03', '2017-01-04'],\n                  dtype='datetime64[ns]', freq='D')\n    \"\"\"\n\n    if freq is None and com._any_none(periods, start, end):\n        freq = 'D'\n\n    return DatetimeIndex(start=start, end=end, periods=periods,\n                         freq=freq, tz=tz, normalize=normalize, name=name,\n                         closed=closed, **kwargs)\n\n\ndef bdate_range(start=None, end=None, periods=None, freq='B', tz=None,\n                normalize=True, name=None, weekmask=None, holidays=None,\n                closed=None, **kwargs):\n    \"\"\"\n    Return a fixed frequency DatetimeIndex, with business day as the default\n    frequency\n\n    Parameters\n    ----------\n    start : string or datetime-like, default None\n        Left bound for generating dates.\n    end : string or datetime-like, default None\n        Right bound for generating dates.\n    periods : integer, default None\n        Number of periods to generate.\n    freq : string or DateOffset, default 'B' (business daily)\n        Frequency strings can have multiples, e.g. '5H'.\n    tz : string or None\n        Time zone name for returning localized DatetimeIndex, for example\n        Asia/Beijing.\n    normalize : bool, default False\n        Normalize start/end dates to midnight before generating date range.\n    name : string, default None\n        Name of the resulting DatetimeIndex.\n    weekmask : string or None, default None\n        Weekmask of valid business days, passed to ``numpy.busdaycalendar``,\n        only used when custom frequency strings are passed.  The default\n        value None is equivalent to 'Mon Tue Wed Thu Fri'.\n\n        .. versionadded:: 0.21.0\n\n    holidays : list-like or None, default None\n        Dates to exclude from the set of valid business days, passed to\n        ``numpy.busdaycalendar``, only used when custom frequency strings\n        are passed.\n\n        .. versionadded:: 0.21.0\n\n    closed : string, default None\n        Make the interval closed with respect to the given frequency to\n        the 'left', 'right', or both sides (None).\n    **kwargs\n        For compatibility. Has no effect on the result.\n\n    Notes\n    -----\n    Of the four parameters: ``start``, ``end``, ``periods``, and ``freq``,\n    exactly three must be specified.  Specifying ``freq`` is a requirement\n    for ``bdate_range``.  Use ``date_range`` if specifying ``freq`` is not\n    desired.\n\n    To learn more about the frequency strings, please see `this link\n    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.\n\n    Returns\n    -------\n    DatetimeIndex\n\n    Examples\n    --------\n    Note how the two weekend days are skipped in the result.\n\n    >>> pd.bdate_range(start='1/1/2018', end='1/08/2018')\n    DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-08'],\n              dtype='datetime64[ns]', freq='B')\n    \"\"\"\n    if freq is None:\n        msg = 'freq must be specified for bdate_range; use date_range instead'\n        raise TypeError(msg)\n\n    if is_string_like(freq) and freq.startswith('C'):\n        try:\n            weekmask = weekmask or 'Mon Tue Wed Thu Fri'\n            freq = prefix_mapping[freq](holidays=holidays, weekmask=weekmask)\n        except (KeyError, TypeError):\n            msg = 'invalid custom frequency string: {freq}'.format(freq=freq)\n            raise ValueError(msg)\n    elif holidays or weekmask:\n        msg = ('a custom frequency string is required when holidays or '\n               'weekmask are passed, got frequency {freq}').format(freq=freq)\n        raise ValueError(msg)\n\n    return DatetimeIndex(start=start, end=end, periods=periods,\n                         freq=freq, tz=tz, normalize=normalize, name=name,\n                         closed=closed, **kwargs)\n\n\ndef cdate_range(start=None, end=None, periods=None, freq='C', tz=None,\n                normalize=True, name=None, closed=None, **kwargs):\n    \"\"\"\n    Return a fixed frequency DatetimeIndex, with CustomBusinessDay as the\n    default frequency\n\n    .. deprecated:: 0.21.0\n\n    Parameters\n    ----------\n    start : string or datetime-like, default None\n        Left bound for generating dates\n    end : string or datetime-like, default None\n        Right bound for generating dates\n    periods : integer, default None\n        Number of periods to generate\n    freq : string or DateOffset, default 'C' (CustomBusinessDay)\n        Frequency strings can have multiples, e.g. '5H'\n    tz : string, default None\n        Time zone name for returning localized DatetimeIndex, for example\n        Asia/Beijing\n    normalize : bool, default False\n        Normalize start/end dates to midnight before generating date range\n    name : string, default None\n        Name of the resulting DatetimeIndex\n    weekmask : string, Default 'Mon Tue Wed Thu Fri'\n        weekmask of valid business days, passed to ``numpy.busdaycalendar``\n    holidays : list\n        list/array of dates to exclude from the set of valid business days,\n        passed to ``numpy.busdaycalendar``\n    closed : string, default None\n        Make the interval closed with respect to the given frequency to\n        the 'left', 'right', or both sides (None)\n\n    Notes\n    -----\n    Of the three parameters: ``start``, ``end``, and ``periods``, exactly two\n    must be specified.\n\n    To learn more about the frequency strings, please see `this link\n    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.\n\n    Returns\n    -------\n    rng : DatetimeIndex\n    \"\"\"\n    warnings.warn(\"cdate_range is deprecated and will be removed in a future \"\n                  \"version, instead use pd.bdate_range(..., freq='{freq}')\"\n                  .format(freq=freq), FutureWarning, stacklevel=2)\n\n    if freq == 'C':\n        holidays = kwargs.pop('holidays', [])\n        weekmask = kwargs.pop('weekmask', 'Mon Tue Wed Thu Fri')\n        freq = CDay(holidays=holidays, weekmask=weekmask)\n    return DatetimeIndex(start=start, end=end, periods=periods, freq=freq,\n                         tz=tz, normalize=normalize, name=name,\n                         closed=closed, **kwargs)\n\n\ndef _time_to_micros(time):\n    seconds = time.hour * 60 * 60 + 60 * time.minute + time.second\n    return 1000000 * seconds + time.microsecond\n"
    },
    {
      "filename": "pandas/core/indexes/multi.py",
      "content": "\n# pylint: disable=E1101,E1103,W0232\nimport datetime\nfrom sys import getsizeof\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import (\n    Timestamp, algos as libalgos, index as libindex, lib, tslibs)\nimport pandas.compat as compat\nfrom pandas.compat import lrange, lzip, map, range, zip\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import PerformanceWarning, UnsortedIndexError\nfrom pandas.util._decorators import Appender, cache_readonly\n\nfrom pandas.core.dtypes.common import (\n    ensure_int64, ensure_platform_int, is_categorical_dtype, is_hashable,\n    is_integer, is_iterator, is_list_like, is_object_dtype, is_scalar,\n    pandas_dtype)\nfrom pandas.core.dtypes.dtypes import ExtensionDtype, PandasExtensionDtype\nfrom pandas.core.dtypes.missing import array_equivalent, isna\n\nimport pandas.core.algorithms as algos\nimport pandas.core.common as com\nfrom pandas.core.config import get_option\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.base import (\n    Index, InvalidIndexError, _index_shared_docs, ensure_index)\nfrom pandas.core.indexes.frozen import FrozenList, _ensure_frozen\nimport pandas.core.missing as missing\n\nfrom pandas.io.formats.printing import pprint_thing\n\n_index_doc_kwargs = dict(ibase._index_doc_kwargs)\n_index_doc_kwargs.update(\n    dict(klass='MultiIndex',\n         target_klass='MultiIndex or list of tuples'))\n\n\nclass MultiIndexUIntEngine(libindex.BaseMultiIndexCodesEngine,\n                           libindex.UInt64Engine):\n    \"\"\"\n    This class manages a MultiIndex by mapping label combinations to positive\n    integers.\n    \"\"\"\n    _base = libindex.UInt64Engine\n\n    def _codes_to_ints(self, codes):\n        \"\"\"\n        Transform combination(s) of uint64 in one uint64 (each), in a strictly\n        monotonic way (i.e. respecting the lexicographic order of integer\n        combinations): see BaseMultiIndexCodesEngine documentation.\n\n        Parameters\n        ----------\n        codes : 1- or 2-dimensional array of dtype uint64\n            Combinations of integers (one per row)\n\n        Returns\n        ------\n        int_keys : scalar or 1-dimensional array, of dtype uint64\n            Integer(s) representing one combination (each)\n        \"\"\"\n        # Shift the representation of each level by the pre-calculated number\n        # of bits:\n        codes <<= self.offsets\n\n        # Now sum and OR are in fact interchangeable. This is a simple\n        # composition of the (disjunct) significant bits of each level (i.e.\n        # each column in \"codes\") in a single positive integer:\n        if codes.ndim == 1:\n            # Single key\n            return np.bitwise_or.reduce(codes)\n\n        # Multiple keys\n        return np.bitwise_or.reduce(codes, axis=1)\n\n\nclass MultiIndexPyIntEngine(libindex.BaseMultiIndexCodesEngine,\n                            libindex.ObjectEngine):\n    \"\"\"\n    This class manages those (extreme) cases in which the number of possible\n    label combinations overflows the 64 bits integers, and uses an ObjectEngine\n    containing Python integers.\n    \"\"\"\n    _base = libindex.ObjectEngine\n\n    def _codes_to_ints(self, codes):\n        \"\"\"\n        Transform combination(s) of uint64 in one Python integer (each), in a\n        strictly monotonic way (i.e. respecting the lexicographic order of\n        integer combinations): see BaseMultiIndexCodesEngine documentation.\n\n        Parameters\n        ----------\n        codes : 1- or 2-dimensional array of dtype uint64\n            Combinations of integers (one per row)\n\n        Returns\n        ------\n        int_keys : int, or 1-dimensional array of dtype object\n            Integer(s) representing one combination (each)\n        \"\"\"\n\n        # Shift the representation of each level by the pre-calculated number\n        # of bits. Since this can overflow uint64, first make sure we are\n        # working with Python integers:\n        codes = codes.astype('object') << self.offsets\n\n        # Now sum and OR are in fact interchangeable. This is a simple\n        # composition of the (disjunct) significant bits of each level (i.e.\n        # each column in \"codes\") in a single positive integer (per row):\n        if codes.ndim == 1:\n            # Single key\n            return np.bitwise_or.reduce(codes)\n\n        # Multiple keys\n        return np.bitwise_or.reduce(codes, axis=1)\n\n\nclass MultiIndex(Index):\n    \"\"\"\n    A multi-level, or hierarchical, index object for pandas objects\n\n    Parameters\n    ----------\n    levels : sequence of arrays\n        The unique labels for each level\n    labels : sequence of arrays\n        Integers for each level designating which label at each location\n    sortorder : optional int\n        Level of sortedness (must be lexicographically sorted by that\n        level)\n    names : optional sequence of objects\n        Names for each of the index levels. (name is accepted for compat)\n    copy : boolean, default False\n        Copy the meta-data\n    verify_integrity : boolean, default True\n        Check that the levels/labels are consistent and valid\n\n    Examples\n    ---------\n    A new ``MultiIndex`` is typically constructed using one of the helper\n    methods :meth:`MultiIndex.from_arrays`, :meth:`MultiIndex.from_product`\n    and :meth:`MultiIndex.from_tuples`. For example (using ``.from_arrays``):\n\n    >>> arrays = [[1, 1, 2, 2], ['red', 'blue', 'red', 'blue']]\n    >>> pd.MultiIndex.from_arrays(arrays, names=('number', 'color'))\n    MultiIndex(levels=[[1, 2], ['blue', 'red']],\n           labels=[[0, 0, 1, 1], [1, 0, 1, 0]],\n           names=['number', 'color'])\n\n    See further examples for how to construct a MultiIndex in the doc strings\n    of the mentioned helper methods.\n\n    Notes\n    -----\n    See the `user guide\n    <http://pandas.pydata.org/pandas-docs/stable/advanced.html>`_ for more.\n\n    See Also\n    --------\n    MultiIndex.from_arrays  : Convert list of arrays to MultiIndex.\n    MultiIndex.from_product : Create a MultiIndex from the cartesian product\n                              of iterables.\n    MultiIndex.from_tuples  : Convert list of tuples to a MultiIndex.\n    Index : The base pandas Index type.\n\n    Attributes\n    ----------\n    names\n    levels\n    labels\n    nlevels\n    levshape\n\n    Methods\n    -------\n    from_arrays\n    from_tuples\n    from_product\n    set_levels\n    set_labels\n    to_frame\n    to_flat_index\n    is_lexsorted\n    sortlevel\n    droplevel\n    swaplevel\n    reorder_levels\n    remove_unused_levels\n    \"\"\"\n\n    # initialize to zero-length tuples to make everything work\n    _typ = 'multiindex'\n    _names = FrozenList()\n    _levels = FrozenList()\n    _labels = FrozenList()\n    _comparables = ['names']\n    rename = Index.set_names\n\n    def __new__(cls, levels=None, labels=None, sortorder=None, names=None,\n                dtype=None, copy=False, name=None,\n                verify_integrity=True, _set_identity=True):\n\n        # compat with Index\n        if name is not None:\n            names = name\n        if levels is None or labels is None:\n            raise TypeError(\"Must pass both levels and labels\")\n        if len(levels) != len(labels):\n            raise ValueError('Length of levels and labels must be the same.')\n        if len(levels) == 0:\n            raise ValueError('Must pass non-zero number of levels/labels')\n\n        result = object.__new__(MultiIndex)\n\n        # we've already validated levels and labels, so shortcut here\n        result._set_levels(levels, copy=copy, validate=False)\n        result._set_labels(labels, copy=copy, validate=False)\n\n        if names is not None:\n            # handles name validation\n            result._set_names(names)\n\n        if sortorder is not None:\n            result.sortorder = int(sortorder)\n        else:\n            result.sortorder = sortorder\n\n        if verify_integrity:\n            result._verify_integrity()\n        if _set_identity:\n            result._reset_identity()\n        return result\n\n    def _verify_integrity(self, labels=None, levels=None):\n        \"\"\"\n\n        Parameters\n        ----------\n        labels : optional list\n            Labels to check for validity. Defaults to current labels.\n        levels : optional list\n            Levels to check for validity. Defaults to current levels.\n\n        Raises\n        ------\n        ValueError\n            If length of levels and labels don't match, if any label would\n            exceed level bounds, or there are any duplicate levels.\n        \"\"\"\n        # NOTE: Currently does not check, among other things, that cached\n        # nlevels matches nor that sortorder matches actually sortorder.\n        labels = labels or self.labels\n        levels = levels or self.levels\n\n        if len(levels) != len(labels):\n            raise ValueError(\"Length of levels and labels must match. NOTE:\"\n                             \" this index is in an inconsistent state.\")\n        label_length = len(self.labels[0])\n        for i, (level, label) in enumerate(zip(levels, labels)):\n            if len(label) != label_length:\n                raise ValueError(\"Unequal label lengths: %s\" %\n                                 ([len(lab) for lab in labels]))\n            if len(label) and label.max() >= len(level):\n                raise ValueError(\"On level %d, label max (%d) >= length of\"\n                                 \" level  (%d). NOTE: this index is in an\"\n                                 \" inconsistent state\" % (i, label.max(),\n                                                          len(level)))\n            if not level.is_unique:\n                raise ValueError(\"Level values must be unique: {values} on \"\n                                 \"level {level}\".format(\n                                     values=[value for value in level],\n                                     level=i))\n\n    @property\n    def levels(self):\n        return self._levels\n\n    @property\n    def _is_homogeneous_type(self):\n        \"\"\"Whether the levels of a MultiIndex all have the same dtype.\n\n        This looks at the dtypes of the levels.\n\n        See Also\n        --------\n        Index._is_homogeneous_type\n        DataFrame._is_homogeneous_type\n\n        Examples\n        --------\n        >>> MultiIndex.from_tuples([\n        ...     ('a', 'b'), ('a', 'c')])._is_homogeneous_type\n        True\n        >>> MultiIndex.from_tuples([\n        ...     ('a', 1), ('a', 2)])._is_homogeneous_type\n        False\n        \"\"\"\n        return len({x.dtype for x in self.levels}) <= 1\n\n    def _set_levels(self, levels, level=None, copy=False, validate=True,\n                    verify_integrity=False):\n        # This is NOT part of the levels property because it should be\n        # externally not allowed to set levels. User beware if you change\n        # _levels directly\n        if validate and len(levels) == 0:\n            raise ValueError('Must set non-zero number of levels.')\n        if validate and level is None and len(levels) != self.nlevels:\n            raise ValueError('Length of levels must match number of levels.')\n        if validate and level is not None and len(levels) != len(level):\n            raise ValueError('Length of levels must match length of level.')\n\n        if level is None:\n            new_levels = FrozenList(\n                ensure_index(lev, copy=copy)._shallow_copy()\n                for lev in levels)\n        else:\n            level = [self._get_level_number(l) for l in level]\n            new_levels = list(self._levels)\n            for l, v in zip(level, levels):\n                new_levels[l] = ensure_index(v, copy=copy)._shallow_copy()\n            new_levels = FrozenList(new_levels)\n\n        if verify_integrity:\n            self._verify_integrity(levels=new_levels)\n\n        names = self.names\n        self._levels = new_levels\n        if any(names):\n            self._set_names(names)\n\n        self._tuples = None\n        self._reset_cache()\n\n    def set_levels(self, levels, level=None, inplace=False,\n                   verify_integrity=True):\n        \"\"\"\n        Set new levels on MultiIndex. Defaults to returning\n        new index.\n\n        Parameters\n        ----------\n        levels : sequence or list of sequence\n            new level(s) to apply\n        level : int, level name, or sequence of int/level names (default None)\n            level(s) to set (None for all levels)\n        inplace : bool\n            if True, mutates in place\n        verify_integrity : bool (default True)\n            if True, checks that levels and labels are compatible\n\n        Returns\n        -------\n        new index (of same type and class...etc)\n\n        Examples\n        --------\n        >>> idx = pd.MultiIndex.from_tuples([(1, u'one'), (1, u'two'),\n                                            (2, u'one'), (2, u'two')],\n                                            names=['foo', 'bar'])\n        >>> idx.set_levels([['a','b'], [1,2]])\n        MultiIndex(levels=[[u'a', u'b'], [1, 2]],\n                   labels=[[0, 0, 1, 1], [0, 1, 0, 1]],\n                   names=[u'foo', u'bar'])\n        >>> idx.set_levels(['a','b'], level=0)\n        MultiIndex(levels=[[u'a', u'b'], [u'one', u'two']],\n                   labels=[[0, 0, 1, 1], [0, 1, 0, 1]],\n                   names=[u'foo', u'bar'])\n        >>> idx.set_levels(['a','b'], level='bar')\n        MultiIndex(levels=[[1, 2], [u'a', u'b']],\n                   labels=[[0, 0, 1, 1], [0, 1, 0, 1]],\n                   names=[u'foo', u'bar'])\n        >>> idx.set_levels([['a','b'], [1,2]], level=[0,1])\n        MultiIndex(levels=[[u'a', u'b'], [1, 2]],\n                   labels=[[0, 0, 1, 1], [0, 1, 0, 1]],\n                   names=[u'foo', u'bar'])\n        \"\"\"\n        if is_list_like(levels) and not isinstance(levels, Index):\n            levels = list(levels)\n\n        if level is not None and not is_list_like(level):\n            if not is_list_like(levels):\n                raise TypeError(\"Levels must be list-like\")\n            if is_list_like(levels[0]):\n                raise TypeError(\"Levels must be list-like\")\n            level = [level]\n            levels = [levels]\n        elif level is None or is_list_like(level):\n            if not is_list_like(levels) or not is_list_like(levels[0]):\n                raise TypeError(\"Levels must be list of lists-like\")\n\n        if inplace:\n            idx = self\n        else:\n            idx = self._shallow_copy()\n        idx._reset_identity()\n        idx._set_levels(levels, level=level, validate=True,\n                        verify_integrity=verify_integrity)\n        if not inplace:\n            return idx\n\n    @property\n    def labels(self):\n        return self._labels\n\n    def _set_labels(self, labels, level=None, copy=False, validate=True,\n                    verify_integrity=False):\n\n        if validate and level is None and len(labels) != self.nlevels:\n            raise ValueError(\"Length of labels must match number of levels\")\n        if validate and level is not None and len(labels) != len(level):\n            raise ValueError('Length of labels must match length of levels.')\n\n        if level is None:\n            new_labels = FrozenList(\n                _ensure_frozen(lab, lev, copy=copy)._shallow_copy()\n                for lev, lab in zip(self.levels, labels))\n        else:\n            level = [self._get_level_number(l) for l in level]\n            new_labels = list(self._labels)\n            for lev_idx, lab in zip(level, labels):\n                lev = self.levels[lev_idx]\n                new_labels[lev_idx] = _ensure_frozen(\n                    lab, lev, copy=copy)._shallow_copy()\n            new_labels = FrozenList(new_labels)\n\n        if verify_integrity:\n            self._verify_integrity(labels=new_labels)\n\n        self._labels = new_labels\n        self._tuples = None\n        self._reset_cache()\n\n    def set_labels(self, labels, level=None, inplace=False,\n                   verify_integrity=True):\n        \"\"\"\n        Set new labels on MultiIndex. Defaults to returning\n        new index.\n\n        Parameters\n        ----------\n        labels : sequence or list of sequence\n            new labels to apply\n        level : int, level name, or sequence of int/level names (default None)\n            level(s) to set (None for all levels)\n        inplace : bool\n            if True, mutates in place\n        verify_integrity : bool (default True)\n            if True, checks that levels and labels are compatible\n\n        Returns\n        -------\n        new index (of same type and class...etc)\n\n        Examples\n        --------\n        >>> idx = pd.MultiIndex.from_tuples([(1, u'one'), (1, u'two'),\n                                            (2, u'one'), (2, u'two')],\n                                            names=['foo', 'bar'])\n        >>> idx.set_labels([[1,0,1,0], [0,0,1,1]])\n        MultiIndex(levels=[[1, 2], [u'one', u'two']],\n                   labels=[[1, 0, 1, 0], [0, 0, 1, 1]],\n                   names=[u'foo', u'bar'])\n        >>> idx.set_labels([1,0,1,0], level=0)\n        MultiIndex(levels=[[1, 2], [u'one', u'two']],\n                   labels=[[1, 0, 1, 0], [0, 1, 0, 1]],\n                   names=[u'foo', u'bar'])\n        >>> idx.set_labels([0,0,1,1], level='bar')\n        MultiIndex(levels=[[1, 2], [u'one', u'two']],\n                   labels=[[0, 0, 1, 1], [0, 0, 1, 1]],\n                   names=[u'foo', u'bar'])\n        >>> idx.set_labels([[1,0,1,0], [0,0,1,1]], level=[0,1])\n        MultiIndex(levels=[[1, 2], [u'one', u'two']],\n                   labels=[[1, 0, 1, 0], [0, 0, 1, 1]],\n                   names=[u'foo', u'bar'])\n        \"\"\"\n        if level is not None and not is_list_like(level):\n            if not is_list_like(labels):\n                raise TypeError(\"Labels must be list-like\")\n            if is_list_like(labels[0]):\n                raise TypeError(\"Labels must be list-like\")\n            level = [level]\n            labels = [labels]\n        elif level is None or is_list_like(level):\n            if not is_list_like(labels) or not is_list_like(labels[0]):\n                raise TypeError(\"Labels must be list of lists-like\")\n\n        if inplace:\n            idx = self\n        else:\n            idx = self._shallow_copy()\n        idx._reset_identity()\n        idx._set_labels(labels, level=level, verify_integrity=verify_integrity)\n        if not inplace:\n            return idx\n\n    def copy(self, names=None, dtype=None, levels=None, labels=None,\n             deep=False, _set_identity=False, **kwargs):\n        \"\"\"\n        Make a copy of this object. Names, dtype, levels and labels can be\n        passed and will be set on new copy.\n\n        Parameters\n        ----------\n        names : sequence, optional\n        dtype : numpy dtype or pandas type, optional\n        levels : sequence, optional\n        labels : sequence, optional\n\n        Returns\n        -------\n        copy : MultiIndex\n\n        Notes\n        -----\n        In most cases, there should be no functional difference from using\n        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n        This could be potentially expensive on large MultiIndex objects.\n        \"\"\"\n        name = kwargs.get('name')\n        names = self._validate_names(name=name, names=names, deep=deep)\n\n        if deep:\n            from copy import deepcopy\n            if levels is None:\n                levels = deepcopy(self.levels)\n            if labels is None:\n                labels = deepcopy(self.labels)\n        else:\n            if levels is None:\n                levels = self.levels\n            if labels is None:\n                labels = self.labels\n        return MultiIndex(levels=levels, labels=labels, names=names,\n                          sortorder=self.sortorder, verify_integrity=False,\n                          _set_identity=_set_identity)\n\n    def __array__(self, dtype=None):\n        \"\"\" the array interface, return my values \"\"\"\n        return self.values\n\n    def view(self, cls=None):\n        \"\"\" this is defined as a copy with the same identity \"\"\"\n        result = self.copy()\n        result._id = self._id\n        return result\n\n    def _shallow_copy_with_infer(self, values, **kwargs):\n        # On equal MultiIndexes the difference is empty.\n        # Therefore, an empty MultiIndex is returned GH13490\n        if len(values) == 0:\n            return MultiIndex(levels=[[] for _ in range(self.nlevels)],\n                              labels=[[] for _ in range(self.nlevels)],\n                              **kwargs)\n        return self._shallow_copy(values, **kwargs)\n\n    @Appender(_index_shared_docs['__contains__'] % _index_doc_kwargs)\n    def __contains__(self, key):\n        hash(key)\n        try:\n            self.get_loc(key)\n            return True\n        except (LookupError, TypeError):\n            return False\n\n    contains = __contains__\n\n    @Appender(_index_shared_docs['_shallow_copy'])\n    def _shallow_copy(self, values=None, **kwargs):\n        if values is not None:\n            names = kwargs.pop('names', kwargs.pop('name', self.names))\n            # discards freq\n            kwargs.pop('freq', None)\n            return MultiIndex.from_tuples(values, names=names, **kwargs)\n        return self.view()\n\n    @cache_readonly\n    def dtype(self):\n        return np.dtype('O')\n\n    def _is_memory_usage_qualified(self):\n        \"\"\" return a boolean if we need a qualified .info display \"\"\"\n        def f(l):\n            return 'mixed' in l or 'string' in l or 'unicode' in l\n        return any(f(l) for l in self._inferred_type_levels)\n\n    @Appender(Index.memory_usage.__doc__)\n    def memory_usage(self, deep=False):\n        # we are overwriting our base class to avoid\n        # computing .values here which could materialize\n        # a tuple representation uncessarily\n        return self._nbytes(deep)\n\n    @cache_readonly\n    def nbytes(self):\n        \"\"\" return the number of bytes in the underlying data \"\"\"\n        return self._nbytes(False)\n\n    def _nbytes(self, deep=False):\n        \"\"\"\n        return the number of bytes in the underlying data\n        deeply introspect the level data if deep=True\n\n        include the engine hashtable\n\n        *this is in internal routine*\n\n        \"\"\"\n\n        # for implementations with no useful getsizeof (PyPy)\n        objsize = 24\n\n        level_nbytes = sum(i.memory_usage(deep=deep) for i in self.levels)\n        label_nbytes = sum(i.nbytes for i in self.labels)\n        names_nbytes = sum(getsizeof(i, objsize) for i in self.names)\n        result = level_nbytes + label_nbytes + names_nbytes\n\n        # include our engine hashtable\n        result += self._engine.sizeof(deep=deep)\n        return result\n\n    def _format_attrs(self):\n        \"\"\"\n        Return a list of tuples of the (attr,formatted_value)\n        \"\"\"\n        attrs = [\n            ('levels', ibase.default_pprint(self._levels,\n                                            max_seq_items=False)),\n            ('labels', ibase.default_pprint(self._labels,\n                                            max_seq_items=False))]\n        if com._any_not_none(*self.names):\n            attrs.append(('names', ibase.default_pprint(self.names)))\n        if self.sortorder is not None:\n            attrs.append(('sortorder', ibase.default_pprint(self.sortorder)))\n        return attrs\n\n    def _format_space(self):\n        return \"\\n%s\" % (' ' * (len(self.__class__.__name__) + 1))\n\n    def _format_data(self, name=None):\n        # we are formatting thru the attributes\n        return None\n\n    def __len__(self):\n        return len(self.labels[0])\n\n    def _get_names(self):\n        return FrozenList(level.name for level in self.levels)\n\n    def _set_names(self, names, level=None, validate=True):\n        \"\"\"\n        Set new names on index. Each name has to be a hashable type.\n\n        Parameters\n        ----------\n        values : str or sequence\n            name(s) to set\n        level : int, level name, or sequence of int/level names (default None)\n            If the index is a MultiIndex (hierarchical), level(s) to set (None\n            for all levels).  Otherwise level must be None\n        validate : boolean, default True\n            validate that the names match level lengths\n\n        Raises\n        ------\n        TypeError if each name is not hashable.\n\n        Notes\n        -----\n        sets names on levels. WARNING: mutates!\n\n        Note that you generally want to set this *after* changing levels, so\n        that it only acts on copies\n        \"\"\"\n        # GH 15110\n        # Don't allow a single string for names in a MultiIndex\n        if names is not None and not is_list_like(names):\n            raise ValueError('Names should be list-like for a MultiIndex')\n        names = list(names)\n\n        if validate and level is not None and len(names) != len(level):\n            raise ValueError('Length of names must match length of level.')\n        if validate and level is None and len(names) != self.nlevels:\n            raise ValueError('Length of names must match number of levels in '\n                             'MultiIndex.')\n\n        if level is None:\n            level = range(self.nlevels)\n        else:\n            level = [self._get_level_number(l) for l in level]\n\n        # set the name\n        for l, name in zip(level, names):\n            if name is not None:\n                # GH 20527\n                # All items in 'names' need to be hashable:\n                if not is_hashable(name):\n                    raise TypeError('{}.name must be a hashable type'\n                                    .format(self.__class__.__name__))\n            self.levels[l].rename(name, inplace=True)\n\n    names = property(fset=_set_names, fget=_get_names,\n                     doc=\"Names of levels in MultiIndex\")\n\n    def _format_native_types(self, na_rep='nan', **kwargs):\n        new_levels = []\n        new_labels = []\n\n        # go through the levels and format them\n        for level, label in zip(self.levels, self.labels):\n            level = level._format_native_types(na_rep=na_rep, **kwargs)\n            # add nan values, if there are any\n            mask = (label == -1)\n            if mask.any():\n                nan_index = len(level)\n                level = np.append(level, na_rep)\n                label = label.values()\n                label[mask] = nan_index\n            new_levels.append(level)\n            new_labels.append(label)\n\n        if len(new_levels) == 1:\n            return Index(new_levels[0])._format_native_types()\n        else:\n            # reconstruct the multi-index\n            mi = MultiIndex(levels=new_levels, labels=new_labels,\n                            names=self.names, sortorder=self.sortorder,\n                            verify_integrity=False)\n            return mi.values\n\n    @Appender(_index_shared_docs['_get_grouper_for_level'])\n    def _get_grouper_for_level(self, mapper, level):\n        indexer = self.labels[level]\n        level_index = self.levels[level]\n\n        if mapper is not None:\n            # Handle group mapping function and return\n            level_values = self.levels[level].take(indexer)\n            grouper = level_values.map(mapper)\n            return grouper, None, None\n\n        labels, uniques = algos.factorize(indexer, sort=True)\n\n        if len(uniques) > 0 and uniques[0] == -1:\n            # Handle NAs\n            mask = indexer != -1\n            ok_labels, uniques = algos.factorize(indexer[mask],\n                                                 sort=True)\n\n            labels = np.empty(len(indexer), dtype=indexer.dtype)\n            labels[mask] = ok_labels\n            labels[~mask] = -1\n\n        if len(uniques) < len(level_index):\n            # Remove unobserved levels from level_index\n            level_index = level_index.take(uniques)\n\n        grouper = level_index.take(labels)\n\n        return grouper, labels, level_index\n\n    @property\n    def _constructor(self):\n        return MultiIndex.from_tuples\n\n    @cache_readonly\n    def inferred_type(self):\n        return 'mixed'\n\n    def _get_level_number(self, level):\n        count = self.names.count(level)\n        if (count > 1) and not is_integer(level):\n            raise ValueError('The name %s occurs multiple times, use a '\n                             'level number' % level)\n        try:\n            level = self.names.index(level)\n        except ValueError:\n            if not is_integer(level):\n                raise KeyError('Level %s not found' % str(level))\n            elif level < 0:\n                level += self.nlevels\n                if level < 0:\n                    orig_level = level - self.nlevels\n                    raise IndexError('Too many levels: Index has only %d '\n                                     'levels, %d is not a valid level number' %\n                                     (self.nlevels, orig_level))\n            # Note: levels are zero-based\n            elif level >= self.nlevels:\n                raise IndexError('Too many levels: Index has only %d levels, '\n                                 'not %d' % (self.nlevels, level + 1))\n        return level\n\n    _tuples = None\n\n    @cache_readonly\n    def _engine(self):\n        # Calculate the number of bits needed to represent labels in each\n        # level, as log2 of their sizes (including -1 for NaN):\n        sizes = np.ceil(np.log2([len(l) + 1 for l in self.levels]))\n\n        # Sum bit counts, starting from the _right_....\n        lev_bits = np.cumsum(sizes[::-1])[::-1]\n\n        # ... in order to obtain offsets such that sorting the combination of\n        # shifted codes (one for each level, resulting in a unique integer) is\n        # equivalent to sorting lexicographically the codes themselves. Notice\n        # that each level needs to be shifted by the number of bits needed to\n        # represent the _previous_ ones:\n        offsets = np.concatenate([lev_bits[1:], [0]]).astype('uint64')\n\n        # Check the total number of bits needed for our representation:\n        if lev_bits[0] > 64:\n            # The levels would overflow a 64 bit uint - use Python integers:\n            return MultiIndexPyIntEngine(self.levels, self.labels, offsets)\n        return MultiIndexUIntEngine(self.levels, self.labels, offsets)\n\n    @property\n    def values(self):\n        if self._tuples is not None:\n            return self._tuples\n\n        values = []\n\n        for i in range(self.nlevels):\n            vals = self._get_level_values(i)\n            if is_categorical_dtype(vals):\n                vals = vals.get_values()\n            if (isinstance(vals.dtype, (PandasExtensionDtype, ExtensionDtype))\n                    or hasattr(vals, '_box_values')):\n                vals = vals.astype(object)\n            vals = np.array(vals, copy=False)\n            values.append(vals)\n\n        self._tuples = lib.fast_zip(values)\n        return self._tuples\n\n    @property\n    def _has_complex_internals(self):\n        # to disable groupby tricks\n        return True\n\n    @cache_readonly\n    def is_monotonic_increasing(self):\n        \"\"\"\n        return if the index is monotonic increasing (only equal or\n        increasing) values.\n        \"\"\"\n\n        # reversed() because lexsort() wants the most significant key last.\n        values = [self._get_level_values(i).values\n                  for i in reversed(range(len(self.levels)))]\n        try:\n            sort_order = np.lexsort(values)\n            return Index(sort_order).is_monotonic\n        except TypeError:\n\n            # we have mixed types and np.lexsort is not happy\n            return Index(self.values).is_monotonic\n\n    @cache_readonly\n    def is_monotonic_decreasing(self):\n        \"\"\"\n        return if the index is monotonic decreasing (only equal or\n        decreasing) values.\n        \"\"\"\n        # monotonic decreasing if and only if reverse is monotonic increasing\n        return self[::-1].is_monotonic_increasing\n\n    @cache_readonly\n    def _have_mixed_levels(self):\n        \"\"\" return a boolean list indicated if we have mixed levels \"\"\"\n        return ['mixed' in l for l in self._inferred_type_levels]\n\n    @cache_readonly\n    def _inferred_type_levels(self):\n        \"\"\" return a list of the inferred types, one for each level \"\"\"\n        return [i.inferred_type for i in self.levels]\n\n    @cache_readonly\n    def _hashed_values(self):\n        \"\"\" return a uint64 ndarray of my hashed values \"\"\"\n        from pandas.core.util.hashing import hash_tuples\n        return hash_tuples(self)\n\n    def _hashed_indexing_key(self, key):\n        \"\"\"\n        validate and return the hash for the provided key\n\n        *this is internal for use for the cython routines*\n\n        Parameters\n        ----------\n        key : string or tuple\n\n        Returns\n        -------\n        np.uint64\n\n        Notes\n        -----\n        we need to stringify if we have mixed levels\n\n        \"\"\"\n        from pandas.core.util.hashing import hash_tuples, hash_tuple\n\n        if not isinstance(key, tuple):\n            return hash_tuples(key)\n\n        if not len(key) == self.nlevels:\n            raise KeyError\n\n        def f(k, stringify):\n            if stringify and not isinstance(k, compat.string_types):\n                k = str(k)\n            return k\n        key = tuple(f(k, stringify)\n                    for k, stringify in zip(key, self._have_mixed_levels))\n        return hash_tuple(key)\n\n    @Appender(Index.duplicated.__doc__)\n    def duplicated(self, keep='first'):\n        from pandas.core.sorting import get_group_index\n        from pandas._libs.hashtable import duplicated_int64\n\n        shape = map(len, self.levels)\n        ids = get_group_index(self.labels, shape, sort=False, xnull=False)\n\n        return duplicated_int64(ids, keep)\n\n    def fillna(self, value=None, downcast=None):\n        \"\"\"\n        fillna is not implemented for MultiIndex\n        \"\"\"\n        raise NotImplementedError('isna is not defined for MultiIndex')\n\n    @Appender(_index_shared_docs['dropna'])\n    def dropna(self, how='any'):\n        nans = [label == -1 for label in self.labels]\n        if how == 'any':\n            indexer = np.any(nans, axis=0)\n        elif how == 'all':\n            indexer = np.all(nans, axis=0)\n        else:\n            raise ValueError(\"invalid how option: {0}\".format(how))\n\n        new_labels = [label[~indexer] for label in self.labels]\n        return self.copy(labels=new_labels, deep=True)\n\n    def get_value(self, series, key):\n        # somewhat broken encapsulation\n        from pandas.core.indexing import maybe_droplevels\n\n        # Label-based\n        s = com.values_from_object(series)\n        k = com.values_from_object(key)\n\n        def _try_mi(k):\n            # TODO: what if a level contains tuples??\n            loc = self.get_loc(k)\n            new_values = series._values[loc]\n            new_index = self[loc]\n            new_index = maybe_droplevels(new_index, k)\n            return series._constructor(new_values, index=new_index,\n                                       name=series.name).__finalize__(self)\n\n        try:\n            return self._engine.get_value(s, k)\n        except KeyError as e1:\n            try:\n                return _try_mi(key)\n            except KeyError:\n                pass\n\n            try:\n                return libindex.get_value_at(s, k)\n            except IndexError:\n                raise\n            except TypeError:\n                # generator/iterator-like\n                if is_iterator(key):\n                    raise InvalidIndexError(key)\n                else:\n                    raise e1\n            except Exception:  # pragma: no cover\n                raise e1\n        except TypeError:\n\n            # a Timestamp will raise a TypeError in a multi-index\n            # rather than a KeyError, try it here\n            # note that a string that 'looks' like a Timestamp will raise\n            # a KeyError! (GH5725)\n            if (isinstance(key, (datetime.datetime, np.datetime64)) or\n                    (compat.PY3 and isinstance(key, compat.string_types))):\n                try:\n                    return _try_mi(key)\n                except KeyError:\n                    raise\n                except (IndexError, ValueError, TypeError):\n                    pass\n\n                try:\n                    return _try_mi(Timestamp(key))\n                except (KeyError, TypeError,\n                        IndexError, ValueError, tslibs.OutOfBoundsDatetime):\n                    pass\n\n            raise InvalidIndexError(key)\n\n    def _get_level_values(self, level, unique=False):\n        \"\"\"\n        Return vector of label values for requested level,\n        equal to the length of the index\n\n        **this is an internal method**\n\n        Parameters\n        ----------\n        level : int level\n        unique : bool, default False\n            if True, drop duplicated values\n\n        Returns\n        -------\n        values : ndarray\n        \"\"\"\n\n        values = self.levels[level]\n        labels = self.labels[level]\n        if unique:\n            labels = algos.unique(labels)\n        filled = algos.take_1d(values._values, labels,\n                               fill_value=values._na_value)\n        values = values._shallow_copy(filled)\n        return values\n\n    def get_level_values(self, level):\n        \"\"\"\n        Return vector of label values for requested level,\n        equal to the length of the index.\n\n        Parameters\n        ----------\n        level : int or str\n            ``level`` is either the integer position of the level in the\n            MultiIndex, or the name of the level.\n\n        Returns\n        -------\n        values : Index\n            ``values`` is a level of this MultiIndex converted to\n            a single :class:`Index` (or subclass thereof).\n\n        Examples\n        ---------\n\n        Create a MultiIndex:\n\n        >>> mi = pd.MultiIndex.from_arrays((list('abc'), list('def')))\n        >>> mi.names = ['level_1', 'level_2']\n\n        Get level values by supplying level as either integer or name:\n\n        >>> mi.get_level_values(0)\n        Index(['a', 'b', 'c'], dtype='object', name='level_1')\n        >>> mi.get_level_values('level_2')\n        Index(['d', 'e', 'f'], dtype='object', name='level_2')\n        \"\"\"\n        level = self._get_level_number(level)\n        values = self._get_level_values(level)\n        return values\n\n    @Appender(_index_shared_docs['index_unique'] % _index_doc_kwargs)\n    def unique(self, level=None):\n\n        if level is None:\n            return super(MultiIndex, self).unique()\n        else:\n            level = self._get_level_number(level)\n            return self._get_level_values(level=level, unique=True)\n\n    def format(self, space=2, sparsify=None, adjoin=True, names=False,\n               na_rep=None, formatter=None):\n        if len(self) == 0:\n            return []\n\n        stringified_levels = []\n        for lev, lab in zip(self.levels, self.labels):\n            na = na_rep if na_rep is not None else _get_na_rep(lev.dtype.type)\n\n            if len(lev) > 0:\n\n                formatted = lev.take(lab).format(formatter=formatter)\n\n                # we have some NA\n                mask = lab == -1\n                if mask.any():\n                    formatted = np.array(formatted, dtype=object)\n                    formatted[mask] = na\n                    formatted = formatted.tolist()\n\n            else:\n                # weird all NA case\n                formatted = [pprint_thing(na if isna(x) else x,\n                                          escape_chars=('\\t', '\\r', '\\n'))\n                             for x in algos.take_1d(lev._values, lab)]\n            stringified_levels.append(formatted)\n\n        result_levels = []\n        for lev, name in zip(stringified_levels, self.names):\n            level = []\n\n            if names:\n                level.append(pprint_thing(name,\n                                          escape_chars=('\\t', '\\r', '\\n'))\n                             if name is not None else '')\n\n            level.extend(np.array(lev, dtype=object))\n            result_levels.append(level)\n\n        if sparsify is None:\n            sparsify = get_option(\"display.multi_sparse\")\n\n        if sparsify:\n            sentinel = ''\n            # GH3547\n            # use value of sparsify as sentinel,  unless it's an obvious\n            # \"Truthey\" value\n            if sparsify not in [True, 1]:\n                sentinel = sparsify\n            # little bit of a kludge job for #1217\n            result_levels = _sparsify(result_levels, start=int(names),\n                                      sentinel=sentinel)\n\n        if adjoin:\n            from pandas.io.formats.format import _get_adjustment\n            adj = _get_adjustment()\n            return adj.adjoin(space, *result_levels).split('\\n')\n        else:\n            return result_levels\n\n    def _to_safe_for_reshape(self):\n        \"\"\" convert to object if we are a categorical \"\"\"\n        return self.set_levels([i._to_safe_for_reshape() for i in self.levels])\n\n    def to_frame(self, index=True, name=None):\n        \"\"\"\n        Create a DataFrame with the levels of the MultiIndex as columns.\n\n        Column ordering is determined by the DataFrame constructor with data as\n        a dict.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        index : boolean, default True\n            Set the index of the returned DataFrame as the original MultiIndex.\n\n        name : list / sequence of strings, optional\n            The passed names should substitute index level names.\n\n        Returns\n        -------\n        DataFrame : a DataFrame containing the original MultiIndex data.\n\n        See Also\n        --------\n        DataFrame\n        \"\"\"\n\n        from pandas import DataFrame\n        if name is not None:\n            if not is_list_like(name):\n                raise TypeError(\"'name' must be a list / sequence \"\n                                \"of column names.\")\n\n            if len(name) != len(self.levels):\n                raise ValueError(\"'name' should have same length as \"\n                                 \"number of levels on index.\")\n            idx_names = name\n        else:\n            idx_names = self.names\n\n        result = DataFrame({(name or level):\n                            self._get_level_values(level)\n                            for name, level in\n                            zip(idx_names, range(len(self.levels)))},\n                           copy=False)\n        if index:\n            result.index = self\n        return result\n\n    def to_hierarchical(self, n_repeat, n_shuffle=1):\n        \"\"\"\n        .. deprecated:: 0.24.0\n\n        Return a MultiIndex reshaped to conform to the\n        shapes given by n_repeat and n_shuffle.\n\n        Useful to replicate and rearrange a MultiIndex for combination\n        with another Index with n_repeat items.\n\n        Parameters\n        ----------\n        n_repeat : int\n            Number of times to repeat the labels on self\n        n_shuffle : int\n            Controls the reordering of the labels. If the result is going\n            to be an inner level in a MultiIndex, n_shuffle will need to be\n            greater than one. The size of each label must divisible by\n            n_shuffle.\n\n        Returns\n        -------\n        MultiIndex\n\n        Examples\n        --------\n        >>> idx = pd.MultiIndex.from_tuples([(1, u'one'), (1, u'two'),\n                                            (2, u'one'), (2, u'two')])\n        >>> idx.to_hierarchical(3)\n        MultiIndex(levels=[[1, 2], [u'one', u'two']],\n                   labels=[[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n                           [0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1]])\n        \"\"\"\n        levels = self.levels\n        labels = [np.repeat(x, n_repeat) for x in self.labels]\n        # Assumes that each label is divisible by n_shuffle\n        labels = [x.reshape(n_shuffle, -1).ravel(order='F') for x in labels]\n        names = self.names\n        warnings.warn(\"Method .to_hierarchical is deprecated and will \"\n                      \"be removed in a future version\",\n                      FutureWarning, stacklevel=2)\n        return MultiIndex(levels=levels, labels=labels, names=names)\n\n    def to_flat_index(self):\n        \"\"\"\n        Convert a MultiIndex to an Index of Tuples containing the level values.\n\n        .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        pd.Index\n            Index with the MultiIndex data represented in Tuples.\n\n        Notes\n        -----\n        This method will simply return the caller if called by anything other\n        than a MultiIndex.\n\n        Examples\n        --------\n        >>> index = pd.MultiIndex.from_product(\n        ...     [['foo', 'bar'], ['baz', 'qux']],\n        ...     names=['a', 'b'])\n        >>> index.to_flat_index()\n        Index([('foo', 'baz'), ('foo', 'qux'),\n               ('bar', 'baz'), ('bar', 'qux')],\n              dtype='object')\n        \"\"\"\n        return Index(self.values, tupleize_cols=False)\n\n    @property\n    def is_all_dates(self):\n        return False\n\n    def is_lexsorted(self):\n        \"\"\"\n        Return True if the labels are lexicographically sorted\n        \"\"\"\n        return self.lexsort_depth == self.nlevels\n\n    @cache_readonly\n    def lexsort_depth(self):\n        if self.sortorder is not None:\n            if self.sortorder == 0:\n                return self.nlevels\n            else:\n                return 0\n\n        int64_labels = [ensure_int64(lab) for lab in self.labels]\n        for k in range(self.nlevels, 0, -1):\n            if libalgos.is_lexsorted(int64_labels[:k]):\n                return k\n\n        return 0\n\n    @classmethod\n    def from_arrays(cls, arrays, sortorder=None, names=None):\n        \"\"\"\n        Convert arrays to MultiIndex\n\n        Parameters\n        ----------\n        arrays : list / sequence of array-likes\n            Each array-like gives one level's value for each data point.\n            len(arrays) is the number of levels.\n        sortorder : int or None\n            Level of sortedness (must be lexicographically sorted by that\n            level)\n\n        Returns\n        -------\n        index : MultiIndex\n\n        Examples\n        --------\n        >>> arrays = [[1, 1, 2, 2], ['red', 'blue', 'red', 'blue']]\n        >>> pd.MultiIndex.from_arrays(arrays, names=('number', 'color'))\n\n        See Also\n        --------\n        MultiIndex.from_tuples : Convert list of tuples to MultiIndex.\n        MultiIndex.from_product : Make a MultiIndex from cartesian product\n                                  of iterables.\n        \"\"\"\n        if not is_list_like(arrays):\n            raise TypeError(\"Input must be a list / sequence of array-likes.\")\n        elif is_iterator(arrays):\n            arrays = list(arrays)\n\n        # Check if lengths of all arrays are equal or not,\n        # raise ValueError, if not\n        for i in range(1, len(arrays)):\n            if len(arrays[i]) != len(arrays[i - 1]):\n                raise ValueError('all arrays must be same length')\n\n        from pandas.core.arrays.categorical import _factorize_from_iterables\n\n        labels, levels = _factorize_from_iterables(arrays)\n        if names is None:\n            names = [getattr(arr, \"name\", None) for arr in arrays]\n\n        return MultiIndex(levels=levels, labels=labels, sortorder=sortorder,\n                          names=names, verify_integrity=False)\n\n    @classmethod\n    def from_tuples(cls, tuples, sortorder=None, names=None):\n        \"\"\"\n        Convert list of tuples to MultiIndex\n\n        Parameters\n        ----------\n        tuples : list / sequence of tuple-likes\n            Each tuple is the index of one row/column.\n        sortorder : int or None\n            Level of sortedness (must be lexicographically sorted by that\n            level)\n\n        Returns\n        -------\n        index : MultiIndex\n\n        Examples\n        --------\n        >>> tuples = [(1, u'red'), (1, u'blue'),\n                      (2, u'red'), (2, u'blue')]\n        >>> pd.MultiIndex.from_tuples(tuples, names=('number', 'color'))\n\n        See Also\n        --------\n        MultiIndex.from_arrays : Convert list of arrays to MultiIndex\n        MultiIndex.from_product : Make a MultiIndex from cartesian product\n                                  of iterables\n        \"\"\"\n        if not is_list_like(tuples):\n            raise TypeError('Input must be a list / sequence of tuple-likes.')\n        elif is_iterator(tuples):\n            tuples = list(tuples)\n\n        if len(tuples) == 0:\n            if names is None:\n                msg = 'Cannot infer number of levels from empty list'\n                raise TypeError(msg)\n            arrays = [[]] * len(names)\n        elif isinstance(tuples, (np.ndarray, Index)):\n            if isinstance(tuples, Index):\n                tuples = tuples._values\n\n            arrays = list(lib.tuples_to_object_array(tuples).T)\n        elif isinstance(tuples, list):\n            arrays = list(lib.to_object_array_tuples(tuples).T)\n        else:\n            arrays = lzip(*tuples)\n\n        return MultiIndex.from_arrays(arrays, sortorder=sortorder, names=names)\n\n    @classmethod\n    def from_product(cls, iterables, sortorder=None, names=None):\n        \"\"\"\n        Make a MultiIndex from the cartesian product of multiple iterables\n\n        Parameters\n        ----------\n        iterables : list / sequence of iterables\n            Each iterable has unique labels for each level of the index.\n        sortorder : int or None\n            Level of sortedness (must be lexicographically sorted by that\n            level).\n        names : list / sequence of strings or None\n            Names for the levels in the index.\n\n        Returns\n        -------\n        index : MultiIndex\n\n        Examples\n        --------\n        >>> numbers = [0, 1, 2]\n        >>> colors = [u'green', u'purple']\n        >>> pd.MultiIndex.from_product([numbers, colors],\n                                       names=['number', 'color'])\n        MultiIndex(levels=[[0, 1, 2], [u'green', u'purple']],\n                   labels=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]],\n                   names=[u'number', u'color'])\n\n        See Also\n        --------\n        MultiIndex.from_arrays : Convert list of arrays to MultiIndex.\n        MultiIndex.from_tuples : Convert list of tuples to MultiIndex.\n        \"\"\"\n        from pandas.core.arrays.categorical import _factorize_from_iterables\n        from pandas.core.reshape.util import cartesian_product\n\n        if not is_list_like(iterables):\n            raise TypeError(\"Input must be a list / sequence of iterables.\")\n        elif is_iterator(iterables):\n            iterables = list(iterables)\n\n        labels, levels = _factorize_from_iterables(iterables)\n        labels = cartesian_product(labels)\n        return MultiIndex(levels, labels, sortorder=sortorder, names=names)\n\n    def _sort_levels_monotonic(self):\n        \"\"\"\n        .. versionadded:: 0.20.0\n\n        This is an *internal* function.\n\n        Create a new MultiIndex from the current to monotonically sorted\n        items IN the levels. This does not actually make the entire MultiIndex\n        monotonic, JUST the levels.\n\n        The resulting MultiIndex will have the same outward\n        appearance, meaning the same .values and ordering. It will also\n        be .equals() to the original.\n\n        Returns\n        -------\n        MultiIndex\n\n        Examples\n        --------\n\n        >>> i = pd.MultiIndex(levels=[['a', 'b'], ['bb', 'aa']],\n                              labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n        >>> i\n        MultiIndex(levels=[['a', 'b'], ['bb', 'aa']],\n                   labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n\n        >>> i.sort_monotonic()\n        MultiIndex(levels=[['a', 'b'], ['aa', 'bb']],\n                   labels=[[0, 0, 1, 1], [1, 0, 1, 0]])\n\n        \"\"\"\n\n        if self.is_lexsorted() and self.is_monotonic:\n            return self\n\n        new_levels = []\n        new_labels = []\n\n        for lev, lab in zip(self.levels, self.labels):\n\n            if not lev.is_monotonic:\n                try:\n                    # indexer to reorder the levels\n                    indexer = lev.argsort()\n                except TypeError:\n                    pass\n                else:\n                    lev = lev.take(indexer)\n\n                    # indexer to reorder the labels\n                    indexer = ensure_int64(indexer)\n                    ri = lib.get_reverse_indexer(indexer, len(indexer))\n                    lab = algos.take_1d(ri, lab)\n\n            new_levels.append(lev)\n            new_labels.append(lab)\n\n        return MultiIndex(new_levels, new_labels,\n                          names=self.names, sortorder=self.sortorder,\n                          verify_integrity=False)\n\n    def remove_unused_levels(self):\n        \"\"\"\n        Create a new MultiIndex from the current that removes\n        unused levels, meaning that they are not expressed in the labels.\n\n        The resulting MultiIndex will have the same outward\n        appearance, meaning the same .values and ordering. It will also\n        be .equals() to the original.\n\n        .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        MultiIndex\n\n        Examples\n        --------\n        >>> i = pd.MultiIndex.from_product([range(2), list('ab')])\n        MultiIndex(levels=[[0, 1], ['a', 'b']],\n                   labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n\n        >>> i[2:]\n        MultiIndex(levels=[[0, 1], ['a', 'b']],\n                   labels=[[1, 1], [0, 1]])\n\n        The 0 from the first level is not represented\n        and can be removed\n\n        >>> i[2:].remove_unused_levels()\n        MultiIndex(levels=[[1], ['a', 'b']],\n                   labels=[[0, 0], [0, 1]])\n        \"\"\"\n\n        new_levels = []\n        new_labels = []\n\n        changed = False\n        for lev, lab in zip(self.levels, self.labels):\n\n            # Since few levels are typically unused, bincount() is more\n            # efficient than unique() - however it only accepts positive values\n            # (and drops order):\n            uniques = np.where(np.bincount(lab + 1) > 0)[0] - 1\n            has_na = int(len(uniques) and (uniques[0] == -1))\n\n            if len(uniques) != len(lev) + has_na:\n                # We have unused levels\n                changed = True\n\n                # Recalculate uniques, now preserving order.\n                # Can easily be cythonized by exploiting the already existing\n                # \"uniques\" and stop parsing \"lab\" when all items are found:\n                uniques = algos.unique(lab)\n                if has_na:\n                    na_idx = np.where(uniques == -1)[0]\n                    # Just ensure that -1 is in first position:\n                    uniques[[0, na_idx[0]]] = uniques[[na_idx[0], 0]]\n\n                # labels get mapped from uniques to 0:len(uniques)\n                # -1 (if present) is mapped to last position\n                label_mapping = np.zeros(len(lev) + has_na)\n                # ... and reassigned value -1:\n                label_mapping[uniques] = np.arange(len(uniques)) - has_na\n\n                lab = label_mapping[lab]\n\n                # new levels are simple\n                lev = lev.take(uniques[has_na:])\n\n            new_levels.append(lev)\n            new_labels.append(lab)\n\n        result = self._shallow_copy()\n\n        if changed:\n            result._reset_identity()\n            result._set_levels(new_levels, validate=False)\n            result._set_labels(new_labels, validate=False)\n\n        return result\n\n    @property\n    def nlevels(self):\n        \"\"\"Integer number of levels in this MultiIndex.\"\"\"\n        return len(self.levels)\n\n    @property\n    def levshape(self):\n        \"\"\"A tuple with the length of each level.\"\"\"\n        return tuple(len(x) for x in self.levels)\n\n    def __reduce__(self):\n        \"\"\"Necessary for making this object picklable\"\"\"\n        d = dict(levels=[lev for lev in self.levels],\n                 labels=[label for label in self.labels],\n                 sortorder=self.sortorder, names=list(self.names))\n        return ibase._new_Index, (self.__class__, d), None\n\n    def __setstate__(self, state):\n        \"\"\"Necessary for making this object picklable\"\"\"\n\n        if isinstance(state, dict):\n            levels = state.get('levels')\n            labels = state.get('labels')\n            sortorder = state.get('sortorder')\n            names = state.get('names')\n\n        elif isinstance(state, tuple):\n\n            nd_state, own_state = state\n            levels, labels, sortorder, names = own_state\n\n        self._set_levels([Index(x) for x in levels], validate=False)\n        self._set_labels(labels)\n        self._set_names(names)\n        self.sortorder = sortorder\n        self._verify_integrity()\n        self._reset_identity()\n\n    def __getitem__(self, key):\n        if is_scalar(key):\n            key = com.cast_scalar_indexer(key)\n\n            retval = []\n            for lev, lab in zip(self.levels, self.labels):\n                if lab[key] == -1:\n                    retval.append(np.nan)\n                else:\n                    retval.append(lev[lab[key]])\n\n            return tuple(retval)\n        else:\n            if com.is_bool_indexer(key):\n                key = np.asarray(key, dtype=bool)\n                sortorder = self.sortorder\n            else:\n                # cannot be sure whether the result will be sorted\n                sortorder = None\n\n                if isinstance(key, Index):\n                    key = np.asarray(key)\n\n            new_labels = [lab[key] for lab in self.labels]\n\n            return MultiIndex(levels=self.levels, labels=new_labels,\n                              names=self.names, sortorder=sortorder,\n                              verify_integrity=False)\n\n    @Appender(_index_shared_docs['take'] % _index_doc_kwargs)\n    def take(self, indices, axis=0, allow_fill=True,\n             fill_value=None, **kwargs):\n        nv.validate_take(tuple(), kwargs)\n        indices = ensure_platform_int(indices)\n        taken = self._assert_take_fillable(self.labels, indices,\n                                           allow_fill=allow_fill,\n                                           fill_value=fill_value,\n                                           na_value=-1)\n        return MultiIndex(levels=self.levels, labels=taken,\n                          names=self.names, verify_integrity=False)\n\n    def _assert_take_fillable(self, values, indices, allow_fill=True,\n                              fill_value=None, na_value=None):\n        \"\"\" Internal method to handle NA filling of take \"\"\"\n        # only fill if we are passing a non-None fill_value\n        if allow_fill and fill_value is not None:\n            if (indices < -1).any():\n                msg = ('When allow_fill=True and fill_value is not None, '\n                       'all indices must be >= -1')\n                raise ValueError(msg)\n            taken = [lab.take(indices) for lab in self.labels]\n            mask = indices == -1\n            if mask.any():\n                masked = []\n                for new_label in taken:\n                    label_values = new_label.values()\n                    label_values[mask] = na_value\n                    masked.append(np.asarray(label_values))\n                taken = masked\n        else:\n            taken = [lab.take(indices) for lab in self.labels]\n        return taken\n\n    def append(self, other):\n        \"\"\"\n        Append a collection of Index options together\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        appended : Index\n        \"\"\"\n        if not isinstance(other, (list, tuple)):\n            other = [other]\n\n        if all((isinstance(o, MultiIndex) and o.nlevels >= self.nlevels)\n               for o in other):\n            arrays = []\n            for i in range(self.nlevels):\n                label = self._get_level_values(i)\n                appended = [o._get_level_values(i) for o in other]\n                arrays.append(label.append(appended))\n            return MultiIndex.from_arrays(arrays, names=self.names)\n\n        to_concat = (self.values, ) + tuple(k._values for k in other)\n        new_tuples = np.concatenate(to_concat)\n\n        # if all(isinstance(x, MultiIndex) for x in other):\n        try:\n            return MultiIndex.from_tuples(new_tuples, names=self.names)\n        except (TypeError, IndexError):\n            return Index(new_tuples)\n\n    def argsort(self, *args, **kwargs):\n        return self.values.argsort(*args, **kwargs)\n\n    def repeat(self, repeats, *args, **kwargs):\n        nv.validate_repeat(args, kwargs)\n        return MultiIndex(levels=self.levels,\n                          labels=[label.view(np.ndarray).repeat(repeats)\n                                  for label in self.labels], names=self.names,\n                          sortorder=self.sortorder, verify_integrity=False)\n\n    def where(self, cond, other=None):\n        raise NotImplementedError(\".where is not supported for \"\n                                  \"MultiIndex operations\")\n\n    def drop(self, labels, level=None, errors='raise'):\n        \"\"\"\n        Make new MultiIndex with passed list of labels deleted\n\n        Parameters\n        ----------\n        labels : array-like\n            Must be a list of tuples\n        level : int or level name, default None\n\n        Returns\n        -------\n        dropped : MultiIndex\n        \"\"\"\n        if level is not None:\n            return self._drop_from_level(labels, level)\n\n        try:\n            if not isinstance(labels, (np.ndarray, Index)):\n                labels = com.index_labels_to_array(labels)\n            indexer = self.get_indexer(labels)\n            mask = indexer == -1\n            if mask.any():\n                if errors != 'ignore':\n                    raise ValueError('labels %s not contained in axis' %\n                                     labels[mask])\n        except Exception:\n            pass\n\n        inds = []\n        for label in labels:\n            try:\n                loc = self.get_loc(label)\n                # get_loc returns either an integer, a slice, or a boolean\n                # mask\n                if isinstance(loc, int):\n                    inds.append(loc)\n                elif isinstance(loc, slice):\n                    inds.extend(lrange(loc.start, loc.stop))\n                elif com.is_bool_indexer(loc):\n                    if self.lexsort_depth == 0:\n                        warnings.warn('dropping on a non-lexsorted multi-index'\n                                      ' without a level parameter may impact '\n                                      'performance.',\n                                      PerformanceWarning,\n                                      stacklevel=3)\n                    loc = loc.nonzero()[0]\n                    inds.extend(loc)\n                else:\n                    msg = 'unsupported indexer of type {}'.format(type(loc))\n                    raise AssertionError(msg)\n            except KeyError:\n                if errors != 'ignore':\n                    raise\n\n        return self.delete(inds)\n\n    def _drop_from_level(self, labels, level):\n        labels = com.index_labels_to_array(labels)\n        i = self._get_level_number(level)\n        index = self.levels[i]\n        values = index.get_indexer(labels)\n\n        mask = ~algos.isin(self.labels[i], values)\n\n        return self[mask]\n\n    def swaplevel(self, i=-2, j=-1):\n        \"\"\"\n        Swap level i with level j.\n\n        Calling this method does not change the ordering of the values.\n\n        Parameters\n        ----------\n        i : int, str, default -2\n            First level of index to be swapped. Can pass level name as string.\n            Type of parameters can be mixed.\n        j : int, str, default -1\n            Second level of index to be swapped. Can pass level name as string.\n            Type of parameters can be mixed.\n\n        Returns\n        -------\n        MultiIndex\n            A new MultiIndex\n\n        .. versionchanged:: 0.18.1\n\n           The indexes ``i`` and ``j`` are now optional, and default to\n           the two innermost levels of the index.\n\n        See Also\n        --------\n        Series.swaplevel : Swap levels i and j in a MultiIndex.\n        Dataframe.swaplevel : Swap levels i and j in a MultiIndex on a\n            particular axis.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex(levels=[['a', 'b'], ['bb', 'aa']],\n        ...                    labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n        >>> mi\n        MultiIndex(levels=[['a', 'b'], ['bb', 'aa']],\n           labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n        >>> mi.swaplevel(0, 1)\n        MultiIndex(levels=[['bb', 'aa'], ['a', 'b']],\n           labels=[[0, 1, 0, 1], [0, 0, 1, 1]])\n        \"\"\"\n        new_levels = list(self.levels)\n        new_labels = list(self.labels)\n        new_names = list(self.names)\n\n        i = self._get_level_number(i)\n        j = self._get_level_number(j)\n\n        new_levels[i], new_levels[j] = new_levels[j], new_levels[i]\n        new_labels[i], new_labels[j] = new_labels[j], new_labels[i]\n        new_names[i], new_names[j] = new_names[j], new_names[i]\n\n        return MultiIndex(levels=new_levels, labels=new_labels,\n                          names=new_names, verify_integrity=False)\n\n    def reorder_levels(self, order):\n        \"\"\"\n        Rearrange levels using input order. May not drop or duplicate levels\n\n        Parameters\n        ----------\n        \"\"\"\n        order = [self._get_level_number(i) for i in order]\n        if len(order) != self.nlevels:\n            raise AssertionError('Length of order must be same as '\n                                 'number of levels (%d), got %d' %\n                                 (self.nlevels, len(order)))\n        new_levels = [self.levels[i] for i in order]\n        new_labels = [self.labels[i] for i in order]\n        new_names = [self.names[i] for i in order]\n\n        return MultiIndex(levels=new_levels, labels=new_labels,\n                          names=new_names, verify_integrity=False)\n\n    def __getslice__(self, i, j):\n        return self.__getitem__(slice(i, j))\n\n    def _get_labels_for_sorting(self):\n        \"\"\"\n        we categorizing our labels by using the\n        available catgories (all, not just observed)\n        excluding any missing ones (-1); this is in preparation\n        for sorting, where we need to disambiguate that -1 is not\n        a valid valid\n        \"\"\"\n        from pandas.core.arrays import Categorical\n\n        def cats(label):\n            return np.arange(np.array(label).max() + 1 if len(label) else 0,\n                             dtype=label.dtype)\n\n        return [Categorical.from_codes(label, cats(label), ordered=True)\n                for label in self.labels]\n\n    def sortlevel(self, level=0, ascending=True, sort_remaining=True):\n        \"\"\"\n        Sort MultiIndex at the requested level. The result will respect the\n        original ordering of the associated factor at that level.\n\n        Parameters\n        ----------\n        level : list-like, int or str, default 0\n            If a string is given, must be a name of the level\n            If list-like must be names or ints of levels.\n        ascending : boolean, default True\n            False to sort in descending order\n            Can also be a list to specify a directed ordering\n        sort_remaining : sort by the remaining levels after level\n\n        Returns\n        -------\n        sorted_index : pd.MultiIndex\n            Resulting index\n        indexer : np.ndarray\n            Indices of output values in original index\n        \"\"\"\n        from pandas.core.sorting import indexer_from_factorized\n\n        if isinstance(level, (compat.string_types, int)):\n            level = [level]\n        level = [self._get_level_number(lev) for lev in level]\n        sortorder = None\n\n        # we have a directed ordering via ascending\n        if isinstance(ascending, list):\n            if not len(level) == len(ascending):\n                raise ValueError(\"level must have same length as ascending\")\n\n            from pandas.core.sorting import lexsort_indexer\n            indexer = lexsort_indexer([self.labels[lev] for lev in level],\n                                      orders=ascending)\n\n        # level ordering\n        else:\n\n            labels = list(self.labels)\n            shape = list(self.levshape)\n\n            # partition labels and shape\n            primary = tuple(labels.pop(lev - i) for i, lev in enumerate(level))\n            primshp = tuple(shape.pop(lev - i) for i, lev in enumerate(level))\n\n            if sort_remaining:\n                primary += primary + tuple(labels)\n                primshp += primshp + tuple(shape)\n            else:\n                sortorder = level[0]\n\n            indexer = indexer_from_factorized(primary, primshp,\n                                              compress=False)\n\n            if not ascending:\n                indexer = indexer[::-1]\n\n        indexer = ensure_platform_int(indexer)\n        new_labels = [lab.take(indexer) for lab in self.labels]\n\n        new_index = MultiIndex(labels=new_labels, levels=self.levels,\n                               names=self.names, sortorder=sortorder,\n                               verify_integrity=False)\n\n        return new_index, indexer\n\n    def _convert_listlike_indexer(self, keyarr, kind=None):\n        \"\"\"\n        Parameters\n        ----------\n        keyarr : list-like\n            Indexer to convert.\n\n        Returns\n        -------\n        tuple (indexer, keyarr)\n            indexer is an ndarray or None if cannot convert\n            keyarr are tuple-safe keys\n        \"\"\"\n        indexer, keyarr = super(MultiIndex, self)._convert_listlike_indexer(\n            keyarr, kind=kind)\n\n        # are we indexing a specific level\n        if indexer is None and len(keyarr) and not isinstance(keyarr[0],\n                                                              tuple):\n            level = 0\n            _, indexer = self.reindex(keyarr, level=level)\n\n            # take all\n            if indexer is None:\n                indexer = np.arange(len(self))\n\n            check = self.levels[0].get_indexer(keyarr)\n            mask = check == -1\n            if mask.any():\n                raise KeyError('%s not in index' % keyarr[mask])\n\n        return indexer, keyarr\n\n    @Appender(_index_shared_docs['get_indexer'] % _index_doc_kwargs)\n    def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        method = missing.clean_reindex_fill_method(method)\n        target = ensure_index(target)\n\n        # empty indexer\n        if is_list_like(target) and not len(target):\n            return ensure_platform_int(np.array([]))\n\n        if not isinstance(target, MultiIndex):\n            try:\n                target = MultiIndex.from_tuples(target)\n            except (TypeError, ValueError):\n\n                # let's instead try with a straight Index\n                if method is None:\n                    return Index(self.values).get_indexer(target,\n                                                          method=method,\n                                                          limit=limit,\n                                                          tolerance=tolerance)\n\n        if not self.is_unique:\n            raise ValueError('Reindexing only valid with uniquely valued '\n                             'Index objects')\n\n        if method == 'pad' or method == 'backfill':\n            if tolerance is not None:\n                raise NotImplementedError(\"tolerance not implemented yet \"\n                                          'for MultiIndex')\n            indexer = self._engine.get_indexer(target, method, limit)\n        elif method == 'nearest':\n            raise NotImplementedError(\"method='nearest' not implemented yet \"\n                                      'for MultiIndex; see GitHub issue 9365')\n        else:\n            indexer = self._engine.get_indexer(target)\n\n        return ensure_platform_int(indexer)\n\n    @Appender(_index_shared_docs['get_indexer_non_unique'] % _index_doc_kwargs)\n    def get_indexer_non_unique(self, target):\n        return super(MultiIndex, self).get_indexer_non_unique(target)\n\n    def reindex(self, target, method=None, level=None, limit=None,\n                tolerance=None):\n        \"\"\"\n        Create index with target's values (move/add/delete values as necessary)\n\n        Returns\n        -------\n        new_index : pd.MultiIndex\n            Resulting index\n        indexer : np.ndarray or None\n            Indices of output values in original index\n\n        \"\"\"\n        # GH6552: preserve names when reindexing to non-named target\n        # (i.e. neither Index nor Series).\n        preserve_names = not hasattr(target, 'names')\n\n        if level is not None:\n            if method is not None:\n                raise TypeError('Fill method not supported if level passed')\n\n            # GH7774: preserve dtype/tz if target is empty and not an Index.\n            # target may be an iterator\n            target = ibase._ensure_has_len(target)\n            if len(target) == 0 and not isinstance(target, Index):\n                idx = self.levels[level]\n                attrs = idx._get_attributes_dict()\n                attrs.pop('freq', None)  # don't preserve freq\n                target = type(idx)._simple_new(np.empty(0, dtype=idx.dtype),\n                                               **attrs)\n            else:\n                target = ensure_index(target)\n            target, indexer, _ = self._join_level(target, level, how='right',\n                                                  return_indexers=True,\n                                                  keep_order=False)\n        else:\n            target = ensure_index(target)\n            if self.equals(target):\n                indexer = None\n            else:\n                if self.is_unique:\n                    indexer = self.get_indexer(target, method=method,\n                                               limit=limit,\n                                               tolerance=tolerance)\n                else:\n                    raise ValueError(\"cannot handle a non-unique multi-index!\")\n\n        if not isinstance(target, MultiIndex):\n            if indexer is None:\n                target = self\n            elif (indexer >= 0).all():\n                target = self.take(indexer)\n            else:\n                # hopefully?\n                target = MultiIndex.from_tuples(target)\n\n        if (preserve_names and target.nlevels == self.nlevels and\n                target.names != self.names):\n            target = target.copy(deep=False)\n            target.names = self.names\n\n        return target, indexer\n\n    def get_slice_bound(self, label, side, kind):\n\n        if not isinstance(label, tuple):\n            label = label,\n        return self._partial_tup_index(label, side=side)\n\n    def slice_locs(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        For an ordered MultiIndex, compute the slice locations for input\n        labels.\n\n        The input labels can be tuples representing partial levels, e.g. for a\n        MultiIndex with 3 levels, you can pass a single value (corresponding to\n        the first level), or a 1-, 2-, or 3-tuple.\n\n        Parameters\n        ----------\n        start : label or tuple, default None\n            If None, defaults to the beginning\n        end : label or tuple\n            If None, defaults to the end\n        step : int or None\n            Slice step\n        kind : string, optional, defaults None\n\n        Returns\n        -------\n        (start, end) : (int, int)\n\n        Notes\n        -----\n        This method only works if the MultiIndex is properly lexsorted. So,\n        if only the first 2 levels of a 3-level MultiIndex are lexsorted,\n        you can only pass two levels to ``.slice_locs``.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([list('abbd'), list('deff')],\n        ...                                names=['A', 'B'])\n\n        Get the slice locations from the beginning of 'b' in the first level\n        until the end of the multiindex:\n\n        >>> mi.slice_locs(start='b')\n        (1, 4)\n\n        Like above, but stop at the end of 'b' in the first level and 'f' in\n        the second level:\n\n        >>> mi.slice_locs(start='b', end=('b', 'f'))\n        (1, 3)\n\n        See Also\n        --------\n        MultiIndex.get_loc : Get location for a label or a tuple of labels.\n        MultiIndex.get_locs : Get location for a label/slice/list/mask or a\n                              sequence of such.\n        \"\"\"\n        # This function adds nothing to its parent implementation (the magic\n        # happens in get_slice_bound method), but it adds meaningful doc.\n        return super(MultiIndex, self).slice_locs(start, end, step, kind=kind)\n\n    def _partial_tup_index(self, tup, side='left'):\n        if len(tup) > self.lexsort_depth:\n            raise UnsortedIndexError(\n                'Key length (%d) was greater than MultiIndex'\n                ' lexsort depth (%d)' %\n                (len(tup), self.lexsort_depth))\n\n        n = len(tup)\n        start, end = 0, len(self)\n        zipped = zip(tup, self.levels, self.labels)\n        for k, (lab, lev, labs) in enumerate(zipped):\n            section = labs[start:end]\n\n            if lab not in lev:\n                if not lev.is_type_compatible(lib.infer_dtype([lab])):\n                    raise TypeError('Level type mismatch: %s' % lab)\n\n                # short circuit\n                loc = lev.searchsorted(lab, side=side)\n                if side == 'right' and loc >= 0:\n                    loc -= 1\n                return start + section.searchsorted(loc, side=side)\n\n            idx = lev.get_loc(lab)\n            if k < n - 1:\n                end = start + section.searchsorted(idx, side='right')\n                start = start + section.searchsorted(idx, side='left')\n            else:\n                return start + section.searchsorted(idx, side=side)\n\n    def get_loc(self, key, method=None):\n        \"\"\"\n        Get location for a label or a tuple of labels as an integer, slice or\n        boolean mask.\n\n        Parameters\n        ----------\n        key : label or tuple of labels (one for each level)\n        method : None\n\n        Returns\n        -------\n        loc : int, slice object or boolean mask\n            If the key is past the lexsort depth, the return may be a\n            boolean mask array, otherwise it is always a slice or int.\n\n        Examples\n        ---------\n        >>> mi = pd.MultiIndex.from_arrays([list('abb'), list('def')])\n\n        >>> mi.get_loc('b')\n        slice(1, 3, None)\n\n        >>> mi.get_loc(('b', 'e'))\n        1\n\n        Notes\n        ------\n        The key cannot be a slice, list of same-level labels, a boolean mask,\n        or a sequence of such. If you want to use those, use\n        :meth:`MultiIndex.get_locs` instead.\n\n        See Also\n        --------\n        Index.get_loc : The get_loc method for (single-level) index.\n        MultiIndex.slice_locs : Get slice location given start label(s) and\n                                end label(s).\n        MultiIndex.get_locs : Get location for a label/slice/list/mask or a\n                              sequence of such.\n        \"\"\"\n        if method is not None:\n            raise NotImplementedError('only the default get_loc method is '\n                                      'currently supported for MultiIndex')\n\n        def _maybe_to_slice(loc):\n            \"\"\"convert integer indexer to boolean mask or slice if possible\"\"\"\n            if not isinstance(loc, np.ndarray) or loc.dtype != 'int64':\n                return loc\n\n            loc = lib.maybe_indices_to_slice(loc, len(self))\n            if isinstance(loc, slice):\n                return loc\n\n            mask = np.empty(len(self), dtype='bool')\n            mask.fill(False)\n            mask[loc] = True\n            return mask\n\n        if not isinstance(key, tuple):\n            loc = self._get_level_indexer(key, level=0)\n            return _maybe_to_slice(loc)\n\n        keylen = len(key)\n        if self.nlevels < keylen:\n            raise KeyError('Key length ({0}) exceeds index depth ({1})'\n                           ''.format(keylen, self.nlevels))\n\n        if keylen == self.nlevels and self.is_unique:\n            return self._engine.get_loc(key)\n\n        # -- partial selection or non-unique index\n        # break the key into 2 parts based on the lexsort_depth of the index;\n        # the first part returns a continuous slice of the index; the 2nd part\n        # needs linear search within the slice\n        i = self.lexsort_depth\n        lead_key, follow_key = key[:i], key[i:]\n        start, stop = (self.slice_locs(lead_key, lead_key)\n                       if lead_key else (0, len(self)))\n\n        if start == stop:\n            raise KeyError(key)\n\n        if not follow_key:\n            return slice(start, stop)\n\n        warnings.warn('indexing past lexsort depth may impact performance.',\n                      PerformanceWarning, stacklevel=10)\n\n        loc = np.arange(start, stop, dtype='int64')\n\n        for i, k in enumerate(follow_key, len(lead_key)):\n            mask = self.labels[i][loc] == self.levels[i].get_loc(k)\n            if not mask.all():\n                loc = loc[mask]\n            if not len(loc):\n                raise KeyError(key)\n\n        return (_maybe_to_slice(loc) if len(loc) != stop - start else\n                slice(start, stop))\n\n    def get_loc_level(self, key, level=0, drop_level=True):\n        \"\"\"\n        Get both the location for the requested label(s) and the\n        resulting sliced index.\n\n        Parameters\n        ----------\n        key : label or sequence of labels\n        level : int/level name or list thereof, optional\n        drop_level : bool, default True\n            if ``False``, the resulting index will not drop any level.\n\n        Returns\n        -------\n        loc : A 2-tuple where the elements are:\n              Element 0: int, slice object or boolean array\n              Element 1: The resulting sliced multiindex/index. If the key\n              contains all levels, this will be ``None``.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([list('abb'), list('def')],\n        ...                                names=['A', 'B'])\n\n        >>> mi.get_loc_level('b')\n        (slice(1, 3, None), Index(['e', 'f'], dtype='object', name='B'))\n\n        >>> mi.get_loc_level('e', level='B')\n        (array([False,  True, False], dtype=bool),\n        Index(['b'], dtype='object', name='A'))\n\n        >>> mi.get_loc_level(['b', 'e'])\n        (1, None)\n\n        See Also\n        ---------\n        MultiIndex.get_loc  : Get location for a label or a tuple of labels.\n        MultiIndex.get_locs : Get location for a label/slice/list/mask or a\n                              sequence of such.\n        \"\"\"\n\n        def maybe_droplevels(indexer, levels, drop_level):\n            if not drop_level:\n                return self[indexer]\n            # kludgearound\n            orig_index = new_index = self[indexer]\n            levels = [self._get_level_number(i) for i in levels]\n            for i in sorted(levels, reverse=True):\n                try:\n                    new_index = new_index.droplevel(i)\n                except ValueError:\n\n                    # no dropping here\n                    return orig_index\n            return new_index\n\n        if isinstance(level, (tuple, list)):\n            if len(key) != len(level):\n                raise AssertionError('Key for location must have same '\n                                     'length as number of levels')\n            result = None\n            for lev, k in zip(level, key):\n                loc, new_index = self.get_loc_level(k, level=lev)\n                if isinstance(loc, slice):\n                    mask = np.zeros(len(self), dtype=bool)\n                    mask[loc] = True\n                    loc = mask\n\n                result = loc if result is None else result & loc\n\n            return result, maybe_droplevels(result, level, drop_level)\n\n        level = self._get_level_number(level)\n\n        # kludge for #1796\n        if isinstance(key, list):\n            key = tuple(key)\n\n        if isinstance(key, tuple) and level == 0:\n\n            try:\n                if key in self.levels[0]:\n                    indexer = self._get_level_indexer(key, level=level)\n                    new_index = maybe_droplevels(indexer, [0], drop_level)\n                    return indexer, new_index\n            except TypeError:\n                pass\n\n            if not any(isinstance(k, slice) for k in key):\n\n                # partial selection\n                # optionally get indexer to avoid re-calculation\n                def partial_selection(key, indexer=None):\n                    if indexer is None:\n                        indexer = self.get_loc(key)\n                    ilevels = [i for i in range(len(key))\n                               if key[i] != slice(None, None)]\n                    return indexer, maybe_droplevels(indexer, ilevels,\n                                                     drop_level)\n\n                if len(key) == self.nlevels and self.is_unique:\n                    # Complete key in unique index -> standard get_loc\n                    return (self._engine.get_loc(key), None)\n                else:\n                    return partial_selection(key)\n            else:\n                indexer = None\n                for i, k in enumerate(key):\n                    if not isinstance(k, slice):\n                        k = self._get_level_indexer(k, level=i)\n                        if isinstance(k, slice):\n                            # everything\n                            if k.start == 0 and k.stop == len(self):\n                                k = slice(None, None)\n                        else:\n                            k_index = k\n\n                    if isinstance(k, slice):\n                        if k == slice(None, None):\n                            continue\n                        else:\n                            raise TypeError(key)\n\n                    if indexer is None:\n                        indexer = k_index\n                    else:  # pragma: no cover\n                        indexer &= k_index\n                if indexer is None:\n                    indexer = slice(None, None)\n                ilevels = [i for i in range(len(key))\n                           if key[i] != slice(None, None)]\n                return indexer, maybe_droplevels(indexer, ilevels, drop_level)\n        else:\n            indexer = self._get_level_indexer(key, level=level)\n            return indexer, maybe_droplevels(indexer, [level], drop_level)\n\n    def _get_level_indexer(self, key, level=0, indexer=None):\n        # return an indexer, boolean array or a slice showing where the key is\n        # in the totality of values\n        # if the indexer is provided, then use this\n\n        level_index = self.levels[level]\n        labels = self.labels[level]\n\n        def convert_indexer(start, stop, step, indexer=indexer, labels=labels):\n            # given the inputs and the labels/indexer, compute an indexer set\n            # if we have a provided indexer, then this need not consider\n            # the entire labels set\n\n            r = np.arange(start, stop, step)\n            if indexer is not None and len(indexer) != len(labels):\n\n                # we have an indexer which maps the locations in the labels\n                # that we have already selected (and is not an indexer for the\n                # entire set) otherwise this is wasteful so we only need to\n                # examine locations that are in this set the only magic here is\n                # that the result are the mappings to the set that we have\n                # selected\n                from pandas import Series\n                mapper = Series(indexer)\n                indexer = labels.take(ensure_platform_int(indexer))\n                result = Series(Index(indexer).isin(r).nonzero()[0])\n                m = result.map(mapper)._ndarray_values\n\n            else:\n                m = np.zeros(len(labels), dtype=bool)\n                m[np.in1d(labels, r,\n                          assume_unique=Index(labels).is_unique)] = True\n\n            return m\n\n        if isinstance(key, slice):\n            # handle a slice, returnig a slice if we can\n            # otherwise a boolean indexer\n\n            try:\n                if key.start is not None:\n                    start = level_index.get_loc(key.start)\n                else:\n                    start = 0\n                if key.stop is not None:\n                    stop = level_index.get_loc(key.stop)\n                else:\n                    stop = len(level_index) - 1\n                step = key.step\n            except KeyError:\n\n                # we have a partial slice (like looking up a partial date\n                # string)\n                start = stop = level_index.slice_indexer(key.start, key.stop,\n                                                         key.step, kind='loc')\n                step = start.step\n\n            if isinstance(start, slice) or isinstance(stop, slice):\n                # we have a slice for start and/or stop\n                # a partial date slicer on a DatetimeIndex generates a slice\n                # note that the stop ALREADY includes the stopped point (if\n                # it was a string sliced)\n                return convert_indexer(start.start, stop.stop, step)\n\n            elif level > 0 or self.lexsort_depth == 0 or step is not None:\n                # need to have like semantics here to right\n                # searching as when we are using a slice\n                # so include the stop+1 (so we include stop)\n                return convert_indexer(start, stop + 1, step)\n            else:\n                # sorted, so can return slice object -> view\n                i = labels.searchsorted(start, side='left')\n                j = labels.searchsorted(stop, side='right')\n                return slice(i, j, step)\n\n        else:\n\n            code = level_index.get_loc(key)\n\n            if level > 0 or self.lexsort_depth == 0:\n                # Desired level is not sorted\n                locs = np.array(labels == code, dtype=bool, copy=False)\n                if not locs.any():\n                    # The label is present in self.levels[level] but unused:\n                    raise KeyError(key)\n                return locs\n\n            i = labels.searchsorted(code, side='left')\n            j = labels.searchsorted(code, side='right')\n            if i == j:\n                # The label is present in self.levels[level] but unused:\n                raise KeyError(key)\n            return slice(i, j)\n\n    def get_locs(self, seq):\n        \"\"\"\n        Get location for a given label/slice/list/mask or a sequence of such as\n        an array of integers.\n\n        Parameters\n        ----------\n        seq : label/slice/list/mask or a sequence of such\n           You should use one of the above for each level.\n           If a level should not be used, set it to ``slice(None)``.\n\n        Returns\n        -------\n        locs : array of integers suitable for passing to iloc\n\n        Examples\n        ---------\n        >>> mi = pd.MultiIndex.from_arrays([list('abb'), list('def')])\n\n        >>> mi.get_locs('b')\n        array([1, 2], dtype=int64)\n\n        >>> mi.get_locs([slice(None), ['e', 'f']])\n        array([1, 2], dtype=int64)\n\n        >>> mi.get_locs([[True, False, True], slice('e', 'f')])\n        array([2], dtype=int64)\n\n        See Also\n        --------\n        MultiIndex.get_loc : Get location for a label or a tuple of labels.\n        MultiIndex.slice_locs : Get slice location given start label(s) and\n                                end label(s).\n        \"\"\"\n        from .numeric import Int64Index\n\n        # must be lexsorted to at least as many levels\n        true_slices = [i for (i, s) in enumerate(com.is_true_slices(seq)) if s]\n        if true_slices and true_slices[-1] >= self.lexsort_depth:\n            raise UnsortedIndexError('MultiIndex slicing requires the index '\n                                     'to be lexsorted: slicing on levels {0}, '\n                                     'lexsort depth {1}'\n                                     .format(true_slices, self.lexsort_depth))\n        # indexer\n        # this is the list of all values that we want to select\n        n = len(self)\n        indexer = None\n\n        def _convert_to_indexer(r):\n            # return an indexer\n            if isinstance(r, slice):\n                m = np.zeros(n, dtype=bool)\n                m[r] = True\n                r = m.nonzero()[0]\n            elif com.is_bool_indexer(r):\n                if len(r) != n:\n                    raise ValueError(\"cannot index with a boolean indexer \"\n                                     \"that is not the same length as the \"\n                                     \"index\")\n                r = r.nonzero()[0]\n            return Int64Index(r)\n\n        def _update_indexer(idxr, indexer=indexer):\n            if indexer is None:\n                indexer = Index(np.arange(n))\n            if idxr is None:\n                return indexer\n            return indexer & idxr\n\n        for i, k in enumerate(seq):\n\n            if com.is_bool_indexer(k):\n                # a boolean indexer, must be the same length!\n                k = np.asarray(k)\n                indexer = _update_indexer(_convert_to_indexer(k),\n                                          indexer=indexer)\n\n            elif is_list_like(k):\n                # a collection of labels to include from this level (these\n                # are or'd)\n                indexers = None\n                for x in k:\n                    try:\n                        idxrs = _convert_to_indexer(\n                            self._get_level_indexer(x, level=i,\n                                                    indexer=indexer))\n                        indexers = (idxrs if indexers is None\n                                    else indexers | idxrs)\n                    except KeyError:\n\n                        # ignore not founds\n                        continue\n\n                if indexers is not None:\n                    indexer = _update_indexer(indexers, indexer=indexer)\n                else:\n                    # no matches we are done\n                    return Int64Index([])._ndarray_values\n\n            elif com.is_null_slice(k):\n                # empty slice\n                indexer = _update_indexer(None, indexer=indexer)\n\n            elif isinstance(k, slice):\n\n                # a slice, include BOTH of the labels\n                indexer = _update_indexer(_convert_to_indexer(\n                    self._get_level_indexer(k, level=i, indexer=indexer)),\n                    indexer=indexer)\n            else:\n                # a single label\n                indexer = _update_indexer(_convert_to_indexer(\n                    self.get_loc_level(k, level=i, drop_level=False)[0]),\n                    indexer=indexer)\n\n        # empty indexer\n        if indexer is None:\n            return Int64Index([])._ndarray_values\n        return indexer._ndarray_values\n\n    def truncate(self, before=None, after=None):\n        \"\"\"\n        Slice index between two labels / tuples, return new MultiIndex\n\n        Parameters\n        ----------\n        before : label or tuple, can be partial. Default None\n            None defaults to start\n        after : label or tuple, can be partial. Default None\n            None defaults to end\n\n        Returns\n        -------\n        truncated : MultiIndex\n        \"\"\"\n        if after and before and after < before:\n            raise ValueError('after < before')\n\n        i, j = self.levels[0].slice_locs(before, after)\n        left, right = self.slice_locs(before, after)\n\n        new_levels = list(self.levels)\n        new_levels[0] = new_levels[0][i:j]\n\n        new_labels = [lab[left:right] for lab in self.labels]\n        new_labels[0] = new_labels[0] - i\n\n        return MultiIndex(levels=new_levels, labels=new_labels,\n                          verify_integrity=False)\n\n    def equals(self, other):\n        \"\"\"\n        Determines if two MultiIndex objects have the same labeling information\n        (the levels themselves do not necessarily have to be the same)\n\n        See Also\n        --------\n        equal_levels\n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n\n        if not isinstance(other, MultiIndex):\n            other_vals = com.values_from_object(ensure_index(other))\n            return array_equivalent(self._ndarray_values, other_vals)\n\n        if self.nlevels != other.nlevels:\n            return False\n\n        if len(self) != len(other):\n            return False\n\n        for i in range(self.nlevels):\n            slabels = self.labels[i]\n            slabels = slabels[slabels != -1]\n            svalues = algos.take_nd(np.asarray(self.levels[i]._values),\n                                    slabels, allow_fill=False)\n\n            olabels = other.labels[i]\n            olabels = olabels[olabels != -1]\n            ovalues = algos.take_nd(\n                np.asarray(other.levels[i]._values),\n                olabels, allow_fill=False)\n\n            # since we use NaT both datetime64 and timedelta64\n            # we can have a situation where a level is typed say\n            # timedelta64 in self (IOW it has other values than NaT)\n            # but types datetime64 in other (where its all NaT)\n            # but these are equivalent\n            if len(svalues) == 0 and len(ovalues) == 0:\n                continue\n\n            if not array_equivalent(svalues, ovalues):\n                return False\n\n        return True\n\n    def equal_levels(self, other):\n        \"\"\"\n        Return True if the levels of both MultiIndex objects are the same\n\n        \"\"\"\n        if self.nlevels != other.nlevels:\n            return False\n\n        for i in range(self.nlevels):\n            if not self.levels[i].equals(other.levels[i]):\n                return False\n        return True\n\n    def union(self, other):\n        \"\"\"\n        Form the union of two MultiIndex objects, sorting if possible\n\n        Parameters\n        ----------\n        other : MultiIndex or array / Index of tuples\n\n        Returns\n        -------\n        Index\n\n        >>> index.union(index2)\n        \"\"\"\n        self._assert_can_do_setop(other)\n        other, result_names = self._convert_can_do_setop(other)\n\n        if len(other) == 0 or self.equals(other):\n            return self\n\n        uniq_tuples = lib.fast_unique_multiple([self._ndarray_values,\n                                                other._ndarray_values])\n        return MultiIndex.from_arrays(lzip(*uniq_tuples), sortorder=0,\n                                      names=result_names)\n\n    def intersection(self, other):\n        \"\"\"\n        Form the intersection of two MultiIndex objects, sorting if possible\n\n        Parameters\n        ----------\n        other : MultiIndex or array / Index of tuples\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        self._assert_can_do_setop(other)\n        other, result_names = self._convert_can_do_setop(other)\n\n        if self.equals(other):\n            return self\n\n        self_tuples = self._ndarray_values\n        other_tuples = other._ndarray_values\n        uniq_tuples = sorted(set(self_tuples) & set(other_tuples))\n        if len(uniq_tuples) == 0:\n            return MultiIndex(levels=self.levels,\n                              labels=[[]] * self.nlevels,\n                              names=result_names, verify_integrity=False)\n        else:\n            return MultiIndex.from_arrays(lzip(*uniq_tuples), sortorder=0,\n                                          names=result_names)\n\n    def difference(self, other, sort=True):\n        \"\"\"\n        Compute sorted set difference of two MultiIndex objects\n\n        Parameters\n        ----------\n        other : MultiIndex\n        sort : bool, default True\n            Sort the resulting MultiIndex if possible\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        diff : MultiIndex\n        \"\"\"\n        self._assert_can_do_setop(other)\n        other, result_names = self._convert_can_do_setop(other)\n\n        if len(other) == 0:\n            return self\n\n        if self.equals(other):\n            return MultiIndex(levels=self.levels,\n                              labels=[[]] * self.nlevels,\n                              names=result_names, verify_integrity=False)\n\n        this = self._get_unique_index()\n\n        indexer = this.get_indexer(other)\n        indexer = indexer.take((indexer != -1).nonzero()[0])\n\n        label_diff = np.setdiff1d(np.arange(this.size), indexer,\n                                  assume_unique=True)\n        difference = this.values.take(label_diff)\n        if sort:\n            difference = sorted(difference)\n\n        if len(difference) == 0:\n            return MultiIndex(levels=[[]] * self.nlevels,\n                              labels=[[]] * self.nlevels,\n                              names=result_names, verify_integrity=False)\n        else:\n            return MultiIndex.from_tuples(difference, sortorder=0,\n                                          names=result_names)\n\n    @Appender(_index_shared_docs['astype'])\n    def astype(self, dtype, copy=True):\n        dtype = pandas_dtype(dtype)\n        if is_categorical_dtype(dtype):\n            msg = '> 1 ndim Categorical are not supported at this time'\n            raise NotImplementedError(msg)\n        elif not is_object_dtype(dtype):\n            msg = ('Setting {cls} dtype to anything other than object '\n                   'is not supported').format(cls=self.__class__)\n            raise TypeError(msg)\n        elif copy is True:\n            return self._shallow_copy()\n        return self\n\n    def _convert_can_do_setop(self, other):\n        result_names = self.names\n\n        if not hasattr(other, 'names'):\n            if len(other) == 0:\n                other = MultiIndex(levels=[[]] * self.nlevels,\n                                   labels=[[]] * self.nlevels,\n                                   verify_integrity=False)\n            else:\n                msg = 'other must be a MultiIndex or a list of tuples'\n                try:\n                    other = MultiIndex.from_tuples(other)\n                except TypeError:\n                    raise TypeError(msg)\n        else:\n            result_names = self.names if self.names == other.names else None\n        return other, result_names\n\n    def insert(self, loc, item):\n        \"\"\"\n        Make new MultiIndex inserting new item at location\n\n        Parameters\n        ----------\n        loc : int\n        item : tuple\n            Must be same length as number of levels in the MultiIndex\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        # Pad the key with empty strings if lower levels of the key\n        # aren't specified:\n        if not isinstance(item, tuple):\n            item = (item, ) + ('', ) * (self.nlevels - 1)\n        elif len(item) != self.nlevels:\n            raise ValueError('Item must have length equal to number of '\n                             'levels.')\n\n        new_levels = []\n        new_labels = []\n        for k, level, labels in zip(item, self.levels, self.labels):\n            if k not in level:\n                # have to insert into level\n                # must insert at end otherwise you have to recompute all the\n                # other labels\n                lev_loc = len(level)\n                level = level.insert(lev_loc, k)\n            else:\n                lev_loc = level.get_loc(k)\n\n            new_levels.append(level)\n            new_labels.append(np.insert(ensure_int64(labels), loc, lev_loc))\n\n        return MultiIndex(levels=new_levels, labels=new_labels,\n                          names=self.names, verify_integrity=False)\n\n    def delete(self, loc):\n        \"\"\"\n        Make new index with passed location deleted\n\n        Returns\n        -------\n        new_index : MultiIndex\n        \"\"\"\n        new_labels = [np.delete(lab, loc) for lab in self.labels]\n        return MultiIndex(levels=self.levels, labels=new_labels,\n                          names=self.names, verify_integrity=False)\n\n    def _wrap_joined_index(self, joined, other):\n        names = self.names if self.names == other.names else None\n        return MultiIndex.from_tuples(joined, names=names)\n\n    @Appender(Index.isin.__doc__)\n    def isin(self, values, level=None):\n        if level is None:\n            values = MultiIndex.from_tuples(values,\n                                            names=self.names).values\n            return algos.isin(self.values, values)\n        else:\n            num = self._get_level_number(level)\n            levs = self.levels[num]\n            labs = self.labels[num]\n\n            sought_labels = levs.isin(values).nonzero()[0]\n            if levs.size == 0:\n                return np.zeros(len(labs), dtype=np.bool_)\n            else:\n                return np.lib.arraysetops.in1d(labs, sought_labels)\n\n\nMultiIndex._add_numeric_methods_disabled()\nMultiIndex._add_numeric_methods_add_sub_disabled()\nMultiIndex._add_logical_methods_disabled()\n\n\ndef _sparsify(label_list, start=0, sentinel=''):\n    pivoted = lzip(*label_list)\n    k = len(label_list)\n\n    result = pivoted[:start + 1]\n    prev = pivoted[start]\n\n    for cur in pivoted[start + 1:]:\n        sparse_cur = []\n\n        for i, (p, t) in enumerate(zip(prev, cur)):\n            if i == k - 1:\n                sparse_cur.append(t)\n                result.append(sparse_cur)\n                break\n\n            if p == t:\n                sparse_cur.append(sentinel)\n            else:\n                sparse_cur.extend(cur[i:])\n                result.append(sparse_cur)\n                break\n\n        prev = cur\n\n    return lzip(*result)\n\n\ndef _get_na_rep(dtype):\n    return {np.datetime64: 'NaT', np.timedelta64: 'NaT'}.get(dtype, 'NaN')\n"
    },
    {
      "filename": "pandas/core/panel.py",
      "content": "\"\"\"\nContains data structures designed for manipulating panel (3-dimensional) data\n\"\"\"\n# pylint: disable=E1103,W0231,W0212,W0621\nfrom __future__ import division\n\nimport warnings\n\nimport numpy as np\n\nimport pandas.compat as compat\nfrom pandas.compat import OrderedDict, map, range, u, zip\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import Appender, Substitution, deprecate_kwarg\nfrom pandas.util._validators import validate_axis_style_args\n\nfrom pandas.core.dtypes.cast import (\n    cast_scalar_to_array, infer_dtype_from_scalar, maybe_cast_item)\nfrom pandas.core.dtypes.common import (\n    is_integer, is_list_like, is_scalar, is_string_like)\nfrom pandas.core.dtypes.missing import notna\n\nimport pandas.core.common as com\nfrom pandas.core.frame import DataFrame\nfrom pandas.core.generic import NDFrame, _shared_docs\nfrom pandas.core.index import (\n    Index, MultiIndex, _get_objs_combined_axis, ensure_index)\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexing import maybe_droplevels\nfrom pandas.core.internals import (\n    BlockManager, create_block_manager_from_arrays,\n    create_block_manager_from_blocks)\nimport pandas.core.ops as ops\nfrom pandas.core.reshape.util import cartesian_product\nfrom pandas.core.series import Series\n\nfrom pandas.io.formats.printing import pprint_thing\n\n_shared_doc_kwargs = dict(\n    axes='items, major_axis, minor_axis',\n    klass=\"Panel\",\n    axes_single_arg=\"{0, 1, 2, 'items', 'major_axis', 'minor_axis'}\",\n    optional_mapper='', optional_axis='', optional_labels='')\n_shared_doc_kwargs['args_transpose'] = (\n    \"three positional arguments: each one of\\n{ax_single}\".format(\n        ax_single=_shared_doc_kwargs['axes_single_arg']))\n\n\ndef _ensure_like_indices(time, panels):\n    \"\"\"\n    Makes sure that time and panels are conformable\n    \"\"\"\n    n_time = len(time)\n    n_panel = len(panels)\n    u_panels = np.unique(panels)  # this sorts!\n    u_time = np.unique(time)\n    if len(u_time) == n_time:\n        time = np.tile(u_time, len(u_panels))\n    if len(u_panels) == n_panel:\n        panels = np.repeat(u_panels, len(u_time))\n    return time, panels\n\n\ndef panel_index(time, panels, names=None):\n    \"\"\"\n    Returns a multi-index suitable for a panel-like DataFrame\n\n    Parameters\n    ----------\n    time : array-like\n        Time index, does not have to repeat\n    panels : array-like\n        Panel index, does not have to repeat\n    names : list, optional\n        List containing the names of the indices\n\n    Returns\n    -------\n    multi_index : MultiIndex\n        Time index is the first level, the panels are the second level.\n\n    Examples\n    --------\n    >>> years = range(1960,1963)\n    >>> panels = ['A', 'B', 'C']\n    >>> panel_idx = panel_index(years, panels)\n    >>> panel_idx\n    MultiIndex([(1960, 'A'), (1961, 'A'), (1962, 'A'), (1960, 'B'),\n                (1961, 'B'), (1962, 'B'), (1960, 'C'), (1961, 'C'),\n                (1962, 'C')], dtype=object)\n\n    or\n\n    >>> years = np.repeat(range(1960,1963), 3)\n    >>> panels = np.tile(['A', 'B', 'C'], 3)\n    >>> panel_idx = panel_index(years, panels)\n    >>> panel_idx\n    MultiIndex([(1960, 'A'), (1960, 'B'), (1960, 'C'), (1961, 'A'),\n                (1961, 'B'), (1961, 'C'), (1962, 'A'), (1962, 'B'),\n                (1962, 'C')], dtype=object)\n    \"\"\"\n    if names is None:\n        names = ['time', 'panel']\n    time, panels = _ensure_like_indices(time, panels)\n    return MultiIndex.from_arrays([time, panels], sortorder=None, names=names)\n\n\nclass Panel(NDFrame):\n    \"\"\"\n    Represents wide format panel data, stored as 3-dimensional array.\n\n    .. deprecated:: 0.20.0\n        The recommended way to represent 3-D data are with a MultiIndex on a\n        DataFrame via the :attr:`~Panel.to_frame()` method or with the\n        `xarray package <http://xarray.pydata.org/en/stable/>`__.\n        Pandas provides a :attr:`~Panel.to_xarray()` method to automate this\n        conversion.\n\n    Parameters\n    ----------\n    data : ndarray (items x major x minor), or dict of DataFrames\n    items : Index or array-like\n        axis=0\n    major_axis : Index or array-like\n        axis=1\n    minor_axis : Index or array-like\n        axis=2\n    dtype : dtype, default None\n        Data type to force, otherwise infer\n    copy : boolean, default False\n        Copy data from inputs. Only affects DataFrame / 2d ndarray input\n    \"\"\"\n\n    @property\n    def _constructor(self):\n        return type(self)\n\n    _constructor_sliced = DataFrame\n\n    def __init__(self, data=None, items=None, major_axis=None, minor_axis=None,\n                 copy=False, dtype=None):\n        # deprecation GH13563\n        warnings.warn(\"\\nPanel is deprecated and will be removed in a \"\n                      \"future version.\\nThe recommended way to represent \"\n                      \"these types of 3-dimensional data are with a \"\n                      \"MultiIndex on a DataFrame, via the \"\n                      \"Panel.to_frame() method\\n\"\n                      \"Alternatively, you can use the xarray package \"\n                      \"http://xarray.pydata.org/en/stable/.\\n\"\n                      \"Pandas provides a `.to_xarray()` method to help \"\n                      \"automate this conversion.\\n\",\n                      FutureWarning, stacklevel=3)\n\n        self._init_data(data=data, items=items, major_axis=major_axis,\n                        minor_axis=minor_axis, copy=copy, dtype=dtype)\n\n    def _init_data(self, data, copy, dtype, **kwargs):\n        \"\"\"\n        Generate ND initialization; axes are passed\n        as required objects to __init__\n        \"\"\"\n        if data is None:\n            data = {}\n        if dtype is not None:\n            dtype = self._validate_dtype(dtype)\n\n        passed_axes = [kwargs.pop(a, None) for a in self._AXIS_ORDERS]\n\n        if kwargs:\n            raise TypeError('_init_data() got an unexpected keyword '\n                            'argument \"{0}\"'.format(list(kwargs.keys())[0]))\n\n        axes = None\n        if isinstance(data, BlockManager):\n            if com._any_not_none(*passed_axes):\n                axes = [x if x is not None else y\n                        for x, y in zip(passed_axes, data.axes)]\n            mgr = data\n        elif isinstance(data, dict):\n            mgr = self._init_dict(data, passed_axes, dtype=dtype)\n            copy = False\n            dtype = None\n        elif isinstance(data, (np.ndarray, list)):\n            mgr = self._init_matrix(data, passed_axes, dtype=dtype, copy=copy)\n            copy = False\n            dtype = None\n        elif is_scalar(data) and com._all_not_none(*passed_axes):\n            values = cast_scalar_to_array([len(x) for x in passed_axes],\n                                          data, dtype=dtype)\n            mgr = self._init_matrix(values, passed_axes, dtype=values.dtype,\n                                    copy=False)\n            copy = False\n        else:  # pragma: no cover\n            raise ValueError('Panel constructor not properly called!')\n\n        NDFrame.__init__(self, mgr, axes=axes, copy=copy, dtype=dtype)\n\n    def _init_dict(self, data, axes, dtype=None):\n        haxis = axes.pop(self._info_axis_number)\n\n        # prefilter if haxis passed\n        if haxis is not None:\n            haxis = ensure_index(haxis)\n            data = OrderedDict((k, v)\n                               for k, v in compat.iteritems(data)\n                               if k in haxis)\n        else:\n            keys = com.dict_keys_to_ordered_list(data)\n            haxis = Index(keys)\n\n        for k, v in compat.iteritems(data):\n            if isinstance(v, dict):\n                data[k] = self._constructor_sliced(v)\n\n        # extract axis for remaining axes & create the slicemap\n        raxes = [self._extract_axis(self, data, axis=i) if a is None else a\n                 for i, a in enumerate(axes)]\n        raxes_sm = self._extract_axes_for_slice(self, raxes)\n\n        # shallow copy\n        arrays = []\n        haxis_shape = [len(a) for a in raxes]\n        for h in haxis:\n            v = values = data.get(h)\n            if v is None:\n                values = np.empty(haxis_shape, dtype=dtype)\n                values.fill(np.nan)\n            elif isinstance(v, self._constructor_sliced):\n                d = raxes_sm.copy()\n                d['copy'] = False\n                v = v.reindex(**d)\n                if dtype is not None:\n                    v = v.astype(dtype)\n                values = v.values\n            arrays.append(values)\n\n        return self._init_arrays(arrays, haxis, [haxis] + raxes)\n\n    def _init_arrays(self, arrays, arr_names, axes):\n        return create_block_manager_from_arrays(arrays, arr_names, axes)\n\n    @classmethod\n    def from_dict(cls, data, intersect=False, orient='items', dtype=None):\n        \"\"\"\n        Construct Panel from dict of DataFrame objects\n\n        Parameters\n        ----------\n        data : dict\n            {field : DataFrame}\n        intersect : boolean\n            Intersect indexes of input DataFrames\n        orient : {'items', 'minor'}, default 'items'\n            The \"orientation\" of the data. If the keys of the passed dict\n            should be the items of the result panel, pass 'items'\n            (default). Otherwise if the columns of the values of the passed\n            DataFrame objects should be the items (which in the case of\n            mixed-dtype data you should do), instead pass 'minor'\n        dtype : dtype, default None\n            Data type to force, otherwise infer\n\n        Returns\n        -------\n        Panel\n        \"\"\"\n        from collections import defaultdict\n\n        orient = orient.lower()\n        if orient == 'minor':\n            new_data = defaultdict(OrderedDict)\n            for col, df in compat.iteritems(data):\n                for item, s in compat.iteritems(df):\n                    new_data[item][col] = s\n            data = new_data\n        elif orient != 'items':  # pragma: no cover\n            raise ValueError('Orientation must be one of {items, minor}.')\n\n        d = cls._homogenize_dict(cls, data, intersect=intersect, dtype=dtype)\n        ks = list(d['data'].keys())\n        if not isinstance(d['data'], OrderedDict):\n            ks = list(sorted(ks))\n        d[cls._info_axis_name] = Index(ks)\n        return cls(**d)\n\n    def __getitem__(self, key):\n        key = com.apply_if_callable(key, self)\n\n        if isinstance(self._info_axis, MultiIndex):\n            return self._getitem_multilevel(key)\n        if not (is_list_like(key) or isinstance(key, slice)):\n            return super(Panel, self).__getitem__(key)\n        return self.loc[key]\n\n    def _getitem_multilevel(self, key):\n        info = self._info_axis\n        loc = info.get_loc(key)\n        if isinstance(loc, (slice, np.ndarray)):\n            new_index = info[loc]\n            result_index = maybe_droplevels(new_index, key)\n            slices = [loc] + [slice(None)] * (self._AXIS_LEN - 1)\n            new_values = self.values[slices]\n\n            d = self._construct_axes_dict(self._AXIS_ORDERS[1:])\n            d[self._info_axis_name] = result_index\n            result = self._constructor(new_values, **d)\n            return result\n        else:\n            return self._get_item_cache(key)\n\n    def _init_matrix(self, data, axes, dtype=None, copy=False):\n        values = self._prep_ndarray(self, data, copy=copy)\n\n        if dtype is not None:\n            try:\n                values = values.astype(dtype)\n            except Exception:\n                raise ValueError('failed to cast to '\n                                 '{datatype}'.format(datatype=dtype))\n\n        shape = values.shape\n        fixed_axes = []\n        for i, ax in enumerate(axes):\n            if ax is None:\n                ax = ibase.default_index(shape[i])\n            else:\n                ax = ensure_index(ax)\n            fixed_axes.append(ax)\n\n        return create_block_manager_from_blocks([values], fixed_axes)\n\n    # ----------------------------------------------------------------------\n    # Comparison methods\n\n    def _compare_constructor(self, other, func):\n        if not self._indexed_same(other):\n            raise Exception('Can only compare identically-labeled '\n                            'same type objects')\n\n        new_data = {}\n        for col in self._info_axis:\n            new_data[col] = func(self[col], other[col])\n\n        d = self._construct_axes_dict(copy=False)\n        return self._constructor(data=new_data, **d)\n\n    # ----------------------------------------------------------------------\n    # Magic methods\n\n    def __unicode__(self):\n        \"\"\"\n        Return a string representation for a particular Panel\n\n        Invoked by unicode(df) in py2 only.\n        Yields a Unicode String in both py2/py3.\n        \"\"\"\n\n        class_name = str(self.__class__)\n\n        dims = u('Dimensions: {dimensions}'.format(dimensions=' x '.join(\n            [\"{shape} ({axis})\".format(shape=shape, axis=axis) for axis, shape\n             in zip(self._AXIS_ORDERS, self.shape)])))\n\n        def axis_pretty(a):\n            v = getattr(self, a)\n            if len(v) > 0:\n                return u('{ax} axis: {x} to {y}'.format(ax=a.capitalize(),\n                                                        x=pprint_thing(v[0]),\n                                                        y=pprint_thing(v[-1])))\n            else:\n                return u('{ax} axis: None'.format(ax=a.capitalize()))\n\n        output = '\\n'.join(\n            [class_name, dims] + [axis_pretty(a) for a in self._AXIS_ORDERS])\n        return output\n\n    def _get_plane_axes_index(self, axis):\n        \"\"\"\n        Get my plane axes indexes: these are already\n        (as compared with higher level planes),\n        as we are returning a DataFrame axes indexes\n        \"\"\"\n        axis_name = self._get_axis_name(axis)\n\n        if axis_name == 'major_axis':\n            index = 'minor_axis'\n            columns = 'items'\n        if axis_name == 'minor_axis':\n            index = 'major_axis'\n            columns = 'items'\n        elif axis_name == 'items':\n            index = 'major_axis'\n            columns = 'minor_axis'\n\n        return index, columns\n\n    def _get_plane_axes(self, axis):\n        \"\"\"\n        Get my plane axes indexes: these are already\n        (as compared with higher level planes),\n        as we are returning a DataFrame axes\n        \"\"\"\n        return [self._get_axis(axi)\n                for axi in self._get_plane_axes_index(axis)]\n\n    fromDict = from_dict\n\n    def to_sparse(self, *args, **kwargs):\n        \"\"\"\n        NOT IMPLEMENTED: do not call this method, as sparsifying is not\n        supported for Panel objects and will raise an error.\n\n        Convert to SparsePanel\n        \"\"\"\n        raise NotImplementedError(\"sparsifying is not supported \"\n                                  \"for Panel objects\")\n\n    def to_excel(self, path, na_rep='', engine=None, **kwargs):\n        \"\"\"\n        Write each DataFrame in Panel to a separate excel sheet\n\n        Parameters\n        ----------\n        path : string or ExcelWriter object\n            File path or existing ExcelWriter\n        na_rep : string, default ''\n            Missing data representation\n        engine : string, default None\n            write engine to use - you can also set this via the options\n            ``io.excel.xlsx.writer``, ``io.excel.xls.writer``, and\n            ``io.excel.xlsm.writer``.\n\n        Other Parameters\n        ----------------\n        float_format : string, default None\n            Format string for floating point numbers\n        cols : sequence, optional\n            Columns to write\n        header : boolean or list of string, default True\n            Write out column names. If a list of string is given it is\n            assumed to be aliases for the column names\n        index : boolean, default True\n            Write row names (index)\n        index_label : string or sequence, default None\n            Column label for index column(s) if desired. If None is given, and\n            `header` and `index` are True, then the index names are used. A\n            sequence should be given if the DataFrame uses MultiIndex.\n        startrow : upper left cell row to dump data frame\n        startcol : upper left cell column to dump data frame\n\n        Notes\n        -----\n        Keyword arguments (and na_rep) are passed to the ``to_excel`` method\n        for each DataFrame written.\n        \"\"\"\n        from pandas.io.excel import ExcelWriter\n\n        if isinstance(path, compat.string_types):\n            writer = ExcelWriter(path, engine=engine)\n        else:\n            writer = path\n        kwargs['na_rep'] = na_rep\n\n        for item, df in self.iteritems():\n            name = str(item)\n            df.to_excel(writer, name, **kwargs)\n        writer.save()\n\n    def as_matrix(self):\n        self._consolidate_inplace()\n        return self._data.as_array()\n\n    # ----------------------------------------------------------------------\n    # Getting and setting elements\n\n    def get_value(self, *args, **kwargs):\n        \"\"\"Quickly retrieve single value at (item, major, minor) location\n\n        .. deprecated:: 0.21.0\n\n        Please use .at[] or .iat[] accessors.\n\n        Parameters\n        ----------\n        item : item label (panel item)\n        major : major axis label (panel item row)\n        minor : minor axis label (panel item column)\n        takeable : interpret the passed labels as indexers, default False\n\n        Returns\n        -------\n        value : scalar value\n        \"\"\"\n        warnings.warn(\"get_value is deprecated and will be removed \"\n                      \"in a future release. Please use \"\n                      \".at[] or .iat[] accessors instead\", FutureWarning,\n                      stacklevel=2)\n        return self._get_value(*args, **kwargs)\n\n    def _get_value(self, *args, **kwargs):\n        nargs = len(args)\n        nreq = self._AXIS_LEN\n\n        # require an arg for each axis\n        if nargs != nreq:\n            raise TypeError('There must be an argument for each axis, you gave'\n                            ' {0} args, but {1} are required'.format(nargs,\n                                                                     nreq))\n        takeable = kwargs.pop('takeable', None)\n\n        if kwargs:\n            raise TypeError('get_value() got an unexpected keyword '\n                            'argument \"{0}\"'.format(list(kwargs.keys())[0]))\n\n        if takeable is True:\n            lower = self._iget_item_cache(args[0])\n        else:\n            lower = self._get_item_cache(args[0])\n\n        return lower._get_value(*args[1:], takeable=takeable)\n    _get_value.__doc__ = get_value.__doc__\n\n    def set_value(self, *args, **kwargs):\n        \"\"\"Quickly set single value at (item, major, minor) location\n\n        .. deprecated:: 0.21.0\n\n        Please use .at[] or .iat[] accessors.\n\n        Parameters\n        ----------\n        item : item label (panel item)\n        major : major axis label (panel item row)\n        minor : minor axis label (panel item column)\n        value : scalar\n        takeable : interpret the passed labels as indexers, default False\n\n        Returns\n        -------\n        panel : Panel\n            If label combo is contained, will be reference to calling Panel,\n            otherwise a new object\n        \"\"\"\n        warnings.warn(\"set_value is deprecated and will be removed \"\n                      \"in a future release. Please use \"\n                      \".at[] or .iat[] accessors instead\", FutureWarning,\n                      stacklevel=2)\n        return self._set_value(*args, **kwargs)\n\n    def _set_value(self, *args, **kwargs):\n        # require an arg for each axis and the value\n        nargs = len(args)\n        nreq = self._AXIS_LEN + 1\n\n        if nargs != nreq:\n            raise TypeError('There must be an argument for each axis plus the '\n                            'value provided, you gave {0} args, but {1} are '\n                            'required'.format(nargs, nreq))\n        takeable = kwargs.pop('takeable', None)\n\n        if kwargs:\n            raise TypeError('set_value() got an unexpected keyword '\n                            'argument \"{0}\"'.format(list(kwargs.keys())[0]))\n\n        try:\n            if takeable is True:\n                lower = self._iget_item_cache(args[0])\n            else:\n                lower = self._get_item_cache(args[0])\n\n            lower._set_value(*args[1:], takeable=takeable)\n            return self\n        except KeyError:\n            axes = self._expand_axes(args)\n            d = self._construct_axes_dict_from(self, axes, copy=False)\n            result = self.reindex(**d)\n            args = list(args)\n            likely_dtype, args[-1] = infer_dtype_from_scalar(args[-1])\n            made_bigger = not np.array_equal(axes[0], self._info_axis)\n            # how to make this logic simpler?\n            if made_bigger:\n                maybe_cast_item(result, args[0], likely_dtype)\n\n            return result._set_value(*args)\n    _set_value.__doc__ = set_value.__doc__\n\n    def _box_item_values(self, key, values):\n        if self.ndim == values.ndim:\n            result = self._constructor(values)\n\n            # a dup selection will yield a full ndim\n            if result._get_axis(0).is_unique:\n                result = result[key]\n\n            return result\n\n        d = self._construct_axes_dict_for_slice(self._AXIS_ORDERS[1:])\n        return self._constructor_sliced(values, **d)\n\n    def __setitem__(self, key, value):\n        key = com.apply_if_callable(key, self)\n        shape = tuple(self.shape)\n        if isinstance(value, self._constructor_sliced):\n            value = value.reindex(\n                **self._construct_axes_dict_for_slice(self._AXIS_ORDERS[1:]))\n            mat = value.values\n        elif isinstance(value, np.ndarray):\n            if value.shape != shape[1:]:\n                raise ValueError('shape of value must be {0}, shape of given '\n                                 'object was {1}'.format(\n                                     shape[1:], tuple(map(int, value.shape))))\n            mat = np.asarray(value)\n        elif is_scalar(value):\n            mat = cast_scalar_to_array(shape[1:], value)\n        else:\n            raise TypeError('Cannot set item of '\n                            'type: {dtype!s}'.format(dtype=type(value)))\n\n        mat = mat.reshape(tuple([1]) + shape[1:])\n        NDFrame._set_item(self, key, mat)\n\n    def _unpickle_panel_compat(self, state):  # pragma: no cover\n        \"Unpickle the panel\"\n        from pandas.io.pickle import _unpickle_array\n\n        _unpickle = _unpickle_array\n        vals, items, major, minor = state\n\n        items = _unpickle(items)\n        major = _unpickle(major)\n        minor = _unpickle(minor)\n        values = _unpickle(vals)\n        wp = Panel(values, items, major, minor)\n        self._data = wp._data\n\n    def conform(self, frame, axis='items'):\n        \"\"\"\n        Conform input DataFrame to align with chosen axis pair.\n\n        Parameters\n        ----------\n        frame : DataFrame\n        axis : {'items', 'major', 'minor'}\n\n            Axis the input corresponds to. E.g., if axis='major', then\n            the frame's columns would be items, and the index would be\n            values of the minor axis\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        axes = self._get_plane_axes(axis)\n        return frame.reindex(**self._extract_axes_for_slice(self, axes))\n\n    def head(self, n=5):\n        raise NotImplementedError\n\n    def tail(self, n=5):\n        raise NotImplementedError\n\n    def round(self, decimals=0, *args, **kwargs):\n        \"\"\"\n        Round each value in Panel to a specified number of decimal places.\n\n        .. versionadded:: 0.18.0\n\n        Parameters\n        ----------\n        decimals : int\n            Number of decimal places to round to (default: 0).\n            If decimals is negative, it specifies the number of\n            positions to the left of the decimal point.\n\n        Returns\n        -------\n        Panel object\n\n        See Also\n        --------\n        numpy.around\n        \"\"\"\n        nv.validate_round(args, kwargs)\n\n        if is_integer(decimals):\n            result = np.apply_along_axis(np.round, 0, self.values)\n            return self._wrap_result(result, axis=0)\n        raise TypeError(\"decimals must be an integer\")\n\n    def _needs_reindex_multi(self, axes, method, level):\n        \"\"\" don't allow a multi reindex on Panel or above ndim \"\"\"\n        return False\n\n    def align(self, other, **kwargs):\n        raise NotImplementedError\n\n    def dropna(self, axis=0, how='any', inplace=False):\n        \"\"\"\n        Drop 2D from panel, holding passed axis constant\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Axis to hold constant. E.g. axis=1 will drop major_axis entries\n            having a certain amount of NA data\n        how : {'all', 'any'}, default 'any'\n            'any': one or more values are NA in the DataFrame along the\n            axis. For 'all' they all must be.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n\n        Returns\n        -------\n        dropped : Panel\n        \"\"\"\n        axis = self._get_axis_number(axis)\n\n        values = self.values\n        mask = notna(values)\n\n        for ax in reversed(sorted(set(range(self._AXIS_LEN)) - {axis})):\n            mask = mask.sum(ax)\n\n        per_slice = np.prod(values.shape[:axis] + values.shape[axis + 1:])\n\n        if how == 'all':\n            cond = mask > 0\n        else:\n            cond = mask == per_slice\n\n        new_ax = self._get_axis(axis)[cond]\n        result = self.reindex_axis(new_ax, axis=axis)\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result\n\n    def _combine(self, other, func, axis=0):\n        if isinstance(other, Panel):\n            return self._combine_panel(other, func)\n        elif isinstance(other, DataFrame):\n            return self._combine_frame(other, func, axis=axis)\n        elif is_scalar(other):\n            return self._combine_const(other, func)\n        else:\n            raise NotImplementedError(\n                \"{otype!s} is not supported in combine operation with \"\n                \"{selftype!s}\".format(otype=type(other), selftype=type(self)))\n\n    def _combine_const(self, other, func):\n        with np.errstate(all='ignore'):\n            new_values = func(self.values, other)\n        d = self._construct_axes_dict()\n        return self._constructor(new_values, **d)\n\n    def _combine_frame(self, other, func, axis=0):\n        index, columns = self._get_plane_axes(axis)\n        axis = self._get_axis_number(axis)\n\n        other = other.reindex(index=index, columns=columns)\n\n        with np.errstate(all='ignore'):\n            if axis == 0:\n                new_values = func(self.values, other.values)\n            elif axis == 1:\n                new_values = func(self.values.swapaxes(0, 1), other.values.T)\n                new_values = new_values.swapaxes(0, 1)\n            elif axis == 2:\n                new_values = func(self.values.swapaxes(0, 2), other.values)\n                new_values = new_values.swapaxes(0, 2)\n\n        return self._constructor(new_values, self.items, self.major_axis,\n                                 self.minor_axis)\n\n    def _combine_panel(self, other, func):\n        items = self.items.union(other.items)\n        major = self.major_axis.union(other.major_axis)\n        minor = self.minor_axis.union(other.minor_axis)\n\n        # could check that everything's the same size, but forget it\n        this = self.reindex(items=items, major=major, minor=minor)\n        other = other.reindex(items=items, major=major, minor=minor)\n\n        with np.errstate(all='ignore'):\n            result_values = func(this.values, other.values)\n\n        return self._constructor(result_values, items, major, minor)\n\n    def major_xs(self, key):\n        \"\"\"\n        Return slice of panel along major axis\n\n        Parameters\n        ----------\n        key : object\n            Major axis label\n\n        Returns\n        -------\n        y : DataFrame\n            index -> minor axis, columns -> items\n\n        Notes\n        -----\n        major_xs is only for getting, not setting values.\n\n        MultiIndex Slicers is a generic way to get/set values on any level or\n        levels and is a superset of major_xs functionality, see\n        :ref:`MultiIndex Slicers <advanced.mi_slicers>`\n        \"\"\"\n        return self.xs(key, axis=self._AXIS_LEN - 2)\n\n    def minor_xs(self, key):\n        \"\"\"\n        Return slice of panel along minor axis\n\n        Parameters\n        ----------\n        key : object\n            Minor axis label\n\n        Returns\n        -------\n        y : DataFrame\n            index -> major axis, columns -> items\n\n        Notes\n        -----\n        minor_xs is only for getting, not setting values.\n\n        MultiIndex Slicers is a generic way to get/set values on any level or\n        levels and is a superset of minor_xs functionality, see\n        :ref:`MultiIndex Slicers <advanced.mi_slicers>`\n        \"\"\"\n        return self.xs(key, axis=self._AXIS_LEN - 1)\n\n    def xs(self, key, axis=1):\n        \"\"\"\n        Return slice of panel along selected axis\n\n        Parameters\n        ----------\n        key : object\n            Label\n        axis : {'items', 'major', 'minor}, default 1/'major'\n\n        Returns\n        -------\n        y : ndim(self)-1\n\n        Notes\n        -----\n        xs is only for getting, not setting values.\n\n        MultiIndex Slicers is a generic way to get/set values on any level or\n        levels and is a superset of xs functionality, see\n        :ref:`MultiIndex Slicers <advanced.mi_slicers>`\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        if axis == 0:\n            return self[key]\n\n        self._consolidate_inplace()\n        axis_number = self._get_axis_number(axis)\n        new_data = self._data.xs(key, axis=axis_number, copy=False)\n        result = self._construct_return_type(new_data)\n        copy = new_data.is_mixed_type\n        result._set_is_copy(self, copy=copy)\n        return result\n\n    _xs = xs\n\n    def _ixs(self, i, axis=0):\n        \"\"\"\n        i : int, slice, or sequence of integers\n        axis : int\n        \"\"\"\n\n        ax = self._get_axis(axis)\n        key = ax[i]\n\n        # xs cannot handle a non-scalar key, so just reindex here\n        # if we have a multi-index and a single tuple, then its a reduction\n        # (GH 7516)\n        if not (isinstance(ax, MultiIndex) and isinstance(key, tuple)):\n            if is_list_like(key):\n                indexer = {self._get_axis_name(axis): key}\n                return self.reindex(**indexer)\n\n        # a reduction\n        if axis == 0:\n            values = self._data.iget(i)\n            return self._box_item_values(key, values)\n\n        # xs by position\n        self._consolidate_inplace()\n        new_data = self._data.xs(i, axis=axis, copy=True, takeable=True)\n        return self._construct_return_type(new_data)\n\n    def groupby(self, function, axis='major'):\n        \"\"\"\n        Group data on given axis, returning GroupBy object\n\n        Parameters\n        ----------\n        function : callable\n            Mapping function for chosen access\n        axis : {'major', 'minor', 'items'}, default 'major'\n\n        Returns\n        -------\n        grouped : PanelGroupBy\n        \"\"\"\n        from pandas.core.groupby import PanelGroupBy\n        axis = self._get_axis_number(axis)\n        return PanelGroupBy(self, function, axis=axis)\n\n    def to_frame(self, filter_observations=True):\n        \"\"\"\n        Transform wide format into long (stacked) format as DataFrame whose\n        columns are the Panel's items and whose index is a MultiIndex formed\n        of the Panel's major and minor axes.\n\n        Parameters\n        ----------\n        filter_observations : boolean, default True\n            Drop (major, minor) pairs without a complete set of observations\n            across all the items\n\n        Returns\n        -------\n        y : DataFrame\n        \"\"\"\n        _, N, K = self.shape\n\n        if filter_observations:\n            # shaped like the return DataFrame\n            mask = notna(self.values).all(axis=0)\n            # size = mask.sum()\n            selector = mask.ravel()\n        else:\n            # size = N * K\n            selector = slice(None, None)\n\n        data = {}\n        for item in self.items:\n            data[item] = self[item].values.ravel()[selector]\n\n        def construct_multi_parts(idx, n_repeat, n_shuffle=1):\n            # Replicates and shuffles MultiIndex, returns individual attributes\n            labels = [np.repeat(x, n_repeat) for x in idx.labels]\n            # Assumes that each label is divisible by n_shuffle\n            labels = [x.reshape(n_shuffle, -1).ravel(order='F')\n                      for x in labels]\n            labels = [x[selector] for x in labels]\n            levels = idx.levels\n            names = idx.names\n            return labels, levels, names\n\n        def construct_index_parts(idx, major=True):\n            levels = [idx]\n            if major:\n                labels = [np.arange(N).repeat(K)[selector]]\n                names = idx.name or 'major'\n            else:\n                labels = np.arange(K).reshape(1, K)[np.zeros(N, dtype=int)]\n                labels = [labels.ravel()[selector]]\n                names = idx.name or 'minor'\n            names = [names]\n            return labels, levels, names\n\n        if isinstance(self.major_axis, MultiIndex):\n            major_labels, major_levels, major_names = construct_multi_parts(\n                self.major_axis, n_repeat=K)\n        else:\n            major_labels, major_levels, major_names = construct_index_parts(\n                self.major_axis)\n\n        if isinstance(self.minor_axis, MultiIndex):\n            minor_labels, minor_levels, minor_names = construct_multi_parts(\n                self.minor_axis, n_repeat=N, n_shuffle=K)\n        else:\n            minor_labels, minor_levels, minor_names = construct_index_parts(\n                self.minor_axis, major=False)\n\n        levels = major_levels + minor_levels\n        labels = major_labels + minor_labels\n        names = major_names + minor_names\n\n        index = MultiIndex(levels=levels, labels=labels, names=names,\n                           verify_integrity=False)\n\n        return DataFrame(data, index=index, columns=self.items)\n\n    def apply(self, func, axis='major', **kwargs):\n        \"\"\"\n        Applies function along axis (or axes) of the Panel\n\n        Parameters\n        ----------\n        func : function\n            Function to apply to each combination of 'other' axes\n            e.g. if axis = 'items', the combination of major_axis/minor_axis\n            will each be passed as a Series; if axis = ('items', 'major'),\n            DataFrames of items & major axis will be passed\n        axis : {'items', 'minor', 'major'}, or {0, 1, 2}, or a tuple with two\n            axes\n        Additional keyword arguments will be passed as keywords to the function\n\n        Examples\n        --------\n\n        Returns a Panel with the square root of each element\n\n        >>> p = pd.Panel(np.random.rand(4, 3, 2))  # doctest: +SKIP\n        >>> p.apply(np.sqrt)\n\n        Equivalent to p.sum(1), returning a DataFrame\n\n        >>> p.apply(lambda x: x.sum(), axis=1)  # doctest: +SKIP\n\n        Equivalent to previous:\n\n        >>> p.apply(lambda x: x.sum(), axis='major')  # doctest: +SKIP\n\n        Return the shapes of each DataFrame over axis 2 (i.e the shapes of\n        items x major), as a Series\n\n        >>> p.apply(lambda x: x.shape, axis=(0,1))  # doctest: +SKIP\n\n        Returns\n        -------\n        result : Panel, DataFrame, or Series\n        \"\"\"\n\n        if kwargs and not isinstance(func, np.ufunc):\n            f = lambda x: func(x, **kwargs)\n        else:\n            f = func\n\n        # 2d-slabs\n        if isinstance(axis, (tuple, list)) and len(axis) == 2:\n            return self._apply_2d(f, axis=axis)\n\n        axis = self._get_axis_number(axis)\n\n        # try ufunc like\n        if isinstance(f, np.ufunc):\n            try:\n                with np.errstate(all='ignore'):\n                    result = np.apply_along_axis(func, axis, self.values)\n                return self._wrap_result(result, axis=axis)\n            except (AttributeError):\n                pass\n\n        # 1d\n        return self._apply_1d(f, axis=axis)\n\n    def _apply_1d(self, func, axis):\n\n        axis_name = self._get_axis_name(axis)\n        ndim = self.ndim\n        values = self.values\n\n        # iter thru the axes\n        slice_axis = self._get_axis(axis)\n        slice_indexer = [0] * (ndim - 1)\n        indexer = np.zeros(ndim, 'O')\n        indlist = list(range(ndim))\n        indlist.remove(axis)\n        indexer[axis] = slice(None, None)\n        indexer.put(indlist, slice_indexer)\n        planes = [self._get_axis(axi) for axi in indlist]\n        shape = np.array(self.shape).take(indlist)\n\n        # all the iteration points\n        points = cartesian_product(planes)\n\n        results = []\n        for i in range(np.prod(shape)):\n\n            # construct the object\n            pts = tuple(p[i] for p in points)\n            indexer.put(indlist, slice_indexer)\n\n            obj = Series(values[tuple(indexer)], index=slice_axis, name=pts)\n            result = func(obj)\n\n            results.append(result)\n\n            # increment the indexer\n            slice_indexer[-1] += 1\n            n = -1\n            while (slice_indexer[n] >= shape[n]) and (n > (1 - ndim)):\n                slice_indexer[n - 1] += 1\n                slice_indexer[n] = 0\n                n -= 1\n\n        # empty object\n        if not len(results):\n            return self._constructor(**self._construct_axes_dict())\n\n        # same ndim as current\n        if isinstance(results[0], Series):\n            arr = np.vstack([r.values for r in results])\n            arr = arr.T.reshape(tuple([len(slice_axis)] + list(shape)))\n            tranp = np.array([axis] + indlist).argsort()\n            arr = arr.transpose(tuple(list(tranp)))\n            return self._constructor(arr, **self._construct_axes_dict())\n\n        # ndim-1 shape\n        results = np.array(results).reshape(shape)\n        if results.ndim == 2 and axis_name != self._info_axis_name:\n            results = results.T\n            planes = planes[::-1]\n        return self._construct_return_type(results, planes)\n\n    def _apply_2d(self, func, axis):\n        \"\"\" handle 2-d slices, equiv to iterating over the other axis \"\"\"\n\n        ndim = self.ndim\n        axis = [self._get_axis_number(a) for a in axis]\n\n        # construct slabs, in 2-d this is a DataFrame result\n        indexer_axis = list(range(ndim))\n        for a in axis:\n            indexer_axis.remove(a)\n        indexer_axis = indexer_axis[0]\n\n        slicer = [slice(None, None)] * ndim\n        ax = self._get_axis(indexer_axis)\n\n        results = []\n        for i, e in enumerate(ax):\n            slicer[indexer_axis] = i\n            sliced = self.iloc[tuple(slicer)]\n\n            obj = func(sliced)\n            results.append((e, obj))\n\n        return self._construct_return_type(dict(results))\n\n    def _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,\n                filter_type=None, **kwds):\n        if numeric_only:\n            raise NotImplementedError('Panel.{0} does not implement '\n                                      'numeric_only.'.format(name))\n\n        if axis is None and filter_type == 'bool':\n            # labels = None\n            # constructor = None\n            axis_number = None\n            axis_name = None\n        else:\n            # TODO: Make other agg func handle axis=None properly\n            axis = self._get_axis_number(axis)\n            # labels = self._get_agg_axis(axis)\n            # constructor = self._constructor\n            axis_name = self._get_axis_name(axis)\n            axis_number = self._get_axis_number(axis_name)\n\n        f = lambda x: op(x, axis=axis_number, skipna=skipna, **kwds)\n\n        with np.errstate(all='ignore'):\n            result = f(self.values)\n\n        if axis is None and filter_type == 'bool':\n            return np.bool_(result)\n        axes = self._get_plane_axes(axis_name)\n        if result.ndim == 2 and axis_name != self._info_axis_name:\n            result = result.T\n\n        return self._construct_return_type(result, axes)\n\n    def _construct_return_type(self, result, axes=None):\n        \"\"\" return the type for the ndim of the result \"\"\"\n        ndim = getattr(result, 'ndim', None)\n\n        # need to assume they are the same\n        if ndim is None:\n            if isinstance(result, dict):\n                ndim = getattr(list(compat.itervalues(result))[0], 'ndim', 0)\n\n                # have a dict, so top-level is +1 dim\n                if ndim != 0:\n                    ndim += 1\n\n        # scalar\n        if ndim == 0:\n            return Series(result)\n\n        # same as self\n        elif self.ndim == ndim:\n            # return the construction dictionary for these axes\n            if axes is None:\n                return self._constructor(result)\n            return self._constructor(result, **self._construct_axes_dict())\n\n        # sliced\n        elif self.ndim == ndim + 1:\n            if axes is None:\n                return self._constructor_sliced(result)\n            return self._constructor_sliced(\n                result, **self._extract_axes_for_slice(self, axes))\n\n        raise ValueError('invalid _construct_return_type [self->{self}] '\n                         '[result->{result}]'.format(self=self, result=result))\n\n    def _wrap_result(self, result, axis):\n        axis = self._get_axis_name(axis)\n        axes = self._get_plane_axes(axis)\n        if result.ndim == 2 and axis != self._info_axis_name:\n            result = result.T\n\n        return self._construct_return_type(result, axes)\n\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(NDFrame.reindex.__doc__)\n    def reindex(self, *args, **kwargs):\n        major = kwargs.pop(\"major\", None)\n        minor = kwargs.pop('minor', None)\n\n        if major is not None:\n            if kwargs.get(\"major_axis\"):\n                raise TypeError(\"Cannot specify both 'major' and 'major_axis'\")\n            kwargs['major_axis'] = major\n        if minor is not None:\n            if kwargs.get(\"minor_axis\"):\n                raise TypeError(\"Cannot specify both 'minor' and 'minor_axis'\")\n\n            kwargs['minor_axis'] = minor\n        axes = validate_axis_style_args(self, args, kwargs, 'labels',\n                                        'reindex')\n        kwargs.update(axes)\n        kwargs.pop('axis', None)\n        kwargs.pop('labels', None)\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", FutureWarning)\n            # do not warn about constructing Panel when reindexing\n            result = super(Panel, self).reindex(**kwargs)\n        return result\n\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(NDFrame.rename.__doc__)\n    def rename(self, items=None, major_axis=None, minor_axis=None, **kwargs):\n        major_axis = (major_axis if major_axis is not None else\n                      kwargs.pop('major', None))\n        minor_axis = (minor_axis if minor_axis is not None else\n                      kwargs.pop('minor', None))\n        return super(Panel, self).rename(items=items, major_axis=major_axis,\n                                         minor_axis=minor_axis, **kwargs)\n\n    @Appender(_shared_docs['reindex_axis'] % _shared_doc_kwargs)\n    def reindex_axis(self, labels, axis=0, method=None, level=None, copy=True,\n                     limit=None, fill_value=np.nan):\n        return super(Panel, self).reindex_axis(labels=labels, axis=axis,\n                                               method=method, level=level,\n                                               copy=copy, limit=limit,\n                                               fill_value=fill_value)\n\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(NDFrame.transpose.__doc__)\n    def transpose(self, *args, **kwargs):\n        # check if a list of axes was passed in instead as a\n        # single *args element\n        if (len(args) == 1 and hasattr(args[0], '__iter__') and\n                not is_string_like(args[0])):\n            axes = args[0]\n        else:\n            axes = args\n\n        if 'axes' in kwargs and axes:\n            raise TypeError(\"transpose() got multiple values for \"\n                            \"keyword argument 'axes'\")\n        elif not axes:\n            axes = kwargs.pop('axes', ())\n\n        return super(Panel, self).transpose(*axes, **kwargs)\n\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(NDFrame.fillna.__doc__)\n    def fillna(self, value=None, method=None, axis=None, inplace=False,\n               limit=None, downcast=None, **kwargs):\n        return super(Panel, self).fillna(value=value, method=method, axis=axis,\n                                         inplace=inplace, limit=limit,\n                                         downcast=downcast, **kwargs)\n\n    def count(self, axis='major'):\n        \"\"\"\n        Return number of observations over requested axis.\n\n        Parameters\n        ----------\n        axis : {'items', 'major', 'minor'} or {0, 1, 2}\n\n        Returns\n        -------\n        count : DataFrame\n        \"\"\"\n        i = self._get_axis_number(axis)\n\n        values = self.values\n        mask = np.isfinite(values)\n        result = mask.sum(axis=i, dtype='int64')\n\n        return self._wrap_result(result, axis)\n\n    def shift(self, periods=1, freq=None, axis='major'):\n        \"\"\"\n        Shift index by desired number of periods with an optional time freq.\n        The shifted data will not include the dropped periods and the\n        shifted axis will be smaller than the original. This is different\n        from the behavior of DataFrame.shift()\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to move, can be positive or negative\n        freq : DateOffset, timedelta, or time rule string, optional\n        axis : {'items', 'major', 'minor'} or {0, 1, 2}\n\n        Returns\n        -------\n        shifted : Panel\n        \"\"\"\n        if freq:\n            return self.tshift(periods, freq, axis=axis)\n\n        return super(Panel, self).slice_shift(periods, axis=axis)\n\n    def tshift(self, periods=1, freq=None, axis='major'):\n        return super(Panel, self).tshift(periods, freq, axis)\n\n    def join(self, other, how='left', lsuffix='', rsuffix=''):\n        \"\"\"\n        Join items with other Panel either on major and minor axes column\n\n        Parameters\n        ----------\n        other : Panel or list of Panels\n            Index should be similar to one of the columns in this one\n        how : {'left', 'right', 'outer', 'inner'}\n            How to handle indexes of the two objects. Default: 'left'\n            for joining on index, None otherwise\n            * left: use calling frame's index\n            * right: use input frame's index\n            * outer: form union of indexes\n            * inner: use intersection of indexes\n        lsuffix : string\n            Suffix to use from left frame's overlapping columns\n        rsuffix : string\n            Suffix to use from right frame's overlapping columns\n\n        Returns\n        -------\n        joined : Panel\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        if isinstance(other, Panel):\n            join_major, join_minor = self._get_join_index(other, how)\n            this = self.reindex(major=join_major, minor=join_minor)\n            other = other.reindex(major=join_major, minor=join_minor)\n            merged_data = this._data.merge(other._data, lsuffix, rsuffix)\n            return self._constructor(merged_data)\n        else:\n            if lsuffix or rsuffix:\n                raise ValueError('Suffixes not supported when passing '\n                                 'multiple panels')\n\n            if how == 'left':\n                how = 'outer'\n                join_axes = [self.major_axis, self.minor_axis]\n            elif how == 'right':\n                raise ValueError('Right join not supported with multiple '\n                                 'panels')\n            else:\n                join_axes = None\n\n            return concat([self] + list(other), axis=0, join=how,\n                          join_axes=join_axes, verify_integrity=True)\n\n    @deprecate_kwarg(old_arg_name='raise_conflict', new_arg_name='errors',\n                     mapping={False: 'ignore', True: 'raise'})\n    def update(self, other, join='left', overwrite=True, filter_func=None,\n               errors='ignore'):\n        \"\"\"\n        Modify Panel in place using non-NA values from other Panel.\n\n        May also use object coercible to Panel. Will align on items.\n\n        Parameters\n        ----------\n        other : Panel, or object coercible to Panel\n            The object from which the caller will be udpated.\n        join : {'left', 'right', 'outer', 'inner'}, default 'left'\n            How individual DataFrames are joined.\n        overwrite : bool, default True\n            If True then overwrite values for common keys in the calling Panel.\n        filter_func : callable(1d-array) -> 1d-array<bool>, default None\n            Can choose to replace values other than NA. Return True for values\n            that should be updated.\n        errors : {'raise', 'ignore'}, default 'ignore'\n            If 'raise', will raise an error if a DataFrame and other both.\n\n            .. versionchanged :: 0.24.0\n               Changed from `raise_conflict=False|True`\n               to `errors='ignore'|'raise'`.\n\n        See Also\n        --------\n        DataFrame.update : Similar method for DataFrames.\n        dict.update : Similar method for dictionaries.\n        \"\"\"\n\n        if not isinstance(other, self._constructor):\n            other = self._constructor(other)\n\n        axis_name = self._info_axis_name\n        axis_values = self._info_axis\n        other = other.reindex(**{axis_name: axis_values})\n\n        for frame in axis_values:\n            self[frame].update(other[frame], join=join, overwrite=overwrite,\n                               filter_func=filter_func, errors=errors)\n\n    def _get_join_index(self, other, how):\n        if how == 'left':\n            join_major, join_minor = self.major_axis, self.minor_axis\n        elif how == 'right':\n            join_major, join_minor = other.major_axis, other.minor_axis\n        elif how == 'inner':\n            join_major = self.major_axis.intersection(other.major_axis)\n            join_minor = self.minor_axis.intersection(other.minor_axis)\n        elif how == 'outer':\n            join_major = self.major_axis.union(other.major_axis)\n            join_minor = self.minor_axis.union(other.minor_axis)\n        return join_major, join_minor\n\n    # miscellaneous data creation\n    @staticmethod\n    def _extract_axes(self, data, axes, **kwargs):\n        \"\"\" return a list of the axis indices \"\"\"\n        return [self._extract_axis(self, data, axis=i, **kwargs)\n                for i, a in enumerate(axes)]\n\n    @staticmethod\n    def _extract_axes_for_slice(self, axes):\n        \"\"\" return the slice dictionary for these axes \"\"\"\n        return {self._AXIS_SLICEMAP[i]: a for i, a in\n                zip(self._AXIS_ORDERS[self._AXIS_LEN - len(axes):], axes)}\n\n    @staticmethod\n    def _prep_ndarray(self, values, copy=True):\n        if not isinstance(values, np.ndarray):\n            values = np.asarray(values)\n            # NumPy strings are a pain, convert to object\n            if issubclass(values.dtype.type, compat.string_types):\n                values = np.array(values, dtype=object, copy=True)\n        else:\n            if copy:\n                values = values.copy()\n        if values.ndim != self._AXIS_LEN:\n            raise ValueError(\"The number of dimensions required is {0}, \"\n                             \"but the number of dimensions of the \"\n                             \"ndarray given was {1}\".format(self._AXIS_LEN,\n                                                            values.ndim))\n        return values\n\n    @staticmethod\n    def _homogenize_dict(self, frames, intersect=True, dtype=None):\n        \"\"\"\n        Conform set of _constructor_sliced-like objects to either\n        an intersection of indices / columns or a union.\n\n        Parameters\n        ----------\n        frames : dict\n        intersect : boolean, default True\n\n        Returns\n        -------\n        dict of aligned results & indices\n        \"\"\"\n\n        result = dict()\n        # caller differs dict/ODict, preserved type\n        if isinstance(frames, OrderedDict):\n            result = OrderedDict()\n\n        adj_frames = OrderedDict()\n        for k, v in compat.iteritems(frames):\n            if isinstance(v, dict):\n                adj_frames[k] = self._constructor_sliced(v)\n            else:\n                adj_frames[k] = v\n\n        axes = self._AXIS_ORDERS[1:]\n        axes_dict = {a: ax for a, ax in zip(axes, self._extract_axes(\n                     self, adj_frames, axes, intersect=intersect))}\n\n        reindex_dict = {self._AXIS_SLICEMAP[a]: axes_dict[a] for a in axes}\n        reindex_dict['copy'] = False\n        for key, frame in compat.iteritems(adj_frames):\n            if frame is not None:\n                result[key] = frame.reindex(**reindex_dict)\n            else:\n                result[key] = None\n\n        axes_dict['data'] = result\n        axes_dict['dtype'] = dtype\n        return axes_dict\n\n    @staticmethod\n    def _extract_axis(self, data, axis=0, intersect=False):\n\n        index = None\n        if len(data) == 0:\n            index = Index([])\n        elif len(data) > 0:\n            raw_lengths = []\n\n        have_raw_arrays = False\n        have_frames = False\n\n        for v in data.values():\n            if isinstance(v, self._constructor_sliced):\n                have_frames = True\n            elif v is not None:\n                have_raw_arrays = True\n                raw_lengths.append(v.shape[axis])\n\n        if have_frames:\n            # we want the \"old\" behavior here, of sorting only\n            # 1. we're doing a union (intersect=False)\n            # 2. the indices are not aligned.\n            index = _get_objs_combined_axis(data.values(), axis=axis,\n                                            intersect=intersect, sort=None)\n\n        if have_raw_arrays:\n            lengths = list(set(raw_lengths))\n            if len(lengths) > 1:\n                raise ValueError('ndarrays must match shape on '\n                                 'axis {ax}'.format(ax=axis))\n\n            if have_frames:\n                if lengths[0] != len(index):\n                    raise AssertionError('Length of data and index must match')\n            else:\n                index = Index(np.arange(lengths[0]))\n\n        if index is None:\n            index = Index([])\n\n        return ensure_index(index)\n\n    def sort_values(self, *args, **kwargs):\n        \"\"\"\n        NOT IMPLEMENTED: do not call this method, as sorting values is not\n        supported for Panel objects and will raise an error.\n        \"\"\"\n        super(Panel, self).sort_values(*args, **kwargs)\n\n\nPanel._setup_axes(axes=['items', 'major_axis', 'minor_axis'], info_axis=0,\n                  stat_axis=1, aliases={'major': 'major_axis',\n                                        'minor': 'minor_axis'},\n                  slicers={'major_axis': 'index',\n                           'minor_axis': 'columns'},\n                  docs={})\n\nops.add_special_arithmetic_methods(Panel)\nops.add_flex_arithmetic_methods(Panel)\nPanel._add_numeric_operations()\n"
    },
    {
      "filename": "pandas/core/resample.py",
      "content": "import copy\nfrom datetime import timedelta\nfrom textwrap import dedent\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import lib\nfrom pandas._libs.tslibs import NaT, Timestamp\nfrom pandas._libs.tslibs.period import IncompatibleFrequency\nimport pandas.compat as compat\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import Appender, Substitution\n\nfrom pandas.core.dtypes.generic import ABCDataFrame, ABCSeries\n\nimport pandas as pd\nimport pandas.core.algorithms as algos\nfrom pandas.core.generic import _shared_docs\nfrom pandas.core.groupby.base import GroupByMixin\nfrom pandas.core.groupby.generic import PanelGroupBy, SeriesGroupBy\nfrom pandas.core.groupby.groupby import (\n    GroupBy, _GroupBy, _pipe_template, groupby)\nfrom pandas.core.groupby.grouper import Grouper\nfrom pandas.core.groupby.ops import BinGrouper\nfrom pandas.core.indexes.datetimes import DatetimeIndex, date_range\nfrom pandas.core.indexes.period import PeriodIndex\nfrom pandas.core.indexes.timedeltas import TimedeltaIndex\n\nfrom pandas.tseries.frequencies import is_subperiod, is_superperiod, to_offset\nfrom pandas.tseries.offsets import (\n    DateOffset, Day, Nano, Tick, delta_to_nanoseconds)\n\n_shared_docs_kwargs = dict()\n\n\nclass Resampler(_GroupBy):\n\n    \"\"\"\n    Class for resampling datetimelike data, a groupby-like operation.\n    See aggregate, transform, and apply functions on this object.\n\n    It's easiest to use obj.resample(...) to use Resampler.\n\n    Parameters\n    ----------\n    obj : pandas object\n    groupby : a TimeGrouper object\n    axis : int, default 0\n    kind : str or None\n        'period', 'timestamp' to override default index treatement\n\n    Notes\n    -----\n    After resampling, see aggregate, apply, and transform functions.\n\n    Returns\n    -------\n    a Resampler of the appropriate type\n    \"\"\"\n\n    # to the groupby descriptor\n    _attributes = ['freq', 'axis', 'closed', 'label', 'convention',\n                   'loffset', 'base', 'kind']\n\n    def __init__(self, obj, groupby=None, axis=0, kind=None, **kwargs):\n        self.groupby = groupby\n        self.keys = None\n        self.sort = True\n        self.axis = axis\n        self.kind = kind\n        self.squeeze = False\n        self.group_keys = True\n        self.as_index = True\n        self.exclusions = set()\n        self.binner = None\n        self.grouper = None\n\n        if self.groupby is not None:\n            self.groupby._set_grouper(self._convert_obj(obj), sort=True)\n\n    def __unicode__(self):\n        \"\"\" provide a nice str repr of our rolling object \"\"\"\n        attrs = [\"{k}={v}\".format(k=k, v=getattr(self.groupby, k))\n                 for k in self._attributes if\n                 getattr(self.groupby, k, None) is not None]\n        return \"{klass} [{attrs}]\".format(klass=self.__class__.__name__,\n                                          attrs=', '.join(attrs))\n\n    def __getattr__(self, attr):\n        if attr in self._internal_names_set:\n            return object.__getattribute__(self, attr)\n        if attr in self._attributes:\n            return getattr(self.groupby, attr)\n        if attr in self.obj:\n            return self[attr]\n\n        return object.__getattribute__(self, attr)\n\n    def __iter__(self):\n        \"\"\"\n        Resampler iterator\n\n        Returns\n        -------\n        Generator yielding sequence of (name, subsetted object)\n        for each group\n\n        See Also\n        --------\n        GroupBy.__iter__\n        \"\"\"\n        self._set_binner()\n        return super(Resampler, self).__iter__()\n\n    @property\n    def obj(self):\n        return self.groupby.obj\n\n    @property\n    def ax(self):\n        return self.groupby.ax\n\n    @property\n    def _typ(self):\n        \"\"\" masquerade for compat as a Series or a DataFrame \"\"\"\n        if isinstance(self._selected_obj, pd.Series):\n            return 'series'\n        return 'dataframe'\n\n    @property\n    def _from_selection(self):\n        \"\"\" is the resampling from a DataFrame column or MultiIndex level \"\"\"\n        # upsampling and PeriodIndex resampling do not work\n        # with selection, this state used to catch and raise an error\n        return (self.groupby is not None and\n                (self.groupby.key is not None or\n                 self.groupby.level is not None))\n\n    def _convert_obj(self, obj):\n        \"\"\"\n        provide any conversions for the object in order to correctly handle\n\n        Parameters\n        ----------\n        obj : the object to be resampled\n\n        Returns\n        -------\n        obj : converted object\n        \"\"\"\n        obj = obj._consolidate()\n        return obj\n\n    def _get_binner_for_time(self):\n        raise AbstractMethodError(self)\n\n    def _set_binner(self):\n        \"\"\"\n        setup our binners\n        cache these as we are an immutable object\n        \"\"\"\n\n        if self.binner is None:\n            self.binner, self.grouper = self._get_binner()\n\n    def _get_binner(self):\n        \"\"\"\n        create the BinGrouper, assume that self.set_grouper(obj)\n        has already been called\n        \"\"\"\n\n        binner, bins, binlabels = self._get_binner_for_time()\n        bin_grouper = BinGrouper(bins, binlabels, indexer=self.groupby.indexer)\n        return binner, bin_grouper\n\n    def _assure_grouper(self):\n        \"\"\" make sure that we are creating our binner & grouper \"\"\"\n        self._set_binner()\n\n    @Substitution(klass='Resampler',\n                  versionadded='.. versionadded:: 0.23.0',\n                  examples=\"\"\"\n>>> df = pd.DataFrame({'A': [1, 2, 3, 4]},\n...                   index=pd.date_range('2012-08-02', periods=4))\n>>> df\n            A\n2012-08-02  1\n2012-08-03  2\n2012-08-04  3\n2012-08-05  4\n\nTo get the difference between each 2-day period's maximum and minimum value in\none pass, you can do\n\n>>> df.resample('2D').pipe(lambda x: x.max() - x.min())\n            A\n2012-08-02  1\n2012-08-04  1\"\"\")\n    @Appender(_pipe_template)\n    def pipe(self, func, *args, **kwargs):\n        return super(Resampler, self).pipe(func, *args, **kwargs)\n\n    _agg_doc = dedent(\"\"\"\n    Examples\n    --------\n    >>> s = pd.Series([1,2,3,4,5],\n                      index=pd.date_range('20130101', periods=5,freq='s'))\n    2013-01-01 00:00:00    1\n    2013-01-01 00:00:01    2\n    2013-01-01 00:00:02    3\n    2013-01-01 00:00:03    4\n    2013-01-01 00:00:04    5\n    Freq: S, dtype: int64\n\n    >>> r = s.resample('2s')\n    DatetimeIndexResampler [freq=<2 * Seconds>, axis=0, closed=left,\n                            label=left, convention=start, base=0]\n\n    >>> r.agg(np.sum)\n    2013-01-01 00:00:00    3\n    2013-01-01 00:00:02    7\n    2013-01-01 00:00:04    5\n    Freq: 2S, dtype: int64\n\n    >>> r.agg(['sum','mean','max'])\n                         sum  mean  max\n    2013-01-01 00:00:00    3   1.5    2\n    2013-01-01 00:00:02    7   3.5    4\n    2013-01-01 00:00:04    5   5.0    5\n\n    >>> r.agg({'result' : lambda x: x.mean() / x.std(),\n               'total' : np.sum})\n                         total    result\n    2013-01-01 00:00:00      3  2.121320\n    2013-01-01 00:00:02      7  4.949747\n    2013-01-01 00:00:04      5       NaN\n\n    See Also\n    --------\n    pandas.DataFrame.groupby.aggregate\n    pandas.DataFrame.resample.transform\n    pandas.DataFrame.aggregate\n    \"\"\")\n\n    @Appender(_agg_doc)\n    @Appender(_shared_docs['aggregate'] % dict(\n        klass='DataFrame',\n        versionadded='',\n        axis=''))\n    def aggregate(self, func, *args, **kwargs):\n\n        self._set_binner()\n        result, how = self._aggregate(func, *args, **kwargs)\n        if result is None:\n            how = func\n            grouper = None\n            result = self._groupby_and_aggregate(how,\n                                                 grouper,\n                                                 *args,\n                                                 **kwargs)\n\n        result = self._apply_loffset(result)\n        return result\n\n    agg = aggregate\n    apply = aggregate\n\n    def transform(self, arg, *args, **kwargs):\n        \"\"\"\n        Call function producing a like-indexed Series on each group and return\n        a Series with the transformed values\n\n        Parameters\n        ----------\n        func : function\n            To apply to each group. Should return a Series with the same index\n\n        Examples\n        --------\n        >>> resampled.transform(lambda x: (x - x.mean()) / x.std())\n\n        Returns\n        -------\n        transformed : Series\n        \"\"\"\n        return self._selected_obj.groupby(self.groupby).transform(\n            arg, *args, **kwargs)\n\n    def _downsample(self, f):\n        raise AbstractMethodError(self)\n\n    def _upsample(self, f, limit=None, fill_value=None):\n        raise AbstractMethodError(self)\n\n    def _gotitem(self, key, ndim, subset=None):\n        \"\"\"\n        sub-classes to define\n        return a sliced object\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : 1,2\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n        self._set_binner()\n        grouper = self.grouper\n        if subset is None:\n            subset = self.obj\n        grouped = groupby(subset, by=None, grouper=grouper, axis=self.axis)\n\n        # try the key selection\n        try:\n            return grouped[key]\n        except KeyError:\n            return grouped\n\n    def _groupby_and_aggregate(self, how, grouper=None, *args, **kwargs):\n        \"\"\" re-evaluate the obj with a groupby aggregation \"\"\"\n\n        if grouper is None:\n            self._set_binner()\n            grouper = self.grouper\n\n        obj = self._selected_obj\n\n        try:\n            grouped = groupby(obj, by=None, grouper=grouper, axis=self.axis)\n        except TypeError:\n\n            # panel grouper\n            grouped = PanelGroupBy(obj, grouper=grouper, axis=self.axis)\n\n        try:\n            if isinstance(obj, ABCDataFrame) and compat.callable(how):\n                # Check if the function is reducing or not.\n                result = grouped._aggregate_item_by_item(how, *args, **kwargs)\n            else:\n                result = grouped.aggregate(how, *args, **kwargs)\n        except Exception:\n\n            # we have a non-reducing function\n            # try to evaluate\n            result = grouped.apply(how, *args, **kwargs)\n\n        result = self._apply_loffset(result)\n        return self._wrap_result(result)\n\n    def _apply_loffset(self, result):\n        \"\"\"\n        if loffset is set, offset the result index\n\n        This is NOT an idempotent routine, it will be applied\n        exactly once to the result.\n\n        Parameters\n        ----------\n        result : Series or DataFrame\n            the result of resample\n        \"\"\"\n\n        needs_offset = (\n            isinstance(self.loffset, (DateOffset, timedelta,\n                                      np.timedelta64)) and\n            isinstance(result.index, DatetimeIndex) and\n            len(result.index) > 0\n        )\n\n        if needs_offset:\n            result.index = result.index + self.loffset\n\n        self.loffset = None\n        return result\n\n    def _get_resampler_for_grouping(self, groupby, **kwargs):\n        \"\"\" return the correct class for resampling with groupby \"\"\"\n        return self._resampler_for_grouping(self, groupby=groupby, **kwargs)\n\n    def _wrap_result(self, result):\n        \"\"\" potentially wrap any results \"\"\"\n        if isinstance(result, ABCSeries) and self._selection is not None:\n            result.name = self._selection\n\n        if isinstance(result, ABCSeries) and result.empty:\n            obj = self.obj\n            result.index = obj.index._shallow_copy(freq=to_offset(self.freq))\n            result.name = getattr(obj, 'name', None)\n\n        return result\n\n    def pad(self, limit=None):\n        \"\"\"\n        Forward fill the values\n\n        Parameters\n        ----------\n        limit : integer, optional\n            limit of how many values to fill\n\n        Returns\n        -------\n        an upsampled Series\n\n        See Also\n        --------\n        Series.fillna\n        DataFrame.fillna\n        \"\"\"\n        return self._upsample('pad', limit=limit)\n    ffill = pad\n\n    def nearest(self, limit=None):\n        \"\"\"\n        Resample by using the nearest value.\n\n        When resampling data, missing values may appear (e.g., when the\n        resampling frequency is higher than the original frequency).\n        The `nearest` method will replace ``NaN`` values that appeared in\n        the resampled data with the value from the nearest member of the\n        sequence, based on the index value.\n        Missing values that existed in the original data will not be modified.\n        If `limit` is given, fill only this many values in each direction for\n        each of the original values.\n\n        Parameters\n        ----------\n        limit : int, optional\n            Limit of how many values to fill.\n\n            .. versionadded:: 0.21.0\n\n        Returns\n        -------\n        Series or DataFrame\n            An upsampled Series or DataFrame with ``NaN`` values filled with\n            their nearest value.\n\n        See Also\n        --------\n        backfill : Backward fill the new missing values in the resampled data.\n        pad : Forward fill ``NaN`` values.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2],\n        ...               index=pd.date_range('20180101',\n        ...                                   periods=2,\n        ...                                   freq='1h'))\n        >>> s\n        2018-01-01 00:00:00    1\n        2018-01-01 01:00:00    2\n        Freq: H, dtype: int64\n\n        >>> s.resample('15min').nearest()\n        2018-01-01 00:00:00    1\n        2018-01-01 00:15:00    1\n        2018-01-01 00:30:00    2\n        2018-01-01 00:45:00    2\n        2018-01-01 01:00:00    2\n        Freq: 15T, dtype: int64\n\n        Limit the number of upsampled values imputed by the nearest:\n\n        >>> s.resample('15min').nearest(limit=1)\n        2018-01-01 00:00:00    1.0\n        2018-01-01 00:15:00    1.0\n        2018-01-01 00:30:00    NaN\n        2018-01-01 00:45:00    2.0\n        2018-01-01 01:00:00    2.0\n        Freq: 15T, dtype: float64\n        \"\"\"\n        return self._upsample('nearest', limit=limit)\n\n    def backfill(self, limit=None):\n        \"\"\"\n        Backward fill the new missing values in the resampled data.\n\n        In statistics, imputation is the process of replacing missing data with\n        substituted values [1]_. When resampling data, missing values may\n        appear (e.g., when the resampling frequency is higher than the original\n        frequency). The backward fill will replace NaN values that appeared in\n        the resampled data with the next value in the original sequence.\n        Missing values that existed in the original data will not be modified.\n\n        Parameters\n        ----------\n        limit : integer, optional\n            Limit of how many values to fill.\n\n        Returns\n        -------\n        Series, DataFrame\n            An upsampled Series or DataFrame with backward filled NaN values.\n\n        See Also\n        --------\n        bfill : Alias of backfill.\n        fillna : Fill NaN values using the specified method, which can be\n            'backfill'.\n        nearest : Fill NaN values with nearest neighbor starting from center.\n        pad : Forward fill NaN values.\n        pandas.Series.fillna : Fill NaN values in the Series using the\n            specified method, which can be 'backfill'.\n        pandas.DataFrame.fillna : Fill NaN values in the DataFrame using the\n            specified method, which can be 'backfill'.\n\n        References\n        ----------\n        .. [1] https://en.wikipedia.org/wiki/Imputation_(statistics)\n\n        Examples\n        --------\n\n        Resampling a Series:\n\n        >>> s = pd.Series([1, 2, 3],\n        ...               index=pd.date_range('20180101', periods=3, freq='h'))\n        >>> s\n        2018-01-01 00:00:00    1\n        2018-01-01 01:00:00    2\n        2018-01-01 02:00:00    3\n        Freq: H, dtype: int64\n\n        >>> s.resample('30min').backfill()\n        2018-01-01 00:00:00    1\n        2018-01-01 00:30:00    2\n        2018-01-01 01:00:00    2\n        2018-01-01 01:30:00    3\n        2018-01-01 02:00:00    3\n        Freq: 30T, dtype: int64\n\n        >>> s.resample('15min').backfill(limit=2)\n        2018-01-01 00:00:00    1.0\n        2018-01-01 00:15:00    NaN\n        2018-01-01 00:30:00    2.0\n        2018-01-01 00:45:00    2.0\n        2018-01-01 01:00:00    2.0\n        2018-01-01 01:15:00    NaN\n        2018-01-01 01:30:00    3.0\n        2018-01-01 01:45:00    3.0\n        2018-01-01 02:00:00    3.0\n        Freq: 15T, dtype: float64\n\n        Resampling a DataFrame that has missing values:\n\n        >>> df = pd.DataFrame({'a': [2, np.nan, 6], 'b': [1, 3, 5]},\n        ...                   index=pd.date_range('20180101', periods=3,\n        ...                                       freq='h'))\n        >>> df\n                               a  b\n        2018-01-01 00:00:00  2.0  1\n        2018-01-01 01:00:00  NaN  3\n        2018-01-01 02:00:00  6.0  5\n\n        >>> df.resample('30min').backfill()\n                               a  b\n        2018-01-01 00:00:00  2.0  1\n        2018-01-01 00:30:00  NaN  3\n        2018-01-01 01:00:00  NaN  3\n        2018-01-01 01:30:00  6.0  5\n        2018-01-01 02:00:00  6.0  5\n\n        >>> df.resample('15min').backfill(limit=2)\n                               a    b\n        2018-01-01 00:00:00  2.0  1.0\n        2018-01-01 00:15:00  NaN  NaN\n        2018-01-01 00:30:00  NaN  3.0\n        2018-01-01 00:45:00  NaN  3.0\n        2018-01-01 01:00:00  NaN  3.0\n        2018-01-01 01:15:00  NaN  NaN\n        2018-01-01 01:30:00  6.0  5.0\n        2018-01-01 01:45:00  6.0  5.0\n        2018-01-01 02:00:00  6.0  5.0\n        \"\"\"\n        return self._upsample('backfill', limit=limit)\n    bfill = backfill\n\n    def fillna(self, method, limit=None):\n        \"\"\"\n        Fill missing values introduced by upsampling.\n\n        In statistics, imputation is the process of replacing missing data with\n        substituted values [1]_. When resampling data, missing values may\n        appear (e.g., when the resampling frequency is higher than the original\n        frequency).\n\n        Missing values that existed in the original data will\n        not be modified.\n\n        Parameters\n        ----------\n        method : {'pad', 'backfill', 'ffill', 'bfill', 'nearest'}\n            Method to use for filling holes in resampled data\n\n            * 'pad' or 'ffill': use previous valid observation to fill gap\n              (forward fill).\n            * 'backfill' or 'bfill': use next valid observation to fill gap.\n            * 'nearest': use nearest valid observation to fill gap.\n\n        limit : integer, optional\n            Limit of how many consecutive missing values to fill.\n\n        Returns\n        -------\n        Series or DataFrame\n            An upsampled Series or DataFrame with missing values filled.\n\n        See Also\n        --------\n        backfill : Backward fill NaN values in the resampled data.\n        pad : Forward fill NaN values in the resampled data.\n        nearest : Fill NaN values in the resampled data\n            with nearest neighbor starting from center.\n        interpolate : Fill NaN values using interpolation.\n        pandas.Series.fillna : Fill NaN values in the Series using the\n            specified method, which can be 'bfill' and 'ffill'.\n        pandas.DataFrame.fillna : Fill NaN values in the DataFrame using the\n            specified method, which can be 'bfill' and 'ffill'.\n\n        Examples\n        --------\n        Resampling a Series:\n\n        >>> s = pd.Series([1, 2, 3],\n        ...               index=pd.date_range('20180101', periods=3, freq='h'))\n        >>> s\n        2018-01-01 00:00:00    1\n        2018-01-01 01:00:00    2\n        2018-01-01 02:00:00    3\n        Freq: H, dtype: int64\n\n        Without filling the missing values you get:\n\n        >>> s.resample(\"30min\").asfreq()\n        2018-01-01 00:00:00    1.0\n        2018-01-01 00:30:00    NaN\n        2018-01-01 01:00:00    2.0\n        2018-01-01 01:30:00    NaN\n        2018-01-01 02:00:00    3.0\n        Freq: 30T, dtype: float64\n\n        >>> s.resample('30min').fillna(\"backfill\")\n        2018-01-01 00:00:00    1\n        2018-01-01 00:30:00    2\n        2018-01-01 01:00:00    2\n        2018-01-01 01:30:00    3\n        2018-01-01 02:00:00    3\n        Freq: 30T, dtype: int64\n\n        >>> s.resample('15min').fillna(\"backfill\", limit=2)\n        2018-01-01 00:00:00    1.0\n        2018-01-01 00:15:00    NaN\n        2018-01-01 00:30:00    2.0\n        2018-01-01 00:45:00    2.0\n        2018-01-01 01:00:00    2.0\n        2018-01-01 01:15:00    NaN\n        2018-01-01 01:30:00    3.0\n        2018-01-01 01:45:00    3.0\n        2018-01-01 02:00:00    3.0\n        Freq: 15T, dtype: float64\n\n        >>> s.resample('30min').fillna(\"pad\")\n        2018-01-01 00:00:00    1\n        2018-01-01 00:30:00    1\n        2018-01-01 01:00:00    2\n        2018-01-01 01:30:00    2\n        2018-01-01 02:00:00    3\n        Freq: 30T, dtype: int64\n\n        >>> s.resample('30min').fillna(\"nearest\")\n        2018-01-01 00:00:00    1\n        2018-01-01 00:30:00    2\n        2018-01-01 01:00:00    2\n        2018-01-01 01:30:00    3\n        2018-01-01 02:00:00    3\n        Freq: 30T, dtype: int64\n\n        Missing values present before the upsampling are not affected.\n\n        >>> sm = pd.Series([1, None, 3],\n        ...               index=pd.date_range('20180101', periods=3, freq='h'))\n        >>> sm\n        2018-01-01 00:00:00    1.0\n        2018-01-01 01:00:00    NaN\n        2018-01-01 02:00:00    3.0\n        Freq: H, dtype: float64\n\n        >>> sm.resample('30min').fillna('backfill')\n        2018-01-01 00:00:00    1.0\n        2018-01-01 00:30:00    NaN\n        2018-01-01 01:00:00    NaN\n        2018-01-01 01:30:00    3.0\n        2018-01-01 02:00:00    3.0\n        Freq: 30T, dtype: float64\n\n        >>> sm.resample('30min').fillna('pad')\n        2018-01-01 00:00:00    1.0\n        2018-01-01 00:30:00    1.0\n        2018-01-01 01:00:00    NaN\n        2018-01-01 01:30:00    NaN\n        2018-01-01 02:00:00    3.0\n        Freq: 30T, dtype: float64\n\n        >>> sm.resample('30min').fillna('nearest')\n        2018-01-01 00:00:00    1.0\n        2018-01-01 00:30:00    NaN\n        2018-01-01 01:00:00    NaN\n        2018-01-01 01:30:00    3.0\n        2018-01-01 02:00:00    3.0\n        Freq: 30T, dtype: float64\n\n        DataFrame resampling is done column-wise. All the same options are\n        available.\n\n        >>> df = pd.DataFrame({'a': [2, np.nan, 6], 'b': [1, 3, 5]},\n        ...                   index=pd.date_range('20180101', periods=3,\n        ...                                       freq='h'))\n        >>> df\n                               a  b\n        2018-01-01 00:00:00  2.0  1\n        2018-01-01 01:00:00  NaN  3\n        2018-01-01 02:00:00  6.0  5\n\n        >>> df.resample('30min').fillna(\"bfill\")\n                               a  b\n        2018-01-01 00:00:00  2.0  1\n        2018-01-01 00:30:00  NaN  3\n        2018-01-01 01:00:00  NaN  3\n        2018-01-01 01:30:00  6.0  5\n        2018-01-01 02:00:00  6.0  5\n\n        References\n        ----------\n        .. [1] https://en.wikipedia.org/wiki/Imputation_(statistics)\n        \"\"\"\n        return self._upsample(method, limit=limit)\n\n    @Appender(_shared_docs['interpolate'] % _shared_docs_kwargs)\n    def interpolate(self, method='linear', axis=0, limit=None, inplace=False,\n                    limit_direction='forward', limit_area=None,\n                    downcast=None, **kwargs):\n        \"\"\"\n        Interpolate values according to different methods.\n\n        .. versionadded:: 0.18.1\n        \"\"\"\n        result = self._upsample(None)\n        return result.interpolate(method=method, axis=axis, limit=limit,\n                                  inplace=inplace,\n                                  limit_direction=limit_direction,\n                                  limit_area=limit_area,\n                                  downcast=downcast, **kwargs)\n\n    def asfreq(self, fill_value=None):\n        \"\"\"\n        return the values at the new freq,\n        essentially a reindex\n\n        Parameters\n        ----------\n        fill_value : scalar, optional\n            Value to use for missing values, applied during upsampling (note\n            this does not fill NaNs that already were present).\n\n            .. versionadded:: 0.20.0\n\n        See Also\n        --------\n        Series.asfreq\n        DataFrame.asfreq\n        \"\"\"\n        return self._upsample('asfreq', fill_value=fill_value)\n\n    def std(self, ddof=1, *args, **kwargs):\n        \"\"\"\n        Compute standard deviation of groups, excluding missing values\n\n        Parameters\n        ----------\n        ddof : integer, default 1\n        degrees of freedom\n        \"\"\"\n        nv.validate_resampler_func('std', args, kwargs)\n        return self._downsample('std', ddof=ddof)\n\n    def var(self, ddof=1, *args, **kwargs):\n        \"\"\"\n        Compute variance of groups, excluding missing values\n\n        Parameters\n        ----------\n        ddof : integer, default 1\n        degrees of freedom\n        \"\"\"\n        nv.validate_resampler_func('var', args, kwargs)\n        return self._downsample('var', ddof=ddof)\n\n    @Appender(GroupBy.size.__doc__)\n    def size(self):\n        # It's a special case as higher level does return\n        # a copy of 0-len objects. GH14962\n        result = self._downsample('size')\n        if not len(self.ax) and isinstance(self._selected_obj, ABCDataFrame):\n            result = pd.Series([], index=result.index, dtype='int64')\n        return result\n\n    def quantile(self, q=0.5, **kwargs):\n        \"\"\"\n        Return value at the given quantile.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n\n        See Also\n        --------\n        Series.quantile\n        DataFrame.quantile\n        DataFrameGroupBy.quantile\n        \"\"\"\n        return self._downsample('quantile', q=q, **kwargs)\n\n\n# downsample methods\nfor method in ['sum', 'prod']:\n\n    def f(self, _method=method, min_count=0, *args, **kwargs):\n        nv.validate_resampler_func(_method, args, kwargs)\n        return self._downsample(_method, min_count=min_count)\n    f.__doc__ = getattr(GroupBy, method).__doc__\n    setattr(Resampler, method, f)\n\n\n# downsample methods\nfor method in ['min', 'max', 'first', 'last', 'mean', 'sem',\n               'median', 'ohlc']:\n\n    def f(self, _method=method, *args, **kwargs):\n        nv.validate_resampler_func(_method, args, kwargs)\n        return self._downsample(_method)\n    f.__doc__ = getattr(GroupBy, method).__doc__\n    setattr(Resampler, method, f)\n\n# groupby & aggregate methods\nfor method in ['count']:\n    def f(self, _method=method):\n        return self._downsample(_method)\n    f.__doc__ = getattr(GroupBy, method).__doc__\n    setattr(Resampler, method, f)\n\n# series only methods\nfor method in ['nunique']:\n    def f(self, _method=method):\n        return self._downsample(_method)\n    f.__doc__ = getattr(SeriesGroupBy, method).__doc__\n    setattr(Resampler, method, f)\n\n\ndef _maybe_process_deprecations(r, how=None, fill_method=None, limit=None):\n    \"\"\" potentially we might have a deprecation warning, show it\n    but call the appropriate methods anyhow \"\"\"\n\n    if how is not None:\n\n        # .resample(..., how='sum')\n        if isinstance(how, compat.string_types):\n            method = \"{0}()\".format(how)\n\n            # .resample(..., how=lambda x: ....)\n        else:\n            method = \".apply(<func>)\"\n\n        # if we have both a how and fill_method, then show\n        # the following warning\n        if fill_method is None:\n            warnings.warn(\"how in .resample() is deprecated\\n\"\n                          \"the new syntax is \"\n                          \".resample(...).{method}\".format(\n                              method=method),\n                          FutureWarning, stacklevel=3)\n        r = r.aggregate(how)\n\n    if fill_method is not None:\n\n        # show the prior function call\n        method = '.' + method if how is not None else ''\n\n        args = \"limit={0}\".format(limit) if limit is not None else \"\"\n        warnings.warn(\"fill_method is deprecated to .resample()\\n\"\n                      \"the new syntax is .resample(...){method}\"\n                      \".{fill_method}({args})\".format(\n                          method=method,\n                          fill_method=fill_method,\n                          args=args),\n                      FutureWarning, stacklevel=3)\n\n        if how is not None:\n            r = getattr(r, fill_method)(limit=limit)\n        else:\n            r = r.aggregate(fill_method, limit=limit)\n\n    return r\n\n\nclass _GroupByMixin(GroupByMixin):\n    \"\"\" provide the groupby facilities \"\"\"\n\n    def __init__(self, obj, *args, **kwargs):\n\n        parent = kwargs.pop('parent', None)\n        groupby = kwargs.pop('groupby', None)\n        if parent is None:\n            parent = obj\n\n        # initialize our GroupByMixin object with\n        # the resampler attributes\n        for attr in self._attributes:\n            setattr(self, attr, kwargs.get(attr, getattr(parent, attr)))\n\n        super(_GroupByMixin, self).__init__(None)\n        self._groupby = groupby\n        self._groupby.mutated = True\n        self._groupby.grouper.mutated = True\n        self.groupby = copy.copy(parent.groupby)\n\n    def _apply(self, f, grouper=None, *args, **kwargs):\n        \"\"\"\n        dispatch to _upsample; we are stripping all of the _upsample kwargs and\n        performing the original function call on the grouped object\n        \"\"\"\n\n        def func(x):\n            x = self._shallow_copy(x, groupby=self.groupby)\n\n            if isinstance(f, compat.string_types):\n                return getattr(x, f)(**kwargs)\n\n            return x.apply(f, *args, **kwargs)\n\n        result = self._groupby.apply(func)\n        return self._wrap_result(result)\n\n    _upsample = _apply\n    _downsample = _apply\n    _groupby_and_aggregate = _apply\n\n\nclass DatetimeIndexResampler(Resampler):\n\n    @property\n    def _resampler_for_grouping(self):\n        return DatetimeIndexResamplerGroupby\n\n    def _get_binner_for_time(self):\n\n        # this is how we are actually creating the bins\n        if self.kind == 'period':\n            return self.groupby._get_time_period_bins(self.ax)\n        return self.groupby._get_time_bins(self.ax)\n\n    def _downsample(self, how, **kwargs):\n        \"\"\"\n        Downsample the cython defined function\n\n        Parameters\n        ----------\n        how : string / cython mapped function\n        **kwargs : kw args passed to how function\n        \"\"\"\n        self._set_binner()\n        how = self._is_cython_func(how) or how\n        ax = self.ax\n        obj = self._selected_obj\n\n        if not len(ax):\n            # reset to the new freq\n            obj = obj.copy()\n            obj.index.freq = self.freq\n            return obj\n\n        # do we have a regular frequency\n        if ax.freq is not None or ax.inferred_freq is not None:\n\n            if len(self.grouper.binlabels) > len(ax) and how is None:\n\n                # let's do an asfreq\n                return self.asfreq()\n\n        # we are downsampling\n        # we want to call the actual grouper method here\n        result = obj.groupby(\n            self.grouper, axis=self.axis).aggregate(how, **kwargs)\n\n        result = self._apply_loffset(result)\n        return self._wrap_result(result)\n\n    def _adjust_binner_for_upsample(self, binner):\n        \"\"\"\n        Adjust our binner when upsampling.\n        The range of a new index should not be outside specified range\n        \"\"\"\n        if self.closed == 'right':\n            binner = binner[1:]\n        else:\n            binner = binner[:-1]\n        return binner\n\n    def _upsample(self, method, limit=None, fill_value=None):\n        \"\"\"\n        method : string {'backfill', 'bfill', 'pad',\n            'ffill', 'asfreq'} method for upsampling\n        limit : int, default None\n            Maximum size gap to fill when reindexing\n        fill_value : scalar, default None\n            Value to use for missing values\n\n        See Also\n        --------\n        .fillna\n\n        \"\"\"\n        self._set_binner()\n        if self.axis:\n            raise AssertionError('axis must be 0')\n        if self._from_selection:\n            raise ValueError(\"Upsampling from level= or on= selection\"\n                             \" is not supported, use .set_index(...)\"\n                             \" to explicitly set index to\"\n                             \" datetime-like\")\n\n        ax = self.ax\n        obj = self._selected_obj\n        binner = self.binner\n        res_index = self._adjust_binner_for_upsample(binner)\n\n        # if we have the same frequency as our axis, then we are equal sampling\n        if limit is None and to_offset(ax.inferred_freq) == self.freq:\n            result = obj.copy()\n            result.index = res_index\n        else:\n            result = obj.reindex(res_index, method=method,\n                                 limit=limit, fill_value=fill_value)\n\n        result = self._apply_loffset(result)\n        return self._wrap_result(result)\n\n    def _wrap_result(self, result):\n        result = super(DatetimeIndexResampler, self)._wrap_result(result)\n\n        # we may have a different kind that we were asked originally\n        # convert if needed\n        if self.kind == 'period' and not isinstance(result.index, PeriodIndex):\n            result.index = result.index.to_period(self.freq)\n        return result\n\n\nclass DatetimeIndexResamplerGroupby(_GroupByMixin, DatetimeIndexResampler):\n    \"\"\"\n    Provides a resample of a groupby implementation\n\n    .. versionadded:: 0.18.1\n\n    \"\"\"\n    @property\n    def _constructor(self):\n        return DatetimeIndexResampler\n\n\nclass PeriodIndexResampler(DatetimeIndexResampler):\n\n    @property\n    def _resampler_for_grouping(self):\n        return PeriodIndexResamplerGroupby\n\n    def _get_binner_for_time(self):\n        if self.kind == 'timestamp':\n            return super(PeriodIndexResampler, self)._get_binner_for_time()\n        return self.groupby._get_period_bins(self.ax)\n\n    def _convert_obj(self, obj):\n        obj = super(PeriodIndexResampler, self)._convert_obj(obj)\n\n        if self._from_selection:\n            # see GH 14008, GH 12871\n            msg = (\"Resampling from level= or on= selection\"\n                   \" with a PeriodIndex is not currently supported,\"\n                   \" use .set_index(...) to explicitly set index\")\n            raise NotImplementedError(msg)\n\n        if self.loffset is not None:\n            # Cannot apply loffset/timedelta to PeriodIndex -> convert to\n            # timestamps\n            self.kind = 'timestamp'\n\n        # convert to timestamp\n        if self.kind == 'timestamp':\n            obj = obj.to_timestamp(how=self.convention)\n\n        return obj\n\n    def _downsample(self, how, **kwargs):\n        \"\"\"\n        Downsample the cython defined function\n\n        Parameters\n        ----------\n        how : string / cython mapped function\n        **kwargs : kw args passed to how function\n        \"\"\"\n\n        # we may need to actually resample as if we are timestamps\n        if self.kind == 'timestamp':\n            return super(PeriodIndexResampler, self)._downsample(how, **kwargs)\n\n        how = self._is_cython_func(how) or how\n        ax = self.ax\n\n        if is_subperiod(ax.freq, self.freq):\n            # Downsampling\n            return self._groupby_and_aggregate(how, grouper=self.grouper,\n                                               **kwargs)\n        elif is_superperiod(ax.freq, self.freq):\n            if how == 'ohlc':\n                # GH #13083\n                # upsampling to subperiods is handled as an asfreq, which works\n                # for pure aggregating/reducing methods\n                # OHLC reduces along the time dimension, but creates multiple\n                # values for each period -> handle by _groupby_and_aggregate()\n                return self._groupby_and_aggregate(how, grouper=self.grouper)\n            return self.asfreq()\n        elif ax.freq == self.freq:\n            return self.asfreq()\n\n        raise IncompatibleFrequency(\n            'Frequency {} cannot be resampled to {}, as they are not '\n            'sub or super periods'.format(ax.freq, self.freq))\n\n    def _upsample(self, method, limit=None, fill_value=None):\n        \"\"\"\n        method : string {'backfill', 'bfill', 'pad', 'ffill'}\n            method for upsampling\n        limit : int, default None\n            Maximum size gap to fill when reindexing\n        fill_value : scalar, default None\n            Value to use for missing values\n\n        See Also\n        --------\n        .fillna\n\n        \"\"\"\n\n        # we may need to actually resample as if we are timestamps\n        if self.kind == 'timestamp':\n            return super(PeriodIndexResampler, self)._upsample(\n                method, limit=limit, fill_value=fill_value)\n\n        self._set_binner()\n        ax = self.ax\n        obj = self.obj\n        new_index = self.binner\n\n        # Start vs. end of period\n        memb = ax.asfreq(self.freq, how=self.convention)\n\n        # Get the fill indexer\n        indexer = memb.get_indexer(new_index, method=method, limit=limit)\n        return self._wrap_result(_take_new_index(\n            obj, indexer, new_index, axis=self.axis))\n\n\nclass PeriodIndexResamplerGroupby(_GroupByMixin, PeriodIndexResampler):\n    \"\"\"\n    Provides a resample of a groupby implementation\n\n    .. versionadded:: 0.18.1\n\n    \"\"\"\n    @property\n    def _constructor(self):\n        return PeriodIndexResampler\n\n\nclass TimedeltaIndexResampler(DatetimeIndexResampler):\n\n    @property\n    def _resampler_for_grouping(self):\n        return TimedeltaIndexResamplerGroupby\n\n    def _get_binner_for_time(self):\n        return self.groupby._get_time_delta_bins(self.ax)\n\n    def _adjust_binner_for_upsample(self, binner):\n        \"\"\"\n        Adjust our binner when upsampling.\n        The range of a new index is allowed to be greater than original range\n        so we don't need to change the length of a binner, GH 13022\n        \"\"\"\n        return binner\n\n\nclass TimedeltaIndexResamplerGroupby(_GroupByMixin, TimedeltaIndexResampler):\n    \"\"\"\n    Provides a resample of a groupby implementation\n\n    .. versionadded:: 0.18.1\n\n    \"\"\"\n    @property\n    def _constructor(self):\n        return TimedeltaIndexResampler\n\n\ndef resample(obj, kind=None, **kwds):\n    \"\"\" create a TimeGrouper and return our resampler \"\"\"\n    tg = TimeGrouper(**kwds)\n    return tg._get_resampler(obj, kind=kind)\n\n\nresample.__doc__ = Resampler.__doc__\n\n\ndef get_resampler_for_grouping(groupby, rule, how=None, fill_method=None,\n                               limit=None, kind=None, **kwargs):\n    \"\"\" return our appropriate resampler when grouping as well \"\"\"\n\n    # .resample uses 'on' similar to how .groupby uses 'key'\n    kwargs['key'] = kwargs.pop('on', None)\n\n    tg = TimeGrouper(freq=rule, **kwargs)\n    resampler = tg._get_resampler(groupby.obj, kind=kind)\n    r = resampler._get_resampler_for_grouping(groupby=groupby)\n    return _maybe_process_deprecations(r,\n                                       how=how,\n                                       fill_method=fill_method,\n                                       limit=limit)\n\n\nclass TimeGrouper(Grouper):\n    \"\"\"\n    Custom groupby class for time-interval grouping\n\n    Parameters\n    ----------\n    freq : pandas date offset or offset alias for identifying bin edges\n    closed : closed end of interval; 'left' or 'right'\n    label : interval boundary to use for labeling; 'left' or 'right'\n    convention : {'start', 'end', 'e', 's'}\n        If axis is PeriodIndex\n    \"\"\"\n    _attributes = Grouper._attributes + ('closed', 'label', 'how',\n                                         'loffset', 'kind', 'convention',\n                                         'base')\n\n    def __init__(self, freq='Min', closed=None, label=None, how='mean',\n                 axis=0, fill_method=None, limit=None, loffset=None,\n                 kind=None, convention=None, base=0, **kwargs):\n        # Check for correctness of the keyword arguments which would\n        # otherwise silently use the default if misspelled\n        if label not in {None, 'left', 'right'}:\n            raise ValueError('Unsupported value {} for `label`'.format(label))\n        if closed not in {None, 'left', 'right'}:\n            raise ValueError('Unsupported value {} for `closed`'.format(\n                closed))\n        if convention not in {None, 'start', 'end', 'e', 's'}:\n            raise ValueError('Unsupported value {} for `convention`'\n                             .format(convention))\n\n        freq = to_offset(freq)\n\n        end_types = {'M', 'A', 'Q', 'BM', 'BA', 'BQ', 'W'}\n        rule = freq.rule_code\n        if (rule in end_types or\n                ('-' in rule and rule[:rule.find('-')] in end_types)):\n            if closed is None:\n                closed = 'right'\n            if label is None:\n                label = 'right'\n        else:\n            if closed is None:\n                closed = 'left'\n            if label is None:\n                label = 'left'\n\n        self.closed = closed\n        self.label = label\n        self.kind = kind\n\n        self.convention = convention or 'E'\n        self.convention = self.convention.lower()\n\n        if isinstance(loffset, compat.string_types):\n            loffset = to_offset(loffset)\n        self.loffset = loffset\n\n        self.how = how\n        self.fill_method = fill_method\n        self.limit = limit\n        self.base = base\n\n        # always sort time groupers\n        kwargs['sort'] = True\n\n        super(TimeGrouper, self).__init__(freq=freq, axis=axis, **kwargs)\n\n    def _get_resampler(self, obj, kind=None):\n        \"\"\"\n        return my resampler or raise if we have an invalid axis\n\n        Parameters\n        ----------\n        obj : input object\n        kind : string, optional\n            'period','timestamp','timedelta' are valid\n\n        Returns\n        -------\n        a Resampler\n\n        Raises\n        ------\n        TypeError if incompatible axis\n\n        \"\"\"\n        self._set_grouper(obj)\n\n        ax = self.ax\n        if isinstance(ax, DatetimeIndex):\n            return DatetimeIndexResampler(obj,\n                                          groupby=self,\n                                          kind=kind,\n                                          axis=self.axis)\n        elif isinstance(ax, PeriodIndex) or kind == 'period':\n            return PeriodIndexResampler(obj,\n                                        groupby=self,\n                                        kind=kind,\n                                        axis=self.axis)\n        elif isinstance(ax, TimedeltaIndex):\n            return TimedeltaIndexResampler(obj,\n                                           groupby=self,\n                                           axis=self.axis)\n\n        raise TypeError(\"Only valid with DatetimeIndex, \"\n                        \"TimedeltaIndex or PeriodIndex, \"\n                        \"but got an instance of %r\" % type(ax).__name__)\n\n    def _get_grouper(self, obj, validate=True):\n        # create the resampler and return our binner\n        r = self._get_resampler(obj)\n        r._set_binner()\n        return r.binner, r.grouper, r.obj\n\n    def _get_time_bins(self, ax):\n        if not isinstance(ax, DatetimeIndex):\n            raise TypeError('axis must be a DatetimeIndex, but got '\n                            'an instance of %r' % type(ax).__name__)\n\n        if len(ax) == 0:\n            binner = labels = DatetimeIndex(\n                data=[], freq=self.freq, name=ax.name)\n            return binner, [], labels\n\n        first, last = _get_range_edges(ax.min(), ax.max(), self.freq,\n                                       closed=self.closed,\n                                       base=self.base)\n        tz = ax.tz\n        # GH #12037\n        # use first/last directly instead of call replace() on them\n        # because replace() will swallow the nanosecond part\n        # thus last bin maybe slightly before the end if the end contains\n        # nanosecond part and lead to `Values falls after last bin` error\n        binner = labels = DatetimeIndex(freq=self.freq,\n                                        start=first,\n                                        end=last,\n                                        tz=tz,\n                                        name=ax.name)\n\n        # GH 15549\n        # In edge case of tz-aware resapmling binner last index can be\n        # less than the last variable in data object, this happens because of\n        # DST time change\n        if len(binner) > 1 and binner[-1] < last:\n            extra_date_range = pd.date_range(binner[-1], last + self.freq,\n                                             freq=self.freq, tz=tz,\n                                             name=ax.name)\n            binner = labels = binner.append(extra_date_range[1:])\n\n        # a little hack\n        trimmed = False\n        if (len(binner) > 2 and binner[-2] == last and\n                self.closed == 'right'):\n\n            binner = binner[:-1]\n            trimmed = True\n\n        ax_values = ax.asi8\n        binner, bin_edges = self._adjust_bin_edges(binner, ax_values)\n\n        # general version, knowing nothing about relative frequencies\n        bins = lib.generate_bins_dt64(\n            ax_values, bin_edges, self.closed, hasnans=ax.hasnans)\n\n        if self.closed == 'right':\n            labels = binner\n            if self.label == 'right':\n                labels = labels[1:]\n            elif not trimmed:\n                labels = labels[:-1]\n        else:\n            if self.label == 'right':\n                labels = labels[1:]\n            elif not trimmed:\n                labels = labels[:-1]\n\n        if ax.hasnans:\n            binner = binner.insert(0, NaT)\n            labels = labels.insert(0, NaT)\n\n        # if we end up with more labels than bins\n        # adjust the labels\n        # GH4076\n        if len(bins) < len(labels):\n            labels = labels[:len(bins)]\n\n        return binner, bins, labels\n\n    def _adjust_bin_edges(self, binner, ax_values):\n        # Some hacks for > daily data, see #1471, #1458, #1483\n\n        if self.freq != 'D' and is_superperiod(self.freq, 'D'):\n            if self.closed == 'right':\n                # GH 21459, GH 9119: Adjust the bins relative to the wall time\n                bin_edges = binner.tz_localize(None)\n                bin_edges = bin_edges + timedelta(1) - Nano(1)\n                bin_edges = bin_edges.tz_localize(binner.tz).asi8\n            else:\n                bin_edges = binner.asi8\n\n            # intraday values on last day\n            if bin_edges[-2] > ax_values.max():\n                bin_edges = bin_edges[:-1]\n                binner = binner[:-1]\n        else:\n            bin_edges = binner.asi8\n        return binner, bin_edges\n\n    def _get_time_delta_bins(self, ax):\n        if not isinstance(ax, TimedeltaIndex):\n            raise TypeError('axis must be a TimedeltaIndex, but got '\n                            'an instance of %r' % type(ax).__name__)\n\n        if not len(ax):\n            binner = labels = TimedeltaIndex(\n                data=[], freq=self.freq, name=ax.name)\n            return binner, [], labels\n\n        start, end = ax.min(), ax.max()\n        labels = binner = TimedeltaIndex(start=start,\n                                         end=end,\n                                         freq=self.freq,\n                                         name=ax.name)\n\n        end_stamps = labels + self.freq\n        bins = ax.searchsorted(end_stamps, side='left')\n\n        # Addresses GH #10530\n        if self.base > 0:\n            labels += type(self.freq)(self.base)\n\n        return binner, bins, labels\n\n    def _get_time_period_bins(self, ax):\n        if not isinstance(ax, DatetimeIndex):\n            raise TypeError('axis must be a DatetimeIndex, but got '\n                            'an instance of %r' % type(ax).__name__)\n\n        freq = self.freq\n\n        if not len(ax):\n            binner = labels = PeriodIndex(data=[], freq=freq, name=ax.name)\n            return binner, [], labels\n\n        labels = binner = PeriodIndex(start=ax[0],\n                                      end=ax[-1],\n                                      freq=freq,\n                                      name=ax.name)\n\n        end_stamps = (labels + freq).asfreq(freq, 's').to_timestamp()\n        if ax.tzinfo:\n            end_stamps = end_stamps.tz_localize(ax.tzinfo)\n        bins = ax.searchsorted(end_stamps, side='left')\n\n        return binner, bins, labels\n\n    def _get_period_bins(self, ax):\n        if not isinstance(ax, PeriodIndex):\n            raise TypeError('axis must be a PeriodIndex, but got '\n                            'an instance of %r' % type(ax).__name__)\n\n        memb = ax.asfreq(self.freq, how=self.convention)\n\n        # NaT handling as in pandas._lib.lib.generate_bins_dt64()\n        nat_count = 0\n        if memb.hasnans:\n            nat_count = np.sum(memb._isnan)\n            memb = memb[~memb._isnan]\n\n        # if index contains no valid (non-NaT) values, return empty index\n        if not len(memb):\n            binner = labels = PeriodIndex(\n                data=[], freq=self.freq, name=ax.name)\n            return binner, [], labels\n\n        start = ax.min().asfreq(self.freq, how=self.convention)\n        end = ax.max().asfreq(self.freq, how='end')\n\n        labels = binner = PeriodIndex(start=start, end=end,\n                                      freq=self.freq, name=ax.name)\n\n        i8 = memb.asi8\n        freq_mult = self.freq.n\n\n        # when upsampling to subperiods, we need to generate enough bins\n        expected_bins_count = len(binner) * freq_mult\n        i8_extend = expected_bins_count - (i8[-1] - i8[0])\n        rng = np.arange(i8[0], i8[-1] + i8_extend, freq_mult)\n        rng += freq_mult\n        bins = memb.searchsorted(rng, side='left')\n\n        if nat_count > 0:\n            # NaT handling as in pandas._lib.lib.generate_bins_dt64()\n            # shift bins by the number of NaT\n            bins += nat_count\n            bins = np.insert(bins, 0, nat_count)\n            binner = binner.insert(0, NaT)\n            labels = labels.insert(0, NaT)\n\n        return binner, bins, labels\n\n\ndef _take_new_index(obj, indexer, new_index, axis=0):\n    from pandas.core.api import Series, DataFrame\n\n    if isinstance(obj, Series):\n        new_values = algos.take_1d(obj.values, indexer)\n        return Series(new_values, index=new_index, name=obj.name)\n    elif isinstance(obj, DataFrame):\n        if axis == 1:\n            raise NotImplementedError(\"axis 1 is not supported\")\n        return DataFrame(obj._data.reindex_indexer(\n            new_axis=new_index, indexer=indexer, axis=1))\n    else:\n        raise ValueError(\"'obj' should be either a Series or a DataFrame\")\n\n\ndef _get_range_edges(first, last, offset, closed='left', base=0):\n    if isinstance(offset, Tick):\n        is_day = isinstance(offset, Day)\n        day_nanos = delta_to_nanoseconds(timedelta(1))\n\n        # #1165\n        if (is_day and day_nanos % offset.nanos == 0) or not is_day:\n            return _adjust_dates_anchored(first, last, offset,\n                                          closed=closed, base=base)\n\n    else:\n        first = first.normalize()\n        last = last.normalize()\n\n    if closed == 'left':\n        first = Timestamp(offset.rollback(first))\n    else:\n        first = Timestamp(first - offset)\n\n    last = Timestamp(last + offset)\n\n    return first, last\n\n\ndef _adjust_dates_anchored(first, last, offset, closed='right', base=0):\n    # First and last offsets should be calculated from the start day to fix an\n    # error cause by resampling across multiple days when a one day period is\n    # not a multiple of the frequency.\n    #\n    # See https://github.com/pandas-dev/pandas/issues/8683\n\n    # GH 10117 & GH 19375. If first and last contain timezone information,\n    # Perform the calculation in UTC in order to avoid localizing on an\n    # Ambiguous or Nonexistent time.\n    first_tzinfo = first.tzinfo\n    last_tzinfo = last.tzinfo\n    start_day_nanos = first.normalize().value\n    if first_tzinfo is not None:\n        first = first.tz_convert('UTC')\n    if last_tzinfo is not None:\n        last = last.tz_convert('UTC')\n\n    base_nanos = (base % offset.n) * offset.nanos // offset.n\n    start_day_nanos += base_nanos\n\n    foffset = (first.value - start_day_nanos) % offset.nanos\n    loffset = (last.value - start_day_nanos) % offset.nanos\n\n    if closed == 'right':\n        if foffset > 0:\n            # roll back\n            fresult = first.value - foffset\n        else:\n            fresult = first.value - offset.nanos\n\n        if loffset > 0:\n            # roll forward\n            lresult = last.value + (offset.nanos - loffset)\n        else:\n            # already the end of the road\n            lresult = last.value\n    else:  # closed == 'left'\n        if foffset > 0:\n            fresult = first.value - foffset\n        else:\n            # start of the road\n            fresult = first.value\n\n        if loffset > 0:\n            # roll forward\n            lresult = last.value + (offset.nanos - loffset)\n        else:\n            lresult = last.value + offset.nanos\n    fresult = Timestamp(fresult)\n    lresult = Timestamp(lresult)\n    if first_tzinfo is not None:\n        fresult = fresult.tz_localize('UTC').tz_convert(first_tzinfo)\n    if last_tzinfo is not None:\n        lresult = lresult.tz_localize('UTC').tz_convert(last_tzinfo)\n    return fresult, lresult\n\n\ndef asfreq(obj, freq, method=None, how=None, normalize=False, fill_value=None):\n    \"\"\"\n    Utility frequency conversion method for Series/DataFrame\n    \"\"\"\n    if isinstance(obj.index, PeriodIndex):\n        if method is not None:\n            raise NotImplementedError(\"'method' argument is not supported\")\n\n        if how is None:\n            how = 'E'\n\n        new_obj = obj.copy()\n        new_obj.index = obj.index.asfreq(freq, how=how)\n\n    elif len(obj.index) == 0:\n        new_obj = obj.copy()\n        new_obj.index = obj.index._shallow_copy(freq=to_offset(freq))\n\n    else:\n        dti = date_range(obj.index[0], obj.index[-1], freq=freq)\n        dti.name = obj.index.name\n        new_obj = obj.reindex(dti, method=method, fill_value=fill_value)\n        if normalize:\n            new_obj.index = new_obj.index.normalize()\n\n    return new_obj\n"
    },
    {
      "filename": "pandas/core/reshape/merge.py",
      "content": "\"\"\"\nSQL-style merge routines\n\"\"\"\n\nimport copy\nimport string\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import hashtable as libhashtable, join as libjoin, lib\nimport pandas.compat as compat\nfrom pandas.compat import filter, lzip, map, range, zip\nfrom pandas.errors import MergeError\nfrom pandas.util._decorators import Appender, Substitution\n\nfrom pandas.core.dtypes.common import (\n    ensure_float64, ensure_int64, ensure_object, is_array_like, is_bool,\n    is_bool_dtype, is_categorical_dtype, is_datetime64_dtype,\n    is_datetime64tz_dtype, is_datetimelike, is_dtype_equal, is_float_dtype,\n    is_int64_dtype, is_int_or_datetime_dtype, is_integer, is_integer_dtype,\n    is_list_like, is_number, is_numeric_dtype, needs_i8_conversion)\nfrom pandas.core.dtypes.missing import isnull, na_value_for_dtype\n\nfrom pandas import Categorical, DataFrame, Index, MultiIndex, Series, Timedelta\nimport pandas.core.algorithms as algos\nfrom pandas.core.arrays.categorical import _recode_for_categories\nimport pandas.core.common as com\nfrom pandas.core.frame import _merge_doc\nfrom pandas.core.internals import (\n    concatenate_block_managers, items_overlap_with_suffix)\nimport pandas.core.sorting as sorting\nfrom pandas.core.sorting import is_int64_overflow_possible\n\n\n@Substitution('\\nleft : DataFrame')\n@Appender(_merge_doc, indents=0)\ndef merge(left, right, how='inner', on=None, left_on=None, right_on=None,\n          left_index=False, right_index=False, sort=False,\n          suffixes=('_x', '_y'), copy=True, indicator=False,\n          validate=None):\n    op = _MergeOperation(left, right, how=how, on=on, left_on=left_on,\n                         right_on=right_on, left_index=left_index,\n                         right_index=right_index, sort=sort, suffixes=suffixes,\n                         copy=copy, indicator=indicator,\n                         validate=validate)\n    return op.get_result()\n\n\nif __debug__:\n    merge.__doc__ = _merge_doc % '\\nleft : DataFrame'\n\n\ndef _groupby_and_merge(by, on, left, right, _merge_pieces,\n                       check_duplicates=True):\n    \"\"\"\n    groupby & merge; we are always performing a left-by type operation\n\n    Parameters\n    ----------\n    by: field to group\n    on: duplicates field\n    left: left frame\n    right: right frame\n    _merge_pieces: function for merging\n    check_duplicates: boolean, default True\n        should we check & clean duplicates\n    \"\"\"\n\n    pieces = []\n    if not isinstance(by, (list, tuple)):\n        by = [by]\n\n    lby = left.groupby(by, sort=False)\n\n    # if we can groupby the rhs\n    # then we can get vastly better perf\n    try:\n\n        # we will check & remove duplicates if indicated\n        if check_duplicates:\n            if on is None:\n                on = []\n            elif not isinstance(on, (list, tuple)):\n                on = [on]\n\n            if right.duplicated(by + on).any():\n                right = right.drop_duplicates(by + on, keep='last')\n        rby = right.groupby(by, sort=False)\n    except KeyError:\n        rby = None\n\n    for key, lhs in lby:\n\n        if rby is None:\n            rhs = right\n        else:\n            try:\n                rhs = right.take(rby.indices[key])\n            except KeyError:\n                # key doesn't exist in left\n                lcols = lhs.columns.tolist()\n                cols = lcols + [r for r in right.columns\n                                if r not in set(lcols)]\n                merged = lhs.reindex(columns=cols)\n                merged.index = range(len(merged))\n                pieces.append(merged)\n                continue\n\n        merged = _merge_pieces(lhs, rhs)\n\n        # make sure join keys are in the merged\n        # TODO, should _merge_pieces do this?\n        for k in by:\n            try:\n                if k in merged:\n                    merged[k] = key\n            except KeyError:\n                pass\n\n        pieces.append(merged)\n\n    # preserve the original order\n    # if we have a missing piece this can be reset\n    from pandas.core.reshape.concat import concat\n    result = concat(pieces, ignore_index=True)\n    result = result.reindex(columns=pieces[0].columns, copy=False)\n    return result, lby\n\n\ndef merge_ordered(left, right, on=None,\n                  left_on=None, right_on=None,\n                  left_by=None, right_by=None,\n                  fill_method=None, suffixes=('_x', '_y'),\n                  how='outer'):\n    \"\"\"Perform merge with optional filling/interpolation designed for ordered\n    data like time series data. Optionally perform group-wise merge (see\n    examples)\n\n    Parameters\n    ----------\n    left : DataFrame\n    right : DataFrame\n    on : label or list\n        Field names to join on. Must be found in both DataFrames.\n    left_on : label or list, or array-like\n        Field names to join on in left DataFrame. Can be a vector or list of\n        vectors of the length of the DataFrame to use a particular vector as\n        the join key instead of columns\n    right_on : label or list, or array-like\n        Field names to join on in right DataFrame or vector/list of vectors per\n        left_on docs\n    left_by : column name or list of column names\n        Group left DataFrame by group columns and merge piece by piece with\n        right DataFrame\n    right_by : column name or list of column names\n        Group right DataFrame by group columns and merge piece by piece with\n        left DataFrame\n    fill_method : {'ffill', None}, default None\n        Interpolation method for data\n    suffixes : 2-length sequence (tuple, list, ...)\n        Suffix to apply to overlapping column names in the left and right\n        side, respectively\n    how : {'left', 'right', 'outer', 'inner'}, default 'outer'\n        * left: use only keys from left frame (SQL: left outer join)\n        * right: use only keys from right frame (SQL: right outer join)\n        * outer: use union of keys from both frames (SQL: full outer join)\n        * inner: use intersection of keys from both frames (SQL: inner join)\n\n        .. versionadded:: 0.19.0\n\n    Examples\n    --------\n    >>> A                      >>> B\n          key  lvalue group        key  rvalue\n    0   a       1     a        0     b       1\n    1   c       2     a        1     c       2\n    2   e       3     a        2     d       3\n    3   a       1     b\n    4   c       2     b\n    5   e       3     b\n\n    >>> merge_ordered(A, B, fill_method='ffill', left_by='group')\n      group key  lvalue  rvalue\n    0     a   a       1     NaN\n    1     a   b       1     1.0\n    2     a   c       2     2.0\n    3     a   d       2     3.0\n    4     a   e       3     3.0\n    5     b   a       1     NaN\n    6     b   b       1     1.0\n    7     b   c       2     2.0\n    8     b   d       2     3.0\n    9     b   e       3     3.0\n\n    Returns\n    -------\n    merged : DataFrame\n        The output type will the be same as 'left', if it is a subclass\n        of DataFrame.\n\n    See Also\n    --------\n    merge\n    merge_asof\n    \"\"\"\n    def _merger(x, y):\n        # perform the ordered merge operation\n        op = _OrderedMerge(x, y, on=on, left_on=left_on, right_on=right_on,\n                           suffixes=suffixes, fill_method=fill_method,\n                           how=how)\n        return op.get_result()\n\n    if left_by is not None and right_by is not None:\n        raise ValueError('Can only group either left or right frames')\n    elif left_by is not None:\n        result, _ = _groupby_and_merge(left_by, on, left, right,\n                                       lambda x, y: _merger(x, y),\n                                       check_duplicates=False)\n    elif right_by is not None:\n        result, _ = _groupby_and_merge(right_by, on, right, left,\n                                       lambda x, y: _merger(y, x),\n                                       check_duplicates=False)\n    else:\n        result = _merger(left, right)\n    return result\n\n\ndef merge_asof(left, right, on=None,\n               left_on=None, right_on=None,\n               left_index=False, right_index=False,\n               by=None, left_by=None, right_by=None,\n               suffixes=('_x', '_y'),\n               tolerance=None,\n               allow_exact_matches=True,\n               direction='backward'):\n    \"\"\"Perform an asof merge. This is similar to a left-join except that we\n    match on nearest key rather than equal keys.\n\n    Both DataFrames must be sorted by the key.\n\n    For each row in the left DataFrame:\n\n      - A \"backward\" search selects the last row in the right DataFrame whose\n        'on' key is less than or equal to the left's key.\n\n      - A \"forward\" search selects the first row in the right DataFrame whose\n        'on' key is greater than or equal to the left's key.\n\n      - A \"nearest\" search selects the row in the right DataFrame whose 'on'\n        key is closest in absolute distance to the left's key.\n\n    The default is \"backward\" and is compatible in versions below 0.20.0.\n    The direction parameter was added in version 0.20.0 and introduces\n    \"forward\" and \"nearest\".\n\n    Optionally match on equivalent keys with 'by' before searching with 'on'.\n\n    .. versionadded:: 0.19.0\n\n    Parameters\n    ----------\n    left : DataFrame\n    right : DataFrame\n    on : label\n        Field name to join on. Must be found in both DataFrames.\n        The data MUST be ordered. Furthermore this must be a numeric column,\n        such as datetimelike, integer, or float. On or left_on/right_on\n        must be given.\n    left_on : label\n        Field name to join on in left DataFrame.\n    right_on : label\n        Field name to join on in right DataFrame.\n    left_index : boolean\n        Use the index of the left DataFrame as the join key.\n\n        .. versionadded:: 0.19.2\n\n    right_index : boolean\n        Use the index of the right DataFrame as the join key.\n\n        .. versionadded:: 0.19.2\n\n    by : column name or list of column names\n        Match on these columns before performing merge operation.\n    left_by : column name\n        Field names to match on in the left DataFrame.\n\n        .. versionadded:: 0.19.2\n\n    right_by : column name\n        Field names to match on in the right DataFrame.\n\n        .. versionadded:: 0.19.2\n\n    suffixes : 2-length sequence (tuple, list, ...)\n        Suffix to apply to overlapping column names in the left and right\n        side, respectively.\n    tolerance : integer or Timedelta, optional, default None\n        Select asof tolerance within this range; must be compatible\n        with the merge index.\n    allow_exact_matches : boolean, default True\n\n        - If True, allow matching with the same 'on' value\n          (i.e. less-than-or-equal-to / greater-than-or-equal-to)\n        - If False, don't match the same 'on' value\n          (i.e., strictly less-than / strictly greater-than)\n\n    direction : 'backward' (default), 'forward', or 'nearest'\n        Whether to search for prior, subsequent, or closest matches.\n\n        .. versionadded:: 0.20.0\n\n    Returns\n    -------\n    merged : DataFrame\n\n    Examples\n    --------\n    >>> left = pd.DataFrame({'a': [1, 5, 10], 'left_val': ['a', 'b', 'c']})\n    >>> left\n        a left_val\n    0   1        a\n    1   5        b\n    2  10        c\n\n    >>> right = pd.DataFrame({'a': [1, 2, 3, 6, 7],\n    ...                       'right_val': [1, 2, 3, 6, 7]})\n    >>> right\n       a  right_val\n    0  1          1\n    1  2          2\n    2  3          3\n    3  6          6\n    4  7          7\n\n    >>> pd.merge_asof(left, right, on='a')\n        a left_val  right_val\n    0   1        a          1\n    1   5        b          3\n    2  10        c          7\n\n    >>> pd.merge_asof(left, right, on='a', allow_exact_matches=False)\n        a left_val  right_val\n    0   1        a        NaN\n    1   5        b        3.0\n    2  10        c        7.0\n\n    >>> pd.merge_asof(left, right, on='a', direction='forward')\n        a left_val  right_val\n    0   1        a        1.0\n    1   5        b        6.0\n    2  10        c        NaN\n\n    >>> pd.merge_asof(left, right, on='a', direction='nearest')\n        a left_val  right_val\n    0   1        a          1\n    1   5        b          6\n    2  10        c          7\n\n    We can use indexed DataFrames as well.\n\n    >>> left = pd.DataFrame({'left_val': ['a', 'b', 'c']}, index=[1, 5, 10])\n    >>> left\n       left_val\n    1         a\n    5         b\n    10        c\n\n    >>> right = pd.DataFrame({'right_val': [1, 2, 3, 6, 7]},\n    ...                      index=[1, 2, 3, 6, 7])\n    >>> right\n       right_val\n    1          1\n    2          2\n    3          3\n    6          6\n    7          7\n\n    >>> pd.merge_asof(left, right, left_index=True, right_index=True)\n       left_val  right_val\n    1         a          1\n    5         b          3\n    10        c          7\n\n    Here is a real-world times-series example\n\n    >>> quotes\n                         time ticker     bid     ask\n    0 2016-05-25 13:30:00.023   GOOG  720.50  720.93\n    1 2016-05-25 13:30:00.023   MSFT   51.95   51.96\n    2 2016-05-25 13:30:00.030   MSFT   51.97   51.98\n    3 2016-05-25 13:30:00.041   MSFT   51.99   52.00\n    4 2016-05-25 13:30:00.048   GOOG  720.50  720.93\n    5 2016-05-25 13:30:00.049   AAPL   97.99   98.01\n    6 2016-05-25 13:30:00.072   GOOG  720.50  720.88\n    7 2016-05-25 13:30:00.075   MSFT   52.01   52.03\n\n    >>> trades\n                         time ticker   price  quantity\n    0 2016-05-25 13:30:00.023   MSFT   51.95        75\n    1 2016-05-25 13:30:00.038   MSFT   51.95       155\n    2 2016-05-25 13:30:00.048   GOOG  720.77       100\n    3 2016-05-25 13:30:00.048   GOOG  720.92       100\n    4 2016-05-25 13:30:00.048   AAPL   98.00       100\n\n    By default we are taking the asof of the quotes\n\n    >>> pd.merge_asof(trades, quotes,\n    ...                       on='time',\n    ...                       by='ticker')\n                         time ticker   price  quantity     bid     ask\n    0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n    1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n    2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n    3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n    4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n\n    We only asof within 2ms between the quote time and the trade time\n\n    >>> pd.merge_asof(trades, quotes,\n    ...                       on='time',\n    ...                       by='ticker',\n    ...                       tolerance=pd.Timedelta('2ms'))\n                         time ticker   price  quantity     bid     ask\n    0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n    1 2016-05-25 13:30:00.038   MSFT   51.95       155     NaN     NaN\n    2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n    3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n    4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n\n    We only asof within 10ms between the quote time and the trade time\n    and we exclude exact matches on time. However *prior* data will\n    propagate forward\n\n    >>> pd.merge_asof(trades, quotes,\n    ...                       on='time',\n    ...                       by='ticker',\n    ...                       tolerance=pd.Timedelta('10ms'),\n    ...                       allow_exact_matches=False)\n                         time ticker   price  quantity     bid     ask\n    0 2016-05-25 13:30:00.023   MSFT   51.95        75     NaN     NaN\n    1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n    2 2016-05-25 13:30:00.048   GOOG  720.77       100     NaN     NaN\n    3 2016-05-25 13:30:00.048   GOOG  720.92       100     NaN     NaN\n    4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n\n    See Also\n    --------\n    merge\n    merge_ordered\n    \"\"\"\n    op = _AsOfMerge(left, right,\n                    on=on, left_on=left_on, right_on=right_on,\n                    left_index=left_index, right_index=right_index,\n                    by=by, left_by=left_by, right_by=right_by,\n                    suffixes=suffixes,\n                    how='asof', tolerance=tolerance,\n                    allow_exact_matches=allow_exact_matches,\n                    direction=direction)\n    return op.get_result()\n\n\n# TODO: transformations??\n# TODO: only copy DataFrames when modification necessary\nclass _MergeOperation(object):\n    \"\"\"\n    Perform a database (SQL) merge operation between two DataFrame objects\n    using either columns as keys or their row indexes\n    \"\"\"\n    _merge_type = 'merge'\n\n    def __init__(self, left, right, how='inner', on=None,\n                 left_on=None, right_on=None, axis=1,\n                 left_index=False, right_index=False, sort=True,\n                 suffixes=('_x', '_y'), copy=True, indicator=False,\n                 validate=None):\n        left = validate_operand(left)\n        right = validate_operand(right)\n        self.left = self.orig_left = left\n        self.right = self.orig_right = right\n        self.how = how\n        self.axis = axis\n\n        self.on = com.maybe_make_list(on)\n        self.left_on = com.maybe_make_list(left_on)\n        self.right_on = com.maybe_make_list(right_on)\n\n        self.copy = copy\n        self.suffixes = suffixes\n        self.sort = sort\n\n        self.left_index = left_index\n        self.right_index = right_index\n\n        self.indicator = indicator\n\n        if isinstance(self.indicator, compat.string_types):\n            self.indicator_name = self.indicator\n        elif isinstance(self.indicator, bool):\n            self.indicator_name = '_merge' if self.indicator else None\n        else:\n            raise ValueError(\n                'indicator option can only accept boolean or string arguments')\n\n        if not is_bool(left_index):\n            raise ValueError(\n                'left_index parameter must be of type bool, not '\n                '{left_index}'.format(left_index=type(left_index)))\n        if not is_bool(right_index):\n            raise ValueError(\n                'right_index parameter must be of type bool, not '\n                '{right_index}'.format(right_index=type(right_index)))\n\n        # warn user when merging between different levels\n        if left.columns.nlevels != right.columns.nlevels:\n            msg = ('merging between different levels can give an unintended '\n                   'result ({left} levels on the left, {right} on the right)'\n                   ).format(left=left.columns.nlevels,\n                            right=right.columns.nlevels)\n            warnings.warn(msg, UserWarning)\n\n        self._validate_specification()\n\n        # note this function has side effects\n        (self.left_join_keys,\n         self.right_join_keys,\n         self.join_names) = self._get_merge_keys()\n\n        # validate the merge keys dtypes. We may need to coerce\n        # to avoid incompat dtypes\n        self._maybe_coerce_merge_keys()\n\n        # If argument passed to validate,\n        # check if columns specified as unique\n        # are in fact unique.\n        if validate is not None:\n            self._validate(validate)\n\n    def get_result(self):\n        if self.indicator:\n            self.left, self.right = self._indicator_pre_merge(\n                self.left, self.right)\n\n        join_index, left_indexer, right_indexer = self._get_join_info()\n\n        ldata, rdata = self.left._data, self.right._data\n        lsuf, rsuf = self.suffixes\n\n        llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n                                                     rdata.items, rsuf)\n\n        lindexers = {1: left_indexer} if left_indexer is not None else {}\n        rindexers = {1: right_indexer} if right_indexer is not None else {}\n\n        result_data = concatenate_block_managers(\n            [(ldata, lindexers), (rdata, rindexers)],\n            axes=[llabels.append(rlabels), join_index],\n            concat_axis=0, copy=self.copy)\n\n        typ = self.left._constructor\n        result = typ(result_data).__finalize__(self, method=self._merge_type)\n\n        if self.indicator:\n            result = self._indicator_post_merge(result)\n\n        self._maybe_add_join_keys(result, left_indexer, right_indexer)\n\n        self._maybe_restore_index_levels(result)\n\n        return result\n\n    def _indicator_pre_merge(self, left, right):\n\n        columns = left.columns.union(right.columns)\n\n        for i in ['_left_indicator', '_right_indicator']:\n            if i in columns:\n                raise ValueError(\"Cannot use `indicator=True` option when \"\n                                 \"data contains a column named {name}\"\n                                 .format(name=i))\n        if self.indicator_name in columns:\n            raise ValueError(\n                \"Cannot use name of an existing column for indicator column\")\n\n        left = left.copy()\n        right = right.copy()\n\n        left['_left_indicator'] = 1\n        left['_left_indicator'] = left['_left_indicator'].astype('int8')\n\n        right['_right_indicator'] = 2\n        right['_right_indicator'] = right['_right_indicator'].astype('int8')\n\n        return left, right\n\n    def _indicator_post_merge(self, result):\n\n        result['_left_indicator'] = result['_left_indicator'].fillna(0)\n        result['_right_indicator'] = result['_right_indicator'].fillna(0)\n\n        result[self.indicator_name] = Categorical((result['_left_indicator'] +\n                                                   result['_right_indicator']),\n                                                  categories=[1, 2, 3])\n        result[self.indicator_name] = (\n            result[self.indicator_name]\n            .cat.rename_categories(['left_only', 'right_only', 'both']))\n\n        result = result.drop(labels=['_left_indicator', '_right_indicator'],\n                             axis=1)\n        return result\n\n    def _maybe_restore_index_levels(self, result):\n        \"\"\"\n        Restore index levels specified as `on` parameters\n\n        Here we check for cases where `self.left_on` and `self.right_on` pairs\n        each reference an index level in their respective DataFrames. The\n        joined columns corresponding to these pairs are then restored to the\n        index of `result`.\n\n        **Note:** This method has side effects. It modifies `result` in-place\n\n        Parameters\n        ----------\n        result: DataFrame\n            merge result\n\n        Returns\n        -------\n        None\n        \"\"\"\n        names_to_restore = []\n        for name, left_key, right_key in zip(self.join_names,\n                                             self.left_on,\n                                             self.right_on):\n            if (self.orig_left._is_level_reference(left_key) and\n                    self.orig_right._is_level_reference(right_key) and\n                    name not in result.index.names):\n\n                names_to_restore.append(name)\n\n        if names_to_restore:\n            result.set_index(names_to_restore, inplace=True)\n\n    def _maybe_add_join_keys(self, result, left_indexer, right_indexer):\n\n        left_has_missing = None\n        right_has_missing = None\n\n        keys = zip(self.join_names, self.left_on, self.right_on)\n        for i, (name, lname, rname) in enumerate(keys):\n            if not _should_fill(lname, rname):\n                continue\n\n            take_left, take_right = None, None\n\n            if name in result:\n\n                if left_indexer is not None and right_indexer is not None:\n                    if name in self.left:\n\n                        if left_has_missing is None:\n                            left_has_missing = (left_indexer == -1).any()\n\n                        if left_has_missing:\n                            take_right = self.right_join_keys[i]\n\n                            if not is_dtype_equal(result[name].dtype,\n                                                  self.left[name].dtype):\n                                take_left = self.left[name]._values\n\n                    elif name in self.right:\n\n                        if right_has_missing is None:\n                            right_has_missing = (right_indexer == -1).any()\n\n                        if right_has_missing:\n                            take_left = self.left_join_keys[i]\n\n                            if not is_dtype_equal(result[name].dtype,\n                                                  self.right[name].dtype):\n                                take_right = self.right[name]._values\n\n            elif left_indexer is not None \\\n                    and is_array_like(self.left_join_keys[i]):\n                take_left = self.left_join_keys[i]\n                take_right = self.right_join_keys[i]\n\n            if take_left is not None or take_right is not None:\n\n                if take_left is None:\n                    lvals = result[name]._values\n                else:\n                    lfill = na_value_for_dtype(take_left.dtype)\n                    lvals = algos.take_1d(take_left, left_indexer,\n                                          fill_value=lfill)\n\n                if take_right is None:\n                    rvals = result[name]._values\n                else:\n                    rfill = na_value_for_dtype(take_right.dtype)\n                    rvals = algos.take_1d(take_right, right_indexer,\n                                          fill_value=rfill)\n\n                # if we have an all missing left_indexer\n                # make sure to just use the right values\n                mask = left_indexer == -1\n                if mask.all():\n                    key_col = rvals\n                else:\n                    key_col = Index(lvals).where(~mask, rvals)\n\n                if result._is_label_reference(name):\n                    result[name] = key_col\n                elif result._is_level_reference(name):\n                    if isinstance(result.index, MultiIndex):\n                        idx_list = [result.index.get_level_values(level_name)\n                                    if level_name != name else key_col\n                                    for level_name in result.index.names]\n\n                        result.set_index(idx_list, inplace=True)\n                    else:\n                        result.index = Index(key_col, name=name)\n                else:\n                    result.insert(i, name or 'key_{i}'.format(i=i), key_col)\n\n    def _get_join_indexers(self):\n        \"\"\" return the join indexers \"\"\"\n        return _get_join_indexers(self.left_join_keys,\n                                  self.right_join_keys,\n                                  sort=self.sort,\n                                  how=self.how)\n\n    def _get_join_info(self):\n        left_ax = self.left._data.axes[self.axis]\n        right_ax = self.right._data.axes[self.axis]\n\n        if self.left_index and self.right_index and self.how != 'asof':\n            join_index, left_indexer, right_indexer = \\\n                left_ax.join(right_ax, how=self.how, return_indexers=True,\n                             sort=self.sort)\n        elif self.right_index and self.how == 'left':\n            join_index, left_indexer, right_indexer = \\\n                _left_join_on_index(left_ax, right_ax, self.left_join_keys,\n                                    sort=self.sort)\n\n        elif self.left_index and self.how == 'right':\n            join_index, right_indexer, left_indexer = \\\n                _left_join_on_index(right_ax, left_ax, self.right_join_keys,\n                                    sort=self.sort)\n        else:\n            (left_indexer,\n             right_indexer) = self._get_join_indexers()\n\n            if self.right_index:\n                if len(self.left) > 0:\n                    join_index = self.left.index.take(left_indexer)\n                else:\n                    join_index = self.right.index.take(right_indexer)\n                    left_indexer = np.array([-1] * len(join_index))\n            elif self.left_index:\n                if len(self.right) > 0:\n                    join_index = self.right.index.take(right_indexer)\n                else:\n                    join_index = self.left.index.take(left_indexer)\n                    right_indexer = np.array([-1] * len(join_index))\n            else:\n                join_index = Index(np.arange(len(left_indexer)))\n\n        if len(join_index) == 0:\n            join_index = join_index.astype(object)\n        return join_index, left_indexer, right_indexer\n\n    def _get_merge_keys(self):\n        \"\"\"\n        Note: has side effects (copy/delete key columns)\n\n        Parameters\n        ----------\n        left\n        right\n        on\n\n        Returns\n        -------\n        left_keys, right_keys\n        \"\"\"\n        left_keys = []\n        right_keys = []\n        join_names = []\n        right_drop = []\n        left_drop = []\n\n        left, right = self.left, self.right\n\n        is_lkey = lambda x: is_array_like(x) and len(x) == len(left)\n        is_rkey = lambda x: is_array_like(x) and len(x) == len(right)\n\n        # Note that pd.merge_asof() has separate 'on' and 'by' parameters. A\n        # user could, for example, request 'left_index' and 'left_by'. In a\n        # regular pd.merge(), users cannot specify both 'left_index' and\n        # 'left_on'. (Instead, users have a MultiIndex). That means the\n        # self.left_on in this function is always empty in a pd.merge(), but\n        # a pd.merge_asof(left_index=True, left_by=...) will result in a\n        # self.left_on array with a None in the middle of it. This requires\n        # a work-around as designated in the code below.\n        # See _validate_specification() for where this happens.\n\n        # ugh, spaghetti re #733\n        if _any(self.left_on) and _any(self.right_on):\n            for lk, rk in zip(self.left_on, self.right_on):\n                if is_lkey(lk):\n                    left_keys.append(lk)\n                    if is_rkey(rk):\n                        right_keys.append(rk)\n                        join_names.append(None)  # what to do?\n                    else:\n                        if rk is not None:\n                            right_keys.append(\n                                right._get_label_or_level_values(rk))\n                            join_names.append(rk)\n                        else:\n                            # work-around for merge_asof(right_index=True)\n                            right_keys.append(right.index)\n                            join_names.append(right.index.name)\n                else:\n                    if not is_rkey(rk):\n                        if rk is not None:\n                            right_keys.append(\n                                right._get_label_or_level_values(rk))\n                        else:\n                            # work-around for merge_asof(right_index=True)\n                            right_keys.append(right.index)\n                        if lk is not None and lk == rk:\n                            # avoid key upcast in corner case (length-0)\n                            if len(left) > 0:\n                                right_drop.append(rk)\n                            else:\n                                left_drop.append(lk)\n                    else:\n                        right_keys.append(rk)\n                    if lk is not None:\n                        left_keys.append(left._get_label_or_level_values(lk))\n                        join_names.append(lk)\n                    else:\n                        # work-around for merge_asof(left_index=True)\n                        left_keys.append(left.index)\n                        join_names.append(left.index.name)\n        elif _any(self.left_on):\n            for k in self.left_on:\n                if is_lkey(k):\n                    left_keys.append(k)\n                    join_names.append(None)\n                else:\n                    left_keys.append(left._get_label_or_level_values(k))\n                    join_names.append(k)\n            if isinstance(self.right.index, MultiIndex):\n                right_keys = [lev._values.take(lab)\n                              for lev, lab in zip(self.right.index.levels,\n                                                  self.right.index.labels)]\n            else:\n                right_keys = [self.right.index.values]\n        elif _any(self.right_on):\n            for k in self.right_on:\n                if is_rkey(k):\n                    right_keys.append(k)\n                    join_names.append(None)\n                else:\n                    right_keys.append(right._get_label_or_level_values(k))\n                    join_names.append(k)\n            if isinstance(self.left.index, MultiIndex):\n                left_keys = [lev._values.take(lab)\n                             for lev, lab in zip(self.left.index.levels,\n                                                 self.left.index.labels)]\n            else:\n                left_keys = [self.left.index.values]\n\n        if left_drop:\n            self.left = self.left._drop_labels_or_levels(left_drop)\n\n        if right_drop:\n            self.right = self.right._drop_labels_or_levels(right_drop)\n\n        return left_keys, right_keys, join_names\n\n    def _maybe_coerce_merge_keys(self):\n        # we have valid mergees but we may have to further\n        # coerce these if they are originally incompatible types\n        #\n        # for example if these are categorical, but are not dtype_equal\n        # or if we have object and integer dtypes\n\n        for lk, rk, name in zip(self.left_join_keys,\n                                self.right_join_keys,\n                                self.join_names):\n            if (len(lk) and not len(rk)) or (not len(lk) and len(rk)):\n                continue\n\n            lk_is_cat = is_categorical_dtype(lk)\n            rk_is_cat = is_categorical_dtype(rk)\n\n            # if either left or right is a categorical\n            # then the must match exactly in categories & ordered\n            if lk_is_cat and rk_is_cat:\n                if lk.is_dtype_equal(rk):\n                    continue\n\n            elif lk_is_cat or rk_is_cat:\n                pass\n\n            elif is_dtype_equal(lk.dtype, rk.dtype):\n                continue\n\n            msg = (\"You are trying to merge on {lk_dtype} and \"\n                   \"{rk_dtype} columns. If you wish to proceed \"\n                   \"you should use pd.concat\".format(lk_dtype=lk.dtype,\n                                                     rk_dtype=rk.dtype))\n\n            # if we are numeric, then allow differing\n            # kinds to proceed, eg. int64 and int8, int and float\n            # further if we are object, but we infer to\n            # the same, then proceed\n            if is_numeric_dtype(lk) and is_numeric_dtype(rk):\n                if lk.dtype.kind == rk.dtype.kind:\n                    pass\n\n                # check whether ints and floats\n                elif is_integer_dtype(rk) and is_float_dtype(lk):\n                    if not (lk == lk.astype(rk.dtype))[~np.isnan(lk)].all():\n                        warnings.warn('You are merging on int and float '\n                                      'columns where the float values '\n                                      'are not equal to their int '\n                                      'representation', UserWarning)\n\n                elif is_float_dtype(rk) and is_integer_dtype(lk):\n                    if not (rk == rk.astype(lk.dtype))[~np.isnan(rk)].all():\n                        warnings.warn('You are merging on int and float '\n                                      'columns where the float values '\n                                      'are not equal to their int '\n                                      'representation', UserWarning)\n\n                # let's infer and see if we are ok\n                elif lib.infer_dtype(lk) == lib.infer_dtype(rk):\n                    pass\n\n            # Check if we are trying to merge on obviously\n            # incompatible dtypes GH 9780, GH 15800\n\n            # boolean values are considered as numeric, but are still allowed\n            # to be merged on object boolean values\n            elif ((is_numeric_dtype(lk) and not is_bool_dtype(lk))\n                    and not is_numeric_dtype(rk)):\n                raise ValueError(msg)\n            elif (not is_numeric_dtype(lk)\n                    and (is_numeric_dtype(rk) and not is_bool_dtype(rk))):\n                raise ValueError(msg)\n            elif is_datetimelike(lk) and not is_datetimelike(rk):\n                raise ValueError(msg)\n            elif not is_datetimelike(lk) and is_datetimelike(rk):\n                raise ValueError(msg)\n            elif is_datetime64tz_dtype(lk) and not is_datetime64tz_dtype(rk):\n                raise ValueError(msg)\n            elif not is_datetime64tz_dtype(lk) and is_datetime64tz_dtype(rk):\n                raise ValueError(msg)\n\n            # Houston, we have a problem!\n            # let's coerce to object if the dtypes aren't\n            # categorical, otherwise coerce to the category\n            # dtype. If we coerced categories to object,\n            # then we would lose type information on some\n            # columns, and end up trying to merge\n            # incompatible dtypes. See GH 16900.\n            else:\n                if name in self.left.columns:\n                    typ = lk.categories.dtype if lk_is_cat else object\n                    self.left = self.left.assign(\n                        **{name: self.left[name].astype(typ)})\n                if name in self.right.columns:\n                    typ = rk.categories.dtype if rk_is_cat else object\n                    self.right = self.right.assign(\n                        **{name: self.right[name].astype(typ)})\n\n    def _validate_specification(self):\n        # Hm, any way to make this logic less complicated??\n        if self.on is None and self.left_on is None and self.right_on is None:\n\n            if self.left_index and self.right_index:\n                self.left_on, self.right_on = (), ()\n            elif self.left_index:\n                if self.right_on is None:\n                    raise MergeError('Must pass right_on or right_index=True')\n            elif self.right_index:\n                if self.left_on is None:\n                    raise MergeError('Must pass left_on or left_index=True')\n            else:\n                # use the common columns\n                common_cols = self.left.columns.intersection(\n                    self.right.columns)\n                if len(common_cols) == 0:\n                    raise MergeError(\n                        'No common columns to perform merge on. '\n                        'Merge options: left_on={lon}, right_on={ron}, '\n                        'left_index={lidx}, right_index={ridx}'\n                        .format(lon=self.left_on, ron=self.right_on,\n                                lidx=self.left_index, ridx=self.right_index))\n                if not common_cols.is_unique:\n                    raise MergeError(\"Data columns not unique: {common!r}\"\n                                     .format(common=common_cols))\n                self.left_on = self.right_on = common_cols\n        elif self.on is not None:\n            if self.left_on is not None or self.right_on is not None:\n                raise MergeError('Can only pass argument \"on\" OR \"left_on\" '\n                                 'and \"right_on\", not a combination of both.')\n            self.left_on = self.right_on = self.on\n        elif self.left_on is not None:\n            n = len(self.left_on)\n            if self.right_index:\n                if len(self.left_on) != self.right.index.nlevels:\n                    raise ValueError('len(left_on) must equal the number '\n                                     'of levels in the index of \"right\"')\n                self.right_on = [None] * n\n        elif self.right_on is not None:\n            n = len(self.right_on)\n            if self.left_index:\n                if len(self.right_on) != self.left.index.nlevels:\n                    raise ValueError('len(right_on) must equal the number '\n                                     'of levels in the index of \"left\"')\n                self.left_on = [None] * n\n        if len(self.right_on) != len(self.left_on):\n            raise ValueError(\"len(right_on) must equal len(left_on)\")\n\n    def _validate(self, validate):\n\n        # Check uniqueness of each\n        if self.left_index:\n            left_unique = self.orig_left.index.is_unique\n        else:\n            left_unique = MultiIndex.from_arrays(self.left_join_keys\n                                                 ).is_unique\n\n        if self.right_index:\n            right_unique = self.orig_right.index.is_unique\n        else:\n            right_unique = MultiIndex.from_arrays(self.right_join_keys\n                                                  ).is_unique\n\n        # Check data integrity\n        if validate in [\"one_to_one\", \"1:1\"]:\n            if not left_unique and not right_unique:\n                raise MergeError(\"Merge keys are not unique in either left\"\n                                 \" or right dataset; not a one-to-one merge\")\n            elif not left_unique:\n                raise MergeError(\"Merge keys are not unique in left dataset;\"\n                                 \" not a one-to-one merge\")\n            elif not right_unique:\n                raise MergeError(\"Merge keys are not unique in right dataset;\"\n                                 \" not a one-to-one merge\")\n\n        elif validate in [\"one_to_many\", \"1:m\"]:\n            if not left_unique:\n                raise MergeError(\"Merge keys are not unique in left dataset;\"\n                                 \"not a one-to-many merge\")\n\n        elif validate in [\"many_to_one\", \"m:1\"]:\n            if not right_unique:\n                raise MergeError(\"Merge keys are not unique in right dataset;\"\n                                 \" not a many-to-one merge\")\n\n        elif validate in ['many_to_many', 'm:m']:\n            pass\n\n        else:\n            raise ValueError(\"Not a valid argument for validate\")\n\n\ndef _get_join_indexers(left_keys, right_keys, sort=False, how='inner',\n                       **kwargs):\n    \"\"\"\n\n    Parameters\n    ----------\n    left_keys: ndarray, Index, Series\n    right_keys: ndarray, Index, Series\n    sort: boolean, default False\n    how: string {'inner', 'outer', 'left', 'right'}, default 'inner'\n\n    Returns\n    -------\n    tuple of (left_indexer, right_indexer)\n        indexers into the left_keys, right_keys\n\n    \"\"\"\n    from functools import partial\n\n    assert len(left_keys) == len(right_keys), \\\n        'left_key and right_keys must be the same length'\n\n    # bind `sort` arg. of _factorize_keys\n    fkeys = partial(_factorize_keys, sort=sort)\n\n    # get left & right join labels and num. of levels at each location\n    llab, rlab, shape = map(list, zip(* map(fkeys, left_keys, right_keys)))\n\n    # get flat i8 keys from label lists\n    lkey, rkey = _get_join_keys(llab, rlab, shape, sort)\n\n    # factorize keys to a dense i8 space\n    # `count` is the num. of unique keys\n    # set(lkey) | set(rkey) == range(count)\n    lkey, rkey, count = fkeys(lkey, rkey)\n\n    # preserve left frame order if how == 'left' and sort == False\n    kwargs = copy.copy(kwargs)\n    if how == 'left':\n        kwargs['sort'] = sort\n    join_func = _join_functions[how]\n\n    return join_func(lkey, rkey, count, **kwargs)\n\n\ndef _restore_dropped_levels_multijoin(left, right, dropped_level_names,\n                                      join_index, lindexer, rindexer):\n    \"\"\"\n    *this is an internal non-public method*\n\n    Returns the levels, labels and names of a multi-index to multi-index join.\n    Depending on the type of join, this method restores the appropriate\n    dropped levels of the joined multi-index.\n    The method relies on lidx, rindexer which hold the index positions of\n    left and right, where a join was feasible\n\n    Parameters\n    ----------\n    left : MultiIndex\n        left index\n    right : MultiIndex\n        right index\n    dropped_level_names : str array\n        list of non-common level names\n    join_index : MultiIndex\n        the index of the join between the\n        common levels of left and right\n    lindexer : intp array\n        left indexer\n    rindexer : intp array\n        right indexer\n\n    Returns\n    -------\n    levels : list of Index\n        levels of combined multiindexes\n    labels : intp array\n        labels of combined multiindexes\n    names : str array\n        names of combined multiindexes\n\n    \"\"\"\n\n    def _convert_to_mulitindex(index):\n        if isinstance(index, MultiIndex):\n            return index\n        else:\n            return MultiIndex.from_arrays([index.values],\n                                          names=[index.name])\n\n    # For multi-multi joins with one overlapping level,\n    # the returned index if of type Index\n    # Assure that join_index is of type MultiIndex\n    # so that dropped levels can be appended\n    join_index = _convert_to_mulitindex(join_index)\n\n    join_levels = join_index.levels\n    join_labels = join_index.labels\n    join_names = join_index.names\n\n    # lindexer and rindexer hold the indexes where the join occurred\n    # for left and right respectively. If left/right is None then\n    # the join occurred on all indices of left/right\n    if lindexer is None:\n        lindexer = range(left.size)\n\n    if rindexer is None:\n        rindexer = range(right.size)\n\n    # Iterate through the levels that must be restored\n    for dropped_level_name in dropped_level_names:\n        if dropped_level_name in left.names:\n            idx = left\n            indexer = lindexer\n        else:\n            idx = right\n            indexer = rindexer\n\n        # The index of the level name to be restored\n        name_idx = idx.names.index(dropped_level_name)\n\n        restore_levels = idx.levels[name_idx]\n        # Inject -1 in the labels list where a join was not possible\n        # IOW indexer[i]=-1\n        labels = idx.labels[name_idx]\n        restore_labels = algos.take_nd(labels, indexer, fill_value=-1)\n\n        join_levels = join_levels + [restore_levels]\n        join_labels = join_labels + [restore_labels]\n        join_names = join_names + [dropped_level_name]\n\n    return join_levels, join_labels, join_names\n\n\nclass _OrderedMerge(_MergeOperation):\n    _merge_type = 'ordered_merge'\n\n    def __init__(self, left, right, on=None, left_on=None, right_on=None,\n                 left_index=False, right_index=False, axis=1,\n                 suffixes=('_x', '_y'), copy=True,\n                 fill_method=None, how='outer'):\n\n        self.fill_method = fill_method\n        _MergeOperation.__init__(self, left, right, on=on, left_on=left_on,\n                                 left_index=left_index,\n                                 right_index=right_index,\n                                 right_on=right_on, axis=axis,\n                                 how=how, suffixes=suffixes,\n                                 sort=True  # factorize sorts\n                                 )\n\n    def get_result(self):\n        join_index, left_indexer, right_indexer = self._get_join_info()\n\n        # this is a bit kludgy\n        ldata, rdata = self.left._data, self.right._data\n        lsuf, rsuf = self.suffixes\n\n        llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n                                                     rdata.items, rsuf)\n\n        if self.fill_method == 'ffill':\n            left_join_indexer = libjoin.ffill_indexer(left_indexer)\n            right_join_indexer = libjoin.ffill_indexer(right_indexer)\n        else:\n            left_join_indexer = left_indexer\n            right_join_indexer = right_indexer\n\n        lindexers = {\n            1: left_join_indexer} if left_join_indexer is not None else {}\n        rindexers = {\n            1: right_join_indexer} if right_join_indexer is not None else {}\n\n        result_data = concatenate_block_managers(\n            [(ldata, lindexers), (rdata, rindexers)],\n            axes=[llabels.append(rlabels), join_index],\n            concat_axis=0, copy=self.copy)\n\n        typ = self.left._constructor\n        result = typ(result_data).__finalize__(self, method=self._merge_type)\n\n        self._maybe_add_join_keys(result, left_indexer, right_indexer)\n\n        return result\n\n\ndef _asof_function(direction):\n    name = 'asof_join_{dir}'.format(dir=direction)\n    return getattr(libjoin, name, None)\n\n\ndef _asof_by_function(direction):\n    name = 'asof_join_{dir}_on_X_by_Y'.format(dir=direction)\n    return getattr(libjoin, name, None)\n\n\n_type_casters = {\n    'int64_t': ensure_int64,\n    'double': ensure_float64,\n    'object': ensure_object,\n}\n\n\ndef _get_cython_type_upcast(dtype):\n    \"\"\" Upcast a dtype to 'int64_t', 'double', or 'object' \"\"\"\n    if is_integer_dtype(dtype):\n        return 'int64_t'\n    elif is_float_dtype(dtype):\n        return 'double'\n    else:\n        return 'object'\n\n\nclass _AsOfMerge(_OrderedMerge):\n    _merge_type = 'asof_merge'\n\n    def __init__(self, left, right, on=None, left_on=None, right_on=None,\n                 left_index=False, right_index=False,\n                 by=None, left_by=None, right_by=None,\n                 axis=1, suffixes=('_x', '_y'), copy=True,\n                 fill_method=None,\n                 how='asof', tolerance=None,\n                 allow_exact_matches=True,\n                 direction='backward'):\n\n        self.by = by\n        self.left_by = left_by\n        self.right_by = right_by\n        self.tolerance = tolerance\n        self.allow_exact_matches = allow_exact_matches\n        self.direction = direction\n\n        _OrderedMerge.__init__(self, left, right, on=on, left_on=left_on,\n                               right_on=right_on, left_index=left_index,\n                               right_index=right_index, axis=axis,\n                               how=how, suffixes=suffixes,\n                               fill_method=fill_method)\n\n    def _validate_specification(self):\n        super(_AsOfMerge, self)._validate_specification()\n\n        # we only allow on to be a single item for on\n        if len(self.left_on) != 1 and not self.left_index:\n            raise MergeError(\"can only asof on a key for left\")\n\n        if len(self.right_on) != 1 and not self.right_index:\n            raise MergeError(\"can only asof on a key for right\")\n\n        if self.left_index and isinstance(self.left.index, MultiIndex):\n            raise MergeError(\"left can only have one index\")\n\n        if self.right_index and isinstance(self.right.index, MultiIndex):\n            raise MergeError(\"right can only have one index\")\n\n        # set 'by' columns\n        if self.by is not None:\n            if self.left_by is not None or self.right_by is not None:\n                raise MergeError('Can only pass by OR left_by '\n                                 'and right_by')\n            self.left_by = self.right_by = self.by\n        if self.left_by is None and self.right_by is not None:\n            raise MergeError('missing left_by')\n        if self.left_by is not None and self.right_by is None:\n            raise MergeError('missing right_by')\n\n        # add 'by' to our key-list so we can have it in the\n        # output as a key\n        if self.left_by is not None:\n            if not is_list_like(self.left_by):\n                self.left_by = [self.left_by]\n            if not is_list_like(self.right_by):\n                self.right_by = [self.right_by]\n\n            if len(self.left_by) != len(self.right_by):\n                raise MergeError('left_by and right_by must be same length')\n\n            self.left_on = self.left_by + list(self.left_on)\n            self.right_on = self.right_by + list(self.right_on)\n\n        # check 'direction' is valid\n        if self.direction not in ['backward', 'forward', 'nearest']:\n            raise MergeError('direction invalid: {direction}'\n                             .format(direction=self.direction))\n\n    @property\n    def _asof_key(self):\n        \"\"\" This is our asof key, the 'on' \"\"\"\n        return self.left_on[-1]\n\n    def _get_merge_keys(self):\n\n        # note this function has side effects\n        (left_join_keys,\n         right_join_keys,\n         join_names) = super(_AsOfMerge, self)._get_merge_keys()\n\n        # validate index types are the same\n        for i, (lk, rk) in enumerate(zip(left_join_keys, right_join_keys)):\n            if not is_dtype_equal(lk.dtype, rk.dtype):\n                raise MergeError(\"incompatible merge keys [{i}] {lkdtype} and \"\n                                 \"{rkdtype}, must be the same type\"\n                                 .format(i=i, lkdtype=lk.dtype,\n                                         rkdtype=rk.dtype))\n\n        # validate tolerance; must be a Timedelta if we have a DTI\n        if self.tolerance is not None:\n\n            if self.left_index:\n                lt = self.left.index\n            else:\n                lt = left_join_keys[-1]\n\n            msg = (\"incompatible tolerance {tolerance}, must be compat \"\n                   \"with type {lkdtype}\".format(\n                       tolerance=type(self.tolerance),\n                       lkdtype=lt.dtype))\n\n            if is_datetime64_dtype(lt) or is_datetime64tz_dtype(lt):\n                if not isinstance(self.tolerance, Timedelta):\n                    raise MergeError(msg)\n                if self.tolerance < Timedelta(0):\n                    raise MergeError(\"tolerance must be positive\")\n\n            elif is_int64_dtype(lt):\n                if not is_integer(self.tolerance):\n                    raise MergeError(msg)\n                if self.tolerance < 0:\n                    raise MergeError(\"tolerance must be positive\")\n\n            elif is_float_dtype(lt):\n                if not is_number(self.tolerance):\n                    raise MergeError(msg)\n                if self.tolerance < 0:\n                    raise MergeError(\"tolerance must be positive\")\n\n            else:\n                raise MergeError(\"key must be integer, timestamp or float\")\n\n        # validate allow_exact_matches\n        if not is_bool(self.allow_exact_matches):\n            msg = \"allow_exact_matches must be boolean, passed {passed}\"\n            raise MergeError(msg.format(passed=self.allow_exact_matches))\n\n        return left_join_keys, right_join_keys, join_names\n\n    def _get_join_indexers(self):\n        \"\"\" return the join indexers \"\"\"\n\n        def flip(xs):\n            \"\"\" unlike np.transpose, this returns an array of tuples \"\"\"\n            labels = list(string.ascii_lowercase[:len(xs)])\n            dtypes = [x.dtype for x in xs]\n            labeled_dtypes = list(zip(labels, dtypes))\n            return np.array(lzip(*xs), labeled_dtypes)\n\n        # values to compare\n        left_values = (self.left.index.values if self.left_index else\n                       self.left_join_keys[-1])\n        right_values = (self.right.index.values if self.right_index else\n                        self.right_join_keys[-1])\n        tolerance = self.tolerance\n\n        # we require sortedness and non-null values in the join keys\n        msg_sorted = \"{side} keys must be sorted\"\n        msg_missings = \"Merge keys contain null values on {side} side\"\n\n        if not Index(left_values).is_monotonic:\n            if isnull(left_values).any():\n                raise ValueError(msg_missings.format(side='left'))\n            else:\n                raise ValueError(msg_sorted.format(side='left'))\n\n        if not Index(right_values).is_monotonic:\n            if isnull(right_values).any():\n                raise ValueError(msg_missings.format(side='right'))\n            else:\n                raise ValueError(msg_sorted.format(side='right'))\n\n        # initial type conversion as needed\n        if needs_i8_conversion(left_values):\n            left_values = left_values.view('i8')\n            right_values = right_values.view('i8')\n            if tolerance is not None:\n                tolerance = tolerance.value\n\n        # a \"by\" parameter requires special handling\n        if self.left_by is not None:\n            # remove 'on' parameter from values if one existed\n            if self.left_index and self.right_index:\n                left_by_values = self.left_join_keys\n                right_by_values = self.right_join_keys\n            else:\n                left_by_values = self.left_join_keys[0:-1]\n                right_by_values = self.right_join_keys[0:-1]\n\n            # get tuple representation of values if more than one\n            if len(left_by_values) == 1:\n                left_by_values = left_by_values[0]\n                right_by_values = right_by_values[0]\n            else:\n                left_by_values = flip(left_by_values)\n                right_by_values = flip(right_by_values)\n\n            # upcast 'by' parameter because HashTable is limited\n            by_type = _get_cython_type_upcast(left_by_values.dtype)\n            by_type_caster = _type_casters[by_type]\n            left_by_values = by_type_caster(left_by_values)\n            right_by_values = by_type_caster(right_by_values)\n\n            # choose appropriate function by type\n            func = _asof_by_function(self.direction)\n            return func(left_values,\n                        right_values,\n                        left_by_values,\n                        right_by_values,\n                        self.allow_exact_matches,\n                        tolerance)\n        else:\n            # choose appropriate function by type\n            func = _asof_function(self.direction)\n            return func(left_values,\n                        right_values,\n                        self.allow_exact_matches,\n                        tolerance)\n\n\ndef _get_multiindex_indexer(join_keys, index, sort):\n    from functools import partial\n\n    # bind `sort` argument\n    fkeys = partial(_factorize_keys, sort=sort)\n\n    # left & right join labels and num. of levels at each location\n    rlab, llab, shape = map(list, zip(* map(fkeys, index.levels, join_keys)))\n    if sort:\n        rlab = list(map(np.take, rlab, index.labels))\n    else:\n        i8copy = lambda a: a.astype('i8', subok=False, copy=True)\n        rlab = list(map(i8copy, index.labels))\n\n    # fix right labels if there were any nulls\n    for i in range(len(join_keys)):\n        mask = index.labels[i] == -1\n        if mask.any():\n            # check if there already was any nulls at this location\n            # if there was, it is factorized to `shape[i] - 1`\n            a = join_keys[i][llab[i] == shape[i] - 1]\n            if a.size == 0 or not a[0] != a[0]:\n                shape[i] += 1\n\n            rlab[i][mask] = shape[i] - 1\n\n    # get flat i8 join keys\n    lkey, rkey = _get_join_keys(llab, rlab, shape, sort)\n\n    # factorize keys to a dense i8 space\n    lkey, rkey, count = fkeys(lkey, rkey)\n\n    return libjoin.left_outer_join(lkey, rkey, count, sort=sort)\n\n\ndef _get_single_indexer(join_key, index, sort=False):\n    left_key, right_key, count = _factorize_keys(join_key, index, sort=sort)\n\n    left_indexer, right_indexer = libjoin.left_outer_join(\n        ensure_int64(left_key),\n        ensure_int64(right_key),\n        count, sort=sort)\n\n    return left_indexer, right_indexer\n\n\ndef _left_join_on_index(left_ax, right_ax, join_keys, sort=False):\n    if len(join_keys) > 1:\n        if not ((isinstance(right_ax, MultiIndex) and\n                 len(join_keys) == right_ax.nlevels)):\n            raise AssertionError(\"If more than one join key is given then \"\n                                 \"'right_ax' must be a MultiIndex and the \"\n                                 \"number of join keys must be the number of \"\n                                 \"levels in right_ax\")\n\n        left_indexer, right_indexer = \\\n            _get_multiindex_indexer(join_keys, right_ax, sort=sort)\n    else:\n        jkey = join_keys[0]\n\n        left_indexer, right_indexer = \\\n            _get_single_indexer(jkey, right_ax, sort=sort)\n\n    if sort or len(left_ax) != len(left_indexer):\n        # if asked to sort or there are 1-to-many matches\n        join_index = left_ax.take(left_indexer)\n        return join_index, left_indexer, right_indexer\n\n    # left frame preserves order & length of its index\n    return left_ax, None, right_indexer\n\n\ndef _right_outer_join(x, y, max_groups):\n    right_indexer, left_indexer = libjoin.left_outer_join(y, x, max_groups)\n    return left_indexer, right_indexer\n\n\n_join_functions = {\n    'inner': libjoin.inner_join,\n    'left': libjoin.left_outer_join,\n    'right': _right_outer_join,\n    'outer': libjoin.full_outer_join,\n}\n\n\ndef _factorize_keys(lk, rk, sort=True):\n    if is_datetime64tz_dtype(lk) and is_datetime64tz_dtype(rk):\n        lk = lk.values\n        rk = rk.values\n\n    # if we exactly match in categories, allow us to factorize on codes\n    if (is_categorical_dtype(lk) and\n            is_categorical_dtype(rk) and\n            lk.is_dtype_equal(rk)):\n        klass = libhashtable.Int64Factorizer\n\n        if lk.categories.equals(rk.categories):\n            rk = rk.codes\n        else:\n            # Same categories in different orders -> recode\n            rk = _recode_for_categories(rk.codes, rk.categories, lk.categories)\n\n        lk = ensure_int64(lk.codes)\n        rk = ensure_int64(rk)\n    elif is_int_or_datetime_dtype(lk) and is_int_or_datetime_dtype(rk):\n        klass = libhashtable.Int64Factorizer\n        lk = ensure_int64(com.values_from_object(lk))\n        rk = ensure_int64(com.values_from_object(rk))\n    else:\n        klass = libhashtable.Factorizer\n        lk = ensure_object(lk)\n        rk = ensure_object(rk)\n\n    rizer = klass(max(len(lk), len(rk)))\n\n    llab = rizer.factorize(lk)\n    rlab = rizer.factorize(rk)\n\n    count = rizer.get_count()\n\n    if sort:\n        uniques = rizer.uniques.to_array()\n        llab, rlab = _sort_labels(uniques, llab, rlab)\n\n    # NA group\n    lmask = llab == -1\n    lany = lmask.any()\n    rmask = rlab == -1\n    rany = rmask.any()\n\n    if lany or rany:\n        if lany:\n            np.putmask(llab, lmask, count)\n        if rany:\n            np.putmask(rlab, rmask, count)\n        count += 1\n\n    return llab, rlab, count\n\n\ndef _sort_labels(uniques, left, right):\n    if not isinstance(uniques, np.ndarray):\n        # tuplesafe\n        uniques = Index(uniques).values\n\n    llength = len(left)\n    labels = np.concatenate([left, right])\n\n    _, new_labels = sorting.safe_sort(uniques, labels, na_sentinel=-1)\n    new_labels = ensure_int64(new_labels)\n    new_left, new_right = new_labels[:llength], new_labels[llength:]\n\n    return new_left, new_right\n\n\ndef _get_join_keys(llab, rlab, shape, sort):\n\n    # how many levels can be done without overflow\n    pred = lambda i: not is_int64_overflow_possible(shape[:i])\n    nlev = next(filter(pred, range(len(shape), 0, -1)))\n\n    # get keys for the first `nlev` levels\n    stride = np.prod(shape[1:nlev], dtype='i8')\n    lkey = stride * llab[0].astype('i8', subok=False, copy=False)\n    rkey = stride * rlab[0].astype('i8', subok=False, copy=False)\n\n    for i in range(1, nlev):\n        with np.errstate(divide='ignore'):\n            stride //= shape[i]\n        lkey += llab[i] * stride\n        rkey += rlab[i] * stride\n\n    if nlev == len(shape):  # all done!\n        return lkey, rkey\n\n    # densify current keys to avoid overflow\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)\n\n    llab = [lkey] + llab[nlev:]\n    rlab = [rkey] + rlab[nlev:]\n    shape = [count] + shape[nlev:]\n\n    return _get_join_keys(llab, rlab, shape, sort)\n\n\ndef _should_fill(lname, rname):\n    if (not isinstance(lname, compat.string_types) or\n            not isinstance(rname, compat.string_types)):\n        return True\n    return lname == rname\n\n\ndef _any(x):\n    return x is not None and com._any_not_none(*x)\n\n\ndef validate_operand(obj):\n    if isinstance(obj, DataFrame):\n        return obj\n    elif isinstance(obj, Series):\n        if obj.name is None:\n            raise ValueError('Cannot merge a Series without a name')\n        else:\n            return obj.to_frame()\n    else:\n        raise TypeError('Can only merge Series or DataFrame objects, '\n                        'a {obj} was passed'.format(obj=type(obj)))\n"
    },
    {
      "filename": "pandas/core/reshape/pivot.py",
      "content": "# pylint: disable=E1103\nimport numpy as np\n\nfrom pandas.compat import lrange, range, zip\nfrom pandas.util._decorators import Appender, Substitution\n\nfrom pandas.core.dtypes.cast import maybe_downcast_to_dtype\nfrom pandas.core.dtypes.common import is_integer_dtype, is_list_like, is_scalar\nfrom pandas.core.dtypes.generic import ABCDataFrame, ABCSeries\n\nfrom pandas import compat\nimport pandas.core.common as com\nfrom pandas.core.frame import _shared_docs\nfrom pandas.core.groupby import Grouper\nfrom pandas.core.index import Index, MultiIndex, _get_objs_combined_axis\nfrom pandas.core.reshape.concat import concat\nfrom pandas.core.reshape.util import cartesian_product\nfrom pandas.core.series import Series\n\n\n# Note: We need to make sure `frame` is imported before `pivot`, otherwise\n# _shared_docs['pivot_table'] will not yet exist.  TODO: Fix this dependency\n@Substitution('\\ndata : DataFrame')\n@Appender(_shared_docs['pivot_table'], indents=1)\ndef pivot_table(data, values=None, index=None, columns=None, aggfunc='mean',\n                fill_value=None, margins=False, dropna=True,\n                margins_name='All'):\n    index = _convert_by(index)\n    columns = _convert_by(columns)\n\n    if isinstance(aggfunc, list):\n        pieces = []\n        keys = []\n        for func in aggfunc:\n            table = pivot_table(data, values=values, index=index,\n                                columns=columns,\n                                fill_value=fill_value, aggfunc=func,\n                                margins=margins, margins_name=margins_name)\n            pieces.append(table)\n            keys.append(getattr(func, '__name__', func))\n\n        return concat(pieces, keys=keys, axis=1)\n\n    keys = index + columns\n\n    values_passed = values is not None\n    if values_passed:\n        if is_list_like(values):\n            values_multi = True\n            values = list(values)\n        else:\n            values_multi = False\n            values = [values]\n\n        # GH14938 Make sure value labels are in data\n        for i in values:\n            if i not in data:\n                raise KeyError(i)\n\n        to_filter = []\n        for x in keys + values:\n            if isinstance(x, Grouper):\n                x = x.key\n            try:\n                if x in data:\n                    to_filter.append(x)\n            except TypeError:\n                pass\n        if len(to_filter) < len(data.columns):\n            data = data[to_filter]\n\n    else:\n        values = data.columns\n        for key in keys:\n            try:\n                values = values.drop(key)\n            except (TypeError, ValueError, KeyError):\n                pass\n        values = list(values)\n\n    # group by the cartesian product of the grouper\n    # if we have a categorical\n    grouped = data.groupby(keys, observed=False)\n    agged = grouped.agg(aggfunc)\n    if dropna and isinstance(agged, ABCDataFrame) and len(agged.columns):\n        agged = agged.dropna(how='all')\n\n        # gh-21133\n        # we want to down cast if\n        # the original values are ints\n        # as we grouped with a NaN value\n        # and then dropped, coercing to floats\n        for v in [v for v in values if v in data and v in agged]:\n            if (is_integer_dtype(data[v]) and\n                    not is_integer_dtype(agged[v])):\n                agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)\n\n    table = agged\n    if table.index.nlevels > 1:\n        # Related GH #17123\n        # If index_names are integers, determine whether the integers refer\n        # to the level position or name.\n        index_names = agged.index.names[:len(index)]\n        to_unstack = []\n        for i in range(len(index), len(keys)):\n            name = agged.index.names[i]\n            if name is None or name in index_names:\n                to_unstack.append(i)\n            else:\n                to_unstack.append(name)\n        table = agged.unstack(to_unstack)\n\n    if not dropna:\n        from pandas import MultiIndex\n        if table.index.nlevels > 1:\n            m = MultiIndex.from_arrays(cartesian_product(table.index.levels),\n                                       names=table.index.names)\n            table = table.reindex(m, axis=0)\n\n        if table.columns.nlevels > 1:\n            m = MultiIndex.from_arrays(cartesian_product(table.columns.levels),\n                                       names=table.columns.names)\n            table = table.reindex(m, axis=1)\n\n    if isinstance(table, ABCDataFrame):\n        table = table.sort_index(axis=1)\n\n    if fill_value is not None:\n        table = table.fillna(value=fill_value, downcast='infer')\n\n    if margins:\n        if dropna:\n            data = data[data.notna().all(axis=1)]\n        table = _add_margins(table, data, values, rows=index,\n                             cols=columns, aggfunc=aggfunc,\n                             observed=dropna,\n                             margins_name=margins_name, fill_value=fill_value)\n\n    # discard the top level\n    if (values_passed and not values_multi and not table.empty and\n            (table.columns.nlevels > 1)):\n        table = table[values[0]]\n\n    if len(index) == 0 and len(columns) > 0:\n        table = table.T\n\n    # GH 15193 Make sure empty columns are removed if dropna=True\n    if isinstance(table, ABCDataFrame) and dropna:\n        table = table.dropna(how='all', axis=1)\n\n    return table\n\n\ndef _add_margins(table, data, values, rows, cols, aggfunc,\n                 observed=None, margins_name='All', fill_value=None):\n    if not isinstance(margins_name, compat.string_types):\n        raise ValueError('margins_name argument must be a string')\n\n    msg = u'Conflicting name \"{name}\" in margins'.format(name=margins_name)\n    for level in table.index.names:\n        if margins_name in table.index.get_level_values(level):\n            raise ValueError(msg)\n\n    grand_margin = _compute_grand_margin(data, values, aggfunc, margins_name)\n\n    # could be passed a Series object with no 'columns'\n    if hasattr(table, 'columns'):\n        for level in table.columns.names[1:]:\n            if margins_name in table.columns.get_level_values(level):\n                raise ValueError(msg)\n\n    if len(rows) > 1:\n        key = (margins_name,) + ('',) * (len(rows) - 1)\n    else:\n        key = margins_name\n\n    if not values and isinstance(table, ABCSeries):\n        # If there are no values and the table is a series, then there is only\n        # one column in the data. Compute grand margin and return it.\n        return table.append(Series({key: grand_margin[margins_name]}))\n\n    if values:\n        marginal_result_set = _generate_marginal_results(table, data, values,\n                                                         rows, cols, aggfunc,\n                                                         observed,\n                                                         grand_margin,\n                                                         margins_name)\n        if not isinstance(marginal_result_set, tuple):\n            return marginal_result_set\n        result, margin_keys, row_margin = marginal_result_set\n    else:\n        marginal_result_set = _generate_marginal_results_without_values(\n            table, data, rows, cols, aggfunc, observed, margins_name)\n        if not isinstance(marginal_result_set, tuple):\n            return marginal_result_set\n        result, margin_keys, row_margin = marginal_result_set\n    row_margin = row_margin.reindex(result.columns, fill_value=fill_value)\n    # populate grand margin\n    for k in margin_keys:\n        if isinstance(k, compat.string_types):\n            row_margin[k] = grand_margin[k]\n        else:\n            row_margin[k] = grand_margin[k[0]]\n\n    from pandas import DataFrame\n    margin_dummy = DataFrame(row_margin, columns=[key]).T\n\n    row_names = result.index.names\n    try:\n        for dtype in set(result.dtypes):\n            cols = result.select_dtypes([dtype]).columns\n            margin_dummy[cols] = margin_dummy[cols].astype(dtype)\n        result = result.append(margin_dummy)\n    except TypeError:\n\n        # we cannot reshape, so coerce the axis\n        result.index = result.index._to_safe_for_reshape()\n        result = result.append(margin_dummy)\n    result.index.names = row_names\n\n    return result\n\n\ndef _compute_grand_margin(data, values, aggfunc,\n                          margins_name='All'):\n\n    if values:\n        grand_margin = {}\n        for k, v in data[values].iteritems():\n            try:\n                if isinstance(aggfunc, compat.string_types):\n                    grand_margin[k] = getattr(v, aggfunc)()\n                elif isinstance(aggfunc, dict):\n                    if isinstance(aggfunc[k], compat.string_types):\n                        grand_margin[k] = getattr(v, aggfunc[k])()\n                    else:\n                        grand_margin[k] = aggfunc[k](v)\n                else:\n                    grand_margin[k] = aggfunc(v)\n            except TypeError:\n                pass\n        return grand_margin\n    else:\n        return {margins_name: aggfunc(data.index)}\n\n\ndef _generate_marginal_results(table, data, values, rows, cols, aggfunc,\n                               observed,\n                               grand_margin,\n                               margins_name='All'):\n    if len(cols) > 0:\n        # need to \"interleave\" the margins\n        table_pieces = []\n        margin_keys = []\n\n        def _all_key(key):\n            return (key, margins_name) + ('',) * (len(cols) - 1)\n\n        if len(rows) > 0:\n            margin = data[rows + values].groupby(\n                rows, observed=observed).agg(aggfunc)\n            cat_axis = 1\n\n            for key, piece in table.groupby(level=0,\n                                            axis=cat_axis,\n                                            observed=observed):\n                all_key = _all_key(key)\n\n                # we are going to mutate this, so need to copy!\n                piece = piece.copy()\n                try:\n                    piece[all_key] = margin[key]\n                except TypeError:\n\n                    # we cannot reshape, so coerce the axis\n                    piece.set_axis(piece._get_axis(\n                                   cat_axis)._to_safe_for_reshape(),\n                                   axis=cat_axis, inplace=True)\n                    piece[all_key] = margin[key]\n\n                table_pieces.append(piece)\n                margin_keys.append(all_key)\n        else:\n            margin = grand_margin\n            cat_axis = 0\n            for key, piece in table.groupby(level=0,\n                                            axis=cat_axis,\n                                            observed=observed):\n                all_key = _all_key(key)\n                table_pieces.append(piece)\n                table_pieces.append(Series(margin[key], index=[all_key]))\n                margin_keys.append(all_key)\n\n        result = concat(table_pieces, axis=cat_axis)\n\n        if len(rows) == 0:\n            return result\n    else:\n        result = table\n        margin_keys = table.columns\n\n    if len(cols) > 0:\n        row_margin = data[cols + values].groupby(\n            cols, observed=observed).agg(aggfunc)\n        row_margin = row_margin.stack()\n\n        # slight hack\n        new_order = [len(cols)] + lrange(len(cols))\n        row_margin.index = row_margin.index.reorder_levels(new_order)\n    else:\n        row_margin = Series(np.nan, index=result.columns)\n\n    return result, margin_keys, row_margin\n\n\ndef _generate_marginal_results_without_values(\n        table, data, rows, cols, aggfunc,\n        observed, margins_name='All'):\n    if len(cols) > 0:\n        # need to \"interleave\" the margins\n        margin_keys = []\n\n        def _all_key():\n            if len(cols) == 1:\n                return margins_name\n            return (margins_name, ) + ('', ) * (len(cols) - 1)\n\n        if len(rows) > 0:\n            margin = data[rows].groupby(rows,\n                                        observed=observed).apply(aggfunc)\n            all_key = _all_key()\n            table[all_key] = margin\n            result = table\n            margin_keys.append(all_key)\n\n        else:\n            margin = data.groupby(level=0,\n                                  axis=0,\n                                  observed=observed).apply(aggfunc)\n            all_key = _all_key()\n            table[all_key] = margin\n            result = table\n            margin_keys.append(all_key)\n            return result\n    else:\n        result = table\n        margin_keys = table.columns\n\n    if len(cols):\n        row_margin = data[cols].groupby(cols, observed=observed).apply(aggfunc)\n    else:\n        row_margin = Series(np.nan, index=result.columns)\n\n    return result, margin_keys, row_margin\n\n\ndef _convert_by(by):\n    if by is None:\n        by = []\n    elif (is_scalar(by) or\n          isinstance(by, (np.ndarray, Index, ABCSeries, Grouper)) or\n          hasattr(by, '__call__')):\n        by = [by]\n    else:\n        by = list(by)\n    return by\n\n\n@Substitution('\\ndata : DataFrame')\n@Appender(_shared_docs['pivot'], indents=1)\ndef pivot(data, index=None, columns=None, values=None):\n    if values is None:\n        cols = [columns] if index is None else [index, columns]\n        append = index is None\n        indexed = data.set_index(cols, append=append)\n    else:\n        if index is None:\n            index = data.index\n        else:\n            index = data[index]\n        index = MultiIndex.from_arrays([index, data[columns]])\n\n        if is_list_like(values) and not isinstance(values, tuple):\n            # Exclude tuple because it is seen as a single column name\n            indexed = data._constructor(data[values].values, index=index,\n                                        columns=values)\n        else:\n            indexed = data._constructor_sliced(data[values].values,\n                                               index=index)\n    return indexed.unstack(columns)\n\n\ndef crosstab(index, columns, values=None, rownames=None, colnames=None,\n             aggfunc=None, margins=False, margins_name='All', dropna=True,\n             normalize=False):\n    \"\"\"\n    Compute a simple cross-tabulation of two (or more) factors. By default\n    computes a frequency table of the factors unless an array of values and an\n    aggregation function are passed\n\n    Parameters\n    ----------\n    index : array-like, Series, or list of arrays/Series\n        Values to group by in the rows\n    columns : array-like, Series, or list of arrays/Series\n        Values to group by in the columns\n    values : array-like, optional\n        Array of values to aggregate according to the factors.\n        Requires `aggfunc` be specified.\n    rownames : sequence, default None\n        If passed, must match number of row arrays passed\n    colnames : sequence, default None\n        If passed, must match number of column arrays passed\n    aggfunc : function, optional\n        If specified, requires `values` be specified as well\n    margins : boolean, default False\n        Add row/column margins (subtotals)\n    margins_name : string, default 'All'\n        Name of the row / column that will contain the totals\n        when margins is True.\n\n        .. versionadded:: 0.21.0\n\n    dropna : boolean, default True\n        Do not include columns whose entries are all NaN\n    normalize : boolean, {'all', 'index', 'columns'}, or {0,1}, default False\n        Normalize by dividing all values by the sum of values.\n\n        - If passed 'all' or `True`, will normalize over all values.\n        - If passed 'index' will normalize over each row.\n        - If passed 'columns' will normalize over each column.\n        - If margins is `True`, will also normalize margin values.\n\n        .. versionadded:: 0.18.1\n\n    Notes\n    -----\n    Any Series passed will have their name attributes used unless row or column\n    names for the cross-tabulation are specified.\n\n    Any input passed containing Categorical data will have **all** of its\n    categories included in the cross-tabulation, even if the actual data does\n    not contain any instances of a particular category.\n\n    In the event that there aren't overlapping indexes an empty DataFrame will\n    be returned.\n\n    Examples\n    --------\n    >>> a = np.array([\"foo\", \"foo\", \"foo\", \"foo\", \"bar\", \"bar\",\n    ...               \"bar\", \"bar\", \"foo\", \"foo\", \"foo\"], dtype=object)\n    >>> b = np.array([\"one\", \"one\", \"one\", \"two\", \"one\", \"one\",\n    ...               \"one\", \"two\", \"two\", \"two\", \"one\"], dtype=object)\n    >>> c = np.array([\"dull\", \"dull\", \"shiny\", \"dull\", \"dull\", \"shiny\",\n    ...               \"shiny\", \"dull\", \"shiny\", \"shiny\", \"shiny\"],\n    ...               dtype=object)\n\n    >>> pd.crosstab(a, [b, c], rownames=['a'], colnames=['b', 'c'])\n    ... # doctest: +NORMALIZE_WHITESPACE\n    b   one        two\n    c   dull shiny dull shiny\n    a\n    bar    1     2    1     0\n    foo    2     2    1     2\n\n    >>> foo = pd.Categorical(['a', 'b'], categories=['a', 'b', 'c'])\n    >>> bar = pd.Categorical(['d', 'e'], categories=['d', 'e', 'f'])\n    >>> crosstab(foo, bar)  # 'c' and 'f' are not represented in the data,\n                            # and will not be shown in the output because\n                            # dropna is True by default. Set 'dropna=False'\n                            # to preserve categories with no data\n    ... # doctest: +SKIP\n    col_0  d  e\n    row_0\n    a      1  0\n    b      0  1\n\n    >>> crosstab(foo, bar, dropna=False)  # 'c' and 'f' are not represented\n                            # in the data, but they still will be counted\n                            # and shown in the output\n    ... # doctest: +SKIP\n    col_0  d  e  f\n    row_0\n    a      1  0  0\n    b      0  1  0\n    c      0  0  0\n\n    Returns\n    -------\n    crosstab : DataFrame\n    \"\"\"\n\n    index = com.maybe_make_list(index)\n    columns = com.maybe_make_list(columns)\n\n    rownames = _get_names(index, rownames, prefix='row')\n    colnames = _get_names(columns, colnames, prefix='col')\n\n    common_idx = _get_objs_combined_axis(index + columns, intersect=True,\n                                         sort=False)\n\n    data = {}\n    data.update(zip(rownames, index))\n    data.update(zip(colnames, columns))\n\n    if values is None and aggfunc is not None:\n        raise ValueError(\"aggfunc cannot be used without values.\")\n\n    if values is not None and aggfunc is None:\n        raise ValueError(\"values cannot be used without an aggfunc.\")\n\n    from pandas import DataFrame\n    df = DataFrame(data, index=common_idx)\n    if values is None:\n        df['__dummy__'] = 0\n        kwargs = {'aggfunc': len, 'fill_value': 0}\n    else:\n        df['__dummy__'] = values\n        kwargs = {'aggfunc': aggfunc}\n\n    table = df.pivot_table('__dummy__', index=rownames, columns=colnames,\n                           margins=margins, margins_name=margins_name,\n                           dropna=dropna, **kwargs)\n\n    # Post-process\n    if normalize is not False:\n        table = _normalize(table, normalize=normalize, margins=margins,\n                           margins_name=margins_name)\n\n    return table\n\n\ndef _normalize(table, normalize, margins, margins_name='All'):\n\n    if not isinstance(normalize, bool) and not isinstance(normalize,\n                                                          compat.string_types):\n        axis_subs = {0: 'index', 1: 'columns'}\n        try:\n            normalize = axis_subs[normalize]\n        except KeyError:\n            raise ValueError(\"Not a valid normalize argument\")\n\n    if margins is False:\n\n        # Actual Normalizations\n        normalizers = {\n            'all': lambda x: x / x.sum(axis=1).sum(axis=0),\n            'columns': lambda x: x / x.sum(),\n            'index': lambda x: x.div(x.sum(axis=1), axis=0)\n        }\n\n        normalizers[True] = normalizers['all']\n\n        try:\n            f = normalizers[normalize]\n        except KeyError:\n            raise ValueError(\"Not a valid normalize argument\")\n\n        table = f(table)\n        table = table.fillna(0)\n\n    elif margins is True:\n\n        column_margin = table.loc[:, margins_name].drop(margins_name)\n        index_margin = table.loc[margins_name, :].drop(margins_name)\n        table = table.drop(margins_name, axis=1).drop(margins_name)\n        # to keep index and columns names\n        table_index_names = table.index.names\n        table_columns_names = table.columns.names\n\n        # Normalize core\n        table = _normalize(table, normalize=normalize, margins=False)\n\n        # Fix Margins\n        if normalize == 'columns':\n            column_margin = column_margin / column_margin.sum()\n            table = concat([table, column_margin], axis=1)\n            table = table.fillna(0)\n\n        elif normalize == 'index':\n            index_margin = index_margin / index_margin.sum()\n            table = table.append(index_margin)\n            table = table.fillna(0)\n\n        elif normalize == \"all\" or normalize is True:\n            column_margin = column_margin / column_margin.sum()\n            index_margin = index_margin / index_margin.sum()\n            index_margin.loc[margins_name] = 1\n            table = concat([table, column_margin], axis=1)\n            table = table.append(index_margin)\n\n            table = table.fillna(0)\n\n        else:\n            raise ValueError(\"Not a valid normalize argument\")\n\n        table.index.names = table_index_names\n        table.columns.names = table_columns_names\n\n    else:\n        raise ValueError(\"Not a valid margins argument\")\n\n    return table\n\n\ndef _get_names(arrs, names, prefix='row'):\n    if names is None:\n        names = []\n        for i, arr in enumerate(arrs):\n            if isinstance(arr, ABCSeries) and arr.name is not None:\n                names.append(arr.name)\n            else:\n                names.append('{prefix}_{i}'.format(prefix=prefix, i=i))\n    else:\n        if len(names) != len(arrs):\n            raise AssertionError('arrays and names must have the same length')\n        if not isinstance(names, list):\n            names = list(names)\n\n    return names\n"
    },
    {
      "filename": "pandas/core/util/hashing.py",
      "content": "\"\"\"\ndata hash pandas / numpy objects\n\"\"\"\nimport itertools\n\nimport numpy as np\n\nfrom pandas._libs import hashing, tslibs\n\nfrom pandas.core.dtypes.cast import infer_dtype_from_scalar\nfrom pandas.core.dtypes.common import (\n    is_categorical_dtype, is_extension_array_dtype, is_list_like)\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame, ABCIndexClass, ABCMultiIndex, ABCSeries)\nfrom pandas.core.dtypes.missing import isna\n\n# 16 byte long hashing key\n_default_hash_key = '0123456789123456'\n\n\ndef _combine_hash_arrays(arrays, num_items):\n    \"\"\"\n    Parameters\n    ----------\n    arrays : generator\n    num_items : int\n\n    Should be the same as CPython's tupleobject.c\n    \"\"\"\n    try:\n        first = next(arrays)\n    except StopIteration:\n        return np.array([], dtype=np.uint64)\n\n    arrays = itertools.chain([first], arrays)\n\n    mult = np.uint64(1000003)\n    out = np.zeros_like(first) + np.uint64(0x345678)\n    for i, a in enumerate(arrays):\n        inverse_i = num_items - i\n        out ^= a\n        out *= mult\n        mult += np.uint64(82520 + inverse_i + inverse_i)\n    assert i + 1 == num_items, 'Fed in wrong num_items'\n    out += np.uint64(97531)\n    return out\n\n\ndef hash_pandas_object(obj, index=True, encoding='utf8', hash_key=None,\n                       categorize=True):\n    \"\"\"\n    Return a data hash of the Index/Series/DataFrame\n\n    .. versionadded:: 0.19.2\n\n    Parameters\n    ----------\n    index : boolean, default True\n        include the index in the hash (if Series/DataFrame)\n    encoding : string, default 'utf8'\n        encoding for data & key when strings\n    hash_key : string key to encode, default to _default_hash_key\n    categorize : bool, default True\n        Whether to first categorize object arrays before hashing. This is more\n        efficient when the array contains duplicate values.\n\n        .. versionadded:: 0.20.0\n\n    Returns\n    -------\n    Series of uint64, same length as the object\n    \"\"\"\n    from pandas import Series\n    if hash_key is None:\n        hash_key = _default_hash_key\n\n    if isinstance(obj, ABCMultiIndex):\n        return Series(hash_tuples(obj, encoding, hash_key),\n                      dtype='uint64', copy=False)\n\n    if isinstance(obj, ABCIndexClass):\n        h = hash_array(obj.values, encoding, hash_key,\n                       categorize).astype('uint64', copy=False)\n        h = Series(h, index=obj, dtype='uint64', copy=False)\n    elif isinstance(obj, ABCSeries):\n        h = hash_array(obj.values, encoding, hash_key,\n                       categorize).astype('uint64', copy=False)\n        if index:\n            index_iter = (hash_pandas_object(obj.index,\n                                             index=False,\n                                             encoding=encoding,\n                                             hash_key=hash_key,\n                                             categorize=categorize).values\n                          for _ in [None])\n            arrays = itertools.chain([h], index_iter)\n            h = _combine_hash_arrays(arrays, 2)\n\n        h = Series(h, index=obj.index, dtype='uint64', copy=False)\n\n    elif isinstance(obj, ABCDataFrame):\n        hashes = (hash_array(series.values) for _, series in obj.iteritems())\n        num_items = len(obj.columns)\n        if index:\n            index_hash_generator = (hash_pandas_object(obj.index,\n                                                       index=False,\n                                                       encoding=encoding,\n                                                       hash_key=hash_key,\n                                                       categorize=categorize).values  # noqa\n                                    for _ in [None])\n            num_items += 1\n            hashes = itertools.chain(hashes, index_hash_generator)\n        h = _combine_hash_arrays(hashes, num_items)\n\n        h = Series(h, index=obj.index, dtype='uint64', copy=False)\n    else:\n        raise TypeError(\"Unexpected type for hashing %s\" % type(obj))\n    return h\n\n\ndef hash_tuples(vals, encoding='utf8', hash_key=None):\n    \"\"\"\n    Hash an MultiIndex / list-of-tuples efficiently\n\n    .. versionadded:: 0.20.0\n\n    Parameters\n    ----------\n    vals : MultiIndex, list-of-tuples, or single tuple\n    encoding : string, default 'utf8'\n    hash_key : string key to encode, default to _default_hash_key\n\n    Returns\n    -------\n    ndarray of hashed values array\n    \"\"\"\n    is_tuple = False\n    if isinstance(vals, tuple):\n        vals = [vals]\n        is_tuple = True\n    elif not is_list_like(vals):\n        raise TypeError(\"must be convertible to a list-of-tuples\")\n\n    from pandas import Categorical, MultiIndex\n\n    if not isinstance(vals, ABCMultiIndex):\n        vals = MultiIndex.from_tuples(vals)\n\n    # create a list-of-Categoricals\n    vals = [Categorical(vals.labels[level],\n                        vals.levels[level],\n                        ordered=False,\n                        fastpath=True)\n            for level in range(vals.nlevels)]\n\n    # hash the list-of-ndarrays\n    hashes = (_hash_categorical(cat,\n                                encoding=encoding,\n                                hash_key=hash_key)\n              for cat in vals)\n    h = _combine_hash_arrays(hashes, len(vals))\n    if is_tuple:\n        h = h[0]\n\n    return h\n\n\ndef hash_tuple(val, encoding='utf8', hash_key=None):\n    \"\"\"\n    Hash a single tuple efficiently\n\n    Parameters\n    ----------\n    val : single tuple\n    encoding : string, default 'utf8'\n    hash_key : string key to encode, default to _default_hash_key\n\n    Returns\n    -------\n    hash\n\n    \"\"\"\n    hashes = (_hash_scalar(v, encoding=encoding, hash_key=hash_key)\n              for v in val)\n\n    h = _combine_hash_arrays(hashes, len(val))[0]\n\n    return h\n\n\ndef _hash_categorical(c, encoding, hash_key):\n    \"\"\"\n    Hash a Categorical by hashing its categories, and then mapping the codes\n    to the hashes\n\n    Parameters\n    ----------\n    c : Categorical\n    encoding : string, default 'utf8'\n    hash_key : string key to encode, default to _default_hash_key\n\n    Returns\n    -------\n    ndarray of hashed values array, same size as len(c)\n    \"\"\"\n    # Convert ExtensionArrays to ndarrays\n    values = np.asarray(c.categories.values)\n    hashed = hash_array(values, encoding, hash_key,\n                        categorize=False)\n\n    # we have uint64, as we don't directly support missing values\n    # we don't want to use take_nd which will coerce to float\n    # instead, directly construct the result with a\n    # max(np.uint64) as the missing value indicator\n    #\n    # TODO: GH 15362\n\n    mask = c.isna()\n    if len(hashed):\n        result = hashed.take(c.codes)\n    else:\n        result = np.zeros(len(mask), dtype='uint64')\n\n    if mask.any():\n        result[mask] = np.iinfo(np.uint64).max\n\n    return result\n\n\ndef hash_array(vals, encoding='utf8', hash_key=None, categorize=True):\n    \"\"\"\n    Given a 1d array, return an array of deterministic integers.\n\n    .. versionadded:: 0.19.2\n\n    Parameters\n    ----------\n    vals : ndarray, Categorical\n    encoding : string, default 'utf8'\n        encoding for data & key when strings\n    hash_key : string key to encode, default to _default_hash_key\n    categorize : bool, default True\n        Whether to first categorize object arrays before hashing. This is more\n        efficient when the array contains duplicate values.\n\n        .. versionadded:: 0.20.0\n\n    Returns\n    -------\n    1d uint64 numpy array of hash values, same length as the vals\n    \"\"\"\n\n    if not hasattr(vals, 'dtype'):\n        raise TypeError(\"must pass a ndarray-like\")\n    dtype = vals.dtype\n\n    if hash_key is None:\n        hash_key = _default_hash_key\n\n    # For categoricals, we hash the categories, then remap the codes to the\n    # hash values. (This check is above the complex check so that we don't ask\n    # numpy if categorical is a subdtype of complex, as it will choke).\n    if is_categorical_dtype(dtype):\n        return _hash_categorical(vals, encoding, hash_key)\n    elif is_extension_array_dtype(dtype):\n        vals, _ = vals._values_for_factorize()\n        dtype = vals.dtype\n\n    # we'll be working with everything as 64-bit values, so handle this\n    # 128-bit value early\n    if np.issubdtype(dtype, np.complex128):\n        return hash_array(vals.real) + 23 * hash_array(vals.imag)\n\n    # First, turn whatever array this is into unsigned 64-bit ints, if we can\n    # manage it.\n    elif isinstance(dtype, np.bool):\n        vals = vals.astype('u8')\n    elif issubclass(dtype.type, (np.datetime64, np.timedelta64)):\n        vals = vals.view('i8').astype('u8', copy=False)\n    elif issubclass(dtype.type, np.number) and dtype.itemsize <= 8:\n        vals = vals.view('u{}'.format(vals.dtype.itemsize)).astype('u8')\n    else:\n        # With repeated values, its MUCH faster to categorize object dtypes,\n        # then hash and rename categories. We allow skipping the categorization\n        # when the values are known/likely to be unique.\n        if categorize:\n            from pandas import factorize, Categorical, Index\n            codes, categories = factorize(vals, sort=False)\n            cat = Categorical(codes, Index(categories),\n                              ordered=False, fastpath=True)\n            return _hash_categorical(cat, encoding, hash_key)\n\n        try:\n            vals = hashing.hash_object_array(vals, hash_key, encoding)\n        except TypeError:\n            # we have mixed types\n            vals = hashing.hash_object_array(vals.astype(str).astype(object),\n                                             hash_key, encoding)\n\n    # Then, redistribute these 64-bit ints within the space of 64-bit ints\n    vals ^= vals >> 30\n    vals *= np.uint64(0xbf58476d1ce4e5b9)\n    vals ^= vals >> 27\n    vals *= np.uint64(0x94d049bb133111eb)\n    vals ^= vals >> 31\n    return vals\n\n\ndef _hash_scalar(val, encoding='utf8', hash_key=None):\n    \"\"\"\n    Hash scalar value\n\n    Returns\n    -------\n    1d uint64 numpy array of hash value, of length 1\n    \"\"\"\n\n    if isna(val):\n        # this is to be consistent with the _hash_categorical implementation\n        return np.array([np.iinfo(np.uint64).max], dtype='u8')\n\n    if getattr(val, 'tzinfo', None) is not None:\n        # for tz-aware datetimes, we need the underlying naive UTC value and\n        # not the tz aware object or pd extension type (as\n        # infer_dtype_from_scalar would do)\n        if not isinstance(val, tslibs.Timestamp):\n            val = tslibs.Timestamp(val)\n        val = val.tz_convert(None)\n\n    dtype, val = infer_dtype_from_scalar(val)\n    vals = np.array([val], dtype=dtype)\n\n    return hash_array(vals, hash_key=hash_key, encoding=encoding,\n                      categorize=False)\n"
    },
    {
      "filename": "pandas/core/window.py",
      "content": "\"\"\"\n\nprovide a generic structure to support window functions,\nsimilar to how we have a Groupby object\n\n\n\"\"\"\nfrom __future__ import division\n\nfrom collections import defaultdict\nfrom datetime import timedelta\nfrom textwrap import dedent\nimport warnings\n\nimport numpy as np\n\nimport pandas._libs.window as libwindow\nimport pandas.compat as compat\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import Appender, Substitution, cache_readonly\n\nfrom pandas.core.dtypes.common import (\n    ensure_float64, is_bool, is_float_dtype, is_integer, is_integer_dtype,\n    is_list_like, is_scalar, is_timedelta64_dtype, needs_i8_conversion)\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame, ABCDateOffset, ABCDatetimeIndex, ABCPeriodIndex, ABCSeries,\n    ABCTimedeltaIndex)\n\nfrom pandas.core.base import PandasObject, SelectionMixin\nimport pandas.core.common as com\nfrom pandas.core.generic import _shared_docs\nfrom pandas.core.groupby.base import GroupByMixin\n\n_shared_docs = dict(**_shared_docs)\n_doc_template = \"\"\"\n\nReturns\n-------\nsame type as input\n\nSee Also\n--------\npandas.Series.%(name)s\npandas.DataFrame.%(name)s\n\"\"\"\n\n\nclass _Window(PandasObject, SelectionMixin):\n    _attributes = ['window', 'min_periods', 'center', 'win_type',\n                   'axis', 'on', 'closed']\n    exclusions = set()\n\n    def __init__(self, obj, window=None, min_periods=None,\n                 center=False, win_type=None, axis=0, on=None, closed=None,\n                 **kwargs):\n\n        self.__dict__.update(kwargs)\n        self.blocks = []\n        self.obj = obj\n        self.on = on\n        self.closed = closed\n        self.window = window\n        self.min_periods = min_periods\n        self.center = center\n        self.win_type = win_type\n        self.win_freq = None\n        self.axis = obj._get_axis_number(axis) if axis is not None else None\n        self.validate()\n\n    @property\n    def _constructor(self):\n        return Window\n\n    @property\n    def is_datetimelike(self):\n        return None\n\n    @property\n    def _on(self):\n        return None\n\n    @property\n    def is_freq_type(self):\n        return self.win_type == 'freq'\n\n    def validate(self):\n        if self.center is not None and not is_bool(self.center):\n            raise ValueError(\"center must be a boolean\")\n        if (self.min_periods is not None and\n                not is_integer(self.min_periods)):\n            raise ValueError(\"min_periods must be an integer\")\n        if (self.closed is not None and\n                self.closed not in ['right', 'both', 'left', 'neither']):\n            raise ValueError(\"closed must be 'right', 'left', 'both' or \"\n                             \"'neither'\")\n\n    def _convert_freq(self):\n        \"\"\" resample according to the how, return a new object \"\"\"\n\n        obj = self._selected_obj\n        index = None\n        return obj, index\n\n    def _create_blocks(self):\n        \"\"\" split data into blocks & return conformed data \"\"\"\n\n        obj, index = self._convert_freq()\n        if index is not None:\n            index = self._on\n\n        # filter out the on from the object\n        if self.on is not None:\n            if obj.ndim == 2:\n                obj = obj.reindex(columns=obj.columns.difference([self.on]),\n                                  copy=False)\n        blocks = obj._to_dict_of_blocks(copy=False).values()\n\n        return blocks, obj, index\n\n    def _gotitem(self, key, ndim, subset=None):\n        \"\"\"\n        sub-classes to define\n        return a sliced object\n\n        Parameters\n        ----------\n        key : str / list of selections\n        ndim : 1,2\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n\n        # create a new object to prevent aliasing\n        if subset is None:\n            subset = self.obj\n        self = self._shallow_copy(subset)\n        self._reset_cache()\n        if subset.ndim == 2:\n            if is_scalar(key) and key in subset or is_list_like(key):\n                self._selection = key\n        return self\n\n    def __getattr__(self, attr):\n        if attr in self._internal_names_set:\n            return object.__getattribute__(self, attr)\n        if attr in self.obj:\n            return self[attr]\n\n        raise AttributeError(\"%r object has no attribute %r\" %\n                             (type(self).__name__, attr))\n\n    def _dir_additions(self):\n        return self.obj._dir_additions()\n\n    def _get_window(self, other=None):\n        return self.window\n\n    @property\n    def _window_type(self):\n        return self.__class__.__name__\n\n    def __unicode__(self):\n        \"\"\" provide a nice str repr of our rolling object \"\"\"\n\n        attrs = [\"{k}={v}\".format(k=k, v=getattr(self, k))\n                 for k in self._attributes\n                 if getattr(self, k, None) is not None]\n        return \"{klass} [{attrs}]\".format(klass=self._window_type,\n                                          attrs=','.join(attrs))\n\n    def __iter__(self):\n        url = 'https://github.com/pandas-dev/pandas/issues/11704'\n        raise NotImplementedError('See issue #11704 {url}'.format(url=url))\n\n    def _get_index(self, index=None):\n        \"\"\"\n        Return index as ndarrays\n\n        Returns\n        -------\n        tuple of (index, index_as_ndarray)\n        \"\"\"\n\n        if self.is_freq_type:\n            if index is None:\n                index = self._on\n            return index, index.asi8\n        return index, index\n\n    def _prep_values(self, values=None, kill_inf=True):\n\n        if values is None:\n            values = getattr(self._selected_obj, 'values', self._selected_obj)\n\n        # GH #12373 : rolling functions error on float32 data\n        # make sure the data is coerced to float64\n        if is_float_dtype(values.dtype):\n            values = ensure_float64(values)\n        elif is_integer_dtype(values.dtype):\n            values = ensure_float64(values)\n        elif needs_i8_conversion(values.dtype):\n            raise NotImplementedError(\"ops for {action} for this \"\n                                      \"dtype {dtype} are not \"\n                                      \"implemented\".format(\n                                          action=self._window_type,\n                                          dtype=values.dtype))\n        else:\n            try:\n                values = ensure_float64(values)\n            except (ValueError, TypeError):\n                raise TypeError(\"cannot handle this type -> {0}\"\n                                \"\".format(values.dtype))\n\n        if kill_inf:\n            values = values.copy()\n            values[np.isinf(values)] = np.NaN\n\n        return values\n\n    def _wrap_result(self, result, block=None, obj=None):\n        \"\"\" wrap a single result \"\"\"\n\n        if obj is None:\n            obj = self._selected_obj\n        index = obj.index\n\n        if isinstance(result, np.ndarray):\n\n            # coerce if necessary\n            if block is not None:\n                if is_timedelta64_dtype(block.values.dtype):\n                    from pandas import to_timedelta\n                    result = to_timedelta(\n                        result.ravel(), unit='ns').values.reshape(result.shape)\n\n            if result.ndim == 1:\n                from pandas import Series\n                return Series(result, index, name=obj.name)\n\n            return type(obj)(result, index=index, columns=block.columns)\n        return result\n\n    def _wrap_results(self, results, blocks, obj):\n        \"\"\"\n        wrap the results\n\n        Parameters\n        ----------\n        results : list of ndarrays\n        blocks : list of blocks\n        obj : conformed data (may be resampled)\n        \"\"\"\n\n        from pandas import Series, concat\n        from pandas.core.index import ensure_index\n\n        final = []\n        for result, block in zip(results, blocks):\n\n            result = self._wrap_result(result, block=block, obj=obj)\n            if result.ndim == 1:\n                return result\n            final.append(result)\n\n        # if we have an 'on' column\n        # we want to put it back into the results\n        # in the same location\n        columns = self._selected_obj.columns\n        if self.on is not None and not self._on.equals(obj.index):\n\n            name = self._on.name\n            final.append(Series(self._on, index=obj.index, name=name))\n\n            if self._selection is not None:\n\n                selection = ensure_index(self._selection)\n\n                # need to reorder to include original location of\n                # the on column (if its not already there)\n                if name not in selection:\n                    columns = self.obj.columns\n                    indexer = columns.get_indexer(selection.tolist() + [name])\n                    columns = columns.take(sorted(indexer))\n\n        if not len(final):\n            return obj.astype('float64')\n        return concat(final, axis=1).reindex(columns=columns, copy=False)\n\n    def _center_window(self, result, window):\n        \"\"\" center the result in the window \"\"\"\n        if self.axis > result.ndim - 1:\n            raise ValueError(\"Requested axis is larger then no. of argument \"\n                             \"dimensions\")\n\n        offset = _offset(window, True)\n        if offset > 0:\n            if isinstance(result, (ABCSeries, ABCDataFrame)):\n                result = result.slice_shift(-offset, axis=self.axis)\n            else:\n                lead_indexer = [slice(None)] * result.ndim\n                lead_indexer[self.axis] = slice(offset, None)\n                result = np.copy(result[tuple(lead_indexer)])\n        return result\n\n    def aggregate(self, arg, *args, **kwargs):\n        result, how = self._aggregate(arg, *args, **kwargs)\n        if result is None:\n            return self.apply(arg, raw=False, args=args, kwargs=kwargs)\n        return result\n\n    agg = aggregate\n\n    _shared_docs['sum'] = dedent(\"\"\"\n    Calculate %(name)s sum of given DataFrame or Series.\n\n    Parameters\n    ----------\n    *args, **kwargs\n        For compatibility with other %(name)s methods. Has no effect\n        on the computed value.\n\n    Returns\n    -------\n    Series or DataFrame\n        Same type as the input, with the same index, containing the\n        %(name)s sum.\n\n    See Also\n    --------\n    Series.sum : Reducing sum for Series.\n    DataFrame.sum : Reducing sum for DataFrame.\n\n    Examples\n    --------\n    >>> s = pd.Series([1, 2, 3, 4, 5])\n    >>> s\n    0    1\n    1    2\n    2    3\n    3    4\n    4    5\n    dtype: int64\n\n    >>> s.rolling(3).sum()\n    0     NaN\n    1     NaN\n    2     6.0\n    3     9.0\n    4    12.0\n    dtype: float64\n\n    >>> s.expanding(3).sum()\n    0     NaN\n    1     NaN\n    2     6.0\n    3    10.0\n    4    15.0\n    dtype: float64\n\n    >>> s.rolling(3, center=True).sum()\n    0     NaN\n    1     6.0\n    2     9.0\n    3    12.0\n    4     NaN\n    dtype: float64\n\n    For DataFrame, each %(name)s sum is computed column-wise.\n\n    >>> df = pd.DataFrame({\"A\": s, \"B\": s ** 2})\n    >>> df\n       A   B\n    0  1   1\n    1  2   4\n    2  3   9\n    3  4  16\n    4  5  25\n\n    >>> df.rolling(3).sum()\n          A     B\n    0   NaN   NaN\n    1   NaN   NaN\n    2   6.0  14.0\n    3   9.0  29.0\n    4  12.0  50.0\n    \"\"\")\n\n    _shared_docs['mean'] = dedent(\"\"\"\n    Calculate the %(name)s mean of the values.\n\n    Parameters\n    ----------\n    *args\n        Under Review.\n    **kwargs\n        Under Review.\n\n    Returns\n    -------\n    Series or DataFrame\n        Returned object type is determined by the caller of the %(name)s\n        calculation.\n\n    See Also\n    --------\n    Series.%(name)s : Calling object with Series data.\n    DataFrame.%(name)s : Calling object with DataFrames.\n    Series.mean : Equivalent method for Series.\n    DataFrame.mean : Equivalent method for DataFrame.\n\n    Examples\n    --------\n    The below examples will show rolling mean calculations with window sizes of\n    two and three, respectively.\n\n    >>> s = pd.Series([1, 2, 3, 4])\n    >>> s.rolling(2).mean()\n    0    NaN\n    1    1.5\n    2    2.5\n    3    3.5\n    dtype: float64\n\n    >>> s.rolling(3).mean()\n    0    NaN\n    1    NaN\n    2    2.0\n    3    3.0\n    dtype: float64\n    \"\"\")\n\n\nclass Window(_Window):\n    \"\"\"\n    Provides rolling window calculations.\n\n    .. versionadded:: 0.18.0\n\n    Parameters\n    ----------\n    window : int, or offset\n        Size of the moving window. This is the number of observations used for\n        calculating the statistic. Each window will be a fixed size.\n\n        If its an offset then this will be the time period of each window. Each\n        window will be a variable sized based on the observations included in\n        the time-period. This is only valid for datetimelike indexes. This is\n        new in 0.19.0\n    min_periods : int, default None\n        Minimum number of observations in window required to have a value\n        (otherwise result is NA). For a window that is specified by an offset,\n        `min_periods` will default to 1. Otherwise, `min_periods` will default\n        to the size of the window.\n    center : bool, default False\n        Set the labels at the center of the window.\n    win_type : str, default None\n        Provide a window type. If ``None``, all points are evenly weighted.\n        See the notes below for further information.\n    on : str, optional\n        For a DataFrame, column on which to calculate\n        the rolling window, rather than the index\n    axis : int or str, default 0\n    closed : str, default None\n        Make the interval closed on the 'right', 'left', 'both' or\n        'neither' endpoints.\n        For offset-based windows, it defaults to 'right'.\n        For fixed windows, defaults to 'both'. Remaining cases not implemented\n        for fixed windows.\n\n        .. versionadded:: 0.20.0\n\n    Returns\n    -------\n    a Window or Rolling sub-classed for the particular operation\n\n    Examples\n    --------\n\n    >>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n    >>> df\n         B\n    0  0.0\n    1  1.0\n    2  2.0\n    3  NaN\n    4  4.0\n\n    Rolling sum with a window length of 2, using the 'triang'\n    window type.\n\n    >>> df.rolling(2, win_type='triang').sum()\n         B\n    0  NaN\n    1  1.0\n    2  2.5\n    3  NaN\n    4  NaN\n\n    Rolling sum with a window length of 2, min_periods defaults\n    to the window length.\n\n    >>> df.rolling(2).sum()\n         B\n    0  NaN\n    1  1.0\n    2  3.0\n    3  NaN\n    4  NaN\n\n    Same as above, but explicitly set the min_periods\n\n    >>> df.rolling(2, min_periods=1).sum()\n         B\n    0  0.0\n    1  1.0\n    2  3.0\n    3  2.0\n    4  4.0\n\n    A ragged (meaning not-a-regular frequency), time-indexed DataFrame\n\n    >>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]},\n    ...                   index = [pd.Timestamp('20130101 09:00:00'),\n    ...                            pd.Timestamp('20130101 09:00:02'),\n    ...                            pd.Timestamp('20130101 09:00:03'),\n    ...                            pd.Timestamp('20130101 09:00:05'),\n    ...                            pd.Timestamp('20130101 09:00:06')])\n\n    >>> df\n                           B\n    2013-01-01 09:00:00  0.0\n    2013-01-01 09:00:02  1.0\n    2013-01-01 09:00:03  2.0\n    2013-01-01 09:00:05  NaN\n    2013-01-01 09:00:06  4.0\n\n    Contrasting to an integer rolling window, this will roll a variable\n    length window corresponding to the time period.\n    The default for min_periods is 1.\n\n    >>> df.rolling('2s').sum()\n                           B\n    2013-01-01 09:00:00  0.0\n    2013-01-01 09:00:02  1.0\n    2013-01-01 09:00:03  3.0\n    2013-01-01 09:00:05  NaN\n    2013-01-01 09:00:06  4.0\n\n    Notes\n    -----\n    By default, the result is set to the right edge of the window. This can be\n    changed to the center of the window by setting ``center=True``.\n\n    To learn more about the offsets & frequency strings, please see `this link\n    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.\n\n    The recognized win_types are:\n\n    * ``boxcar``\n    * ``triang``\n    * ``blackman``\n    * ``hamming``\n    * ``bartlett``\n    * ``parzen``\n    * ``bohman``\n    * ``blackmanharris``\n    * ``nuttall``\n    * ``barthann``\n    * ``kaiser`` (needs beta)\n    * ``gaussian`` (needs std)\n    * ``general_gaussian`` (needs power, width)\n    * ``slepian`` (needs width).\n\n    If ``win_type=None`` all points are evenly weighted. To learn more about\n    different window types see `scipy.signal window functions\n    <https://docs.scipy.org/doc/scipy/reference/signal.html#window-functions>`__.\n\n    See Also\n    --------\n    expanding : Provides expanding transformations.\n    ewm : Provides exponential weighted functions.\n    \"\"\"\n\n    def validate(self):\n        super(Window, self).validate()\n\n        window = self.window\n        if isinstance(window, (list, tuple, np.ndarray)):\n            pass\n        elif is_integer(window):\n            if window <= 0:\n                raise ValueError(\"window must be > 0 \")\n            try:\n                import scipy.signal as sig\n            except ImportError:\n                raise ImportError('Please install scipy to generate window '\n                                  'weight')\n\n            if not isinstance(self.win_type, compat.string_types):\n                raise ValueError('Invalid win_type {0}'.format(self.win_type))\n            if getattr(sig, self.win_type, None) is None:\n                raise ValueError('Invalid win_type {0}'.format(self.win_type))\n        else:\n            raise ValueError('Invalid window {0}'.format(window))\n\n    def _prep_window(self, **kwargs):\n        \"\"\"\n        provide validation for our window type, return the window\n        we have already been validated\n        \"\"\"\n\n        window = self._get_window()\n        if isinstance(window, (list, tuple, np.ndarray)):\n            return com.asarray_tuplesafe(window).astype(float)\n        elif is_integer(window):\n            import scipy.signal as sig\n\n            # the below may pop from kwargs\n            def _validate_win_type(win_type, kwargs):\n                arg_map = {'kaiser': ['beta'],\n                           'gaussian': ['std'],\n                           'general_gaussian': ['power', 'width'],\n                           'slepian': ['width']}\n                if win_type in arg_map:\n                    return tuple([win_type] + _pop_args(win_type,\n                                                        arg_map[win_type],\n                                                        kwargs))\n                return win_type\n\n            def _pop_args(win_type, arg_names, kwargs):\n                msg = '%s window requires %%s' % win_type\n                all_args = []\n                for n in arg_names:\n                    if n not in kwargs:\n                        raise ValueError(msg % n)\n                    all_args.append(kwargs.pop(n))\n                return all_args\n\n            win_type = _validate_win_type(self.win_type, kwargs)\n            # GH #15662. `False` makes symmetric window, rather than periodic.\n            return sig.get_window(win_type, window, False).astype(float)\n\n    def _apply_window(self, mean=True, **kwargs):\n        \"\"\"\n        Applies a moving window of type ``window_type`` on the data.\n\n        Parameters\n        ----------\n        mean : bool, default True\n            If True computes weighted mean, else weighted sum\n\n        Returns\n        -------\n        y : same type as input argument\n\n        \"\"\"\n        window = self._prep_window(**kwargs)\n        center = self.center\n\n        blocks, obj, index = self._create_blocks()\n        results = []\n        for b in blocks:\n            try:\n                values = self._prep_values(b.values)\n            except TypeError:\n                results.append(b.values.copy())\n                continue\n\n            if values.size == 0:\n                results.append(values.copy())\n                continue\n\n            offset = _offset(window, center)\n            additional_nans = np.array([np.NaN] * offset)\n\n            def f(arg, *args, **kwargs):\n                minp = _use_window(self.min_periods, len(window))\n                return libwindow.roll_window(np.concatenate((arg,\n                                                             additional_nans))\n                                             if center else arg, window, minp,\n                                             avg=mean)\n\n            result = np.apply_along_axis(f, self.axis, values)\n\n            if center:\n                result = self._center_window(result, window)\n            results.append(result)\n\n        return self._wrap_results(results, blocks, obj)\n\n    _agg_doc = dedent(\"\"\"\n    Examples\n    --------\n\n    >>> df = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'])\n    >>> df\n              A         B         C\n    0 -2.385977 -0.102758  0.438822\n    1 -1.004295  0.905829 -0.954544\n    2  0.735167 -0.165272 -1.619346\n    3 -0.702657 -1.340923 -0.706334\n    4 -0.246845  0.211596 -0.901819\n    5  2.463718  3.157577 -1.380906\n    6 -1.142255  2.340594 -0.039875\n    7  1.396598 -1.647453  1.677227\n    8 -0.543425  1.761277 -0.220481\n    9 -0.640505  0.289374 -1.550670\n\n    >>> df.rolling(3, win_type='boxcar').agg('mean')\n              A         B         C\n    0       NaN       NaN       NaN\n    1       NaN       NaN       NaN\n    2 -0.885035  0.212600 -0.711689\n    3 -0.323928 -0.200122 -1.093408\n    4 -0.071445 -0.431533 -1.075833\n    5  0.504739  0.676083 -0.996353\n    6  0.358206  1.903256 -0.774200\n    7  0.906020  1.283573  0.085482\n    8 -0.096361  0.818139  0.472290\n    9  0.070889  0.134399 -0.031308\n\n    See Also\n    --------\n    pandas.DataFrame.rolling.aggregate\n    pandas.DataFrame.aggregate\n\n    \"\"\")\n\n    @Appender(_agg_doc)\n    @Appender(_shared_docs['aggregate'] % dict(\n        versionadded='',\n        klass='Series/DataFrame',\n        axis=''))\n    def aggregate(self, arg, *args, **kwargs):\n        result, how = self._aggregate(arg, *args, **kwargs)\n        if result is None:\n\n            # these must apply directly\n            result = arg(self)\n\n        return result\n\n    agg = aggregate\n\n    @Substitution(name='window')\n    @Appender(_shared_docs['sum'])\n    def sum(self, *args, **kwargs):\n        nv.validate_window_func('sum', args, kwargs)\n        return self._apply_window(mean=False, **kwargs)\n\n    @Substitution(name='window')\n    @Appender(_shared_docs['mean'])\n    def mean(self, *args, **kwargs):\n        nv.validate_window_func('mean', args, kwargs)\n        return self._apply_window(mean=True, **kwargs)\n\n\nclass _GroupByMixin(GroupByMixin):\n    \"\"\" provide the groupby facilities \"\"\"\n\n    def __init__(self, obj, *args, **kwargs):\n        parent = kwargs.pop('parent', None)  # noqa\n        groupby = kwargs.pop('groupby', None)\n        if groupby is None:\n            groupby, obj = obj, obj.obj\n        self._groupby = groupby\n        self._groupby.mutated = True\n        self._groupby.grouper.mutated = True\n        super(GroupByMixin, self).__init__(obj, *args, **kwargs)\n\n    count = GroupByMixin._dispatch('count')\n    corr = GroupByMixin._dispatch('corr', other=None, pairwise=None)\n    cov = GroupByMixin._dispatch('cov', other=None, pairwise=None)\n\n    def _apply(self, func, name, window=None, center=None,\n               check_minp=None, **kwargs):\n        \"\"\"\n        dispatch to apply; we are stripping all of the _apply kwargs and\n        performing the original function call on the grouped object\n        \"\"\"\n\n        def f(x, name=name, *args):\n            x = self._shallow_copy(x)\n\n            if isinstance(name, compat.string_types):\n                return getattr(x, name)(*args, **kwargs)\n\n            return x.apply(name, *args, **kwargs)\n\n        return self._groupby.apply(f)\n\n\nclass _Rolling(_Window):\n\n    @property\n    def _constructor(self):\n        return Rolling\n\n    def _apply(self, func, name=None, window=None, center=None,\n               check_minp=None, **kwargs):\n        \"\"\"\n        Rolling statistical measure using supplied function. Designed to be\n        used with passed-in Cython array-based functions.\n\n        Parameters\n        ----------\n        func : str/callable to apply\n        name : str, optional\n           name of this function\n        window : int/array, default to _get_window()\n        center : bool, default to self.center\n        check_minp : function, default to _use_window\n\n        Returns\n        -------\n        y : type of input\n        \"\"\"\n        if center is None:\n            center = self.center\n        if window is None:\n            window = self._get_window()\n\n        if check_minp is None:\n            check_minp = _use_window\n\n        blocks, obj, index = self._create_blocks()\n        index, indexi = self._get_index(index=index)\n        results = []\n        for b in blocks:\n            values = self._prep_values(b.values)\n\n            if values.size == 0:\n                results.append(values.copy())\n                continue\n\n            # if we have a string function name, wrap it\n            if isinstance(func, compat.string_types):\n                cfunc = getattr(libwindow, func, None)\n                if cfunc is None:\n                    raise ValueError(\"we do not support this function \"\n                                     \"in libwindow.{func}\".format(func=func))\n\n                def func(arg, window, min_periods=None, closed=None):\n                    minp = check_minp(min_periods, window)\n                    # ensure we are only rolling on floats\n                    arg = ensure_float64(arg)\n                    return cfunc(arg,\n                                 window, minp, indexi, closed, **kwargs)\n\n            # calculation function\n            if center:\n                offset = _offset(window, center)\n                additional_nans = np.array([np.NaN] * offset)\n\n                def calc(x):\n                    return func(np.concatenate((x, additional_nans)),\n                                window, min_periods=self.min_periods,\n                                closed=self.closed)\n            else:\n\n                def calc(x):\n                    return func(x, window, min_periods=self.min_periods,\n                                closed=self.closed)\n\n            with np.errstate(all='ignore'):\n                if values.ndim > 1:\n                    result = np.apply_along_axis(calc, self.axis, values)\n                else:\n                    result = calc(values)\n\n            if center:\n                result = self._center_window(result, window)\n\n            results.append(result)\n\n        return self._wrap_results(results, blocks, obj)\n\n\nclass _Rolling_and_Expanding(_Rolling):\n\n    _shared_docs['count'] = dedent(r\"\"\"\n    The %(name)s count of any non-NaN observations inside the window.\n\n    Returns\n    -------\n    Series or DataFrame\n        Returned object type is determined by the caller of the %(name)s\n        calculation.\n\n    See Also\n    --------\n    pandas.Series.%(name)s : Calling object with Series data.\n    pandas.DataFrame.%(name)s : Calling object with DataFrames.\n    pandas.DataFrame.count : Count of the full DataFrame.\n\n    Examples\n    --------\n    >>> s = pd.Series([2, 3, np.nan, 10])\n    >>> s.rolling(2).count()\n    0    1.0\n    1    2.0\n    2    1.0\n    3    1.0\n    dtype: float64\n    >>> s.rolling(3).count()\n    0    1.0\n    1    2.0\n    2    2.0\n    3    2.0\n    dtype: float64\n    >>> s.rolling(4).count()\n    0    1.0\n    1    2.0\n    2    2.0\n    3    3.0\n    dtype: float64\n    \"\"\")\n\n    def count(self):\n\n        blocks, obj, index = self._create_blocks()\n        # Validate the index\n        self._get_index(index=index)\n\n        window = self._get_window()\n        window = min(window, len(obj)) if not self.center else window\n\n        results = []\n        for b in blocks:\n            result = b.notna().astype(int)\n            result = self._constructor(result, window=window, min_periods=0,\n                                       center=self.center,\n                                       closed=self.closed).sum()\n            results.append(result)\n\n        return self._wrap_results(results, blocks, obj)\n\n    _shared_docs['apply'] = dedent(r\"\"\"\n    %(name)s function apply\n\n    Parameters\n    ----------\n    func : function\n        Must produce a single value from an ndarray input if ``raw=True``\n        or a Series if ``raw=False``\n    raw : bool, default None\n        * ``False`` : passes each row or column as a Series to the\n          function.\n        * ``True`` or ``None`` : the passed function will receive ndarray\n          objects instead.\n          If you are just applying a NumPy reduction function this will\n          achieve much better performance.\n\n        The `raw` parameter is required and will show a FutureWarning if\n        not passed. In the future `raw` will default to False.\n\n        .. versionadded:: 0.23.0\n\n    \\*args and \\*\\*kwargs are passed to the function\"\"\")\n\n    def apply(self, func, raw=None, args=(), kwargs={}):\n        from pandas import Series\n\n        # TODO: _level is unused?\n        _level = kwargs.pop('_level', None)  # noqa\n        window = self._get_window()\n        offset = _offset(window, self.center)\n        index, indexi = self._get_index()\n\n        # TODO: default is for backward compat\n        # change to False in the future\n        if raw is None:\n            warnings.warn(\n                \"Currently, 'apply' passes the values as ndarrays to the \"\n                \"applied function. In the future, this will change to passing \"\n                \"it as Series objects. You need to specify 'raw=True' to keep \"\n                \"the current behaviour, and you can pass 'raw=False' to \"\n                \"silence this warning\", FutureWarning, stacklevel=3)\n            raw = True\n\n        def f(arg, window, min_periods, closed):\n            minp = _use_window(min_periods, window)\n            if not raw:\n                arg = Series(arg, index=self.obj.index)\n            return libwindow.roll_generic(\n                arg, window, minp, indexi,\n                closed, offset, func, raw, args, kwargs)\n\n        return self._apply(f, func, args=args, kwargs=kwargs,\n                           center=False, raw=raw)\n\n    def sum(self, *args, **kwargs):\n        nv.validate_window_func('sum', args, kwargs)\n        return self._apply('roll_sum', 'sum', **kwargs)\n\n    _shared_docs['max'] = dedent(\"\"\"\n    %(name)s maximum\n    \"\"\")\n\n    def max(self, *args, **kwargs):\n        nv.validate_window_func('max', args, kwargs)\n        return self._apply('roll_max', 'max', **kwargs)\n\n    _shared_docs['min'] = dedent(\"\"\"\n    Calculate the %(name)s minimum.\n\n    Parameters\n    ----------\n    **kwargs\n        Under Review.\n\n    Returns\n    -------\n    Series or DataFrame\n        Returned object type is determined by the caller of the %(name)s\n        calculation.\n\n    See Also\n    --------\n    Series.%(name)s : Calling object with a Series.\n    DataFrame.%(name)s : Calling object with a DataFrame.\n    Series.min : Similar method for Series.\n    DataFrame.min : Similar method for DataFrame.\n\n    Examples\n    --------\n    Performing a rolling minimum with a window size of 3.\n\n    >>> s = pd.Series([4, 3, 5, 2, 6])\n    >>> s.rolling(3).min()\n    0    NaN\n    1    NaN\n    2    3.0\n    3    2.0\n    4    2.0\n    dtype: float64\n    \"\"\")\n\n    def min(self, *args, **kwargs):\n        nv.validate_window_func('min', args, kwargs)\n        return self._apply('roll_min', 'min', **kwargs)\n\n    def mean(self, *args, **kwargs):\n        nv.validate_window_func('mean', args, kwargs)\n        return self._apply('roll_mean', 'mean', **kwargs)\n\n    _shared_docs['median'] = dedent(\"\"\"\n    Calculate the %(name)s median.\n\n    Parameters\n    ----------\n    **kwargs\n        For compatibility with other %(name)s methods. Has no effect\n        on the computed median.\n\n    Returns\n    -------\n    Series or DataFrame\n        Returned type is the same as the original object.\n\n    See Also\n    --------\n    Series.%(name)s : Calling object with Series data.\n    DataFrame.%(name)s : Calling object with DataFrames.\n    Series.median : Equivalent method for Series.\n    DataFrame.median : Equivalent method for DataFrame.\n\n    Examples\n    --------\n    Compute the rolling median of a series with a window size of 3.\n\n    >>> s = pd.Series([0, 1, 2, 3, 4])\n    >>> s.rolling(3).median()\n    0    NaN\n    1    NaN\n    2    1.0\n    3    2.0\n    4    3.0\n    dtype: float64\n    \"\"\")\n\n    def median(self, **kwargs):\n        return self._apply('roll_median_c', 'median', **kwargs)\n\n    _shared_docs['std'] = dedent(\"\"\"\n    Calculate %(name)s standard deviation.\n\n    Normalized by N-1 by default. This can be changed using the `ddof`\n    argument.\n\n    Parameters\n    ----------\n    ddof : int, default 1\n        Delta Degrees of Freedom.  The divisor used in calculations\n        is ``N - ddof``, where ``N`` represents the number of elements.\n    *args, **kwargs\n        For NumPy compatibility. No additional arguments are used.\n\n    Returns\n    -------\n    Series or DataFrame\n        Returns the same object type as the caller of the %(name)s calculation.\n\n    See Also\n    --------\n    Series.%(name)s : Calling object with Series data.\n    DataFrame.%(name)s : Calling object with DataFrames.\n    Series.std : Equivalent method for Series.\n    DataFrame.std : Equivalent method for DataFrame.\n    numpy.std : Equivalent method for Numpy array.\n\n    Notes\n    -----\n    The default `ddof` of 1 used in Series.std is different than the default\n    `ddof` of 0 in numpy.std.\n\n    A minimum of one period is required for the rolling calculation.\n\n    Examples\n    --------\n    >>> s = pd.Series([5, 5, 6, 7, 5, 5, 5])\n    >>> s.rolling(3).std()\n    0         NaN\n    1         NaN\n    2    0.577350\n    3    1.000000\n    4    1.000000\n    5    1.154701\n    6    0.000000\n    dtype: float64\n\n    >>> s.expanding(3).std()\n    0         NaN\n    1         NaN\n    2    0.577350\n    3    0.957427\n    4    0.894427\n    5    0.836660\n    6    0.786796\n    dtype: float64\n    \"\"\")\n\n    def std(self, ddof=1, *args, **kwargs):\n        nv.validate_window_func('std', args, kwargs)\n        window = self._get_window()\n        index, indexi = self._get_index()\n\n        def f(arg, *args, **kwargs):\n            minp = _require_min_periods(1)(self.min_periods, window)\n            return _zsqrt(libwindow.roll_var(arg, window, minp, indexi,\n                                             self.closed, ddof))\n\n        return self._apply(f, 'std', check_minp=_require_min_periods(1),\n                           ddof=ddof, **kwargs)\n\n    _shared_docs['var'] = dedent(\"\"\"\n    Calculate unbiased %(name)s variance.\n\n    Normalized by N-1 by default. This can be changed using the `ddof`\n    argument.\n\n    Parameters\n    ----------\n    ddof : int, default 1\n        Delta Degrees of Freedom.  The divisor used in calculations\n        is ``N - ddof``, where ``N`` represents the number of elements.\n    *args, **kwargs\n        For NumPy compatibility. No additional arguments are used.\n\n    Returns\n    -------\n    Series or DataFrame\n        Returns the same object type as the caller of the %(name)s calculation.\n\n    See Also\n    --------\n    Series.%(name)s : Calling object with Series data.\n    DataFrame.%(name)s : Calling object with DataFrames.\n    Series.var : Equivalent method for Series.\n    DataFrame.var : Equivalent method for DataFrame.\n    numpy.var : Equivalent method for Numpy array.\n\n    Notes\n    -----\n    The default `ddof` of 1 used in :meth:`Series.var` is different than the\n    default `ddof` of 0 in :func:`numpy.var`.\n\n    A minimum of 1 period is required for the rolling calculation.\n\n    Examples\n    --------\n    >>> s = pd.Series([5, 5, 6, 7, 5, 5, 5])\n    >>> s.rolling(3).var()\n    0         NaN\n    1         NaN\n    2    0.333333\n    3    1.000000\n    4    1.000000\n    5    1.333333\n    6    0.000000\n    dtype: float64\n\n    >>> s.expanding(3).var()\n    0         NaN\n    1         NaN\n    2    0.333333\n    3    0.916667\n    4    0.800000\n    5    0.700000\n    6    0.619048\n    dtype: float64\n    \"\"\")\n\n    def var(self, ddof=1, *args, **kwargs):\n        nv.validate_window_func('var', args, kwargs)\n        return self._apply('roll_var', 'var',\n                           check_minp=_require_min_periods(1), ddof=ddof,\n                           **kwargs)\n\n    _shared_docs['skew'] = \"\"\"Unbiased %(name)s skewness\"\"\"\n\n    def skew(self, **kwargs):\n        return self._apply('roll_skew', 'skew',\n                           check_minp=_require_min_periods(3), **kwargs)\n\n    _shared_docs['kurt'] = dedent(\"\"\"\n    Calculate unbiased %(name)s kurtosis.\n\n    This function uses Fisher's definition of kurtosis without bias.\n\n    Parameters\n    ----------\n    **kwargs\n        Under Review.\n\n    Returns\n    -------\n    Series or DataFrame\n        Returned object type is determined by the caller of the %(name)s\n        calculation\n\n    See Also\n    --------\n    Series.%(name)s : Calling object with Series data.\n    DataFrame.%(name)s : Calling object with DataFrames.\n    Series.kurt : Equivalent method for Series.\n    DataFrame.kurt : Equivalent method for DataFrame.\n    scipy.stats.skew : Third moment of a probability density.\n    scipy.stats.kurtosis : Reference SciPy method.\n\n    Notes\n    -----\n    A minimum of 4 periods is required for the %(name)s calculation.\n    \"\"\")\n\n    def kurt(self, **kwargs):\n        return self._apply('roll_kurt', 'kurt',\n                           check_minp=_require_min_periods(4), **kwargs)\n\n    _shared_docs['quantile'] = dedent(\"\"\"\n    %(name)s quantile.\n\n    Parameters\n    ----------\n    quantile : float\n        Quantile to compute. 0 <= quantile <= 1.\n    interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n        .. versionadded:: 0.23.0\n\n        This optional parameter specifies the interpolation method to use,\n        when the desired quantile lies between two data points `i` and `j`:\n\n            * linear: `i + (j - i) * fraction`, where `fraction` is the\n              fractional part of the index surrounded by `i` and `j`.\n            * lower: `i`.\n            * higher: `j`.\n            * nearest: `i` or `j` whichever is nearest.\n            * midpoint: (`i` + `j`) / 2.\n    **kwargs:\n        For compatibility with other %(name)s methods. Has no effect on\n        the result.\n\n    Returns\n    -------\n    Series or DataFrame\n        Returned object type is determined by the caller of the %(name)s\n        calculation.\n\n    Examples\n    --------\n    >>> s = pd.Series([1, 2, 3, 4])\n    >>> s.rolling(2).quantile(.4, interpolation='lower')\n    0    NaN\n    1    1.0\n    2    2.0\n    3    3.0\n    dtype: float64\n\n    >>> s.rolling(2).quantile(.4, interpolation='midpoint')\n    0    NaN\n    1    1.5\n    2    2.5\n    3    3.5\n    dtype: float64\n\n    See Also\n    --------\n    pandas.Series.quantile : Computes value at the given quantile over all data\n        in Series.\n    pandas.DataFrame.quantile : Computes values at the given quantile over\n        requested axis in DataFrame.\n    \"\"\")\n\n    def quantile(self, quantile, interpolation='linear', **kwargs):\n        window = self._get_window()\n        index, indexi = self._get_index()\n\n        def f(arg, *args, **kwargs):\n            minp = _use_window(self.min_periods, window)\n            if quantile == 1.0:\n                return libwindow.roll_max(arg, window, minp, indexi,\n                                          self.closed)\n            elif quantile == 0.0:\n                return libwindow.roll_min(arg, window, minp, indexi,\n                                          self.closed)\n            else:\n                return libwindow.roll_quantile(arg, window, minp, indexi,\n                                               self.closed, quantile,\n                                               interpolation)\n\n        return self._apply(f, 'quantile', quantile=quantile,\n                           **kwargs)\n\n    _shared_docs['cov'] = dedent(\"\"\"\n    %(name)s sample covariance\n\n    Parameters\n    ----------\n    other : Series, DataFrame, or ndarray, optional\n        if not supplied then will default to self and produce pairwise output\n    pairwise : bool, default None\n        If False then only matching columns between self and other will be used\n        and the output will be a DataFrame.\n        If True then all pairwise combinations will be calculated and the\n        output will be a MultiIndexed DataFrame in the case of DataFrame\n        inputs. In the case of missing elements, only complete pairwise\n        observations will be used.\n    ddof : int, default 1\n        Delta Degrees of Freedom.  The divisor used in calculations\n        is ``N - ddof``, where ``N`` represents the number of elements.\"\"\")\n\n    def cov(self, other=None, pairwise=None, ddof=1, **kwargs):\n        if other is None:\n            other = self._selected_obj\n            # only default unset\n            pairwise = True if pairwise is None else pairwise\n        other = self._shallow_copy(other)\n\n        # GH 16058: offset window\n        if self.is_freq_type:\n            window = self.win_freq\n        else:\n            window = self._get_window(other)\n\n        def _get_cov(X, Y):\n            # GH #12373 : rolling functions error on float32 data\n            # to avoid potential overflow, cast the data to float64\n            X = X.astype('float64')\n            Y = Y.astype('float64')\n            mean = lambda x: x.rolling(window, self.min_periods,\n                                       center=self.center).mean(**kwargs)\n            count = (X + Y).rolling(window=window,\n                                    center=self.center).count(**kwargs)\n            bias_adj = count / (count - ddof)\n            return (mean(X * Y) - mean(X) * mean(Y)) * bias_adj\n\n        return _flex_binary_moment(self._selected_obj, other._selected_obj,\n                                   _get_cov, pairwise=bool(pairwise))\n\n    _shared_docs['corr'] = dedent(\"\"\"\n    Calculate %(name)s correlation.\n\n    Parameters\n    ----------\n    other : Series, DataFrame, or ndarray, optional\n        If not supplied then will default to self.\n    pairwise : bool, default None\n        Calculate pairwise combinations of columns within a\n        DataFrame. If `other` is not specified, defaults to `True`,\n        otherwise defaults to `False`.\n        Not relevant for :class:`~pandas.Series`.\n    **kwargs\n        Unused.\n\n    Returns\n    -------\n    Series or DataFrame\n        Returned object type is determined by the caller of the\n        %(name)s calculation.\n\n    See Also\n    --------\n    Series.%(name)s : Calling object with Series data.\n    DataFrame.%(name)s : Calling object with DataFrames.\n    Series.corr : Equivalent method for Series.\n    DataFrame.corr : Equivalent method for DataFrame.\n    %(name)s.cov : Similar method to calculate covariance.\n    numpy.corrcoef : NumPy Pearson's correlation calculation.\n\n    Notes\n    -----\n    This function uses Pearson's definition of correlation\n    (https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).\n\n    When `other` is not specified, the output will be self correlation (e.g.\n    all 1's), except for :class:`~pandas.DataFrame` inputs with `pairwise`\n    set to `True`.\n\n    Function will return ``NaN`` for correlations of equal valued sequences;\n    this is the result of a 0/0 division error.\n\n    When `pairwise` is set to `False`, only matching columns between `self` and\n    `other` will be used.\n\n    When `pairwise` is set to `True`, the output will be a MultiIndex DataFrame\n    with the original index on the first level, and the `other` DataFrame\n    columns on the second level.\n\n    In the case of missing elements, only complete pairwise observations\n    will be used.\n\n    Examples\n    --------\n    The below example shows a rolling calculation with a window size of\n    four matching the equivalent function call using :meth:`numpy.corrcoef`.\n\n    >>> v1 = [3, 3, 3, 5, 8]\n    >>> v2 = [3, 4, 4, 4, 8]\n    >>> fmt = \"{0:.6f}\"  # limit the printed precision to 6 digits\n    >>> # numpy returns a 2X2 array, the correlation coefficient\n    >>> # is the number at entry [0][1]\n    >>> print(fmt.format(np.corrcoef(v1[:-1], v2[:-1])[0][1]))\n    0.333333\n    >>> print(fmt.format(np.corrcoef(v1[1:], v2[1:])[0][1]))\n    0.916949\n    >>> s1 = pd.Series(v1)\n    >>> s2 = pd.Series(v2)\n    >>> s1.rolling(4).corr(s2)\n    0         NaN\n    1         NaN\n    2         NaN\n    3    0.333333\n    4    0.916949\n    dtype: float64\n\n    The below example shows a similar rolling calculation on a\n    DataFrame using the pairwise option.\n\n    >>> matrix = np.array([[51., 35.], [49., 30.], [47., 32.],\\\n    [46., 31.], [50., 36.]])\n    >>> print(np.corrcoef(matrix[:-1,0], matrix[:-1,1]).round(7))\n    [[1.         0.6263001]\n     [0.6263001  1.       ]]\n    >>> print(np.corrcoef(matrix[1:,0], matrix[1:,1]).round(7))\n    [[1.         0.5553681]\n     [0.5553681  1.        ]]\n    >>> df = pd.DataFrame(matrix, columns=['X','Y'])\n    >>> df\n          X     Y\n    0  51.0  35.0\n    1  49.0  30.0\n    2  47.0  32.0\n    3  46.0  31.0\n    4  50.0  36.0\n    >>> df.rolling(4).corr(pairwise=True)\n                X         Y\n    0 X       NaN       NaN\n      Y       NaN       NaN\n    1 X       NaN       NaN\n      Y       NaN       NaN\n    2 X       NaN       NaN\n      Y       NaN       NaN\n    3 X  1.000000  0.626300\n      Y  0.626300  1.000000\n    4 X  1.000000  0.555368\n      Y  0.555368  1.000000\n\"\"\")\n\n    def corr(self, other=None, pairwise=None, **kwargs):\n        if other is None:\n            other = self._selected_obj\n            # only default unset\n            pairwise = True if pairwise is None else pairwise\n        other = self._shallow_copy(other)\n        window = self._get_window(other)\n\n        def _get_corr(a, b):\n            a = a.rolling(window=window, min_periods=self.min_periods,\n                          center=self.center)\n            b = b.rolling(window=window, min_periods=self.min_periods,\n                          center=self.center)\n\n            return a.cov(b, **kwargs) / (a.std(**kwargs) * b.std(**kwargs))\n\n        return _flex_binary_moment(self._selected_obj, other._selected_obj,\n                                   _get_corr, pairwise=bool(pairwise))\n\n\nclass Rolling(_Rolling_and_Expanding):\n\n    @cache_readonly\n    def is_datetimelike(self):\n        return isinstance(self._on,\n                          (ABCDatetimeIndex,\n                           ABCTimedeltaIndex,\n                           ABCPeriodIndex))\n\n    @cache_readonly\n    def _on(self):\n\n        if self.on is None:\n            return self.obj.index\n        elif (isinstance(self.obj, ABCDataFrame) and\n              self.on in self.obj.columns):\n            from pandas import Index\n            return Index(self.obj[self.on])\n        else:\n            raise ValueError(\"invalid on specified as {0}, \"\n                             \"must be a column (if DataFrame) \"\n                             \"or None\".format(self.on))\n\n    def validate(self):\n        super(Rolling, self).validate()\n\n        # we allow rolling on a datetimelike index\n        if ((self.obj.empty or self.is_datetimelike) and\n                isinstance(self.window, (compat.string_types, ABCDateOffset,\n                                         timedelta))):\n\n            self._validate_monotonic()\n            freq = self._validate_freq()\n\n            # we don't allow center\n            if self.center:\n                raise NotImplementedError(\"center is not implemented \"\n                                          \"for datetimelike and offset \"\n                                          \"based windows\")\n\n            # this will raise ValueError on non-fixed freqs\n            self.win_freq = self.window\n            self.window = freq.nanos\n            self.win_type = 'freq'\n\n            # min_periods must be an integer\n            if self.min_periods is None:\n                self.min_periods = 1\n\n        elif not is_integer(self.window):\n            raise ValueError(\"window must be an integer\")\n        elif self.window < 0:\n            raise ValueError(\"window must be non-negative\")\n\n        if not self.is_datetimelike and self.closed is not None:\n            raise ValueError(\"closed only implemented for datetimelike \"\n                             \"and offset based windows\")\n\n    def _validate_monotonic(self):\n        \"\"\" validate on is monotonic \"\"\"\n        if not self._on.is_monotonic:\n            formatted = self.on or 'index'\n            raise ValueError(\"{0} must be \"\n                             \"monotonic\".format(formatted))\n\n    def _validate_freq(self):\n        \"\"\" validate & return window frequency \"\"\"\n        from pandas.tseries.frequencies import to_offset\n        try:\n            return to_offset(self.window)\n        except (TypeError, ValueError):\n            raise ValueError(\"passed window {0} is not \"\n                             \"compatible with a datetimelike \"\n                             \"index\".format(self.window))\n\n    _agg_doc = dedent(\"\"\"\n    Examples\n    --------\n\n    >>> df = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'])\n    >>> df\n              A         B         C\n    0 -2.385977 -0.102758  0.438822\n    1 -1.004295  0.905829 -0.954544\n    2  0.735167 -0.165272 -1.619346\n    3 -0.702657 -1.340923 -0.706334\n    4 -0.246845  0.211596 -0.901819\n    5  2.463718  3.157577 -1.380906\n    6 -1.142255  2.340594 -0.039875\n    7  1.396598 -1.647453  1.677227\n    8 -0.543425  1.761277 -0.220481\n    9 -0.640505  0.289374 -1.550670\n\n    >>> df.rolling(3).sum()\n              A         B         C\n    0       NaN       NaN       NaN\n    1       NaN       NaN       NaN\n    2 -2.655105  0.637799 -2.135068\n    3 -0.971785 -0.600366 -3.280224\n    4 -0.214334 -1.294599 -3.227500\n    5  1.514216  2.028250 -2.989060\n    6  1.074618  5.709767 -2.322600\n    7  2.718061  3.850718  0.256446\n    8 -0.289082  2.454418  1.416871\n    9  0.212668  0.403198 -0.093924\n\n    >>> df.rolling(3).agg({'A':'sum', 'B':'min'})\n              A         B\n    0       NaN       NaN\n    1       NaN       NaN\n    2 -2.655105 -0.165272\n    3 -0.971785 -1.340923\n    4 -0.214334 -1.340923\n    5  1.514216 -1.340923\n    6  1.074618  0.211596\n    7  2.718061 -1.647453\n    8 -0.289082 -1.647453\n    9  0.212668 -1.647453\n\n    See Also\n    --------\n    pandas.Series.rolling\n    pandas.DataFrame.rolling\n    \"\"\")\n\n    @Appender(_agg_doc)\n    @Appender(_shared_docs['aggregate'] % dict(\n        versionadded='',\n        klass='Series/DataFrame',\n        axis=''))\n    def aggregate(self, arg, *args, **kwargs):\n        return super(Rolling, self).aggregate(arg, *args, **kwargs)\n\n    agg = aggregate\n\n    @Substitution(name='rolling')\n    @Appender(_shared_docs['count'])\n    def count(self):\n\n        # different impl for freq counting\n        if self.is_freq_type:\n            return self._apply('roll_count', 'count')\n\n        return super(Rolling, self).count()\n\n    @Substitution(name='rolling')\n    @Appender(_doc_template)\n    @Appender(_shared_docs['apply'])\n    def apply(self, func, raw=None, args=(), kwargs={}):\n        return super(Rolling, self).apply(\n            func, raw=raw, args=args, kwargs=kwargs)\n\n    @Substitution(name='rolling')\n    @Appender(_shared_docs['sum'])\n    def sum(self, *args, **kwargs):\n        nv.validate_rolling_func('sum', args, kwargs)\n        return super(Rolling, self).sum(*args, **kwargs)\n\n    @Substitution(name='rolling')\n    @Appender(_doc_template)\n    @Appender(_shared_docs['max'])\n    def max(self, *args, **kwargs):\n        nv.validate_rolling_func('max', args, kwargs)\n        return super(Rolling, self).max(*args, **kwargs)\n\n    @Substitution(name='rolling')\n    @Appender(_shared_docs['min'])\n    def min(self, *args, **kwargs):\n        nv.validate_rolling_func('min', args, kwargs)\n        return super(Rolling, self).min(*args, **kwargs)\n\n    @Substitution(name='rolling')\n    @Appender(_shared_docs['mean'])\n    def mean(self, *args, **kwargs):\n        nv.validate_rolling_func('mean', args, kwargs)\n        return super(Rolling, self).mean(*args, **kwargs)\n\n    @Substitution(name='rolling')\n    @Appender(_shared_docs['median'])\n    def median(self, **kwargs):\n        return super(Rolling, self).median(**kwargs)\n\n    @Substitution(name='rolling')\n    @Appender(_shared_docs['std'])\n    def std(self, ddof=1, *args, **kwargs):\n        nv.validate_rolling_func('std', args, kwargs)\n        return super(Rolling, self).std(ddof=ddof, **kwargs)\n\n    @Substitution(name='rolling')\n    @Appender(_shared_docs['var'])\n    def var(self, ddof=1, *args, **kwargs):\n        nv.validate_rolling_func('var', args, kwargs)\n        return super(Rolling, self).var(ddof=ddof, **kwargs)\n\n    @Substitution(name='rolling')\n    @Appender(_doc_template)\n    @Appender(_shared_docs['skew'])\n    def skew(self, **kwargs):\n        return super(Rolling, self).skew(**kwargs)\n\n    _agg_doc = dedent(\"\"\"\n    Examples\n    --------\n\n    The example below will show a rolling calculation with a window size of\n    four matching the equivalent function call using `scipy.stats`.\n\n    >>> arr = [1, 2, 3, 4, 999]\n    >>> fmt = \"{0:.6f}\"  # limit the printed precision to 6 digits\n    >>> import scipy.stats\n    >>> print(fmt.format(scipy.stats.kurtosis(arr[:-1], bias=False)))\n    -1.200000\n    >>> print(fmt.format(scipy.stats.kurtosis(arr[1:], bias=False)))\n    3.999946\n    >>> s = pd.Series(arr)\n    >>> s.rolling(4).kurt()\n    0         NaN\n    1         NaN\n    2         NaN\n    3   -1.200000\n    4    3.999946\n    dtype: float64\n    \"\"\")\n\n    @Appender(_agg_doc)\n    @Substitution(name='rolling')\n    @Appender(_shared_docs['kurt'])\n    def kurt(self, **kwargs):\n        return super(Rolling, self).kurt(**kwargs)\n\n    @Substitution(name='rolling')\n    @Appender(_shared_docs['quantile'])\n    def quantile(self, quantile, interpolation='linear', **kwargs):\n        return super(Rolling, self).quantile(quantile=quantile,\n                                             interpolation=interpolation,\n                                             **kwargs)\n\n    @Substitution(name='rolling')\n    @Appender(_doc_template)\n    @Appender(_shared_docs['cov'])\n    def cov(self, other=None, pairwise=None, ddof=1, **kwargs):\n        return super(Rolling, self).cov(other=other, pairwise=pairwise,\n                                        ddof=ddof, **kwargs)\n\n    @Substitution(name='rolling')\n    @Appender(_shared_docs['corr'])\n    def corr(self, other=None, pairwise=None, **kwargs):\n        return super(Rolling, self).corr(other=other, pairwise=pairwise,\n                                         **kwargs)\n\n\nclass RollingGroupby(_GroupByMixin, Rolling):\n    \"\"\"\n    Provides a rolling groupby implementation\n\n    .. versionadded:: 0.18.1\n\n    \"\"\"\n    @property\n    def _constructor(self):\n        return Rolling\n\n    def _gotitem(self, key, ndim, subset=None):\n\n        # we are setting the index on the actual object\n        # here so our index is carried thru to the selected obj\n        # when we do the splitting for the groupby\n        if self.on is not None:\n            self._groupby.obj = self._groupby.obj.set_index(self._on)\n            self.on = None\n        return super(RollingGroupby, self)._gotitem(key, ndim, subset=subset)\n\n    def _validate_monotonic(self):\n        \"\"\"\n        validate that on is monotonic;\n        we don't care for groupby.rolling\n        because we have already validated at a higher\n        level\n        \"\"\"\n        pass\n\n\nclass Expanding(_Rolling_and_Expanding):\n    \"\"\"\n    Provides expanding transformations.\n\n    .. versionadded:: 0.18.0\n\n    Parameters\n    ----------\n    min_periods : int, default 1\n        Minimum number of observations in window required to have a value\n        (otherwise result is NA).\n    center : bool, default False\n        Set the labels at the center of the window.\n    axis : int or str, default 0\n\n    Returns\n    -------\n    a Window sub-classed for the particular operation\n\n    Examples\n    --------\n\n    >>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n         B\n    0  0.0\n    1  1.0\n    2  2.0\n    3  NaN\n    4  4.0\n\n    >>> df.expanding(2).sum()\n         B\n    0  NaN\n    1  1.0\n    2  3.0\n    3  3.0\n    4  7.0\n\n    Notes\n    -----\n    By default, the result is set to the right edge of the window. This can be\n    changed to the center of the window by setting ``center=True``.\n\n    See Also\n    --------\n    rolling : Provides rolling window calculations.\n    ewm : Provides exponential weighted functions.\n    \"\"\"\n\n    _attributes = ['min_periods', 'center', 'axis']\n\n    def __init__(self, obj, min_periods=1, center=False, axis=0,\n                 **kwargs):\n        super(Expanding, self).__init__(obj=obj, min_periods=min_periods,\n                                        center=center, axis=axis)\n\n    @property\n    def _constructor(self):\n        return Expanding\n\n    def _get_window(self, other=None):\n        \"\"\"\n        Get the window length over which to perform some operation.\n\n        Parameters\n        ----------\n        other : object, default None\n            The other object that is involved in the operation.\n            Such an object is involved for operations like covariance.\n\n        Returns\n        -------\n        window : int\n            The window length.\n        \"\"\"\n        axis = self.obj._get_axis(self.axis)\n        length = len(axis) + (other is not None) * len(axis)\n\n        other = self.min_periods or -1\n        return max(length, other)\n\n    _agg_doc = dedent(\"\"\"\n    Examples\n    --------\n\n    >>> df = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'])\n    >>> df\n              A         B         C\n    0 -2.385977 -0.102758  0.438822\n    1 -1.004295  0.905829 -0.954544\n    2  0.735167 -0.165272 -1.619346\n    3 -0.702657 -1.340923 -0.706334\n    4 -0.246845  0.211596 -0.901819\n    5  2.463718  3.157577 -1.380906\n    6 -1.142255  2.340594 -0.039875\n    7  1.396598 -1.647453  1.677227\n    8 -0.543425  1.761277 -0.220481\n    9 -0.640505  0.289374 -1.550670\n\n    >>> df.ewm(alpha=0.5).mean()\n              A         B         C\n    0 -2.385977 -0.102758  0.438822\n    1 -1.464856  0.569633 -0.490089\n    2 -0.207700  0.149687 -1.135379\n    3 -0.471677 -0.645305 -0.906555\n    4 -0.355635 -0.203033 -0.904111\n    5  1.076417  1.503943 -1.146293\n    6 -0.041654  1.925562 -0.588728\n    7  0.680292  0.132049  0.548693\n    8  0.067236  0.948257  0.163353\n    9 -0.286980  0.618493 -0.694496\n\n    See Also\n    --------\n    pandas.DataFrame.expanding.aggregate\n    pandas.DataFrame.rolling.aggregate\n    pandas.DataFrame.aggregate\n    \"\"\")\n\n    @Appender(_agg_doc)\n    @Appender(_shared_docs['aggregate'] % dict(\n        versionadded='',\n        klass='Series/DataFrame',\n        axis=''))\n    def aggregate(self, arg, *args, **kwargs):\n        return super(Expanding, self).aggregate(arg, *args, **kwargs)\n\n    agg = aggregate\n\n    @Substitution(name='expanding')\n    @Appender(_shared_docs['count'])\n    def count(self, **kwargs):\n        return super(Expanding, self).count(**kwargs)\n\n    @Substitution(name='expanding')\n    @Appender(_doc_template)\n    @Appender(_shared_docs['apply'])\n    def apply(self, func, raw=None, args=(), kwargs={}):\n        return super(Expanding, self).apply(\n            func, raw=raw, args=args, kwargs=kwargs)\n\n    @Substitution(name='expanding')\n    @Appender(_shared_docs['sum'])\n    def sum(self, *args, **kwargs):\n        nv.validate_expanding_func('sum', args, kwargs)\n        return super(Expanding, self).sum(*args, **kwargs)\n\n    @Substitution(name='expanding')\n    @Appender(_doc_template)\n    @Appender(_shared_docs['max'])\n    def max(self, *args, **kwargs):\n        nv.validate_expanding_func('max', args, kwargs)\n        return super(Expanding, self).max(*args, **kwargs)\n\n    @Substitution(name='expanding')\n    @Appender(_shared_docs['min'])\n    def min(self, *args, **kwargs):\n        nv.validate_expanding_func('min', args, kwargs)\n        return super(Expanding, self).min(*args, **kwargs)\n\n    @Substitution(name='expanding')\n    @Appender(_shared_docs['mean'])\n    def mean(self, *args, **kwargs):\n        nv.validate_expanding_func('mean', args, kwargs)\n        return super(Expanding, self).mean(*args, **kwargs)\n\n    @Substitution(name='expanding')\n    @Appender(_shared_docs['median'])\n    def median(self, **kwargs):\n        return super(Expanding, self).median(**kwargs)\n\n    @Substitution(name='expanding')\n    @Appender(_shared_docs['std'])\n    def std(self, ddof=1, *args, **kwargs):\n        nv.validate_expanding_func('std', args, kwargs)\n        return super(Expanding, self).std(ddof=ddof, **kwargs)\n\n    @Substitution(name='expanding')\n    @Appender(_shared_docs['var'])\n    def var(self, ddof=1, *args, **kwargs):\n        nv.validate_expanding_func('var', args, kwargs)\n        return super(Expanding, self).var(ddof=ddof, **kwargs)\n\n    @Substitution(name='expanding')\n    @Appender(_doc_template)\n    @Appender(_shared_docs['skew'])\n    def skew(self, **kwargs):\n        return super(Expanding, self).skew(**kwargs)\n\n    _agg_doc = dedent(\"\"\"\n    Examples\n    --------\n\n    The example below will show an expanding calculation with a window size of\n    four matching the equivalent function call using `scipy.stats`.\n\n    >>> arr = [1, 2, 3, 4, 999]\n    >>> import scipy.stats\n    >>> fmt = \"{0:.6f}\"  # limit the printed precision to 6 digits\n    >>> print(fmt.format(scipy.stats.kurtosis(arr[:-1], bias=False)))\n    -1.200000\n    >>> print(fmt.format(scipy.stats.kurtosis(arr, bias=False)))\n    4.999874\n    >>> s = pd.Series(arr)\n    >>> s.expanding(4).kurt()\n    0         NaN\n    1         NaN\n    2         NaN\n    3   -1.200000\n    4    4.999874\n    dtype: float64\n    \"\"\")\n\n    @Appender(_agg_doc)\n    @Substitution(name='expanding')\n    @Appender(_shared_docs['kurt'])\n    def kurt(self, **kwargs):\n        return super(Expanding, self).kurt(**kwargs)\n\n    @Substitution(name='expanding')\n    @Appender(_shared_docs['quantile'])\n    def quantile(self, quantile, interpolation='linear', **kwargs):\n        return super(Expanding, self).quantile(quantile=quantile,\n                                               interpolation=interpolation,\n                                               **kwargs)\n\n    @Substitution(name='expanding')\n    @Appender(_doc_template)\n    @Appender(_shared_docs['cov'])\n    def cov(self, other=None, pairwise=None, ddof=1, **kwargs):\n        return super(Expanding, self).cov(other=other, pairwise=pairwise,\n                                          ddof=ddof, **kwargs)\n\n    @Substitution(name='expanding')\n    @Appender(_shared_docs['corr'])\n    def corr(self, other=None, pairwise=None, **kwargs):\n        return super(Expanding, self).corr(other=other, pairwise=pairwise,\n                                           **kwargs)\n\n\nclass ExpandingGroupby(_GroupByMixin, Expanding):\n    \"\"\"\n    Provides a expanding groupby implementation\n\n    .. versionadded:: 0.18.1\n\n    \"\"\"\n    @property\n    def _constructor(self):\n        return Expanding\n\n\n_bias_template = \"\"\"\n\nParameters\n----------\nbias : bool, default False\n    Use a standard estimation bias correction\n\"\"\"\n\n_pairwise_template = \"\"\"\n\nParameters\n----------\nother : Series, DataFrame, or ndarray, optional\n    if not supplied then will default to self and produce pairwise output\npairwise : bool, default None\n    If False then only matching columns between self and other will be used and\n    the output will be a DataFrame.\n    If True then all pairwise combinations will be calculated and the output\n    will be a MultiIndex DataFrame in the case of DataFrame inputs.\n    In the case of missing elements, only complete pairwise observations will\n    be used.\nbias : bool, default False\n   Use a standard estimation bias correction\n\"\"\"\n\n\nclass EWM(_Rolling):\n    r\"\"\"\n    Provides exponential weighted functions\n\n    .. versionadded:: 0.18.0\n\n    Parameters\n    ----------\n    com : float, optional\n        Specify decay in terms of center of mass,\n        :math:`\\alpha = 1 / (1 + com),\\text{ for } com \\geq 0`\n    span : float, optional\n        Specify decay in terms of span,\n        :math:`\\alpha = 2 / (span + 1),\\text{ for } span \\geq 1`\n    halflife : float, optional\n        Specify decay in terms of half-life,\n        :math:`\\alpha = 1 - exp(log(0.5) / halflife),\\text{ for } halflife > 0`\n    alpha : float, optional\n        Specify smoothing factor :math:`\\alpha` directly,\n        :math:`0 < \\alpha \\leq 1`\n\n        .. versionadded:: 0.18.0\n\n    min_periods : int, default 0\n        Minimum number of observations in window required to have a value\n        (otherwise result is NA).\n    adjust : bool, default True\n        Divide by decaying adjustment factor in beginning periods to account\n        for imbalance in relative weightings (viewing EWMA as a moving average)\n    ignore_na : bool, default False\n        Ignore missing values when calculating weights;\n        specify True to reproduce pre-0.15.0 behavior\n\n    Returns\n    -------\n    a Window sub-classed for the particular operation\n\n    Examples\n    --------\n\n    >>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n         B\n    0  0.0\n    1  1.0\n    2  2.0\n    3  NaN\n    4  4.0\n\n    >>> df.ewm(com=0.5).mean()\n              B\n    0  0.000000\n    1  0.750000\n    2  1.615385\n    3  1.615385\n    4  3.670213\n\n    Notes\n    -----\n    Exactly one of center of mass, span, half-life, and alpha must be provided.\n    Allowed values and relationship between the parameters are specified in the\n    parameter descriptions above; see the link at the end of this section for\n    a detailed explanation.\n\n    When adjust is True (default), weighted averages are calculated using\n    weights (1-alpha)**(n-1), (1-alpha)**(n-2), ..., 1-alpha, 1.\n\n    When adjust is False, weighted averages are calculated recursively as:\n       weighted_average[0] = arg[0];\n       weighted_average[i] = (1-alpha)*weighted_average[i-1] + alpha*arg[i].\n\n    When ignore_na is False (default), weights are based on absolute positions.\n    For example, the weights of x and y used in calculating the final weighted\n    average of [x, None, y] are (1-alpha)**2 and 1 (if adjust is True), and\n    (1-alpha)**2 and alpha (if adjust is False).\n\n    When ignore_na is True (reproducing pre-0.15.0 behavior), weights are based\n    on relative positions. For example, the weights of x and y used in\n    calculating the final weighted average of [x, None, y] are 1-alpha and 1\n    (if adjust is True), and 1-alpha and alpha (if adjust is False).\n\n    More details can be found at\n    http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows\n\n    See Also\n    --------\n    rolling : Provides rolling window calculations.\n    expanding : Provides expanding transformations.\n    \"\"\"\n    _attributes = ['com', 'min_periods', 'adjust', 'ignore_na', 'axis']\n\n    def __init__(self, obj, com=None, span=None, halflife=None, alpha=None,\n                 min_periods=0, adjust=True, ignore_na=False,\n                 axis=0):\n        self.obj = obj\n        self.com = _get_center_of_mass(com, span, halflife, alpha)\n        self.min_periods = min_periods\n        self.adjust = adjust\n        self.ignore_na = ignore_na\n        self.axis = axis\n        self.on = None\n\n    @property\n    def _constructor(self):\n        return EWM\n\n    _agg_doc = dedent(\"\"\"\n    Examples\n    --------\n\n    >>> df = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'])\n    >>> df\n              A         B         C\n    0 -2.385977 -0.102758  0.438822\n    1 -1.004295  0.905829 -0.954544\n    2  0.735167 -0.165272 -1.619346\n    3 -0.702657 -1.340923 -0.706334\n    4 -0.246845  0.211596 -0.901819\n    5  2.463718  3.157577 -1.380906\n    6 -1.142255  2.340594 -0.039875\n    7  1.396598 -1.647453  1.677227\n    8 -0.543425  1.761277 -0.220481\n    9 -0.640505  0.289374 -1.550670\n\n    >>> df.ewm(alpha=0.5).mean()\n              A         B         C\n    0 -2.385977 -0.102758  0.438822\n    1 -1.464856  0.569633 -0.490089\n    2 -0.207700  0.149687 -1.135379\n    3 -0.471677 -0.645305 -0.906555\n    4 -0.355635 -0.203033 -0.904111\n    5  1.076417  1.503943 -1.146293\n    6 -0.041654  1.925562 -0.588728\n    7  0.680292  0.132049  0.548693\n    8  0.067236  0.948257  0.163353\n    9 -0.286980  0.618493 -0.694496\n\n    See Also\n    --------\n    pandas.DataFrame.rolling.aggregate\n    \"\"\")\n\n    @Appender(_agg_doc)\n    @Appender(_shared_docs['aggregate'] % dict(\n        versionadded='',\n        klass='Series/DataFrame',\n        axis=''))\n    def aggregate(self, arg, *args, **kwargs):\n        return super(EWM, self).aggregate(arg, *args, **kwargs)\n\n    agg = aggregate\n\n    def _apply(self, func, **kwargs):\n        \"\"\"Rolling statistical measure using supplied function. Designed to be\n        used with passed-in Cython array-based functions.\n\n        Parameters\n        ----------\n        func : str/callable to apply\n\n        Returns\n        -------\n        y : same type as input argument\n        \"\"\"\n        blocks, obj, index = self._create_blocks()\n        results = []\n        for b in blocks:\n            try:\n                values = self._prep_values(b.values)\n            except TypeError:\n                results.append(b.values.copy())\n                continue\n\n            if values.size == 0:\n                results.append(values.copy())\n                continue\n\n            # if we have a string function name, wrap it\n            if isinstance(func, compat.string_types):\n                cfunc = getattr(libwindow, func, None)\n                if cfunc is None:\n                    raise ValueError(\"we do not support this function \"\n                                     \"in libwindow.{func}\".format(func=func))\n\n                def func(arg):\n                    return cfunc(arg, self.com, int(self.adjust),\n                                 int(self.ignore_na), int(self.min_periods))\n\n            results.append(np.apply_along_axis(func, self.axis, values))\n\n        return self._wrap_results(results, blocks, obj)\n\n    @Substitution(name='ewm')\n    @Appender(_doc_template)\n    def mean(self, *args, **kwargs):\n        \"\"\"exponential weighted moving average\"\"\"\n        nv.validate_window_func('mean', args, kwargs)\n        return self._apply('ewma', **kwargs)\n\n    @Substitution(name='ewm')\n    @Appender(_doc_template)\n    @Appender(_bias_template)\n    def std(self, bias=False, *args, **kwargs):\n        \"\"\"exponential weighted moving stddev\"\"\"\n        nv.validate_window_func('std', args, kwargs)\n        return _zsqrt(self.var(bias=bias, **kwargs))\n\n    vol = std\n\n    @Substitution(name='ewm')\n    @Appender(_doc_template)\n    @Appender(_bias_template)\n    def var(self, bias=False, *args, **kwargs):\n        \"\"\"exponential weighted moving variance\"\"\"\n        nv.validate_window_func('var', args, kwargs)\n\n        def f(arg):\n            return libwindow.ewmcov(arg, arg, self.com, int(self.adjust),\n                                    int(self.ignore_na), int(self.min_periods),\n                                    int(bias))\n\n        return self._apply(f, **kwargs)\n\n    @Substitution(name='ewm')\n    @Appender(_doc_template)\n    @Appender(_pairwise_template)\n    def cov(self, other=None, pairwise=None, bias=False, **kwargs):\n        \"\"\"exponential weighted sample covariance\"\"\"\n        if other is None:\n            other = self._selected_obj\n            # only default unset\n            pairwise = True if pairwise is None else pairwise\n        other = self._shallow_copy(other)\n\n        def _get_cov(X, Y):\n            X = self._shallow_copy(X)\n            Y = self._shallow_copy(Y)\n            cov = libwindow.ewmcov(X._prep_values(), Y._prep_values(),\n                                   self.com, int(self.adjust),\n                                   int(self.ignore_na), int(self.min_periods),\n                                   int(bias))\n            return X._wrap_result(cov)\n\n        return _flex_binary_moment(self._selected_obj, other._selected_obj,\n                                   _get_cov, pairwise=bool(pairwise))\n\n    @Substitution(name='ewm')\n    @Appender(_doc_template)\n    @Appender(_pairwise_template)\n    def corr(self, other=None, pairwise=None, **kwargs):\n        \"\"\"exponential weighted sample correlation\"\"\"\n        if other is None:\n            other = self._selected_obj\n            # only default unset\n            pairwise = True if pairwise is None else pairwise\n        other = self._shallow_copy(other)\n\n        def _get_corr(X, Y):\n            X = self._shallow_copy(X)\n            Y = self._shallow_copy(Y)\n\n            def _cov(x, y):\n                return libwindow.ewmcov(x, y, self.com, int(self.adjust),\n                                        int(self.ignore_na),\n                                        int(self.min_periods),\n                                        1)\n\n            x_values = X._prep_values()\n            y_values = Y._prep_values()\n            with np.errstate(all='ignore'):\n                cov = _cov(x_values, y_values)\n                x_var = _cov(x_values, x_values)\n                y_var = _cov(y_values, y_values)\n                corr = cov / _zsqrt(x_var * y_var)\n            return X._wrap_result(corr)\n\n        return _flex_binary_moment(self._selected_obj, other._selected_obj,\n                                   _get_corr, pairwise=bool(pairwise))\n\n# Helper Funcs\n\n\ndef _flex_binary_moment(arg1, arg2, f, pairwise=False):\n\n    if not (isinstance(arg1, (np.ndarray, ABCSeries, ABCDataFrame)) and\n            isinstance(arg2, (np.ndarray, ABCSeries, ABCDataFrame))):\n        raise TypeError(\"arguments to moment function must be of type \"\n                        \"np.ndarray/Series/DataFrame\")\n\n    if (isinstance(arg1, (np.ndarray, ABCSeries)) and\n            isinstance(arg2, (np.ndarray, ABCSeries))):\n        X, Y = _prep_binary(arg1, arg2)\n        return f(X, Y)\n\n    elif isinstance(arg1, ABCDataFrame):\n        from pandas import DataFrame\n\n        def dataframe_from_int_dict(data, frame_template):\n            result = DataFrame(data, index=frame_template.index)\n            if len(result.columns) > 0:\n                result.columns = frame_template.columns[result.columns]\n            return result\n\n        results = {}\n        if isinstance(arg2, ABCDataFrame):\n            if pairwise is False:\n                if arg1 is arg2:\n                    # special case in order to handle duplicate column names\n                    for i, col in enumerate(arg1.columns):\n                        results[i] = f(arg1.iloc[:, i], arg2.iloc[:, i])\n                    return dataframe_from_int_dict(results, arg1)\n                else:\n                    if not arg1.columns.is_unique:\n                        raise ValueError(\"'arg1' columns are not unique\")\n                    if not arg2.columns.is_unique:\n                        raise ValueError(\"'arg2' columns are not unique\")\n                    with warnings.catch_warnings(record=True):\n                        warnings.simplefilter(\"ignore\", RuntimeWarning)\n                        X, Y = arg1.align(arg2, join='outer')\n                    X = X + 0 * Y\n                    Y = Y + 0 * X\n\n                    with warnings.catch_warnings(record=True):\n                        warnings.simplefilter(\"ignore\", RuntimeWarning)\n                        res_columns = arg1.columns.union(arg2.columns)\n                    for col in res_columns:\n                        if col in X and col in Y:\n                            results[col] = f(X[col], Y[col])\n                    return DataFrame(results, index=X.index,\n                                     columns=res_columns)\n            elif pairwise is True:\n                results = defaultdict(dict)\n                for i, k1 in enumerate(arg1.columns):\n                    for j, k2 in enumerate(arg2.columns):\n                        if j < i and arg2 is arg1:\n                            # Symmetric case\n                            results[i][j] = results[j][i]\n                        else:\n                            results[i][j] = f(*_prep_binary(arg1.iloc[:, i],\n                                                            arg2.iloc[:, j]))\n\n                from pandas import MultiIndex, concat\n\n                result_index = arg1.index.union(arg2.index)\n                if len(result_index):\n\n                    # construct result frame\n                    result = concat(\n                        [concat([results[i][j]\n                                 for j, c in enumerate(arg2.columns)],\n                                ignore_index=True)\n                         for i, c in enumerate(arg1.columns)],\n                        ignore_index=True,\n                        axis=1)\n                    result.columns = arg1.columns\n\n                    # set the index and reorder\n                    if arg2.columns.nlevels > 1:\n                        result.index = MultiIndex.from_product(\n                            arg2.columns.levels + [result_index])\n                        result = result.reorder_levels([2, 0, 1]).sort_index()\n                    else:\n                        result.index = MultiIndex.from_product(\n                            [range(len(arg2.columns)),\n                             range(len(result_index))])\n                        result = result.swaplevel(1, 0).sort_index()\n                        result.index = MultiIndex.from_product(\n                            [result_index] + [arg2.columns])\n                else:\n\n                    # empty result\n                    result = DataFrame(\n                        index=MultiIndex(levels=[arg1.index, arg2.columns],\n                                         labels=[[], []]),\n                        columns=arg2.columns,\n                        dtype='float64')\n\n                # reset our index names to arg1 names\n                # reset our column names to arg2 names\n                # careful not to mutate the original names\n                result.columns = result.columns.set_names(\n                    arg1.columns.names)\n                result.index = result.index.set_names(\n                    result_index.names + arg2.columns.names)\n\n                return result\n\n            else:\n                raise ValueError(\"'pairwise' is not True/False\")\n        else:\n            results = {}\n            for i, col in enumerate(arg1.columns):\n                results[i] = f(*_prep_binary(arg1.iloc[:, i], arg2))\n            return dataframe_from_int_dict(results, arg1)\n\n    else:\n        return _flex_binary_moment(arg2, arg1, f)\n\n\ndef _get_center_of_mass(comass, span, halflife, alpha):\n    valid_count = com.count_not_none(comass, span, halflife, alpha)\n    if valid_count > 1:\n        raise ValueError(\"comass, span, halflife, and alpha \"\n                         \"are mutually exclusive\")\n\n    # Convert to center of mass; domain checks ensure 0 < alpha <= 1\n    if comass is not None:\n        if comass < 0:\n            raise ValueError(\"comass must satisfy: comass >= 0\")\n    elif span is not None:\n        if span < 1:\n            raise ValueError(\"span must satisfy: span >= 1\")\n        comass = (span - 1) / 2.\n    elif halflife is not None:\n        if halflife <= 0:\n            raise ValueError(\"halflife must satisfy: halflife > 0\")\n        decay = 1 - np.exp(np.log(0.5) / halflife)\n        comass = 1 / decay - 1\n    elif alpha is not None:\n        if alpha <= 0 or alpha > 1:\n            raise ValueError(\"alpha must satisfy: 0 < alpha <= 1\")\n        comass = (1.0 - alpha) / alpha\n    else:\n        raise ValueError(\"Must pass one of comass, span, halflife, or alpha\")\n\n    return float(comass)\n\n\ndef _offset(window, center):\n    if not is_integer(window):\n        window = len(window)\n    offset = (window - 1) / 2. if center else 0\n    try:\n        return int(offset)\n    except TypeError:\n        return offset.astype(int)\n\n\ndef _require_min_periods(p):\n    def _check_func(minp, window):\n        if minp is None:\n            return window\n        else:\n            return max(p, minp)\n\n    return _check_func\n\n\ndef _use_window(minp, window):\n    if minp is None:\n        return window\n    else:\n        return minp\n\n\ndef _zsqrt(x):\n    with np.errstate(all='ignore'):\n        result = np.sqrt(x)\n        mask = x < 0\n\n    if isinstance(x, ABCDataFrame):\n        if mask.values.any():\n            result[mask] = 0\n    else:\n        if mask.any():\n            result[mask] = 0\n\n    return result\n\n\ndef _prep_binary(arg1, arg2):\n    if not isinstance(arg2, type(arg1)):\n        raise Exception('Input arrays must be of the same type!')\n\n    # mask out values, this also makes a common index...\n    X = arg1 + 0 * arg2\n    Y = arg2 + 0 * arg1\n\n    return X, Y\n\n\n# Top-level exports\n\n\ndef rolling(obj, win_type=None, **kwds):\n    if not isinstance(obj, (ABCSeries, ABCDataFrame)):\n        raise TypeError('invalid type: %s' % type(obj))\n\n    if win_type is not None:\n        return Window(obj, win_type=win_type, **kwds)\n\n    return Rolling(obj, **kwds)\n\n\nrolling.__doc__ = Window.__doc__\n\n\ndef expanding(obj, **kwds):\n    if not isinstance(obj, (ABCSeries, ABCDataFrame)):\n        raise TypeError('invalid type: %s' % type(obj))\n\n    return Expanding(obj, **kwds)\n\n\nexpanding.__doc__ = Expanding.__doc__\n\n\ndef ewm(obj, **kwds):\n    if not isinstance(obj, (ABCSeries, ABCDataFrame)):\n        raise TypeError('invalid type: %s' % type(obj))\n\n    return EWM(obj, **kwds)\n\n\newm.__doc__ = EWM.__doc__\n"
    },
    {
      "filename": "pandas/errors/__init__.py",
      "content": "# flake8: noqa\n\n\"\"\"\nExpose public exceptions & warnings\n\"\"\"\n\nfrom pandas._libs.tslibs import OutOfBoundsDatetime\n\n\nclass PerformanceWarning(Warning):\n    \"\"\"\n    Warning raised when there is a possible\n    performance impact.\n    \"\"\"\n\nclass UnsupportedFunctionCall(ValueError):\n    \"\"\"\n    Exception raised when attempting to call a numpy function\n    on a pandas object, but that function is not supported by\n    the object e.g. ``np.cumsum(groupby_object)``.\n    \"\"\"\n\nclass UnsortedIndexError(KeyError):\n    \"\"\"\n    Error raised when attempting to get a slice of a MultiIndex,\n    and the index has not been lexsorted. Subclass of `KeyError`.\n\n    .. versionadded:: 0.20.0\n    \"\"\"\n\n\nclass ParserError(ValueError):\n    \"\"\"\n    Exception that is raised by an error encountered in `pd.read_csv`.\n    \"\"\"\n\n\nclass DtypeWarning(Warning):\n    \"\"\"\n    Warning raised when reading different dtypes in a column from a file.\n\n    Raised for a dtype incompatibility. This can happen whenever `read_csv`\n    or `read_table` encounter non-uniform dtypes in a column(s) of a given\n    CSV file.\n\n    See Also\n    --------\n    pandas.read_csv : Read CSV (comma-separated) file into a DataFrame.\n    pandas.read_table : Read general delimited file into a DataFrame.\n\n    Notes\n    -----\n    This warning is issued when dealing with larger files because the dtype\n    checking happens per chunk read.\n\n    Despite the warning, the CSV file is read with mixed types in a single\n    column which will be an object type. See the examples below to better\n    understand this issue.\n\n    Examples\n    --------\n    This example creates and reads a large CSV file with a column that contains\n    `int` and `str`.\n\n    >>> df = pd.DataFrame({'a': (['1'] * 100000 + ['X'] * 100000 +\n    ...                          ['1'] * 100000),\n    ...                    'b': ['b'] * 300000})\n    >>> df.to_csv('test.csv', index=False)\n    >>> df2 = pd.read_csv('test.csv')\n    ... # DtypeWarning: Columns (0) have mixed types\n\n    Important to notice that ``df2`` will contain both `str` and `int` for the\n    same input, '1'.\n\n    >>> df2.iloc[262140, 0]\n    '1'\n    >>> type(df2.iloc[262140, 0])\n    <class 'str'>\n    >>> df2.iloc[262150, 0]\n    1\n    >>> type(df2.iloc[262150, 0])\n    <class 'int'>\n\n    One way to solve this issue is using the `dtype` parameter in the\n    `read_csv` and `read_table` functions to explicit the conversion:\n\n    >>> df2 = pd.read_csv('test.csv', sep=',', dtype={'a': str})\n\n    No warning was issued.\n\n    >>> import os\n    >>> os.remove('test.csv')\n    \"\"\"\n\n\nclass EmptyDataError(ValueError):\n    \"\"\"\n    Exception that is thrown in `pd.read_csv` (by both the C and\n    Python engines) when empty data or header is encountered.\n    \"\"\"\n\n\nclass ParserWarning(Warning):\n    \"\"\"\n    Warning raised when reading a file that doesn't use the default 'c' parser.\n\n    Raised by `pd.read_csv` and `pd.read_table` when it is necessary to change\n    parsers, generally from the default 'c' parser to 'python'.\n\n    It happens due to a lack of support or functionality for parsing a\n    particular attribute of a CSV file with the requested engine.\n\n    Currently, 'c' unsupported options include the following parameters:\n\n    1. `sep` other than a single character (e.g. regex separators)\n    2. `skipfooter` higher than 0\n    3. `sep=None` with `delim_whitespace=False`\n\n    The warning can be avoided by adding `engine='python'` as a parameter in\n    `pd.read_csv` and `pd.read_table` methods.\n\n    See Also\n    --------\n    pd.read_csv : Read CSV (comma-separated) file into DataFrame.\n    pd.read_table : Read general delimited file into DataFrame.\n\n    Examples\n    --------\n    Using a `sep` in `pd.read_csv` other than a single character:\n\n    >>> import io\n    >>> csv = u'''a;b;c\n    ...           1;1,8\n    ...           1;2,1'''\n    >>> df = pd.read_csv(io.StringIO(csv), sep='[;,]')  # doctest: +SKIP\n    ... # ParserWarning: Falling back to the 'python' engine...\n\n    Adding `engine='python'` to `pd.read_csv` removes the Warning:\n\n    >>> df = pd.read_csv(io.StringIO(csv), sep='[;,]', engine='python')\n    \"\"\"\n\n\nclass MergeError(ValueError):\n    \"\"\"\n    Error raised when problems arise during merging due to problems\n    with input data. Subclass of `ValueError`.\n    \"\"\"\n\n\nclass NullFrequencyError(ValueError):\n    \"\"\"\n    Error raised when a null `freq` attribute is used in an operation\n    that needs a non-null frequency, particularly `DatetimeIndex.shift`,\n    `TimedeltaIndex.shift`, `PeriodIndex.shift`.\n    \"\"\"\n\n\nclass AccessorRegistrationWarning(Warning):\n    \"\"\"Warning for attribute conflicts in accessor registration.\"\"\"\n\n\nclass AbstractMethodError(NotImplementedError):\n    \"\"\"Raise this error instead of NotImplementedError for abstract methods\n    while keeping compatibility with Python 2 and Python 3.\n    \"\"\"\n\n    def __init__(self, class_instance, methodtype='method'):\n        types = {'method', 'classmethod', 'staticmethod', 'property'}\n        if methodtype not in types:\n            msg = 'methodtype must be one of {}, got {} instead.'.format(\n                methodtype, types)\n            raise ValueError(msg)\n        self.methodtype = methodtype\n        self.class_instance = class_instance\n\n    def __str__(self):\n        if self.methodtype == 'classmethod':\n            name = self.class_instance.__name__\n        else:\n            name = self.class_instance.__class__.__name__\n        msg = \"This {methodtype} must be defined in the concrete class {name}\"\n        return (msg.format(methodtype=self.methodtype, name=name))\n"
    },
    {
      "filename": "pandas/io/formats/style.py",
      "content": "\"\"\"\nModule for applying conditional formatting to\nDataFrames and Series.\n\"\"\"\nfrom collections import MutableMapping, defaultdict\nfrom contextlib import contextmanager\nimport copy\nfrom functools import partial\nfrom itertools import product\nfrom uuid import uuid1\n\nimport numpy as np\n\nfrom pandas.compat import range\nfrom pandas.util._decorators import Appender\n\nfrom pandas.core.dtypes.common import is_float, is_string_like\nfrom pandas.core.dtypes.generic import ABCSeries\n\nimport pandas as pd\nfrom pandas.api.types import is_list_like\nimport pandas.core.common as com\nfrom pandas.core.config import get_option\nfrom pandas.core.generic import _shared_docs\nfrom pandas.core.indexing import _maybe_numeric_slice, _non_reducing_slice\n\ntry:\n    from jinja2 import (\n        PackageLoader, Environment, ChoiceLoader, FileSystemLoader\n    )\nexcept ImportError:\n    raise ImportError(\"pandas.Styler requires jinja2. \"\n                      \"Please install with `conda install Jinja2`\\n\"\n                      \"or `pip install Jinja2`\")\n\n\ntry:\n    import matplotlib.pyplot as plt\n    from matplotlib import colors\n    has_mpl = True\nexcept ImportError:\n    has_mpl = False\n    no_mpl_message = \"{0} requires matplotlib.\"\n\n\n@contextmanager\ndef _mpl(func):\n    if has_mpl:\n        yield plt, colors\n    else:\n        raise ImportError(no_mpl_message.format(func.__name__))\n\n\nclass Styler(object):\n    \"\"\"\n    Helps style a DataFrame or Series according to the\n    data with HTML and CSS.\n\n    Parameters\n    ----------\n    data : Series or DataFrame\n    precision : int\n        precision to round floats to, defaults to pd.options.display.precision\n    table_styles : list-like, default None\n        list of {selector: (attr, value)} dicts; see Notes\n    uuid : str, default None\n        a unique identifier to avoid CSS collisions; generated automatically\n    caption : str, default None\n        caption to attach to the table\n    cell_ids : bool, default True\n        If True, each cell will have an ``id`` attribute in their HTML tag.\n        The ``id`` takes the form ``T_<uuid>_row<num_row>_col<num_col>``\n        where ``<uuid>`` is the unique identifier, ``<num_row>`` is the row\n        number and ``<num_col>`` is the column number.\n\n    Attributes\n    ----------\n    env : Jinja2 Environment\n    template : Jinja2 Template\n    loader : Jinja2 Loader\n\n    Notes\n    -----\n    Most styling will be done by passing style functions into\n    ``Styler.apply`` or ``Styler.applymap``. Style functions should\n    return values with strings containing CSS ``'attr: value'`` that will\n    be applied to the indicated cells.\n\n    If using in the Jupyter notebook, Styler has defined a ``_repr_html_``\n    to automatically render itself. Otherwise call Styler.render to get\n    the generated HTML.\n\n    CSS classes are attached to the generated HTML\n\n    * Index and Column names include ``index_name`` and ``level<k>``\n      where `k` is its level in a MultiIndex\n    * Index label cells include\n\n      * ``row_heading``\n      * ``row<n>`` where `n` is the numeric position of the row\n      * ``level<k>`` where `k` is the level in a MultiIndex\n\n    * Column label cells include\n      * ``col_heading``\n      * ``col<n>`` where `n` is the numeric position of the column\n      * ``evel<k>`` where `k` is the level in a MultiIndex\n\n    * Blank cells include ``blank``\n    * Data cells include ``data``\n\n    See Also\n    --------\n    pandas.DataFrame.style\n    \"\"\"\n    loader = PackageLoader(\"pandas\", \"io/formats/templates\")\n    env = Environment(\n        loader=loader,\n        trim_blocks=True,\n    )\n    template = env.get_template(\"html.tpl\")\n\n    def __init__(self, data, precision=None, table_styles=None, uuid=None,\n                 caption=None, table_attributes=None, cell_ids=True):\n        self.ctx = defaultdict(list)\n        self._todo = []\n\n        if not isinstance(data, (pd.Series, pd.DataFrame)):\n            raise TypeError(\"``data`` must be a Series or DataFrame\")\n        if data.ndim == 1:\n            data = data.to_frame()\n        if not data.index.is_unique or not data.columns.is_unique:\n            raise ValueError(\"style is not supported for non-unique indices.\")\n\n        self.data = data\n        self.index = data.index\n        self.columns = data.columns\n\n        self.uuid = uuid\n        self.table_styles = table_styles\n        self.caption = caption\n        if precision is None:\n            precision = get_option('display.precision')\n        self.precision = precision\n        self.table_attributes = table_attributes\n        self.hidden_index = False\n        self.hidden_columns = []\n        self.cell_ids = cell_ids\n\n        # display_funcs maps (row, col) -> formatting function\n\n        def default_display_func(x):\n            if is_float(x):\n                return '{:>.{precision}g}'.format(x, precision=self.precision)\n            else:\n                return x\n\n        self._display_funcs = defaultdict(lambda: default_display_func)\n\n    def _repr_html_(self):\n        \"\"\"Hooks into Jupyter notebook rich display system.\"\"\"\n        return self.render()\n\n    @Appender(_shared_docs['to_excel'] % dict(\n        axes='index, columns', klass='Styler',\n        axes_single_arg=\"{0 or 'index', 1 or 'columns'}\",\n        optional_by=\"\"\"\n            by : str or list of str\n                Name or list of names which refer to the axis items.\"\"\",\n        versionadded_to_excel='\\n    .. versionadded:: 0.20'))\n    def to_excel(self, excel_writer, sheet_name='Sheet1', na_rep='',\n                 float_format=None, columns=None, header=True, index=True,\n                 index_label=None, startrow=0, startcol=0, engine=None,\n                 merge_cells=True, encoding=None, inf_rep='inf', verbose=True,\n                 freeze_panes=None):\n\n        from pandas.io.formats.excel import ExcelFormatter\n        formatter = ExcelFormatter(self, na_rep=na_rep, cols=columns,\n                                   header=header,\n                                   float_format=float_format, index=index,\n                                   index_label=index_label,\n                                   merge_cells=merge_cells,\n                                   inf_rep=inf_rep)\n        formatter.write(excel_writer, sheet_name=sheet_name, startrow=startrow,\n                        startcol=startcol, freeze_panes=freeze_panes,\n                        engine=engine)\n\n    def _translate(self):\n        \"\"\"\n        Convert the DataFrame in `self.data` and the attrs from `_build_styles`\n        into a dictionary of {head, body, uuid, cellstyle}\n        \"\"\"\n        table_styles = self.table_styles or []\n        caption = self.caption\n        ctx = self.ctx\n        precision = self.precision\n        hidden_index = self.hidden_index\n        hidden_columns = self.hidden_columns\n        uuid = self.uuid or str(uuid1()).replace(\"-\", \"_\")\n        ROW_HEADING_CLASS = \"row_heading\"\n        COL_HEADING_CLASS = \"col_heading\"\n        INDEX_NAME_CLASS = \"index_name\"\n\n        DATA_CLASS = \"data\"\n        BLANK_CLASS = \"blank\"\n        BLANK_VALUE = \"\"\n\n        def format_attr(pair):\n            return \"{key}={value}\".format(**pair)\n\n        # for sparsifying a MultiIndex\n        idx_lengths = _get_level_lengths(self.index)\n        col_lengths = _get_level_lengths(self.columns, hidden_columns)\n\n        cell_context = dict()\n\n        n_rlvls = self.data.index.nlevels\n        n_clvls = self.data.columns.nlevels\n        rlabels = self.data.index.tolist()\n        clabels = self.data.columns.tolist()\n\n        if n_rlvls == 1:\n            rlabels = [[x] for x in rlabels]\n        if n_clvls == 1:\n            clabels = [[x] for x in clabels]\n        clabels = list(zip(*clabels))\n\n        cellstyle = []\n        head = []\n\n        for r in range(n_clvls):\n            # Blank for Index columns...\n            row_es = [{\"type\": \"th\",\n                       \"value\": BLANK_VALUE,\n                       \"display_value\": BLANK_VALUE,\n                       \"is_visible\": not hidden_index,\n                       \"class\": \" \".join([BLANK_CLASS])}] * (n_rlvls - 1)\n\n            # ... except maybe the last for columns.names\n            name = self.data.columns.names[r]\n            cs = [BLANK_CLASS if name is None else INDEX_NAME_CLASS,\n                  \"level{lvl}\".format(lvl=r)]\n            name = BLANK_VALUE if name is None else name\n            row_es.append({\"type\": \"th\",\n                           \"value\": name,\n                           \"display_value\": name,\n                           \"class\": \" \".join(cs),\n                           \"is_visible\": not hidden_index})\n\n            if clabels:\n                for c, value in enumerate(clabels[r]):\n                    cs = [COL_HEADING_CLASS, \"level{lvl}\".format(lvl=r),\n                          \"col{col}\".format(col=c)]\n                    cs.extend(cell_context.get(\n                        \"col_headings\", {}).get(r, {}).get(c, []))\n                    es = {\n                        \"type\": \"th\",\n                        \"value\": value,\n                        \"display_value\": value,\n                        \"class\": \" \".join(cs),\n                        \"is_visible\": _is_visible(c, r, col_lengths),\n                    }\n                    colspan = col_lengths.get((r, c), 0)\n                    if colspan > 1:\n                        es[\"attributes\"] = [\n                            format_attr({\"key\": \"colspan\", \"value\": colspan})\n                        ]\n                    row_es.append(es)\n                head.append(row_es)\n\n        if (self.data.index.names and\n                com._any_not_none(*self.data.index.names) and\n                not hidden_index):\n            index_header_row = []\n\n            for c, name in enumerate(self.data.index.names):\n                cs = [INDEX_NAME_CLASS,\n                      \"level{lvl}\".format(lvl=c)]\n                name = '' if name is None else name\n                index_header_row.append({\"type\": \"th\", \"value\": name,\n                                         \"class\": \" \".join(cs)})\n\n            index_header_row.extend(\n                [{\"type\": \"th\",\n                  \"value\": BLANK_VALUE,\n                  \"class\": \" \".join([BLANK_CLASS])\n                  }] * (len(clabels[0]) - len(hidden_columns)))\n\n            head.append(index_header_row)\n\n        body = []\n        for r, idx in enumerate(self.data.index):\n            row_es = []\n            for c, value in enumerate(rlabels[r]):\n                rid = [ROW_HEADING_CLASS, \"level{lvl}\".format(lvl=c),\n                       \"row{row}\".format(row=r)]\n                es = {\n                    \"type\": \"th\",\n                    \"is_visible\": (_is_visible(r, c, idx_lengths) and\n                                   not hidden_index),\n                    \"value\": value,\n                    \"display_value\": value,\n                    \"id\": \"_\".join(rid[1:]),\n                    \"class\": \" \".join(rid)\n                }\n                rowspan = idx_lengths.get((c, r), 0)\n                if rowspan > 1:\n                    es[\"attributes\"] = [\n                        format_attr({\"key\": \"rowspan\", \"value\": rowspan})\n                    ]\n                row_es.append(es)\n\n            for c, col in enumerate(self.data.columns):\n                cs = [DATA_CLASS, \"row{row}\".format(row=r),\n                      \"col{col}\".format(col=c)]\n                cs.extend(cell_context.get(\"data\", {}).get(r, {}).get(c, []))\n                formatter = self._display_funcs[(r, c)]\n                value = self.data.iloc[r, c]\n                row_dict = {\"type\": \"td\",\n                            \"value\": value,\n                            \"class\": \" \".join(cs),\n                            \"display_value\": formatter(value),\n                            \"is_visible\": (c not in hidden_columns)}\n                # only add an id if the cell has a style\n                if (self.cell_ids or\n                        not(len(ctx[r, c]) == 1 and ctx[r, c][0] == '')):\n                    row_dict[\"id\"] = \"_\".join(cs[1:])\n                row_es.append(row_dict)\n                props = []\n                for x in ctx[r, c]:\n                    # have to handle empty styles like ['']\n                    if x.count(\":\"):\n                        props.append(x.split(\":\"))\n                    else:\n                        props.append(['', ''])\n                cellstyle.append({'props': props,\n                                  'selector': \"row{row}_col{col}\"\n                                  .format(row=r, col=c)})\n            body.append(row_es)\n\n        table_attr = self.table_attributes\n        use_mathjax = get_option(\"display.html.use_mathjax\")\n        if not use_mathjax:\n            table_attr = table_attr or ''\n            if 'class=\"' in table_attr:\n                table_attr = table_attr.replace('class=\"',\n                                                'class=\"tex2jax_ignore ')\n            else:\n                table_attr += ' class=\"tex2jax_ignore\"'\n\n        return dict(head=head, cellstyle=cellstyle, body=body, uuid=uuid,\n                    precision=precision, table_styles=table_styles,\n                    caption=caption, table_attributes=table_attr)\n\n    def format(self, formatter, subset=None):\n        \"\"\"\n        Format the text display value of cells.\n\n        .. versionadded:: 0.18.0\n\n        Parameters\n        ----------\n        formatter : str, callable, or dict\n        subset : IndexSlice\n            An argument to ``DataFrame.loc`` that restricts which elements\n            ``formatter`` is applied to.\n\n        Returns\n        -------\n        self : Styler\n\n        Notes\n        -----\n\n        ``formatter`` is either an ``a`` or a dict ``{column name: a}`` where\n        ``a`` is one of\n\n        - str: this will be wrapped in: ``a.format(x)``\n        - callable: called with the value of an individual cell\n\n        The default display value for numeric values is the \"general\" (``g``)\n        format with ``pd.options.display.precision`` precision.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame(np.random.randn(4, 2), columns=['a', 'b'])\n        >>> df.style.format(\"{:.2%}\")\n        >>> df['c'] = ['a', 'b', 'c', 'd']\n        >>> df.style.format({'c': str.upper})\n        \"\"\"\n        if subset is None:\n            row_locs = range(len(self.data))\n            col_locs = range(len(self.data.columns))\n        else:\n            subset = _non_reducing_slice(subset)\n            if len(subset) == 1:\n                subset = subset, self.data.columns\n\n            sub_df = self.data.loc[subset]\n            row_locs = self.data.index.get_indexer_for(sub_df.index)\n            col_locs = self.data.columns.get_indexer_for(sub_df.columns)\n\n        if isinstance(formatter, MutableMapping):\n            for col, col_formatter in formatter.items():\n                # formatter must be callable, so '{}' are converted to lambdas\n                col_formatter = _maybe_wrap_formatter(col_formatter)\n                col_num = self.data.columns.get_indexer_for([col])[0]\n\n                for row_num in row_locs:\n                    self._display_funcs[(row_num, col_num)] = col_formatter\n        else:\n            # single scalar to format all cells with\n            locs = product(*(row_locs, col_locs))\n            for i, j in locs:\n                formatter = _maybe_wrap_formatter(formatter)\n                self._display_funcs[(i, j)] = formatter\n        return self\n\n    def render(self, **kwargs):\n        \"\"\"Render the built up styles to HTML\n\n        Parameters\n        ----------\n        `**kwargs` : Any additional keyword arguments are passed through\n        to ``self.template.render``. This is useful when you need to provide\n        additional variables for a custom template.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        rendered : str\n            the rendered HTML\n\n        Notes\n        -----\n        ``Styler`` objects have defined the ``_repr_html_`` method\n        which automatically calls ``self.render()`` when it's the\n        last item in a Notebook cell. When calling ``Styler.render()``\n        directly, wrap the result in ``IPython.display.HTML`` to view\n        the rendered HTML in the notebook.\n\n        Pandas uses the following keys in render. Arguments passed\n        in ``**kwargs`` take precedence, so think carefully if you want\n        to override them:\n\n        * head\n        * cellstyle\n        * body\n        * uuid\n        * precision\n        * table_styles\n        * caption\n        * table_attributes\n        \"\"\"\n        self._compute()\n        # TODO: namespace all the pandas keys\n        d = self._translate()\n        # filter out empty styles, every cell will have a class\n        # but the list of props may just be [['', '']].\n        # so we have the neested anys below\n        trimmed = [x for x in d['cellstyle']\n                   if any(any(y) for y in x['props'])]\n        d['cellstyle'] = trimmed\n        d.update(kwargs)\n        return self.template.render(**d)\n\n    def _update_ctx(self, attrs):\n        \"\"\"\n        update the state of the Styler. Collects a mapping\n        of {index_label: ['<property>: <value>']}\n\n        attrs : Series or DataFrame\n        should contain strings of '<property>: <value>;<prop2>: <val2>'\n        Whitespace shouldn't matter and the final trailing ';' shouldn't\n        matter.\n        \"\"\"\n        for row_label, v in attrs.iterrows():\n            for col_label, col in v.iteritems():\n                i = self.index.get_indexer([row_label])[0]\n                j = self.columns.get_indexer([col_label])[0]\n                for pair in col.rstrip(\";\").split(\";\"):\n                    self.ctx[(i, j)].append(pair)\n\n    def _copy(self, deepcopy=False):\n        styler = Styler(self.data, precision=self.precision,\n                        caption=self.caption, uuid=self.uuid,\n                        table_styles=self.table_styles)\n        if deepcopy:\n            styler.ctx = copy.deepcopy(self.ctx)\n            styler._todo = copy.deepcopy(self._todo)\n        else:\n            styler.ctx = self.ctx\n            styler._todo = self._todo\n        return styler\n\n    def __copy__(self):\n        \"\"\"\n        Deep copy by default.\n        \"\"\"\n        return self._copy(deepcopy=False)\n\n    def __deepcopy__(self, memo):\n        return self._copy(deepcopy=True)\n\n    def clear(self):\n        \"\"\"\"Reset\" the styler, removing any previously applied styles.\n        Returns None.\n        \"\"\"\n        self.ctx.clear()\n        self._todo = []\n\n    def _compute(self):\n        \"\"\"\n        Execute the style functions built up in `self._todo`.\n\n        Relies on the conventions that all style functions go through\n        .apply or .applymap. The append styles to apply as tuples of\n\n        (application method, *args, **kwargs)\n        \"\"\"\n        r = self\n        for func, args, kwargs in self._todo:\n            r = func(self)(*args, **kwargs)\n        return r\n\n    def _apply(self, func, axis=0, subset=None, **kwargs):\n        subset = slice(None) if subset is None else subset\n        subset = _non_reducing_slice(subset)\n        data = self.data.loc[subset]\n        if axis is not None:\n            result = data.apply(func, axis=axis,\n                                result_type='expand', **kwargs)\n            result.columns = data.columns\n        else:\n            result = func(data, **kwargs)\n            if not isinstance(result, pd.DataFrame):\n                raise TypeError(\n                    \"Function {func!r} must return a DataFrame when \"\n                    \"passed to `Styler.apply` with axis=None\"\n                    .format(func=func))\n            if not (result.index.equals(data.index) and\n                    result.columns.equals(data.columns)):\n                msg = ('Result of {func!r} must have identical index and '\n                       'columns as the input'.format(func=func))\n                raise ValueError(msg)\n\n        result_shape = result.shape\n        expected_shape = self.data.loc[subset].shape\n        if result_shape != expected_shape:\n            msg = (\"Function {func!r} returned the wrong shape.\\n\"\n                   \"Result has shape: {res}\\n\"\n                   \"Expected shape:   {expect}\".format(func=func,\n                                                       res=result.shape,\n                                                       expect=expected_shape))\n            raise ValueError(msg)\n        self._update_ctx(result)\n        return self\n\n    def apply(self, func, axis=0, subset=None, **kwargs):\n        \"\"\"\n        Apply a function column-wise, row-wise, or table-wise,\n        updating the HTML representation with the result.\n\n        Parameters\n        ----------\n        func : function\n            ``func`` should take a Series or DataFrame (depending\n            on ``axis``), and return an object with the same shape.\n            Must return a DataFrame with identical index and\n            column labels when ``axis=None``\n        axis : int, str or None\n            apply to each column (``axis=0`` or ``'index'``)\n            or to each row (``axis=1`` or ``'columns'``) or\n            to the entire DataFrame at once with ``axis=None``\n        subset : IndexSlice\n            a valid indexer to limit ``data`` to *before* applying the\n            function. Consider using a pandas.IndexSlice\n        kwargs : dict\n            pass along to ``func``\n\n        Returns\n        -------\n        self : Styler\n\n        Notes\n        -----\n        The output shape of ``func`` should match the input, i.e. if\n        ``x`` is the input row, column, or table (depending on ``axis``),\n        then ``func(x).shape == x.shape`` should be true.\n\n        This is similar to ``DataFrame.apply``, except that ``axis=None``\n        applies the function to the entire DataFrame at once,\n        rather than column-wise or row-wise.\n\n        Examples\n        --------\n        >>> def highlight_max(x):\n        ...     return ['background-color: yellow' if v == x.max() else ''\n                        for v in x]\n        ...\n        >>> df = pd.DataFrame(np.random.randn(5, 2))\n        >>> df.style.apply(highlight_max)\n        \"\"\"\n        self._todo.append((lambda instance: getattr(instance, '_apply'),\n                           (func, axis, subset), kwargs))\n        return self\n\n    def _applymap(self, func, subset=None, **kwargs):\n        func = partial(func, **kwargs)  # applymap doesn't take kwargs?\n        if subset is None:\n            subset = pd.IndexSlice[:]\n        subset = _non_reducing_slice(subset)\n        result = self.data.loc[subset].applymap(func)\n        self._update_ctx(result)\n        return self\n\n    def applymap(self, func, subset=None, **kwargs):\n        \"\"\"\n        Apply a function elementwise, updating the HTML\n        representation with the result.\n\n        Parameters\n        ----------\n        func : function\n            ``func`` should take a scalar and return a scalar\n        subset : IndexSlice\n            a valid indexer to limit ``data`` to *before* applying the\n            function. Consider using a pandas.IndexSlice\n        kwargs : dict\n            pass along to ``func``\n\n        Returns\n        -------\n        self : Styler\n\n        See Also\n        --------\n        Styler.where\n        \"\"\"\n        self._todo.append((lambda instance: getattr(instance, '_applymap'),\n                           (func, subset), kwargs))\n        return self\n\n    def where(self, cond, value, other=None, subset=None, **kwargs):\n        \"\"\"\n        Apply a function elementwise, updating the HTML\n        representation with a style which is selected in\n        accordance with the return value of a function.\n\n        .. versionadded:: 0.21.0\n\n        Parameters\n        ----------\n        cond : callable\n            ``cond`` should take a scalar and return a boolean\n        value : str\n            applied when ``cond`` returns true\n        other : str\n            applied when ``cond`` returns false\n        subset : IndexSlice\n            a valid indexer to limit ``data`` to *before* applying the\n            function. Consider using a pandas.IndexSlice\n        kwargs : dict\n            pass along to ``cond``\n\n        Returns\n        -------\n        self : Styler\n\n        See Also\n        --------\n        Styler.applymap\n        \"\"\"\n\n        if other is None:\n            other = ''\n\n        return self.applymap(lambda val: value if cond(val) else other,\n                             subset=subset, **kwargs)\n\n    def set_precision(self, precision):\n        \"\"\"\n        Set the precision used to render.\n\n        Parameters\n        ----------\n        precision : int\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        self.precision = precision\n        return self\n\n    def set_table_attributes(self, attributes):\n        \"\"\"\n        Set the table attributes. These are the items\n        that show up in the opening ``<table>`` tag in addition\n        to to automatic (by default) id.\n\n        Parameters\n        ----------\n        attributes : string\n\n        Returns\n        -------\n        self : Styler\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.random.randn(10, 4))\n        >>> df.style.set_table_attributes('class=\"pure-table\"')\n        # ... <table class=\"pure-table\"> ...\n        \"\"\"\n        self.table_attributes = attributes\n        return self\n\n    def export(self):\n        \"\"\"\n        Export the styles to applied to the current Styler.\n        Can be applied to a second style with ``Styler.use``.\n\n        Returns\n        -------\n        styles : list\n\n        See Also\n        --------\n        Styler.use\n        \"\"\"\n        return self._todo\n\n    def use(self, styles):\n        \"\"\"\n        Set the styles on the current Styler, possibly using styles\n        from ``Styler.export``.\n\n        Parameters\n        ----------\n        styles : list\n            list of style functions\n\n        Returns\n        -------\n        self : Styler\n\n        See Also\n        --------\n        Styler.export\n        \"\"\"\n        self._todo.extend(styles)\n        return self\n\n    def set_uuid(self, uuid):\n        \"\"\"\n        Set the uuid for a Styler.\n\n        Parameters\n        ----------\n        uuid : str\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        self.uuid = uuid\n        return self\n\n    def set_caption(self, caption):\n        \"\"\"\n        Set the caption on a Styler\n\n        Parameters\n        ----------\n        caption : str\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        self.caption = caption\n        return self\n\n    def set_table_styles(self, table_styles):\n        \"\"\"\n        Set the table styles on a Styler. These are placed in a\n        ``<style>`` tag before the generated HTML table.\n\n        Parameters\n        ----------\n        table_styles : list\n            Each individual table_style should be a dictionary with\n            ``selector`` and ``props`` keys. ``selector`` should be a CSS\n            selector that the style will be applied to (automatically\n            prefixed by the table's UUID) and ``props`` should be a list of\n            tuples with ``(attribute, value)``.\n\n        Returns\n        -------\n        self : Styler\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.random.randn(10, 4))\n        >>> df.style.set_table_styles(\n        ...     [{'selector': 'tr:hover',\n        ...       'props': [('background-color', 'yellow')]}]\n        ... )\n        \"\"\"\n        self.table_styles = table_styles\n        return self\n\n    def hide_index(self):\n        \"\"\"\n        Hide any indices from rendering.\n\n        .. versionadded:: 0.23.0\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        self.hidden_index = True\n        return self\n\n    def hide_columns(self, subset):\n        \"\"\"\n        Hide columns from rendering.\n\n        .. versionadded:: 0.23.0\n\n        Parameters\n        ----------\n        subset : IndexSlice\n            An argument to ``DataFrame.loc`` that identifies which columns\n            are hidden.\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        subset = _non_reducing_slice(subset)\n        hidden_df = self.data.loc[subset]\n        self.hidden_columns = self.columns.get_indexer_for(hidden_df.columns)\n        return self\n\n    # -----------------------------------------------------------------------\n    # A collection of \"builtin\" styles\n    # -----------------------------------------------------------------------\n\n    @staticmethod\n    def _highlight_null(v, null_color):\n        return ('background-color: {color}'.format(color=null_color)\n                if pd.isna(v) else '')\n\n    def highlight_null(self, null_color='red'):\n        \"\"\"\n        Shade the background ``null_color`` for missing values.\n\n        Parameters\n        ----------\n        null_color : str\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        self.applymap(self._highlight_null, null_color=null_color)\n        return self\n\n    def background_gradient(self, cmap='PuBu', low=0, high=0, axis=0,\n                            subset=None, text_color_threshold=0.408):\n        \"\"\"\n        Color the background in a gradient according to\n        the data in each column (optionally row).\n        Requires matplotlib.\n\n        Parameters\n        ----------\n        cmap : str or colormap\n            matplotlib colormap\n        low, high : float\n            compress the range by these values.\n        axis : int or str\n            1 or 'columns' for columnwise, 0 or 'index' for rowwise\n        subset : IndexSlice\n            a valid slice for ``data`` to limit the style application to\n        text_color_threshold : float or int\n            luminance threshold for determining text color. Facilitates text\n            visibility across varying background colors. From 0 to 1.\n            0 = all text is dark colored, 1 = all text is light colored.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        self : Styler\n\n        Notes\n        -----\n        Set ``text_color_threshold`` or tune ``low`` and ``high`` to keep the\n        text legible by not using the entire range of the color map. The range\n        of the data is extended by ``low * (x.max() - x.min())`` and ``high *\n        (x.max() - x.min())`` before normalizing.\n\n        Raises\n        ------\n        ValueError\n            If ``text_color_threshold`` is not a value from 0 to 1.\n        \"\"\"\n        subset = _maybe_numeric_slice(self.data, subset)\n        subset = _non_reducing_slice(subset)\n        self.apply(self._background_gradient, cmap=cmap, subset=subset,\n                   axis=axis, low=low, high=high,\n                   text_color_threshold=text_color_threshold)\n        return self\n\n    @staticmethod\n    def _background_gradient(s, cmap='PuBu', low=0, high=0,\n                             text_color_threshold=0.408):\n        \"\"\"Color background in a range according to the data.\"\"\"\n        if (not isinstance(text_color_threshold, (float, int)) or\n                not 0 <= text_color_threshold <= 1):\n            msg = \"`text_color_threshold` must be a value from 0 to 1.\"\n            raise ValueError(msg)\n\n        with _mpl(Styler.background_gradient) as (plt, colors):\n            smin = s.values.min()\n            smax = s.values.max()\n            rng = smax - smin\n            # extend lower / upper bounds, compresses color range\n            norm = colors.Normalize(smin - (rng * low), smax + (rng * high))\n            # matplotlib colors.Normalize modifies inplace?\n            # https://github.com/matplotlib/matplotlib/issues/5427\n            rgbas = plt.cm.get_cmap(cmap)(norm(s.values))\n\n            def relative_luminance(rgba):\n                \"\"\"\n                Calculate relative luminance of a color.\n\n                The calculation adheres to the W3C standards\n                (https://www.w3.org/WAI/GL/wiki/Relative_luminance)\n\n                Parameters\n                ----------\n                color : rgb or rgba tuple\n\n                Returns\n                -------\n                float\n                    The relative luminance as a value from 0 to 1\n                \"\"\"\n                r, g, b = (\n                    x / 12.92 if x <= 0.03928 else ((x + 0.055) / 1.055 ** 2.4)\n                    for x in rgba[:3]\n                )\n                return 0.2126 * r + 0.7152 * g + 0.0722 * b\n\n            def css(rgba):\n                dark = relative_luminance(rgba) < text_color_threshold\n                text_color = '#f1f1f1' if dark else '#000000'\n                return 'background-color: {b};color: {c};'.format(\n                    b=colors.rgb2hex(rgba), c=text_color\n                )\n\n            if s.ndim == 1:\n                return [css(rgba) for rgba in rgbas]\n            else:\n                return pd.DataFrame(\n                    [[css(rgba) for rgba in row] for row in rgbas],\n                    index=s.index, columns=s.columns\n                )\n\n    def set_properties(self, subset=None, **kwargs):\n        \"\"\"\n        Convenience method for setting one or more non-data dependent\n        properties or each cell.\n\n        Parameters\n        ----------\n        subset : IndexSlice\n            a valid slice for ``data`` to limit the style application to\n        kwargs : dict\n            property: value pairs to be set for each cell\n\n        Returns\n        -------\n        self : Styler\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.random.randn(10, 4))\n        >>> df.style.set_properties(color=\"white\", align=\"right\")\n        >>> df.style.set_properties(**{'background-color': 'yellow'})\n        \"\"\"\n        values = ';'.join('{p}: {v}'.format(p=p, v=v)\n                          for p, v in kwargs.items())\n        f = lambda x: values\n        return self.applymap(f, subset=subset)\n\n    @staticmethod\n    def _bar(s, align, colors, width=100, vmin=None, vmax=None):\n        \"\"\"Draw bar chart in dataframe cells\"\"\"\n\n        # Get input value range.\n        smin = s.min() if vmin is None else vmin\n        if isinstance(smin, ABCSeries):\n            smin = smin.min()\n        smax = s.max() if vmax is None else vmax\n        if isinstance(smax, ABCSeries):\n            smax = smax.max()\n        if align == 'mid':\n            smin = min(0, smin)\n            smax = max(0, smax)\n        elif align == 'zero':\n            # For \"zero\" mode, we want the range to be symmetrical around zero.\n            smax = max(abs(smin), abs(smax))\n            smin = -smax\n        # Transform to percent-range of linear-gradient\n        normed = width * (s.values - smin) / (smax - smin + 1e-12)\n        zero = -width * smin / (smax - smin + 1e-12)\n\n        def css_bar(start, end, color):\n            \"\"\"Generate CSS code to draw a bar from start to end.\"\"\"\n            css = 'width: 10em; height: 80%;'\n            if end > start:\n                css += 'background: linear-gradient(90deg,'\n                if start > 0:\n                    css += ' transparent {s:.1f}%, {c} {s:.1f}%, '.format(\n                        s=start, c=color\n                    )\n                css += '{c} {e:.1f}%, transparent {e:.1f}%)'.format(\n                    e=min(end, width), c=color,\n                )\n            return css\n\n        def css(x):\n            if pd.isna(x):\n                return ''\n\n            # avoid deprecated indexing `colors[x > zero]`\n            color = colors[1] if x > zero else colors[0]\n\n            if align == 'left':\n                return css_bar(0, x, color)\n            else:\n                return css_bar(min(x, zero), max(x, zero), color)\n\n        if s.ndim == 1:\n            return [css(x) for x in normed]\n        else:\n            return pd.DataFrame(\n                [[css(x) for x in row] for row in normed],\n                index=s.index, columns=s.columns\n            )\n\n    def bar(self, subset=None, axis=0, color='#d65f5f', width=100,\n            align='left', vmin=None, vmax=None):\n        \"\"\"\n        Draw bar chart in the cell backgrounds.\n\n        Parameters\n        ----------\n        subset : IndexSlice, optional\n            A valid slice for `data` to limit the style application to.\n        axis : int, str or None, default 0\n            Apply to each column (`axis=0` or `'index'`)\n            or to each row (`axis=1` or `'columns'`) or\n            to the entire DataFrame at once with `axis=None`.\n        color : str or 2-tuple/list\n            If a str is passed, the color is the same for both\n            negative and positive numbers. If 2-tuple/list is used, the\n            first element is the color_negative and the second is the\n            color_positive (eg: ['#d65f5f', '#5fba7d']).\n        width : float, default 100\n            A number between 0 or 100. The largest value will cover `width`\n            percent of the cell's width.\n        align : {'left', 'zero',' mid'}, default 'left'\n            How to align the bars with the cells.\n\n            - 'left' : the min value starts at the left of the cell.\n            - 'zero' : a value of zero is located at the center of the cell.\n            - 'mid' : the center of the cell is at (max-min)/2, or\n              if values are all negative (positive) the zero is aligned\n              at the right (left) of the cell.\n\n              .. versionadded:: 0.20.0\n\n        vmin : float, optional\n            Minimum bar value, defining the left hand limit\n            of the bar drawing range, lower values are clipped to `vmin`.\n            When None (default): the minimum value of the data will be used.\n\n            .. versionadded:: 0.24.0\n\n        vmax : float, optional\n            Maximum bar value, defining the right hand limit\n            of the bar drawing range, higher values are clipped to `vmax`.\n            When None (default): the maximum value of the data will be used.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        if align not in ('left', 'zero', 'mid'):\n            raise ValueError(\"`align` must be one of {'left', 'zero',' mid'}\")\n\n        if not (is_list_like(color)):\n            color = [color, color]\n        elif len(color) == 1:\n            color = [color[0], color[0]]\n        elif len(color) > 2:\n            raise ValueError(\"`color` must be string or a list-like\"\n                             \" of length 2: [`color_neg`, `color_pos`]\"\n                             \" (eg: color=['#d65f5f', '#5fba7d'])\")\n\n        subset = _maybe_numeric_slice(self.data, subset)\n        subset = _non_reducing_slice(subset)\n        self.apply(self._bar, subset=subset, axis=axis,\n                   align=align, colors=color, width=width,\n                   vmin=vmin, vmax=vmax)\n\n        return self\n\n    def highlight_max(self, subset=None, color='yellow', axis=0):\n        \"\"\"\n        Highlight the maximum by shading the background\n\n        Parameters\n        ----------\n        subset : IndexSlice, default None\n            a valid slice for ``data`` to limit the style application to\n        color : str, default 'yellow'\n        axis : int, str, or None; default 0\n            0 or 'index' for columnwise (default), 1 or 'columns' for rowwise,\n            or ``None`` for tablewise\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        return self._highlight_handler(subset=subset, color=color, axis=axis,\n                                       max_=True)\n\n    def highlight_min(self, subset=None, color='yellow', axis=0):\n        \"\"\"\n        Highlight the minimum by shading the background\n\n        Parameters\n        ----------\n        subset : IndexSlice, default None\n            a valid slice for ``data`` to limit the style application to\n        color : str, default 'yellow'\n        axis : int, str, or None; default 0\n            0 or 'index' for columnwise (default), 1 or 'columns' for rowwise,\n            or ``None`` for tablewise\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        return self._highlight_handler(subset=subset, color=color, axis=axis,\n                                       max_=False)\n\n    def _highlight_handler(self, subset=None, color='yellow', axis=None,\n                           max_=True):\n        subset = _non_reducing_slice(_maybe_numeric_slice(self.data, subset))\n        self.apply(self._highlight_extrema, color=color, axis=axis,\n                   subset=subset, max_=max_)\n        return self\n\n    @staticmethod\n    def _highlight_extrema(data, color='yellow', max_=True):\n        \"\"\"Highlight the min or max in a Series or DataFrame\"\"\"\n        attr = 'background-color: {0}'.format(color)\n        if data.ndim == 1:  # Series from .apply\n            if max_:\n                extrema = data == data.max()\n            else:\n                extrema = data == data.min()\n            return [attr if v else '' for v in extrema]\n        else:  # DataFrame from .tee\n            if max_:\n                extrema = data == data.max().max()\n            else:\n                extrema = data == data.min().min()\n            return pd.DataFrame(np.where(extrema, attr, ''),\n                                index=data.index, columns=data.columns)\n\n    @classmethod\n    def from_custom_template(cls, searchpath, name):\n        \"\"\"\n        Factory function for creating a subclass of ``Styler``\n        with a custom template and Jinja environment.\n\n        Parameters\n        ----------\n        searchpath : str or list\n            Path or paths of directories containing the templates\n        name : str\n            Name of your custom template to use for rendering\n\n        Returns\n        -------\n        MyStyler : subclass of Styler\n            has the correct ``env`` and ``template`` class attributes set.\n        \"\"\"\n        loader = ChoiceLoader([\n            FileSystemLoader(searchpath),\n            cls.loader,\n        ])\n\n        class MyStyler(cls):\n            env = Environment(loader=loader)\n            template = env.get_template(name)\n\n        return MyStyler\n\n\ndef _is_visible(idx_row, idx_col, lengths):\n    \"\"\"\n    Index -> {(idx_row, idx_col): bool})\n    \"\"\"\n    return (idx_col, idx_row) in lengths\n\n\ndef _get_level_lengths(index, hidden_elements=None):\n    \"\"\"\n    Given an index, find the level length for each element.\n    Optional argument is a list of index positions which\n    should not be visible.\n\n    Result is a dictionary of (level, inital_position): span\n    \"\"\"\n    sentinel = com.sentinel_factory()\n    levels = index.format(sparsify=sentinel, adjoin=False, names=False)\n\n    if hidden_elements is None:\n        hidden_elements = []\n\n    lengths = {}\n    if index.nlevels == 1:\n        for i, value in enumerate(levels):\n            if(i not in hidden_elements):\n                lengths[(0, i)] = 1\n        return lengths\n\n    for i, lvl in enumerate(levels):\n        for j, row in enumerate(lvl):\n            if not get_option('display.multi_sparse'):\n                lengths[(i, j)] = 1\n            elif (row != sentinel) and (j not in hidden_elements):\n                last_label = j\n                lengths[(i, last_label)] = 1\n            elif (row != sentinel):\n                # even if its hidden, keep track of it in case\n                # length >1 and later elements are visible\n                last_label = j\n                lengths[(i, last_label)] = 0\n            elif(j not in hidden_elements):\n                lengths[(i, last_label)] += 1\n\n    non_zero_lengths = {}\n    for element, length in lengths.items():\n        if(length >= 1):\n            non_zero_lengths[element] = length\n\n    return non_zero_lengths\n\n\ndef _maybe_wrap_formatter(formatter):\n    if is_string_like(formatter):\n        return lambda x: formatter.format(x)\n    elif callable(formatter):\n        return formatter\n    else:\n        msg = (\"Expected a template string or callable, got {formatter} \"\n               \"instead\".format(formatter=formatter))\n        raise TypeError(msg)\n"
    },
    {
      "filename": "pandas/io/json/normalize.py",
      "content": "# ---------------------------------------------------------------------\n# JSON normalization routines\n\nfrom collections import defaultdict\nimport copy\n\nimport numpy as np\n\nfrom pandas._libs.writers import convert_json_to_lines\n\nfrom pandas import DataFrame, compat\n\n\ndef _convert_to_line_delimits(s):\n    \"\"\"Helper function that converts json lists to line delimited json.\"\"\"\n\n    # Determine we have a JSON list to turn to lines otherwise just return the\n    # json object, only lists can\n    if not s[0] == '[' and s[-1] == ']':\n        return s\n    s = s[1:-1]\n\n    return convert_json_to_lines(s)\n\n\ndef nested_to_record(ds, prefix=\"\", sep=\".\", level=0):\n    \"\"\"a simplified json_normalize\n\n    converts a nested dict into a flat dict (\"record\"), unlike json_normalize,\n    it does not attempt to extract a subset of the data.\n\n    Parameters\n    ----------\n    ds : dict or list of dicts\n    prefix: the prefix, optional, default: \"\"\n    sep : string, default '.'\n        Nested records will generate names separated by sep,\n        e.g., for sep='.', { 'foo' : { 'bar' : 0 } } -> foo.bar\n\n        .. versionadded:: 0.20.0\n\n    level: the number of levels in the jason string, optional, default: 0\n\n    Returns\n    -------\n    d - dict or list of dicts, matching `ds`\n\n    Examples\n    --------\n\n    IN[52]: nested_to_record(dict(flat1=1,dict1=dict(c=1,d=2),\n                                  nested=dict(e=dict(c=1,d=2),d=2)))\n    Out[52]:\n    {'dict1.c': 1,\n     'dict1.d': 2,\n     'flat1': 1,\n     'nested.d': 2,\n     'nested.e.c': 1,\n     'nested.e.d': 2}\n    \"\"\"\n    singleton = False\n    if isinstance(ds, dict):\n        ds = [ds]\n        singleton = True\n\n    new_ds = []\n    for d in ds:\n\n        new_d = copy.deepcopy(d)\n        for k, v in d.items():\n            # each key gets renamed with prefix\n            if not isinstance(k, compat.string_types):\n                k = str(k)\n            if level == 0:\n                newkey = k\n            else:\n                newkey = prefix + sep + k\n\n            # only dicts gets recurse-flattend\n            # only at level>1 do we rename the rest of the keys\n            if not isinstance(v, dict):\n                if level != 0:  # so we skip copying for top level, common case\n                    v = new_d.pop(k)\n                    new_d[newkey] = v\n                continue\n            else:\n                v = new_d.pop(k)\n                new_d.update(nested_to_record(v, newkey, sep, level + 1))\n        new_ds.append(new_d)\n\n    if singleton:\n        return new_ds[0]\n    return new_ds\n\n\ndef json_normalize(data, record_path=None, meta=None,\n                   meta_prefix=None,\n                   record_prefix=None,\n                   errors='raise',\n                   sep='.'):\n    \"\"\"\n    \"Normalize\" semi-structured JSON data into a flat table\n\n    Parameters\n    ----------\n    data : dict or list of dicts\n        Unserialized JSON objects\n    record_path : string or list of strings, default None\n        Path in each object to list of records. If not passed, data will be\n        assumed to be an array of records\n    meta : list of paths (string or list of strings), default None\n        Fields to use as metadata for each record in resulting table\n    meta_prefix : string, default None\n    record_prefix : string, default None\n        If True, prefix records with dotted (?) path, e.g. foo.bar.field if\n        path to records is ['foo', 'bar']\n    errors : {'raise', 'ignore'}, default 'raise'\n\n        * 'ignore' : will ignore KeyError if keys listed in meta are not\n          always present\n        * 'raise' : will raise KeyError if keys listed in meta are not\n          always present\n\n        .. versionadded:: 0.20.0\n\n    sep : string, default '.'\n        Nested records will generate names separated by sep,\n        e.g., for sep='.', { 'foo' : { 'bar' : 0 } } -> foo.bar\n\n        .. versionadded:: 0.20.0\n\n    Returns\n    -------\n    frame : DataFrame\n\n    Examples\n    --------\n\n    >>> from pandas.io.json import json_normalize\n    >>> data = [{'id': 1, 'name': {'first': 'Coleen', 'last': 'Volk'}},\n    ...         {'name': {'given': 'Mose', 'family': 'Regner'}},\n    ...         {'id': 2, 'name': 'Faye Raker'}]\n    >>> json_normalize(data)\n        id        name name.family name.first name.given name.last\n    0  1.0         NaN         NaN     Coleen        NaN      Volk\n    1  NaN         NaN      Regner        NaN       Mose       NaN\n    2  2.0  Faye Raker         NaN        NaN        NaN       NaN\n\n    >>> data = [{'state': 'Florida',\n    ...          'shortname': 'FL',\n    ...          'info': {\n    ...               'governor': 'Rick Scott'\n    ...          },\n    ...          'counties': [{'name': 'Dade', 'population': 12345},\n    ...                      {'name': 'Broward', 'population': 40000},\n    ...                      {'name': 'Palm Beach', 'population': 60000}]},\n    ...         {'state': 'Ohio',\n    ...          'shortname': 'OH',\n    ...          'info': {\n    ...               'governor': 'John Kasich'\n    ...          },\n    ...          'counties': [{'name': 'Summit', 'population': 1234},\n    ...                       {'name': 'Cuyahoga', 'population': 1337}]}]\n    >>> result = json_normalize(data, 'counties', ['state', 'shortname',\n    ...                                           ['info', 'governor']])\n    >>> result\n             name  population info.governor    state shortname\n    0        Dade       12345    Rick Scott  Florida        FL\n    1     Broward       40000    Rick Scott  Florida        FL\n    2  Palm Beach       60000    Rick Scott  Florida        FL\n    3      Summit        1234   John Kasich     Ohio        OH\n    4    Cuyahoga        1337   John Kasich     Ohio        OH\n\n    >>> data = {'A': [1, 2]}\n    >>> json_normalize(data, 'A', record_prefix='Prefix.')\n        Prefix.0\n    0          1\n    1          2\n    \"\"\"\n    def _pull_field(js, spec):\n        result = js\n        if isinstance(spec, list):\n            for field in spec:\n                result = result[field]\n        else:\n            result = result[spec]\n\n        return result\n\n    if isinstance(data, list) and not data:\n        return DataFrame()\n\n    # A bit of a hackjob\n    if isinstance(data, dict):\n        data = [data]\n\n    if record_path is None:\n        if any([isinstance(x, dict)\n                for x in compat.itervalues(y)] for y in data):\n            # naive normalization, this is idempotent for flat records\n            # and potentially will inflate the data considerably for\n            # deeply nested structures:\n            #  {VeryLong: { b: 1,c:2}} -> {VeryLong.b:1 ,VeryLong.c:@}\n            #\n            # TODO: handle record value which are lists, at least error\n            #       reasonably\n            data = nested_to_record(data, sep=sep)\n        return DataFrame(data)\n    elif not isinstance(record_path, list):\n        record_path = [record_path]\n\n    if meta is None:\n        meta = []\n    elif not isinstance(meta, list):\n        meta = [meta]\n\n    meta = [m if isinstance(m, list) else [m] for m in meta]\n\n    # Disastrously inefficient for now\n    records = []\n    lengths = []\n\n    meta_vals = defaultdict(list)\n    if not isinstance(sep, compat.string_types):\n        sep = str(sep)\n    meta_keys = [sep.join(val) for val in meta]\n\n    def _recursive_extract(data, path, seen_meta, level=0):\n        if len(path) > 1:\n            for obj in data:\n                for val, key in zip(meta, meta_keys):\n                    if level + 1 == len(val):\n                        seen_meta[key] = _pull_field(obj, val[-1])\n\n                _recursive_extract(obj[path[0]], path[1:],\n                                   seen_meta, level=level + 1)\n        else:\n            for obj in data:\n                recs = _pull_field(obj, path[0])\n\n                # For repeating the metadata later\n                lengths.append(len(recs))\n\n                for val, key in zip(meta, meta_keys):\n                    if level + 1 > len(val):\n                        meta_val = seen_meta[key]\n                    else:\n                        try:\n                            meta_val = _pull_field(obj, val[level:])\n                        except KeyError as e:\n                            if errors == 'ignore':\n                                meta_val = np.nan\n                            else:\n                                raise KeyError(\"Try running with \"\n                                               \"errors='ignore' as key \"\n                                               \"{err} is not always present\"\n                                               .format(err=e))\n                    meta_vals[key].append(meta_val)\n\n                records.extend(recs)\n\n    _recursive_extract(data, record_path, {}, level=0)\n\n    result = DataFrame(records)\n\n    if record_prefix is not None:\n        result = result.rename(\n            columns=lambda x: \"{p}{c}\".format(p=record_prefix, c=x))\n\n    # Data types, a problem\n    for k, v in compat.iteritems(meta_vals):\n        if meta_prefix is not None:\n            k = meta_prefix + k\n\n        if k in result:\n            raise ValueError('Conflicting metadata name {name}, '\n                             'need distinguishing prefix '.format(name=k))\n\n        result[k] = np.array(v).repeat(lengths)\n\n    return result\n"
    },
    {
      "filename": "pandas/io/packers.py",
      "content": "\"\"\"\nMsgpack serializer support for reading and writing pandas data structures\nto disk\n\nportions of msgpack_numpy package, by Lev Givon were incorporated\ninto this module (and tests_packers.py)\n\nLicense\n=======\n\nCopyright (c) 2013, Lev Givon.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n* Redistributions of source code must retain the above copyright\n  notice, this list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above\n  copyright notice, this list of conditions and the following\n  disclaimer in the documentation and/or other materials provided\n  with the distribution.\n* Neither the name of Lev Givon nor the names of any\n  contributors may be used to endorse or promote products derived\n  from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\"\"\"\n\nfrom datetime import date, datetime, timedelta\nimport os\nfrom textwrap import dedent\nimport warnings\n\nfrom dateutil.parser import parse\nimport numpy as np\n\nimport pandas.compat as compat\nfrom pandas.compat import u, u_safe\nfrom pandas.errors import PerformanceWarning\nfrom pandas.util._move import (\n    BadMove as _BadMove, move_into_mutable_buffer as _move_into_mutable_buffer)\n\nfrom pandas.core.dtypes.common import (\n    is_categorical_dtype, is_object_dtype, needs_i8_conversion, pandas_dtype)\n\nfrom pandas import (  # noqa:F401\n    Categorical, CategoricalIndex, DataFrame, DatetimeIndex, Float64Index,\n    Index, Int64Index, Interval, IntervalIndex, MultiIndex, NaT, Panel, Period,\n    PeriodIndex, RangeIndex, Series, TimedeltaIndex, Timestamp)\nfrom pandas.core import internals\nfrom pandas.core.arrays import IntervalArray, PeriodArray\nfrom pandas.core.arrays.sparse import BlockIndex, IntIndex\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.internals import BlockManager, _safe_reshape, make_block\nfrom pandas.core.sparse.api import SparseDataFrame, SparseSeries\n\nfrom pandas.io.common import _stringify_path, get_filepath_or_buffer\nfrom pandas.io.msgpack import ExtType, Packer as _Packer, Unpacker as _Unpacker\n\n# check which compression libs we have installed\ntry:\n    import zlib\n\n    def _check_zlib():\n        pass\nexcept ImportError:\n    def _check_zlib():\n        raise ImportError('zlib is not installed')\n\n_check_zlib.__doc__ = dedent(\n    \"\"\"\\\n    Check if zlib is installed.\n\n    Raises\n    ------\n    ImportError\n        Raised when zlib is not installed.\n    \"\"\",\n)\n\ntry:\n    import blosc\n\n    def _check_blosc():\n        pass\nexcept ImportError:\n    def _check_blosc():\n        raise ImportError('blosc is not installed')\n\n_check_blosc.__doc__ = dedent(\n    \"\"\"\\\n    Check if blosc is installed.\n\n    Raises\n    ------\n    ImportError\n        Raised when blosc is not installed.\n    \"\"\",\n)\n\n# until we can pass this into our conversion functions,\n# this is pretty hacky\ncompressor = None\n\n\ndef to_msgpack(path_or_buf, *args, **kwargs):\n    \"\"\"\n    msgpack (serialize) object to input file path\n\n    THIS IS AN EXPERIMENTAL LIBRARY and the storage format\n    may not be stable until a future release.\n\n    Parameters\n    ----------\n    path_or_buf : string File path, buffer-like, or None\n                  if None, return generated string\n    args : an object or objects to serialize\n    encoding : encoding for unicode objects\n    append : boolean whether to append to an existing msgpack\n             (default is False)\n    compress : type of compressor (zlib or blosc), default to None (no\n               compression)\n    \"\"\"\n    global compressor\n    compressor = kwargs.pop('compress', None)\n    if compressor:\n        compressor = u(compressor)\n    append = kwargs.pop('append', None)\n    if append:\n        mode = 'a+b'\n    else:\n        mode = 'wb'\n\n    def writer(fh):\n        for a in args:\n            fh.write(pack(a, **kwargs))\n\n    path_or_buf = _stringify_path(path_or_buf)\n    if isinstance(path_or_buf, compat.string_types):\n        with open(path_or_buf, mode) as fh:\n            writer(fh)\n    elif path_or_buf is None:\n        buf = compat.BytesIO()\n        writer(buf)\n        return buf.getvalue()\n    else:\n        writer(path_or_buf)\n\n\ndef read_msgpack(path_or_buf, encoding='utf-8', iterator=False, **kwargs):\n    \"\"\"\n    Load msgpack pandas object from the specified\n    file path\n\n    THIS IS AN EXPERIMENTAL LIBRARY and the storage format\n    may not be stable until a future release.\n\n    Parameters\n    ----------\n    path_or_buf : string File path, BytesIO like or string\n    encoding : Encoding for decoding msgpack str type\n    iterator : boolean, if True, return an iterator to the unpacker\n               (default is False)\n\n    Returns\n    -------\n    obj : same type as object stored in file\n    \"\"\"\n    path_or_buf, _, _, should_close = get_filepath_or_buffer(path_or_buf)\n    if iterator:\n        return Iterator(path_or_buf)\n\n    def read(fh):\n        unpacked_obj = list(unpack(fh, encoding=encoding, **kwargs))\n        if len(unpacked_obj) == 1:\n            return unpacked_obj[0]\n\n        if should_close:\n            try:\n                path_or_buf.close()\n            except IOError:\n                pass\n        return unpacked_obj\n\n    # see if we have an actual file\n    if isinstance(path_or_buf, compat.string_types):\n        try:\n            exists = os.path.exists(path_or_buf)\n        except (TypeError, ValueError):\n            exists = False\n\n        if exists:\n            with open(path_or_buf, 'rb') as fh:\n                return read(fh)\n\n    if isinstance(path_or_buf, compat.binary_type):\n        # treat as a binary-like\n        fh = None\n        try:\n            # We can't distinguish between a path and a buffer of bytes in\n            # Python 2 so instead assume the first byte of a valid path is\n            # less than 0x80.\n            if compat.PY3 or ord(path_or_buf[0]) >= 0x80:\n                fh = compat.BytesIO(path_or_buf)\n                return read(fh)\n        finally:\n            if fh is not None:\n                fh.close()\n    elif hasattr(path_or_buf, 'read') and compat.callable(path_or_buf.read):\n        # treat as a buffer like\n        return read(path_or_buf)\n\n    raise ValueError('path_or_buf needs to be a string file path or file-like')\n\n\ndtype_dict = {21: np.dtype('M8[ns]'),\n              u('datetime64[ns]'): np.dtype('M8[ns]'),\n              u('datetime64[us]'): np.dtype('M8[us]'),\n              22: np.dtype('m8[ns]'),\n              u('timedelta64[ns]'): np.dtype('m8[ns]'),\n              u('timedelta64[us]'): np.dtype('m8[us]'),\n\n              # this is platform int, which we need to remap to np.int64\n              # for compat on windows platforms\n              7: np.dtype('int64'),\n              'category': 'category'\n              }\n\n\ndef dtype_for(t):\n    \"\"\" return my dtype mapping, whether number or name \"\"\"\n    if t in dtype_dict:\n        return dtype_dict[t]\n    return np.typeDict.get(t, t)\n\n\nc2f_dict = {'complex': np.float64,\n            'complex128': np.float64,\n            'complex64': np.float32}\n\n# numpy 1.6.1 compat\nif hasattr(np, 'float128'):\n    c2f_dict['complex256'] = np.float128\n\n\ndef c2f(r, i, ctype_name):\n    \"\"\"\n    Convert strings to complex number instance with specified numpy type.\n    \"\"\"\n\n    ftype = c2f_dict[ctype_name]\n    return np.typeDict[ctype_name](ftype(r) + 1j * ftype(i))\n\n\ndef convert(values):\n    \"\"\" convert the numpy values to a list \"\"\"\n\n    dtype = values.dtype\n\n    if is_categorical_dtype(values):\n        return values\n\n    elif is_object_dtype(dtype):\n        return values.ravel().tolist()\n\n    if needs_i8_conversion(dtype):\n        values = values.view('i8')\n    v = values.ravel()\n\n    if compressor == 'zlib':\n        _check_zlib()\n\n        # return string arrays like they are\n        if dtype == np.object_:\n            return v.tolist()\n\n        # convert to a bytes array\n        v = v.tostring()\n        return ExtType(0, zlib.compress(v))\n\n    elif compressor == 'blosc':\n        _check_blosc()\n\n        # return string arrays like they are\n        if dtype == np.object_:\n            return v.tolist()\n\n        # convert to a bytes array\n        v = v.tostring()\n        return ExtType(0, blosc.compress(v, typesize=dtype.itemsize))\n\n    # ndarray (on original dtype)\n    return ExtType(0, v.tostring())\n\n\ndef unconvert(values, dtype, compress=None):\n\n    as_is_ext = isinstance(values, ExtType) and values.code == 0\n\n    if as_is_ext:\n        values = values.data\n\n    if is_categorical_dtype(dtype):\n        return values\n\n    elif is_object_dtype(dtype):\n        return np.array(values, dtype=object)\n\n    dtype = pandas_dtype(dtype).base\n\n    if not as_is_ext:\n        values = values.encode('latin1')\n\n    if compress:\n        if compress == u'zlib':\n            _check_zlib()\n            decompress = zlib.decompress\n        elif compress == u'blosc':\n            _check_blosc()\n            decompress = blosc.decompress\n        else:\n            raise ValueError(\"compress must be one of 'zlib' or 'blosc'\")\n\n        try:\n            return np.frombuffer(\n                _move_into_mutable_buffer(decompress(values)),\n                dtype=dtype,\n            )\n        except _BadMove as e:\n            # Pull the decompressed data off of the `_BadMove` exception.\n            # We don't just store this in the locals because we want to\n            # minimize the risk of giving users access to a `bytes` object\n            # whose data is also given to a mutable buffer.\n            values = e.args[0]\n            if len(values) > 1:\n                # The empty string and single characters are memoized in many\n                # string creating functions in the capi. This case should not\n                # warn even though we need to make a copy because we are only\n                # copying at most 1 byte.\n                warnings.warn(\n                    'copying data after decompressing; this may mean that'\n                    ' decompress is caching its result',\n                    PerformanceWarning,\n                )\n                # fall through to copying `np.fromstring`\n\n    # Copy the bytes into a numpy array.\n    buf = np.frombuffer(values, dtype=dtype)\n    buf = buf.copy()  # required to not mutate the original data\n    buf.flags.writeable = True\n    return buf\n\n\ndef encode(obj):\n    \"\"\"\n    Data encoder\n    \"\"\"\n    tobj = type(obj)\n    if isinstance(obj, Index):\n        if isinstance(obj, RangeIndex):\n            return {u'typ': u'range_index',\n                    u'klass': u(obj.__class__.__name__),\n                    u'name': getattr(obj, 'name', None),\n                    u'start': getattr(obj, '_start', None),\n                    u'stop': getattr(obj, '_stop', None),\n                    u'step': getattr(obj, '_step', None)}\n        elif isinstance(obj, PeriodIndex):\n            return {u'typ': u'period_index',\n                    u'klass': u(obj.__class__.__name__),\n                    u'name': getattr(obj, 'name', None),\n                    u'freq': u_safe(getattr(obj, 'freqstr', None)),\n                    u'dtype': u(obj.dtype.name),\n                    u'data': convert(obj.asi8),\n                    u'compress': compressor}\n        elif isinstance(obj, DatetimeIndex):\n            tz = getattr(obj, 'tz', None)\n\n            # store tz info and data as UTC\n            if tz is not None:\n                tz = u(tz.zone)\n                obj = obj.tz_convert('UTC')\n            return {u'typ': u'datetime_index',\n                    u'klass': u(obj.__class__.__name__),\n                    u'name': getattr(obj, 'name', None),\n                    u'dtype': u(obj.dtype.name),\n                    u'data': convert(obj.asi8),\n                    u'freq': u_safe(getattr(obj, 'freqstr', None)),\n                    u'tz': tz,\n                    u'compress': compressor}\n        elif isinstance(obj, (IntervalIndex, IntervalArray)):\n            if isinstance(obj, IntervalIndex):\n                typ = u'interval_index'\n            else:\n                typ = u'interval_array'\n            return {u'typ': typ,\n                    u'klass': u(obj.__class__.__name__),\n                    u'name': getattr(obj, 'name', None),\n                    u'left': getattr(obj, 'left', None),\n                    u'right': getattr(obj, 'right', None),\n                    u'closed': getattr(obj, 'closed', None)}\n        elif isinstance(obj, MultiIndex):\n            return {u'typ': u'multi_index',\n                    u'klass': u(obj.__class__.__name__),\n                    u'names': getattr(obj, 'names', None),\n                    u'dtype': u(obj.dtype.name),\n                    u'data': convert(obj.values),\n                    u'compress': compressor}\n        else:\n            return {u'typ': u'index',\n                    u'klass': u(obj.__class__.__name__),\n                    u'name': getattr(obj, 'name', None),\n                    u'dtype': u(obj.dtype.name),\n                    u'data': convert(obj.values),\n                    u'compress': compressor}\n\n    elif isinstance(obj, Categorical):\n        return {u'typ': u'category',\n                u'klass': u(obj.__class__.__name__),\n                u'name': getattr(obj, 'name', None),\n                u'codes': obj.codes,\n                u'categories': obj.categories,\n                u'ordered': obj.ordered,\n                u'compress': compressor}\n\n    elif isinstance(obj, Series):\n        if isinstance(obj, SparseSeries):\n            raise NotImplementedError(\n                'msgpack sparse series is not implemented'\n            )\n            # d = {'typ': 'sparse_series',\n            #     'klass': obj.__class__.__name__,\n            #     'dtype': obj.dtype.name,\n            #     'index': obj.index,\n            #     'sp_index': obj.sp_index,\n            #     'sp_values': convert(obj.sp_values),\n            #     'compress': compressor}\n            # for f in ['name', 'fill_value', 'kind']:\n            #    d[f] = getattr(obj, f, None)\n            # return d\n        else:\n            return {u'typ': u'series',\n                    u'klass': u(obj.__class__.__name__),\n                    u'name': getattr(obj, 'name', None),\n                    u'index': obj.index,\n                    u'dtype': u(obj.dtype.name),\n                    u'data': convert(obj.values),\n                    u'compress': compressor}\n    elif issubclass(tobj, NDFrame):\n        if isinstance(obj, SparseDataFrame):\n            raise NotImplementedError(\n                'msgpack sparse frame is not implemented'\n            )\n            # d = {'typ': 'sparse_dataframe',\n            #     'klass': obj.__class__.__name__,\n            #     'columns': obj.columns}\n            # for f in ['default_fill_value', 'default_kind']:\n            #    d[f] = getattr(obj, f, None)\n            # d['data'] = dict([(name, ss)\n            #                 for name, ss in compat.iteritems(obj)])\n            # return d\n        else:\n\n            data = obj._data\n            if not data.is_consolidated():\n                data = data.consolidate()\n\n            # the block manager\n            return {u'typ': u'block_manager',\n                    u'klass': u(obj.__class__.__name__),\n                    u'axes': data.axes,\n                    u'blocks': [{u'locs': b.mgr_locs.as_array,\n                                 u'values': convert(b.values),\n                                 u'shape': b.values.shape,\n                                 u'dtype': u(b.dtype.name),\n                                 u'klass': u(b.__class__.__name__),\n                                 u'compress': compressor} for b in data.blocks]\n                    }\n\n    elif isinstance(obj, (datetime, date, np.datetime64, timedelta,\n                          np.timedelta64)) or obj is NaT:\n        if isinstance(obj, Timestamp):\n            tz = obj.tzinfo\n            if tz is not None:\n                tz = u(tz.zone)\n            freq = obj.freq\n            if freq is not None:\n                freq = u(freq.freqstr)\n            return {u'typ': u'timestamp',\n                    u'value': obj.value,\n                    u'freq': freq,\n                    u'tz': tz}\n        if obj is NaT:\n            return {u'typ': u'nat'}\n        elif isinstance(obj, np.timedelta64):\n            return {u'typ': u'timedelta64',\n                    u'data': obj.view('i8')}\n        elif isinstance(obj, timedelta):\n            return {u'typ': u'timedelta',\n                    u'data': (obj.days, obj.seconds, obj.microseconds)}\n        elif isinstance(obj, np.datetime64):\n            return {u'typ': u'datetime64',\n                    u'data': u(str(obj))}\n        elif isinstance(obj, datetime):\n            return {u'typ': u'datetime',\n                    u'data': u(obj.isoformat())}\n        elif isinstance(obj, date):\n            return {u'typ': u'date',\n                    u'data': u(obj.isoformat())}\n        raise Exception(\"cannot encode this datetimelike object: %s\" % obj)\n    elif isinstance(obj, Period):\n        return {u'typ': u'period',\n                u'ordinal': obj.ordinal,\n                u'freq': u_safe(obj.freqstr)}\n    elif isinstance(obj, Interval):\n        return {u'typ': u'interval',\n                u'left': obj.left,\n                u'right': obj.right,\n                u'closed': obj.closed}\n    elif isinstance(obj, BlockIndex):\n        return {u'typ': u'block_index',\n                u'klass': u(obj.__class__.__name__),\n                u'blocs': obj.blocs,\n                u'blengths': obj.blengths,\n                u'length': obj.length}\n    elif isinstance(obj, IntIndex):\n        return {u'typ': u'int_index',\n                u'klass': u(obj.__class__.__name__),\n                u'indices': obj.indices,\n                u'length': obj.length}\n    elif isinstance(obj, np.ndarray):\n        return {u'typ': u'ndarray',\n                u'shape': obj.shape,\n                u'ndim': obj.ndim,\n                u'dtype': u(obj.dtype.name),\n                u'data': convert(obj),\n                u'compress': compressor}\n    elif isinstance(obj, np.number):\n        if np.iscomplexobj(obj):\n            return {u'typ': u'np_scalar',\n                    u'sub_typ': u'np_complex',\n                    u'dtype': u(obj.dtype.name),\n                    u'real': u(obj.real.__repr__()),\n                    u'imag': u(obj.imag.__repr__())}\n        else:\n            return {u'typ': u'np_scalar',\n                    u'dtype': u(obj.dtype.name),\n                    u'data': u(obj.__repr__())}\n    elif isinstance(obj, complex):\n        return {u'typ': u'np_complex',\n                u'real': u(obj.real.__repr__()),\n                u'imag': u(obj.imag.__repr__())}\n\n    return obj\n\n\ndef decode(obj):\n    \"\"\"\n    Decoder for deserializing numpy data types.\n    \"\"\"\n\n    typ = obj.get(u'typ')\n    if typ is None:\n        return obj\n    elif typ == u'timestamp':\n        freq = obj[u'freq'] if 'freq' in obj else obj[u'offset']\n        return Timestamp(obj[u'value'], tz=obj[u'tz'], freq=freq)\n    elif typ == u'nat':\n        return NaT\n    elif typ == u'period':\n        return Period(ordinal=obj[u'ordinal'], freq=obj[u'freq'])\n    elif typ == u'index':\n        dtype = dtype_for(obj[u'dtype'])\n        data = unconvert(obj[u'data'], dtype,\n                         obj.get(u'compress'))\n        return globals()[obj[u'klass']](data, dtype=dtype, name=obj[u'name'])\n    elif typ == u'range_index':\n        return globals()[obj[u'klass']](obj[u'start'],\n                                        obj[u'stop'],\n                                        obj[u'step'],\n                                        name=obj[u'name'])\n    elif typ == u'multi_index':\n        dtype = dtype_for(obj[u'dtype'])\n        data = unconvert(obj[u'data'], dtype,\n                         obj.get(u'compress'))\n        data = [tuple(x) for x in data]\n        return globals()[obj[u'klass']].from_tuples(data, names=obj[u'names'])\n    elif typ == u'period_index':\n        data = unconvert(obj[u'data'], np.int64, obj.get(u'compress'))\n        d = dict(name=obj[u'name'], freq=obj[u'freq'])\n        freq = d.pop('freq', None)\n        return globals()[obj[u'klass']](PeriodArray(data, freq), **d)\n\n    elif typ == u'datetime_index':\n        data = unconvert(obj[u'data'], np.int64, obj.get(u'compress'))\n        d = dict(name=obj[u'name'], freq=obj[u'freq'], verify_integrity=False)\n        result = globals()[obj[u'klass']](data, **d)\n        tz = obj[u'tz']\n\n        # reverse tz conversion\n        if tz is not None:\n            result = result.tz_localize('UTC').tz_convert(tz)\n        return result\n\n    elif typ in (u'interval_index', 'interval_array'):\n        return globals()[obj[u'klass']].from_arrays(obj[u'left'],\n                                                    obj[u'right'],\n                                                    obj[u'closed'],\n                                                    name=obj[u'name'])\n    elif typ == u'category':\n        from_codes = globals()[obj[u'klass']].from_codes\n        return from_codes(codes=obj[u'codes'],\n                          categories=obj[u'categories'],\n                          ordered=obj[u'ordered'])\n\n    elif typ == u'interval':\n        return Interval(obj[u'left'], obj[u'right'], obj[u'closed'])\n    elif typ == u'series':\n        dtype = dtype_for(obj[u'dtype'])\n        pd_dtype = pandas_dtype(dtype)\n\n        index = obj[u'index']\n        result = globals()[obj[u'klass']](unconvert(obj[u'data'], dtype,\n                                                    obj[u'compress']),\n                                          index=index,\n                                          dtype=pd_dtype,\n                                          name=obj[u'name'])\n        return result\n\n    elif typ == u'block_manager':\n        axes = obj[u'axes']\n\n        def create_block(b):\n            values = _safe_reshape(unconvert(\n                b[u'values'], dtype_for(b[u'dtype']),\n                b[u'compress']), b[u'shape'])\n\n            # locs handles duplicate column names, and should be used instead\n            # of items; see GH 9618\n            if u'locs' in b:\n                placement = b[u'locs']\n            else:\n                placement = axes[0].get_indexer(b[u'items'])\n            return make_block(values=values,\n                              klass=getattr(internals, b[u'klass']),\n                              placement=placement,\n                              dtype=b[u'dtype'])\n\n        blocks = [create_block(b) for b in obj[u'blocks']]\n        return globals()[obj[u'klass']](BlockManager(blocks, axes))\n    elif typ == u'datetime':\n        return parse(obj[u'data'])\n    elif typ == u'datetime64':\n        return np.datetime64(parse(obj[u'data']))\n    elif typ == u'date':\n        return parse(obj[u'data']).date()\n    elif typ == u'timedelta':\n        return timedelta(*obj[u'data'])\n    elif typ == u'timedelta64':\n        return np.timedelta64(int(obj[u'data']))\n    # elif typ == 'sparse_series':\n    #    dtype = dtype_for(obj['dtype'])\n    #    return globals()[obj['klass']](\n    #        unconvert(obj['sp_values'], dtype, obj['compress']),\n    #        sparse_index=obj['sp_index'], index=obj['index'],\n    #        fill_value=obj['fill_value'], kind=obj['kind'], name=obj['name'])\n    # elif typ == 'sparse_dataframe':\n    #    return globals()[obj['klass']](\n    #        obj['data'], columns=obj['columns'],\n    #        default_fill_value=obj['default_fill_value'],\n    #        default_kind=obj['default_kind']\n    #    )\n    # elif typ == 'sparse_panel':\n    #    return globals()[obj['klass']](\n    #        obj['data'], items=obj['items'],\n    #        default_fill_value=obj['default_fill_value'],\n    #        default_kind=obj['default_kind'])\n    elif typ == u'block_index':\n        return globals()[obj[u'klass']](obj[u'length'], obj[u'blocs'],\n                                        obj[u'blengths'])\n    elif typ == u'int_index':\n        return globals()[obj[u'klass']](obj[u'length'], obj[u'indices'])\n    elif typ == u'ndarray':\n        return unconvert(obj[u'data'], np.typeDict[obj[u'dtype']],\n                         obj.get(u'compress')).reshape(obj[u'shape'])\n    elif typ == u'np_scalar':\n        if obj.get(u'sub_typ') == u'np_complex':\n            return c2f(obj[u'real'], obj[u'imag'], obj[u'dtype'])\n        else:\n            dtype = dtype_for(obj[u'dtype'])\n            try:\n                return dtype(obj[u'data'])\n            except (ValueError, TypeError):\n                return dtype.type(obj[u'data'])\n    elif typ == u'np_complex':\n        return complex(obj[u'real'] + u'+' + obj[u'imag'] + u'j')\n    elif isinstance(obj, (dict, list, set)):\n        return obj\n    else:\n        return obj\n\n\ndef pack(o, default=encode,\n         encoding='utf-8', unicode_errors='strict', use_single_float=False,\n         autoreset=1, use_bin_type=1):\n    \"\"\"\n    Pack an object and return the packed bytes.\n    \"\"\"\n\n    return Packer(default=default, encoding=encoding,\n                  unicode_errors=unicode_errors,\n                  use_single_float=use_single_float,\n                  autoreset=autoreset,\n                  use_bin_type=use_bin_type).pack(o)\n\n\ndef unpack(packed, object_hook=decode,\n           list_hook=None, use_list=False, encoding='utf-8',\n           unicode_errors='strict', object_pairs_hook=None,\n           max_buffer_size=0, ext_hook=ExtType):\n    \"\"\"\n    Unpack a packed object, return an iterator\n    Note: packed lists will be returned as tuples\n    \"\"\"\n\n    return Unpacker(packed, object_hook=object_hook,\n                    list_hook=list_hook,\n                    use_list=use_list, encoding=encoding,\n                    unicode_errors=unicode_errors,\n                    object_pairs_hook=object_pairs_hook,\n                    max_buffer_size=max_buffer_size,\n                    ext_hook=ext_hook)\n\n\nclass Packer(_Packer):\n\n    def __init__(self, default=encode,\n                 encoding='utf-8',\n                 unicode_errors='strict',\n                 use_single_float=False,\n                 autoreset=1,\n                 use_bin_type=1):\n        super(Packer, self).__init__(default=default,\n                                     encoding=encoding,\n                                     unicode_errors=unicode_errors,\n                                     use_single_float=use_single_float,\n                                     autoreset=autoreset,\n                                     use_bin_type=use_bin_type)\n\n\nclass Unpacker(_Unpacker):\n\n    def __init__(self, file_like=None, read_size=0, use_list=False,\n                 object_hook=decode,\n                 object_pairs_hook=None, list_hook=None, encoding='utf-8',\n                 unicode_errors='strict', max_buffer_size=0, ext_hook=ExtType):\n        super(Unpacker, self).__init__(file_like=file_like,\n                                       read_size=read_size,\n                                       use_list=use_list,\n                                       object_hook=object_hook,\n                                       object_pairs_hook=object_pairs_hook,\n                                       list_hook=list_hook,\n                                       encoding=encoding,\n                                       unicode_errors=unicode_errors,\n                                       max_buffer_size=max_buffer_size,\n                                       ext_hook=ext_hook)\n\n\nclass Iterator(object):\n\n    \"\"\" manage the unpacking iteration,\n        close the file on completion \"\"\"\n\n    def __init__(self, path, **kwargs):\n        self.path = path\n        self.kwargs = kwargs\n\n    def __iter__(self):\n\n        needs_closing = True\n        try:\n\n            # see if we have an actual file\n            if isinstance(self.path, compat.string_types):\n\n                try:\n                    path_exists = os.path.exists(self.path)\n                except TypeError:\n                    path_exists = False\n\n                if path_exists:\n                    fh = open(self.path, 'rb')\n                else:\n                    fh = compat.BytesIO(self.path)\n\n            else:\n\n                if not hasattr(self.path, 'read'):\n                    fh = compat.BytesIO(self.path)\n\n                else:\n\n                    # a file-like\n                    needs_closing = False\n                    fh = self.path\n\n            unpacker = unpack(fh)\n            for o in unpacker:\n                yield o\n        finally:\n            if needs_closing:\n                fh.close()\n"
    },
    {
      "filename": "pandas/io/parquet.py",
      "content": "\"\"\" parquet compat \"\"\"\n\nfrom distutils.version import LooseVersion\nfrom warnings import catch_warnings\n\nfrom pandas.compat import string_types\nfrom pandas.errors import AbstractMethodError\n\nfrom pandas import DataFrame, get_option\n\nfrom pandas.io.common import get_filepath_or_buffer, is_s3_url\n\n\ndef get_engine(engine):\n    \"\"\" return our implementation \"\"\"\n\n    if engine == 'auto':\n        engine = get_option('io.parquet.engine')\n\n    if engine == 'auto':\n        # try engines in this order\n        try:\n            return PyArrowImpl()\n        except ImportError:\n            pass\n\n        try:\n            return FastParquetImpl()\n        except ImportError:\n            pass\n\n        raise ImportError(\"Unable to find a usable engine; \"\n                          \"tried using: 'pyarrow', 'fastparquet'.\\n\"\n                          \"pyarrow or fastparquet is required for parquet \"\n                          \"support\")\n\n    if engine not in ['pyarrow', 'fastparquet']:\n        raise ValueError(\"engine must be one of 'pyarrow', 'fastparquet'\")\n\n    if engine == 'pyarrow':\n        return PyArrowImpl()\n    elif engine == 'fastparquet':\n        return FastParquetImpl()\n\n\nclass BaseImpl(object):\n\n    api = None  # module\n\n    @staticmethod\n    def validate_dataframe(df):\n\n        if not isinstance(df, DataFrame):\n            raise ValueError(\"to_parquet only supports IO with DataFrames\")\n\n        # must have value column names (strings only)\n        if df.columns.inferred_type not in {'string', 'unicode'}:\n            raise ValueError(\"parquet must have string column names\")\n\n        # index level names must be strings\n        valid_names = all(\n            isinstance(name, string_types)\n            for name in df.index.names\n            if name is not None\n        )\n        if not valid_names:\n            raise ValueError(\"Index level names must be strings\")\n\n    def write(self, df, path, compression, **kwargs):\n        raise AbstractMethodError(self)\n\n    def read(self, path, columns=None, **kwargs):\n        raise AbstractMethodError(self)\n\n\nclass PyArrowImpl(BaseImpl):\n\n    def __init__(self):\n        # since pandas is a dependency of pyarrow\n        # we need to import on first use\n        try:\n            import pyarrow\n            import pyarrow.parquet\n        except ImportError:\n            raise ImportError(\n                \"pyarrow is required for parquet support\\n\\n\"\n                \"you can install via conda\\n\"\n                \"conda install pyarrow -c conda-forge\\n\"\n                \"\\nor via pip\\n\"\n                \"pip install -U pyarrow\\n\"\n            )\n        if LooseVersion(pyarrow.__version__) < '0.7.0':\n            raise ImportError(\n                \"pyarrow >= 0.7.0 is required for parquet support\\n\\n\"\n                \"you can install via conda\\n\"\n                \"conda install pyarrow -c conda-forge\\n\"\n                \"\\nor via pip\\n\"\n                \"pip install -U pyarrow\\n\"\n            )\n\n        self.api = pyarrow\n\n    def write(self, df, path, compression='snappy',\n              coerce_timestamps='ms', index=None, partition_cols=None,\n              **kwargs):\n        self.validate_dataframe(df)\n        path, _, _, _ = get_filepath_or_buffer(path, mode='wb')\n\n        if index is None:\n            from_pandas_kwargs = {}\n        else:\n            from_pandas_kwargs = {'preserve_index': index}\n        table = self.api.Table.from_pandas(df, **from_pandas_kwargs)\n        if partition_cols is not None:\n            self.api.parquet.write_to_dataset(\n                table, path, compression=compression,\n                coerce_timestamps=coerce_timestamps,\n                partition_cols=partition_cols, **kwargs)\n        else:\n            self.api.parquet.write_table(\n                table, path, compression=compression,\n                coerce_timestamps=coerce_timestamps, **kwargs)\n\n    def read(self, path, columns=None, **kwargs):\n        path, _, _, should_close = get_filepath_or_buffer(path)\n\n        kwargs['use_pandas_metadata'] = True\n        result = self.api.parquet.read_table(path, columns=columns,\n                                             **kwargs).to_pandas()\n        if should_close:\n            try:\n                path.close()\n            except:  # noqa: flake8\n                pass\n\n        return result\n\n\nclass FastParquetImpl(BaseImpl):\n\n    def __init__(self):\n        # since pandas is a dependency of fastparquet\n        # we need to import on first use\n        try:\n            import fastparquet\n        except ImportError:\n            raise ImportError(\n                \"fastparquet is required for parquet support\\n\\n\"\n                \"you can install via conda\\n\"\n                \"conda install fastparquet -c conda-forge\\n\"\n                \"\\nor via pip\\n\"\n                \"pip install -U fastparquet\"\n            )\n        if LooseVersion(fastparquet.__version__) < '0.1.2':\n            raise ImportError(\n                \"fastparquet >= 0.1.2 is required for parquet \"\n                \"support\\n\\n\"\n                \"you can install via conda\\n\"\n                \"conda install fastparquet -c conda-forge\\n\"\n                \"\\nor via pip\\n\"\n                \"pip install -U fastparquet\"\n            )\n        self.api = fastparquet\n\n    def write(self, df, path, compression='snappy', index=None,\n              partition_cols=None, **kwargs):\n        self.validate_dataframe(df)\n        # thriftpy/protocol/compact.py:339:\n        # DeprecationWarning: tostring() is deprecated.\n        # Use tobytes() instead.\n\n        if 'partition_on' in kwargs and partition_cols is not None:\n            raise ValueError(\"Cannot use both partition_on and \"\n                             \"partition_cols. Use partition_cols for \"\n                             \"partitioning data\")\n        elif 'partition_on' in kwargs:\n            partition_cols = kwargs.pop('partition_on')\n\n        if partition_cols is not None:\n            kwargs['file_scheme'] = 'hive'\n\n        if is_s3_url(path):\n            # path is s3:// so we need to open the s3file in 'wb' mode.\n            # TODO: Support 'ab'\n\n            path, _, _, _ = get_filepath_or_buffer(path, mode='wb')\n            # And pass the opened s3file to the fastparquet internal impl.\n            kwargs['open_with'] = lambda path, _: path\n        else:\n            path, _, _, _ = get_filepath_or_buffer(path)\n\n        with catch_warnings(record=True):\n            self.api.write(path, df, compression=compression,\n                           write_index=index, partition_on=partition_cols,\n                           **kwargs)\n\n    def read(self, path, columns=None, **kwargs):\n        if is_s3_url(path):\n            # When path is s3:// an S3File is returned.\n            # We need to retain the original path(str) while also\n            # pass the S3File().open function to fsatparquet impl.\n            s3, _, _, should_close = get_filepath_or_buffer(path)\n            try:\n                parquet_file = self.api.ParquetFile(path, open_with=s3.s3.open)\n            finally:\n                s3.close()\n        else:\n            path, _, _, _ = get_filepath_or_buffer(path)\n            parquet_file = self.api.ParquetFile(path)\n\n        return parquet_file.to_pandas(columns=columns, **kwargs)\n\n\ndef to_parquet(df, path, engine='auto', compression='snappy', index=None,\n               partition_cols=None, **kwargs):\n    \"\"\"\n    Write a DataFrame to the parquet format.\n\n    Parameters\n    ----------\n    path : str\n        File path or Root Directory path. Will be used as Root Directory path\n        while writing a partitioned dataset.\n\n        .. versionchanged:: 0.24.0\n\n    engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n        Parquet library to use. If 'auto', then the option\n        ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n        behavior is to try 'pyarrow', falling back to 'fastparquet' if\n        'pyarrow' is unavailable.\n    compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'\n        Name of the compression to use. Use ``None`` for no compression.\n    index : bool, default None\n        If ``True``, include the dataframe's index(es) in the file output. If\n        ``False``, they will not be written to the file. If ``None``, the\n        engine's default behavior will be used.\n\n        .. versionadded 0.24.0\n\n    partition_cols : list, optional, default None\n        Column names by which to partition the dataset\n        Columns are partitioned in the order they are given\n\n        .. versionadded:: 0.24.0\n\n    kwargs\n        Additional keyword arguments passed to the engine\n    \"\"\"\n    impl = get_engine(engine)\n    return impl.write(df, path, compression=compression, index=index,\n                      partition_cols=partition_cols, **kwargs)\n\n\ndef read_parquet(path, engine='auto', columns=None, **kwargs):\n    \"\"\"\n    Load a parquet object from the file path, returning a DataFrame.\n\n    .. versionadded 0.21.0\n\n    Parameters\n    ----------\n    path : string\n        File path\n    columns : list, default=None\n        If not None, only these columns will be read from the file.\n\n        .. versionadded 0.21.1\n    engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n        Parquet library to use. If 'auto', then the option\n        ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n        behavior is to try 'pyarrow', falling back to 'fastparquet' if\n        'pyarrow' is unavailable.\n    kwargs are passed to the engine\n\n    Returns\n    -------\n    DataFrame\n    \"\"\"\n\n    impl = get_engine(engine)\n    return impl.read(path, columns=columns, **kwargs)\n"
    },
    {
      "filename": "pandas/io/pytables.py",
      "content": "# pylint: disable-msg=E1101,W0613,W0603\n\"\"\"\nHigh level interface to PyTables for reading and writing pandas data structures\nto disk\n\"\"\"\n\nimport copy\nfrom datetime import date, datetime\nfrom distutils.version import LooseVersion\nimport itertools\nimport os\nimport re\nimport time\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import algos, lib, writers as libwriters\nfrom pandas._libs.tslibs import timezones\nfrom pandas.compat import PY3, filter, lrange, range, string_types\nfrom pandas.errors import PerformanceWarning\n\nfrom pandas.core.dtypes.common import (\n    ensure_int64, ensure_object, ensure_platform_int, is_categorical_dtype,\n    is_datetime64_dtype, is_datetime64tz_dtype, is_list_like,\n    is_timedelta64_dtype)\nfrom pandas.core.dtypes.missing import array_equivalent\n\nfrom pandas import (\n    DataFrame, DatetimeIndex, Index, Int64Index, MultiIndex, Panel,\n    PeriodIndex, Series, SparseDataFrame, SparseSeries, TimedeltaIndex, compat,\n    concat, isna, to_datetime)\nfrom pandas.core import config\nfrom pandas.core.algorithms import match, unique\nfrom pandas.core.arrays.categorical import (\n    Categorical, _factorize_from_iterables)\nfrom pandas.core.arrays.sparse import BlockIndex, IntIndex\nfrom pandas.core.base import StringMixin\nimport pandas.core.common as com\nfrom pandas.core.computation.pytables import Expr, maybe_expression\nfrom pandas.core.config import get_option\nfrom pandas.core.index import ensure_index\nfrom pandas.core.internals import (\n    BlockManager, _block2d_to_blocknd, _block_shape, _factor_indexer,\n    make_block)\n\nfrom pandas.io.common import _stringify_path\nfrom pandas.io.formats.printing import adjoin, pprint_thing\n\n# versioning attribute\n_version = '0.15.2'\n\n# encoding\n# PY3 encoding if we don't specify\n_default_encoding = 'UTF-8'\n\n\ndef _ensure_decoded(s):\n    \"\"\" if we have bytes, decode them to unicode \"\"\"\n    if isinstance(s, np.bytes_):\n        s = s.decode('UTF-8')\n    return s\n\n\ndef _ensure_encoding(encoding):\n    # set the encoding if we need\n    if encoding is None:\n        if PY3:\n            encoding = _default_encoding\n    return encoding\n\n\ndef _ensure_str(name):\n    \"\"\"Ensure that an index / column name is a str (python 3) or\n    unicode (python 2); otherwise they may be np.string dtype.\n    Non-string dtypes are passed through unchanged.\n\n    https://github.com/pandas-dev/pandas/issues/13492\n    \"\"\"\n    if isinstance(name, compat.string_types):\n        name = compat.text_type(name)\n    return name\n\n\nTerm = Expr\n\n\ndef _ensure_term(where, scope_level):\n    \"\"\"\n    ensure that the where is a Term or a list of Term\n    this makes sure that we are capturing the scope of variables\n    that are passed\n    create the terms here with a frame_level=2 (we are 2 levels down)\n    \"\"\"\n\n    # only consider list/tuple here as an ndarray is automatically a coordinate\n    # list\n    level = scope_level + 1\n    if isinstance(where, (list, tuple)):\n        wlist = []\n        for w in filter(lambda x: x is not None, where):\n            if not maybe_expression(w):\n                wlist.append(w)\n            else:\n                wlist.append(Term(w, scope_level=level))\n        where = wlist\n    elif maybe_expression(where):\n        where = Term(where, scope_level=level)\n    return where\n\n\nclass PossibleDataLossError(Exception):\n    pass\n\n\nclass ClosedFileError(Exception):\n    pass\n\n\nclass IncompatibilityWarning(Warning):\n    pass\n\n\nincompatibility_doc = \"\"\"\nwhere criteria is being ignored as this version [%s] is too old (or\nnot-defined), read the file in and write it out to a new file to upgrade (with\nthe copy_to method)\n\"\"\"\n\n\nclass AttributeConflictWarning(Warning):\n    pass\n\n\nattribute_conflict_doc = \"\"\"\nthe [%s] attribute of the existing index is [%s] which conflicts with the new\n[%s], resetting the attribute to None\n\"\"\"\n\n\nclass DuplicateWarning(Warning):\n    pass\n\n\nduplicate_doc = \"\"\"\nduplicate entries in table, taking most recently appended\n\"\"\"\n\nperformance_doc = \"\"\"\nyour performance may suffer as PyTables will pickle object types that it cannot\nmap directly to c-types [inferred_type->%s,key->%s] [items->%s]\n\"\"\"\n\n# formats\n_FORMAT_MAP = {\n    u'f': 'fixed',\n    u'fixed': 'fixed',\n    u't': 'table',\n    u'table': 'table',\n}\n\nformat_deprecate_doc = \"\"\"\nthe table keyword has been deprecated\nuse the format='fixed(f)|table(t)' keyword instead\n  fixed(f) : specifies the Fixed format\n             and is the default for put operations\n  table(t) : specifies the Table format\n             and is the default for append operations\n\"\"\"\n\n# map object types\n_TYPE_MAP = {\n\n    Series: u'series',\n    SparseSeries: u'sparse_series',\n    DataFrame: u'frame',\n    SparseDataFrame: u'sparse_frame',\n    Panel: u'wide',\n}\n\n# storer class map\n_STORER_MAP = {\n    u'Series': 'LegacySeriesFixed',\n    u'DataFrame': 'LegacyFrameFixed',\n    u'DataMatrix': 'LegacyFrameFixed',\n    u'series': 'SeriesFixed',\n    u'sparse_series': 'SparseSeriesFixed',\n    u'frame': 'FrameFixed',\n    u'sparse_frame': 'SparseFrameFixed',\n    u'wide': 'PanelFixed',\n}\n\n# table class map\n_TABLE_MAP = {\n    u'generic_table': 'GenericTable',\n    u'appendable_series': 'AppendableSeriesTable',\n    u'appendable_multiseries': 'AppendableMultiSeriesTable',\n    u'appendable_frame': 'AppendableFrameTable',\n    u'appendable_multiframe': 'AppendableMultiFrameTable',\n    u'appendable_panel': 'AppendablePanelTable',\n    u'worm': 'WORMTable',\n    u'legacy_frame': 'LegacyFrameTable',\n    u'legacy_panel': 'LegacyPanelTable',\n}\n\n# axes map\n_AXES_MAP = {\n    DataFrame: [0],\n    Panel: [1, 2]\n}\n\n# register our configuration options\ndropna_doc = \"\"\"\n: boolean\n    drop ALL nan rows when appending to a table\n\"\"\"\nformat_doc = \"\"\"\n: format\n    default format writing format, if None, then\n    put will default to 'fixed' and append will default to 'table'\n\"\"\"\n\nwith config.config_prefix('io.hdf'):\n    config.register_option('dropna_table', False, dropna_doc,\n                           validator=config.is_bool)\n    config.register_option(\n        'default_format', None, format_doc,\n        validator=config.is_one_of_factory(['fixed', 'table', None])\n    )\n\n# oh the troubles to reduce import time\n_table_mod = None\n_table_file_open_policy_is_strict = False\n\n\ndef _tables():\n    global _table_mod\n    global _table_file_open_policy_is_strict\n    if _table_mod is None:\n        import tables\n        _table_mod = tables\n\n        # version requirements\n        if LooseVersion(tables.__version__) < LooseVersion('3.0.0'):\n            raise ImportError(\"PyTables version >= 3.0.0 is required\")\n\n        # set the file open policy\n        # return the file open policy; this changes as of pytables 3.1\n        # depending on the HDF5 version\n        try:\n            _table_file_open_policy_is_strict = (\n                tables.file._FILE_OPEN_POLICY == 'strict')\n        except AttributeError:\n            pass\n\n    return _table_mod\n\n# interface to/from ###\n\n\ndef to_hdf(path_or_buf, key, value, mode=None, complevel=None, complib=None,\n           append=None, **kwargs):\n    \"\"\" store this object, close it if we opened it \"\"\"\n\n    if append:\n        f = lambda store: store.append(key, value, **kwargs)\n    else:\n        f = lambda store: store.put(key, value, **kwargs)\n\n    path_or_buf = _stringify_path(path_or_buf)\n    if isinstance(path_or_buf, string_types):\n        with HDFStore(path_or_buf, mode=mode, complevel=complevel,\n                      complib=complib) as store:\n            f(store)\n    else:\n        f(path_or_buf)\n\n\ndef read_hdf(path_or_buf, key=None, mode='r', **kwargs):\n    \"\"\"\n    Read from the store, close it if we opened it.\n\n    Retrieve pandas object stored in file, optionally based on where\n    criteria\n\n    Parameters\n    ----------\n    path_or_buf : string, buffer or path object\n        Path to the file to open, or an open :class:`pandas.HDFStore` object.\n        Supports any object implementing the ``__fspath__`` protocol.\n        This includes :class:`pathlib.Path` and py._path.local.LocalPath\n        objects.\n\n        .. versionadded:: 0.19.0 support for pathlib, py.path.\n        .. versionadded:: 0.21.0 support for __fspath__ protocol.\n\n    key : object, optional\n        The group identifier in the store. Can be omitted if the HDF file\n        contains a single pandas object.\n    mode : {'r', 'r+', 'a'}, optional\n        Mode to use when opening the file. Ignored if path_or_buf is a\n        :class:`pandas.HDFStore`. Default is 'r'.\n    where : list, optional\n        A list of Term (or convertible) objects.\n    start : int, optional\n        Row number to start selection.\n    stop  : int, optional\n        Row number to stop selection.\n    columns : list, optional\n        A list of columns names to return.\n    iterator : bool, optional\n        Return an iterator object.\n    chunksize : int, optional\n        Number of rows to include in an iteration when using an iterator.\n    errors : str, default 'strict'\n        Specifies how encoding and decoding errors are to be handled.\n        See the errors argument for :func:`open` for a full list\n        of options.\n    **kwargs\n        Additional keyword arguments passed to HDFStore.\n\n    Returns\n    -------\n    item : object\n        The selected object. Return type depends on the object stored.\n\n    See Also\n    --------\n    pandas.DataFrame.to_hdf : Write a HDF file from a DataFrame.\n    pandas.HDFStore : Low-level access to HDF files.\n\n    Examples\n    --------\n    >>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])\n    >>> df.to_hdf('./store.h5', 'data')\n    >>> reread = pd.read_hdf('./store.h5')\n    \"\"\"\n\n    if mode not in ['r', 'r+', 'a']:\n        raise ValueError('mode {0} is not allowed while performing a read. '\n                         'Allowed modes are r, r+ and a.'.format(mode))\n    # grab the scope\n    if 'where' in kwargs:\n        kwargs['where'] = _ensure_term(kwargs['where'], scope_level=1)\n\n    if isinstance(path_or_buf, HDFStore):\n        if not path_or_buf.is_open:\n            raise IOError('The HDFStore must be open for reading.')\n\n        store = path_or_buf\n        auto_close = False\n    else:\n        path_or_buf = _stringify_path(path_or_buf)\n        if not isinstance(path_or_buf, string_types):\n            raise NotImplementedError('Support for generic buffers has not '\n                                      'been implemented.')\n        try:\n            exists = os.path.exists(path_or_buf)\n\n        # if filepath is too long\n        except (TypeError, ValueError):\n            exists = False\n\n        if not exists:\n            raise compat.FileNotFoundError(\n                'File %s does not exist' % path_or_buf)\n\n        store = HDFStore(path_or_buf, mode=mode, **kwargs)\n        # can't auto open/close if we are using an iterator\n        # so delegate to the iterator\n        auto_close = True\n\n    try:\n        if key is None:\n            groups = store.groups()\n            if len(groups) == 0:\n                raise ValueError('No dataset in HDF5 file.')\n            candidate_only_group = groups[0]\n\n            # For the HDF file to have only one dataset, all other groups\n            # should then be metadata groups for that candidate group. (This\n            # assumes that the groups() method enumerates parent groups\n            # before their children.)\n            for group_to_check in groups[1:]:\n                if not _is_metadata_of(group_to_check, candidate_only_group):\n                    raise ValueError('key must be provided when HDF5 file '\n                                     'contains multiple datasets.')\n            key = candidate_only_group._v_pathname\n        return store.select(key, auto_close=auto_close, **kwargs)\n    except (ValueError, TypeError):\n        # if there is an error, close the store\n        try:\n            store.close()\n        except AttributeError:\n            pass\n\n        raise\n\n\ndef _is_metadata_of(group, parent_group):\n    \"\"\"Check if a given group is a metadata group for a given parent_group.\"\"\"\n    if group._v_depth <= parent_group._v_depth:\n        return False\n\n    current = group\n    while current._v_depth > 1:\n        parent = current._v_parent\n        if parent == parent_group and current._v_name == 'meta':\n            return True\n        current = current._v_parent\n    return False\n\n\nclass HDFStore(StringMixin):\n\n    \"\"\"\n    dict-like IO interface for storing pandas objects in PyTables\n    either Fixed or Table format.\n\n    Parameters\n    ----------\n    path : string\n        File path to HDF5 file\n    mode : {'a', 'w', 'r', 'r+'}, default 'a'\n\n        ``'r'``\n            Read-only; no data can be modified.\n        ``'w'``\n            Write; a new file is created (an existing file with the same\n            name would be deleted).\n        ``'a'``\n            Append; an existing file is opened for reading and writing,\n            and if the file does not exist it is created.\n        ``'r+'``\n            It is similar to ``'a'``, but the file must already exist.\n    complevel : int, 0-9, default None\n            Specifies a compression level for data.\n            A value of 0 disables compression.\n    complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib'\n            Specifies the compression library to be used.\n            As of v0.20.2 these additional compressors for Blosc are supported\n            (default if no compressor specified: 'blosc:blosclz'):\n            {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy',\n             'blosc:zlib', 'blosc:zstd'}.\n            Specifying a compression library which is not available issues\n            a ValueError.\n    fletcher32 : bool, default False\n            If applying compression use the fletcher32 checksum\n\n    Examples\n    --------\n    >>> bar = pd.DataFrame(np.random.randn(10, 4))\n    >>> store = pd.HDFStore('test.h5')\n    >>> store['foo'] = bar   # write to HDF5\n    >>> bar = store['foo']   # retrieve\n    >>> store.close()\n    \"\"\"\n\n    def __init__(self, path, mode=None, complevel=None, complib=None,\n                 fletcher32=False, **kwargs):\n        try:\n            import tables  # noqa\n        except ImportError as ex:  # pragma: no cover\n            raise ImportError('HDFStore requires PyTables, \"{ex}\" problem '\n                              'importing'.format(ex=str(ex)))\n\n        if complib is not None and complib not in tables.filters.all_complibs:\n            raise ValueError(\n                \"complib only supports {libs} compression.\".format(\n                    libs=tables.filters.all_complibs))\n\n        if complib is None and complevel is not None:\n            complib = tables.filters.default_complib\n\n        self._path = _stringify_path(path)\n        if mode is None:\n            mode = 'a'\n        self._mode = mode\n        self._handle = None\n        self._complevel = complevel if complevel else 0\n        self._complib = complib\n        self._fletcher32 = fletcher32\n        self._filters = None\n        self.open(mode=mode, **kwargs)\n\n    def __fspath__(self):\n        return self._path\n\n    @property\n    def root(self):\n        \"\"\" return the root node \"\"\"\n        self._check_if_open()\n        return self._handle.root\n\n    @property\n    def filename(self):\n        return self._path\n\n    def __getitem__(self, key):\n        return self.get(key)\n\n    def __setitem__(self, key, value):\n        self.put(key, value)\n\n    def __delitem__(self, key):\n        return self.remove(key)\n\n    def __getattr__(self, name):\n        \"\"\" allow attribute access to get stores \"\"\"\n        try:\n            return self.get(name)\n        except (KeyError, ClosedFileError):\n            pass\n        raise AttributeError(\"'%s' object has no attribute '%s'\" %\n                             (type(self).__name__, name))\n\n    def __contains__(self, key):\n        \"\"\" check for existence of this key\n              can match the exact pathname or the pathnm w/o the leading '/'\n              \"\"\"\n        node = self.get_node(key)\n        if node is not None:\n            name = node._v_pathname\n            if name == key or name[1:] == key:\n                return True\n        return False\n\n    def __len__(self):\n        return len(self.groups())\n\n    def __unicode__(self):\n        return '%s\\nFile path: %s\\n' % (type(self), pprint_thing(self._path))\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def keys(self):\n        \"\"\"\n        Return a (potentially unordered) list of the keys corresponding to the\n        objects stored in the HDFStore. These are ABSOLUTE path-names (e.g.\n        have the leading '/'\n        \"\"\"\n        return [n._v_pathname for n in self.groups()]\n\n    def __iter__(self):\n        return iter(self.keys())\n\n    def items(self):\n        \"\"\"\n        iterate on key->group\n        \"\"\"\n        for g in self.groups():\n            yield g._v_pathname, g\n\n    iteritems = items\n\n    def open(self, mode='a', **kwargs):\n        \"\"\"\n        Open the file in the specified mode\n\n        Parameters\n        ----------\n        mode : {'a', 'w', 'r', 'r+'}, default 'a'\n            See HDFStore docstring or tables.open_file for info about modes\n        \"\"\"\n        tables = _tables()\n\n        if self._mode != mode:\n\n            # if we are changing a write mode to read, ok\n            if self._mode in ['a', 'w'] and mode in ['r', 'r+']:\n                pass\n            elif mode in ['w']:\n\n                # this would truncate, raise here\n                if self.is_open:\n                    raise PossibleDataLossError(\n                        \"Re-opening the file [{0}] with mode [{1}] \"\n                        \"will delete the current file!\"\n                        .format(self._path, self._mode)\n                    )\n\n            self._mode = mode\n\n        # close and reopen the handle\n        if self.is_open:\n            self.close()\n\n        if self._complevel and self._complevel > 0:\n            self._filters = _tables().Filters(self._complevel, self._complib,\n                                              fletcher32=self._fletcher32)\n\n        try:\n            self._handle = tables.open_file(self._path, self._mode, **kwargs)\n        except (IOError) as e:  # pragma: no cover\n            if 'can not be written' in str(e):\n                print('Opening %s in read-only mode' % self._path)\n                self._handle = tables.open_file(self._path, 'r', **kwargs)\n            else:\n                raise\n\n        except (ValueError) as e:\n\n            # trap PyTables >= 3.1 FILE_OPEN_POLICY exception\n            # to provide an updated message\n            if 'FILE_OPEN_POLICY' in str(e):\n                e = ValueError(\n                    \"PyTables [{version}] no longer supports opening multiple \"\n                    \"files\\n\"\n                    \"even in read-only mode on this HDF5 version \"\n                    \"[{hdf_version}]. You can accept this\\n\"\n                    \"and not open the same file multiple times at once,\\n\"\n                    \"upgrade the HDF5 version, or downgrade to PyTables 3.0.0 \"\n                    \"which allows\\n\"\n                    \"files to be opened multiple times at once\\n\"\n                    .format(version=tables.__version__,\n                            hdf_version=tables.get_hdf5_version()))\n\n            raise e\n\n        except (Exception) as e:\n\n            # trying to read from a non-existent file causes an error which\n            # is not part of IOError, make it one\n            if self._mode == 'r' and 'Unable to open/create file' in str(e):\n                raise IOError(str(e))\n            raise\n\n    def close(self):\n        \"\"\"\n        Close the PyTables file handle\n        \"\"\"\n        if self._handle is not None:\n            self._handle.close()\n        self._handle = None\n\n    @property\n    def is_open(self):\n        \"\"\"\n        return a boolean indicating whether the file is open\n        \"\"\"\n        if self._handle is None:\n            return False\n        return bool(self._handle.isopen)\n\n    def flush(self, fsync=False):\n        \"\"\"\n        Force all buffered modifications to be written to disk.\n\n        Parameters\n        ----------\n        fsync : bool (default False)\n          call ``os.fsync()`` on the file handle to force writing to disk.\n\n        Notes\n        -----\n        Without ``fsync=True``, flushing may not guarantee that the OS writes\n        to disk. With fsync, the operation will block until the OS claims the\n        file has been written; however, other caching layers may still\n        interfere.\n        \"\"\"\n        if self._handle is not None:\n            self._handle.flush()\n            if fsync:\n                try:\n                    os.fsync(self._handle.fileno())\n                except OSError:\n                    pass\n\n    def get(self, key):\n        \"\"\"\n        Retrieve pandas object stored in file\n\n        Parameters\n        ----------\n        key : object\n\n        Returns\n        -------\n        obj : same type as object stored in file\n        \"\"\"\n        group = self.get_node(key)\n        if group is None:\n            raise KeyError('No object named %s in the file' % key)\n        return self._read_group(group)\n\n    def select(self, key, where=None, start=None, stop=None, columns=None,\n               iterator=False, chunksize=None, auto_close=False, **kwargs):\n        \"\"\"\n        Retrieve pandas object stored in file, optionally based on where\n        criteria\n\n        Parameters\n        ----------\n        key : object\n        where : list of Term (or convertible) objects, optional\n        start : integer (defaults to None), row number to start selection\n        stop  : integer (defaults to None), row number to stop selection\n        columns : a list of columns that if not None, will limit the return\n            columns\n        iterator : boolean, return an iterator, default False\n        chunksize : nrows to include in iteration, return an iterator\n        auto_close : boolean, should automatically close the store when\n            finished, default is False\n\n        Returns\n        -------\n        The selected object\n        \"\"\"\n        group = self.get_node(key)\n        if group is None:\n            raise KeyError('No object named %s in the file' % key)\n\n        # create the storer and axes\n        where = _ensure_term(where, scope_level=1)\n        s = self._create_storer(group)\n        s.infer_axes()\n\n        # function to call on iteration\n        def func(_start, _stop, _where):\n            return s.read(start=_start, stop=_stop,\n                          where=_where,\n                          columns=columns)\n\n        # create the iterator\n        it = TableIterator(self, s, func, where=where, nrows=s.nrows,\n                           start=start, stop=stop, iterator=iterator,\n                           chunksize=chunksize, auto_close=auto_close)\n\n        return it.get_result()\n\n    def select_as_coordinates(\n            self, key, where=None, start=None, stop=None, **kwargs):\n        \"\"\"\n        return the selection as an Index\n\n        Parameters\n        ----------\n        key : object\n        where : list of Term (or convertible) objects, optional\n        start : integer (defaults to None), row number to start selection\n        stop  : integer (defaults to None), row number to stop selection\n        \"\"\"\n        where = _ensure_term(where, scope_level=1)\n        return self.get_storer(key).read_coordinates(where=where, start=start,\n                                                     stop=stop, **kwargs)\n\n    def select_column(self, key, column, **kwargs):\n        \"\"\"\n        return a single column from the table. This is generally only useful to\n        select an indexable\n\n        Parameters\n        ----------\n        key : object\n        column: the column of interest\n\n        Exceptions\n        ----------\n        raises KeyError if the column is not found (or key is not a valid\n            store)\n        raises ValueError if the column can not be extracted individually (it\n            is part of a data block)\n\n        \"\"\"\n        return self.get_storer(key).read_column(column=column, **kwargs)\n\n    def select_as_multiple(self, keys, where=None, selector=None, columns=None,\n                           start=None, stop=None, iterator=False,\n                           chunksize=None, auto_close=False, **kwargs):\n        \"\"\" Retrieve pandas objects from multiple tables\n\n        Parameters\n        ----------\n        keys : a list of the tables\n        selector : the table to apply the where criteria (defaults to keys[0]\n            if not supplied)\n        columns : the columns I want back\n        start : integer (defaults to None), row number to start selection\n        stop  : integer (defaults to None), row number to stop selection\n        iterator : boolean, return an iterator, default False\n        chunksize : nrows to include in iteration, return an iterator\n\n        Exceptions\n        ----------\n        raises KeyError if keys or selector is not found or keys is empty\n        raises TypeError if keys is not a list or tuple\n        raises ValueError if the tables are not ALL THE SAME DIMENSIONS\n        \"\"\"\n\n        # default to single select\n        where = _ensure_term(where, scope_level=1)\n        if isinstance(keys, (list, tuple)) and len(keys) == 1:\n            keys = keys[0]\n        if isinstance(keys, string_types):\n            return self.select(key=keys, where=where, columns=columns,\n                               start=start, stop=stop, iterator=iterator,\n                               chunksize=chunksize, **kwargs)\n\n        if not isinstance(keys, (list, tuple)):\n            raise TypeError(\"keys must be a list/tuple\")\n\n        if not len(keys):\n            raise ValueError(\"keys must have a non-zero length\")\n\n        if selector is None:\n            selector = keys[0]\n\n        # collect the tables\n        tbls = [self.get_storer(k) for k in keys]\n        s = self.get_storer(selector)\n\n        # validate rows\n        nrows = None\n        for t, k in itertools.chain([(s, selector)], zip(tbls, keys)):\n            if t is None:\n                raise KeyError(\"Invalid table [%s]\" % k)\n            if not t.is_table:\n                raise TypeError(\n                    \"object [%s] is not a table, and cannot be used in all \"\n                    \"select as multiple\" % t.pathname\n                )\n\n            if nrows is None:\n                nrows = t.nrows\n            elif t.nrows != nrows:\n                raise ValueError(\n                    \"all tables must have exactly the same nrows!\")\n\n        # axis is the concentation axes\n        axis = list({t.non_index_axes[0][0] for t in tbls})[0]\n\n        def func(_start, _stop, _where):\n\n            # retrieve the objs, _where is always passed as a set of\n            # coordinates here\n            objs = [t.read(where=_where, columns=columns, start=_start,\n                           stop=_stop, **kwargs) for t in tbls]\n\n            # concat and return\n            return concat(objs, axis=axis,\n                          verify_integrity=False)._consolidate()\n\n        # create the iterator\n        it = TableIterator(self, s, func, where=where, nrows=nrows,\n                           start=start, stop=stop, iterator=iterator,\n                           chunksize=chunksize, auto_close=auto_close)\n\n        return it.get_result(coordinates=True)\n\n    def put(self, key, value, format=None, append=False, **kwargs):\n        \"\"\"\n        Store object in HDFStore\n\n        Parameters\n        ----------\n        key      : object\n        value    : {Series, DataFrame, Panel}\n        format   : 'fixed(f)|table(t)', default is 'fixed'\n            fixed(f) : Fixed format\n                       Fast writing/reading. Not-appendable, nor searchable\n            table(t) : Table format\n                       Write as a PyTables Table structure which may perform\n                       worse but allow more flexible operations like searching\n                       / selecting subsets of the data\n        append   : boolean, default False\n            This will force Table format, append the input data to the\n            existing.\n        data_columns : list of columns to create as data columns, or True to\n            use all columns. See\n            `here <http://pandas.pydata.org/pandas-docs/stable/io.html#query-via-data-columns>`__ # noqa\n        encoding : default None, provide an encoding for strings\n        dropna   : boolean, default False, do not write an ALL nan row to\n            the store settable by the option 'io.hdf.dropna_table'\n        \"\"\"\n        if format is None:\n            format = get_option(\"io.hdf.default_format\") or 'fixed'\n        kwargs = self._validate_format(format, kwargs)\n        self._write_to_group(key, value, append=append, **kwargs)\n\n    def remove(self, key, where=None, start=None, stop=None):\n        \"\"\"\n        Remove pandas object partially by specifying the where condition\n\n        Parameters\n        ----------\n        key : string\n            Node to remove or delete rows from\n        where : list of Term (or convertible) objects, optional\n        start : integer (defaults to None), row number to start selection\n        stop  : integer (defaults to None), row number to stop selection\n\n        Returns\n        -------\n        number of rows removed (or None if not a Table)\n\n        Exceptions\n        ----------\n        raises KeyError if key is not a valid store\n\n        \"\"\"\n        where = _ensure_term(where, scope_level=1)\n        try:\n            s = self.get_storer(key)\n        except KeyError:\n            # the key is not a valid store, re-raising KeyError\n            raise\n        except Exception:\n\n            if where is not None:\n                raise ValueError(\n                    \"trying to remove a node with a non-None where clause!\")\n\n            # we are actually trying to remove a node (with children)\n            s = self.get_node(key)\n            if s is not None:\n                s._f_remove(recursive=True)\n                return None\n\n        # remove the node\n        if com._all_none(where, start, stop):\n            s.group._f_remove(recursive=True)\n\n        # delete from the table\n        else:\n            if not s.is_table:\n                raise ValueError(\n                    'can only remove with where on objects written as tables')\n            return s.delete(where=where, start=start, stop=stop)\n\n    def append(self, key, value, format=None, append=True, columns=None,\n               dropna=None, **kwargs):\n        \"\"\"\n        Append to Table in file. Node must already exist and be Table\n        format.\n\n        Parameters\n        ----------\n        key : object\n        value : {Series, DataFrame, Panel}\n        format : 'table' is the default\n            table(t) : table format\n                       Write as a PyTables Table structure which may perform\n                       worse but allow more flexible operations like searching\n                       / selecting subsets of the data\n        append       : boolean, default True, append the input data to the\n            existing\n        data_columns :  list of columns, or True, default None\n            List of columns to create as indexed data columns for on-disk\n            queries, or True to use all columns. By default only the axes\n            of the object are indexed. See `here\n            <http://pandas.pydata.org/pandas-docs/stable/io.html#query-via-data-columns>`__.\n        min_itemsize : dict of columns that specify minimum string sizes\n        nan_rep      : string to use as string nan represenation\n        chunksize    : size to chunk the writing\n        expectedrows : expected TOTAL row size of this table\n        encoding     : default None, provide an encoding for strings\n        dropna       : boolean, default False, do not write an ALL nan row to\n            the store settable by the option 'io.hdf.dropna_table'\n\n        Notes\n        -----\n        Does *not* check if data being appended overlaps with existing\n        data in the table, so be careful\n        \"\"\"\n        if columns is not None:\n            raise TypeError(\"columns is not a supported keyword in append, \"\n                            \"try data_columns\")\n\n        if dropna is None:\n            dropna = get_option(\"io.hdf.dropna_table\")\n        if format is None:\n            format = get_option(\"io.hdf.default_format\") or 'table'\n        kwargs = self._validate_format(format, kwargs)\n        self._write_to_group(key, value, append=append, dropna=dropna,\n                             **kwargs)\n\n    def append_to_multiple(self, d, value, selector, data_columns=None,\n                           axes=None, dropna=False, **kwargs):\n        \"\"\"\n        Append to multiple tables\n\n        Parameters\n        ----------\n        d : a dict of table_name to table_columns, None is acceptable as the\n            values of one node (this will get all the remaining columns)\n        value : a pandas object\n        selector : a string that designates the indexable table; all of its\n            columns will be designed as data_columns, unless data_columns is\n            passed, in which case these are used\n        data_columns : list of columns to create as data columns, or True to\n            use all columns\n        dropna : if evaluates to True, drop rows from all tables if any single\n                 row in each table has all NaN. Default False.\n\n        Notes\n        -----\n        axes parameter is currently not accepted\n\n        \"\"\"\n        if axes is not None:\n            raise TypeError(\"axes is currently not accepted as a parameter to\"\n                            \" append_to_multiple; you can create the \"\n                            \"tables independently instead\")\n\n        if not isinstance(d, dict):\n            raise ValueError(\n                \"append_to_multiple must have a dictionary specified as the \"\n                \"way to split the value\"\n            )\n\n        if selector not in d:\n            raise ValueError(\n                \"append_to_multiple requires a selector that is in passed dict\"\n            )\n\n        # figure out the splitting axis (the non_index_axis)\n        axis = list(set(range(value.ndim)) - set(_AXES_MAP[type(value)]))[0]\n\n        # figure out how to split the value\n        remain_key = None\n        remain_values = []\n        for k, v in d.items():\n            if v is None:\n                if remain_key is not None:\n                    raise ValueError(\n                        \"append_to_multiple can only have one value in d that \"\n                        \"is None\"\n                    )\n                remain_key = k\n            else:\n                remain_values.extend(v)\n        if remain_key is not None:\n            ordered = value.axes[axis]\n            ordd = ordered.difference(Index(remain_values))\n            ordd = sorted(ordered.get_indexer(ordd))\n            d[remain_key] = ordered.take(ordd)\n\n        # data_columns\n        if data_columns is None:\n            data_columns = d[selector]\n\n        # ensure rows are synchronized across the tables\n        if dropna:\n            idxs = (value[cols].dropna(how='all').index for cols in d.values())\n            valid_index = next(idxs)\n            for index in idxs:\n                valid_index = valid_index.intersection(index)\n            value = value.loc[valid_index]\n\n        # append\n        for k, v in d.items():\n            dc = data_columns if k == selector else None\n\n            # compute the val\n            val = value.reindex(v, axis=axis)\n\n            self.append(k, val, data_columns=dc, **kwargs)\n\n    def create_table_index(self, key, **kwargs):\n        \"\"\" Create a pytables index on the table\n        Parameters\n        ----------\n        key : object (the node to index)\n\n        Exceptions\n        ----------\n        raises if the node is not a table\n\n        \"\"\"\n\n        # version requirements\n        _tables()\n        s = self.get_storer(key)\n        if s is None:\n            return\n\n        if not s.is_table:\n            raise TypeError(\n                \"cannot create table index on a Fixed format store\")\n        s.create_index(**kwargs)\n\n    def groups(self):\n        \"\"\"return a list of all the top-level nodes (that are not themselves a\n        pandas storage object)\n        \"\"\"\n        _tables()\n        self._check_if_open()\n        return [\n            g for g in self._handle.walk_groups()\n            if (not isinstance(g, _table_mod.link.Link) and\n                (getattr(g._v_attrs, 'pandas_type', None) or\n                 getattr(g, 'table', None) or\n                (isinstance(g, _table_mod.table.Table) and\n                 g._v_name != u'table')))\n        ]\n\n    def walk(self, where=\"/\"):\n        \"\"\" Walk the pytables group hierarchy for pandas objects\n\n        This generator will yield the group path, subgroups and pandas object\n        names for each group.\n        Any non-pandas PyTables objects that are not a group will be ignored.\n\n        The `where` group itself is listed first (preorder), then each of its\n        child groups (following an alphanumerical order) is also traversed,\n        following the same procedure.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        where : str, optional\n            Group where to start walking.\n            If not supplied, the root group is used.\n\n        Yields\n        ------\n        path : str\n            Full path to a group (without trailing '/')\n        groups : list of str\n            names of the groups contained in `path`\n        leaves : list of str\n            names of the pandas objects contained in `path`\n        \"\"\"\n        _tables()\n        self._check_if_open()\n        for g in self._handle.walk_groups(where):\n            if getattr(g._v_attrs, 'pandas_type', None) is not None:\n                continue\n\n            groups = []\n            leaves = []\n            for child in g._v_children.values():\n                pandas_type = getattr(child._v_attrs, 'pandas_type', None)\n                if pandas_type is None:\n                    if isinstance(child, _table_mod.group.Group):\n                        groups.append(child._v_name)\n                else:\n                    leaves.append(child._v_name)\n\n            yield (g._v_pathname.rstrip('/'), groups, leaves)\n\n    def get_node(self, key):\n        \"\"\" return the node with the key or None if it does not exist \"\"\"\n        self._check_if_open()\n        try:\n            if not key.startswith('/'):\n                key = '/' + key\n            return self._handle.get_node(self.root, key)\n        except _table_mod.exceptions.NoSuchNodeError:\n            return None\n\n    def get_storer(self, key):\n        \"\"\" return the storer object for a key, raise if not in the file \"\"\"\n        group = self.get_node(key)\n        if group is None:\n            raise KeyError('No object named {} in the file'.format(key))\n\n        s = self._create_storer(group)\n        s.infer_axes()\n        return s\n\n    def copy(self, file, mode='w', propindexes=True, keys=None, complib=None,\n             complevel=None, fletcher32=False, overwrite=True):\n        \"\"\" copy the existing store to a new file, upgrading in place\n\n            Parameters\n            ----------\n            propindexes: restore indexes in copied file (defaults to True)\n            keys       : list of keys to include in the copy (defaults to all)\n            overwrite  : overwrite (remove and replace) existing nodes in the\n                new store (default is True)\n            mode, complib, complevel, fletcher32 same as in HDFStore.__init__\n\n            Returns\n            -------\n            open file handle of the new store\n\n        \"\"\"\n        new_store = HDFStore(\n            file,\n            mode=mode,\n            complib=complib,\n            complevel=complevel,\n            fletcher32=fletcher32)\n        if keys is None:\n            keys = list(self.keys())\n        if not isinstance(keys, (tuple, list)):\n            keys = [keys]\n        for k in keys:\n            s = self.get_storer(k)\n            if s is not None:\n\n                if k in new_store:\n                    if overwrite:\n                        new_store.remove(k)\n\n                data = self.select(k)\n                if s.is_table:\n\n                    index = False\n                    if propindexes:\n                        index = [a.name for a in s.axes if a.is_indexed]\n                    new_store.append(\n                        k, data, index=index,\n                        data_columns=getattr(s, 'data_columns', None),\n                        encoding=s.encoding\n                    )\n                else:\n                    new_store.put(k, data, encoding=s.encoding)\n\n        return new_store\n\n    def info(self):\n        \"\"\"\n        print detailed information on the store\n\n        .. versionadded:: 0.21.0\n        \"\"\"\n        output = '%s\\nFile path: %s\\n' % (type(self), pprint_thing(self._path))\n        if self.is_open:\n            lkeys = sorted(list(self.keys()))\n            if len(lkeys):\n                keys = []\n                values = []\n\n                for k in lkeys:\n                    try:\n                        s = self.get_storer(k)\n                        if s is not None:\n                            keys.append(pprint_thing(s.pathname or k))\n                            values.append(\n                                pprint_thing(s or 'invalid_HDFStore node'))\n                    except Exception as detail:\n                        keys.append(k)\n                        values.append(\"[invalid_HDFStore node: %s]\"\n                                      % pprint_thing(detail))\n\n                output += adjoin(12, keys, values)\n            else:\n                output += 'Empty'\n        else:\n            output += \"File is CLOSED\"\n\n        return output\n\n    # private methods ######\n    def _check_if_open(self):\n        if not self.is_open:\n            raise ClosedFileError(\"{0} file is not open!\".format(self._path))\n\n    def _validate_format(self, format, kwargs):\n        \"\"\" validate / deprecate formats; return the new kwargs \"\"\"\n        kwargs = kwargs.copy()\n\n        # validate\n        try:\n            kwargs['format'] = _FORMAT_MAP[format.lower()]\n        except KeyError:\n            raise TypeError(\"invalid HDFStore format specified [{0}]\"\n                            .format(format))\n\n        return kwargs\n\n    def _create_storer(self, group, format=None, value=None, append=False,\n                       **kwargs):\n        \"\"\" return a suitable class to operate \"\"\"\n\n        def error(t):\n            raise TypeError(\n                \"cannot properly create the storer for: [%s] [group->%s,\"\n                \"value->%s,format->%s,append->%s,kwargs->%s]\"\n                % (t, group, type(value), format, append, kwargs)\n            )\n\n        pt = _ensure_decoded(getattr(group._v_attrs, 'pandas_type', None))\n        tt = _ensure_decoded(getattr(group._v_attrs, 'table_type', None))\n\n        # infer the pt from the passed value\n        if pt is None:\n            if value is None:\n\n                _tables()\n                if (getattr(group, 'table', None) or\n                        isinstance(group, _table_mod.table.Table)):\n                    pt = u'frame_table'\n                    tt = u'generic_table'\n                else:\n                    raise TypeError(\n                        \"cannot create a storer if the object is not existing \"\n                        \"nor a value are passed\")\n            else:\n\n                try:\n                    pt = _TYPE_MAP[type(value)]\n                except KeyError:\n                    error('_TYPE_MAP')\n\n                # we are actually a table\n                if format == 'table':\n                    pt += u'_table'\n\n        # a storer node\n        if u'table' not in pt:\n            try:\n                return globals()[_STORER_MAP[pt]](self, group, **kwargs)\n            except KeyError:\n                error('_STORER_MAP')\n\n        # existing node (and must be a table)\n        if tt is None:\n\n            # if we are a writer, determine the tt\n            if value is not None:\n\n                if pt == u'series_table':\n                    index = getattr(value, 'index', None)\n                    if index is not None:\n                        if index.nlevels == 1:\n                            tt = u'appendable_series'\n                        elif index.nlevels > 1:\n                            tt = u'appendable_multiseries'\n                elif pt == u'frame_table':\n                    index = getattr(value, 'index', None)\n                    if index is not None:\n                        if index.nlevels == 1:\n                            tt = u'appendable_frame'\n                        elif index.nlevels > 1:\n                            tt = u'appendable_multiframe'\n                elif pt == u'wide_table':\n                    tt = u'appendable_panel'\n                elif pt == u'ndim_table':\n                    tt = u'appendable_ndim'\n\n            else:\n\n                # distiguish between a frame/table\n                tt = u'legacy_panel'\n                try:\n                    fields = group.table._v_attrs.fields\n                    if len(fields) == 1 and fields[0] == u'value':\n                        tt = u'legacy_frame'\n                except IndexError:\n                    pass\n\n        try:\n            return globals()[_TABLE_MAP[tt]](self, group, **kwargs)\n        except KeyError:\n            error('_TABLE_MAP')\n\n    def _write_to_group(self, key, value, format, index=True, append=False,\n                        complib=None, encoding=None, **kwargs):\n        group = self.get_node(key)\n\n        # remove the node if we are not appending\n        if group is not None and not append:\n            self._handle.remove_node(group, recursive=True)\n            group = None\n\n        # we don't want to store a table node at all if are object is 0-len\n        # as there are not dtypes\n        if getattr(value, 'empty', None) and (format == 'table' or append):\n            return\n\n        if group is None:\n            paths = key.split('/')\n\n            # recursively create the groups\n            path = '/'\n            for p in paths:\n                if not len(p):\n                    continue\n                new_path = path\n                if not path.endswith('/'):\n                    new_path += '/'\n                new_path += p\n                group = self.get_node(new_path)\n                if group is None:\n                    group = self._handle.create_group(path, p)\n                path = new_path\n\n        s = self._create_storer(group, format, value, append=append,\n                                encoding=encoding, **kwargs)\n        if append:\n            # raise if we are trying to append to a Fixed format,\n            #       or a table that exists (and we are putting)\n            if (not s.is_table or\n                    (s.is_table and format == 'fixed' and s.is_exists)):\n                raise ValueError('Can only append to Tables')\n            if not s.is_exists:\n                s.set_object_info()\n        else:\n            s.set_object_info()\n\n        if not s.is_table and complib:\n            raise ValueError(\n                'Compression not supported on Fixed format stores'\n            )\n\n        # write the object\n        s.write(obj=value, append=append, complib=complib, **kwargs)\n\n        if s.is_table and index:\n            s.create_index(columns=index)\n\n    def _read_group(self, group, **kwargs):\n        s = self._create_storer(group)\n        s.infer_axes()\n        return s.read(**kwargs)\n\n\nclass TableIterator(object):\n\n    \"\"\" define the iteration interface on a table\n\n        Parameters\n        ----------\n\n        store : the reference store\n        s     : the referred storer\n        func  : the function to execute the query\n        where : the where of the query\n        nrows : the rows to iterate on\n        start : the passed start value (default is None)\n        stop  : the passed stop value (default is None)\n        iterator : boolean, whether to use the default iterator\n        chunksize : the passed chunking value (default is 50000)\n        auto_close : boolean, automatically close the store at the end of\n            iteration, default is False\n        kwargs : the passed kwargs\n        \"\"\"\n\n    def __init__(self, store, s, func, where, nrows, start=None, stop=None,\n                 iterator=False, chunksize=None, auto_close=False):\n        self.store = store\n        self.s = s\n        self.func = func\n        self.where = where\n\n        # set start/stop if they are not set if we are a table\n        if self.s.is_table:\n            if nrows is None:\n                nrows = 0\n            if start is None:\n                start = 0\n            if stop is None:\n                stop = nrows\n            stop = min(nrows, stop)\n\n        self.nrows = nrows\n        self.start = start\n        self.stop = stop\n\n        self.coordinates = None\n        if iterator or chunksize is not None:\n            if chunksize is None:\n                chunksize = 100000\n            self.chunksize = int(chunksize)\n        else:\n            self.chunksize = None\n\n        self.auto_close = auto_close\n\n    def __iter__(self):\n\n        # iterate\n        current = self.start\n        while current < self.stop:\n\n            stop = min(current + self.chunksize, self.stop)\n            value = self.func(None, None, self.coordinates[current:stop])\n            current = stop\n            if value is None or not len(value):\n                continue\n\n            yield value\n\n        self.close()\n\n    def close(self):\n        if self.auto_close:\n            self.store.close()\n\n    def get_result(self, coordinates=False):\n\n        #  return the actual iterator\n        if self.chunksize is not None:\n            if not self.s.is_table:\n                raise TypeError(\n                    \"can only use an iterator or chunksize on a table\")\n\n            self.coordinates = self.s.read_coordinates(where=self.where)\n\n            return self\n\n        # if specified read via coordinates (necessary for multiple selections\n        if coordinates:\n            where = self.s.read_coordinates(where=self.where, start=self.start,\n                                            stop=self.stop)\n        else:\n            where = self.where\n\n        # directly return the result\n        results = self.func(self.start, self.stop, where)\n        self.close()\n        return results\n\n\nclass IndexCol(StringMixin):\n\n    \"\"\" an index column description class\n\n        Parameters\n        ----------\n\n        axis   : axis which I reference\n        values : the ndarray like converted values\n        kind   : a string description of this type\n        typ    : the pytables type\n        pos    : the position in the pytables\n\n        \"\"\"\n    is_an_indexable = True\n    is_data_indexable = True\n    _info_fields = ['freq', 'tz', 'index_name']\n\n    def __init__(self, values=None, kind=None, typ=None, cname=None,\n                 itemsize=None, name=None, axis=None, kind_attr=None,\n                 pos=None, freq=None, tz=None, index_name=None, **kwargs):\n        self.values = values\n        self.kind = kind\n        self.typ = typ\n        self.itemsize = itemsize\n        self.name = name\n        self.cname = cname\n        self.kind_attr = kind_attr\n        self.axis = axis\n        self.pos = pos\n        self.freq = freq\n        self.tz = tz\n        self.index_name = index_name\n        self.table = None\n        self.meta = None\n        self.metadata = None\n\n        if name is not None:\n            self.set_name(name, kind_attr)\n        if pos is not None:\n            self.set_pos(pos)\n\n    def set_name(self, name, kind_attr=None):\n        \"\"\" set the name of this indexer \"\"\"\n        self.name = name\n        self.kind_attr = kind_attr or \"%s_kind\" % name\n        if self.cname is None:\n            self.cname = name\n\n        return self\n\n    def set_axis(self, axis):\n        \"\"\" set the axis over which I index \"\"\"\n        self.axis = axis\n\n        return self\n\n    def set_pos(self, pos):\n        \"\"\" set the position of this column in the Table \"\"\"\n        self.pos = pos\n        if pos is not None and self.typ is not None:\n            self.typ._v_pos = pos\n        return self\n\n    def set_table(self, table):\n        self.table = table\n        return self\n\n    def __unicode__(self):\n        temp = tuple(\n            map(pprint_thing,\n                    (self.name,\n                     self.cname,\n                     self.axis,\n                     self.pos,\n                     self.kind)))\n        return \"name->%s,cname->%s,axis->%s,pos->%s,kind->%s\" % temp\n\n    def __eq__(self, other):\n        \"\"\" compare 2 col items \"\"\"\n        return all(getattr(self, a, None) == getattr(other, a, None)\n                   for a in ['name', 'cname', 'axis', 'pos'])\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    @property\n    def is_indexed(self):\n        \"\"\" return whether I am an indexed column \"\"\"\n        try:\n            return getattr(self.table.cols, self.cname).is_indexed\n        except AttributeError:\n            False\n\n    def copy(self):\n        new_self = copy.copy(self)\n        return new_self\n\n    def infer(self, handler):\n        \"\"\"infer this column from the table: create and return a new object\"\"\"\n        table = handler.table\n        new_self = self.copy()\n        new_self.set_table(table)\n        new_self.get_attr()\n        new_self.read_metadata(handler)\n        return new_self\n\n    def convert(self, values, nan_rep, encoding, errors):\n        \"\"\" set the values from this selection: take = take ownership \"\"\"\n\n        # values is a recarray\n        if values.dtype.fields is not None:\n            values = values[self.cname]\n\n        values = _maybe_convert(values, self.kind, encoding, errors)\n\n        kwargs = dict()\n        if self.freq is not None:\n            kwargs['freq'] = _ensure_decoded(self.freq)\n        if self.index_name is not None:\n            kwargs['name'] = _ensure_decoded(self.index_name)\n        # making an Index instance could throw a number of different errors\n        try:\n            self.values = Index(values, **kwargs)\n        except Exception:  # noqa: E722\n\n            # if the output freq is different that what we recorded,\n            # it should be None (see also 'doc example part 2')\n            if 'freq' in kwargs:\n                kwargs['freq'] = None\n            self.values = Index(values, **kwargs)\n\n        self.values = _set_tz(self.values, self.tz)\n\n        return self\n\n    def take_data(self):\n        \"\"\" return the values & release the memory \"\"\"\n        self.values, values = None, self.values\n        return values\n\n    @property\n    def attrs(self):\n        return self.table._v_attrs\n\n    @property\n    def description(self):\n        return self.table.description\n\n    @property\n    def col(self):\n        \"\"\" return my current col description \"\"\"\n        return getattr(self.description, self.cname, None)\n\n    @property\n    def cvalues(self):\n        \"\"\" return my cython values \"\"\"\n        return self.values\n\n    def __iter__(self):\n        return iter(self.values)\n\n    def maybe_set_size(self, min_itemsize=None):\n        \"\"\" maybe set a string col itemsize:\n               min_itemsize can be an integer or a dict with this columns name\n               with an integer size \"\"\"\n        if _ensure_decoded(self.kind) == u'string':\n\n            if isinstance(min_itemsize, dict):\n                min_itemsize = min_itemsize.get(self.name)\n\n            if min_itemsize is not None and self.typ.itemsize < min_itemsize:\n                self.typ = _tables(\n                ).StringCol(itemsize=min_itemsize, pos=self.pos)\n\n    def validate(self, handler, append):\n        self.validate_names()\n\n    def validate_names(self):\n        pass\n\n    def validate_and_set(self, handler, append):\n        self.set_table(handler.table)\n        self.validate_col()\n        self.validate_attr(append)\n        self.validate_metadata(handler)\n        self.write_metadata(handler)\n        self.set_attr()\n\n    def validate_col(self, itemsize=None):\n        \"\"\" validate this column: return the compared against itemsize \"\"\"\n\n        # validate this column for string truncation (or reset to the max size)\n        if _ensure_decoded(self.kind) == u'string':\n            c = self.col\n            if c is not None:\n                if itemsize is None:\n                    itemsize = self.itemsize\n                if c.itemsize < itemsize:\n                    raise ValueError(\n                        \"Trying to store a string with len [%s] in [%s] \"\n                        \"column but\\nthis column has a limit of [%s]!\\n\"\n                        \"Consider using min_itemsize to preset the sizes on \"\n                        \"these columns\" % (itemsize, self.cname, c.itemsize))\n                return c.itemsize\n\n        return None\n\n    def validate_attr(self, append):\n        # check for backwards incompatibility\n        if append:\n            existing_kind = getattr(self.attrs, self.kind_attr, None)\n            if existing_kind is not None and existing_kind != self.kind:\n                raise TypeError(\"incompatible kind in col [%s - %s]\" %\n                                (existing_kind, self.kind))\n\n    def update_info(self, info):\n        \"\"\" set/update the info for this indexable with the key/value\n            if there is a conflict raise/warn as needed \"\"\"\n\n        for key in self._info_fields:\n\n            value = getattr(self, key, None)\n            idx = _get_info(info, self.name)\n\n            existing_value = idx.get(key)\n            if key in idx and value is not None and existing_value != value:\n\n                # frequency/name just warn\n                if key in ['freq', 'index_name']:\n                    ws = attribute_conflict_doc % (key, existing_value, value)\n                    warnings.warn(ws, AttributeConflictWarning, stacklevel=6)\n\n                    # reset\n                    idx[key] = None\n                    setattr(self, key, None)\n\n                else:\n                    raise ValueError(\n                        \"invalid info for [%s] for [%s], existing_value [%s] \"\n                        \"conflicts with new value [%s]\"\n                        % (self.name, key, existing_value, value))\n            else:\n                if value is not None or existing_value is not None:\n                    idx[key] = value\n\n        return self\n\n    def set_info(self, info):\n        \"\"\" set my state from the passed info \"\"\"\n        idx = info.get(self.name)\n        if idx is not None:\n            self.__dict__.update(idx)\n\n    def get_attr(self):\n        \"\"\" set the kind for this column \"\"\"\n        self.kind = getattr(self.attrs, self.kind_attr, None)\n\n    def set_attr(self):\n        \"\"\" set the kind for this column \"\"\"\n        setattr(self.attrs, self.kind_attr, self.kind)\n\n    def read_metadata(self, handler):\n        \"\"\" retrieve the metadata for this columns \"\"\"\n        self.metadata = handler.read_metadata(self.cname)\n\n    def validate_metadata(self, handler):\n        \"\"\" validate that kind=category does not change the categories \"\"\"\n        if self.meta == 'category':\n            new_metadata = self.metadata\n            cur_metadata = handler.read_metadata(self.cname)\n            if (new_metadata is not None and cur_metadata is not None and\n                    not array_equivalent(new_metadata, cur_metadata)):\n                raise ValueError(\"cannot append a categorical with \"\n                                 \"different categories to the existing\")\n\n    def write_metadata(self, handler):\n        \"\"\" set the meta data \"\"\"\n        if self.metadata is not None:\n            handler.write_metadata(self.cname, self.metadata)\n\n\nclass GenericIndexCol(IndexCol):\n\n    \"\"\" an index which is not represented in the data of the table \"\"\"\n\n    @property\n    def is_indexed(self):\n        return False\n\n    def convert(self, values, nan_rep, encoding, errors):\n        \"\"\" set the values from this selection: take = take ownership \"\"\"\n\n        self.values = Int64Index(np.arange(self.table.nrows))\n        return self\n\n    def get_attr(self):\n        pass\n\n    def set_attr(self):\n        pass\n\n\nclass DataCol(IndexCol):\n\n    \"\"\" a data holding column, by definition this is not indexable\n\n        Parameters\n        ----------\n\n        data   : the actual data\n        cname  : the column name in the table to hold the data (typically\n                 values)\n        meta   : a string description of the metadata\n        metadata : the actual metadata\n        \"\"\"\n    is_an_indexable = False\n    is_data_indexable = False\n    _info_fields = ['tz', 'ordered']\n\n    @classmethod\n    def create_for_block(\n            cls, i=None, name=None, cname=None, version=None, **kwargs):\n        \"\"\" return a new datacol with the block i \"\"\"\n\n        if cname is None:\n            cname = name or 'values_block_%d' % i\n        if name is None:\n            name = cname\n\n        # prior to 0.10.1, we named values blocks like: values_block_0 an the\n        # name values_0\n        try:\n            if version[0] == 0 and version[1] <= 10 and version[2] == 0:\n                m = re.search(r\"values_block_(\\d+)\", name)\n                if m:\n                    name = \"values_%s\" % m.groups()[0]\n        except IndexError:\n            pass\n\n        return cls(name=name, cname=cname, **kwargs)\n\n    def __init__(self, values=None, kind=None, typ=None,\n                 cname=None, data=None, meta=None, metadata=None,\n                 block=None, **kwargs):\n        super(DataCol, self).__init__(values=values, kind=kind, typ=typ,\n                                      cname=cname, **kwargs)\n        self.dtype = None\n        self.dtype_attr = u'{}_dtype'.format(self.name)\n        self.meta = meta\n        self.meta_attr = u'{}_meta'.format(self.name)\n        self.set_data(data)\n        self.set_metadata(metadata)\n\n    def __unicode__(self):\n        temp = tuple(\n            map(pprint_thing,\n                    (self.name,\n                     self.cname,\n                     self.dtype,\n                     self.kind,\n                     self.shape)))\n        return \"name->%s,cname->%s,dtype->%s,kind->%s,shape->%s\" % temp\n\n    def __eq__(self, other):\n        \"\"\" compare 2 col items \"\"\"\n        return all(getattr(self, a, None) == getattr(other, a, None)\n                   for a in ['name', 'cname', 'dtype', 'pos'])\n\n    def set_data(self, data, dtype=None):\n        self.data = data\n        if data is not None:\n            if dtype is not None:\n                self.dtype = dtype\n                self.set_kind()\n            elif self.dtype is None:\n                self.dtype = data.dtype.name\n                self.set_kind()\n\n    def take_data(self):\n        \"\"\" return the data & release the memory \"\"\"\n        self.data, data = None, self.data\n        return data\n\n    def set_metadata(self, metadata):\n        \"\"\" record the metadata \"\"\"\n        if metadata is not None:\n            metadata = np.array(metadata, copy=False).ravel()\n        self.metadata = metadata\n\n    def set_kind(self):\n        # set my kind if we can\n\n        if self.dtype is not None:\n            dtype = _ensure_decoded(self.dtype)\n\n            if dtype.startswith(u'string') or dtype.startswith(u'bytes'):\n                self.kind = 'string'\n            elif dtype.startswith(u'float'):\n                self.kind = 'float'\n            elif dtype.startswith(u'complex'):\n                self.kind = 'complex'\n            elif dtype.startswith(u'int') or dtype.startswith(u'uint'):\n                self.kind = 'integer'\n            elif dtype.startswith(u'date'):\n                self.kind = 'datetime'\n            elif dtype.startswith(u'timedelta'):\n                self.kind = 'timedelta'\n            elif dtype.startswith(u'bool'):\n                self.kind = 'bool'\n            else:\n                raise AssertionError(\n                    \"cannot interpret dtype of [%s] in [%s]\" % (dtype, self))\n\n            # set my typ if we need\n            if self.typ is None:\n                self.typ = getattr(self.description, self.cname, None)\n\n    def set_atom(self, block, block_items, existing_col, min_itemsize,\n                 nan_rep, info, encoding=None, errors='strict'):\n        \"\"\" create and setup my atom from the block b \"\"\"\n\n        self.values = list(block_items)\n\n        # short-cut certain block types\n        if block.is_categorical:\n            return self.set_atom_categorical(block, items=block_items,\n                                             info=info)\n        elif block.is_datetimetz:\n            return self.set_atom_datetime64tz(block, info=info)\n        elif block.is_datetime:\n            return self.set_atom_datetime64(block)\n        elif block.is_timedelta:\n            return self.set_atom_timedelta64(block)\n        elif block.is_complex:\n            return self.set_atom_complex(block)\n\n        dtype = block.dtype.name\n        inferred_type = lib.infer_dtype(block.values)\n\n        if inferred_type == 'date':\n            raise TypeError(\n                \"[date] is not implemented as a table column\")\n        elif inferred_type == 'datetime':\n            # after 8260\n            # this only would be hit for a mutli-timezone dtype\n            # which is an error\n\n            raise TypeError(\n                \"too many timezones in this block, create separate \"\n                \"data columns\"\n            )\n        elif inferred_type == 'unicode':\n            raise TypeError(\n                \"[unicode] is not implemented as a table column\")\n\n        # this is basically a catchall; if say a datetime64 has nans then will\n        # end up here ###\n        elif inferred_type == 'string' or dtype == 'object':\n            self.set_atom_string(\n                block, block_items,\n                existing_col,\n                min_itemsize,\n                nan_rep,\n                encoding,\n                errors)\n\n        # set as a data block\n        else:\n            self.set_atom_data(block)\n\n    def get_atom_string(self, block, itemsize):\n        return _tables().StringCol(itemsize=itemsize, shape=block.shape[0])\n\n    def set_atom_string(self, block, block_items, existing_col, min_itemsize,\n                        nan_rep, encoding, errors):\n        # fill nan items with myself, don't disturb the blocks by\n        # trying to downcast\n        block = block.fillna(nan_rep, downcast=False)\n        if isinstance(block, list):\n            block = block[0]\n        data = block.values\n\n        # see if we have a valid string type\n        inferred_type = lib.infer_dtype(data.ravel())\n        if inferred_type != 'string':\n\n            # we cannot serialize this data, so report an exception on a column\n            # by column basis\n            for i, item in enumerate(block_items):\n\n                col = block.iget(i)\n                inferred_type = lib.infer_dtype(col.ravel())\n                if inferred_type != 'string':\n                    raise TypeError(\n                        \"Cannot serialize the column [%s] because\\n\"\n                        \"its data contents are [%s] object dtype\"\n                        % (item, inferred_type)\n                    )\n\n        # itemsize is the maximum length of a string (along any dimension)\n        data_converted = _convert_string_array(data, encoding, errors)\n        itemsize = data_converted.itemsize\n\n        # specified min_itemsize?\n        if isinstance(min_itemsize, dict):\n            min_itemsize = int(min_itemsize.get(\n                self.name) or min_itemsize.get('values') or 0)\n        itemsize = max(min_itemsize or 0, itemsize)\n\n        # check for column in the values conflicts\n        if existing_col is not None:\n            eci = existing_col.validate_col(itemsize)\n            if eci > itemsize:\n                itemsize = eci\n\n        self.itemsize = itemsize\n        self.kind = 'string'\n        self.typ = self.get_atom_string(block, itemsize)\n        self.set_data(data_converted.astype('|S%d' % itemsize, copy=False))\n\n    def get_atom_coltype(self, kind=None):\n        \"\"\" return the PyTables column class for this column \"\"\"\n        if kind is None:\n            kind = self.kind\n        if self.kind.startswith('uint'):\n            col_name = \"UInt%sCol\" % kind[4:]\n        else:\n            col_name = \"%sCol\" % kind.capitalize()\n\n        return getattr(_tables(), col_name)\n\n    def get_atom_data(self, block, kind=None):\n        return self.get_atom_coltype(kind=kind)(shape=block.shape[0])\n\n    def set_atom_complex(self, block):\n        self.kind = block.dtype.name\n        itemsize = int(self.kind.split('complex')[-1]) // 8\n        self.typ = _tables().ComplexCol(\n            itemsize=itemsize, shape=block.shape[0])\n        self.set_data(block.values.astype(self.typ.type, copy=False))\n\n    def set_atom_data(self, block):\n        self.kind = block.dtype.name\n        self.typ = self.get_atom_data(block)\n        self.set_data(block.values.astype(self.typ.type, copy=False))\n\n    def set_atom_categorical(self, block, items, info=None, values=None):\n        # currently only supports a 1-D categorical\n        # in a 1-D block\n\n        values = block.values\n        codes = values.codes\n        self.kind = 'integer'\n        self.dtype = codes.dtype.name\n        if values.ndim > 1:\n            raise NotImplementedError(\"only support 1-d categoricals\")\n        if len(items) > 1:\n            raise NotImplementedError(\"only support single block categoricals\")\n\n        # write the codes; must be in a block shape\n        self.ordered = values.ordered\n        self.typ = self.get_atom_data(block, kind=codes.dtype.name)\n        self.set_data(_block_shape(codes))\n\n        # write the categories\n        self.meta = 'category'\n        self.set_metadata(block.values.categories)\n\n        # update the info\n        self.update_info(info)\n\n    def get_atom_datetime64(self, block):\n        return _tables().Int64Col(shape=block.shape[0])\n\n    def set_atom_datetime64(self, block, values=None):\n        self.kind = 'datetime64'\n        self.typ = self.get_atom_datetime64(block)\n        if values is None:\n            values = block.values.view('i8')\n        self.set_data(values, 'datetime64')\n\n    def set_atom_datetime64tz(self, block, info, values=None):\n\n        if values is None:\n            values = block.values\n\n        # convert this column to i8 in UTC, and save the tz\n        values = values.asi8.reshape(block.shape)\n\n        # store a converted timezone\n        self.tz = _get_tz(block.values.tz)\n        self.update_info(info)\n\n        self.kind = 'datetime64'\n        self.typ = self.get_atom_datetime64(block)\n        self.set_data(values, 'datetime64')\n\n    def get_atom_timedelta64(self, block):\n        return _tables().Int64Col(shape=block.shape[0])\n\n    def set_atom_timedelta64(self, block, values=None):\n        self.kind = 'timedelta64'\n        self.typ = self.get_atom_timedelta64(block)\n        if values is None:\n            values = block.values.view('i8')\n        self.set_data(values, 'timedelta64')\n\n    @property\n    def shape(self):\n        return getattr(self.data, 'shape', None)\n\n    @property\n    def cvalues(self):\n        \"\"\" return my cython values \"\"\"\n        return self.data\n\n    def validate_attr(self, append):\n        \"\"\"validate that we have the same order as the existing & same dtype\"\"\"\n        if append:\n            existing_fields = getattr(self.attrs, self.kind_attr, None)\n            if (existing_fields is not None and\n                    existing_fields != list(self.values)):\n                raise ValueError(\"appended items do not match existing items\"\n                                 \" in table!\")\n\n            existing_dtype = getattr(self.attrs, self.dtype_attr, None)\n            if (existing_dtype is not None and\n                    existing_dtype != self.dtype):\n                raise ValueError(\"appended items dtype do not match existing \"\n                                 \"items dtype in table!\")\n\n    def convert(self, values, nan_rep, encoding, errors):\n        \"\"\"set the data from this selection (and convert to the correct dtype\n        if we can)\n        \"\"\"\n\n        # values is a recarray\n        if values.dtype.fields is not None:\n            values = values[self.cname]\n\n        self.set_data(values)\n\n        # use the meta if needed\n        meta = _ensure_decoded(self.meta)\n\n        # convert to the correct dtype\n        if self.dtype is not None:\n            dtype = _ensure_decoded(self.dtype)\n\n            # reverse converts\n            if dtype == u'datetime64':\n\n                # recreate with tz if indicated\n                self.data = _set_tz(self.data, self.tz, coerce=True)\n\n            elif dtype == u'timedelta64':\n                self.data = np.asarray(self.data, dtype='m8[ns]')\n            elif dtype == u'date':\n                try:\n                    self.data = np.asarray(\n                        [date.fromordinal(v) for v in self.data], dtype=object)\n                except ValueError:\n                    self.data = np.asarray(\n                        [date.fromtimestamp(v) for v in self.data],\n                        dtype=object)\n            elif dtype == u'datetime':\n                self.data = np.asarray(\n                    [datetime.fromtimestamp(v) for v in self.data],\n                    dtype=object)\n\n            elif meta == u'category':\n\n                # we have a categorical\n                categories = self.metadata\n                codes = self.data.ravel()\n\n                # if we have stored a NaN in the categories\n                # then strip it; in theory we could have BOTH\n                # -1s in the codes and nulls :<\n                if categories is None:\n                    # Handle case of NaN-only categorical columns in which case\n                    # the categories are an empty array; when this is stored,\n                    # pytables cannot write a zero-len array, so on readback\n                    # the categories would be None and `read_hdf()` would fail.\n                    categories = Index([], dtype=np.float64)\n                else:\n                    mask = isna(categories)\n                    if mask.any():\n                        categories = categories[~mask]\n                        codes[codes != -1] -= mask.astype(int).cumsum().values\n\n                self.data = Categorical.from_codes(codes,\n                                                   categories=categories,\n                                                   ordered=self.ordered)\n\n            else:\n\n                try:\n                    self.data = self.data.astype(dtype, copy=False)\n                except TypeError:\n                    self.data = self.data.astype('O', copy=False)\n\n        # convert nans / decode\n        if _ensure_decoded(self.kind) == u'string':\n            self.data = _unconvert_string_array(\n                self.data, nan_rep=nan_rep, encoding=encoding, errors=errors)\n\n        return self\n\n    def get_attr(self):\n        \"\"\" get the data for this column \"\"\"\n        self.values = getattr(self.attrs, self.kind_attr, None)\n        self.dtype = getattr(self.attrs, self.dtype_attr, None)\n        self.meta = getattr(self.attrs, self.meta_attr, None)\n        self.set_kind()\n\n    def set_attr(self):\n        \"\"\" set the data for this column \"\"\"\n        setattr(self.attrs, self.kind_attr, self.values)\n        setattr(self.attrs, self.meta_attr, self.meta)\n        if self.dtype is not None:\n            setattr(self.attrs, self.dtype_attr, self.dtype)\n\n\nclass DataIndexableCol(DataCol):\n\n    \"\"\" represent a data column that can be indexed \"\"\"\n    is_data_indexable = True\n\n    def validate_names(self):\n        if not Index(self.values).is_object():\n            raise ValueError(\"cannot have non-object label DataIndexableCol\")\n\n    def get_atom_string(self, block, itemsize):\n        return _tables().StringCol(itemsize=itemsize)\n\n    def get_atom_data(self, block, kind=None):\n        return self.get_atom_coltype(kind=kind)()\n\n    def get_atom_datetime64(self, block):\n        return _tables().Int64Col()\n\n    def get_atom_timedelta64(self, block):\n        return _tables().Int64Col()\n\n\nclass GenericDataIndexableCol(DataIndexableCol):\n\n    \"\"\" represent a generic pytables data column \"\"\"\n\n    def get_attr(self):\n        pass\n\n\nclass Fixed(StringMixin):\n\n    \"\"\" represent an object in my store\n        facilitate read/write of various types of objects\n        this is an abstract base class\n\n        Parameters\n        ----------\n\n        parent : my parent HDFStore\n        group  : the group node where the table resides\n        \"\"\"\n    pandas_kind = None\n    obj_type = None\n    ndim = None\n    is_table = False\n\n    def __init__(self, parent, group, encoding=None, errors='strict',\n                 **kwargs):\n        self.parent = parent\n        self.group = group\n        self.encoding = _ensure_encoding(encoding)\n        self.errors = errors\n        self.set_version()\n\n    @property\n    def is_old_version(self):\n        return (self.version[0] <= 0 and self.version[1] <= 10 and\n                self.version[2] < 1)\n\n    def set_version(self):\n        \"\"\" compute and set our version \"\"\"\n        version = _ensure_decoded(\n            getattr(self.group._v_attrs, 'pandas_version', None))\n        try:\n            self.version = tuple(int(x) for x in version.split('.'))\n            if len(self.version) == 2:\n                self.version = self.version + (0,)\n        except AttributeError:\n            self.version = (0, 0, 0)\n\n    @property\n    def pandas_type(self):\n        return _ensure_decoded(getattr(self.group._v_attrs,\n                                       'pandas_type', None))\n\n    @property\n    def format_type(self):\n        return 'fixed'\n\n    def __unicode__(self):\n        \"\"\" return a pretty representation of myself \"\"\"\n        self.infer_axes()\n        s = self.shape\n        if s is not None:\n            if isinstance(s, (list, tuple)):\n                s = \"[%s]\" % ','.join(pprint_thing(x) for x in s)\n            return \"%-12.12s (shape->%s)\" % (self.pandas_type, s)\n        return self.pandas_type\n\n    def set_object_info(self):\n        \"\"\" set my pandas type & version \"\"\"\n        self.attrs.pandas_type = str(self.pandas_kind)\n        self.attrs.pandas_version = str(_version)\n        self.set_version()\n\n    def copy(self):\n        new_self = copy.copy(self)\n        return new_self\n\n    @property\n    def storage_obj_type(self):\n        return self.obj_type\n\n    @property\n    def shape(self):\n        return self.nrows\n\n    @property\n    def pathname(self):\n        return self.group._v_pathname\n\n    @property\n    def _handle(self):\n        return self.parent._handle\n\n    @property\n    def _filters(self):\n        return self.parent._filters\n\n    @property\n    def _complevel(self):\n        return self.parent._complevel\n\n    @property\n    def _fletcher32(self):\n        return self.parent._fletcher32\n\n    @property\n    def _complib(self):\n        return self.parent._complib\n\n    @property\n    def attrs(self):\n        return self.group._v_attrs\n\n    def set_attrs(self):\n        \"\"\" set our object attributes \"\"\"\n        pass\n\n    def get_attrs(self):\n        \"\"\" get our object attributes \"\"\"\n        pass\n\n    @property\n    def storable(self):\n        \"\"\" return my storable \"\"\"\n        return self.group\n\n    @property\n    def is_exists(self):\n        return False\n\n    @property\n    def nrows(self):\n        return getattr(self.storable, 'nrows', None)\n\n    def validate(self, other):\n        \"\"\" validate against an existing storable \"\"\"\n        if other is None:\n            return\n        return True\n\n    def validate_version(self, where=None):\n        \"\"\" are we trying to operate on an old version? \"\"\"\n        return True\n\n    def infer_axes(self):\n        \"\"\" infer the axes of my storer\n              return a boolean indicating if we have a valid storer or not \"\"\"\n\n        s = self.storable\n        if s is None:\n            return False\n        self.get_attrs()\n        return True\n\n    def read(self, **kwargs):\n        raise NotImplementedError(\n            \"cannot read on an abstract storer: subclasses should implement\")\n\n    def write(self, **kwargs):\n        raise NotImplementedError(\n            \"cannot write on an abstract storer: sublcasses should implement\")\n\n    def delete(self, where=None, start=None, stop=None, **kwargs):\n        \"\"\"\n        support fully deleting the node in its entirety (only) - where\n        specification must be None\n        \"\"\"\n        if com._all_none(where, start, stop):\n            self._handle.remove_node(self.group, recursive=True)\n            return None\n\n        raise TypeError(\"cannot delete on an abstract storer\")\n\n\nclass GenericFixed(Fixed):\n\n    \"\"\" a generified fixed version \"\"\"\n    _index_type_map = {DatetimeIndex: 'datetime', PeriodIndex: 'period'}\n    _reverse_index_map = {v: k for k, v in compat.iteritems(_index_type_map)}\n    attributes = []\n\n    # indexer helpders\n    def _class_to_alias(self, cls):\n        return self._index_type_map.get(cls, '')\n\n    def _alias_to_class(self, alias):\n        if isinstance(alias, type):  # pragma: no cover\n            # compat: for a short period of time master stored types\n            return alias\n        return self._reverse_index_map.get(alias, Index)\n\n    def _get_index_factory(self, klass):\n        if klass == DatetimeIndex:\n            def f(values, freq=None, tz=None):\n                # data are already in UTC, localize and convert if tz present\n                result = DatetimeIndex._simple_new(values.values, name=None,\n                                                   freq=freq)\n                if tz is not None:\n                    result = result.tz_localize('UTC').tz_convert(tz)\n                return result\n            return f\n        elif klass == PeriodIndex:\n            def f(values, freq=None, tz=None):\n                return PeriodIndex._simple_new(values, name=None, freq=freq)\n            return f\n\n        return klass\n\n    def validate_read(self, kwargs):\n        \"\"\"\n        remove table keywords from kwargs and return\n        raise if any keywords are passed which are not-None\n        \"\"\"\n        kwargs = copy.copy(kwargs)\n\n        columns = kwargs.pop('columns', None)\n        if columns is not None:\n            raise TypeError(\"cannot pass a column specification when reading \"\n                            \"a Fixed format store. this store must be \"\n                            \"selected in its entirety\")\n        where = kwargs.pop('where', None)\n        if where is not None:\n            raise TypeError(\"cannot pass a where specification when reading \"\n                            \"from a Fixed format store. this store must be \"\n                            \"selected in its entirety\")\n        return kwargs\n\n    @property\n    def is_exists(self):\n        return True\n\n    def set_attrs(self):\n        \"\"\" set our object attributes \"\"\"\n        self.attrs.encoding = self.encoding\n        self.attrs.errors = self.errors\n\n    def get_attrs(self):\n        \"\"\" retrieve our attributes \"\"\"\n        self.encoding = _ensure_encoding(getattr(self.attrs, 'encoding', None))\n        self.errors = getattr(self.attrs, 'errors', 'strict')\n        for n in self.attributes:\n            setattr(self, n, _ensure_decoded(getattr(self.attrs, n, None)))\n\n    def write(self, obj, **kwargs):\n        self.set_attrs()\n\n    def read_array(self, key, start=None, stop=None):\n        \"\"\" read an array for the specified node (off of group \"\"\"\n        import tables\n        node = getattr(self.group, key)\n        attrs = node._v_attrs\n\n        transposed = getattr(attrs, 'transposed', False)\n\n        if isinstance(node, tables.VLArray):\n            ret = node[0][start:stop]\n        else:\n            dtype = getattr(attrs, 'value_type', None)\n            shape = getattr(attrs, 'shape', None)\n\n            if shape is not None:\n                # length 0 axis\n                ret = np.empty(shape, dtype=dtype)\n            else:\n                ret = node[start:stop]\n\n            if dtype == u'datetime64':\n\n                # reconstruct a timezone if indicated\n                ret = _set_tz(ret, getattr(attrs, 'tz', None), coerce=True)\n\n            elif dtype == u'timedelta64':\n                ret = np.asarray(ret, dtype='m8[ns]')\n\n        if transposed:\n            return ret.T\n        else:\n            return ret\n\n    def read_index(self, key, **kwargs):\n        variety = _ensure_decoded(getattr(self.attrs, '%s_variety' % key))\n\n        if variety == u'multi':\n            return self.read_multi_index(key, **kwargs)\n        elif variety == u'block':\n            return self.read_block_index(key, **kwargs)\n        elif variety == u'sparseint':\n            return self.read_sparse_intindex(key, **kwargs)\n        elif variety == u'regular':\n            _, index = self.read_index_node(getattr(self.group, key), **kwargs)\n            return index\n        else:  # pragma: no cover\n            raise TypeError('unrecognized index variety: %s' % variety)\n\n    def write_index(self, key, index):\n        if isinstance(index, MultiIndex):\n            setattr(self.attrs, '%s_variety' % key, 'multi')\n            self.write_multi_index(key, index)\n        elif isinstance(index, BlockIndex):\n            setattr(self.attrs, '%s_variety' % key, 'block')\n            self.write_block_index(key, index)\n        elif isinstance(index, IntIndex):\n            setattr(self.attrs, '%s_variety' % key, 'sparseint')\n            self.write_sparse_intindex(key, index)\n        else:\n            setattr(self.attrs, '%s_variety' % key, 'regular')\n            converted = _convert_index(index, self.encoding, self.errors,\n                                       self.format_type).set_name('index')\n\n            self.write_array(key, converted.values)\n\n            node = getattr(self.group, key)\n            node._v_attrs.kind = converted.kind\n            node._v_attrs.name = index.name\n\n            if isinstance(index, (DatetimeIndex, PeriodIndex)):\n                node._v_attrs.index_class = self._class_to_alias(type(index))\n\n            if hasattr(index, 'freq'):\n                node._v_attrs.freq = index.freq\n\n            if hasattr(index, 'tz') and index.tz is not None:\n                node._v_attrs.tz = _get_tz(index.tz)\n\n    def write_block_index(self, key, index):\n        self.write_array('%s_blocs' % key, index.blocs)\n        self.write_array('%s_blengths' % key, index.blengths)\n        setattr(self.attrs, '%s_length' % key, index.length)\n\n    def read_block_index(self, key, **kwargs):\n        length = getattr(self.attrs, '%s_length' % key)\n        blocs = self.read_array('%s_blocs' % key, **kwargs)\n        blengths = self.read_array('%s_blengths' % key, **kwargs)\n        return BlockIndex(length, blocs, blengths)\n\n    def write_sparse_intindex(self, key, index):\n        self.write_array('%s_indices' % key, index.indices)\n        setattr(self.attrs, '%s_length' % key, index.length)\n\n    def read_sparse_intindex(self, key, **kwargs):\n        length = getattr(self.attrs, '%s_length' % key)\n        indices = self.read_array('%s_indices' % key, **kwargs)\n        return IntIndex(length, indices)\n\n    def write_multi_index(self, key, index):\n        setattr(self.attrs, '%s_nlevels' % key, index.nlevels)\n\n        for i, (lev, lab, name) in enumerate(zip(index.levels,\n                                                 index.labels,\n                                                 index.names)):\n            # write the level\n            level_key = '%s_level%d' % (key, i)\n            conv_level = _convert_index(lev, self.encoding, self.errors,\n                                        self.format_type).set_name(level_key)\n            self.write_array(level_key, conv_level.values)\n            node = getattr(self.group, level_key)\n            node._v_attrs.kind = conv_level.kind\n            node._v_attrs.name = name\n\n            # write the name\n            setattr(node._v_attrs, '%s_name%d' % (key, i), name)\n\n            # write the labels\n            label_key = '%s_label%d' % (key, i)\n            self.write_array(label_key, lab)\n\n    def read_multi_index(self, key, **kwargs):\n        nlevels = getattr(self.attrs, '%s_nlevels' % key)\n\n        levels = []\n        labels = []\n        names = []\n        for i in range(nlevels):\n            level_key = '%s_level%d' % (key, i)\n            name, lev = self.read_index_node(getattr(self.group, level_key),\n                                             **kwargs)\n            levels.append(lev)\n            names.append(name)\n\n            label_key = '%s_label%d' % (key, i)\n            lab = self.read_array(label_key, **kwargs)\n            labels.append(lab)\n\n        return MultiIndex(levels=levels, labels=labels, names=names,\n                          verify_integrity=True)\n\n    def read_index_node(self, node, start=None, stop=None):\n        data = node[start:stop]\n        # If the index was an empty array write_array_empty() will\n        # have written a sentinel. Here we relace it with the original.\n        if ('shape' in node._v_attrs and\n                self._is_empty_array(getattr(node._v_attrs, 'shape'))):\n            data = np.empty(getattr(node._v_attrs, 'shape'),\n                            dtype=getattr(node._v_attrs, 'value_type'))\n        kind = _ensure_decoded(node._v_attrs.kind)\n        name = None\n\n        if 'name' in node._v_attrs:\n            name = _ensure_str(node._v_attrs.name)\n\n        index_class = self._alias_to_class(_ensure_decoded(\n            getattr(node._v_attrs, 'index_class', '')))\n        factory = self._get_index_factory(index_class)\n\n        kwargs = {}\n        if u'freq' in node._v_attrs:\n            kwargs['freq'] = node._v_attrs['freq']\n\n        if u'tz' in node._v_attrs:\n            kwargs['tz'] = node._v_attrs['tz']\n\n        if kind in (u'date', u'datetime'):\n            index = factory(_unconvert_index(data, kind,\n                                             encoding=self.encoding,\n                                             errors=self.errors),\n                            dtype=object, **kwargs)\n        else:\n            index = factory(_unconvert_index(data, kind,\n                                             encoding=self.encoding,\n                                             errors=self.errors), **kwargs)\n\n        index.name = name\n\n        return name, index\n\n    def write_array_empty(self, key, value):\n        \"\"\" write a 0-len array \"\"\"\n\n        # ugly hack for length 0 axes\n        arr = np.empty((1,) * value.ndim)\n        self._handle.create_array(self.group, key, arr)\n        getattr(self.group, key)._v_attrs.value_type = str(value.dtype)\n        getattr(self.group, key)._v_attrs.shape = value.shape\n\n    def _is_empty_array(self, shape):\n        \"\"\"Returns true if any axis is zero length.\"\"\"\n        return any(x == 0 for x in shape)\n\n    def write_array(self, key, value, items=None):\n        if key in self.group:\n            self._handle.remove_node(self.group, key)\n\n        # Transform needed to interface with pytables row/col notation\n        empty_array = self._is_empty_array(value.shape)\n        transposed = False\n\n        if is_categorical_dtype(value):\n            raise NotImplementedError('Cannot store a category dtype in '\n                                      'a HDF5 dataset that uses format='\n                                      '\"fixed\". Use format=\"table\".')\n\n        if not empty_array:\n            value = value.T\n            transposed = True\n\n        if self._filters is not None:\n            atom = None\n            try:\n                # get the atom for this datatype\n                atom = _tables().Atom.from_dtype(value.dtype)\n            except ValueError:\n                pass\n\n            if atom is not None:\n                # create an empty chunked array and fill it from value\n                if not empty_array:\n                    ca = self._handle.create_carray(self.group, key, atom,\n                                                    value.shape,\n                                                    filters=self._filters)\n                    ca[:] = value\n                    getattr(self.group, key)._v_attrs.transposed = transposed\n\n                else:\n                    self.write_array_empty(key, value)\n\n                return\n\n        if value.dtype.type == np.object_:\n\n            # infer the type, warn if we have a non-string type here (for\n            # performance)\n            inferred_type = lib.infer_dtype(value.ravel())\n            if empty_array:\n                pass\n            elif inferred_type == 'string':\n                pass\n            else:\n                try:\n                    items = list(items)\n                except TypeError:\n                    pass\n                ws = performance_doc % (inferred_type, key, items)\n                warnings.warn(ws, PerformanceWarning, stacklevel=7)\n\n            vlarr = self._handle.create_vlarray(self.group, key,\n                                                _tables().ObjectAtom())\n            vlarr.append(value)\n        else:\n            if empty_array:\n                self.write_array_empty(key, value)\n            else:\n                if is_datetime64_dtype(value.dtype):\n                    self._handle.create_array(\n                        self.group, key, value.view('i8'))\n                    getattr(\n                        self.group, key)._v_attrs.value_type = 'datetime64'\n                elif is_datetime64tz_dtype(value.dtype):\n                    # store as UTC\n                    # with a zone\n                    self._handle.create_array(self.group, key,\n                                              value.asi8)\n\n                    node = getattr(self.group, key)\n                    node._v_attrs.tz = _get_tz(value.tz)\n                    node._v_attrs.value_type = 'datetime64'\n                elif is_timedelta64_dtype(value.dtype):\n                    self._handle.create_array(\n                        self.group, key, value.view('i8'))\n                    getattr(\n                        self.group, key)._v_attrs.value_type = 'timedelta64'\n                else:\n                    self._handle.create_array(self.group, key, value)\n\n        getattr(self.group, key)._v_attrs.transposed = transposed\n\n\nclass LegacyFixed(GenericFixed):\n\n    def read_index_legacy(self, key, start=None, stop=None):\n        node = getattr(self.group, key)\n        data = node[start:stop]\n        kind = node._v_attrs.kind\n        return _unconvert_index_legacy(data, kind, encoding=self.encoding,\n                                       errors=self.errors)\n\n\nclass LegacySeriesFixed(LegacyFixed):\n\n    def read(self, **kwargs):\n        kwargs = self.validate_read(kwargs)\n        index = self.read_index_legacy('index')\n        values = self.read_array('values')\n        return Series(values, index=index)\n\n\nclass LegacyFrameFixed(LegacyFixed):\n\n    def read(self, **kwargs):\n        kwargs = self.validate_read(kwargs)\n        index = self.read_index_legacy('index')\n        columns = self.read_index_legacy('columns')\n        values = self.read_array('values')\n        return DataFrame(values, index=index, columns=columns)\n\n\nclass SeriesFixed(GenericFixed):\n    pandas_kind = u'series'\n    attributes = ['name']\n\n    @property\n    def shape(self):\n        try:\n            return len(getattr(self.group, 'values')),\n        except (TypeError, AttributeError):\n            return None\n\n    def read(self, **kwargs):\n        kwargs = self.validate_read(kwargs)\n        index = self.read_index('index', **kwargs)\n        values = self.read_array('values', **kwargs)\n        return Series(values, index=index, name=self.name)\n\n    def write(self, obj, **kwargs):\n        super(SeriesFixed, self).write(obj, **kwargs)\n        self.write_index('index', obj.index)\n        self.write_array('values', obj.values)\n        self.attrs.name = obj.name\n\n\nclass SparseFixed(GenericFixed):\n\n    def validate_read(self, kwargs):\n        \"\"\"\n        we don't support start, stop kwds in Sparse\n        \"\"\"\n        kwargs = super(SparseFixed, self).validate_read(kwargs)\n        if 'start' in kwargs or 'stop' in kwargs:\n            raise NotImplementedError(\"start and/or stop are not supported \"\n                                      \"in fixed Sparse reading\")\n        return kwargs\n\n\nclass SparseSeriesFixed(SparseFixed):\n    pandas_kind = u'sparse_series'\n    attributes = ['name', 'fill_value', 'kind']\n\n    def read(self, **kwargs):\n        kwargs = self.validate_read(kwargs)\n        index = self.read_index('index')\n        sp_values = self.read_array('sp_values')\n        sp_index = self.read_index('sp_index')\n        return SparseSeries(sp_values, index=index, sparse_index=sp_index,\n                            kind=self.kind or u'block',\n                            fill_value=self.fill_value,\n                            name=self.name)\n\n    def write(self, obj, **kwargs):\n        super(SparseSeriesFixed, self).write(obj, **kwargs)\n        self.write_index('index', obj.index)\n        self.write_index('sp_index', obj.sp_index)\n        self.write_array('sp_values', obj.sp_values)\n        self.attrs.name = obj.name\n        self.attrs.fill_value = obj.fill_value\n        self.attrs.kind = obj.kind\n\n\nclass SparseFrameFixed(SparseFixed):\n    pandas_kind = u'sparse_frame'\n    attributes = ['default_kind', 'default_fill_value']\n\n    def read(self, **kwargs):\n        kwargs = self.validate_read(kwargs)\n        columns = self.read_index('columns')\n        sdict = {}\n        for c in columns:\n            key = 'sparse_series_%s' % c\n            s = SparseSeriesFixed(self.parent, getattr(self.group, key))\n            s.infer_axes()\n            sdict[c] = s.read()\n        return SparseDataFrame(sdict, columns=columns,\n                               default_kind=self.default_kind,\n                               default_fill_value=self.default_fill_value)\n\n    def write(self, obj, **kwargs):\n        \"\"\" write it as a collection of individual sparse series \"\"\"\n        super(SparseFrameFixed, self).write(obj, **kwargs)\n        for name, ss in compat.iteritems(obj):\n            key = 'sparse_series_%s' % name\n            if key not in self.group._v_children:\n                node = self._handle.create_group(self.group, key)\n            else:\n                node = getattr(self.group, key)\n            s = SparseSeriesFixed(self.parent, node)\n            s.write(ss)\n        self.attrs.default_fill_value = obj.default_fill_value\n        self.attrs.default_kind = obj.default_kind\n        self.write_index('columns', obj.columns)\n\n\nclass BlockManagerFixed(GenericFixed):\n    attributes = ['ndim', 'nblocks']\n    is_shape_reversed = False\n\n    @property\n    def shape(self):\n        try:\n            ndim = self.ndim\n\n            # items\n            items = 0\n            for i in range(self.nblocks):\n                node = getattr(self.group, 'block%d_items' % i)\n                shape = getattr(node, 'shape', None)\n                if shape is not None:\n                    items += shape[0]\n\n            # data shape\n            node = getattr(self.group, 'block0_values')\n            shape = getattr(node, 'shape', None)\n            if shape is not None:\n                shape = list(shape[0:(ndim - 1)])\n            else:\n                shape = []\n\n            shape.append(items)\n\n            # hacky - this works for frames, but is reversed for panels\n            if self.is_shape_reversed:\n                shape = shape[::-1]\n\n            return shape\n        except AttributeError:\n            return None\n\n    def read(self, start=None, stop=None, **kwargs):\n        # start, stop applied to rows, so 0th axis only\n\n        kwargs = self.validate_read(kwargs)\n        select_axis = self.obj_type()._get_block_manager_axis(0)\n\n        axes = []\n        for i in range(self.ndim):\n\n            _start, _stop = (start, stop) if i == select_axis else (None, None)\n            ax = self.read_index('axis%d' % i, start=_start, stop=_stop)\n            axes.append(ax)\n\n        items = axes[0]\n        blocks = []\n        for i in range(self.nblocks):\n\n            blk_items = self.read_index('block%d_items' % i)\n            values = self.read_array('block%d_values' % i,\n                                     start=_start, stop=_stop)\n            blk = make_block(values,\n                             placement=items.get_indexer(blk_items))\n            blocks.append(blk)\n\n        return self.obj_type(BlockManager(blocks, axes))\n\n    def write(self, obj, **kwargs):\n        super(BlockManagerFixed, self).write(obj, **kwargs)\n        data = obj._data\n        if not data.is_consolidated():\n            data = data.consolidate()\n\n        self.attrs.ndim = data.ndim\n        for i, ax in enumerate(data.axes):\n            if i == 0:\n                if not ax.is_unique:\n                    raise ValueError(\n                        \"Columns index has to be unique for fixed format\")\n            self.write_index('axis%d' % i, ax)\n\n        # Supporting mixed-type DataFrame objects...nontrivial\n        self.attrs.nblocks = len(data.blocks)\n        for i, blk in enumerate(data.blocks):\n            # I have no idea why, but writing values before items fixed #2299\n            blk_items = data.items.take(blk.mgr_locs)\n            self.write_array('block%d_values' % i, blk.values, items=blk_items)\n            self.write_index('block%d_items' % i, blk_items)\n\n\nclass FrameFixed(BlockManagerFixed):\n    pandas_kind = u'frame'\n    obj_type = DataFrame\n\n\nclass PanelFixed(BlockManagerFixed):\n    pandas_kind = u'wide'\n    obj_type = Panel\n    is_shape_reversed = True\n\n    def write(self, obj, **kwargs):\n        obj._consolidate_inplace()\n        return super(PanelFixed, self).write(obj, **kwargs)\n\n\nclass Table(Fixed):\n\n    \"\"\" represent a table:\n          facilitate read/write of various types of tables\n\n        Attrs in Table Node\n        -------------------\n        These are attributes that are store in the main table node, they are\n        necessary to recreate these tables when read back in.\n\n        index_axes    : a list of tuples of the (original indexing axis and\n            index column)\n        non_index_axes: a list of tuples of the (original index axis and\n            columns on a non-indexing axis)\n        values_axes   : a list of the columns which comprise the data of this\n            table\n        data_columns  : a list of the columns that we are allowing indexing\n            (these become single columns in values_axes), or True to force all\n            columns\n        nan_rep       : the string to use for nan representations for string\n            objects\n        levels        : the names of levels\n        metadata      : the names of the metadata columns\n\n        \"\"\"\n    pandas_kind = u'wide_table'\n    table_type = None\n    levels = 1\n    is_table = True\n    is_shape_reversed = False\n\n    def __init__(self, *args, **kwargs):\n        super(Table, self).__init__(*args, **kwargs)\n        self.index_axes = []\n        self.non_index_axes = []\n        self.values_axes = []\n        self.data_columns = []\n        self.metadata = []\n        self.info = dict()\n        self.nan_rep = None\n        self.selection = None\n\n    @property\n    def table_type_short(self):\n        return self.table_type.split('_')[0]\n\n    @property\n    def format_type(self):\n        return 'table'\n\n    def __unicode__(self):\n        \"\"\" return a pretty representatgion of myself \"\"\"\n        self.infer_axes()\n        dc = \",dc->[%s]\" % ','.join(\n            self.data_columns) if len(self.data_columns) else ''\n\n        ver = ''\n        if self.is_old_version:\n            ver = \"[%s]\" % '.'.join(str(x) for x in self.version)\n\n        return \"%-12.12s%s (typ->%s,nrows->%s,ncols->%s,indexers->[%s]%s)\" % (\n            self.pandas_type, ver, self.table_type_short, self.nrows,\n            self.ncols, ','.join(a.name for a in self.index_axes), dc\n        )\n\n    def __getitem__(self, c):\n        \"\"\" return the axis for c \"\"\"\n        for a in self.axes:\n            if c == a.name:\n                return a\n        return None\n\n    def validate(self, other):\n        \"\"\" validate against an existing table \"\"\"\n        if other is None:\n            return\n\n        if other.table_type != self.table_type:\n            raise TypeError(\"incompatible table_type with existing [%s - %s]\" %\n                            (other.table_type, self.table_type))\n\n        for c in ['index_axes', 'non_index_axes', 'values_axes']:\n            sv = getattr(self, c, None)\n            ov = getattr(other, c, None)\n            if sv != ov:\n\n                # show the error for the specific axes\n                for i, sax in enumerate(sv):\n                    oax = ov[i]\n                    if sax != oax:\n                        raise ValueError(\n                            \"invalid combinate of [%s] on appending data [%s] \"\n                            \"vs current table [%s]\" % (c, sax, oax))\n\n                # should never get here\n                raise Exception(\n                    \"invalid combinate of [%s] on appending data [%s] vs \"\n                    \"current table [%s]\" % (c, sv, ov))\n\n    @property\n    def is_multi_index(self):\n        \"\"\"the levels attribute is 1 or a list in the case of a multi-index\"\"\"\n        return isinstance(self.levels, list)\n\n    def validate_metadata(self, existing):\n        \"\"\" create / validate metadata \"\"\"\n        self.metadata = [\n            c.name for c in self.values_axes if c.metadata is not None]\n\n    def validate_multiindex(self, obj):\n        \"\"\"validate that we can store the multi-index; reset and return the\n        new object\n        \"\"\"\n        levels = [l if l is not None else \"level_{0}\".format(i)\n                  for i, l in enumerate(obj.index.names)]\n        try:\n            return obj.reset_index(), levels\n        except ValueError:\n            raise ValueError(\"duplicate names/columns in the multi-index when \"\n                             \"storing as a table\")\n\n    @property\n    def nrows_expected(self):\n        \"\"\" based on our axes, compute the expected nrows \"\"\"\n        return np.prod([i.cvalues.shape[0] for i in self.index_axes])\n\n    @property\n    def is_exists(self):\n        \"\"\" has this table been created \"\"\"\n        return u'table' in self.group\n\n    @property\n    def storable(self):\n        return getattr(self.group, 'table', None)\n\n    @property\n    def table(self):\n        \"\"\" return the table group (this is my storable) \"\"\"\n        return self.storable\n\n    @property\n    def dtype(self):\n        return self.table.dtype\n\n    @property\n    def description(self):\n        return self.table.description\n\n    @property\n    def axes(self):\n        return itertools.chain(self.index_axes, self.values_axes)\n\n    @property\n    def ncols(self):\n        \"\"\" the number of total columns in the values axes \"\"\"\n        return sum(len(a.values) for a in self.values_axes)\n\n    @property\n    def is_transposed(self):\n        return False\n\n    @property\n    def data_orientation(self):\n        \"\"\"return a tuple of my permutated axes, non_indexable at the front\"\"\"\n        return tuple(itertools.chain([int(a[0]) for a in self.non_index_axes],\n                                     [int(a.axis) for a in self.index_axes]))\n\n    def queryables(self):\n        \"\"\" return a dict of the kinds allowable columns for this object \"\"\"\n\n        # compute the values_axes queryables\n        return dict(\n            [(a.cname, a) for a in self.index_axes] +\n            [(self.storage_obj_type._AXIS_NAMES[axis], None)\n             for axis, values in self.non_index_axes] +\n            [(v.cname, v) for v in self.values_axes\n             if v.name in set(self.data_columns)]\n        )\n\n    def index_cols(self):\n        \"\"\" return a list of my index cols \"\"\"\n        return [(i.axis, i.cname) for i in self.index_axes]\n\n    def values_cols(self):\n        \"\"\" return a list of my values cols \"\"\"\n        return [i.cname for i in self.values_axes]\n\n    def _get_metadata_path(self, key):\n        \"\"\" return the metadata pathname for this key \"\"\"\n        return \"{group}/meta/{key}/meta\".format(group=self.group._v_pathname,\n                                                key=key)\n\n    def write_metadata(self, key, values):\n        \"\"\"\n        write out a meta data array to the key as a fixed-format Series\n\n        Parameters\n        ----------\n        key : string\n        values : ndarray\n\n        \"\"\"\n        values = Series(values)\n        self.parent.put(self._get_metadata_path(key), values, format='table',\n                        encoding=self.encoding, errors=self.errors,\n                        nan_rep=self.nan_rep)\n\n    def read_metadata(self, key):\n        \"\"\" return the meta data array for this key \"\"\"\n        if getattr(getattr(self.group, 'meta', None), key, None) is not None:\n            return self.parent.select(self._get_metadata_path(key))\n        return None\n\n    def set_info(self):\n        \"\"\" update our table index info \"\"\"\n        self.attrs.info = self.info\n\n    def set_attrs(self):\n        \"\"\" set our table type & indexables \"\"\"\n        self.attrs.table_type = str(self.table_type)\n        self.attrs.index_cols = self.index_cols()\n        self.attrs.values_cols = self.values_cols()\n        self.attrs.non_index_axes = self.non_index_axes\n        self.attrs.data_columns = self.data_columns\n        self.attrs.nan_rep = self.nan_rep\n        self.attrs.encoding = self.encoding\n        self.attrs.errors = self.errors\n        self.attrs.levels = self.levels\n        self.attrs.metadata = self.metadata\n        self.set_info()\n\n    def get_attrs(self):\n        \"\"\" retrieve our attributes \"\"\"\n        self.non_index_axes = getattr(\n            self.attrs, 'non_index_axes', None) or []\n        self.data_columns = getattr(\n            self.attrs, 'data_columns', None) or []\n        self.info = getattr(\n            self.attrs, 'info', None) or dict()\n        self.nan_rep = getattr(self.attrs, 'nan_rep', None)\n        self.encoding = _ensure_encoding(\n            getattr(self.attrs, 'encoding', None))\n        self.errors = getattr(self.attrs, 'errors', 'strict')\n        self.levels = getattr(\n            self.attrs, 'levels', None) or []\n        self.index_axes = [\n            a.infer(self) for a in self.indexables if a.is_an_indexable\n        ]\n        self.values_axes = [\n            a.infer(self) for a in self.indexables if not a.is_an_indexable\n        ]\n        self.metadata = getattr(\n            self.attrs, 'metadata', None) or []\n\n    def validate_version(self, where=None):\n        \"\"\" are we trying to operate on an old version? \"\"\"\n        if where is not None:\n            if (self.version[0] <= 0 and self.version[1] <= 10 and\n                    self.version[2] < 1):\n                ws = incompatibility_doc % '.'.join(\n                    [str(x) for x in self.version])\n                warnings.warn(ws, IncompatibilityWarning)\n\n    def validate_min_itemsize(self, min_itemsize):\n        \"\"\"validate the min_itemisze doesn't contain items that are not in the\n        axes this needs data_columns to be defined\n        \"\"\"\n        if min_itemsize is None:\n            return\n        if not isinstance(min_itemsize, dict):\n            return\n\n        q = self.queryables()\n        for k, v in min_itemsize.items():\n\n            # ok, apply generally\n            if k == 'values':\n                continue\n            if k not in q:\n                raise ValueError(\n                    \"min_itemsize has the key [%s] which is not an axis or \"\n                    \"data_column\" % k)\n\n    @property\n    def indexables(self):\n        \"\"\" create/cache the indexables if they don't exist \"\"\"\n        if self._indexables is None:\n\n            self._indexables = []\n\n            # index columns\n            self._indexables.extend([\n                IndexCol(name=name, axis=axis, pos=i)\n                for i, (axis, name) in enumerate(self.attrs.index_cols)\n            ])\n\n            # values columns\n            dc = set(self.data_columns)\n            base_pos = len(self._indexables)\n\n            def f(i, c):\n                klass = DataCol\n                if c in dc:\n                    klass = DataIndexableCol\n                return klass.create_for_block(i=i, name=c, pos=base_pos + i,\n                                              version=self.version)\n\n            self._indexables.extend(\n                [f(i, c) for i, c in enumerate(self.attrs.values_cols)])\n\n        return self._indexables\n\n    def create_index(self, columns=None, optlevel=None, kind=None):\n        \"\"\"\n        Create a pytables index on the specified columns\n          note: cannot index Time64Col() or ComplexCol currently;\n          PyTables must be >= 3.0\n\n        Parameters\n        ----------\n        columns : False (don't create an index), True (create all columns\n            index), None or list_like (the indexers to index)\n        optlevel: optimization level (defaults to 6)\n        kind    : kind of index (defaults to 'medium')\n\n        Exceptions\n        ----------\n        raises if the node is not a table\n\n        \"\"\"\n\n        if not self.infer_axes():\n            return\n        if columns is False:\n            return\n\n        # index all indexables and data_columns\n        if columns is None or columns is True:\n            columns = [a.cname for a in self.axes if a.is_data_indexable]\n        if not isinstance(columns, (tuple, list)):\n            columns = [columns]\n\n        kw = dict()\n        if optlevel is not None:\n            kw['optlevel'] = optlevel\n        if kind is not None:\n            kw['kind'] = kind\n\n        table = self.table\n        for c in columns:\n            v = getattr(table.cols, c, None)\n            if v is not None:\n\n                # remove the index if the kind/optlevel have changed\n                if v.is_indexed:\n                    index = v.index\n                    cur_optlevel = index.optlevel\n                    cur_kind = index.kind\n\n                    if kind is not None and cur_kind != kind:\n                        v.remove_index()\n                    else:\n                        kw['kind'] = cur_kind\n\n                    if optlevel is not None and cur_optlevel != optlevel:\n                        v.remove_index()\n                    else:\n                        kw['optlevel'] = cur_optlevel\n\n                # create the index\n                if not v.is_indexed:\n                    if v.type.startswith('complex'):\n                        raise TypeError(\n                            'Columns containing complex values can be stored '\n                            'but cannot'\n                            ' be indexed when using table format. Either use '\n                            'fixed format, set index=False, or do not include '\n                            'the columns containing complex values to '\n                            'data_columns when initializing the table.')\n                    v.create_index(**kw)\n\n    def read_axes(self, where, **kwargs):\n        \"\"\"create and return the axes sniffed from the table: return boolean\n        for success\n        \"\"\"\n\n        # validate the version\n        self.validate_version(where)\n\n        # infer the data kind\n        if not self.infer_axes():\n            return False\n\n        # create the selection\n        self.selection = Selection(self, where=where, **kwargs)\n        values = self.selection.select()\n\n        # convert the data\n        for a in self.axes:\n            a.set_info(self.info)\n            a.convert(values, nan_rep=self.nan_rep, encoding=self.encoding,\n                      errors=self.errors)\n\n        return True\n\n    def get_object(self, obj):\n        \"\"\" return the data for this obj \"\"\"\n        return obj\n\n    def validate_data_columns(self, data_columns, min_itemsize):\n        \"\"\"take the input data_columns and min_itemize and create a data\n        columns spec\n        \"\"\"\n\n        if not len(self.non_index_axes):\n            return []\n\n        axis, axis_labels = self.non_index_axes[0]\n        info = self.info.get(axis, dict())\n        if info.get('type') == 'MultiIndex' and data_columns:\n            raise ValueError(\"cannot use a multi-index on axis [{0}] with \"\n                             \"data_columns {1}\".format(axis, data_columns))\n\n        # evaluate the passed data_columns, True == use all columns\n        # take only valide axis labels\n        if data_columns is True:\n            data_columns = list(axis_labels)\n        elif data_columns is None:\n            data_columns = []\n\n        # if min_itemsize is a dict, add the keys (exclude 'values')\n        if isinstance(min_itemsize, dict):\n\n            existing_data_columns = set(data_columns)\n            data_columns.extend([\n                k for k in min_itemsize.keys()\n                if k != 'values' and k not in existing_data_columns\n            ])\n\n        # return valid columns in the order of our axis\n        return [c for c in data_columns if c in axis_labels]\n\n    def create_axes(self, axes, obj, validate=True, nan_rep=None,\n                    data_columns=None, min_itemsize=None, **kwargs):\n        \"\"\" create and return the axes\n        leagcy tables create an indexable column, indexable index,\n        non-indexable fields\n\n            Parameters:\n            -----------\n            axes: a list of the axes in order to create (names or numbers of\n                the axes)\n            obj : the object to create axes on\n            validate: validate the obj against an existing object already\n                written\n            min_itemsize: a dict of the min size for a column in bytes\n            nan_rep : a values to use for string column nan_rep\n            encoding : the encoding for string values\n            data_columns : a list of columns that we want to create separate to\n                allow indexing (or True will force all columns)\n\n        \"\"\"\n\n        # set the default axes if needed\n        if axes is None:\n            try:\n                axes = _AXES_MAP[type(obj)]\n            except KeyError:\n                raise TypeError(\"cannot properly create the storer for: \"\n                                \"[group->%s,value->%s]\"\n                                % (self.group._v_name, type(obj)))\n\n        # map axes to numbers\n        axes = [obj._get_axis_number(a) for a in axes]\n\n        # do we have an existing table (if so, use its axes & data_columns)\n        if self.infer_axes():\n            existing_table = self.copy()\n            existing_table.infer_axes()\n            axes = [a.axis for a in existing_table.index_axes]\n            data_columns = existing_table.data_columns\n            nan_rep = existing_table.nan_rep\n            self.encoding = existing_table.encoding\n            self.errors = existing_table.errors\n            self.info = copy.copy(existing_table.info)\n        else:\n            existing_table = None\n\n        # currently support on ndim-1 axes\n        if len(axes) != self.ndim - 1:\n            raise ValueError(\n                \"currently only support ndim-1 indexers in an AppendableTable\")\n\n        # create according to the new data\n        self.non_index_axes = []\n        self.data_columns = []\n\n        # nan_representation\n        if nan_rep is None:\n            nan_rep = 'nan'\n\n        self.nan_rep = nan_rep\n\n        # create axes to index and non_index\n        index_axes_map = dict()\n        for i, a in enumerate(obj.axes):\n\n            if i in axes:\n                name = obj._AXIS_NAMES[i]\n                index_axes_map[i] = _convert_index(\n                    a, self.encoding, self.errors, self.format_type\n                ).set_name(name).set_axis(i)\n            else:\n\n                # we might be able to change the axes on the appending data if\n                # necessary\n                append_axis = list(a)\n                if existing_table is not None:\n                    indexer = len(self.non_index_axes)\n                    exist_axis = existing_table.non_index_axes[indexer][1]\n                    if not array_equivalent(np.array(append_axis),\n                                            np.array(exist_axis)):\n\n                        # ahah! -> reindex\n                        if array_equivalent(np.array(sorted(append_axis)),\n                                            np.array(sorted(exist_axis))):\n                            append_axis = exist_axis\n\n                # the non_index_axes info\n                info = _get_info(self.info, i)\n                info['names'] = list(a.names)\n                info['type'] = a.__class__.__name__\n\n                self.non_index_axes.append((i, append_axis))\n\n        # set axis positions (based on the axes)\n        self.index_axes = [\n            index_axes_map[a].set_pos(j).update_info(self.info)\n            for j, a in enumerate(axes)\n        ]\n        j = len(self.index_axes)\n\n        # check for column conflicts\n        for a in self.axes:\n            a.maybe_set_size(min_itemsize=min_itemsize)\n\n        # reindex by our non_index_axes & compute data_columns\n        for a in self.non_index_axes:\n            obj = _reindex_axis(obj, a[0], a[1])\n\n        def get_blk_items(mgr, blocks):\n            return [mgr.items.take(blk.mgr_locs) for blk in blocks]\n\n        # figure out data_columns and get out blocks\n        block_obj = self.get_object(obj)._consolidate()\n        blocks = block_obj._data.blocks\n        blk_items = get_blk_items(block_obj._data, blocks)\n        if len(self.non_index_axes):\n            axis, axis_labels = self.non_index_axes[0]\n            data_columns = self.validate_data_columns(\n                data_columns, min_itemsize)\n            if len(data_columns):\n                mgr = block_obj.reindex(\n                    Index(axis_labels).difference(Index(data_columns)),\n                    axis=axis\n                )._data\n\n                blocks = list(mgr.blocks)\n                blk_items = get_blk_items(mgr, blocks)\n                for c in data_columns:\n                    mgr = block_obj.reindex([c], axis=axis)._data\n                    blocks.extend(mgr.blocks)\n                    blk_items.extend(get_blk_items(mgr, mgr.blocks))\n\n        # reorder the blocks in the same order as the existing_table if we can\n        if existing_table is not None:\n            by_items = {tuple(b_items.tolist()): (b, b_items)\n                        for b, b_items in zip(blocks, blk_items)}\n            new_blocks = []\n            new_blk_items = []\n            for ea in existing_table.values_axes:\n                items = tuple(ea.values)\n                try:\n                    b, b_items = by_items.pop(items)\n                    new_blocks.append(b)\n                    new_blk_items.append(b_items)\n                except (IndexError, KeyError):\n                    raise ValueError(\n                        \"cannot match existing table structure for [%s] on \"\n                        \"appending data\" % ','.join(pprint_thing(item) for\n                                                    item in items))\n            blocks = new_blocks\n            blk_items = new_blk_items\n\n        # add my values\n        self.values_axes = []\n        for i, (b, b_items) in enumerate(zip(blocks, blk_items)):\n\n            # shape of the data column are the indexable axes\n            klass = DataCol\n            name = None\n\n            # we have a data_column\n            if (data_columns and len(b_items) == 1 and\n                    b_items[0] in data_columns):\n                klass = DataIndexableCol\n                name = b_items[0]\n                self.data_columns.append(name)\n\n            # make sure that we match up the existing columns\n            # if we have an existing table\n            if existing_table is not None and validate:\n                try:\n                    existing_col = existing_table.values_axes[i]\n                except (IndexError, KeyError):\n                    raise ValueError(\"Incompatible appended table [%s] with \"\n                                     \"existing table [%s]\"\n                                     % (blocks, existing_table.values_axes))\n            else:\n                existing_col = None\n\n            try:\n                col = klass.create_for_block(\n                    i=i, name=name, version=self.version)\n                col.set_atom(block=b, block_items=b_items,\n                             existing_col=existing_col,\n                             min_itemsize=min_itemsize,\n                             nan_rep=nan_rep,\n                             encoding=self.encoding,\n                             errors=self.errors,\n                             info=self.info)\n                col.set_pos(j)\n\n                self.values_axes.append(col)\n            except (NotImplementedError, ValueError, TypeError) as e:\n                raise e\n            except Exception as detail:\n                raise Exception(\n                    \"cannot find the correct atom type -> \"\n                    \"[dtype->%s,items->%s] %s\"\n                    % (b.dtype.name, b_items, str(detail))\n                )\n            j += 1\n\n        # validate our min_itemsize\n        self.validate_min_itemsize(min_itemsize)\n\n        # validate our metadata\n        self.validate_metadata(existing_table)\n\n        # validate the axes if we have an existing table\n        if validate:\n            self.validate(existing_table)\n\n    def process_axes(self, obj, columns=None):\n        \"\"\" process axes filters \"\"\"\n\n        # make a copy to avoid side effects\n        if columns is not None:\n            columns = list(columns)\n\n        # make sure to include levels if we have them\n        if columns is not None and self.is_multi_index:\n            for n in self.levels:\n                if n not in columns:\n                    columns.insert(0, n)\n\n        # reorder by any non_index_axes & limit to the select columns\n        for axis, labels in self.non_index_axes:\n            obj = _reindex_axis(obj, axis, labels, columns)\n\n        # apply the selection filters (but keep in the same order)\n        if self.selection.filter is not None:\n            for field, op, filt in self.selection.filter.format():\n\n                def process_filter(field, filt):\n\n                    for axis_name in obj._AXIS_NAMES.values():\n                        axis_number = obj._get_axis_number(axis_name)\n                        axis_values = obj._get_axis(axis_name)\n\n                        # see if the field is the name of an axis\n                        if field == axis_name:\n\n                            # if we have a multi-index, then need to include\n                            # the levels\n                            if self.is_multi_index:\n                                filt = filt.union(Index(self.levels))\n\n                            takers = op(axis_values, filt)\n                            return obj.loc._getitem_axis(takers,\n                                                         axis=axis_number)\n\n                        # this might be the name of a file IN an axis\n                        elif field in axis_values:\n\n                            # we need to filter on this dimension\n                            values = ensure_index(getattr(obj, field).values)\n                            filt = ensure_index(filt)\n\n                            # hack until we support reversed dim flags\n                            if isinstance(obj, DataFrame):\n                                axis_number = 1 - axis_number\n                            takers = op(values, filt)\n                            return obj.loc._getitem_axis(takers,\n                                                         axis=axis_number)\n\n                    raise ValueError(\n                        \"cannot find the field [%s] for filtering!\" % field)\n\n                obj = process_filter(field, filt)\n\n        return obj\n\n    def create_description(self, complib=None, complevel=None,\n                           fletcher32=False, expectedrows=None):\n        \"\"\" create the description of the table from the axes & values \"\"\"\n\n        # provided expected rows if its passed\n        if expectedrows is None:\n            expectedrows = max(self.nrows_expected, 10000)\n\n        d = dict(name='table', expectedrows=expectedrows)\n\n        # description from the axes & values\n        d['description'] = {a.cname: a.typ for a in self.axes}\n\n        if complib:\n            if complevel is None:\n                complevel = self._complevel or 9\n            filters = _tables().Filters(\n                complevel=complevel, complib=complib,\n                fletcher32=fletcher32 or self._fletcher32)\n            d['filters'] = filters\n        elif self._filters is not None:\n            d['filters'] = self._filters\n\n        return d\n\n    def read_coordinates(self, where=None, start=None, stop=None, **kwargs):\n        \"\"\"select coordinates (row numbers) from a table; return the\n        coordinates object\n        \"\"\"\n\n        # validate the version\n        self.validate_version(where)\n\n        # infer the data kind\n        if not self.infer_axes():\n            return False\n\n        # create the selection\n        self.selection = Selection(\n            self, where=where, start=start, stop=stop, **kwargs)\n        coords = self.selection.select_coords()\n        if self.selection.filter is not None:\n            for field, op, filt in self.selection.filter.format():\n                data = self.read_column(\n                    field, start=coords.min(), stop=coords.max() + 1)\n                coords = coords[\n                    op(data.iloc[coords - coords.min()], filt).values]\n\n        return Index(coords)\n\n    def read_column(self, column, where=None, start=None, stop=None):\n        \"\"\"return a single column from the table, generally only indexables\n        are interesting\n        \"\"\"\n\n        # validate the version\n        self.validate_version()\n\n        # infer the data kind\n        if not self.infer_axes():\n            return False\n\n        if where is not None:\n            raise TypeError(\"read_column does not currently accept a where \"\n                            \"clause\")\n\n        # find the axes\n        for a in self.axes:\n            if column == a.name:\n\n                if not a.is_data_indexable:\n                    raise ValueError(\n                        \"column [%s] can not be extracted individually; it is \"\n                        \"not data indexable\" % column)\n\n                # column must be an indexable or a data column\n                c = getattr(self.table.cols, column)\n                a.set_info(self.info)\n                return Series(_set_tz(a.convert(c[start:stop],\n                                                nan_rep=self.nan_rep,\n                                                encoding=self.encoding,\n                                                errors=self.errors\n                                                ).take_data(),\n                                      a.tz, True), name=column)\n\n        raise KeyError(\"column [%s] not found in the table\" % column)\n\n\nclass WORMTable(Table):\n\n    \"\"\" a write-once read-many table: this format DOES NOT ALLOW appending to a\n         table. writing is a one-time operation the data are stored in a format\n         that allows for searching the data on disk\n         \"\"\"\n    table_type = u'worm'\n\n    def read(self, **kwargs):\n        \"\"\" read the indices and the indexing array, calculate offset rows and\n        return \"\"\"\n        raise NotImplementedError(\"WORMTable needs to implement read\")\n\n    def write(self, **kwargs):\n        \"\"\" write in a format that we can search later on (but cannot append\n               to): write out the indices and the values using _write_array\n               (e.g. a CArray) create an indexing table so that we can search\n        \"\"\"\n        raise NotImplementedError(\"WORKTable needs to implement write\")\n\n\nclass LegacyTable(Table):\n\n    \"\"\" an appendable table: allow append/query/delete operations to a\n          (possibly) already existing appendable table this table ALLOWS\n          append (but doesn't require them), and stores the data in a format\n          that can be easily searched\n\n    \"\"\"\n    _indexables = [\n        IndexCol(name='index', axis=1, pos=0),\n        IndexCol(name='column', axis=2, pos=1, index_kind='columns_kind'),\n        DataCol(name='fields', cname='values', kind_attr='fields', pos=2)\n    ]\n    table_type = u'legacy'\n    ndim = 3\n\n    def write(self, **kwargs):\n        raise TypeError(\"write operations are not allowed on legacy tables!\")\n\n    def read(self, where=None, columns=None, **kwargs):\n        \"\"\"we have n indexable columns, with an arbitrary number of data\n        axes\n        \"\"\"\n\n        if not self.read_axes(where=where, **kwargs):\n            return None\n\n        lst_vals = [a.values for a in self.index_axes]\n        labels, levels = _factorize_from_iterables(lst_vals)\n        # labels and levels are tuples but lists are expected\n        labels = list(labels)\n        levels = list(levels)\n        N = [len(lvl) for lvl in levels]\n\n        # compute the key\n        key = _factor_indexer(N[1:], labels)\n\n        objs = []\n        if len(unique(key)) == len(key):\n\n            sorter, _ = algos.groupsort_indexer(\n                ensure_int64(key), np.prod(N))\n            sorter = ensure_platform_int(sorter)\n\n            # create the objs\n            for c in self.values_axes:\n\n                # the data need to be sorted\n                sorted_values = c.take_data().take(sorter, axis=0)\n                if sorted_values.ndim == 1:\n                    sorted_values = sorted_values.reshape(\n                        (sorted_values.shape[0], 1))\n\n                take_labels = [l.take(sorter) for l in labels]\n                items = Index(c.values)\n                block = _block2d_to_blocknd(\n                    values=sorted_values, placement=np.arange(len(items)),\n                    shape=tuple(N), labels=take_labels, ref_items=items)\n\n                # create the object\n                mgr = BlockManager([block], [items] + levels)\n                obj = self.obj_type(mgr)\n\n                # permute if needed\n                if self.is_transposed:\n                    obj = obj.transpose(\n                        *tuple(Series(self.data_orientation).argsort()))\n\n                objs.append(obj)\n\n        else:\n            warnings.warn(duplicate_doc, DuplicateWarning, stacklevel=5)\n\n            # reconstruct\n            long_index = MultiIndex.from_arrays(\n                [i.values for i in self.index_axes])\n\n            for c in self.values_axes:\n                lp = DataFrame(c.data, index=long_index, columns=c.values)\n\n                # need a better algorithm\n                tuple_index = long_index.values\n\n                unique_tuples = unique(tuple_index)\n                unique_tuples = com.asarray_tuplesafe(unique_tuples)\n\n                indexer = match(unique_tuples, tuple_index)\n                indexer = ensure_platform_int(indexer)\n\n                new_index = long_index.take(indexer)\n                new_values = lp.values.take(indexer, axis=0)\n\n                lp = DataFrame(new_values, index=new_index, columns=lp.columns)\n                objs.append(lp.to_panel())\n\n        # create the composite object\n        if len(objs) == 1:\n            wp = objs[0]\n        else:\n            wp = concat(objs, axis=0, verify_integrity=False)._consolidate()\n\n        # apply the selection filters & axis orderings\n        wp = self.process_axes(wp, columns=columns)\n\n        return wp\n\n\nclass LegacyFrameTable(LegacyTable):\n\n    \"\"\" support the legacy frame table \"\"\"\n    pandas_kind = u'frame_table'\n    table_type = u'legacy_frame'\n    obj_type = Panel\n\n    def read(self, *args, **kwargs):\n        return super(LegacyFrameTable, self).read(*args, **kwargs)['value']\n\n\nclass LegacyPanelTable(LegacyTable):\n\n    \"\"\" support the legacy panel table \"\"\"\n    table_type = u'legacy_panel'\n    obj_type = Panel\n\n\nclass AppendableTable(LegacyTable):\n\n    \"\"\" suppor the new appendable table formats \"\"\"\n    _indexables = None\n    table_type = u'appendable'\n\n    def write(self, obj, axes=None, append=False, complib=None,\n              complevel=None, fletcher32=None, min_itemsize=None,\n              chunksize=None, expectedrows=None, dropna=False, **kwargs):\n\n        if not append and self.is_exists:\n            self._handle.remove_node(self.group, 'table')\n\n        # create the axes\n        self.create_axes(axes=axes, obj=obj, validate=append,\n                         min_itemsize=min_itemsize,\n                         **kwargs)\n\n        for a in self.axes:\n            a.validate(self, append)\n\n        if not self.is_exists:\n\n            # create the table\n            options = self.create_description(complib=complib,\n                                              complevel=complevel,\n                                              fletcher32=fletcher32,\n                                              expectedrows=expectedrows)\n\n            # set the table attributes\n            self.set_attrs()\n\n            # create the table\n            self._handle.create_table(self.group, **options)\n        else:\n            pass\n            # table = self.table\n\n        # update my info\n        self.set_info()\n\n        # validate the axes and set the kinds\n        for a in self.axes:\n            a.validate_and_set(self, append)\n\n        # add the rows\n        self.write_data(chunksize, dropna=dropna)\n\n    def write_data(self, chunksize, dropna=False):\n        \"\"\" we form the data into a 2-d including indexes,values,mask\n            write chunk-by-chunk \"\"\"\n\n        names = self.dtype.names\n        nrows = self.nrows_expected\n\n        # if dropna==True, then drop ALL nan rows\n        masks = []\n        if dropna:\n\n            for a in self.values_axes:\n\n                # figure the mask: only do if we can successfully process this\n                # column, otherwise ignore the mask\n                mask = isna(a.data).all(axis=0)\n                if isinstance(mask, np.ndarray):\n                    masks.append(mask.astype('u1', copy=False))\n\n        # consolidate masks\n        if len(masks):\n            mask = masks[0]\n            for m in masks[1:]:\n                mask = mask & m\n            mask = mask.ravel()\n        else:\n            mask = None\n\n        # broadcast the indexes if needed\n        indexes = [a.cvalues for a in self.index_axes]\n        nindexes = len(indexes)\n        bindexes = []\n        for i, idx in enumerate(indexes):\n\n            # broadcast to all other indexes except myself\n            if i > 0 and i < nindexes:\n                repeater = np.prod(\n                    [indexes[bi].shape[0] for bi in range(0, i)])\n                idx = np.tile(idx, repeater)\n\n            if i < nindexes - 1:\n                repeater = np.prod([indexes[bi].shape[0]\n                                    for bi in range(i + 1, nindexes)])\n                idx = np.repeat(idx, repeater)\n\n            bindexes.append(idx)\n\n        # transpose the values so first dimension is last\n        # reshape the values if needed\n        values = [a.take_data() for a in self.values_axes]\n        values = [v.transpose(np.roll(np.arange(v.ndim), v.ndim - 1))\n                  for v in values]\n        bvalues = []\n        for i, v in enumerate(values):\n            new_shape = (nrows,) + self.dtype[names[nindexes + i]].shape\n            bvalues.append(values[i].reshape(new_shape))\n\n        # write the chunks\n        if chunksize is None:\n            chunksize = 100000\n\n        rows = np.empty(min(chunksize, nrows), dtype=self.dtype)\n        chunks = int(nrows / chunksize) + 1\n        for i in range(chunks):\n            start_i = i * chunksize\n            end_i = min((i + 1) * chunksize, nrows)\n            if start_i >= end_i:\n                break\n\n            self.write_data_chunk(\n                rows,\n                indexes=[a[start_i:end_i] for a in bindexes],\n                mask=mask[start_i:end_i] if mask is not None else None,\n                values=[v[start_i:end_i] for v in bvalues])\n\n    def write_data_chunk(self, rows, indexes, mask, values):\n        \"\"\"\n        Parameters\n        ----------\n        rows : an empty memory space where we are putting the chunk\n        indexes : an array of the indexes\n        mask : an array of the masks\n        values : an array of the values\n        \"\"\"\n\n        # 0 len\n        for v in values:\n            if not np.prod(v.shape):\n                return\n\n        try:\n            nrows = indexes[0].shape[0]\n            if nrows != len(rows):\n                rows = np.empty(nrows, dtype=self.dtype)\n            names = self.dtype.names\n            nindexes = len(indexes)\n\n            # indexes\n            for i, idx in enumerate(indexes):\n                rows[names[i]] = idx\n\n            # values\n            for i, v in enumerate(values):\n                rows[names[i + nindexes]] = v\n\n            # mask\n            if mask is not None:\n                m = ~mask.ravel().astype(bool, copy=False)\n                if not m.all():\n                    rows = rows[m]\n\n        except Exception as detail:\n            raise Exception(\"cannot create row-data -> %s\" % detail)\n\n        try:\n            if len(rows):\n                self.table.append(rows)\n                self.table.flush()\n        except Exception as detail:\n            raise TypeError(\"tables cannot write this data -> %s\" % detail)\n\n    def delete(self, where=None, start=None, stop=None, **kwargs):\n\n        # delete all rows (and return the nrows)\n        if where is None or not len(where):\n            if start is None and stop is None:\n                nrows = self.nrows\n                self._handle.remove_node(self.group, recursive=True)\n            else:\n                # pytables<3.0 would remove a single row with stop=None\n                if stop is None:\n                    stop = self.nrows\n                nrows = self.table.remove_rows(start=start, stop=stop)\n                self.table.flush()\n            return nrows\n\n        # infer the data kind\n        if not self.infer_axes():\n            return None\n\n        # create the selection\n        table = self.table\n        self.selection = Selection(\n            self, where, start=start, stop=stop, **kwargs)\n        values = self.selection.select_coords()\n\n        # delete the rows in reverse order\n        sorted_series = Series(values).sort_values()\n        ln = len(sorted_series)\n\n        if ln:\n\n            # construct groups of consecutive rows\n            diff = sorted_series.diff()\n            groups = list(diff[diff > 1].index)\n\n            # 1 group\n            if not len(groups):\n                groups = [0]\n\n            # final element\n            if groups[-1] != ln:\n                groups.append(ln)\n\n            # initial element\n            if groups[0] != 0:\n                groups.insert(0, 0)\n\n            # we must remove in reverse order!\n            pg = groups.pop()\n            for g in reversed(groups):\n                rows = sorted_series.take(lrange(g, pg))\n                table.remove_rows(start=rows[rows.index[0]\n                                             ], stop=rows[rows.index[-1]] + 1)\n                pg = g\n\n            self.table.flush()\n\n        # return the number of rows removed\n        return ln\n\n\nclass AppendableFrameTable(AppendableTable):\n\n    \"\"\" suppor the new appendable table formats \"\"\"\n    pandas_kind = u'frame_table'\n    table_type = u'appendable_frame'\n    ndim = 2\n    obj_type = DataFrame\n\n    @property\n    def is_transposed(self):\n        return self.index_axes[0].axis == 1\n\n    def get_object(self, obj):\n        \"\"\" these are written transposed \"\"\"\n        if self.is_transposed:\n            obj = obj.T\n        return obj\n\n    def read(self, where=None, columns=None, **kwargs):\n\n        if not self.read_axes(where=where, **kwargs):\n            return None\n\n        info = (self.info.get(self.non_index_axes[0][0], dict())\n                if len(self.non_index_axes) else dict())\n        index = self.index_axes[0].values\n        frames = []\n        for a in self.values_axes:\n\n            # we could have a multi-index constructor here\n            # ensure_index doesn't recognized our list-of-tuples here\n            if info.get('type') == 'MultiIndex':\n                cols = MultiIndex.from_tuples(a.values)\n            else:\n                cols = Index(a.values)\n            names = info.get('names')\n            if names is not None:\n                cols.set_names(names, inplace=True)\n\n            if self.is_transposed:\n                values = a.cvalues\n                index_ = cols\n                cols_ = Index(index, name=getattr(index, 'name', None))\n            else:\n                values = a.cvalues.T\n                index_ = Index(index, name=getattr(index, 'name', None))\n                cols_ = cols\n\n            # if we have a DataIndexableCol, its shape will only be 1 dim\n            if values.ndim == 1 and isinstance(values, np.ndarray):\n                values = values.reshape((1, values.shape[0]))\n\n            block = make_block(values, placement=np.arange(len(cols_)))\n            mgr = BlockManager([block], [cols_, index_])\n            frames.append(DataFrame(mgr))\n\n        if len(frames) == 1:\n            df = frames[0]\n        else:\n            df = concat(frames, axis=1)\n\n        # apply the selection filters & axis orderings\n        df = self.process_axes(df, columns=columns)\n\n        return df\n\n\nclass AppendableSeriesTable(AppendableFrameTable):\n    \"\"\" support the new appendable table formats \"\"\"\n    pandas_kind = u'series_table'\n    table_type = u'appendable_series'\n    ndim = 2\n    obj_type = Series\n    storage_obj_type = DataFrame\n\n    @property\n    def is_transposed(self):\n        return False\n\n    def get_object(self, obj):\n        return obj\n\n    def write(self, obj, data_columns=None, **kwargs):\n        \"\"\" we are going to write this as a frame table \"\"\"\n        if not isinstance(obj, DataFrame):\n            name = obj.name or 'values'\n            obj = DataFrame({name: obj}, index=obj.index)\n            obj.columns = [name]\n        return super(AppendableSeriesTable, self).write(\n            obj=obj, data_columns=obj.columns.tolist(), **kwargs)\n\n    def read(self, columns=None, **kwargs):\n\n        is_multi_index = self.is_multi_index\n        if columns is not None and is_multi_index:\n            for n in self.levels:\n                if n not in columns:\n                    columns.insert(0, n)\n        s = super(AppendableSeriesTable, self).read(columns=columns, **kwargs)\n        if is_multi_index:\n            s.set_index(self.levels, inplace=True)\n\n        s = s.iloc[:, 0]\n\n        # remove the default name\n        if s.name == 'values':\n            s.name = None\n        return s\n\n\nclass AppendableMultiSeriesTable(AppendableSeriesTable):\n    \"\"\" support the new appendable table formats \"\"\"\n    pandas_kind = u'series_table'\n    table_type = u'appendable_multiseries'\n\n    def write(self, obj, **kwargs):\n        \"\"\" we are going to write this as a frame table \"\"\"\n        name = obj.name or 'values'\n        obj, self.levels = self.validate_multiindex(obj)\n        cols = list(self.levels)\n        cols.append(name)\n        obj.columns = cols\n        return super(AppendableMultiSeriesTable, self).write(obj=obj, **kwargs)\n\n\nclass GenericTable(AppendableFrameTable):\n    \"\"\" a table that read/writes the generic pytables table format \"\"\"\n    pandas_kind = u'frame_table'\n    table_type = u'generic_table'\n    ndim = 2\n    obj_type = DataFrame\n\n    @property\n    def pandas_type(self):\n        return self.pandas_kind\n\n    @property\n    def storable(self):\n        return getattr(self.group, 'table', None) or self.group\n\n    def get_attrs(self):\n        \"\"\" retrieve our attributes \"\"\"\n        self.non_index_axes = []\n        self.nan_rep = None\n        self.levels = []\n\n        self.index_axes = [a.infer(self)\n                           for a in self.indexables if a.is_an_indexable]\n        self.values_axes = [a.infer(self)\n                            for a in self.indexables if not a.is_an_indexable]\n        self.data_columns = [a.name for a in self.values_axes]\n\n    @property\n    def indexables(self):\n        \"\"\" create the indexables from the table description \"\"\"\n        if self._indexables is None:\n\n            d = self.description\n\n            # the index columns is just a simple index\n            self._indexables = [GenericIndexCol(name='index', axis=0)]\n\n            for i, n in enumerate(d._v_names):\n\n                dc = GenericDataIndexableCol(\n                    name=n, pos=i, values=[n], version=self.version)\n                self._indexables.append(dc)\n\n        return self._indexables\n\n    def write(self, **kwargs):\n        raise NotImplementedError(\"cannot write on an generic table\")\n\n\nclass AppendableMultiFrameTable(AppendableFrameTable):\n\n    \"\"\" a frame with a multi-index \"\"\"\n    table_type = u'appendable_multiframe'\n    obj_type = DataFrame\n    ndim = 2\n    _re_levels = re.compile(r\"^level_\\d+$\")\n\n    @property\n    def table_type_short(self):\n        return u'appendable_multi'\n\n    def write(self, obj, data_columns=None, **kwargs):\n        if data_columns is None:\n            data_columns = []\n        elif data_columns is True:\n            data_columns = obj.columns.tolist()\n        obj, self.levels = self.validate_multiindex(obj)\n        for n in self.levels:\n            if n not in data_columns:\n                data_columns.insert(0, n)\n        return super(AppendableMultiFrameTable, self).write(\n            obj=obj, data_columns=data_columns, **kwargs)\n\n    def read(self, **kwargs):\n\n        df = super(AppendableMultiFrameTable, self).read(**kwargs)\n        df = df.set_index(self.levels)\n\n        # remove names for 'level_%d'\n        df.index = df.index.set_names([\n            None if self._re_levels.search(l) else l for l in df.index.names\n        ])\n\n        return df\n\n\nclass AppendablePanelTable(AppendableTable):\n\n    \"\"\" suppor the new appendable table formats \"\"\"\n    table_type = u'appendable_panel'\n    ndim = 3\n    obj_type = Panel\n\n    def get_object(self, obj):\n        \"\"\" these are written transposed \"\"\"\n        if self.is_transposed:\n            obj = obj.transpose(*self.data_orientation)\n        return obj\n\n    @property\n    def is_transposed(self):\n        return self.data_orientation != tuple(range(self.ndim))\n\n\ndef _reindex_axis(obj, axis, labels, other=None):\n    ax = obj._get_axis(axis)\n    labels = ensure_index(labels)\n\n    # try not to reindex even if other is provided\n    # if it equals our current index\n    if other is not None:\n        other = ensure_index(other)\n    if (other is None or labels.equals(other)) and labels.equals(ax):\n        return obj\n\n    labels = ensure_index(labels.unique())\n    if other is not None:\n        labels = ensure_index(other.unique()) & labels\n    if not labels.equals(ax):\n        slicer = [slice(None, None)] * obj.ndim\n        slicer[axis] = labels\n        obj = obj.loc[tuple(slicer)]\n    return obj\n\n\ndef _get_info(info, name):\n    \"\"\" get/create the info for this name \"\"\"\n    try:\n        idx = info[name]\n    except KeyError:\n        idx = info[name] = dict()\n    return idx\n\n# tz to/from coercion\n\n\ndef _get_tz(tz):\n    \"\"\" for a tz-aware type, return an encoded zone \"\"\"\n    zone = timezones.get_timezone(tz)\n    if zone is None:\n        zone = tz.utcoffset().total_seconds()\n    return zone\n\n\ndef _set_tz(values, tz, preserve_UTC=False, coerce=False):\n    \"\"\"\n    coerce the values to a DatetimeIndex if tz is set\n    preserve the input shape if possible\n\n    Parameters\n    ----------\n    values : ndarray\n    tz : string/pickled tz object\n    preserve_UTC : boolean,\n        preserve the UTC of the result\n    coerce : if we do not have a passed timezone, coerce to M8[ns] ndarray\n    \"\"\"\n    if tz is not None:\n        name = getattr(values, 'name', None)\n        values = values.ravel()\n        tz = timezones.get_timezone(_ensure_decoded(tz))\n        values = DatetimeIndex(values, name=name)\n        if values.tz is None:\n            values = values.tz_localize('UTC').tz_convert(tz)\n        if preserve_UTC:\n            if tz == 'UTC':\n                values = list(values)\n    elif coerce:\n        values = np.asarray(values, dtype='M8[ns]')\n\n    return values\n\n\ndef _convert_index(index, encoding=None, errors='strict', format_type=None):\n    index_name = getattr(index, 'name', None)\n\n    if isinstance(index, DatetimeIndex):\n        converted = index.asi8\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\n                        freq=getattr(index, 'freq', None),\n                        tz=getattr(index, 'tz', None),\n                        index_name=index_name)\n    elif isinstance(index, TimedeltaIndex):\n        converted = index.asi8\n        return IndexCol(converted, 'timedelta64', _tables().Int64Col(),\n                        freq=getattr(index, 'freq', None),\n                        index_name=index_name)\n    elif isinstance(index, (Int64Index, PeriodIndex)):\n        atom = _tables().Int64Col()\n        # avoid to store ndarray of Period objects\n        return IndexCol(index._ndarray_values, 'integer', atom,\n                        freq=getattr(index, 'freq', None),\n                        index_name=index_name)\n\n    if isinstance(index, MultiIndex):\n        raise TypeError('MultiIndex not supported here!')\n\n    inferred_type = lib.infer_dtype(index)\n\n    values = np.asarray(index)\n\n    if inferred_type == 'datetime64':\n        converted = values.view('i8')\n        return IndexCol(converted, 'datetime64', _tables().Int64Col(),\n                        freq=getattr(index, 'freq', None),\n                        tz=getattr(index, 'tz', None),\n                        index_name=index_name)\n    elif inferred_type == 'timedelta64':\n        converted = values.view('i8')\n        return IndexCol(converted, 'timedelta64', _tables().Int64Col(),\n                        freq=getattr(index, 'freq', None),\n                        index_name=index_name)\n    elif inferred_type == 'datetime':\n        converted = np.asarray([(time.mktime(v.timetuple()) +\n                                 v.microsecond / 1E6) for v in values],\n                               dtype=np.float64)\n        return IndexCol(converted, 'datetime', _tables().Time64Col(),\n                        index_name=index_name)\n    elif inferred_type == 'date':\n        converted = np.asarray([v.toordinal() for v in values],\n                               dtype=np.int32)\n        return IndexCol(converted, 'date', _tables().Time32Col(),\n                        index_name=index_name)\n    elif inferred_type == 'string':\n        # atom = _tables().ObjectAtom()\n        # return np.asarray(values, dtype='O'), 'object', atom\n\n        converted = _convert_string_array(values, encoding, errors)\n        itemsize = converted.dtype.itemsize\n        return IndexCol(\n            converted, 'string', _tables().StringCol(itemsize),\n            itemsize=itemsize, index_name=index_name\n        )\n    elif inferred_type == 'unicode':\n        if format_type == 'fixed':\n            atom = _tables().ObjectAtom()\n            return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\n                            index_name=index_name)\n        raise TypeError(\n            \"[unicode] is not supported as a in index type for [{0}] formats\"\n            .format(format_type)\n        )\n\n    elif inferred_type == 'integer':\n        # take a guess for now, hope the values fit\n        atom = _tables().Int64Col()\n        return IndexCol(np.asarray(values, dtype=np.int64), 'integer', atom,\n                        index_name=index_name)\n    elif inferred_type == 'floating':\n        atom = _tables().Float64Col()\n        return IndexCol(np.asarray(values, dtype=np.float64), 'float', atom,\n                        index_name=index_name)\n    else:  # pragma: no cover\n        atom = _tables().ObjectAtom()\n        return IndexCol(np.asarray(values, dtype='O'), 'object', atom,\n                        index_name=index_name)\n\n\ndef _unconvert_index(data, kind, encoding=None, errors='strict'):\n    kind = _ensure_decoded(kind)\n    if kind == u'datetime64':\n        index = DatetimeIndex(data)\n    elif kind == u'timedelta64':\n        index = TimedeltaIndex(data)\n    elif kind == u'datetime':\n        index = np.asarray([datetime.fromtimestamp(v) for v in data],\n                           dtype=object)\n    elif kind == u'date':\n        try:\n            index = np.asarray(\n                [date.fromordinal(v) for v in data], dtype=object)\n        except (ValueError):\n            index = np.asarray(\n                [date.fromtimestamp(v) for v in data], dtype=object)\n    elif kind in (u'integer', u'float'):\n        index = np.asarray(data)\n    elif kind in (u'string'):\n        index = _unconvert_string_array(data, nan_rep=None, encoding=encoding,\n                                        errors=errors)\n    elif kind == u'object':\n        index = np.asarray(data[0])\n    else:  # pragma: no cover\n        raise ValueError('unrecognized index type %s' % kind)\n    return index\n\n\ndef _unconvert_index_legacy(data, kind, legacy=False, encoding=None,\n                            errors='strict'):\n    kind = _ensure_decoded(kind)\n    if kind == u'datetime':\n        index = to_datetime(data)\n    elif kind in (u'integer'):\n        index = np.asarray(data, dtype=object)\n    elif kind in (u'string'):\n        index = _unconvert_string_array(data, nan_rep=None, encoding=encoding,\n                                        errors=errors)\n    else:  # pragma: no cover\n        raise ValueError('unrecognized index type %s' % kind)\n    return index\n\n\ndef _convert_string_array(data, encoding, errors, itemsize=None):\n    \"\"\"\n    we take a string-like that is object dtype and coerce to a fixed size\n    string type\n\n    Parameters\n    ----------\n    data : a numpy array of object dtype\n    encoding : None or string-encoding\n    errors : handler for encoding errors\n    itemsize : integer, optional, defaults to the max length of the strings\n\n    Returns\n    -------\n    data in a fixed-length string dtype, encoded to bytes if needed\n    \"\"\"\n\n    # encode if needed\n    if encoding is not None and len(data):\n        data = Series(data.ravel()).str.encode(\n            encoding, errors).values.reshape(data.shape)\n\n    # create the sized dtype\n    if itemsize is None:\n        ensured = ensure_object(data.ravel())\n        itemsize = max(1, libwriters.max_len_string_array(ensured))\n\n    data = np.asarray(data, dtype=\"S%d\" % itemsize)\n    return data\n\n\ndef _unconvert_string_array(data, nan_rep=None, encoding=None,\n                            errors='strict'):\n    \"\"\"\n    inverse of _convert_string_array\n\n    Parameters\n    ----------\n    data : fixed length string dtyped array\n    nan_rep : the storage repr of NaN, optional\n    encoding : the encoding of the data, optional\n    errors : handler for encoding errors, default 'strict'\n\n    Returns\n    -------\n    an object array of the decoded data\n\n    \"\"\"\n    shape = data.shape\n    data = np.asarray(data.ravel(), dtype=object)\n\n    # guard against a None encoding in PY3 (because of a legacy\n    # where the passed encoding is actually None)\n    encoding = _ensure_encoding(encoding)\n    if encoding is not None and len(data):\n\n        itemsize = libwriters.max_len_string_array(ensure_object(data))\n        if compat.PY3:\n            dtype = \"U{0}\".format(itemsize)\n        else:\n            dtype = \"S{0}\".format(itemsize)\n\n        if isinstance(data[0], compat.binary_type):\n            data = Series(data).str.decode(encoding, errors=errors).values\n        else:\n            data = data.astype(dtype, copy=False).astype(object, copy=False)\n\n    if nan_rep is None:\n        nan_rep = 'nan'\n\n    data = libwriters.string_array_replace_from_nan_rep(data, nan_rep)\n    return data.reshape(shape)\n\n\ndef _maybe_convert(values, val_kind, encoding, errors):\n    if _need_convert(val_kind):\n        conv = _get_converter(val_kind, encoding, errors)\n        # conv = np.frompyfunc(conv, 1, 1)\n        values = conv(values)\n    return values\n\n\ndef _get_converter(kind, encoding, errors):\n    kind = _ensure_decoded(kind)\n    if kind == 'datetime64':\n        return lambda x: np.asarray(x, dtype='M8[ns]')\n    elif kind == 'datetime':\n        return lambda x: to_datetime(x, cache=True).to_pydatetime()\n    elif kind == 'string':\n        return lambda x: _unconvert_string_array(x, encoding=encoding,\n                                                 errors=errors)\n    else:  # pragma: no cover\n        raise ValueError('invalid kind %s' % kind)\n\n\ndef _need_convert(kind):\n    kind = _ensure_decoded(kind)\n    if kind in (u'datetime', u'datetime64', u'string'):\n        return True\n    return False\n\n\nclass Selection(object):\n\n    \"\"\"\n    Carries out a selection operation on a tables.Table object.\n\n    Parameters\n    ----------\n    table : a Table object\n    where : list of Terms (or convertible to)\n    start, stop: indices to start and/or stop selection\n\n    \"\"\"\n\n    def __init__(self, table, where=None, start=None, stop=None):\n        self.table = table\n        self.where = where\n        self.start = start\n        self.stop = stop\n        self.condition = None\n        self.filter = None\n        self.terms = None\n        self.coordinates = None\n\n        if is_list_like(where):\n\n            # see if we have a passed coordinate like\n            try:\n                inferred = lib.infer_dtype(where)\n                if inferred == 'integer' or inferred == 'boolean':\n                    where = np.asarray(where)\n                    if where.dtype == np.bool_:\n                        start, stop = self.start, self.stop\n                        if start is None:\n                            start = 0\n                        if stop is None:\n                            stop = self.table.nrows\n                        self.coordinates = np.arange(start, stop)[where]\n                    elif issubclass(where.dtype.type, np.integer):\n                        if ((self.start is not None and\n                                (where < self.start).any()) or\n                            (self.stop is not None and\n                                (where >= self.stop).any())):\n                            raise ValueError(\n                                \"where must have index locations >= start and \"\n                                \"< stop\"\n                            )\n                        self.coordinates = where\n\n            except ValueError:\n                pass\n\n        if self.coordinates is None:\n\n            self.terms = self.generate(where)\n\n            # create the numexpr & the filter\n            if self.terms is not None:\n                self.condition, self.filter = self.terms.evaluate()\n\n    def generate(self, where):\n        \"\"\" where can be a : dict,list,tuple,string \"\"\"\n        if where is None:\n            return None\n\n        q = self.table.queryables()\n        try:\n            return Expr(where, queryables=q, encoding=self.table.encoding)\n        except NameError:\n            # raise a nice message, suggesting that the user should use\n            # data_columns\n            raise ValueError(\n                \"The passed where expression: {0}\\n\"\n                \"            contains an invalid variable reference\\n\"\n                \"            all of the variable references must be a \"\n                \"reference to\\n\"\n                \"            an axis (e.g. 'index' or 'columns'), or a \"\n                \"data_column\\n\"\n                \"            The currently defined references are: {1}\\n\"\n                .format(where, ','.join(q.keys()))\n            )\n\n    def select(self):\n        \"\"\"\n        generate the selection\n        \"\"\"\n        if self.condition is not None:\n            return self.table.table.read_where(self.condition.format(),\n                                               start=self.start,\n                                               stop=self.stop)\n        elif self.coordinates is not None:\n            return self.table.table.read_coordinates(self.coordinates)\n        return self.table.table.read(start=self.start, stop=self.stop)\n\n    def select_coords(self):\n        \"\"\"\n        generate the selection\n        \"\"\"\n        start, stop = self.start, self.stop\n        nrows = self.table.nrows\n        if start is None:\n            start = 0\n        elif start < 0:\n            start += nrows\n        if self.stop is None:\n            stop = nrows\n        elif stop < 0:\n            stop += nrows\n\n        if self.condition is not None:\n            return self.table.table.get_where_list(self.condition.format(),\n                                                   start=start, stop=stop,\n                                                   sort=True)\n        elif self.coordinates is not None:\n            return self.coordinates\n\n        return np.arange(start, stop)\n\n# utilities ###\n\n\ndef timeit(key, df, fn=None, remove=True, **kwargs):\n    if fn is None:\n        fn = 'timeit.h5'\n    store = HDFStore(fn, mode='w')\n    store.append(key, df, **kwargs)\n    store.close()\n\n    if remove:\n        os.remove(fn)\n"
    },
    {
      "filename": "pandas/io/sql.py",
      "content": "# -*- coding: utf-8 -*-\n\"\"\"\nCollection of query wrappers / abstractions to both facilitate data\nretrieval and to reduce dependency on DB-specific API.\n\"\"\"\n\nfrom __future__ import division, print_function\n\nfrom contextlib import contextmanager\nfrom datetime import date, datetime, time\nimport re\nimport warnings\n\nimport numpy as np\n\nimport pandas._libs.lib as lib\nfrom pandas.compat import (\n    map, raise_with_traceback, string_types, text_type, zip)\n\nfrom pandas.core.dtypes.common import (\n    is_datetime64tz_dtype, is_dict_like, is_list_like)\nfrom pandas.core.dtypes.dtypes import DatetimeTZDtype\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core.api import DataFrame, Series\nfrom pandas.core.base import PandasObject\nfrom pandas.core.tools.datetimes import to_datetime\n\n\nclass SQLAlchemyRequired(ImportError):\n    pass\n\n\nclass DatabaseError(IOError):\n    pass\n\n\n# -----------------------------------------------------------------------------\n# -- Helper functions\n\n_SQLALCHEMY_INSTALLED = None\n\n\ndef _is_sqlalchemy_connectable(con):\n    global _SQLALCHEMY_INSTALLED\n    if _SQLALCHEMY_INSTALLED is None:\n        try:\n            import sqlalchemy\n            _SQLALCHEMY_INSTALLED = True\n\n            from distutils.version import LooseVersion\n            ver = sqlalchemy.__version__\n            # For sqlalchemy versions < 0.8.2, the BIGINT type is recognized\n            # for a sqlite engine, which results in a warning when trying to\n            # read/write a DataFrame with int64 values. (GH7433)\n            if LooseVersion(ver) < LooseVersion('0.8.2'):\n                from sqlalchemy import BigInteger\n                from sqlalchemy.ext.compiler import compiles\n\n                @compiles(BigInteger, 'sqlite')\n                def compile_big_int_sqlite(type_, compiler, **kw):\n                    return 'INTEGER'\n        except ImportError:\n            _SQLALCHEMY_INSTALLED = False\n\n    if _SQLALCHEMY_INSTALLED:\n        import sqlalchemy\n        return isinstance(con, sqlalchemy.engine.Connectable)\n    else:\n        return False\n\n\ndef _convert_params(sql, params):\n    \"\"\"Convert SQL and params args to DBAPI2.0 compliant format.\"\"\"\n    args = [sql]\n    if params is not None:\n        if hasattr(params, 'keys'):  # test if params is a mapping\n            args += [params]\n        else:\n            args += [list(params)]\n    return args\n\n\ndef _handle_date_column(col, utc=None, format=None):\n    if isinstance(format, dict):\n        return to_datetime(col, errors='ignore', **format)\n    else:\n        # Allow passing of formatting string for integers\n        # GH17855\n        if format is None and (issubclass(col.dtype.type, np.floating) or\n                               issubclass(col.dtype.type, np.integer)):\n            format = 's'\n        if format in ['D', 'd', 'h', 'm', 's', 'ms', 'us', 'ns']:\n            return to_datetime(col, errors='coerce', unit=format, utc=utc)\n        elif is_datetime64tz_dtype(col):\n            # coerce to UTC timezone\n            # GH11216\n            return (to_datetime(col, errors='coerce')\n                    .astype('datetime64[ns, UTC]'))\n        else:\n            return to_datetime(col, errors='coerce', format=format, utc=utc)\n\n\ndef _parse_date_columns(data_frame, parse_dates):\n    \"\"\"\n    Force non-datetime columns to be read as such.\n    Supports both string formatted and integer timestamp columns.\n    \"\"\"\n    # handle non-list entries for parse_dates gracefully\n    if parse_dates is True or parse_dates is None or parse_dates is False:\n        parse_dates = []\n\n    if not hasattr(parse_dates, '__iter__'):\n        parse_dates = [parse_dates]\n\n    for col_name in parse_dates:\n        df_col = data_frame[col_name]\n        try:\n            fmt = parse_dates[col_name]\n        except TypeError:\n            fmt = None\n        data_frame[col_name] = _handle_date_column(df_col, format=fmt)\n\n    # we want to coerce datetime64_tz dtypes for now\n    # we could in theory do a 'nice' conversion from a FixedOffset tz\n    # GH11216\n    for col_name, df_col in data_frame.iteritems():\n        if is_datetime64tz_dtype(df_col):\n            data_frame[col_name] = _handle_date_column(df_col)\n\n    return data_frame\n\n\ndef _wrap_result(data, columns, index_col=None, coerce_float=True,\n                 parse_dates=None):\n    \"\"\"Wrap result set of query in a DataFrame.\"\"\"\n\n    frame = DataFrame.from_records(data, columns=columns,\n                                   coerce_float=coerce_float)\n\n    _parse_date_columns(frame, parse_dates)\n\n    if index_col is not None:\n        frame.set_index(index_col, inplace=True)\n\n    return frame\n\n\ndef execute(sql, con, cur=None, params=None):\n    \"\"\"\n    Execute the given SQL query using the provided connection object.\n\n    Parameters\n    ----------\n    sql : string\n        SQL query to be executed.\n    con : SQLAlchemy connectable(engine/connection) or sqlite3 connection\n        Using SQLAlchemy makes it possible to use any DB supported by the\n        library.\n        If a DBAPI2 object, only sqlite3 is supported.\n    cur : deprecated, cursor is obtained from connection, default: None\n    params : list or tuple, optional, default: None\n        List of parameters to pass to execute method.\n\n    Returns\n    -------\n    Results Iterable\n    \"\"\"\n    if cur is None:\n        pandas_sql = pandasSQL_builder(con)\n    else:\n        pandas_sql = pandasSQL_builder(cur, is_cursor=True)\n    args = _convert_params(sql, params)\n    return pandas_sql.execute(*args)\n\n\n# -----------------------------------------------------------------------------\n# -- Read and write to DataFrames\n\ndef read_sql_table(table_name, con, schema=None, index_col=None,\n                   coerce_float=True, parse_dates=None, columns=None,\n                   chunksize=None):\n    \"\"\"Read SQL database table into a DataFrame.\n\n    Given a table name and a SQLAlchemy connectable, returns a DataFrame.\n    This function does not support DBAPI connections.\n\n    Parameters\n    ----------\n    table_name : string\n        Name of SQL table in database.\n    con : SQLAlchemy connectable (or database string URI)\n        SQLite DBAPI connection mode not supported.\n    schema : string, default None\n        Name of SQL schema in database to query (if database flavor\n        supports this). Uses default schema if None (default).\n    index_col : string or list of strings, optional, default: None\n        Column(s) to set as index(MultiIndex).\n    coerce_float : boolean, default True\n        Attempts to convert values of non-string, non-numeric objects (like\n        decimal.Decimal) to floating point. Can result in loss of Precision.\n    parse_dates : list or dict, default: None\n        - List of column names to parse as dates.\n        - Dict of ``{column_name: format string}`` where format string is\n          strftime compatible in case of parsing string times or is one of\n          (D, s, ns, ms, us) in case of parsing integer timestamps.\n        - Dict of ``{column_name: arg dict}``, where the arg dict corresponds\n          to the keyword arguments of :func:`pandas.to_datetime`\n          Especially useful with databases without native Datetime support,\n          such as SQLite.\n    columns : list, default: None\n        List of column names to select from SQL table\n    chunksize : int, default None\n        If specified, returns an iterator where `chunksize` is the number of\n        rows to include in each chunk.\n\n    Returns\n    -------\n    DataFrame\n\n    Notes\n    -----\n    Any datetime values with time zone information will be converted to UTC.\n\n    See Also\n    --------\n    read_sql_query : Read SQL query into a DataFrame.\n    read_sql\n    \"\"\"\n\n    con = _engine_builder(con)\n    if not _is_sqlalchemy_connectable(con):\n        raise NotImplementedError(\"read_sql_table only supported for \"\n                                  \"SQLAlchemy connectable.\")\n    import sqlalchemy\n    from sqlalchemy.schema import MetaData\n    meta = MetaData(con, schema=schema)\n    try:\n        meta.reflect(only=[table_name], views=True)\n    except sqlalchemy.exc.InvalidRequestError:\n        raise ValueError(\"Table %s not found\" % table_name)\n\n    pandas_sql = SQLDatabase(con, meta=meta)\n    table = pandas_sql.read_table(\n        table_name, index_col=index_col, coerce_float=coerce_float,\n        parse_dates=parse_dates, columns=columns, chunksize=chunksize)\n\n    if table is not None:\n        return table\n    else:\n        raise ValueError(\"Table %s not found\" % table_name, con)\n\n\ndef read_sql_query(sql, con, index_col=None, coerce_float=True, params=None,\n                   parse_dates=None, chunksize=None):\n    \"\"\"Read SQL query into a DataFrame.\n\n    Returns a DataFrame corresponding to the result set of the query\n    string. Optionally provide an `index_col` parameter to use one of the\n    columns as the index, otherwise default integer index will be used.\n\n    Parameters\n    ----------\n    sql : string SQL query or SQLAlchemy Selectable (select or text object)\n        SQL query to be executed.\n    con : SQLAlchemy connectable(engine/connection), database string URI,\n        or sqlite3 DBAPI2 connection\n        Using SQLAlchemy makes it possible to use any DB supported by that\n        library.\n        If a DBAPI2 object, only sqlite3 is supported.\n    index_col : string or list of strings, optional, default: None\n        Column(s) to set as index(MultiIndex).\n    coerce_float : boolean, default True\n        Attempts to convert values of non-string, non-numeric objects (like\n        decimal.Decimal) to floating point. Useful for SQL result sets.\n    params : list, tuple or dict, optional, default: None\n        List of parameters to pass to execute method.  The syntax used\n        to pass parameters is database driver dependent. Check your\n        database driver documentation for which of the five syntax styles,\n        described in PEP 249's paramstyle, is supported.\n        Eg. for psycopg2, uses %(name)s so use params={'name' : 'value'}\n    parse_dates : list or dict, default: None\n        - List of column names to parse as dates.\n        - Dict of ``{column_name: format string}`` where format string is\n          strftime compatible in case of parsing string times, or is one of\n          (D, s, ns, ms, us) in case of parsing integer timestamps.\n        - Dict of ``{column_name: arg dict}``, where the arg dict corresponds\n          to the keyword arguments of :func:`pandas.to_datetime`\n          Especially useful with databases without native Datetime support,\n          such as SQLite.\n    chunksize : int, default None\n        If specified, return an iterator where `chunksize` is the number of\n        rows to include in each chunk.\n\n    Returns\n    -------\n    DataFrame\n\n    Notes\n    -----\n    Any datetime values with time zone information parsed via the `parse_dates`\n    parameter will be converted to UTC.\n\n    See Also\n    --------\n    read_sql_table : Read SQL database table into a DataFrame.\n    read_sql\n    \"\"\"\n    pandas_sql = pandasSQL_builder(con)\n    return pandas_sql.read_query(\n        sql, index_col=index_col, params=params, coerce_float=coerce_float,\n        parse_dates=parse_dates, chunksize=chunksize)\n\n\ndef read_sql(sql, con, index_col=None, coerce_float=True, params=None,\n             parse_dates=None, columns=None, chunksize=None):\n    \"\"\"\n    Read SQL query or database table into a DataFrame.\n\n    This function is a convenience wrapper around ``read_sql_table`` and\n    ``read_sql_query`` (for backward compatibility). It will delegate\n    to the specific function depending on the provided input. A SQL query\n    will be routed to ``read_sql_query``, while a database table name will\n    be routed to ``read_sql_table``. Note that the delegated function might\n    have more specific notes about their functionality not listed here.\n\n    Parameters\n    ----------\n    sql : string or SQLAlchemy Selectable (select or text object)\n        SQL query to be executed or a table name.\n    con : SQLAlchemy connectable (engine/connection) or database string URI\n        or DBAPI2 connection (fallback mode)\n\n        Using SQLAlchemy makes it possible to use any DB supported by that\n        library. If a DBAPI2 object, only sqlite3 is supported.\n    index_col : string or list of strings, optional, default: None\n        Column(s) to set as index(MultiIndex).\n    coerce_float : boolean, default True\n        Attempts to convert values of non-string, non-numeric objects (like\n        decimal.Decimal) to floating point, useful for SQL result sets.\n    params : list, tuple or dict, optional, default: None\n        List of parameters to pass to execute method.  The syntax used\n        to pass parameters is database driver dependent. Check your\n        database driver documentation for which of the five syntax styles,\n        described in PEP 249's paramstyle, is supported.\n        Eg. for psycopg2, uses %(name)s so use params={'name' : 'value'}\n    parse_dates : list or dict, default: None\n        - List of column names to parse as dates.\n        - Dict of ``{column_name: format string}`` where format string is\n          strftime compatible in case of parsing string times, or is one of\n          (D, s, ns, ms, us) in case of parsing integer timestamps.\n        - Dict of ``{column_name: arg dict}``, where the arg dict corresponds\n          to the keyword arguments of :func:`pandas.to_datetime`\n          Especially useful with databases without native Datetime support,\n          such as SQLite.\n    columns : list, default: None\n        List of column names to select from SQL table (only used when reading\n        a table).\n    chunksize : int, default None\n        If specified, return an iterator where `chunksize` is the\n        number of rows to include in each chunk.\n\n    Returns\n    -------\n    DataFrame\n\n    See Also\n    --------\n    read_sql_table : Read SQL database table into a DataFrame.\n    read_sql_query : Read SQL query into a DataFrame.\n    \"\"\"\n    pandas_sql = pandasSQL_builder(con)\n\n    if isinstance(pandas_sql, SQLiteDatabase):\n        return pandas_sql.read_query(\n            sql, index_col=index_col, params=params,\n            coerce_float=coerce_float, parse_dates=parse_dates,\n            chunksize=chunksize)\n\n    try:\n        _is_table_name = pandas_sql.has_table(sql)\n    except (ImportError, AttributeError):\n        _is_table_name = False\n\n    if _is_table_name:\n        pandas_sql.meta.reflect(only=[sql])\n        return pandas_sql.read_table(\n            sql, index_col=index_col, coerce_float=coerce_float,\n            parse_dates=parse_dates, columns=columns, chunksize=chunksize)\n    else:\n        return pandas_sql.read_query(\n            sql, index_col=index_col, params=params,\n            coerce_float=coerce_float, parse_dates=parse_dates,\n            chunksize=chunksize)\n\n\ndef to_sql(frame, name, con, schema=None, if_exists='fail', index=True,\n           index_label=None, chunksize=None, dtype=None):\n    \"\"\"\n    Write records stored in a DataFrame to a SQL database.\n\n    Parameters\n    ----------\n    frame : DataFrame, Series\n    name : string\n        Name of SQL table.\n    con : SQLAlchemy connectable(engine/connection) or database string URI\n        or sqlite3 DBAPI2 connection\n        Using SQLAlchemy makes it possible to use any DB supported by that\n        library.\n        If a DBAPI2 object, only sqlite3 is supported.\n    schema : string, default None\n        Name of SQL schema in database to write to (if database flavor\n        supports this). If None, use default schema (default).\n    if_exists : {'fail', 'replace', 'append'}, default 'fail'\n        - fail: If table exists, do nothing.\n        - replace: If table exists, drop it, recreate it, and insert data.\n        - append: If table exists, insert data. Create if does not exist.\n    index : boolean, default True\n        Write DataFrame index as a column.\n    index_label : string or sequence, default None\n        Column label for index column(s). If None is given (default) and\n        `index` is True, then the index names are used.\n        A sequence should be given if the DataFrame uses MultiIndex.\n    chunksize : int, default None\n        If not None, then rows will be written in batches of this size at a\n        time.  If None, all rows will be written at once.\n    dtype : single SQLtype or dict of column name to SQL type, default None\n        Optional specifying the datatype for columns. The SQL type should\n        be a SQLAlchemy type, or a string for sqlite3 fallback connection.\n        If all columns are of the same type, one single value can be used.\n    \"\"\"\n    if if_exists not in ('fail', 'replace', 'append'):\n        raise ValueError(\"'{0}' is not valid for if_exists\".format(if_exists))\n\n    pandas_sql = pandasSQL_builder(con, schema=schema)\n\n    if isinstance(frame, Series):\n        frame = frame.to_frame()\n    elif not isinstance(frame, DataFrame):\n        raise NotImplementedError(\"'frame' argument should be either a \"\n                                  \"Series or a DataFrame\")\n\n    pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index,\n                      index_label=index_label, schema=schema,\n                      chunksize=chunksize, dtype=dtype)\n\n\ndef has_table(table_name, con, schema=None):\n    \"\"\"\n    Check if DataBase has named table.\n\n    Parameters\n    ----------\n    table_name: string\n        Name of SQL table.\n    con: SQLAlchemy connectable(engine/connection) or sqlite3 DBAPI2 connection\n        Using SQLAlchemy makes it possible to use any DB supported by that\n        library.\n        If a DBAPI2 object, only sqlite3 is supported.\n    schema : string, default None\n        Name of SQL schema in database to write to (if database flavor supports\n        this). If None, use default schema (default).\n\n    Returns\n    -------\n    boolean\n    \"\"\"\n    pandas_sql = pandasSQL_builder(con, schema=schema)\n    return pandas_sql.has_table(table_name)\n\n\ntable_exists = has_table\n\n\ndef _engine_builder(con):\n    \"\"\"\n    Returns a SQLAlchemy engine from a URI (if con is a string)\n    else it just return con without modifying it.\n    \"\"\"\n    global _SQLALCHEMY_INSTALLED\n    if isinstance(con, string_types):\n        try:\n            import sqlalchemy\n        except ImportError:\n            _SQLALCHEMY_INSTALLED = False\n        else:\n            con = sqlalchemy.create_engine(con)\n            return con\n\n    return con\n\n\ndef pandasSQL_builder(con, schema=None, meta=None,\n                      is_cursor=False):\n    \"\"\"\n    Convenience function to return the correct PandasSQL subclass based on the\n    provided parameters.\n    \"\"\"\n    # When support for DBAPI connections is removed,\n    # is_cursor should not be necessary.\n    con = _engine_builder(con)\n    if _is_sqlalchemy_connectable(con):\n        return SQLDatabase(con, schema=schema, meta=meta)\n    elif isinstance(con, string_types):\n        raise ImportError(\"Using URI string without sqlalchemy installed.\")\n    else:\n        return SQLiteDatabase(con, is_cursor=is_cursor)\n\n\nclass SQLTable(PandasObject):\n    \"\"\"\n    For mapping Pandas tables to SQL tables.\n    Uses fact that table is reflected by SQLAlchemy to\n    do better type conversions.\n    Also holds various flags needed to avoid having to\n    pass them between functions all the time.\n    \"\"\"\n    # TODO: support for multiIndex\n\n    def __init__(self, name, pandas_sql_engine, frame=None, index=True,\n                 if_exists='fail', prefix='pandas', index_label=None,\n                 schema=None, keys=None, dtype=None):\n        self.name = name\n        self.pd_sql = pandas_sql_engine\n        self.prefix = prefix\n        self.frame = frame\n        self.index = self._index_name(index, index_label)\n        self.schema = schema\n        self.if_exists = if_exists\n        self.keys = keys\n        self.dtype = dtype\n\n        if frame is not None:\n            # We want to initialize based on a dataframe\n            self.table = self._create_table_setup()\n        else:\n            # no data provided, read-only mode\n            self.table = self.pd_sql.get_table(self.name, self.schema)\n\n        if self.table is None:\n            raise ValueError(\"Could not init table '%s'\" % name)\n\n    def exists(self):\n        return self.pd_sql.has_table(self.name, self.schema)\n\n    def sql_schema(self):\n        from sqlalchemy.schema import CreateTable\n        return str(CreateTable(self.table).compile(self.pd_sql.connectable))\n\n    def _execute_create(self):\n        # Inserting table into database, add to MetaData object\n        self.table = self.table.tometadata(self.pd_sql.meta)\n        self.table.create()\n\n    def create(self):\n        if self.exists():\n            if self.if_exists == 'fail':\n                raise ValueError(\"Table '%s' already exists.\" % self.name)\n            elif self.if_exists == 'replace':\n                self.pd_sql.drop_table(self.name, self.schema)\n                self._execute_create()\n            elif self.if_exists == 'append':\n                pass\n            else:\n                raise ValueError(\n                    \"'{0}' is not valid for if_exists\".format(self.if_exists))\n        else:\n            self._execute_create()\n\n    def insert_statement(self):\n        return self.table.insert()\n\n    def insert_data(self):\n        if self.index is not None:\n            temp = self.frame.copy()\n            temp.index.names = self.index\n            try:\n                temp.reset_index(inplace=True)\n            except ValueError as err:\n                raise ValueError(\n                    \"duplicate name in index/columns: {0}\".format(err))\n        else:\n            temp = self.frame\n\n        column_names = list(map(text_type, temp.columns))\n        ncols = len(column_names)\n        data_list = [None] * ncols\n        blocks = temp._data.blocks\n\n        for b in blocks:\n            if b.is_datetime:\n                # return datetime.datetime objects\n                if b.is_datetimetz:\n                    # GH 9086: Ensure we return datetimes with timezone info\n                    # Need to return 2-D data; DatetimeIndex is 1D\n                    d = b.values.to_pydatetime()\n                    d = np.expand_dims(d, axis=0)\n                else:\n                    # convert to microsecond resolution for datetime.datetime\n                    d = b.values.astype('M8[us]').astype(object)\n            else:\n                d = np.array(b.get_values(), dtype=object)\n\n            # replace NaN with None\n            if b._can_hold_na:\n                mask = isna(d)\n                d[mask] = None\n\n            for col_loc, col in zip(b.mgr_locs, d):\n                data_list[col_loc] = col\n\n        return column_names, data_list\n\n    def _execute_insert(self, conn, keys, data_iter):\n        data = [dict(zip(keys, row)) for row in data_iter]\n        conn.execute(self.insert_statement(), data)\n\n    def insert(self, chunksize=None):\n        keys, data_list = self.insert_data()\n\n        nrows = len(self.frame)\n\n        if nrows == 0:\n            return\n\n        if chunksize is None:\n            chunksize = nrows\n        elif chunksize == 0:\n            raise ValueError('chunksize argument should be non-zero')\n\n        chunks = int(nrows / chunksize) + 1\n\n        with self.pd_sql.run_transaction() as conn:\n            for i in range(chunks):\n                start_i = i * chunksize\n                end_i = min((i + 1) * chunksize, nrows)\n                if start_i >= end_i:\n                    break\n\n                chunk_iter = zip(*[arr[start_i:end_i] for arr in data_list])\n                self._execute_insert(conn, keys, chunk_iter)\n\n    def _query_iterator(self, result, chunksize, columns, coerce_float=True,\n                        parse_dates=None):\n        \"\"\"Return generator through chunked result set.\"\"\"\n\n        while True:\n            data = result.fetchmany(chunksize)\n            if not data:\n                break\n            else:\n                self.frame = DataFrame.from_records(\n                    data, columns=columns, coerce_float=coerce_float)\n\n                self._harmonize_columns(parse_dates=parse_dates)\n\n                if self.index is not None:\n                    self.frame.set_index(self.index, inplace=True)\n\n                yield self.frame\n\n    def read(self, coerce_float=True, parse_dates=None, columns=None,\n             chunksize=None):\n\n        if columns is not None and len(columns) > 0:\n            from sqlalchemy import select\n            cols = [self.table.c[n] for n in columns]\n            if self.index is not None:\n                [cols.insert(0, self.table.c[idx]) for idx in self.index[::-1]]\n            sql_select = select(cols)\n        else:\n            sql_select = self.table.select()\n\n        result = self.pd_sql.execute(sql_select)\n        column_names = result.keys()\n\n        if chunksize is not None:\n            return self._query_iterator(result, chunksize, column_names,\n                                        coerce_float=coerce_float,\n                                        parse_dates=parse_dates)\n        else:\n            data = result.fetchall()\n            self.frame = DataFrame.from_records(\n                data, columns=column_names, coerce_float=coerce_float)\n\n            self._harmonize_columns(parse_dates=parse_dates)\n\n            if self.index is not None:\n                self.frame.set_index(self.index, inplace=True)\n\n            return self.frame\n\n    def _index_name(self, index, index_label):\n        # for writing: index=True to include index in sql table\n        if index is True:\n            nlevels = self.frame.index.nlevels\n            # if index_label is specified, set this as index name(s)\n            if index_label is not None:\n                if not isinstance(index_label, list):\n                    index_label = [index_label]\n                if len(index_label) != nlevels:\n                    raise ValueError(\n                        \"Length of 'index_label' should match number of \"\n                        \"levels, which is {0}\".format(nlevels))\n                else:\n                    return index_label\n            # return the used column labels for the index columns\n            if (nlevels == 1 and 'index' not in self.frame.columns and\n                    self.frame.index.name is None):\n                return ['index']\n            else:\n                return [l if l is not None else \"level_{0}\".format(i)\n                        for i, l in enumerate(self.frame.index.names)]\n\n        # for reading: index=(list of) string to specify column to set as index\n        elif isinstance(index, string_types):\n            return [index]\n        elif isinstance(index, list):\n            return index\n        else:\n            return None\n\n    def _get_column_names_and_types(self, dtype_mapper):\n        column_names_and_types = []\n        if self.index is not None:\n            for i, idx_label in enumerate(self.index):\n                idx_type = dtype_mapper(\n                    self.frame.index._get_level_values(i))\n                column_names_and_types.append((text_type(idx_label),\n                                              idx_type, True))\n\n        column_names_and_types += [\n            (text_type(self.frame.columns[i]),\n             dtype_mapper(self.frame.iloc[:, i]),\n             False)\n            for i in range(len(self.frame.columns))\n        ]\n\n        return column_names_and_types\n\n    def _create_table_setup(self):\n        from sqlalchemy import Table, Column, PrimaryKeyConstraint\n\n        column_names_and_types = self._get_column_names_and_types(\n            self._sqlalchemy_type\n        )\n\n        columns = [Column(name, typ, index=is_index)\n                   for name, typ, is_index in column_names_and_types]\n\n        if self.keys is not None:\n            if not is_list_like(self.keys):\n                keys = [self.keys]\n            else:\n                keys = self.keys\n            pkc = PrimaryKeyConstraint(*keys, name=self.name + '_pk')\n            columns.append(pkc)\n\n        schema = self.schema or self.pd_sql.meta.schema\n\n        # At this point, attach to new metadata, only attach to self.meta\n        # once table is created.\n        from sqlalchemy.schema import MetaData\n        meta = MetaData(self.pd_sql, schema=schema)\n\n        return Table(self.name, meta, *columns, schema=schema)\n\n    def _harmonize_columns(self, parse_dates=None):\n        \"\"\"\n        Make the DataFrame's column types align with the SQL table\n        column types.\n        Need to work around limited NA value support. Floats are always\n        fine, ints must always be floats if there are Null values.\n        Booleans are hard because converting bool column with None replaces\n        all Nones with false. Therefore only convert bool if there are no\n        NA values.\n        Datetimes should already be converted to np.datetime64 if supported,\n        but here we also force conversion if required.\n        \"\"\"\n        # handle non-list entries for parse_dates gracefully\n        if parse_dates is True or parse_dates is None or parse_dates is False:\n            parse_dates = []\n\n        if not hasattr(parse_dates, '__iter__'):\n            parse_dates = [parse_dates]\n\n        for sql_col in self.table.columns:\n            col_name = sql_col.name\n            try:\n                df_col = self.frame[col_name]\n                # the type the dataframe column should have\n                col_type = self._get_dtype(sql_col.type)\n\n                if (col_type is datetime or col_type is date or\n                        col_type is DatetimeTZDtype):\n                    # Convert tz-aware Datetime SQL columns to UTC\n                    utc = col_type is DatetimeTZDtype\n                    self.frame[col_name] = _handle_date_column(df_col, utc=utc)\n                elif col_type is float:\n                    # floats support NA, can always convert!\n                    self.frame[col_name] = df_col.astype(col_type, copy=False)\n\n                elif len(df_col) == df_col.count():\n                    # No NA values, can convert ints and bools\n                    if col_type is np.dtype('int64') or col_type is bool:\n                        self.frame[col_name] = df_col.astype(\n                            col_type, copy=False)\n\n                # Handle date parsing\n                if col_name in parse_dates:\n                    try:\n                        fmt = parse_dates[col_name]\n                    except TypeError:\n                        fmt = None\n                    self.frame[col_name] = _handle_date_column(\n                        df_col, format=fmt)\n\n            except KeyError:\n                pass  # this column not in results\n\n    def _get_notna_col_dtype(self, col):\n        \"\"\"\n        Infer datatype of the Series col.  In case the dtype of col is 'object'\n        and it contains NA values, this infers the datatype of the not-NA\n        values.  Needed for inserting typed data containing NULLs, GH8778.\n        \"\"\"\n        col_for_inference = col\n        if col.dtype == 'object':\n            notnadata = col[~isna(col)]\n            if len(notnadata):\n                col_for_inference = notnadata\n\n        return lib.infer_dtype(col_for_inference)\n\n    def _sqlalchemy_type(self, col):\n\n        dtype = self.dtype or {}\n        if col.name in dtype:\n            return self.dtype[col.name]\n\n        col_type = self._get_notna_col_dtype(col)\n\n        from sqlalchemy.types import (BigInteger, Integer, Float,\n                                      Text, Boolean,\n                                      DateTime, Date, Time, TIMESTAMP)\n\n        if col_type == 'datetime64' or col_type == 'datetime':\n            # GH 9086: TIMESTAMP is the suggested type if the column contains\n            # timezone information\n            try:\n                if col.dt.tz is not None:\n                    return TIMESTAMP(timezone=True)\n            except AttributeError:\n                # The column is actually a DatetimeIndex\n                if col.tz is not None:\n                    return TIMESTAMP(timezone=True)\n            return DateTime\n        if col_type == 'timedelta64':\n            warnings.warn(\"the 'timedelta' type is not supported, and will be \"\n                          \"written as integer values (ns frequency) to the \"\n                          \"database.\", UserWarning, stacklevel=8)\n            return BigInteger\n        elif col_type == 'floating':\n            if col.dtype == 'float32':\n                return Float(precision=23)\n            else:\n                return Float(precision=53)\n        elif col_type == 'integer':\n            if col.dtype == 'int32':\n                return Integer\n            else:\n                return BigInteger\n        elif col_type == 'boolean':\n            return Boolean\n        elif col_type == 'date':\n            return Date\n        elif col_type == 'time':\n            return Time\n        elif col_type == 'complex':\n            raise ValueError('Complex datatypes not supported')\n\n        return Text\n\n    def _get_dtype(self, sqltype):\n        from sqlalchemy.types import (Integer, Float, Boolean, DateTime,\n                                      Date, TIMESTAMP)\n\n        if isinstance(sqltype, Float):\n            return float\n        elif isinstance(sqltype, Integer):\n            # TODO: Refine integer size.\n            return np.dtype('int64')\n        elif isinstance(sqltype, TIMESTAMP):\n            # we have a timezone capable type\n            if not sqltype.timezone:\n                return datetime\n            return DatetimeTZDtype\n        elif isinstance(sqltype, DateTime):\n            # Caution: np.datetime64 is also a subclass of np.number.\n            return datetime\n        elif isinstance(sqltype, Date):\n            return date\n        elif isinstance(sqltype, Boolean):\n            return bool\n        return object\n\n\nclass PandasSQL(PandasObject):\n    \"\"\"\n    Subclasses Should define read_sql and to_sql.\n    \"\"\"\n\n    def read_sql(self, *args, **kwargs):\n        raise ValueError(\"PandasSQL must be created with an SQLAlchemy \"\n                         \"connectable or sqlite connection\")\n\n    def to_sql(self, *args, **kwargs):\n        raise ValueError(\"PandasSQL must be created with an SQLAlchemy \"\n                         \"connectable or sqlite connection\")\n\n\nclass SQLDatabase(PandasSQL):\n    \"\"\"\n    This class enables conversion between DataFrame and SQL databases\n    using SQLAlchemy to handle DataBase abstraction.\n\n    Parameters\n    ----------\n    engine : SQLAlchemy connectable\n        Connectable to connect with the database. Using SQLAlchemy makes it\n        possible to use any DB supported by that library.\n    schema : string, default None\n        Name of SQL schema in database to write to (if database flavor\n        supports this). If None, use default schema (default).\n    meta : SQLAlchemy MetaData object, default None\n        If provided, this MetaData object is used instead of a newly\n        created. This allows to specify database flavor specific\n        arguments in the MetaData object.\n\n    \"\"\"\n\n    def __init__(self, engine, schema=None, meta=None):\n        self.connectable = engine\n        if not meta:\n            from sqlalchemy.schema import MetaData\n            meta = MetaData(self.connectable, schema=schema)\n\n        self.meta = meta\n\n    @contextmanager\n    def run_transaction(self):\n        with self.connectable.begin() as tx:\n            if hasattr(tx, 'execute'):\n                yield tx\n            else:\n                yield self.connectable\n\n    def execute(self, *args, **kwargs):\n        \"\"\"Simple passthrough to SQLAlchemy connectable\"\"\"\n        return self.connectable.execute(*args, **kwargs)\n\n    def read_table(self, table_name, index_col=None, coerce_float=True,\n                   parse_dates=None, columns=None, schema=None,\n                   chunksize=None):\n        \"\"\"Read SQL database table into a DataFrame.\n\n        Parameters\n        ----------\n        table_name : string\n            Name of SQL table in database.\n        index_col : string, optional, default: None\n            Column to set as index.\n        coerce_float : boolean, default True\n            Attempts to convert values of non-string, non-numeric objects\n            (like decimal.Decimal) to floating point. This can result in\n            loss of precision.\n        parse_dates : list or dict, default: None\n            - List of column names to parse as dates.\n            - Dict of ``{column_name: format string}`` where format string is\n              strftime compatible in case of parsing string times, or is one of\n              (D, s, ns, ms, us) in case of parsing integer timestamps.\n            - Dict of ``{column_name: arg}``, where the arg corresponds\n              to the keyword arguments of :func:`pandas.to_datetime`.\n              Especially useful with databases without native Datetime support,\n              such as SQLite.\n        columns : list, default: None\n            List of column names to select from SQL table.\n        schema : string, default None\n            Name of SQL schema in database to query (if database flavor\n            supports this).  If specified, this overwrites the default\n            schema of the SQL database object.\n        chunksize : int, default None\n            If specified, return an iterator where `chunksize` is the number\n            of rows to include in each chunk.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        pandas.read_sql_table\n        SQLDatabase.read_query\n\n        \"\"\"\n        table = SQLTable(table_name, self, index=index_col, schema=schema)\n        return table.read(coerce_float=coerce_float,\n                          parse_dates=parse_dates, columns=columns,\n                          chunksize=chunksize)\n\n    @staticmethod\n    def _query_iterator(result, chunksize, columns, index_col=None,\n                        coerce_float=True, parse_dates=None):\n        \"\"\"Return generator through chunked result set\"\"\"\n\n        while True:\n            data = result.fetchmany(chunksize)\n            if not data:\n                break\n            else:\n                yield _wrap_result(data, columns, index_col=index_col,\n                                   coerce_float=coerce_float,\n                                   parse_dates=parse_dates)\n\n    def read_query(self, sql, index_col=None, coerce_float=True,\n                   parse_dates=None, params=None, chunksize=None):\n        \"\"\"Read SQL query into a DataFrame.\n\n        Parameters\n        ----------\n        sql : string\n            SQL query to be executed.\n        index_col : string, optional, default: None\n            Column name to use as index for the returned DataFrame object.\n        coerce_float : boolean, default True\n            Attempt to convert values of non-string, non-numeric objects (like\n            decimal.Decimal) to floating point, useful for SQL result sets.\n        params : list, tuple or dict, optional, default: None\n            List of parameters to pass to execute method.  The syntax used\n            to pass parameters is database driver dependent. Check your\n            database driver documentation for which of the five syntax styles,\n            described in PEP 249's paramstyle, is supported.\n            Eg. for psycopg2, uses %(name)s so use params={'name' : 'value'}\n        parse_dates : list or dict, default: None\n            - List of column names to parse as dates.\n            - Dict of ``{column_name: format string}`` where format string is\n              strftime compatible in case of parsing string times, or is one of\n              (D, s, ns, ms, us) in case of parsing integer timestamps.\n            - Dict of ``{column_name: arg dict}``, where the arg dict\n              corresponds to the keyword arguments of\n              :func:`pandas.to_datetime` Especially useful with databases\n              without native Datetime support, such as SQLite.\n        chunksize : int, default None\n            If specified, return an iterator where `chunksize` is the number\n            of rows to include in each chunk.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        read_sql_table : Read SQL database table into a DataFrame.\n        read_sql\n\n        \"\"\"\n        args = _convert_params(sql, params)\n\n        result = self.execute(*args)\n        columns = result.keys()\n\n        if chunksize is not None:\n            return self._query_iterator(result, chunksize, columns,\n                                        index_col=index_col,\n                                        coerce_float=coerce_float,\n                                        parse_dates=parse_dates)\n        else:\n            data = result.fetchall()\n            frame = _wrap_result(data, columns, index_col=index_col,\n                                 coerce_float=coerce_float,\n                                 parse_dates=parse_dates)\n            return frame\n\n    read_sql = read_query\n\n    def to_sql(self, frame, name, if_exists='fail', index=True,\n               index_label=None, schema=None, chunksize=None, dtype=None):\n        \"\"\"\n        Write records stored in a DataFrame to a SQL database.\n\n        Parameters\n        ----------\n        frame : DataFrame\n        name : string\n            Name of SQL table.\n        if_exists : {'fail', 'replace', 'append'}, default 'fail'\n            - fail: If table exists, do nothing.\n            - replace: If table exists, drop it, recreate it, and insert data.\n            - append: If table exists, insert data. Create if does not exist.\n        index : boolean, default True\n            Write DataFrame index as a column.\n        index_label : string or sequence, default None\n            Column label for index column(s). If None is given (default) and\n            `index` is True, then the index names are used.\n            A sequence should be given if the DataFrame uses MultiIndex.\n        schema : string, default None\n            Name of SQL schema in database to write to (if database flavor\n            supports this). If specified, this overwrites the default\n            schema of the SQLDatabase object.\n        chunksize : int, default None\n            If not None, then rows will be written in batches of this size at a\n            time.  If None, all rows will be written at once.\n        dtype : single type or dict of column name to SQL type, default None\n            Optional specifying the datatype for columns. The SQL type should\n            be a SQLAlchemy type. If all columns are of the same type, one\n            single value can be used.\n\n        \"\"\"\n        if dtype and not is_dict_like(dtype):\n            dtype = {col_name: dtype for col_name in frame}\n\n        if dtype is not None:\n            from sqlalchemy.types import to_instance, TypeEngine\n            for col, my_type in dtype.items():\n                if not isinstance(to_instance(my_type), TypeEngine):\n                    raise ValueError('The type of %s is not a SQLAlchemy '\n                                     'type ' % col)\n\n        table = SQLTable(name, self, frame=frame, index=index,\n                         if_exists=if_exists, index_label=index_label,\n                         schema=schema, dtype=dtype)\n        table.create()\n        table.insert(chunksize)\n        if (not name.isdigit() and not name.islower()):\n            # check for potentially case sensitivity issues (GH7815)\n            # Only check when name is not a number and name is not lower case\n            engine = self.connectable.engine\n            with self.connectable.connect() as conn:\n                table_names = engine.table_names(\n                    schema=schema or self.meta.schema,\n                    connection=conn,\n                )\n            if name not in table_names:\n                msg = (\n                    \"The provided table name '{0}' is not found exactly as \"\n                    \"such in the database after writing the table, possibly \"\n                    \"due to case sensitivity issues. Consider using lower \"\n                    \"case table names.\"\n                ).format(name)\n                warnings.warn(msg, UserWarning)\n\n    @property\n    def tables(self):\n        return self.meta.tables\n\n    def has_table(self, name, schema=None):\n        return self.connectable.run_callable(\n            self.connectable.dialect.has_table,\n            name,\n            schema or self.meta.schema,\n        )\n\n    def get_table(self, table_name, schema=None):\n        schema = schema or self.meta.schema\n        if schema:\n            tbl = self.meta.tables.get('.'.join([schema, table_name]))\n        else:\n            tbl = self.meta.tables.get(table_name)\n\n        # Avoid casting double-precision floats into decimals\n        from sqlalchemy import Numeric\n        for column in tbl.columns:\n            if isinstance(column.type, Numeric):\n                column.type.asdecimal = False\n\n        return tbl\n\n    def drop_table(self, table_name, schema=None):\n        schema = schema or self.meta.schema\n        if self.has_table(table_name, schema):\n            self.meta.reflect(only=[table_name], schema=schema)\n            self.get_table(table_name, schema).drop()\n            self.meta.clear()\n\n    def _create_sql_schema(self, frame, table_name, keys=None, dtype=None):\n        table = SQLTable(table_name, self, frame=frame, index=False, keys=keys,\n                         dtype=dtype)\n        return str(table.sql_schema())\n\n\n# ---- SQL without SQLAlchemy ---\n# sqlite-specific sql strings and handler class\n# dictionary used for readability purposes\n_SQL_TYPES = {\n    'string': 'TEXT',\n    'floating': 'REAL',\n    'integer': 'INTEGER',\n    'datetime': 'TIMESTAMP',\n    'date': 'DATE',\n    'time': 'TIME',\n    'boolean': 'INTEGER',\n}\n\n\ndef _get_unicode_name(name):\n    try:\n        uname = text_type(name).encode(\"utf-8\", \"strict\").decode(\"utf-8\")\n    except UnicodeError:\n        raise ValueError(\"Cannot convert identifier to UTF-8: '%s'\" % name)\n    return uname\n\n\ndef _get_valid_sqlite_name(name):\n    # See http://stackoverflow.com/questions/6514274/how-do-you-escape-strings\\\n    # -for-sqlite-table-column-names-in-python\n    # Ensure the string can be encoded as UTF-8.\n    # Ensure the string does not include any NUL characters.\n    # Replace all \" with \"\".\n    # Wrap the entire thing in double quotes.\n\n    uname = _get_unicode_name(name)\n    if not len(uname):\n        raise ValueError(\"Empty table or column name specified\")\n\n    nul_index = uname.find(\"\\x00\")\n    if nul_index >= 0:\n        raise ValueError('SQLite identifier cannot contain NULs')\n    return '\"' + uname.replace('\"', '\"\"') + '\"'\n\n\n_SAFE_NAMES_WARNING = (\"The spaces in these column names will not be changed. \"\n                       \"In pandas versions < 0.14, spaces were converted to \"\n                       \"underscores.\")\n\n\nclass SQLiteTable(SQLTable):\n    \"\"\"\n    Patch the SQLTable for fallback support.\n    Instead of a table variable just use the Create Table statement.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        # GH 8341\n        # register an adapter callable for datetime.time object\n        import sqlite3\n        # this will transform time(12,34,56,789) into '12:34:56.000789'\n        # (this is what sqlalchemy does)\n        sqlite3.register_adapter(time, lambda _: _.strftime(\"%H:%M:%S.%f\"))\n        super(SQLiteTable, self).__init__(*args, **kwargs)\n\n    def sql_schema(self):\n        return str(\";\\n\".join(self.table))\n\n    def _execute_create(self):\n        with self.pd_sql.run_transaction() as conn:\n            for stmt in self.table:\n                conn.execute(stmt)\n\n    def insert_statement(self):\n        names = list(map(text_type, self.frame.columns))\n        wld = '?'  # wildcard char\n        escape = _get_valid_sqlite_name\n\n        if self.index is not None:\n            [names.insert(0, idx) for idx in self.index[::-1]]\n\n        bracketed_names = [escape(column) for column in names]\n        col_names = ','.join(bracketed_names)\n        wildcards = ','.join([wld] * len(names))\n        insert_statement = 'INSERT INTO %s (%s) VALUES (%s)' % (\n            escape(self.name), col_names, wildcards)\n        return insert_statement\n\n    def _execute_insert(self, conn, keys, data_iter):\n        data_list = list(data_iter)\n        conn.executemany(self.insert_statement(), data_list)\n\n    def _create_table_setup(self):\n        \"\"\"\n        Return a list of SQL statements that creates a table reflecting the\n        structure of a DataFrame.  The first entry will be a CREATE TABLE\n        statement while the rest will be CREATE INDEX statements.\n        \"\"\"\n        column_names_and_types = self._get_column_names_and_types(\n            self._sql_type_name\n        )\n\n        pat = re.compile(r'\\s+')\n        column_names = [col_name for col_name, _, _ in column_names_and_types]\n        if any(map(pat.search, column_names)):\n            warnings.warn(_SAFE_NAMES_WARNING, stacklevel=6)\n\n        escape = _get_valid_sqlite_name\n\n        create_tbl_stmts = [escape(cname) + ' ' + ctype\n                            for cname, ctype, _ in column_names_and_types]\n\n        if self.keys is not None and len(self.keys):\n            if not is_list_like(self.keys):\n                keys = [self.keys]\n            else:\n                keys = self.keys\n            cnames_br = \", \".join(escape(c) for c in keys)\n            create_tbl_stmts.append(\n                \"CONSTRAINT {tbl}_pk PRIMARY KEY ({cnames_br})\".format(\n                    tbl=self.name, cnames_br=cnames_br))\n\n        create_stmts = [\"CREATE TABLE \" + escape(self.name) + \" (\\n\" +\n                        ',\\n  '.join(create_tbl_stmts) + \"\\n)\"]\n\n        ix_cols = [cname for cname, _, is_index in column_names_and_types\n                   if is_index]\n        if len(ix_cols):\n            cnames = \"_\".join(ix_cols)\n            cnames_br = \",\".join(escape(c) for c in ix_cols)\n            create_stmts.append(\n                \"CREATE INDEX \" + escape(\"ix_\" + self.name + \"_\" + cnames) +\n                \"ON \" + escape(self.name) + \" (\" + cnames_br + \")\")\n\n        return create_stmts\n\n    def _sql_type_name(self, col):\n        dtype = self.dtype or {}\n        if col.name in dtype:\n            return dtype[col.name]\n\n        col_type = self._get_notna_col_dtype(col)\n        if col_type == 'timedelta64':\n            warnings.warn(\"the 'timedelta' type is not supported, and will be \"\n                          \"written as integer values (ns frequency) to the \"\n                          \"database.\", UserWarning, stacklevel=8)\n            col_type = \"integer\"\n\n        elif col_type == \"datetime64\":\n            col_type = \"datetime\"\n\n        elif col_type == \"empty\":\n            col_type = \"string\"\n\n        elif col_type == \"complex\":\n            raise ValueError('Complex datatypes not supported')\n\n        if col_type not in _SQL_TYPES:\n            col_type = \"string\"\n\n        return _SQL_TYPES[col_type]\n\n\nclass SQLiteDatabase(PandasSQL):\n    \"\"\"\n    Version of SQLDatabase to support SQLite connections (fallback without\n    SQLAlchemy). This should only be used internally.\n\n    Parameters\n    ----------\n    con : sqlite connection object\n\n    \"\"\"\n\n    def __init__(self, con, is_cursor=False):\n        self.is_cursor = is_cursor\n        self.con = con\n\n    @contextmanager\n    def run_transaction(self):\n        cur = self.con.cursor()\n        try:\n            yield cur\n            self.con.commit()\n        except Exception:\n            self.con.rollback()\n            raise\n        finally:\n            cur.close()\n\n    def execute(self, *args, **kwargs):\n        if self.is_cursor:\n            cur = self.con\n        else:\n            cur = self.con.cursor()\n        try:\n            if kwargs:\n                cur.execute(*args, **kwargs)\n            else:\n                cur.execute(*args)\n            return cur\n        except Exception as exc:\n            try:\n                self.con.rollback()\n            except Exception:  # pragma: no cover\n                ex = DatabaseError(\"Execution failed on sql: %s\\n%s\\nunable\"\n                                   \" to rollback\" % (args[0], exc))\n                raise_with_traceback(ex)\n\n            ex = DatabaseError(\n                \"Execution failed on sql '%s': %s\" % (args[0], exc))\n            raise_with_traceback(ex)\n\n    @staticmethod\n    def _query_iterator(cursor, chunksize, columns, index_col=None,\n                        coerce_float=True, parse_dates=None):\n        \"\"\"Return generator through chunked result set\"\"\"\n\n        while True:\n            data = cursor.fetchmany(chunksize)\n            if type(data) == tuple:\n                data = list(data)\n            if not data:\n                cursor.close()\n                break\n            else:\n                yield _wrap_result(data, columns, index_col=index_col,\n                                   coerce_float=coerce_float,\n                                   parse_dates=parse_dates)\n\n    def read_query(self, sql, index_col=None, coerce_float=True, params=None,\n                   parse_dates=None, chunksize=None):\n\n        args = _convert_params(sql, params)\n        cursor = self.execute(*args)\n        columns = [col_desc[0] for col_desc in cursor.description]\n\n        if chunksize is not None:\n            return self._query_iterator(cursor, chunksize, columns,\n                                        index_col=index_col,\n                                        coerce_float=coerce_float,\n                                        parse_dates=parse_dates)\n        else:\n            data = self._fetchall_as_list(cursor)\n            cursor.close()\n\n            frame = _wrap_result(data, columns, index_col=index_col,\n                                 coerce_float=coerce_float,\n                                 parse_dates=parse_dates)\n            return frame\n\n    def _fetchall_as_list(self, cur):\n        result = cur.fetchall()\n        if not isinstance(result, list):\n            result = list(result)\n        return result\n\n    def to_sql(self, frame, name, if_exists='fail', index=True,\n               index_label=None, schema=None, chunksize=None, dtype=None):\n        \"\"\"\n        Write records stored in a DataFrame to a SQL database.\n\n        Parameters\n        ----------\n        frame: DataFrame\n        name: string\n            Name of SQL table.\n        if_exists: {'fail', 'replace', 'append'}, default 'fail'\n            fail: If table exists, do nothing.\n            replace: If table exists, drop it, recreate it, and insert data.\n            append: If table exists, insert data. Create if it does not exist.\n        index : boolean, default True\n            Write DataFrame index as a column\n        index_label : string or sequence, default None\n            Column label for index column(s). If None is given (default) and\n            `index` is True, then the index names are used.\n            A sequence should be given if the DataFrame uses MultiIndex.\n        schema : string, default None\n            Ignored parameter included for compatibility with SQLAlchemy\n            version of ``to_sql``.\n        chunksize : int, default None\n            If not None, then rows will be written in batches of this\n            size at a time. If None, all rows will be written at once.\n        dtype : single type or dict of column name to SQL type, default None\n            Optional specifying the datatype for columns. The SQL type should\n            be a string. If all columns are of the same type, one single value\n            can be used.\n\n        \"\"\"\n        if dtype and not is_dict_like(dtype):\n            dtype = {col_name: dtype for col_name in frame}\n\n        if dtype is not None:\n            for col, my_type in dtype.items():\n                if not isinstance(my_type, str):\n                    raise ValueError('%s (%s) not a string' % (\n                        col, str(my_type)))\n\n        table = SQLiteTable(name, self, frame=frame, index=index,\n                            if_exists=if_exists, index_label=index_label,\n                            dtype=dtype)\n        table.create()\n        table.insert(chunksize)\n\n    def has_table(self, name, schema=None):\n        # TODO(wesm): unused?\n        # escape = _get_valid_sqlite_name\n        # esc_name = escape(name)\n\n        wld = '?'\n        query = (\"SELECT name FROM sqlite_master \"\n                 \"WHERE type='table' AND name=%s;\") % wld\n\n        return len(self.execute(query, [name, ]).fetchall()) > 0\n\n    def get_table(self, table_name, schema=None):\n        return None  # not supported in fallback mode\n\n    def drop_table(self, name, schema=None):\n        drop_sql = \"DROP TABLE %s\" % _get_valid_sqlite_name(name)\n        self.execute(drop_sql)\n\n    def _create_sql_schema(self, frame, table_name, keys=None, dtype=None):\n        table = SQLiteTable(table_name, self, frame=frame, index=False,\n                            keys=keys, dtype=dtype)\n        return str(table.sql_schema())\n\n\ndef get_schema(frame, name, keys=None, con=None, dtype=None):\n    \"\"\"\n    Get the SQL db table schema for the given frame.\n\n    Parameters\n    ----------\n    frame : DataFrame\n    name : string\n        name of SQL table\n    keys : string or sequence, default: None\n        columns to use a primary key\n    con: an open SQL database connection object or a SQLAlchemy connectable\n        Using SQLAlchemy makes it possible to use any DB supported by that\n        library, default: None\n        If a DBAPI2 object, only sqlite3 is supported.\n    dtype : dict of column name to SQL type, default None\n        Optional specifying the datatype for columns. The SQL type should\n        be a SQLAlchemy type, or a string for sqlite3 fallback connection.\n\n    \"\"\"\n\n    pandas_sql = pandasSQL_builder(con=con)\n    return pandas_sql._create_sql_schema(frame, name, keys=keys, dtype=dtype)\n"
    },
    {
      "filename": "pandas/plotting/_core.py",
      "content": "# being a bit too dynamic\n# pylint: disable=E1101\nfrom __future__ import division\n\nimport warnings\nimport re\nfrom collections import namedtuple\nfrom distutils.version import LooseVersion\n\nimport numpy as np\n\nfrom pandas.util._decorators import cache_readonly, Appender\nfrom pandas.compat import range, lrange, map, zip, string_types\nimport pandas.compat as compat\nfrom pandas.errors import AbstractMethodError\n\nimport pandas.core.common as com\nfrom pandas.core.base import PandasObject\nfrom pandas.core.config import get_option\nfrom pandas.core.generic import _shared_docs, _shared_doc_kwargs\n\nfrom pandas.core.dtypes.missing import isna, notna, remove_na_arraylike\nfrom pandas.core.dtypes.common import (\n    is_list_like,\n    is_integer,\n    is_number,\n    is_hashable,\n    is_iterator)\nfrom pandas.core.dtypes.generic import (\n    ABCSeries, ABCDataFrame, ABCPeriodIndex, ABCMultiIndex, ABCIndexClass)\n\nfrom pandas.io.formats.printing import pprint_thing\n\nfrom pandas.plotting._compat import _mpl_ge_3_0_0\nfrom pandas.plotting._style import (plot_params,\n                                    _get_standard_colors)\nfrom pandas.plotting._tools import (_subplots, _flatten, table,\n                                    _handle_shared_axes, _get_all_lines,\n                                    _get_xlim, _set_ticks_props,\n                                    format_date_labels)\n\ntry:\n    from pandas.plotting import _converter\nexcept ImportError:\n    _HAS_MPL = False\nelse:\n    _HAS_MPL = True\n    if get_option('plotting.matplotlib.register_converters'):\n        _converter.register(explicit=True)\n\n\ndef _raise_if_no_mpl():\n    # TODO(mpl_converter): remove once converter is explicit\n    if not _HAS_MPL:\n        raise ImportError(\"matplotlib is required for plotting.\")\n\n\ndef _get_standard_kind(kind):\n    return {'density': 'kde'}.get(kind, kind)\n\n\ndef _gca(rc=None):\n    import matplotlib.pyplot as plt\n    with plt.rc_context(rc):\n        return plt.gca()\n\n\ndef _gcf():\n    import matplotlib.pyplot as plt\n    return plt.gcf()\n\n\nclass MPLPlot(object):\n    \"\"\"\n    Base class for assembling a pandas plot using matplotlib\n\n    Parameters\n    ----------\n    data :\n\n    \"\"\"\n\n    @property\n    def _kind(self):\n        \"\"\"Specify kind str. Must be overridden in child class\"\"\"\n        raise NotImplementedError\n\n    _layout_type = 'vertical'\n    _default_rot = 0\n    orientation = None\n    _pop_attributes = ['label', 'style', 'logy', 'logx', 'loglog',\n                       'mark_right', 'stacked']\n    _attr_defaults = {'logy': False, 'logx': False, 'loglog': False,\n                      'mark_right': True, 'stacked': False}\n\n    def __init__(self, data, kind=None, by=None, subplots=False, sharex=None,\n                 sharey=False, use_index=True,\n                 figsize=None, grid=None, legend=True, rot=None,\n                 ax=None, fig=None, title=None, xlim=None, ylim=None,\n                 xticks=None, yticks=None,\n                 sort_columns=False, fontsize=None,\n                 secondary_y=False, colormap=None,\n                 table=False, layout=None, **kwds):\n\n        _raise_if_no_mpl()\n        _converter._WARN = False\n        self.data = data\n        self.by = by\n\n        self.kind = kind\n\n        self.sort_columns = sort_columns\n\n        self.subplots = subplots\n\n        if sharex is None:\n            if ax is None:\n                self.sharex = True\n            else:\n                # if we get an axis, the users should do the visibility\n                # setting...\n                self.sharex = False\n        else:\n            self.sharex = sharex\n\n        self.sharey = sharey\n        self.figsize = figsize\n        self.layout = layout\n\n        self.xticks = xticks\n        self.yticks = yticks\n        self.xlim = xlim\n        self.ylim = ylim\n        self.title = title\n        self.use_index = use_index\n\n        self.fontsize = fontsize\n\n        if rot is not None:\n            self.rot = rot\n            # need to know for format_date_labels since it's rotated to 30 by\n            # default\n            self._rot_set = True\n        else:\n            self._rot_set = False\n            self.rot = self._default_rot\n\n        if grid is None:\n            grid = False if secondary_y else self.plt.rcParams['axes.grid']\n\n        self.grid = grid\n        self.legend = legend\n        self.legend_handles = []\n        self.legend_labels = []\n\n        for attr in self._pop_attributes:\n            value = kwds.pop(attr, self._attr_defaults.get(attr, None))\n            setattr(self, attr, value)\n\n        self.ax = ax\n        self.fig = fig\n        self.axes = None\n\n        # parse errorbar input if given\n        xerr = kwds.pop('xerr', None)\n        yerr = kwds.pop('yerr', None)\n        self.errors = {}\n        for kw, err in zip(['xerr', 'yerr'], [xerr, yerr]):\n            self.errors[kw] = self._parse_errorbars(kw, err)\n\n        if not isinstance(secondary_y, (bool, tuple, list,\n                                        np.ndarray, ABCIndexClass)):\n            secondary_y = [secondary_y]\n        self.secondary_y = secondary_y\n\n        # ugly TypeError if user passes matplotlib's `cmap` name.\n        # Probably better to accept either.\n        if 'cmap' in kwds and colormap:\n            raise TypeError(\"Only specify one of `cmap` and `colormap`.\")\n        elif 'cmap' in kwds:\n            self.colormap = kwds.pop('cmap')\n        else:\n            self.colormap = colormap\n\n        self.table = table\n\n        self.kwds = kwds\n\n        self._validate_color_args()\n\n    def _validate_color_args(self):\n        if 'color' not in self.kwds and 'colors' in self.kwds:\n            warnings.warn((\"'colors' is being deprecated. Please use 'color'\"\n                           \"instead of 'colors'\"))\n            colors = self.kwds.pop('colors')\n            self.kwds['color'] = colors\n\n        if ('color' in self.kwds and self.nseries == 1 and\n                not is_list_like(self.kwds['color'])):\n            # support series.plot(color='green')\n            self.kwds['color'] = [self.kwds['color']]\n\n        if ('color' in self.kwds and isinstance(self.kwds['color'], tuple) and\n                self.nseries == 1 and len(self.kwds['color']) in (3, 4)):\n            # support RGB and RGBA tuples in series plot\n            self.kwds['color'] = [self.kwds['color']]\n\n        if ('color' in self.kwds or 'colors' in self.kwds) and \\\n                self.colormap is not None:\n            warnings.warn(\"'color' and 'colormap' cannot be used \"\n                          \"simultaneously. Using 'color'\")\n\n        if 'color' in self.kwds and self.style is not None:\n            if is_list_like(self.style):\n                styles = self.style\n            else:\n                styles = [self.style]\n            # need only a single match\n            for s in styles:\n                if re.match('^[a-z]+?', s) is not None:\n                    raise ValueError(\n                        \"Cannot pass 'style' string with a color \"\n                        \"symbol and 'color' keyword argument. Please\"\n                        \" use one or the other or pass 'style' \"\n                        \"without a color symbol\")\n\n    def _iter_data(self, data=None, keep_index=False, fillna=None):\n        if data is None:\n            data = self.data\n        if fillna is not None:\n            data = data.fillna(fillna)\n\n        # TODO: unused?\n        # if self.sort_columns:\n        #     columns = com.try_sort(data.columns)\n        # else:\n        #     columns = data.columns\n\n        for col, values in data.iteritems():\n            if keep_index is True:\n                yield col, values\n            else:\n                yield col, values.values\n\n    @property\n    def nseries(self):\n        if self.data.ndim == 1:\n            return 1\n        else:\n            return self.data.shape[1]\n\n    def draw(self):\n        self.plt.draw_if_interactive()\n\n    def generate(self):\n        self._args_adjust()\n        self._compute_plot_data()\n        self._setup_subplots()\n        self._make_plot()\n        self._add_table()\n        self._make_legend()\n        self._adorn_subplots()\n\n        for ax in self.axes:\n            self._post_plot_logic_common(ax, self.data)\n            self._post_plot_logic(ax, self.data)\n\n    def _args_adjust(self):\n        pass\n\n    def _has_plotted_object(self, ax):\n        \"\"\"check whether ax has data\"\"\"\n        return (len(ax.lines) != 0 or\n                len(ax.artists) != 0 or\n                len(ax.containers) != 0)\n\n    def _maybe_right_yaxis(self, ax, axes_num):\n        if not self.on_right(axes_num):\n            # secondary axes may be passed via ax kw\n            return self._get_ax_layer(ax)\n\n        if hasattr(ax, 'right_ax'):\n            # if it has right_ax proparty, ``ax`` must be left axes\n            return ax.right_ax\n        elif hasattr(ax, 'left_ax'):\n            # if it has left_ax proparty, ``ax`` must be right axes\n            return ax\n        else:\n            # otherwise, create twin axes\n            orig_ax, new_ax = ax, ax.twinx()\n            # TODO: use Matplotlib public API when available\n            new_ax._get_lines = orig_ax._get_lines\n            new_ax._get_patches_for_fill = orig_ax._get_patches_for_fill\n            orig_ax.right_ax, new_ax.left_ax = new_ax, orig_ax\n\n            if not self._has_plotted_object(orig_ax):  # no data on left y\n                orig_ax.get_yaxis().set_visible(False)\n            return new_ax\n\n    def _setup_subplots(self):\n        if self.subplots:\n            fig, axes = _subplots(naxes=self.nseries,\n                                  sharex=self.sharex, sharey=self.sharey,\n                                  figsize=self.figsize, ax=self.ax,\n                                  layout=self.layout,\n                                  layout_type=self._layout_type)\n        else:\n            if self.ax is None:\n                fig = self.plt.figure(figsize=self.figsize)\n                axes = fig.add_subplot(111)\n            else:\n                fig = self.ax.get_figure()\n                if self.figsize is not None:\n                    fig.set_size_inches(self.figsize)\n                axes = self.ax\n\n        axes = _flatten(axes)\n\n        if self.logx or self.loglog:\n            [a.set_xscale('log') for a in axes]\n        if self.logy or self.loglog:\n            [a.set_yscale('log') for a in axes]\n\n        self.fig = fig\n        self.axes = axes\n\n    @property\n    def result(self):\n        \"\"\"\n        Return result axes\n        \"\"\"\n        if self.subplots:\n            if self.layout is not None and not is_list_like(self.ax):\n                return self.axes.reshape(*self.layout)\n            else:\n                return self.axes\n        else:\n            sec_true = isinstance(self.secondary_y, bool) and self.secondary_y\n            all_sec = (is_list_like(self.secondary_y) and\n                       len(self.secondary_y) == self.nseries)\n            if (sec_true or all_sec):\n                # if all data is plotted on secondary, return right axes\n                return self._get_ax_layer(self.axes[0], primary=False)\n            else:\n                return self.axes[0]\n\n    def _compute_plot_data(self):\n        data = self.data\n\n        if isinstance(data, ABCSeries):\n            label = self.label\n            if label is None and data.name is None:\n                label = 'None'\n            data = data.to_frame(name=label)\n\n        # GH16953, _convert is needed as fallback, for ``Series``\n        # with ``dtype == object``\n        data = data._convert(datetime=True, timedelta=True)\n        numeric_data = data.select_dtypes(include=[np.number,\n                                                   \"datetime\",\n                                                   \"datetimetz\",\n                                                   \"timedelta\"])\n\n        try:\n            is_empty = numeric_data.empty\n        except AttributeError:\n            is_empty = not len(numeric_data)\n\n        # no empty frames or series allowed\n        if is_empty:\n            raise TypeError('Empty {0!r}: no numeric data to '\n                            'plot'.format(numeric_data.__class__.__name__))\n\n        self.data = numeric_data\n\n    def _make_plot(self):\n        raise AbstractMethodError(self)\n\n    def _add_table(self):\n        if self.table is False:\n            return\n        elif self.table is True:\n            data = self.data.transpose()\n        else:\n            data = self.table\n        ax = self._get_ax(0)\n        table(ax, data)\n\n    def _post_plot_logic_common(self, ax, data):\n        \"\"\"Common post process for each axes\"\"\"\n\n        def get_label(i):\n            try:\n                return pprint_thing(data.index[i])\n            except Exception:\n                return ''\n\n        if self.orientation == 'vertical' or self.orientation is None:\n            if self._need_to_set_index:\n                xticklabels = [get_label(x) for x in ax.get_xticks()]\n                ax.set_xticklabels(xticklabels)\n            self._apply_axis_properties(ax.xaxis, rot=self.rot,\n                                        fontsize=self.fontsize)\n            self._apply_axis_properties(ax.yaxis, fontsize=self.fontsize)\n\n            if hasattr(ax, 'right_ax'):\n                self._apply_axis_properties(ax.right_ax.yaxis,\n                                            fontsize=self.fontsize)\n\n        elif self.orientation == 'horizontal':\n            if self._need_to_set_index:\n                yticklabels = [get_label(y) for y in ax.get_yticks()]\n                ax.set_yticklabels(yticklabels)\n            self._apply_axis_properties(ax.yaxis, rot=self.rot,\n                                        fontsize=self.fontsize)\n            self._apply_axis_properties(ax.xaxis, fontsize=self.fontsize)\n\n            if hasattr(ax, 'right_ax'):\n                self._apply_axis_properties(ax.right_ax.yaxis,\n                                            fontsize=self.fontsize)\n        else:  # pragma no cover\n            raise ValueError\n\n    def _post_plot_logic(self, ax, data):\n        \"\"\"Post process for each axes. Overridden in child classes\"\"\"\n        pass\n\n    def _adorn_subplots(self):\n        \"\"\"Common post process unrelated to data\"\"\"\n        if len(self.axes) > 0:\n            all_axes = self._get_subplots()\n            nrows, ncols = self._get_axes_layout()\n            _handle_shared_axes(axarr=all_axes, nplots=len(all_axes),\n                                naxes=nrows * ncols, nrows=nrows,\n                                ncols=ncols, sharex=self.sharex,\n                                sharey=self.sharey)\n\n        for ax in self.axes:\n            if self.yticks is not None:\n                ax.set_yticks(self.yticks)\n\n            if self.xticks is not None:\n                ax.set_xticks(self.xticks)\n\n            if self.ylim is not None:\n                ax.set_ylim(self.ylim)\n\n            if self.xlim is not None:\n                ax.set_xlim(self.xlim)\n\n            ax.grid(self.grid)\n\n        if self.title:\n            if self.subplots:\n                if is_list_like(self.title):\n                    if len(self.title) != self.nseries:\n                        msg = ('The length of `title` must equal the number '\n                               'of columns if using `title` of type `list` '\n                               'and `subplots=True`.\\n'\n                               'length of title = {}\\n'\n                               'number of columns = {}').format(\n                            len(self.title), self.nseries)\n                        raise ValueError(msg)\n\n                    for (ax, title) in zip(self.axes, self.title):\n                        ax.set_title(title)\n                else:\n                    self.fig.suptitle(self.title)\n            else:\n                if is_list_like(self.title):\n                    msg = ('Using `title` of type `list` is not supported '\n                           'unless `subplots=True` is passed')\n                    raise ValueError(msg)\n                self.axes[0].set_title(self.title)\n\n    def _apply_axis_properties(self, axis, rot=None, fontsize=None):\n        labels = axis.get_majorticklabels() + axis.get_minorticklabels()\n        for label in labels:\n            if rot is not None:\n                label.set_rotation(rot)\n            if fontsize is not None:\n                label.set_fontsize(fontsize)\n\n    @property\n    def legend_title(self):\n        if not isinstance(self.data.columns, ABCMultiIndex):\n            name = self.data.columns.name\n            if name is not None:\n                name = pprint_thing(name)\n            return name\n        else:\n            stringified = map(pprint_thing,\n                              self.data.columns.names)\n            return ','.join(stringified)\n\n    def _add_legend_handle(self, handle, label, index=None):\n        if label is not None:\n            if self.mark_right and index is not None:\n                if self.on_right(index):\n                    label = label + ' (right)'\n            self.legend_handles.append(handle)\n            self.legend_labels.append(label)\n\n    def _make_legend(self):\n        ax, leg = self._get_ax_legend(self.axes[0])\n\n        handles = []\n        labels = []\n        title = ''\n\n        if not self.subplots:\n            if leg is not None:\n                title = leg.get_title().get_text()\n                handles = leg.legendHandles\n                labels = [x.get_text() for x in leg.get_texts()]\n\n            if self.legend:\n                if self.legend == 'reverse':\n                    self.legend_handles = reversed(self.legend_handles)\n                    self.legend_labels = reversed(self.legend_labels)\n\n                handles += self.legend_handles\n                labels += self.legend_labels\n                if self.legend_title is not None:\n                    title = self.legend_title\n\n            if len(handles) > 0:\n                ax.legend(handles, labels, loc='best', title=title)\n\n        elif self.subplots and self.legend:\n            for ax in self.axes:\n                if ax.get_visible():\n                    ax.legend(loc='best')\n\n    def _get_ax_legend(self, ax):\n        leg = ax.get_legend()\n        other_ax = (getattr(ax, 'left_ax', None) or\n                    getattr(ax, 'right_ax', None))\n        other_leg = None\n        if other_ax is not None:\n            other_leg = other_ax.get_legend()\n        if leg is None and other_leg is not None:\n            leg = other_leg\n            ax = other_ax\n        return ax, leg\n\n    @cache_readonly\n    def plt(self):\n        import matplotlib.pyplot as plt\n        return plt\n\n    _need_to_set_index = False\n\n    def _get_xticks(self, convert_period=False):\n        index = self.data.index\n        is_datetype = index.inferred_type in ('datetime', 'date',\n                                              'datetime64', 'time')\n\n        if self.use_index:\n            if convert_period and isinstance(index, ABCPeriodIndex):\n                self.data = self.data.reindex(index=index.sort_values())\n                x = self.data.index.to_timestamp()._mpl_repr()\n            elif index.is_numeric():\n                \"\"\"\n                Matplotlib supports numeric values or datetime objects as\n                xaxis values. Taking LBYL approach here, by the time\n                matplotlib raises exception when using non numeric/datetime\n                values for xaxis, several actions are already taken by plt.\n                \"\"\"\n                x = index._mpl_repr()\n            elif is_datetype:\n                self.data = self.data[notna(self.data.index)]\n                self.data = self.data.sort_index()\n                x = self.data.index._mpl_repr()\n            else:\n                self._need_to_set_index = True\n                x = lrange(len(index))\n        else:\n            x = lrange(len(index))\n\n        return x\n\n    @classmethod\n    def _plot(cls, ax, x, y, style=None, is_errorbar=False, **kwds):\n        mask = isna(y)\n        if mask.any():\n            y = np.ma.array(y)\n            y = np.ma.masked_where(mask, y)\n\n        if isinstance(x, ABCIndexClass):\n            x = x._mpl_repr()\n\n        if is_errorbar:\n            if 'xerr' in kwds:\n                kwds['xerr'] = np.array(kwds.get('xerr'))\n            if 'yerr' in kwds:\n                kwds['yerr'] = np.array(kwds.get('yerr'))\n            return ax.errorbar(x, y, **kwds)\n        else:\n            # prevent style kwarg from going to errorbar, where it is\n            # unsupported\n            if style is not None:\n                args = (x, y, style)\n            else:\n                args = (x, y)\n            return ax.plot(*args, **kwds)\n\n    def _get_index_name(self):\n        if isinstance(self.data.index, ABCMultiIndex):\n            name = self.data.index.names\n            if com._any_not_none(*name):\n                name = ','.join(pprint_thing(x) for x in name)\n            else:\n                name = None\n        else:\n            name = self.data.index.name\n            if name is not None:\n                name = pprint_thing(name)\n\n        return name\n\n    @classmethod\n    def _get_ax_layer(cls, ax, primary=True):\n        \"\"\"get left (primary) or right (secondary) axes\"\"\"\n        if primary:\n            return getattr(ax, 'left_ax', ax)\n        else:\n            return getattr(ax, 'right_ax', ax)\n\n    def _get_ax(self, i):\n        # get the twinx ax if appropriate\n        if self.subplots:\n            ax = self.axes[i]\n            ax = self._maybe_right_yaxis(ax, i)\n            self.axes[i] = ax\n        else:\n            ax = self.axes[0]\n            ax = self._maybe_right_yaxis(ax, i)\n\n        ax.get_yaxis().set_visible(True)\n        return ax\n\n    def on_right(self, i):\n        if isinstance(self.secondary_y, bool):\n            return self.secondary_y\n\n        if isinstance(self.secondary_y, (tuple, list,\n                                         np.ndarray, ABCIndexClass)):\n            return self.data.columns[i] in self.secondary_y\n\n    def _apply_style_colors(self, colors, kwds, col_num, label):\n        \"\"\"\n        Manage style and color based on column number and its label.\n        Returns tuple of appropriate style and kwds which \"color\" may be added.\n        \"\"\"\n        style = None\n        if self.style is not None:\n            if isinstance(self.style, list):\n                try:\n                    style = self.style[col_num]\n                except IndexError:\n                    pass\n            elif isinstance(self.style, dict):\n                style = self.style.get(label, style)\n            else:\n                style = self.style\n\n        has_color = 'color' in kwds or self.colormap is not None\n        nocolor_style = style is None or re.match('[a-z]+', style) is None\n        if (has_color or self.subplots) and nocolor_style:\n            kwds['color'] = colors[col_num % len(colors)]\n        return style, kwds\n\n    def _get_colors(self, num_colors=None, color_kwds='color'):\n        if num_colors is None:\n            num_colors = self.nseries\n\n        return _get_standard_colors(num_colors=num_colors,\n                                    colormap=self.colormap,\n                                    color=self.kwds.get(color_kwds))\n\n    def _parse_errorbars(self, label, err):\n        \"\"\"\n        Look for error keyword arguments and return the actual errorbar data\n        or return the error DataFrame/dict\n\n        Error bars can be specified in several ways:\n            Series: the user provides a pandas.Series object of the same\n                    length as the data\n            ndarray: provides a np.ndarray of the same length as the data\n            DataFrame/dict: error values are paired with keys matching the\n                    key in the plotted DataFrame\n            str: the name of the column within the plotted DataFrame\n        \"\"\"\n\n        if err is None:\n            return None\n\n        def match_labels(data, e):\n            e = e.reindex(data.index)\n            return e\n\n        # key-matched DataFrame\n        if isinstance(err, ABCDataFrame):\n\n            err = match_labels(self.data, err)\n        # key-matched dict\n        elif isinstance(err, dict):\n            pass\n\n        # Series of error values\n        elif isinstance(err, ABCSeries):\n            # broadcast error series across data\n            err = match_labels(self.data, err)\n            err = np.atleast_2d(err)\n            err = np.tile(err, (self.nseries, 1))\n\n        # errors are a column in the dataframe\n        elif isinstance(err, string_types):\n            evalues = self.data[err].values\n            self.data = self.data[self.data.columns.drop(err)]\n            err = np.atleast_2d(evalues)\n            err = np.tile(err, (self.nseries, 1))\n\n        elif is_list_like(err):\n            if is_iterator(err):\n                err = np.atleast_2d(list(err))\n            else:\n                # raw error values\n                err = np.atleast_2d(err)\n\n            err_shape = err.shape\n\n            # asymmetrical error bars\n            if err.ndim == 3:\n                if (err_shape[0] != self.nseries) or \\\n                        (err_shape[1] != 2) or \\\n                        (err_shape[2] != len(self.data)):\n                    msg = \"Asymmetrical error bars should be provided \" + \\\n                        \"with the shape (%u, 2, %u)\" % \\\n                        (self.nseries, len(self.data))\n                    raise ValueError(msg)\n\n            # broadcast errors to each data series\n            if len(err) == 1:\n                err = np.tile(err, (self.nseries, 1))\n\n        elif is_number(err):\n            err = np.tile([err], (self.nseries, len(self.data)))\n\n        else:\n            msg = \"No valid {label} detected\".format(label=label)\n            raise ValueError(msg)\n\n        return err\n\n    def _get_errorbars(self, label=None, index=None, xerr=True, yerr=True):\n        errors = {}\n\n        for kw, flag in zip(['xerr', 'yerr'], [xerr, yerr]):\n            if flag:\n                err = self.errors[kw]\n                # user provided label-matched dataframe of errors\n                if isinstance(err, (ABCDataFrame, dict)):\n                    if label is not None and label in err.keys():\n                        err = err[label]\n                    else:\n                        err = None\n                elif index is not None and err is not None:\n                    err = err[index]\n\n                if err is not None:\n                    errors[kw] = err\n        return errors\n\n    def _get_subplots(self):\n        from matplotlib.axes import Subplot\n        return [ax for ax in self.axes[0].get_figure().get_axes()\n                if isinstance(ax, Subplot)]\n\n    def _get_axes_layout(self):\n        axes = self._get_subplots()\n        x_set = set()\n        y_set = set()\n        for ax in axes:\n            # check axes coordinates to estimate layout\n            points = ax.get_position().get_points()\n            x_set.add(points[0][0])\n            y_set.add(points[0][1])\n        return (len(y_set), len(x_set))\n\n\nclass PlanePlot(MPLPlot):\n    \"\"\"\n    Abstract class for plotting on plane, currently scatter and hexbin.\n    \"\"\"\n\n    _layout_type = 'single'\n\n    def __init__(self, data, x, y, **kwargs):\n        MPLPlot.__init__(self, data, **kwargs)\n        if x is None or y is None:\n            raise ValueError(self._kind + ' requires an x and y column')\n        if is_integer(x) and not self.data.columns.holds_integer():\n            x = self.data.columns[x]\n        if is_integer(y) and not self.data.columns.holds_integer():\n            y = self.data.columns[y]\n        if len(self.data[x]._get_numeric_data()) == 0:\n            raise ValueError(self._kind + ' requires x column to be numeric')\n        if len(self.data[y]._get_numeric_data()) == 0:\n            raise ValueError(self._kind + ' requires y column to be numeric')\n\n        self.x = x\n        self.y = y\n\n    @property\n    def nseries(self):\n        return 1\n\n    def _post_plot_logic(self, ax, data):\n        x, y = self.x, self.y\n        ax.set_ylabel(pprint_thing(y))\n        ax.set_xlabel(pprint_thing(x))\n\n    def _plot_colorbar(self, ax, **kwds):\n        # Addresses issues #10611 and #10678:\n        # When plotting scatterplots and hexbinplots in IPython\n        # inline backend the colorbar axis height tends not to\n        # exactly match the parent axis height.\n        # The difference is due to small fractional differences\n        # in floating points with similar representation.\n        # To deal with this, this method forces the colorbar\n        # height to take the height of the parent axes.\n        # For a more detailed description of the issue\n        # see the following link:\n        # https://github.com/ipython/ipython/issues/11215\n        img = ax.collections[0]\n        cbar = self.fig.colorbar(img, ax=ax, **kwds)\n\n        if _mpl_ge_3_0_0():\n            # The workaround below is no longer necessary.\n            return\n\n        points = ax.get_position().get_points()\n        cbar_points = cbar.ax.get_position().get_points()\n\n        cbar.ax.set_position([cbar_points[0, 0],\n                              points[0, 1],\n                              cbar_points[1, 0] - cbar_points[0, 0],\n                              points[1, 1] - points[0, 1]])\n        # To see the discrepancy in axis heights uncomment\n        # the following two lines:\n        # print(points[1, 1] - points[0, 1])\n        # print(cbar_points[1, 1] - cbar_points[0, 1])\n\n\nclass ScatterPlot(PlanePlot):\n    _kind = 'scatter'\n\n    def __init__(self, data, x, y, s=None, c=None, **kwargs):\n        if s is None:\n            # hide the matplotlib default for size, in case we want to change\n            # the handling of this argument later\n            s = 20\n        super(ScatterPlot, self).__init__(data, x, y, s=s, **kwargs)\n        if is_integer(c) and not self.data.columns.holds_integer():\n            c = self.data.columns[c]\n        self.c = c\n\n    def _make_plot(self):\n        x, y, c, data = self.x, self.y, self.c, self.data\n        ax = self.axes[0]\n\n        c_is_column = is_hashable(c) and c in self.data.columns\n\n        # plot a colorbar only if a colormap is provided or necessary\n        cb = self.kwds.pop('colorbar', self.colormap or c_is_column)\n\n        # pandas uses colormap, matplotlib uses cmap.\n        cmap = self.colormap or 'Greys'\n        cmap = self.plt.cm.get_cmap(cmap)\n        color = self.kwds.pop(\"color\", None)\n        if c is not None and color is not None:\n            raise TypeError('Specify exactly one of `c` and `color`')\n        elif c is None and color is None:\n            c_values = self.plt.rcParams['patch.facecolor']\n        elif color is not None:\n            c_values = color\n        elif c_is_column:\n            c_values = self.data[c].values\n        else:\n            c_values = c\n\n        if self.legend and hasattr(self, 'label'):\n            label = self.label\n        else:\n            label = None\n        scatter = ax.scatter(data[x].values, data[y].values, c=c_values,\n                             label=label, cmap=cmap, **self.kwds)\n        if cb:\n            cbar_label = c if c_is_column else ''\n            self._plot_colorbar(ax, label=cbar_label)\n\n        if label is not None:\n            self._add_legend_handle(scatter, label)\n        else:\n            self.legend = False\n\n        errors_x = self._get_errorbars(label=x, index=0, yerr=False)\n        errors_y = self._get_errorbars(label=y, index=0, xerr=False)\n        if len(errors_x) > 0 or len(errors_y) > 0:\n            err_kwds = dict(errors_x, **errors_y)\n            err_kwds['ecolor'] = scatter.get_facecolor()[0]\n            ax.errorbar(data[x].values, data[y].values,\n                        linestyle='none', **err_kwds)\n\n\nclass HexBinPlot(PlanePlot):\n    _kind = 'hexbin'\n\n    def __init__(self, data, x, y, C=None, **kwargs):\n        super(HexBinPlot, self).__init__(data, x, y, **kwargs)\n        if is_integer(C) and not self.data.columns.holds_integer():\n            C = self.data.columns[C]\n        self.C = C\n\n    def _make_plot(self):\n        x, y, data, C = self.x, self.y, self.data, self.C\n        ax = self.axes[0]\n        # pandas uses colormap, matplotlib uses cmap.\n        cmap = self.colormap or 'BuGn'\n        cmap = self.plt.cm.get_cmap(cmap)\n        cb = self.kwds.pop('colorbar', True)\n\n        if C is None:\n            c_values = None\n        else:\n            c_values = data[C].values\n\n        ax.hexbin(data[x].values, data[y].values, C=c_values, cmap=cmap,\n                  **self.kwds)\n        if cb:\n            self._plot_colorbar(ax)\n\n    def _make_legend(self):\n        pass\n\n\nclass LinePlot(MPLPlot):\n    _kind = 'line'\n    _default_rot = 0\n    orientation = 'vertical'\n\n    def __init__(self, data, **kwargs):\n        MPLPlot.__init__(self, data, **kwargs)\n        if self.stacked:\n            self.data = self.data.fillna(value=0)\n        self.x_compat = plot_params['x_compat']\n        if 'x_compat' in self.kwds:\n            self.x_compat = bool(self.kwds.pop('x_compat'))\n\n    def _is_ts_plot(self):\n        # this is slightly deceptive\n        return not self.x_compat and self.use_index and self._use_dynamic_x()\n\n    def _use_dynamic_x(self):\n        from pandas.plotting._timeseries import _use_dynamic_x\n        return _use_dynamic_x(self._get_ax(0), self.data)\n\n    def _make_plot(self):\n        if self._is_ts_plot():\n            from pandas.plotting._timeseries import _maybe_convert_index\n            data = _maybe_convert_index(self._get_ax(0), self.data)\n\n            x = data.index      # dummy, not used\n            plotf = self._ts_plot\n            it = self._iter_data(data=data, keep_index=True)\n        else:\n            x = self._get_xticks(convert_period=True)\n            plotf = self._plot\n            it = self._iter_data()\n\n        stacking_id = self._get_stacking_id()\n        is_errorbar = com._any_not_none(*self.errors.values())\n\n        colors = self._get_colors()\n        for i, (label, y) in enumerate(it):\n            ax = self._get_ax(i)\n            kwds = self.kwds.copy()\n            style, kwds = self._apply_style_colors(colors, kwds, i, label)\n\n            errors = self._get_errorbars(label=label, index=i)\n            kwds = dict(kwds, **errors)\n\n            label = pprint_thing(label)  # .encode('utf-8')\n            kwds['label'] = label\n\n            newlines = plotf(ax, x, y, style=style, column_num=i,\n                             stacking_id=stacking_id,\n                             is_errorbar=is_errorbar,\n                             **kwds)\n            self._add_legend_handle(newlines[0], label, index=i)\n\n            lines = _get_all_lines(ax)\n            left, right = _get_xlim(lines)\n            ax.set_xlim(left, right)\n\n    @classmethod\n    def _plot(cls, ax, x, y, style=None, column_num=None,\n              stacking_id=None, **kwds):\n        # column_num is used to get the target column from protf in line and\n        # area plots\n        if column_num == 0:\n            cls._initialize_stacker(ax, stacking_id, len(y))\n        y_values = cls._get_stacked_values(ax, stacking_id, y, kwds['label'])\n        lines = MPLPlot._plot(ax, x, y_values, style=style, **kwds)\n        cls._update_stacker(ax, stacking_id, y)\n        return lines\n\n    @classmethod\n    def _ts_plot(cls, ax, x, data, style=None, **kwds):\n        from pandas.plotting._timeseries import (_maybe_resample,\n                                                 _decorate_axes,\n                                                 format_dateaxis)\n        # accept x to be consistent with normal plot func,\n        # x is not passed to tsplot as it uses data.index as x coordinate\n        # column_num must be in kwds for stacking purpose\n        freq, data = _maybe_resample(data, ax, kwds)\n\n        # Set ax with freq info\n        _decorate_axes(ax, freq, kwds)\n        # digging deeper\n        if hasattr(ax, 'left_ax'):\n            _decorate_axes(ax.left_ax, freq, kwds)\n        if hasattr(ax, 'right_ax'):\n            _decorate_axes(ax.right_ax, freq, kwds)\n        ax._plot_data.append((data, cls._kind, kwds))\n\n        lines = cls._plot(ax, data.index, data.values, style=style, **kwds)\n        # set date formatter, locators and rescale limits\n        format_dateaxis(ax, ax.freq, data.index)\n        return lines\n\n    def _get_stacking_id(self):\n        if self.stacked:\n            return id(self.data)\n        else:\n            return None\n\n    @classmethod\n    def _initialize_stacker(cls, ax, stacking_id, n):\n        if stacking_id is None:\n            return\n        if not hasattr(ax, '_stacker_pos_prior'):\n            ax._stacker_pos_prior = {}\n        if not hasattr(ax, '_stacker_neg_prior'):\n            ax._stacker_neg_prior = {}\n        ax._stacker_pos_prior[stacking_id] = np.zeros(n)\n        ax._stacker_neg_prior[stacking_id] = np.zeros(n)\n\n    @classmethod\n    def _get_stacked_values(cls, ax, stacking_id, values, label):\n        if stacking_id is None:\n            return values\n        if not hasattr(ax, '_stacker_pos_prior'):\n            # stacker may not be initialized for subplots\n            cls._initialize_stacker(ax, stacking_id, len(values))\n\n        if (values >= 0).all():\n            return ax._stacker_pos_prior[stacking_id] + values\n        elif (values <= 0).all():\n            return ax._stacker_neg_prior[stacking_id] + values\n\n        raise ValueError('When stacked is True, each column must be either '\n                         'all positive or negative.'\n                         '{0} contains both positive and negative values'\n                         .format(label))\n\n    @classmethod\n    def _update_stacker(cls, ax, stacking_id, values):\n        if stacking_id is None:\n            return\n        if (values >= 0).all():\n            ax._stacker_pos_prior[stacking_id] += values\n        elif (values <= 0).all():\n            ax._stacker_neg_prior[stacking_id] += values\n\n    def _post_plot_logic(self, ax, data):\n        condition = (not self._use_dynamic_x() and\n                     data.index.is_all_dates and\n                     not self.subplots or\n                     (self.subplots and self.sharex))\n\n        index_name = self._get_index_name()\n\n        if condition:\n            # irregular TS rotated 30 deg. by default\n            # probably a better place to check / set this.\n            if not self._rot_set:\n                self.rot = 30\n            format_date_labels(ax, rot=self.rot)\n\n        if index_name is not None and self.use_index:\n            ax.set_xlabel(index_name)\n\n\nclass AreaPlot(LinePlot):\n    _kind = 'area'\n\n    def __init__(self, data, **kwargs):\n        kwargs.setdefault('stacked', True)\n        data = data.fillna(value=0)\n        LinePlot.__init__(self, data, **kwargs)\n\n        if not self.stacked:\n            # use smaller alpha to distinguish overlap\n            self.kwds.setdefault('alpha', 0.5)\n\n        if self.logy or self.loglog:\n            raise ValueError(\"Log-y scales are not supported in area plot\")\n\n    @classmethod\n    def _plot(cls, ax, x, y, style=None, column_num=None,\n              stacking_id=None, is_errorbar=False, **kwds):\n\n        if column_num == 0:\n            cls._initialize_stacker(ax, stacking_id, len(y))\n        y_values = cls._get_stacked_values(ax, stacking_id, y, kwds['label'])\n\n        # need to remove label, because subplots uses mpl legend as it is\n        line_kwds = kwds.copy()\n        line_kwds.pop('label')\n        lines = MPLPlot._plot(ax, x, y_values, style=style, **line_kwds)\n\n        # get data from the line to get coordinates for fill_between\n        xdata, y_values = lines[0].get_data(orig=False)\n\n        # unable to use ``_get_stacked_values`` here to get starting point\n        if stacking_id is None:\n            start = np.zeros(len(y))\n        elif (y >= 0).all():\n            start = ax._stacker_pos_prior[stacking_id]\n        elif (y <= 0).all():\n            start = ax._stacker_neg_prior[stacking_id]\n        else:\n            start = np.zeros(len(y))\n\n        if 'color' not in kwds:\n            kwds['color'] = lines[0].get_color()\n\n        rect = ax.fill_between(xdata, start, y_values, **kwds)\n        cls._update_stacker(ax, stacking_id, y)\n\n        # LinePlot expects list of artists\n        res = [rect]\n        return res\n\n    def _post_plot_logic(self, ax, data):\n        LinePlot._post_plot_logic(self, ax, data)\n\n        if self.ylim is None:\n            if (data >= 0).all().all():\n                ax.set_ylim(0, None)\n            elif (data <= 0).all().all():\n                ax.set_ylim(None, 0)\n\n\nclass BarPlot(MPLPlot):\n    _kind = 'bar'\n    _default_rot = 90\n    orientation = 'vertical'\n\n    def __init__(self, data, **kwargs):\n        # we have to treat a series differently than a\n        # 1-column DataFrame w.r.t. color handling\n        self._is_series = isinstance(data, ABCSeries)\n        self.bar_width = kwargs.pop('width', 0.5)\n        pos = kwargs.pop('position', 0.5)\n        kwargs.setdefault('align', 'center')\n        self.tick_pos = np.arange(len(data))\n\n        self.bottom = kwargs.pop('bottom', 0)\n        self.left = kwargs.pop('left', 0)\n\n        self.log = kwargs.pop('log', False)\n        MPLPlot.__init__(self, data, **kwargs)\n\n        if self.stacked or self.subplots:\n            self.tickoffset = self.bar_width * pos\n            if kwargs['align'] == 'edge':\n                self.lim_offset = self.bar_width / 2\n            else:\n                self.lim_offset = 0\n        else:\n            if kwargs['align'] == 'edge':\n                w = self.bar_width / self.nseries\n                self.tickoffset = self.bar_width * (pos - 0.5) + w * 0.5\n                self.lim_offset = w * 0.5\n            else:\n                self.tickoffset = self.bar_width * pos\n                self.lim_offset = 0\n\n        self.ax_pos = self.tick_pos - self.tickoffset\n\n    def _args_adjust(self):\n        if is_list_like(self.bottom):\n            self.bottom = np.array(self.bottom)\n        if is_list_like(self.left):\n            self.left = np.array(self.left)\n\n    @classmethod\n    def _plot(cls, ax, x, y, w, start=0, log=False, **kwds):\n        return ax.bar(x, y, w, bottom=start, log=log, **kwds)\n\n    @property\n    def _start_base(self):\n        return self.bottom\n\n    def _make_plot(self):\n        import matplotlib as mpl\n\n        colors = self._get_colors()\n        ncolors = len(colors)\n\n        pos_prior = neg_prior = np.zeros(len(self.data))\n        K = self.nseries\n\n        for i, (label, y) in enumerate(self._iter_data(fillna=0)):\n            ax = self._get_ax(i)\n            kwds = self.kwds.copy()\n            if self._is_series:\n                kwds['color'] = colors\n            else:\n                kwds['color'] = colors[i % ncolors]\n\n            errors = self._get_errorbars(label=label, index=i)\n            kwds = dict(kwds, **errors)\n\n            label = pprint_thing(label)\n\n            if (('yerr' in kwds) or ('xerr' in kwds)) \\\n                    and (kwds.get('ecolor') is None):\n                kwds['ecolor'] = mpl.rcParams['xtick.color']\n\n            start = 0\n            if self.log and (y >= 1).all():\n                start = 1\n            start = start + self._start_base\n\n            if self.subplots:\n                w = self.bar_width / 2\n                rect = self._plot(ax, self.ax_pos + w, y, self.bar_width,\n                                  start=start, label=label,\n                                  log=self.log, **kwds)\n                ax.set_title(label)\n            elif self.stacked:\n                mask = y > 0\n                start = np.where(mask, pos_prior, neg_prior) + self._start_base\n                w = self.bar_width / 2\n                rect = self._plot(ax, self.ax_pos + w, y, self.bar_width,\n                                  start=start, label=label,\n                                  log=self.log, **kwds)\n                pos_prior = pos_prior + np.where(mask, y, 0)\n                neg_prior = neg_prior + np.where(mask, 0, y)\n            else:\n                w = self.bar_width / K\n                rect = self._plot(ax, self.ax_pos + (i + 0.5) * w, y, w,\n                                  start=start, label=label,\n                                  log=self.log, **kwds)\n            self._add_legend_handle(rect, label, index=i)\n\n    def _post_plot_logic(self, ax, data):\n        if self.use_index:\n            str_index = [pprint_thing(key) for key in data.index]\n        else:\n            str_index = [pprint_thing(key) for key in range(data.shape[0])]\n        name = self._get_index_name()\n\n        s_edge = self.ax_pos[0] - 0.25 + self.lim_offset\n        e_edge = self.ax_pos[-1] + 0.25 + self.bar_width + self.lim_offset\n\n        self._decorate_ticks(ax, name, str_index, s_edge, e_edge)\n\n    def _decorate_ticks(self, ax, name, ticklabels, start_edge, end_edge):\n        ax.set_xlim((start_edge, end_edge))\n        ax.set_xticks(self.tick_pos)\n        ax.set_xticklabels(ticklabels)\n        if name is not None and self.use_index:\n            ax.set_xlabel(name)\n\n\nclass BarhPlot(BarPlot):\n    _kind = 'barh'\n    _default_rot = 0\n    orientation = 'horizontal'\n\n    @property\n    def _start_base(self):\n        return self.left\n\n    @classmethod\n    def _plot(cls, ax, x, y, w, start=0, log=False, **kwds):\n        return ax.barh(x, y, w, left=start, log=log, **kwds)\n\n    def _decorate_ticks(self, ax, name, ticklabels, start_edge, end_edge):\n        # horizontal bars\n        ax.set_ylim((start_edge, end_edge))\n        ax.set_yticks(self.tick_pos)\n        ax.set_yticklabels(ticklabels)\n        if name is not None and self.use_index:\n            ax.set_ylabel(name)\n\n\nclass HistPlot(LinePlot):\n    _kind = 'hist'\n\n    def __init__(self, data, bins=10, bottom=0, **kwargs):\n        self.bins = bins        # use mpl default\n        self.bottom = bottom\n        # Do not call LinePlot.__init__ which may fill nan\n        MPLPlot.__init__(self, data, **kwargs)\n\n    def _args_adjust(self):\n        if is_integer(self.bins):\n            # create common bin edge\n            values = (self.data._convert(datetime=True)._get_numeric_data())\n            values = np.ravel(values)\n            values = values[~isna(values)]\n\n            hist, self.bins = np.histogram(\n                values, bins=self.bins,\n                range=self.kwds.get('range', None),\n                weights=self.kwds.get('weights', None))\n\n        if is_list_like(self.bottom):\n            self.bottom = np.array(self.bottom)\n\n    @classmethod\n    def _plot(cls, ax, y, style=None, bins=None, bottom=0, column_num=0,\n              stacking_id=None, **kwds):\n        if column_num == 0:\n            cls._initialize_stacker(ax, stacking_id, len(bins) - 1)\n        y = y[~isna(y)]\n\n        base = np.zeros(len(bins) - 1)\n        bottom = bottom + \\\n            cls._get_stacked_values(ax, stacking_id, base, kwds['label'])\n        # ignore style\n        n, bins, patches = ax.hist(y, bins=bins, bottom=bottom, **kwds)\n        cls._update_stacker(ax, stacking_id, n)\n        return patches\n\n    def _make_plot(self):\n        colors = self._get_colors()\n        stacking_id = self._get_stacking_id()\n\n        for i, (label, y) in enumerate(self._iter_data()):\n            ax = self._get_ax(i)\n\n            kwds = self.kwds.copy()\n\n            label = pprint_thing(label)\n            kwds['label'] = label\n\n            style, kwds = self._apply_style_colors(colors, kwds, i, label)\n            if style is not None:\n                kwds['style'] = style\n\n            kwds = self._make_plot_keywords(kwds, y)\n            artists = self._plot(ax, y, column_num=i,\n                                 stacking_id=stacking_id, **kwds)\n            self._add_legend_handle(artists[0], label, index=i)\n\n    def _make_plot_keywords(self, kwds, y):\n        \"\"\"merge BoxPlot/KdePlot properties to passed kwds\"\"\"\n        # y is required for KdePlot\n        kwds['bottom'] = self.bottom\n        kwds['bins'] = self.bins\n        return kwds\n\n    def _post_plot_logic(self, ax, data):\n        if self.orientation == 'horizontal':\n            ax.set_xlabel('Frequency')\n        else:\n            ax.set_ylabel('Frequency')\n\n    @property\n    def orientation(self):\n        if self.kwds.get('orientation', None) == 'horizontal':\n            return 'horizontal'\n        else:\n            return 'vertical'\n\n\n_kde_docstring = \"\"\"\n        Generate Kernel Density Estimate plot using Gaussian kernels.\n\n        In statistics, `kernel density estimation`_ (KDE) is a non-parametric\n        way to estimate the probability density function (PDF) of a random\n        variable. This function uses Gaussian kernels and includes automatic\n        bandwidth determination.\n\n        .. _kernel density estimation:\n            https://en.wikipedia.org/wiki/Kernel_density_estimation\n\n        Parameters\n        ----------\n        bw_method : str, scalar or callable, optional\n            The method used to calculate the estimator bandwidth. This can be\n            'scott', 'silverman', a scalar constant or a callable.\n            If None (default), 'scott' is used.\n            See :class:`scipy.stats.gaussian_kde` for more information.\n        ind : NumPy array or integer, optional\n            Evaluation points for the estimated PDF. If None (default),\n            1000 equally spaced points are used. If `ind` is a NumPy array, the\n            KDE is evaluated at the points passed. If `ind` is an integer,\n            `ind` number of equally spaced points are used.\n        **kwds : optional\n            Additional keyword arguments are documented in\n            :meth:`pandas.%(this-datatype)s.plot`.\n\n        Returns\n        -------\n        axes : matplotlib.axes.Axes or numpy.ndarray of them\n\n        See Also\n        --------\n        scipy.stats.gaussian_kde : Representation of a kernel-density\n            estimate using Gaussian kernels. This is the function used\n            internally to estimate the PDF.\n        %(sibling-datatype)s.plot.kde : Generate a KDE plot for a\n            %(sibling-datatype)s.\n\n        Examples\n        --------\n        %(examples)s\n        \"\"\"\n\n\nclass KdePlot(HistPlot):\n    _kind = 'kde'\n    orientation = 'vertical'\n\n    def __init__(self, data, bw_method=None, ind=None, **kwargs):\n        MPLPlot.__init__(self, data, **kwargs)\n        self.bw_method = bw_method\n        self.ind = ind\n\n    def _args_adjust(self):\n        pass\n\n    def _get_ind(self, y):\n        if self.ind is None:\n            # np.nanmax() and np.nanmin() ignores the missing values\n            sample_range = np.nanmax(y) - np.nanmin(y)\n            ind = np.linspace(np.nanmin(y) - 0.5 * sample_range,\n                              np.nanmax(y) + 0.5 * sample_range, 1000)\n        elif is_integer(self.ind):\n            sample_range = np.nanmax(y) - np.nanmin(y)\n            ind = np.linspace(np.nanmin(y) - 0.5 * sample_range,\n                              np.nanmax(y) + 0.5 * sample_range, self.ind)\n        else:\n            ind = self.ind\n        return ind\n\n    @classmethod\n    def _plot(cls, ax, y, style=None, bw_method=None, ind=None,\n              column_num=None, stacking_id=None, **kwds):\n        from scipy.stats import gaussian_kde\n        from scipy import __version__ as spv\n\n        y = remove_na_arraylike(y)\n\n        if LooseVersion(spv) >= '0.11.0':\n            gkde = gaussian_kde(y, bw_method=bw_method)\n        else:\n            gkde = gaussian_kde(y)\n            if bw_method is not None:\n                msg = ('bw_method was added in Scipy 0.11.0.' +\n                       ' Scipy version in use is {spv}.'.format(spv=spv))\n                warnings.warn(msg)\n\n        y = gkde.evaluate(ind)\n        lines = MPLPlot._plot(ax, ind, y, style=style, **kwds)\n        return lines\n\n    def _make_plot_keywords(self, kwds, y):\n        kwds['bw_method'] = self.bw_method\n        kwds['ind'] = self._get_ind(y)\n        return kwds\n\n    def _post_plot_logic(self, ax, data):\n        ax.set_ylabel('Density')\n\n\nclass PiePlot(MPLPlot):\n    _kind = 'pie'\n    _layout_type = 'horizontal'\n\n    def __init__(self, data, kind=None, **kwargs):\n        data = data.fillna(value=0)\n        if (data < 0).any().any():\n            raise ValueError(\"{0} doesn't allow negative values\".format(kind))\n        MPLPlot.__init__(self, data, kind=kind, **kwargs)\n\n    def _args_adjust(self):\n        self.grid = False\n        self.logy = False\n        self.logx = False\n        self.loglog = False\n\n    def _validate_color_args(self):\n        pass\n\n    def _make_plot(self):\n        colors = self._get_colors(\n            num_colors=len(self.data), color_kwds='colors')\n        self.kwds.setdefault('colors', colors)\n\n        for i, (label, y) in enumerate(self._iter_data()):\n            ax = self._get_ax(i)\n            if label is not None:\n                label = pprint_thing(label)\n                ax.set_ylabel(label)\n\n            kwds = self.kwds.copy()\n\n            def blank_labeler(label, value):\n                if value == 0:\n                    return ''\n                else:\n                    return label\n\n            idx = [pprint_thing(v) for v in self.data.index]\n            labels = kwds.pop('labels', idx)\n            # labels is used for each wedge's labels\n            # Blank out labels for values of 0 so they don't overlap\n            # with nonzero wedges\n            if labels is not None:\n                blabels = [blank_labeler(l, value) for\n                           l, value in zip(labels, y)]\n            else:\n                blabels = None\n            results = ax.pie(y, labels=blabels, **kwds)\n\n            if kwds.get('autopct', None) is not None:\n                patches, texts, autotexts = results\n            else:\n                patches, texts = results\n                autotexts = []\n\n            if self.fontsize is not None:\n                for t in texts + autotexts:\n                    t.set_fontsize(self.fontsize)\n\n            # leglabels is used for legend labels\n            leglabels = labels if labels is not None else idx\n            for p, l in zip(patches, leglabels):\n                self._add_legend_handle(p, l)\n\n\nclass BoxPlot(LinePlot):\n    _kind = 'box'\n    _layout_type = 'horizontal'\n\n    _valid_return_types = (None, 'axes', 'dict', 'both')\n    # namedtuple to hold results\n    BP = namedtuple(\"Boxplot\", ['ax', 'lines'])\n\n    def __init__(self, data, return_type='axes', **kwargs):\n        # Do not call LinePlot.__init__ which may fill nan\n        if return_type not in self._valid_return_types:\n            raise ValueError(\n                \"return_type must be {None, 'axes', 'dict', 'both'}\")\n\n        self.return_type = return_type\n        MPLPlot.__init__(self, data, **kwargs)\n\n    def _args_adjust(self):\n        if self.subplots:\n            # Disable label ax sharing. Otherwise, all subplots shows last\n            # column label\n            if self.orientation == 'vertical':\n                self.sharex = False\n            else:\n                self.sharey = False\n\n    @classmethod\n    def _plot(cls, ax, y, column_num=None, return_type='axes', **kwds):\n        if y.ndim == 2:\n            y = [remove_na_arraylike(v) for v in y]\n            # Boxplot fails with empty arrays, so need to add a NaN\n            #   if any cols are empty\n            # GH 8181\n            y = [v if v.size > 0 else np.array([np.nan]) for v in y]\n        else:\n            y = remove_na_arraylike(y)\n        bp = ax.boxplot(y, **kwds)\n\n        if return_type == 'dict':\n            return bp, bp\n        elif return_type == 'both':\n            return cls.BP(ax=ax, lines=bp), bp\n        else:\n            return ax, bp\n\n    def _validate_color_args(self):\n        if 'color' in self.kwds:\n            if self.colormap is not None:\n                warnings.warn(\"'color' and 'colormap' cannot be used \"\n                              \"simultaneously. Using 'color'\")\n            self.color = self.kwds.pop('color')\n\n            if isinstance(self.color, dict):\n                valid_keys = ['boxes', 'whiskers', 'medians', 'caps']\n                for key, values in compat.iteritems(self.color):\n                    if key not in valid_keys:\n                        raise ValueError(\"color dict contains invalid \"\n                                         \"key '{0}' \"\n                                         \"The key must be either {1}\"\n                                         .format(key, valid_keys))\n        else:\n            self.color = None\n\n        # get standard colors for default\n        colors = _get_standard_colors(num_colors=3,\n                                      colormap=self.colormap,\n                                      color=None)\n        # use 2 colors by default, for box/whisker and median\n        # flier colors isn't needed here\n        # because it can be specified by ``sym`` kw\n        self._boxes_c = colors[0]\n        self._whiskers_c = colors[0]\n        self._medians_c = colors[2]\n        self._caps_c = 'k'          # mpl default\n\n    def _get_colors(self, num_colors=None, color_kwds='color'):\n        pass\n\n    def maybe_color_bp(self, bp):\n        if isinstance(self.color, dict):\n            boxes = self.color.get('boxes', self._boxes_c)\n            whiskers = self.color.get('whiskers', self._whiskers_c)\n            medians = self.color.get('medians', self._medians_c)\n            caps = self.color.get('caps', self._caps_c)\n        else:\n            # Other types are forwarded to matplotlib\n            # If None, use default colors\n            boxes = self.color or self._boxes_c\n            whiskers = self.color or self._whiskers_c\n            medians = self.color or self._medians_c\n            caps = self.color or self._caps_c\n\n        from matplotlib.artist import setp\n        setp(bp['boxes'], color=boxes, alpha=1)\n        setp(bp['whiskers'], color=whiskers, alpha=1)\n        setp(bp['medians'], color=medians, alpha=1)\n        setp(bp['caps'], color=caps, alpha=1)\n\n    def _make_plot(self):\n        if self.subplots:\n            from pandas.core.series import Series\n            self._return_obj = Series()\n\n            for i, (label, y) in enumerate(self._iter_data()):\n                ax = self._get_ax(i)\n                kwds = self.kwds.copy()\n\n                ret, bp = self._plot(ax, y, column_num=i,\n                                     return_type=self.return_type, **kwds)\n                self.maybe_color_bp(bp)\n                self._return_obj[label] = ret\n\n                label = [pprint_thing(label)]\n                self._set_ticklabels(ax, label)\n        else:\n            y = self.data.values.T\n            ax = self._get_ax(0)\n            kwds = self.kwds.copy()\n\n            ret, bp = self._plot(ax, y, column_num=0,\n                                 return_type=self.return_type, **kwds)\n            self.maybe_color_bp(bp)\n            self._return_obj = ret\n\n            labels = [l for l, _ in self._iter_data()]\n            labels = [pprint_thing(l) for l in labels]\n            if not self.use_index:\n                labels = [pprint_thing(key) for key in range(len(labels))]\n            self._set_ticklabels(ax, labels)\n\n    def _set_ticklabels(self, ax, labels):\n        if self.orientation == 'vertical':\n            ax.set_xticklabels(labels)\n        else:\n            ax.set_yticklabels(labels)\n\n    def _make_legend(self):\n        pass\n\n    def _post_plot_logic(self, ax, data):\n        pass\n\n    @property\n    def orientation(self):\n        if self.kwds.get('vert', True):\n            return 'vertical'\n        else:\n            return 'horizontal'\n\n    @property\n    def result(self):\n        if self.return_type is None:\n            return super(BoxPlot, self).result\n        else:\n            return self._return_obj\n\n\n# kinds supported by both dataframe and series\n_common_kinds = ['line', 'bar', 'barh',\n                 'kde', 'density', 'area', 'hist', 'box']\n# kinds supported by dataframe\n_dataframe_kinds = ['scatter', 'hexbin']\n# kinds supported only by series or dataframe single column\n_series_kinds = ['pie']\n_all_kinds = _common_kinds + _dataframe_kinds + _series_kinds\n\n_klasses = [LinePlot, BarPlot, BarhPlot, KdePlot, HistPlot, BoxPlot,\n            ScatterPlot, HexBinPlot, AreaPlot, PiePlot]\n\n_plot_klass = {}\nfor klass in _klasses:\n    _plot_klass[klass._kind] = klass\n\n\ndef _plot(data, x=None, y=None, subplots=False,\n          ax=None, kind='line', **kwds):\n    kind = _get_standard_kind(kind.lower().strip())\n    if kind in _all_kinds:\n        klass = _plot_klass[kind]\n    else:\n        raise ValueError(\"%r is not a valid plot kind\" % kind)\n\n    if kind in _dataframe_kinds:\n        if isinstance(data, ABCDataFrame):\n            plot_obj = klass(data, x=x, y=y, subplots=subplots, ax=ax,\n                             kind=kind, **kwds)\n        else:\n            raise ValueError(\"plot kind %r can only be used for data frames\"\n                             % kind)\n\n    elif kind in _series_kinds:\n        if isinstance(data, ABCDataFrame):\n            if y is None and subplots is False:\n                msg = \"{0} requires either y column or 'subplots=True'\"\n                raise ValueError(msg.format(kind))\n            elif y is not None:\n                if is_integer(y) and not data.columns.holds_integer():\n                    y = data.columns[y]\n                # converted to series actually. copy to not modify\n                data = data[y].copy()\n                data.index.name = y\n        plot_obj = klass(data, subplots=subplots, ax=ax, kind=kind, **kwds)\n    else:\n        if isinstance(data, ABCDataFrame):\n            data_cols = data.columns\n            if x is not None:\n                if is_integer(x) and not data.columns.holds_integer():\n                    x = data_cols[x]\n                elif not isinstance(data[x], ABCSeries):\n                    raise ValueError(\"x must be a label or position\")\n                data = data.set_index(x)\n\n            if y is not None:\n                # check if we have y as int or list of ints\n                int_ylist = is_list_like(y) and all(is_integer(c) for c in y)\n                int_y_arg = is_integer(y) or int_ylist\n                if int_y_arg and not data.columns.holds_integer():\n                    y = data_cols[y]\n\n                label_kw = kwds['label'] if 'label' in kwds else False\n                for kw in ['xerr', 'yerr']:\n                    if (kw in kwds) and \\\n                        (isinstance(kwds[kw], string_types) or\n                            is_integer(kwds[kw])):\n                        try:\n                            kwds[kw] = data[kwds[kw]]\n                        except (IndexError, KeyError, TypeError):\n                            pass\n\n                # don't overwrite\n                data = data[y].copy()\n\n                if isinstance(data, ABCSeries):\n                    label_name = label_kw or y\n                    data.name = label_name\n                else:\n                    match = is_list_like(label_kw) and len(label_kw) == len(y)\n                    if label_kw and not match:\n                        raise ValueError(\n                            \"label should be list-like and same length as y\"\n                        )\n                    label_name = label_kw or data.columns\n                    data.columns = label_name\n\n        plot_obj = klass(data, subplots=subplots, ax=ax, kind=kind, **kwds)\n\n    plot_obj.generate()\n    plot_obj.draw()\n    return plot_obj.result\n\n\ndf_kind = \"\"\"- 'scatter' : scatter plot\n        - 'hexbin' : hexbin plot\"\"\"\nseries_kind = \"\"\n\ndf_coord = \"\"\"x : label or position, default None\n    y : label, position or list of label, positions, default None\n        Allows plotting of one column versus another\"\"\"\nseries_coord = \"\"\n\ndf_unique = \"\"\"stacked : boolean, default False in line and\n        bar plots, and True in area plot. If True, create stacked plot.\n    sort_columns : boolean, default False\n        Sort column names to determine plot ordering\n    secondary_y : boolean or sequence, default False\n        Whether to plot on the secondary y-axis\n        If a list/tuple, which columns to plot on secondary y-axis\"\"\"\nseries_unique = \"\"\"label : label argument to provide to plot\n    secondary_y : boolean or sequence of ints, default False\n        If True then y-axis will be on the right\"\"\"\n\ndf_ax = \"\"\"ax : matplotlib axes object, default None\n    subplots : boolean, default False\n        Make separate subplots for each column\n    sharex : boolean, default True if ax is None else False\n        In case subplots=True, share x axis and set some x axis labels to\n        invisible; defaults to True if ax is None otherwise False if an ax\n        is passed in; Be aware, that passing in both an ax and sharex=True\n        will alter all x axis labels for all axis in a figure!\n    sharey : boolean, default False\n        In case subplots=True, share y axis and set some y axis labels to\n        invisible\n    layout : tuple (optional)\n        (rows, columns) for the layout of subplots\"\"\"\nseries_ax = \"\"\"ax : matplotlib axes object\n        If not passed, uses gca()\"\"\"\n\ndf_note = \"\"\"- If `kind` = 'scatter' and the argument `c` is the name of a dataframe\n      column, the values of that column are used to color each point.\n    - If `kind` = 'hexbin', you can control the size of the bins with the\n      `gridsize` argument. By default, a histogram of the counts around each\n      `(x, y)` point is computed. You can specify alternative aggregations\n      by passing values to the `C` and `reduce_C_function` arguments.\n      `C` specifies the value at each `(x, y)` point and `reduce_C_function`\n      is a function of one argument that reduces all the values in a bin to\n      a single number (e.g. `mean`, `max`, `sum`, `std`).\"\"\"\nseries_note = \"\"\n\n_shared_doc_df_kwargs = dict(klass='DataFrame', klass_obj='df',\n                             klass_kind=df_kind, klass_coord=df_coord,\n                             klass_ax=df_ax, klass_unique=df_unique,\n                             klass_note=df_note)\n_shared_doc_series_kwargs = dict(klass='Series', klass_obj='s',\n                                 klass_kind=series_kind,\n                                 klass_coord=series_coord, klass_ax=series_ax,\n                                 klass_unique=series_unique,\n                                 klass_note=series_note)\n\n_shared_docs['plot'] = \"\"\"\n    Make plots of %(klass)s using matplotlib / pylab.\n\n    *New in version 0.17.0:* Each plot kind has a corresponding method on the\n    ``%(klass)s.plot`` accessor:\n    ``%(klass_obj)s.plot(kind='line')`` is equivalent to\n    ``%(klass_obj)s.plot.line()``.\n\n    Parameters\n    ----------\n    data : %(klass)s\n    %(klass_coord)s\n    kind : str\n        - 'line' : line plot (default)\n        - 'bar' : vertical bar plot\n        - 'barh' : horizontal bar plot\n        - 'hist' : histogram\n        - 'box' : boxplot\n        - 'kde' : Kernel Density Estimation plot\n        - 'density' : same as 'kde'\n        - 'area' : area plot\n        - 'pie' : pie plot\n        %(klass_kind)s\n    %(klass_ax)s\n    figsize : a tuple (width, height) in inches\n    use_index : boolean, default True\n        Use index as ticks for x axis\n    title : string or list\n        Title to use for the plot. If a string is passed, print the string at\n        the top of the figure. If a list is passed and `subplots` is True,\n        print each item in the list above the corresponding subplot.\n    grid : boolean, default None (matlab style default)\n        Axis grid lines\n    legend : False/True/'reverse'\n        Place legend on axis subplots\n    style : list or dict\n        matplotlib line style per column\n    logx : boolean, default False\n        Use log scaling on x axis\n    logy : boolean, default False\n        Use log scaling on y axis\n    loglog : boolean, default False\n        Use log scaling on both x and y axes\n    xticks : sequence\n        Values to use for the xticks\n    yticks : sequence\n        Values to use for the yticks\n    xlim : 2-tuple/list\n    ylim : 2-tuple/list\n    rot : int, default None\n        Rotation for ticks (xticks for vertical, yticks for horizontal plots)\n    fontsize : int, default None\n        Font size for xticks and yticks\n    colormap : str or matplotlib colormap object, default None\n        Colormap to select colors from. If string, load colormap with that name\n        from matplotlib.\n    colorbar : boolean, optional\n        If True, plot colorbar (only relevant for 'scatter' and 'hexbin' plots)\n    position : float\n        Specify relative alignments for bar plot layout.\n        From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)\n    table : boolean, Series or DataFrame, default False\n        If True, draw a table using the data in the DataFrame and the data will\n        be transposed to meet matplotlib's default layout.\n        If a Series or DataFrame is passed, use passed data to draw a table.\n    yerr : DataFrame, Series, array-like, dict and str\n        See :ref:`Plotting with Error Bars <visualization.errorbars>` for\n        detail.\n    xerr : same types as yerr.\n    %(klass_unique)s\n    mark_right : boolean, default True\n        When using a secondary_y axis, automatically mark the column\n        labels with \"(right)\" in the legend\n    `**kwds` : keywords\n        Options to pass to matplotlib plotting method\n\n    Returns\n    -------\n    axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n\n    Notes\n    -----\n\n    - See matplotlib documentation online for more on this subject\n    - If `kind` = 'bar' or 'barh', you can specify relative alignments\n      for bar plot layout by `position` keyword.\n      From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)\n    %(klass_note)s\n\n    \"\"\"\n\n\n@Appender(_shared_docs['plot'] % _shared_doc_df_kwargs)\ndef plot_frame(data, x=None, y=None, kind='line', ax=None,\n               subplots=False, sharex=None, sharey=False, layout=None,\n               figsize=None, use_index=True, title=None, grid=None,\n               legend=True, style=None, logx=False, logy=False, loglog=False,\n               xticks=None, yticks=None, xlim=None, ylim=None,\n               rot=None, fontsize=None, colormap=None, table=False,\n               yerr=None, xerr=None,\n               secondary_y=False, sort_columns=False,\n               **kwds):\n    return _plot(data, kind=kind, x=x, y=y, ax=ax,\n                 subplots=subplots, sharex=sharex, sharey=sharey,\n                 layout=layout, figsize=figsize, use_index=use_index,\n                 title=title, grid=grid, legend=legend,\n                 style=style, logx=logx, logy=logy, loglog=loglog,\n                 xticks=xticks, yticks=yticks, xlim=xlim, ylim=ylim,\n                 rot=rot, fontsize=fontsize, colormap=colormap, table=table,\n                 yerr=yerr, xerr=xerr,\n                 secondary_y=secondary_y, sort_columns=sort_columns,\n                 **kwds)\n\n\n@Appender(_shared_docs['plot'] % _shared_doc_series_kwargs)\ndef plot_series(data, kind='line', ax=None,                    # Series unique\n                figsize=None, use_index=True, title=None, grid=None,\n                legend=False, style=None, logx=False, logy=False, loglog=False,\n                xticks=None, yticks=None, xlim=None, ylim=None,\n                rot=None, fontsize=None, colormap=None, table=False,\n                yerr=None, xerr=None,\n                label=None, secondary_y=False,                 # Series unique\n                **kwds):\n\n    import matplotlib.pyplot as plt\n    if ax is None and len(plt.get_fignums()) > 0:\n        ax = _gca()\n        ax = MPLPlot._get_ax_layer(ax)\n    return _plot(data, kind=kind, ax=ax,\n                 figsize=figsize, use_index=use_index, title=title,\n                 grid=grid, legend=legend,\n                 style=style, logx=logx, logy=logy, loglog=loglog,\n                 xticks=xticks, yticks=yticks, xlim=xlim, ylim=ylim,\n                 rot=rot, fontsize=fontsize, colormap=colormap, table=table,\n                 yerr=yerr, xerr=xerr,\n                 label=label, secondary_y=secondary_y,\n                 **kwds)\n\n\n_shared_docs['boxplot'] = \"\"\"\n    Make a box plot from DataFrame columns.\n\n    Make a box-and-whisker plot from DataFrame columns, optionally grouped\n    by some other columns. A box plot is a method for graphically depicting\n    groups of numerical data through their quartiles.\n    The box extends from the Q1 to Q3 quartile values of the data,\n    with a line at the median (Q2). The whiskers extend from the edges\n    of box to show the range of the data. The position of the whiskers\n    is set by default to `1.5 * IQR (IQR = Q3 - Q1)` from the edges of the box.\n    Outlier points are those past the end of the whiskers.\n\n    For further details see\n    Wikipedia's entry for `boxplot <https://en.wikipedia.org/wiki/Box_plot>`_.\n\n    Parameters\n    ----------\n    column : str or list of str, optional\n        Column name or list of names, or vector.\n        Can be any valid input to :meth:`pandas.DataFrame.groupby`.\n    by : str or array-like, optional\n        Column in the DataFrame to :meth:`pandas.DataFrame.groupby`.\n        One box-plot will be done per value of columns in `by`.\n    ax : object of class matplotlib.axes.Axes, optional\n        The matplotlib axes to be used by boxplot.\n    fontsize : float or str\n        Tick label font size in points or as a string (e.g., `large`).\n    rot : int or float, default 0\n        The rotation angle of labels (in degrees)\n        with respect to the screen coordinate system.\n    grid : boolean, default True\n        Setting this to True will show the grid.\n    figsize : A tuple (width, height) in inches\n        The size of the figure to create in matplotlib.\n    layout : tuple (rows, columns), optional\n        For example, (3, 5) will display the subplots\n        using 3 columns and 5 rows, starting from the top-left.\n    return_type : {'axes', 'dict', 'both'} or None, default 'axes'\n        The kind of object to return. The default is ``axes``.\n\n        * 'axes' returns the matplotlib axes the boxplot is drawn on.\n        * 'dict' returns a dictionary whose values are the matplotlib\n          Lines of the boxplot.\n        * 'both' returns a namedtuple with the axes and dict.\n        * when grouping with ``by``, a Series mapping columns to\n          ``return_type`` is returned.\n\n          If ``return_type`` is `None`, a NumPy array\n          of axes with the same shape as ``layout`` is returned.\n    **kwds\n        All other plotting keyword arguments to be passed to\n        :func:`matplotlib.pyplot.boxplot`.\n\n    Returns\n    -------\n    result :\n\n        The return type depends on the `return_type` parameter:\n\n        * 'axes' : object of class matplotlib.axes.Axes\n        * 'dict' : dict of matplotlib.lines.Line2D objects\n        * 'both' : a namedtuple with structure (ax, lines)\n\n        For data grouped with ``by``:\n\n        * :class:`~pandas.Series`\n        * :class:`~numpy.array` (for ``return_type = None``)\n\n    See Also\n    --------\n    Series.plot.hist: Make a histogram.\n    matplotlib.pyplot.boxplot : Matplotlib equivalent plot.\n\n    Notes\n    -----\n    Use ``return_type='dict'`` when you want to tweak the appearance\n    of the lines after plotting. In this case a dict containing the Lines\n    making up the boxes, caps, fliers, medians, and whiskers is returned.\n\n    Examples\n    --------\n\n    Boxplots can be created for every column in the dataframe\n    by ``df.boxplot()`` or indicating the columns to be used:\n\n    .. plot::\n        :context: close-figs\n\n        >>> np.random.seed(1234)\n        >>> df = pd.DataFrame(np.random.randn(10,4),\n        ...                   columns=['Col1', 'Col2', 'Col3', 'Col4'])\n        >>> boxplot = df.boxplot(column=['Col1', 'Col2', 'Col3'])\n\n    Boxplots of variables distributions grouped by the values of a third\n    variable can be created using the option ``by``. For instance:\n\n    .. plot::\n        :context: close-figs\n\n        >>> df = pd.DataFrame(np.random.randn(10, 2),\n        ...                   columns=['Col1', 'Col2'])\n        >>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A',\n        ...                      'B', 'B', 'B', 'B', 'B'])\n        >>> boxplot = df.boxplot(by='X')\n\n    A list of strings (i.e. ``['X', 'Y']``) can be passed to boxplot\n    in order to group the data by combination of the variables in the x-axis:\n\n    .. plot::\n        :context: close-figs\n\n        >>> df = pd.DataFrame(np.random.randn(10,3),\n        ...                   columns=['Col1', 'Col2', 'Col3'])\n        >>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A',\n        ...                      'B', 'B', 'B', 'B', 'B'])\n        >>> df['Y'] = pd.Series(['A', 'B', 'A', 'B', 'A',\n        ...                      'B', 'A', 'B', 'A', 'B'])\n        >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by=['X', 'Y'])\n\n    The layout of boxplot can be adjusted giving a tuple to ``layout``:\n\n    .. plot::\n        :context: close-figs\n\n        >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n        ...                      layout=(2, 1))\n\n    Additional formatting can be done to the boxplot, like suppressing the grid\n    (``grid=False``), rotating the labels in the x-axis (i.e. ``rot=45``)\n    or changing the fontsize (i.e. ``fontsize=15``):\n\n    .. plot::\n        :context: close-figs\n\n        >>> boxplot = df.boxplot(grid=False, rot=45, fontsize=15)\n\n    The parameter ``return_type`` can be used to select the type of element\n    returned by `boxplot`.  When ``return_type='axes'`` is selected,\n    the matplotlib axes on which the boxplot is drawn are returned:\n\n        >>> boxplot = df.boxplot(column=['Col1','Col2'], return_type='axes')\n        >>> type(boxplot)\n        <class 'matplotlib.axes._subplots.AxesSubplot'>\n\n    When grouping with ``by``, a Series mapping columns to ``return_type``\n    is returned:\n\n        >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n        ...                      return_type='axes')\n        >>> type(boxplot)\n        <class 'pandas.core.series.Series'>\n\n    If ``return_type`` is `None`, a NumPy array of axes with the same shape\n    as ``layout`` is returned:\n\n        >>> boxplot =  df.boxplot(column=['Col1', 'Col2'], by='X',\n        ...                       return_type=None)\n        >>> type(boxplot)\n        <class 'numpy.ndarray'>\n    \"\"\"\n\n\n@Appender(_shared_docs['boxplot'] % _shared_doc_kwargs)\ndef boxplot(data, column=None, by=None, ax=None, fontsize=None,\n            rot=0, grid=True, figsize=None, layout=None, return_type=None,\n            **kwds):\n\n    # validate return_type:\n    if return_type not in BoxPlot._valid_return_types:\n        raise ValueError(\"return_type must be {'axes', 'dict', 'both'}\")\n\n    if isinstance(data, ABCSeries):\n        data = data.to_frame('x')\n        column = 'x'\n\n    def _get_colors():\n        return _get_standard_colors(color=kwds.get('color'), num_colors=1)\n\n    def maybe_color_bp(bp):\n        if 'color' not in kwds:\n            from matplotlib.artist import setp\n            setp(bp['boxes'], color=colors[0], alpha=1)\n            setp(bp['whiskers'], color=colors[0], alpha=1)\n            setp(bp['medians'], color=colors[2], alpha=1)\n\n    def plot_group(keys, values, ax):\n        keys = [pprint_thing(x) for x in keys]\n        values = [np.asarray(remove_na_arraylike(v)) for v in values]\n        bp = ax.boxplot(values, **kwds)\n        if fontsize is not None:\n            ax.tick_params(axis='both', labelsize=fontsize)\n        if kwds.get('vert', 1):\n            ax.set_xticklabels(keys, rotation=rot)\n        else:\n            ax.set_yticklabels(keys, rotation=rot)\n        maybe_color_bp(bp)\n\n        # Return axes in multiplot case, maybe revisit later # 985\n        if return_type == 'dict':\n            return bp\n        elif return_type == 'both':\n            return BoxPlot.BP(ax=ax, lines=bp)\n        else:\n            return ax\n\n    colors = _get_colors()\n    if column is None:\n        columns = None\n    else:\n        if isinstance(column, (list, tuple)):\n            columns = column\n        else:\n            columns = [column]\n\n    if by is not None:\n        # Prefer array return type for 2-D plots to match the subplot layout\n        # https://github.com/pandas-dev/pandas/pull/12216#issuecomment-241175580\n        result = _grouped_plot_by_column(plot_group, data, columns=columns,\n                                         by=by, grid=grid, figsize=figsize,\n                                         ax=ax, layout=layout,\n                                         return_type=return_type)\n    else:\n        if return_type is None:\n            return_type = 'axes'\n        if layout is not None:\n            raise ValueError(\"The 'layout' keyword is not supported when \"\n                             \"'by' is None\")\n\n        if ax is None:\n            rc = {'figure.figsize': figsize} if figsize is not None else {}\n            ax = _gca(rc)\n        data = data._get_numeric_data()\n        if columns is None:\n            columns = data.columns\n        else:\n            data = data[columns]\n\n        result = plot_group(columns, data.values.T, ax)\n        ax.grid(grid)\n\n    return result\n\n\n@Appender(_shared_docs['boxplot'] % _shared_doc_kwargs)\ndef boxplot_frame(self, column=None, by=None, ax=None, fontsize=None, rot=0,\n                  grid=True, figsize=None, layout=None,\n                  return_type=None, **kwds):\n    import matplotlib.pyplot as plt\n    _converter._WARN = False\n    ax = boxplot(self, column=column, by=by, ax=ax, fontsize=fontsize,\n                 grid=grid, rot=rot, figsize=figsize, layout=layout,\n                 return_type=return_type, **kwds)\n    plt.draw_if_interactive()\n    return ax\n\n\ndef scatter_plot(data, x, y, by=None, ax=None, figsize=None, grid=False,\n                 **kwargs):\n    \"\"\"\n    Make a scatter plot from two DataFrame columns\n\n    Parameters\n    ----------\n    data : DataFrame\n    x : Column name for the x-axis values\n    y : Column name for the y-axis values\n    ax : Matplotlib axis object\n    figsize : A tuple (width, height) in inches\n    grid : Setting this to True will show the grid\n    kwargs : other plotting keyword arguments\n        To be passed to scatter function\n\n    Returns\n    -------\n    fig : matplotlib.Figure\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    kwargs.setdefault('edgecolors', 'none')\n\n    def plot_group(group, ax):\n        xvals = group[x].values\n        yvals = group[y].values\n        ax.scatter(xvals, yvals, **kwargs)\n        ax.grid(grid)\n\n    if by is not None:\n        fig = _grouped_plot(plot_group, data, by=by, figsize=figsize, ax=ax)\n    else:\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        else:\n            fig = ax.get_figure()\n        plot_group(data, ax)\n        ax.set_ylabel(pprint_thing(y))\n        ax.set_xlabel(pprint_thing(x))\n\n        ax.grid(grid)\n\n    return fig\n\n\ndef hist_frame(data, column=None, by=None, grid=True, xlabelsize=None,\n               xrot=None, ylabelsize=None, yrot=None, ax=None, sharex=False,\n               sharey=False, figsize=None, layout=None, bins=10, **kwds):\n    \"\"\"\n    Make a histogram of the DataFrame's.\n\n    A `histogram`_ is a representation of the distribution of data.\n    This function calls :meth:`matplotlib.pyplot.hist`, on each series in\n    the DataFrame, resulting in one histogram per column.\n\n    .. _histogram: https://en.wikipedia.org/wiki/Histogram\n\n    Parameters\n    ----------\n    data : DataFrame\n        The pandas object holding the data.\n    column : string or sequence\n        If passed, will be used to limit data to a subset of columns.\n    by : object, optional\n        If passed, then used to form histograms for separate groups.\n    grid : boolean, default True\n        Whether to show axis grid lines.\n    xlabelsize : int, default None\n        If specified changes the x-axis label size.\n    xrot : float, default None\n        Rotation of x axis labels. For example, a value of 90 displays the\n        x labels rotated 90 degrees clockwise.\n    ylabelsize : int, default None\n        If specified changes the y-axis label size.\n    yrot : float, default None\n        Rotation of y axis labels. For example, a value of 90 displays the\n        y labels rotated 90 degrees clockwise.\n    ax : Matplotlib axes object, default None\n        The axes to plot the histogram on.\n    sharex : boolean, default True if ax is None else False\n        In case subplots=True, share x axis and set some x axis labels to\n        invisible; defaults to True if ax is None otherwise False if an ax\n        is passed in.\n        Note that passing in both an ax and sharex=True will alter all x axis\n        labels for all subplots in a figure.\n    sharey : boolean, default False\n        In case subplots=True, share y axis and set some y axis labels to\n        invisible.\n    figsize : tuple\n        The size in inches of the figure to create. Uses the value in\n        `matplotlib.rcParams` by default.\n    layout : tuple, optional\n        Tuple of (rows, columns) for the layout of the histograms.\n    bins : integer or sequence, default 10\n        Number of histogram bins to be used. If an integer is given, bins + 1\n        bin edges are calculated and returned. If bins is a sequence, gives\n        bin edges, including left edge of first bin and right edge of last\n        bin. In this case, bins is returned unmodified.\n    **kwds\n        All other plotting keyword arguments to be passed to\n        :meth:`matplotlib.pyplot.hist`.\n\n    Returns\n    -------\n    axes : matplotlib.AxesSubplot or numpy.ndarray of them\n\n    See Also\n    --------\n    matplotlib.pyplot.hist : Plot a histogram using matplotlib.\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        This example draws a histogram based on the length and width of\n        some animals, displayed in three bins\n\n        >>> df = pd.DataFrame({\n        ...     'length': [1.5, 0.5, 1.2, 0.9, 3],\n        ...     'width': [0.7, 0.2, 0.15, 0.2, 1.1]\n        ...     }, index= ['pig', 'rabbit', 'duck', 'chicken', 'horse'])\n        >>> hist = df.hist(bins=3)\n    \"\"\"\n    _raise_if_no_mpl()\n    _converter._WARN = False\n    if by is not None:\n        axes = grouped_hist(data, column=column, by=by, ax=ax, grid=grid,\n                            figsize=figsize, sharex=sharex, sharey=sharey,\n                            layout=layout, bins=bins, xlabelsize=xlabelsize,\n                            xrot=xrot, ylabelsize=ylabelsize,\n                            yrot=yrot, **kwds)\n        return axes\n\n    if column is not None:\n        if not isinstance(column, (list, np.ndarray, ABCIndexClass)):\n            column = [column]\n        data = data[column]\n    data = data._get_numeric_data()\n    naxes = len(data.columns)\n\n    fig, axes = _subplots(naxes=naxes, ax=ax, squeeze=False,\n                          sharex=sharex, sharey=sharey, figsize=figsize,\n                          layout=layout)\n    _axes = _flatten(axes)\n\n    for i, col in enumerate(com.try_sort(data.columns)):\n        ax = _axes[i]\n        ax.hist(data[col].dropna().values, bins=bins, **kwds)\n        ax.set_title(col)\n        ax.grid(grid)\n\n    _set_ticks_props(axes, xlabelsize=xlabelsize, xrot=xrot,\n                     ylabelsize=ylabelsize, yrot=yrot)\n    fig.subplots_adjust(wspace=0.3, hspace=0.3)\n\n    return axes\n\n\ndef hist_series(self, by=None, ax=None, grid=True, xlabelsize=None,\n                xrot=None, ylabelsize=None, yrot=None, figsize=None,\n                bins=10, **kwds):\n    \"\"\"\n    Draw histogram of the input series using matplotlib\n\n    Parameters\n    ----------\n    by : object, optional\n        If passed, then used to form histograms for separate groups\n    ax : matplotlib axis object\n        If not passed, uses gca()\n    grid : boolean, default True\n        Whether to show axis grid lines\n    xlabelsize : int, default None\n        If specified changes the x-axis label size\n    xrot : float, default None\n        rotation of x axis labels\n    ylabelsize : int, default None\n        If specified changes the y-axis label size\n    yrot : float, default None\n        rotation of y axis labels\n    figsize : tuple, default None\n        figure size in inches by default\n    bins : integer or sequence, default 10\n        Number of histogram bins to be used. If an integer is given, bins + 1\n        bin edges are calculated and returned. If bins is a sequence, gives\n        bin edges, including left edge of first bin and right edge of last\n        bin. In this case, bins is returned unmodified.\n    bins : integer, default 10\n        Number of histogram bins to be used\n    `**kwds` : keywords\n        To be passed to the actual plotting function\n\n    See Also\n    --------\n    matplotlib.axes.Axes.hist : Plot a histogram using matplotlib.\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    if by is None:\n        if kwds.get('layout', None) is not None:\n            raise ValueError(\"The 'layout' keyword is not supported when \"\n                             \"'by' is None\")\n        # hack until the plotting interface is a bit more unified\n        fig = kwds.pop('figure', plt.gcf() if plt.get_fignums() else\n                       plt.figure(figsize=figsize))\n        if (figsize is not None and tuple(figsize) !=\n                tuple(fig.get_size_inches())):\n            fig.set_size_inches(*figsize, forward=True)\n        if ax is None:\n            ax = fig.gca()\n        elif ax.get_figure() != fig:\n            raise AssertionError('passed axis not bound to passed figure')\n        values = self.dropna().values\n\n        ax.hist(values, bins=bins, **kwds)\n        ax.grid(grid)\n        axes = np.array([ax])\n\n        _set_ticks_props(axes, xlabelsize=xlabelsize, xrot=xrot,\n                         ylabelsize=ylabelsize, yrot=yrot)\n\n    else:\n        if 'figure' in kwds:\n            raise ValueError(\"Cannot pass 'figure' when using the \"\n                             \"'by' argument, since a new 'Figure' instance \"\n                             \"will be created\")\n        axes = grouped_hist(self, by=by, ax=ax, grid=grid, figsize=figsize,\n                            bins=bins, xlabelsize=xlabelsize, xrot=xrot,\n                            ylabelsize=ylabelsize, yrot=yrot, **kwds)\n\n    if hasattr(axes, 'ndim'):\n        if axes.ndim == 1 and len(axes) == 1:\n            return axes[0]\n    return axes\n\n\ndef grouped_hist(data, column=None, by=None, ax=None, bins=50, figsize=None,\n                 layout=None, sharex=False, sharey=False, rot=90, grid=True,\n                 xlabelsize=None, xrot=None, ylabelsize=None, yrot=None,\n                 **kwargs):\n    \"\"\"\n    Grouped histogram\n\n    Parameters\n    ----------\n    data : Series/DataFrame\n    column : object, optional\n    by : object, optional\n    ax : axes, optional\n    bins : int, default 50\n    figsize : tuple, optional\n    layout : optional\n    sharex : boolean, default False\n    sharey : boolean, default False\n    rot : int, default 90\n    grid : bool, default True\n    kwargs : dict, keyword arguments passed to matplotlib.Axes.hist\n\n    Returns\n    -------\n    axes : collection of Matplotlib Axes\n    \"\"\"\n    _raise_if_no_mpl()\n    _converter._WARN = False\n\n    def plot_group(group, ax):\n        ax.hist(group.dropna().values, bins=bins, **kwargs)\n\n    xrot = xrot or rot\n\n    fig, axes = _grouped_plot(plot_group, data, column=column,\n                              by=by, sharex=sharex, sharey=sharey, ax=ax,\n                              figsize=figsize, layout=layout, rot=rot)\n\n    _set_ticks_props(axes, xlabelsize=xlabelsize, xrot=xrot,\n                     ylabelsize=ylabelsize, yrot=yrot)\n\n    fig.subplots_adjust(bottom=0.15, top=0.9, left=0.1, right=0.9,\n                        hspace=0.5, wspace=0.3)\n    return axes\n\n\ndef boxplot_frame_groupby(grouped, subplots=True, column=None, fontsize=None,\n                          rot=0, grid=True, ax=None, figsize=None,\n                          layout=None, sharex=False, sharey=True, **kwds):\n    \"\"\"\n    Make box plots from DataFrameGroupBy data.\n\n    Parameters\n    ----------\n    grouped : Grouped DataFrame\n    subplots :\n        * ``False`` - no subplots will be used\n        * ``True`` - create a subplot for each group\n    column : column name or list of names, or vector\n        Can be any valid input to groupby\n    fontsize : int or string\n    rot : label rotation angle\n    grid : Setting this to True will show the grid\n    ax : Matplotlib axis object, default None\n    figsize : A tuple (width, height) in inches\n    layout : tuple (optional)\n        (rows, columns) for the layout of the plot\n    sharex : bool, default False\n        Whether x-axes will be shared among subplots\n\n        .. versionadded:: 0.23.1\n    sharey : bool, default True\n        Whether y-axes will be shared among subplots\n\n        .. versionadded:: 0.23.1\n    `**kwds` : Keyword Arguments\n        All other plotting keyword arguments to be passed to\n        matplotlib's boxplot function\n\n    Returns\n    -------\n    dict of key/value = group key/DataFrame.boxplot return value\n    or DataFrame.boxplot return value in case subplots=figures=False\n\n    Examples\n    --------\n    >>> import itertools\n    >>> tuples = [t for t in itertools.product(range(1000), range(4))]\n    >>> index = pd.MultiIndex.from_tuples(tuples, names=['lvl0', 'lvl1'])\n    >>> data = np.random.randn(len(index),4)\n    >>> df = pd.DataFrame(data, columns=list('ABCD'), index=index)\n    >>>\n    >>> grouped = df.groupby(level='lvl1')\n    >>> boxplot_frame_groupby(grouped)\n    >>>\n    >>> grouped = df.unstack(level='lvl1').groupby(level=0, axis=1)\n    >>> boxplot_frame_groupby(grouped, subplots=False)\n    \"\"\"\n    _raise_if_no_mpl()\n    _converter._WARN = False\n    if subplots is True:\n        naxes = len(grouped)\n        fig, axes = _subplots(naxes=naxes, squeeze=False,\n                              ax=ax, sharex=sharex, sharey=sharey,\n                              figsize=figsize, layout=layout)\n        axes = _flatten(axes)\n\n        from pandas.core.series import Series\n        ret = Series()\n        for (key, group), ax in zip(grouped, axes):\n            d = group.boxplot(ax=ax, column=column, fontsize=fontsize,\n                              rot=rot, grid=grid, **kwds)\n            ax.set_title(pprint_thing(key))\n            ret.loc[key] = d\n        fig.subplots_adjust(bottom=0.15, top=0.9, left=0.1,\n                            right=0.9, wspace=0.2)\n    else:\n        from pandas.core.reshape.concat import concat\n        keys, frames = zip(*grouped)\n        if grouped.axis == 0:\n            df = concat(frames, keys=keys, axis=1)\n        else:\n            if len(frames) > 1:\n                df = frames[0].join(frames[1::])\n            else:\n                df = frames[0]\n        ret = df.boxplot(column=column, fontsize=fontsize, rot=rot,\n                         grid=grid, ax=ax, figsize=figsize,\n                         layout=layout, **kwds)\n    return ret\n\n\ndef _grouped_plot(plotf, data, column=None, by=None, numeric_only=True,\n                  figsize=None, sharex=True, sharey=True, layout=None,\n                  rot=0, ax=None, **kwargs):\n\n    if figsize == 'default':\n        # allowed to specify mpl default with 'default'\n        warnings.warn(\"figsize='default' is deprecated. Specify figure\"\n                      \"size by tuple instead\", FutureWarning, stacklevel=4)\n        figsize = None\n\n    grouped = data.groupby(by)\n    if column is not None:\n        grouped = grouped[column]\n\n    naxes = len(grouped)\n    fig, axes = _subplots(naxes=naxes, figsize=figsize,\n                          sharex=sharex, sharey=sharey, ax=ax,\n                          layout=layout)\n\n    _axes = _flatten(axes)\n\n    for i, (key, group) in enumerate(grouped):\n        ax = _axes[i]\n        if numeric_only and isinstance(group, ABCDataFrame):\n            group = group._get_numeric_data()\n        plotf(group, ax, **kwargs)\n        ax.set_title(pprint_thing(key))\n\n    return fig, axes\n\n\ndef _grouped_plot_by_column(plotf, data, columns=None, by=None,\n                            numeric_only=True, grid=False,\n                            figsize=None, ax=None, layout=None,\n                            return_type=None, **kwargs):\n    grouped = data.groupby(by)\n    if columns is None:\n        if not isinstance(by, (list, tuple)):\n            by = [by]\n        columns = data._get_numeric_data().columns.difference(by)\n    naxes = len(columns)\n    fig, axes = _subplots(naxes=naxes, sharex=True, sharey=True,\n                          figsize=figsize, ax=ax, layout=layout)\n\n    _axes = _flatten(axes)\n\n    ax_values = []\n\n    for i, col in enumerate(columns):\n        ax = _axes[i]\n        gp_col = grouped[col]\n        keys, values = zip(*gp_col)\n        re_plotf = plotf(keys, values, ax, **kwargs)\n        ax.set_title(col)\n        ax.set_xlabel(pprint_thing(by))\n        ax_values.append(re_plotf)\n        ax.grid(grid)\n\n    from pandas.core.series import Series\n    result = Series(ax_values, index=columns)\n\n    # Return axes in multiplot case, maybe revisit later # 985\n    if return_type is None:\n        result = axes\n\n    byline = by[0] if len(by) == 1 else by\n    fig.suptitle('Boxplot grouped by {byline}'.format(byline=byline))\n    fig.subplots_adjust(bottom=0.15, top=0.9, left=0.1, right=0.9, wspace=0.2)\n\n    return result\n\n\nclass BasePlotMethods(PandasObject):\n\n    def __init__(self, data):\n        self._parent = data  # can be Series or DataFrame\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass SeriesPlotMethods(BasePlotMethods):\n    \"\"\"Series plotting accessor and method\n\n    Examples\n    --------\n    >>> s.plot.line()\n    >>> s.plot.bar()\n    >>> s.plot.hist()\n\n    Plotting methods can also be accessed by calling the accessor as a method\n    with the ``kind`` argument:\n    ``s.plot(kind='line')`` is equivalent to ``s.plot.line()``\n    \"\"\"\n\n    def __call__(self, kind='line', ax=None,\n                 figsize=None, use_index=True, title=None, grid=None,\n                 legend=False, style=None, logx=False, logy=False,\n                 loglog=False, xticks=None, yticks=None,\n                 xlim=None, ylim=None,\n                 rot=None, fontsize=None, colormap=None, table=False,\n                 yerr=None, xerr=None,\n                 label=None, secondary_y=False, **kwds):\n        return plot_series(self._parent, kind=kind, ax=ax, figsize=figsize,\n                           use_index=use_index, title=title, grid=grid,\n                           legend=legend, style=style, logx=logx, logy=logy,\n                           loglog=loglog, xticks=xticks, yticks=yticks,\n                           xlim=xlim, ylim=ylim, rot=rot, fontsize=fontsize,\n                           colormap=colormap, table=table, yerr=yerr,\n                           xerr=xerr, label=label, secondary_y=secondary_y,\n                           **kwds)\n    __call__.__doc__ = plot_series.__doc__\n\n    def line(self, **kwds):\n        \"\"\"\n        Line plot\n\n        Parameters\n        ----------\n        `**kwds` : optional\n            Additional keyword arguments are documented in\n            :meth:`pandas.Series.plot`.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n\n        Examples\n        --------\n\n        .. plot::\n            :context: close-figs\n\n            >>> s = pd.Series([1, 3, 2])\n            >>> s.plot.line()\n        \"\"\"\n        return self(kind='line', **kwds)\n\n    def bar(self, **kwds):\n        \"\"\"\n        Vertical bar plot\n\n        Parameters\n        ----------\n        `**kwds` : optional\n            Additional keyword arguments are documented in\n            :meth:`pandas.Series.plot`.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n        \"\"\"\n        return self(kind='bar', **kwds)\n\n    def barh(self, **kwds):\n        \"\"\"\n        Horizontal bar plot\n\n        Parameters\n        ----------\n        `**kwds` : optional\n            Additional keyword arguments are documented in\n            :meth:`pandas.Series.plot`.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n        \"\"\"\n        return self(kind='barh', **kwds)\n\n    def box(self, **kwds):\n        \"\"\"\n        Boxplot\n\n        Parameters\n        ----------\n        `**kwds` : optional\n            Additional keyword arguments are documented in\n            :meth:`pandas.Series.plot`.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n        \"\"\"\n        return self(kind='box', **kwds)\n\n    def hist(self, bins=10, **kwds):\n        \"\"\"\n        Histogram\n\n        Parameters\n        ----------\n        bins : integer, default 10\n            Number of histogram bins to be used\n        `**kwds` : optional\n            Additional keyword arguments are documented in\n            :meth:`pandas.Series.plot`.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n        \"\"\"\n        return self(kind='hist', bins=bins, **kwds)\n\n    @Appender(_kde_docstring % {\n        'this-datatype': 'Series',\n        'sibling-datatype': 'DataFrame',\n        'examples': \"\"\"\n        Given a Series of points randomly sampled from an unknown\n        distribution, estimate its PDF using KDE with automatic\n        bandwidth determination and plot the results, evaluating them at\n        1000 equally spaced points (default):\n\n        .. plot::\n            :context: close-figs\n\n            >>> s = pd.Series([1, 2, 2.5, 3, 3.5, 4, 5])\n            >>> ax = s.plot.kde()\n\n        A scalar bandwidth can be specified. Using a small bandwidth value can\n        lead to over-fitting, while using a large bandwidth value may result\n        in under-fitting:\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = s.plot.kde(bw_method=0.3)\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = s.plot.kde(bw_method=3)\n\n        Finally, the `ind` parameter determines the evaluation points for the\n        plot of the estimated PDF:\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = s.plot.kde(ind=[1, 2, 3, 4, 5])\n        \"\"\".strip()\n    })\n    def kde(self, bw_method=None, ind=None, **kwds):\n        return self(kind='kde', bw_method=bw_method, ind=ind, **kwds)\n\n    density = kde\n\n    def area(self, **kwds):\n        \"\"\"\n        Area plot\n\n        Parameters\n        ----------\n        `**kwds` : optional\n            Additional keyword arguments are documented in\n            :meth:`pandas.Series.plot`.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n        \"\"\"\n        return self(kind='area', **kwds)\n\n    def pie(self, **kwds):\n        \"\"\"\n        Pie chart\n\n        Parameters\n        ----------\n        `**kwds` : optional\n            Additional keyword arguments are documented in\n            :meth:`pandas.Series.plot`.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n        \"\"\"\n        return self(kind='pie', **kwds)\n\n\nclass FramePlotMethods(BasePlotMethods):\n    \"\"\"DataFrame plotting accessor and method\n\n    Examples\n    --------\n    >>> df.plot.line()\n    >>> df.plot.scatter('x', 'y')\n    >>> df.plot.hexbin()\n\n    These plotting methods can also be accessed by calling the accessor as a\n    method with the ``kind`` argument:\n    ``df.plot(kind='line')`` is equivalent to ``df.plot.line()``\n    \"\"\"\n\n    def __call__(self, x=None, y=None, kind='line', ax=None,\n                 subplots=False, sharex=None, sharey=False, layout=None,\n                 figsize=None, use_index=True, title=None, grid=None,\n                 legend=True, style=None, logx=False, logy=False, loglog=False,\n                 xticks=None, yticks=None, xlim=None, ylim=None,\n                 rot=None, fontsize=None, colormap=None, table=False,\n                 yerr=None, xerr=None,\n                 secondary_y=False, sort_columns=False, **kwds):\n        return plot_frame(self._parent, kind=kind, x=x, y=y, ax=ax,\n                          subplots=subplots, sharex=sharex, sharey=sharey,\n                          layout=layout, figsize=figsize, use_index=use_index,\n                          title=title, grid=grid, legend=legend, style=style,\n                          logx=logx, logy=logy, loglog=loglog, xticks=xticks,\n                          yticks=yticks, xlim=xlim, ylim=ylim, rot=rot,\n                          fontsize=fontsize, colormap=colormap, table=table,\n                          yerr=yerr, xerr=xerr, secondary_y=secondary_y,\n                          sort_columns=sort_columns, **kwds)\n    __call__.__doc__ = plot_frame.__doc__\n\n    def line(self, x=None, y=None, **kwds):\n        \"\"\"\n        Plot DataFrame columns as lines.\n\n        This function is useful to plot lines using DataFrame's values\n        as coordinates.\n\n        Parameters\n        ----------\n        x : int or str, optional\n            Columns to use for the horizontal axis.\n            Either the location or the label of the columns to be used.\n            By default, it will use the DataFrame indices.\n        y : int, str, or list of them, optional\n            The values to be plotted.\n            Either the location or the label of the columns to be used.\n            By default, it will use the remaining DataFrame numeric columns.\n        **kwds\n            Keyword arguments to pass on to :meth:`pandas.DataFrame.plot`.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or :class:`numpy.ndarray`\n            Returns an ndarray when ``subplots=True``.\n\n        See Also\n        --------\n        matplotlib.pyplot.plot : Plot y versus x as lines and/or markers.\n\n        Examples\n        --------\n\n        .. plot::\n            :context: close-figs\n\n            The following example shows the populations for some animals\n            over the years.\n\n            >>> df = pd.DataFrame({\n            ...    'pig': [20, 18, 489, 675, 1776],\n            ...    'horse': [4, 25, 281, 600, 1900]\n            ...    }, index=[1990, 1997, 2003, 2009, 2014])\n            >>> lines = df.plot.line()\n\n        .. plot::\n           :context: close-figs\n\n           An example with subplots, so an array of axes is returned.\n\n           >>> axes = df.plot.line(subplots=True)\n           >>> type(axes)\n           <class 'numpy.ndarray'>\n\n        .. plot::\n            :context: close-figs\n\n            The following example shows the relationship between both\n            populations.\n\n            >>> lines = df.plot.line(x='pig', y='horse')\n        \"\"\"\n        return self(kind='line', x=x, y=y, **kwds)\n\n    def bar(self, x=None, y=None, **kwds):\n        \"\"\"\n        Vertical bar plot.\n\n        A bar plot is a plot that presents categorical data with\n        rectangular bars with lengths proportional to the values that they\n        represent. A bar plot shows comparisons among discrete categories. One\n        axis of the plot shows the specific categories being compared, and the\n        other axis represents a measured value.\n\n        Parameters\n        ----------\n        x : label or position, optional\n            Allows plotting of one column versus another. If not specified,\n            the index of the DataFrame is used.\n        y : label or position, optional\n            Allows plotting of one column versus another. If not specified,\n            all numerical columns are used.\n        **kwds\n            Additional keyword arguments are documented in\n            :meth:`pandas.DataFrame.plot`.\n\n        Returns\n        -------\n        axes : matplotlib.axes.Axes or np.ndarray of them\n            An ndarray is returned with one :class:`matplotlib.axes.Axes`\n            per column when ``subplots=True``.\n\n        See Also\n        --------\n        pandas.DataFrame.plot.barh : Horizontal bar plot.\n        pandas.DataFrame.plot : Make plots of a DataFrame.\n        matplotlib.pyplot.bar : Make a bar plot with matplotlib.\n\n        Examples\n        --------\n        Basic plot.\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n            >>> ax = df.plot.bar(x='lab', y='val', rot=0)\n\n        Plot a whole dataframe to a bar plot. Each column is assigned a\n        distinct color, and each row is nested in a group along the\n        horizontal axis.\n\n        .. plot::\n            :context: close-figs\n\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n            >>> index = ['snail', 'pig', 'elephant',\n            ...          'rabbit', 'giraffe', 'coyote', 'horse']\n            >>> df = pd.DataFrame({'speed': speed,\n            ...                    'lifespan': lifespan}, index=index)\n            >>> ax = df.plot.bar(rot=0)\n\n        Instead of nesting, the figure can be split by column with\n        ``subplots=True``. In this case, a :class:`numpy.ndarray` of\n        :class:`matplotlib.axes.Axes` are returned.\n\n        .. plot::\n            :context: close-figs\n\n            >>> axes = df.plot.bar(rot=0, subplots=True)\n            >>> axes[1].legend(loc=2)  # doctest: +SKIP\n\n        Plot a single column.\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = df.plot.bar(y='speed', rot=0)\n\n        Plot only selected categories for the DataFrame.\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = df.plot.bar(x='lifespan', rot=0)\n        \"\"\"\n        return self(kind='bar', x=x, y=y, **kwds)\n\n    def barh(self, x=None, y=None, **kwds):\n        \"\"\"\n        Make a horizontal bar plot.\n\n        A horizontal bar plot is a plot that presents quantitative data with\n        rectangular bars with lengths proportional to the values that they\n        represent. A bar plot shows comparisons among discrete categories. One\n        axis of the plot shows the specific categories being compared, and the\n        other axis represents a measured value.\n\n        Parameters\n        ----------\n        x : label or position, default DataFrame.index\n            Column to be used for categories.\n        y : label or position, default All numeric columns in dataframe\n            Columns to be plotted from the DataFrame.\n        **kwds\n            Keyword arguments to pass on to :meth:`pandas.DataFrame.plot`.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them.\n\n        See Also\n        --------\n        pandas.DataFrame.plot.bar: Vertical bar plot.\n        pandas.DataFrame.plot : Make plots of DataFrame using matplotlib.\n        matplotlib.axes.Axes.bar : Plot a vertical bar plot using matplotlib.\n\n        Examples\n        --------\n        Basic example\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n            >>> ax = df.plot.barh(x='lab', y='val')\n\n        Plot a whole DataFrame to a horizontal bar plot\n\n        .. plot::\n            :context: close-figs\n\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n            >>> index = ['snail', 'pig', 'elephant',\n            ...          'rabbit', 'giraffe', 'coyote', 'horse']\n            >>> df = pd.DataFrame({'speed': speed,\n            ...                    'lifespan': lifespan}, index=index)\n            >>> ax = df.plot.barh()\n\n        Plot a column of the DataFrame to a horizontal bar plot\n\n        .. plot::\n            :context: close-figs\n\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n            >>> index = ['snail', 'pig', 'elephant',\n            ...          'rabbit', 'giraffe', 'coyote', 'horse']\n            >>> df = pd.DataFrame({'speed': speed,\n            ...                    'lifespan': lifespan}, index=index)\n            >>> ax = df.plot.barh(y='speed')\n\n        Plot DataFrame versus the desired column\n\n        .. plot::\n            :context: close-figs\n\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n            >>> index = ['snail', 'pig', 'elephant',\n            ...          'rabbit', 'giraffe', 'coyote', 'horse']\n            >>> df = pd.DataFrame({'speed': speed,\n            ...                    'lifespan': lifespan}, index=index)\n            >>> ax = df.plot.barh(x='lifespan')\n        \"\"\"\n        return self(kind='barh', x=x, y=y, **kwds)\n\n    def box(self, by=None, **kwds):\n        r\"\"\"\n        Make a box plot of the DataFrame columns.\n\n        A box plot is a method for graphically depicting groups of numerical\n        data through their quartiles.\n        The box extends from the Q1 to Q3 quartile values of the data,\n        with a line at the median (Q2). The whiskers extend from the edges\n        of box to show the range of the data. The position of the whiskers\n        is set by default to 1.5*IQR (IQR = Q3 - Q1) from the edges of the\n        box. Outlier points are those past the end of the whiskers.\n\n        For further details see Wikipedia's\n        entry for `boxplot <https://en.wikipedia.org/wiki/Box_plot>`__.\n\n        A consideration when using this chart is that the box and the whiskers\n        can overlap, which is very common when plotting small sets of data.\n\n        Parameters\n        ----------\n        by : string or sequence\n            Column in the DataFrame to group by.\n        **kwds : optional\n            Additional keywords are documented in\n            :meth:`pandas.DataFrame.plot`.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n\n        See Also\n        --------\n        pandas.DataFrame.boxplot: Another method to draw a box plot.\n        pandas.Series.plot.box: Draw a box plot from a Series object.\n        matplotlib.pyplot.boxplot: Draw a box plot in matplotlib.\n\n        Examples\n        --------\n        Draw a box plot from a DataFrame with four columns of randomly\n        generated data.\n\n        .. plot::\n            :context: close-figs\n\n            >>> data = np.random.randn(25, 4)\n            >>> df = pd.DataFrame(data, columns=list('ABCD'))\n            >>> ax = df.plot.box()\n        \"\"\"\n        return self(kind='box', by=by, **kwds)\n\n    def hist(self, by=None, bins=10, **kwds):\n        \"\"\"\n        Draw one histogram of the DataFrame's columns.\n\n        A histogram is a representation of the distribution of data.\n        This function groups the values of all given Series in the DataFrame\n        into bins and draws all bins in one :class:`matplotlib.axes.Axes`.\n        This is useful when the DataFrame's Series are in a similar scale.\n\n        Parameters\n        ----------\n        by : str or sequence, optional\n            Column in the DataFrame to group by.\n        bins : int, default 10\n            Number of histogram bins to be used.\n        **kwds\n            Additional keyword arguments are documented in\n            :meth:`pandas.DataFrame.plot`.\n\n        Returns\n        -------\n        axes : matplotlib.AxesSubplot histogram.\n\n        See Also\n        --------\n        DataFrame.hist : Draw histograms per DataFrame's Series.\n        Series.hist : Draw a histogram with Series' data.\n\n        Examples\n        --------\n        When we draw a dice 6000 times, we expect to get each value around 1000\n        times. But when we draw two dices and sum the result, the distribution\n        is going to be quite different. A histogram illustrates those\n        distributions.\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = pd.DataFrame(\n            ...     np.random.randint(1, 7, 6000),\n            ...     columns = ['one'])\n            >>> df['two'] = df['one'] + np.random.randint(1, 7, 6000)\n            >>> ax = df.plot.hist(bins=12, alpha=0.5)\n        \"\"\"\n        return self(kind='hist', by=by, bins=bins, **kwds)\n\n    @Appender(_kde_docstring % {\n        'this-datatype': 'DataFrame',\n        'sibling-datatype': 'Series',\n        'examples': \"\"\"\n        Given several Series of points randomly sampled from unknown\n        distributions, estimate their PDFs using KDE with automatic\n        bandwidth determination and plot the results, evaluating them at\n        1000 equally spaced points (default):\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = pd.DataFrame({\n            ...     'x': [1, 2, 2.5, 3, 3.5, 4, 5],\n            ...     'y': [4, 4, 4.5, 5, 5.5, 6, 6],\n            ... })\n            >>> ax = df.plot.kde()\n\n        A scalar bandwidth can be specified. Using a small bandwidth value can\n        lead to over-fitting, while using a large bandwidth value may result\n        in under-fitting:\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = df.plot.kde(bw_method=0.3)\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = df.plot.kde(bw_method=3)\n\n        Finally, the `ind` parameter determines the evaluation points for the\n        plot of the estimated PDF:\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = df.plot.kde(ind=[1, 2, 3, 4, 5, 6])\n        \"\"\".strip()\n    })\n    def kde(self, bw_method=None, ind=None, **kwds):\n        return self(kind='kde', bw_method=bw_method, ind=ind, **kwds)\n\n    density = kde\n\n    def area(self, x=None, y=None, **kwds):\n        \"\"\"\n        Draw a stacked area plot.\n\n        An area plot displays quantitative data visually.\n        This function wraps the matplotlib area function.\n\n        Parameters\n        ----------\n        x : label or position, optional\n            Coordinates for the X axis. By default uses the index.\n        y : label or position, optional\n            Column to plot. By default uses all columns.\n        stacked : bool, default True\n            Area plots are stacked by default. Set to False to create a\n            unstacked plot.\n        **kwds : optional\n            Additional keyword arguments are documented in\n            :meth:`pandas.DataFrame.plot`.\n\n        Returns\n        -------\n        matplotlib.axes.Axes or numpy.ndarray\n            Area plot, or array of area plots if subplots is True\n\n        See Also\n        --------\n        DataFrame.plot : Make plots of DataFrame using matplotlib / pylab.\n\n        Examples\n        --------\n        Draw an area plot based on basic business metrics:\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = pd.DataFrame({\n            ...     'sales': [3, 2, 3, 9, 10, 6],\n            ...     'signups': [5, 5, 6, 12, 14, 13],\n            ...     'visits': [20, 42, 28, 62, 81, 50],\n            ... }, index=pd.date_range(start='2018/01/01', end='2018/07/01',\n            ...                        freq='M'))\n            >>> ax = df.plot.area()\n\n        Area plots are stacked by default. To produce an unstacked plot,\n        pass ``stacked=False``:\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = df.plot.area(stacked=False)\n\n        Draw an area plot for a single column:\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = df.plot.area(y='sales')\n\n        Draw with a different `x`:\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = pd.DataFrame({\n            ...     'sales': [3, 2, 3],\n            ...     'visits': [20, 42, 28],\n            ...     'day': [1, 2, 3],\n            ... })\n            >>> ax = df.plot.area(x='day')\n        \"\"\"\n        return self(kind='area', x=x, y=y, **kwds)\n\n    def pie(self, y=None, **kwds):\n        \"\"\"\n        Generate a pie plot.\n\n        A pie plot is a proportional representation of the numerical data in a\n        column. This function wraps :meth:`matplotlib.pyplot.pie` for the\n        specified column. If no column reference is passed and\n        ``subplots=True`` a pie plot is drawn for each numerical column\n        independently.\n\n        Parameters\n        ----------\n        y : int or label, optional\n            Label or position of the column to plot.\n            If not provided, ``subplots=True`` argument must be passed.\n        **kwds\n            Keyword arguments to pass on to :meth:`pandas.DataFrame.plot`.\n\n        Returns\n        -------\n        axes : matplotlib.axes.Axes or np.ndarray of them.\n            A NumPy array is returned when `subplots` is True.\n\n        See Also\n        --------\n        Series.plot.pie : Generate a pie plot for a Series.\n        DataFrame.plot : Make plots of a DataFrame.\n\n        Examples\n        --------\n        In the example below we have a DataFrame with the information about\n        planet's mass and radius. We pass the the 'mass' column to the\n        pie function to get a pie plot.\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = pd.DataFrame({'mass': [0.330, 4.87 , 5.97],\n            ...                    'radius': [2439.7, 6051.8, 6378.1]},\n            ...                   index=['Mercury', 'Venus', 'Earth'])\n            >>> plot = df.plot.pie(y='mass', figsize=(5, 5))\n\n        .. plot::\n            :context: close-figs\n\n            >>> plot = df.plot.pie(subplots=True, figsize=(6, 3))\n        \"\"\"\n        return self(kind='pie', y=y, **kwds)\n\n    def scatter(self, x, y, s=None, c=None, **kwds):\n        \"\"\"\n        Create a scatter plot with varying marker point size and color.\n\n        The coordinates of each point are defined by two dataframe columns and\n        filled circles are used to represent each point. This kind of plot is\n        useful to see complex correlations between two variables. Points could\n        be for instance natural 2D coordinates like longitude and latitude in\n        a map or, in general, any pair of metrics that can be plotted against\n        each other.\n\n        Parameters\n        ----------\n        x : int or str\n            The column name or column position to be used as horizontal\n            coordinates for each point.\n        y : int or str\n            The column name or column position to be used as vertical\n            coordinates for each point.\n        s : scalar or array_like, optional\n            The size of each point. Possible values are:\n\n            - A single scalar so all points have the same size.\n\n            - A sequence of scalars, which will be used for each point's size\n              recursively. For instance, when passing [2,14] all points size\n              will be either 2 or 14, alternatively.\n\n        c : str, int or array_like, optional\n            The color of each point. Possible values are:\n\n            - A single color string referred to by name, RGB or RGBA code,\n              for instance 'red' or '#a98d19'.\n\n            - A sequence of color strings referred to by name, RGB or RGBA\n              code, which will be used for each point's color recursively. For\n              instance ['green','yellow'] all points will be filled in green or\n              yellow, alternatively.\n\n            - A column name or position whose values will be used to color the\n              marker points according to a colormap.\n\n        **kwds\n            Keyword arguments to pass on to :meth:`pandas.DataFrame.plot`.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n\n        See Also\n        --------\n        matplotlib.pyplot.scatter : Scatter plot using multiple input data\n            formats.\n\n        Examples\n        --------\n        Let's see how to draw a scatter plot using coordinates from the values\n        in a DataFrame's columns.\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = pd.DataFrame([[5.1, 3.5, 0], [4.9, 3.0, 0], [7.0, 3.2, 1],\n            ...                    [6.4, 3.2, 1], [5.9, 3.0, 2]],\n            ...                   columns=['length', 'width', 'species'])\n            >>> ax1 = df.plot.scatter(x='length',\n            ...                       y='width',\n            ...                       c='DarkBlue')\n\n        And now with the color determined by a column as well.\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax2 = df.plot.scatter(x='length',\n            ...                       y='width',\n            ...                       c='species',\n            ...                       colormap='viridis')\n        \"\"\"\n        return self(kind='scatter', x=x, y=y, c=c, s=s, **kwds)\n\n    def hexbin(self, x, y, C=None, reduce_C_function=None, gridsize=None,\n               **kwds):\n        \"\"\"\n        Generate a hexagonal binning plot.\n\n        Generate a hexagonal binning plot of `x` versus `y`. If `C` is `None`\n        (the default), this is a histogram of the number of occurrences\n        of the observations at ``(x[i], y[i])``.\n\n        If `C` is specified, specifies values at given coordinates\n        ``(x[i], y[i])``. These values are accumulated for each hexagonal\n        bin and then reduced according to `reduce_C_function`,\n        having as default the NumPy's mean function (:meth:`numpy.mean`).\n        (If `C` is specified, it must also be a 1-D sequence\n        of the same length as `x` and `y`, or a column label.)\n\n        Parameters\n        ----------\n        x : int or str\n            The column label or position for x points.\n        y : int or str\n            The column label or position for y points.\n        C : int or str, optional\n            The column label or position for the value of `(x, y)` point.\n        reduce_C_function : callable, default `np.mean`\n            Function of one argument that reduces all the values in a bin to\n            a single number (e.g. `np.mean`, `np.max`, `np.sum`, `np.std`).\n        gridsize : int or tuple of (int, int), default 100\n            The number of hexagons in the x-direction.\n            The corresponding number of hexagons in the y-direction is\n            chosen in a way that the hexagons are approximately regular.\n            Alternatively, gridsize can be a tuple with two elements\n            specifying the number of hexagons in the x-direction and the\n            y-direction.\n        **kwds\n            Additional keyword arguments are documented in\n            :meth:`pandas.DataFrame.plot`.\n\n        Returns\n        -------\n        matplotlib.AxesSubplot\n            The matplotlib ``Axes`` on which the hexbin is plotted.\n\n        See Also\n        --------\n        DataFrame.plot : Make plots of a DataFrame.\n        matplotlib.pyplot.hexbin : Hexagonal binning plot using matplotlib,\n            the matplotlib function that is used under the hood.\n\n        Examples\n        --------\n        The following examples are generated with random data from\n        a normal distribution.\n\n        .. plot::\n            :context: close-figs\n\n            >>> n = 10000\n            >>> df = pd.DataFrame({'x': np.random.randn(n),\n            ...                    'y': np.random.randn(n)})\n            >>> ax = df.plot.hexbin(x='x', y='y', gridsize=20)\n\n        The next example uses `C` and `np.sum` as `reduce_C_function`.\n        Note that `'observations'` values ranges from 1 to 5 but the result\n        plot shows values up to more than 25. This is because of the\n        `reduce_C_function`.\n\n        .. plot::\n            :context: close-figs\n\n            >>> n = 500\n            >>> df = pd.DataFrame({\n            ...     'coord_x': np.random.uniform(-3, 3, size=n),\n            ...     'coord_y': np.random.uniform(30, 50, size=n),\n            ...     'observations': np.random.randint(1,5, size=n)\n            ...     })\n            >>> ax = df.plot.hexbin(x='coord_x',\n            ...                     y='coord_y',\n            ...                     C='observations',\n            ...                     reduce_C_function=np.sum,\n            ...                     gridsize=10,\n            ...                     cmap=\"viridis\")\n        \"\"\"\n        if reduce_C_function is not None:\n            kwds['reduce_C_function'] = reduce_C_function\n        if gridsize is not None:\n            kwds['gridsize'] = gridsize\n        return self(kind='hexbin', x=x, y=y, C=C, **kwds)\n"
    },
    {
      "filename": "pandas/tseries/offsets.py",
      "content": "# -*- coding: utf-8 -*-\nfrom datetime import date, datetime, timedelta\nimport functools\nimport operator\n\nfrom dateutil.easter import easter\nimport numpy as np\n\nfrom pandas._libs.tslibs import (\n    NaT, OutOfBoundsDatetime, Timedelta, Timestamp, ccalendar, conversion,\n    delta_to_nanoseconds, frequencies as libfrequencies, normalize_date,\n    offsets as liboffsets)\nfrom pandas._libs.tslibs.offsets import (\n    ApplyTypeError, BaseOffset, _get_calendar, _is_normalized, _to_dt64,\n    apply_index_wraps, as_datetime, roll_yearday, shift_month)\nimport pandas.compat as compat\nfrom pandas.compat import range\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import cache_readonly\n\nfrom pandas.core.dtypes.generic import ABCPeriod\n\nfrom pandas.core.tools.datetimes import to_datetime\n\n__all__ = ['Day', 'BusinessDay', 'BDay', 'CustomBusinessDay', 'CDay',\n           'CBMonthEnd', 'CBMonthBegin',\n           'MonthBegin', 'BMonthBegin', 'MonthEnd', 'BMonthEnd',\n           'SemiMonthEnd', 'SemiMonthBegin',\n           'BusinessHour', 'CustomBusinessHour',\n           'YearBegin', 'BYearBegin', 'YearEnd', 'BYearEnd',\n           'QuarterBegin', 'BQuarterBegin', 'QuarterEnd', 'BQuarterEnd',\n           'LastWeekOfMonth', 'FY5253Quarter', 'FY5253',\n           'Week', 'WeekOfMonth', 'Easter',\n           'Hour', 'Minute', 'Second', 'Milli', 'Micro', 'Nano',\n           'DateOffset', 'CalendarDay']\n\n# convert to/from datetime/timestamp to allow invalid Timestamp ranges to\n# pass thru\n\n\ndef as_timestamp(obj):\n    if isinstance(obj, Timestamp):\n        return obj\n    try:\n        return Timestamp(obj)\n    except (OutOfBoundsDatetime):\n        pass\n    return obj\n\n\ndef apply_wraps(func):\n    @functools.wraps(func)\n    def wrapper(self, other):\n        if other is NaT:\n            return NaT\n        elif isinstance(other, (timedelta, Tick, DateOffset)):\n            # timedelta path\n            return func(self, other)\n        elif isinstance(other, (np.datetime64, datetime, date)):\n            other = as_timestamp(other)\n\n        tz = getattr(other, 'tzinfo', None)\n        nano = getattr(other, 'nanosecond', 0)\n\n        try:\n            if self._adjust_dst and isinstance(other, Timestamp):\n                other = other.tz_localize(None)\n\n            result = func(self, other)\n\n            if self._adjust_dst:\n                result = conversion.localize_pydatetime(result, tz)\n\n            result = Timestamp(result)\n            if self.normalize:\n                result = result.normalize()\n\n            # nanosecond may be deleted depending on offset process\n            if not self.normalize and nano != 0:\n                if not isinstance(self, Nano) and result.nanosecond != nano:\n                    if result.tz is not None:\n                        # convert to UTC\n                        value = conversion.tz_convert_single(\n                            result.value, 'UTC', result.tz)\n                    else:\n                        value = result.value\n                    result = Timestamp(value + nano)\n\n            if tz is not None and result.tzinfo is None:\n                result = conversion.localize_pydatetime(result, tz)\n\n        except OutOfBoundsDatetime:\n            result = func(self, as_datetime(other))\n\n            if self.normalize:\n                # normalize_date returns normal datetime\n                result = normalize_date(result)\n\n            if tz is not None and result.tzinfo is None:\n                result = conversion.localize_pydatetime(result, tz)\n\n        return result\n    return wrapper\n\n\n# ---------------------------------------------------------------------\n# DateOffset\n\n\nclass DateOffset(BaseOffset):\n    \"\"\"\n    Standard kind of date increment used for a date range.\n\n    Works exactly like relativedelta in terms of the keyword args you\n    pass in, use of the keyword n is discouraged-- you would be better\n    off specifying n in the keywords you use, but regardless it is\n    there for you. n is needed for DateOffset subclasses.\n\n    DateOffets work as follows.  Each offset specify a set of dates\n    that conform to the DateOffset.  For example, Bday defines this\n    set to be the set of dates that are weekdays (M-F).  To test if a\n    date is in the set of a DateOffset dateOffset we can use the\n    onOffset method: dateOffset.onOffset(date).\n\n    If a date is not on a valid date, the rollback and rollforward\n    methods can be used to roll the date to the nearest valid date\n    before/after the date.\n\n    DateOffsets can be created to move dates forward a given number of\n    valid dates.  For example, Bday(2) can be added to a date to move\n    it two business days forward.  If the date does not start on a\n    valid date, first it is moved to a valid date.  Thus pseudo code\n    is:\n\n    def __add__(date):\n      date = rollback(date) # does nothing if date is valid\n      return date + <n number of periods>\n\n    When a date offset is created for a negative number of periods,\n    the date is first rolled forward.  The pseudo code is:\n\n    def __add__(date):\n      date = rollforward(date) # does nothing is date is valid\n      return date + <n number of periods>\n\n    Zero presents a problem.  Should it roll forward or back?  We\n    arbitrarily have it rollforward:\n\n    date + BDay(0) == BDay.rollforward(date)\n\n    Since 0 is a bit weird, we suggest avoiding its use.\n\n    Parameters\n    ----------\n    n : int, default 1\n        The number of time periods the offset represents.\n    normalize : bool, default False\n        Whether to round the result of a DateOffset addition down to the\n        previous midnight.\n    **kwds\n        Temporal parameter that add to or replace the offset value.\n\n        Parameters that **add** to the offset (like Timedelta):\n\n        - years\n        - months\n        - weeks\n        - days\n        - hours\n        - minutes\n        - seconds\n        - microseconds\n        - nanoseconds\n\n        Parameters that **replace** the offset value:\n\n        - year\n        - month\n        - day\n        - weekday\n        - hour\n        - minute\n        - second\n        - microsecond\n        - nanosecond\n\n    See Also\n    --------\n    dateutil.relativedelta.relativedelta\n\n    Examples\n    --------\n    >>> ts = pd.Timestamp('2017-01-01 09:10:11')\n    >>> ts + DateOffset(months=3)\n    Timestamp('2017-04-01 09:10:11')\n\n    >>> ts = pd.Timestamp('2017-01-01 09:10:11')\n    >>> ts + DateOffset(month=3)\n    Timestamp('2017-03-01 09:10:11')\n    \"\"\"\n    _params = cache_readonly(BaseOffset._params.fget)\n    _use_relativedelta = False\n    _adjust_dst = False\n    _attributes = frozenset(['n', 'normalize'] +\n                            list(liboffsets.relativedelta_kwds))\n\n    # default for prior pickles\n    normalize = False\n\n    def __init__(self, n=1, normalize=False, **kwds):\n        BaseOffset.__init__(self, n, normalize)\n\n        off, use_rd = liboffsets._determine_offset(kwds)\n        object.__setattr__(self, \"_offset\", off)\n        object.__setattr__(self, \"_use_relativedelta\", use_rd)\n        for key in kwds:\n            val = kwds[key]\n            object.__setattr__(self, key, val)\n\n    @apply_wraps\n    def apply(self, other):\n        if self._use_relativedelta:\n            other = as_datetime(other)\n\n        if len(self.kwds) > 0:\n            tzinfo = getattr(other, 'tzinfo', None)\n            if tzinfo is not None and self._use_relativedelta:\n                # perform calculation in UTC\n                other = other.replace(tzinfo=None)\n\n            if self.n > 0:\n                for i in range(self.n):\n                    other = other + self._offset\n            else:\n                for i in range(-self.n):\n                    other = other - self._offset\n\n            if tzinfo is not None and self._use_relativedelta:\n                # bring tz back from UTC calculation\n                other = conversion.localize_pydatetime(other, tzinfo)\n\n            return as_timestamp(other)\n        else:\n            return other + timedelta(self.n)\n\n    @apply_index_wraps\n    def apply_index(self, i):\n        \"\"\"\n        Vectorized apply of DateOffset to DatetimeIndex,\n        raises NotImplentedError for offsets without a\n        vectorized implementation\n\n        Parameters\n        ----------\n        i : DatetimeIndex\n\n        Returns\n        -------\n        y : DatetimeIndex\n        \"\"\"\n\n        if type(self) is not DateOffset:\n            raise NotImplementedError(\"DateOffset subclass {name} \"\n                                      \"does not have a vectorized \"\n                                      \"implementation\".format(\n                                          name=self.__class__.__name__))\n        kwds = self.kwds\n        relativedelta_fast = {'years', 'months', 'weeks', 'days', 'hours',\n                              'minutes', 'seconds', 'microseconds'}\n        # relativedelta/_offset path only valid for base DateOffset\n        if (self._use_relativedelta and\n                set(kwds).issubset(relativedelta_fast)):\n\n            months = ((kwds.get('years', 0) * 12 +\n                       kwds.get('months', 0)) * self.n)\n            if months:\n                shifted = liboffsets.shift_months(i.asi8, months)\n                i = i._shallow_copy(shifted)\n\n            weeks = (kwds.get('weeks', 0)) * self.n\n            if weeks:\n                # integer addition on PeriodIndex is deprecated,\n                #   so we directly use _time_shift instead\n                asper = i.to_period('W')\n                shifted = asper._data._time_shift(weeks)\n                i = shifted.to_timestamp() + i.to_perioddelta('W')\n\n            timedelta_kwds = {k: v for k, v in kwds.items()\n                              if k in ['days', 'hours', 'minutes',\n                                       'seconds', 'microseconds']}\n            if timedelta_kwds:\n                delta = Timedelta(**timedelta_kwds)\n                i = i + (self.n * delta)\n            return i\n        elif not self._use_relativedelta and hasattr(self, '_offset'):\n            # timedelta\n            return i + (self._offset * self.n)\n        else:\n            # relativedelta with other keywords\n            kwd = set(kwds) - relativedelta_fast\n            raise NotImplementedError(\"DateOffset with relativedelta \"\n                                      \"keyword(s) {kwd} not able to be \"\n                                      \"applied vectorized\".format(kwd=kwd))\n\n    def isAnchored(self):\n        # TODO: Does this make sense for the general case?  It would help\n        # if there were a canonical docstring for what isAnchored means.\n        return (self.n == 1)\n\n    # TODO: Combine this with BusinessMixin version by defining a whitelisted\n    # set of attributes on each object rather than the existing behavior of\n    # iterating over internal ``__dict__``\n    def _repr_attrs(self):\n        exclude = {'n', 'inc', 'normalize'}\n        attrs = []\n        for attr in sorted(self.__dict__):\n            if attr.startswith('_') or attr == 'kwds':\n                continue\n            elif attr not in exclude:\n                value = getattr(self, attr)\n                attrs.append('{attr}={value}'.format(attr=attr, value=value))\n\n        out = ''\n        if attrs:\n            out += ': ' + ', '.join(attrs)\n        return out\n\n    @property\n    def name(self):\n        return self.rule_code\n\n    def rollback(self, dt):\n        \"\"\"Roll provided date backward to next offset only if not on offset\"\"\"\n        dt = as_timestamp(dt)\n        if not self.onOffset(dt):\n            dt = dt - self.__class__(1, normalize=self.normalize, **self.kwds)\n        return dt\n\n    def rollforward(self, dt):\n        \"\"\"Roll provided date forward to next offset only if not on offset\"\"\"\n        dt = as_timestamp(dt)\n        if not self.onOffset(dt):\n            dt = dt + self.__class__(1, normalize=self.normalize, **self.kwds)\n        return dt\n\n    def onOffset(self, dt):\n        if self.normalize and not _is_normalized(dt):\n            return False\n        # XXX, see #1395\n        if type(self) == DateOffset or isinstance(self, Tick):\n            return True\n\n        # Default (slow) method for determining if some date is a member of the\n        # date range generated by this offset. Subclasses may have this\n        # re-implemented in a nicer way.\n        a = dt\n        b = ((dt + self) - self)\n        return a == b\n\n    # way to get around weirdness with rule_code\n    @property\n    def _prefix(self):\n        raise NotImplementedError('Prefix not defined')\n\n    @property\n    def rule_code(self):\n        return self._prefix\n\n    @cache_readonly\n    def freqstr(self):\n        try:\n            code = self.rule_code\n        except NotImplementedError:\n            return repr(self)\n\n        if self.n != 1:\n            fstr = '{n}{code}'.format(n=self.n, code=code)\n        else:\n            fstr = code\n\n        try:\n            if self._offset:\n                fstr += self._offset_str()\n        except AttributeError:\n            # TODO: standardize `_offset` vs `offset` naming convention\n            pass\n\n        return fstr\n\n    def _offset_str(self):\n        return ''\n\n    @property\n    def nanos(self):\n        raise ValueError(\"{name} is a non-fixed frequency\".format(name=self))\n\n\nclass SingleConstructorOffset(DateOffset):\n    @classmethod\n    def _from_name(cls, suffix=None):\n        # default _from_name calls cls with no args\n        if suffix:\n            raise ValueError(\"Bad freq suffix {suffix}\".format(suffix=suffix))\n        return cls()\n\n\nclass _CustomMixin(object):\n    \"\"\"\n    Mixin for classes that define and validate calendar, holidays,\n    and weekdays attributes\n    \"\"\"\n    def __init__(self, weekmask, holidays, calendar):\n        calendar, holidays = _get_calendar(weekmask=weekmask,\n                                           holidays=holidays,\n                                           calendar=calendar)\n        # Custom offset instances are identified by the\n        # following two attributes. See DateOffset._params()\n        # holidays, weekmask\n\n        object.__setattr__(self, \"weekmask\", weekmask)\n        object.__setattr__(self, \"holidays\", holidays)\n        object.__setattr__(self, \"calendar\", calendar)\n\n\nclass BusinessMixin(object):\n    \"\"\" Mixin to business types to provide related functions \"\"\"\n\n    @property\n    def offset(self):\n        \"\"\"Alias for self._offset\"\"\"\n        # Alias for backward compat\n        return self._offset\n\n    def _repr_attrs(self):\n        if self.offset:\n            attrs = ['offset={offset!r}'.format(offset=self.offset)]\n        else:\n            attrs = None\n        out = ''\n        if attrs:\n            out += ': ' + ', '.join(attrs)\n        return out\n\n\nclass BusinessDay(BusinessMixin, SingleConstructorOffset):\n    \"\"\"\n    DateOffset subclass representing possibly n business days\n    \"\"\"\n    _prefix = 'B'\n    _adjust_dst = True\n    _attributes = frozenset(['n', 'normalize', 'offset'])\n\n    def __init__(self, n=1, normalize=False, offset=timedelta(0)):\n        BaseOffset.__init__(self, n, normalize)\n        object.__setattr__(self, \"_offset\", offset)\n\n    def _offset_str(self):\n        def get_str(td):\n            off_str = ''\n            if td.days > 0:\n                off_str += str(td.days) + 'D'\n            if td.seconds > 0:\n                s = td.seconds\n                hrs = int(s / 3600)\n                if hrs != 0:\n                    off_str += str(hrs) + 'H'\n                    s -= hrs * 3600\n                mts = int(s / 60)\n                if mts != 0:\n                    off_str += str(mts) + 'Min'\n                    s -= mts * 60\n                if s != 0:\n                    off_str += str(s) + 's'\n            if td.microseconds > 0:\n                off_str += str(td.microseconds) + 'us'\n            return off_str\n\n        if isinstance(self.offset, timedelta):\n            zero = timedelta(0, 0, 0)\n            if self.offset >= zero:\n                off_str = '+' + get_str(self.offset)\n            else:\n                off_str = '-' + get_str(-self.offset)\n            return off_str\n        else:\n            return '+' + repr(self.offset)\n\n    @apply_wraps\n    def apply(self, other):\n        if isinstance(other, datetime):\n            n = self.n\n            wday = other.weekday()\n\n            # avoid slowness below by operating on weeks first\n            weeks = n // 5\n            if n <= 0 and wday > 4:\n                # roll forward\n                n += 1\n\n            n -= 5 * weeks\n\n            # n is always >= 0 at this point\n            if n == 0 and wday > 4:\n                # roll back\n                days = 4 - wday\n            elif wday > 4:\n                # roll forward\n                days = (7 - wday) + (n - 1)\n            elif wday + n <= 4:\n                # shift by n days without leaving the current week\n                days = n\n            else:\n                # shift by n days plus 2 to get past the weekend\n                days = n + 2\n\n            result = other + timedelta(days=7 * weeks + days)\n            if self.offset:\n                result = result + self.offset\n            return result\n\n        elif isinstance(other, (timedelta, Tick)):\n            return BDay(self.n, offset=self.offset + other,\n                        normalize=self.normalize)\n        else:\n            raise ApplyTypeError('Only know how to combine business day with '\n                                 'datetime or timedelta.')\n\n    @apply_index_wraps\n    def apply_index(self, i):\n        time = i.to_perioddelta('D')\n        # to_period rolls forward to next BDay; track and\n        # reduce n where it does when rolling forward\n        asper = i.to_period('B')\n        if self.n > 0:\n            shifted = (i.to_perioddelta('B') - time).asi8 != 0\n\n            # Integer-array addition is deprecated, so we use\n            # _time_shift directly\n            roll = np.where(shifted, self.n - 1, self.n)\n            shifted = asper._data._addsub_int_array(roll, operator.add)\n        else:\n            # Integer addition is deprecated, so we use _time_shift directly\n            roll = self.n\n            shifted = asper._data._time_shift(roll)\n\n        result = shifted.to_timestamp() + time\n        return result\n\n    def onOffset(self, dt):\n        if self.normalize and not _is_normalized(dt):\n            return False\n        return dt.weekday() < 5\n\n\nclass BusinessHourMixin(BusinessMixin):\n\n    def __init__(self, start='09:00', end='17:00', offset=timedelta(0)):\n        # must be validated here to equality check\n        start = liboffsets._validate_business_time(start)\n        object.__setattr__(self, \"start\", start)\n        end = liboffsets._validate_business_time(end)\n        object.__setattr__(self, \"end\", end)\n        object.__setattr__(self, \"_offset\", offset)\n\n    @cache_readonly\n    def next_bday(self):\n        \"\"\"used for moving to next businessday\"\"\"\n        if self.n >= 0:\n            nb_offset = 1\n        else:\n            nb_offset = -1\n        if self._prefix.startswith('C'):\n            # CustomBusinessHour\n            return CustomBusinessDay(n=nb_offset,\n                                     weekmask=self.weekmask,\n                                     holidays=self.holidays,\n                                     calendar=self.calendar)\n        else:\n            return BusinessDay(n=nb_offset)\n\n    @cache_readonly\n    def _get_daytime_flag(self):\n        if self.start == self.end:\n            raise ValueError('start and end must not be the same')\n        elif self.start < self.end:\n            return True\n        else:\n            return False\n\n    def _next_opening_time(self, other):\n        \"\"\"\n        If n is positive, return tomorrow's business day opening time.\n        Otherwise yesterday's business day's opening time.\n\n        Opening time always locates on BusinessDay.\n        Otherwise, closing time may not if business hour extends over midnight.\n        \"\"\"\n        if not self.next_bday.onOffset(other):\n            other = other + self.next_bday\n        else:\n            if self.n >= 0 and self.start < other.time():\n                other = other + self.next_bday\n            elif self.n < 0 and other.time() < self.start:\n                other = other + self.next_bday\n        return datetime(other.year, other.month, other.day,\n                        self.start.hour, self.start.minute)\n\n    def _prev_opening_time(self, other):\n        \"\"\"\n        If n is positive, return yesterday's business day opening time.\n        Otherwise yesterday business day's opening time.\n        \"\"\"\n        if not self.next_bday.onOffset(other):\n            other = other - self.next_bday\n        else:\n            if self.n >= 0 and other.time() < self.start:\n                other = other - self.next_bday\n            elif self.n < 0 and other.time() > self.start:\n                other = other - self.next_bday\n        return datetime(other.year, other.month, other.day,\n                        self.start.hour, self.start.minute)\n\n    @cache_readonly\n    def _get_business_hours_by_sec(self):\n        \"\"\"\n        Return business hours in a day by seconds.\n        \"\"\"\n        if self._get_daytime_flag:\n            # create dummy datetime to calculate businesshours in a day\n            dtstart = datetime(2014, 4, 1, self.start.hour, self.start.minute)\n            until = datetime(2014, 4, 1, self.end.hour, self.end.minute)\n            return (until - dtstart).total_seconds()\n        else:\n            dtstart = datetime(2014, 4, 1, self.start.hour, self.start.minute)\n            until = datetime(2014, 4, 2, self.end.hour, self.end.minute)\n            return (until - dtstart).total_seconds()\n\n    @apply_wraps\n    def rollback(self, dt):\n        \"\"\"Roll provided date backward to next offset only if not on offset\"\"\"\n        if not self.onOffset(dt):\n            businesshours = self._get_business_hours_by_sec\n            if self.n >= 0:\n                dt = self._prev_opening_time(\n                    dt) + timedelta(seconds=businesshours)\n            else:\n                dt = self._next_opening_time(\n                    dt) + timedelta(seconds=businesshours)\n        return dt\n\n    @apply_wraps\n    def rollforward(self, dt):\n        \"\"\"Roll provided date forward to next offset only if not on offset\"\"\"\n        if not self.onOffset(dt):\n            if self.n >= 0:\n                return self._next_opening_time(dt)\n            else:\n                return self._prev_opening_time(dt)\n        return dt\n\n    @apply_wraps\n    def apply(self, other):\n        daytime = self._get_daytime_flag\n        businesshours = self._get_business_hours_by_sec\n        bhdelta = timedelta(seconds=businesshours)\n\n        if isinstance(other, datetime):\n            # used for detecting edge condition\n            nanosecond = getattr(other, 'nanosecond', 0)\n            # reset timezone and nanosecond\n            # other may be a Timestamp, thus not use replace\n            other = datetime(other.year, other.month, other.day,\n                             other.hour, other.minute,\n                             other.second, other.microsecond)\n            n = self.n\n            if n >= 0:\n                if (other.time() == self.end or\n                        not self._onOffset(other, businesshours)):\n                    other = self._next_opening_time(other)\n            else:\n                if other.time() == self.start:\n                    # adjustment to move to previous business day\n                    other = other - timedelta(seconds=1)\n                if not self._onOffset(other, businesshours):\n                    other = self._next_opening_time(other)\n                    other = other + bhdelta\n\n            bd, r = divmod(abs(n * 60), businesshours // 60)\n            if n < 0:\n                bd, r = -bd, -r\n\n            if bd != 0:\n                skip_bd = BusinessDay(n=bd)\n                # midnight business hour may not on BusinessDay\n                if not self.next_bday.onOffset(other):\n                    remain = other - self._prev_opening_time(other)\n                    other = self._next_opening_time(other + skip_bd) + remain\n                else:\n                    other = other + skip_bd\n\n            hours, minutes = divmod(r, 60)\n            result = other + timedelta(hours=hours, minutes=minutes)\n\n            # because of previous adjustment, time will be larger than start\n            if ((daytime and (result.time() < self.start or\n                              self.end < result.time())) or\n                    not daytime and (self.end < result.time() < self.start)):\n                if n >= 0:\n                    bday_edge = self._prev_opening_time(other)\n                    bday_edge = bday_edge + bhdelta\n                    # calculate remainder\n                    bday_remain = result - bday_edge\n                    result = self._next_opening_time(other)\n                    result += bday_remain\n                else:\n                    bday_edge = self._next_opening_time(other)\n                    bday_remain = result - bday_edge\n                    result = self._next_opening_time(result) + bhdelta\n                    result += bday_remain\n            # edge handling\n            if n >= 0:\n                if result.time() == self.end:\n                    result = self._next_opening_time(result)\n            else:\n                if result.time() == self.start and nanosecond == 0:\n                    # adjustment to move to previous business day\n                    result = self._next_opening_time(\n                        result - timedelta(seconds=1)) + bhdelta\n\n            return result\n        else:\n            # TODO: Figure out the end of this sente\n            raise ApplyTypeError(\n                'Only know how to combine business hour with ')\n\n    def onOffset(self, dt):\n        if self.normalize and not _is_normalized(dt):\n            return False\n\n        if dt.tzinfo is not None:\n            dt = datetime(dt.year, dt.month, dt.day, dt.hour,\n                          dt.minute, dt.second, dt.microsecond)\n        # Valid BH can be on the different BusinessDay during midnight\n        # Distinguish by the time spent from previous opening time\n        businesshours = self._get_business_hours_by_sec\n        return self._onOffset(dt, businesshours)\n\n    def _onOffset(self, dt, businesshours):\n        \"\"\"\n        Slight speedups using calculated values\n        \"\"\"\n        # if self.normalize and not _is_normalized(dt):\n        #     return False\n        # Valid BH can be on the different BusinessDay during midnight\n        # Distinguish by the time spent from previous opening time\n        if self.n >= 0:\n            op = self._prev_opening_time(dt)\n        else:\n            op = self._next_opening_time(dt)\n        span = (dt - op).total_seconds()\n        if span <= businesshours:\n            return True\n        else:\n            return False\n\n    def _repr_attrs(self):\n        out = super(BusinessHourMixin, self)._repr_attrs()\n        start = self.start.strftime('%H:%M')\n        end = self.end.strftime('%H:%M')\n        attrs = ['{prefix}={start}-{end}'.format(prefix=self._prefix,\n                                                 start=start, end=end)]\n        out += ': ' + ', '.join(attrs)\n        return out\n\n\nclass BusinessHour(BusinessHourMixin, SingleConstructorOffset):\n    \"\"\"\n    DateOffset subclass representing possibly n business days\n\n    .. versionadded:: 0.16.1\n    \"\"\"\n    _prefix = 'BH'\n    _anchor = 0\n    _attributes = frozenset(['n', 'normalize', 'start', 'end', 'offset'])\n\n    def __init__(self, n=1, normalize=False, start='09:00',\n                 end='17:00', offset=timedelta(0)):\n        BaseOffset.__init__(self, n, normalize)\n        super(BusinessHour, self).__init__(start=start, end=end, offset=offset)\n\n\nclass CustomBusinessDay(_CustomMixin, BusinessDay):\n    \"\"\"\n    DateOffset subclass representing possibly n custom business days,\n    excluding holidays\n\n    Parameters\n    ----------\n    n : int, default 1\n    normalize : bool, default False\n        Normalize start/end dates to midnight before generating date range\n    weekmask : str, Default 'Mon Tue Wed Thu Fri'\n        weekmask of valid business days, passed to ``numpy.busdaycalendar``\n    holidays : list\n        list/array of dates to exclude from the set of valid business days,\n        passed to ``numpy.busdaycalendar``\n    calendar : pd.HolidayCalendar or np.busdaycalendar\n    offset : timedelta, default timedelta(0)\n    \"\"\"\n    _prefix = 'C'\n    _attributes = frozenset(['n', 'normalize',\n                             'weekmask', 'holidays', 'calendar', 'offset'])\n\n    def __init__(self, n=1, normalize=False, weekmask='Mon Tue Wed Thu Fri',\n                 holidays=None, calendar=None, offset=timedelta(0)):\n        BaseOffset.__init__(self, n, normalize)\n        object.__setattr__(self, \"_offset\", offset)\n\n        _CustomMixin.__init__(self, weekmask, holidays, calendar)\n\n    @apply_wraps\n    def apply(self, other):\n        if self.n <= 0:\n            roll = 'forward'\n        else:\n            roll = 'backward'\n\n        if isinstance(other, datetime):\n            date_in = other\n            np_dt = np.datetime64(date_in.date())\n\n            np_incr_dt = np.busday_offset(np_dt, self.n, roll=roll,\n                                          busdaycal=self.calendar)\n\n            dt_date = np_incr_dt.astype(datetime)\n            result = datetime.combine(dt_date, date_in.time())\n\n            if self.offset:\n                result = result + self.offset\n            return result\n\n        elif isinstance(other, (timedelta, Tick)):\n            return BDay(self.n, offset=self.offset + other,\n                        normalize=self.normalize)\n        else:\n            raise ApplyTypeError('Only know how to combine trading day with '\n                                 'datetime, datetime64 or timedelta.')\n\n    def apply_index(self, i):\n        raise NotImplementedError\n\n    def onOffset(self, dt):\n        if self.normalize and not _is_normalized(dt):\n            return False\n        day64 = _to_dt64(dt, 'datetime64[D]')\n        return np.is_busday(day64, busdaycal=self.calendar)\n\n\nclass CustomBusinessHour(_CustomMixin, BusinessHourMixin,\n                         SingleConstructorOffset):\n    \"\"\"\n    DateOffset subclass representing possibly n custom business days\n\n    .. versionadded:: 0.18.1\n    \"\"\"\n    _prefix = 'CBH'\n    _anchor = 0\n    _attributes = frozenset(['n', 'normalize',\n                             'weekmask', 'holidays', 'calendar',\n                             'start', 'end', 'offset'])\n\n    def __init__(self, n=1, normalize=False, weekmask='Mon Tue Wed Thu Fri',\n                 holidays=None, calendar=None,\n                 start='09:00', end='17:00', offset=timedelta(0)):\n        BaseOffset.__init__(self, n, normalize)\n        object.__setattr__(self, \"_offset\", offset)\n\n        _CustomMixin.__init__(self, weekmask, holidays, calendar)\n        BusinessHourMixin.__init__(self, start=start, end=end, offset=offset)\n\n\n# ---------------------------------------------------------------------\n# Month-Based Offset Classes\n\n\nclass MonthOffset(SingleConstructorOffset):\n    _adjust_dst = True\n    _attributes = frozenset(['n', 'normalize'])\n\n    __init__ = BaseOffset.__init__\n\n    @property\n    def name(self):\n        if self.isAnchored:\n            return self.rule_code\n        else:\n            month = ccalendar.MONTH_ALIASES[self.n]\n            return \"{code}-{month}\".format(code=self.rule_code,\n                                           month=month)\n\n    def onOffset(self, dt):\n        if self.normalize and not _is_normalized(dt):\n            return False\n        return dt.day == self._get_offset_day(dt)\n\n    @apply_wraps\n    def apply(self, other):\n        compare_day = self._get_offset_day(other)\n        n = liboffsets.roll_convention(other.day, self.n, compare_day)\n        return shift_month(other, n, self._day_opt)\n\n    @apply_index_wraps\n    def apply_index(self, i):\n        shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return i._shallow_copy(shifted)\n\n\nclass MonthEnd(MonthOffset):\n    \"\"\"DateOffset of one month end\"\"\"\n    _prefix = 'M'\n    _day_opt = 'end'\n\n\nclass MonthBegin(MonthOffset):\n    \"\"\"DateOffset of one month at beginning\"\"\"\n    _prefix = 'MS'\n    _day_opt = 'start'\n\n\nclass BusinessMonthEnd(MonthOffset):\n    \"\"\"DateOffset increments between business EOM dates\"\"\"\n    _prefix = 'BM'\n    _day_opt = 'business_end'\n\n\nclass BusinessMonthBegin(MonthOffset):\n    \"\"\"DateOffset of one business month at beginning\"\"\"\n    _prefix = 'BMS'\n    _day_opt = 'business_start'\n\n\nclass _CustomBusinessMonth(_CustomMixin, BusinessMixin, MonthOffset):\n    \"\"\"\n    DateOffset subclass representing one custom business month, incrementing\n    between [BEGIN/END] of month dates\n\n    Parameters\n    ----------\n    n : int, default 1\n    normalize : bool, default False\n        Normalize start/end dates to midnight before generating date range\n    weekmask : str, Default 'Mon Tue Wed Thu Fri'\n        weekmask of valid business days, passed to ``numpy.busdaycalendar``\n    holidays : list\n        list/array of dates to exclude from the set of valid business days,\n        passed to ``numpy.busdaycalendar``\n    calendar : pd.HolidayCalendar or np.busdaycalendar\n    offset : timedelta, default timedelta(0)\n    \"\"\"\n    _attributes = frozenset(['n', 'normalize',\n                             'weekmask', 'holidays', 'calendar', 'offset'])\n\n    onOffset = DateOffset.onOffset        # override MonthOffset method\n    apply_index = DateOffset.apply_index  # override MonthOffset method\n\n    def __init__(self, n=1, normalize=False, weekmask='Mon Tue Wed Thu Fri',\n                 holidays=None, calendar=None, offset=timedelta(0)):\n        BaseOffset.__init__(self, n, normalize)\n        object.__setattr__(self, \"_offset\", offset)\n\n        _CustomMixin.__init__(self, weekmask, holidays, calendar)\n\n    @cache_readonly\n    def cbday_roll(self):\n        \"\"\"Define default roll function to be called in apply method\"\"\"\n        cbday = CustomBusinessDay(n=self.n, normalize=False, **self.kwds)\n\n        if self._prefix.endswith('S'):\n            # MonthBegin\n            roll_func = cbday.rollforward\n        else:\n            # MonthEnd\n            roll_func = cbday.rollback\n        return roll_func\n\n    @cache_readonly\n    def m_offset(self):\n        if self._prefix.endswith('S'):\n            # MonthBegin\n            moff = MonthBegin(n=1, normalize=False)\n        else:\n            # MonthEnd\n            moff = MonthEnd(n=1, normalize=False)\n        return moff\n\n    @cache_readonly\n    def month_roll(self):\n        \"\"\"Define default roll function to be called in apply method\"\"\"\n        if self._prefix.endswith('S'):\n            # MonthBegin\n            roll_func = self.m_offset.rollback\n        else:\n            # MonthEnd\n            roll_func = self.m_offset.rollforward\n        return roll_func\n\n    @apply_wraps\n    def apply(self, other):\n        # First move to month offset\n        cur_month_offset_date = self.month_roll(other)\n\n        # Find this custom month offset\n        compare_date = self.cbday_roll(cur_month_offset_date)\n        n = liboffsets.roll_convention(other.day, self.n, compare_date.day)\n\n        new = cur_month_offset_date + n * self.m_offset\n        result = self.cbday_roll(new)\n        return result\n\n\nclass CustomBusinessMonthEnd(_CustomBusinessMonth):\n    # TODO(py27): Replace condition with Subsitution after dropping Py27\n    if _CustomBusinessMonth.__doc__:\n        __doc__ = _CustomBusinessMonth.__doc__.replace('[BEGIN/END]', 'end')\n    _prefix = 'CBM'\n\n\nclass CustomBusinessMonthBegin(_CustomBusinessMonth):\n    # TODO(py27): Replace condition with Subsitution after dropping Py27\n    if _CustomBusinessMonth.__doc__:\n        __doc__ = _CustomBusinessMonth.__doc__.replace('[BEGIN/END]',\n                                                       'beginning')\n    _prefix = 'CBMS'\n\n\n# ---------------------------------------------------------------------\n# Semi-Month Based Offset Classes\n\nclass SemiMonthOffset(DateOffset):\n    _adjust_dst = True\n    _default_day_of_month = 15\n    _min_day_of_month = 2\n    _attributes = frozenset(['n', 'normalize', 'day_of_month'])\n\n    def __init__(self, n=1, normalize=False, day_of_month=None):\n        BaseOffset.__init__(self, n, normalize)\n\n        if day_of_month is None:\n            object.__setattr__(self, \"day_of_month\",\n                               self._default_day_of_month)\n        else:\n            object.__setattr__(self, \"day_of_month\", int(day_of_month))\n        if not self._min_day_of_month <= self.day_of_month <= 27:\n            msg = 'day_of_month must be {min}<=day_of_month<=27, got {day}'\n            raise ValueError(msg.format(min=self._min_day_of_month,\n                                        day=self.day_of_month))\n\n    @classmethod\n    def _from_name(cls, suffix=None):\n        return cls(day_of_month=suffix)\n\n    @property\n    def rule_code(self):\n        suffix = '-{day_of_month}'.format(day_of_month=self.day_of_month)\n        return self._prefix + suffix\n\n    @apply_wraps\n    def apply(self, other):\n        # shift `other` to self.day_of_month, incrementing `n` if necessary\n        n = liboffsets.roll_convention(other.day, self.n, self.day_of_month)\n\n        days_in_month = ccalendar.get_days_in_month(other.year, other.month)\n\n        # For SemiMonthBegin on other.day == 1 and\n        # SemiMonthEnd on other.day == days_in_month,\n        # shifting `other` to `self.day_of_month` _always_ requires\n        # incrementing/decrementing `n`, regardless of whether it is\n        # initially positive.\n        if type(self) is SemiMonthBegin and (self.n <= 0 and other.day == 1):\n            n -= 1\n        elif type(self) is SemiMonthEnd and (self.n > 0 and\n                                             other.day == days_in_month):\n            n += 1\n\n        return self._apply(n, other)\n\n    def _apply(self, n, other):\n        \"\"\"Handle specific apply logic for child classes\"\"\"\n        raise AbstractMethodError(self)\n\n    @apply_index_wraps\n    def apply_index(self, i):\n        # determine how many days away from the 1st of the month we are\n        dti = i\n        days_from_start = i.to_perioddelta('M').asi8\n        delta = Timedelta(days=self.day_of_month - 1).value\n\n        # get boolean array for each element before the day_of_month\n        before_day_of_month = days_from_start < delta\n\n        # get boolean array for each element after the day_of_month\n        after_day_of_month = days_from_start > delta\n\n        # determine the correct n for each date in i\n        roll = self._get_roll(i, before_day_of_month, after_day_of_month)\n\n        # isolate the time since it will be striped away one the next line\n        time = i.to_perioddelta('D')\n\n        # apply the correct number of months\n\n        # integer-array addition on PeriodIndex is deprecated,\n        #  so we use _addsub_int_array directly\n        asper = i.to_period('M')\n        shifted = asper._data._addsub_int_array(roll // 2, operator.add)\n        i = type(dti)(shifted.to_timestamp())\n\n        # apply the correct day\n        i = self._apply_index_days(i, roll)\n\n        return i + time\n\n    def _get_roll(self, i, before_day_of_month, after_day_of_month):\n        \"\"\"Return an array with the correct n for each date in i.\n\n        The roll array is based on the fact that i gets rolled back to\n        the first day of the month.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _apply_index_days(self, i, roll):\n        \"\"\"Apply the correct day for each date in i\"\"\"\n        raise AbstractMethodError(self)\n\n\nclass SemiMonthEnd(SemiMonthOffset):\n    \"\"\"\n    Two DateOffset's per month repeating on the last\n    day of the month and day_of_month.\n\n    .. versionadded:: 0.19.0\n\n    Parameters\n    ----------\n    n : int\n    normalize : bool, default False\n    day_of_month : int, {1, 3,...,27}, default 15\n    \"\"\"\n    _prefix = 'SM'\n    _min_day_of_month = 1\n\n    def onOffset(self, dt):\n        if self.normalize and not _is_normalized(dt):\n            return False\n        days_in_month = ccalendar.get_days_in_month(dt.year, dt.month)\n        return dt.day in (self.day_of_month, days_in_month)\n\n    def _apply(self, n, other):\n        months = n // 2\n        day = 31 if n % 2 else self.day_of_month\n        return shift_month(other, months, day)\n\n    def _get_roll(self, i, before_day_of_month, after_day_of_month):\n        n = self.n\n        is_month_end = i.is_month_end\n        if n > 0:\n            roll_end = np.where(is_month_end, 1, 0)\n            roll_before = np.where(before_day_of_month, n, n + 1)\n            roll = roll_end + roll_before\n        elif n == 0:\n            roll_after = np.where(after_day_of_month, 2, 0)\n            roll_before = np.where(~after_day_of_month, 1, 0)\n            roll = roll_before + roll_after\n        else:\n            roll = np.where(after_day_of_month, n + 2, n + 1)\n        return roll\n\n    def _apply_index_days(self, i, roll):\n        \"\"\"Add days portion of offset to DatetimeIndex i\n\n        Parameters\n        ----------\n        i : DatetimeIndex\n        roll : ndarray[int64_t]\n\n        Returns\n        -------\n        result : DatetimeIndex\n        \"\"\"\n        nanos = (roll % 2) * Timedelta(days=self.day_of_month).value\n        i += nanos.astype('timedelta64[ns]')\n        return i + Timedelta(days=-1)\n\n\nclass SemiMonthBegin(SemiMonthOffset):\n    \"\"\"\n    Two DateOffset's per month repeating on the first\n    day of the month and day_of_month.\n\n    .. versionadded:: 0.19.0\n\n    Parameters\n    ----------\n    n : int\n    normalize : bool, default False\n    day_of_month : int, {2, 3,...,27}, default 15\n    \"\"\"\n    _prefix = 'SMS'\n\n    def onOffset(self, dt):\n        if self.normalize and not _is_normalized(dt):\n            return False\n        return dt.day in (1, self.day_of_month)\n\n    def _apply(self, n, other):\n        months = n // 2 + n % 2\n        day = 1 if n % 2 else self.day_of_month\n        return shift_month(other, months, day)\n\n    def _get_roll(self, i, before_day_of_month, after_day_of_month):\n        n = self.n\n        is_month_start = i.is_month_start\n        if n > 0:\n            roll = np.where(before_day_of_month, n, n + 1)\n        elif n == 0:\n            roll_start = np.where(is_month_start, 0, 1)\n            roll_after = np.where(after_day_of_month, 1, 0)\n            roll = roll_start + roll_after\n        else:\n            roll_after = np.where(after_day_of_month, n + 2, n + 1)\n            roll_start = np.where(is_month_start, -1, 0)\n            roll = roll_after + roll_start\n        return roll\n\n    def _apply_index_days(self, i, roll):\n        \"\"\"Add days portion of offset to DatetimeIndex i\n\n        Parameters\n        ----------\n        i : DatetimeIndex\n        roll : ndarray[int64_t]\n\n        Returns\n        -------\n        result : DatetimeIndex\n        \"\"\"\n        nanos = (roll % 2) * Timedelta(days=self.day_of_month - 1).value\n        return i + nanos.astype('timedelta64[ns]')\n\n\n# ---------------------------------------------------------------------\n# Week-Based Offset Classes\n\nclass Week(DateOffset):\n    \"\"\"\n    Weekly offset\n\n    Parameters\n    ----------\n    weekday : int, default None\n        Always generate specific day of week. 0 for Monday\n    \"\"\"\n    _adjust_dst = True\n    _inc = timedelta(weeks=1)\n    _prefix = 'W'\n    _attributes = frozenset(['n', 'normalize', 'weekday'])\n\n    def __init__(self, n=1, normalize=False, weekday=None):\n        BaseOffset.__init__(self, n, normalize)\n        object.__setattr__(self, \"weekday\", weekday)\n\n        if self.weekday is not None:\n            if self.weekday < 0 or self.weekday > 6:\n                raise ValueError('Day must be 0<=day<=6, got {day}'\n                                 .format(day=self.weekday))\n\n    def isAnchored(self):\n        return (self.n == 1 and self.weekday is not None)\n\n    @apply_wraps\n    def apply(self, other):\n        if self.weekday is None:\n            return other + self.n * self._inc\n\n        k = self.n\n        otherDay = other.weekday()\n        if otherDay != self.weekday:\n            other = other + timedelta((self.weekday - otherDay) % 7)\n            if k > 0:\n                k -= 1\n\n        return other + timedelta(weeks=k)\n\n    @apply_index_wraps\n    def apply_index(self, i):\n        if self.weekday is None:\n            # integer addition on PeriodIndex is deprecated,\n            #  so we use _time_shift directly\n            shifted = i.to_period('W')._data._time_shift(self.n)\n            return shifted.to_timestamp() + i.to_perioddelta('W')\n        else:\n            return self._end_apply_index(i)\n\n    def _end_apply_index(self, dtindex):\n        \"\"\"Add self to the given DatetimeIndex, specialized for case where\n        self.weekday is non-null.\n\n        Parameters\n        ----------\n        dtindex : DatetimeIndex\n\n        Returns\n        -------\n        result : DatetimeIndex\n        \"\"\"\n        off = dtindex.to_perioddelta('D')\n\n        base, mult = libfrequencies.get_freq_code(self.freqstr)\n        base_period = dtindex.to_period(base)\n        if self.n > 0:\n            # when adding, dates on end roll to next\n            normed = dtindex - off + Timedelta(1, 'D') - Timedelta(1, 'ns')\n            roll = np.where(base_period.to_timestamp(how='end') == normed,\n                            self.n, self.n - 1)\n            # integer-array addition on PeriodIndex is deprecated,\n            #  so we use _addsub_int_array directly\n            shifted = base_period._data._addsub_int_array(roll, operator.add)\n            base = shifted.to_timestamp(how='end')\n        else:\n            # integer addition on PeriodIndex is deprecated,\n            #  so we use _time_shift directly\n            roll = self.n\n            base = base_period._data._time_shift(roll).to_timestamp(how='end')\n\n        return base + off + Timedelta(1, 'ns') - Timedelta(1, 'D')\n\n    def onOffset(self, dt):\n        if self.normalize and not _is_normalized(dt):\n            return False\n        elif self.weekday is None:\n            return True\n        return dt.weekday() == self.weekday\n\n    @property\n    def rule_code(self):\n        suffix = ''\n        if self.weekday is not None:\n            weekday = ccalendar.int_to_weekday[self.weekday]\n            suffix = '-{weekday}'.format(weekday=weekday)\n        return self._prefix + suffix\n\n    @classmethod\n    def _from_name(cls, suffix=None):\n        if not suffix:\n            weekday = None\n        else:\n            weekday = ccalendar.weekday_to_int[suffix]\n        return cls(weekday=weekday)\n\n\nclass _WeekOfMonthMixin(object):\n    \"\"\"Mixin for methods common to WeekOfMonth and LastWeekOfMonth\"\"\"\n    @apply_wraps\n    def apply(self, other):\n        compare_day = self._get_offset_day(other)\n\n        months = self.n\n        if months > 0 and compare_day > other.day:\n            months -= 1\n        elif months <= 0 and compare_day < other.day:\n            months += 1\n\n        shifted = shift_month(other, months, 'start')\n        to_day = self._get_offset_day(shifted)\n        return liboffsets.shift_day(shifted, to_day - shifted.day)\n\n    def onOffset(self, dt):\n        if self.normalize and not _is_normalized(dt):\n            return False\n        return dt.day == self._get_offset_day(dt)\n\n\nclass WeekOfMonth(_WeekOfMonthMixin, DateOffset):\n    \"\"\"\n    Describes monthly dates like \"the Tuesday of the 2nd week of each month\"\n\n    Parameters\n    ----------\n    n : int\n    week : {0, 1, 2, 3, ...}, default 0\n        0 is 1st week of month, 1 2nd week, etc.\n    weekday : {0, 1, ..., 6}, default 0\n        0: Mondays\n        1: Tuesdays\n        2: Wednesdays\n        3: Thursdays\n        4: Fridays\n        5: Saturdays\n        6: Sundays\n    \"\"\"\n    _prefix = 'WOM'\n    _adjust_dst = True\n    _attributes = frozenset(['n', 'normalize', 'week', 'weekday'])\n\n    def __init__(self, n=1, normalize=False, week=0, weekday=0):\n        BaseOffset.__init__(self, n, normalize)\n        object.__setattr__(self, \"weekday\", weekday)\n        object.__setattr__(self, \"week\", week)\n\n        if self.weekday < 0 or self.weekday > 6:\n            raise ValueError('Day must be 0<=day<=6, got {day}'\n                             .format(day=self.weekday))\n        if self.week < 0 or self.week > 3:\n            raise ValueError('Week must be 0<=week<=3, got {week}'\n                             .format(week=self.week))\n\n    def _get_offset_day(self, other):\n        \"\"\"\n        Find the day in the same month as other that has the same\n        weekday as self.weekday and is the self.week'th such day in the month.\n\n        Parameters\n        ----------\n        other : datetime\n\n        Returns\n        -------\n        day : int\n        \"\"\"\n        mstart = datetime(other.year, other.month, 1)\n        wday = mstart.weekday()\n        shift_days = (self.weekday - wday) % 7\n        return 1 + shift_days + self.week * 7\n\n    @property\n    def rule_code(self):\n        weekday = ccalendar.int_to_weekday.get(self.weekday, '')\n        return '{prefix}-{week}{weekday}'.format(prefix=self._prefix,\n                                                 week=self.week + 1,\n                                                 weekday=weekday)\n\n    @classmethod\n    def _from_name(cls, suffix=None):\n        if not suffix:\n            raise ValueError(\"Prefix {prefix!r} requires a suffix.\"\n                             .format(prefix=cls._prefix))\n        # TODO: handle n here...\n        # only one digit weeks (1 --> week 0, 2 --> week 1, etc.)\n        week = int(suffix[0]) - 1\n        weekday = ccalendar.weekday_to_int[suffix[1:]]\n        return cls(week=week, weekday=weekday)\n\n\nclass LastWeekOfMonth(_WeekOfMonthMixin, DateOffset):\n    \"\"\"\n    Describes monthly dates in last week of month like \"the last Tuesday of\n    each month\"\n\n    Parameters\n    ----------\n    n : int, default 1\n    weekday : {0, 1, ..., 6}, default 0\n        0: Mondays\n        1: Tuesdays\n        2: Wednesdays\n        3: Thursdays\n        4: Fridays\n        5: Saturdays\n        6: Sundays\n    \"\"\"\n    _prefix = 'LWOM'\n    _adjust_dst = True\n    _attributes = frozenset(['n', 'normalize', 'weekday'])\n\n    def __init__(self, n=1, normalize=False, weekday=0):\n        BaseOffset.__init__(self, n, normalize)\n        object.__setattr__(self, \"weekday\", weekday)\n\n        if self.n == 0:\n            raise ValueError('N cannot be 0')\n\n        if self.weekday < 0 or self.weekday > 6:\n            raise ValueError('Day must be 0<=day<=6, got {day}'\n                             .format(day=self.weekday))\n\n    def _get_offset_day(self, other):\n        \"\"\"\n        Find the day in the same month as other that has the same\n        weekday as self.weekday and is the last such day in the month.\n\n        Parameters\n        ----------\n        other: datetime\n\n        Returns\n        -------\n        day: int\n        \"\"\"\n        dim = ccalendar.get_days_in_month(other.year, other.month)\n        mend = datetime(other.year, other.month, dim)\n        wday = mend.weekday()\n        shift_days = (wday - self.weekday) % 7\n        return dim - shift_days\n\n    @property\n    def rule_code(self):\n        weekday = ccalendar.int_to_weekday.get(self.weekday, '')\n        return '{prefix}-{weekday}'.format(prefix=self._prefix,\n                                           weekday=weekday)\n\n    @classmethod\n    def _from_name(cls, suffix=None):\n        if not suffix:\n            raise ValueError(\"Prefix {prefix!r} requires a suffix.\"\n                             .format(prefix=cls._prefix))\n        # TODO: handle n here...\n        weekday = ccalendar.weekday_to_int[suffix]\n        return cls(weekday=weekday)\n\n# ---------------------------------------------------------------------\n# Quarter-Based Offset Classes\n\n\nclass QuarterOffset(DateOffset):\n    \"\"\"Quarter representation - doesn't call super\"\"\"\n    _default_startingMonth = None\n    _from_name_startingMonth = None\n    _adjust_dst = True\n    _attributes = frozenset(['n', 'normalize', 'startingMonth'])\n    # TODO: Consider combining QuarterOffset and YearOffset __init__ at some\n    #       point.  Also apply_index, onOffset, rule_code if\n    #       startingMonth vs month attr names are resolved\n\n    def __init__(self, n=1, normalize=False, startingMonth=None):\n        BaseOffset.__init__(self, n, normalize)\n\n        if startingMonth is None:\n            startingMonth = self._default_startingMonth\n        object.__setattr__(self, \"startingMonth\", startingMonth)\n\n    def isAnchored(self):\n        return (self.n == 1 and self.startingMonth is not None)\n\n    @classmethod\n    def _from_name(cls, suffix=None):\n        kwargs = {}\n        if suffix:\n            kwargs['startingMonth'] = ccalendar.MONTH_TO_CAL_NUM[suffix]\n        else:\n            if cls._from_name_startingMonth is not None:\n                kwargs['startingMonth'] = cls._from_name_startingMonth\n        return cls(**kwargs)\n\n    @property\n    def rule_code(self):\n        month = ccalendar.MONTH_ALIASES[self.startingMonth]\n        return '{prefix}-{month}'.format(prefix=self._prefix, month=month)\n\n    @apply_wraps\n    def apply(self, other):\n        # months_since: find the calendar quarter containing other.month,\n        # e.g. if other.month == 8, the calendar quarter is [Jul, Aug, Sep].\n        # Then find the month in that quarter containing an onOffset date for\n        # self.  `months_since` is the number of months to shift other.month\n        # to get to this on-offset month.\n        months_since = other.month % 3 - self.startingMonth % 3\n        qtrs = liboffsets.roll_qtrday(other, self.n, self.startingMonth,\n                                      day_opt=self._day_opt, modby=3)\n        months = qtrs * 3 - months_since\n        return shift_month(other, months, self._day_opt)\n\n    def onOffset(self, dt):\n        if self.normalize and not _is_normalized(dt):\n            return False\n        mod_month = (dt.month - self.startingMonth) % 3\n        return mod_month == 0 and dt.day == self._get_offset_day(dt)\n\n    @apply_index_wraps\n    def apply_index(self, dtindex):\n        shifted = liboffsets.shift_quarters(dtindex.asi8, self.n,\n                                            self.startingMonth, self._day_opt)\n        return dtindex._shallow_copy(shifted)\n\n\nclass BQuarterEnd(QuarterOffset):\n    \"\"\"DateOffset increments between business Quarter dates\n    startingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, ...\n    startingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, ...\n    startingMonth = 3 corresponds to dates like 3/30/2007, 6/29/2007, ...\n    \"\"\"\n    _outputName = 'BusinessQuarterEnd'\n    _default_startingMonth = 3\n    _from_name_startingMonth = 12\n    _prefix = 'BQ'\n    _day_opt = 'business_end'\n\n\n# TODO: This is basically the same as BQuarterEnd\nclass BQuarterBegin(QuarterOffset):\n    _outputName = \"BusinessQuarterBegin\"\n    # I suspect this is wrong for *all* of them.\n    _default_startingMonth = 3\n    _from_name_startingMonth = 1\n    _prefix = 'BQS'\n    _day_opt = 'business_start'\n\n\nclass QuarterEnd(QuarterOffset):\n    \"\"\"DateOffset increments between business Quarter dates\n    startingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, ...\n    startingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, ...\n    startingMonth = 3 corresponds to dates like 3/31/2007, 6/30/2007, ...\n    \"\"\"\n    _outputName = 'QuarterEnd'\n    _default_startingMonth = 3\n    _prefix = 'Q'\n    _day_opt = 'end'\n\n\nclass QuarterBegin(QuarterOffset):\n    _outputName = 'QuarterBegin'\n    _default_startingMonth = 3\n    _from_name_startingMonth = 1\n    _prefix = 'QS'\n    _day_opt = 'start'\n\n\n# ---------------------------------------------------------------------\n# Year-Based Offset Classes\n\nclass YearOffset(DateOffset):\n    \"\"\"DateOffset that just needs a month\"\"\"\n    _adjust_dst = True\n    _attributes = frozenset(['n', 'normalize', 'month'])\n\n    def _get_offset_day(self, other):\n        # override BaseOffset method to use self.month instead of other.month\n        # TODO: there may be a more performant way to do this\n        return liboffsets.get_day_of_month(other.replace(month=self.month),\n                                           self._day_opt)\n\n    @apply_wraps\n    def apply(self, other):\n        years = roll_yearday(other, self.n, self.month, self._day_opt)\n        months = years * 12 + (self.month - other.month)\n        return shift_month(other, months, self._day_opt)\n\n    @apply_index_wraps\n    def apply_index(self, dtindex):\n        shifted = liboffsets.shift_quarters(dtindex.asi8, self.n,\n                                            self.month, self._day_opt,\n                                            modby=12)\n        return dtindex._shallow_copy(shifted)\n\n    def onOffset(self, dt):\n        if self.normalize and not _is_normalized(dt):\n            return False\n        return dt.month == self.month and dt.day == self._get_offset_day(dt)\n\n    def __init__(self, n=1, normalize=False, month=None):\n        BaseOffset.__init__(self, n, normalize)\n\n        month = month if month is not None else self._default_month\n        object.__setattr__(self, \"month\", month)\n\n        if self.month < 1 or self.month > 12:\n            raise ValueError('Month must go from 1 to 12')\n\n    @classmethod\n    def _from_name(cls, suffix=None):\n        kwargs = {}\n        if suffix:\n            kwargs['month'] = ccalendar.MONTH_TO_CAL_NUM[suffix]\n        return cls(**kwargs)\n\n    @property\n    def rule_code(self):\n        month = ccalendar.MONTH_ALIASES[self.month]\n        return '{prefix}-{month}'.format(prefix=self._prefix, month=month)\n\n\nclass BYearEnd(YearOffset):\n    \"\"\"DateOffset increments between business EOM dates\"\"\"\n    _outputName = 'BusinessYearEnd'\n    _default_month = 12\n    _prefix = 'BA'\n    _day_opt = 'business_end'\n\n\nclass BYearBegin(YearOffset):\n    \"\"\"DateOffset increments between business year begin dates\"\"\"\n    _outputName = 'BusinessYearBegin'\n    _default_month = 1\n    _prefix = 'BAS'\n    _day_opt = 'business_start'\n\n\nclass YearEnd(YearOffset):\n    \"\"\"DateOffset increments between calendar year ends\"\"\"\n    _default_month = 12\n    _prefix = 'A'\n    _day_opt = 'end'\n\n\nclass YearBegin(YearOffset):\n    \"\"\"DateOffset increments between calendar year begin dates\"\"\"\n    _default_month = 1\n    _prefix = 'AS'\n    _day_opt = 'start'\n\n\n# ---------------------------------------------------------------------\n# Special Offset Classes\n\nclass FY5253(DateOffset):\n    \"\"\"\n    Describes 52-53 week fiscal year. This is also known as a 4-4-5 calendar.\n\n    It is used by companies that desire that their\n    fiscal year always end on the same day of the week.\n\n    It is a method of managing accounting periods.\n    It is a common calendar structure for some industries,\n    such as retail, manufacturing and parking industry.\n\n    For more information see:\n    http://en.wikipedia.org/wiki/4-4-5_calendar\n\n    The year may either:\n    - end on the last X day of the Y month.\n    - end on the last X day closest to the last day of the Y month.\n\n    X is a specific day of the week.\n    Y is a certain month of the year\n\n    Parameters\n    ----------\n    n : int\n    weekday : {0, 1, ..., 6}\n        0: Mondays\n        1: Tuesdays\n        2: Wednesdays\n        3: Thursdays\n        4: Fridays\n        5: Saturdays\n        6: Sundays\n    startingMonth : The month in which fiscal years end. {1, 2, ... 12}\n    variation : str\n        {\"nearest\", \"last\"} for \"LastOfMonth\" or \"NearestEndMonth\"\n    \"\"\"\n    _prefix = 'RE'\n    _adjust_dst = True\n    _attributes = frozenset(['weekday', 'startingMonth', 'variation'])\n\n    def __init__(self, n=1, normalize=False, weekday=0, startingMonth=1,\n                 variation=\"nearest\"):\n        BaseOffset.__init__(self, n, normalize)\n        object.__setattr__(self, \"startingMonth\", startingMonth)\n        object.__setattr__(self, \"weekday\", weekday)\n\n        object.__setattr__(self, \"variation\", variation)\n\n        if self.n == 0:\n            raise ValueError('N cannot be 0')\n\n        if self.variation not in [\"nearest\", \"last\"]:\n            raise ValueError('{variation} is not a valid variation'\n                             .format(variation=self.variation))\n\n    def isAnchored(self):\n        return (self.n == 1 and\n                self.startingMonth is not None and\n                self.weekday is not None)\n\n    def onOffset(self, dt):\n        if self.normalize and not _is_normalized(dt):\n            return False\n        dt = datetime(dt.year, dt.month, dt.day)\n        year_end = self.get_year_end(dt)\n\n        if self.variation == \"nearest\":\n            # We have to check the year end of \"this\" cal year AND the previous\n            return (year_end == dt or\n                    self.get_year_end(shift_month(dt, -1, None)) == dt)\n        else:\n            return year_end == dt\n\n    @apply_wraps\n    def apply(self, other):\n        norm = Timestamp(other).normalize()\n\n        n = self.n\n        prev_year = self.get_year_end(\n            datetime(other.year - 1, self.startingMonth, 1))\n        cur_year = self.get_year_end(\n            datetime(other.year, self.startingMonth, 1))\n        next_year = self.get_year_end(\n            datetime(other.year + 1, self.startingMonth, 1))\n\n        prev_year = conversion.localize_pydatetime(prev_year, other.tzinfo)\n        cur_year = conversion.localize_pydatetime(cur_year, other.tzinfo)\n        next_year = conversion.localize_pydatetime(next_year, other.tzinfo)\n\n        # Note: next_year.year == other.year + 1, so we will always\n        # have other < next_year\n        if norm == prev_year:\n            n -= 1\n        elif norm == cur_year:\n            pass\n        elif n > 0:\n            if norm < prev_year:\n                n -= 2\n            elif prev_year < norm < cur_year:\n                n -= 1\n            elif cur_year < norm < next_year:\n                pass\n        else:\n            if cur_year < norm < next_year:\n                n += 1\n            elif prev_year < norm < cur_year:\n                pass\n            elif (norm.year == prev_year.year and norm < prev_year and\n                  prev_year - norm <= timedelta(6)):\n                # GH#14774, error when next_year.year == cur_year.year\n                # e.g. prev_year == datetime(2004, 1, 3),\n                # other == datetime(2004, 1, 1)\n                n -= 1\n            else:\n                assert False\n\n        shifted = datetime(other.year + n, self.startingMonth, 1)\n        result = self.get_year_end(shifted)\n        result = datetime(result.year, result.month, result.day,\n                          other.hour, other.minute, other.second,\n                          other.microsecond)\n        return result\n\n    def get_year_end(self, dt):\n        assert dt.tzinfo is None\n\n        dim = ccalendar.get_days_in_month(dt.year, self.startingMonth)\n        target_date = datetime(dt.year, self.startingMonth, dim)\n        wkday_diff = self.weekday - target_date.weekday()\n        if wkday_diff == 0:\n            # year_end is the same for \"last\" and \"nearest\" cases\n            return target_date\n\n        if self.variation == \"last\":\n            days_forward = (wkday_diff % 7) - 7\n\n            # days_forward is always negative, so we always end up\n            # in the same year as dt\n            return target_date + timedelta(days=days_forward)\n        else:\n            # variation == \"nearest\":\n            days_forward = wkday_diff % 7\n            if days_forward <= 3:\n                # The upcoming self.weekday is closer than the previous one\n                return target_date + timedelta(days_forward)\n            else:\n                # The previous self.weekday is closer than the upcoming one\n                return target_date + timedelta(days_forward - 7)\n\n    @property\n    def rule_code(self):\n        prefix = self._prefix\n        suffix = self.get_rule_code_suffix()\n        return \"{prefix}-{suffix}\".format(prefix=prefix, suffix=suffix)\n\n    def _get_suffix_prefix(self):\n        if self.variation == \"nearest\":\n            return 'N'\n        else:\n            return 'L'\n\n    def get_rule_code_suffix(self):\n        prefix = self._get_suffix_prefix()\n        month = ccalendar.MONTH_ALIASES[self.startingMonth]\n        weekday = ccalendar.int_to_weekday[self.weekday]\n        return '{prefix}-{month}-{weekday}'.format(prefix=prefix, month=month,\n                                                   weekday=weekday)\n\n    @classmethod\n    def _parse_suffix(cls, varion_code, startingMonth_code, weekday_code):\n        if varion_code == \"N\":\n            variation = \"nearest\"\n        elif varion_code == \"L\":\n            variation = \"last\"\n        else:\n            raise ValueError(\"Unable to parse varion_code: \"\n                             \"{code}\".format(code=varion_code))\n\n        startingMonth = ccalendar.MONTH_TO_CAL_NUM[startingMonth_code]\n        weekday = ccalendar.weekday_to_int[weekday_code]\n\n        return {\"weekday\": weekday,\n                \"startingMonth\": startingMonth,\n                \"variation\": variation}\n\n    @classmethod\n    def _from_name(cls, *args):\n        return cls(**cls._parse_suffix(*args))\n\n\nclass FY5253Quarter(DateOffset):\n    \"\"\"\n    DateOffset increments between business quarter dates\n    for 52-53 week fiscal year (also known as a 4-4-5 calendar).\n\n    It is used by companies that desire that their\n    fiscal year always end on the same day of the week.\n\n    It is a method of managing accounting periods.\n    It is a common calendar structure for some industries,\n    such as retail, manufacturing and parking industry.\n\n    For more information see:\n    http://en.wikipedia.org/wiki/4-4-5_calendar\n\n    The year may either:\n    - end on the last X day of the Y month.\n    - end on the last X day closest to the last day of the Y month.\n\n    X is a specific day of the week.\n    Y is a certain month of the year\n\n    startingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, ...\n    startingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, ...\n    startingMonth = 3 corresponds to dates like 3/30/2007, 6/29/2007, ...\n\n    Parameters\n    ----------\n    n : int\n    weekday : {0, 1, ..., 6}\n        0: Mondays\n        1: Tuesdays\n        2: Wednesdays\n        3: Thursdays\n        4: Fridays\n        5: Saturdays\n        6: Sundays\n    startingMonth : The month in which fiscal years end. {1, 2, ... 12}\n    qtr_with_extra_week : The quarter number that has the leap\n        or 14 week when needed. {1,2,3,4}\n    variation : str\n        {\"nearest\", \"last\"} for \"LastOfMonth\" or \"NearestEndMonth\"\n    \"\"\"\n\n    _prefix = 'REQ'\n    _adjust_dst = True\n    _attributes = frozenset(['weekday', 'startingMonth', 'qtr_with_extra_week',\n                             'variation'])\n\n    def __init__(self, n=1, normalize=False, weekday=0, startingMonth=1,\n                 qtr_with_extra_week=1, variation=\"nearest\"):\n        BaseOffset.__init__(self, n, normalize)\n\n        object.__setattr__(self, \"startingMonth\", startingMonth)\n        object.__setattr__(self, \"weekday\", weekday)\n        object.__setattr__(self, \"qtr_with_extra_week\", qtr_with_extra_week)\n        object.__setattr__(self, \"variation\", variation)\n\n        if self.n == 0:\n            raise ValueError('N cannot be 0')\n\n    @cache_readonly\n    def _offset(self):\n        return FY5253(startingMonth=self.startingMonth,\n                      weekday=self.weekday,\n                      variation=self.variation)\n\n    def isAnchored(self):\n        return self.n == 1 and self._offset.isAnchored()\n\n    def _rollback_to_year(self, other):\n        \"\"\"roll `other` back to the most recent date that was on a fiscal year\n        end.  Return the date of that year-end, the number of full quarters\n        elapsed between that year-end and other, and the remaining Timedelta\n        since the most recent quarter-end.\n\n        Parameters\n        ----------\n        other : datetime or Timestamp\n\n        Returns\n        -------\n        tuple of\n        prev_year_end : Timestamp giving most recent fiscal year end\n        num_qtrs : int\n        tdelta : Timedelta\n        \"\"\"\n        num_qtrs = 0\n\n        norm = Timestamp(other).tz_localize(None)\n        start = self._offset.rollback(norm)\n        # Note: start <= norm and self._offset.onOffset(start)\n\n        if start < norm:\n            # roll adjustment\n            qtr_lens = self.get_weeks(norm)\n\n            # check thet qtr_lens is consistent with self._offset addition\n            end = liboffsets.shift_day(start, days=7 * sum(qtr_lens))\n            assert self._offset.onOffset(end), (start, end, qtr_lens)\n\n            tdelta = norm - start\n            for qlen in qtr_lens:\n                if qlen * 7 <= tdelta.days:\n                    num_qtrs += 1\n                    tdelta -= Timedelta(days=qlen * 7)\n                else:\n                    break\n        else:\n            tdelta = Timedelta(0)\n\n        # Note: we always have tdelta.value >= 0\n        return start, num_qtrs, tdelta\n\n    @apply_wraps\n    def apply(self, other):\n        # Note: self.n == 0 is not allowed.\n        n = self.n\n\n        prev_year_end, num_qtrs, tdelta = self._rollback_to_year(other)\n        res = prev_year_end\n        n += num_qtrs\n        if self.n <= 0 and tdelta.value > 0:\n            n += 1\n\n        # Possible speedup by handling years first.\n        years = n // 4\n        if years:\n            res += self._offset * years\n            n -= years * 4\n\n        # Add an extra day to make *sure* we are getting the quarter lengths\n        # for the upcoming year, not the previous year\n        qtr_lens = self.get_weeks(res + Timedelta(days=1))\n\n        # Note: we always have 0 <= n < 4\n        weeks = sum(qtr_lens[:n])\n        if weeks:\n            res = liboffsets.shift_day(res, days=weeks * 7)\n\n        return res\n\n    def get_weeks(self, dt):\n        ret = [13] * 4\n\n        year_has_extra_week = self.year_has_extra_week(dt)\n\n        if year_has_extra_week:\n            ret[self.qtr_with_extra_week - 1] = 14\n\n        return ret\n\n    def year_has_extra_week(self, dt):\n        # Avoid round-down errors --> normalize to get\n        # e.g. '370D' instead of '360D23H'\n        norm = Timestamp(dt).normalize().tz_localize(None)\n\n        next_year_end = self._offset.rollforward(norm)\n        prev_year_end = norm - self._offset\n        weeks_in_year = (next_year_end - prev_year_end).days / 7\n        assert weeks_in_year in [52, 53], weeks_in_year\n        return weeks_in_year == 53\n\n    def onOffset(self, dt):\n        if self.normalize and not _is_normalized(dt):\n            return False\n        if self._offset.onOffset(dt):\n            return True\n\n        next_year_end = dt - self._offset\n\n        qtr_lens = self.get_weeks(dt)\n\n        current = next_year_end\n        for qtr_len in qtr_lens:\n            current = liboffsets.shift_day(current, days=qtr_len * 7)\n            if dt == current:\n                return True\n        return False\n\n    @property\n    def rule_code(self):\n        suffix = self._offset.get_rule_code_suffix()\n        qtr = self.qtr_with_extra_week\n        return \"{prefix}-{suffix}-{qtr}\".format(prefix=self._prefix,\n                                                suffix=suffix, qtr=qtr)\n\n    @classmethod\n    def _from_name(cls, *args):\n        return cls(**dict(FY5253._parse_suffix(*args[:-1]),\n                          qtr_with_extra_week=int(args[-1])))\n\n\nclass Easter(DateOffset):\n    \"\"\"\n    DateOffset for the Easter holiday using\n    logic defined in dateutil.  Right now uses\n    the revised method which is valid in years\n    1583-4099.\n    \"\"\"\n    _adjust_dst = True\n    _attributes = frozenset(['n', 'normalize'])\n\n    __init__ = BaseOffset.__init__\n\n    @apply_wraps\n    def apply(self, other):\n        current_easter = easter(other.year)\n        current_easter = datetime(current_easter.year,\n                                  current_easter.month, current_easter.day)\n        current_easter = conversion.localize_pydatetime(current_easter,\n                                                        other.tzinfo)\n\n        n = self.n\n        if n >= 0 and other < current_easter:\n            n -= 1\n        elif n < 0 and other > current_easter:\n            n += 1\n        # TODO: Why does this handle the 0 case the opposite of others?\n\n        # NOTE: easter returns a datetime.date so we have to convert to type of\n        # other\n        new = easter(other.year + n)\n        new = datetime(new.year, new.month, new.day, other.hour,\n                       other.minute, other.second, other.microsecond)\n        return new\n\n    def onOffset(self, dt):\n        if self.normalize and not _is_normalized(dt):\n            return False\n        return date(dt.year, dt.month, dt.day) == easter(dt.year)\n\n\nclass CalendarDay(SingleConstructorOffset):\n    \"\"\"\n    Calendar day offset. Respects calendar arithmetic as opposed to Day which\n    respects absolute time.\n    \"\"\"\n    _adjust_dst = True\n    _inc = Timedelta(days=1)\n    _prefix = 'CD'\n    _attributes = frozenset(['n', 'normalize'])\n\n    def __init__(self, n=1, normalize=False):\n        BaseOffset.__init__(self, n, normalize)\n\n    @apply_wraps\n    def apply(self, other):\n        \"\"\"\n        Apply scalar arithmetic with CalendarDay offset. Incoming datetime\n        objects can be tz-aware or naive.\n        \"\"\"\n        if type(other) == type(self):\n            # Add other CalendarDays\n            return type(self)(self.n + other.n, normalize=self.normalize)\n        tzinfo = getattr(other, 'tzinfo', None)\n        if tzinfo is not None:\n            other = other.replace(tzinfo=None)\n\n        other = other + self.n * self._inc\n\n        if tzinfo is not None:\n            # This can raise a AmbiguousTimeError or NonExistentTimeError\n            other = conversion.localize_pydatetime(other, tzinfo)\n\n        try:\n            return as_timestamp(other)\n        except TypeError:\n            raise TypeError(\"Cannot perform arithmetic between {other} and \"\n                            \"CalendarDay\".format(other=type(other)))\n\n    @apply_index_wraps\n    def apply_index(self, i):\n        \"\"\"\n        Apply the CalendarDay offset to a DatetimeIndex. Incoming DatetimeIndex\n        objects are assumed to be tz_naive\n        \"\"\"\n        return i + self.n * self._inc\n\n\n# ---------------------------------------------------------------------\n# Ticks\n\n\ndef _tick_comp(op):\n    assert op not in [operator.eq, operator.ne]\n\n    def f(self, other):\n        try:\n            return op(self.delta, other.delta)\n        except AttributeError:\n            # comparing with a non-Tick object\n            raise TypeError(\"Invalid comparison between {cls} and {typ}\"\n                            .format(cls=type(self).__name__,\n                                    typ=type(other).__name__))\n\n    f.__name__ = '__{opname}__'.format(opname=op.__name__)\n    return f\n\n\nclass Tick(liboffsets._Tick, SingleConstructorOffset):\n    _inc = Timedelta(microseconds=1000)\n    _prefix = 'undefined'\n    _attributes = frozenset(['n', 'normalize'])\n\n    def __init__(self, n=1, normalize=False):\n        BaseOffset.__init__(self, n, normalize)\n        if normalize:\n            raise ValueError(\"Tick offset with `normalize=True` are not \"\n                             \"allowed.\")  # GH#21427\n\n    __gt__ = _tick_comp(operator.gt)\n    __ge__ = _tick_comp(operator.ge)\n    __lt__ = _tick_comp(operator.lt)\n    __le__ = _tick_comp(operator.le)\n\n    def __add__(self, other):\n        if isinstance(other, Tick):\n            if type(self) == type(other):\n                return type(self)(self.n + other.n)\n            else:\n                return _delta_to_tick(self.delta + other.delta)\n        elif isinstance(other, ABCPeriod):\n            return other + self\n        try:\n            return self.apply(other)\n        except ApplyTypeError:\n            return NotImplemented\n        except OverflowError:\n            raise OverflowError(\"the add operation between {self} and {other} \"\n                                \"will overflow\".format(self=self, other=other))\n\n    def __eq__(self, other):\n        if isinstance(other, compat.string_types):\n            from pandas.tseries.frequencies import to_offset\n            try:\n                # GH#23524 if to_offset fails, we are dealing with an\n                #  incomparable type so == is False and != is True\n                other = to_offset(other)\n            except ValueError:\n                # e.g. \"infer\"\n                return False\n\n        if isinstance(other, Tick):\n            return self.delta == other.delta\n        else:\n            return False\n\n    # This is identical to DateOffset.__hash__, but has to be redefined here\n    # for Python 3, because we've redefined __eq__.\n    def __hash__(self):\n        return hash(self._params)\n\n    def __ne__(self, other):\n        if isinstance(other, compat.string_types):\n            from pandas.tseries.frequencies import to_offset\n            try:\n                # GH#23524 if to_offset fails, we are dealing with an\n                #  incomparable type so == is False and != is True\n                other = to_offset(other)\n            except ValueError:\n                # e.g. \"infer\"\n                return True\n\n        if isinstance(other, Tick):\n            return self.delta != other.delta\n        else:\n            return True\n\n    @property\n    def delta(self):\n        return self.n * self._inc\n\n    @property\n    def nanos(self):\n        return delta_to_nanoseconds(self.delta)\n\n    # TODO: Should Tick have its own apply_index?\n    def apply(self, other):\n        # Timestamp can handle tz and nano sec, thus no need to use apply_wraps\n        if isinstance(other, Timestamp):\n\n            # GH 15126\n            # in order to avoid a recursive\n            # call of __add__ and __radd__ if there is\n            # an exception, when we call using the + operator,\n            # we directly call the known method\n            result = other.__add__(self)\n            if result == NotImplemented:\n                raise OverflowError\n            return result\n        elif isinstance(other, (datetime, np.datetime64, date)):\n            return as_timestamp(other) + self\n\n        if isinstance(other, timedelta):\n            return other + self.delta\n        elif isinstance(other, type(self)):\n            return type(self)(self.n + other.n)\n\n        raise ApplyTypeError('Unhandled type: {type_str}'\n                             .format(type_str=type(other).__name__))\n\n    def isAnchored(self):\n        return False\n\n\ndef _delta_to_tick(delta):\n    if delta.microseconds == 0:\n        if delta.seconds == 0:\n            return Day(delta.days)\n        else:\n            seconds = delta.days * 86400 + delta.seconds\n            if seconds % 3600 == 0:\n                return Hour(seconds / 3600)\n            elif seconds % 60 == 0:\n                return Minute(seconds / 60)\n            else:\n                return Second(seconds)\n    else:\n        nanos = delta_to_nanoseconds(delta)\n        if nanos % 1000000 == 0:\n            return Milli(nanos // 1000000)\n        elif nanos % 1000 == 0:\n            return Micro(nanos // 1000)\n        else:  # pragma: no cover\n            return Nano(nanos)\n\n\nclass Day(Tick):\n    _inc = Timedelta(days=1)\n    _prefix = 'D'\n\n\nclass Hour(Tick):\n    _inc = Timedelta(hours=1)\n    _prefix = 'H'\n\n\nclass Minute(Tick):\n    _inc = Timedelta(minutes=1)\n    _prefix = 'T'\n\n\nclass Second(Tick):\n    _inc = Timedelta(seconds=1)\n    _prefix = 'S'\n\n\nclass Milli(Tick):\n    _inc = Timedelta(milliseconds=1)\n    _prefix = 'L'\n\n\nclass Micro(Tick):\n    _inc = Timedelta(microseconds=1)\n    _prefix = 'U'\n\n\nclass Nano(Tick):\n    _inc = Timedelta(nanoseconds=1)\n    _prefix = 'N'\n\n\nBDay = BusinessDay\nBMonthEnd = BusinessMonthEnd\nBMonthBegin = BusinessMonthBegin\nCBMonthEnd = CustomBusinessMonthEnd\nCBMonthBegin = CustomBusinessMonthBegin\nCDay = CustomBusinessDay\n\n# ---------------------------------------------------------------------\n\n\ndef generate_range(start=None, end=None, periods=None,\n                   offset=BDay(), time_rule=None):\n    \"\"\"\n    Generates a sequence of dates corresponding to the specified time\n    offset. Similar to dateutil.rrule except uses pandas DateOffset\n    objects to represent time increments\n\n    Parameters\n    ----------\n    start : datetime (default None)\n    end : datetime (default None)\n    periods : int, (default None)\n    offset : DateOffset, (default BDay())\n    time_rule : (legacy) name of DateOffset object to be used, optional\n        Corresponds with names expected by tseries.frequencies.get_offset\n\n    Notes\n    -----\n    * This method is faster for generating weekdays than dateutil.rrule\n    * At least two of (start, end, periods) must be specified.\n    * If both start and end are specified, the returned dates will\n    satisfy start <= date <= end.\n    * If both time_rule and offset are specified, time_rule supersedes offset.\n\n    Returns\n    -------\n    dates : generator object\n\n    \"\"\"\n    if time_rule is not None:\n        from pandas.tseries.frequencies import get_offset\n\n        offset = get_offset(time_rule)\n\n    start = to_datetime(start)\n    end = to_datetime(end)\n\n    if start and not offset.onOffset(start):\n        start = offset.rollforward(start)\n\n    elif end and not offset.onOffset(end):\n        end = offset.rollback(end)\n\n    if periods is None and end < start and offset.n >= 0:\n        end = None\n        periods = 0\n\n    if end is None:\n        end = start + (periods - 1) * offset\n\n    if start is None:\n        start = end - (periods - 1) * offset\n\n    cur = start\n    if offset.n >= 0:\n        while cur <= end:\n            yield cur\n\n            # faster than cur + offset\n            next_date = offset.apply(cur)\n            if next_date <= cur:\n                raise ValueError('Offset {offset} did not increment date'\n                                 .format(offset=offset))\n            cur = next_date\n    else:\n        while cur >= end:\n            yield cur\n\n            # faster than cur + offset\n            next_date = offset.apply(cur)\n            if next_date >= cur:\n                raise ValueError('Offset {offset} did not decrement date'\n                                 .format(offset=offset))\n            cur = next_date\n\n\nprefix_mapping = {offset._prefix: offset for offset in [\n    YearBegin,                 # 'AS'\n    YearEnd,                   # 'A'\n    BYearBegin,                # 'BAS'\n    BYearEnd,                  # 'BA'\n    BusinessDay,               # 'B'\n    BusinessMonthBegin,        # 'BMS'\n    BusinessMonthEnd,          # 'BM'\n    BQuarterEnd,               # 'BQ'\n    BQuarterBegin,             # 'BQS'\n    BusinessHour,              # 'BH'\n    CustomBusinessDay,         # 'C'\n    CustomBusinessMonthEnd,    # 'CBM'\n    CustomBusinessMonthBegin,  # 'CBMS'\n    CustomBusinessHour,        # 'CBH'\n    MonthEnd,                  # 'M'\n    MonthBegin,                # 'MS'\n    Nano,                      # 'N'\n    SemiMonthEnd,              # 'SM'\n    SemiMonthBegin,            # 'SMS'\n    Week,                      # 'W'\n    Second,                    # 'S'\n    Minute,                    # 'T'\n    Micro,                     # 'U'\n    QuarterEnd,                # 'Q'\n    QuarterBegin,              # 'QS'\n    Milli,                     # 'L'\n    Hour,                      # 'H'\n    Day,                       # 'D'\n    WeekOfMonth,               # 'WOM'\n    FY5253,\n    FY5253Quarter,\n    CalendarDay                # 'CD'\n]}\n"
    },
    {
      "filename": "pandas/util/_decorators.py",
      "content": "from functools import WRAPPER_ASSIGNMENTS, update_wrapper, wraps\nimport inspect\nfrom textwrap import dedent, wrap\nimport warnings\n\nfrom pandas._libs.properties import cache_readonly  # noqa\nfrom pandas.compat import PY2, callable, signature\n\n\ndef deprecate(name, alternative, version, alt_name=None,\n              klass=None, stacklevel=2, msg=None):\n    \"\"\"Return a new function that emits a deprecation warning on use.\n\n    To use this method for a deprecated function, another function\n    `alternative` with the same signature must exist. The deprecated\n    function will emit a deprecation warning, and in the docstring\n    it will contain the deprecation directive with the provided version\n    so it can be detected for future removal.\n\n    Parameters\n    ----------\n    name : str\n        Name of function to deprecate.\n    alternative : func\n        Function to use instead.\n    version : str\n        Version of pandas in which the method has been deprecated.\n    alt_name : str, optional\n        Name to use in preference of alternative.__name__.\n    klass : Warning, default FutureWarning\n    stacklevel : int, default 2\n    msg : str\n        The message to display in the warning.\n        Default is '{name} is deprecated. Use {alt_name} instead.'\n    \"\"\"\n\n    alt_name = alt_name or alternative.__name__\n    klass = klass or FutureWarning\n    warning_msg = msg or '{} is deprecated, use {} instead'.format(name,\n                                                                   alt_name)\n\n    # adding deprecated directive to the docstring\n    msg = msg or 'Use `{alt_name}` instead.'.format(alt_name=alt_name)\n    msg = '\\n    '.join(wrap(msg, 70))\n\n    @Substitution(version=version, msg=msg)\n    @Appender(alternative.__doc__)\n    def wrapper(*args, **kwargs):\n        \"\"\"\n        .. deprecated:: %(version)s\n\n           %(msg)s\n\n        \"\"\"\n        warnings.warn(warning_msg, klass, stacklevel=stacklevel)\n        return alternative(*args, **kwargs)\n\n    # Since we are using Substitution to create the required docstring,\n    # remove that from the attributes that should be assigned to the wrapper\n    assignments = tuple(x for x in WRAPPER_ASSIGNMENTS if x != '__doc__')\n    update_wrapper(wrapper, alternative, assigned=assignments)\n\n    return wrapper\n\n\ndef deprecate_kwarg(old_arg_name, new_arg_name, mapping=None, stacklevel=2):\n    \"\"\"\n    Decorator to deprecate a keyword argument of a function.\n\n    Parameters\n    ----------\n    old_arg_name : str\n        Name of argument in function to deprecate\n    new_arg_name : str or None\n        Name of preferred argument in function. Use None to raise warning that\n        ``old_arg_name`` keyword is deprecated.\n    mapping : dict or callable\n        If mapping is present, use it to translate old arguments to\n        new arguments. A callable must do its own value checking;\n        values not found in a dict will be forwarded unchanged.\n\n    Examples\n    --------\n    The following deprecates 'cols', using 'columns' instead\n\n    >>> @deprecate_kwarg(old_arg_name='cols', new_arg_name='columns')\n    ... def f(columns=''):\n    ...     print(columns)\n    ...\n    >>> f(columns='should work ok')\n    should work ok\n\n    >>> f(cols='should raise warning')\n    FutureWarning: cols is deprecated, use columns instead\n      warnings.warn(msg, FutureWarning)\n    should raise warning\n\n    >>> f(cols='should error', columns=\"can\\'t pass do both\")\n    TypeError: Can only specify 'cols' or 'columns', not both\n\n    >>> @deprecate_kwarg('old', 'new', {'yes': True, 'no': False})\n    ... def f(new=False):\n    ...     print('yes!' if new else 'no!')\n    ...\n    >>> f(old='yes')\n    FutureWarning: old='yes' is deprecated, use new=True instead\n      warnings.warn(msg, FutureWarning)\n    yes!\n\n    To raise a warning that a keyword will be removed entirely in the future\n\n    >>> @deprecate_kwarg(old_arg_name='cols', new_arg_name=None)\n    ... def f(cols='', another_param=''):\n    ...     print(cols)\n    ...\n    >>> f(cols='should raise warning')\n    FutureWarning: the 'cols' keyword is deprecated and will be removed in a\n    future version please takes steps to stop use of 'cols'\n    should raise warning\n    >>> f(another_param='should not raise warning')\n    should not raise warning\n\n    >>> f(cols='should raise warning', another_param='')\n    FutureWarning: the 'cols' keyword is deprecated and will be removed in a\n    future version please takes steps to stop use of 'cols'\n    should raise warning\n    \"\"\"\n\n    if mapping is not None and not hasattr(mapping, 'get') and \\\n            not callable(mapping):\n        raise TypeError(\"mapping from old to new argument values \"\n                        \"must be dict or callable!\")\n\n    def _deprecate_kwarg(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            old_arg_value = kwargs.pop(old_arg_name, None)\n\n            if new_arg_name is None and old_arg_value is not None:\n                msg = (\n                    \"the '{old_name}' keyword is deprecated and will be \"\n                    \"removed in a future version. \"\n                    \"Please take steps to stop the use of '{old_name}'\"\n                ).format(old_name=old_arg_name)\n                warnings.warn(msg, FutureWarning, stacklevel=stacklevel)\n                kwargs[old_arg_name] = old_arg_value\n                return func(*args, **kwargs)\n\n            if old_arg_value is not None:\n                if mapping is not None:\n                    if hasattr(mapping, 'get'):\n                        new_arg_value = mapping.get(old_arg_value,\n                                                    old_arg_value)\n                    else:\n                        new_arg_value = mapping(old_arg_value)\n                    msg = (\"the {old_name}={old_val!r} keyword is deprecated, \"\n                           \"use {new_name}={new_val!r} instead\"\n                           ).format(old_name=old_arg_name,\n                                    old_val=old_arg_value,\n                                    new_name=new_arg_name,\n                                    new_val=new_arg_value)\n                else:\n                    new_arg_value = old_arg_value\n                    msg = (\"the '{old_name}' keyword is deprecated, \"\n                           \"use '{new_name}' instead\"\n                           ).format(old_name=old_arg_name,\n                                    new_name=new_arg_name)\n\n                warnings.warn(msg, FutureWarning, stacklevel=stacklevel)\n                if kwargs.get(new_arg_name, None) is not None:\n                    msg = (\"Can only specify '{old_name}' or '{new_name}', \"\n                           \"not both\").format(old_name=old_arg_name,\n                                              new_name=new_arg_name)\n                    raise TypeError(msg)\n                else:\n                    kwargs[new_arg_name] = new_arg_value\n            return func(*args, **kwargs)\n        return wrapper\n    return _deprecate_kwarg\n\n\ndef rewrite_axis_style_signature(name, extra_params):\n    def decorate(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n\n        if not PY2:\n            kind = inspect.Parameter.POSITIONAL_OR_KEYWORD\n            params = [\n                inspect.Parameter('self', kind),\n                inspect.Parameter(name, kind, default=None),\n                inspect.Parameter('index', kind, default=None),\n                inspect.Parameter('columns', kind, default=None),\n                inspect.Parameter('axis', kind, default=None),\n            ]\n\n            for pname, default in extra_params:\n                params.append(inspect.Parameter(pname, kind, default=default))\n\n            sig = inspect.Signature(params)\n\n            func.__signature__ = sig\n        return wrapper\n    return decorate\n\n# Substitution and Appender are derived from matplotlib.docstring (1.1.0)\n# module http://matplotlib.org/users/license.html\n\n\nclass Substitution(object):\n    \"\"\"\n    A decorator to take a function's docstring and perform string\n    substitution on it.\n\n    This decorator should be robust even if func.__doc__ is None\n    (for example, if -OO was passed to the interpreter)\n\n    Usage: construct a docstring.Substitution with a sequence or\n    dictionary suitable for performing substitution; then\n    decorate a suitable function with the constructed object. e.g.\n\n    sub_author_name = Substitution(author='Jason')\n\n    @sub_author_name\n    def some_function(x):\n        \"%(author)s wrote this function\"\n\n    # note that some_function.__doc__ is now \"Jason wrote this function\"\n\n    One can also use positional arguments.\n\n    sub_first_last_names = Substitution('Edgar Allen', 'Poe')\n\n    @sub_first_last_names\n    def some_function(x):\n        \"%s %s wrote the Raven\"\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        if (args and kwargs):\n            raise AssertionError(\"Only positional or keyword args are allowed\")\n\n        self.params = args or kwargs\n\n    def __call__(self, func):\n        func.__doc__ = func.__doc__ and func.__doc__ % self.params\n        return func\n\n    def update(self, *args, **kwargs):\n        \"\"\"\n        Update self.params with supplied args.\n\n        If called, we assume self.params is a dict.\n        \"\"\"\n\n        self.params.update(*args, **kwargs)\n\n    @classmethod\n    def from_params(cls, params):\n        \"\"\"\n        In the case where the params is a mutable sequence (list or dictionary)\n        and it may change before this class is called, one may explicitly use a\n        reference to the params rather than using *args or **kwargs which will\n        copy the values and not reference them.\n        \"\"\"\n        result = cls()\n        result.params = params\n        return result\n\n\nclass Appender(object):\n    \"\"\"\n    A function decorator that will append an addendum to the docstring\n    of the target function.\n\n    This decorator should be robust even if func.__doc__ is None\n    (for example, if -OO was passed to the interpreter).\n\n    Usage: construct a docstring.Appender with a string to be joined to\n    the original docstring. An optional 'join' parameter may be supplied\n    which will be used to join the docstring and addendum. e.g.\n\n    add_copyright = Appender(\"Copyright (c) 2009\", join='\\n')\n\n    @add_copyright\n    def my_dog(has='fleas'):\n        \"This docstring will have a copyright below\"\n        pass\n    \"\"\"\n\n    def __init__(self, addendum, join='', indents=0):\n        if indents > 0:\n            self.addendum = indent(addendum, indents=indents)\n        else:\n            self.addendum = addendum\n        self.join = join\n\n    def __call__(self, func):\n        func.__doc__ = func.__doc__ if func.__doc__ else ''\n        self.addendum = self.addendum if self.addendum else ''\n        docitems = [func.__doc__, self.addendum]\n        func.__doc__ = dedent(self.join.join(docitems))\n        return func\n\n\ndef indent(text, indents=1):\n    if not text or not isinstance(text, str):\n        return ''\n    jointext = ''.join(['\\n'] + ['    '] * indents)\n    return jointext.join(text.split('\\n'))\n\n\ndef make_signature(func):\n    \"\"\"\n    Returns a tuple containing the paramenter list with defaults\n    and parameter list.\n\n    Examples\n    --------\n    >>> def f(a, b, c=2):\n    >>>     return a * b * c\n    >>> print(make_signature(f))\n    (['a', 'b', 'c=2'], ['a', 'b', 'c'])\n    \"\"\"\n\n    spec = signature(func)\n    if spec.defaults is None:\n        n_wo_defaults = len(spec.args)\n        defaults = ('',) * n_wo_defaults\n    else:\n        n_wo_defaults = len(spec.args) - len(spec.defaults)\n        defaults = ('',) * n_wo_defaults + tuple(spec.defaults)\n    args = []\n    for var, default in zip(spec.args, defaults):\n        args.append(var if default == '' else var + '=' + repr(default))\n    if spec.varargs:\n        args.append('*' + spec.varargs)\n    if spec.keywords:\n        args.append('**' + spec.keywords)\n    return args, spec.args\n"
    }
  ]
}