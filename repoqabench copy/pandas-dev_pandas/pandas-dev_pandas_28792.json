{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "28792",
  "issue_description": "# DOC: Fix SA04 errors in docstrings\n\nThere are issues in some docstrings where the See Also section is not complete. All of the following errors exist because there is no description for a referenced function or method. If these can get fixed then CI will be able to check all of the See Also section errors! The command to check for the errors is `./scripts/validate_docstrings.py --errors=SA04`\r\n\r\n```\r\npandas.melt: Missing description for See Also \"DataFrame.melt\" reference\r\npandas.melt: Missing description for See Also \"pivot_table\" reference\r\npandas.melt: Missing description for See Also \"DataFrame.pivot\" reference\r\npandas.melt: Missing description for See Also \"Series.explode\" reference\r\npandas.merge_ordered: Missing description for See Also \"merge\" reference\r\npandas.merge_ordered: Missing description for See Also \"merge_asof\" reference\r\npandas.merge_asof: Missing description for See Also \"merge\" reference\r\npandas.merge_asof: Missing description for See Also \"merge_ordered\" reference\r\npandas.unique: Missing description for See Also \"Index.unique\" reference\r\npandas.unique: Missing description for See Also \"Series.unique\" reference\r\npandas.eval: Missing description for See Also \"DataFrame.query\" reference\r\npandas.eval: Missing description for See Also \"DataFrame.eval\" reference\r\npandas.api.extensions.register_dataframe_accessor: Missing description for See Also \"register_series_accessor\" reference\r\npandas.api.extensions.register_dataframe_accessor: Missing description for See Also \"register_index_accessor\" reference\r\npandas.api.extensions.register_series_accessor: Missing description for See Also \"register_dataframe_accessor\" reference\r\npandas.api.extensions.register_series_accessor: Missing description for See Also \"register_index_accessor\" reference\r\npandas.api.extensions.register_index_accessor: Missing description for See Also \"register_dataframe_accessor\" reference\r\npandas.api.extensions.register_index_accessor: Missing description for See Also \"register_series_accessor\" reference\r\npandas.api.extensions.ExtensionDtype: Missing description for See Also \"extensions.register_extension_dtype\" reference\r\npandas.api.extensions.ExtensionDtype: Missing description for See Also \"extensions.ExtensionArray\" reference\r\npandas.api.extensions.ExtensionArray._from_factorized: Missing description for See Also \"factorize\" reference\r\npandas.api.extensions.ExtensionArray._from_factorized: Missing description for See Also \"ExtensionArray.factorize\" reference\r\npandas.api.extensions.ExtensionArray._values_for_argsort: Missing description for See Also \"ExtensionArray.argsort\" reference\r\npandas.api.extensions.ExtensionArray.take: Missing description for See Also \"numpy.take\" reference\r\npandas.api.extensions.ExtensionArray.take: Missing description for See Also \"api.extensions.take\" reference\r\npandas.io.formats.style.Styler: Missing description for See Also \"DataFrame.style\" reference\r\npandas.io.formats.style.Styler.applymap: Missing description for See Also \"Styler.where\" reference\r\npandas.io.formats.style.Styler.where: Missing description for See Also \"Styler.applymap\" reference\r\npandas.io.formats.style.Styler.export: Missing description for See Also \"Styler.use\" reference\r\npandas.io.formats.style.Styler.use: Missing description for See Also \"Styler.export\" reference\r\npandas.DataFrame.shape: Missing description for See Also \"ndarray.shape\" reference\r\npandas.DataFrame.empty: Missing description for See Also \"Series.dropna\" reference\r\npandas.DataFrame.empty: Missing description for See Also \"DataFrame.dropna\" reference\r\npandas.DataFrame.pipe: Missing description for See Also \"DataFrame.apply\" reference\r\npandas.DataFrame.pipe: Missing description for See Also \"DataFrame.applymap\" reference\r\npandas.DataFrame.pipe: Missing description for See Also \"Series.map\" reference\r\npandas.DataFrame.corr: Missing description for See Also \"DataFrame.corrwith\" reference\r\npandas.DataFrame.corr: Missing description for See Also \"Series.corr\" reference\r\npandas.DataFrame.corrwith: Missing description for See Also \"DataFrame.corr\" reference\r\npandas.DataFrame.filter: Missing description for See Also \"DataFrame.loc\" reference\r\npandas.DataFrame.idxmax: Missing description for See Also \"Series.idxmax\" reference\r\npandas.DataFrame.idxmin: Missing description for See Also \"Series.idxmin\" reference\r\npandas.DataFrame.melt: Missing description for See Also \"melt\" reference\r\npandas.DataFrame.melt: Missing description for See Also \"pivot_table\" reference\r\npandas.DataFrame.melt: Missing description for See Also \"DataFrame.pivot\" reference\r\npandas.DataFrame.melt: Missing description for See Also \"Series.explode\" reference\r\npandas.DataFrame.asfreq: Missing description for See Also \"reindex\" reference\r\npandas.DataFrame.to_json: Missing description for See Also \"read_json\" reference\r\npandas.DataFrame.style: Missing description for See Also \"io.formats.style.Styler\" reference\r\npandas.Series.put: Missing description for See Also \"numpy.ndarray.put\" reference\r\npandas.Series.to_list: Missing description for See Also \"numpy.ndarray.tolist\" reference\r\npandas.Series.add: Missing description for See Also \"Series.radd\" reference\r\npandas.Series.sub: Missing description for See Also \"Series.rsub\" reference\r\npandas.Series.mul: Missing description for See Also \"Series.rmul\" reference\r\npandas.Series.div: Missing description for See Also \"Series.rtruediv\" reference\r\npandas.Series.truediv: Missing description for See Also \"Series.rtruediv\" reference\r\npandas.Series.floordiv: Missing description for See Also \"Series.rfloordiv\" reference\r\npandas.Series.mod: Missing description for See Also \"Series.rmod\" reference\r\npandas.Series.pow: Missing description for See Also \"Series.rpow\" reference\r\npandas.Series.radd: Missing description for See Also \"Series.add\" reference\r\npandas.Series.rsub: Missing description for See Also \"Series.sub\" reference\r\npandas.Series.rmul: Missing description for See Also \"Series.mul\" reference\r\npandas.Series.rdiv: Missing description for See Also \"Series.truediv\" reference\r\npandas.Series.rtruediv: Missing description for See Also \"Series.truediv\" reference\r\npandas.Series.rfloordiv: Missing description for See Also \"Series.floordiv\" reference\r\npandas.Series.rmod: Missing description for See Also \"Series.mod\" reference\r\npandas.Series.rpow: Missing description for See Also \"Series.pow\" reference\r\npandas.Series.lt: Missing description for See Also \"Series.None\" reference\r\npandas.Series.gt: Missing description for See Also \"Series.None\" reference\r\npandas.Series.le: Missing description for See Also \"Series.None\" reference\r\npandas.Series.ge: Missing description for See Also \"Series.None\" reference\r\npandas.Series.ne: Missing description for See Also \"Series.None\" reference\r\npandas.Series.eq: Missing description for See Also \"Series.None\" reference\r\npandas.Series.pipe: Missing description for See Also \"DataFrame.apply\" reference\r\npandas.Series.pipe: Missing description for See Also \"DataFrame.applymap\" reference\r\npandas.Series.pipe: Missing description for See Also \"Series.map\" reference\r\npandas.Series.quantile: Missing description for See Also \"core.window.Rolling.quantile\" reference\r\npandas.Series.quantile: Missing description for See Also \"numpy.percentile\" reference\r\npandas.Series.filter: Missing description for See Also \"DataFrame.loc\" reference\r\npandas.Series.argsort: Missing description for See Also \"numpy.ndarray.argsort\" reference\r\npandas.Series.searchsorted: Missing description for See Also \"numpy.searchsorted\" reference\r\npandas.Series.ravel: Missing description for See Also \"numpy.ndarray.ravel\" reference\r\npandas.Series.asfreq: Missing description for See Also \"reindex\" reference\r\npandas.Series.dt.to_pytimedelta: Missing description for See Also \"datetime.timedelta\" reference\r\npandas.Series.cat.categories: Missing description for See Also \"rename_categories\" reference\r\npandas.Series.cat.categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.Series.cat.categories: Missing description for See Also \"add_categories\" reference\r\npandas.Series.cat.categories: Missing description for See Also \"remove_categories\" reference\r\npandas.Series.cat.categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.Series.cat.categories: Missing description for See Also \"set_categories\" reference\r\npandas.Series.cat.rename_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.Series.cat.rename_categories: Missing description for See Also \"add_categories\" reference\r\npandas.Series.cat.rename_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.Series.cat.rename_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.Series.cat.rename_categories: Missing description for See Also \"set_categories\" reference\r\npandas.Series.cat.reorder_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.Series.cat.reorder_categories: Missing description for See Also \"add_categories\" reference\r\npandas.Series.cat.reorder_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.Series.cat.reorder_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.Series.cat.reorder_categories: Missing description for See Also \"set_categories\" reference\r\npandas.Series.cat.add_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.Series.cat.add_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.Series.cat.add_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.Series.cat.add_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.Series.cat.add_categories: Missing description for See Also \"set_categories\" reference\r\npandas.Series.cat.remove_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.Series.cat.remove_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.Series.cat.remove_categories: Missing description for See Also \"add_categories\" reference\r\npandas.Series.cat.remove_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.Series.cat.remove_categories: Missing description for See Also \"set_categories\" reference\r\npandas.Series.cat.remove_unused_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.Series.cat.remove_unused_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.Series.cat.remove_unused_categories: Missing description for See Also \"add_categories\" reference\r\npandas.Series.cat.remove_unused_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.Series.cat.remove_unused_categories: Missing description for See Also \"set_categories\" reference\r\npandas.Series.cat.set_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.Series.cat.set_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.Series.cat.set_categories: Missing description for See Also \"add_categories\" reference\r\npandas.Series.cat.set_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.Series.cat.set_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.Series.to_json: Missing description for See Also \"read_json\" reference\r\npandas.Index: Missing description for See Also \"DatetimeIndex\" reference\r\npandas.Index: Missing description for See Also \"TimedeltaIndex\" reference\r\npandas.Index: Missing description for See Also \"PeriodIndex\" reference\r\npandas.Index: Missing description for See Also \"Int64Index\" reference\r\npandas.Index: Missing description for See Also \"UInt64Index\" reference\r\npandas.Index: Missing description for See Also \"Float64Index\" reference\r\npandas.Index.memory_usage: Missing description for See Also \"numpy.ndarray.nbytes\" reference\r\npandas.Index.argmin: Missing description for See Also \"numpy.ndarray.argmin\" reference\r\npandas.Index.argmax: Missing description for See Also \"numpy.ndarray.argmax\" reference\r\npandas.Index.take: Missing description for See Also \"numpy.ndarray.take\" reference\r\npandas.Index.putmask: Missing description for See Also \"numpy.ndarray.putmask\" reference\r\npandas.Index.unique: Missing description for See Also \"unique\" reference\r\npandas.Index.unique: Missing description for See Also \"Series.unique\" reference\r\npandas.Index.ravel: Missing description for See Also \"numpy.ndarray.ravel\" reference\r\npandas.Index.to_list: Missing description for See Also \"numpy.ndarray.tolist\" reference\r\npandas.Index.searchsorted: Missing description for See Also \"numpy.searchsorted\" reference\r\npandas.CategoricalIndex.rename_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.CategoricalIndex.rename_categories: Missing description for See Also \"add_categories\" reference\r\npandas.CategoricalIndex.rename_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.CategoricalIndex.rename_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.CategoricalIndex.rename_categories: Missing description for See Also \"set_categories\" reference\r\npandas.CategoricalIndex.reorder_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.CategoricalIndex.reorder_categories: Missing description for See Also \"add_categories\" reference\r\npandas.CategoricalIndex.reorder_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.CategoricalIndex.reorder_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.CategoricalIndex.reorder_categories: Missing description for See Also \"set_categories\" reference\r\npandas.CategoricalIndex.add_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.CategoricalIndex.add_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.CategoricalIndex.add_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.CategoricalIndex.add_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.CategoricalIndex.add_categories: Missing description for See Also \"set_categories\" reference\r\npandas.CategoricalIndex.remove_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.CategoricalIndex.remove_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.CategoricalIndex.remove_categories: Missing description for See Also \"add_categories\" reference\r\npandas.CategoricalIndex.remove_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.CategoricalIndex.remove_categories: Missing description for See Also \"set_categories\" reference\r\npandas.CategoricalIndex.remove_unused_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.CategoricalIndex.remove_unused_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.CategoricalIndex.remove_unused_categories: Missing description for See Also \"add_categories\" reference\r\npandas.CategoricalIndex.remove_unused_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.CategoricalIndex.remove_unused_categories: Missing description for See Also \"set_categories\" reference\r\npandas.CategoricalIndex.set_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.CategoricalIndex.set_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.CategoricalIndex.set_categories: Missing description for See Also \"add_categories\" reference\r\npandas.CategoricalIndex.set_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.CategoricalIndex.set_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.MultiIndex.to_frame: Missing description for See Also \"DataFrame\" reference\r\npandas.DatetimeIndex.indexer_at_time: Missing description for See Also \"indexer_between_time\" reference\r\npandas.DatetimeIndex.indexer_at_time: Missing description for See Also \"DataFrame.at_time\" reference\r\npandas.DatetimeIndex.indexer_between_time: Missing description for See Also \"indexer_at_time\" reference\r\npandas.DatetimeIndex.indexer_between_time: Missing description for See Also \"DataFrame.between_time\" reference\r\npandas.DatetimeIndex.mean: Missing description for See Also \"numpy.ndarray.mean\" reference\r\npandas.TimedeltaIndex.mean: Missing description for See Also \"numpy.ndarray.mean\" reference\r\npandas.core.window.rolling.Rolling.aggregate: Missing description for See Also \"Series.rolling\" reference\r\npandas.core.window.rolling.Rolling.aggregate: Missing description for See Also \"DataFrame.rolling\" reference\r\npandas.core.window.expanding.Expanding.aggregate: Missing description for See Also \"DataFrame.expanding.aggregate\" reference\r\npandas.core.window.expanding.Expanding.aggregate: Missing description for See Also \"DataFrame.rolling.aggregate\" reference\r\npandas.core.window.expanding.Expanding.aggregate: Missing description for See Also \"DataFrame.aggregate\" reference\r\npandas.plotting.deregister_matplotlib_converters: Missing description for See Also \"register_matplotlib_converters\" reference\r\npandas.plotting.register_matplotlib_converters: Missing description for See Also \"deregister_matplotlib_converters\" reference\r\npandas.read_html: Missing description for See Also \"read_csv\" reference\r\npandas.read_sql_query: Missing description for See Also \"read_sql\" reference\r\npandas.tseries.frequencies.to_offset: Missing description for See Also \"DateOffset\" reference\r\npandas.core.groupby.GroupBy.all: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.all: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.any: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.any: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.bfill: Missing description for See Also \"Series.backfill\" reference\r\npandas.core.groupby.GroupBy.bfill: Missing description for See Also \"DataFrame.backfill\" reference\r\npandas.core.groupby.GroupBy.bfill: Missing description for See Also \"Series.fillna\" reference\r\npandas.core.groupby.GroupBy.bfill: Missing description for See Also \"DataFrame.fillna\" reference\r\npandas.core.groupby.GroupBy.count: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.count: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.cummax: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.cummax: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.cummin: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.cummin: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.cumprod: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.cumprod: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.cumsum: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.cumsum: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.ffill: Missing description for See Also \"Series.pad\" reference\r\npandas.core.groupby.GroupBy.ffill: Missing description for See Also \"DataFrame.pad\" reference\r\npandas.core.groupby.GroupBy.ffill: Missing description for See Also \"Series.fillna\" reference\r\npandas.core.groupby.GroupBy.ffill: Missing description for See Also \"DataFrame.fillna\" reference\r\npandas.core.groupby.GroupBy.head: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.head: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.mean: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.mean: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.median: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.median: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.nth: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.nth: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.ohlc: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.ohlc: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.rank: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.rank: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.pct_change: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.pct_change: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.size: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.size: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.sem: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.sem: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.std: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.std: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.var: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.var: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.tail: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.tail: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.all: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.all: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.any: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.any: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.bfill: Missing description for See Also \"Series.backfill\" reference\r\npandas.core.groupby.DataFrameGroupBy.bfill: Missing description for See Also \"DataFrame.backfill\" reference\r\npandas.core.groupby.DataFrameGroupBy.bfill: Missing description for See Also \"Series.fillna\" reference\r\npandas.core.groupby.DataFrameGroupBy.bfill: Missing description for See Also \"DataFrame.fillna\" reference\r\npandas.core.groupby.DataFrameGroupBy.corr: Missing description for See Also \"DataFrame.corrwith\" reference\r\npandas.core.groupby.DataFrameGroupBy.corr: Missing description for See Also \"Series.corr\" reference\r\npandas.core.groupby.DataFrameGroupBy.cummax: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.cummax: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.cummin: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.cummin: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.cumprod: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.cumprod: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.cumsum: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.cumsum: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.ffill: Missing description for See Also \"Series.pad\" reference\r\npandas.core.groupby.DataFrameGroupBy.ffill: Missing description for See Also \"DataFrame.pad\" reference\r\npandas.core.groupby.DataFrameGroupBy.ffill: Missing description for See Also \"Series.fillna\" reference\r\npandas.core.groupby.DataFrameGroupBy.ffill: Missing description for See Also \"DataFrame.fillna\" reference\r\npandas.core.groupby.DataFrameGroupBy.idxmax: Missing description for See Also \"Series.idxmax\" reference\r\npandas.core.groupby.DataFrameGroupBy.idxmin: Missing description for See Also \"Series.idxmin\" reference\r\npandas.core.groupby.DataFrameGroupBy.pct_change: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.pct_change: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.rank: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.rank: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.shift: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.shift: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.size: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.size: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.corrwith: Missing description for See Also \"DataFrame.corr\" reference\r\npandas.core.resample.Resampler.__iter__: Missing description for See Also \"GroupBy.__iter__\" reference\r\npandas.core.resample.Resampler.apply: Missing description for See Also \"DataFrame.groupby.aggregate\" reference\r\npandas.core.resample.Resampler.apply: Missing description for See Also \"DataFrame.resample.transform\" reference\r\npandas.core.resample.Resampler.apply: Missing description for See Also \"DataFrame.aggregate\" reference\r\npandas.core.resample.Resampler.aggregate: Missing description for See Also \"DataFrame.groupby.aggregate\" reference\r\npandas.core.resample.Resampler.aggregate: Missing description for See Also \"DataFrame.resample.transform\" reference\r\npandas.core.resample.Resampler.aggregate: Missing description for See Also \"DataFrame.aggregate\" reference\r\npandas.core.resample.Resampler.ffill: Missing description for See Also \"Series.fillna\" reference\r\npandas.core.resample.Resampler.ffill: Missing description for See Also \"DataFrame.fillna\" reference\r\npandas.core.resample.Resampler.pad: Missing description for See Also \"Series.fillna\" reference\r\npandas.core.resample.Resampler.pad: Missing description for See Also \"DataFrame.fillna\" reference\r\npandas.core.resample.Resampler.asfreq: Missing description for See Also \"Series.asfreq\" reference\r\npandas.core.resample.Resampler.asfreq: Missing description for See Also \"DataFrame.asfreq\" reference\r\npandas.core.resample.Resampler.count: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.resample.Resampler.count: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.resample.Resampler.mean: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.resample.Resampler.mean: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.resample.Resampler.median: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.resample.Resampler.median: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.resample.Resampler.ohlc: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.resample.Resampler.ohlc: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.resample.Resampler.size: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.resample.Resampler.size: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.resample.Resampler.sem: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.resample.Resampler.sem: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.resample.Resampler.quantile: Missing description for See Also \"Series.quantile\" reference\r\npandas.core.resample.Resampler.quantile: Missing description for See Also \"DataFrame.quantile\" reference\r\npandas.core.resample.Resampler.quantile: Missing description for See Also \"DataFrameGroupBy.quantile\" reference\r\npandas.Timedelta.isoformat: Missing description for See Also \"Timestamp.isoformat\" reference\r\npandas.CategoricalDtype: Missing description for See Also \"Categorical\" reference\r\npandas.Categorical.categories: Missing description for See Also \"rename_categories\" reference\r\npandas.Categorical.categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.Categorical.categories: Missing description for See Also \"add_categories\" reference\r\npandas.Categorical.categories: Missing description for See Also \"remove_categories\" reference\r\npandas.Categorical.categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.Categorical.categories: Missing description for See Also \"set_categories\" reference\r\npandas.Series.as_matrix: Missing description for See Also \"DataFrame.values\" reference\r\npandas.Series.compress: Missing description for See Also \"numpy.ndarray.compress\" reference\r\npandas.Series.divide: Missing description for See Also \"Series.rtruediv\" reference\r\npandas.Series.divmod: Missing description for See Also \"Series.rdivmod\" reference\r\npandas.Series.multiply: Missing description for See Also \"Series.rmul\" reference\r\npandas.Series.nonzero: Missing description for See Also \"numpy.nonzero\" reference\r\npandas.Series.rdivmod: Missing description for See Also \"Series.divmod\" reference\r\npandas.Series.subtract: Missing description for See Also \"Series.rsub\" reference\r\npandas.Series.tolist: Missing description for See Also \"numpy.ndarray.tolist\" reference\r\npandas.DataFrame.as_matrix: Missing description for See Also \"DataFrame.values\" reference\r\n```",
  "issue_comments": [
    {
      "id": 538649174,
      "user": "MetalDent",
      "body": "Hi, anyone working on this?"
    },
    {
      "id": 538655645,
      "user": "ChiefMilesEdgeworth",
      "body": "Not right now. Just posted yesterday, so go for it!"
    },
    {
      "id": 538657826,
      "user": "MetalDent",
      "body": "@Nabel0721 what fix can you recommend?"
    },
    {
      "id": 538658160,
      "user": "ChiefMilesEdgeworth",
      "body": "If you find any in shared docs, that’ll fix the issue in multiple places. (For instance, I think Series.pipe and DataFrame.pipe come from the same shared_doc value). Other than that, just go for whatever you’d like! These don’t need to be done all at once. "
    },
    {
      "id": 538781208,
      "user": "suparnasnair",
      "body": "Hi @Nabel0721, may I work on this issue. I was able to locate some of these issues in the code documentation. I'm a newbie to contributing to open source, I'll try my best."
    },
    {
      "id": 538781372,
      "user": "ChiefMilesEdgeworth",
      "body": "For sure. I opened the issue, but that doesn't mean I'm in charge of it. If you want to work on something go ahead!"
    },
    {
      "id": 545257244,
      "user": "textractor",
      "body": "I'd like to help with this if help is still needed. I tried running the validate script but got an error:\r\n\r\n```\r\n$ ./scripts/validate_docstrings.py --errors=SA04\r\n\r\n  File \"./scripts/validate_docstrings.py\", line 62\r\n    DIRECTIVE_PATTERN = re.compile(rf\"^\\s*\\.\\. ({'|'.join(DIRECTIVES)})(?!::)\", re.I | re.M)\r\n                                                                             ^\r\n\r\nSyntaxError: invalid syntax\r\n```\r\n"
    },
    {
      "id": 545878580,
      "user": "ChiefMilesEdgeworth",
      "body": "@textractor Try running it with `python3 /scripts/validate_docstrings.py` instead. The shebang at the top is `usr/bin/python`, which works great if you only have python3 installed on your computer, but breaks it a little if you have 2 and 3."
    },
    {
      "id": 589959379,
      "user": "AdrianMastronardi",
      "body": "Hi, I'm working fixing these errors on `core/series.py`. I have a doubt regarding `core.window.Rolling.quantile`.\r\n\r\nIf understand correctly this should be the change to: `core.window.Rolling.quantile : Calculate the %(name)s quantile.`\r\n\r\nAppreciate your feedback. Thanks."
    },
    {
      "id": 635604956,
      "user": "willpeppo",
      "body": "is there still work to be done on this issue ?   I'm new and would like to help if there is."
    },
    {
      "id": 637605353,
      "user": "simonjayhawkins",
      "body": "> is there still work to be done on this issue ? I'm new and would like to help if there is.\r\n\r\n`./scripts/validate_docstrings.py --errors=SA04` should give you a list of what's still outstanding"
    },
    {
      "id": 638304645,
      "user": "willpeppo",
      "body": "Thank you.  This is my first contribution so I'm working through it piece by piece.  I just pushed my very first commit on PR06 documentation errors and it failed in the \"web and docs\" build.  Do you happen to know why?   I'm assuming that's an issue I need to resolve ?  Thanks..."
    },
    {
      "id": 711055181,
      "user": "nagesh-chowdaiah",
      "body": "Is this ticket still open and need some help?\r\n"
    },
    {
      "id": 711154761,
      "user": "simonjayhawkins",
      "body": "see https://github.com/pandas-dev/pandas/issues/28792#issuecomment-637605353"
    },
    {
      "id": 711161742,
      "user": "nagesh-chowdaiah",
      "body": "take"
    },
    {
      "id": 711168909,
      "user": "nagesh-chowdaiah",
      "body": "@simonjayhawkins Created https://github.com/pandas-dev/pandas/pull/37219 partial fix please review."
    },
    {
      "id": 712383552,
      "user": "nagesh-chowdaiah",
      "body": "@simonjayhawkins Completed this task.  \r\nExcept this. Not sure from where is this.\r\nNone:None:SA04:pandas.Timedelta.isoformat:Missing description for See Also \"Timestamp.isoformat\" reference"
    },
    {
      "id": 907049008,
      "user": "Varun270",
      "body": "@suparnasnair Can you please tell me that in \"pandas.DataFrame.empty\" and \"pandas.Series.subtract\" what are DataFrame, Series are? Are these the folder name or Python file name? Because I am not able to see any folder with such name."
    },
    {
      "id": 974599622,
      "user": "mroeschke",
      "body": "Closing as a duplicate of https://github.com/pandas-dev/pandas/issues/28792"
    }
  ],
  "text_context": "# DOC: Fix SA04 errors in docstrings\n\nThere are issues in some docstrings where the See Also section is not complete. All of the following errors exist because there is no description for a referenced function or method. If these can get fixed then CI will be able to check all of the See Also section errors! The command to check for the errors is `./scripts/validate_docstrings.py --errors=SA04`\r\n\r\n```\r\npandas.melt: Missing description for See Also \"DataFrame.melt\" reference\r\npandas.melt: Missing description for See Also \"pivot_table\" reference\r\npandas.melt: Missing description for See Also \"DataFrame.pivot\" reference\r\npandas.melt: Missing description for See Also \"Series.explode\" reference\r\npandas.merge_ordered: Missing description for See Also \"merge\" reference\r\npandas.merge_ordered: Missing description for See Also \"merge_asof\" reference\r\npandas.merge_asof: Missing description for See Also \"merge\" reference\r\npandas.merge_asof: Missing description for See Also \"merge_ordered\" reference\r\npandas.unique: Missing description for See Also \"Index.unique\" reference\r\npandas.unique: Missing description for See Also \"Series.unique\" reference\r\npandas.eval: Missing description for See Also \"DataFrame.query\" reference\r\npandas.eval: Missing description for See Also \"DataFrame.eval\" reference\r\npandas.api.extensions.register_dataframe_accessor: Missing description for See Also \"register_series_accessor\" reference\r\npandas.api.extensions.register_dataframe_accessor: Missing description for See Also \"register_index_accessor\" reference\r\npandas.api.extensions.register_series_accessor: Missing description for See Also \"register_dataframe_accessor\" reference\r\npandas.api.extensions.register_series_accessor: Missing description for See Also \"register_index_accessor\" reference\r\npandas.api.extensions.register_index_accessor: Missing description for See Also \"register_dataframe_accessor\" reference\r\npandas.api.extensions.register_index_accessor: Missing description for See Also \"register_series_accessor\" reference\r\npandas.api.extensions.ExtensionDtype: Missing description for See Also \"extensions.register_extension_dtype\" reference\r\npandas.api.extensions.ExtensionDtype: Missing description for See Also \"extensions.ExtensionArray\" reference\r\npandas.api.extensions.ExtensionArray._from_factorized: Missing description for See Also \"factorize\" reference\r\npandas.api.extensions.ExtensionArray._from_factorized: Missing description for See Also \"ExtensionArray.factorize\" reference\r\npandas.api.extensions.ExtensionArray._values_for_argsort: Missing description for See Also \"ExtensionArray.argsort\" reference\r\npandas.api.extensions.ExtensionArray.take: Missing description for See Also \"numpy.take\" reference\r\npandas.api.extensions.ExtensionArray.take: Missing description for See Also \"api.extensions.take\" reference\r\npandas.io.formats.style.Styler: Missing description for See Also \"DataFrame.style\" reference\r\npandas.io.formats.style.Styler.applymap: Missing description for See Also \"Styler.where\" reference\r\npandas.io.formats.style.Styler.where: Missing description for See Also \"Styler.applymap\" reference\r\npandas.io.formats.style.Styler.export: Missing description for See Also \"Styler.use\" reference\r\npandas.io.formats.style.Styler.use: Missing description for See Also \"Styler.export\" reference\r\npandas.DataFrame.shape: Missing description for See Also \"ndarray.shape\" reference\r\npandas.DataFrame.empty: Missing description for See Also \"Series.dropna\" reference\r\npandas.DataFrame.empty: Missing description for See Also \"DataFrame.dropna\" reference\r\npandas.DataFrame.pipe: Missing description for See Also \"DataFrame.apply\" reference\r\npandas.DataFrame.pipe: Missing description for See Also \"DataFrame.applymap\" reference\r\npandas.DataFrame.pipe: Missing description for See Also \"Series.map\" reference\r\npandas.DataFrame.corr: Missing description for See Also \"DataFrame.corrwith\" reference\r\npandas.DataFrame.corr: Missing description for See Also \"Series.corr\" reference\r\npandas.DataFrame.corrwith: Missing description for See Also \"DataFrame.corr\" reference\r\npandas.DataFrame.filter: Missing description for See Also \"DataFrame.loc\" reference\r\npandas.DataFrame.idxmax: Missing description for See Also \"Series.idxmax\" reference\r\npandas.DataFrame.idxmin: Missing description for See Also \"Series.idxmin\" reference\r\npandas.DataFrame.melt: Missing description for See Also \"melt\" reference\r\npandas.DataFrame.melt: Missing description for See Also \"pivot_table\" reference\r\npandas.DataFrame.melt: Missing description for See Also \"DataFrame.pivot\" reference\r\npandas.DataFrame.melt: Missing description for See Also \"Series.explode\" reference\r\npandas.DataFrame.asfreq: Missing description for See Also \"reindex\" reference\r\npandas.DataFrame.to_json: Missing description for See Also \"read_json\" reference\r\npandas.DataFrame.style: Missing description for See Also \"io.formats.style.Styler\" reference\r\npandas.Series.put: Missing description for See Also \"numpy.ndarray.put\" reference\r\npandas.Series.to_list: Missing description for See Also \"numpy.ndarray.tolist\" reference\r\npandas.Series.add: Missing description for See Also \"Series.radd\" reference\r\npandas.Series.sub: Missing description for See Also \"Series.rsub\" reference\r\npandas.Series.mul: Missing description for See Also \"Series.rmul\" reference\r\npandas.Series.div: Missing description for See Also \"Series.rtruediv\" reference\r\npandas.Series.truediv: Missing description for See Also \"Series.rtruediv\" reference\r\npandas.Series.floordiv: Missing description for See Also \"Series.rfloordiv\" reference\r\npandas.Series.mod: Missing description for See Also \"Series.rmod\" reference\r\npandas.Series.pow: Missing description for See Also \"Series.rpow\" reference\r\npandas.Series.radd: Missing description for See Also \"Series.add\" reference\r\npandas.Series.rsub: Missing description for See Also \"Series.sub\" reference\r\npandas.Series.rmul: Missing description for See Also \"Series.mul\" reference\r\npandas.Series.rdiv: Missing description for See Also \"Series.truediv\" reference\r\npandas.Series.rtruediv: Missing description for See Also \"Series.truediv\" reference\r\npandas.Series.rfloordiv: Missing description for See Also \"Series.floordiv\" reference\r\npandas.Series.rmod: Missing description for See Also \"Series.mod\" reference\r\npandas.Series.rpow: Missing description for See Also \"Series.pow\" reference\r\npandas.Series.lt: Missing description for See Also \"Series.None\" reference\r\npandas.Series.gt: Missing description for See Also \"Series.None\" reference\r\npandas.Series.le: Missing description for See Also \"Series.None\" reference\r\npandas.Series.ge: Missing description for See Also \"Series.None\" reference\r\npandas.Series.ne: Missing description for See Also \"Series.None\" reference\r\npandas.Series.eq: Missing description for See Also \"Series.None\" reference\r\npandas.Series.pipe: Missing description for See Also \"DataFrame.apply\" reference\r\npandas.Series.pipe: Missing description for See Also \"DataFrame.applymap\" reference\r\npandas.Series.pipe: Missing description for See Also \"Series.map\" reference\r\npandas.Series.quantile: Missing description for See Also \"core.window.Rolling.quantile\" reference\r\npandas.Series.quantile: Missing description for See Also \"numpy.percentile\" reference\r\npandas.Series.filter: Missing description for See Also \"DataFrame.loc\" reference\r\npandas.Series.argsort: Missing description for See Also \"numpy.ndarray.argsort\" reference\r\npandas.Series.searchsorted: Missing description for See Also \"numpy.searchsorted\" reference\r\npandas.Series.ravel: Missing description for See Also \"numpy.ndarray.ravel\" reference\r\npandas.Series.asfreq: Missing description for See Also \"reindex\" reference\r\npandas.Series.dt.to_pytimedelta: Missing description for See Also \"datetime.timedelta\" reference\r\npandas.Series.cat.categories: Missing description for See Also \"rename_categories\" reference\r\npandas.Series.cat.categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.Series.cat.categories: Missing description for See Also \"add_categories\" reference\r\npandas.Series.cat.categories: Missing description for See Also \"remove_categories\" reference\r\npandas.Series.cat.categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.Series.cat.categories: Missing description for See Also \"set_categories\" reference\r\npandas.Series.cat.rename_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.Series.cat.rename_categories: Missing description for See Also \"add_categories\" reference\r\npandas.Series.cat.rename_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.Series.cat.rename_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.Series.cat.rename_categories: Missing description for See Also \"set_categories\" reference\r\npandas.Series.cat.reorder_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.Series.cat.reorder_categories: Missing description for See Also \"add_categories\" reference\r\npandas.Series.cat.reorder_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.Series.cat.reorder_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.Series.cat.reorder_categories: Missing description for See Also \"set_categories\" reference\r\npandas.Series.cat.add_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.Series.cat.add_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.Series.cat.add_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.Series.cat.add_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.Series.cat.add_categories: Missing description for See Also \"set_categories\" reference\r\npandas.Series.cat.remove_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.Series.cat.remove_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.Series.cat.remove_categories: Missing description for See Also \"add_categories\" reference\r\npandas.Series.cat.remove_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.Series.cat.remove_categories: Missing description for See Also \"set_categories\" reference\r\npandas.Series.cat.remove_unused_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.Series.cat.remove_unused_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.Series.cat.remove_unused_categories: Missing description for See Also \"add_categories\" reference\r\npandas.Series.cat.remove_unused_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.Series.cat.remove_unused_categories: Missing description for See Also \"set_categories\" reference\r\npandas.Series.cat.set_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.Series.cat.set_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.Series.cat.set_categories: Missing description for See Also \"add_categories\" reference\r\npandas.Series.cat.set_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.Series.cat.set_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.Series.to_json: Missing description for See Also \"read_json\" reference\r\npandas.Index: Missing description for See Also \"DatetimeIndex\" reference\r\npandas.Index: Missing description for See Also \"TimedeltaIndex\" reference\r\npandas.Index: Missing description for See Also \"PeriodIndex\" reference\r\npandas.Index: Missing description for See Also \"Int64Index\" reference\r\npandas.Index: Missing description for See Also \"UInt64Index\" reference\r\npandas.Index: Missing description for See Also \"Float64Index\" reference\r\npandas.Index.memory_usage: Missing description for See Also \"numpy.ndarray.nbytes\" reference\r\npandas.Index.argmin: Missing description for See Also \"numpy.ndarray.argmin\" reference\r\npandas.Index.argmax: Missing description for See Also \"numpy.ndarray.argmax\" reference\r\npandas.Index.take: Missing description for See Also \"numpy.ndarray.take\" reference\r\npandas.Index.putmask: Missing description for See Also \"numpy.ndarray.putmask\" reference\r\npandas.Index.unique: Missing description for See Also \"unique\" reference\r\npandas.Index.unique: Missing description for See Also \"Series.unique\" reference\r\npandas.Index.ravel: Missing description for See Also \"numpy.ndarray.ravel\" reference\r\npandas.Index.to_list: Missing description for See Also \"numpy.ndarray.tolist\" reference\r\npandas.Index.searchsorted: Missing description for See Also \"numpy.searchsorted\" reference\r\npandas.CategoricalIndex.rename_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.CategoricalIndex.rename_categories: Missing description for See Also \"add_categories\" reference\r\npandas.CategoricalIndex.rename_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.CategoricalIndex.rename_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.CategoricalIndex.rename_categories: Missing description for See Also \"set_categories\" reference\r\npandas.CategoricalIndex.reorder_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.CategoricalIndex.reorder_categories: Missing description for See Also \"add_categories\" reference\r\npandas.CategoricalIndex.reorder_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.CategoricalIndex.reorder_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.CategoricalIndex.reorder_categories: Missing description for See Also \"set_categories\" reference\r\npandas.CategoricalIndex.add_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.CategoricalIndex.add_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.CategoricalIndex.add_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.CategoricalIndex.add_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.CategoricalIndex.add_categories: Missing description for See Also \"set_categories\" reference\r\npandas.CategoricalIndex.remove_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.CategoricalIndex.remove_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.CategoricalIndex.remove_categories: Missing description for See Also \"add_categories\" reference\r\npandas.CategoricalIndex.remove_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.CategoricalIndex.remove_categories: Missing description for See Also \"set_categories\" reference\r\npandas.CategoricalIndex.remove_unused_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.CategoricalIndex.remove_unused_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.CategoricalIndex.remove_unused_categories: Missing description for See Also \"add_categories\" reference\r\npandas.CategoricalIndex.remove_unused_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.CategoricalIndex.remove_unused_categories: Missing description for See Also \"set_categories\" reference\r\npandas.CategoricalIndex.set_categories: Missing description for See Also \"rename_categories\" reference\r\npandas.CategoricalIndex.set_categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.CategoricalIndex.set_categories: Missing description for See Also \"add_categories\" reference\r\npandas.CategoricalIndex.set_categories: Missing description for See Also \"remove_categories\" reference\r\npandas.CategoricalIndex.set_categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.MultiIndex.to_frame: Missing description for See Also \"DataFrame\" reference\r\npandas.DatetimeIndex.indexer_at_time: Missing description for See Also \"indexer_between_time\" reference\r\npandas.DatetimeIndex.indexer_at_time: Missing description for See Also \"DataFrame.at_time\" reference\r\npandas.DatetimeIndex.indexer_between_time: Missing description for See Also \"indexer_at_time\" reference\r\npandas.DatetimeIndex.indexer_between_time: Missing description for See Also \"DataFrame.between_time\" reference\r\npandas.DatetimeIndex.mean: Missing description for See Also \"numpy.ndarray.mean\" reference\r\npandas.TimedeltaIndex.mean: Missing description for See Also \"numpy.ndarray.mean\" reference\r\npandas.core.window.rolling.Rolling.aggregate: Missing description for See Also \"Series.rolling\" reference\r\npandas.core.window.rolling.Rolling.aggregate: Missing description for See Also \"DataFrame.rolling\" reference\r\npandas.core.window.expanding.Expanding.aggregate: Missing description for See Also \"DataFrame.expanding.aggregate\" reference\r\npandas.core.window.expanding.Expanding.aggregate: Missing description for See Also \"DataFrame.rolling.aggregate\" reference\r\npandas.core.window.expanding.Expanding.aggregate: Missing description for See Also \"DataFrame.aggregate\" reference\r\npandas.plotting.deregister_matplotlib_converters: Missing description for See Also \"register_matplotlib_converters\" reference\r\npandas.plotting.register_matplotlib_converters: Missing description for See Also \"deregister_matplotlib_converters\" reference\r\npandas.read_html: Missing description for See Also \"read_csv\" reference\r\npandas.read_sql_query: Missing description for See Also \"read_sql\" reference\r\npandas.tseries.frequencies.to_offset: Missing description for See Also \"DateOffset\" reference\r\npandas.core.groupby.GroupBy.all: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.all: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.any: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.any: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.bfill: Missing description for See Also \"Series.backfill\" reference\r\npandas.core.groupby.GroupBy.bfill: Missing description for See Also \"DataFrame.backfill\" reference\r\npandas.core.groupby.GroupBy.bfill: Missing description for See Also \"Series.fillna\" reference\r\npandas.core.groupby.GroupBy.bfill: Missing description for See Also \"DataFrame.fillna\" reference\r\npandas.core.groupby.GroupBy.count: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.count: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.cummax: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.cummax: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.cummin: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.cummin: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.cumprod: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.cumprod: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.cumsum: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.cumsum: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.ffill: Missing description for See Also \"Series.pad\" reference\r\npandas.core.groupby.GroupBy.ffill: Missing description for See Also \"DataFrame.pad\" reference\r\npandas.core.groupby.GroupBy.ffill: Missing description for See Also \"Series.fillna\" reference\r\npandas.core.groupby.GroupBy.ffill: Missing description for See Also \"DataFrame.fillna\" reference\r\npandas.core.groupby.GroupBy.head: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.head: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.mean: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.mean: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.median: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.median: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.nth: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.nth: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.ohlc: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.ohlc: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.rank: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.rank: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.pct_change: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.pct_change: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.size: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.size: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.sem: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.sem: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.std: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.std: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.var: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.var: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.GroupBy.tail: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.GroupBy.tail: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.all: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.all: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.any: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.any: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.bfill: Missing description for See Also \"Series.backfill\" reference\r\npandas.core.groupby.DataFrameGroupBy.bfill: Missing description for See Also \"DataFrame.backfill\" reference\r\npandas.core.groupby.DataFrameGroupBy.bfill: Missing description for See Also \"Series.fillna\" reference\r\npandas.core.groupby.DataFrameGroupBy.bfill: Missing description for See Also \"DataFrame.fillna\" reference\r\npandas.core.groupby.DataFrameGroupBy.corr: Missing description for See Also \"DataFrame.corrwith\" reference\r\npandas.core.groupby.DataFrameGroupBy.corr: Missing description for See Also \"Series.corr\" reference\r\npandas.core.groupby.DataFrameGroupBy.cummax: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.cummax: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.cummin: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.cummin: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.cumprod: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.cumprod: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.cumsum: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.cumsum: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.ffill: Missing description for See Also \"Series.pad\" reference\r\npandas.core.groupby.DataFrameGroupBy.ffill: Missing description for See Also \"DataFrame.pad\" reference\r\npandas.core.groupby.DataFrameGroupBy.ffill: Missing description for See Also \"Series.fillna\" reference\r\npandas.core.groupby.DataFrameGroupBy.ffill: Missing description for See Also \"DataFrame.fillna\" reference\r\npandas.core.groupby.DataFrameGroupBy.idxmax: Missing description for See Also \"Series.idxmax\" reference\r\npandas.core.groupby.DataFrameGroupBy.idxmin: Missing description for See Also \"Series.idxmin\" reference\r\npandas.core.groupby.DataFrameGroupBy.pct_change: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.pct_change: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.rank: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.rank: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.shift: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.shift: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.size: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.size: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.groupby.DataFrameGroupBy.corrwith: Missing description for See Also \"DataFrame.corr\" reference\r\npandas.core.resample.Resampler.__iter__: Missing description for See Also \"GroupBy.__iter__\" reference\r\npandas.core.resample.Resampler.apply: Missing description for See Also \"DataFrame.groupby.aggregate\" reference\r\npandas.core.resample.Resampler.apply: Missing description for See Also \"DataFrame.resample.transform\" reference\r\npandas.core.resample.Resampler.apply: Missing description for See Also \"DataFrame.aggregate\" reference\r\npandas.core.resample.Resampler.aggregate: Missing description for See Also \"DataFrame.groupby.aggregate\" reference\r\npandas.core.resample.Resampler.aggregate: Missing description for See Also \"DataFrame.resample.transform\" reference\r\npandas.core.resample.Resampler.aggregate: Missing description for See Also \"DataFrame.aggregate\" reference\r\npandas.core.resample.Resampler.ffill: Missing description for See Also \"Series.fillna\" reference\r\npandas.core.resample.Resampler.ffill: Missing description for See Also \"DataFrame.fillna\" reference\r\npandas.core.resample.Resampler.pad: Missing description for See Also \"Series.fillna\" reference\r\npandas.core.resample.Resampler.pad: Missing description for See Also \"DataFrame.fillna\" reference\r\npandas.core.resample.Resampler.asfreq: Missing description for See Also \"Series.asfreq\" reference\r\npandas.core.resample.Resampler.asfreq: Missing description for See Also \"DataFrame.asfreq\" reference\r\npandas.core.resample.Resampler.count: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.resample.Resampler.count: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.resample.Resampler.mean: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.resample.Resampler.mean: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.resample.Resampler.median: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.resample.Resampler.median: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.resample.Resampler.ohlc: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.resample.Resampler.ohlc: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.resample.Resampler.size: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.resample.Resampler.size: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.resample.Resampler.sem: Missing description for See Also \"Series.groupby\" reference\r\npandas.core.resample.Resampler.sem: Missing description for See Also \"DataFrame.groupby\" reference\r\npandas.core.resample.Resampler.quantile: Missing description for See Also \"Series.quantile\" reference\r\npandas.core.resample.Resampler.quantile: Missing description for See Also \"DataFrame.quantile\" reference\r\npandas.core.resample.Resampler.quantile: Missing description for See Also \"DataFrameGroupBy.quantile\" reference\r\npandas.Timedelta.isoformat: Missing description for See Also \"Timestamp.isoformat\" reference\r\npandas.CategoricalDtype: Missing description for See Also \"Categorical\" reference\r\npandas.Categorical.categories: Missing description for See Also \"rename_categories\" reference\r\npandas.Categorical.categories: Missing description for See Also \"reorder_categories\" reference\r\npandas.Categorical.categories: Missing description for See Also \"add_categories\" reference\r\npandas.Categorical.categories: Missing description for See Also \"remove_categories\" reference\r\npandas.Categorical.categories: Missing description for See Also \"remove_unused_categories\" reference\r\npandas.Categorical.categories: Missing description for See Also \"set_categories\" reference\r\npandas.Series.as_matrix: Missing description for See Also \"DataFrame.values\" reference\r\npandas.Series.compress: Missing description for See Also \"numpy.ndarray.compress\" reference\r\npandas.Series.divide: Missing description for See Also \"Series.rtruediv\" reference\r\npandas.Series.divmod: Missing description for See Also \"Series.rdivmod\" reference\r\npandas.Series.multiply: Missing description for See Also \"Series.rmul\" reference\r\npandas.Series.nonzero: Missing description for See Also \"numpy.nonzero\" reference\r\npandas.Series.rdivmod: Missing description for See Also \"Series.divmod\" reference\r\npandas.Series.subtract: Missing description for See Also \"Series.rsub\" reference\r\npandas.Series.tolist: Missing description for See Also \"numpy.ndarray.tolist\" reference\r\npandas.DataFrame.as_matrix: Missing description for See Also \"DataFrame.values\" reference\r\n```\n\nHi, anyone working on this?\n\nNot right now. Just posted yesterday, so go for it!\n\n@Nabel0721 what fix can you recommend?\n\nIf you find any in shared docs, that’ll fix the issue in multiple places. (For instance, I think Series.pipe and DataFrame.pipe come from the same shared_doc value). Other than that, just go for whatever you’d like! These don’t need to be done all at once. \n\nHi @Nabel0721, may I work on this issue. I was able to locate some of these issues in the code documentation. I'm a newbie to contributing to open source, I'll try my best.\n\nFor sure. I opened the issue, but that doesn't mean I'm in charge of it. If you want to work on something go ahead!\n\nI'd like to help with this if help is still needed. I tried running the validate script but got an error:\r\n\r\n```\r\n$ ./scripts/validate_docstrings.py --errors=SA04\r\n\r\n  File \"./scripts/validate_docstrings.py\", line 62\r\n    DIRECTIVE_PATTERN = re.compile(rf\"^\\s*\\.\\. ({'|'.join(DIRECTIVES)})(?!::)\", re.I | re.M)\r\n                                                                             ^\r\n\r\nSyntaxError: invalid syntax\r\n```\r\n\n\n@textractor Try running it with `python3 /scripts/validate_docstrings.py` instead. The shebang at the top is `usr/bin/python`, which works great if you only have python3 installed on your computer, but breaks it a little if you have 2 and 3.\n\nHi, I'm working fixing these errors on `core/series.py`. I have a doubt regarding `core.window.Rolling.quantile`.\r\n\r\nIf understand correctly this should be the change to: `core.window.Rolling.quantile : Calculate the %(name)s quantile.`\r\n\r\nAppreciate your feedback. Thanks.\n\nis there still work to be done on this issue ?   I'm new and would like to help if there is.\n\n> is there still work to be done on this issue ? I'm new and would like to help if there is.\r\n\r\n`./scripts/validate_docstrings.py --errors=SA04` should give you a list of what's still outstanding\n\nThank you.  This is my first contribution so I'm working through it piece by piece.  I just pushed my very first commit on PR06 documentation errors and it failed in the \"web and docs\" build.  Do you happen to know why?   I'm assuming that's an issue I need to resolve ?  Thanks...\n\nIs this ticket still open and need some help?\r\n\n\nsee https://github.com/pandas-dev/pandas/issues/28792#issuecomment-637605353\n\ntake\n\n@simonjayhawkins Created https://github.com/pandas-dev/pandas/pull/37219 partial fix please review.\n\n@simonjayhawkins Completed this task.  \r\nExcept this. Not sure from where is this.\r\nNone:None:SA04:pandas.Timedelta.isoformat:Missing description for See Also \"Timestamp.isoformat\" reference\n\n@suparnasnair Can you please tell me that in \"pandas.DataFrame.empty\" and \"pandas.Series.subtract\" what are DataFrame, Series are? Are these the folder name or Python file name? Because I am not able to see any folder with such name.\n\nClosing as a duplicate of https://github.com/pandas-dev/pandas/issues/28792",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/37219",
  "code_context": [
    {
      "filename": "pandas/core/arrays/base.py",
      "content": "\"\"\"\nAn interface for extending pandas with custom arrays.\n\n.. warning::\n\n   This is an experimental API and subject to breaking changes\n   without warning.\n\"\"\"\nimport operator\nfrom typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union, cast\n\nimport numpy as np\n\nfrom pandas._libs import lib\nfrom pandas._typing import ArrayLike\nfrom pandas.compat import set_function_name\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import Appender, Substitution\nfrom pandas.util._validators import validate_fillna_kwargs\n\nfrom pandas.core.dtypes.cast import maybe_cast_to_extension_array\nfrom pandas.core.dtypes.common import (\n    is_array_like,\n    is_dtype_equal,\n    is_list_like,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import ExtensionDtype\nfrom pandas.core.dtypes.generic import ABCDataFrame, ABCIndexClass, ABCSeries\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core import ops\nfrom pandas.core.algorithms import factorize_array, unique\nfrom pandas.core.missing import get_fill_func\nfrom pandas.core.sorting import nargminmax, nargsort\n\n_extension_array_shared_docs: Dict[str, str] = dict()\n\n\nclass ExtensionArray:\n    \"\"\"\n    Abstract base class for custom 1-D array types.\n\n    pandas will recognize instances of this class as proper arrays\n    with a custom type and will not attempt to coerce them to objects. They\n    may be stored directly inside a :class:`DataFrame` or :class:`Series`.\n\n    Attributes\n    ----------\n    dtype\n    nbytes\n    ndim\n    shape\n\n    Methods\n    -------\n    argsort\n    astype\n    copy\n    dropna\n    factorize\n    fillna\n    equals\n    isna\n    ravel\n    repeat\n    searchsorted\n    shift\n    take\n    unique\n    view\n    _concat_same_type\n    _formatter\n    _from_factorized\n    _from_sequence\n    _from_sequence_of_strings\n    _reduce\n    _values_for_argsort\n    _values_for_factorize\n\n    Notes\n    -----\n    The interface includes the following abstract methods that must be\n    implemented by subclasses:\n\n    * _from_sequence\n    * _from_factorized\n    * __getitem__\n    * __len__\n    * __eq__\n    * dtype\n    * nbytes\n    * isna\n    * take\n    * copy\n    * _concat_same_type\n\n    A default repr displaying the type, (truncated) data, length,\n    and dtype is provided. It can be customized or replaced by\n    by overriding:\n\n    * __repr__ : A default repr for the ExtensionArray.\n    * _formatter : Print scalars inside a Series or DataFrame.\n\n    Some methods require casting the ExtensionArray to an ndarray of Python\n    objects with ``self.astype(object)``, which may be expensive. When\n    performance is a concern, we highly recommend overriding the following\n    methods:\n\n    * fillna\n    * dropna\n    * unique\n    * factorize / _values_for_factorize\n    * argsort / _values_for_argsort\n    * searchsorted\n\n    The remaining methods implemented on this class should be performant,\n    as they only compose abstract methods. Still, a more efficient\n    implementation may be available, and these methods can be overridden.\n\n    One can implement methods to handle array reductions.\n\n    * _reduce\n\n    One can implement methods to handle parsing from strings that will be used\n    in methods such as ``pandas.io.parsers.read_csv``.\n\n    * _from_sequence_of_strings\n\n    This class does not inherit from 'abc.ABCMeta' for performance reasons.\n    Methods and properties required by the interface raise\n    ``pandas.errors.AbstractMethodError`` and no ``register`` method is\n    provided for registering virtual subclasses.\n\n    ExtensionArrays are limited to 1 dimension.\n\n    They may be backed by none, one, or many NumPy arrays. For example,\n    ``pandas.Categorical`` is an extension array backed by two arrays,\n    one for codes and one for categories. An array of IPv6 address may\n    be backed by a NumPy structured array with two fields, one for the\n    lower 64 bits and one for the upper 64 bits. Or they may be backed\n    by some other storage type, like Python lists. Pandas makes no\n    assumptions on how the data are stored, just that it can be converted\n    to a NumPy array.\n    The ExtensionArray interface does not impose any rules on how this data\n    is stored. However, currently, the backing data cannot be stored in\n    attributes called ``.values`` or ``._values`` to ensure full compatibility\n    with pandas internals. But other names as ``.data``, ``._data``,\n    ``._items``, ... can be freely used.\n\n    If implementing NumPy's ``__array_ufunc__`` interface, pandas expects\n    that\n\n    1. You defer by returning ``NotImplemented`` when any Series are present\n       in `inputs`. Pandas will extract the arrays and call the ufunc again.\n    2. You define a ``_HANDLED_TYPES`` tuple as an attribute on the class.\n       Pandas inspect this to determine whether the ufunc is valid for the\n       types present.\n\n    See :ref:`extending.extension.ufunc` for more.\n\n    By default, ExtensionArrays are not hashable.  Immutable subclasses may\n    override this behavior.\n    \"\"\"\n\n    # '_typ' is for pandas.core.dtypes.generic.ABCExtensionArray.\n    # Don't override this.\n    _typ = \"extension\"\n\n    # ------------------------------------------------------------------------\n    # Constructors\n    # ------------------------------------------------------------------------\n\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy=False):\n        \"\"\"\n        Construct a new ExtensionArray from a sequence of scalars.\n\n        Parameters\n        ----------\n        scalars : Sequence\n            Each element will be an instance of the scalar type for this\n            array, ``cls.dtype.type`` or be converted into this type in this method.\n        dtype : dtype, optional\n            Construct for this particular dtype. This should be a Dtype\n            compatible with the ExtensionArray.\n        copy : bool, default False\n            If True, copy the underlying data.\n\n        Returns\n        -------\n        ExtensionArray\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    @classmethod\n    def _from_sequence_of_strings(cls, strings, dtype=None, copy=False):\n        \"\"\"\n        Construct a new ExtensionArray from a sequence of strings.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        strings : Sequence\n            Each element will be an instance of the scalar type for this\n            array, ``cls.dtype.type``.\n        dtype : dtype, optional\n            Construct for this particular dtype. This should be a Dtype\n            compatible with the ExtensionArray.\n        copy : bool, default False\n            If True, copy the underlying data.\n\n        Returns\n        -------\n        ExtensionArray\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        \"\"\"\n        Reconstruct an ExtensionArray after factorization.\n\n        Parameters\n        ----------\n        values : ndarray\n            An integer ndarray with the factorized values.\n        original : ExtensionArray\n            The original ExtensionArray that factorize was called on.\n\n        See Also\n        --------\n        factorize : Top-level factorize method that dispatches here.\n        ExtensionArray.factorize : Encode the extension array as an enumerated type.\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    # ------------------------------------------------------------------------\n    # Must be a Sequence\n    # ------------------------------------------------------------------------\n\n    def __getitem__(self, item):\n        # type (Any) -> Any\n        \"\"\"\n        Select a subset of self.\n\n        Parameters\n        ----------\n        item : int, slice, or ndarray\n            * int: The position in 'self' to get.\n\n            * slice: A slice object, where 'start', 'stop', and 'step' are\n              integers or None\n\n            * ndarray: A 1-d boolean NumPy ndarray the same length as 'self'\n\n        Returns\n        -------\n        item : scalar or ExtensionArray\n\n        Notes\n        -----\n        For scalar ``item``, return a scalar value suitable for the array's\n        type. This should be an instance of ``self.dtype.type``.\n\n        For slice ``key``, return an instance of ``ExtensionArray``, even\n        if the slice is length 0 or 1.\n\n        For a boolean mask, return an instance of ``ExtensionArray``, filtered\n        to the values where ``item`` is True.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def __setitem__(self, key: Union[int, np.ndarray], value: Any) -> None:\n        \"\"\"\n        Set one or more values inplace.\n\n        This method is not required to satisfy the pandas extension array\n        interface.\n\n        Parameters\n        ----------\n        key : int, ndarray, or slice\n            When called from, e.g. ``Series.__setitem__``, ``key`` will be\n            one of\n\n            * scalar int\n            * ndarray of integers.\n            * boolean ndarray\n            * slice object\n\n        value : ExtensionDtype.type, Sequence[ExtensionDtype.type], or object\n            value or values to be set of ``key``.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        # Some notes to the ExtensionArray implementor who may have ended up\n        # here. While this method is not required for the interface, if you\n        # *do* choose to implement __setitem__, then some semantics should be\n        # observed:\n        #\n        # * Setting multiple values : ExtensionArrays should support setting\n        #   multiple values at once, 'key' will be a sequence of integers and\n        #  'value' will be a same-length sequence.\n        #\n        # * Broadcasting : For a sequence 'key' and a scalar 'value',\n        #   each position in 'key' should be set to 'value'.\n        #\n        # * Coercion : Most users will expect basic coercion to work. For\n        #   example, a string like '2018-01-01' is coerced to a datetime\n        #   when setting on a datetime64ns array. In general, if the\n        #   __init__ method coerces that value, then so should __setitem__\n        # Note, also, that Series/DataFrame.where internally use __setitem__\n        # on a copy of the data.\n        raise NotImplementedError(f\"{type(self)} does not implement __setitem__.\")\n\n    def __len__(self) -> int:\n        \"\"\"\n        Length of this array\n\n        Returns\n        -------\n        length : int\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over elements of the array.\n        \"\"\"\n        # This needs to be implemented so that pandas recognizes extension\n        # arrays as list-like. The default implementation makes successive\n        # calls to ``__getitem__``, which may be slower than necessary.\n        for i in range(len(self)):\n            yield self[i]\n\n    def __eq__(self, other: Any) -> ArrayLike:\n        \"\"\"\n        Return for `self == other` (element-wise equality).\n        \"\"\"\n        # Implementer note: this should return a boolean numpy ndarray or\n        # a boolean ExtensionArray.\n        # When `other` is one of Series, Index, or DataFrame, this method should\n        # return NotImplemented (to ensure that those objects are responsible for\n        # first unpacking the arrays, and then dispatch the operation to the\n        # underlying arrays)\n        raise AbstractMethodError(self)\n\n    def __ne__(self, other: Any) -> ArrayLike:\n        \"\"\"\n        Return for `self != other` (element-wise in-equality).\n        \"\"\"\n        return ~(self == other)\n\n    def to_numpy(\n        self, dtype=None, copy: bool = False, na_value=lib.no_default\n    ) -> np.ndarray:\n        \"\"\"\n        Convert to a NumPy ndarray.\n\n        .. versionadded:: 1.0.0\n\n        This is similar to :meth:`numpy.asarray`, but may provide additional control\n        over how the conversion is done.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to pass to :meth:`numpy.asarray`.\n        copy : bool, default False\n            Whether to ensure that the returned value is a not a view on\n            another array. Note that ``copy=False`` does not *ensure* that\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n            a copy is made, even if not strictly necessary.\n        na_value : Any, optional\n            The value to use for missing values. The default value depends\n            on `dtype` and the type of the array.\n\n        Returns\n        -------\n        numpy.ndarray\n        \"\"\"\n        result = np.asarray(self, dtype=dtype)\n        if copy or na_value is not lib.no_default:\n            result = result.copy()\n        if na_value is not lib.no_default:\n            result[self.isna()] = na_value\n        return result\n\n    # ------------------------------------------------------------------------\n    # Required attributes\n    # ------------------------------------------------------------------------\n\n    @property\n    def dtype(self) -> ExtensionDtype:\n        \"\"\"\n        An instance of 'ExtensionDtype'.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def shape(self) -> Tuple[int, ...]:\n        \"\"\"\n        Return a tuple of the array dimensions.\n        \"\"\"\n        return (len(self),)\n\n    @property\n    def size(self) -> int:\n        \"\"\"\n        The number of elements in the array.\n        \"\"\"\n        return np.prod(self.shape)\n\n    @property\n    def ndim(self) -> int:\n        \"\"\"\n        Extension Arrays are only allowed to be 1-dimensional.\n        \"\"\"\n        return 1\n\n    @property\n    def nbytes(self) -> int:\n        \"\"\"\n        The number of bytes needed to store this object in memory.\n        \"\"\"\n        # If this is expensive to compute, return an approximate lower bound\n        # on the number of bytes needed.\n        raise AbstractMethodError(self)\n\n    # ------------------------------------------------------------------------\n    # Additional Methods\n    # ------------------------------------------------------------------------\n\n    def astype(self, dtype, copy=True):\n        \"\"\"\n        Cast to a NumPy array with 'dtype'.\n\n        Parameters\n        ----------\n        dtype : str or dtype\n            Typecode or data-type to which the array is cast.\n        copy : bool, default True\n            Whether to copy the data, even if not necessary. If False,\n            a copy is made only if the old dtype does not match the\n            new dtype.\n\n        Returns\n        -------\n        array : ndarray\n            NumPy ndarray with 'dtype' for its dtype.\n        \"\"\"\n        from pandas.core.arrays.string_ import StringDtype\n\n        dtype = pandas_dtype(dtype)\n        if is_dtype_equal(dtype, self.dtype):\n            if not copy:\n                return self\n            elif copy:\n                return self.copy()\n        if isinstance(dtype, StringDtype):  # allow conversion to StringArrays\n            return dtype.construct_array_type()._from_sequence(self, copy=False)\n\n        return np.array(self, dtype=dtype, copy=copy)\n\n    def isna(self) -> ArrayLike:\n        \"\"\"\n        A 1-D array indicating if each value is missing.\n\n        Returns\n        -------\n        na_values : Union[np.ndarray, ExtensionArray]\n            In most cases, this should return a NumPy ndarray. For\n            exceptional cases like ``SparseArray``, where returning\n            an ndarray would be expensive, an ExtensionArray may be\n            returned.\n\n        Notes\n        -----\n        If returning an ExtensionArray, then\n\n        * ``na_values._is_boolean`` should be True\n        * `na_values` should implement :func:`ExtensionArray._reduce`\n        * ``na_values.any`` and ``na_values.all`` should be implemented\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _values_for_argsort(self) -> np.ndarray:\n        \"\"\"\n        Return values for sorting.\n\n        Returns\n        -------\n        ndarray\n            The transformed values should maintain the ordering between values\n            within the array.\n\n        See Also\n        --------\n        ExtensionArray.argsort : Return the indices that would sort this array.\n        \"\"\"\n        # Note: this is used in `ExtensionArray.argsort`.\n        return np.array(self)\n\n    def argsort(\n        self,\n        ascending: bool = True,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        *args,\n        **kwargs,\n    ) -> np.ndarray:\n        \"\"\"\n        Return the indices that would sort this array.\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            Whether the indices should result in an ascending\n            or descending sort.\n        kind : {'quicksort', 'mergesort', 'heapsort'}, optional\n            Sorting algorithm.\n        *args, **kwargs:\n            Passed through to :func:`numpy.argsort`.\n\n        Returns\n        -------\n        ndarray\n            Array of indices that sort ``self``. If NaN values are contained,\n            NaN values are placed at the end.\n\n        See Also\n        --------\n        numpy.argsort : Sorting implementation used internally.\n        \"\"\"\n        # Implementor note: You have two places to override the behavior of\n        # argsort.\n        # 1. _values_for_argsort : construct the values passed to np.argsort\n        # 2. argsort : total control over sorting.\n        ascending = nv.validate_argsort_with_ascending(ascending, args, kwargs)\n\n        values = self._values_for_argsort()\n        result = nargsort(\n            values,\n            kind=kind,\n            ascending=ascending,\n            na_position=na_position,\n            mask=np.asarray(self.isna()),\n        )\n        return result\n\n    def argmin(self):\n        \"\"\"\n        Return the index of minimum value.\n\n        In case of multiple occurrences of the minimum value, the index\n        corresponding to the first occurrence is returned.\n\n        Returns\n        -------\n        int\n\n        See Also\n        --------\n        ExtensionArray.argmax\n        \"\"\"\n        return nargminmax(self, \"argmin\")\n\n    def argmax(self):\n        \"\"\"\n        Return the index of maximum value.\n\n        In case of multiple occurrences of the maximum value, the index\n        corresponding to the first occurrence is returned.\n\n        Returns\n        -------\n        int\n\n        See Also\n        --------\n        ExtensionArray.argmin\n        \"\"\"\n        return nargminmax(self, \"argmax\")\n\n    def fillna(self, value=None, method=None, limit=None):\n        \"\"\"\n        Fill NA/NaN values using the specified method.\n\n        Parameters\n        ----------\n        value : scalar, array-like\n            If a scalar value is passed it is used to fill all missing values.\n            Alternatively, an array-like 'value' can be given. It's expected\n            that the array-like have the same length as 'self'.\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n            Method to use for filling holes in reindexed Series\n            pad / ffill: propagate last valid observation forward to next valid\n            backfill / bfill: use NEXT valid observation to fill gap.\n        limit : int, default None\n            If method is specified, this is the maximum number of consecutive\n            NaN values to forward/backward fill. In other words, if there is\n            a gap with more than this number of consecutive NaNs, it will only\n            be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled.\n\n        Returns\n        -------\n        ExtensionArray\n            With NA/NaN filled.\n        \"\"\"\n        value, method = validate_fillna_kwargs(value, method)\n\n        mask = self.isna()\n\n        if is_array_like(value):\n            if len(value) != len(self):\n                raise ValueError(\n                    f\"Length of 'value' does not match. Got ({len(value)}) \"\n                    f\"expected {len(self)}\"\n                )\n            value = value[mask]\n\n        if mask.any():\n            if method is not None:\n                func = get_fill_func(method)\n                new_values = func(self.astype(object), limit=limit, mask=mask)\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\n            else:\n                # fill with value\n                new_values = self.copy()\n                new_values[mask] = value\n        else:\n            new_values = self.copy()\n        return new_values\n\n    def dropna(self):\n        \"\"\"\n        Return ExtensionArray without NA values.\n\n        Returns\n        -------\n        valid : ExtensionArray\n        \"\"\"\n        return self[~self.isna()]\n\n    def shift(self, periods: int = 1, fill_value: object = None) -> \"ExtensionArray\":\n        \"\"\"\n        Shift values by desired number.\n\n        Newly introduced missing values are filled with\n        ``self.dtype.na_value``.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        periods : int, default 1\n            The number of periods to shift. Negative values are allowed\n            for shifting backwards.\n\n        fill_value : object, optional\n            The scalar value to use for newly introduced missing values.\n            The default is ``self.dtype.na_value``.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        ExtensionArray\n            Shifted.\n\n        Notes\n        -----\n        If ``self`` is empty or ``periods`` is 0, a copy of ``self`` is\n        returned.\n\n        If ``periods > len(self)``, then an array of size\n        len(self) is returned, with all values filled with\n        ``self.dtype.na_value``.\n        \"\"\"\n        # Note: this implementation assumes that `self.dtype.na_value` can be\n        # stored in an instance of your ExtensionArray with `self.dtype`.\n        if not len(self) or periods == 0:\n            return self.copy()\n\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n\n        empty = self._from_sequence(\n            [fill_value] * min(abs(periods), len(self)), dtype=self.dtype\n        )\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods) :]\n            b = empty\n        return self._concat_same_type([a, b])\n\n    def unique(self):\n        \"\"\"\n        Compute the ExtensionArray of unique values.\n\n        Returns\n        -------\n        uniques : ExtensionArray\n        \"\"\"\n        uniques = unique(self.astype(object))\n        return self._from_sequence(uniques, dtype=self.dtype)\n\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        \"\"\"\n        Find indices where elements should be inserted to maintain order.\n\n        .. versionadded:: 0.24.0\n\n        Find the indices into a sorted array `self` (a) such that, if the\n        corresponding elements in `value` were inserted before the indices,\n        the order of `self` would be preserved.\n\n        Assuming that `self` is sorted:\n\n        ======  ================================\n        `side`  returned index `i` satisfies\n        ======  ================================\n        left    ``self[i-1] < value <= self[i]``\n        right   ``self[i-1] <= value < self[i]``\n        ======  ================================\n\n        Parameters\n        ----------\n        value : array_like\n            Values to insert into `self`.\n        side : {'left', 'right'}, optional\n            If 'left', the index of the first suitable location found is given.\n            If 'right', return the last such index.  If there is no suitable\n            index, return either 0 or N (where N is the length of `self`).\n        sorter : 1-D array_like, optional\n            Optional array of integer indices that sort array a into ascending\n            order. They are typically the result of argsort.\n\n        Returns\n        -------\n        array of ints\n            Array of insertion points with the same shape as `value`.\n\n        See Also\n        --------\n        numpy.searchsorted : Similar method from NumPy.\n        \"\"\"\n        # Note: the base tests provided by pandas only test the basics.\n        # We do not test\n        # 1. Values outside the range of the `data_for_sorting` fixture\n        # 2. Values between the values in the `data_for_sorting` fixture\n        # 3. Missing values.\n        arr = self.astype(object)\n        return arr.searchsorted(value, side=side, sorter=sorter)\n\n    def equals(self, other: object) -> bool:\n        \"\"\"\n        Return if another array is equivalent to this array.\n\n        Equivalent means that both arrays have the same shape and dtype, and\n        all values compare equal. Missing values in the same location are\n        considered equal (in contrast with normal equality).\n\n        Parameters\n        ----------\n        other : ExtensionArray\n            Array to compare to this Array.\n\n        Returns\n        -------\n        boolean\n            Whether the arrays are equivalent.\n        \"\"\"\n        if not type(self) == type(other):\n            return False\n        other = cast(ExtensionArray, other)\n        if not is_dtype_equal(self.dtype, other.dtype):\n            return False\n        elif not len(self) == len(other):\n            return False\n        else:\n            equal_values = self == other\n            if isinstance(equal_values, ExtensionArray):\n                # boolean array with NA -> fill with False\n                equal_values = equal_values.fillna(False)\n            equal_na = self.isna() & other.isna()\n            return bool((equal_values | equal_na).all())\n\n    def _values_for_factorize(self) -> Tuple[np.ndarray, Any]:\n        \"\"\"\n        Return an array and missing value suitable for factorization.\n\n        Returns\n        -------\n        values : ndarray\n\n            An array suitable for factorization. This should maintain order\n            and be a supported dtype (Float64, Int64, UInt64, String, Object).\n            By default, the extension array is cast to object dtype.\n        na_value : object\n            The value in `values` to consider missing. This will be treated\n            as NA in the factorization routines, so it will be coded as\n            `na_sentinel` and not included in `uniques`. By default,\n            ``np.nan`` is used.\n\n        Notes\n        -----\n        The values returned by this method are also used in\n        :func:`pandas.util.hash_pandas_object`.\n        \"\"\"\n        return self.astype(object), np.nan\n\n    def factorize(self, na_sentinel: int = -1) -> Tuple[np.ndarray, \"ExtensionArray\"]:\n        \"\"\"\n        Encode the extension array as an enumerated type.\n\n        Parameters\n        ----------\n        na_sentinel : int, default -1\n            Value to use in the `codes` array to indicate missing values.\n\n        Returns\n        -------\n        codes : ndarray\n            An integer NumPy array that's an indexer into the original\n            ExtensionArray.\n        uniques : ExtensionArray\n            An ExtensionArray containing the unique values of `self`.\n\n            .. note::\n\n               uniques will *not* contain an entry for the NA value of\n               the ExtensionArray if there are any missing values present\n               in `self`.\n\n        See Also\n        --------\n        factorize : Top-level factorize method that dispatches here.\n\n        Notes\n        -----\n        :meth:`pandas.factorize` offers a `sort` keyword as well.\n        \"\"\"\n        # Implementer note: There are two ways to override the behavior of\n        # pandas.factorize\n        # 1. _values_for_factorize and _from_factorize.\n        #    Specify the values passed to pandas' internal factorization\n        #    routines, and how to convert from those values back to the\n        #    original ExtensionArray.\n        # 2. ExtensionArray.factorize.\n        #    Complete control over factorization.\n        arr, na_value = self._values_for_factorize()\n\n        codes, uniques = factorize_array(\n            arr, na_sentinel=na_sentinel, na_value=na_value\n        )\n\n        uniques = self._from_factorized(uniques, self)\n        return codes, uniques\n\n    _extension_array_shared_docs[\n        \"repeat\"\n    ] = \"\"\"\n        Repeat elements of a %(klass)s.\n\n        Returns a new %(klass)s where each element of the current %(klass)s\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            %(klass)s.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        repeated_array : %(klass)s\n            Newly created %(klass)s with repeated elements.\n\n        See Also\n        --------\n        Series.repeat : Equivalent function for Series.\n        Index.repeat : Equivalent function for Index.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n        ExtensionArray.take : Take arbitrary positions.\n\n        Examples\n        --------\n        >>> cat = pd.Categorical(['a', 'b', 'c'])\n        >>> cat\n        ['a', 'b', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> cat.repeat(2)\n        ['a', 'a', 'b', 'b', 'c', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> cat.repeat([1, 2, 3])\n        ['a', 'b', 'b', 'c', 'c', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        \"\"\"\n\n    @Substitution(klass=\"ExtensionArray\")\n    @Appender(_extension_array_shared_docs[\"repeat\"])\n    def repeat(self, repeats, axis=None):\n        nv.validate_repeat(tuple(), dict(axis=axis))\n        ind = np.arange(len(self)).repeat(repeats)\n        return self.take(ind)\n\n    # ------------------------------------------------------------------------\n    # Indexing methods\n    # ------------------------------------------------------------------------\n\n    def take(\n        self, indices: Sequence[int], allow_fill: bool = False, fill_value: Any = None\n    ) -> \"ExtensionArray\":\n        \"\"\"\n        Take elements from an array.\n\n        Parameters\n        ----------\n        indices : sequence of int\n            Indices to be taken.\n        allow_fill : bool, default False\n            How to handle negative values in `indices`.\n\n            * False: negative values in `indices` indicate positional indices\n              from the right (the default). This is similar to\n              :func:`numpy.take`.\n\n            * True: negative values in `indices` indicate\n              missing values. These values are set to `fill_value`. Any other\n              other negative values raise a ``ValueError``.\n\n        fill_value : any, optional\n            Fill value to use for NA-indices when `allow_fill` is True.\n            This may be ``None``, in which case the default NA value for\n            the type, ``self.dtype.na_value``, is used.\n\n            For many ExtensionArrays, there will be two representations of\n            `fill_value`: a user-facing \"boxed\" scalar, and a low-level\n            physical NA value. `fill_value` should be the user-facing version,\n            and the implementation should handle translating that to the\n            physical version for processing the take if necessary.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Raises\n        ------\n        IndexError\n            When the indices are out of bounds for the array.\n        ValueError\n            When `indices` contains negative values other than ``-1``\n            and `allow_fill` is True.\n\n        See Also\n        --------\n        numpy.take : Take elements from an array along an axis.\n        api.extensions.take : Take elements from an array.\n\n        Notes\n        -----\n        ExtensionArray.take is called by ``Series.__getitem__``, ``.loc``,\n        ``iloc``, when `indices` is a sequence of values. Additionally,\n        it's called by :meth:`Series.reindex`, or any other method\n        that causes realignment, with a `fill_value`.\n\n        Examples\n        --------\n        Here's an example implementation, which relies on casting the\n        extension array to object dtype. This uses the helper method\n        :func:`pandas.api.extensions.take`.\n\n        .. code-block:: python\n\n           def take(self, indices, allow_fill=False, fill_value=None):\n               from pandas.core.algorithms import take\n\n               # If the ExtensionArray is backed by an ndarray, then\n               # just pass that here instead of coercing to object.\n               data = self.astype(object)\n\n               if allow_fill and fill_value is None:\n                   fill_value = self.dtype.na_value\n\n               # fill value should always be translated from the scalar\n               # type for the array, to the physical storage type for\n               # the data, before passing to take.\n\n               result = take(data, indices, fill_value=fill_value,\n                             allow_fill=allow_fill)\n               return self._from_sequence(result, dtype=self.dtype)\n        \"\"\"\n        # Implementer note: The `fill_value` parameter should be a user-facing\n        # value, an instance of self.dtype.type. When passed `fill_value=None`,\n        # the default of `self.dtype.na_value` should be used.\n        # This may differ from the physical storage type your ExtensionArray\n        # uses. In this case, your implementation is responsible for casting\n        # the user-facing type to the storage type, before using\n        # pandas.api.extensions.take\n        raise AbstractMethodError(self)\n\n    def copy(self) -> \"ExtensionArray\":\n        \"\"\"\n        Return a copy of the array.\n\n        Returns\n        -------\n        ExtensionArray\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def view(self, dtype=None) -> ArrayLike:\n        \"\"\"\n        Return a view on the array.\n\n        Parameters\n        ----------\n        dtype : str, np.dtype, or ExtensionDtype, optional\n            Default None.\n\n        Returns\n        -------\n        ExtensionArray or np.ndarray\n            A view on the :class:`ExtensionArray`'s data.\n        \"\"\"\n        # NB:\n        # - This must return a *new* object referencing the same data, not self.\n        # - The only case that *must* be implemented is with dtype=None,\n        #   giving a view with the same dtype as self.\n        if dtype is not None:\n            raise NotImplementedError(dtype)\n        return self[:]\n\n    # ------------------------------------------------------------------------\n    # Printing\n    # ------------------------------------------------------------------------\n\n    def __repr__(self) -> str:\n        from pandas.io.formats.printing import format_object_summary\n\n        # the short repr has no trailing newline, while the truncated\n        # repr does. So we include a newline in our template, and strip\n        # any trailing newlines from format_object_summary\n        data = format_object_summary(\n            self, self._formatter(), indent_for_name=False\n        ).rstrip(\", \\n\")\n        class_name = f\"<{type(self).__name__}>\\n\"\n        return f\"{class_name}{data}\\nLength: {len(self)}, dtype: {self.dtype}\"\n\n    def _formatter(self, boxed: bool = False) -> Callable[[Any], Optional[str]]:\n        \"\"\"\n        Formatting function for scalar values.\n\n        This is used in the default '__repr__'. The returned formatting\n        function receives instances of your scalar type.\n\n        Parameters\n        ----------\n        boxed : bool, default False\n            An indicated for whether or not your array is being printed\n            within a Series, DataFrame, or Index (True), or just by\n            itself (False). This may be useful if you want scalar values\n            to appear differently within a Series versus on its own (e.g.\n            quoted or not).\n\n        Returns\n        -------\n        Callable[[Any], str]\n            A callable that gets instances of the scalar type and\n            returns a string. By default, :func:`repr` is used\n            when ``boxed=False`` and :func:`str` is used when\n            ``boxed=True``.\n        \"\"\"\n        if boxed:\n            return str\n        return repr\n\n    # ------------------------------------------------------------------------\n    # Reshaping\n    # ------------------------------------------------------------------------\n\n    def transpose(self, *axes) -> \"ExtensionArray\":\n        \"\"\"\n        Return a transposed view on this array.\n\n        Because ExtensionArrays are always 1D, this is a no-op.  It is included\n        for compatibility with np.ndarray.\n        \"\"\"\n        return self[:]\n\n    @property\n    def T(self) -> \"ExtensionArray\":\n        return self.transpose()\n\n    def ravel(self, order=\"C\") -> \"ExtensionArray\":\n        \"\"\"\n        Return a flattened view on this array.\n\n        Parameters\n        ----------\n        order : {None, 'C', 'F', 'A', 'K'}, default 'C'\n\n        Returns\n        -------\n        ExtensionArray\n\n        Notes\n        -----\n        - Because ExtensionArrays are 1D-only, this is a no-op.\n        - The \"order\" argument is ignored, is for compatibility with NumPy.\n        \"\"\"\n        return self\n\n    @classmethod\n    def _concat_same_type(\n        cls, to_concat: Sequence[\"ExtensionArray\"]\n    ) -> \"ExtensionArray\":\n        \"\"\"\n        Concatenate multiple array of this dtype.\n\n        Parameters\n        ----------\n        to_concat : sequence of this type\n\n        Returns\n        -------\n        ExtensionArray\n        \"\"\"\n        # Implementer note: this method will only be called with a sequence of\n        # ExtensionArrays of this class and with the same dtype as self. This\n        # should allow \"easy\" concatenation (no upcasting needed), and result\n        # in a new ExtensionArray of the same dtype.\n        # Note: this strict behaviour is only guaranteed starting with pandas 1.1\n        raise AbstractMethodError(cls)\n\n    # The _can_hold_na attribute is set to True so that pandas internals\n    # will use the ExtensionDtype.na_value as the NA value in operations\n    # such as take(), reindex(), shift(), etc.  In addition, those results\n    # will then be of the ExtensionArray subclass rather than an array\n    # of objects\n    _can_hold_na = True\n\n    def _reduce(self, name: str, skipna: bool = True, **kwargs):\n        \"\"\"\n        Return a scalar result of performing the reduction operation.\n\n        Parameters\n        ----------\n        name : str\n            Name of the function, supported values are:\n            { any, all, min, max, sum, mean, median, prod,\n            std, var, sem, kurt, skew }.\n        skipna : bool, default True\n            If True, skip NaN values.\n        **kwargs\n            Additional keyword arguments passed to the reduction function.\n            Currently, `ddof` is the only supported kwarg.\n\n        Returns\n        -------\n        scalar\n\n        Raises\n        ------\n        TypeError : subclass does not define reductions\n        \"\"\"\n        raise TypeError(f\"cannot perform {name} with type {self.dtype}\")\n\n    def __hash__(self):\n        raise TypeError(f\"unhashable type: {repr(type(self).__name__)}\")\n\n\nclass ExtensionOpsMixin:\n    \"\"\"\n    A base class for linking the operators to their dunder names.\n\n    .. note::\n\n       You may want to set ``__array_priority__`` if you want your\n       implementation to be called when involved in binary operations\n       with NumPy arrays.\n    \"\"\"\n\n    @classmethod\n    def _create_arithmetic_method(cls, op):\n        raise AbstractMethodError(cls)\n\n    @classmethod\n    def _add_arithmetic_ops(cls):\n        setattr(cls, \"__add__\", cls._create_arithmetic_method(operator.add))\n        setattr(cls, \"__radd__\", cls._create_arithmetic_method(ops.radd))\n        setattr(cls, \"__sub__\", cls._create_arithmetic_method(operator.sub))\n        setattr(cls, \"__rsub__\", cls._create_arithmetic_method(ops.rsub))\n        setattr(cls, \"__mul__\", cls._create_arithmetic_method(operator.mul))\n        setattr(cls, \"__rmul__\", cls._create_arithmetic_method(ops.rmul))\n        setattr(cls, \"__pow__\", cls._create_arithmetic_method(operator.pow))\n        setattr(cls, \"__rpow__\", cls._create_arithmetic_method(ops.rpow))\n        setattr(cls, \"__mod__\", cls._create_arithmetic_method(operator.mod))\n        setattr(cls, \"__rmod__\", cls._create_arithmetic_method(ops.rmod))\n        setattr(cls, \"__floordiv__\", cls._create_arithmetic_method(operator.floordiv))\n        setattr(cls, \"__rfloordiv__\", cls._create_arithmetic_method(ops.rfloordiv))\n        setattr(cls, \"__truediv__\", cls._create_arithmetic_method(operator.truediv))\n        setattr(cls, \"__rtruediv__\", cls._create_arithmetic_method(ops.rtruediv))\n        setattr(cls, \"__divmod__\", cls._create_arithmetic_method(divmod))\n        setattr(cls, \"__rdivmod__\", cls._create_arithmetic_method(ops.rdivmod))\n\n    @classmethod\n    def _create_comparison_method(cls, op):\n        raise AbstractMethodError(cls)\n\n    @classmethod\n    def _add_comparison_ops(cls):\n        setattr(cls, \"__eq__\", cls._create_comparison_method(operator.eq))\n        setattr(cls, \"__ne__\", cls._create_comparison_method(operator.ne))\n        setattr(cls, \"__lt__\", cls._create_comparison_method(operator.lt))\n        setattr(cls, \"__gt__\", cls._create_comparison_method(operator.gt))\n        setattr(cls, \"__le__\", cls._create_comparison_method(operator.le))\n        setattr(cls, \"__ge__\", cls._create_comparison_method(operator.ge))\n\n    @classmethod\n    def _create_logical_method(cls, op):\n        raise AbstractMethodError(cls)\n\n    @classmethod\n    def _add_logical_ops(cls):\n        setattr(cls, \"__and__\", cls._create_logical_method(operator.and_))\n        setattr(cls, \"__rand__\", cls._create_logical_method(ops.rand_))\n        setattr(cls, \"__or__\", cls._create_logical_method(operator.or_))\n        setattr(cls, \"__ror__\", cls._create_logical_method(ops.ror_))\n        setattr(cls, \"__xor__\", cls._create_logical_method(operator.xor))\n        setattr(cls, \"__rxor__\", cls._create_logical_method(ops.rxor))\n\n\nclass ExtensionScalarOpsMixin(ExtensionOpsMixin):\n    \"\"\"\n    A mixin for defining  ops on an ExtensionArray.\n\n    It is assumed that the underlying scalar objects have the operators\n    already defined.\n\n    Notes\n    -----\n    If you have defined a subclass MyExtensionArray(ExtensionArray), then\n    use MyExtensionArray(ExtensionArray, ExtensionScalarOpsMixin) to\n    get the arithmetic operators.  After the definition of MyExtensionArray,\n    insert the lines\n\n    MyExtensionArray._add_arithmetic_ops()\n    MyExtensionArray._add_comparison_ops()\n\n    to link the operators to your class.\n\n    .. note::\n\n       You may want to set ``__array_priority__`` if you want your\n       implementation to be called when involved in binary operations\n       with NumPy arrays.\n    \"\"\"\n\n    @classmethod\n    def _create_method(cls, op, coerce_to_dtype=True, result_dtype=None):\n        \"\"\"\n        A class method that returns a method that will correspond to an\n        operator for an ExtensionArray subclass, by dispatching to the\n        relevant operator defined on the individual elements of the\n        ExtensionArray.\n\n        Parameters\n        ----------\n        op : function\n            An operator that takes arguments op(a, b)\n        coerce_to_dtype : bool, default True\n            boolean indicating whether to attempt to convert\n            the result to the underlying ExtensionArray dtype.\n            If it's not possible to create a new ExtensionArray with the\n            values, an ndarray is returned instead.\n\n        Returns\n        -------\n        Callable[[Any, Any], Union[ndarray, ExtensionArray]]\n            A method that can be bound to a class. When used, the method\n            receives the two arguments, one of which is the instance of\n            this class, and should return an ExtensionArray or an ndarray.\n\n            Returning an ndarray may be necessary when the result of the\n            `op` cannot be stored in the ExtensionArray. The dtype of the\n            ndarray uses NumPy's normal inference rules.\n\n        Examples\n        --------\n        Given an ExtensionArray subclass called MyExtensionArray, use\n\n            __add__ = cls._create_method(operator.add)\n\n        in the class definition of MyExtensionArray to create the operator\n        for addition, that will be based on the operator implementation\n        of the underlying elements of the ExtensionArray\n        \"\"\"\n\n        def _binop(self, other):\n            def convert_values(param):\n                if isinstance(param, ExtensionArray) or is_list_like(param):\n                    ovalues = param\n                else:  # Assume its an object\n                    ovalues = [param] * len(self)\n                return ovalues\n\n            if isinstance(other, (ABCSeries, ABCIndexClass, ABCDataFrame)):\n                # rely on pandas to unbox and dispatch to us\n                return NotImplemented\n\n            lvalues = self\n            rvalues = convert_values(other)\n\n            # If the operator is not defined for the underlying objects,\n            # a TypeError should be raised\n            res = [op(a, b) for (a, b) in zip(lvalues, rvalues)]\n\n            def _maybe_convert(arr):\n                if coerce_to_dtype:\n                    # https://github.com/pandas-dev/pandas/issues/22850\n                    # We catch all regular exceptions here, and fall back\n                    # to an ndarray.\n                    res = maybe_cast_to_extension_array(type(self), arr)\n                    if not isinstance(res, type(self)):\n                        # exception raised in _from_sequence; ensure we have ndarray\n                        res = np.asarray(arr)\n                else:\n                    res = np.asarray(arr, dtype=result_dtype)\n                return res\n\n            if op.__name__ in {\"divmod\", \"rdivmod\"}:\n                a, b = zip(*res)\n                return _maybe_convert(a), _maybe_convert(b)\n\n            return _maybe_convert(res)\n\n        op_name = f\"__{op.__name__}__\"\n        return set_function_name(_binop, op_name, cls)\n\n    @classmethod\n    def _create_arithmetic_method(cls, op):\n        return cls._create_method(op)\n\n    @classmethod\n    def _create_comparison_method(cls, op):\n        return cls._create_method(op, coerce_to_dtype=False, result_dtype=bool)\n"
    },
    {
      "filename": "pandas/core/arrays/boolean.py",
      "content": "import numbers\nfrom typing import TYPE_CHECKING, List, Tuple, Type, Union\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import lib, missing as libmissing\nfrom pandas._typing import ArrayLike\nfrom pandas.compat.numpy import function as nv\n\nfrom pandas.core.dtypes.common import (\n    is_bool_dtype,\n    is_extension_array_dtype,\n    is_float,\n    is_float_dtype,\n    is_integer_dtype,\n    is_list_like,\n    is_numeric_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import register_extension_dtype\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core import ops\n\nfrom .masked import BaseMaskedArray, BaseMaskedDtype\n\nif TYPE_CHECKING:\n    import pyarrow\n\n\n@register_extension_dtype\nclass BooleanDtype(BaseMaskedDtype):\n    \"\"\"\n    Extension dtype for boolean data.\n\n    .. versionadded:: 1.0.0\n\n    .. warning::\n\n       BooleanDtype is considered experimental. The implementation and\n       parts of the API may change without warning.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Examples\n    --------\n    >>> pd.BooleanDtype()\n    BooleanDtype\n    \"\"\"\n\n    name = \"boolean\"\n\n    # mypy: https://github.com/python/mypy/issues/4125\n    @property\n    def type(self) -> Type:  # type: ignore[override]\n        return np.bool_\n\n    @property\n    def kind(self) -> str:\n        return \"b\"\n\n    @property\n    def numpy_dtype(self) -> np.dtype:\n        return np.dtype(\"bool\")\n\n    @classmethod\n    def construct_array_type(cls) -> Type[\"BooleanArray\"]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        return BooleanArray\n\n    def __repr__(self) -> str:\n        return \"BooleanDtype\"\n\n    @property\n    def _is_boolean(self) -> bool:\n        return True\n\n    @property\n    def _is_numeric(self) -> bool:\n        return True\n\n    def __from_arrow__(\n        self, array: Union[\"pyarrow.Array\", \"pyarrow.ChunkedArray\"]\n    ) -> \"BooleanArray\":\n        \"\"\"\n        Construct BooleanArray from pyarrow Array/ChunkedArray.\n        \"\"\"\n        import pyarrow\n\n        if isinstance(array, pyarrow.Array):\n            chunks = [array]\n        else:\n            # pyarrow.ChunkedArray\n            chunks = array.chunks\n\n        results = []\n        for arr in chunks:\n            # TODO should optimize this without going through object array\n            bool_arr = BooleanArray._from_sequence(np.array(arr))\n            results.append(bool_arr)\n\n        return BooleanArray._concat_same_type(results)\n\n\ndef coerce_to_array(\n    values, mask=None, copy: bool = False\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Coerce the input values array to numpy arrays with a mask.\n\n    Parameters\n    ----------\n    values : 1D list-like\n    mask : bool 1D array, optional\n    copy : bool, default False\n        if True, copy the input\n\n    Returns\n    -------\n    tuple of (values, mask)\n    \"\"\"\n    if isinstance(values, BooleanArray):\n        if mask is not None:\n            raise ValueError(\"cannot pass mask for BooleanArray input\")\n        values, mask = values._data, values._mask\n        if copy:\n            values = values.copy()\n            mask = mask.copy()\n        return values, mask\n\n    mask_values = None\n    if isinstance(values, np.ndarray) and values.dtype == np.bool_:\n        if copy:\n            values = values.copy()\n    elif isinstance(values, np.ndarray) and is_numeric_dtype(values.dtype):\n        mask_values = isna(values)\n\n        values_bool = np.zeros(len(values), dtype=bool)\n        values_bool[~mask_values] = values[~mask_values].astype(bool)\n\n        if not np.all(\n            values_bool[~mask_values].astype(values.dtype) == values[~mask_values]\n        ):\n            raise TypeError(\"Need to pass bool-like values\")\n\n        values = values_bool\n    else:\n        values_object = np.asarray(values, dtype=object)\n\n        inferred_dtype = lib.infer_dtype(values_object, skipna=True)\n        integer_like = (\"floating\", \"integer\", \"mixed-integer-float\")\n        if inferred_dtype not in (\"boolean\", \"empty\") + integer_like:\n            raise TypeError(\"Need to pass bool-like values\")\n\n        mask_values = isna(values_object)\n        values = np.zeros(len(values), dtype=bool)\n        values[~mask_values] = values_object[~mask_values].astype(bool)\n\n        # if the values were integer-like, validate it were actually 0/1's\n        if inferred_dtype in integer_like:\n            if not np.all(\n                values[~mask_values].astype(float)\n                == values_object[~mask_values].astype(float)\n            ):\n                raise TypeError(\"Need to pass bool-like values\")\n\n    if mask is None and mask_values is None:\n        mask = np.zeros(len(values), dtype=bool)\n    elif mask is None:\n        mask = mask_values\n    else:\n        if isinstance(mask, np.ndarray) and mask.dtype == np.bool_:\n            if mask_values is not None:\n                mask = mask | mask_values\n            else:\n                if copy:\n                    mask = mask.copy()\n        else:\n            mask = np.array(mask, dtype=bool)\n            if mask_values is not None:\n                mask = mask | mask_values\n\n    if not values.ndim == 1:\n        raise ValueError(\"values must be a 1D list-like\")\n    if not mask.ndim == 1:\n        raise ValueError(\"mask must be a 1D list-like\")\n\n    return values, mask\n\n\nclass BooleanArray(BaseMaskedArray):\n    \"\"\"\n    Array of boolean (True/False) data with missing values.\n\n    This is a pandas Extension array for boolean data, under the hood\n    represented by 2 numpy arrays: a boolean array with the data and\n    a boolean array with the mask (True indicating missing).\n\n    BooleanArray implements Kleene logic (sometimes called three-value\n    logic) for logical operations. See :ref:`boolean.kleene` for more.\n\n    To construct an BooleanArray from generic array-like input, use\n    :func:`pandas.array` specifying ``dtype=\"boolean\"`` (see examples\n    below).\n\n    .. versionadded:: 1.0.0\n\n    .. warning::\n\n       BooleanArray is considered experimental. The implementation and\n       parts of the API may change without warning.\n\n    Parameters\n    ----------\n    values : numpy.ndarray\n        A 1-d boolean-dtype array with the data.\n    mask : numpy.ndarray\n        A 1-d boolean-dtype array indicating missing values (True\n        indicates missing).\n    copy : bool, default False\n        Whether to copy the `values` and `mask` arrays.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Returns\n    -------\n    BooleanArray\n\n    Examples\n    --------\n    Create an BooleanArray with :func:`pandas.array`:\n\n    >>> pd.array([True, False, None], dtype=\"boolean\")\n    <BooleanArray>\n    [True, False, <NA>]\n    Length: 3, dtype: boolean\n    \"\"\"\n\n    # The value used to fill '_data' to avoid upcasting\n    _internal_fill_value = False\n\n    def __init__(self, values: np.ndarray, mask: np.ndarray, copy: bool = False):\n        if not (isinstance(values, np.ndarray) and values.dtype == np.bool_):\n            raise TypeError(\n                \"values should be boolean numpy array. Use \"\n                \"the 'pd.array' function instead\"\n            )\n        self._dtype = BooleanDtype()\n        super().__init__(values, mask, copy=copy)\n\n    @property\n    def dtype(self) -> BooleanDtype:\n        return self._dtype\n\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy: bool = False) -> \"BooleanArray\":\n        if dtype:\n            assert dtype == \"boolean\"\n        values, mask = coerce_to_array(scalars, copy=copy)\n        return BooleanArray(values, mask)\n\n    @classmethod\n    def _from_sequence_of_strings(\n        cls, strings: List[str], dtype=None, copy: bool = False\n    ) -> \"BooleanArray\":\n        def map_string(s):\n            if isna(s):\n                return s\n            elif s in [\"True\", \"TRUE\", \"true\", \"1\", \"1.0\"]:\n                return True\n            elif s in [\"False\", \"FALSE\", \"false\", \"0\", \"0.0\"]:\n                return False\n            else:\n                raise ValueError(f\"{s} cannot be cast to bool\")\n\n        scalars = [map_string(x) for x in strings]\n        return cls._from_sequence(scalars, dtype, copy)\n\n    _HANDLED_TYPES = (np.ndarray, numbers.Number, bool, np.bool_)\n\n    def __array_ufunc__(self, ufunc, method: str, *inputs, **kwargs):\n        # For BooleanArray inputs, we apply the ufunc to ._data\n        # and mask the result.\n        if method == \"reduce\":\n            # Not clear how to handle missing values in reductions. Raise.\n            raise NotImplementedError(\"The 'reduce' method is not supported.\")\n        out = kwargs.get(\"out\", ())\n\n        for x in inputs + out:\n            if not isinstance(x, self._HANDLED_TYPES + (BooleanArray,)):\n                return NotImplemented\n\n        # for binary ops, use our custom dunder methods\n        result = ops.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        mask = np.zeros(len(self), dtype=bool)\n        inputs2 = []\n        for x in inputs:\n            if isinstance(x, BooleanArray):\n                mask |= x._mask\n                inputs2.append(x._data)\n            else:\n                inputs2.append(x)\n\n        def reconstruct(x):\n            # we don't worry about scalar `x` here, since we\n            # raise for reduce up above.\n\n            if is_bool_dtype(x.dtype):\n                m = mask.copy()\n                return BooleanArray(x, m)\n            else:\n                x[mask] = np.nan\n            return x\n\n        result = getattr(ufunc, method)(*inputs2, **kwargs)\n        if isinstance(result, tuple):\n            tuple(reconstruct(x) for x in result)\n        else:\n            return reconstruct(result)\n\n    def _coerce_to_array(self, value) -> Tuple[np.ndarray, np.ndarray]:\n        return coerce_to_array(value)\n\n    def astype(self, dtype, copy: bool = True) -> ArrayLike:\n        \"\"\"\n        Cast to a NumPy array or ExtensionArray with 'dtype'.\n\n        Parameters\n        ----------\n        dtype : str or dtype\n            Typecode or data-type to which the array is cast.\n        copy : bool, default True\n            Whether to copy the data, even if not necessary. If False,\n            a copy is made only if the old dtype does not match the\n            new dtype.\n\n        Returns\n        -------\n        ndarray or ExtensionArray\n            NumPy ndarray, BooleanArray or IntegerArray with 'dtype' for its dtype.\n\n        Raises\n        ------\n        TypeError\n            if incompatible type with an BooleanDtype, equivalent of same_kind\n            casting\n        \"\"\"\n        from pandas.core.arrays.string_ import StringDtype\n\n        dtype = pandas_dtype(dtype)\n\n        if isinstance(dtype, BooleanDtype):\n            values, mask = coerce_to_array(self, copy=copy)\n            if not copy:\n                return self\n            else:\n                return BooleanArray(values, mask, copy=False)\n        elif isinstance(dtype, StringDtype):\n            return dtype.construct_array_type()._from_sequence(self, copy=False)\n\n        if is_bool_dtype(dtype):\n            # astype_nansafe converts np.nan to True\n            if self._hasna:\n                raise ValueError(\"cannot convert float NaN to bool\")\n            else:\n                return self._data.astype(dtype, copy=copy)\n        if is_extension_array_dtype(dtype) and is_integer_dtype(dtype):\n            from pandas.core.arrays import IntegerArray\n\n            return IntegerArray(\n                self._data.astype(dtype.numpy_dtype), self._mask.copy(), copy=False\n            )\n        # for integer, error if there are missing values\n        if is_integer_dtype(dtype):\n            if self._hasna:\n                raise ValueError(\"cannot convert NA to integer\")\n        # for float dtype, ensure we use np.nan before casting (numpy cannot\n        # deal with pd.NA)\n        na_value = self._na_value\n        if is_float_dtype(dtype):\n            na_value = np.nan\n        # coerce\n        return self.to_numpy(dtype=dtype, na_value=na_value, copy=False)\n\n    def _values_for_argsort(self) -> np.ndarray:\n        \"\"\"\n        Return values for sorting.\n\n        Returns\n        -------\n        ndarray\n            The transformed values should maintain the ordering between values\n            within the array.\n\n        See Also\n        --------\n        ExtensionArray.argsort : Return the indices that would sort this array.\n        \"\"\"\n        data = self._data.copy()\n        data[self._mask] = -1\n        return data\n\n    def any(self, skipna: bool = True, **kwargs):\n        \"\"\"\n        Return whether any element is True.\n\n        Returns False unless there is at least one element that is True.\n        By default, NAs are skipped. If ``skipna=False`` is specified and\n        missing values are present, similar :ref:`Kleene logic <boolean.kleene>`\n        is used as for logical operations.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Exclude NA values. If the entire array is NA and `skipna` is\n            True, then the result will be False, as for an empty array.\n            If `skipna` is False, the result will still be True if there is\n            at least one element that is True, otherwise NA will be returned\n            if there are NA's present.\n        **kwargs : any, default None\n            Additional keywords have no effect but might be accepted for\n            compatibility with NumPy.\n\n        Returns\n        -------\n        bool or :attr:`pandas.NA`\n\n        See Also\n        --------\n        numpy.any : Numpy version of this method.\n        BooleanArray.all : Return whether all elements are True.\n\n        Examples\n        --------\n        The result indicates whether any element is True (and by default\n        skips NAs):\n\n        >>> pd.array([True, False, True]).any()\n        True\n        >>> pd.array([True, False, pd.NA]).any()\n        True\n        >>> pd.array([False, False, pd.NA]).any()\n        False\n        >>> pd.array([], dtype=\"boolean\").any()\n        False\n        >>> pd.array([pd.NA], dtype=\"boolean\").any()\n        False\n\n        With ``skipna=False``, the result can be NA if this is logically\n        required (whether ``pd.NA`` is True or False influences the result):\n\n        >>> pd.array([True, False, pd.NA]).any(skipna=False)\n        True\n        >>> pd.array([False, False, pd.NA]).any(skipna=False)\n        <NA>\n        \"\"\"\n        kwargs.pop(\"axis\", None)\n        nv.validate_any((), kwargs)\n\n        values = self._data.copy()\n        np.putmask(values, self._mask, False)\n        result = values.any()\n        if skipna:\n            return result\n        else:\n            if result or len(self) == 0 or not self._mask.any():\n                return result\n            else:\n                return self.dtype.na_value\n\n    def all(self, skipna: bool = True, **kwargs):\n        \"\"\"\n        Return whether all elements are True.\n\n        Returns True unless there is at least one element that is False.\n        By default, NAs are skipped. If ``skipna=False`` is specified and\n        missing values are present, similar :ref:`Kleene logic <boolean.kleene>`\n        is used as for logical operations.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Exclude NA values. If the entire array is NA and `skipna` is\n            True, then the result will be True, as for an empty array.\n            If `skipna` is False, the result will still be False if there is\n            at least one element that is False, otherwise NA will be returned\n            if there are NA's present.\n        **kwargs : any, default None\n            Additional keywords have no effect but might be accepted for\n            compatibility with NumPy.\n\n        Returns\n        -------\n        bool or :attr:`pandas.NA`\n\n        See Also\n        --------\n        numpy.all : Numpy version of this method.\n        BooleanArray.any : Return whether any element is True.\n\n        Examples\n        --------\n        The result indicates whether any element is True (and by default\n        skips NAs):\n\n        >>> pd.array([True, True, pd.NA]).all()\n        True\n        >>> pd.array([True, False, pd.NA]).all()\n        False\n        >>> pd.array([], dtype=\"boolean\").all()\n        True\n        >>> pd.array([pd.NA], dtype=\"boolean\").all()\n        True\n\n        With ``skipna=False``, the result can be NA if this is logically\n        required (whether ``pd.NA`` is True or False influences the result):\n\n        >>> pd.array([True, True, pd.NA]).all(skipna=False)\n        <NA>\n        >>> pd.array([True, False, pd.NA]).all(skipna=False)\n        False\n        \"\"\"\n        kwargs.pop(\"axis\", None)\n        nv.validate_all((), kwargs)\n\n        values = self._data.copy()\n        np.putmask(values, self._mask, True)\n        result = values.all()\n\n        if skipna:\n            return result\n        else:\n            if not result or len(self) == 0 or not self._mask.any():\n                return result\n            else:\n                return self.dtype.na_value\n\n    def _logical_method(self, other, op):\n\n        assert op.__name__ in {\"or_\", \"ror_\", \"and_\", \"rand_\", \"xor\", \"rxor\"}\n        other_is_booleanarray = isinstance(other, BooleanArray)\n        other_is_scalar = lib.is_scalar(other)\n        mask = None\n\n        if other_is_booleanarray:\n            other, mask = other._data, other._mask\n        elif is_list_like(other):\n            other = np.asarray(other, dtype=\"bool\")\n            if other.ndim > 1:\n                raise NotImplementedError(\"can only perform ops with 1-d structures\")\n            other, mask = coerce_to_array(other, copy=False)\n        elif isinstance(other, np.bool_):\n            other = other.item()\n\n        if other_is_scalar and not (other is libmissing.NA or lib.is_bool(other)):\n            raise TypeError(\n                \"'other' should be pandas.NA or a bool. \"\n                f\"Got {type(other).__name__} instead.\"\n            )\n\n        if not other_is_scalar and len(self) != len(other):\n            raise ValueError(\"Lengths must match to compare\")\n\n        if op.__name__ in {\"or_\", \"ror_\"}:\n            result, mask = ops.kleene_or(self._data, other, self._mask, mask)\n        elif op.__name__ in {\"and_\", \"rand_\"}:\n            result, mask = ops.kleene_and(self._data, other, self._mask, mask)\n        elif op.__name__ in {\"xor\", \"rxor\"}:\n            result, mask = ops.kleene_xor(self._data, other, self._mask, mask)\n\n        return BooleanArray(result, mask)\n\n    def _cmp_method(self, other, op):\n        from pandas.arrays import FloatingArray, IntegerArray\n\n        if isinstance(other, (IntegerArray, FloatingArray)):\n            return NotImplemented\n\n        mask = None\n\n        if isinstance(other, BooleanArray):\n            other, mask = other._data, other._mask\n\n        elif is_list_like(other):\n            other = np.asarray(other)\n            if other.ndim > 1:\n                raise NotImplementedError(\"can only perform ops with 1-d structures\")\n            if len(self) != len(other):\n                raise ValueError(\"Lengths must match to compare\")\n\n        if other is libmissing.NA:\n            # numpy does not handle pd.NA well as \"other\" scalar (it returns\n            # a scalar False instead of an array)\n            result = np.zeros_like(self._data)\n            mask = np.ones_like(self._data)\n        else:\n            # numpy will show a DeprecationWarning on invalid elementwise\n            # comparisons, this will raise in the future\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                with np.errstate(all=\"ignore\"):\n                    result = op(self._data, other)\n\n            # nans propagate\n            if mask is None:\n                mask = self._mask.copy()\n            else:\n                mask = self._mask | mask\n\n        return BooleanArray(result, mask, copy=False)\n\n    def _arith_method(self, other, op):\n        mask = None\n        op_name = op.__name__\n\n        if isinstance(other, BooleanArray):\n            other, mask = other._data, other._mask\n\n        elif is_list_like(other):\n            other = np.asarray(other)\n            if other.ndim > 1:\n                raise NotImplementedError(\"can only perform ops with 1-d structures\")\n            if len(self) != len(other):\n                raise ValueError(\"Lengths must match\")\n\n        # nans propagate\n        if mask is None:\n            mask = self._mask\n            if other is libmissing.NA:\n                mask |= True\n        else:\n            mask = self._mask | mask\n\n        if other is libmissing.NA:\n            # if other is NA, the result will be all NA and we can't run the\n            # actual op, so we need to choose the resulting dtype manually\n            if op_name in {\"floordiv\", \"rfloordiv\", \"mod\", \"rmod\", \"pow\", \"rpow\"}:\n                dtype = \"int8\"\n            else:\n                dtype = \"bool\"\n            result = np.zeros(len(self._data), dtype=dtype)\n        else:\n            with np.errstate(all=\"ignore\"):\n                result = op(self._data, other)\n\n        # divmod returns a tuple\n        if op_name == \"divmod\":\n            div, mod = result\n            return (\n                self._maybe_mask_result(div, mask, other, \"floordiv\"),\n                self._maybe_mask_result(mod, mask, other, \"mod\"),\n            )\n\n        return self._maybe_mask_result(result, mask, other, op_name)\n\n    def _reduce(self, name: str, skipna: bool = True, **kwargs):\n\n        if name in {\"any\", \"all\"}:\n            return getattr(self, name)(skipna=skipna, **kwargs)\n\n        return super()._reduce(name, skipna, **kwargs)\n\n    def _maybe_mask_result(self, result, mask, other, op_name: str):\n        \"\"\"\n        Parameters\n        ----------\n        result : array-like\n        mask : array-like bool\n        other : scalar or array-like\n        op_name : str\n        \"\"\"\n        # if we have a float operand we are by-definition\n        # a float result\n        # or our op is a divide\n        if (is_float_dtype(other) or is_float(other)) or (\n            op_name in [\"rtruediv\", \"truediv\"]\n        ):\n            result[mask] = np.nan\n            return result\n\n        if is_bool_dtype(result):\n            return BooleanArray(result, mask, copy=False)\n\n        elif is_integer_dtype(result):\n            from pandas.core.arrays import IntegerArray\n\n            return IntegerArray(result, mask, copy=False)\n        else:\n            result[mask] = np.nan\n            return result\n"
    },
    {
      "filename": "pandas/core/arrays/categorical.py",
      "content": "from csv import QUOTE_NONNUMERIC\nfrom functools import partial\nimport operator\nfrom shutil import get_terminal_size\nfrom typing import Dict, Hashable, List, Type, Union, cast\nfrom warnings import warn\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import NaT, algos as libalgos, hashtable as htable\nfrom pandas._typing import ArrayLike, Dtype, Ordered, Scalar\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import cache_readonly, deprecate_kwarg\nfrom pandas.util._validators import validate_bool_kwarg, validate_fillna_kwargs\n\nfrom pandas.core.dtypes.cast import (\n    coerce_indexer_dtype,\n    maybe_cast_to_extension_array,\n    maybe_infer_to_datetimelike,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_int64,\n    ensure_object,\n    is_categorical_dtype,\n    is_datetime64_dtype,\n    is_dict_like,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_hashable,\n    is_integer_dtype,\n    is_list_like,\n    is_object_dtype,\n    is_scalar,\n    is_timedelta64_dtype,\n    needs_i8_conversion,\n)\nfrom pandas.core.dtypes.dtypes import CategoricalDtype\nfrom pandas.core.dtypes.generic import ABCIndexClass, ABCSeries\nfrom pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna, notna\n\nfrom pandas.core import ops\nfrom pandas.core.accessor import PandasDelegate, delegate_names\nimport pandas.core.algorithms as algorithms\nfrom pandas.core.algorithms import factorize, get_data_algo, take_1d, unique1d\nfrom pandas.core.arrays._mixins import NDArrayBackedExtensionArray\nfrom pandas.core.base import ExtensionArray, NoNewAttributesMixin, PandasObject\nimport pandas.core.common as com\nfrom pandas.core.construction import array, extract_array, sanitize_array\nfrom pandas.core.indexers import deprecate_ndim_indexing\nfrom pandas.core.missing import interpolate_2d\nfrom pandas.core.ops.common import unpack_zerodim_and_defer\nfrom pandas.core.sorting import nargsort\nfrom pandas.core.strings.object_array import ObjectStringArrayMixin\n\nfrom pandas.io.formats import console\n\n\ndef _cat_compare_op(op):\n    opname = f\"__{op.__name__}__\"\n    fill_value = True if op is operator.ne else False\n\n    @unpack_zerodim_and_defer(opname)\n    def func(self, other):\n        hashable = is_hashable(other)\n        if is_list_like(other) and len(other) != len(self) and not hashable:\n            # in hashable case we may have a tuple that is itself a category\n            raise ValueError(\"Lengths must match.\")\n\n        if not self.ordered:\n            if opname in [\"__lt__\", \"__gt__\", \"__le__\", \"__ge__\"]:\n                raise TypeError(\n                    \"Unordered Categoricals can only compare equality or not\"\n                )\n        if isinstance(other, Categorical):\n            # Two Categoricals can only be be compared if the categories are\n            # the same (maybe up to ordering, depending on ordered)\n\n            msg = \"Categoricals can only be compared if 'categories' are the same.\"\n            if not self.is_dtype_equal(other):\n                raise TypeError(msg)\n\n            if not self.ordered and not self.categories.equals(other.categories):\n                # both unordered and different order\n                other_codes = _get_codes_for_values(other, self.categories)\n            else:\n                other_codes = other._codes\n\n            ret = op(self._codes, other_codes)\n            mask = (self._codes == -1) | (other_codes == -1)\n            if mask.any():\n                ret[mask] = fill_value\n            return ret\n\n        if hashable:\n            if other in self.categories:\n                i = self._unbox_scalar(other)\n                ret = op(self._codes, i)\n\n                if opname not in {\"__eq__\", \"__ge__\", \"__gt__\"}:\n                    # GH#29820 performance trick; get_loc will always give i>=0,\n                    #  so in the cases (__ne__, __le__, __lt__) the setting\n                    #  here is a no-op, so can be skipped.\n                    mask = self._codes == -1\n                    ret[mask] = fill_value\n                return ret\n            else:\n                return ops.invalid_comparison(self, other, op)\n        else:\n            # allow categorical vs object dtype array comparisons for equality\n            # these are only positional comparisons\n            if opname not in [\"__eq__\", \"__ne__\"]:\n                raise TypeError(\n                    f\"Cannot compare a Categorical for op {opname} with \"\n                    f\"type {type(other)}.\\nIf you want to compare values, \"\n                    \"use 'np.asarray(cat) <op> other'.\"\n                )\n\n            if isinstance(other, ExtensionArray) and needs_i8_conversion(other.dtype):\n                # We would return NotImplemented here, but that messes up\n                #  ExtensionIndex's wrapped methods\n                return op(other, self)\n            return getattr(np.array(self), opname)(np.array(other))\n\n    func.__name__ = opname\n\n    return func\n\n\ndef contains(cat, key, container):\n    \"\"\"\n    Helper for membership check for ``key`` in ``cat``.\n\n    This is a helper method for :method:`__contains__`\n    and :class:`CategoricalIndex.__contains__`.\n\n    Returns True if ``key`` is in ``cat.categories`` and the\n    location of ``key`` in ``categories`` is in ``container``.\n\n    Parameters\n    ----------\n    cat : :class:`Categorical`or :class:`categoricalIndex`\n    key : a hashable object\n        The key to check membership for.\n    container : Container (e.g. list-like or mapping)\n        The container to check for membership in.\n\n    Returns\n    -------\n    is_in : bool\n        True if ``key`` is in ``self.categories`` and location of\n        ``key`` in ``categories`` is in ``container``, else False.\n\n    Notes\n    -----\n    This method does not check for NaN values. Do that separately\n    before calling this method.\n    \"\"\"\n    hash(key)\n\n    # get location of key in categories.\n    # If a KeyError, the key isn't in categories, so logically\n    #  can't be in container either.\n    try:\n        loc = cat.categories.get_loc(key)\n    except (KeyError, TypeError):\n        return False\n\n    # loc is the location of key in categories, but also the *value*\n    # for key in container. So, `key` may be in categories,\n    # but still not in `container`. Example ('b' in categories,\n    # but not in values):\n    # 'b' in Categorical(['a'], categories=['a', 'b'])  # False\n    if is_scalar(loc):\n        return loc in container\n    else:\n        # if categories is an IntervalIndex, loc is an array.\n        return any(loc_ in container for loc_ in loc)\n\n\nclass Categorical(NDArrayBackedExtensionArray, PandasObject, ObjectStringArrayMixin):\n    \"\"\"\n    Represent a categorical variable in classic R / S-plus fashion.\n\n    `Categoricals` can only take on only a limited, and usually fixed, number\n    of possible values (`categories`). In contrast to statistical categorical\n    variables, a `Categorical` might have an order, but numerical operations\n    (additions, divisions, ...) are not possible.\n\n    All values of the `Categorical` are either in `categories` or `np.nan`.\n    Assigning values outside of `categories` will raise a `ValueError`. Order\n    is defined by the order of the `categories`, not lexical order of the\n    values.\n\n    Parameters\n    ----------\n    values : list-like\n        The values of the categorical. If categories are given, values not in\n        categories will be replaced with NaN.\n    categories : Index-like (unique), optional\n        The unique categories for this categorical. If not given, the\n        categories are assumed to be the unique values of `values` (sorted, if\n        possible, otherwise in the order in which they appear).\n    ordered : bool, default False\n        Whether or not this categorical is treated as a ordered categorical.\n        If True, the resulting categorical will be ordered.\n        An ordered categorical respects, when sorted, the order of its\n        `categories` attribute (which in turn is the `categories` argument, if\n        provided).\n    dtype : CategoricalDtype\n        An instance of ``CategoricalDtype`` to use for this categorical.\n\n    Attributes\n    ----------\n    categories : Index\n        The categories of this categorical\n    codes : ndarray\n        The codes (integer positions, which point to the categories) of this\n        categorical, read only.\n    ordered : bool\n        Whether or not this Categorical is ordered.\n    dtype : CategoricalDtype\n        The instance of ``CategoricalDtype`` storing the ``categories``\n        and ``ordered``.\n\n    Methods\n    -------\n    from_codes\n    __array__\n\n    Raises\n    ------\n    ValueError\n        If the categories do not validate.\n    TypeError\n        If an explicit ``ordered=True`` is given but no `categories` and the\n        `values` are not sortable.\n\n    See Also\n    --------\n    CategoricalDtype : Type for categorical data.\n    CategoricalIndex : An Index with an underlying ``Categorical``.\n\n    Notes\n    -----\n    See the `user guide\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html>`_\n    for more.\n\n    Examples\n    --------\n    >>> pd.Categorical([1, 2, 3, 1, 2, 3])\n    [1, 2, 3, 1, 2, 3]\n    Categories (3, int64): [1, 2, 3]\n\n    >>> pd.Categorical(['a', 'b', 'c', 'a', 'b', 'c'])\n    ['a', 'b', 'c', 'a', 'b', 'c']\n    Categories (3, object): ['a', 'b', 'c']\n\n    Missing values are not included as a category.\n\n    >>> c = pd.Categorical([1, 2, 3, 1, 2, 3, np.nan])\n    >>> c\n    [1, 2, 3, 1, 2, 3, NaN]\n    Categories (3, int64): [1, 2, 3]\n\n    However, their presence is indicated in the `codes` attribute\n    by code `-1`.\n\n    >>> c.codes\n    array([ 0,  1,  2,  0,  1,  2, -1], dtype=int8)\n\n    Ordered `Categoricals` can be sorted according to the custom order\n    of the categories and can have a min and max value.\n\n    >>> c = pd.Categorical(['a', 'b', 'c', 'a', 'b', 'c'], ordered=True,\n    ...                    categories=['c', 'b', 'a'])\n    >>> c\n    ['a', 'b', 'c', 'a', 'b', 'c']\n    Categories (3, object): ['c' < 'b' < 'a']\n    >>> c.min()\n    'c'\n    \"\"\"\n\n    # For comparisons, so that numpy uses our implementation if the compare\n    # ops, which raise\n    __array_priority__ = 1000\n    _dtype = CategoricalDtype(ordered=False)\n    # tolist is not actually deprecated, just suppressed in the __dir__\n    _deprecations = PandasObject._deprecations | frozenset([\"tolist\"])\n    _typ = \"categorical\"\n    _can_hold_na = True\n\n    def __init__(\n        self, values, categories=None, ordered=None, dtype=None, fastpath=False\n    ):\n\n        dtype = CategoricalDtype._from_values_or_dtype(\n            values, categories, ordered, dtype\n        )\n        # At this point, dtype is always a CategoricalDtype, but\n        # we may have dtype.categories be None, and we need to\n        # infer categories in a factorization step further below\n\n        if fastpath:\n            self._codes = coerce_indexer_dtype(values, dtype.categories)\n            self._dtype = self._dtype.update_dtype(dtype)\n            return\n\n        # null_mask indicates missing values we want to exclude from inference.\n        # This means: only missing values in list-likes (not arrays/ndframes).\n        null_mask = np.array(False)\n\n        # sanitize input\n        if is_categorical_dtype(values):\n            if dtype.categories is None:\n                dtype = CategoricalDtype(values.categories, dtype.ordered)\n        elif not isinstance(values, (ABCIndexClass, ABCSeries)):\n            # sanitize_array coerces np.nan to a string under certain versions\n            # of numpy\n            values = maybe_infer_to_datetimelike(values, convert_dates=True)\n            if not isinstance(values, np.ndarray):\n                values = com.convert_to_list_like(values)\n\n                # By convention, empty lists result in object dtype:\n                sanitize_dtype = np.dtype(\"O\") if len(values) == 0 else None\n                null_mask = isna(values)\n                if null_mask.any():\n                    values = [values[idx] for idx in np.where(~null_mask)[0]]\n                values = sanitize_array(values, None, dtype=sanitize_dtype)\n\n        if dtype.categories is None:\n            try:\n                codes, categories = factorize(values, sort=True)\n            except TypeError as err:\n                codes, categories = factorize(values, sort=False)\n                if dtype.ordered:\n                    # raise, as we don't have a sortable data structure and so\n                    # the user should give us one by specifying categories\n                    raise TypeError(\n                        \"'values' is not ordered, please \"\n                        \"explicitly specify the categories order \"\n                        \"by passing in a categories argument.\"\n                    ) from err\n            except ValueError as err:\n\n                # TODO(EA2D)\n                raise NotImplementedError(\n                    \"> 1 ndim Categorical are not supported at this time\"\n                ) from err\n\n            # we're inferring from values\n            dtype = CategoricalDtype(categories, dtype.ordered)\n\n        elif is_categorical_dtype(values.dtype):\n            old_codes = (\n                values._values.codes if isinstance(values, ABCSeries) else values.codes\n            )\n            codes = recode_for_categories(\n                old_codes, values.dtype.categories, dtype.categories\n            )\n\n        else:\n            codes = _get_codes_for_values(values, dtype.categories)\n\n        if null_mask.any():\n            # Reinsert -1 placeholders for previously removed missing values\n            full_codes = -np.ones(null_mask.shape, dtype=codes.dtype)\n            full_codes[~null_mask] = codes\n            codes = full_codes\n\n        self._dtype = self._dtype.update_dtype(dtype)\n        self._codes = coerce_indexer_dtype(codes, dtype.categories)\n\n    @property\n    def dtype(self) -> CategoricalDtype:\n        \"\"\"\n        The :class:`~pandas.api.types.CategoricalDtype` for this instance.\n        \"\"\"\n        return self._dtype\n\n    @property\n    def _constructor(self) -> Type[\"Categorical\"]:\n        return Categorical\n\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy=False):\n        return Categorical(scalars, dtype=dtype)\n\n    def astype(self, dtype: Dtype, copy: bool = True) -> ArrayLike:\n        \"\"\"\n        Coerce this type to another dtype\n\n        Parameters\n        ----------\n        dtype : numpy dtype or pandas type\n        copy : bool, default True\n            By default, astype always returns a newly allocated object.\n            If copy is set to False and dtype is categorical, the original\n            object is returned.\n        \"\"\"\n        if is_categorical_dtype(dtype):\n            dtype = cast(Union[str, CategoricalDtype], dtype)\n\n            # GH 10696/18593\n            dtype = self.dtype.update_dtype(dtype)\n            self = self.copy() if copy else self\n            if dtype == self.dtype:\n                return self\n            return self._set_dtype(dtype)\n        if is_extension_array_dtype(dtype):\n            return array(self, dtype=dtype, copy=copy)\n        if is_integer_dtype(dtype) and self.isna().any():\n            raise ValueError(\"Cannot convert float NaN to integer\")\n        return np.array(self, dtype=dtype, copy=copy)\n\n    @cache_readonly\n    def itemsize(self) -> int:\n        \"\"\"\n        return the size of a single category\n        \"\"\"\n        return self.categories.itemsize\n\n    def tolist(self) -> List[Scalar]:\n        \"\"\"\n        Return a list of the values.\n\n        These are each a scalar type, which is a Python scalar\n        (for str, int, float) or a pandas scalar\n        (for Timestamp/Timedelta/Interval/Period)\n        \"\"\"\n        return list(self)\n\n    to_list = tolist\n\n    @classmethod\n    def _from_inferred_categories(\n        cls, inferred_categories, inferred_codes, dtype, true_values=None\n    ):\n        \"\"\"\n        Construct a Categorical from inferred values.\n\n        For inferred categories (`dtype` is None) the categories are sorted.\n        For explicit `dtype`, the `inferred_categories` are cast to the\n        appropriate type.\n\n        Parameters\n        ----------\n        inferred_categories : Index\n        inferred_codes : Index\n        dtype : CategoricalDtype or 'category'\n        true_values : list, optional\n            If none are provided, the default ones are\n            \"True\", \"TRUE\", and \"true.\"\n\n        Returns\n        -------\n        Categorical\n        \"\"\"\n        from pandas import Index, to_datetime, to_numeric, to_timedelta\n\n        cats = Index(inferred_categories)\n        known_categories = (\n            isinstance(dtype, CategoricalDtype) and dtype.categories is not None\n        )\n\n        if known_categories:\n            # Convert to a specialized type with `dtype` if specified.\n            if dtype.categories.is_numeric():\n                cats = to_numeric(inferred_categories, errors=\"coerce\")\n            elif is_datetime64_dtype(dtype.categories):\n                cats = to_datetime(inferred_categories, errors=\"coerce\")\n            elif is_timedelta64_dtype(dtype.categories):\n                cats = to_timedelta(inferred_categories, errors=\"coerce\")\n            elif dtype.categories.is_boolean():\n                if true_values is None:\n                    true_values = [\"True\", \"TRUE\", \"true\"]\n\n                cats = cats.isin(true_values)\n\n        if known_categories:\n            # Recode from observation order to dtype.categories order.\n            categories = dtype.categories\n            codes = recode_for_categories(inferred_codes, cats, categories)\n        elif not cats.is_monotonic_increasing:\n            # Sort categories and recode for unknown categories.\n            unsorted = cats.copy()\n            categories = cats.sort_values()\n\n            codes = recode_for_categories(inferred_codes, unsorted, categories)\n            dtype = CategoricalDtype(categories, ordered=False)\n        else:\n            dtype = CategoricalDtype(cats, ordered=False)\n            codes = inferred_codes\n\n        return cls(codes, dtype=dtype, fastpath=True)\n\n    @classmethod\n    def from_codes(cls, codes, categories=None, ordered=None, dtype=None):\n        \"\"\"\n        Make a Categorical type from codes and categories or dtype.\n\n        This constructor is useful if you already have codes and\n        categories/dtype and so do not need the (computation intensive)\n        factorization step, which is usually done on the constructor.\n\n        If your data does not follow this convention, please use the normal\n        constructor.\n\n        Parameters\n        ----------\n        codes : array-like of int\n            An integer array, where each integer points to a category in\n            categories or dtype.categories, or else is -1 for NaN.\n        categories : index-like, optional\n            The categories for the categorical. Items need to be unique.\n            If the categories are not given here, then they must be provided\n            in `dtype`.\n        ordered : bool, optional\n            Whether or not this categorical is treated as an ordered\n            categorical. If not given here or in `dtype`, the resulting\n            categorical will be unordered.\n        dtype : CategoricalDtype or \"category\", optional\n            If :class:`CategoricalDtype`, cannot be used together with\n            `categories` or `ordered`.\n\n            .. versionadded:: 0.24.0\n\n               When `dtype` is provided, neither `categories` nor `ordered`\n               should be provided.\n\n        Returns\n        -------\n        Categorical\n\n        Examples\n        --------\n        >>> dtype = pd.CategoricalDtype(['a', 'b'], ordered=True)\n        >>> pd.Categorical.from_codes(codes=[0, 1, 0, 1], dtype=dtype)\n        ['a', 'b', 'a', 'b']\n        Categories (2, object): ['a' < 'b']\n        \"\"\"\n        dtype = CategoricalDtype._from_values_or_dtype(\n            categories=categories, ordered=ordered, dtype=dtype\n        )\n        if dtype.categories is None:\n            msg = (\n                \"The categories must be provided in 'categories' or \"\n                \"'dtype'. Both were None.\"\n            )\n            raise ValueError(msg)\n\n        if is_extension_array_dtype(codes) and is_integer_dtype(codes):\n            # Avoid the implicit conversion of Int to object\n            if isna(codes).any():\n                raise ValueError(\"codes cannot contain NA values\")\n            codes = codes.to_numpy(dtype=np.int64)\n        else:\n            codes = np.asarray(codes)\n        if len(codes) and not is_integer_dtype(codes):\n            raise ValueError(\"codes need to be array-like integers\")\n\n        if len(codes) and (codes.max() >= len(dtype.categories) or codes.min() < -1):\n            raise ValueError(\"codes need to be between -1 and len(categories)-1\")\n\n        return cls(codes, dtype=dtype, fastpath=True)\n\n    # ------------------------------------------------------------------\n    # Categories/Codes/Ordered\n\n    @property\n    def categories(self):\n        \"\"\"\n        The categories of this categorical.\n\n        Setting assigns new values to each category (effectively a rename of\n        each individual category).\n\n        The assigned value has to be a list-like object. All items must be\n        unique and the number of items in the new categories must be the same\n        as the number of items in the old categories.\n\n        Assigning to `categories` is a inplace operation!\n\n        Raises\n        ------\n        ValueError\n            If the new categories do not validate as categories or if the\n            number of new categories is unequal the number of old categories\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n        \"\"\"\n        return self.dtype.categories\n\n    @categories.setter\n    def categories(self, categories):\n        new_dtype = CategoricalDtype(categories, ordered=self.ordered)\n        if self.dtype.categories is not None and len(self.dtype.categories) != len(\n            new_dtype.categories\n        ):\n            raise ValueError(\n                \"new categories need to have the same number of \"\n                \"items as the old categories!\"\n            )\n        self._dtype = new_dtype\n\n    @property\n    def ordered(self) -> Ordered:\n        \"\"\"\n        Whether the categories have an ordered relationship.\n        \"\"\"\n        return self.dtype.ordered\n\n    @property\n    def codes(self) -> np.ndarray:\n        \"\"\"\n        The category codes of this categorical.\n\n        Codes are an array of integers which are the positions of the actual\n        values in the categories array.\n\n        There is no setter, use the other categorical methods and the normal item\n        setter to change values in the categorical.\n\n        Returns\n        -------\n        ndarray[int]\n            A non-writable view of the `codes` array.\n        \"\"\"\n        v = self._codes.view()\n        v.flags.writeable = False\n        return v\n\n    def _set_categories(self, categories, fastpath=False):\n        \"\"\"\n        Sets new categories inplace\n\n        Parameters\n        ----------\n        fastpath : bool, default False\n           Don't perform validation of the categories for uniqueness or nulls\n\n        Examples\n        --------\n        >>> c = pd.Categorical(['a', 'b'])\n        >>> c\n        ['a', 'b']\n        Categories (2, object): ['a', 'b']\n\n        >>> c._set_categories(pd.Index(['a', 'c']))\n        >>> c\n        ['a', 'c']\n        Categories (2, object): ['a', 'c']\n        \"\"\"\n        if fastpath:\n            new_dtype = CategoricalDtype._from_fastpath(categories, self.ordered)\n        else:\n            new_dtype = CategoricalDtype(categories, ordered=self.ordered)\n        if (\n            not fastpath\n            and self.dtype.categories is not None\n            and len(new_dtype.categories) != len(self.dtype.categories)\n        ):\n            raise ValueError(\n                \"new categories need to have the same number of \"\n                \"items than the old categories!\"\n            )\n\n        self._dtype = new_dtype\n\n    def _set_dtype(self, dtype: CategoricalDtype) -> \"Categorical\":\n        \"\"\"\n        Internal method for directly updating the CategoricalDtype\n\n        Parameters\n        ----------\n        dtype : CategoricalDtype\n\n        Notes\n        -----\n        We don't do any validation here. It's assumed that the dtype is\n        a (valid) instance of `CategoricalDtype`.\n        \"\"\"\n        codes = recode_for_categories(self.codes, self.categories, dtype.categories)\n        return type(self)(codes, dtype=dtype, fastpath=True)\n\n    def set_ordered(self, value, inplace=False):\n        \"\"\"\n        Set the ordered attribute to the boolean value.\n\n        Parameters\n        ----------\n        value : bool\n           Set whether this categorical is ordered (True) or not (False).\n        inplace : bool, default False\n           Whether or not to set the ordered attribute in-place or return\n           a copy of this categorical with ordered set to the value.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        new_dtype = CategoricalDtype(self.categories, ordered=value)\n        cat = self if inplace else self.copy()\n        cat._dtype = new_dtype\n        if not inplace:\n            return cat\n\n    def as_ordered(self, inplace=False):\n        \"\"\"\n        Set the Categorical to be ordered.\n\n        Parameters\n        ----------\n        inplace : bool, default False\n           Whether or not to set the ordered attribute in-place or return\n           a copy of this categorical with ordered set to True.\n\n        Returns\n        -------\n        Categorical or None\n            Ordered Categorical or None if ``inplace=True``.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        return self.set_ordered(True, inplace=inplace)\n\n    def as_unordered(self, inplace=False):\n        \"\"\"\n        Set the Categorical to be unordered.\n\n        Parameters\n        ----------\n        inplace : bool, default False\n           Whether or not to set the ordered attribute in-place or return\n           a copy of this categorical with ordered set to False.\n\n        Returns\n        -------\n        Categorical or None\n            Unordered Categorical or None if ``inplace=True``.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        return self.set_ordered(False, inplace=inplace)\n\n    def set_categories(self, new_categories, ordered=None, rename=False, inplace=False):\n        \"\"\"\n        Set the categories to the specified new_categories.\n\n        `new_categories` can include new categories (which will result in\n        unused categories) or remove old categories (which results in values\n        set to NaN). If `rename==True`, the categories will simple be renamed\n        (less or more items than in old categories will result in values set to\n        NaN or in unused categories respectively).\n\n        This method can be used to perform more than one action of adding,\n        removing, and reordering simultaneously and is therefore faster than\n        performing the individual steps via the more specialised methods.\n\n        On the other hand this methods does not do checks (e.g., whether the\n        old categories are included in the new categories on a reorder), which\n        can result in surprising changes, for example when using special string\n        dtypes, which does not considers a S1 string equal to a single char\n        python string.\n\n        Parameters\n        ----------\n        new_categories : Index-like\n           The categories in new order.\n        ordered : bool, default False\n           Whether or not the categorical is treated as a ordered categorical.\n           If not given, do not change the ordered information.\n        rename : bool, default False\n           Whether or not the new_categories should be considered as a rename\n           of the old categories or as reordered categories.\n        inplace : bool, default False\n           Whether or not to reorder the categories in-place or return a copy\n           of this categorical with reordered categories.\n\n        Returns\n        -------\n        Categorical with reordered categories or None if inplace.\n\n        Raises\n        ------\n        ValueError\n            If new_categories does not validate as categories\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if ordered is None:\n            ordered = self.dtype.ordered\n        new_dtype = CategoricalDtype(new_categories, ordered=ordered)\n\n        cat = self if inplace else self.copy()\n        if rename:\n            if cat.dtype.categories is not None and len(new_dtype.categories) < len(\n                cat.dtype.categories\n            ):\n                # remove all _codes which are larger and set to -1/NaN\n                cat._codes[cat._codes >= len(new_dtype.categories)] = -1\n        else:\n            codes = recode_for_categories(\n                cat.codes, cat.categories, new_dtype.categories\n            )\n            cat._codes = codes\n        cat._dtype = new_dtype\n\n        if not inplace:\n            return cat\n\n    def rename_categories(self, new_categories, inplace=False):\n        \"\"\"\n        Rename categories.\n\n        Parameters\n        ----------\n        new_categories : list-like, dict-like or callable\n\n            New categories which will replace old categories.\n\n            * list-like: all items must be unique and the number of items in\n              the new categories must match the existing number of categories.\n\n            * dict-like: specifies a mapping from\n              old categories to new. Categories not contained in the mapping\n              are passed through and extra categories in the mapping are\n              ignored.\n\n            * callable : a callable that is called on all items in the old\n              categories and whose return values comprise the new categories.\n\n        inplace : bool, default False\n            Whether or not to rename the categories inplace or return a copy of\n            this categorical with renamed categories.\n\n        Returns\n        -------\n        cat : Categorical or None\n            Categorical with removed categories or None if ``inplace=True``.\n\n        Raises\n        ------\n        ValueError\n            If new categories are list-like and do not have the same number of\n            items than the current categories or do not validate as categories\n\n        See Also\n        --------\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n\n        Examples\n        --------\n        >>> c = pd.Categorical(['a', 'a', 'b'])\n        >>> c.rename_categories([0, 1])\n        [0, 0, 1]\n        Categories (2, int64): [0, 1]\n\n        For dict-like ``new_categories``, extra keys are ignored and\n        categories not in the dictionary are passed through\n\n        >>> c.rename_categories({'a': 'A', 'c': 'C'})\n        ['A', 'A', 'b']\n        Categories (2, object): ['A', 'b']\n\n        You may also provide a callable to create the new categories\n\n        >>> c.rename_categories(lambda x: x.upper())\n        ['A', 'A', 'B']\n        Categories (2, object): ['A', 'B']\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        cat = self if inplace else self.copy()\n\n        if is_dict_like(new_categories):\n            cat.categories = [new_categories.get(item, item) for item in cat.categories]\n        elif callable(new_categories):\n            cat.categories = [new_categories(item) for item in cat.categories]\n        else:\n            cat.categories = new_categories\n        if not inplace:\n            return cat\n\n    def reorder_categories(self, new_categories, ordered=None, inplace=False):\n        \"\"\"\n        Reorder categories as specified in new_categories.\n\n        `new_categories` need to include all old categories and no new category\n        items.\n\n        Parameters\n        ----------\n        new_categories : Index-like\n           The categories in new order.\n        ordered : bool, optional\n           Whether or not the categorical is treated as a ordered categorical.\n           If not given, do not change the ordered information.\n        inplace : bool, default False\n           Whether or not to reorder the categories inplace or return a copy of\n           this categorical with reordered categories.\n\n        Returns\n        -------\n        cat : Categorical or None\n            Categorical with removed categories or None if ``inplace=True``.\n\n        Raises\n        ------\n        ValueError\n            If the new categories do not contain all old category items or any\n            new ones\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if set(self.dtype.categories) != set(new_categories):\n            raise ValueError(\n                \"items in new_categories are not the same as in old categories\"\n            )\n        return self.set_categories(new_categories, ordered=ordered, inplace=inplace)\n\n    def add_categories(self, new_categories, inplace=False):\n        \"\"\"\n        Add new categories.\n\n        `new_categories` will be included at the last/highest place in the\n        categories and will be unused directly after this call.\n\n        Parameters\n        ----------\n        new_categories : category or list-like of category\n           The new categories to be included.\n        inplace : bool, default False\n           Whether or not to add the categories inplace or return a copy of\n           this categorical with added categories.\n\n        Returns\n        -------\n        cat : Categorical or None\n            Categorical with new categories added or None if ``inplace=True``.\n\n        Raises\n        ------\n        ValueError\n            If the new categories include old categories or do not validate as\n            categories\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if not is_list_like(new_categories):\n            new_categories = [new_categories]\n        already_included = set(new_categories) & set(self.dtype.categories)\n        if len(already_included) != 0:\n            raise ValueError(\n                f\"new categories must not include old categories: {already_included}\"\n            )\n        new_categories = list(self.dtype.categories) + list(new_categories)\n        new_dtype = CategoricalDtype(new_categories, self.ordered)\n\n        cat = self if inplace else self.copy()\n        cat._dtype = new_dtype\n        cat._codes = coerce_indexer_dtype(cat._codes, new_dtype.categories)\n        if not inplace:\n            return cat\n\n    def remove_categories(self, removals, inplace=False):\n        \"\"\"\n        Remove the specified categories.\n\n        `removals` must be included in the old categories. Values which were in\n        the removed categories will be set to NaN\n\n        Parameters\n        ----------\n        removals : category or list of categories\n           The categories which should be removed.\n        inplace : bool, default False\n           Whether or not to remove the categories inplace or return a copy of\n           this categorical with removed categories.\n\n        Returns\n        -------\n        cat : Categorical or None\n            Categorical with removed categories or None if ``inplace=True``.\n\n        Raises\n        ------\n        ValueError\n            If the removals are not contained in the categories\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if not is_list_like(removals):\n            removals = [removals]\n\n        removal_set = set(removals)\n        not_included = removal_set - set(self.dtype.categories)\n        new_categories = [c for c in self.dtype.categories if c not in removal_set]\n\n        # GH 10156\n        if any(isna(removals)):\n            not_included = {x for x in not_included if notna(x)}\n            new_categories = [x for x in new_categories if notna(x)]\n\n        if len(not_included) != 0:\n            raise ValueError(f\"removals must all be in old categories: {not_included}\")\n\n        return self.set_categories(\n            new_categories, ordered=self.ordered, rename=False, inplace=inplace\n        )\n\n    def remove_unused_categories(self, inplace=False):\n        \"\"\"\n        Remove categories which are not used.\n\n        Parameters\n        ----------\n        inplace : bool, default False\n           Whether or not to drop unused categories inplace or return a copy of\n           this categorical with unused categories dropped.\n\n        Returns\n        -------\n        cat : Categorical or None\n            Categorical with unused categories dropped or None if ``inplace=True``.\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        set_categories : Set the categories to the specified ones.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        cat = self if inplace else self.copy()\n        idx, inv = np.unique(cat._codes, return_inverse=True)\n\n        if idx.size != 0 and idx[0] == -1:  # na sentinel\n            idx, inv = idx[1:], inv - 1\n\n        new_categories = cat.dtype.categories.take(idx)\n        new_dtype = CategoricalDtype._from_fastpath(\n            new_categories, ordered=self.ordered\n        )\n        cat._dtype = new_dtype\n        cat._codes = coerce_indexer_dtype(inv, new_dtype.categories)\n\n        if not inplace:\n            return cat\n\n    # ------------------------------------------------------------------\n\n    def map(self, mapper):\n        \"\"\"\n        Map categories using input correspondence (dict, Series, or function).\n\n        Maps the categories to new categories. If the mapping correspondence is\n        one-to-one the result is a :class:`~pandas.Categorical` which has the\n        same order property as the original, otherwise a :class:`~pandas.Index`\n        is returned. NaN values are unaffected.\n\n        If a `dict` or :class:`~pandas.Series` is used any unmapped category is\n        mapped to `NaN`. Note that if this happens an :class:`~pandas.Index`\n        will be returned.\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            Mapping correspondence.\n\n        Returns\n        -------\n        pandas.Categorical or pandas.Index\n            Mapped categorical.\n\n        See Also\n        --------\n        CategoricalIndex.map : Apply a mapping correspondence on a\n            :class:`~pandas.CategoricalIndex`.\n        Index.map : Apply a mapping correspondence on an\n            :class:`~pandas.Index`.\n        Series.map : Apply a mapping correspondence on a\n            :class:`~pandas.Series`.\n        Series.apply : Apply more complex functions on a\n            :class:`~pandas.Series`.\n\n        Examples\n        --------\n        >>> cat = pd.Categorical(['a', 'b', 'c'])\n        >>> cat\n        ['a', 'b', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> cat.map(lambda x: x.upper())\n        ['A', 'B', 'C']\n        Categories (3, object): ['A', 'B', 'C']\n        >>> cat.map({'a': 'first', 'b': 'second', 'c': 'third'})\n        ['first', 'second', 'third']\n        Categories (3, object): ['first', 'second', 'third']\n\n        If the mapping is one-to-one the ordering of the categories is\n        preserved:\n\n        >>> cat = pd.Categorical(['a', 'b', 'c'], ordered=True)\n        >>> cat\n        ['a', 'b', 'c']\n        Categories (3, object): ['a' < 'b' < 'c']\n        >>> cat.map({'a': 3, 'b': 2, 'c': 1})\n        [3, 2, 1]\n        Categories (3, int64): [3 < 2 < 1]\n\n        If the mapping is not one-to-one an :class:`~pandas.Index` is returned:\n\n        >>> cat.map({'a': 'first', 'b': 'second', 'c': 'first'})\n        Index(['first', 'second', 'first'], dtype='object')\n\n        If a `dict` is used, all unmapped categories are mapped to `NaN` and\n        the result is an :class:`~pandas.Index`:\n\n        >>> cat.map({'a': 'first', 'b': 'second'})\n        Index(['first', 'second', nan], dtype='object')\n        \"\"\"\n        new_categories = self.categories.map(mapper)\n        try:\n            return self.from_codes(\n                self._codes.copy(), categories=new_categories, ordered=self.ordered\n            )\n        except ValueError:\n            # NA values are represented in self._codes with -1\n            # np.take causes NA values to take final element in new_categories\n            if np.any(self._codes == -1):\n                new_categories = new_categories.insert(len(new_categories), np.nan)\n            return np.take(new_categories, self._codes)\n\n    __eq__ = _cat_compare_op(operator.eq)\n    __ne__ = _cat_compare_op(operator.ne)\n    __lt__ = _cat_compare_op(operator.lt)\n    __gt__ = _cat_compare_op(operator.gt)\n    __le__ = _cat_compare_op(operator.le)\n    __ge__ = _cat_compare_op(operator.ge)\n\n    # -------------------------------------------------------------\n    # Validators; ideally these can be de-duplicated\n\n    def _validate_where_value(self, value):\n        if is_scalar(value):\n            return self._validate_fill_value(value)\n        return self._validate_listlike(value)\n\n    def _validate_insert_value(self, value) -> int:\n        return self._validate_fill_value(value)\n\n    def _validate_searchsorted_value(self, value):\n        # searchsorted is very performance sensitive. By converting codes\n        # to same dtype as self.codes, we get much faster performance.\n        if is_scalar(value):\n            codes = self._unbox_scalar(value)\n        else:\n            locs = [self.categories.get_loc(x) for x in value]\n            codes = np.array(locs, dtype=self.codes.dtype)\n        return codes\n\n    def _validate_fill_value(self, fill_value):\n        \"\"\"\n        Convert a user-facing fill_value to a representation to use with our\n        underlying ndarray, raising ValueError if this is not possible.\n\n        Parameters\n        ----------\n        fill_value : object\n\n        Returns\n        -------\n        fill_value : int\n\n        Raises\n        ------\n        ValueError\n        \"\"\"\n\n        if is_valid_nat_for_dtype(fill_value, self.categories.dtype):\n            fill_value = -1\n        elif fill_value in self.categories:\n            fill_value = self._unbox_scalar(fill_value)\n        else:\n            raise ValueError(\n                f\"'fill_value={fill_value}' is not present \"\n                \"in this Categorical's categories\"\n            )\n        return fill_value\n\n    # -------------------------------------------------------------\n\n    def __array__(self, dtype=None) -> np.ndarray:\n        \"\"\"\n        The numpy array interface.\n\n        Returns\n        -------\n        numpy.array\n            A numpy array of either the specified dtype or,\n            if dtype==None (default), the same dtype as\n            categorical.categories.dtype.\n        \"\"\"\n        ret = take_1d(self.categories.values, self._codes)\n        if dtype and not is_dtype_equal(dtype, self.categories.dtype):\n            return np.asarray(ret, dtype)\n        if is_extension_array_dtype(ret):\n            # When we're a Categorical[ExtensionArray], like Interval,\n            # we need to ensure __array__ get's all the way to an\n            # ndarray.\n            ret = np.asarray(ret)\n        return ret\n\n    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n        # for binary ops, use our custom dunder methods\n        result = ops.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        # for all other cases, raise for now (similarly as what happens in\n        # Series.__array_prepare__)\n        raise TypeError(\n            f\"Object with dtype {self.dtype} cannot perform \"\n            f\"the numpy op {ufunc.__name__}\"\n        )\n\n    def __setstate__(self, state):\n        \"\"\"Necessary for making this object picklable\"\"\"\n        if not isinstance(state, dict):\n            raise Exception(\"invalid pickle state\")\n\n        if \"_dtype\" not in state:\n            state[\"_dtype\"] = CategoricalDtype(state[\"_categories\"], state[\"_ordered\"])\n\n        for k, v in state.items():\n            setattr(self, k, v)\n\n    @property\n    def nbytes(self) -> int:\n        return self._codes.nbytes + self.dtype.categories.values.nbytes\n\n    def memory_usage(self, deep: bool = False) -> int:\n        \"\"\"\n        Memory usage of my values\n\n        Parameters\n        ----------\n        deep : bool\n            Introspect the data deeply, interrogate\n            `object` dtypes for system-level memory consumption\n\n        Returns\n        -------\n        bytes used\n\n        Notes\n        -----\n        Memory usage does not include memory consumed by elements that\n        are not components of the array if deep=False\n\n        See Also\n        --------\n        numpy.ndarray.nbytes\n        \"\"\"\n        return self._codes.nbytes + self.dtype.categories.memory_usage(deep=deep)\n\n    def isna(self):\n        \"\"\"\n        Detect missing values\n\n        Missing values (-1 in .codes) are detected.\n\n        Returns\n        -------\n        a boolean array of whether my values are null\n\n        See Also\n        --------\n        isna : Top-level isna.\n        isnull : Alias of isna.\n        Categorical.notna : Boolean inverse of Categorical.isna.\n\n        \"\"\"\n        ret = self._codes == -1\n        return ret\n\n    isnull = isna\n\n    def notna(self):\n        \"\"\"\n        Inverse of isna\n\n        Both missing values (-1 in .codes) and NA as a category are detected as\n        null.\n\n        Returns\n        -------\n        a boolean array of whether my values are not null\n\n        See Also\n        --------\n        notna : Top-level notna.\n        notnull : Alias of notna.\n        Categorical.isna : Boolean inverse of Categorical.notna.\n\n        \"\"\"\n        return ~self.isna()\n\n    notnull = notna\n\n    def value_counts(self, dropna=True):\n        \"\"\"\n        Return a Series containing counts of each category.\n\n        Every category will have an entry, even those with a count of 0.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't include counts of NaN.\n\n        Returns\n        -------\n        counts : Series\n\n        See Also\n        --------\n        Series.value_counts\n        \"\"\"\n        from pandas import CategoricalIndex, Series\n\n        code, cat = self._codes, self.categories\n        ncat, mask = len(cat), 0 <= code\n        ix, clean = np.arange(ncat), mask.all()\n\n        if dropna or clean:\n            obs = code if clean else code[mask]\n            count = np.bincount(obs, minlength=ncat or 0)\n        else:\n            count = np.bincount(np.where(mask, code, ncat))\n            ix = np.append(ix, -1)\n\n        ix = self._from_backing_data(ix)\n\n        return Series(count, index=CategoricalIndex(ix), dtype=\"int64\")\n\n    def _internal_get_values(self):\n        \"\"\"\n        Return the values.\n\n        For internal compatibility with pandas formatting.\n\n        Returns\n        -------\n        np.ndarray or Index\n            A numpy array of the same dtype as categorical.categories.dtype or\n            Index if datetime / periods.\n        \"\"\"\n        # if we are a datetime and period index, return Index to keep metadata\n        if needs_i8_conversion(self.categories.dtype):\n            return self.categories.take(self._codes, fill_value=NaT)\n        elif is_integer_dtype(self.categories) and -1 in self._codes:\n            return self.categories.astype(\"object\").take(self._codes, fill_value=np.nan)\n        return np.array(self)\n\n    def check_for_ordered(self, op):\n        \"\"\" assert that we are ordered \"\"\"\n        if not self.ordered:\n            raise TypeError(\n                f\"Categorical is not ordered for operation {op}\\n\"\n                \"you can use .as_ordered() to change the \"\n                \"Categorical to an ordered one\\n\"\n            )\n\n    def argsort(self, ascending=True, kind=\"quicksort\", **kwargs):\n        \"\"\"\n        Return the indices that would sort the Categorical.\n\n        .. versionchanged:: 0.25.0\n\n           Changed to sort missing values at the end.\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            Whether the indices should result in an ascending\n            or descending sort.\n        kind : {'quicksort', 'mergesort', 'heapsort'}, optional\n            Sorting algorithm.\n        **kwargs:\n            passed through to :func:`numpy.argsort`.\n\n        Returns\n        -------\n        numpy.array\n\n        See Also\n        --------\n        numpy.ndarray.argsort\n\n        Notes\n        -----\n        While an ordering is applied to the category values, arg-sorting\n        in this context refers more to organizing and grouping together\n        based on matching category values. Thus, this function can be\n        called on an unordered Categorical instance unlike the functions\n        'Categorical.min' and 'Categorical.max'.\n\n        Examples\n        --------\n        >>> pd.Categorical(['b', 'b', 'a', 'c']).argsort()\n        array([2, 0, 1, 3])\n\n        >>> cat = pd.Categorical(['b', 'b', 'a', 'c'],\n        ...                      categories=['c', 'b', 'a'],\n        ...                      ordered=True)\n        >>> cat.argsort()\n        array([3, 0, 1, 2])\n\n        Missing values are placed at the end\n\n        >>> cat = pd.Categorical([2, None, 1])\n        >>> cat.argsort()\n        array([2, 0, 1])\n        \"\"\"\n        return super().argsort(ascending=ascending, kind=kind, **kwargs)\n\n    def sort_values(\n        self, inplace: bool = False, ascending: bool = True, na_position: str = \"last\"\n    ):\n        \"\"\"\n        Sort the Categorical by category value returning a new\n        Categorical by default.\n\n        While an ordering is applied to the category values, sorting in this\n        context refers more to organizing and grouping together based on\n        matching category values. Thus, this function can be called on an\n        unordered Categorical instance unlike the functions 'Categorical.min'\n        and 'Categorical.max'.\n\n        Parameters\n        ----------\n        inplace : bool, default False\n            Do operation in place.\n        ascending : bool, default True\n            Order ascending. Passing False orders descending. The\n            ordering parameter provides the method by which the\n            category values are organized.\n        na_position : {'first', 'last'} (optional, default='last')\n            'first' puts NaNs at the beginning\n            'last' puts NaNs at the end\n\n        Returns\n        -------\n        Categorical or None\n\n        See Also\n        --------\n        Categorical.sort\n        Series.sort_values\n\n        Examples\n        --------\n        >>> c = pd.Categorical([1, 2, 2, 1, 5])\n        >>> c\n        [1, 2, 2, 1, 5]\n        Categories (3, int64): [1, 2, 5]\n        >>> c.sort_values()\n        [1, 1, 2, 2, 5]\n        Categories (3, int64): [1, 2, 5]\n        >>> c.sort_values(ascending=False)\n        [5, 2, 2, 1, 1]\n        Categories (3, int64): [1, 2, 5]\n\n        Inplace sorting can be done as well:\n\n        >>> c.sort_values(inplace=True)\n        >>> c\n        [1, 1, 2, 2, 5]\n        Categories (3, int64): [1, 2, 5]\n        >>>\n        >>> c = pd.Categorical([1, 2, 2, 1, 5])\n\n        'sort_values' behaviour with NaNs. Note that 'na_position'\n        is independent of the 'ascending' parameter:\n\n        >>> c = pd.Categorical([np.nan, 2, 2, np.nan, 5])\n        >>> c\n        [NaN, 2, 2, NaN, 5]\n        Categories (2, int64): [2, 5]\n        >>> c.sort_values()\n        [2, 2, 5, NaN, NaN]\n        Categories (2, int64): [2, 5]\n        >>> c.sort_values(ascending=False)\n        [5, 2, 2, NaN, NaN]\n        Categories (2, int64): [2, 5]\n        >>> c.sort_values(na_position='first')\n        [NaN, NaN, 2, 2, 5]\n        Categories (2, int64): [2, 5]\n        >>> c.sort_values(ascending=False, na_position='first')\n        [NaN, NaN, 5, 2, 2]\n        Categories (2, int64): [2, 5]\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if na_position not in [\"last\", \"first\"]:\n            raise ValueError(f\"invalid na_position: {repr(na_position)}\")\n\n        sorted_idx = nargsort(self, ascending=ascending, na_position=na_position)\n\n        if inplace:\n            self._codes[:] = self._codes[sorted_idx]\n        else:\n            codes = self._codes[sorted_idx]\n            return self._from_backing_data(codes)\n\n    def _values_for_rank(self):\n        \"\"\"\n        For correctly ranking ordered categorical data. See GH#15420\n\n        Ordered categorical data should be ranked on the basis of\n        codes with -1 translated to NaN.\n\n        Returns\n        -------\n        numpy.array\n\n        \"\"\"\n        from pandas import Series\n\n        if self.ordered:\n            values = self.codes\n            mask = values == -1\n            if mask.any():\n                values = values.astype(\"float64\")\n                values[mask] = np.nan\n        elif self.categories.is_numeric():\n            values = np.array(self)\n        else:\n            #  reorder the categories (so rank can use the float codes)\n            #  instead of passing an object array to rank\n            values = np.array(\n                self.rename_categories(Series(self.categories).rank().values)\n            )\n        return values\n\n    def view(self, dtype=None):\n        if dtype is not None:\n            raise NotImplementedError(dtype)\n        return self._from_backing_data(self._ndarray)\n\n    def to_dense(self):\n        \"\"\"\n        Return my 'dense' representation\n\n        For internal compatibility with numpy arrays.\n\n        Returns\n        -------\n        dense : array\n        \"\"\"\n        warn(\n            \"Categorical.to_dense is deprecated and will be removed in \"\n            \"a future version.  Use np.asarray(cat) instead.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return np.asarray(self)\n\n    def fillna(self, value=None, method=None, limit=None):\n        \"\"\"\n        Fill NA/NaN values using the specified method.\n\n        Parameters\n        ----------\n        value : scalar, dict, Series\n            If a scalar value is passed it is used to fill all missing values.\n            Alternatively, a Series or dict can be used to fill in different\n            values for each index. The value should not be a list. The\n            value(s) passed should either be in the categories or should be\n            NaN.\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n            Method to use for filling holes in reindexed Series\n            pad / ffill: propagate last valid observation forward to next valid\n            backfill / bfill: use NEXT valid observation to fill gap\n        limit : int, default None\n            (Not implemented yet for Categorical!)\n            If method is specified, this is the maximum number of consecutive\n            NaN values to forward/backward fill. In other words, if there is\n            a gap with more than this number of consecutive NaNs, it will only\n            be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled.\n\n        Returns\n        -------\n        filled : Categorical with NA/NaN filled\n        \"\"\"\n        value, method = validate_fillna_kwargs(\n            value, method, validate_scalar_dict_value=False\n        )\n        value = extract_array(value, extract_numpy=True)\n\n        if value is None:\n            value = np.nan\n        if limit is not None:\n            raise NotImplementedError(\n                \"specifying a limit for fillna has not been implemented yet\"\n            )\n\n        if method is not None:\n            # pad / bfill\n\n            # TODO: dispatch when self.categories is EA-dtype\n            values = np.asarray(self).reshape(-1, len(self))\n            values = interpolate_2d(values, method, 0, None).astype(\n                self.categories.dtype\n            )[0]\n            codes = _get_codes_for_values(values, self.categories)\n\n        else:\n            # We copy even if there is nothing to fill\n            codes = self._ndarray.copy()\n            mask = self.isna()\n\n            if isinstance(value, (np.ndarray, Categorical)):\n                # We get ndarray or Categorical if called via Series.fillna,\n                #  where it will unwrap another aligned Series before getting here\n\n                not_categories = ~algorithms.isin(value, self.categories)\n                if not isna(value[not_categories]).all():\n                    # All entries in `value` must either be a category or NA\n                    raise ValueError(\"fill value must be in categories\")\n\n                values_codes = _get_codes_for_values(value, self.categories)\n                codes[mask] = values_codes[mask]\n\n            else:\n                new_code = self._validate_fill_value(value)\n                codes[mask] = new_code\n\n        return self._from_backing_data(codes)\n\n    # ------------------------------------------------------------------\n    # NDArrayBackedExtensionArray compat\n\n    @property\n    def _ndarray(self) -> np.ndarray:\n        return self._codes\n\n    def _from_backing_data(self, arr: np.ndarray) -> \"Categorical\":\n        return self._constructor(arr, dtype=self.dtype, fastpath=True)\n\n    def _box_func(self, i: int):\n        if i == -1:\n            return np.NaN\n        return self.categories[i]\n\n    def _validate_listlike(self, target: ArrayLike) -> np.ndarray:\n        \"\"\"\n        Extract integer codes we can use for comparison.\n\n        Notes\n        -----\n        If a value in target is not present, it gets coded as -1.\n        \"\"\"\n\n        if isinstance(target, Categorical):\n            # Indexing on codes is more efficient if categories are the same,\n            #  so we can apply some optimizations based on the degree of\n            #  dtype-matching.\n            if self.categories.equals(target.categories):\n                # We use the same codes, so can go directly to the engine\n                codes = target.codes\n            elif self.is_dtype_equal(target):\n                # We have the same categories up to a reshuffling of codes.\n                codes = recode_for_categories(\n                    target.codes, target.categories, self.categories\n                )\n            else:\n                code_indexer = self.categories.get_indexer(target.categories)\n                codes = take_1d(code_indexer, target.codes, fill_value=-1)\n        else:\n            codes = self.categories.get_indexer(target)\n\n        return codes\n\n    def _unbox_scalar(self, key) -> int:\n        # searchsorted is very performance sensitive. By converting codes\n        # to same dtype as self.codes, we get much faster performance.\n        code = self.categories.get_loc(key)\n        code = self._codes.dtype.type(code)\n        return code\n\n    def _unbox_listlike(self, value):\n        unboxed = self.categories.get_indexer(value)\n        return unboxed.astype(self._ndarray.dtype, copy=False)\n\n    # ------------------------------------------------------------------\n\n    def take_nd(self, indexer, allow_fill: bool = False, fill_value=None):\n        # GH#27745 deprecate alias that other EAs dont have\n        warn(\n            \"Categorical.take_nd is deprecated, use Categorical.take instead\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return self.take(indexer, allow_fill=allow_fill, fill_value=fill_value)\n\n    def __iter__(self):\n        \"\"\"\n        Returns an Iterator over the values of this Categorical.\n        \"\"\"\n        return iter(self._internal_get_values().tolist())\n\n    def __contains__(self, key) -> bool:\n        \"\"\"\n        Returns True if `key` is in this Categorical.\n        \"\"\"\n        # if key is a NaN, check if any NaN is in self.\n        if is_valid_nat_for_dtype(key, self.categories.dtype):\n            return self.isna().any()\n\n        return contains(self, key, container=self._codes)\n\n    # ------------------------------------------------------------------\n    # Rendering Methods\n\n    def _formatter(self, boxed=False):\n        # Defer to CategoricalFormatter's formatter.\n        return None\n\n    def _tidy_repr(self, max_vals=10, footer=True) -> str:\n        \"\"\"\n        a short repr displaying only max_vals and an optional (but default\n        footer)\n        \"\"\"\n        num = max_vals // 2\n        head = self[:num]._get_repr(length=False, footer=False)\n        tail = self[-(max_vals - num) :]._get_repr(length=False, footer=False)\n\n        result = f\"{head[:-1]}, ..., {tail[1:]}\"\n        if footer:\n            result = f\"{result}\\n{self._repr_footer()}\"\n\n        return str(result)\n\n    def _repr_categories(self):\n        \"\"\"\n        return the base repr for the categories\n        \"\"\"\n        max_categories = (\n            10\n            if get_option(\"display.max_categories\") == 0\n            else get_option(\"display.max_categories\")\n        )\n        from pandas.io.formats import format as fmt\n\n        format_array = partial(\n            fmt.format_array, formatter=None, quoting=QUOTE_NONNUMERIC\n        )\n        if len(self.categories) > max_categories:\n            num = max_categories // 2\n            head = format_array(self.categories[:num])\n            tail = format_array(self.categories[-num:])\n            category_strs = head + [\"...\"] + tail\n        else:\n            category_strs = format_array(self.categories)\n\n        # Strip all leading spaces, which format_array adds for columns...\n        category_strs = [x.strip() for x in category_strs]\n        return category_strs\n\n    def _repr_categories_info(self) -> str:\n        \"\"\"\n        Returns a string representation of the footer.\n        \"\"\"\n        category_strs = self._repr_categories()\n        dtype = str(self.categories.dtype)\n        levheader = f\"Categories ({len(self.categories)}, {dtype}): \"\n        width, height = get_terminal_size()\n        max_width = get_option(\"display.width\") or width\n        if console.in_ipython_frontend():\n            # 0 = no breaks\n            max_width = 0\n        levstring = \"\"\n        start = True\n        cur_col_len = len(levheader)  # header\n        sep_len, sep = (3, \" < \") if self.ordered else (2, \", \")\n        linesep = sep.rstrip() + \"\\n\"  # remove whitespace\n        for val in category_strs:\n            if max_width != 0 and cur_col_len + sep_len + len(val) > max_width:\n                levstring += linesep + (\" \" * (len(levheader) + 1))\n                cur_col_len = len(levheader) + 1  # header + a whitespace\n            elif not start:\n                levstring += sep\n                cur_col_len += len(val)\n            levstring += val\n            start = False\n        # replace to simple save space by\n        return levheader + \"[\" + levstring.replace(\" < ... < \", \" ... \") + \"]\"\n\n    def _repr_footer(self) -> str:\n        info = self._repr_categories_info()\n        return f\"Length: {len(self)}\\n{info}\"\n\n    def _get_repr(self, length=True, na_rep=\"NaN\", footer=True) -> str:\n        from pandas.io.formats import format as fmt\n\n        formatter = fmt.CategoricalFormatter(\n            self, length=length, na_rep=na_rep, footer=footer\n        )\n        result = formatter.to_string()\n        return str(result)\n\n    def __repr__(self) -> str:\n        \"\"\"\n        String representation.\n        \"\"\"\n        _maxlen = 10\n        if len(self._codes) > _maxlen:\n            result = self._tidy_repr(_maxlen)\n        elif len(self._codes) > 0:\n            result = self._get_repr(length=len(self) > _maxlen)\n        else:\n            msg = self._get_repr(length=False, footer=True).replace(\"\\n\", \", \")\n            result = f\"[], {msg}\"\n\n        return result\n\n    # ------------------------------------------------------------------\n\n    def __getitem__(self, key):\n        \"\"\"\n        Return an item.\n        \"\"\"\n        result = super().__getitem__(key)\n        if getattr(result, \"ndim\", 0) > 1:\n            result = result._ndarray\n            deprecate_ndim_indexing(result)\n        return result\n\n    def _validate_setitem_value(self, value):\n        value = extract_array(value, extract_numpy=True)\n\n        # require identical categories set\n        if isinstance(value, Categorical):\n            if not is_dtype_equal(self, value):\n                raise ValueError(\n                    \"Cannot set a Categorical with another, \"\n                    \"without identical categories\"\n                )\n            new_codes = self._validate_listlike(value)\n            value = Categorical.from_codes(new_codes, dtype=self.dtype)\n\n        # wrap scalars and hashable-listlikes in list\n        rvalue = value if not is_hashable(value) else [value]\n\n        from pandas import Index\n\n        to_add = Index(rvalue).difference(self.categories)\n\n        # no assignments of values not in categories, but it's always ok to set\n        # something to np.nan\n        if len(to_add) and not isna(to_add).all():\n            raise ValueError(\n                \"Cannot setitem on a Categorical with a new \"\n                \"category, set the categories first\"\n            )\n\n        return self._unbox_listlike(rvalue)\n\n    def _reverse_indexer(self) -> Dict[Hashable, np.ndarray]:\n        \"\"\"\n        Compute the inverse of a categorical, returning\n        a dict of categories -> indexers.\n\n        *This is an internal function*\n\n        Returns\n        -------\n        dict of categories -> indexers\n\n        Examples\n        --------\n        >>> c = pd.Categorical(list('aabca'))\n        >>> c\n        ['a', 'a', 'b', 'c', 'a']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> c.categories\n        Index(['a', 'b', 'c'], dtype='object')\n        >>> c.codes\n        array([0, 0, 1, 2, 0], dtype=int8)\n        >>> c._reverse_indexer()\n        {'a': array([0, 1, 4]), 'b': array([2]), 'c': array([3])}\n\n        \"\"\"\n        categories = self.categories\n        r, counts = libalgos.groupsort_indexer(\n            self.codes.astype(\"int64\"), categories.size\n        )\n        counts = counts.cumsum()\n        _result = (r[start:end] for start, end in zip(counts, counts[1:]))\n        result = dict(zip(categories, _result))\n        return result\n\n    # ------------------------------------------------------------------\n    # Reductions\n\n    @deprecate_kwarg(old_arg_name=\"numeric_only\", new_arg_name=\"skipna\")\n    def min(self, skipna=True, **kwargs):\n        \"\"\"\n        The minimum value of the object.\n\n        Only ordered `Categoricals` have a minimum!\n\n        .. versionchanged:: 1.0.0\n\n           Returns an NA value on empty arrays\n\n        Raises\n        ------\n        TypeError\n            If the `Categorical` is not `ordered`.\n\n        Returns\n        -------\n        min : the minimum of this `Categorical`\n        \"\"\"\n        nv.validate_min((), kwargs)\n        self.check_for_ordered(\"min\")\n\n        if not len(self._codes):\n            return self.dtype.na_value\n\n        good = self._codes != -1\n        if not good.all():\n            if skipna and good.any():\n                pointer = self._codes[good].min()\n            else:\n                return np.nan\n        else:\n            pointer = self._codes.min()\n        return self.categories[pointer]\n\n    @deprecate_kwarg(old_arg_name=\"numeric_only\", new_arg_name=\"skipna\")\n    def max(self, skipna=True, **kwargs):\n        \"\"\"\n        The maximum value of the object.\n\n        Only ordered `Categoricals` have a maximum!\n\n        .. versionchanged:: 1.0.0\n\n           Returns an NA value on empty arrays\n\n        Raises\n        ------\n        TypeError\n            If the `Categorical` is not `ordered`.\n\n        Returns\n        -------\n        max : the maximum of this `Categorical`\n        \"\"\"\n        nv.validate_max((), kwargs)\n        self.check_for_ordered(\"max\")\n\n        if not len(self._codes):\n            return self.dtype.na_value\n\n        good = self._codes != -1\n        if not good.all():\n            if skipna and good.any():\n                pointer = self._codes[good].max()\n            else:\n                return np.nan\n        else:\n            pointer = self._codes.max()\n        return self.categories[pointer]\n\n    def mode(self, dropna=True):\n        \"\"\"\n        Returns the mode(s) of the Categorical.\n\n        Always returns `Categorical` even if only one value.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        modes : `Categorical` (sorted)\n        \"\"\"\n        codes = self._codes\n        if dropna:\n            good = self._codes != -1\n            codes = self._codes[good]\n        codes = sorted(htable.mode_int64(ensure_int64(codes), dropna))\n        return self._from_backing_data(codes)\n\n    # ------------------------------------------------------------------\n    # ExtensionArray Interface\n\n    def unique(self):\n        \"\"\"\n        Return the ``Categorical`` which ``categories`` and ``codes`` are\n        unique. Unused categories are NOT returned.\n\n        - unordered category: values and categories are sorted by appearance\n          order.\n        - ordered category: values are sorted by appearance order, categories\n          keeps existing order.\n\n        Returns\n        -------\n        unique values : ``Categorical``\n\n        See Also\n        --------\n        pandas.unique\n        CategoricalIndex.unique\n        Series.unique : Return unique values of Series object.\n\n        Examples\n        --------\n        An unordered Categorical will return categories in the\n        order of appearance.\n\n        >>> pd.Categorical(list(\"baabc\")).unique()\n        ['b', 'a', 'c']\n        Categories (3, object): ['b', 'a', 'c']\n\n        >>> pd.Categorical(list(\"baabc\"), categories=list(\"abc\")).unique()\n        ['b', 'a', 'c']\n        Categories (3, object): ['b', 'a', 'c']\n\n        An ordered Categorical preserves the category ordering.\n\n        >>> pd.Categorical(\n        ...     list(\"baabc\"), categories=list(\"abc\"), ordered=True\n        ... ).unique()\n        ['b', 'a', 'c']\n        Categories (3, object): ['a' < 'b' < 'c']\n        \"\"\"\n        # unlike np.unique, unique1d does not sort\n        unique_codes = unique1d(self.codes)\n        cat = self.copy()\n\n        # keep nan in codes\n        cat._codes = unique_codes\n\n        # exclude nan from indexer for categories\n        take_codes = unique_codes[unique_codes != -1]\n        if self.ordered:\n            take_codes = np.sort(take_codes)\n        return cat.set_categories(cat.categories.take(take_codes))\n\n    def _values_for_factorize(self):\n        return self._ndarray, -1\n\n    @classmethod\n    def _from_factorized(cls, uniques, original):\n        return original._constructor(\n            original.categories.take(uniques), dtype=original.dtype\n        )\n\n    def equals(self, other: object) -> bool:\n        \"\"\"\n        Returns True if categorical arrays are equal.\n\n        Parameters\n        ----------\n        other : `Categorical`\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        if not isinstance(other, Categorical):\n            return False\n        elif self.is_dtype_equal(other):\n            other_codes = self._validate_listlike(other)\n            return np.array_equal(self._codes, other_codes)\n        return False\n\n    @classmethod\n    def _concat_same_type(self, to_concat):\n        from pandas.core.dtypes.concat import union_categoricals\n\n        return union_categoricals(to_concat)\n\n    # ------------------------------------------------------------------\n\n    def is_dtype_equal(self, other):\n        \"\"\"\n        Returns True if categoricals are the same dtype\n          same categories, and same ordered\n\n        Parameters\n        ----------\n        other : Categorical\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        try:\n            return hash(self.dtype) == hash(other.dtype)\n        except (AttributeError, TypeError):\n            return False\n\n    def describe(self):\n        \"\"\"\n        Describes this Categorical\n\n        Returns\n        -------\n        description: `DataFrame`\n            A dataframe with frequency and counts by category.\n        \"\"\"\n        counts = self.value_counts(dropna=False)\n        freqs = counts / float(counts.sum())\n\n        from pandas.core.reshape.concat import concat\n\n        result = concat([counts, freqs], axis=1)\n        result.columns = [\"counts\", \"freqs\"]\n        result.index.name = \"categories\"\n\n        return result\n\n    def isin(self, values) -> np.ndarray:\n        \"\"\"\n        Check whether `values` are contained in Categorical.\n\n        Return a boolean NumPy Array showing whether each element in\n        the Categorical matches an element in the passed sequence of\n        `values` exactly.\n\n        Parameters\n        ----------\n        values : set or list-like\n            The sequence of values to test. Passing in a single string will\n            raise a ``TypeError``. Instead, turn a single string into a\n            list of one element.\n\n        Returns\n        -------\n        isin : numpy.ndarray (bool dtype)\n\n        Raises\n        ------\n        TypeError\n          * If `values` is not a set or list-like\n\n        See Also\n        --------\n        pandas.Series.isin : Equivalent method on Series.\n\n        Examples\n        --------\n        >>> s = pd.Categorical(['lama', 'cow', 'lama', 'beetle', 'lama',\n        ...                'hippo'])\n        >>> s.isin(['cow', 'lama'])\n        array([ True,  True,  True, False,  True, False])\n\n        Passing a single string as ``s.isin('lama')`` will raise an error. Use\n        a list of one element instead:\n\n        >>> s.isin(['lama'])\n        array([ True, False,  True, False,  True, False])\n        \"\"\"\n        if not is_list_like(values):\n            values_type = type(values).__name__\n            raise TypeError(\n                \"only list-like objects are allowed to be passed \"\n                f\"to isin(), you passed a [{values_type}]\"\n            )\n        values = sanitize_array(values, None, None)\n        null_mask = np.asarray(isna(values))\n        code_values = self.categories.get_indexer(values)\n        code_values = code_values[null_mask | (code_values >= 0)]\n        return algorithms.isin(self.codes, code_values)\n\n    def replace(self, to_replace, value, inplace: bool = False):\n        \"\"\"\n        Replaces all instances of one value with another\n\n        Parameters\n        ----------\n        to_replace: object\n            The value to be replaced\n\n        value: object\n            The value to replace it with\n\n        inplace: bool\n            Whether the operation is done in-place\n\n        Returns\n        -------\n        None if inplace is True, otherwise the new Categorical after replacement\n\n\n        Examples\n        --------\n        >>> s = pd.Categorical([1, 2, 1, 3])\n        >>> s.replace(1, 3)\n        [3, 2, 3, 3]\n        Categories (2, int64): [2, 3]\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        cat = self if inplace else self.copy()\n\n        # build a dict of (to replace -> value) pairs\n        if is_list_like(to_replace):\n            # if to_replace is list-like and value is scalar\n            replace_dict = {replace_value: value for replace_value in to_replace}\n        else:\n            # if both to_replace and value are scalar\n            replace_dict = {to_replace: value}\n\n        # other cases, like if both to_replace and value are list-like or if\n        # to_replace is a dict, are handled separately in NDFrame\n        for replace_value, new_value in replace_dict.items():\n            if new_value == replace_value:\n                continue\n            if replace_value in cat.categories:\n                if isna(new_value):\n                    cat.remove_categories(replace_value, inplace=True)\n                    continue\n                categories = cat.categories.tolist()\n                index = categories.index(replace_value)\n                if new_value in cat.categories:\n                    value_index = categories.index(new_value)\n                    cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(replace_value, inplace=True)\n                else:\n                    categories[index] = new_value\n                    cat.rename_categories(categories, inplace=True)\n        if not inplace:\n            return cat\n\n    # ------------------------------------------------------------------------\n    # String methods interface\n    def _str_map(self, f, na_value=np.nan, dtype=np.dtype(object)):\n        # Optimization to apply the callable `f` to the categories once\n        # and rebuild the result by `take`ing from the result with the codes.\n        # Returns the same type as the object-dtype implementation though.\n        from pandas.core.arrays import PandasArray\n\n        categories = self.categories\n        codes = self.codes\n        result = PandasArray(categories.to_numpy())._str_map(f, na_value, dtype)\n        return take_1d(result, codes, fill_value=na_value)\n\n    def _str_get_dummies(self, sep=\"|\"):\n        # sep may not be in categories. Just bail on this.\n        from pandas.core.arrays import PandasArray\n\n        return PandasArray(self.astype(str))._str_get_dummies(sep)\n\n\n# The Series.cat accessor\n\n\n@delegate_names(\n    delegate=Categorical, accessors=[\"categories\", \"ordered\"], typ=\"property\"\n)\n@delegate_names(\n    delegate=Categorical,\n    accessors=[\n        \"rename_categories\",\n        \"reorder_categories\",\n        \"add_categories\",\n        \"remove_categories\",\n        \"remove_unused_categories\",\n        \"set_categories\",\n        \"as_ordered\",\n        \"as_unordered\",\n    ],\n    typ=\"method\",\n)\nclass CategoricalAccessor(PandasDelegate, PandasObject, NoNewAttributesMixin):\n    \"\"\"\n    Accessor object for categorical properties of the Series values.\n\n    Be aware that assigning to `categories` is a inplace operation, while all\n    methods return new categorical data per default (but can be called with\n    `inplace=True`).\n\n    Parameters\n    ----------\n    data : Series or CategoricalIndex\n\n    Examples\n    --------\n    >>> s = pd.Series(list(\"abbccc\")).astype(\"category\")\n    >>> s\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n\n    >>> s.cat.categories\n    Index(['a', 'b', 'c'], dtype='object')\n\n    >>> s.cat.rename_categories(list(\"cba\"))\n    0    c\n    1    b\n    2    b\n    3    a\n    4    a\n    5    a\n    dtype: category\n    Categories (3, object): ['c', 'b', 'a']\n\n    >>> s.cat.reorder_categories(list(\"cba\"))\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['c', 'b', 'a']\n\n    >>> s.cat.add_categories([\"d\", \"e\"])\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (5, object): ['a', 'b', 'c', 'd', 'e']\n\n    >>> s.cat.remove_categories([\"a\", \"c\"])\n    0    NaN\n    1      b\n    2      b\n    3    NaN\n    4    NaN\n    5    NaN\n    dtype: category\n    Categories (1, object): ['b']\n\n    >>> s1 = s.cat.add_categories([\"d\", \"e\"])\n    >>> s1.cat.remove_unused_categories()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n\n    >>> s.cat.set_categories(list(\"abcde\"))\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (5, object): ['a', 'b', 'c', 'd', 'e']\n\n    >>> s.cat.as_ordered()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a' < 'b' < 'c']\n\n    >>> s.cat.as_unordered()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n    \"\"\"\n\n    def __init__(self, data):\n        self._validate(data)\n        self._parent = data.values\n        self._index = data.index\n        self._name = data.name\n        self._freeze()\n\n    @staticmethod\n    def _validate(data):\n        if not is_categorical_dtype(data.dtype):\n            raise AttributeError(\"Can only use .cat accessor with a 'category' dtype\")\n\n    def _delegate_property_get(self, name):\n        return getattr(self._parent, name)\n\n    def _delegate_property_set(self, name, new_values):\n        return setattr(self._parent, name, new_values)\n\n    @property\n    def codes(self):\n        \"\"\"\n        Return Series of codes as well as the index.\n        \"\"\"\n        from pandas import Series\n\n        return Series(self._parent.codes, index=self._index)\n\n    def _delegate_method(self, name, *args, **kwargs):\n        from pandas import Series\n\n        method = getattr(self._parent, name)\n        res = method(*args, **kwargs)\n        if res is not None:\n            return Series(res, index=self._index, name=self._name)\n\n\n# utility routines\n\n\ndef _get_codes_for_values(values, categories):\n    \"\"\"\n    utility routine to turn values into codes given the specified categories\n    \"\"\"\n    dtype_equal = is_dtype_equal(values.dtype, categories.dtype)\n\n    if is_extension_array_dtype(categories.dtype) and is_object_dtype(values):\n        # Support inferring the correct extension dtype from an array of\n        # scalar objects. e.g.\n        # Categorical(array[Period, Period], categories=PeriodIndex(...))\n        cls = categories.dtype.construct_array_type()\n        values = maybe_cast_to_extension_array(cls, values)\n        if not isinstance(values, cls):\n            # exception raised in _from_sequence\n            values = ensure_object(values)\n            categories = ensure_object(categories)\n    elif not dtype_equal:\n        values = ensure_object(values)\n        categories = ensure_object(categories)\n\n    if isinstance(categories, ABCIndexClass):\n        return coerce_indexer_dtype(categories.get_indexer_for(values), categories)\n\n    # Only hit here when we've already coerced to object dtypee.\n\n    hash_klass, vals = get_data_algo(values)\n    _, cats = get_data_algo(categories)\n    t = hash_klass(len(cats))\n    t.map_locations(cats)\n    return coerce_indexer_dtype(t.lookup(vals), cats)\n\n\ndef recode_for_categories(codes: np.ndarray, old_categories, new_categories):\n    \"\"\"\n    Convert a set of codes for to a new set of categories\n\n    Parameters\n    ----------\n    codes : np.ndarray\n    old_categories, new_categories : Index\n\n    Returns\n    -------\n    new_codes : np.ndarray[np.int64]\n\n    Examples\n    --------\n    >>> old_cat = pd.Index(['b', 'a', 'c'])\n    >>> new_cat = pd.Index(['a', 'b'])\n    >>> codes = np.array([0, 1, 1, 2])\n    >>> recode_for_categories(codes, old_cat, new_cat)\n    array([ 1,  0,  0, -1], dtype=int8)\n    \"\"\"\n    if len(old_categories) == 0:\n        # All null anyway, so just retain the nulls\n        return codes.copy()\n    elif new_categories.equals(old_categories):\n        # Same categories, so no need to actually recode\n        return codes.copy()\n    indexer = coerce_indexer_dtype(\n        new_categories.get_indexer(old_categories), new_categories\n    )\n    new_codes = take_1d(indexer, codes.copy(), fill_value=-1)\n    return new_codes\n\n\ndef factorize_from_iterable(values):\n    \"\"\"\n    Factorize an input `values` into `categories` and `codes`. Preserves\n    categorical dtype in `categories`.\n\n    *This is an internal function*\n\n    Parameters\n    ----------\n    values : list-like\n\n    Returns\n    -------\n    codes : ndarray\n    categories : Index\n        If `values` has a categorical dtype, then `categories` is\n        a CategoricalIndex keeping the categories and order of `values`.\n    \"\"\"\n    if not is_list_like(values):\n        raise TypeError(\"Input must be list-like\")\n\n    if is_categorical_dtype(values):\n        values = extract_array(values)\n        # The Categorical we want to build has the same categories\n        # as values but its codes are by def [0, ..., len(n_categories) - 1]\n        cat_codes = np.arange(len(values.categories), dtype=values.codes.dtype)\n        categories = Categorical.from_codes(cat_codes, dtype=values.dtype)\n        codes = values.codes\n    else:\n        # The value of ordered is irrelevant since we don't use cat as such,\n        # but only the resulting categories, the order of which is independent\n        # from ordered. Set ordered to False as default. See GH #15457\n        cat = Categorical(values, ordered=False)\n        categories = cat.categories\n        codes = cat.codes\n    return codes, categories\n\n\ndef factorize_from_iterables(iterables):\n    \"\"\"\n    A higher-level wrapper over `factorize_from_iterable`.\n\n    *This is an internal function*\n\n    Parameters\n    ----------\n    iterables : list-like of list-likes\n\n    Returns\n    -------\n    codes_list : list of ndarrays\n    categories_list : list of Indexes\n\n    Notes\n    -----\n    See `factorize_from_iterable` for more info.\n    \"\"\"\n    if len(iterables) == 0:\n        # For consistency, it should return a list of 2 lists.\n        return [[], []]\n    return map(list, zip(*(factorize_from_iterable(it) for it in iterables)))\n"
    },
    {
      "filename": "pandas/core/arrays/datetimes.py",
      "content": "from datetime import datetime, time, timedelta, tzinfo\nfrom typing import Optional, Union\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import lib, tslib\nfrom pandas._libs.tslibs import (\n    BaseOffset,\n    NaT,\n    NaTType,\n    Resolution,\n    Timestamp,\n    conversion,\n    fields,\n    get_resolution,\n    iNaT,\n    ints_to_pydatetime,\n    is_date_array_normalized,\n    normalize_i8_timestamps,\n    timezones,\n    to_offset,\n    tzconversion,\n)\nfrom pandas.errors import PerformanceWarning\n\nfrom pandas.core.dtypes.common import (\n    DT64NS_DTYPE,\n    INT64_DTYPE,\n    is_bool_dtype,\n    is_categorical_dtype,\n    is_datetime64_any_dtype,\n    is_datetime64_dtype,\n    is_datetime64_ns_dtype,\n    is_datetime64tz_dtype,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float_dtype,\n    is_object_dtype,\n    is_period_dtype,\n    is_string_dtype,\n    is_timedelta64_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import DatetimeTZDtype\nfrom pandas.core.dtypes.generic import ABCIndexClass, ABCPandasArray, ABCSeries\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core.algorithms import checked_add_with_arr\nfrom pandas.core.arrays import datetimelike as dtl\nfrom pandas.core.arrays._ranges import generate_regular_range\nimport pandas.core.common as com\n\nfrom pandas.tseries.frequencies import get_period_alias\nfrom pandas.tseries.offsets import BDay, Day, Tick\n\n_midnight = time(0, 0)\n\n\ndef tz_to_dtype(tz):\n    \"\"\"\n    Return a datetime64[ns] dtype appropriate for the given timezone.\n\n    Parameters\n    ----------\n    tz : tzinfo or None\n\n    Returns\n    -------\n    np.dtype or Datetime64TZDType\n    \"\"\"\n    if tz is None:\n        return DT64NS_DTYPE\n    else:\n        return DatetimeTZDtype(tz=tz)\n\n\ndef _field_accessor(name, field, docstring=None):\n    def f(self):\n        values = self._local_timestamps()\n\n        if field in self._bool_ops:\n            if field.endswith((\"start\", \"end\")):\n                freq = self.freq\n                month_kw = 12\n                if freq:\n                    kwds = freq.kwds\n                    month_kw = kwds.get(\"startingMonth\", kwds.get(\"month\", 12))\n\n                result = fields.get_start_end_field(\n                    values, field, self.freqstr, month_kw\n                )\n            else:\n                result = fields.get_date_field(values, field)\n\n            # these return a boolean by-definition\n            return result\n\n        if field in self._object_ops:\n            result = fields.get_date_name_field(values, field)\n            result = self._maybe_mask_results(result, fill_value=None)\n\n        else:\n            result = fields.get_date_field(values, field)\n            result = self._maybe_mask_results(\n                result, fill_value=None, convert=\"float64\"\n            )\n\n        return result\n\n    f.__name__ = name\n    f.__doc__ = docstring\n    return property(f)\n\n\nclass DatetimeArray(dtl.TimelikeOps, dtl.DatelikeOps):\n    \"\"\"\n    Pandas ExtensionArray for tz-naive or tz-aware datetime data.\n\n    .. versionadded:: 0.24.0\n\n    .. warning::\n\n       DatetimeArray is currently experimental, and its API may change\n       without warning. In particular, :attr:`DatetimeArray.dtype` is\n       expected to change to always be an instance of an ``ExtensionDtype``\n       subclass.\n\n    Parameters\n    ----------\n    values : Series, Index, DatetimeArray, ndarray\n        The datetime data.\n\n        For DatetimeArray `values` (or a Series or Index boxing one),\n        `dtype` and `freq` will be extracted from `values`.\n\n    dtype : numpy.dtype or DatetimeTZDtype\n        Note that the only NumPy dtype allowed is 'datetime64[ns]'.\n    freq : str or Offset, optional\n        The frequency.\n    copy : bool, default False\n        Whether to copy the underlying array of values.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n    \"\"\"\n\n    _typ = \"datetimearray\"\n    _scalar_type = Timestamp\n    _recognized_scalars = (datetime, np.datetime64)\n    _is_recognized_dtype = is_datetime64_any_dtype\n\n    # define my properties & methods for delegation\n    _bool_ops = [\n        \"is_month_start\",\n        \"is_month_end\",\n        \"is_quarter_start\",\n        \"is_quarter_end\",\n        \"is_year_start\",\n        \"is_year_end\",\n        \"is_leap_year\",\n    ]\n    _object_ops = [\"freq\", \"tz\"]\n    _field_ops = [\n        \"year\",\n        \"month\",\n        \"day\",\n        \"hour\",\n        \"minute\",\n        \"second\",\n        \"weekofyear\",\n        \"week\",\n        \"weekday\",\n        \"dayofweek\",\n        \"dayofyear\",\n        \"quarter\",\n        \"days_in_month\",\n        \"daysinmonth\",\n        \"microsecond\",\n        \"nanosecond\",\n    ]\n    _other_ops = [\"date\", \"time\", \"timetz\"]\n    _datetimelike_ops = _field_ops + _object_ops + _bool_ops + _other_ops\n    _datetimelike_methods = [\n        \"to_period\",\n        \"tz_localize\",\n        \"tz_convert\",\n        \"normalize\",\n        \"strftime\",\n        \"round\",\n        \"floor\",\n        \"ceil\",\n        \"month_name\",\n        \"day_name\",\n    ]\n\n    # ndim is inherited from ExtensionArray, must exist to ensure\n    #  Timestamp.__richcmp__(DateTimeArray) operates pointwise\n\n    # ensure that operations with numpy arrays defer to our implementation\n    __array_priority__ = 1000\n\n    # -----------------------------------------------------------------\n    # Constructors\n\n    _dtype: Union[np.dtype, DatetimeTZDtype]\n    _freq = None\n\n    def __init__(self, values, dtype=DT64NS_DTYPE, freq=None, copy=False):\n        if isinstance(values, (ABCSeries, ABCIndexClass)):\n            values = values._values\n\n        inferred_freq = getattr(values, \"_freq\", None)\n\n        if isinstance(values, type(self)):\n            # validation\n            dtz = getattr(dtype, \"tz\", None)\n            if dtz and values.tz is None:\n                dtype = DatetimeTZDtype(tz=dtype.tz)\n            elif dtz and values.tz:\n                if not timezones.tz_compare(dtz, values.tz):\n                    msg = (\n                        \"Timezone of the array and 'dtype' do not match. \"\n                        f\"'{dtz}' != '{values.tz}'\"\n                    )\n                    raise TypeError(msg)\n            elif values.tz:\n                dtype = values.dtype\n\n            if freq is None:\n                freq = values.freq\n            values = values._data\n\n        if not isinstance(values, np.ndarray):\n            raise ValueError(\n                f\"Unexpected type '{type(values).__name__}'. 'values' must be \"\n                \"a DatetimeArray ndarray, or Series or Index containing one of those.\"\n            )\n        if values.ndim not in [1, 2]:\n            raise ValueError(\"Only 1-dimensional input arrays are supported.\")\n\n        if values.dtype == \"i8\":\n            # for compat with datetime/timedelta/period shared methods,\n            #  we can sometimes get here with int64 values.  These represent\n            #  nanosecond UTC (or tz-naive) unix timestamps\n            values = values.view(DT64NS_DTYPE)\n\n        if values.dtype != DT64NS_DTYPE:\n            raise ValueError(\n                \"The dtype of 'values' is incorrect. Must be 'datetime64[ns]'. \"\n                f\"Got {values.dtype} instead.\"\n            )\n\n        dtype = _validate_dt64_dtype(dtype)\n\n        if freq == \"infer\":\n            raise ValueError(\n                \"Frequency inference not allowed in DatetimeArray.__init__. \"\n                \"Use 'pd.array()' instead.\"\n            )\n\n        if copy:\n            values = values.copy()\n        if freq:\n            freq = to_offset(freq)\n        if getattr(dtype, \"tz\", None):\n            # https://github.com/pandas-dev/pandas/issues/18595\n            # Ensure that we have a standard timezone for pytz objects.\n            # Without this, things like adding an array of timedeltas and\n            # a  tz-aware Timestamp (with a tz specific to its datetime) will\n            # be incorrect(ish?) for the array as a whole\n            dtype = DatetimeTZDtype(tz=timezones.tz_standardize(dtype.tz))\n\n        self._data = values\n        self._dtype = dtype\n        self._freq = freq\n\n        if inferred_freq is None and freq is not None:\n            type(self)._validate_frequency(self, freq)\n\n    @classmethod\n    def _simple_new(\n        cls, values, freq: Optional[BaseOffset] = None, dtype=DT64NS_DTYPE\n    ) -> \"DatetimeArray\":\n        assert isinstance(values, np.ndarray)\n        if values.dtype != DT64NS_DTYPE:\n            assert values.dtype == \"i8\"\n            values = values.view(DT64NS_DTYPE)\n\n        result = object.__new__(cls)\n        result._data = values\n        result._freq = freq\n        result._dtype = dtype\n        return result\n\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy: bool = False):\n        return cls._from_sequence_not_strict(scalars, dtype=dtype, copy=copy)\n\n    @classmethod\n    def _from_sequence_not_strict(\n        cls,\n        data,\n        dtype=None,\n        copy=False,\n        tz=None,\n        freq=lib.no_default,\n        dayfirst=False,\n        yearfirst=False,\n        ambiguous=\"raise\",\n    ):\n        explicit_none = freq is None\n        freq = freq if freq is not lib.no_default else None\n\n        freq, freq_infer = dtl.maybe_infer_freq(freq)\n\n        subarr, tz, inferred_freq = sequence_to_dt64ns(\n            data,\n            dtype=dtype,\n            copy=copy,\n            tz=tz,\n            dayfirst=dayfirst,\n            yearfirst=yearfirst,\n            ambiguous=ambiguous,\n        )\n\n        freq, freq_infer = dtl.validate_inferred_freq(freq, inferred_freq, freq_infer)\n        if explicit_none:\n            freq = None\n\n        dtype = tz_to_dtype(tz)\n        result = cls._simple_new(subarr, freq=freq, dtype=dtype)\n\n        if inferred_freq is None and freq is not None:\n            # this condition precludes `freq_infer`\n            cls._validate_frequency(result, freq, ambiguous=ambiguous)\n\n        elif freq_infer:\n            # Set _freq directly to bypass duplicative _validate_frequency\n            # check.\n            result._freq = to_offset(result.inferred_freq)\n\n        return result\n\n    @classmethod\n    def _generate_range(\n        cls,\n        start,\n        end,\n        periods,\n        freq,\n        tz=None,\n        normalize=False,\n        ambiguous=\"raise\",\n        nonexistent=\"raise\",\n        closed=None,\n    ):\n\n        periods = dtl.validate_periods(periods)\n        if freq is None and any(x is None for x in [periods, start, end]):\n            raise ValueError(\"Must provide freq argument if no data is supplied\")\n\n        if com.count_not_none(start, end, periods, freq) != 3:\n            raise ValueError(\n                \"Of the four parameters: start, end, periods, \"\n                \"and freq, exactly three must be specified\"\n            )\n        freq = to_offset(freq)\n\n        if start is not None:\n            start = Timestamp(start)\n\n        if end is not None:\n            end = Timestamp(end)\n\n        if start is NaT or end is NaT:\n            raise ValueError(\"Neither `start` nor `end` can be NaT\")\n\n        left_closed, right_closed = dtl.validate_endpoints(closed)\n        start, end, _normalized = _maybe_normalize_endpoints(start, end, normalize)\n        tz = _infer_tz_from_endpoints(start, end, tz)\n\n        if tz is not None:\n            # Localize the start and end arguments\n            start_tz = None if start is None else start.tz\n            end_tz = None if end is None else end.tz\n            start = _maybe_localize_point(\n                start, start_tz, start, freq, tz, ambiguous, nonexistent\n            )\n            end = _maybe_localize_point(\n                end, end_tz, end, freq, tz, ambiguous, nonexistent\n            )\n        if freq is not None:\n            # We break Day arithmetic (fixed 24 hour) here and opt for\n            # Day to mean calendar day (23/24/25 hour). Therefore, strip\n            # tz info from start and day to avoid DST arithmetic\n            if isinstance(freq, Day):\n                if start is not None:\n                    start = start.tz_localize(None)\n                if end is not None:\n                    end = end.tz_localize(None)\n\n            if isinstance(freq, Tick):\n                values = generate_regular_range(start, end, periods, freq)\n            else:\n                xdr = generate_range(start=start, end=end, periods=periods, offset=freq)\n                values = np.array([x.value for x in xdr], dtype=np.int64)\n\n            _tz = start.tz if start is not None else end.tz\n            index = cls._simple_new(values, freq=freq, dtype=tz_to_dtype(_tz))\n\n            if tz is not None and index.tz is None:\n                arr = tzconversion.tz_localize_to_utc(\n                    index.asi8, tz, ambiguous=ambiguous, nonexistent=nonexistent\n                )\n\n                index = cls(arr)\n\n                # index is localized datetime64 array -> have to convert\n                # start/end as well to compare\n                if start is not None:\n                    start = start.tz_localize(tz, ambiguous, nonexistent).asm8\n                if end is not None:\n                    end = end.tz_localize(tz, ambiguous, nonexistent).asm8\n        else:\n            # Create a linearly spaced date_range in local time\n            # Nanosecond-granularity timestamps aren't always correctly\n            # representable with doubles, so we limit the range that we\n            # pass to np.linspace as much as possible\n            arr = (\n                np.linspace(0, end.value - start.value, periods, dtype=\"int64\")\n                + start.value\n            )\n            dtype = tz_to_dtype(tz)\n            index = cls._simple_new(\n                arr.astype(\"M8[ns]\", copy=False), freq=None, dtype=dtype\n            )\n\n        if not left_closed and len(index) and index[0] == start:\n            index = index[1:]\n        if not right_closed and len(index) and index[-1] == end:\n            index = index[:-1]\n\n        dtype = tz_to_dtype(tz)\n        return cls._simple_new(index.asi8, freq=freq, dtype=dtype)\n\n    # -----------------------------------------------------------------\n    # DatetimeLike Interface\n\n    @classmethod\n    def _rebox_native(cls, value: int) -> np.datetime64:\n        return np.int64(value).view(\"M8[ns]\")\n\n    def _unbox_scalar(self, value, setitem: bool = False):\n        if not isinstance(value, self._scalar_type) and value is not NaT:\n            raise ValueError(\"'value' should be a Timestamp.\")\n        if not isna(value):\n            self._check_compatible_with(value, setitem=setitem)\n        return value.value\n\n    def _scalar_from_string(self, value):\n        return Timestamp(value, tz=self.tz)\n\n    def _check_compatible_with(self, other, setitem: bool = False):\n        if other is NaT:\n            return\n        self._assert_tzawareness_compat(other)\n        if setitem:\n            # Stricter check for setitem vs comparison methods\n            if not timezones.tz_compare(self.tz, other.tz):\n                raise ValueError(f\"Timezones don't match. '{self.tz} != {other.tz}'\")\n\n    def _maybe_clear_freq(self):\n        self._freq = None\n\n    # -----------------------------------------------------------------\n    # Descriptive Properties\n\n    def _box_func(self, x) -> Union[Timestamp, NaTType]:\n        return Timestamp(x, freq=self.freq, tz=self.tz)\n\n    @property\n    def dtype(self) -> Union[np.dtype, DatetimeTZDtype]:\n        \"\"\"\n        The dtype for the DatetimeArray.\n\n        .. warning::\n\n           A future version of pandas will change dtype to never be a\n           ``numpy.dtype``. Instead, :attr:`DatetimeArray.dtype` will\n           always be an instance of an ``ExtensionDtype`` subclass.\n\n        Returns\n        -------\n        numpy.dtype or DatetimeTZDtype\n            If the values are tz-naive, then ``np.dtype('datetime64[ns]')``\n            is returned.\n\n            If the values are tz-aware, then the ``DatetimeTZDtype``\n            is returned.\n        \"\"\"\n        return self._dtype\n\n    @property\n    def tz(self):\n        \"\"\"\n        Return timezone, if any.\n\n        Returns\n        -------\n        datetime.tzinfo, pytz.tzinfo.BaseTZInfo, dateutil.tz.tz.tzfile, or None\n            Returns None when the array is tz-naive.\n        \"\"\"\n        # GH 18595\n        return getattr(self.dtype, \"tz\", None)\n\n    @tz.setter\n    def tz(self, value):\n        # GH 3746: Prevent localizing or converting the index by setting tz\n        raise AttributeError(\n            \"Cannot directly set timezone. Use tz_localize() \"\n            \"or tz_convert() as appropriate\"\n        )\n\n    @property\n    def tzinfo(self):\n        \"\"\"\n        Alias for tz attribute\n        \"\"\"\n        return self.tz\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def is_normalized(self):\n        \"\"\"\n        Returns True if all of the dates are at midnight (\"no time\")\n        \"\"\"\n        return is_date_array_normalized(self.asi8, self.tz)\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _resolution_obj(self) -> Resolution:\n        return get_resolution(self.asi8, self.tz)\n\n    # ----------------------------------------------------------------\n    # Array-Like / EA-Interface Methods\n\n    def __array__(self, dtype=None) -> np.ndarray:\n        if dtype is None and self.tz:\n            # The default for tz-aware is object, to preserve tz info\n            dtype = object\n\n        return super().__array__(dtype=dtype)\n\n    def __iter__(self):\n        \"\"\"\n        Return an iterator over the boxed values\n\n        Yields\n        ------\n        tstamp : Timestamp\n        \"\"\"\n        if self.ndim > 1:\n            return (self[n] for n in range(len(self)))\n        else:\n            # convert in chunks of 10k for efficiency\n            data = self.asi8\n            length = len(self)\n            chunksize = 10000\n            chunks = int(length / chunksize) + 1\n            for i in range(chunks):\n                start_i = i * chunksize\n                end_i = min((i + 1) * chunksize, length)\n                converted = ints_to_pydatetime(\n                    data[start_i:end_i], tz=self.tz, freq=self.freq, box=\"timestamp\"\n                )\n                yield from converted\n\n    def astype(self, dtype, copy=True):\n        # We handle\n        #   --> datetime\n        #   --> period\n        # DatetimeLikeArrayMixin Super handles the rest.\n        dtype = pandas_dtype(dtype)\n\n        if is_datetime64_ns_dtype(dtype) and not is_dtype_equal(dtype, self.dtype):\n            # GH#18951: datetime64_ns dtype but not equal means different tz\n            new_tz = getattr(dtype, \"tz\", None)\n            if getattr(self.dtype, \"tz\", None) is None:\n                return self.tz_localize(new_tz)\n            result = self.tz_convert(new_tz)\n            if copy:\n                result = result.copy()\n            if new_tz is None:\n                # Do we want .astype('datetime64[ns]') to be an ndarray.\n                # The astype in Block._astype expects this to return an\n                # ndarray, but we could maybe work around it there.\n                result = result._data\n            return result\n        elif is_datetime64tz_dtype(self.dtype) and is_dtype_equal(self.dtype, dtype):\n            if copy:\n                return self.copy()\n            return self\n        elif is_period_dtype(dtype):\n            return self.to_period(freq=dtype.freq)\n        return dtl.DatetimeLikeArrayMixin.astype(self, dtype, copy)\n\n    # -----------------------------------------------------------------\n    # Rendering Methods\n\n    def _format_native_types(self, na_rep=\"NaT\", date_format=None, **kwargs):\n        from pandas.io.formats.format import get_format_datetime64_from_values\n\n        fmt = get_format_datetime64_from_values(self, date_format)\n\n        return tslib.format_array_from_datetime(\n            self.asi8.ravel(), tz=self.tz, format=fmt, na_rep=na_rep\n        ).reshape(self.shape)\n\n    # -----------------------------------------------------------------\n    # Comparison Methods\n\n    def _has_same_tz(self, other) -> bool:\n\n        # vzone shouldn't be None if value is non-datetime like\n        if isinstance(other, np.datetime64):\n            # convert to Timestamp as np.datetime64 doesn't have tz attr\n            other = Timestamp(other)\n\n        if not hasattr(other, \"tzinfo\"):\n            return False\n        other_tz = other.tzinfo\n        return timezones.tz_compare(self.tzinfo, other_tz)\n\n    def _assert_tzawareness_compat(self, other):\n        # adapted from _Timestamp._assert_tzawareness_compat\n        other_tz = getattr(other, \"tzinfo\", None)\n        other_dtype = getattr(other, \"dtype\", None)\n\n        if is_datetime64tz_dtype(other_dtype):\n            # Get tzinfo from Series dtype\n            other_tz = other.dtype.tz\n        if other is NaT:\n            # pd.NaT quacks both aware and naive\n            pass\n        elif self.tz is None:\n            if other_tz is not None:\n                raise TypeError(\n                    \"Cannot compare tz-naive and tz-aware datetime-like objects.\"\n                )\n        elif other_tz is None:\n            raise TypeError(\n                \"Cannot compare tz-naive and tz-aware datetime-like objects\"\n            )\n\n    # -----------------------------------------------------------------\n    # Arithmetic Methods\n\n    def _sub_datetime_arraylike(self, other):\n        \"\"\"subtract DatetimeArray/Index or ndarray[datetime64]\"\"\"\n        if len(self) != len(other):\n            raise ValueError(\"cannot add indices of unequal length\")\n\n        if isinstance(other, np.ndarray):\n            assert is_datetime64_dtype(other)\n            other = type(self)(other)\n\n        if not self._has_same_tz(other):\n            # require tz compat\n            raise TypeError(\n                f\"{type(self).__name__} subtraction must have the same \"\n                \"timezones or no timezones\"\n            )\n\n        self_i8 = self.asi8\n        other_i8 = other.asi8\n        arr_mask = self._isnan | other._isnan\n        new_values = checked_add_with_arr(self_i8, -other_i8, arr_mask=arr_mask)\n        if self._hasnans or other._hasnans:\n            new_values[arr_mask] = iNaT\n        return new_values.view(\"timedelta64[ns]\")\n\n    def _add_offset(self, offset):\n        if self.ndim == 2:\n            return self.ravel()._add_offset(offset).reshape(self.shape)\n\n        assert not isinstance(offset, Tick)\n        try:\n            if self.tz is not None:\n                values = self.tz_localize(None)\n            else:\n                values = self\n            result = offset._apply_array(values)\n            result = DatetimeArray._simple_new(result)\n            result = result.tz_localize(self.tz)\n\n        except NotImplementedError:\n            warnings.warn(\n                \"Non-vectorized DateOffset being applied to Series or DatetimeIndex\",\n                PerformanceWarning,\n            )\n            result = self.astype(\"O\") + offset\n            if not len(self):\n                # GH#30336 _from_sequence won't be able to infer self.tz\n                return type(self)._from_sequence(result).tz_localize(self.tz)\n\n        return type(self)._from_sequence(result)\n\n    def _sub_datetimelike_scalar(self, other):\n        # subtract a datetime from myself, yielding a ndarray[timedelta64[ns]]\n        assert isinstance(other, (datetime, np.datetime64))\n        assert other is not NaT\n        other = Timestamp(other)\n        if other is NaT:\n            return self - NaT\n\n        if not self._has_same_tz(other):\n            # require tz compat\n            raise TypeError(\n                \"Timestamp subtraction must have the same timezones or no timezones\"\n            )\n\n        i8 = self.asi8\n        result = checked_add_with_arr(i8, -other.value, arr_mask=self._isnan)\n        result = self._maybe_mask_results(result)\n        return result.view(\"timedelta64[ns]\")\n\n    # -----------------------------------------------------------------\n    # Timezone Conversion and Localization Methods\n\n    def _local_timestamps(self):\n        \"\"\"\n        Convert to an i8 (unix-like nanosecond timestamp) representation\n        while keeping the local timezone and not using UTC.\n        This is used to calculate time-of-day information as if the timestamps\n        were timezone-naive.\n        \"\"\"\n        if self.tz is None or timezones.is_utc(self.tz):\n            return self.asi8\n        return tzconversion.tz_convert_from_utc(self.asi8, self.tz)\n\n    def tz_convert(self, tz):\n        \"\"\"\n        Convert tz-aware Datetime Array/Index from one time zone to another.\n\n        Parameters\n        ----------\n        tz : str, pytz.timezone, dateutil.tz.tzfile or None\n            Time zone for time. Corresponding timestamps would be converted\n            to this time zone of the Datetime Array/Index. A `tz` of None will\n            convert to UTC and remove the timezone information.\n\n        Returns\n        -------\n        Array or Index\n\n        Raises\n        ------\n        TypeError\n            If Datetime Array/Index is tz-naive.\n\n        See Also\n        --------\n        DatetimeIndex.tz : A timezone that has a variable offset from UTC.\n        DatetimeIndex.tz_localize : Localize tz-naive DatetimeIndex to a\n            given time zone, or remove timezone from a tz-aware DatetimeIndex.\n\n        Examples\n        --------\n        With the `tz` parameter, we can change the DatetimeIndex\n        to other time zones:\n\n        >>> dti = pd.date_range(start='2014-08-01 09:00',\n        ...                     freq='H', periods=3, tz='Europe/Berlin')\n\n        >>> dti\n        DatetimeIndex(['2014-08-01 09:00:00+02:00',\n                       '2014-08-01 10:00:00+02:00',\n                       '2014-08-01 11:00:00+02:00'],\n                      dtype='datetime64[ns, Europe/Berlin]', freq='H')\n\n        >>> dti.tz_convert('US/Central')\n        DatetimeIndex(['2014-08-01 02:00:00-05:00',\n                       '2014-08-01 03:00:00-05:00',\n                       '2014-08-01 04:00:00-05:00'],\n                      dtype='datetime64[ns, US/Central]', freq='H')\n\n        With the ``tz=None``, we can remove the timezone (after converting\n        to UTC if necessary):\n\n        >>> dti = pd.date_range(start='2014-08-01 09:00', freq='H',\n        ...                     periods=3, tz='Europe/Berlin')\n\n        >>> dti\n        DatetimeIndex(['2014-08-01 09:00:00+02:00',\n                       '2014-08-01 10:00:00+02:00',\n                       '2014-08-01 11:00:00+02:00'],\n                        dtype='datetime64[ns, Europe/Berlin]', freq='H')\n\n        >>> dti.tz_convert(None)\n        DatetimeIndex(['2014-08-01 07:00:00',\n                       '2014-08-01 08:00:00',\n                       '2014-08-01 09:00:00'],\n                        dtype='datetime64[ns]', freq='H')\n        \"\"\"\n        tz = timezones.maybe_get_tz(tz)\n\n        if self.tz is None:\n            # tz naive, use tz_localize\n            raise TypeError(\n                \"Cannot convert tz-naive timestamps, use tz_localize to localize\"\n            )\n\n        # No conversion since timestamps are all UTC to begin with\n        dtype = tz_to_dtype(tz)\n        return self._simple_new(self.asi8, dtype=dtype, freq=self.freq)\n\n    def tz_localize(self, tz, ambiguous=\"raise\", nonexistent=\"raise\"):\n        \"\"\"\n        Localize tz-naive Datetime Array/Index to tz-aware\n        Datetime Array/Index.\n\n        This method takes a time zone (tz) naive Datetime Array/Index object\n        and makes this time zone aware. It does not move the time to another\n        time zone.\n        Time zone localization helps to switch from time zone aware to time\n        zone unaware objects.\n\n        Parameters\n        ----------\n        tz : str, pytz.timezone, dateutil.tz.tzfile or None\n            Time zone to convert timestamps to. Passing ``None`` will\n            remove the time zone information preserving local time.\n        ambiguous : 'infer', 'NaT', bool array, default 'raise'\n            When clocks moved backward due to DST, ambiguous times may arise.\n            For example in Central European Time (UTC+01), when going from\n            03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at\n            00:30:00 UTC and at 01:30:00 UTC. In such a situation, the\n            `ambiguous` parameter dictates how ambiguous times should be\n            handled.\n\n            - 'infer' will attempt to infer fall dst-transition hours based on\n              order\n            - bool-ndarray where True signifies a DST time, False signifies a\n              non-DST time (note that this flag is only applicable for\n              ambiguous times)\n            - 'NaT' will return NaT where there are ambiguous times\n            - 'raise' will raise an AmbiguousTimeError if there are ambiguous\n              times.\n\n        nonexistent : 'shift_forward', 'shift_backward, 'NaT', timedelta, \\\ndefault 'raise'\n            A nonexistent time does not exist in a particular timezone\n            where clocks moved forward due to DST.\n\n            - 'shift_forward' will shift the nonexistent time forward to the\n              closest existing time\n            - 'shift_backward' will shift the nonexistent time backward to the\n              closest existing time\n            - 'NaT' will return NaT where there are nonexistent times\n            - timedelta objects will shift nonexistent times by the timedelta\n            - 'raise' will raise an NonExistentTimeError if there are\n              nonexistent times.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        Same type as self\n            Array/Index converted to the specified time zone.\n\n        Raises\n        ------\n        TypeError\n            If the Datetime Array/Index is tz-aware and tz is not None.\n\n        See Also\n        --------\n        DatetimeIndex.tz_convert : Convert tz-aware DatetimeIndex from\n            one time zone to another.\n\n        Examples\n        --------\n        >>> tz_naive = pd.date_range('2018-03-01 09:00', periods=3)\n        >>> tz_naive\n        DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n                       '2018-03-03 09:00:00'],\n                      dtype='datetime64[ns]', freq='D')\n\n        Localize DatetimeIndex in US/Eastern time zone:\n\n        >>> tz_aware = tz_naive.tz_localize(tz='US/Eastern')\n        >>> tz_aware\n        DatetimeIndex(['2018-03-01 09:00:00-05:00',\n                       '2018-03-02 09:00:00-05:00',\n                       '2018-03-03 09:00:00-05:00'],\n                      dtype='datetime64[ns, US/Eastern]', freq=None)\n\n        With the ``tz=None``, we can remove the time zone information\n        while keeping the local time (not converted to UTC):\n\n        >>> tz_aware.tz_localize(None)\n        DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n                       '2018-03-03 09:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n\n        Be careful with DST changes. When there is sequential data, pandas can\n        infer the DST time:\n\n        >>> s = pd.to_datetime(pd.Series(['2018-10-28 01:30:00',\n        ...                               '2018-10-28 02:00:00',\n        ...                               '2018-10-28 02:30:00',\n        ...                               '2018-10-28 02:00:00',\n        ...                               '2018-10-28 02:30:00',\n        ...                               '2018-10-28 03:00:00',\n        ...                               '2018-10-28 03:30:00']))\n        >>> s.dt.tz_localize('CET', ambiguous='infer')\n        0   2018-10-28 01:30:00+02:00\n        1   2018-10-28 02:00:00+02:00\n        2   2018-10-28 02:30:00+02:00\n        3   2018-10-28 02:00:00+01:00\n        4   2018-10-28 02:30:00+01:00\n        5   2018-10-28 03:00:00+01:00\n        6   2018-10-28 03:30:00+01:00\n        dtype: datetime64[ns, CET]\n\n        In some cases, inferring the DST is impossible. In such cases, you can\n        pass an ndarray to the ambiguous parameter to set the DST explicitly\n\n        >>> s = pd.to_datetime(pd.Series(['2018-10-28 01:20:00',\n        ...                               '2018-10-28 02:36:00',\n        ...                               '2018-10-28 03:46:00']))\n        >>> s.dt.tz_localize('CET', ambiguous=np.array([True, True, False]))\n        0   2018-10-28 01:20:00+02:00\n        1   2018-10-28 02:36:00+02:00\n        2   2018-10-28 03:46:00+01:00\n        dtype: datetime64[ns, CET]\n\n        If the DST transition causes nonexistent times, you can shift these\n        dates forward or backwards with a timedelta object or `'shift_forward'`\n        or `'shift_backwards'`.\n\n        >>> s = pd.to_datetime(pd.Series(['2015-03-29 02:30:00',\n        ...                               '2015-03-29 03:30:00']))\n        >>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_forward')\n        0   2015-03-29 03:00:00+02:00\n        1   2015-03-29 03:30:00+02:00\n        dtype: datetime64[ns, Europe/Warsaw]\n\n        >>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_backward')\n        0   2015-03-29 01:59:59.999999999+01:00\n        1   2015-03-29 03:30:00+02:00\n        dtype: datetime64[ns, Europe/Warsaw]\n\n        >>> s.dt.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H'))\n        0   2015-03-29 03:30:00+02:00\n        1   2015-03-29 03:30:00+02:00\n        dtype: datetime64[ns, Europe/Warsaw]\n        \"\"\"\n        nonexistent_options = (\"raise\", \"NaT\", \"shift_forward\", \"shift_backward\")\n        if nonexistent not in nonexistent_options and not isinstance(\n            nonexistent, timedelta\n        ):\n            raise ValueError(\n                \"The nonexistent argument must be one of 'raise', \"\n                \"'NaT', 'shift_forward', 'shift_backward' or \"\n                \"a timedelta object\"\n            )\n\n        if self.tz is not None:\n            if tz is None:\n                new_dates = tzconversion.tz_convert_from_utc(self.asi8, self.tz)\n            else:\n                raise TypeError(\"Already tz-aware, use tz_convert to convert.\")\n        else:\n            tz = timezones.maybe_get_tz(tz)\n            # Convert to UTC\n\n            new_dates = tzconversion.tz_localize_to_utc(\n                self.asi8, tz, ambiguous=ambiguous, nonexistent=nonexistent\n            )\n        new_dates = new_dates.view(DT64NS_DTYPE)\n        dtype = tz_to_dtype(tz)\n\n        freq = None\n        if timezones.is_utc(tz) or (len(self) == 1 and not isna(new_dates[0])):\n            # we can preserve freq\n            # TODO: Also for fixed-offsets\n            freq = self.freq\n        elif tz is None and self.tz is None:\n            # no-op\n            freq = self.freq\n        return self._simple_new(new_dates, dtype=dtype, freq=freq)\n\n    # ----------------------------------------------------------------\n    # Conversion Methods - Vectorized analogues of Timestamp methods\n\n    def to_pydatetime(self) -> np.ndarray:\n        \"\"\"\n        Return Datetime Array/Index as object ndarray of datetime.datetime\n        objects.\n\n        Returns\n        -------\n        datetimes : ndarray\n        \"\"\"\n        return ints_to_pydatetime(self.asi8, tz=self.tz)\n\n    def normalize(self):\n        \"\"\"\n        Convert times to midnight.\n\n        The time component of the date-time is converted to midnight i.e.\n        00:00:00. This is useful in cases, when the time does not matter.\n        Length is unaltered. The timezones are unaffected.\n\n        This method is available on Series with datetime values under\n        the ``.dt`` accessor, and directly on Datetime Array/Index.\n\n        Returns\n        -------\n        DatetimeArray, DatetimeIndex or Series\n            The same type as the original data. Series will have the same\n            name and index. DatetimeIndex will have the same name.\n\n        See Also\n        --------\n        floor : Floor the datetimes to the specified freq.\n        ceil : Ceil the datetimes to the specified freq.\n        round : Round the datetimes to the specified freq.\n\n        Examples\n        --------\n        >>> idx = pd.date_range(start='2014-08-01 10:00', freq='H',\n        ...                     periods=3, tz='Asia/Calcutta')\n        >>> idx\n        DatetimeIndex(['2014-08-01 10:00:00+05:30',\n                       '2014-08-01 11:00:00+05:30',\n                       '2014-08-01 12:00:00+05:30'],\n                        dtype='datetime64[ns, Asia/Calcutta]', freq='H')\n        >>> idx.normalize()\n        DatetimeIndex(['2014-08-01 00:00:00+05:30',\n                       '2014-08-01 00:00:00+05:30',\n                       '2014-08-01 00:00:00+05:30'],\n                       dtype='datetime64[ns, Asia/Calcutta]', freq=None)\n        \"\"\"\n        new_values = normalize_i8_timestamps(self.asi8, self.tz)\n        return type(self)(new_values)._with_freq(\"infer\").tz_localize(self.tz)\n\n    def to_period(self, freq=None):\n        \"\"\"\n        Cast to PeriodArray/Index at a particular frequency.\n\n        Converts DatetimeArray/Index to PeriodArray/Index.\n\n        Parameters\n        ----------\n        freq : str or Offset, optional\n            One of pandas' :ref:`offset strings <timeseries.offset_aliases>`\n            or an Offset object. Will be inferred by default.\n\n        Returns\n        -------\n        PeriodArray/Index\n\n        Raises\n        ------\n        ValueError\n            When converting a DatetimeArray/Index with non-regular values,\n            so that a frequency cannot be inferred.\n\n        See Also\n        --------\n        PeriodIndex: Immutable ndarray holding ordinal values.\n        DatetimeIndex.to_pydatetime: Return DatetimeIndex as object.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"y\": [1, 2, 3]},\n        ...                   index=pd.to_datetime([\"2000-03-31 00:00:00\",\n        ...                                         \"2000-05-31 00:00:00\",\n        ...                                         \"2000-08-31 00:00:00\"]))\n        >>> df.index.to_period(\"M\")\n        PeriodIndex(['2000-03', '2000-05', '2000-08'],\n                    dtype='period[M]', freq='M')\n\n        Infer the daily frequency\n\n        >>> idx = pd.date_range(\"2017-01-01\", periods=2)\n        >>> idx.to_period()\n        PeriodIndex(['2017-01-01', '2017-01-02'],\n                    dtype='period[D]', freq='D')\n        \"\"\"\n        from pandas.core.arrays import PeriodArray\n\n        if self.tz is not None:\n            warnings.warn(\n                \"Converting to PeriodArray/Index representation \"\n                \"will drop timezone information.\",\n                UserWarning,\n            )\n\n        if freq is None:\n            freq = self.freqstr or self.inferred_freq\n\n            if freq is None:\n                raise ValueError(\n                    \"You must pass a freq argument as current index has none.\"\n                )\n\n            res = get_period_alias(freq)\n\n            #  https://github.com/pandas-dev/pandas/issues/33358\n            if res is None:\n                res = freq\n\n            freq = res\n\n        return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)\n\n    def to_perioddelta(self, freq):\n        \"\"\"\n        Calculate TimedeltaArray of difference between index\n        values and index converted to PeriodArray at specified\n        freq. Used for vectorized offsets.\n\n        Parameters\n        ----------\n        freq : Period frequency\n\n        Returns\n        -------\n        TimedeltaArray/Index\n        \"\"\"\n        # Deprecaation GH#34853\n        warnings.warn(\n            \"to_perioddelta is deprecated and will be removed in a \"\n            \"future version.  \"\n            \"Use `dtindex - dtindex.to_period(freq).to_timestamp()` instead\",\n            FutureWarning,\n            stacklevel=3,\n        )\n        from pandas.core.arrays.timedeltas import TimedeltaArray\n\n        i8delta = self.asi8 - self.to_period(freq).to_timestamp().asi8\n        m8delta = i8delta.view(\"m8[ns]\")\n        return TimedeltaArray(m8delta)\n\n    # -----------------------------------------------------------------\n    # Properties - Vectorized Timestamp Properties/Methods\n\n    def month_name(self, locale=None):\n        \"\"\"\n        Return the month names of the DateTimeIndex with specified locale.\n\n        Parameters\n        ----------\n        locale : str, optional\n            Locale determining the language in which to return the month name.\n            Default is English locale.\n\n        Returns\n        -------\n        Index\n            Index of month names.\n\n        Examples\n        --------\n        >>> idx = pd.date_range(start='2018-01', freq='M', periods=3)\n        >>> idx\n        DatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31'],\n                      dtype='datetime64[ns]', freq='M')\n        >>> idx.month_name()\n        Index(['January', 'February', 'March'], dtype='object')\n        \"\"\"\n        values = self._local_timestamps()\n\n        result = fields.get_date_name_field(values, \"month_name\", locale=locale)\n        result = self._maybe_mask_results(result, fill_value=None)\n        return result\n\n    def day_name(self, locale=None):\n        \"\"\"\n        Return the day names of the DateTimeIndex with specified locale.\n\n        Parameters\n        ----------\n        locale : str, optional\n            Locale determining the language in which to return the day name.\n            Default is English locale.\n\n        Returns\n        -------\n        Index\n            Index of day names.\n\n        Examples\n        --------\n        >>> idx = pd.date_range(start='2018-01-01', freq='D', periods=3)\n        >>> idx\n        DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03'],\n                      dtype='datetime64[ns]', freq='D')\n        >>> idx.day_name()\n        Index(['Monday', 'Tuesday', 'Wednesday'], dtype='object')\n        \"\"\"\n        values = self._local_timestamps()\n\n        result = fields.get_date_name_field(values, \"day_name\", locale=locale)\n        result = self._maybe_mask_results(result, fill_value=None)\n        return result\n\n    @property\n    def time(self):\n        \"\"\"\n        Returns numpy array of datetime.time. The time part of the Timestamps.\n        \"\"\"\n        # If the Timestamps have a timezone that is not UTC,\n        # convert them into their i8 representation while\n        # keeping their timezone and not using UTC\n        timestamps = self._local_timestamps()\n\n        return ints_to_pydatetime(timestamps, box=\"time\")\n\n    @property\n    def timetz(self):\n        \"\"\"\n        Returns numpy array of datetime.time also containing timezone\n        information. The time part of the Timestamps.\n        \"\"\"\n        return ints_to_pydatetime(self.asi8, self.tz, box=\"time\")\n\n    @property\n    def date(self):\n        \"\"\"\n        Returns numpy array of python datetime.date objects (namely, the date\n        part of Timestamps without timezone information).\n        \"\"\"\n        # If the Timestamps have a timezone that is not UTC,\n        # convert them into their i8 representation while\n        # keeping their timezone and not using UTC\n        timestamps = self._local_timestamps()\n\n        return ints_to_pydatetime(timestamps, box=\"date\")\n\n    def isocalendar(self):\n        \"\"\"\n        Returns a DataFrame with the year, week, and day calculated according to\n        the ISO 8601 standard.\n\n        .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        DataFrame\n            with columns year, week and day\n\n        See Also\n        --------\n        Timestamp.isocalendar : Function return a 3-tuple containing ISO year,\n            week number, and weekday for the given Timestamp object.\n        datetime.date.isocalendar : Return a named tuple object with\n            three components: year, week and weekday.\n\n        Examples\n        --------\n        >>> idx = pd.date_range(start='2019-12-29', freq='D', periods=4)\n        >>> idx.isocalendar()\n                    year  week  day\n        2019-12-29  2019    52    7\n        2019-12-30  2020     1    1\n        2019-12-31  2020     1    2\n        2020-01-01  2020     1    3\n        >>> idx.isocalendar().week\n        2019-12-29    52\n        2019-12-30     1\n        2019-12-31     1\n        2020-01-01     1\n        Freq: D, Name: week, dtype: UInt32\n        \"\"\"\n        from pandas import DataFrame\n\n        values = self._local_timestamps()\n        sarray = fields.build_isocalendar_sarray(values)\n        iso_calendar_df = DataFrame(\n            sarray, columns=[\"year\", \"week\", \"day\"], dtype=\"UInt32\"\n        )\n        if self._hasnans:\n            iso_calendar_df.iloc[self._isnan] = None\n        return iso_calendar_df\n\n    @property\n    def weekofyear(self):\n        \"\"\"\n        The week ordinal of the year.\n\n        .. deprecated:: 1.1.0\n\n        weekofyear and week have been deprecated.\n        Please use DatetimeIndex.isocalendar().week instead.\n        \"\"\"\n        warnings.warn(\n            \"weekofyear and week have been deprecated, please use \"\n            \"DatetimeIndex.isocalendar().week instead, which returns \"\n            \"a Series.  To exactly reproduce the behavior of week and \"\n            \"weekofyear and return an Index, you may call \"\n            \"pd.Int64Index(idx.isocalendar().week)\",\n            FutureWarning,\n            stacklevel=3,\n        )\n        week_series = self.isocalendar().week\n        if week_series.hasnans:\n            return week_series.to_numpy(dtype=\"float64\", na_value=np.nan)\n        return week_series.to_numpy(dtype=\"int64\")\n\n    week = weekofyear\n\n    year = _field_accessor(\n        \"year\",\n        \"Y\",\n        \"\"\"\n        The year of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"Y\")\n        ... )\n        >>> datetime_series\n        0   2000-12-31\n        1   2001-12-31\n        2   2002-12-31\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.year\n        0    2000\n        1    2001\n        2    2002\n        dtype: int64\n        \"\"\",\n    )\n    month = _field_accessor(\n        \"month\",\n        \"M\",\n        \"\"\"\n        The month as January=1, December=12.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"M\")\n        ... )\n        >>> datetime_series\n        0   2000-01-31\n        1   2000-02-29\n        2   2000-03-31\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.month\n        0    1\n        1    2\n        2    3\n        dtype: int64\n        \"\"\",\n    )\n    day = _field_accessor(\n        \"day\",\n        \"D\",\n        \"\"\"\n        The day of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"D\")\n        ... )\n        >>> datetime_series\n        0   2000-01-01\n        1   2000-01-02\n        2   2000-01-03\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.day\n        0    1\n        1    2\n        2    3\n        dtype: int64\n        \"\"\",\n    )\n    hour = _field_accessor(\n        \"hour\",\n        \"h\",\n        \"\"\"\n        The hours of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"h\")\n        ... )\n        >>> datetime_series\n        0   2000-01-01 00:00:00\n        1   2000-01-01 01:00:00\n        2   2000-01-01 02:00:00\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.hour\n        0    0\n        1    1\n        2    2\n        dtype: int64\n        \"\"\",\n    )\n    minute = _field_accessor(\n        \"minute\",\n        \"m\",\n        \"\"\"\n        The minutes of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"T\")\n        ... )\n        >>> datetime_series\n        0   2000-01-01 00:00:00\n        1   2000-01-01 00:01:00\n        2   2000-01-01 00:02:00\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.minute\n        0    0\n        1    1\n        2    2\n        dtype: int64\n        \"\"\",\n    )\n    second = _field_accessor(\n        \"second\",\n        \"s\",\n        \"\"\"\n        The seconds of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"s\")\n        ... )\n        >>> datetime_series\n        0   2000-01-01 00:00:00\n        1   2000-01-01 00:00:01\n        2   2000-01-01 00:00:02\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.second\n        0    0\n        1    1\n        2    2\n        dtype: int64\n        \"\"\",\n    )\n    microsecond = _field_accessor(\n        \"microsecond\",\n        \"us\",\n        \"\"\"\n        The microseconds of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"us\")\n        ... )\n        >>> datetime_series\n        0   2000-01-01 00:00:00.000000\n        1   2000-01-01 00:00:00.000001\n        2   2000-01-01 00:00:00.000002\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.microsecond\n        0       0\n        1       1\n        2       2\n        dtype: int64\n        \"\"\",\n    )\n    nanosecond = _field_accessor(\n        \"nanosecond\",\n        \"ns\",\n        \"\"\"\n        The nanoseconds of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"ns\")\n        ... )\n        >>> datetime_series\n        0   2000-01-01 00:00:00.000000000\n        1   2000-01-01 00:00:00.000000001\n        2   2000-01-01 00:00:00.000000002\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.nanosecond\n        0       0\n        1       1\n        2       2\n        dtype: int64\n        \"\"\",\n    )\n    _dayofweek_doc = \"\"\"\n    The day of the week with Monday=0, Sunday=6.\n\n    Return the day of the week. It is assumed the week starts on\n    Monday, which is denoted by 0 and ends on Sunday which is denoted\n    by 6. This method is available on both Series with datetime\n    values (using the `dt` accessor) or DatetimeIndex.\n\n    Returns\n    -------\n    Series or Index\n        Containing integers indicating the day number.\n\n    See Also\n    --------\n    Series.dt.dayofweek : Alias.\n    Series.dt.weekday : Alias.\n    Series.dt.day_name : Returns the name of the day of the week.\n\n    Examples\n    --------\n    >>> s = pd.date_range('2016-12-31', '2017-01-08', freq='D').to_series()\n    >>> s.dt.dayofweek\n    2016-12-31    5\n    2017-01-01    6\n    2017-01-02    0\n    2017-01-03    1\n    2017-01-04    2\n    2017-01-05    3\n    2017-01-06    4\n    2017-01-07    5\n    2017-01-08    6\n    Freq: D, dtype: int64\n    \"\"\"\n    dayofweek = _field_accessor(\"dayofweek\", \"dow\", _dayofweek_doc)\n    weekday = dayofweek\n\n    dayofyear = _field_accessor(\n        \"dayofyear\",\n        \"doy\",\n        \"\"\"\n        The ordinal day of the year.\n        \"\"\",\n    )\n    quarter = _field_accessor(\n        \"quarter\",\n        \"q\",\n        \"\"\"\n        The quarter of the date.\n        \"\"\",\n    )\n    days_in_month = _field_accessor(\n        \"days_in_month\",\n        \"dim\",\n        \"\"\"\n        The number of days in the month.\n        \"\"\",\n    )\n    daysinmonth = days_in_month\n    _is_month_doc = \"\"\"\n        Indicates whether the date is the {first_or_last} day of the month.\n\n        Returns\n        -------\n        Series or array\n            For Series, returns a Series with boolean values.\n            For DatetimeIndex, returns a boolean array.\n\n        See Also\n        --------\n        is_month_start : Return a boolean indicating whether the date\n            is the first day of the month.\n        is_month_end : Return a boolean indicating whether the date\n            is the last day of the month.\n\n        Examples\n        --------\n        This method is available on Series with datetime values under\n        the ``.dt`` accessor, and directly on DatetimeIndex.\n\n        >>> s = pd.Series(pd.date_range(\"2018-02-27\", periods=3))\n        >>> s\n        0   2018-02-27\n        1   2018-02-28\n        2   2018-03-01\n        dtype: datetime64[ns]\n        >>> s.dt.is_month_start\n        0    False\n        1    False\n        2    True\n        dtype: bool\n        >>> s.dt.is_month_end\n        0    False\n        1    True\n        2    False\n        dtype: bool\n\n        >>> idx = pd.date_range(\"2018-02-27\", periods=3)\n        >>> idx.is_month_start\n        array([False, False, True])\n        >>> idx.is_month_end\n        array([False, True, False])\n    \"\"\"\n    is_month_start = _field_accessor(\n        \"is_month_start\", \"is_month_start\", _is_month_doc.format(first_or_last=\"first\")\n    )\n\n    is_month_end = _field_accessor(\n        \"is_month_end\", \"is_month_end\", _is_month_doc.format(first_or_last=\"last\")\n    )\n\n    is_quarter_start = _field_accessor(\n        \"is_quarter_start\",\n        \"is_quarter_start\",\n        \"\"\"\n        Indicator for whether the date is the first day of a quarter.\n\n        Returns\n        -------\n        is_quarter_start : Series or DatetimeIndex\n            The same type as the original data with boolean values. Series will\n            have the same name and index. DatetimeIndex will have the same\n            name.\n\n        See Also\n        --------\n        quarter : Return the quarter of the date.\n        is_quarter_end : Similar property for indicating the quarter start.\n\n        Examples\n        --------\n        This method is available on Series with datetime values under\n        the ``.dt`` accessor, and directly on DatetimeIndex.\n\n        >>> df = pd.DataFrame({'dates': pd.date_range(\"2017-03-30\",\n        ...                   periods=4)})\n        >>> df.assign(quarter=df.dates.dt.quarter,\n        ...           is_quarter_start=df.dates.dt.is_quarter_start)\n               dates  quarter  is_quarter_start\n        0 2017-03-30        1             False\n        1 2017-03-31        1             False\n        2 2017-04-01        2              True\n        3 2017-04-02        2             False\n\n        >>> idx = pd.date_range('2017-03-30', periods=4)\n        >>> idx\n        DatetimeIndex(['2017-03-30', '2017-03-31', '2017-04-01', '2017-04-02'],\n                      dtype='datetime64[ns]', freq='D')\n\n        >>> idx.is_quarter_start\n        array([False, False,  True, False])\n        \"\"\",\n    )\n    is_quarter_end = _field_accessor(\n        \"is_quarter_end\",\n        \"is_quarter_end\",\n        \"\"\"\n        Indicator for whether the date is the last day of a quarter.\n\n        Returns\n        -------\n        is_quarter_end : Series or DatetimeIndex\n            The same type as the original data with boolean values. Series will\n            have the same name and index. DatetimeIndex will have the same\n            name.\n\n        See Also\n        --------\n        quarter : Return the quarter of the date.\n        is_quarter_start : Similar property indicating the quarter start.\n\n        Examples\n        --------\n        This method is available on Series with datetime values under\n        the ``.dt`` accessor, and directly on DatetimeIndex.\n\n        >>> df = pd.DataFrame({'dates': pd.date_range(\"2017-03-30\",\n        ...                    periods=4)})\n        >>> df.assign(quarter=df.dates.dt.quarter,\n        ...           is_quarter_end=df.dates.dt.is_quarter_end)\n               dates  quarter    is_quarter_end\n        0 2017-03-30        1             False\n        1 2017-03-31        1              True\n        2 2017-04-01        2             False\n        3 2017-04-02        2             False\n\n        >>> idx = pd.date_range('2017-03-30', periods=4)\n        >>> idx\n        DatetimeIndex(['2017-03-30', '2017-03-31', '2017-04-01', '2017-04-02'],\n                      dtype='datetime64[ns]', freq='D')\n\n        >>> idx.is_quarter_end\n        array([False,  True, False, False])\n        \"\"\",\n    )\n    is_year_start = _field_accessor(\n        \"is_year_start\",\n        \"is_year_start\",\n        \"\"\"\n        Indicate whether the date is the first day of a year.\n\n        Returns\n        -------\n        Series or DatetimeIndex\n            The same type as the original data with boolean values. Series will\n            have the same name and index. DatetimeIndex will have the same\n            name.\n\n        See Also\n        --------\n        is_year_end : Similar property indicating the last day of the year.\n\n        Examples\n        --------\n        This method is available on Series with datetime values under\n        the ``.dt`` accessor, and directly on DatetimeIndex.\n\n        >>> dates = pd.Series(pd.date_range(\"2017-12-30\", periods=3))\n        >>> dates\n        0   2017-12-30\n        1   2017-12-31\n        2   2018-01-01\n        dtype: datetime64[ns]\n\n        >>> dates.dt.is_year_start\n        0    False\n        1    False\n        2    True\n        dtype: bool\n\n        >>> idx = pd.date_range(\"2017-12-30\", periods=3)\n        >>> idx\n        DatetimeIndex(['2017-12-30', '2017-12-31', '2018-01-01'],\n                      dtype='datetime64[ns]', freq='D')\n\n        >>> idx.is_year_start\n        array([False, False,  True])\n        \"\"\",\n    )\n    is_year_end = _field_accessor(\n        \"is_year_end\",\n        \"is_year_end\",\n        \"\"\"\n        Indicate whether the date is the last day of the year.\n\n        Returns\n        -------\n        Series or DatetimeIndex\n            The same type as the original data with boolean values. Series will\n            have the same name and index. DatetimeIndex will have the same\n            name.\n\n        See Also\n        --------\n        is_year_start : Similar property indicating the start of the year.\n\n        Examples\n        --------\n        This method is available on Series with datetime values under\n        the ``.dt`` accessor, and directly on DatetimeIndex.\n\n        >>> dates = pd.Series(pd.date_range(\"2017-12-30\", periods=3))\n        >>> dates\n        0   2017-12-30\n        1   2017-12-31\n        2   2018-01-01\n        dtype: datetime64[ns]\n\n        >>> dates.dt.is_year_end\n        0    False\n        1     True\n        2    False\n        dtype: bool\n\n        >>> idx = pd.date_range(\"2017-12-30\", periods=3)\n        >>> idx\n        DatetimeIndex(['2017-12-30', '2017-12-31', '2018-01-01'],\n                      dtype='datetime64[ns]', freq='D')\n\n        >>> idx.is_year_end\n        array([False,  True, False])\n        \"\"\",\n    )\n    is_leap_year = _field_accessor(\n        \"is_leap_year\",\n        \"is_leap_year\",\n        \"\"\"\n        Boolean indicator if the date belongs to a leap year.\n\n        A leap year is a year, which has 366 days (instead of 365) including\n        29th of February as an intercalary day.\n        Leap years are years which are multiples of four with the exception\n        of years divisible by 100 but not by 400.\n\n        Returns\n        -------\n        Series or ndarray\n             Booleans indicating if dates belong to a leap year.\n\n        Examples\n        --------\n        This method is available on Series with datetime values under\n        the ``.dt`` accessor, and directly on DatetimeIndex.\n\n        >>> idx = pd.date_range(\"2012-01-01\", \"2015-01-01\", freq=\"Y\")\n        >>> idx\n        DatetimeIndex(['2012-12-31', '2013-12-31', '2014-12-31'],\n                      dtype='datetime64[ns]', freq='A-DEC')\n        >>> idx.is_leap_year\n        array([ True, False, False])\n\n        >>> dates_series = pd.Series(idx)\n        >>> dates_series\n        0   2012-12-31\n        1   2013-12-31\n        2   2014-12-31\n        dtype: datetime64[ns]\n        >>> dates_series.dt.is_leap_year\n        0     True\n        1    False\n        2    False\n        dtype: bool\n        \"\"\",\n    )\n\n    def to_julian_date(self):\n        \"\"\"\n        Convert Datetime Array to float64 ndarray of Julian Dates.\n        0 Julian date is noon January 1, 4713 BC.\n        https://en.wikipedia.org/wiki/Julian_day\n        \"\"\"\n\n        # http://mysite.verizon.net/aesir_research/date/jdalg2.htm\n        year = np.asarray(self.year)\n        month = np.asarray(self.month)\n        day = np.asarray(self.day)\n        testarr = month < 3\n        year[testarr] -= 1\n        month[testarr] += 12\n        return (\n            day\n            + np.fix((153 * month - 457) / 5)\n            + 365 * year\n            + np.floor(year / 4)\n            - np.floor(year / 100)\n            + np.floor(year / 400)\n            + 1_721_118.5\n            + (\n                self.hour\n                + self.minute / 60.0\n                + self.second / 3600.0\n                + self.microsecond / 3600.0 / 1e6\n                + self.nanosecond / 3600.0 / 1e9\n            )\n            / 24.0\n        )\n\n\n# -------------------------------------------------------------------\n# Constructor Helpers\n\n\ndef sequence_to_dt64ns(\n    data,\n    dtype=None,\n    copy=False,\n    tz=None,\n    dayfirst=False,\n    yearfirst=False,\n    ambiguous=\"raise\",\n):\n    \"\"\"\n    Parameters\n    ----------\n    data : list-like\n    dtype : dtype, str, or None, default None\n    copy : bool, default False\n    tz : tzinfo, str, or None, default None\n    dayfirst : bool, default False\n    yearfirst : bool, default False\n    ambiguous : str, bool, or arraylike, default 'raise'\n        See pandas._libs.tslibs.tzconversion.tz_localize_to_utc.\n\n    Returns\n    -------\n    result : numpy.ndarray\n        The sequence converted to a numpy array with dtype ``datetime64[ns]``.\n    tz : tzinfo or None\n        Either the user-provided tzinfo or one inferred from the data.\n    inferred_freq : Tick or None\n        The inferred frequency of the sequence.\n\n    Raises\n    ------\n    TypeError : PeriodDType data is passed\n    \"\"\"\n\n    inferred_freq = None\n\n    dtype = _validate_dt64_dtype(dtype)\n    tz = timezones.maybe_get_tz(tz)\n\n    if not hasattr(data, \"dtype\"):\n        # e.g. list, tuple\n        if np.ndim(data) == 0:\n            # i.e. generator\n            data = list(data)\n        data = np.asarray(data)\n        copy = False\n    elif isinstance(data, ABCSeries):\n        data = data._values\n    if isinstance(data, ABCPandasArray):\n        data = data.to_numpy()\n\n    if hasattr(data, \"freq\"):\n        # i.e. DatetimeArray/Index\n        inferred_freq = data.freq\n\n    # if dtype has an embedded tz, capture it\n    tz = validate_tz_from_dtype(dtype, tz)\n\n    if isinstance(data, ABCIndexClass):\n        if data.nlevels > 1:\n            # Without this check, data._data below is None\n            raise TypeError(\"Cannot create a DatetimeArray from a MultiIndex.\")\n        data = data._data\n\n    # By this point we are assured to have either a numpy array or Index\n    data, copy = maybe_convert_dtype(data, copy)\n    data_dtype = getattr(data, \"dtype\", None)\n\n    if is_object_dtype(data_dtype) or is_string_dtype(data_dtype):\n        # TODO: We do not have tests specific to string-dtypes,\n        #  also complex or categorical or other extension\n        copy = False\n        if lib.infer_dtype(data, skipna=False) == \"integer\":\n            data = data.astype(np.int64)\n        else:\n            # data comes back here as either i8 to denote UTC timestamps\n            #  or M8[ns] to denote wall times\n            data, inferred_tz = objects_to_datetime64ns(\n                data, dayfirst=dayfirst, yearfirst=yearfirst\n            )\n            tz = _maybe_infer_tz(tz, inferred_tz)\n        data_dtype = data.dtype\n\n    # `data` may have originally been a Categorical[datetime64[ns, tz]],\n    # so we need to handle these types.\n    if is_datetime64tz_dtype(data_dtype):\n        # DatetimeArray -> ndarray\n        tz = _maybe_infer_tz(tz, data.tz)\n        result = data._data\n\n    elif is_datetime64_dtype(data_dtype):\n        # tz-naive DatetimeArray or ndarray[datetime64]\n        data = getattr(data, \"_data\", data)\n        if data.dtype != DT64NS_DTYPE:\n            data = conversion.ensure_datetime64ns(data)\n\n        if tz is not None:\n            # Convert tz-naive to UTC\n            tz = timezones.maybe_get_tz(tz)\n            data = tzconversion.tz_localize_to_utc(\n                data.view(\"i8\"), tz, ambiguous=ambiguous\n            )\n            data = data.view(DT64NS_DTYPE)\n\n        assert data.dtype == DT64NS_DTYPE, data.dtype\n        result = data\n\n    else:\n        # must be integer dtype otherwise\n        # assume this data are epoch timestamps\n        if tz:\n            tz = timezones.maybe_get_tz(tz)\n\n        if data.dtype != INT64_DTYPE:\n            data = data.astype(np.int64, copy=False)\n        result = data.view(DT64NS_DTYPE)\n\n    if copy:\n        # TODO: should this be deepcopy?\n        result = result.copy()\n\n    assert isinstance(result, np.ndarray), type(result)\n    assert result.dtype == \"M8[ns]\", result.dtype\n\n    # We have to call this again after possibly inferring a tz above\n    validate_tz_from_dtype(dtype, tz)\n\n    return result, tz, inferred_freq\n\n\ndef objects_to_datetime64ns(\n    data,\n    dayfirst,\n    yearfirst,\n    utc=False,\n    errors=\"raise\",\n    require_iso8601=False,\n    allow_object=False,\n):\n    \"\"\"\n    Convert data to array of timestamps.\n\n    Parameters\n    ----------\n    data : np.ndarray[object]\n    dayfirst : bool\n    yearfirst : bool\n    utc : bool, default False\n        Whether to convert timezone-aware timestamps to UTC.\n    errors : {'raise', 'ignore', 'coerce'}\n    require_iso8601 : bool, default False\n    allow_object : bool\n        Whether to return an object-dtype ndarray instead of raising if the\n        data contains more than one timezone.\n\n    Returns\n    -------\n    result : ndarray\n        np.int64 dtype if returned values represent UTC timestamps\n        np.datetime64[ns] if returned values represent wall times\n        object if mixed timezones\n    inferred_tz : tzinfo or None\n\n    Raises\n    ------\n    ValueError : if data cannot be converted to datetimes\n    \"\"\"\n    assert errors in [\"raise\", \"ignore\", \"coerce\"]\n\n    # if str-dtype, convert\n    data = np.array(data, copy=False, dtype=np.object_)\n\n    try:\n        result, tz_parsed = tslib.array_to_datetime(\n            data,\n            errors=errors,\n            utc=utc,\n            dayfirst=dayfirst,\n            yearfirst=yearfirst,\n            require_iso8601=require_iso8601,\n        )\n    except ValueError as e:\n        try:\n            values, tz_parsed = conversion.datetime_to_datetime64(data)\n            # If tzaware, these values represent unix timestamps, so we\n            #  return them as i8 to distinguish from wall times\n            return values.view(\"i8\"), tz_parsed\n        except (ValueError, TypeError):\n            raise e\n\n    if tz_parsed is not None:\n        # We can take a shortcut since the datetime64 numpy array\n        #  is in UTC\n        # Return i8 values to denote unix timestamps\n        return result.view(\"i8\"), tz_parsed\n    elif is_datetime64_dtype(result):\n        # returning M8[ns] denotes wall-times; since tz is None\n        #  the distinction is a thin one\n        return result, tz_parsed\n    elif is_object_dtype(result):\n        # GH#23675 when called via `pd.to_datetime`, returning an object-dtype\n        #  array is allowed.  When called via `pd.DatetimeIndex`, we can\n        #  only accept datetime64 dtype, so raise TypeError if object-dtype\n        #  is returned, as that indicates the values can be recognized as\n        #  datetimes but they have conflicting timezones/awareness\n        if allow_object:\n            return result, tz_parsed\n        raise TypeError(result)\n    else:  # pragma: no cover\n        # GH#23675 this TypeError should never be hit, whereas the TypeError\n        #  in the object-dtype branch above is reachable.\n        raise TypeError(result)\n\n\ndef maybe_convert_dtype(data, copy):\n    \"\"\"\n    Convert data based on dtype conventions, issuing deprecation warnings\n    or errors where appropriate.\n\n    Parameters\n    ----------\n    data : np.ndarray or pd.Index\n    copy : bool\n\n    Returns\n    -------\n    data : np.ndarray or pd.Index\n    copy : bool\n\n    Raises\n    ------\n    TypeError : PeriodDType data is passed\n    \"\"\"\n    if not hasattr(data, \"dtype\"):\n        # e.g. collections.deque\n        return data, copy\n\n    if is_float_dtype(data.dtype):\n        # Note: we must cast to datetime64[ns] here in order to treat these\n        #  as wall-times instead of UTC timestamps.\n        data = data.astype(DT64NS_DTYPE)\n        copy = False\n        # TODO: deprecate this behavior to instead treat symmetrically\n        #  with integer dtypes.  See discussion in GH#23675\n\n    elif is_timedelta64_dtype(data.dtype) or is_bool_dtype(data.dtype):\n        # GH#29794 enforcing deprecation introduced in GH#23539\n        raise TypeError(f\"dtype {data.dtype} cannot be converted to datetime64[ns]\")\n    elif is_period_dtype(data.dtype):\n        # Note: without explicitly raising here, PeriodIndex\n        #  test_setops.test_join_does_not_recur fails\n        raise TypeError(\n            \"Passing PeriodDtype data is invalid. Use `data.to_timestamp()` instead\"\n        )\n\n    elif is_categorical_dtype(data.dtype):\n        # GH#18664 preserve tz in going DTI->Categorical->DTI\n        # TODO: cases where we need to do another pass through this func,\n        #  e.g. the categories are timedelta64s\n        data = data.categories.take(data.codes, fill_value=NaT)._values\n        copy = False\n\n    elif is_extension_array_dtype(data.dtype) and not is_datetime64tz_dtype(data.dtype):\n        # Includes categorical\n        # TODO: We have no tests for these\n        data = np.array(data, dtype=np.object_)\n        copy = False\n\n    return data, copy\n\n\n# -------------------------------------------------------------------\n# Validation and Inference\n\n\ndef _maybe_infer_tz(\n    tz: Optional[tzinfo], inferred_tz: Optional[tzinfo]\n) -> Optional[tzinfo]:\n    \"\"\"\n    If a timezone is inferred from data, check that it is compatible with\n    the user-provided timezone, if any.\n\n    Parameters\n    ----------\n    tz : tzinfo or None\n    inferred_tz : tzinfo or None\n\n    Returns\n    -------\n    tz : tzinfo or None\n\n    Raises\n    ------\n    TypeError : if both timezones are present but do not match\n    \"\"\"\n    if tz is None:\n        tz = inferred_tz\n    elif inferred_tz is None:\n        pass\n    elif not timezones.tz_compare(tz, inferred_tz):\n        raise TypeError(\n            f\"data is already tz-aware {inferred_tz}, unable to \"\n            f\"set specified tz: {tz}\"\n        )\n    return tz\n\n\ndef _validate_dt64_dtype(dtype):\n    \"\"\"\n    Check that a dtype, if passed, represents either a numpy datetime64[ns]\n    dtype or a pandas DatetimeTZDtype.\n\n    Parameters\n    ----------\n    dtype : object\n\n    Returns\n    -------\n    dtype : None, numpy.dtype, or DatetimeTZDtype\n\n    Raises\n    ------\n    ValueError : invalid dtype\n\n    Notes\n    -----\n    Unlike validate_tz_from_dtype, this does _not_ allow non-existent\n    tz errors to go through\n    \"\"\"\n    if dtype is not None:\n        dtype = pandas_dtype(dtype)\n        if is_dtype_equal(dtype, np.dtype(\"M8\")):\n            # no precision, disallowed GH#24806\n            msg = (\n                \"Passing in 'datetime64' dtype with no precision is not allowed. \"\n                \"Please pass in 'datetime64[ns]' instead.\"\n            )\n            raise ValueError(msg)\n\n        if (isinstance(dtype, np.dtype) and dtype != DT64NS_DTYPE) or not isinstance(\n            dtype, (np.dtype, DatetimeTZDtype)\n        ):\n            raise ValueError(\n                f\"Unexpected value for 'dtype': '{dtype}'. \"\n                \"Must be 'datetime64[ns]' or DatetimeTZDtype'.\"\n            )\n    return dtype\n\n\ndef validate_tz_from_dtype(dtype, tz: Optional[tzinfo]) -> Optional[tzinfo]:\n    \"\"\"\n    If the given dtype is a DatetimeTZDtype, extract the implied\n    tzinfo object from it and check that it does not conflict with the given\n    tz.\n\n    Parameters\n    ----------\n    dtype : dtype, str\n    tz : None, tzinfo\n\n    Returns\n    -------\n    tz : consensus tzinfo\n\n    Raises\n    ------\n    ValueError : on tzinfo mismatch\n    \"\"\"\n    if dtype is not None:\n        if isinstance(dtype, str):\n            try:\n                dtype = DatetimeTZDtype.construct_from_string(dtype)\n            except TypeError:\n                # Things like `datetime64[ns]`, which is OK for the\n                # constructors, but also nonsense, which should be validated\n                # but not by us. We *do* allow non-existent tz errors to\n                # go through\n                pass\n        dtz = getattr(dtype, \"tz\", None)\n        if dtz is not None:\n            if tz is not None and not timezones.tz_compare(tz, dtz):\n                raise ValueError(\"cannot supply both a tz and a dtype with a tz\")\n            tz = dtz\n\n        if tz is not None and is_datetime64_dtype(dtype):\n            # We also need to check for the case where the user passed a\n            #  tz-naive dtype (i.e. datetime64[ns])\n            if tz is not None and not timezones.tz_compare(tz, dtz):\n                raise ValueError(\n                    \"cannot supply both a tz and a \"\n                    \"timezone-naive dtype (i.e. datetime64[ns])\"\n                )\n\n    return tz\n\n\ndef _infer_tz_from_endpoints(\n    start: Timestamp, end: Timestamp, tz: Optional[tzinfo]\n) -> Optional[tzinfo]:\n    \"\"\"\n    If a timezone is not explicitly given via `tz`, see if one can\n    be inferred from the `start` and `end` endpoints.  If more than one\n    of these inputs provides a timezone, require that they all agree.\n\n    Parameters\n    ----------\n    start : Timestamp\n    end : Timestamp\n    tz : tzinfo or None\n\n    Returns\n    -------\n    tz : tzinfo or None\n\n    Raises\n    ------\n    TypeError : if start and end timezones do not agree\n    \"\"\"\n    try:\n        inferred_tz = timezones.infer_tzinfo(start, end)\n    except AssertionError as err:\n        # infer_tzinfo raises AssertionError if passed mismatched timezones\n        raise TypeError(\n            \"Start and end cannot both be tz-aware with different timezones\"\n        ) from err\n\n    inferred_tz = timezones.maybe_get_tz(inferred_tz)\n    tz = timezones.maybe_get_tz(tz)\n\n    if tz is not None and inferred_tz is not None:\n        if not timezones.tz_compare(inferred_tz, tz):\n            raise AssertionError(\"Inferred time zone not equal to passed time zone\")\n\n    elif inferred_tz is not None:\n        tz = inferred_tz\n\n    return tz\n\n\ndef _maybe_normalize_endpoints(\n    start: Optional[Timestamp], end: Optional[Timestamp], normalize: bool\n):\n    _normalized = True\n\n    if start is not None:\n        if normalize:\n            start = start.normalize()\n            _normalized = True\n        else:\n            _normalized = _normalized and start.time() == _midnight\n\n    if end is not None:\n        if normalize:\n            end = end.normalize()\n            _normalized = True\n        else:\n            _normalized = _normalized and end.time() == _midnight\n\n    return start, end, _normalized\n\n\ndef _maybe_localize_point(ts, is_none, is_not_none, freq, tz, ambiguous, nonexistent):\n    \"\"\"\n    Localize a start or end Timestamp to the timezone of the corresponding\n    start or end Timestamp\n\n    Parameters\n    ----------\n    ts : start or end Timestamp to potentially localize\n    is_none : argument that should be None\n    is_not_none : argument that should not be None\n    freq : Tick, DateOffset, or None\n    tz : str, timezone object or None\n    ambiguous: str, localization behavior for ambiguous times\n    nonexistent: str, localization behavior for nonexistent times\n\n    Returns\n    -------\n    ts : Timestamp\n    \"\"\"\n    # Make sure start and end are timezone localized if:\n    # 1) freq = a Timedelta-like frequency (Tick)\n    # 2) freq = None i.e. generating a linspaced range\n    if is_none is None and is_not_none is not None:\n        # Note: We can't ambiguous='infer' a singular ambiguous time; however,\n        # we have historically defaulted ambiguous=False\n        ambiguous = ambiguous if ambiguous != \"infer\" else False\n        localize_args = {\"ambiguous\": ambiguous, \"nonexistent\": nonexistent, \"tz\": None}\n        if isinstance(freq, Tick) or freq is None:\n            localize_args[\"tz\"] = tz\n        ts = ts.tz_localize(**localize_args)\n    return ts\n\n\ndef generate_range(start=None, end=None, periods=None, offset=BDay()):\n    \"\"\"\n    Generates a sequence of dates corresponding to the specified time\n    offset. Similar to dateutil.rrule except uses pandas DateOffset\n    objects to represent time increments.\n\n    Parameters\n    ----------\n    start : datetime, (default None)\n    end : datetime, (default None)\n    periods : int, (default None)\n    offset : DateOffset, (default BDay())\n\n    Notes\n    -----\n    * This method is faster for generating weekdays than dateutil.rrule\n    * At least two of (start, end, periods) must be specified.\n    * If both start and end are specified, the returned dates will\n    satisfy start <= date <= end.\n\n    Returns\n    -------\n    dates : generator object\n    \"\"\"\n    offset = to_offset(offset)\n\n    start = Timestamp(start)\n    start = start if start is not NaT else None\n    end = Timestamp(end)\n    end = end if end is not NaT else None\n\n    if start and not offset.is_on_offset(start):\n        start = offset.rollforward(start)\n\n    elif end and not offset.is_on_offset(end):\n        end = offset.rollback(end)\n\n    if periods is None and end < start and offset.n >= 0:\n        end = None\n        periods = 0\n\n    if end is None:\n        end = start + (periods - 1) * offset\n\n    if start is None:\n        start = end - (periods - 1) * offset\n\n    cur = start\n    if offset.n >= 0:\n        while cur <= end:\n            yield cur\n\n            if cur == end:\n                # GH#24252 avoid overflows by not performing the addition\n                # in offset.apply unless we have to\n                break\n\n            # faster than cur + offset\n            next_date = offset.apply(cur)\n            if next_date <= cur:\n                raise ValueError(f\"Offset {offset} did not increment date\")\n            cur = next_date\n    else:\n        while cur >= end:\n            yield cur\n\n            if cur == end:\n                # GH#24252 avoid overflows by not performing the addition\n                # in offset.apply unless we have to\n                break\n\n            # faster than cur + offset\n            next_date = offset.apply(cur)\n            if next_date >= cur:\n                raise ValueError(f\"Offset {offset} did not decrement date\")\n            cur = next_date\n"
    },
    {
      "filename": "pandas/core/arrays/integer.py",
      "content": "from datetime import timedelta\nimport numbers\nfrom typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Type, Union\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import Timedelta, iNaT, lib, missing as libmissing\nfrom pandas._typing import ArrayLike, DtypeObj\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import cache_readonly\n\nfrom pandas.core.dtypes.base import register_extension_dtype\nfrom pandas.core.dtypes.common import (\n    is_bool_dtype,\n    is_datetime64_dtype,\n    is_float,\n    is_float_dtype,\n    is_integer,\n    is_integer_dtype,\n    is_list_like,\n    is_object_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core import ops\nfrom pandas.core.ops import invalid_comparison\nfrom pandas.core.tools.numeric import to_numeric\n\nfrom .masked import BaseMaskedArray, BaseMaskedDtype\n\nif TYPE_CHECKING:\n    import pyarrow\n\n\nclass _IntegerDtype(BaseMaskedDtype):\n    \"\"\"\n    An ExtensionDtype to hold a single size & kind of integer dtype.\n\n    These specific implementations are subclasses of the non-public\n    _IntegerDtype. For example we have Int8Dtype to represent signed int 8s.\n\n    The attributes name & type are set when these subclasses are created.\n    \"\"\"\n\n    def __repr__(self) -> str:\n        sign = \"U\" if self.is_unsigned_integer else \"\"\n        return f\"{sign}Int{8 * self.itemsize}Dtype()\"\n\n    @cache_readonly\n    def is_signed_integer(self) -> bool:\n        return self.kind == \"i\"\n\n    @cache_readonly\n    def is_unsigned_integer(self) -> bool:\n        return self.kind == \"u\"\n\n    @property\n    def _is_numeric(self) -> bool:\n        return True\n\n    @classmethod\n    def construct_array_type(cls) -> Type[\"IntegerArray\"]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        return IntegerArray\n\n    def _get_common_dtype(self, dtypes: List[DtypeObj]) -> Optional[DtypeObj]:\n        # we only handle nullable EA dtypes and numeric numpy dtypes\n        if not all(\n            isinstance(t, BaseMaskedDtype)\n            or (\n                isinstance(t, np.dtype)\n                and (np.issubdtype(t, np.number) or np.issubdtype(t, np.bool_))\n            )\n            for t in dtypes\n        ):\n            return None\n        np_dtype = np.find_common_type(\n            [t.numpy_dtype if isinstance(t, BaseMaskedDtype) else t for t in dtypes], []\n        )\n        if np.issubdtype(np_dtype, np.integer):\n            return INT_STR_TO_DTYPE[str(np_dtype)]\n        elif np.issubdtype(np_dtype, np.floating):\n            from pandas.core.arrays.floating import FLOAT_STR_TO_DTYPE\n\n            return FLOAT_STR_TO_DTYPE[str(np_dtype)]\n        return None\n\n    def __from_arrow__(\n        self, array: Union[\"pyarrow.Array\", \"pyarrow.ChunkedArray\"]\n    ) -> \"IntegerArray\":\n        \"\"\"\n        Construct IntegerArray from pyarrow Array/ChunkedArray.\n        \"\"\"\n        import pyarrow\n\n        from pandas.core.arrays._arrow_utils import pyarrow_array_to_numpy_and_mask\n\n        pyarrow_type = pyarrow.from_numpy_dtype(self.type)\n        if not array.type.equals(pyarrow_type):\n            array = array.cast(pyarrow_type)\n\n        if isinstance(array, pyarrow.Array):\n            chunks = [array]\n        else:\n            # pyarrow.ChunkedArray\n            chunks = array.chunks\n\n        results = []\n        for arr in chunks:\n            data, mask = pyarrow_array_to_numpy_and_mask(arr, dtype=self.type)\n            int_arr = IntegerArray(data.copy(), ~mask, copy=False)\n            results.append(int_arr)\n\n        return IntegerArray._concat_same_type(results)\n\n\ndef integer_array(values, dtype=None, copy: bool = False) -> \"IntegerArray\":\n    \"\"\"\n    Infer and return an integer array of the values.\n\n    Parameters\n    ----------\n    values : 1D list-like\n    dtype : dtype, optional\n        dtype to coerce\n    copy : bool, default False\n\n    Returns\n    -------\n    IntegerArray\n\n    Raises\n    ------\n    TypeError if incompatible types\n    \"\"\"\n    values, mask = coerce_to_array(values, dtype=dtype, copy=copy)\n    return IntegerArray(values, mask)\n\n\ndef safe_cast(values, dtype, copy: bool):\n    \"\"\"\n    Safely cast the values to the dtype if they\n    are equivalent, meaning floats must be equivalent to the\n    ints.\n\n    \"\"\"\n    try:\n        return values.astype(dtype, casting=\"safe\", copy=copy)\n    except TypeError as err:\n\n        casted = values.astype(dtype, copy=copy)\n        if (casted == values).all():\n            return casted\n\n        raise TypeError(\n            f\"cannot safely cast non-equivalent {values.dtype} to {np.dtype(dtype)}\"\n        ) from err\n\n\ndef coerce_to_array(\n    values, dtype, mask=None, copy: bool = False\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Coerce the input values array to numpy arrays with a mask\n\n    Parameters\n    ----------\n    values : 1D list-like\n    dtype : integer dtype\n    mask : bool 1D array, optional\n    copy : bool, default False\n        if True, copy the input\n\n    Returns\n    -------\n    tuple of (values, mask)\n    \"\"\"\n    # if values is integer numpy array, preserve it's dtype\n    if dtype is None and hasattr(values, \"dtype\"):\n        if is_integer_dtype(values.dtype):\n            dtype = values.dtype\n\n    if dtype is not None:\n        if isinstance(dtype, str) and (\n            dtype.startswith(\"Int\") or dtype.startswith(\"UInt\")\n        ):\n            # Avoid DeprecationWarning from NumPy about np.dtype(\"Int64\")\n            # https://github.com/numpy/numpy/pull/7476\n            dtype = dtype.lower()\n\n        if not issubclass(type(dtype), _IntegerDtype):\n            try:\n                dtype = INT_STR_TO_DTYPE[str(np.dtype(dtype))]\n            except KeyError as err:\n                raise ValueError(f\"invalid dtype specified {dtype}\") from err\n\n    if isinstance(values, IntegerArray):\n        values, mask = values._data, values._mask\n        if dtype is not None:\n            values = values.astype(dtype.numpy_dtype, copy=False)\n\n        if copy:\n            values = values.copy()\n            mask = mask.copy()\n        return values, mask\n\n    values = np.array(values, copy=copy)\n    if is_object_dtype(values):\n        inferred_type = lib.infer_dtype(values, skipna=True)\n        if inferred_type == \"empty\":\n            values = np.empty(len(values))\n            values.fill(np.nan)\n        elif inferred_type not in [\n            \"floating\",\n            \"integer\",\n            \"mixed-integer\",\n            \"integer-na\",\n            \"mixed-integer-float\",\n        ]:\n            raise TypeError(f\"{values.dtype} cannot be converted to an IntegerDtype\")\n\n    elif is_bool_dtype(values) and is_integer_dtype(dtype):\n        values = np.array(values, dtype=int, copy=copy)\n\n    elif not (is_integer_dtype(values) or is_float_dtype(values)):\n        raise TypeError(f\"{values.dtype} cannot be converted to an IntegerDtype\")\n\n    if mask is None:\n        mask = isna(values)\n    else:\n        assert len(mask) == len(values)\n\n    if not values.ndim == 1:\n        raise TypeError(\"values must be a 1D list-like\")\n    if not mask.ndim == 1:\n        raise TypeError(\"mask must be a 1D list-like\")\n\n    # infer dtype if needed\n    if dtype is None:\n        dtype = np.dtype(\"int64\")\n    else:\n        dtype = dtype.type\n\n    # if we are float, let's make sure that we can\n    # safely cast\n\n    # we copy as need to coerce here\n    if mask.any():\n        values = values.copy()\n        values[mask] = 1\n        values = safe_cast(values, dtype, copy=False)\n    else:\n        values = safe_cast(values, dtype, copy=False)\n\n    return values, mask\n\n\nclass IntegerArray(BaseMaskedArray):\n    \"\"\"\n    Array of integer (optional missing) values.\n\n    .. versionadded:: 0.24.0\n\n    .. versionchanged:: 1.0.0\n\n       Now uses :attr:`pandas.NA` as the missing value rather\n       than :attr:`numpy.nan`.\n\n    .. warning::\n\n       IntegerArray is currently experimental, and its API or internal\n       implementation may change without warning.\n\n    We represent an IntegerArray with 2 numpy arrays:\n\n    - data: contains a numpy integer array of the appropriate dtype\n    - mask: a boolean array holding a mask on the data, True is missing\n\n    To construct an IntegerArray from generic array-like input, use\n    :func:`pandas.array` with one of the integer dtypes (see examples).\n\n    See :ref:`integer_na` for more.\n\n    Parameters\n    ----------\n    values : numpy.ndarray\n        A 1-d integer-dtype array.\n    mask : numpy.ndarray\n        A 1-d boolean-dtype array indicating missing values.\n    copy : bool, default False\n        Whether to copy the `values` and `mask`.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Returns\n    -------\n    IntegerArray\n\n    Examples\n    --------\n    Create an IntegerArray with :func:`pandas.array`.\n\n    >>> int_array = pd.array([1, None, 3], dtype=pd.Int32Dtype())\n    >>> int_array\n    <IntegerArray>\n    [1, <NA>, 3]\n    Length: 3, dtype: Int32\n\n    String aliases for the dtypes are also available. They are capitalized.\n\n    >>> pd.array([1, None, 3], dtype='Int32')\n    <IntegerArray>\n    [1, <NA>, 3]\n    Length: 3, dtype: Int32\n\n    >>> pd.array([1, None, 3], dtype='UInt16')\n    <IntegerArray>\n    [1, <NA>, 3]\n    Length: 3, dtype: UInt16\n    \"\"\"\n\n    # The value used to fill '_data' to avoid upcasting\n    _internal_fill_value = 1\n\n    @cache_readonly\n    def dtype(self) -> _IntegerDtype:\n        return INT_STR_TO_DTYPE[str(self._data.dtype)]\n\n    def __init__(self, values: np.ndarray, mask: np.ndarray, copy: bool = False):\n        if not (isinstance(values, np.ndarray) and values.dtype.kind in [\"i\", \"u\"]):\n            raise TypeError(\n                \"values should be integer numpy array. Use \"\n                \"the 'pd.array' function instead\"\n            )\n        super().__init__(values, mask, copy=copy)\n\n    def __neg__(self):\n        return type(self)(-self._data, self._mask)\n\n    def __pos__(self):\n        return self\n\n    def __abs__(self):\n        return type(self)(np.abs(self._data), self._mask)\n\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy: bool = False) -> \"IntegerArray\":\n        return integer_array(scalars, dtype=dtype, copy=copy)\n\n    @classmethod\n    def _from_sequence_of_strings(\n        cls, strings, dtype=None, copy: bool = False\n    ) -> \"IntegerArray\":\n        scalars = to_numeric(strings, errors=\"raise\")\n        return cls._from_sequence(scalars, dtype, copy)\n\n    _HANDLED_TYPES = (np.ndarray, numbers.Number)\n\n    def __array_ufunc__(self, ufunc, method: str, *inputs, **kwargs):\n        # For IntegerArray inputs, we apply the ufunc to ._data\n        # and mask the result.\n        if method == \"reduce\":\n            # Not clear how to handle missing values in reductions. Raise.\n            raise NotImplementedError(\"The 'reduce' method is not supported.\")\n        out = kwargs.get(\"out\", ())\n\n        for x in inputs + out:\n            if not isinstance(x, self._HANDLED_TYPES + (IntegerArray,)):\n                return NotImplemented\n\n        # for binary ops, use our custom dunder methods\n        result = ops.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        mask = np.zeros(len(self), dtype=bool)\n        inputs2 = []\n        for x in inputs:\n            if isinstance(x, IntegerArray):\n                mask |= x._mask\n                inputs2.append(x._data)\n            else:\n                inputs2.append(x)\n\n        def reconstruct(x):\n            # we don't worry about scalar `x` here, since we\n            # raise for reduce up above.\n\n            if is_integer_dtype(x.dtype):\n                m = mask.copy()\n                return IntegerArray(x, m)\n            else:\n                x[mask] = np.nan\n            return x\n\n        result = getattr(ufunc, method)(*inputs2, **kwargs)\n        if isinstance(result, tuple):\n            return tuple(reconstruct(x) for x in result)\n        else:\n            return reconstruct(result)\n\n    def _coerce_to_array(self, value) -> Tuple[np.ndarray, np.ndarray]:\n        return coerce_to_array(value, dtype=self.dtype)\n\n    def astype(self, dtype, copy: bool = True) -> ArrayLike:\n        \"\"\"\n        Cast to a NumPy array or ExtensionArray with 'dtype'.\n\n        Parameters\n        ----------\n        dtype : str or dtype\n            Typecode or data-type to which the array is cast.\n        copy : bool, default True\n            Whether to copy the data, even if not necessary. If False,\n            a copy is made only if the old dtype does not match the\n            new dtype.\n\n        Returns\n        -------\n        ndarray or ExtensionArray\n            NumPy ndarray, BooleanArray or IntegerArray with 'dtype' for its dtype.\n\n        Raises\n        ------\n        TypeError\n            if incompatible type with an IntegerDtype, equivalent of same_kind\n            casting\n        \"\"\"\n        from pandas.core.arrays.masked import BaseMaskedDtype\n        from pandas.core.arrays.string_ import StringDtype\n\n        dtype = pandas_dtype(dtype)\n\n        # if the dtype is exactly the same, we can fastpath\n        if self.dtype == dtype:\n            # return the same object for copy=False\n            return self.copy() if copy else self\n        # if we are astyping to another nullable masked dtype, we can fastpath\n        if isinstance(dtype, BaseMaskedDtype):\n            data = self._data.astype(dtype.numpy_dtype, copy=copy)\n            # mask is copied depending on whether the data was copied, and\n            # not directly depending on the `copy` keyword\n            mask = self._mask if data is self._data else self._mask.copy()\n            return dtype.construct_array_type()(data, mask, copy=False)\n        elif isinstance(dtype, StringDtype):\n            return dtype.construct_array_type()._from_sequence(self, copy=False)\n\n        # coerce\n        if is_float_dtype(dtype):\n            # In astype, we consider dtype=float to also mean na_value=np.nan\n            na_value = np.nan\n        elif is_datetime64_dtype(dtype):\n            na_value = np.datetime64(\"NaT\")\n        else:\n            na_value = lib.no_default\n\n        return self.to_numpy(dtype=dtype, na_value=na_value, copy=False)\n\n    def _values_for_argsort(self) -> np.ndarray:\n        \"\"\"\n        Return values for sorting.\n\n        Returns\n        -------\n        ndarray\n            The transformed values should maintain the ordering between values\n            within the array.\n\n        See Also\n        --------\n        ExtensionArray.argsort : Return the indices that would sort this array.\n        \"\"\"\n        data = self._data.copy()\n        if self._mask.any():\n            data[self._mask] = data.min() - 1\n        return data\n\n    def _cmp_method(self, other, op):\n        from pandas.core.arrays import BaseMaskedArray, BooleanArray\n\n        mask = None\n\n        if isinstance(other, BaseMaskedArray):\n            other, mask = other._data, other._mask\n\n        elif is_list_like(other):\n            other = np.asarray(other)\n            if other.ndim > 1:\n                raise NotImplementedError(\"can only perform ops with 1-d structures\")\n            if len(self) != len(other):\n                raise ValueError(\"Lengths must match to compare\")\n\n        if other is libmissing.NA:\n            # numpy does not handle pd.NA well as \"other\" scalar (it returns\n            # a scalar False instead of an array)\n            # This may be fixed by NA.__array_ufunc__. Revisit this check\n            # once that's implemented.\n            result = np.zeros(self._data.shape, dtype=\"bool\")\n            mask = np.ones(self._data.shape, dtype=\"bool\")\n        else:\n            with warnings.catch_warnings():\n                # numpy may show a FutureWarning:\n                #     elementwise comparison failed; returning scalar instead,\n                #     but in the future will perform elementwise comparison\n                # before returning NotImplemented. We fall back to the correct\n                # behavior today, so that should be fine to ignore.\n                warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                with np.errstate(all=\"ignore\"):\n                    method = getattr(self._data, f\"__{op.__name__}__\")\n                    result = method(other)\n\n                if result is NotImplemented:\n                    result = invalid_comparison(self._data, other, op)\n\n        # nans propagate\n        if mask is None:\n            mask = self._mask.copy()\n        else:\n            mask = self._mask | mask\n\n        return BooleanArray(result, mask)\n\n    def _arith_method(self, other, op):\n        op_name = op.__name__\n        omask = None\n\n        if getattr(other, \"ndim\", 0) > 1:\n            raise NotImplementedError(\"can only perform ops with 1-d structures\")\n\n        if isinstance(other, IntegerArray):\n            other, omask = other._data, other._mask\n\n        elif is_list_like(other):\n            other = np.asarray(other)\n            if other.ndim > 1:\n                raise NotImplementedError(\"can only perform ops with 1-d structures\")\n            if len(self) != len(other):\n                raise ValueError(\"Lengths must match\")\n            if not (is_float_dtype(other) or is_integer_dtype(other)):\n                raise TypeError(\"can only perform ops with numeric values\")\n\n        elif isinstance(other, (timedelta, np.timedelta64)):\n            other = Timedelta(other)\n\n        else:\n            if not (is_float(other) or is_integer(other) or other is libmissing.NA):\n                raise TypeError(\"can only perform ops with numeric values\")\n\n        if omask is None:\n            mask = self._mask.copy()\n            if other is libmissing.NA:\n                mask |= True\n        else:\n            mask = self._mask | omask\n\n        if op_name == \"pow\":\n            # 1 ** x is 1.\n            mask = np.where((self._data == 1) & ~self._mask, False, mask)\n            # x ** 0 is 1.\n            if omask is not None:\n                mask = np.where((other == 0) & ~omask, False, mask)\n            elif other is not libmissing.NA:\n                mask = np.where(other == 0, False, mask)\n\n        elif op_name == \"rpow\":\n            # 1 ** x is 1.\n            if omask is not None:\n                mask = np.where((other == 1) & ~omask, False, mask)\n            elif other is not libmissing.NA:\n                mask = np.where(other == 1, False, mask)\n            # x ** 0 is 1.\n            mask = np.where((self._data == 0) & ~self._mask, False, mask)\n\n        if other is libmissing.NA:\n            result = np.ones_like(self._data)\n        else:\n            with np.errstate(all=\"ignore\"):\n                result = op(self._data, other)\n\n        # divmod returns a tuple\n        if op_name == \"divmod\":\n            div, mod = result\n            return (\n                self._maybe_mask_result(div, mask, other, \"floordiv\"),\n                self._maybe_mask_result(mod, mask, other, \"mod\"),\n            )\n\n        return self._maybe_mask_result(result, mask, other, op_name)\n\n    def sum(self, skipna=True, min_count=0, **kwargs):\n        nv.validate_sum((), kwargs)\n        return super()._reduce(\"sum\", skipna=skipna, min_count=min_count)\n\n    def prod(self, skipna=True, min_count=0, **kwargs):\n        nv.validate_prod((), kwargs)\n        return super()._reduce(\"prod\", skipna=skipna, min_count=min_count)\n\n    def min(self, skipna=True, **kwargs):\n        nv.validate_min((), kwargs)\n        return super()._reduce(\"min\", skipna=skipna)\n\n    def max(self, skipna=True, **kwargs):\n        nv.validate_max((), kwargs)\n        return super()._reduce(\"max\", skipna=skipna)\n\n    def _maybe_mask_result(self, result, mask, other, op_name: str):\n        \"\"\"\n        Parameters\n        ----------\n        result : array-like\n        mask : array-like bool\n        other : scalar or array-like\n        op_name : str\n        \"\"\"\n        # if we have a float operand we are by-definition\n        # a float result\n        # or our op is a divide\n        if (is_float_dtype(other) or is_float(other)) or (\n            op_name in [\"rtruediv\", \"truediv\"]\n        ):\n            result[mask] = np.nan\n            return result\n\n        if result.dtype == \"timedelta64[ns]\":\n            from pandas.core.arrays import TimedeltaArray\n\n            result[mask] = iNaT\n            return TimedeltaArray._simple_new(result)\n\n        return type(self)(result, mask, copy=False)\n\n\n_dtype_docstring = \"\"\"\nAn ExtensionDtype for {dtype} integer data.\n\n.. versionchanged:: 1.0.0\n\n   Now uses :attr:`pandas.NA` as its missing value,\n   rather than :attr:`numpy.nan`.\n\nAttributes\n----------\nNone\n\nMethods\n-------\nNone\n\"\"\"\n\n# create the Dtype\n\n\n@register_extension_dtype\nclass Int8Dtype(_IntegerDtype):\n    type = np.int8\n    name = \"Int8\"\n    __doc__ = _dtype_docstring.format(dtype=\"int8\")\n\n\n@register_extension_dtype\nclass Int16Dtype(_IntegerDtype):\n    type = np.int16\n    name = \"Int16\"\n    __doc__ = _dtype_docstring.format(dtype=\"int16\")\n\n\n@register_extension_dtype\nclass Int32Dtype(_IntegerDtype):\n    type = np.int32\n    name = \"Int32\"\n    __doc__ = _dtype_docstring.format(dtype=\"int32\")\n\n\n@register_extension_dtype\nclass Int64Dtype(_IntegerDtype):\n    type = np.int64\n    name = \"Int64\"\n    __doc__ = _dtype_docstring.format(dtype=\"int64\")\n\n\n@register_extension_dtype\nclass UInt8Dtype(_IntegerDtype):\n    type = np.uint8\n    name = \"UInt8\"\n    __doc__ = _dtype_docstring.format(dtype=\"uint8\")\n\n\n@register_extension_dtype\nclass UInt16Dtype(_IntegerDtype):\n    type = np.uint16\n    name = \"UInt16\"\n    __doc__ = _dtype_docstring.format(dtype=\"uint16\")\n\n\n@register_extension_dtype\nclass UInt32Dtype(_IntegerDtype):\n    type = np.uint32\n    name = \"UInt32\"\n    __doc__ = _dtype_docstring.format(dtype=\"uint32\")\n\n\n@register_extension_dtype\nclass UInt64Dtype(_IntegerDtype):\n    type = np.uint64\n    name = \"UInt64\"\n    __doc__ = _dtype_docstring.format(dtype=\"uint64\")\n\n\nINT_STR_TO_DTYPE: Dict[str, _IntegerDtype] = {\n    \"int8\": Int8Dtype(),\n    \"int16\": Int16Dtype(),\n    \"int32\": Int32Dtype(),\n    \"int64\": Int64Dtype(),\n    \"uint8\": UInt8Dtype(),\n    \"uint16\": UInt16Dtype(),\n    \"uint32\": UInt32Dtype(),\n    \"uint64\": UInt64Dtype(),\n}\n"
    },
    {
      "filename": "pandas/core/dtypes/base.py",
      "content": "\"\"\"\nExtend pandas with custom array types.\n\"\"\"\n\nfrom typing import TYPE_CHECKING, Any, List, Optional, Tuple, Type, Union\n\nimport numpy as np\n\nfrom pandas._typing import DtypeObj\nfrom pandas.errors import AbstractMethodError\n\nfrom pandas.core.dtypes.generic import ABCDataFrame, ABCIndexClass, ABCSeries\n\nif TYPE_CHECKING:\n    from pandas.core.arrays import ExtensionArray\n\n\nclass ExtensionDtype:\n    \"\"\"\n    A custom data type, to be paired with an ExtensionArray.\n\n    See Also\n    --------\n    extensions.register_extension_dtype: Register an ExtensionType\n        with pandas as class decorator.\n    extensions.ExtensionArray: Abstract base class for custom 1-D array types.\n\n    Notes\n    -----\n    The interface includes the following abstract methods that must\n    be implemented by subclasses:\n\n    * type\n    * name\n\n    The following attributes and methods influence the behavior of the dtype in\n    pandas operations\n\n    * _is_numeric\n    * _is_boolean\n    * _get_common_dtype\n\n    Optionally one can override construct_array_type for construction\n    with the name of this dtype via the Registry. See\n    :meth:`extensions.register_extension_dtype`.\n\n    * construct_array_type\n\n    The `na_value` class attribute can be used to set the default NA value\n    for this type. :attr:`numpy.nan` is used by default.\n\n    ExtensionDtypes are required to be hashable. The base class provides\n    a default implementation, which relies on the ``_metadata`` class\n    attribute. ``_metadata`` should be a tuple containing the strings\n    that define your data type. For example, with ``PeriodDtype`` that's\n    the ``freq`` attribute.\n\n    **If you have a parametrized dtype you should set the ``_metadata``\n    class property**.\n\n    Ideally, the attributes in ``_metadata`` will match the\n    parameters to your ``ExtensionDtype.__init__`` (if any). If any of\n    the attributes in ``_metadata`` don't implement the standard\n    ``__eq__`` or ``__hash__``, the default implementations here will not\n    work.\n\n    .. versionchanged:: 0.24.0\n\n       Added ``_metadata``, ``__hash__``, and changed the default definition\n       of ``__eq__``.\n\n    For interaction with Apache Arrow (pyarrow), a ``__from_arrow__`` method\n    can be implemented: this method receives a pyarrow Array or ChunkedArray\n    as only argument and is expected to return the appropriate pandas\n    ExtensionArray for this dtype and the passed values::\n\n        class ExtensionDtype:\n\n            def __from_arrow__(\n                self, array: Union[pyarrow.Array, pyarrow.ChunkedArray]\n            ) -> ExtensionArray:\n                ...\n\n    This class does not inherit from 'abc.ABCMeta' for performance reasons.\n    Methods and properties required by the interface raise\n    ``pandas.errors.AbstractMethodError`` and no ``register`` method is\n    provided for registering virtual subclasses.\n    \"\"\"\n\n    _metadata: Tuple[str, ...] = ()\n\n    def __str__(self) -> str:\n        return self.name\n\n    def __eq__(self, other: Any) -> bool:\n        \"\"\"\n        Check whether 'other' is equal to self.\n\n        By default, 'other' is considered equal if either\n\n        * it's a string matching 'self.name'.\n        * it's an instance of this type and all of the\n          the attributes in ``self._metadata`` are equal between\n          `self` and `other`.\n\n        Parameters\n        ----------\n        other : Any\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        if isinstance(other, str):\n            try:\n                other = self.construct_from_string(other)\n            except TypeError:\n                return False\n        if isinstance(other, type(self)):\n            return all(\n                getattr(self, attr) == getattr(other, attr) for attr in self._metadata\n            )\n        return False\n\n    def __hash__(self) -> int:\n        return hash(tuple(getattr(self, attr) for attr in self._metadata))\n\n    def __ne__(self, other: Any) -> bool:\n        return not self.__eq__(other)\n\n    @property\n    def na_value(self) -> object:\n        \"\"\"\n        Default NA value to use for this type.\n\n        This is used in e.g. ExtensionArray.take. This should be the\n        user-facing \"boxed\" version of the NA value, not the physical NA value\n        for storage.  e.g. for JSONArray, this is an empty dictionary.\n        \"\"\"\n        return np.nan\n\n    @property\n    def type(self) -> Type:\n        \"\"\"\n        The scalar type for the array, e.g. ``int``\n\n        It's expected ``ExtensionArray[item]`` returns an instance\n        of ``ExtensionDtype.type`` for scalar ``item``, assuming\n        that value is valid (not NA). NA values do not need to be\n        instances of `type`.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def kind(self) -> str:\n        \"\"\"\n        A character code (one of 'biufcmMOSUV'), default 'O'\n\n        This should match the NumPy dtype used when the array is\n        converted to an ndarray, which is probably 'O' for object if\n        the extension type cannot be represented as a built-in NumPy\n        type.\n\n        See Also\n        --------\n        numpy.dtype.kind\n        \"\"\"\n        return \"O\"\n\n    @property\n    def name(self) -> str:\n        \"\"\"\n        A string identifying the data type.\n\n        Will be used for display in, e.g. ``Series.dtype``\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def names(self) -> Optional[List[str]]:\n        \"\"\"\n        Ordered list of field names, or None if there are no fields.\n\n        This is for compatibility with NumPy arrays, and may be removed in the\n        future.\n        \"\"\"\n        return None\n\n    @classmethod\n    def construct_array_type(cls) -> Type[\"ExtensionArray\"]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def construct_from_string(cls, string: str):\n        r\"\"\"\n        Construct this type from a string.\n\n        This is useful mainly for data types that accept parameters.\n        For example, a period dtype accepts a frequency parameter that\n        can be set as ``period[H]`` (where H means hourly frequency).\n\n        By default, in the abstract class, just the name of the type is\n        expected. But subclasses can overwrite this method to accept\n        parameters.\n\n        Parameters\n        ----------\n        string : str\n            The name of the type, for example ``category``.\n\n        Returns\n        -------\n        ExtensionDtype\n            Instance of the dtype.\n\n        Raises\n        ------\n        TypeError\n            If a class cannot be constructed from this 'string'.\n\n        Examples\n        --------\n        For extension dtypes with arguments the following may be an\n        adequate implementation.\n\n        >>> @classmethod\n        ... def construct_from_string(cls, string):\n        ...     pattern = re.compile(r\"^my_type\\[(?P<arg_name>.+)\\]$\")\n        ...     match = pattern.match(string)\n        ...     if match:\n        ...         return cls(**match.groupdict())\n        ...     else:\n        ...         raise TypeError(\n        ...             f\"Cannot construct a '{cls.__name__}' from '{string}'\"\n        ...         )\n        \"\"\"\n        if not isinstance(string, str):\n            raise TypeError(\n                f\"'construct_from_string' expects a string, got {type(string)}\"\n            )\n        # error: Non-overlapping equality check (left operand type: \"str\", right\n        #  operand type: \"Callable[[ExtensionDtype], str]\")  [comparison-overlap]\n        assert isinstance(cls.name, str), (cls, type(cls.name))\n        if string != cls.name:\n            raise TypeError(f\"Cannot construct a '{cls.__name__}' from '{string}'\")\n        return cls()\n\n    @classmethod\n    def is_dtype(cls, dtype: object) -> bool:\n        \"\"\"\n        Check if we match 'dtype'.\n\n        Parameters\n        ----------\n        dtype : object\n            The object to check.\n\n        Returns\n        -------\n        bool\n\n        Notes\n        -----\n        The default implementation is True if\n\n        1. ``cls.construct_from_string(dtype)`` is an instance\n           of ``cls``.\n        2. ``dtype`` is an object and is an instance of ``cls``\n        3. ``dtype`` has a ``dtype`` attribute, and any of the above\n           conditions is true for ``dtype.dtype``.\n        \"\"\"\n        dtype = getattr(dtype, \"dtype\", dtype)\n\n        if isinstance(dtype, (ABCSeries, ABCIndexClass, ABCDataFrame, np.dtype)):\n            # https://github.com/pandas-dev/pandas/issues/22960\n            # avoid passing data to `construct_from_string`. This could\n            # cause a FutureWarning from numpy about failing elementwise\n            # comparison from, e.g., comparing DataFrame == 'category'.\n            return False\n        elif dtype is None:\n            return False\n        elif isinstance(dtype, cls):\n            return True\n        if isinstance(dtype, str):\n            try:\n                return cls.construct_from_string(dtype) is not None\n            except TypeError:\n                return False\n        return False\n\n    @property\n    def _is_numeric(self) -> bool:\n        \"\"\"\n        Whether columns with this dtype should be considered numeric.\n\n        By default ExtensionDtypes are assumed to be non-numeric.\n        They'll be excluded from operations that exclude non-numeric\n        columns, like (groupby) reductions, plotting, etc.\n        \"\"\"\n        return False\n\n    @property\n    def _is_boolean(self) -> bool:\n        \"\"\"\n        Whether this dtype should be considered boolean.\n\n        By default, ExtensionDtypes are assumed to be non-numeric.\n        Setting this to True will affect the behavior of several places,\n        e.g.\n\n        * is_bool\n        * boolean indexing\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return False\n\n    def _get_common_dtype(self, dtypes: List[DtypeObj]) -> Optional[DtypeObj]:\n        \"\"\"\n        Return the common dtype, if one exists.\n\n        Used in `find_common_type` implementation. This is for example used\n        to determine the resulting dtype in a concat operation.\n\n        If no common dtype exists, return None (which gives the other dtypes\n        the chance to determine a common dtype). If all dtypes in the list\n        return None, then the common dtype will be \"object\" dtype (this means\n        it is never needed to return \"object\" dtype from this method itself).\n\n        Parameters\n        ----------\n        dtypes : list of dtypes\n            The dtypes for which to determine a common dtype. This is a list\n            of np.dtype or ExtensionDtype instances.\n\n        Returns\n        -------\n        Common dtype (np.dtype or ExtensionDtype) or None\n        \"\"\"\n        if len(set(dtypes)) == 1:\n            # only itself\n            return self\n        else:\n            return None\n\n\ndef register_extension_dtype(cls: Type[ExtensionDtype]) -> Type[ExtensionDtype]:\n    \"\"\"\n    Register an ExtensionType with pandas as class decorator.\n\n    .. versionadded:: 0.24.0\n\n    This enables operations like ``.astype(name)`` for the name\n    of the ExtensionDtype.\n\n    Returns\n    -------\n    callable\n        A class decorator.\n\n    Examples\n    --------\n    >>> from pandas.api.extensions import register_extension_dtype\n    >>> from pandas.api.extensions import ExtensionDtype\n    >>> @register_extension_dtype\n    ... class MyExtensionDtype(ExtensionDtype):\n    ...     name = \"myextension\"\n    \"\"\"\n    registry.register(cls)\n    return cls\n\n\nclass Registry:\n    \"\"\"\n    Registry for dtype inference.\n\n    The registry allows one to map a string repr of a extension\n    dtype to an extension dtype. The string alias can be used in several\n    places, including\n\n    * Series and Index constructors\n    * :meth:`pandas.array`\n    * :meth:`pandas.Series.astype`\n\n    Multiple extension types can be registered.\n    These are tried in order.\n    \"\"\"\n\n    def __init__(self):\n        self.dtypes: List[Type[ExtensionDtype]] = []\n\n    def register(self, dtype: Type[ExtensionDtype]) -> None:\n        \"\"\"\n        Parameters\n        ----------\n        dtype : ExtensionDtype class\n        \"\"\"\n        if not issubclass(dtype, ExtensionDtype):\n            raise ValueError(\"can only register pandas extension dtypes\")\n\n        self.dtypes.append(dtype)\n\n    def find(\n        self, dtype: Union[Type[ExtensionDtype], str]\n    ) -> Optional[Type[ExtensionDtype]]:\n        \"\"\"\n        Parameters\n        ----------\n        dtype : Type[ExtensionDtype] or str\n\n        Returns\n        -------\n        return the first matching dtype, otherwise return None\n        \"\"\"\n        if not isinstance(dtype, str):\n            dtype_type = dtype\n            if not isinstance(dtype, type):\n                dtype_type = type(dtype)\n            if issubclass(dtype_type, ExtensionDtype):\n                return dtype\n\n            return None\n\n        for dtype_type in self.dtypes:\n            try:\n                return dtype_type.construct_from_string(dtype)\n            except TypeError:\n                pass\n\n        return None\n\n\nregistry = Registry()\n"
    },
    {
      "filename": "pandas/core/frame.py",
      "content": "\"\"\"\nDataFrame\n---------\nAn efficient 2D container for potentially mixed-type time series or other\nlabeled data series.\n\nSimilar to its R counterpart, data.frame, except providing automatic data\nalignment and a host of useful data manipulation methods having to do with the\nlabeling information\n\"\"\"\nfrom __future__ import annotations\n\nimport collections\nfrom collections import abc\nimport datetime\nfrom io import StringIO\nimport itertools\nfrom textwrap import dedent\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    AnyStr,\n    Dict,\n    FrozenSet,\n    Hashable,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\nimport warnings\n\nimport numpy as np\nimport numpy.ma as ma\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import algos as libalgos, lib, properties\nfrom pandas._libs.lib import no_default\nfrom pandas._typing import (\n    AggFuncType,\n    ArrayLike,\n    Axes,\n    Axis,\n    CompressionOptions,\n    Dtype,\n    FilePathOrBuffer,\n    FrameOrSeriesUnion,\n    IndexKeyFunc,\n    Label,\n    Level,\n    Renamer,\n    StorageOptions,\n    ValueKeyFunc,\n)\nfrom pandas.compat._optional import import_optional_dependency\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import (\n    Appender,\n    Substitution,\n    deprecate_kwarg,\n    doc,\n    rewrite_axis_style_signature,\n)\nfrom pandas.util._validators import (\n    validate_axis_style_args,\n    validate_bool_kwarg,\n    validate_percentile,\n)\n\nfrom pandas.core.dtypes.cast import (\n    cast_scalar_to_array,\n    coerce_to_dtypes,\n    construct_1d_arraylike_from_scalar,\n    find_common_type,\n    infer_dtype_from_scalar,\n    invalidate_string_dtypes,\n    maybe_cast_to_datetime,\n    maybe_casted_values,\n    maybe_convert_platform,\n    maybe_downcast_to_dtype,\n    maybe_infer_to_datetimelike,\n    maybe_upcast,\n    validate_numeric_casting,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_int64,\n    ensure_platform_int,\n    infer_dtype_from_object,\n    is_bool_dtype,\n    is_dataclass,\n    is_datetime64_any_dtype,\n    is_dict_like,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float,\n    is_float_dtype,\n    is_hashable,\n    is_integer,\n    is_integer_dtype,\n    is_iterator,\n    is_list_like,\n    is_named_tuple,\n    is_object_dtype,\n    is_scalar,\n    is_sequence,\n    needs_i8_conversion,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.missing import isna, notna\n\nfrom pandas.core import algorithms, common as com, nanops, ops\nfrom pandas.core.accessor import CachedAccessor\nfrom pandas.core.aggregation import (\n    aggregate,\n    reconstruct_func,\n    relabel_result,\n    transform,\n)\nfrom pandas.core.arraylike import OpsMixin\nfrom pandas.core.arrays import Categorical, ExtensionArray\nfrom pandas.core.arrays.sparse import SparseFrameAccessor\nfrom pandas.core.construction import extract_array\nfrom pandas.core.generic import NDFrame, _shared_docs\nfrom pandas.core.indexes import base as ibase\nfrom pandas.core.indexes.api import Index, ensure_index, ensure_index_from_sequences\nfrom pandas.core.indexes.multi import MultiIndex, maybe_droplevels\nfrom pandas.core.indexing import check_bool_indexer, convert_to_index_sliceable\nfrom pandas.core.internals import BlockManager\nfrom pandas.core.internals.construction import (\n    arrays_to_mgr,\n    dataclasses_to_dicts,\n    get_names_from_index,\n    init_dict,\n    init_ndarray,\n    masked_rec_array_to_mgr,\n    reorder_arrays,\n    sanitize_index,\n    to_arrays,\n)\nfrom pandas.core.reshape.melt import melt\nfrom pandas.core.series import Series\nfrom pandas.core.sorting import get_group_index, lexsort_indexer, nargsort\n\nfrom pandas.io.common import get_filepath_or_buffer\nfrom pandas.io.formats import console, format as fmt\nfrom pandas.io.formats.info import DataFrameInfo\nimport pandas.plotting\n\nif TYPE_CHECKING:\n    from pandas.core.groupby.generic import DataFrameGroupBy\n\n    from pandas.io.formats.style import Styler\n\n# ---------------------------------------------------------------------\n# Docstring templates\n\n_shared_doc_kwargs = dict(\n    axes=\"index, columns\",\n    klass=\"DataFrame\",\n    axes_single_arg=\"{0 or 'index', 1 or 'columns'}\",\n    axis=\"\"\"axis : {0 or 'index', 1 or 'columns'}, default 0\n        If 0 or 'index': apply function to each column.\n        If 1 or 'columns': apply function to each row.\"\"\",\n    optional_by=\"\"\"\n        by : str or list of str\n            Name or list of names to sort by.\n\n            - if `axis` is 0 or `'index'` then `by` may contain index\n              levels and/or column labels.\n            - if `axis` is 1 or `'columns'` then `by` may contain column\n              levels and/or index labels.\"\"\",\n    optional_labels=\"\"\"labels : array-like, optional\n            New labels / index to conform the axis specified by 'axis' to.\"\"\",\n    optional_axis=\"\"\"axis : int or str, optional\n            Axis to target. Can be either the axis name ('index', 'columns')\n            or number (0, 1).\"\"\",\n)\n\n_numeric_only_doc = \"\"\"numeric_only : boolean, default None\n    Include only float, int, boolean data. If None, will attempt to use\n    everything, then use only numeric data\n\"\"\"\n\n_merge_doc = \"\"\"\nMerge DataFrame or named Series objects with a database-style join.\n\nThe join is done on columns or indexes. If joining columns on\ncolumns, the DataFrame indexes *will be ignored*. Otherwise if joining indexes\non indexes or indexes on a column or columns, the index will be passed on.\n\nParameters\n----------%s\nright : DataFrame or named Series\n    Object to merge with.\nhow : {'left', 'right', 'outer', 'inner'}, default 'inner'\n    Type of merge to be performed.\n\n    * left: use only keys from left frame, similar to a SQL left outer join;\n      preserve key order.\n    * right: use only keys from right frame, similar to a SQL right outer join;\n      preserve key order.\n    * outer: use union of keys from both frames, similar to a SQL full outer\n      join; sort keys lexicographically.\n    * inner: use intersection of keys from both frames, similar to a SQL inner\n      join; preserve the order of the left keys.\non : label or list\n    Column or index level names to join on. These must be found in both\n    DataFrames. If `on` is None and not merging on indexes then this defaults\n    to the intersection of the columns in both DataFrames.\nleft_on : label or list, or array-like\n    Column or index level names to join on in the left DataFrame. Can also\n    be an array or list of arrays of the length of the left DataFrame.\n    These arrays are treated as if they are columns.\nright_on : label or list, or array-like\n    Column or index level names to join on in the right DataFrame. Can also\n    be an array or list of arrays of the length of the right DataFrame.\n    These arrays are treated as if they are columns.\nleft_index : bool, default False\n    Use the index from the left DataFrame as the join key(s). If it is a\n    MultiIndex, the number of keys in the other DataFrame (either the index\n    or a number of columns) must match the number of levels.\nright_index : bool, default False\n    Use the index from the right DataFrame as the join key. Same caveats as\n    left_index.\nsort : bool, default False\n    Sort the join keys lexicographically in the result DataFrame. If False,\n    the order of the join keys depends on the join type (how keyword).\nsuffixes : list-like, default is (\"_x\", \"_y\")\n    A length-2 sequence where each element is optionally a string\n    indicating the suffix to add to overlapping column names in\n    `left` and `right` respectively. Pass a value of `None` instead\n    of a string to indicate that the column name from `left` or\n    `right` should be left as-is, with no suffix. At least one of the\n    values must not be None.\ncopy : bool, default True\n    If False, avoid copy if possible.\nindicator : bool or str, default False\n    If True, adds a column to the output DataFrame called \"_merge\" with\n    information on the source of each row. The column can be given a different\n    name by providing a string argument. The column will have a Categorical\n    type with the value of \"left_only\" for observations whose merge key only\n    appears in the left DataFrame, \"right_only\" for observations\n    whose merge key only appears in the right DataFrame, and \"both\"\n    if the observation's merge key is found in both DataFrames.\n\nvalidate : str, optional\n    If specified, checks if merge is of specified type.\n\n    * \"one_to_one\" or \"1:1\": check if merge keys are unique in both\n      left and right datasets.\n    * \"one_to_many\" or \"1:m\": check if merge keys are unique in left\n      dataset.\n    * \"many_to_one\" or \"m:1\": check if merge keys are unique in right\n      dataset.\n    * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.\n\nReturns\n-------\nDataFrame\n    A DataFrame of the two merged objects.\n\nSee Also\n--------\nmerge_ordered : Merge with optional filling/interpolation.\nmerge_asof : Merge on nearest keys.\nDataFrame.join : Similar method using indices.\n\nNotes\n-----\nSupport for specifying index levels as the `on`, `left_on`, and\n`right_on` parameters was added in version 0.23.0\nSupport for merging named Series objects was added in version 0.24.0\n\nExamples\n--------\n>>> df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [1, 2, 3, 5]})\n>>> df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [5, 6, 7, 8]})\n>>> df1\n    lkey value\n0   foo      1\n1   bar      2\n2   baz      3\n3   foo      5\n>>> df2\n    rkey value\n0   foo      5\n1   bar      6\n2   baz      7\n3   foo      8\n\nMerge df1 and df2 on the lkey and rkey columns. The value columns have\nthe default suffixes, _x and _y, appended.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey')\n  lkey  value_x rkey  value_y\n0  foo        1  foo        5\n1  foo        1  foo        8\n2  foo        5  foo        5\n3  foo        5  foo        8\n4  bar        2  bar        6\n5  baz        3  baz        7\n\nMerge DataFrames df1 and df2 with specified left and right suffixes\nappended to any overlapping columns.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey',\n...           suffixes=('_left', '_right'))\n  lkey  value_left rkey  value_right\n0  foo           1  foo            5\n1  foo           1  foo            8\n2  foo           5  foo            5\n3  foo           5  foo            8\n4  bar           2  bar            6\n5  baz           3  baz            7\n\nMerge DataFrames df1 and df2, but raise an exception if the DataFrames have\nany overlapping columns.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))\nTraceback (most recent call last):\n...\nValueError: columns overlap but no suffix specified:\n    Index(['value'], dtype='object')\n\"\"\"\n\n\n# -----------------------------------------------------------------------\n# DataFrame class\n\n\nclass DataFrame(NDFrame, OpsMixin):\n    \"\"\"\n    Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\n    Data structure also contains labeled axes (rows and columns).\n    Arithmetic operations align on both row and column labels. Can be\n    thought of as a dict-like container for Series objects. The primary\n    pandas data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n        Dict can contain Series, arrays, constants, or list-like objects. If\n        data is a dict, column order follows insertion-order.\n\n        .. versionchanged:: 0.25.0\n           If data is a list of dicts, column order follows insertion-order.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing information part of input data and no index provided.\n    columns : Index or array-like\n        Column labels to use for resulting frame. Will default to\n        RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is allowed. If None, infer.\n    copy : bool, default False\n        Copy data from inputs. Only affects DataFrame / 2d ndarray input.\n\n    See Also\n    --------\n    DataFrame.from_records : Constructor from tuples, also record arrays.\n    DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n    read_csv : Read a comma-separated values (csv) file into DataFrame.\n    read_table : Read general delimited file into DataFrame.\n    read_clipboard : Read text from clipboard into DataFrame.\n\n    Examples\n    --------\n    Constructing DataFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> df = pd.DataFrame(data=d)\n    >>> df\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> df.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> df = pd.DataFrame(data=d, dtype=np.int8)\n    >>> df.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing DataFrame from numpy ndarray:\n\n    >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> df2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n    \"\"\"\n\n    _internal_names_set = {\"columns\", \"index\"} | NDFrame._internal_names_set\n    _typ = \"dataframe\"\n\n    @property\n    def _constructor(self) -> Type[DataFrame]:\n        return DataFrame\n\n    _constructor_sliced: Type[Series] = Series\n    _deprecations: FrozenSet[str] = NDFrame._deprecations | frozenset([])\n    _accessors: Set[str] = {\"sparse\"}\n\n    @property\n    def _constructor_expanddim(self):\n        # GH#31549 raising NotImplementedError on a property causes trouble\n        #  for `inspect`\n        def constructor(*args, **kwargs):\n            raise NotImplementedError(\"Not supported for DataFrames!\")\n\n        return constructor\n\n    # ----------------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self,\n        data=None,\n        index: Optional[Axes] = None,\n        columns: Optional[Axes] = None,\n        dtype: Optional[Dtype] = None,\n        copy: bool = False,\n    ):\n        if data is None:\n            data = {}\n        if dtype is not None:\n            dtype = self._validate_dtype(dtype)\n\n        if isinstance(data, DataFrame):\n            data = data._mgr\n\n        if isinstance(data, BlockManager):\n            if index is None and columns is None and dtype is None and copy is False:\n                # GH#33357 fastpath\n                NDFrame.__init__(self, data)\n                return\n\n            mgr = self._init_mgr(\n                data, axes=dict(index=index, columns=columns), dtype=dtype, copy=copy\n            )\n\n        elif isinstance(data, dict):\n            mgr = init_dict(data, index, columns, dtype=dtype)\n        elif isinstance(data, ma.MaskedArray):\n            import numpy.ma.mrecords as mrecords\n\n            # masked recarray\n            if isinstance(data, mrecords.MaskedRecords):\n                mgr = masked_rec_array_to_mgr(data, index, columns, dtype, copy)\n\n            # a masked array\n            else:\n                mask = ma.getmaskarray(data)\n                if mask.any():\n                    data, fill_value = maybe_upcast(data, copy=True)\n                    data.soften_mask()  # set hardmask False if it was True\n                    data[mask] = fill_value\n                else:\n                    data = data.copy()\n                mgr = init_ndarray(data, index, columns, dtype=dtype, copy=copy)\n\n        elif isinstance(data, (np.ndarray, Series, Index)):\n            if data.dtype.names:\n                data_columns = list(data.dtype.names)\n                data = {k: data[k] for k in data_columns}\n                if columns is None:\n                    columns = data_columns\n                mgr = init_dict(data, index, columns, dtype=dtype)\n            elif getattr(data, \"name\", None) is not None:\n                mgr = init_dict({data.name: data}, index, columns, dtype=dtype)\n            else:\n                mgr = init_ndarray(data, index, columns, dtype=dtype, copy=copy)\n\n        # For data is list-like, or Iterable (will consume into list)\n        elif isinstance(data, abc.Iterable) and not isinstance(data, (str, bytes)):\n            if not isinstance(data, (abc.Sequence, ExtensionArray)):\n                data = list(data)\n            if len(data) > 0:\n                if is_dataclass(data[0]):\n                    data = dataclasses_to_dicts(data)\n                if is_list_like(data[0]) and getattr(data[0], \"ndim\", 1) == 1:\n                    if is_named_tuple(data[0]) and columns is None:\n                        columns = data[0]._fields\n                    arrays, columns = to_arrays(data, columns, dtype=dtype)\n                    columns = ensure_index(columns)\n\n                    # set the index\n                    if index is None:\n                        if isinstance(data[0], Series):\n                            index = get_names_from_index(data)\n                        elif isinstance(data[0], Categorical):\n                            index = ibase.default_index(len(data[0]))\n                        else:\n                            index = ibase.default_index(len(data))\n\n                    mgr = arrays_to_mgr(arrays, columns, index, columns, dtype=dtype)\n                else:\n                    mgr = init_ndarray(data, index, columns, dtype=dtype, copy=copy)\n            else:\n                mgr = init_dict({}, index, columns, dtype=dtype)\n        # For data is scalar\n        else:\n            if index is None or columns is None:\n                raise ValueError(\"DataFrame constructor not properly called!\")\n\n            if not dtype:\n                dtype, _ = infer_dtype_from_scalar(data, pandas_dtype=True)\n\n            # For data is a scalar extension dtype\n            if is_extension_array_dtype(dtype):\n\n                values = [\n                    construct_1d_arraylike_from_scalar(data, len(index), dtype)\n                    for _ in range(len(columns))\n                ]\n                mgr = arrays_to_mgr(values, columns, index, columns, dtype=None)\n            else:\n                # Attempt to coerce to a numpy array\n                try:\n                    arr = np.array(data, dtype=dtype, copy=copy)\n                except (ValueError, TypeError) as err:\n                    exc = TypeError(\n                        \"DataFrame constructor called with \"\n                        f\"incompatible data and dtype: {err}\"\n                    )\n                    raise exc from err\n\n                if arr.ndim != 0:\n                    raise ValueError(\"DataFrame constructor not properly called!\")\n\n                values = cast_scalar_to_array(\n                    (len(index), len(columns)), data, dtype=dtype\n                )\n\n                mgr = init_ndarray(\n                    values, index, columns, dtype=values.dtype, copy=False\n                )\n\n        NDFrame.__init__(self, mgr)\n\n    # ----------------------------------------------------------------------\n\n    @property\n    def axes(self) -> List[Index]:\n        \"\"\"\n        Return a list representing the axes of the DataFrame.\n\n        It has the row axis labels and column axis labels as the only members.\n        They are returned in that order.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.axes\n        [RangeIndex(start=0, stop=2, step=1), Index(['col1', 'col2'],\n        dtype='object')]\n        \"\"\"\n        return [self.index, self.columns]\n\n    @property\n    def shape(self) -> Tuple[int, int]:\n        \"\"\"\n        Return a tuple representing the dimensionality of the DataFrame.\n\n        See Also\n        --------\n        ndarray.shape : Tuple of array dimensions.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.shape\n        (2, 2)\n\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4],\n        ...                    'col3': [5, 6]})\n        >>> df.shape\n        (2, 3)\n        \"\"\"\n        return len(self.index), len(self.columns)\n\n    @property\n    def _is_homogeneous_type(self) -> bool:\n        \"\"\"\n        Whether all the columns in a DataFrame have the same type.\n\n        Returns\n        -------\n        bool\n\n        See Also\n        --------\n        Index._is_homogeneous_type : Whether the object has a single\n            dtype.\n        MultiIndex._is_homogeneous_type : Whether all the levels of a\n            MultiIndex have the same dtype.\n\n        Examples\n        --------\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3, 4]})._is_homogeneous_type\n        True\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.0]})._is_homogeneous_type\n        False\n\n        Items with the same type but different sizes are considered\n        different types.\n\n        >>> DataFrame({\n        ...    \"A\": np.array([1, 2], dtype=np.int32),\n        ...    \"B\": np.array([1, 2], dtype=np.int64)})._is_homogeneous_type\n        False\n        \"\"\"\n        if self._mgr.any_extension_types:\n            return len({block.dtype for block in self._mgr.blocks}) == 1\n        else:\n            return not self._is_mixed_type\n\n    @property\n    def _can_fast_transpose(self) -> bool:\n        \"\"\"\n        Can we transpose this DataFrame without creating any new array objects.\n        \"\"\"\n        if self._mgr.any_extension_types:\n            # TODO(EA2D) special case would be unnecessary with 2D EAs\n            return False\n        return len(self._mgr.blocks) == 1\n\n    # ----------------------------------------------------------------------\n    # Rendering Methods\n\n    def _repr_fits_vertical_(self) -> bool:\n        \"\"\"\n        Check length against max_rows.\n        \"\"\"\n        max_rows = get_option(\"display.max_rows\")\n        return len(self) <= max_rows\n\n    def _repr_fits_horizontal_(self, ignore_width: bool = False) -> bool:\n        \"\"\"\n        Check if full repr fits in horizontal boundaries imposed by the display\n        options width and max_columns.\n\n        In case of non-interactive session, no boundaries apply.\n\n        `ignore_width` is here so ipynb+HTML output can behave the way\n        users expect. display.max_columns remains in effect.\n        GH3541, GH3573\n        \"\"\"\n        width, height = console.get_console_size()\n        max_columns = get_option(\"display.max_columns\")\n        nb_columns = len(self.columns)\n\n        # exceed max columns\n        if (max_columns and nb_columns > max_columns) or (\n            (not ignore_width) and width and nb_columns > (width // 2)\n        ):\n            return False\n\n        # used by repr_html under IPython notebook or scripts ignore terminal\n        # dims\n        if ignore_width or not console.in_interactive_session():\n            return True\n\n        if get_option(\"display.width\") is not None or console.in_ipython_frontend():\n            # check at least the column row for excessive width\n            max_rows = 1\n        else:\n            max_rows = get_option(\"display.max_rows\")\n\n        # when auto-detecting, so width=None and not in ipython front end\n        # check whether repr fits horizontal by actually checking\n        # the width of the rendered repr\n        buf = StringIO()\n\n        # only care about the stuff we'll actually print out\n        # and to_string on entire frame may be expensive\n        d = self\n\n        if not (max_rows is None):  # unlimited rows\n            # min of two, where one may be None\n            d = d.iloc[: min(max_rows, len(d))]\n        else:\n            return True\n\n        d.to_string(buf=buf)\n        value = buf.getvalue()\n        repr_width = max(len(l) for l in value.split(\"\\n\"))\n\n        return repr_width < width\n\n    def _info_repr(self) -> bool:\n        \"\"\"\n        True if the repr should show the info view.\n        \"\"\"\n        info_repr_option = get_option(\"display.large_repr\") == \"info\"\n        return info_repr_option and not (\n            self._repr_fits_horizontal_() and self._repr_fits_vertical_()\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular DataFrame.\n        \"\"\"\n        buf = StringIO(\"\")\n        if self._info_repr():\n            self.info(buf=buf)\n            return buf.getvalue()\n\n        max_rows = get_option(\"display.max_rows\")\n        min_rows = get_option(\"display.min_rows\")\n        max_cols = get_option(\"display.max_columns\")\n        max_colwidth = get_option(\"display.max_colwidth\")\n        show_dimensions = get_option(\"display.show_dimensions\")\n        if get_option(\"display.expand_frame_repr\"):\n            width, _ = console.get_console_size()\n        else:\n            width = None\n        self.to_string(\n            buf=buf,\n            max_rows=max_rows,\n            min_rows=min_rows,\n            max_cols=max_cols,\n            line_width=width,\n            max_colwidth=max_colwidth,\n            show_dimensions=show_dimensions,\n        )\n\n        return buf.getvalue()\n\n    def _repr_html_(self) -> Optional[str]:\n        \"\"\"\n        Return a html representation for a particular DataFrame.\n\n        Mainly for IPython notebook.\n        \"\"\"\n        if self._info_repr():\n            buf = StringIO(\"\")\n            self.info(buf=buf)\n            # need to escape the <class>, should be the first line.\n            val = buf.getvalue().replace(\"<\", r\"&lt;\", 1)\n            val = val.replace(\">\", r\"&gt;\", 1)\n            return \"<pre>\" + val + \"</pre>\"\n\n        if get_option(\"display.notebook_repr_html\"):\n            max_rows = get_option(\"display.max_rows\")\n            min_rows = get_option(\"display.min_rows\")\n            max_cols = get_option(\"display.max_columns\")\n            show_dimensions = get_option(\"display.show_dimensions\")\n\n            formatter = fmt.DataFrameFormatter(\n                self,\n                columns=None,\n                col_space=None,\n                na_rep=\"NaN\",\n                formatters=None,\n                float_format=None,\n                sparsify=None,\n                justify=None,\n                index_names=True,\n                header=True,\n                index=True,\n                bold_rows=True,\n                escape=True,\n                max_rows=max_rows,\n                min_rows=min_rows,\n                max_cols=max_cols,\n                show_dimensions=show_dimensions,\n                decimal=\".\",\n            )\n            return fmt.DataFrameRenderer(formatter).to_html(notebook=True)\n        else:\n            return None\n\n    @Substitution(\n        header_type=\"bool or sequence\",\n        header=\"Write out the column names. If a list of strings \"\n        \"is given, it is assumed to be aliases for the \"\n        \"column names\",\n        col_space_type=\"int, list or dict of int\",\n        col_space=\"The minimum width of each column\",\n    )\n    @Substitution(shared_params=fmt.common_docstring, returns=fmt.return_docstring)\n    def to_string(\n        self,\n        buf: Optional[FilePathOrBuffer[str]] = None,\n        columns: Optional[Sequence[str]] = None,\n        col_space: Optional[int] = None,\n        header: Union[bool, Sequence[str]] = True,\n        index: bool = True,\n        na_rep: str = \"NaN\",\n        formatters: Optional[fmt.FormattersType] = None,\n        float_format: Optional[fmt.FloatFormatType] = None,\n        sparsify: Optional[bool] = None,\n        index_names: bool = True,\n        justify: Optional[str] = None,\n        max_rows: Optional[int] = None,\n        min_rows: Optional[int] = None,\n        max_cols: Optional[int] = None,\n        show_dimensions: bool = False,\n        decimal: str = \".\",\n        line_width: Optional[int] = None,\n        max_colwidth: Optional[int] = None,\n        encoding: Optional[str] = None,\n    ) -> Optional[str]:\n        \"\"\"\n        Render a DataFrame to a console-friendly tabular output.\n        %(shared_params)s\n        line_width : int, optional\n            Width to wrap a line in characters.\n        max_colwidth : int, optional\n            Max width to truncate each column in characters. By default, no limit.\n\n            .. versionadded:: 1.0.0\n        encoding : str, default \"utf-8\"\n            Set character encoding.\n\n            .. versionadded:: 1.0\n        %(returns)s\n        See Also\n        --------\n        to_html : Convert DataFrame to HTML.\n\n        Examples\n        --------\n        >>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n        >>> df = pd.DataFrame(d)\n        >>> print(df.to_string())\n           col1  col2\n        0     1     4\n        1     2     5\n        2     3     6\n        \"\"\"\n        from pandas import option_context\n\n        with option_context(\"display.max_colwidth\", max_colwidth):\n            formatter = fmt.DataFrameFormatter(\n                self,\n                columns=columns,\n                col_space=col_space,\n                na_rep=na_rep,\n                formatters=formatters,\n                float_format=float_format,\n                sparsify=sparsify,\n                justify=justify,\n                index_names=index_names,\n                header=header,\n                index=index,\n                min_rows=min_rows,\n                max_rows=max_rows,\n                max_cols=max_cols,\n                show_dimensions=show_dimensions,\n                decimal=decimal,\n            )\n            return fmt.DataFrameRenderer(formatter).to_string(\n                buf=buf,\n                encoding=encoding,\n                line_width=line_width,\n            )\n\n    # ----------------------------------------------------------------------\n\n    @property\n    def style(self) -> Styler:\n        \"\"\"\n        Returns a Styler object.\n\n        Contains methods for building a styled HTML representation of the DataFrame.\n\n        See Also\n        --------\n        io.formats.style.Styler : Helps style a DataFrame or Series according to the\n            data with HTML and CSS.\n        \"\"\"\n        from pandas.io.formats.style import Styler\n\n        return Styler(self)\n\n    _shared_docs[\n        \"items\"\n    ] = r\"\"\"\n        Iterate over (column name, Series) pairs.\n\n        Iterates over the DataFrame columns, returning a tuple with\n        the column name and the content as a Series.\n\n        Yields\n        ------\n        label : object\n            The column names for the DataFrame being iterated over.\n        content : Series\n            The column entries belonging to each label, as a Series.\n\n        See Also\n        --------\n        DataFrame.iterrows : Iterate over DataFrame rows as\n            (index, Series) pairs.\n        DataFrame.itertuples : Iterate over DataFrame rows as namedtuples\n            of the values.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],\n        ...                   'population': [1864, 22000, 80000]},\n        ...                   index=['panda', 'polar', 'koala'])\n        >>> df\n                species   population\n        panda   bear      1864\n        polar   bear      22000\n        koala   marsupial 80000\n        >>> for label, content in df.items():\n        ...     print(f'label: {label}')\n        ...     print(f'content: {content}', sep='\\n')\n        ...\n        label: species\n        content:\n        panda         bear\n        polar         bear\n        koala    marsupial\n        Name: species, dtype: object\n        label: population\n        content:\n        panda     1864\n        polar    22000\n        koala    80000\n        Name: population, dtype: int64\n        \"\"\"\n\n    @Appender(_shared_docs[\"items\"])\n    def items(self) -> Iterable[Tuple[Label, Series]]:\n        if self.columns.is_unique and hasattr(self, \"_item_cache\"):\n            for k in self.columns:\n                yield k, self._get_item_cache(k)\n        else:\n            for i, k in enumerate(self.columns):\n                yield k, self._ixs(i, axis=1)\n\n    @Appender(_shared_docs[\"items\"])\n    def iteritems(self) -> Iterable[Tuple[Label, Series]]:\n        yield from self.items()\n\n    def iterrows(self) -> Iterable[Tuple[Label, Series]]:\n        \"\"\"\n        Iterate over DataFrame rows as (index, Series) pairs.\n\n        Yields\n        ------\n        index : label or tuple of label\n            The index of the row. A tuple for a `MultiIndex`.\n        data : Series\n            The data of the row as a Series.\n\n        it : generator\n            A generator that iterates over the rows of the frame.\n\n        See Also\n        --------\n        DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values.\n        DataFrame.items : Iterate over (column name, Series) pairs.\n\n        Notes\n        -----\n        1. Because ``iterrows`` returns a Series for each row,\n           it does **not** preserve dtypes across the rows (dtypes are\n           preserved across columns for DataFrames). For example,\n\n           >>> df = pd.DataFrame([[1, 1.5]], columns=['int', 'float'])\n           >>> row = next(df.iterrows())[1]\n           >>> row\n           int      1.0\n           float    1.5\n           Name: 0, dtype: float64\n           >>> print(row['int'].dtype)\n           float64\n           >>> print(df['int'].dtype)\n           int64\n\n           To preserve dtypes while iterating over the rows, it is better\n           to use :meth:`itertuples` which returns namedtuples of the values\n           and which is generally faster than ``iterrows``.\n\n        2. You should **never modify** something you are iterating over.\n           This is not guaranteed to work in all cases. Depending on the\n           data types, the iterator returns a copy and not a view, and writing\n           to it will have no effect.\n        \"\"\"\n        columns = self.columns\n        klass = self._constructor_sliced\n        for k, v in zip(self.index, self.values):\n            s = klass(v, index=columns, name=k)\n            yield k, s\n\n    def itertuples(self, index: bool = True, name: Optional[str] = \"Pandas\"):\n        \"\"\"\n        Iterate over DataFrame rows as namedtuples.\n\n        Parameters\n        ----------\n        index : bool, default True\n            If True, return the index as the first element of the tuple.\n        name : str or None, default \"Pandas\"\n            The name of the returned namedtuples or None to return regular\n            tuples.\n\n        Returns\n        -------\n        iterator\n            An object to iterate over namedtuples for each row in the\n            DataFrame with the first field possibly being the index and\n            following fields being the column values.\n\n        See Also\n        --------\n        DataFrame.iterrows : Iterate over DataFrame rows as (index, Series)\n            pairs.\n        DataFrame.items : Iterate over (column name, Series) pairs.\n\n        Notes\n        -----\n        The column names will be renamed to positional names if they are\n        invalid Python identifiers, repeated, or start with an underscore.\n        On python versions < 3.7 regular tuples are returned for DataFrames\n        with a large number of columns (>254).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]},\n        ...                   index=['dog', 'hawk'])\n        >>> df\n              num_legs  num_wings\n        dog          4          0\n        hawk         2          2\n        >>> for row in df.itertuples():\n        ...     print(row)\n        ...\n        Pandas(Index='dog', num_legs=4, num_wings=0)\n        Pandas(Index='hawk', num_legs=2, num_wings=2)\n\n        By setting the `index` parameter to False we can remove the index\n        as the first element of the tuple:\n\n        >>> for row in df.itertuples(index=False):\n        ...     print(row)\n        ...\n        Pandas(num_legs=4, num_wings=0)\n        Pandas(num_legs=2, num_wings=2)\n\n        With the `name` parameter set we set a custom name for the yielded\n        namedtuples:\n\n        >>> for row in df.itertuples(name='Animal'):\n        ...     print(row)\n        ...\n        Animal(Index='dog', num_legs=4, num_wings=0)\n        Animal(Index='hawk', num_legs=2, num_wings=2)\n        \"\"\"\n        arrays = []\n        fields = list(self.columns)\n        if index:\n            arrays.append(self.index)\n            fields.insert(0, \"Index\")\n\n        # use integer indexing because of possible duplicate column names\n        arrays.extend(self.iloc[:, k] for k in range(len(self.columns)))\n\n        if name is not None:\n            # https://github.com/python/mypy/issues/9046\n            # error: namedtuple() expects a string literal as the first argument\n            itertuple = collections.namedtuple(  # type: ignore[misc]\n                name, fields, rename=True\n            )\n            return map(itertuple._make, zip(*arrays))\n\n        # fallback to regular tuples\n        return zip(*arrays)\n\n    def __len__(self) -> int:\n        \"\"\"\n        Returns length of info axis, but here we use the index.\n        \"\"\"\n        return len(self.index)\n\n    def dot(self, other):\n        \"\"\"\n        Compute the matrix multiplication between the DataFrame and other.\n\n        This method computes the matrix product between the DataFrame and the\n        values of an other Series, DataFrame or a numpy array.\n\n        It can also be called using ``self @ other`` in Python >= 3.5.\n\n        Parameters\n        ----------\n        other : Series, DataFrame or array-like\n            The other object to compute the matrix product with.\n\n        Returns\n        -------\n        Series or DataFrame\n            If other is a Series, return the matrix product between self and\n            other as a Series. If other is a DataFrame or a numpy.array, return\n            the matrix product of self and other in a DataFrame of a np.array.\n\n        See Also\n        --------\n        Series.dot: Similar method for Series.\n\n        Notes\n        -----\n        The dimensions of DataFrame and other must be compatible in order to\n        compute the matrix multiplication. In addition, the column names of\n        DataFrame and the index of other must contain the same values, as they\n        will be aligned prior to the multiplication.\n\n        The dot method for Series computes the inner product, instead of the\n        matrix product here.\n\n        Examples\n        --------\n        Here we multiply a DataFrame with a Series.\n\n        >>> df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]])\n        >>> s = pd.Series([1, 1, 2, 1])\n        >>> df.dot(s)\n        0    -4\n        1     5\n        dtype: int64\n\n        Here we multiply a DataFrame with another DataFrame.\n\n        >>> other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]])\n        >>> df.dot(other)\n            0   1\n        0   1   4\n        1   2   2\n\n        Note that the dot method give the same result as @\n\n        >>> df @ other\n            0   1\n        0   1   4\n        1   2   2\n\n        The dot method works also if other is an np.array.\n\n        >>> arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]])\n        >>> df.dot(arr)\n            0   1\n        0   1   4\n        1   2   2\n\n        Note how shuffling of the objects does not change the result.\n\n        >>> s2 = s.reindex([1, 0, 2, 3])\n        >>> df.dot(s2)\n        0    -4\n        1     5\n        dtype: int64\n        \"\"\"\n        if isinstance(other, (Series, DataFrame)):\n            common = self.columns.union(other.index)\n            if len(common) > len(self.columns) or len(common) > len(other.index):\n                raise ValueError(\"matrices are not aligned\")\n\n            left = self.reindex(columns=common, copy=False)\n            right = other.reindex(index=common, copy=False)\n            lvals = left.values\n            rvals = right._values\n        else:\n            left = self\n            lvals = self.values\n            rvals = np.asarray(other)\n            if lvals.shape[1] != rvals.shape[0]:\n                raise ValueError(\n                    f\"Dot product shape mismatch, {lvals.shape} vs {rvals.shape}\"\n                )\n\n        if isinstance(other, DataFrame):\n            return self._constructor(\n                np.dot(lvals, rvals), index=left.index, columns=other.columns\n            )\n        elif isinstance(other, Series):\n            return self._constructor_sliced(np.dot(lvals, rvals), index=left.index)\n        elif isinstance(rvals, (np.ndarray, Index)):\n            result = np.dot(lvals, rvals)\n            if result.ndim == 2:\n                return self._constructor(result, index=left.index)\n            else:\n                return self._constructor_sliced(result, index=left.index)\n        else:  # pragma: no cover\n            raise TypeError(f\"unsupported type: {type(other)}\")\n\n    def __matmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(other)\n\n    def __rmatmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        try:\n            return self.T.dot(np.transpose(other)).T\n        except ValueError as err:\n            if \"shape mismatch\" not in str(err):\n                raise\n            # GH#21581 give exception message for original shapes\n            msg = f\"shapes {np.shape(other)} and {self.shape} not aligned\"\n            raise ValueError(msg) from err\n\n    # ----------------------------------------------------------------------\n    # IO methods (to / from other formats)\n\n    @classmethod\n    def from_dict(cls, data, orient=\"columns\", dtype=None, columns=None) -> DataFrame:\n        \"\"\"\n        Construct DataFrame from dict of array-like or dicts.\n\n        Creates DataFrame object from dictionary by columns or by index\n        allowing dtype specification.\n\n        Parameters\n        ----------\n        data : dict\n            Of the form {field : array-like} or {field : dict}.\n        orient : {'columns', 'index'}, default 'columns'\n            The \"orientation\" of the data. If the keys of the passed dict\n            should be the columns of the resulting DataFrame, pass 'columns'\n            (default). Otherwise if the keys should be rows, pass 'index'.\n        dtype : dtype, default None\n            Data type to force, otherwise infer.\n        columns : list, default None\n            Column labels to use when ``orient='index'``. Raises a ValueError\n            if used with ``orient='columns'``.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.from_records : DataFrame from structured ndarray, sequence\n            of tuples or dicts, or DataFrame.\n        DataFrame : DataFrame object creation using constructor.\n\n        Examples\n        --------\n        By default the keys of the dict become the DataFrame columns:\n\n        >>> data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}\n        >>> pd.DataFrame.from_dict(data)\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n\n        Specify ``orient='index'`` to create the DataFrame using dictionary\n        keys as rows:\n\n        >>> data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']}\n        >>> pd.DataFrame.from_dict(data, orient='index')\n               0  1  2  3\n        row_1  3  2  1  0\n        row_2  a  b  c  d\n\n        When using the 'index' orientation, the column names can be\n        specified manually:\n\n        >>> pd.DataFrame.from_dict(data, orient='index',\n        ...                        columns=['A', 'B', 'C', 'D'])\n               A  B  C  D\n        row_1  3  2  1  0\n        row_2  a  b  c  d\n        \"\"\"\n        index = None\n        orient = orient.lower()\n        if orient == \"index\":\n            if len(data) > 0:\n                # TODO speed up Series case\n                if isinstance(list(data.values())[0], (Series, dict)):\n                    data = _from_nested_dict(data)\n                else:\n                    data, index = list(data.values()), list(data.keys())\n        elif orient == \"columns\":\n            if columns is not None:\n                raise ValueError(\"cannot use columns parameter with orient='columns'\")\n        else:  # pragma: no cover\n            raise ValueError(\"only recognize index or columns for orient\")\n\n        return cls(data, index=index, columns=columns, dtype=dtype)\n\n    def to_numpy(\n        self, dtype=None, copy: bool = False, na_value=lib.no_default\n    ) -> np.ndarray:\n        \"\"\"\n        Convert the DataFrame to a NumPy array.\n\n        .. versionadded:: 0.24.0\n\n        By default, the dtype of the returned array will be the common NumPy\n        dtype of all types in the DataFrame. For example, if the dtypes are\n        ``float16`` and ``float32``, the results dtype will be ``float32``.\n        This may require copying data and coercing values, which may be\n        expensive.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to pass to :meth:`numpy.asarray`.\n        copy : bool, default False\n            Whether to ensure that the returned value is not a view on\n            another array. Note that ``copy=False`` does not *ensure* that\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n            a copy is made, even if not strictly necessary.\n        na_value : Any, optional\n            The value to use for missing values. The default value depends\n            on `dtype` and the dtypes of the DataFrame columns.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        numpy.ndarray\n\n        See Also\n        --------\n        Series.to_numpy : Similar method for Series.\n\n        Examples\n        --------\n        >>> pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy()\n        array([[1, 3],\n               [2, 4]])\n\n        With heterogeneous data, the lowest common type will have to\n        be used.\n\n        >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]})\n        >>> df.to_numpy()\n        array([[1. , 3. ],\n               [2. , 4.5]])\n\n        For a mix of numeric and non-numeric types, the output array will\n        have object dtype.\n\n        >>> df['C'] = pd.date_range('2000', periods=2)\n        >>> df.to_numpy()\n        array([[1, 3.0, Timestamp('2000-01-01 00:00:00')],\n               [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object)\n        \"\"\"\n        self._consolidate_inplace()\n        result = self._mgr.as_array(\n            transpose=self._AXIS_REVERSED, dtype=dtype, copy=copy, na_value=na_value\n        )\n        if result.dtype is not dtype:\n            result = np.array(result, dtype=dtype, copy=False)\n\n        return result\n\n    def to_dict(self, orient=\"dict\", into=dict):\n        \"\"\"\n        Convert the DataFrame to a dictionary.\n\n        The type of the key-value pairs can be customized with the parameters\n        (see below).\n\n        Parameters\n        ----------\n        orient : str {'dict', 'list', 'series', 'split', 'records', 'index'}\n            Determines the type of the values of the dictionary.\n\n            - 'dict' (default) : dict like {column -> {index -> value}}\n            - 'list' : dict like {column -> [values]}\n            - 'series' : dict like {column -> Series(values)}\n            - 'split' : dict like\n              {'index' -> [index], 'columns' -> [columns], 'data' -> [values]}\n            - 'records' : list like\n              [{column -> value}, ... , {column -> value}]\n            - 'index' : dict like {index -> {column -> value}}\n\n            Abbreviations are allowed. `s` indicates `series` and `sp`\n            indicates `split`.\n\n        into : class, default dict\n            The collections.abc.Mapping subclass used for all Mappings\n            in the return value.  Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        Returns\n        -------\n        dict, list or collections.abc.Mapping\n            Return a collections.abc.Mapping object representing the DataFrame.\n            The resulting transformation depends on the `orient` parameter.\n\n        See Also\n        --------\n        DataFrame.from_dict: Create a DataFrame from a dictionary.\n        DataFrame.to_json: Convert a DataFrame to JSON format.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2],\n        ...                    'col2': [0.5, 0.75]},\n        ...                   index=['row1', 'row2'])\n        >>> df\n              col1  col2\n        row1     1  0.50\n        row2     2  0.75\n        >>> df.to_dict()\n        {'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}}\n\n        You can specify the return orientation.\n\n        >>> df.to_dict('series')\n        {'col1': row1    1\n                 row2    2\n        Name: col1, dtype: int64,\n        'col2': row1    0.50\n                row2    0.75\n        Name: col2, dtype: float64}\n\n        >>> df.to_dict('split')\n        {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],\n         'data': [[1, 0.5], [2, 0.75]]}\n\n        >>> df.to_dict('records')\n        [{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}]\n\n        >>> df.to_dict('index')\n        {'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}}\n\n        You can also specify the mapping type.\n\n        >>> from collections import OrderedDict, defaultdict\n        >>> df.to_dict(into=OrderedDict)\n        OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])),\n                     ('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))])\n\n        If you want a `defaultdict`, you need to initialize it:\n\n        >>> dd = defaultdict(list)\n        >>> df.to_dict('records', into=dd)\n        [defaultdict(<class 'list'>, {'col1': 1, 'col2': 0.5}),\n         defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})]\n        \"\"\"\n        if not self.columns.is_unique:\n            warnings.warn(\n                \"DataFrame columns are not unique, some columns will be omitted.\",\n                UserWarning,\n                stacklevel=2,\n            )\n        # GH16122\n        into_c = com.standardize_mapping(into)\n\n        orient = orient.lower()\n        # GH32515\n        if orient.startswith((\"d\", \"l\", \"s\", \"r\", \"i\")) and orient not in {\n            \"dict\",\n            \"list\",\n            \"series\",\n            \"split\",\n            \"records\",\n            \"index\",\n        }:\n            warnings.warn(\n                \"Using short name for 'orient' is deprecated. Only the \"\n                \"options: ('dict', list, 'series', 'split', 'records', 'index') \"\n                \"will be used in a future version. Use one of the above \"\n                \"to silence this warning.\",\n                FutureWarning,\n            )\n\n            if orient.startswith(\"d\"):\n                orient = \"dict\"\n            elif orient.startswith(\"l\"):\n                orient = \"list\"\n            elif orient.startswith(\"sp\"):\n                orient = \"split\"\n            elif orient.startswith(\"s\"):\n                orient = \"series\"\n            elif orient.startswith(\"r\"):\n                orient = \"records\"\n            elif orient.startswith(\"i\"):\n                orient = \"index\"\n\n        if orient == \"dict\":\n            return into_c((k, v.to_dict(into)) for k, v in self.items())\n\n        elif orient == \"list\":\n            return into_c((k, v.tolist()) for k, v in self.items())\n\n        elif orient == \"split\":\n            return into_c(\n                (\n                    (\"index\", self.index.tolist()),\n                    (\"columns\", self.columns.tolist()),\n                    (\n                        \"data\",\n                        [\n                            list(map(com.maybe_box_datetimelike, t))\n                            for t in self.itertuples(index=False, name=None)\n                        ],\n                    ),\n                )\n            )\n\n        elif orient == \"series\":\n            return into_c((k, com.maybe_box_datetimelike(v)) for k, v in self.items())\n\n        elif orient == \"records\":\n            columns = self.columns.tolist()\n            rows = (\n                dict(zip(columns, row))\n                for row in self.itertuples(index=False, name=None)\n            )\n            return [\n                into_c((k, com.maybe_box_datetimelike(v)) for k, v in row.items())\n                for row in rows\n            ]\n\n        elif orient == \"index\":\n            if not self.index.is_unique:\n                raise ValueError(\"DataFrame index must be unique for orient='index'.\")\n            return into_c(\n                (t[0], dict(zip(self.columns, t[1:])))\n                for t in self.itertuples(name=None)\n            )\n\n        else:\n            raise ValueError(f\"orient '{orient}' not understood\")\n\n    def to_gbq(\n        self,\n        destination_table,\n        project_id=None,\n        chunksize=None,\n        reauth=False,\n        if_exists=\"fail\",\n        auth_local_webserver=False,\n        table_schema=None,\n        location=None,\n        progress_bar=True,\n        credentials=None,\n    ) -> None:\n        \"\"\"\n        Write a DataFrame to a Google BigQuery table.\n\n        This function requires the `pandas-gbq package\n        <https://pandas-gbq.readthedocs.io>`__.\n\n        See the `How to authenticate with Google BigQuery\n        <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html>`__\n        guide for authentication instructions.\n\n        Parameters\n        ----------\n        destination_table : str\n            Name of table to be written, in the form ``dataset.tablename``.\n        project_id : str, optional\n            Google BigQuery Account project ID. Optional when available from\n            the environment.\n        chunksize : int, optional\n            Number of rows to be inserted in each chunk from the dataframe.\n            Set to ``None`` to load the whole dataframe at once.\n        reauth : bool, default False\n            Force Google BigQuery to re-authenticate the user. This is useful\n            if multiple accounts are used.\n        if_exists : str, default 'fail'\n            Behavior when the destination table exists. Value can be one of:\n\n            ``'fail'``\n                If table exists raise pandas_gbq.gbq.TableCreationError.\n            ``'replace'``\n                If table exists, drop it, recreate it, and insert data.\n            ``'append'``\n                If table exists, insert data. Create if does not exist.\n        auth_local_webserver : bool, default False\n            Use the `local webserver flow`_ instead of the `console flow`_\n            when getting user credentials.\n\n            .. _local webserver flow:\n                https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server\n            .. _console flow:\n                https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console\n\n            *New in version 0.2.0 of pandas-gbq*.\n        table_schema : list of dicts, optional\n            List of BigQuery table fields to which according DataFrame\n            columns conform to, e.g. ``[{'name': 'col1', 'type':\n            'STRING'},...]``. If schema is not provided, it will be\n            generated according to dtypes of DataFrame columns. See\n            BigQuery API documentation on available names of a field.\n\n            *New in version 0.3.1 of pandas-gbq*.\n        location : str, optional\n            Location where the load job should run. See the `BigQuery locations\n            documentation\n            <https://cloud.google.com/bigquery/docs/dataset-locations>`__ for a\n            list of available locations. The location must match that of the\n            target dataset.\n\n            *New in version 0.5.0 of pandas-gbq*.\n        progress_bar : bool, default True\n            Use the library `tqdm` to show the progress bar for the upload,\n            chunk by chunk.\n\n            *New in version 0.5.0 of pandas-gbq*.\n        credentials : google.auth.credentials.Credentials, optional\n            Credentials for accessing Google APIs. Use this parameter to\n            override default credentials, such as to use Compute Engine\n            :class:`google.auth.compute_engine.Credentials` or Service\n            Account :class:`google.oauth2.service_account.Credentials`\n            directly.\n\n            *New in version 0.8.0 of pandas-gbq*.\n\n            .. versionadded:: 0.24.0\n\n        See Also\n        --------\n        pandas_gbq.to_gbq : This function in the pandas-gbq library.\n        read_gbq : Read a DataFrame from Google BigQuery.\n        \"\"\"\n        from pandas.io import gbq\n\n        gbq.to_gbq(\n            self,\n            destination_table,\n            project_id=project_id,\n            chunksize=chunksize,\n            reauth=reauth,\n            if_exists=if_exists,\n            auth_local_webserver=auth_local_webserver,\n            table_schema=table_schema,\n            location=location,\n            progress_bar=progress_bar,\n            credentials=credentials,\n        )\n\n    @classmethod\n    def from_records(\n        cls,\n        data,\n        index=None,\n        exclude=None,\n        columns=None,\n        coerce_float=False,\n        nrows=None,\n    ) -> DataFrame:\n        \"\"\"\n        Convert structured or record ndarray to DataFrame.\n\n        Creates a DataFrame object from a structured ndarray, sequence of\n        tuples or dicts, or DataFrame.\n\n        Parameters\n        ----------\n        data : structured ndarray, sequence of tuples or dicts, or DataFrame\n            Structured input data.\n        index : str, list of fields, array-like\n            Field of array to use as the index, alternately a specific set of\n            input labels to use.\n        exclude : sequence, default None\n            Columns or fields to exclude.\n        columns : sequence, default None\n            Column names to use. If the passed data do not have names\n            associated with them, this argument provides names for the\n            columns. Otherwise this argument indicates the order of the columns\n            in the result (any names not found in the data will become all-NA\n            columns).\n        coerce_float : bool, default False\n            Attempt to convert values of non-string, non-numeric objects (like\n            decimal.Decimal) to floating point, useful for SQL result sets.\n        nrows : int, default None\n            Number of rows to read if data is an iterator.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.from_dict : DataFrame from dict of array-like or dicts.\n        DataFrame : DataFrame object creation using constructor.\n\n        Examples\n        --------\n        Data can be provided as a structured ndarray:\n\n        >>> data = np.array([(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')],\n        ...                 dtype=[('col_1', 'i4'), ('col_2', 'U1')])\n        >>> pd.DataFrame.from_records(data)\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n\n        Data can be provided as a list of dicts:\n\n        >>> data = [{'col_1': 3, 'col_2': 'a'},\n        ...         {'col_1': 2, 'col_2': 'b'},\n        ...         {'col_1': 1, 'col_2': 'c'},\n        ...         {'col_1': 0, 'col_2': 'd'}]\n        >>> pd.DataFrame.from_records(data)\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n\n        Data can be provided as a list of tuples with corresponding columns:\n\n        >>> data = [(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')]\n        >>> pd.DataFrame.from_records(data, columns=['col_1', 'col_2'])\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n        \"\"\"\n        # Make a copy of the input columns so we can modify it\n        if columns is not None:\n            columns = ensure_index(columns)\n\n        if is_iterator(data):\n            if nrows == 0:\n                return cls()\n\n            try:\n                first_row = next(data)\n            except StopIteration:\n                return cls(index=index, columns=columns)\n\n            dtype = None\n            if hasattr(first_row, \"dtype\") and first_row.dtype.names:\n                dtype = first_row.dtype\n\n            values = [first_row]\n\n            if nrows is None:\n                values += data\n            else:\n                values.extend(itertools.islice(data, nrows - 1))\n\n            if dtype is not None:\n                data = np.array(values, dtype=dtype)\n            else:\n                data = values\n\n        if isinstance(data, dict):\n            if columns is None:\n                columns = arr_columns = ensure_index(sorted(data))\n                arrays = [data[k] for k in columns]\n            else:\n                arrays = []\n                arr_columns_list = []\n                for k, v in data.items():\n                    if k in columns:\n                        arr_columns_list.append(k)\n                        arrays.append(v)\n\n                arrays, arr_columns = reorder_arrays(arrays, arr_columns_list, columns)\n\n        elif isinstance(data, (np.ndarray, DataFrame)):\n            arrays, columns = to_arrays(data, columns)\n            if columns is not None:\n                columns = ensure_index(columns)\n            arr_columns = columns\n        else:\n            arrays, arr_columns = to_arrays(data, columns, coerce_float=coerce_float)\n\n            arr_columns = ensure_index(arr_columns)\n            if columns is not None:\n                columns = ensure_index(columns)\n            else:\n                columns = arr_columns\n\n        if exclude is None:\n            exclude = set()\n        else:\n            exclude = set(exclude)\n\n        result_index = None\n        if index is not None:\n            if isinstance(index, str) or not hasattr(index, \"__iter__\"):\n                i = columns.get_loc(index)\n                exclude.add(index)\n                if len(arrays) > 0:\n                    result_index = Index(arrays[i], name=index)\n                else:\n                    result_index = Index([], name=index)\n            else:\n                try:\n                    index_data = [arrays[arr_columns.get_loc(field)] for field in index]\n                except (KeyError, TypeError):\n                    # raised by get_loc, see GH#29258\n                    result_index = index\n                else:\n                    result_index = ensure_index_from_sequences(index_data, names=index)\n                    exclude.update(index)\n\n        if any(exclude):\n            arr_exclude = [x for x in exclude if x in arr_columns]\n            to_remove = [arr_columns.get_loc(col) for col in arr_exclude]\n            arrays = [v for i, v in enumerate(arrays) if i not in to_remove]\n\n            arr_columns = arr_columns.drop(arr_exclude)\n            columns = columns.drop(exclude)\n\n        mgr = arrays_to_mgr(arrays, arr_columns, result_index, columns)\n\n        return cls(mgr)\n\n    def to_records(\n        self, index=True, column_dtypes=None, index_dtypes=None\n    ) -> np.recarray:\n        \"\"\"\n        Convert DataFrame to a NumPy record array.\n\n        Index will be included as the first field of the record array if\n        requested.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Include index in resulting record array, stored in 'index'\n            field or using the index label, if set.\n        column_dtypes : str, type, dict, default None\n            .. versionadded:: 0.24.0\n\n            If a string or type, the data type to store all columns. If\n            a dictionary, a mapping of column names and indices (zero-indexed)\n            to specific data types.\n        index_dtypes : str, type, dict, default None\n            .. versionadded:: 0.24.0\n\n            If a string or type, the data type to store all index levels. If\n            a dictionary, a mapping of index level names and indices\n            (zero-indexed) to specific data types.\n\n            This mapping is applied only if `index=True`.\n\n        Returns\n        -------\n        numpy.recarray\n            NumPy ndarray with the DataFrame labels as fields and each row\n            of the DataFrame as entries.\n\n        See Also\n        --------\n        DataFrame.from_records: Convert structured or record ndarray\n            to DataFrame.\n        numpy.recarray: An ndarray that allows field access using\n            attributes, analogous to typed columns in a\n            spreadsheet.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]},\n        ...                   index=['a', 'b'])\n        >>> df\n           A     B\n        a  1  0.50\n        b  2  0.75\n        >>> df.to_records()\n        rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n                  dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')])\n\n        If the DataFrame index has no label then the recarray field name\n        is set to 'index'. If the index has a label then this is used as the\n        field name:\n\n        >>> df.index = df.index.rename(\"I\")\n        >>> df.to_records()\n        rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n                  dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')])\n\n        The index can be excluded from the record array:\n\n        >>> df.to_records(index=False)\n        rec.array([(1, 0.5 ), (2, 0.75)],\n                  dtype=[('A', '<i8'), ('B', '<f8')])\n\n        Data types can be specified for the columns:\n\n        >>> df.to_records(column_dtypes={\"A\": \"int32\"})\n        rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n                  dtype=[('I', 'O'), ('A', '<i4'), ('B', '<f8')])\n\n        As well as for the index:\n\n        >>> df.to_records(index_dtypes=\"<S2\")\n        rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n                  dtype=[('I', 'S2'), ('A', '<i8'), ('B', '<f8')])\n\n        >>> index_dtypes = f\"<S{df.index.str.len().max()}\"\n        >>> df.to_records(index_dtypes=index_dtypes)\n        rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n                  dtype=[('I', 'S1'), ('A', '<i8'), ('B', '<f8')])\n        \"\"\"\n        if index:\n            if isinstance(self.index, MultiIndex):\n                # array of tuples to numpy cols. copy copy copy\n                ix_vals = list(map(np.array, zip(*self.index._values)))\n            else:\n                ix_vals = [self.index.values]\n\n            arrays = ix_vals + [\n                np.asarray(self.iloc[:, i]) for i in range(len(self.columns))\n            ]\n\n            count = 0\n            index_names = list(self.index.names)\n\n            if isinstance(self.index, MultiIndex):\n                for i, n in enumerate(index_names):\n                    if n is None:\n                        index_names[i] = f\"level_{count}\"\n                        count += 1\n            elif index_names[0] is None:\n                index_names = [\"index\"]\n\n            names = [str(name) for name in itertools.chain(index_names, self.columns)]\n        else:\n            arrays = [np.asarray(self.iloc[:, i]) for i in range(len(self.columns))]\n            names = [str(c) for c in self.columns]\n            index_names = []\n\n        index_len = len(index_names)\n        formats = []\n\n        for i, v in enumerate(arrays):\n            index = i\n\n            # When the names and arrays are collected, we\n            # first collect those in the DataFrame's index,\n            # followed by those in its columns.\n            #\n            # Thus, the total length of the array is:\n            # len(index_names) + len(DataFrame.columns).\n            #\n            # This check allows us to see whether we are\n            # handling a name / array in the index or column.\n            if index < index_len:\n                dtype_mapping = index_dtypes\n                name = index_names[index]\n            else:\n                index -= index_len\n                dtype_mapping = column_dtypes\n                name = self.columns[index]\n\n            # We have a dictionary, so we get the data type\n            # associated with the index or column (which can\n            # be denoted by its name in the DataFrame or its\n            # position in DataFrame's array of indices or\n            # columns, whichever is applicable.\n            if is_dict_like(dtype_mapping):\n                if name in dtype_mapping:\n                    dtype_mapping = dtype_mapping[name]\n                elif index in dtype_mapping:\n                    dtype_mapping = dtype_mapping[index]\n                else:\n                    dtype_mapping = None\n\n            # If no mapping can be found, use the array's\n            # dtype attribute for formatting.\n            #\n            # A valid dtype must either be a type or\n            # string naming a type.\n            if dtype_mapping is None:\n                formats.append(v.dtype)\n            elif isinstance(dtype_mapping, (type, np.dtype, str)):\n                formats.append(dtype_mapping)\n            else:\n                element = \"row\" if i < index_len else \"column\"\n                msg = f\"Invalid dtype {dtype_mapping} specified for {element} {name}\"\n                raise ValueError(msg)\n\n        return np.rec.fromarrays(arrays, dtype={\"names\": names, \"formats\": formats})\n\n    @classmethod\n    def _from_arrays(\n        cls,\n        arrays,\n        columns,\n        index,\n        dtype: Optional[Dtype] = None,\n        verify_integrity: bool = True,\n    ) -> DataFrame:\n        \"\"\"\n        Create DataFrame from a list of arrays corresponding to the columns.\n\n        Parameters\n        ----------\n        arrays : list-like of arrays\n            Each array in the list corresponds to one column, in order.\n        columns : list-like, Index\n            The column names for the resulting DataFrame.\n        index : list-like, Index\n            The rows labels for the resulting DataFrame.\n        dtype : dtype, optional\n            Optional dtype to enforce for all arrays.\n        verify_integrity : bool, default True\n            Validate and homogenize all input. If set to False, it is assumed\n            that all elements of `arrays` are actual arrays how they will be\n            stored in a block (numpy ndarray or ExtensionArray), have the same\n            length as and are aligned with the index, and that `columns` and\n            `index` are ensured to be an Index object.\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n\n        mgr = arrays_to_mgr(\n            arrays,\n            columns,\n            index,\n            columns,\n            dtype=dtype,\n            verify_integrity=verify_integrity,\n        )\n        return cls(mgr)\n\n    @deprecate_kwarg(old_arg_name=\"fname\", new_arg_name=\"path\")\n    def to_stata(\n        self,\n        path: FilePathOrBuffer,\n        convert_dates: Optional[Dict[Label, str]] = None,\n        write_index: bool = True,\n        byteorder: Optional[str] = None,\n        time_stamp: Optional[datetime.datetime] = None,\n        data_label: Optional[str] = None,\n        variable_labels: Optional[Dict[Label, str]] = None,\n        version: Optional[int] = 114,\n        convert_strl: Optional[Sequence[Label]] = None,\n        compression: CompressionOptions = \"infer\",\n        storage_options: StorageOptions = None,\n    ) -> None:\n        \"\"\"\n        Export DataFrame object to Stata dta format.\n\n        Writes the DataFrame to a Stata dataset file.\n        \"dta\" files contain a Stata dataset.\n\n        Parameters\n        ----------\n        path : str, buffer or path object\n            String, path object (pathlib.Path or py._path.local.LocalPath) or\n            object implementing a binary write() function. If using a buffer\n            then the buffer will not be automatically closed after the file\n            data has been written.\n\n            .. versionchanged:: 1.0.0\n\n            Previously this was \"fname\"\n\n        convert_dates : dict\n            Dictionary mapping columns containing datetime types to stata\n            internal format to use when writing the dates. Options are 'tc',\n            'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer\n            or a name. Datetime columns that do not have a conversion type\n            specified will be converted to 'tc'. Raises NotImplementedError if\n            a datetime column has timezone information.\n        write_index : bool\n            Write the index to Stata dataset.\n        byteorder : str\n            Can be \">\", \"<\", \"little\", or \"big\". default is `sys.byteorder`.\n        time_stamp : datetime\n            A datetime to use as file creation date.  Default is the current\n            time.\n        data_label : str, optional\n            A label for the data set.  Must be 80 characters or smaller.\n        variable_labels : dict\n            Dictionary containing columns as keys and variable labels as\n            values. Each label must be 80 characters or smaller.\n        version : {114, 117, 118, 119, None}, default 114\n            Version to use in the output dta file. Set to None to let pandas\n            decide between 118 or 119 formats depending on the number of\n            columns in the frame. Version 114 can be read by Stata 10 and\n            later. Version 117 can be read by Stata 13 or later. Version 118\n            is supported in Stata 14 and later. Version 119 is supported in\n            Stata 15 and later. Version 114 limits string variables to 244\n            characters or fewer while versions 117 and later allow strings\n            with lengths up to 2,000,000 characters. Versions 118 and 119\n            support Unicode characters, and version 119 supports more than\n            32,767 variables.\n\n            .. versionchanged:: 1.0.0\n\n                Added support for formats 118 and 119.\n\n        convert_strl : list, optional\n            List of column names to convert to string columns to Stata StrL\n            format. Only available if version is 117.  Storing strings in the\n            StrL format can produce smaller dta files if strings have more than\n            8 characters and values are repeated.\n        compression : str or dict, default 'infer'\n            For on-the-fly compression of the output dta. If string, specifies\n            compression mode. If dict, value at key 'method' specifies\n            compression mode. Compression mode must be one of {'infer', 'gzip',\n            'bz2', 'zip', 'xz', None}. If compression mode is 'infer' and\n            `fname` is path-like, then detect compression from the following\n            extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n            compression). If dict and compression mode is one of {'zip',\n            'gzip', 'bz2'}, or inferred as one of the above, other entries\n            passed as additional compression options.\n\n            .. versionadded:: 1.1.0\n\n        storage_options : dict, optional\n            Extra options that make sense for a particular storage connection, e.g.\n            host, port, username, password, etc., if using a URL that will\n            be parsed by ``fsspec``, e.g., starting \"s3://\", \"gcs://\". An error\n            will be raised if providing this argument with a local path or\n            a file-like buffer. See the fsspec and backend storage implementation\n            docs for the set of allowed keys and values.\n\n            .. versionadded:: 1.2.0\n\n        Raises\n        ------\n        NotImplementedError\n            * If datetimes contain timezone information\n            * Column dtype is not representable in Stata\n        ValueError\n            * Columns listed in convert_dates are neither datetime64[ns]\n              or datetime.datetime\n            * Column listed in convert_dates is not in DataFrame\n            * Categorical label contains more than 32,000 characters\n\n        See Also\n        --------\n        read_stata : Import Stata data files.\n        io.stata.StataWriter : Low-level writer for Stata data files.\n        io.stata.StataWriter117 : Low-level writer for version 117 files.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'animal': ['falcon', 'parrot', 'falcon',\n        ...                               'parrot'],\n        ...                    'speed': [350, 18, 361, 15]})\n        >>> df.to_stata('animals.dta')  # doctest: +SKIP\n        \"\"\"\n        if version not in (114, 117, 118, 119, None):\n            raise ValueError(\"Only formats 114, 117, 118 and 119 are supported.\")\n        if version == 114:\n            if convert_strl is not None:\n                raise ValueError(\"strl is not supported in format 114\")\n            from pandas.io.stata import StataWriter as statawriter\n        elif version == 117:\n            # mypy: Name 'statawriter' already defined (possibly by an import)\n            from pandas.io.stata import (  # type: ignore[no-redef]\n                StataWriter117 as statawriter,\n            )\n        else:  # versions 118 and 119\n            # mypy: Name 'statawriter' already defined (possibly by an import)\n            from pandas.io.stata import (  # type: ignore[no-redef]\n                StataWriterUTF8 as statawriter,\n            )\n\n        kwargs: Dict[str, Any] = {}\n        if version is None or version >= 117:\n            # strl conversion is only supported >= 117\n            kwargs[\"convert_strl\"] = convert_strl\n        if version is None or version >= 118:\n            # Specifying the version is only supported for UTF8 (118 or 119)\n            kwargs[\"version\"] = version\n\n        # mypy: Too many arguments for \"StataWriter\"\n        writer = statawriter(  # type: ignore[call-arg]\n            path,\n            self,\n            convert_dates=convert_dates,\n            byteorder=byteorder,\n            time_stamp=time_stamp,\n            data_label=data_label,\n            write_index=write_index,\n            variable_labels=variable_labels,\n            compression=compression,\n            storage_options=storage_options,\n            **kwargs,\n        )\n        writer.write_file()\n\n    @deprecate_kwarg(old_arg_name=\"fname\", new_arg_name=\"path\")\n    def to_feather(self, path: FilePathOrBuffer[AnyStr], **kwargs) -> None:\n        \"\"\"\n        Write a DataFrame to the binary Feather format.\n\n        Parameters\n        ----------\n        path : str or file-like object\n            If a string, it will be used as Root Directory path.\n        **kwargs :\n            Additional keywords passed to :func:`pyarrow.feather.write_feather`.\n            Starting with pyarrow 0.17, this includes the `compression`,\n            `compression_level`, `chunksize` and `version` keywords.\n\n            .. versionadded:: 1.1.0\n        \"\"\"\n        from pandas.io.feather_format import to_feather\n\n        to_feather(self, path, **kwargs)\n\n    @doc(\n        Series.to_markdown,\n        klass=_shared_doc_kwargs[\"klass\"],\n        examples=\"\"\"Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     data={\"animal_1\": [\"elk\", \"pig\"], \"animal_2\": [\"dog\", \"quetzal\"]}\n        ... )\n        >>> print(df.to_markdown())\n        |    | animal_1   | animal_2   |\n        |---:|:-----------|:-----------|\n        |  0 | elk        | dog        |\n        |  1 | pig        | quetzal    |\n\n        Output markdown with a tabulate option.\n\n        >>> print(df.to_markdown(tablefmt=\"grid\"))\n        +----+------------+------------+\n        |    | animal_1   | animal_2   |\n        +====+============+============+\n        |  0 | elk        | dog        |\n        +----+------------+------------+\n        |  1 | pig        | quetzal    |\n        +----+------------+------------+\n        \"\"\",\n    )\n    def to_markdown(\n        self,\n        buf: Optional[Union[IO[str], str]] = None,\n        mode: str = \"wt\",\n        index: bool = True,\n        storage_options: StorageOptions = None,\n        **kwargs,\n    ) -> Optional[str]:\n        if \"showindex\" in kwargs:\n            warnings.warn(\n                \"'showindex' is deprecated. Only 'index' will be used \"\n                \"in a future version. Use 'index' to silence this warning.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n\n        kwargs.setdefault(\"headers\", \"keys\")\n        kwargs.setdefault(\"tablefmt\", \"pipe\")\n        kwargs.setdefault(\"showindex\", index)\n        tabulate = import_optional_dependency(\"tabulate\")\n        result = tabulate.tabulate(self, **kwargs)\n        if buf is None:\n            return result\n        ioargs = get_filepath_or_buffer(buf, mode=mode, storage_options=storage_options)\n        assert not isinstance(ioargs.filepath_or_buffer, str)\n        ioargs.filepath_or_buffer.writelines(result)\n        if ioargs.should_close:\n            ioargs.filepath_or_buffer.close()\n        return None\n\n    @deprecate_kwarg(old_arg_name=\"fname\", new_arg_name=\"path\")\n    def to_parquet(\n        self,\n        path: Optional[FilePathOrBuffer] = None,\n        engine: str = \"auto\",\n        compression: Optional[str] = \"snappy\",\n        index: Optional[bool] = None,\n        partition_cols: Optional[List[str]] = None,\n        storage_options: StorageOptions = None,\n        **kwargs,\n    ) -> Optional[bytes]:\n        \"\"\"\n        Write a DataFrame to the binary parquet format.\n\n        This function writes the dataframe as a `parquet file\n        <https://parquet.apache.org/>`_. You can choose different parquet\n        backends, and have the option of compression. See\n        :ref:`the user guide <io.parquet>` for more details.\n\n        Parameters\n        ----------\n        path : str or file-like object, default None\n            If a string, it will be used as Root Directory path\n            when writing a partitioned dataset. By file-like object,\n            we refer to objects with a write() method, such as a file handle\n            (e.g. via builtin open function) or io.BytesIO. The engine\n            fastparquet does not accept file-like objects. If path is None,\n            a bytes object is returned.\n\n            .. versionchanged:: 1.2.0\n\n            Previously this was \"fname\"\n\n        engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n            Parquet library to use. If 'auto', then the option\n            ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n            behavior is to try 'pyarrow', falling back to 'fastparquet' if\n            'pyarrow' is unavailable.\n        compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'\n            Name of the compression to use. Use ``None`` for no compression.\n        index : bool, default None\n            If ``True``, include the dataframe's index(es) in the file output.\n            If ``False``, they will not be written to the file.\n            If ``None``, similar to ``True`` the dataframe's index(es)\n            will be saved. However, instead of being saved as values,\n            the RangeIndex will be stored as a range in the metadata so it\n            doesn't require much space and is faster. Other indexes will\n            be included as columns in the file output.\n\n            .. versionadded:: 0.24.0\n\n        partition_cols : list, optional, default None\n            Column names by which to partition the dataset.\n            Columns are partitioned in the order they are given.\n            Must be None if path is not a string.\n\n            .. versionadded:: 0.24.0\n\n        storage_options : dict, optional\n            Extra options that make sense for a particular storage connection, e.g.\n            host, port, username, password, etc., if using a URL that will\n            be parsed by ``fsspec``, e.g., starting \"s3://\", \"gcs://\". An error\n            will be raised if providing this argument with a local path or\n            a file-like buffer. See the fsspec and backend storage implementation\n            docs for the set of allowed keys and values.\n\n            .. versionadded:: 1.2.0\n\n        **kwargs\n            Additional arguments passed to the parquet library. See\n            :ref:`pandas io <io.parquet>` for more details.\n\n        Returns\n        -------\n        bytes if no path argument is provided else None\n\n        See Also\n        --------\n        read_parquet : Read a parquet file.\n        DataFrame.to_csv : Write a csv file.\n        DataFrame.to_sql : Write to a sql table.\n        DataFrame.to_hdf : Write to hdf.\n\n        Notes\n        -----\n        This function requires either the `fastparquet\n        <https://pypi.org/project/fastparquet>`_ or `pyarrow\n        <https://arrow.apache.org/docs/python/>`_ library.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.to_parquet('df.parquet.gzip',\n        ...               compression='gzip')  # doctest: +SKIP\n        >>> pd.read_parquet('df.parquet.gzip')  # doctest: +SKIP\n           col1  col2\n        0     1     3\n        1     2     4\n\n        If you want to get a buffer to the parquet content you can use a io.BytesIO\n        object, as long as you don't use partition_cols, which creates multiple files.\n\n        >>> import io\n        >>> f = io.BytesIO()\n        >>> df.to_parquet(f)\n        >>> f.seek(0)\n        0\n        >>> content = f.read()\n        \"\"\"\n        from pandas.io.parquet import to_parquet\n\n        return to_parquet(\n            self,\n            path,\n            engine,\n            compression=compression,\n            index=index,\n            partition_cols=partition_cols,\n            storage_options=storage_options,\n            **kwargs,\n        )\n\n    @Substitution(\n        header_type=\"bool\",\n        header=\"Whether to print column labels, default True\",\n        col_space_type=\"str or int, list or dict of int or str\",\n        col_space=\"The minimum width of each column in CSS length \"\n        \"units.  An int is assumed to be px units.\\n\\n\"\n        \"            .. versionadded:: 0.25.0\\n\"\n        \"                Ability to use str\",\n    )\n    @Substitution(shared_params=fmt.common_docstring, returns=fmt.return_docstring)\n    def to_html(\n        self,\n        buf=None,\n        columns=None,\n        col_space=None,\n        header=True,\n        index=True,\n        na_rep=\"NaN\",\n        formatters=None,\n        float_format=None,\n        sparsify=None,\n        index_names=True,\n        justify=None,\n        max_rows=None,\n        max_cols=None,\n        show_dimensions=False,\n        decimal=\".\",\n        bold_rows=True,\n        classes=None,\n        escape=True,\n        notebook=False,\n        border=None,\n        table_id=None,\n        render_links=False,\n        encoding=None,\n    ):\n        \"\"\"\n        Render a DataFrame as an HTML table.\n        %(shared_params)s\n        bold_rows : bool, default True\n            Make the row labels bold in the output.\n        classes : str or list or tuple, default None\n            CSS class(es) to apply to the resulting html table.\n        escape : bool, default True\n            Convert the characters <, >, and & to HTML-safe sequences.\n        notebook : {True, False}, default False\n            Whether the generated HTML is for IPython Notebook.\n        border : int\n            A ``border=border`` attribute is included in the opening\n            `<table>` tag. Default ``pd.options.display.html.border``.\n        encoding : str, default \"utf-8\"\n            Set character encoding.\n\n            .. versionadded:: 1.0\n\n        table_id : str, optional\n            A css id is included in the opening `<table>` tag if specified.\n        render_links : bool, default False\n            Convert URLs to HTML links.\n\n            .. versionadded:: 0.24.0\n        %(returns)s\n        See Also\n        --------\n        to_string : Convert DataFrame to a string.\n        \"\"\"\n        if justify is not None and justify not in fmt._VALID_JUSTIFY_PARAMETERS:\n            raise ValueError(\"Invalid value for justify parameter\")\n\n        formatter = fmt.DataFrameFormatter(\n            self,\n            columns=columns,\n            col_space=col_space,\n            na_rep=na_rep,\n            header=header,\n            index=index,\n            formatters=formatters,\n            float_format=float_format,\n            bold_rows=bold_rows,\n            sparsify=sparsify,\n            justify=justify,\n            index_names=index_names,\n            escape=escape,\n            decimal=decimal,\n            max_rows=max_rows,\n            max_cols=max_cols,\n            show_dimensions=show_dimensions,\n        )\n        # TODO: a generic formatter wld b in DataFrameFormatter\n        return fmt.DataFrameRenderer(formatter).to_html(\n            buf=buf,\n            classes=classes,\n            notebook=notebook,\n            border=border,\n            encoding=encoding,\n            table_id=table_id,\n            render_links=render_links,\n        )\n\n    # ----------------------------------------------------------------------\n    @Substitution(\n        klass=\"DataFrame\",\n        type_sub=\" and columns\",\n        max_cols_sub=(\n            \"\"\"max_cols : int, optional\n                When to switch from the verbose to the truncated output. If the\n                DataFrame has more than `max_cols` columns, the truncated output\n                is used. By default, the setting in\n                ``pandas.options.display.max_info_columns`` is used.\n            \"\"\"\n        ),\n        examples_sub=(\n            \"\"\"\n            >>> int_values = [1, 2, 3, 4, 5]\n            >>> text_values = ['alpha', 'beta', 'gamma', 'delta', 'epsilon']\n            >>> float_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n            >>> df = pd.DataFrame({\"int_col\": int_values, \"text_col\": text_values,\n            ...                   \"float_col\": float_values})\n            >>> df\n                int_col text_col  float_col\n            0        1    alpha       0.00\n            1        2     beta       0.25\n            2        3    gamma       0.50\n            3        4    delta       0.75\n            4        5  epsilon       1.00\n\n            Prints information of all columns:\n\n            >>> df.info(verbose=True)\n            <class 'pandas.core.frame.DataFrame'>\n            RangeIndex: 5 entries, 0 to 4\n            Data columns (total 3 columns):\n             #   Column     Non-Null Count  Dtype\n            ---  ------     --------------  -----\n             0   int_col    5 non-null      int64\n             1   text_col   5 non-null      object\n             2   float_col  5 non-null      float64\n            dtypes: float64(1), int64(1), object(1)\n            memory usage: 248.0+ bytes\n\n            Prints a summary of columns count and its dtypes but not per column\n            information:\n\n            >>> df.info(verbose=False)\n            <class 'pandas.core.frame.DataFrame'>\n            RangeIndex: 5 entries, 0 to 4\n            Columns: 3 entries, int_col to float_col\n            dtypes: float64(1), int64(1), object(1)\n            memory usage: 248.0+ bytes\n\n            Pipe output of DataFrame.info to buffer instead of sys.stdout, get\n            buffer content and writes to a text file:\n\n            >>> import io\n            >>> buffer = io.StringIO()\n            >>> df.info(buf=buffer)\n            >>> s = buffer.getvalue()\n            >>> with open(\"df_info.txt\", \"w\",\n            ...           encoding=\"utf-8\") as f:  # doctest: +SKIP\n            ...     f.write(s)\n            260\n\n            The `memory_usage` parameter allows deep introspection mode, specially\n            useful for big DataFrames and fine-tune memory optimization:\n\n            >>> random_strings_array = np.random.choice(['a', 'b', 'c'], 10 ** 6)\n            >>> df = pd.DataFrame({\n            ...     'column_1': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n            ...     'column_2': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n            ...     'column_3': np.random.choice(['a', 'b', 'c'], 10 ** 6)\n            ... })\n            >>> df.info()\n            <class 'pandas.core.frame.DataFrame'>\n            RangeIndex: 1000000 entries, 0 to 999999\n            Data columns (total 3 columns):\n             #   Column    Non-Null Count    Dtype\n            ---  ------    --------------    -----\n             0   column_1  1000000 non-null  object\n             1   column_2  1000000 non-null  object\n             2   column_3  1000000 non-null  object\n            dtypes: object(3)\n            memory usage: 22.9+ MB\n\n            >>> df.info(memory_usage='deep')\n            <class 'pandas.core.frame.DataFrame'>\n            RangeIndex: 1000000 entries, 0 to 999999\n            Data columns (total 3 columns):\n             #   Column    Non-Null Count    Dtype\n            ---  ------    --------------    -----\n             0   column_1  1000000 non-null  object\n             1   column_2  1000000 non-null  object\n             2   column_3  1000000 non-null  object\n            dtypes: object(3)\n            memory usage: 165.9 MB\"\"\"\n        ),\n        see_also_sub=(\n            \"\"\"\n            DataFrame.describe: Generate descriptive statistics of DataFrame\n                columns.\n            DataFrame.memory_usage: Memory usage of DataFrame columns.\"\"\"\n        ),\n    )\n    @doc(DataFrameInfo.to_buffer)\n    def info(\n        self,\n        verbose: Optional[bool] = None,\n        buf: Optional[IO[str]] = None,\n        max_cols: Optional[int] = None,\n        memory_usage: Optional[Union[bool, str]] = None,\n        null_counts: Optional[bool] = None,\n    ) -> None:\n        info = DataFrameInfo(\n            data=self,\n            memory_usage=memory_usage,\n        )\n        info.to_buffer(\n            buf=buf,\n            max_cols=max_cols,\n            verbose=verbose,\n            show_counts=null_counts,\n        )\n\n    def memory_usage(self, index=True, deep=False) -> Series:\n        \"\"\"\n        Return the memory usage of each column in bytes.\n\n        The memory usage can optionally include the contribution of\n        the index and elements of `object` dtype.\n\n        This value is displayed in `DataFrame.info` by default. This can be\n        suppressed by setting ``pandas.options.display.memory_usage`` to False.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the DataFrame's\n            index in returned Series. If ``index=True``, the memory usage of\n            the index is the first item in the output.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned values.\n\n        Returns\n        -------\n        Series\n            A Series whose index is the original column names and whose values\n            is the memory usage of each column in bytes.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of an\n            ndarray.\n        Series.memory_usage : Bytes consumed by a Series.\n        Categorical : Memory-efficient array for string values with\n            many repeated values.\n        DataFrame.info : Concise summary of a DataFrame.\n\n        Examples\n        --------\n        >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool']\n        >>> data = dict([(t, np.ones(shape=5000, dtype=int).astype(t))\n        ...              for t in dtypes])\n        >>> df = pd.DataFrame(data)\n        >>> df.head()\n           int64  float64            complex128  object  bool\n        0      1      1.0              1.0+0.0j       1  True\n        1      1      1.0              1.0+0.0j       1  True\n        2      1      1.0              1.0+0.0j       1  True\n        3      1      1.0              1.0+0.0j       1  True\n        4      1      1.0              1.0+0.0j       1  True\n\n        >>> df.memory_usage()\n        Index           128\n        int64         40000\n        float64       40000\n        complex128    80000\n        object        40000\n        bool           5000\n        dtype: int64\n\n        >>> df.memory_usage(index=False)\n        int64         40000\n        float64       40000\n        complex128    80000\n        object        40000\n        bool           5000\n        dtype: int64\n\n        The memory footprint of `object` dtype columns is ignored by default:\n\n        >>> df.memory_usage(deep=True)\n        Index            128\n        int64          40000\n        float64        40000\n        complex128     80000\n        object        180000\n        bool            5000\n        dtype: int64\n\n        Use a Categorical for efficient storage of an object-dtype column with\n        many repeated values.\n\n        >>> df['object'].astype('category').memory_usage(deep=True)\n        5216\n        \"\"\"\n        result = self._constructor_sliced(\n            [c.memory_usage(index=False, deep=deep) for col, c in self.items()],\n            index=self.columns,\n        )\n        if index:\n            result = self._constructor_sliced(\n                self.index.memory_usage(deep=deep), index=[\"Index\"]\n            ).append(result)\n        return result\n\n    def transpose(self, *args, copy: bool = False) -> DataFrame:\n        \"\"\"\n        Transpose index and columns.\n\n        Reflect the DataFrame over its main diagonal by writing rows as columns\n        and vice-versa. The property :attr:`.T` is an accessor to the method\n        :meth:`transpose`.\n\n        Parameters\n        ----------\n        *args : tuple, optional\n            Accepted for compatibility with NumPy.\n        copy : bool, default False\n            Whether to copy the data after transposing, even for DataFrames\n            with a single dtype.\n\n            Note that a copy is always required for mixed dtype DataFrames,\n            or for DataFrames with any extension types.\n\n        Returns\n        -------\n        DataFrame\n            The transposed DataFrame.\n\n        See Also\n        --------\n        numpy.transpose : Permute the dimensions of a given array.\n\n        Notes\n        -----\n        Transposing a DataFrame with mixed dtypes will result in a homogeneous\n        DataFrame with the `object` dtype. In such a case, a copy of the data\n        is always made.\n\n        Examples\n        --------\n        **Square DataFrame with homogeneous dtype**\n\n        >>> d1 = {'col1': [1, 2], 'col2': [3, 4]}\n        >>> df1 = pd.DataFrame(data=d1)\n        >>> df1\n           col1  col2\n        0     1     3\n        1     2     4\n\n        >>> df1_transposed = df1.T # or df1.transpose()\n        >>> df1_transposed\n              0  1\n        col1  1  2\n        col2  3  4\n\n        When the dtype is homogeneous in the original DataFrame, we get a\n        transposed DataFrame with the same dtype:\n\n        >>> df1.dtypes\n        col1    int64\n        col2    int64\n        dtype: object\n        >>> df1_transposed.dtypes\n        0    int64\n        1    int64\n        dtype: object\n\n        **Non-square DataFrame with mixed dtypes**\n\n        >>> d2 = {'name': ['Alice', 'Bob'],\n        ...       'score': [9.5, 8],\n        ...       'employed': [False, True],\n        ...       'kids': [0, 0]}\n        >>> df2 = pd.DataFrame(data=d2)\n        >>> df2\n            name  score  employed  kids\n        0  Alice    9.5     False     0\n        1    Bob    8.0      True     0\n\n        >>> df2_transposed = df2.T # or df2.transpose()\n        >>> df2_transposed\n                      0     1\n        name      Alice   Bob\n        score       9.5   8.0\n        employed  False  True\n        kids          0     0\n\n        When the DataFrame has mixed dtypes, we get a transposed DataFrame with\n        the `object` dtype:\n\n        >>> df2.dtypes\n        name         object\n        score       float64\n        employed       bool\n        kids          int64\n        dtype: object\n        >>> df2_transposed.dtypes\n        0    object\n        1    object\n        dtype: object\n        \"\"\"\n        nv.validate_transpose(args, dict())\n        # construct the args\n\n        dtypes = list(self.dtypes)\n        if self._is_homogeneous_type and dtypes and is_extension_array_dtype(dtypes[0]):\n            # We have EAs with the same dtype. We can preserve that dtype in transpose.\n            dtype = dtypes[0]\n            arr_type = dtype.construct_array_type()\n            values = self.values\n\n            new_values = [arr_type._from_sequence(row, dtype=dtype) for row in values]\n            result = self._constructor(\n                dict(zip(self.index, new_values)), index=self.columns\n            )\n\n        else:\n            new_values = self.values.T\n            if copy:\n                new_values = new_values.copy()\n            result = self._constructor(\n                new_values, index=self.columns, columns=self.index\n            )\n\n        return result.__finalize__(self, method=\"transpose\")\n\n    @property\n    def T(self) -> DataFrame:\n        return self.transpose()\n\n    # ----------------------------------------------------------------------\n    # Indexing Methods\n\n    def _ixs(self, i: int, axis: int = 0):\n        \"\"\"\n        Parameters\n        ----------\n        i : int\n        axis : int\n\n        Notes\n        -----\n        If slice passed, the resulting data will be a view.\n        \"\"\"\n        # irow\n        if axis == 0:\n            new_values = self._mgr.fast_xs(i)\n\n            # if we are a copy, mark as such\n            copy = isinstance(new_values, np.ndarray) and new_values.base is None\n            result = self._constructor_sliced(\n                new_values,\n                index=self.columns,\n                name=self.index[i],\n                dtype=new_values.dtype,\n            )\n            result._set_is_copy(self, copy=copy)\n            return result\n\n        # icol\n        else:\n            label = self.columns[i]\n\n            values = self._mgr.iget(i)\n            result = self._box_col_values(values, i)\n\n            # this is a cached value, mark it so\n            result._set_as_cached(label, self)\n\n            return result\n\n    def _get_column_array(self, i: int) -> ArrayLike:\n        \"\"\"\n        Get the values of the i'th column (ndarray or ExtensionArray, as stored\n        in the Block)\n        \"\"\"\n        return self._mgr.iget_values(i)\n\n    def _iter_column_arrays(self) -> Iterator[ArrayLike]:\n        \"\"\"\n        Iterate over the arrays of all columns in order.\n        This returns the values as stored in the Block (ndarray or ExtensionArray).\n        \"\"\"\n        for i in range(len(self.columns)):\n            yield self._get_column_array(i)\n\n    def __getitem__(self, key):\n        key = lib.item_from_zerodim(key)\n        key = com.apply_if_callable(key, self)\n\n        if is_hashable(key):\n            # shortcut if the key is in columns\n            if self.columns.is_unique and key in self.columns:\n                if self.columns.nlevels > 1:\n                    return self._getitem_multilevel(key)\n                return self._get_item_cache(key)\n\n        # Do we have a slicer (on rows)?\n        indexer = convert_to_index_sliceable(self, key)\n        if indexer is not None:\n            # either we have a slice or we have a string that can be converted\n            #  to a slice for partial-string date indexing\n            return self._slice(indexer, axis=0)\n\n        # Do we have a (boolean) DataFrame?\n        if isinstance(key, DataFrame):\n            return self.where(key)\n\n        # Do we have a (boolean) 1d indexer?\n        if com.is_bool_indexer(key):\n            return self._getitem_bool_array(key)\n\n        # We are left with two options: a single key, and a collection of keys,\n        # We interpret tuples as collections only for non-MultiIndex\n        is_single_key = isinstance(key, tuple) or not is_list_like(key)\n\n        if is_single_key:\n            if self.columns.nlevels > 1:\n                return self._getitem_multilevel(key)\n            indexer = self.columns.get_loc(key)\n            if is_integer(indexer):\n                indexer = [indexer]\n        else:\n            if is_iterator(key):\n                key = list(key)\n            indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]\n\n        # take() does not accept boolean indexers\n        if getattr(indexer, \"dtype\", None) == bool:\n            indexer = np.where(indexer)[0]\n\n        data = self._take_with_is_copy(indexer, axis=1)\n\n        if is_single_key:\n            # What does looking for a single key in a non-unique index return?\n            # The behavior is inconsistent. It returns a Series, except when\n            # - the key itself is repeated (test on data.shape, #9519), or\n            # - we have a MultiIndex on columns (test on self.columns, #21309)\n            if data.shape[1] == 1 and not isinstance(self.columns, MultiIndex):\n                # GH#26490 using data[key] can cause RecursionError\n                data = data._get_item_cache(key)\n\n        return data\n\n    def _getitem_bool_array(self, key):\n        # also raises Exception if object array with NA values\n        # warning here just in case -- previously __setitem__ was\n        # reindexing but __getitem__ was not; it seems more reasonable to\n        # go with the __setitem__ behavior since that is more consistent\n        # with all other indexing behavior\n        if isinstance(key, Series) and not key.index.equals(self.index):\n            warnings.warn(\n                \"Boolean Series key will be reindexed to match DataFrame index.\",\n                UserWarning,\n                stacklevel=3,\n            )\n        elif len(key) != len(self.index):\n            raise ValueError(\n                f\"Item wrong length {len(key)} instead of {len(self.index)}.\"\n            )\n\n        # check_bool_indexer will throw exception if Series key cannot\n        # be reindexed to match DataFrame rows\n        key = check_bool_indexer(self.index, key)\n        indexer = key.nonzero()[0]\n        return self._take_with_is_copy(indexer, axis=0)\n\n    def _getitem_multilevel(self, key):\n        # self.columns is a MultiIndex\n        loc = self.columns.get_loc(key)\n        if isinstance(loc, (slice, np.ndarray)):\n            new_columns = self.columns[loc]\n            result_columns = maybe_droplevels(new_columns, key)\n            if self._is_mixed_type:\n                result = self.reindex(columns=new_columns)\n                result.columns = result_columns\n            else:\n                new_values = self.values[:, loc]\n                result = self._constructor(\n                    new_values, index=self.index, columns=result_columns\n                )\n                result = result.__finalize__(self)\n\n            # If there is only one column being returned, and its name is\n            # either an empty string, or a tuple with an empty string as its\n            # first element, then treat the empty string as a placeholder\n            # and return the column as if the user had provided that empty\n            # string in the key. If the result is a Series, exclude the\n            # implied empty string from its name.\n            if len(result.columns) == 1:\n                top = result.columns[0]\n                if isinstance(top, tuple):\n                    top = top[0]\n                if top == \"\":\n                    result = result[\"\"]\n                    if isinstance(result, Series):\n                        result = self._constructor_sliced(\n                            result, index=self.index, name=key\n                        )\n\n            result._set_is_copy(self)\n            return result\n        else:\n            # loc is neither a slice nor ndarray, so must be an int\n            return self._ixs(loc, axis=1)\n\n    def _get_value(self, index, col, takeable: bool = False):\n        \"\"\"\n        Quickly retrieve single value at passed column and index.\n\n        Parameters\n        ----------\n        index : row label\n        col : column label\n        takeable : interpret the index/col as indexers, default False\n\n        Returns\n        -------\n        scalar\n        \"\"\"\n        if takeable:\n            series = self._ixs(col, axis=1)\n            return series._values[index]\n\n        series = self._get_item_cache(col)\n        engine = self.index._engine\n\n        try:\n            loc = engine.get_loc(index)\n            return series._values[loc]\n        except KeyError:\n            # GH 20629\n            if self.index.nlevels > 1:\n                # partial indexing forbidden\n                raise\n\n        # we cannot handle direct indexing\n        # use positional\n        col = self.columns.get_loc(col)\n        index = self.index.get_loc(index)\n        return self._get_value(index, col, takeable=True)\n\n    def __setitem__(self, key, value):\n        key = com.apply_if_callable(key, self)\n\n        # see if we can slice the rows\n        indexer = convert_to_index_sliceable(self, key)\n        if indexer is not None:\n            # either we have a slice or we have a string that can be converted\n            #  to a slice for partial-string date indexing\n            return self._setitem_slice(indexer, value)\n\n        if isinstance(key, DataFrame) or getattr(key, \"ndim\", None) == 2:\n            self._setitem_frame(key, value)\n        elif isinstance(key, (Series, np.ndarray, list, Index)):\n            self._setitem_array(key, value)\n        else:\n            # set column\n            self._set_item(key, value)\n\n    def _setitem_slice(self, key: slice, value):\n        # NB: we can't just use self.loc[key] = value because that\n        #  operates on labels and we need to operate positional for\n        #  backwards-compat, xref GH#31469\n        self._check_setitem_copy()\n        self.iloc._setitem_with_indexer(key, value)\n\n    def _setitem_array(self, key, value):\n        # also raises Exception if object array with NA values\n        if com.is_bool_indexer(key):\n            if len(key) != len(self.index):\n                raise ValueError(\n                    f\"Item wrong length {len(key)} instead of {len(self.index)}!\"\n                )\n            key = check_bool_indexer(self.index, key)\n            indexer = key.nonzero()[0]\n            self._check_setitem_copy()\n            self.iloc._setitem_with_indexer(indexer, value)\n        else:\n            if isinstance(value, DataFrame):\n                if len(value.columns) != len(key):\n                    raise ValueError(\"Columns must be same length as key\")\n                for k1, k2 in zip(key, value.columns):\n                    self[k1] = value[k2]\n            else:\n                self.loc._ensure_listlike_indexer(key, axis=1, value=value)\n                indexer = self.loc._get_listlike_indexer(\n                    key, axis=1, raise_missing=False\n                )[1]\n                self._check_setitem_copy()\n                self.iloc._setitem_with_indexer((slice(None), indexer), value)\n\n    def _setitem_frame(self, key, value):\n        # support boolean setting with DataFrame input, e.g.\n        # df[df > df2] = 0\n        if isinstance(key, np.ndarray):\n            if key.shape != self.shape:\n                raise ValueError(\"Array conditional must be same shape as self\")\n            key = self._constructor(key, **self._construct_axes_dict())\n\n        if key.size and not is_bool_dtype(key.values):\n            raise TypeError(\n                \"Must pass DataFrame or 2-d ndarray with boolean values only\"\n            )\n\n        self._check_inplace_setting(value)\n        self._check_setitem_copy()\n        self._where(-key, value, inplace=True)\n\n    def _iset_item(self, loc: int, value):\n        self._ensure_valid_index(value)\n\n        # technically _sanitize_column expects a label, not a position,\n        #  but the behavior is the same as long as we pass broadcast=False\n        value = self._sanitize_column(loc, value, broadcast=False)\n        NDFrame._iset_item(self, loc, value)\n\n        # check if we are modifying a copy\n        # try to set first as we want an invalid\n        # value exception to occur first\n        if len(self):\n            self._check_setitem_copy()\n\n    def _set_item(self, key, value):\n        \"\"\"\n        Add series to DataFrame in specified column.\n\n        If series is a numpy-array (not a Series/TimeSeries), it must be the\n        same length as the DataFrames index or an error will be thrown.\n\n        Series/TimeSeries will be conformed to the DataFrames index to\n        ensure homogeneity.\n        \"\"\"\n        self._ensure_valid_index(value)\n        value = self._sanitize_column(key, value)\n        NDFrame._set_item(self, key, value)\n\n        # check if we are modifying a copy\n        # try to set first as we want an invalid\n        # value exception to occur first\n        if len(self):\n            self._check_setitem_copy()\n\n    def _set_value(self, index, col, value, takeable: bool = False):\n        \"\"\"\n        Put single value at passed column and index.\n\n        Parameters\n        ----------\n        index : row label\n        col : column label\n        value : scalar\n        takeable : interpret the index/col as indexers, default False\n        \"\"\"\n        try:\n            if takeable is True:\n                series = self._ixs(col, axis=1)\n                series._set_value(index, value, takeable=True)\n                return\n\n            series = self._get_item_cache(col)\n            engine = self.index._engine\n            loc = engine.get_loc(index)\n            validate_numeric_casting(series.dtype, value)\n\n            series._values[loc] = value\n            # Note: trying to use series._set_value breaks tests in\n            #  tests.frame.indexing.test_indexing and tests.indexing.test_partial\n        except (KeyError, TypeError):\n            # set using a non-recursive method & reset the cache\n            if takeable:\n                self.iloc[index, col] = value\n            else:\n                self.loc[index, col] = value\n            self._item_cache.pop(col, None)\n\n    def _ensure_valid_index(self, value):\n        \"\"\"\n        Ensure that if we don't have an index, that we can create one from the\n        passed value.\n        \"\"\"\n        # GH5632, make sure that we are a Series convertible\n        if not len(self.index) and is_list_like(value) and len(value):\n            try:\n                value = Series(value)\n            except (ValueError, NotImplementedError, TypeError) as err:\n                raise ValueError(\n                    \"Cannot set a frame with no defined index \"\n                    \"and a value that cannot be converted to a Series\"\n                ) from err\n\n            # GH31368 preserve name of index\n            index_copy = value.index.copy()\n            if self.index.name is not None:\n                index_copy.name = self.index.name\n\n            self._mgr = self._mgr.reindex_axis(index_copy, axis=1, fill_value=np.nan)\n\n    def _box_col_values(self, values, loc: int) -> Series:\n        \"\"\"\n        Provide boxed values for a column.\n        \"\"\"\n        # Lookup in columns so that if e.g. a str datetime was passed\n        #  we attach the Timestamp object as the name.\n        name = self.columns[loc]\n        klass = self._constructor_sliced\n        return klass(values, index=self.index, name=name, fastpath=True)\n\n    # ----------------------------------------------------------------------\n    # Unsorted\n\n    def query(self, expr, inplace=False, **kwargs):\n        \"\"\"\n        Query the columns of a DataFrame with a boolean expression.\n\n        Parameters\n        ----------\n        expr : str\n            The query string to evaluate.\n\n            You can refer to variables\n            in the environment by prefixing them with an '@' character like\n            ``@a + b``.\n\n            You can refer to column names that are not valid Python variable names\n            by surrounding them in backticks. Thus, column names containing spaces\n            or punctuations (besides underscores) or starting with digits must be\n            surrounded by backticks. (For example, a column named \"Area (cm^2) would\n            be referenced as `Area (cm^2)`). Column names which are Python keywords\n            (like \"list\", \"for\", \"import\", etc) cannot be used.\n\n            For example, if one of your columns is called ``a a`` and you want\n            to sum it with ``b``, your query should be ```a a` + b``.\n\n            .. versionadded:: 0.25.0\n                Backtick quoting introduced.\n\n            .. versionadded:: 1.0.0\n                Expanding functionality of backtick quoting for more than only spaces.\n\n        inplace : bool\n            Whether the query should modify the data in place or return\n            a modified copy.\n        **kwargs\n            See the documentation for :func:`eval` for complete details\n            on the keyword arguments accepted by :meth:`DataFrame.query`.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame resulting from the provided query expression or\n            None if ``inplace=True``.\n\n        See Also\n        --------\n        eval : Evaluate a string describing operations on\n            DataFrame columns.\n        DataFrame.eval : Evaluate a string describing operations on\n            DataFrame columns.\n\n        Notes\n        -----\n        The result of the evaluation of this expression is first passed to\n        :attr:`DataFrame.loc` and if that fails because of a\n        multidimensional key (e.g., a DataFrame) then the result will be passed\n        to :meth:`DataFrame.__getitem__`.\n\n        This method uses the top-level :func:`eval` function to\n        evaluate the passed query.\n\n        The :meth:`~pandas.DataFrame.query` method uses a slightly\n        modified Python syntax by default. For example, the ``&`` and ``|``\n        (bitwise) operators have the precedence of their boolean cousins,\n        :keyword:`and` and :keyword:`or`. This *is* syntactically valid Python,\n        however the semantics are different.\n\n        You can change the semantics of the expression by passing the keyword\n        argument ``parser='python'``. This enforces the same semantics as\n        evaluation in Python space. Likewise, you can pass ``engine='python'``\n        to evaluate an expression using Python itself as a backend. This is not\n        recommended as it is inefficient compared to using ``numexpr`` as the\n        engine.\n\n        The :attr:`DataFrame.index` and\n        :attr:`DataFrame.columns` attributes of the\n        :class:`~pandas.DataFrame` instance are placed in the query namespace\n        by default, which allows you to treat both the index and columns of the\n        frame as a column in the frame.\n        The identifier ``index`` is used for the frame index; you can also\n        use the name of the index to identify it in a query. Please note that\n        Python keywords may not be used as identifiers.\n\n        For further details and examples see the ``query`` documentation in\n        :ref:`indexing <indexing.query>`.\n\n        *Backtick quoted variables*\n\n        Backtick quoted variables are parsed as literal Python code and\n        are converted internally to a Python valid identifier.\n        This can lead to the following problems.\n\n        During parsing a number of disallowed characters inside the backtick\n        quoted string are replaced by strings that are allowed as a Python identifier.\n        These characters include all operators in Python, the space character, the\n        question mark, the exclamation mark, the dollar sign, and the euro sign.\n        For other characters that fall outside the ASCII range (U+0001..U+007F)\n        and those that are not further specified in PEP 3131,\n        the query parser will raise an error.\n        This excludes whitespace different than the space character,\n        but also the hashtag (as it is used for comments) and the backtick\n        itself (backtick can also not be escaped).\n\n        In a special case, quotes that make a pair around a backtick can\n        confuse the parser.\n        For example, ```it's` > `that's``` will raise an error,\n        as it forms a quoted string (``'s > `that'``) with a backtick inside.\n\n        See also the Python documentation about lexical analysis\n        (https://docs.python.org/3/reference/lexical_analysis.html)\n        in combination with the source code in :mod:`pandas.core.computation.parsing`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': range(1, 6),\n        ...                    'B': range(10, 0, -2),\n        ...                    'C C': range(10, 5, -1)})\n        >>> df\n           A   B  C C\n        0  1  10   10\n        1  2   8    9\n        2  3   6    8\n        3  4   4    7\n        4  5   2    6\n        >>> df.query('A > B')\n           A  B  C C\n        4  5  2    6\n\n        The previous expression is equivalent to\n\n        >>> df[df.A > df.B]\n           A  B  C C\n        4  5  2    6\n\n        For columns with spaces in their name, you can use backtick quoting.\n\n        >>> df.query('B == `C C`')\n           A   B  C C\n        0  1  10   10\n\n        The previous expression is equivalent to\n\n        >>> df[df.B == df['C C']]\n           A   B  C C\n        0  1  10   10\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if not isinstance(expr, str):\n            msg = f\"expr must be a string to be evaluated, {type(expr)} given\"\n            raise ValueError(msg)\n        kwargs[\"level\"] = kwargs.pop(\"level\", 0) + 1\n        kwargs[\"target\"] = None\n        res = self.eval(expr, **kwargs)\n\n        try:\n            result = self.loc[res]\n        except ValueError:\n            # when res is multi-dimensional loc raises, but this is sometimes a\n            # valid query\n            result = self[res]\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result\n\n    def eval(self, expr, inplace=False, **kwargs):\n        \"\"\"\n        Evaluate a string describing operations on DataFrame columns.\n\n        Operates on columns only, not specific rows or elements.  This allows\n        `eval` to run arbitrary code, which can make you vulnerable to code\n        injection if you pass user input to this function.\n\n        Parameters\n        ----------\n        expr : str\n            The expression string to evaluate.\n        inplace : bool, default False\n            If the expression contains an assignment, whether to perform the\n            operation inplace and mutate the existing DataFrame. Otherwise,\n            a new DataFrame is returned.\n        **kwargs\n            See the documentation for :func:`eval` for complete details\n            on the keyword arguments accepted by\n            :meth:`~pandas.DataFrame.query`.\n\n        Returns\n        -------\n        ndarray, scalar, pandas object, or None\n            The result of the evaluation or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.query : Evaluates a boolean expression to query the columns\n            of a frame.\n        DataFrame.assign : Can evaluate an expression or function to create new\n            values for a column.\n        eval : Evaluate a Python expression as a string using various\n            backends.\n\n        Notes\n        -----\n        For more details see the API documentation for :func:`~eval`.\n        For detailed examples see :ref:`enhancing performance with eval\n        <enhancingperf.eval>`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': range(1, 6), 'B': range(10, 0, -2)})\n        >>> df\n           A   B\n        0  1  10\n        1  2   8\n        2  3   6\n        3  4   4\n        4  5   2\n        >>> df.eval('A + B')\n        0    11\n        1    10\n        2     9\n        3     8\n        4     7\n        dtype: int64\n\n        Assignment is allowed though by default the original DataFrame is not\n        modified.\n\n        >>> df.eval('C = A + B')\n           A   B   C\n        0  1  10  11\n        1  2   8  10\n        2  3   6   9\n        3  4   4   8\n        4  5   2   7\n        >>> df\n           A   B\n        0  1  10\n        1  2   8\n        2  3   6\n        3  4   4\n        4  5   2\n\n        Use ``inplace=True`` to modify the original DataFrame.\n\n        >>> df.eval('C = A + B', inplace=True)\n        >>> df\n           A   B   C\n        0  1  10  11\n        1  2   8  10\n        2  3   6   9\n        3  4   4   8\n        4  5   2   7\n\n        Multiple columns can be assigned to using multi-line expressions:\n\n        >>> df.eval(\n        ...     '''\n        ... C = A + B\n        ... D = A - B\n        ... '''\n        ... )\n           A   B   C  D\n        0  1  10  11 -9\n        1  2   8  10 -6\n        2  3   6   9 -3\n        3  4   4   8  0\n        4  5   2   7  3\n        \"\"\"\n        from pandas.core.computation.eval import eval as _eval\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        resolvers = kwargs.pop(\"resolvers\", None)\n        kwargs[\"level\"] = kwargs.pop(\"level\", 0) + 1\n        if resolvers is None:\n            index_resolvers = self._get_index_resolvers()\n            column_resolvers = self._get_cleaned_column_resolvers()\n            resolvers = column_resolvers, index_resolvers\n        if \"target\" not in kwargs:\n            kwargs[\"target\"] = self\n        kwargs[\"resolvers\"] = kwargs.get(\"resolvers\", ()) + tuple(resolvers)\n\n        return _eval(expr, inplace=inplace, **kwargs)\n\n    def select_dtypes(self, include=None, exclude=None) -> DataFrame:\n        \"\"\"\n        Return a subset of the DataFrame's columns based on the column dtypes.\n\n        Parameters\n        ----------\n        include, exclude : scalar or list-like\n            A selection of dtypes or strings to be included/excluded. At least\n            one of these parameters must be supplied.\n\n        Returns\n        -------\n        DataFrame\n            The subset of the frame including the dtypes in ``include`` and\n            excluding the dtypes in ``exclude``.\n\n        Raises\n        ------\n        ValueError\n            * If both of ``include`` and ``exclude`` are empty\n            * If ``include`` and ``exclude`` have overlapping elements\n            * If any kind of string dtype is passed in.\n\n        See Also\n        --------\n        DataFrame.dtypes: Return Series with the data type of each column.\n\n        Notes\n        -----\n        * To select all *numeric* types, use ``np.number`` or ``'number'``\n        * To select strings you must use the ``object`` dtype, but note that\n          this will return *all* object dtype columns\n        * See the `numpy dtype hierarchy\n          <https://numpy.org/doc/stable/reference/arrays.scalars.html>`__\n        * To select datetimes, use ``np.datetime64``, ``'datetime'`` or\n          ``'datetime64'``\n        * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or\n          ``'timedelta64'``\n        * To select Pandas categorical dtypes, use ``'category'``\n        * To select Pandas datetimetz dtypes, use ``'datetimetz'`` (new in\n          0.20.0) or ``'datetime64[ns, tz]'``\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'a': [1, 2] * 3,\n        ...                    'b': [True, False] * 3,\n        ...                    'c': [1.0, 2.0] * 3})\n        >>> df\n                a      b  c\n        0       1   True  1.0\n        1       2  False  2.0\n        2       1   True  1.0\n        3       2  False  2.0\n        4       1   True  1.0\n        5       2  False  2.0\n\n        >>> df.select_dtypes(include='bool')\n           b\n        0  True\n        1  False\n        2  True\n        3  False\n        4  True\n        5  False\n\n        >>> df.select_dtypes(include=['float64'])\n           c\n        0  1.0\n        1  2.0\n        2  1.0\n        3  2.0\n        4  1.0\n        5  2.0\n\n        >>> df.select_dtypes(exclude=['int64'])\n               b    c\n        0   True  1.0\n        1  False  2.0\n        2   True  1.0\n        3  False  2.0\n        4   True  1.0\n        5  False  2.0\n        \"\"\"\n        if not is_list_like(include):\n            include = (include,) if include is not None else ()\n        if not is_list_like(exclude):\n            exclude = (exclude,) if exclude is not None else ()\n\n        selection = (frozenset(include), frozenset(exclude))\n\n        if not any(selection):\n            raise ValueError(\"at least one of include or exclude must be nonempty\")\n\n        # convert the myriad valid dtypes object to a single representation\n        include = frozenset(infer_dtype_from_object(x) for x in include)\n        exclude = frozenset(infer_dtype_from_object(x) for x in exclude)\n        for dtypes in (include, exclude):\n            invalidate_string_dtypes(dtypes)\n\n        # can't both include AND exclude!\n        if not include.isdisjoint(exclude):\n            raise ValueError(f\"include and exclude overlap on {(include & exclude)}\")\n\n        # We raise when both include and exclude are empty\n        # Hence, we can just shrink the columns we want to keep\n        keep_these = np.full(self.shape[1], True)\n\n        def extract_unique_dtypes_from_dtypes_set(\n            dtypes_set: FrozenSet[Dtype], unique_dtypes: np.ndarray\n        ) -> List[Dtype]:\n            extracted_dtypes = [\n                unique_dtype\n                for unique_dtype in unique_dtypes\n                # error: Argument 1 to \"tuple\" has incompatible type\n                # \"FrozenSet[Union[ExtensionDtype, str, Any, Type[str],\n                # Type[float], Type[int], Type[complex], Type[bool]]]\";\n                # expected \"Iterable[Union[type, Tuple[Any, ...]]]\"\n                if issubclass(\n                    unique_dtype.type, tuple(dtypes_set)  # type: ignore[arg-type]\n                )\n            ]\n            return extracted_dtypes\n\n        unique_dtypes = self.dtypes.unique()\n\n        if include:\n            included_dtypes = extract_unique_dtypes_from_dtypes_set(\n                include, unique_dtypes\n            )\n            keep_these &= self.dtypes.isin(included_dtypes)\n\n        if exclude:\n            excluded_dtypes = extract_unique_dtypes_from_dtypes_set(\n                exclude, unique_dtypes\n            )\n            keep_these &= ~self.dtypes.isin(excluded_dtypes)\n\n        return self.iloc[:, keep_these.values]\n\n    def insert(self, loc, column, value, allow_duplicates=False) -> None:\n        \"\"\"\n        Insert column into DataFrame at specified location.\n\n        Raises a ValueError if `column` is already contained in the DataFrame,\n        unless `allow_duplicates` is set to True.\n\n        Parameters\n        ----------\n        loc : int\n            Insertion index. Must verify 0 <= loc <= len(columns).\n        column : str, number, or hashable object\n            Label of the inserted column.\n        value : int, Series, or array-like\n        allow_duplicates : bool, optional\n        \"\"\"\n        if allow_duplicates and not self.flags.allows_duplicate_labels:\n            raise ValueError(\n                \"Cannot specify 'allow_duplicates=True' when \"\n                \"'self.flags.allows_duplicate_labels' is False.\"\n            )\n        self._ensure_valid_index(value)\n        value = self._sanitize_column(column, value, broadcast=False)\n        self._mgr.insert(loc, column, value, allow_duplicates=allow_duplicates)\n\n    def assign(self, **kwargs) -> DataFrame:\n        r\"\"\"\n        Assign new columns to a DataFrame.\n\n        Returns a new object with all original columns in addition to new ones.\n        Existing columns that are re-assigned will be overwritten.\n\n        Parameters\n        ----------\n        **kwargs : dict of {str: callable or Series}\n            The column names are keywords. If the values are\n            callable, they are computed on the DataFrame and\n            assigned to the new columns. The callable must not\n            change input DataFrame (though pandas doesn't check it).\n            If the values are not callable, (e.g. a Series, scalar, or array),\n            they are simply assigned.\n\n        Returns\n        -------\n        DataFrame\n            A new DataFrame with the new columns in addition to\n            all the existing columns.\n\n        Notes\n        -----\n        Assigning multiple columns within the same ``assign`` is possible.\n        Later items in '\\*\\*kwargs' may refer to newly created or modified\n        columns in 'df'; items are computed and assigned into 'df' in order.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'temp_c': [17.0, 25.0]},\n        ...                   index=['Portland', 'Berkeley'])\n        >>> df\n                  temp_c\n        Portland    17.0\n        Berkeley    25.0\n\n        Where the value is a callable, evaluated on `df`:\n\n        >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)\n                  temp_c  temp_f\n        Portland    17.0    62.6\n        Berkeley    25.0    77.0\n\n        Alternatively, the same behavior can be achieved by directly\n        referencing an existing Series or sequence:\n\n        >>> df.assign(temp_f=df['temp_c'] * 9 / 5 + 32)\n                  temp_c  temp_f\n        Portland    17.0    62.6\n        Berkeley    25.0    77.0\n\n        You can create multiple columns within the same assign where one\n        of the columns depends on another one defined within the same assign:\n\n        >>> df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32,\n        ...           temp_k=lambda x: (x['temp_f'] +  459.67) * 5 / 9)\n                  temp_c  temp_f  temp_k\n        Portland    17.0    62.6  290.15\n        Berkeley    25.0    77.0  298.15\n        \"\"\"\n        data = self.copy()\n\n        for k, v in kwargs.items():\n            data[k] = com.apply_if_callable(v, data)\n        return data\n\n    def _sanitize_column(self, key, value, broadcast=True):\n        \"\"\"\n        Ensures new columns (which go into the BlockManager as new blocks) are\n        always copied and converted into an array.\n\n        Parameters\n        ----------\n        key : object\n        value : scalar, Series, or array-like\n        broadcast : bool, default True\n            If ``key`` matches multiple duplicate column names in the\n            DataFrame, this parameter indicates whether ``value`` should be\n            tiled so that the returned array contains a (duplicated) column for\n            each occurrence of the key. If False, ``value`` will not be tiled.\n\n        Returns\n        -------\n        numpy.ndarray\n        \"\"\"\n\n        def reindexer(value):\n            # reindex if necessary\n\n            if value.index.equals(self.index) or not len(self.index):\n                value = value._values.copy()\n            else:\n\n                # GH 4107\n                try:\n                    value = value.reindex(self.index)._values\n                except ValueError as err:\n                    # raised in MultiIndex.from_tuples, see test_insert_error_msmgs\n                    if not value.index.is_unique:\n                        # duplicate axis\n                        raise err\n\n                    # other\n                    raise TypeError(\n                        \"incompatible index of inserted column with frame index\"\n                    ) from err\n            return value\n\n        if isinstance(value, Series):\n            value = reindexer(value)\n\n        elif isinstance(value, DataFrame):\n            # align right-hand-side columns if self.columns\n            # is multi-index and self[key] is a sub-frame\n            if isinstance(self.columns, MultiIndex) and key in self.columns:\n                loc = self.columns.get_loc(key)\n                if isinstance(loc, (slice, Series, np.ndarray, Index)):\n                    cols = maybe_droplevels(self.columns[loc], key)\n                    if len(cols) and not cols.equals(value.columns):\n                        value = value.reindex(cols, axis=1)\n            # now align rows\n            value = reindexer(value).T\n\n        elif isinstance(value, ExtensionArray):\n            # Explicitly copy here, instead of in sanitize_index,\n            # as sanitize_index won't copy an EA, even with copy=True\n            value = value.copy()\n            value = sanitize_index(value, self.index)\n\n        elif isinstance(value, Index) or is_sequence(value):\n\n            # turn me into an ndarray\n            value = sanitize_index(value, self.index)\n            if not isinstance(value, (np.ndarray, Index)):\n                if isinstance(value, list) and len(value) > 0:\n                    value = maybe_convert_platform(value)\n                else:\n                    value = com.asarray_tuplesafe(value)\n            elif value.ndim == 2:\n                value = value.copy().T\n            elif isinstance(value, Index):\n                value = value.copy(deep=True)\n            else:\n                value = value.copy()\n\n            # possibly infer to datetimelike\n            if is_object_dtype(value.dtype):\n                value = maybe_infer_to_datetimelike(value)\n\n        else:\n            # cast ignores pandas dtypes. so save the dtype first\n            infer_dtype, _ = infer_dtype_from_scalar(value, pandas_dtype=True)\n\n            # upcast\n            if is_extension_array_dtype(infer_dtype):\n                value = construct_1d_arraylike_from_scalar(\n                    value, len(self.index), infer_dtype\n                )\n            else:\n                value = cast_scalar_to_array(len(self.index), value)\n\n            value = maybe_cast_to_datetime(value, infer_dtype)\n\n        # return internal types directly\n        if is_extension_array_dtype(value):\n            return value\n\n        # broadcast across multiple columns if necessary\n        if broadcast and key in self.columns and value.ndim == 1:\n            if not self.columns.is_unique or isinstance(self.columns, MultiIndex):\n                existing_piece = self[key]\n                if isinstance(existing_piece, DataFrame):\n                    value = np.tile(value, (len(existing_piece.columns), 1))\n\n        return np.atleast_2d(np.asarray(value))\n\n    @property\n    def _series(self):\n        return {\n            item: Series(\n                self._mgr.iget(idx), index=self.index, name=item, fastpath=True\n            )\n            for idx, item in enumerate(self.columns)\n        }\n\n    def lookup(self, row_labels, col_labels) -> np.ndarray:\n        \"\"\"\n        Label-based \"fancy indexing\" function for DataFrame.\n        Given equal-length arrays of row and column labels, return an\n        array of the values corresponding to each (row, col) pair.\n\n        .. deprecated:: 1.2.0\n            DataFrame.lookup is deprecated,\n            use DataFrame.melt and DataFrame.loc instead.\n            For an example see :meth:`~pandas.DataFrame.lookup`\n            in the user guide.\n\n        Parameters\n        ----------\n        row_labels : sequence\n            The row labels to use for lookup.\n        col_labels : sequence\n            The column labels to use for lookup.\n\n        Returns\n        -------\n        numpy.ndarray\n            The found values.\n        \"\"\"\n        msg = (\n            \"The 'lookup' method is deprecated and will be\"\n            \"removed in a future version.\"\n            \"You can use DataFrame.melt and DataFrame.loc\"\n            \"as a substitute.\"\n        )\n        warnings.warn(msg, FutureWarning, stacklevel=2)\n\n        n = len(row_labels)\n        if n != len(col_labels):\n            raise ValueError(\"Row labels must have same size as column labels\")\n        if not (self.index.is_unique and self.columns.is_unique):\n            # GH#33041\n            raise ValueError(\"DataFrame.lookup requires unique index and columns\")\n\n        thresh = 1000\n        if not self._is_mixed_type or n > thresh:\n            values = self.values\n            ridx = self.index.get_indexer(row_labels)\n            cidx = self.columns.get_indexer(col_labels)\n            if (ridx == -1).any():\n                raise KeyError(\"One or more row labels was not found\")\n            if (cidx == -1).any():\n                raise KeyError(\"One or more column labels was not found\")\n            flat_index = ridx * len(self.columns) + cidx\n            result = values.flat[flat_index]\n        else:\n            result = np.empty(n, dtype=\"O\")\n            for i, (r, c) in enumerate(zip(row_labels, col_labels)):\n                result[i] = self._get_value(r, c)\n\n        if is_object_dtype(result):\n            result = lib.maybe_convert_objects(result)\n\n        return result\n\n    # ----------------------------------------------------------------------\n    # Reindexing and alignment\n\n    def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy):\n        frame = self\n\n        columns = axes[\"columns\"]\n        if columns is not None:\n            frame = frame._reindex_columns(\n                columns, method, copy, level, fill_value, limit, tolerance\n            )\n\n        index = axes[\"index\"]\n        if index is not None:\n            frame = frame._reindex_index(\n                index, method, copy, level, fill_value, limit, tolerance\n            )\n\n        return frame\n\n    def _reindex_index(\n        self,\n        new_index,\n        method,\n        copy,\n        level,\n        fill_value=np.nan,\n        limit=None,\n        tolerance=None,\n    ):\n        new_index, indexer = self.index.reindex(\n            new_index, method=method, level=level, limit=limit, tolerance=tolerance\n        )\n        return self._reindex_with_indexers(\n            {0: [new_index, indexer]},\n            copy=copy,\n            fill_value=fill_value,\n            allow_dups=False,\n        )\n\n    def _reindex_columns(\n        self,\n        new_columns,\n        method,\n        copy,\n        level,\n        fill_value=None,\n        limit=None,\n        tolerance=None,\n    ):\n        new_columns, indexer = self.columns.reindex(\n            new_columns, method=method, level=level, limit=limit, tolerance=tolerance\n        )\n        return self._reindex_with_indexers(\n            {1: [new_columns, indexer]},\n            copy=copy,\n            fill_value=fill_value,\n            allow_dups=False,\n        )\n\n    def _reindex_multi(self, axes, copy, fill_value) -> DataFrame:\n        \"\"\"\n        We are guaranteed non-Nones in the axes.\n        \"\"\"\n        new_index, row_indexer = self.index.reindex(axes[\"index\"])\n        new_columns, col_indexer = self.columns.reindex(axes[\"columns\"])\n\n        if row_indexer is not None and col_indexer is not None:\n            indexer = row_indexer, col_indexer\n            new_values = algorithms.take_2d_multi(\n                self.values, indexer, fill_value=fill_value\n            )\n            return self._constructor(new_values, index=new_index, columns=new_columns)\n        else:\n            return self._reindex_with_indexers(\n                {0: [new_index, row_indexer], 1: [new_columns, col_indexer]},\n                copy=copy,\n                fill_value=fill_value,\n            )\n\n    @doc(NDFrame.align, **_shared_doc_kwargs)\n    def align(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy=True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n        broadcast_axis=None,\n    ) -> DataFrame:\n        return super().align(\n            other,\n            join=join,\n            axis=axis,\n            level=level,\n            copy=copy,\n            fill_value=fill_value,\n            method=method,\n            limit=limit,\n            fill_axis=fill_axis,\n            broadcast_axis=broadcast_axis,\n        )\n\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\n        Change the row labels.\n\n        >>> df.set_axis(['a', 'b', 'c'], axis='index')\n           A  B\n        a  1  4\n        b  2  5\n        c  3  6\n\n        Change the column labels.\n\n        >>> df.set_axis(['I', 'II'], axis='columns')\n           I  II\n        0  1   4\n        1  2   5\n        2  3   6\n\n        Now, update the labels inplace.\n\n        >>> df.set_axis(['i', 'ii'], axis='columns', inplace=True)\n        >>> df\n           i  ii\n        0  1   4\n        1  2   5\n        2  3   6\n        \"\"\"\n    )\n    @Substitution(\n        **_shared_doc_kwargs,\n        extended_summary_sub=\" column or\",\n        axis_description_sub=\", and 1 identifies the columns\",\n        see_also_sub=\" or columns\",\n    )\n    @Appender(NDFrame.set_axis.__doc__)\n    def set_axis(self, labels, axis: Axis = 0, inplace: bool = False):\n        return super().set_axis(labels, axis=axis, inplace=inplace)\n\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(NDFrame.reindex.__doc__)\n    @rewrite_axis_style_signature(\n        \"labels\",\n        [\n            (\"method\", None),\n            (\"copy\", True),\n            (\"level\", None),\n            (\"fill_value\", np.nan),\n            (\"limit\", None),\n            (\"tolerance\", None),\n        ],\n    )\n    def reindex(self, *args, **kwargs) -> DataFrame:\n        axes = validate_axis_style_args(self, args, kwargs, \"labels\", \"reindex\")\n        kwargs.update(axes)\n        # Pop these, since the values are in `kwargs` under different names\n        kwargs.pop(\"axis\", None)\n        kwargs.pop(\"labels\", None)\n        return super().reindex(**kwargs)\n\n    def drop(\n        self,\n        labels=None,\n        axis=0,\n        index=None,\n        columns=None,\n        level=None,\n        inplace=False,\n        errors=\"raise\",\n    ):\n        \"\"\"\n        Drop specified labels from rows or columns.\n\n        Remove rows or columns by specifying label names and corresponding\n        axis, or by specifying directly index or column names. When using a\n        multi-index, labels on different levels can be removed by specifying\n        the level.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index or column labels to drop.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Whether to drop labels from the index (0 or 'index') or\n            columns (1 or 'columns').\n        index : single label or list-like\n            Alternative to specifying axis (``labels, axis=0``\n            is equivalent to ``index=labels``).\n        columns : single label or list-like\n            Alternative to specifying axis (``labels, axis=1``\n            is equivalent to ``columns=labels``).\n        level : int or level name, optional\n            For MultiIndex, level from which the labels will be removed.\n        inplace : bool, default False\n            If False, return a copy. Otherwise, do operation\n            inplace and return None.\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and only existing labels are\n            dropped.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame without the removed index or column labels or\n            None if ``inplace=True``.\n\n        Raises\n        ------\n        KeyError\n            If any of the labels is not found in the selected axis.\n\n        See Also\n        --------\n        DataFrame.loc : Label-location based indexer for selection by label.\n        DataFrame.dropna : Return DataFrame with labels on given axis omitted\n            where (all or any) data are missing.\n        DataFrame.drop_duplicates : Return DataFrame with duplicate rows\n            removed, optionally only considering certain columns.\n        Series.drop : Return Series with specified index labels removed.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n        ...                   columns=['A', 'B', 'C', 'D'])\n        >>> df\n           A  B   C   D\n        0  0  1   2   3\n        1  4  5   6   7\n        2  8  9  10  11\n\n        Drop columns\n\n        >>> df.drop(['B', 'C'], axis=1)\n           A   D\n        0  0   3\n        1  4   7\n        2  8  11\n\n        >>> df.drop(columns=['B', 'C'])\n           A   D\n        0  0   3\n        1  4   7\n        2  8  11\n\n        Drop a row by index\n\n        >>> df.drop([0, 1])\n           A  B   C   D\n        2  8  9  10  11\n\n        Drop columns and/or rows of MultiIndex DataFrame\n\n        >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n        ...                              ['speed', 'weight', 'length']],\n        ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n        ...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n        ...                         [250, 150], [1.5, 0.8], [320, 250],\n        ...                         [1, 0.8], [0.3, 0.2]])\n        >>> df\n                        big     small\n        lama    speed   45.0    30.0\n                weight  200.0   100.0\n                length  1.5     1.0\n        cow     speed   30.0    20.0\n                weight  250.0   150.0\n                length  1.5     0.8\n        falcon  speed   320.0   250.0\n                weight  1.0     0.8\n                length  0.3     0.2\n\n        >>> df.drop(index='cow', columns='small')\n                        big\n        lama    speed   45.0\n                weight  200.0\n                length  1.5\n        falcon  speed   320.0\n                weight  1.0\n                length  0.3\n\n        >>> df.drop(index='length', level=1)\n                        big     small\n        lama    speed   45.0    30.0\n                weight  200.0   100.0\n        cow     speed   30.0    20.0\n                weight  250.0   150.0\n        falcon  speed   320.0   250.0\n                weight  1.0     0.8\n        \"\"\"\n        return super().drop(\n            labels=labels,\n            axis=axis,\n            index=index,\n            columns=columns,\n            level=level,\n            inplace=inplace,\n            errors=errors,\n        )\n\n    @rewrite_axis_style_signature(\n        \"mapper\",\n        [(\"copy\", True), (\"inplace\", False), (\"level\", None), (\"errors\", \"ignore\")],\n    )\n    def rename(\n        self,\n        mapper: Optional[Renamer] = None,\n        *,\n        index: Optional[Renamer] = None,\n        columns: Optional[Renamer] = None,\n        axis: Optional[Axis] = None,\n        copy: bool = True,\n        inplace: bool = False,\n        level: Optional[Level] = None,\n        errors: str = \"ignore\",\n    ) -> Optional[DataFrame]:\n        \"\"\"\n        Alter axes labels.\n\n        Function / dict values must be unique (1-to-1). Labels not contained in\n        a dict / Series will be left as-is. Extra labels listed don't throw an\n        error.\n\n        See the :ref:`user guide <basics.rename>` for more.\n\n        Parameters\n        ----------\n        mapper : dict-like or function\n            Dict-like or function transformations to apply to\n            that axis' values. Use either ``mapper`` and ``axis`` to\n            specify the axis to target with ``mapper``, or ``index`` and\n            ``columns``.\n        index : dict-like or function\n            Alternative to specifying axis (``mapper, axis=0``\n            is equivalent to ``index=mapper``).\n        columns : dict-like or function\n            Alternative to specifying axis (``mapper, axis=1``\n            is equivalent to ``columns=mapper``).\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis to target with ``mapper``. Can be either the axis name\n            ('index', 'columns') or number (0, 1). The default is 'index'.\n        copy : bool, default True\n            Also copy underlying data.\n        inplace : bool, default False\n            Whether to return a new DataFrame. If True then value of copy is\n            ignored.\n        level : int or level name, default None\n            In case of a MultiIndex, only rename labels in the specified\n            level.\n        errors : {'ignore', 'raise'}, default 'ignore'\n            If 'raise', raise a `KeyError` when a dict-like `mapper`, `index`,\n            or `columns` contains labels that are not present in the Index\n            being transformed.\n            If 'ignore', existing keys will be renamed and extra keys will be\n            ignored.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with the renamed axis labels or None if ``inplace=True``.\n\n        Raises\n        ------\n        KeyError\n            If any of the labels is not found in the selected axis and\n            \"errors='raise'\".\n\n        See Also\n        --------\n        DataFrame.rename_axis : Set the name of the axis.\n\n        Examples\n        --------\n        ``DataFrame.rename`` supports two calling conventions\n\n        * ``(index=index_mapper, columns=columns_mapper, ...)``\n        * ``(mapper, axis={'index', 'columns'}, ...)``\n\n        We *highly* recommend using keyword arguments to clarify your\n        intent.\n\n        Rename columns using a mapping:\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n        >>> df.rename(columns={\"A\": \"a\", \"B\": \"c\"})\n           a  c\n        0  1  4\n        1  2  5\n        2  3  6\n\n        Rename index using a mapping:\n\n        >>> df.rename(index={0: \"x\", 1: \"y\", 2: \"z\"})\n           A  B\n        x  1  4\n        y  2  5\n        z  3  6\n\n        Cast index labels to a different type:\n\n        >>> df.index\n        RangeIndex(start=0, stop=3, step=1)\n        >>> df.rename(index=str).index\n        Index(['0', '1', '2'], dtype='object')\n\n        >>> df.rename(columns={\"A\": \"a\", \"B\": \"b\", \"C\": \"c\"}, errors=\"raise\")\n        Traceback (most recent call last):\n        KeyError: ['C'] not found in axis\n\n        Using axis-style parameters:\n\n        >>> df.rename(str.lower, axis='columns')\n           a  b\n        0  1  4\n        1  2  5\n        2  3  6\n\n        >>> df.rename({1: 2, 2: 4}, axis='index')\n           A  B\n        0  1  4\n        2  2  5\n        4  3  6\n        \"\"\"\n        return super().rename(\n            mapper=mapper,\n            index=index,\n            columns=columns,\n            axis=axis,\n            copy=copy,\n            inplace=inplace,\n            level=level,\n            errors=errors,\n        )\n\n    @doc(NDFrame.fillna, **_shared_doc_kwargs)\n    def fillna(\n        self,\n        value=None,\n        method=None,\n        axis=None,\n        inplace=False,\n        limit=None,\n        downcast=None,\n    ) -> Optional[DataFrame]:\n        return super().fillna(\n            value=value,\n            method=method,\n            axis=axis,\n            inplace=inplace,\n            limit=limit,\n            downcast=downcast,\n        )\n\n    def pop(self, item: Label) -> Series:\n        \"\"\"\n        Return item and drop from frame. Raise KeyError if not found.\n\n        Parameters\n        ----------\n        item : label\n            Label of column to be popped.\n\n        Returns\n        -------\n        Series\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n        ...                    ('parrot', 'bird', 24.0),\n        ...                    ('lion', 'mammal', 80.5),\n        ...                    ('monkey', 'mammal', np.nan)],\n        ...                   columns=('name', 'class', 'max_speed'))\n        >>> df\n             name   class  max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        >>> df.pop('class')\n        0      bird\n        1      bird\n        2    mammal\n        3    mammal\n        Name: class, dtype: object\n\n        >>> df\n             name  max_speed\n        0  falcon      389.0\n        1  parrot       24.0\n        2    lion       80.5\n        3  monkey        NaN\n        \"\"\"\n        return super().pop(item=item)\n\n    @doc(NDFrame.replace, **_shared_doc_kwargs)\n    def replace(\n        self,\n        to_replace=None,\n        value=None,\n        inplace=False,\n        limit=None,\n        regex=False,\n        method=\"pad\",\n    ):\n        return super().replace(\n            to_replace=to_replace,\n            value=value,\n            inplace=inplace,\n            limit=limit,\n            regex=regex,\n            method=method,\n        )\n\n    def _replace_columnwise(\n        self, mapping: Dict[Label, Tuple[Any, Any]], inplace: bool, regex\n    ):\n        \"\"\"\n        Dispatch to Series.replace column-wise.\n\n\n        Parameters\n        ----------\n        mapping : dict\n            of the form {col: (target, value)}\n        inplace : bool\n        regex : bool or same types as `to_replace` in DataFrame.replace\n\n        Returns\n        -------\n        DataFrame or None\n        \"\"\"\n        # Operate column-wise\n        res = self if inplace else self.copy()\n        ax = self.columns\n\n        for i in range(len(ax)):\n            if ax[i] in mapping:\n                ser = self.iloc[:, i]\n\n                target, value = mapping[ax[i]]\n                newobj = ser.replace(target, value, regex=regex)\n\n                res.iloc[:, i] = newobj\n\n        if inplace:\n            return\n        return res.__finalize__(self)\n\n    @doc(NDFrame.shift, klass=_shared_doc_kwargs[\"klass\"])\n    def shift(\n        self, periods=1, freq=None, axis=0, fill_value=lib.no_default\n    ) -> DataFrame:\n        axis = self._get_axis_number(axis)\n\n        ncols = len(self.columns)\n        if axis == 1 and periods != 0 and fill_value is lib.no_default and ncols > 0:\n            # We will infer fill_value to match the closest column\n\n            if periods > 0:\n                result = self.iloc[:, :-periods]\n                for col in range(min(ncols, abs(periods))):\n                    # TODO(EA2D): doing this in a loop unnecessary with 2D EAs\n                    # Define filler inside loop so we get a copy\n                    filler = self.iloc[:, 0].shift(len(self))\n                    result.insert(0, col, filler, allow_duplicates=True)\n            else:\n                result = self.iloc[:, -periods:]\n                for col in range(min(ncols, abs(periods))):\n                    # Define filler inside loop so we get a copy\n                    filler = self.iloc[:, -1].shift(len(self))\n                    result.insert(\n                        len(result.columns), col, filler, allow_duplicates=True\n                    )\n\n            result.columns = self.columns.copy()\n            return result\n\n        return super().shift(\n            periods=periods, freq=freq, axis=axis, fill_value=fill_value\n        )\n\n    def set_index(\n        self, keys, drop=True, append=False, inplace=False, verify_integrity=False\n    ):\n        \"\"\"\n        Set the DataFrame index using existing columns.\n\n        Set the DataFrame index (row labels) using one or more existing\n        columns or arrays (of the correct length). The index can replace the\n        existing index or expand on it.\n\n        Parameters\n        ----------\n        keys : label or array-like or list of labels/arrays\n            This parameter can be either a single column key, a single array of\n            the same length as the calling DataFrame, or a list containing an\n            arbitrary combination of column keys and arrays. Here, \"array\"\n            encompasses :class:`Series`, :class:`Index`, ``np.ndarray``, and\n            instances of :class:`~collections.abc.Iterator`.\n        drop : bool, default True\n            Delete columns to be used as the new index.\n        append : bool, default False\n            Whether to append columns to existing index.\n        inplace : bool, default False\n            Modify the DataFrame in place (do not create a new object).\n        verify_integrity : bool, default False\n            Check the new index for duplicates. Otherwise defer the check until\n            necessary. Setting to False will improve the performance of this\n            method.\n\n        Returns\n        -------\n        DataFrame or None\n            Changed row labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.reset_index : Opposite of set_index.\n        DataFrame.reindex : Change to new indices or expand indices.\n        DataFrame.reindex_like : Change to same indices as other DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'month': [1, 4, 7, 10],\n        ...                    'year': [2012, 2014, 2013, 2014],\n        ...                    'sale': [55, 40, 84, 31]})\n        >>> df\n           month  year  sale\n        0      1  2012    55\n        1      4  2014    40\n        2      7  2013    84\n        3     10  2014    31\n\n        Set the index to become the 'month' column:\n\n        >>> df.set_index('month')\n               year  sale\n        month\n        1      2012    55\n        4      2014    40\n        7      2013    84\n        10     2014    31\n\n        Create a MultiIndex using columns 'year' and 'month':\n\n        >>> df.set_index(['year', 'month'])\n                    sale\n        year  month\n        2012  1     55\n        2014  4     40\n        2013  7     84\n        2014  10    31\n\n        Create a MultiIndex using an Index and a column:\n\n        >>> df.set_index([pd.Index([1, 2, 3, 4]), 'year'])\n                 month  sale\n           year\n        1  2012  1      55\n        2  2014  4      40\n        3  2013  7      84\n        4  2014  10     31\n\n        Create a MultiIndex using two Series:\n\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> df.set_index([s, s**2])\n              month  year  sale\n        1 1       1  2012    55\n        2 4       4  2014    40\n        3 9       7  2013    84\n        4 16     10  2014    31\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        self._check_inplace_and_allows_duplicate_labels(inplace)\n        if not isinstance(keys, list):\n            keys = [keys]\n\n        err_msg = (\n            'The parameter \"keys\" may be a column key, one-dimensional '\n            \"array, or a list containing only valid column keys and \"\n            \"one-dimensional arrays.\"\n        )\n\n        missing: List[Label] = []\n        for col in keys:\n            if isinstance(col, (Index, Series, np.ndarray, list, abc.Iterator)):\n                # arrays are fine as long as they are one-dimensional\n                # iterators get converted to list below\n                if getattr(col, \"ndim\", 1) != 1:\n                    raise ValueError(err_msg)\n            else:\n                # everything else gets tried as a key; see GH 24969\n                try:\n                    found = col in self.columns\n                except TypeError as err:\n                    raise TypeError(\n                        f\"{err_msg}. Received column of type {type(col)}\"\n                    ) from err\n                else:\n                    if not found:\n                        missing.append(col)\n\n        if missing:\n            raise KeyError(f\"None of {missing} are in the columns\")\n\n        if inplace:\n            frame = self\n        else:\n            frame = self.copy()\n\n        arrays = []\n        names: List[Label] = []\n        if append:\n            names = list(self.index.names)\n            if isinstance(self.index, MultiIndex):\n                for i in range(self.index.nlevels):\n                    arrays.append(self.index._get_level_values(i))\n            else:\n                arrays.append(self.index)\n\n        to_remove: List[Label] = []\n        for col in keys:\n            if isinstance(col, MultiIndex):\n                for n in range(col.nlevels):\n                    arrays.append(col._get_level_values(n))\n                names.extend(col.names)\n            elif isinstance(col, (Index, Series)):\n                # if Index then not MultiIndex (treated above)\n                arrays.append(col)\n                names.append(col.name)\n            elif isinstance(col, (list, np.ndarray)):\n                arrays.append(col)\n                names.append(None)\n            elif isinstance(col, abc.Iterator):\n                arrays.append(list(col))\n                names.append(None)\n            # from here, col can only be a column label\n            else:\n                arrays.append(frame[col]._values)\n                names.append(col)\n                if drop:\n                    to_remove.append(col)\n\n            if len(arrays[-1]) != len(self):\n                # check newest element against length of calling frame, since\n                # ensure_index_from_sequences would not raise for append=False.\n                raise ValueError(\n                    f\"Length mismatch: Expected {len(self)} rows, \"\n                    f\"received array of length {len(arrays[-1])}\"\n                )\n\n        index = ensure_index_from_sequences(arrays, names)\n\n        if verify_integrity and not index.is_unique:\n            duplicates = index[index.duplicated()].unique()\n            raise ValueError(f\"Index has duplicate keys: {duplicates}\")\n\n        # use set to handle duplicate column names gracefully in case of drop\n        for c in set(to_remove):\n            del frame[c]\n\n        # clear up memory usage\n        index._cleanup()\n\n        frame.index = index\n\n        if not inplace:\n            return frame\n\n    def reset_index(\n        self,\n        level: Optional[Union[Hashable, Sequence[Hashable]]] = None,\n        drop: bool = False,\n        inplace: bool = False,\n        col_level: Hashable = 0,\n        col_fill: Label = \"\",\n    ) -> Optional[DataFrame]:\n        \"\"\"\n        Reset the index, or a level of it.\n\n        Reset the index of the DataFrame, and use the default one instead.\n        If the DataFrame has a MultiIndex, this method can remove one or more\n        levels.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default None\n            Only remove the given levels from the index. Removes all levels by\n            default.\n        drop : bool, default False\n            Do not try to insert index into dataframe columns. This resets\n            the index to the default integer index.\n        inplace : bool, default False\n            Modify the DataFrame in place (do not create a new object).\n        col_level : int or str, default 0\n            If the columns have multiple levels, determines which level the\n            labels are inserted into. By default it is inserted into the first\n            level.\n        col_fill : object, default ''\n            If the columns have multiple levels, determines how the other\n            levels are named. If None then the index name is repeated.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with the new index or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.set_index : Opposite of reset_index.\n        DataFrame.reindex : Change to new indices or expand indices.\n        DataFrame.reindex_like : Change to same indices as other DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('bird', 389.0),\n        ...                    ('bird', 24.0),\n        ...                    ('mammal', 80.5),\n        ...                    ('mammal', np.nan)],\n        ...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n        ...                   columns=('class', 'max_speed'))\n        >>> df\n                 class  max_speed\n        falcon    bird      389.0\n        parrot    bird       24.0\n        lion    mammal       80.5\n        monkey  mammal        NaN\n\n        When we reset the index, the old index is added as a column, and a\n        new sequential index is used:\n\n        >>> df.reset_index()\n            index   class  max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        We can use the `drop` parameter to avoid the old index being added as\n        a column:\n\n        >>> df.reset_index(drop=True)\n            class  max_speed\n        0    bird      389.0\n        1    bird       24.0\n        2  mammal       80.5\n        3  mammal        NaN\n\n        You can also use `reset_index` with `MultiIndex`.\n\n        >>> index = pd.MultiIndex.from_tuples([('bird', 'falcon'),\n        ...                                    ('bird', 'parrot'),\n        ...                                    ('mammal', 'lion'),\n        ...                                    ('mammal', 'monkey')],\n        ...                                   names=['class', 'name'])\n        >>> columns = pd.MultiIndex.from_tuples([('speed', 'max'),\n        ...                                      ('species', 'type')])\n        >>> df = pd.DataFrame([(389.0, 'fly'),\n        ...                    ( 24.0, 'fly'),\n        ...                    ( 80.5, 'run'),\n        ...                    (np.nan, 'jump')],\n        ...                   index=index,\n        ...                   columns=columns)\n        >>> df\n                       speed species\n                         max    type\n        class  name\n        bird   falcon  389.0     fly\n               parrot   24.0     fly\n        mammal lion     80.5     run\n               monkey    NaN    jump\n\n        If the index has multiple levels, we can reset a subset of them:\n\n        >>> df.reset_index(level='class')\n                 class  speed species\n                          max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        If we are not dropping the index, by default, it is placed in the top\n        level. We can place it in another level:\n\n        >>> df.reset_index(level='class', col_level=1)\n                        speed species\n                 class    max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        When the index is inserted under another level, we can specify under\n        which one with the parameter `col_fill`:\n\n        >>> df.reset_index(level='class', col_level=1, col_fill='species')\n                      species  speed species\n                        class    max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n\n        If we specify a nonexistent level for `col_fill`, it is created:\n\n        >>> df.reset_index(level='class', col_level=1, col_fill='genus')\n                        genus  speed species\n                        class    max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        self._check_inplace_and_allows_duplicate_labels(inplace)\n        if inplace:\n            new_obj = self\n        else:\n            new_obj = self.copy()\n\n        new_index = ibase.default_index(len(new_obj))\n        if level is not None:\n            if not isinstance(level, (tuple, list)):\n                level = [level]\n            level = [self.index._get_level_number(lev) for lev in level]\n            if len(level) < self.index.nlevels:\n                new_index = self.index.droplevel(level)\n\n        if not drop:\n            to_insert: Iterable[Tuple[Any, Optional[Any]]]\n            if isinstance(self.index, MultiIndex):\n                names = [\n                    (n if n is not None else f\"level_{i}\")\n                    for i, n in enumerate(self.index.names)\n                ]\n                to_insert = zip(self.index.levels, self.index.codes)\n            else:\n                default = \"index\" if \"index\" not in self else \"level_0\"\n                names = [default] if self.index.name is None else [self.index.name]\n                to_insert = ((self.index, None),)\n\n            multi_col = isinstance(self.columns, MultiIndex)\n            for i, (lev, lab) in reversed(list(enumerate(to_insert))):\n                if not (level is None or i in level):\n                    continue\n                name = names[i]\n                if multi_col:\n                    col_name = list(name) if isinstance(name, tuple) else [name]\n                    if col_fill is None:\n                        if len(col_name) not in (1, self.columns.nlevels):\n                            raise ValueError(\n                                \"col_fill=None is incompatible \"\n                                f\"with incomplete column name {name}\"\n                            )\n                        col_fill = col_name[0]\n\n                    lev_num = self.columns._get_level_number(col_level)\n                    name_lst = [col_fill] * lev_num + col_name\n                    missing = self.columns.nlevels - len(name_lst)\n                    name_lst += [col_fill] * missing\n                    name = tuple(name_lst)\n                # to ndarray and maybe infer different dtype\n                level_values = maybe_casted_values(lev, lab)\n                new_obj.insert(0, name, level_values)\n\n        new_obj.index = new_index\n        if not inplace:\n            return new_obj\n\n        return None\n\n    # ----------------------------------------------------------------------\n    # Reindex-based selection methods\n\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs[\"klass\"])\n    def isna(self) -> DataFrame:\n        result = self._constructor(self._mgr.isna(func=isna))\n        return result.__finalize__(self, method=\"isna\")\n\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs[\"klass\"])\n    def isnull(self) -> DataFrame:\n        return self.isna()\n\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs[\"klass\"])\n    def notna(self) -> DataFrame:\n        return ~self.isna()\n\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs[\"klass\"])\n    def notnull(self) -> DataFrame:\n        return ~self.isna()\n\n    def dropna(self, axis=0, how=\"any\", thresh=None, subset=None, inplace=False):\n        \"\"\"\n        Remove missing values.\n\n        See the :ref:`User Guide <missing_data>` for more on which values are\n        considered missing, and how to work with missing data.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Determine if rows or columns which contain missing values are\n            removed.\n\n            * 0, or 'index' : Drop rows which contain missing values.\n            * 1, or 'columns' : Drop columns which contain missing value.\n\n            .. versionchanged:: 1.0.0\n\n               Pass tuple or list to drop on multiple axes.\n               Only a single axis is allowed.\n\n        how : {'any', 'all'}, default 'any'\n            Determine if row or column is removed from DataFrame, when we have\n            at least one NA or all NA.\n\n            * 'any' : If any NA values are present, drop that row or column.\n            * 'all' : If all values are NA, drop that row or column.\n\n        thresh : int, optional\n            Require that many non-NA values.\n        subset : array-like, optional\n            Labels along other axis to consider, e.g. if you are dropping rows\n            these would be a list of columns to include.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with NA entries dropped from it or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.isna: Indicate missing values.\n        DataFrame.notna : Indicate existing (non-missing) values.\n        DataFrame.fillna : Replace missing values.\n        Series.dropna : Drop missing values.\n        Index.dropna : Drop missing indices.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'],\n        ...                    \"toy\": [np.nan, 'Batmobile', 'Bullwhip'],\n        ...                    \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"),\n        ...                             pd.NaT]})\n        >>> df\n               name        toy       born\n        0    Alfred        NaN        NaT\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Drop the rows where at least one element is missing.\n\n        >>> df.dropna()\n             name        toy       born\n        1  Batman  Batmobile 1940-04-25\n\n        Drop the columns where at least one element is missing.\n\n        >>> df.dropna(axis='columns')\n               name\n        0    Alfred\n        1    Batman\n        2  Catwoman\n\n        Drop the rows where all elements are missing.\n\n        >>> df.dropna(how='all')\n               name        toy       born\n        0    Alfred        NaN        NaT\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Keep only the rows with at least 2 non-NA values.\n\n        >>> df.dropna(thresh=2)\n               name        toy       born\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Define in which columns to look for missing values.\n\n        >>> df.dropna(subset=['name', 'toy'])\n               name        toy       born\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Keep the DataFrame with valid entries in the same variable.\n\n        >>> df.dropna(inplace=True)\n        >>> df\n             name        toy       born\n        1  Batman  Batmobile 1940-04-25\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if isinstance(axis, (tuple, list)):\n            # GH20987\n            raise TypeError(\"supplying multiple axes to axis is no longer supported.\")\n\n        axis = self._get_axis_number(axis)\n        agg_axis = 1 - axis\n\n        agg_obj = self\n        if subset is not None:\n            ax = self._get_axis(agg_axis)\n            indices = ax.get_indexer_for(subset)\n            check = indices == -1\n            if check.any():\n                raise KeyError(list(np.compress(check, subset)))\n            agg_obj = self.take(indices, axis=agg_axis)\n\n        count = agg_obj.count(axis=agg_axis)\n\n        if thresh is not None:\n            mask = count >= thresh\n        elif how == \"any\":\n            mask = count == len(agg_obj._get_axis(agg_axis))\n        elif how == \"all\":\n            mask = count > 0\n        else:\n            if how is not None:\n                raise ValueError(f\"invalid how option: {how}\")\n            else:\n                raise TypeError(\"must specify how or thresh\")\n\n        result = self.loc(axis=axis)[mask]\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result\n\n    def drop_duplicates(\n        self,\n        subset: Optional[Union[Hashable, Sequence[Hashable]]] = None,\n        keep: Union[str, bool] = \"first\",\n        inplace: bool = False,\n        ignore_index: bool = False,\n    ) -> Optional[DataFrame]:\n        \"\"\"\n        Return DataFrame with duplicate rows removed.\n\n        Considering certain columns is optional. Indexes, including time indexes\n        are ignored.\n\n        Parameters\n        ----------\n        subset : column label or sequence of labels, optional\n            Only consider certain columns for identifying duplicates, by\n            default use all of the columns.\n        keep : {'first', 'last', False}, default 'first'\n            Determines which duplicates (if any) to keep.\n            - ``first`` : Drop duplicates except for the first occurrence.\n            - ``last`` : Drop duplicates except for the last occurrence.\n            - False : Drop all duplicates.\n        inplace : bool, default False\n            Whether to drop duplicates in place or to return a copy.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, …, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with duplicates removed or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.value_counts: Count unique combinations of columns.\n\n        Examples\n        --------\n        Consider dataset containing ramen rating.\n\n        >>> df = pd.DataFrame({\n        ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n        ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n        ...     'rating': [4, 4, 3.5, 15, 5]\n        ... })\n        >>> df\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        1  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        3  Indomie  pack    15.0\n        4  Indomie  pack     5.0\n\n        By default, it removes duplicate rows based on all columns.\n\n        >>> df.drop_duplicates()\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        3  Indomie  pack    15.0\n        4  Indomie  pack     5.0\n\n        To remove duplicates on specific column(s), use ``subset``.\n\n        >>> df.drop_duplicates(subset=['brand'])\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n\n        To remove duplicates and keep last occurrences, use ``keep``.\n\n        >>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n            brand style  rating\n        1  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        4  Indomie  pack     5.0\n        \"\"\"\n        if self.empty:\n            return self.copy()\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        duplicated = self.duplicated(subset, keep=keep)\n\n        result = self[-duplicated]\n        if ignore_index:\n            result.index = ibase.default_index(len(result))\n\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    def duplicated(\n        self,\n        subset: Optional[Union[Hashable, Sequence[Hashable]]] = None,\n        keep: Union[str, bool] = \"first\",\n    ) -> Series:\n        \"\"\"\n        Return boolean Series denoting duplicate rows.\n\n        Considering certain columns is optional.\n\n        Parameters\n        ----------\n        subset : column label or sequence of labels, optional\n            Only consider certain columns for identifying duplicates, by\n            default use all of the columns.\n        keep : {'first', 'last', False}, default 'first'\n            Determines which duplicates (if any) to mark.\n\n            - ``first`` : Mark duplicates as ``True`` except for the first occurrence.\n            - ``last`` : Mark duplicates as ``True`` except for the last occurrence.\n            - False : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series\n            Boolean series for each duplicated rows.\n\n        See Also\n        --------\n        Index.duplicated : Equivalent method on index.\n        Series.duplicated : Equivalent method on Series.\n        Series.drop_duplicates : Remove duplicate values from Series.\n        DataFrame.drop_duplicates : Remove duplicate values from DataFrame.\n\n        Examples\n        --------\n        Consider dataset containing ramen rating.\n\n        >>> df = pd.DataFrame({\n        ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n        ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n        ...     'rating': [4, 4, 3.5, 15, 5]\n        ... })\n        >>> df\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        1  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        3  Indomie  pack    15.0\n        4  Indomie  pack     5.0\n\n        By default, for each set of duplicated values, the first occurrence\n        is set on False and all others on True.\n\n        >>> df.duplicated()\n        0    False\n        1     True\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True.\n\n        >>> df.duplicated(keep='last')\n        0     True\n        1    False\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        By setting ``keep`` on False, all duplicates are True.\n\n        >>> df.duplicated(keep=False)\n        0     True\n        1     True\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        To find duplicates on specific column(s), use ``subset``.\n\n        >>> df.duplicated(subset=['brand'])\n        0    False\n        1     True\n        2    False\n        3     True\n        4     True\n        dtype: bool\n        \"\"\"\n        from pandas._libs.hashtable import SIZE_HINT_LIMIT, duplicated_int64\n\n        if self.empty:\n            return self._constructor_sliced(dtype=bool)\n\n        def f(vals):\n            labels, shape = algorithms.factorize(\n                vals, size_hint=min(len(self), SIZE_HINT_LIMIT)\n            )\n            return labels.astype(\"i8\", copy=False), len(shape)\n\n        if subset is None:\n            subset = self.columns\n        elif (\n            not np.iterable(subset)\n            or isinstance(subset, str)\n            or isinstance(subset, tuple)\n            and subset in self.columns\n        ):\n            subset = (subset,)\n\n        #  needed for mypy since can't narrow types using np.iterable\n        subset = cast(Iterable, subset)\n\n        # Verify all columns in subset exist in the queried dataframe\n        # Otherwise, raise a KeyError, same as if you try to __getitem__ with a\n        # key that doesn't exist.\n        diff = Index(subset).difference(self.columns)\n        if not diff.empty:\n            raise KeyError(diff)\n\n        vals = (col.values for name, col in self.items() if name in subset)\n        labels, shape = map(list, zip(*map(f, vals)))\n\n        ids = get_group_index(labels, shape, sort=False, xnull=False)\n        result = self._constructor_sliced(duplicated_int64(ids, keep), index=self.index)\n        return result.__finalize__(self, method=\"duplicated\")\n\n    # ----------------------------------------------------------------------\n    # Sorting\n    # TODO: Just move the sort_values doc here.\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(NDFrame.sort_values.__doc__)\n    # error: Signature of \"sort_values\" incompatible with supertype \"NDFrame\"\n    def sort_values(  # type: ignore[override]\n        self,\n        by,\n        axis=0,\n        ascending=True,\n        inplace=False,\n        kind=\"quicksort\",\n        na_position=\"last\",\n        ignore_index=False,\n        key: ValueKeyFunc = None,\n    ):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        axis = self._get_axis_number(axis)\n\n        if not isinstance(by, list):\n            by = [by]\n        if is_sequence(ascending) and len(by) != len(ascending):\n            raise ValueError(\n                f\"Length of ascending ({len(ascending)}) != length of by ({len(by)})\"\n            )\n        if len(by) > 1:\n\n            keys = [self._get_label_or_level_values(x, axis=axis) for x in by]\n\n            # need to rewrap columns in Series to apply key function\n            if key is not None:\n                keys = [Series(k, name=name) for (k, name) in zip(keys, by)]\n\n            indexer = lexsort_indexer(\n                keys, orders=ascending, na_position=na_position, key=key\n            )\n            indexer = ensure_platform_int(indexer)\n        else:\n\n            by = by[0]\n            k = self._get_label_or_level_values(by, axis=axis)\n\n            # need to rewrap column in Series to apply key function\n            if key is not None:\n                k = Series(k, name=by)\n\n            if isinstance(ascending, (tuple, list)):\n                ascending = ascending[0]\n\n            indexer = nargsort(\n                k, kind=kind, ascending=ascending, na_position=na_position, key=key\n            )\n\n        new_data = self._mgr.take(\n            indexer, axis=self._get_block_manager_axis(axis), verify=False\n        )\n\n        if ignore_index:\n            new_data.axes[1] = ibase.default_index(len(indexer))\n\n        result = self._constructor(new_data)\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_values\")\n\n    def sort_index(\n        self,\n        axis=0,\n        level=None,\n        ascending: bool = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        sort_remaining: bool = True,\n        ignore_index: bool = False,\n        key: IndexKeyFunc = None,\n    ):\n        \"\"\"\n        Sort object by labels (along an axis).\n\n        Returns a new DataFrame sorted by label if `inplace` argument is\n        ``False``, otherwise updates the original DataFrame and returns None.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis along which to sort.  The value 0 identifies the rows,\n            and 1 identifies the columns.\n        level : int or level name or list of ints or list of level names\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level individually.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See also ndarray.np.sort for more\n            information.  `mergesort` is the only stable algorithm. For\n            DataFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'last'}, default 'last'\n            Puts NaNs at the beginning if `first`; `last` puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, …, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape. For MultiIndex\n            inputs, the key is applied *per level*.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        DataFrame or None\n            The original DataFrame sorted by the labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.sort_index : Sort Series by the index.\n        DataFrame.sort_values : Sort DataFrame by the value.\n        Series.sort_values : Sort Series by the value.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150],\n        ...                   columns=['A'])\n        >>> df.sort_index()\n             A\n        1    4\n        29   2\n        100  1\n        150  5\n        234  3\n\n        By default, it sorts in ascending order, to sort in descending order,\n        use ``ascending=False``\n\n        >>> df.sort_index(ascending=False)\n             A\n        234  3\n        150  5\n        100  1\n        29   2\n        1    4\n\n        A key function can be specified which is applied to the index before\n        sorting. For a ``MultiIndex`` this is applied to each level separately.\n\n        >>> df = pd.DataFrame({\"a\": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd'])\n        >>> df.sort_index(key=lambda x: x.str.lower())\n           a\n        A  1\n        b  2\n        C  3\n        d  4\n        \"\"\"\n        return super().sort_index(\n            axis,\n            level,\n            ascending,\n            inplace,\n            kind,\n            na_position,\n            sort_remaining,\n            ignore_index,\n            key,\n        )\n\n    def value_counts(\n        self,\n        subset: Optional[Sequence[Label]] = None,\n        normalize: bool = False,\n        sort: bool = True,\n        ascending: bool = False,\n    ):\n        \"\"\"\n        Return a Series containing counts of unique rows in the DataFrame.\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        subset : list-like, optional\n            Columns to use when counting unique combinations.\n        normalize : bool, default False\n            Return proportions rather than frequencies.\n        sort : bool, default True\n            Sort by frequencies.\n        ascending : bool, default False\n            Sort in ascending order.\n\n        Returns\n        -------\n        Series\n\n        See Also\n        --------\n        Series.value_counts: Equivalent method on Series.\n\n        Notes\n        -----\n        The returned Series will have a MultiIndex with one level per input\n        column. By default, rows that contain any NA values are omitted from\n        the result. By default, the resulting Series will be in descending\n        order so that the first element is the most frequently-occurring row.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [2, 4, 4, 6],\n        ...                    'num_wings': [2, 0, 0, 0]},\n        ...                   index=['falcon', 'dog', 'cat', 'ant'])\n        >>> df\n                num_legs  num_wings\n        falcon         2          2\n        dog            4          0\n        cat            4          0\n        ant            6          0\n\n        >>> df.value_counts()\n        num_legs  num_wings\n        4         0            2\n        6         0            1\n        2         2            1\n        dtype: int64\n\n        >>> df.value_counts(sort=False)\n        num_legs  num_wings\n        2         2            1\n        4         0            2\n        6         0            1\n        dtype: int64\n\n        >>> df.value_counts(ascending=True)\n        num_legs  num_wings\n        2         2            1\n        6         0            1\n        4         0            2\n        dtype: int64\n\n        >>> df.value_counts(normalize=True)\n        num_legs  num_wings\n        4         0            0.50\n        6         0            0.25\n        2         2            0.25\n        dtype: float64\n        \"\"\"\n        if subset is None:\n            subset = self.columns.tolist()\n\n        counts = self.groupby(subset).grouper.size()\n\n        if sort:\n            counts = counts.sort_values(ascending=ascending)\n        if normalize:\n            counts /= counts.sum()\n\n        # Force MultiIndex for single column\n        if len(subset) == 1:\n            counts.index = MultiIndex.from_arrays(\n                [counts.index], names=[counts.index.name]\n            )\n\n        return counts\n\n    def nlargest(self, n, columns, keep=\"first\") -> DataFrame:\n        \"\"\"\n        Return the first `n` rows ordered by `columns` in descending order.\n\n        Return the first `n` rows with the largest values in `columns`, in\n        descending order. The columns that are not specified are returned as\n        well, but not used for ordering.\n\n        This method is equivalent to\n        ``df.sort_values(columns, ascending=False).head(n)``, but more\n        performant.\n\n        Parameters\n        ----------\n        n : int\n            Number of rows to return.\n        columns : label or list of labels\n            Column label(s) to order by.\n        keep : {'first', 'last', 'all'}, default 'first'\n            Where there are duplicate values:\n\n            - `first` : prioritize the first occurrence(s)\n            - `last` : prioritize the last occurrence(s)\n            - ``all`` : do not drop any duplicates, even it means\n                        selecting more than `n` items.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        DataFrame\n            The first `n` rows ordered by the given columns in descending\n            order.\n\n        See Also\n        --------\n        DataFrame.nsmallest : Return the first `n` rows ordered by `columns` in\n            ascending order.\n        DataFrame.sort_values : Sort DataFrame by the values.\n        DataFrame.head : Return the first `n` rows without re-ordering.\n\n        Notes\n        -----\n        This function cannot be used with all column types. For example, when\n        specifying columns with `object` or `category` dtypes, ``TypeError`` is\n        raised.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n        ...                                   434000, 434000, 337000, 11300,\n        ...                                   11300, 11300],\n        ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n        ...                            17036, 182, 38, 311],\n        ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n        ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n        ...                   index=[\"Italy\", \"France\", \"Malta\",\n        ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n        ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n        >>> df\n                  population      GDP alpha-2\n        Italy       59000000  1937894      IT\n        France      65000000  2583560      FR\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n        Iceland       337000    17036      IS\n        Nauru          11300      182      NR\n        Tuvalu         11300       38      TV\n        Anguilla       11300      311      AI\n\n        In the following example, we will use ``nlargest`` to select the three\n        rows having the largest values in column \"population\".\n\n        >>> df.nlargest(3, 'population')\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Malta       434000    12011      MT\n\n        When using ``keep='last'``, ties are resolved in reverse order:\n\n        >>> df.nlargest(3, 'population', keep='last')\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Brunei      434000    12128      BN\n\n        When using ``keep='all'``, all duplicate items are maintained:\n\n        >>> df.nlargest(3, 'population', keep='all')\n                  population      GDP alpha-2\n        France      65000000  2583560      FR\n        Italy       59000000  1937894      IT\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n\n        To order by the largest values in column \"population\" and then \"GDP\",\n        we can specify multiple columns like in the next example.\n\n        >>> df.nlargest(3, ['population', 'GDP'])\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Brunei      434000    12128      BN\n        \"\"\"\n        return algorithms.SelectNFrame(self, n=n, keep=keep, columns=columns).nlargest()\n\n    def nsmallest(self, n, columns, keep=\"first\") -> DataFrame:\n        \"\"\"\n        Return the first `n` rows ordered by `columns` in ascending order.\n\n        Return the first `n` rows with the smallest values in `columns`, in\n        ascending order. The columns that are not specified are returned as\n        well, but not used for ordering.\n\n        This method is equivalent to\n        ``df.sort_values(columns, ascending=True).head(n)``, but more\n        performant.\n\n        Parameters\n        ----------\n        n : int\n            Number of items to retrieve.\n        columns : list or str\n            Column name or names to order by.\n        keep : {'first', 'last', 'all'}, default 'first'\n            Where there are duplicate values:\n\n            - ``first`` : take the first occurrence.\n            - ``last`` : take the last occurrence.\n            - ``all`` : do not drop any duplicates, even it means\n              selecting more than `n` items.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.nlargest : Return the first `n` rows ordered by `columns` in\n            descending order.\n        DataFrame.sort_values : Sort DataFrame by the values.\n        DataFrame.head : Return the first `n` rows without re-ordering.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n        ...                                   434000, 434000, 337000, 337000,\n        ...                                   11300, 11300],\n        ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n        ...                            17036, 182, 38, 311],\n        ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n        ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n        ...                   index=[\"Italy\", \"France\", \"Malta\",\n        ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n        ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n        >>> df\n                  population      GDP alpha-2\n        Italy       59000000  1937894      IT\n        France      65000000  2583560      FR\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n        Iceland       337000    17036      IS\n        Nauru         337000      182      NR\n        Tuvalu         11300       38      TV\n        Anguilla       11300      311      AI\n\n        In the following example, we will use ``nsmallest`` to select the\n        three rows having the smallest values in column \"population\".\n\n        >>> df.nsmallest(3, 'population')\n                  population    GDP alpha-2\n        Tuvalu         11300     38      TV\n        Anguilla       11300    311      AI\n        Iceland       337000  17036      IS\n\n        When using ``keep='last'``, ties are resolved in reverse order:\n\n        >>> df.nsmallest(3, 'population', keep='last')\n                  population  GDP alpha-2\n        Anguilla       11300  311      AI\n        Tuvalu         11300   38      TV\n        Nauru         337000  182      NR\n\n        When using ``keep='all'``, all duplicate items are maintained:\n\n        >>> df.nsmallest(3, 'population', keep='all')\n                  population    GDP alpha-2\n        Tuvalu         11300     38      TV\n        Anguilla       11300    311      AI\n        Iceland       337000  17036      IS\n        Nauru         337000    182      NR\n\n        To order by the smallest values in column \"population\" and then \"GDP\", we can\n        specify multiple columns like in the next example.\n\n        >>> df.nsmallest(3, ['population', 'GDP'])\n                  population  GDP alpha-2\n        Tuvalu         11300   38      TV\n        Anguilla       11300  311      AI\n        Nauru         337000  182      NR\n        \"\"\"\n        return algorithms.SelectNFrame(\n            self, n=n, keep=keep, columns=columns\n        ).nsmallest()\n\n    def swaplevel(self, i=-2, j=-1, axis=0) -> DataFrame:\n        \"\"\"\n        Swap levels i and j in a MultiIndex on a particular axis.\n\n        Parameters\n        ----------\n        i, j : int or str\n            Levels of the indices to be swapped. Can pass level name as string.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to swap levels on. 0 or 'index' for row-wise, 1 or\n            'columns' for column-wise.\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        result = self.copy()\n\n        axis = self._get_axis_number(axis)\n\n        if not isinstance(result._get_axis(axis), MultiIndex):  # pragma: no cover\n            raise TypeError(\"Can only swap levels on a hierarchical axis.\")\n\n        if axis == 0:\n            assert isinstance(result.index, MultiIndex)\n            result.index = result.index.swaplevel(i, j)\n        else:\n            assert isinstance(result.columns, MultiIndex)\n            result.columns = result.columns.swaplevel(i, j)\n        return result\n\n    def reorder_levels(self, order, axis=0) -> DataFrame:\n        \"\"\"\n        Rearrange index levels using input order. May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int or list of str\n            List representing new level order. Reference level by number\n            (position) or by key (label).\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Where to reorder levels.\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        if not isinstance(self._get_axis(axis), MultiIndex):  # pragma: no cover\n            raise TypeError(\"Can only reorder levels on a hierarchical axis.\")\n\n        result = self.copy()\n\n        if axis == 0:\n            assert isinstance(result.index, MultiIndex)\n            result.index = result.index.reorder_levels(order)\n        else:\n            assert isinstance(result.columns, MultiIndex)\n            result.columns = result.columns.reorder_levels(order)\n        return result\n\n    # ----------------------------------------------------------------------\n    # Arithmetic Methods\n\n    def _cmp_method(self, other, op):\n        axis = 1  # only relevant for Series other case\n\n        self, other = ops.align_method_FRAME(self, other, axis, flex=False, level=None)\n\n        # See GH#4537 for discussion of scalar op behavior\n        new_data = self._dispatch_frame_op(other, op, axis=axis)\n        return self._construct_result(new_data)\n\n    def _arith_method(self, other, op):\n        if ops.should_reindex_frame_op(self, other, op, 1, 1, None, None):\n            return ops.frame_arith_method_with_reindex(self, other, op)\n\n        axis = 1  # only relevant for Series other case\n\n        self, other = ops.align_method_FRAME(self, other, axis, flex=True, level=None)\n\n        new_data = self._dispatch_frame_op(other, op, axis=axis)\n        return self._construct_result(new_data)\n\n    _logical_method = _arith_method\n\n    def _dispatch_frame_op(self, right, func, axis: Optional[int] = None):\n        \"\"\"\n        Evaluate the frame operation func(left, right) by evaluating\n        column-by-column, dispatching to the Series implementation.\n\n        Parameters\n        ----------\n        right : scalar, Series, or DataFrame\n        func : arithmetic or comparison operator\n        axis : {None, 0, 1}\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        # Get the appropriate array-op to apply to each column/block's values.\n        array_op = ops.get_array_op(func)\n\n        right = lib.item_from_zerodim(right)\n        if not is_list_like(right):\n            # i.e. scalar, faster than checking np.ndim(right) == 0\n            bm = self._mgr.apply(array_op, right=right)\n            return type(self)(bm)\n\n        elif isinstance(right, DataFrame):\n            assert self.index.equals(right.index)\n            assert self.columns.equals(right.columns)\n            # TODO: The previous assertion `assert right._indexed_same(self)`\n            #  fails in cases with empty columns reached via\n            #  _frame_arith_method_with_reindex\n\n            bm = self._mgr.operate_blockwise(right._mgr, array_op)\n            return type(self)(bm)\n\n        elif isinstance(right, Series) and axis == 1:\n            # axis=1 means we want to operate row-by-row\n            assert right.index.equals(self.columns)\n\n            right = right._values\n            # maybe_align_as_frame ensures we do not have an ndarray here\n            assert not isinstance(right, np.ndarray)\n\n            arrays = [array_op(l, r) for l, r in zip(self._iter_column_arrays(), right)]\n\n        elif isinstance(right, Series):\n            assert right.index.equals(self.index)  # Handle other cases later\n            right = right._values\n\n            arrays = [array_op(l, right) for l in self._iter_column_arrays()]\n\n        else:\n            # Remaining cases have less-obvious dispatch rules\n            raise NotImplementedError(right)\n\n        return type(self)._from_arrays(\n            arrays, self.columns, self.index, verify_integrity=False\n        )\n\n    def _combine_frame(self, other: DataFrame, func, fill_value=None):\n        # at this point we have `self._indexed_same(other)`\n\n        if fill_value is None:\n            # since _arith_op may be called in a loop, avoid function call\n            #  overhead if possible by doing this check once\n            _arith_op = func\n\n        else:\n\n            def _arith_op(left, right):\n                # for the mixed_type case where we iterate over columns,\n                # _arith_op(left, right) is equivalent to\n                # left._binop(right, func, fill_value=fill_value)\n                left, right = ops.fill_binop(left, right, fill_value)\n                return func(left, right)\n\n        new_data = self._dispatch_frame_op(other, _arith_op)\n        return new_data\n\n    def _construct_result(self, result) -> DataFrame:\n        \"\"\"\n        Wrap the result of an arithmetic, comparison, or logical operation.\n\n        Parameters\n        ----------\n        result : DataFrame\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        out = self._constructor(result, copy=False)\n        # Pin columns instead of passing to constructor for compat with\n        #  non-unique columns case\n        out.columns = self.columns\n        out.index = self.index\n        return out\n\n    def __divmod__(self, other) -> Tuple[DataFrame, DataFrame]:\n        # Naive implementation, room for optimization\n        div = self // other\n        mod = self - div * other\n        return div, mod\n\n    def __rdivmod__(self, other) -> Tuple[DataFrame, DataFrame]:\n        # Naive implementation, room for optimization\n        div = other // self\n        mod = other - div * self\n        return div, mod\n\n    # ----------------------------------------------------------------------\n    # Combination-Related\n\n    @Appender(\n        \"\"\"\nReturns\n-------\nDataFrame\n    DataFrame that shows the differences stacked side by side.\n\n    The resulting index will be a MultiIndex with 'self' and 'other'\n    stacked alternately at the inner level.\n\nRaises\n------\nValueError\n    When the two DataFrames don't have identical labels or shape.\n\nSee Also\n--------\nSeries.compare : Compare with another Series and show differences.\nDataFrame.equals : Test whether two objects contain the same elements.\n\nNotes\n-----\nMatching NaNs will not appear as a difference.\n\nCan only compare identically-labeled\n(i.e. same shape, identical row and column labels) DataFrames\n\nExamples\n--------\n>>> df = pd.DataFrame(\n...     {\n...         \"col1\": [\"a\", \"a\", \"b\", \"b\", \"a\"],\n...         \"col2\": [1.0, 2.0, 3.0, np.nan, 5.0],\n...         \"col3\": [1.0, 2.0, 3.0, 4.0, 5.0]\n...     },\n...     columns=[\"col1\", \"col2\", \"col3\"],\n... )\n>>> df\n  col1  col2  col3\n0    a   1.0   1.0\n1    a   2.0   2.0\n2    b   3.0   3.0\n3    b   NaN   4.0\n4    a   5.0   5.0\n\n>>> df2 = df.copy()\n>>> df2.loc[0, 'col1'] = 'c'\n>>> df2.loc[2, 'col3'] = 4.0\n>>> df2\n  col1  col2  col3\n0    c   1.0   1.0\n1    a   2.0   2.0\n2    b   3.0   4.0\n3    b   NaN   4.0\n4    a   5.0   5.0\n\nAlign the differences on columns\n\n>>> df.compare(df2)\n  col1       col3\n  self other self other\n0    a     c  NaN   NaN\n2  NaN   NaN  3.0   4.0\n\nStack the differences on rows\n\n>>> df.compare(df2, align_axis=0)\n        col1  col3\n0 self     a   NaN\n  other    c   NaN\n2 self   NaN   3.0\n  other  NaN   4.0\n\nKeep the equal values\n\n>>> df.compare(df2, keep_equal=True)\n  col1       col3\n  self other self other\n0    a     c  1.0   1.0\n2    b     b  3.0   4.0\n\nKeep all original rows and columns\n\n>>> df.compare(df2, keep_shape=True)\n  col1       col2       col3\n  self other self other self other\n0    a     c  NaN   NaN  NaN   NaN\n1  NaN   NaN  NaN   NaN  NaN   NaN\n2  NaN   NaN  NaN   NaN  3.0   4.0\n3  NaN   NaN  NaN   NaN  NaN   NaN\n4  NaN   NaN  NaN   NaN  NaN   NaN\n\nKeep all original rows and columns and also all original values\n\n>>> df.compare(df2, keep_shape=True, keep_equal=True)\n  col1       col2       col3\n  self other self other self other\n0    a     c  1.0   1.0  1.0   1.0\n1    a     a  2.0   2.0  2.0   2.0\n2    b     b  3.0   3.0  3.0   4.0\n3    b     b  NaN   NaN  4.0   4.0\n4    a     a  5.0   5.0  5.0   5.0\n\"\"\"\n    )\n    @Appender(_shared_docs[\"compare\"] % _shared_doc_kwargs)\n    def compare(\n        self,\n        other: DataFrame,\n        align_axis: Axis = 1,\n        keep_shape: bool = False,\n        keep_equal: bool = False,\n    ) -> DataFrame:\n        return super().compare(\n            other=other,\n            align_axis=align_axis,\n            keep_shape=keep_shape,\n            keep_equal=keep_equal,\n        )\n\n    def combine(\n        self, other: DataFrame, func, fill_value=None, overwrite=True\n    ) -> DataFrame:\n        \"\"\"\n        Perform column-wise combine with another DataFrame.\n\n        Combines a DataFrame with `other` DataFrame using `func`\n        to element-wise combine columns. The row and column indexes of the\n        resulting DataFrame will be the union of the two.\n\n        Parameters\n        ----------\n        other : DataFrame\n            The DataFrame to merge column-wise.\n        func : function\n            Function that takes two series as inputs and return a Series or a\n            scalar. Used to merge the two dataframes column by columns.\n        fill_value : scalar value, default None\n            The value to fill NaNs with prior to passing any column to the\n            merge func.\n        overwrite : bool, default True\n            If True, columns in `self` that do not exist in `other` will be\n            overwritten with NaNs.\n\n        Returns\n        -------\n        DataFrame\n            Combination of the provided DataFrames.\n\n        See Also\n        --------\n        DataFrame.combine_first : Combine two DataFrame objects and default to\n            non-null values in frame calling the method.\n\n        Examples\n        --------\n        Combine using a simple function that chooses the smaller column.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2\n        >>> df1.combine(df2, take_smaller)\n           A  B\n        0  0  3\n        1  0  3\n\n        Example using a true element-wise combine function.\n\n        >>> df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine(df2, np.minimum)\n           A  B\n        0  1  2\n        1  0  3\n\n        Using `fill_value` fills Nones prior to passing the column to the\n        merge function.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine(df2, take_smaller, fill_value=-5)\n           A    B\n        0  0 -5.0\n        1  0  4.0\n\n        However, if the same element in both dataframes is None, that None\n        is preserved\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]})\n        >>> df1.combine(df2, take_smaller, fill_value=-5)\n            A    B\n        0  0 -5.0\n        1  0  3.0\n\n        Example that demonstrates the use of `overwrite` and behavior when\n        the axis differ between the dataframes.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2])\n        >>> df1.combine(df2, take_smaller)\n             A    B     C\n        0  NaN  NaN   NaN\n        1  NaN  3.0 -10.0\n        2  NaN  3.0   1.0\n\n        >>> df1.combine(df2, take_smaller, overwrite=False)\n             A    B     C\n        0  0.0  NaN   NaN\n        1  0.0  3.0 -10.0\n        2  NaN  3.0   1.0\n\n        Demonstrating the preference of the passed in dataframe.\n\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2])\n        >>> df2.combine(df1, take_smaller)\n           A    B   C\n        0  0.0  NaN NaN\n        1  0.0  3.0 NaN\n        2  NaN  3.0 NaN\n\n        >>> df2.combine(df1, take_smaller, overwrite=False)\n             A    B   C\n        0  0.0  NaN NaN\n        1  0.0  3.0 1.0\n        2  NaN  3.0 1.0\n        \"\"\"\n        other_idxlen = len(other.index)  # save for compare\n\n        this, other = self.align(other, copy=False)\n        new_index = this.index\n\n        if other.empty and len(new_index) == len(self.index):\n            return self.copy()\n\n        if self.empty and len(other) == other_idxlen:\n            return other.copy()\n\n        # sorts if possible\n        new_columns = this.columns.union(other.columns)\n        do_fill = fill_value is not None\n        result = {}\n        for col in new_columns:\n            series = this[col]\n            otherSeries = other[col]\n\n            this_dtype = series.dtype\n            other_dtype = otherSeries.dtype\n\n            this_mask = isna(series)\n            other_mask = isna(otherSeries)\n\n            # don't overwrite columns unnecessarily\n            # DO propagate if this column is not in the intersection\n            if not overwrite and other_mask.all():\n                result[col] = this[col].copy()\n                continue\n\n            if do_fill:\n                series = series.copy()\n                otherSeries = otherSeries.copy()\n                series[this_mask] = fill_value\n                otherSeries[other_mask] = fill_value\n\n            if col not in self.columns:\n                # If self DataFrame does not have col in other DataFrame,\n                # try to promote series, which is all NaN, as other_dtype.\n                new_dtype = other_dtype\n                try:\n                    series = series.astype(new_dtype, copy=False)\n                except ValueError:\n                    # e.g. new_dtype is integer types\n                    pass\n            else:\n                # if we have different dtypes, possibly promote\n                new_dtype = find_common_type([this_dtype, other_dtype])\n                if not is_dtype_equal(this_dtype, new_dtype):\n                    series = series.astype(new_dtype)\n                if not is_dtype_equal(other_dtype, new_dtype):\n                    otherSeries = otherSeries.astype(new_dtype)\n\n            arr = func(series, otherSeries)\n            arr = maybe_downcast_to_dtype(arr, this_dtype)\n\n            result[col] = arr\n\n        # convert_objects just in case\n        return self._constructor(result, index=new_index, columns=new_columns)\n\n    def combine_first(self, other: DataFrame) -> DataFrame:\n        \"\"\"\n        Update null elements with value in the same location in `other`.\n\n        Combine two DataFrame objects by filling null values in one DataFrame\n        with non-null values from other DataFrame. The row and column indexes\n        of the resulting DataFrame will be the union of the two.\n\n        Parameters\n        ----------\n        other : DataFrame\n            Provided DataFrame to use to fill null values.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.combine : Perform series-wise operation on two DataFrames\n            using a given function.\n\n        Examples\n        --------\n        >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine_first(df2)\n             A    B\n        0  1.0  3.0\n        1  0.0  4.0\n\n        Null values still persist if the location of that null value\n        does not exist in `other`\n\n        >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]})\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2])\n        >>> df1.combine_first(df2)\n             A    B    C\n        0  NaN  4.0  NaN\n        1  0.0  3.0  1.0\n        2  NaN  3.0  1.0\n        \"\"\"\n        import pandas.core.computation.expressions as expressions\n\n        def extract_values(arr):\n            # Does two things:\n            # 1. maybe gets the values from the Series / Index\n            # 2. convert datelike to i8\n            # TODO: extract_array?\n            if isinstance(arr, (Index, Series)):\n                arr = arr._values\n\n            if needs_i8_conversion(arr.dtype):\n                if is_extension_array_dtype(arr.dtype):\n                    arr = arr.asi8\n                else:\n                    arr = arr.view(\"i8\")\n            return arr\n\n        def combiner(x, y):\n            mask = isna(x)\n            # TODO: extract_array?\n            if isinstance(mask, (Index, Series)):\n                mask = mask._values\n\n            x_values = extract_values(x)\n            y_values = extract_values(y)\n\n            # If the column y in other DataFrame is not in first DataFrame,\n            # just return y_values.\n            if y.name not in self.columns:\n                return y_values\n\n            return expressions.where(mask, y_values, x_values)\n\n        return self.combine(other, combiner, overwrite=False)\n\n    def update(\n        self, other, join=\"left\", overwrite=True, filter_func=None, errors=\"ignore\"\n    ) -> None:\n        \"\"\"\n        Modify in place using non-NA values from another DataFrame.\n\n        Aligns on indices. There is no return value.\n\n        Parameters\n        ----------\n        other : DataFrame, or object coercible into a DataFrame\n            Should have at least one matching index/column label\n            with the original DataFrame. If a Series is passed,\n            its name attribute must be set, and that will be\n            used as the column name to align with the original DataFrame.\n        join : {'left'}, default 'left'\n            Only left join is implemented, keeping the index and columns of the\n            original object.\n        overwrite : bool, default True\n            How to handle non-NA values for overlapping keys:\n\n            * True: overwrite original DataFrame's values\n              with values from `other`.\n            * False: only update values that are NA in\n              the original DataFrame.\n\n        filter_func : callable(1d-array) -> bool 1d-array, optional\n            Can choose to replace values other than NA. Return True for values\n            that should be updated.\n        errors : {'raise', 'ignore'}, default 'ignore'\n            If 'raise', will raise a ValueError if the DataFrame and `other`\n            both contain non-NA data in the same place.\n\n            .. versionchanged:: 0.24.0\n               Changed from `raise_conflict=False|True`\n               to `errors='ignore'|'raise'`.\n\n        Returns\n        -------\n        None : method directly changes calling object\n\n        Raises\n        ------\n        ValueError\n            * When `errors='raise'` and there's overlapping non-NA data.\n            * When `errors` is not either `'ignore'` or `'raise'`\n        NotImplementedError\n            * If `join != 'left'`\n\n        See Also\n        --------\n        dict.update : Similar method for dictionaries.\n        DataFrame.merge : For column(s)-on-columns(s) operations.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2, 3],\n        ...                    'B': [400, 500, 600]})\n        >>> new_df = pd.DataFrame({'B': [4, 5, 6],\n        ...                        'C': [7, 8, 9]})\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  1  4\n        1  2  5\n        2  3  6\n\n        The DataFrame's length does not increase as a result of the update,\n        only values at matching index/column labels are updated.\n\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_df = pd.DataFrame({'B': ['d', 'e', 'f', 'g', 'h', 'i']})\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  a  d\n        1  b  e\n        2  c  f\n\n        For Series, it's name attribute must be set.\n\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_column = pd.Series(['d', 'e'], name='B', index=[0, 2])\n        >>> df.update(new_column)\n        >>> df\n           A  B\n        0  a  d\n        1  b  y\n        2  c  e\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_df = pd.DataFrame({'B': ['d', 'e']}, index=[1, 2])\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  a  x\n        1  b  d\n        2  c  e\n\n        If `other` contains NaNs the corresponding values are not updated\n        in the original dataframe.\n\n        >>> df = pd.DataFrame({'A': [1, 2, 3],\n        ...                    'B': [400, 500, 600]})\n        >>> new_df = pd.DataFrame({'B': [4, np.nan, 6]})\n        >>> df.update(new_df)\n        >>> df\n           A      B\n        0  1    4.0\n        1  2  500.0\n        2  3    6.0\n        \"\"\"\n        import pandas.core.computation.expressions as expressions\n\n        # TODO: Support other joins\n        if join != \"left\":  # pragma: no cover\n            raise NotImplementedError(\"Only left join is supported\")\n        if errors not in [\"ignore\", \"raise\"]:\n            raise ValueError(\"The parameter errors must be either 'ignore' or 'raise'\")\n\n        if not isinstance(other, DataFrame):\n            other = DataFrame(other)\n\n        other = other.reindex_like(self)\n\n        for col in self.columns:\n            this = self[col]._values\n            that = other[col]._values\n            if filter_func is not None:\n                with np.errstate(all=\"ignore\"):\n                    mask = ~filter_func(this) | isna(that)\n            else:\n                if errors == \"raise\":\n                    mask_this = notna(that)\n                    mask_that = notna(this)\n                    if any(mask_this & mask_that):\n                        raise ValueError(\"Data overlaps.\")\n\n                if overwrite:\n                    mask = isna(that)\n                else:\n                    mask = notna(this)\n\n            # don't overwrite columns unnecessarily\n            if mask.all():\n                continue\n\n            self[col] = expressions.where(mask, this, that)\n\n    # ----------------------------------------------------------------------\n    # Data reshaping\n    @Appender(\n        \"\"\"\nExamples\n--------\n>>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n...                               'Parrot', 'Parrot'],\n...                    'Max Speed': [380., 370., 24., 26.]})\n>>> df\n   Animal  Max Speed\n0  Falcon      380.0\n1  Falcon      370.0\n2  Parrot       24.0\n3  Parrot       26.0\n>>> df.groupby(['Animal']).mean()\n        Max Speed\nAnimal\nFalcon      375.0\nParrot       25.0\n\n**Hierarchical Indexes**\n\nWe can groupby different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> df = pd.DataFrame({'Max Speed': [390., 350., 30., 20.]},\n...                   index=index)\n>>> df\n                Max Speed\nAnimal Type\nFalcon Captive      390.0\n       Wild         350.0\nParrot Captive       30.0\n       Wild          20.0\n>>> df.groupby(level=0).mean()\n        Max Speed\nAnimal\nFalcon      370.0\nParrot       25.0\n>>> df.groupby(level=\"Type\").mean()\n         Max Speed\nType\nCaptive      210.0\nWild         185.0\n\nWe can also choose to include NA in group keys or not by setting\n`dropna` parameter, the default setting is `True`:\n\n>>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n>>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> df.groupby(by=[\"b\"]).sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\n\n>>> df.groupby(by=[\"b\"], dropna=False).sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\nNaN 1   4\n\n>>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\n>>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> df.groupby(by=\"a\").sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\n\n>>> df.groupby(by=\"a\", dropna=False).sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\nNaN 12.3   33.0\n\"\"\"\n    )\n    @Appender(_shared_docs[\"groupby\"] % _shared_doc_kwargs)\n    def groupby(\n        self,\n        by=None,\n        axis=0,\n        level=None,\n        as_index: bool = True,\n        sort: bool = True,\n        group_keys: bool = True,\n        squeeze: bool = no_default,\n        observed: bool = False,\n        dropna: bool = True,\n    ) -> DataFrameGroupBy:\n        from pandas.core.groupby.generic import DataFrameGroupBy\n\n        if squeeze is not no_default:\n            warnings.warn(\n                (\n                    \"The `squeeze` parameter is deprecated and \"\n                    \"will be removed in a future version.\"\n                ),\n                FutureWarning,\n                stacklevel=2,\n            )\n        else:\n            squeeze = False\n\n        if level is None and by is None:\n            raise TypeError(\"You have to supply one of 'by' and 'level'\")\n        axis = self._get_axis_number(axis)\n\n        return DataFrameGroupBy(\n            obj=self,\n            keys=by,\n            axis=axis,\n            level=level,\n            as_index=as_index,\n            sort=sort,\n            group_keys=group_keys,\n            squeeze=squeeze,\n            observed=observed,\n            dropna=dropna,\n        )\n\n    _shared_docs[\n        \"pivot\"\n    ] = \"\"\"\n        Return reshaped DataFrame organized by given index / column values.\n\n        Reshape data (produce a \"pivot\" table) based on column values. Uses\n        unique values from specified `index` / `columns` to form axes of the\n        resulting DataFrame. This function does not support data\n        aggregation, multiple values will result in a MultiIndex in the\n        columns. See the :ref:`User Guide <reshaping>` for more on reshaping.\n\n        Parameters\n        ----------%s\n        index : str or object or a list of str, optional\n            Column to use to make new frame's index. If None, uses\n            existing index.\n\n            .. versionchanged:: 1.1.0\n               Also accept list of index names.\n\n        columns : str or object or a list of str\n            Column to use to make new frame's columns.\n\n            .. versionchanged:: 1.1.0\n               Also accept list of columns names.\n\n        values : str, object or a list of the previous, optional\n            Column(s) to use for populating new frame's values. If not\n            specified, all remaining columns will be used and the result will\n            have hierarchically indexed columns.\n\n        Returns\n        -------\n        DataFrame\n            Returns reshaped DataFrame.\n\n        Raises\n        ------\n        ValueError:\n            When there are any `index`, `columns` combinations with multiple\n            values. `DataFrame.pivot_table` when you need to aggregate.\n\n        See Also\n        --------\n        DataFrame.pivot_table : Generalization of pivot that can handle\n            duplicate values for one index/column pair.\n        DataFrame.unstack : Pivot based on the index values instead of a\n            column.\n        wide_to_long : Wide panel to long format. Less flexible but more\n            user-friendly than melt.\n\n        Notes\n        -----\n        For finer-tuned control, see hierarchical indexing documentation along\n        with the related stack/unstack methods.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n        ...                            'two'],\n        ...                    'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n        ...                    'baz': [1, 2, 3, 4, 5, 6],\n        ...                    'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\n        >>> df\n            foo   bar  baz  zoo\n        0   one   A    1    x\n        1   one   B    2    y\n        2   one   C    3    z\n        3   two   A    4    q\n        4   two   B    5    w\n        5   two   C    6    t\n\n        >>> df.pivot(index='foo', columns='bar', values='baz')\n        bar  A   B   C\n        foo\n        one  1   2   3\n        two  4   5   6\n\n        >>> df.pivot(index='foo', columns='bar')['baz']\n        bar  A   B   C\n        foo\n        one  1   2   3\n        two  4   5   6\n\n        >>> df.pivot(index='foo', columns='bar', values=['baz', 'zoo'])\n              baz       zoo\n        bar   A  B  C   A  B  C\n        foo\n        one   1  2  3   x  y  z\n        two   4  5  6   q  w  t\n\n        You could also assign a list of column names or a list of index names.\n\n        >>> df = pd.DataFrame({\n        ...        \"lev1\": [1, 1, 1, 2, 2, 2],\n        ...        \"lev2\": [1, 1, 2, 1, 1, 2],\n        ...        \"lev3\": [1, 2, 1, 2, 1, 2],\n        ...        \"lev4\": [1, 2, 3, 4, 5, 6],\n        ...        \"values\": [0, 1, 2, 3, 4, 5]})\n        >>> df\n            lev1 lev2 lev3 lev4 values\n        0   1    1    1    1    0\n        1   1    1    2    2    1\n        2   1    2    1    3    2\n        3   2    1    2    4    3\n        4   2    1    1    5    4\n        5   2    2    2    6    5\n\n        >>> df.pivot(index=\"lev1\", columns=[\"lev2\", \"lev3\"],values=\"values\")\n        lev2    1         2\n        lev3    1    2    1    2\n        lev1\n        1     0.0  1.0  2.0  NaN\n        2     4.0  3.0  NaN  5.0\n\n        >>> df.pivot(index=[\"lev1\", \"lev2\"], columns=[\"lev3\"],values=\"values\")\n              lev3    1    2\n        lev1  lev2\n           1     1  0.0  1.0\n                 2  2.0  NaN\n           2     1  4.0  3.0\n                 2  NaN  5.0\n\n        A ValueError is raised if there are any duplicates.\n\n        >>> df = pd.DataFrame({\"foo\": ['one', 'one', 'two', 'two'],\n        ...                    \"bar\": ['A', 'A', 'B', 'C'],\n        ...                    \"baz\": [1, 2, 3, 4]})\n        >>> df\n           foo bar  baz\n        0  one   A    1\n        1  one   A    2\n        2  two   B    3\n        3  two   C    4\n\n        Notice that the first two rows are the same for our `index`\n        and `columns` arguments.\n\n        >>> df.pivot(index='foo', columns='bar', values='baz')\n        Traceback (most recent call last):\n           ...\n        ValueError: Index contains duplicate entries, cannot reshape\n        \"\"\"\n\n    @Substitution(\"\")\n    @Appender(_shared_docs[\"pivot\"])\n    def pivot(self, index=None, columns=None, values=None) -> DataFrame:\n        from pandas.core.reshape.pivot import pivot\n\n        return pivot(self, index=index, columns=columns, values=values)\n\n    _shared_docs[\n        \"pivot_table\"\n    ] = \"\"\"\n        Create a spreadsheet-style pivot table as a DataFrame.\n\n        The levels in the pivot table will be stored in MultiIndex objects\n        (hierarchical indexes) on the index and columns of the result DataFrame.\n\n        Parameters\n        ----------%s\n        values : column to aggregate, optional\n        index : column, Grouper, array, or list of the previous\n            If an array is passed, it must be the same length as the data. The\n            list can contain any of the other types (except list).\n            Keys to group by on the pivot table index.  If an array is passed,\n            it is being used as the same manner as column values.\n        columns : column, Grouper, array, or list of the previous\n            If an array is passed, it must be the same length as the data. The\n            list can contain any of the other types (except list).\n            Keys to group by on the pivot table column.  If an array is passed,\n            it is being used as the same manner as column values.\n        aggfunc : function, list of functions, dict, default numpy.mean\n            If list of functions passed, the resulting pivot table will have\n            hierarchical columns whose top level are the function names\n            (inferred from the function objects themselves)\n            If dict is passed, the key is column to aggregate and value\n            is function or list of functions.\n        fill_value : scalar, default None\n            Value to replace missing values with (in the resulting pivot table,\n            after aggregation).\n        margins : bool, default False\n            Add all row / columns (e.g. for subtotal / grand totals).\n        dropna : bool, default True\n            Do not include columns whose entries are all NaN.\n        margins_name : str, default 'All'\n            Name of the row / column that will contain the totals\n            when margins is True.\n        observed : bool, default False\n            This only applies if any of the groupers are Categoricals.\n            If True: only show observed values for categorical groupers.\n            If False: show all values for categorical groupers.\n\n            .. versionchanged:: 0.25.0\n\n        Returns\n        -------\n        DataFrame\n            An Excel style pivot table.\n\n        See Also\n        --------\n        DataFrame.pivot : Pivot without aggregation that can handle\n            non-numeric data.\n        DataFrame.melt: Unpivot a DataFrame from wide to long format,\n            optionally leaving identifiers set.\n        wide_to_long : Wide panel to long format. Less flexible but more\n            user-friendly than melt.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\",\n        ...                          \"bar\", \"bar\", \"bar\", \"bar\"],\n        ...                    \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\",\n        ...                          \"one\", \"one\", \"two\", \"two\"],\n        ...                    \"C\": [\"small\", \"large\", \"large\", \"small\",\n        ...                          \"small\", \"large\", \"small\", \"small\",\n        ...                          \"large\"],\n        ...                    \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7],\n        ...                    \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]})\n        >>> df\n             A    B      C  D  E\n        0  foo  one  small  1  2\n        1  foo  one  large  2  4\n        2  foo  one  large  2  5\n        3  foo  two  small  3  5\n        4  foo  two  small  3  6\n        5  bar  one  large  4  6\n        6  bar  one  small  5  8\n        7  bar  two  small  6  9\n        8  bar  two  large  7  9\n\n        This first example aggregates values by taking the sum.\n\n        >>> table = pd.pivot_table(df, values='D', index=['A', 'B'],\n        ...                     columns=['C'], aggfunc=np.sum)\n        >>> table\n        C        large  small\n        A   B\n        bar one    4.0    5.0\n            two    7.0    6.0\n        foo one    4.0    1.0\n            two    NaN    6.0\n\n        We can also fill missing values using the `fill_value` parameter.\n\n        >>> table = pd.pivot_table(df, values='D', index=['A', 'B'],\n        ...                     columns=['C'], aggfunc=np.sum, fill_value=0)\n        >>> table\n        C        large  small\n        A   B\n        bar one      4      5\n            two      7      6\n        foo one      4      1\n            two      0      6\n\n        The next example aggregates by taking the mean across multiple columns.\n\n        >>> table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n        ...                     aggfunc={'D': np.mean,\n        ...                              'E': np.mean})\n        >>> table\n                        D         E\n        A   C\n        bar large  5.500000  7.500000\n            small  5.500000  8.500000\n        foo large  2.000000  4.500000\n            small  2.333333  4.333333\n\n        We can also calculate multiple types of aggregations for any given\n        value column.\n\n        >>> table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n        ...                     aggfunc={'D': np.mean,\n        ...                              'E': [min, max, np.mean]})\n        >>> table\n                        D    E\n                    mean  max      mean  min\n        A   C\n        bar large  5.500000  9.0  7.500000  6.0\n            small  5.500000  9.0  8.500000  8.0\n        foo large  2.000000  5.0  4.500000  4.0\n            small  2.333333  6.0  4.333333  2.0\n        \"\"\"\n\n    @Substitution(\"\")\n    @Appender(_shared_docs[\"pivot_table\"])\n    def pivot_table(\n        self,\n        values=None,\n        index=None,\n        columns=None,\n        aggfunc=\"mean\",\n        fill_value=None,\n        margins=False,\n        dropna=True,\n        margins_name=\"All\",\n        observed=False,\n    ) -> DataFrame:\n        from pandas.core.reshape.pivot import pivot_table\n\n        return pivot_table(\n            self,\n            values=values,\n            index=index,\n            columns=columns,\n            aggfunc=aggfunc,\n            fill_value=fill_value,\n            margins=margins,\n            dropna=dropna,\n            margins_name=margins_name,\n            observed=observed,\n        )\n\n    def stack(self, level=-1, dropna=True):\n        \"\"\"\n        Stack the prescribed level(s) from columns to index.\n\n        Return a reshaped DataFrame or Series having a multi-level\n        index with one or more new inner-most levels compared to the current\n        DataFrame. The new inner-most levels are created by pivoting the\n        columns of the current dataframe:\n\n          - if the columns have a single level, the output is a Series;\n          - if the columns have multiple levels, the new index\n            level(s) is (are) taken from the prescribed level(s) and\n            the output is a DataFrame.\n\n        Parameters\n        ----------\n        level : int, str, list, default -1\n            Level(s) to stack from the column axis onto the index\n            axis, defined as one index or label, or a list of indices\n            or labels.\n        dropna : bool, default True\n            Whether to drop rows in the resulting Frame/Series with\n            missing values. Stacking a column level onto the index\n            axis can create combinations of index and column values\n            that are missing from the original dataframe. See Examples\n            section.\n\n        Returns\n        -------\n        DataFrame or Series\n            Stacked dataframe or series.\n\n        See Also\n        --------\n        DataFrame.unstack : Unstack prescribed level(s) from index axis\n             onto column axis.\n        DataFrame.pivot : Reshape dataframe from long format to wide\n             format.\n        DataFrame.pivot_table : Create a spreadsheet-style pivot table\n             as a DataFrame.\n\n        Notes\n        -----\n        The function is named by analogy with a collection of books\n        being reorganized from being side by side on a horizontal\n        position (the columns of the dataframe) to being stacked\n        vertically on top of each other (in the index of the\n        dataframe).\n\n        Examples\n        --------\n        **Single level columns**\n\n        >>> df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=['weight', 'height'])\n\n        Stacking a dataframe with a single level column axis returns a Series:\n\n        >>> df_single_level_cols\n             weight height\n        cat       0      1\n        dog       2      3\n        >>> df_single_level_cols.stack()\n        cat  weight    0\n             height    1\n        dog  weight    2\n             height    3\n        dtype: int64\n\n        **Multi level columns: simple case**\n\n        >>> multicol1 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n        ...                                        ('weight', 'pounds')])\n        >>> df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=multicol1)\n\n        Stacking a dataframe with a multi-level column axis:\n\n        >>> df_multi_level_cols1\n             weight\n                 kg    pounds\n        cat       1        2\n        dog       2        4\n        >>> df_multi_level_cols1.stack()\n                    weight\n        cat kg           1\n            pounds       2\n        dog kg           2\n            pounds       4\n\n        **Missing values**\n\n        >>> multicol2 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n        ...                                        ('height', 'm')])\n        >>> df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=multicol2)\n\n        It is common to have missing values when stacking a dataframe\n        with multi-level columns, as the stacked dataframe typically\n        has more values than the original dataframe. Missing values\n        are filled with NaNs:\n\n        >>> df_multi_level_cols2\n            weight height\n                kg      m\n        cat    1.0    2.0\n        dog    3.0    4.0\n        >>> df_multi_level_cols2.stack()\n                height  weight\n        cat kg     NaN     1.0\n            m      2.0     NaN\n        dog kg     NaN     3.0\n            m      4.0     NaN\n\n        **Prescribing the level(s) to be stacked**\n\n        The first parameter controls which level or levels are stacked:\n\n        >>> df_multi_level_cols2.stack(0)\n                     kg    m\n        cat height  NaN  2.0\n            weight  1.0  NaN\n        dog height  NaN  4.0\n            weight  3.0  NaN\n        >>> df_multi_level_cols2.stack([0, 1])\n        cat  height  m     2.0\n             weight  kg    1.0\n        dog  height  m     4.0\n             weight  kg    3.0\n        dtype: float64\n\n        **Dropping missing values**\n\n        >>> df_multi_level_cols3 = pd.DataFrame([[None, 1.0], [2.0, 3.0]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=multicol2)\n\n        Note that rows where all values are missing are dropped by\n        default but this behaviour can be controlled via the dropna\n        keyword parameter:\n\n        >>> df_multi_level_cols3\n            weight height\n                kg      m\n        cat    NaN    1.0\n        dog    2.0    3.0\n        >>> df_multi_level_cols3.stack(dropna=False)\n                height  weight\n        cat kg     NaN     NaN\n            m      1.0     NaN\n        dog kg     NaN     2.0\n            m      3.0     NaN\n        >>> df_multi_level_cols3.stack(dropna=True)\n                height  weight\n        cat m      1.0     NaN\n        dog kg     NaN     2.0\n            m      3.0     NaN\n        \"\"\"\n        from pandas.core.reshape.reshape import stack, stack_multiple\n\n        if isinstance(level, (tuple, list)):\n            result = stack_multiple(self, level, dropna=dropna)\n        else:\n            result = stack(self, level, dropna=dropna)\n\n        return result.__finalize__(self, method=\"stack\")\n\n    def explode(\n        self, column: Union[str, Tuple], ignore_index: bool = False\n    ) -> DataFrame:\n        \"\"\"\n        Transform each element of a list-like to a row, replicating index values.\n\n        .. versionadded:: 0.25.0\n\n        Parameters\n        ----------\n        column : str or tuple\n            Column to explode.\n        ignore_index : bool, default False\n            If True, the resulting index will be labeled 0, 1, …, n - 1.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        DataFrame\n            Exploded lists to rows of the subset columns;\n            index will be duplicated for these rows.\n\n        Raises\n        ------\n        ValueError :\n            if columns of the frame are not unique.\n\n        See Also\n        --------\n        DataFrame.unstack : Pivot a level of the (necessarily hierarchical)\n            index labels.\n        DataFrame.melt : Unpivot a DataFrame from wide format to long format.\n        Series.explode : Explode a DataFrame from list-like columns to long format.\n\n        Notes\n        -----\n        This routine will explode list-likes including lists, tuples, sets,\n        Series, and np.ndarray. The result dtype of the subset rows will\n        be object. Scalars will be returned unchanged, and empty list-likes will\n        result in a np.nan for that row. In addition, the ordering of rows in the\n        output will be non-deterministic when exploding sets.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [[1, 2, 3], 'foo', [], [3, 4]], 'B': 1})\n        >>> df\n                   A  B\n        0  [1, 2, 3]  1\n        1        foo  1\n        2         []  1\n        3     [3, 4]  1\n\n        >>> df.explode('A')\n             A  B\n        0    1  1\n        0    2  1\n        0    3  1\n        1  foo  1\n        2  NaN  1\n        3    3  1\n        3    4  1\n        \"\"\"\n        if not (is_scalar(column) or isinstance(column, tuple)):\n            raise ValueError(\"column must be a scalar\")\n        if not self.columns.is_unique:\n            raise ValueError(\"columns must be unique\")\n\n        df = self.reset_index(drop=True)\n        # TODO: use overload to refine return type of reset_index\n        assert df is not None  # needed for mypy\n        result = df[column].explode()\n        result = df.drop([column], axis=1).join(result)\n        if ignore_index:\n            result.index = ibase.default_index(len(result))\n        else:\n            result.index = self.index.take(result.index)\n        result = result.reindex(columns=self.columns, copy=False)\n\n        return result\n\n    def unstack(self, level=-1, fill_value=None):\n        \"\"\"\n        Pivot a level of the (necessarily hierarchical) index labels.\n\n        Returns a DataFrame having a new level of column labels whose inner-most level\n        consists of the pivoted index labels.\n\n        If the index is not a MultiIndex, the output will be a Series\n        (the analogue of stack when the columns are not a MultiIndex).\n\n        Parameters\n        ----------\n        level : int, str, or list of these, default -1 (last level)\n            Level(s) of index to unstack, can pass level name.\n        fill_value : int, str or dict\n            Replace NaN with this value if the unstack produces missing values.\n\n        Returns\n        -------\n        Series or DataFrame\n\n        See Also\n        --------\n        DataFrame.pivot : Pivot a table based on column values.\n        DataFrame.stack : Pivot a level of the column labels (inverse operation\n            from `unstack`).\n\n        Examples\n        --------\n        >>> index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),\n        ...                                    ('two', 'a'), ('two', 'b')])\n        >>> s = pd.Series(np.arange(1.0, 5.0), index=index)\n        >>> s\n        one  a   1.0\n             b   2.0\n        two  a   3.0\n             b   4.0\n        dtype: float64\n\n        >>> s.unstack(level=-1)\n             a   b\n        one  1.0  2.0\n        two  3.0  4.0\n\n        >>> s.unstack(level=0)\n           one  two\n        a  1.0   3.0\n        b  2.0   4.0\n\n        >>> df = s.unstack(level=0)\n        >>> df.unstack()\n        one  a  1.0\n             b  2.0\n        two  a  3.0\n             b  4.0\n        dtype: float64\n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n\n        return unstack(self, level, fill_value)\n\n    @Appender(_shared_docs[\"melt\"] % dict(caller=\"df.melt(\", other=\"melt\"))\n    def melt(\n        self,\n        id_vars=None,\n        value_vars=None,\n        var_name=None,\n        value_name=\"value\",\n        col_level=None,\n        ignore_index=True,\n    ) -> DataFrame:\n\n        return melt(\n            self,\n            id_vars=id_vars,\n            value_vars=value_vars,\n            var_name=var_name,\n            value_name=value_name,\n            col_level=col_level,\n            ignore_index=ignore_index,\n        )\n\n    # ----------------------------------------------------------------------\n    # Time series-related\n\n    @doc(\n        Series.diff,\n        klass=\"Dataframe\",\n        extra_params=\"axis : {0 or 'index', 1 or 'columns'}, default 0\\n    \"\n        \"Take difference over rows (0) or columns (1).\\n\",\n        other_klass=\"Series\",\n        examples=dedent(\n            \"\"\"\n        Difference with previous row\n\n        >>> df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6],\n        ...                    'b': [1, 1, 2, 3, 5, 8],\n        ...                    'c': [1, 4, 9, 16, 25, 36]})\n        >>> df\n           a  b   c\n        0  1  1   1\n        1  2  1   4\n        2  3  2   9\n        3  4  3  16\n        4  5  5  25\n        5  6  8  36\n\n        >>> df.diff()\n             a    b     c\n        0  NaN  NaN   NaN\n        1  1.0  0.0   3.0\n        2  1.0  1.0   5.0\n        3  1.0  1.0   7.0\n        4  1.0  2.0   9.0\n        5  1.0  3.0  11.0\n\n        Difference with previous column\n\n        >>> df.diff(axis=1)\n            a  b   c\n        0 NaN  0   0\n        1 NaN -1   3\n        2 NaN -1   7\n        3 NaN -1  13\n        4 NaN  0  20\n        5 NaN  2  28\n\n        Difference with 3rd previous row\n\n        >>> df.diff(periods=3)\n             a    b     c\n        0  NaN  NaN   NaN\n        1  NaN  NaN   NaN\n        2  NaN  NaN   NaN\n        3  3.0  2.0  15.0\n        4  3.0  4.0  21.0\n        5  3.0  6.0  27.0\n\n        Difference with following row\n\n        >>> df.diff(periods=-1)\n             a    b     c\n        0 -1.0  0.0  -3.0\n        1 -1.0 -1.0  -5.0\n        2 -1.0 -1.0  -7.0\n        3 -1.0 -2.0  -9.0\n        4 -1.0 -3.0 -11.0\n        5  NaN  NaN   NaN\n\n        Overflow in input dtype\n\n        >>> df = pd.DataFrame({'a': [1, 0]}, dtype=np.uint8)\n        >>> df.diff()\n               a\n        0    NaN\n        1  255.0\"\"\"\n        ),\n    )\n    def diff(self, periods: int = 1, axis: Axis = 0) -> DataFrame:\n        if not isinstance(periods, int):\n            if not (is_float(periods) and periods.is_integer()):\n                raise ValueError(\"periods must be an integer\")\n            periods = int(periods)\n\n        bm_axis = self._get_block_manager_axis(axis)\n\n        if bm_axis == 0 and periods != 0:\n            return self - self.shift(periods, axis=axis)\n\n        new_data = self._mgr.diff(n=periods, axis=bm_axis)\n        return self._constructor(new_data)\n\n    # ----------------------------------------------------------------------\n    # Function application\n\n    def _gotitem(\n        self,\n        key: Union[Label, List[Label]],\n        ndim: int,\n        subset: Optional[FrameOrSeriesUnion] = None,\n    ) -> FrameOrSeriesUnion:\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : 1,2\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n        if subset is None:\n            subset = self\n        elif subset.ndim == 1:  # is Series\n            return subset\n\n        # TODO: _shallow_copy(subset)?\n        return subset[key]\n\n    _agg_summary_and_see_also_doc = dedent(\n        \"\"\"\n    The aggregation operations are always performed over an axis, either the\n    index (default) or the column axis. This behavior is different from\n    `numpy` aggregation functions (`mean`, `median`, `prod`, `sum`, `std`,\n    `var`), where the default is to compute the aggregation of the flattened\n    array, e.g., ``numpy.mean(arr_2d)`` as opposed to\n    ``numpy.mean(arr_2d, axis=0)``.\n\n    `agg` is an alias for `aggregate`. Use the alias.\n\n    See Also\n    --------\n    DataFrame.apply : Perform any type of operations.\n    DataFrame.transform : Perform transformation type operations.\n    core.groupby.GroupBy : Perform operations over groups.\n    core.resample.Resampler : Perform operations over resampled bins.\n    core.window.Rolling : Perform operations over rolling window.\n    core.window.Expanding : Perform operations over expanding window.\n    core.window.ExponentialMovingWindow : Perform operation over exponential weighted\n        window.\n    \"\"\"\n    )\n\n    _agg_examples_doc = dedent(\n        \"\"\"\n    Examples\n    --------\n    >>> df = pd.DataFrame([[1, 2, 3],\n    ...                    [4, 5, 6],\n    ...                    [7, 8, 9],\n    ...                    [np.nan, np.nan, np.nan]],\n    ...                   columns=['A', 'B', 'C'])\n\n    Aggregate these functions over the rows.\n\n    >>> df.agg(['sum', 'min'])\n            A     B     C\n    sum  12.0  15.0  18.0\n    min   1.0   2.0   3.0\n\n    Different aggregations per column.\n\n    >>> df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})\n            A    B\n    sum  12.0  NaN\n    min   1.0  2.0\n    max   NaN  8.0\n\n    Aggregate different functions over the columns and rename the index of the resulting\n    DataFrame.\n\n    >>> df.agg(x=('A', max), y=('B', 'min'), z=('C', np.mean))\n         A    B    C\n    x  7.0  NaN  NaN\n    y  NaN  2.0  NaN\n    z  NaN  NaN  6.0\n\n    Aggregate over the columns.\n\n    >>> df.agg(\"mean\", axis=\"columns\")\n    0    2.0\n    1    5.0\n    2    8.0\n    3    NaN\n    dtype: float64\n    \"\"\"\n    )\n\n    @doc(\n        _shared_docs[\"aggregate\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        axis=_shared_doc_kwargs[\"axis\"],\n        see_also=_agg_summary_and_see_also_doc,\n        examples=_agg_examples_doc,\n    )\n    def aggregate(self, func=None, axis=0, *args, **kwargs):\n        axis = self._get_axis_number(axis)\n\n        relabeling, func, columns, order = reconstruct_func(func, **kwargs)\n\n        result = None\n        try:\n            result, how = self._aggregate(func, axis, *args, **kwargs)\n        except TypeError as err:\n            exc = TypeError(\n                \"DataFrame constructor called with \"\n                f\"incompatible data and dtype: {err}\"\n            )\n            raise exc from err\n        if result is None:\n            return self.apply(func, axis=axis, args=args, **kwargs)\n\n        if relabeling:\n            # This is to keep the order to columns occurrence unchanged, and also\n            # keep the order of new columns occurrence unchanged\n\n            # For the return values of reconstruct_func, if relabeling is\n            # False, columns and order will be None.\n            assert columns is not None\n            assert order is not None\n\n            result_in_dict = relabel_result(result, func, columns, order)\n            result = DataFrame(result_in_dict, index=columns)\n\n        return result\n\n    def _aggregate(self, arg, axis=0, *args, **kwargs):\n        if axis == 1:\n            # NDFrame.aggregate returns a tuple, and we need to transpose\n            # only result\n            result, how = aggregate(self.T, arg, *args, **kwargs)\n            result = result.T if result is not None else result\n            return result, how\n        return aggregate(self, arg, *args, **kwargs)\n\n    agg = aggregate\n\n    @doc(\n        _shared_docs[\"transform\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        axis=_shared_doc_kwargs[\"axis\"],\n    )\n    def transform(\n        self, func: AggFuncType, axis: Axis = 0, *args, **kwargs\n    ) -> DataFrame:\n        result = transform(self, func, axis, *args, **kwargs)\n        assert isinstance(result, DataFrame)\n        return result\n\n    def apply(self, func, axis=0, raw=False, result_type=None, args=(), **kwds):\n        \"\"\"\n        Apply a function along an axis of the DataFrame.\n\n        Objects passed to the function are Series objects whose index is\n        either the DataFrame's index (``axis=0``) or the DataFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to apply to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': apply function to each column.\n            * 1 or 'columns': apply function to each row.\n\n        raw : bool, default False\n            Determines if row or column is passed as a Series or ndarray object:\n\n            * ``False`` : passes each row or column as a Series to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just applying a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Series if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the DataFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Series\n            of those. However if the apply function returns a Series these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/series.\n        **kwds\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Series or DataFrame\n            Result of applying ``func`` along the given axis of the\n            DataFrame.\n\n        See Also\n        --------\n        DataFrame.applymap: For elementwise operations.\n        DataFrame.aggregate: Only perform aggregating type operations.\n        DataFrame.transform: Only perform transforming type operations.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> df\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(df)``):\n\n        >>> df.apply(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> df.apply(np.sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> df.apply(np.sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Series\n\n        >>> df.apply(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Series inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Series index.\n\n        >>> df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        \"\"\"\n        from pandas.core.apply import frame_apply\n\n        op = frame_apply(\n            self,\n            func=func,\n            axis=axis,\n            raw=raw,\n            result_type=result_type,\n            args=args,\n            kwds=kwds,\n        )\n        return op.get_result()\n\n    def applymap(self, func, na_action: Optional[str] = None) -> DataFrame:\n        \"\"\"\n        Apply a function to a Dataframe elementwise.\n\n        This method applies a function that accepts and returns a scalar\n        to every element of a DataFrame.\n\n        Parameters\n        ----------\n        func : callable\n            Python function, returns a single value from a single value.\n        na_action : {None, 'ignore'}, default None\n            If ‘ignore’, propagate NaN values, without passing them to func.\n\n            .. versionadded:: 1.2\n\n        Returns\n        -------\n        DataFrame\n            Transformed DataFrame.\n\n        See Also\n        --------\n        DataFrame.apply : Apply a function along input axis of DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])\n        >>> df\n               0      1\n        0  1.000  2.120\n        1  3.356  4.567\n\n        >>> df.applymap(lambda x: len(str(x)))\n           0  1\n        0  3  4\n        1  5  5\n\n        Like Series.map, NA values can be ignored:\n\n        >>> df_copy = df.copy()\n        >>> df_copy.iloc[0, 0] = pd.NA\n        >>> df_copy.applymap(lambda x: len(str(x)), na_action='ignore')\n              0  1\n        0  <NA>  4\n        1     5  5\n\n        Note that a vectorized version of `func` often exists, which will\n        be much faster. You could square each number elementwise.\n\n        >>> df.applymap(lambda x: x**2)\n                   0          1\n        0   1.000000   4.494400\n        1  11.262736  20.857489\n\n        But it's better to avoid applymap in that case.\n\n        >>> df ** 2\n                   0          1\n        0   1.000000   4.494400\n        1  11.262736  20.857489\n        \"\"\"\n        if na_action not in {\"ignore\", None}:\n            raise ValueError(\n                f\"na_action must be 'ignore' or None. Got {repr(na_action)}\"\n            )\n        ignore_na = na_action == \"ignore\"\n\n        # if we have a dtype == 'M8[ns]', provide boxed values\n        def infer(x):\n            if x.empty:\n                return lib.map_infer(x, func, ignore_na=ignore_na)\n            return lib.map_infer(x.astype(object)._values, func, ignore_na=ignore_na)\n\n        return self.apply(infer)\n\n    # ----------------------------------------------------------------------\n    # Merging / joining methods\n\n    def append(\n        self, other, ignore_index=False, verify_integrity=False, sort=False\n    ) -> DataFrame:\n        \"\"\"\n        Append rows of `other` to the end of caller, returning a new object.\n\n        Columns in `other` that are not in the caller are added as new columns.\n\n        Parameters\n        ----------\n        other : DataFrame or Series/dict-like object, or list of these\n            The data to append.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, …, n - 1.\n        verify_integrity : bool, default False\n            If True, raise ValueError on creating index with duplicates.\n        sort : bool, default False\n            Sort columns if the columns of `self` and `other` are not aligned.\n\n            .. versionchanged:: 1.0.0\n\n                Changed to not sort by default.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        concat : General function to concatenate DataFrame or Series objects.\n\n        Notes\n        -----\n        If a list of dict/series is passed and the keys are all contained in\n        the DataFrame's index, the order of the columns in the resulting\n        DataFrame will be unchanged.\n\n        Iteratively appending rows to a DataFrame can be more computationally\n        intensive than a single concatenate. A better solution is to append\n        those rows to a list and then concatenate the list with the original\n        DataFrame all at once.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n        >>> df\n           A  B\n        0  1  2\n        1  3  4\n        >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n        >>> df.append(df2)\n           A  B\n        0  1  2\n        1  3  4\n        0  5  6\n        1  7  8\n\n        With `ignore_index` set to True:\n\n        >>> df.append(df2, ignore_index=True)\n           A  B\n        0  1  2\n        1  3  4\n        2  5  6\n        3  7  8\n\n        The following, while not recommended methods for generating DataFrames,\n        show two ways to generate a DataFrame from multiple data sources.\n\n        Less efficient:\n\n        >>> df = pd.DataFrame(columns=['A'])\n        >>> for i in range(5):\n        ...     df = df.append({'A': i}, ignore_index=True)\n        >>> df\n           A\n        0  0\n        1  1\n        2  2\n        3  3\n        4  4\n\n        More efficient:\n\n        >>> pd.concat([pd.DataFrame([i], columns=['A']) for i in range(5)],\n        ...           ignore_index=True)\n           A\n        0  0\n        1  1\n        2  2\n        3  3\n        4  4\n        \"\"\"\n        if isinstance(other, (Series, dict)):\n            if isinstance(other, dict):\n                if not ignore_index:\n                    raise TypeError(\"Can only append a dict if ignore_index=True\")\n                other = Series(other)\n            if other.name is None and not ignore_index:\n                raise TypeError(\n                    \"Can only append a Series if ignore_index=True \"\n                    \"or if the Series has a name\"\n                )\n\n            index = Index([other.name], name=self.index.name)\n            idx_diff = other.index.difference(self.columns)\n            try:\n                combined_columns = self.columns.append(idx_diff)\n            except TypeError:\n                combined_columns = self.columns.astype(object).append(idx_diff)\n            other = (\n                other.reindex(combined_columns, copy=False)\n                .to_frame()\n                .T.infer_objects()\n                .rename_axis(index.names, copy=False)\n            )\n            if not self.columns.equals(combined_columns):\n                self = self.reindex(columns=combined_columns)\n        elif isinstance(other, list):\n            if not other:\n                pass\n            elif not isinstance(other[0], DataFrame):\n                other = DataFrame(other)\n                if (self.columns.get_indexer(other.columns) >= 0).all():\n                    other = other.reindex(columns=self.columns)\n\n        from pandas.core.reshape.concat import concat\n\n        if isinstance(other, (list, tuple)):\n            to_concat = [self, *other]\n        else:\n            to_concat = [self, other]\n        return concat(\n            to_concat,\n            ignore_index=ignore_index,\n            verify_integrity=verify_integrity,\n            sort=sort,\n        )\n\n    def join(\n        self, other, on=None, how=\"left\", lsuffix=\"\", rsuffix=\"\", sort=False\n    ) -> DataFrame:\n        \"\"\"\n        Join columns of another DataFrame.\n\n        Join columns with `other` DataFrame either on index or on a key\n        column. Efficiently join multiple DataFrame objects by index at once by\n        passing a list.\n\n        Parameters\n        ----------\n        other : DataFrame, Series, or list of DataFrame\n            Index should be similar to one of the columns in this one. If a\n            Series is passed, its name attribute must be set, and that will be\n            used as the column name in the resulting joined DataFrame.\n        on : str, list of str, or array-like, optional\n            Column or index level name(s) in the caller to join on the index\n            in `other`, otherwise joins index-on-index. If multiple\n            values given, the `other` DataFrame must have a MultiIndex. Can\n            pass an array as the join key if it is not already contained in\n            the calling DataFrame. Like an Excel VLOOKUP operation.\n        how : {'left', 'right', 'outer', 'inner'}, default 'left'\n            How to handle the operation of the two objects.\n\n            * left: use calling frame's index (or column if on is specified)\n            * right: use `other`'s index.\n            * outer: form union of calling frame's index (or column if on is\n              specified) with `other`'s index, and sort it.\n              lexicographically.\n            * inner: form intersection of calling frame's index (or column if\n              on is specified) with `other`'s index, preserving the order\n              of the calling's one.\n        lsuffix : str, default ''\n            Suffix to use from left frame's overlapping columns.\n        rsuffix : str, default ''\n            Suffix to use from right frame's overlapping columns.\n        sort : bool, default False\n            Order result DataFrame lexicographically by the join key. If False,\n            the order of the join key depends on the join type (how keyword).\n\n        Returns\n        -------\n        DataFrame\n            A dataframe containing columns from both the caller and `other`.\n\n        See Also\n        --------\n        DataFrame.merge : For column(s)-on-columns(s) operations.\n\n        Notes\n        -----\n        Parameters `on`, `lsuffix`, and `rsuffix` are not supported when\n        passing a list of `DataFrame` objects.\n\n        Support for specifying index levels as the `on` parameter was added\n        in version 0.23.0.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],\n        ...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n\n        >>> df\n          key   A\n        0  K0  A0\n        1  K1  A1\n        2  K2  A2\n        3  K3  A3\n        4  K4  A4\n        5  K5  A5\n\n        >>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],\n        ...                       'B': ['B0', 'B1', 'B2']})\n\n        >>> other\n          key   B\n        0  K0  B0\n        1  K1  B1\n        2  K2  B2\n\n        Join DataFrames using their indexes.\n\n        >>> df.join(other, lsuffix='_caller', rsuffix='_other')\n          key_caller   A key_other    B\n        0         K0  A0        K0   B0\n        1         K1  A1        K1   B1\n        2         K2  A2        K2   B2\n        3         K3  A3       NaN  NaN\n        4         K4  A4       NaN  NaN\n        5         K5  A5       NaN  NaN\n\n        If we want to join using the key columns, we need to set key to be\n        the index in both `df` and `other`. The joined DataFrame will have\n        key as its index.\n\n        >>> df.set_index('key').join(other.set_index('key'))\n              A    B\n        key\n        K0   A0   B0\n        K1   A1   B1\n        K2   A2   B2\n        K3   A3  NaN\n        K4   A4  NaN\n        K5   A5  NaN\n\n        Another option to join using the key columns is to use the `on`\n        parameter. DataFrame.join always uses `other`'s index but we can use\n        any column in `df`. This method preserves the original DataFrame's\n        index in the result.\n\n        >>> df.join(other.set_index('key'), on='key')\n          key   A    B\n        0  K0  A0   B0\n        1  K1  A1   B1\n        2  K2  A2   B2\n        3  K3  A3  NaN\n        4  K4  A4  NaN\n        5  K5  A5  NaN\n        \"\"\"\n        return self._join_compat(\n            other, on=on, how=how, lsuffix=lsuffix, rsuffix=rsuffix, sort=sort\n        )\n\n    def _join_compat(\n        self, other, on=None, how=\"left\", lsuffix=\"\", rsuffix=\"\", sort=False\n    ):\n        from pandas.core.reshape.concat import concat\n        from pandas.core.reshape.merge import merge\n\n        if isinstance(other, Series):\n            if other.name is None:\n                raise ValueError(\"Other Series must have a name\")\n            other = DataFrame({other.name: other})\n\n        if isinstance(other, DataFrame):\n            return merge(\n                self,\n                other,\n                left_on=on,\n                how=how,\n                left_index=on is None,\n                right_index=True,\n                suffixes=(lsuffix, rsuffix),\n                sort=sort,\n            )\n        else:\n            if on is not None:\n                raise ValueError(\n                    \"Joining multiple DataFrames only supported for joining on index\"\n                )\n\n            frames = [self] + list(other)\n\n            can_concat = all(df.index.is_unique for df in frames)\n\n            # join indexes only using concat\n            if can_concat:\n                if how == \"left\":\n                    res = concat(\n                        frames, axis=1, join=\"outer\", verify_integrity=True, sort=sort\n                    )\n                    return res.reindex(self.index, copy=False)\n                else:\n                    return concat(\n                        frames, axis=1, join=how, verify_integrity=True, sort=sort\n                    )\n\n            joined = frames[0]\n\n            for frame in frames[1:]:\n                joined = merge(\n                    joined, frame, how=how, left_index=True, right_index=True\n                )\n\n            return joined\n\n    @Substitution(\"\")\n    @Appender(_merge_doc, indents=2)\n    def merge(\n        self,\n        right,\n        how=\"inner\",\n        on=None,\n        left_on=None,\n        right_on=None,\n        left_index=False,\n        right_index=False,\n        sort=False,\n        suffixes=(\"_x\", \"_y\"),\n        copy=True,\n        indicator=False,\n        validate=None,\n    ) -> DataFrame:\n        from pandas.core.reshape.merge import merge\n\n        return merge(\n            self,\n            right,\n            how=how,\n            on=on,\n            left_on=left_on,\n            right_on=right_on,\n            left_index=left_index,\n            right_index=right_index,\n            sort=sort,\n            suffixes=suffixes,\n            copy=copy,\n            indicator=indicator,\n            validate=validate,\n        )\n\n    def round(self, decimals=0, *args, **kwargs) -> DataFrame:\n        \"\"\"\n        Round a DataFrame to a variable number of decimal places.\n\n        Parameters\n        ----------\n        decimals : int, dict, Series\n            Number of decimal places to round each column to. If an int is\n            given, round each column to the same number of places.\n            Otherwise dict and Series round to variable numbers of places.\n            Column names should be in the keys if `decimals` is a\n            dict-like, or in the index if `decimals` is a Series. Any\n            columns not included in `decimals` will be left as is. Elements\n            of `decimals` which are not columns of the input will be\n            ignored.\n        *args\n            Additional keywords have no effect but might be accepted for\n            compatibility with numpy.\n        **kwargs\n            Additional keywords have no effect but might be accepted for\n            compatibility with numpy.\n\n        Returns\n        -------\n        DataFrame\n            A DataFrame with the affected columns rounded to the specified\n            number of decimal places.\n\n        See Also\n        --------\n        numpy.around : Round a numpy array to the given number of decimals.\n        Series.round : Round a Series to the given number of decimals.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df\n            dogs  cats\n        0  0.21  0.32\n        1  0.01  0.67\n        2  0.66  0.03\n        3  0.21  0.18\n\n        By providing an integer each column is rounded to the same number\n        of decimal places\n\n        >>> df.round(1)\n            dogs  cats\n        0   0.2   0.3\n        1   0.0   0.7\n        2   0.7   0.0\n        3   0.2   0.2\n\n        With a dict, the number of places for specific columns can be\n        specified with the column names as key and the number of decimal\n        places as value\n\n        >>> df.round({'dogs': 1, 'cats': 0})\n            dogs  cats\n        0   0.2   0.0\n        1   0.0   1.0\n        2   0.7   0.0\n        3   0.2   0.0\n\n        Using a Series, the number of places for specific columns can be\n        specified with the column names as index and the number of\n        decimal places as value\n\n        >>> decimals = pd.Series([0, 1], index=['cats', 'dogs'])\n        >>> df.round(decimals)\n            dogs  cats\n        0   0.2   0.0\n        1   0.0   1.0\n        2   0.7   0.0\n        3   0.2   0.0\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        def _dict_round(df, decimals):\n            for col, vals in df.items():\n                try:\n                    yield _series_round(vals, decimals[col])\n                except KeyError:\n                    yield vals\n\n        def _series_round(s, decimals):\n            if is_integer_dtype(s) or is_float_dtype(s):\n                return s.round(decimals)\n            return s\n\n        nv.validate_round(args, kwargs)\n\n        if isinstance(decimals, (dict, Series)):\n            if isinstance(decimals, Series):\n                if not decimals.index.is_unique:\n                    raise ValueError(\"Index of decimals must be unique\")\n            new_cols = list(_dict_round(self, decimals))\n        elif is_integer(decimals):\n            # Dispatch to Series.round\n            new_cols = [_series_round(v, decimals) for _, v in self.items()]\n        else:\n            raise TypeError(\"decimals must be an integer, a dict-like or a Series\")\n\n        if len(new_cols) > 0:\n            return self._constructor(\n                concat(new_cols, axis=1), index=self.index, columns=self.columns\n            )\n        else:\n            return self\n\n    # ----------------------------------------------------------------------\n    # Statistical methods, etc.\n\n    def corr(self, method=\"pearson\", min_periods=1) -> DataFrame:\n        \"\"\"\n        Compute pairwise correlation of columns, excluding NA/null values.\n\n        Parameters\n        ----------\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method of correlation:\n\n            * pearson : standard correlation coefficient\n            * kendall : Kendall Tau correlation coefficient\n            * spearman : Spearman rank correlation\n            * callable: callable with input two 1d ndarrays\n                and returning a float. Note that the returned matrix from corr\n                will have 1 along the diagonals and will be symmetric\n                regardless of the callable's behavior.\n\n                .. versionadded:: 0.24.0\n\n        min_periods : int, optional\n            Minimum number of observations required per pair of columns\n            to have a valid result. Currently only available for Pearson\n            and Spearman correlation.\n\n        Returns\n        -------\n        DataFrame\n            Correlation matrix.\n\n        See Also\n        --------\n        DataFrame.corrwith : Compute pairwise correlation with another\n            DataFrame or Series.\n        Series.corr : Compute the correlation between two Series.\n\n        Examples\n        --------\n        >>> def histogram_intersection(a, b):\n        ...     v = np.minimum(a, b).sum().round(decimals=1)\n        ...     return v\n        >>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df.corr(method=histogram_intersection)\n              dogs  cats\n        dogs   1.0   0.3\n        cats   0.3   1.0\n        \"\"\"\n        numeric_df = self._get_numeric_data()\n        cols = numeric_df.columns\n        idx = cols.copy()\n        mat = numeric_df.to_numpy(dtype=float, na_value=np.nan, copy=False)\n\n        if method == \"pearson\":\n            correl = libalgos.nancorr(mat, minp=min_periods)\n        elif method == \"spearman\":\n            correl = libalgos.nancorr_spearman(mat, minp=min_periods)\n        elif method == \"kendall\" or callable(method):\n            if min_periods is None:\n                min_periods = 1\n            mat = mat.T\n            corrf = nanops.get_corr_func(method)\n            K = len(cols)\n            correl = np.empty((K, K), dtype=float)\n            mask = np.isfinite(mat)\n            for i, ac in enumerate(mat):\n                for j, bc in enumerate(mat):\n                    if i > j:\n                        continue\n\n                    valid = mask[i] & mask[j]\n                    if valid.sum() < min_periods:\n                        c = np.nan\n                    elif i == j:\n                        c = 1.0\n                    elif not valid.all():\n                        c = corrf(ac[valid], bc[valid])\n                    else:\n                        c = corrf(ac, bc)\n                    correl[i, j] = c\n                    correl[j, i] = c\n        else:\n            raise ValueError(\n                \"method must be either 'pearson', \"\n                \"'spearman', 'kendall', or a callable, \"\n                f\"'{method}' was supplied\"\n            )\n\n        return self._constructor(correl, index=idx, columns=cols)\n\n    def cov(\n        self, min_periods: Optional[int] = None, ddof: Optional[int] = 1\n    ) -> DataFrame:\n        \"\"\"\n        Compute pairwise covariance of columns, excluding NA/null values.\n\n        Compute the pairwise covariance among the series of a DataFrame.\n        The returned data frame is the `covariance matrix\n        <https://en.wikipedia.org/wiki/Covariance_matrix>`__ of the columns\n        of the DataFrame.\n\n        Both NA and null values are automatically excluded from the\n        calculation. (See the note below about bias from missing values.)\n        A threshold can be set for the minimum number of\n        observations for each value created. Comparisons with observations\n        below this threshold will be returned as ``NaN``.\n\n        This method is generally used for the analysis of time series data to\n        understand the relationship between different measures\n        across time.\n\n        Parameters\n        ----------\n        min_periods : int, optional\n            Minimum number of observations required per pair of columns\n            to have a valid result.\n\n        ddof : int, default 1\n            Delta degrees of freedom.  The divisor used in calculations\n            is ``N - ddof``, where ``N`` represents the number of elements.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        DataFrame\n            The covariance matrix of the series of the DataFrame.\n\n        See Also\n        --------\n        Series.cov : Compute covariance with another Series.\n        core.window.ExponentialMovingWindow.cov: Exponential weighted sample covariance.\n        core.window.Expanding.cov : Expanding sample covariance.\n        core.window.Rolling.cov : Rolling sample covariance.\n\n        Notes\n        -----\n        Returns the covariance matrix of the DataFrame's time series.\n        The covariance is normalized by N-ddof.\n\n        For DataFrames that have Series that are missing data (assuming that\n        data is `missing at random\n        <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random>`__)\n        the returned covariance matrix will be an unbiased estimate\n        of the variance and covariance between the member Series.\n\n        However, for many applications this estimate may not be acceptable\n        because the estimate covariance matrix is not guaranteed to be positive\n        semi-definite. This could lead to estimate correlations having\n        absolute values which are greater than one, and/or a non-invertible\n        covariance matrix. See `Estimation of covariance matrices\n        <https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_\n        matrices>`__ for more details.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df.cov()\n                  dogs      cats\n        dogs  0.666667 -1.000000\n        cats -1.000000  1.666667\n\n        >>> np.random.seed(42)\n        >>> df = pd.DataFrame(np.random.randn(1000, 5),\n        ...                   columns=['a', 'b', 'c', 'd', 'e'])\n        >>> df.cov()\n                  a         b         c         d         e\n        a  0.998438 -0.020161  0.059277 -0.008943  0.014144\n        b -0.020161  1.059352 -0.008543 -0.024738  0.009826\n        c  0.059277 -0.008543  1.010670 -0.001486 -0.000271\n        d -0.008943 -0.024738 -0.001486  0.921297 -0.013692\n        e  0.014144  0.009826 -0.000271 -0.013692  0.977795\n\n        **Minimum number of periods**\n\n        This method also supports an optional ``min_periods`` keyword\n        that specifies the required minimum number of non-NA observations for\n        each column pair in order to have a valid result:\n\n        >>> np.random.seed(42)\n        >>> df = pd.DataFrame(np.random.randn(20, 3),\n        ...                   columns=['a', 'b', 'c'])\n        >>> df.loc[df.index[:5], 'a'] = np.nan\n        >>> df.loc[df.index[5:10], 'b'] = np.nan\n        >>> df.cov(min_periods=12)\n                  a         b         c\n        a  0.316741       NaN -0.150812\n        b       NaN  1.248003  0.191417\n        c -0.150812  0.191417  0.895202\n        \"\"\"\n        numeric_df = self._get_numeric_data()\n        cols = numeric_df.columns\n        idx = cols.copy()\n        mat = numeric_df.to_numpy(dtype=float, na_value=np.nan, copy=False)\n\n        if notna(mat).all():\n            if min_periods is not None and min_periods > len(mat):\n                base_cov = np.empty((mat.shape[1], mat.shape[1]))\n                base_cov.fill(np.nan)\n            else:\n                base_cov = np.cov(mat.T, ddof=ddof)\n            base_cov = base_cov.reshape((len(cols), len(cols)))\n        else:\n            base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\n\n        return self._constructor(base_cov, index=idx, columns=cols)\n\n    def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:\n        \"\"\"\n        Compute pairwise correlation.\n\n        Pairwise correlation is computed between rows or columns of\n        DataFrame with rows or columns of Series or DataFrame. DataFrames\n        are first aligned along both axes before computing the\n        correlations.\n\n        Parameters\n        ----------\n        other : DataFrame, Series\n            Object with which to compute correlations.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' to compute column-wise, 1 or 'columns' for\n            row-wise.\n        drop : bool, default False\n            Drop missing indices from result.\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method of correlation:\n\n            * pearson : standard correlation coefficient\n            * kendall : Kendall Tau correlation coefficient\n            * spearman : Spearman rank correlation\n            * callable: callable with input two 1d ndarrays\n                and returning a float.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        Series\n            Pairwise correlations.\n\n        See Also\n        --------\n        DataFrame.corr : Compute pairwise correlation of columns.\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        this = self._get_numeric_data()\n\n        if isinstance(other, Series):\n            return this.apply(lambda x: other.corr(x, method=method), axis=axis)\n\n        other = other._get_numeric_data()\n        left, right = this.align(other, join=\"inner\", copy=False)\n\n        if axis == 1:\n            left = left.T\n            right = right.T\n\n        if method == \"pearson\":\n            # mask missing values\n            left = left + right * 0\n            right = right + left * 0\n\n            # demeaned data\n            ldem = left - left.mean()\n            rdem = right - right.mean()\n\n            num = (ldem * rdem).sum()\n            dom = (left.count() - 1) * left.std() * right.std()\n\n            correl = num / dom\n\n        elif method in [\"kendall\", \"spearman\"] or callable(method):\n\n            def c(x):\n                return nanops.nancorr(x[0], x[1], method=method)\n\n            correl = self._constructor_sliced(\n                map(c, zip(left.values.T, right.values.T)), index=left.columns\n            )\n\n        else:\n            raise ValueError(\n                f\"Invalid method {method} was passed, \"\n                \"valid methods are: 'pearson', 'kendall', \"\n                \"'spearman', or callable\"\n            )\n\n        if not drop:\n            # Find non-matching labels along the given axis\n            # and append missing correlations (GH 22375)\n            raxis = 1 if axis == 0 else 0\n            result_index = this._get_axis(raxis).union(other._get_axis(raxis))\n            idx_diff = result_index.difference(correl.index)\n\n            if len(idx_diff) > 0:\n                correl = correl.append(Series([np.nan] * len(idx_diff), index=idx_diff))\n\n        return correl\n\n    # ----------------------------------------------------------------------\n    # ndarray-like stats methods\n\n    def count(self, axis=0, level=None, numeric_only=False):\n        \"\"\"\n        Count non-NA cells for each column or row.\n\n        The values `None`, `NaN`, `NaT`, and optionally `numpy.inf` (depending\n        on `pandas.options.mode.use_inf_as_na`) are considered NA.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            If 0 or 'index' counts are generated for each column.\n            If 1 or 'columns' counts are generated for each row.\n        level : int or str, optional\n            If the axis is a `MultiIndex` (hierarchical), count along a\n            particular `level`, collapsing into a `DataFrame`.\n            A `str` specifies the level name.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n        Returns\n        -------\n        Series or DataFrame\n            For each column/row the number of non-NA/null entries.\n            If `level` is specified returns a `DataFrame`.\n\n        See Also\n        --------\n        Series.count: Number of non-NA elements in a Series.\n        DataFrame.value_counts: Count unique combinations of columns.\n        DataFrame.shape: Number of DataFrame rows and columns (including NA\n            elements).\n        DataFrame.isna: Boolean same-sized DataFrame showing places of NA\n            elements.\n\n        Examples\n        --------\n        Constructing DataFrame from a dictionary:\n\n        >>> df = pd.DataFrame({\"Person\":\n        ...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n        ...                    \"Age\": [24., np.nan, 21., 33, 26],\n        ...                    \"Single\": [False, True, True, True, False]})\n        >>> df\n           Person   Age  Single\n        0    John  24.0   False\n        1    Myla   NaN    True\n        2   Lewis  21.0    True\n        3    John  33.0    True\n        4    Myla  26.0   False\n\n        Notice the uncounted NA values:\n\n        >>> df.count()\n        Person    5\n        Age       4\n        Single    5\n        dtype: int64\n\n        Counts for each **row**:\n\n        >>> df.count(axis='columns')\n        0    3\n        1    2\n        2    3\n        3    3\n        4    3\n        dtype: int64\n\n        Counts for one level of a `MultiIndex`:\n\n        >>> df.set_index([\"Person\", \"Single\"]).count(level=\"Person\")\n                Age\n        Person\n        John      2\n        Lewis     1\n        Myla      1\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        if level is not None:\n            return self._count_level(level, axis=axis, numeric_only=numeric_only)\n\n        if numeric_only:\n            frame = self._get_numeric_data()\n        else:\n            frame = self\n\n        # GH #423\n        if len(frame._get_axis(axis)) == 0:\n            result = self._constructor_sliced(0, index=frame._get_agg_axis(axis))\n        else:\n            if frame._is_mixed_type or frame._mgr.any_extension_types:\n                # the or any_extension_types is really only hit for single-\n                # column frames with an extension array\n                result = notna(frame).sum(axis=axis)\n            else:\n                # GH13407\n                series_counts = notna(frame).sum(axis=axis)\n                counts = series_counts.values\n                result = self._constructor_sliced(\n                    counts, index=frame._get_agg_axis(axis)\n                )\n\n        return result.astype(\"int64\")\n\n    def _count_level(self, level, axis=0, numeric_only=False):\n        if numeric_only:\n            frame = self._get_numeric_data()\n        else:\n            frame = self\n\n        count_axis = frame._get_axis(axis)\n        agg_axis = frame._get_agg_axis(axis)\n\n        if not isinstance(count_axis, MultiIndex):\n            raise TypeError(\n                f\"Can only count levels on hierarchical {self._get_axis_name(axis)}.\"\n            )\n\n        # Mask NaNs: Mask rows or columns where the index level is NaN, and all\n        # values in the DataFrame that are NaN\n        if frame._is_mixed_type:\n            # Since we have mixed types, calling notna(frame.values) might\n            # upcast everything to object\n            values_mask = notna(frame).values\n        else:\n            # But use the speedup when we have homogeneous dtypes\n            values_mask = notna(frame.values)\n\n        index_mask = notna(count_axis.get_level_values(level=level))\n        if axis == 1:\n            mask = index_mask & values_mask\n        else:\n            mask = index_mask.reshape(-1, 1) & values_mask\n\n        if isinstance(level, str):\n            level = count_axis._get_level_number(level)\n\n        level_name = count_axis._names[level]\n        level_index = count_axis.levels[level]._shallow_copy(name=level_name)\n        level_codes = ensure_int64(count_axis.codes[level])\n        counts = lib.count_level_2d(mask, level_codes, len(level_index), axis=axis)\n\n        if axis == 1:\n            result = self._constructor(counts, index=agg_axis, columns=level_index)\n        else:\n            result = self._constructor(counts, index=level_index, columns=agg_axis)\n\n        return result\n\n    def _reduce(\n        self,\n        op,\n        name: str,\n        axis=0,\n        skipna=True,\n        numeric_only=None,\n        filter_type=None,\n        **kwds,\n    ):\n\n        assert filter_type is None or filter_type == \"bool\", filter_type\n        out_dtype = \"bool\" if filter_type == \"bool\" else None\n\n        own_dtypes = [arr.dtype for arr in self._iter_column_arrays()]\n\n        dtype_is_dt = np.array(\n            [is_datetime64_any_dtype(dtype) for dtype in own_dtypes],\n            dtype=bool,\n        )\n        if numeric_only is None and name in [\"mean\", \"median\"] and dtype_is_dt.any():\n            warnings.warn(\n                \"DataFrame.mean and DataFrame.median with numeric_only=None \"\n                \"will include datetime64 and datetime64tz columns in a \"\n                \"future version.\",\n                FutureWarning,\n                stacklevel=5,\n            )\n            cols = self.columns[~dtype_is_dt]\n            self = self[cols]\n\n        any_object = np.array(\n            [is_object_dtype(dtype) for dtype in own_dtypes],\n            dtype=bool,\n        ).any()\n\n        # TODO: Make other agg func handle axis=None properly GH#21597\n        axis = self._get_axis_number(axis)\n        labels = self._get_agg_axis(axis)\n        assert axis in [0, 1]\n\n        def func(values):\n            if is_extension_array_dtype(values.dtype):\n                return extract_array(values)._reduce(name, skipna=skipna, **kwds)\n            else:\n                return op(values, axis=axis, skipna=skipna, **kwds)\n\n        def blk_func(values):\n            if isinstance(values, ExtensionArray):\n                return values._reduce(name, skipna=skipna, **kwds)\n            else:\n                return op(values, axis=1, skipna=skipna, **kwds)\n\n        def _get_data() -> DataFrame:\n            if filter_type is None:\n                data = self._get_numeric_data()\n            else:\n                # GH#25101, GH#24434\n                assert filter_type == \"bool\"\n                data = self._get_bool_data()\n            return data\n\n        if numeric_only is not None or (\n            numeric_only is None\n            and axis == 0\n            and not any_object\n            and not self._mgr.any_extension_types\n        ):\n            # For numeric_only non-None and axis non-None, we know\n            #  which blocks to use and no try/except is needed.\n            #  For numeric_only=None only the case with axis==0 and no object\n            #  dtypes are unambiguous can be handled with BlockManager.reduce\n            # Case with EAs see GH#35881\n            df = self\n            if numeric_only is True:\n                df = _get_data()\n            if axis == 1:\n                df = df.T\n                axis = 0\n\n            ignore_failures = numeric_only is None\n\n            # After possibly _get_data and transposing, we are now in the\n            #  simple case where we can use BlockManager.reduce\n            res, indexer = df._mgr.reduce(blk_func, ignore_failures=ignore_failures)\n            out = df._constructor(res).iloc[0]\n            if out_dtype is not None:\n                out = out.astype(out_dtype)\n            if axis == 0 and is_object_dtype(out.dtype):\n                # GH#35865 careful to cast explicitly to object\n                nvs = coerce_to_dtypes(out.values, df.dtypes.iloc[np.sort(indexer)])\n                out[:] = np.array(nvs, dtype=object)\n            return out\n\n        assert numeric_only is None\n\n        if not self._is_homogeneous_type or self._mgr.any_extension_types:\n            # try to avoid self.values call\n\n            if filter_type is None and axis == 0:\n                # operate column-wise\n\n                # numeric_only must be None here, as other cases caught above\n\n                # this can end up with a non-reduction\n                # but not always. if the types are mixed\n                # with datelike then need to make sure a series\n\n                # we only end up here if we have not specified\n                # numeric_only and yet we have tried a\n                # column-by-column reduction, where we have mixed type.\n                # So let's just do what we can\n                from pandas.core.apply import frame_apply\n\n                opa = frame_apply(\n                    self, func=func, result_type=\"expand\", ignore_failures=True\n                )\n                result = opa.get_result()\n                if result.ndim == self.ndim:\n                    result = result.iloc[0].rename(None)\n                return result\n\n        data = self\n        values = data.values\n\n        try:\n            result = func(values)\n\n        except TypeError:\n            # e.g. in nanops trying to convert strs to float\n\n            data = _get_data()\n            labels = data._get_agg_axis(axis)\n\n            values = data.values\n            with np.errstate(all=\"ignore\"):\n                result = func(values)\n\n        if filter_type == \"bool\" and notna(result).all():\n            result = result.astype(np.bool_)\n        elif filter_type is None and is_object_dtype(result.dtype):\n            try:\n                result = result.astype(np.float64)\n            except (ValueError, TypeError):\n                # try to coerce to the original dtypes item by item if we can\n                if axis == 0:\n                    result = coerce_to_dtypes(result, data.dtypes)\n\n        result = self._constructor_sliced(result, index=labels)\n        return result\n\n    def nunique(self, axis=0, dropna=True) -> Series:\n        \"\"\"\n        Count distinct observations over requested axis.\n\n        Return Series with number of distinct observations. Can ignore NaN\n        values.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for\n            column-wise.\n        dropna : bool, default True\n            Don't include NaN in the counts.\n\n        Returns\n        -------\n        Series\n\n        See Also\n        --------\n        Series.nunique: Method nunique for Series.\n        DataFrame.count: Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [1, 1, 1]})\n        >>> df.nunique()\n        A    3\n        B    1\n        dtype: int64\n\n        >>> df.nunique(axis=1)\n        0    1\n        1    2\n        2    2\n        dtype: int64\n        \"\"\"\n        return self.apply(Series.nunique, axis=axis, dropna=dropna)\n\n    def idxmin(self, axis=0, skipna=True) -> Series:\n        \"\"\"\n        Return index of first occurrence of minimum over requested axis.\n\n        NA/null values are excluded.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n\n        Returns\n        -------\n        Series\n            Indexes of minima along the specified axis.\n\n        Raises\n        ------\n        ValueError\n            * If the row/column is empty\n\n        See Also\n        --------\n        Series.idxmin : Return index of the minimum element.\n\n        Notes\n        -----\n        This method is the DataFrame version of ``ndarray.argmin``.\n\n        Examples\n        --------\n        Consider a dataset containing food consumption in Argentina.\n\n        >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n        ...                    'co2_emissions': [37.2, 19.66, 1712]},\n        ...                    index=['Pork', 'Wheat Products', 'Beef'])\n\n        >>> df\n                        consumption  co2_emissions\n        Pork                  10.51         37.20\n        Wheat Products       103.11         19.66\n        Beef                  55.48       1712.00\n\n        By default, it returns the index for the minimum value in each column.\n\n        >>> df.idxmin()\n        consumption                Pork\n        co2_emissions    Wheat Products\n        dtype: object\n\n        To return the index for the minimum value in each row, use ``axis=\"columns\"``.\n\n        >>> df.idxmin(axis=\"columns\")\n        Pork                consumption\n        Wheat Products    co2_emissions\n        Beef                consumption\n        dtype: object\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        indices = nanops.nanargmin(self.values, axis=axis, skipna=skipna)\n\n        # indices will always be np.ndarray since axis is not None and\n        # values is a 2d array for DataFrame\n        # error: Item \"int\" of \"Union[int, Any]\" has no attribute \"__iter__\"\n        assert isinstance(indices, np.ndarray)  # for mypy\n\n        index = self._get_axis(axis)\n        result = [index[i] if i >= 0 else np.nan for i in indices]\n        return self._constructor_sliced(result, index=self._get_agg_axis(axis))\n\n    def idxmax(self, axis=0, skipna=True) -> Series:\n        \"\"\"\n        Return index of first occurrence of maximum over requested axis.\n\n        NA/null values are excluded.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n\n        Returns\n        -------\n        Series\n            Indexes of maxima along the specified axis.\n\n        Raises\n        ------\n        ValueError\n            * If the row/column is empty\n\n        See Also\n        --------\n        Series.idxmax : Return index of the maximum element.\n\n        Notes\n        -----\n        This method is the DataFrame version of ``ndarray.argmax``.\n\n        Examples\n        --------\n        Consider a dataset containing food consumption in Argentina.\n\n        >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n        ...                    'co2_emissions': [37.2, 19.66, 1712]},\n        ...                    index=['Pork', 'Wheat Products', 'Beef'])\n\n        >>> df\n                        consumption  co2_emissions\n        Pork                  10.51         37.20\n        Wheat Products       103.11         19.66\n        Beef                  55.48       1712.00\n\n        By default, it returns the index for the maximum value in each column.\n\n        >>> df.idxmax()\n        consumption     Wheat Products\n        co2_emissions             Beef\n        dtype: object\n\n        To return the index for the maximum value in each row, use ``axis=\"columns\"``.\n\n        >>> df.idxmax(axis=\"columns\")\n        Pork              co2_emissions\n        Wheat Products     consumption\n        Beef              co2_emissions\n        dtype: object\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        indices = nanops.nanargmax(self.values, axis=axis, skipna=skipna)\n\n        # indices will always be np.ndarray since axis is not None and\n        # values is a 2d array for DataFrame\n        # error: Item \"int\" of \"Union[int, Any]\" has no attribute \"__iter__\"\n        assert isinstance(indices, np.ndarray)  # for mypy\n\n        index = self._get_axis(axis)\n        result = [index[i] if i >= 0 else np.nan for i in indices]\n        return self._constructor_sliced(result, index=self._get_agg_axis(axis))\n\n    def _get_agg_axis(self, axis_num: int) -> Index:\n        \"\"\"\n        Let's be explicit about this.\n        \"\"\"\n        if axis_num == 0:\n            return self.columns\n        elif axis_num == 1:\n            return self.index\n        else:\n            raise ValueError(f\"Axis must be 0 or 1 (got {repr(axis_num)})\")\n\n    def mode(self, axis=0, numeric_only=False, dropna=True) -> DataFrame:\n        \"\"\"\n        Get the mode(s) of each element along the selected axis.\n\n        The mode of a set of values is the value that appears most often.\n        It can be multiple values.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to iterate over while searching for the mode:\n\n            * 0 or 'index' : get mode of each column\n            * 1 or 'columns' : get mode of each row.\n\n        numeric_only : bool, default False\n            If True, only apply to numeric columns.\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        DataFrame\n            The modes of each column or row.\n\n        See Also\n        --------\n        Series.mode : Return the highest frequency value in a Series.\n        Series.value_counts : Return the counts of values in a Series.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('bird', 2, 2),\n        ...                    ('mammal', 4, np.nan),\n        ...                    ('arthropod', 8, 0),\n        ...                    ('bird', 2, np.nan)],\n        ...                   index=('falcon', 'horse', 'spider', 'ostrich'),\n        ...                   columns=('species', 'legs', 'wings'))\n        >>> df\n                   species  legs  wings\n        falcon        bird     2    2.0\n        horse       mammal     4    NaN\n        spider   arthropod     8    0.0\n        ostrich       bird     2    NaN\n\n        By default, missing values are not considered, and the mode of wings\n        are both 0 and 2. Because the resulting DataFrame has two rows,\n        the second row of ``species`` and ``legs`` contains ``NaN``.\n\n        >>> df.mode()\n          species  legs  wings\n        0    bird   2.0    0.0\n        1     NaN   NaN    2.0\n\n        Setting ``dropna=False`` ``NaN`` values are considered and they can be\n        the mode (like for wings).\n\n        >>> df.mode(dropna=False)\n          species  legs  wings\n        0    bird     2    NaN\n\n        Setting ``numeric_only=True``, only the mode of numeric columns is\n        computed, and columns of other types are ignored.\n\n        >>> df.mode(numeric_only=True)\n           legs  wings\n        0   2.0    0.0\n        1   NaN    2.0\n\n        To compute the mode over columns and not rows, use the axis parameter:\n\n        >>> df.mode(axis='columns', numeric_only=True)\n                   0    1\n        falcon   2.0  NaN\n        horse    4.0  NaN\n        spider   0.0  8.0\n        ostrich  2.0  NaN\n        \"\"\"\n        data = self if not numeric_only else self._get_numeric_data()\n\n        def f(s):\n            return s.mode(dropna=dropna)\n\n        return data.apply(f, axis=axis)\n\n    def quantile(self, q=0.5, axis=0, numeric_only=True, interpolation=\"linear\"):\n        \"\"\"\n        Return values at the given quantile over requested axis.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            Value between 0 <= q <= 1, the quantile(s) to compute.\n        axis : {0, 1, 'index', 'columns'}, default 0\n            Equals 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n        numeric_only : bool, default True\n            If False, the quantile of datetime and timedelta data will be\n            computed as well.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            This optional parameter specifies the interpolation method to use,\n            when the desired quantile lies between two data points `i` and `j`:\n\n            * linear: `i + (j - i) * fraction`, where `fraction` is the\n              fractional part of the index surrounded by `i` and `j`.\n            * lower: `i`.\n            * higher: `j`.\n            * nearest: `i` or `j` whichever is nearest.\n            * midpoint: (`i` + `j`) / 2.\n\n        Returns\n        -------\n        Series or DataFrame\n\n            If ``q`` is an array, a DataFrame will be returned where the\n              index is ``q``, the columns are the columns of self, and the\n              values are the quantiles.\n            If ``q`` is a float, a Series will be returned where the\n              index is the columns of self and the values are the quantiles.\n\n        See Also\n        --------\n        core.window.Rolling.quantile: Rolling quantile.\n        numpy.percentile: Numpy function to compute the percentile.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]),\n        ...                   columns=['a', 'b'])\n        >>> df.quantile(.1)\n        a    1.3\n        b    3.7\n        Name: 0.1, dtype: float64\n        >>> df.quantile([.1, .5])\n               a     b\n        0.1  1.3   3.7\n        0.5  2.5  55.0\n\n        Specifying `numeric_only=False` will also compute the quantile of\n        datetime and timedelta data.\n\n        >>> df = pd.DataFrame({'A': [1, 2],\n        ...                    'B': [pd.Timestamp('2010'),\n        ...                          pd.Timestamp('2011')],\n        ...                    'C': [pd.Timedelta('1 days'),\n        ...                          pd.Timedelta('2 days')]})\n        >>> df.quantile(0.5, numeric_only=False)\n        A                    1.5\n        B    2010-07-02 12:00:00\n        C        1 days 12:00:00\n        Name: 0.5, dtype: object\n        \"\"\"\n        validate_percentile(q)\n\n        data = self._get_numeric_data() if numeric_only else self\n        axis = self._get_axis_number(axis)\n        is_transposed = axis == 1\n\n        if is_transposed:\n            data = data.T\n\n        if len(data.columns) == 0:\n            # GH#23925 _get_numeric_data may have dropped all columns\n            cols = Index([], name=self.columns.name)\n            if is_list_like(q):\n                return self._constructor([], index=q, columns=cols)\n            return self._constructor_sliced([], index=cols, name=q, dtype=np.float64)\n\n        result = data._mgr.quantile(\n            qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n        )\n\n        if result.ndim == 2:\n            result = self._constructor(result)\n        else:\n            result = self._constructor_sliced(result, name=q)\n\n        if is_transposed:\n            result = result.T\n\n        return result\n\n    def to_timestamp(\n        self, freq=None, how: str = \"start\", axis: Axis = 0, copy: bool = True\n    ) -> DataFrame:\n        \"\"\"\n        Cast to DatetimeIndex of timestamps, at *beginning* of period.\n\n        Parameters\n        ----------\n        freq : str, default frequency of PeriodIndex\n            Desired frequency.\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to convert (the index by default).\n        copy : bool, default True\n            If False then underlying input data is not copied.\n\n        Returns\n        -------\n        DataFrame with DatetimeIndex\n        \"\"\"\n        new_obj = self.copy(deep=copy)\n\n        axis_name = self._get_axis_name(axis)\n        old_ax = getattr(self, axis_name)\n        new_ax = old_ax.to_timestamp(freq=freq, how=how)\n\n        setattr(new_obj, axis_name, new_ax)\n        return new_obj\n\n    def to_period(self, freq=None, axis: Axis = 0, copy: bool = True) -> DataFrame:\n        \"\"\"\n        Convert DataFrame from DatetimeIndex to PeriodIndex.\n\n        Convert DataFrame from DatetimeIndex to PeriodIndex with desired\n        frequency (inferred from index if not passed).\n\n        Parameters\n        ----------\n        freq : str, default\n            Frequency of the PeriodIndex.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to convert (the index by default).\n        copy : bool, default True\n            If False then underlying input data is not copied.\n\n        Returns\n        -------\n        DataFrame with PeriodIndex\n        \"\"\"\n        new_obj = self.copy(deep=copy)\n\n        axis_name = self._get_axis_name(axis)\n        old_ax = getattr(self, axis_name)\n        new_ax = old_ax.to_period(freq=freq)\n\n        setattr(new_obj, axis_name, new_ax)\n        return new_obj\n\n    def isin(self, values) -> DataFrame:\n        \"\"\"\n        Whether each element in the DataFrame is contained in values.\n\n        Parameters\n        ----------\n        values : iterable, Series, DataFrame or dict\n            The result will only be true at a location if all the\n            labels match. If `values` is a Series, that's the index. If\n            `values` is a dict, the keys must be the column names,\n            which must match. If `values` is a DataFrame,\n            then both the index and column labels must match.\n\n        Returns\n        -------\n        DataFrame\n            DataFrame of booleans showing whether each element in the DataFrame\n            is contained in values.\n\n        See Also\n        --------\n        DataFrame.eq: Equality test for DataFrame.\n        Series.isin: Equivalent method on Series.\n        Series.str.contains: Test if pattern or regex is contained within a\n            string of a Series or Index.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]},\n        ...                   index=['falcon', 'dog'])\n        >>> df\n                num_legs  num_wings\n        falcon         2          2\n        dog            4          0\n\n        When ``values`` is a list check whether every value in the DataFrame\n        is present in the list (which animals have 0 or 2 legs or wings)\n\n        >>> df.isin([0, 2])\n                num_legs  num_wings\n        falcon      True       True\n        dog        False       True\n\n        When ``values`` is a dict, we can pass values to check for each\n        column separately:\n\n        >>> df.isin({'num_wings': [0, 3]})\n                num_legs  num_wings\n        falcon     False      False\n        dog        False       True\n\n        When ``values`` is a Series or DataFrame the index and column must\n        match. Note that 'falcon' does not match based on the number of legs\n        in df2.\n\n        >>> other = pd.DataFrame({'num_legs': [8, 2], 'num_wings': [0, 2]},\n        ...                      index=['spider', 'falcon'])\n        >>> df.isin(other)\n                num_legs  num_wings\n        falcon      True       True\n        dog        False      False\n        \"\"\"\n        if isinstance(values, dict):\n            from pandas.core.reshape.concat import concat\n\n            values = collections.defaultdict(list, values)\n            return concat(\n                (\n                    self.iloc[:, [i]].isin(values[col])\n                    for i, col in enumerate(self.columns)\n                ),\n                axis=1,\n            )\n        elif isinstance(values, Series):\n            if not values.index.is_unique:\n                raise ValueError(\"cannot compute isin with a duplicate axis.\")\n            return self.eq(values.reindex_like(self), axis=\"index\")\n        elif isinstance(values, DataFrame):\n            if not (values.columns.is_unique and values.index.is_unique):\n                raise ValueError(\"cannot compute isin with a duplicate axis.\")\n            return self.eq(values.reindex_like(self))\n        else:\n            if not is_list_like(values):\n                raise TypeError(\n                    \"only list-like or dict-like objects are allowed \"\n                    \"to be passed to DataFrame.isin(), \"\n                    f\"you passed a '{type(values).__name__}'\"\n                )\n            return self._constructor(\n                algorithms.isin(self.values.ravel(), values).reshape(self.shape),\n                self.index,\n                self.columns,\n            )\n\n    # ----------------------------------------------------------------------\n    # Add index and columns\n    _AXIS_ORDERS = [\"index\", \"columns\"]\n    _AXIS_TO_AXIS_NUMBER: Dict[Axis, int] = {\n        **NDFrame._AXIS_TO_AXIS_NUMBER,\n        1: 1,\n        \"columns\": 1,\n    }\n    _AXIS_REVERSED = True\n    _AXIS_LEN = len(_AXIS_ORDERS)\n    _info_axis_number = 1\n    _info_axis_name = \"columns\"\n\n    index: Index = properties.AxisProperty(\n        axis=1, doc=\"The index (row labels) of the DataFrame.\"\n    )\n    columns: Index = properties.AxisProperty(\n        axis=0, doc=\"The column labels of the DataFrame.\"\n    )\n\n    @property\n    def _AXIS_NUMBERS(self) -> Dict[str, int]:\n        \"\"\".. deprecated:: 1.1.0\"\"\"\n        super()._AXIS_NUMBERS\n        return {\"index\": 0, \"columns\": 1}\n\n    @property\n    def _AXIS_NAMES(self) -> Dict[int, str]:\n        \"\"\".. deprecated:: 1.1.0\"\"\"\n        super()._AXIS_NAMES\n        return {0: \"index\", 1: \"columns\"}\n\n    # ----------------------------------------------------------------------\n    # Add plotting methods to DataFrame\n    plot = CachedAccessor(\"plot\", pandas.plotting.PlotAccessor)\n    hist = pandas.plotting.hist_frame\n    boxplot = pandas.plotting.boxplot_frame\n    sparse = CachedAccessor(\"sparse\", SparseFrameAccessor)\n\n\nDataFrame._add_numeric_operations()\n\nops.add_flex_arithmetic_methods(DataFrame)\n\n\ndef _from_nested_dict(data) -> collections.defaultdict:\n    new_data: collections.defaultdict = collections.defaultdict(dict)\n    for index, s in data.items():\n        for col, v in s.items():\n            new_data[col][index] = v\n    return new_data\n"
    },
    {
      "filename": "pandas/core/generic.py",
      "content": "from __future__ import annotations\n\nimport collections\nfrom datetime import timedelta\nimport functools\nimport gc\nimport json\nimport operator\nimport pickle\nimport re\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    FrozenSet,\n    Hashable,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\nimport warnings\nimport weakref\n\nimport numpy as np\n\nfrom pandas._config import config\n\nfrom pandas._libs import lib\nfrom pandas._libs.tslibs import Period, Tick, Timestamp, to_offset\nfrom pandas._typing import (\n    Axis,\n    CompressionOptions,\n    FilePathOrBuffer,\n    FrameOrSeries,\n    IndexKeyFunc,\n    IndexLabel,\n    JSONSerializable,\n    Label,\n    Level,\n    Renamer,\n    StorageOptions,\n    TimedeltaConvertibleTypes,\n    TimestampConvertibleTypes,\n    ValueKeyFunc,\n)\nfrom pandas.compat._optional import import_optional_dependency\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import AbstractMethodError, InvalidIndexError\nfrom pandas.util._decorators import Appender, doc, rewrite_axis_style_signature\nfrom pandas.util._validators import (\n    validate_bool_kwarg,\n    validate_fillna_kwargs,\n    validate_percentile,\n)\n\nfrom pandas.core.dtypes.common import (\n    ensure_int64,\n    ensure_object,\n    ensure_str,\n    is_bool,\n    is_bool_dtype,\n    is_datetime64_any_dtype,\n    is_datetime64tz_dtype,\n    is_dict_like,\n    is_extension_array_dtype,\n    is_float,\n    is_list_like,\n    is_number,\n    is_numeric_dtype,\n    is_object_dtype,\n    is_re_compilable,\n    is_scalar,\n    is_timedelta64_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.generic import ABCDataFrame, ABCSeries\nfrom pandas.core.dtypes.inference import is_hashable\nfrom pandas.core.dtypes.missing import isna, notna\n\nimport pandas as pd\nfrom pandas.core import indexing, missing, nanops\nimport pandas.core.algorithms as algos\nfrom pandas.core.base import PandasObject, SelectionMixin\nimport pandas.core.common as com\nfrom pandas.core.construction import create_series_with_explicit_dtype\nfrom pandas.core.flags import Flags\nfrom pandas.core.indexes import base as ibase\nfrom pandas.core.indexes.api import (\n    DatetimeIndex,\n    Index,\n    MultiIndex,\n    PeriodIndex,\n    RangeIndex,\n    ensure_index,\n)\nfrom pandas.core.internals import BlockManager\nfrom pandas.core.missing import find_valid_index\nfrom pandas.core.ops import align_method_FRAME\nfrom pandas.core.shared_docs import _shared_docs\nfrom pandas.core.sorting import get_indexer_indexer\nfrom pandas.core.window import Expanding, ExponentialMovingWindow, Rolling, Window\n\nfrom pandas.io.formats import format as fmt\nfrom pandas.io.formats.format import (\n    DataFrameFormatter,\n    DataFrameRenderer,\n    format_percentiles,\n)\nfrom pandas.io.formats.printing import pprint_thing\n\nif TYPE_CHECKING:\n    from pandas._libs.tslibs import BaseOffset\n\n    from pandas.core.frame import DataFrame\n    from pandas.core.resample import Resampler\n    from pandas.core.series import Series\n    from pandas.core.window.indexers import BaseIndexer\n\n# goal is to be able to define the docs close to function, while still being\n# able to share\n_shared_docs = {**_shared_docs}\n_shared_doc_kwargs = dict(\n    axes=\"keywords for axes\",\n    klass=\"Series/DataFrame\",\n    axes_single_arg=\"int or labels for object\",\n    args_transpose=\"axes to permute (int or label for object)\",\n    optional_by=\"\"\"\n        by : str or list of str\n            Name or list of names to sort by\"\"\",\n)\n\n\ndef _single_replace(self: \"Series\", to_replace, method, inplace, limit):\n    \"\"\"\n    Replaces values in a Series using the fill method specified when no\n    replacement value is given in the replace method\n    \"\"\"\n    if self.ndim != 1:\n        raise TypeError(\n            f\"cannot replace {to_replace} with method {method} on a \"\n            f\"{type(self).__name__}\"\n        )\n\n    orig_dtype = self.dtype\n    result = self if inplace else self.copy()\n    fill_f = missing.get_fill_func(method)\n\n    mask = missing.mask_missing(result.values, to_replace)\n    values = fill_f(result.values, limit=limit, mask=mask)\n\n    if values.dtype == orig_dtype and inplace:\n        return\n\n    result = pd.Series(values, index=self.index, dtype=self.dtype).__finalize__(self)\n\n    if inplace:\n        self._update_inplace(result)\n        return\n\n    return result\n\n\nbool_t = bool  # Need alias because NDFrame has def bool:\n\n\nclass NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n    \"\"\"\n    N-dimensional analogue of DataFrame. Store multi-dimensional in a\n    size-mutable, labeled data structure\n\n    Parameters\n    ----------\n    data : BlockManager\n    axes : list\n    copy : bool, default False\n    \"\"\"\n\n    _internal_names: List[str] = [\n        \"_mgr\",\n        \"_cacher\",\n        \"_item_cache\",\n        \"_cache\",\n        \"_is_copy\",\n        \"_subtyp\",\n        \"_name\",\n        \"_index\",\n        \"_default_kind\",\n        \"_default_fill_value\",\n        \"_metadata\",\n        \"__array_struct__\",\n        \"__array_interface__\",\n        \"_flags\",\n    ]\n    _internal_names_set: Set[str] = set(_internal_names)\n    _accessors: Set[str] = set()\n    _deprecations: FrozenSet[str] = frozenset([\"get_values\", \"tshift\"])\n    _metadata: List[str] = []\n    _is_copy = None\n    _mgr: BlockManager\n    _attrs: Dict[Optional[Hashable], Any]\n    _typ: str\n\n    # ----------------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self,\n        data: BlockManager,\n        copy: bool = False,\n        attrs: Optional[Mapping[Optional[Hashable], Any]] = None,\n    ):\n        # copy kwarg is retained for mypy compat, is not used\n\n        object.__setattr__(self, \"_is_copy\", None)\n        object.__setattr__(self, \"_mgr\", data)\n        object.__setattr__(self, \"_item_cache\", {})\n        if attrs is None:\n            attrs = {}\n        else:\n            attrs = dict(attrs)\n        object.__setattr__(self, \"_attrs\", attrs)\n        object.__setattr__(self, \"_flags\", Flags(self, allows_duplicate_labels=True))\n\n    @classmethod\n    def _init_mgr(cls, mgr, axes, dtype=None, copy: bool = False) -> BlockManager:\n        \"\"\" passed a manager and a axes dict \"\"\"\n        for a, axe in axes.items():\n            if axe is not None:\n                axe = ensure_index(axe)\n                bm_axis = cls._get_block_manager_axis(a)\n                mgr = mgr.reindex_axis(axe, axis=bm_axis, copy=False)\n\n        # make a copy if explicitly requested\n        if copy:\n            mgr = mgr.copy()\n        if dtype is not None:\n            # avoid further copies if we can\n            if len(mgr.blocks) > 1 or mgr.blocks[0].values.dtype != dtype:\n                mgr = mgr.astype(dtype=dtype)\n        return mgr\n\n    # ----------------------------------------------------------------------\n    # attrs and flags\n\n    @property\n    def attrs(self) -> Dict[Optional[Hashable], Any]:\n        \"\"\"\n        Dictionary of global attributes of this dataset.\n\n        .. warning::\n\n           attrs is experimental and may change without warning.\n\n        See Also\n        --------\n        DataFrame.flags : Global flags applying to this object.\n        \"\"\"\n        if self._attrs is None:\n            self._attrs = {}\n        return self._attrs\n\n    @attrs.setter\n    def attrs(self, value: Mapping[Optional[Hashable], Any]) -> None:\n        self._attrs = dict(value)\n\n    @property\n    def flags(self) -> Flags:\n        \"\"\"\n        Get the properties associated with this pandas object.\n\n        The available flags are\n\n        * :attr:`Flags.allows_duplicate_labels`\n\n        See Also\n        --------\n        Flags : Flags that apply to pandas objects.\n        DataFrame.attrs : Global metadata applying to this dataset.\n\n        Notes\n        -----\n        \"Flags\" differ from \"metadata\". Flags reflect properties of the\n        pandas object (the Series or DataFrame). Metadata refer to properties\n        of the dataset, and should be stored in :attr:`DataFrame.attrs`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [1, 2]})\n        >>> df.flags\n        <Flags(allows_duplicate_labels=True)>\n\n        Flags can be get or set using ``.``\n\n        >>> df.flags.allows_duplicate_labels\n        True\n        >>> df.flags.allows_duplicate_labels = False\n\n        Or by slicing with a key\n\n        >>> df.flags[\"allows_duplicate_labels\"]\n        False\n        >>> df.flags[\"allows_duplicate_labels\"] = True\n        \"\"\"\n        return self._flags\n\n    def set_flags(\n        self: FrameOrSeries,\n        *,\n        copy: bool = False,\n        allows_duplicate_labels: Optional[bool] = None,\n    ) -> FrameOrSeries:\n        \"\"\"\n        Return a new object with updated flags.\n\n        Parameters\n        ----------\n        allows_duplicate_labels : bool, optional\n            Whether the returned object allows duplicate labels.\n\n        Returns\n        -------\n        Series or DataFrame\n            The same type as the caller.\n\n        See Also\n        --------\n        DataFrame.attrs : Global metadata applying to this dataset.\n        DataFrame.flags : Global flags applying to this object.\n\n        Notes\n        -----\n        This method returns a new object that's a view on the same data\n        as the input. Mutating the input or the output values will be reflected\n        in the other.\n\n        This method is intended to be used in method chains.\n\n        \"Flags\" differ from \"metadata\". Flags reflect properties of the\n        pandas object (the Series or DataFrame). Metadata refer to properties\n        of the dataset, and should be stored in :attr:`DataFrame.attrs`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [1, 2]})\n        >>> df.flags.allows_duplicate_labels\n        True\n        >>> df2 = df.set_flags(allows_duplicate_labels=False)\n        >>> df2.flags.allows_duplicate_labels\n        False\n        \"\"\"\n        df = self.copy(deep=copy)\n        if allows_duplicate_labels is not None:\n            df.flags[\"allows_duplicate_labels\"] = allows_duplicate_labels\n        return df\n\n    @classmethod\n    def _validate_dtype(cls, dtype):\n        \"\"\" validate the passed dtype \"\"\"\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n\n            # a compound dtype\n            if dtype.kind == \"V\":\n                raise NotImplementedError(\n                    \"compound dtypes are not implemented \"\n                    f\"in the {cls.__name__} constructor\"\n                )\n\n        return dtype\n\n    # ----------------------------------------------------------------------\n    # Construction\n\n    @property\n    def _constructor(self: FrameOrSeries) -> Type[FrameOrSeries]:\n        \"\"\"\n        Used when a manipulation result has the same dimensions as the\n        original.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def _constructor_sliced(self):\n        \"\"\"\n        Used when a manipulation result has one lower dimension(s) as the\n        original, such as DataFrame single columns slicing.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def _constructor_expanddim(self):\n        \"\"\"\n        Used when a manipulation result has one higher dimension as the\n        original, such as Series.to_frame()\n        \"\"\"\n        raise NotImplementedError\n\n    # ----------------------------------------------------------------------\n    # Internals\n\n    @property\n    def _data(self):\n        # GH#33054 retained because some downstream packages uses this,\n        #  e.g. fastparquet\n        return self._mgr\n\n    # ----------------------------------------------------------------------\n    # Axis\n    _stat_axis_number = 0\n    _stat_axis_name = \"index\"\n    _ix = None\n    _AXIS_ORDERS: List[str]\n    _AXIS_TO_AXIS_NUMBER: Dict[Axis, int] = {0: 0, \"index\": 0, \"rows\": 0}\n    _AXIS_REVERSED: bool\n    _info_axis_number: int\n    _info_axis_name: str\n    _AXIS_LEN: int\n\n    @property\n    def _AXIS_NUMBERS(self) -> Dict[str, int]:\n        \"\"\".. deprecated:: 1.1.0\"\"\"\n        warnings.warn(\"_AXIS_NUMBERS has been deprecated.\", FutureWarning, stacklevel=3)\n        return {\"index\": 0}\n\n    @property\n    def _AXIS_NAMES(self) -> Dict[int, str]:\n        \"\"\".. deprecated:: 1.1.0\"\"\"\n        warnings.warn(\"_AXIS_NAMES has been deprecated.\", FutureWarning, stacklevel=3)\n        return {0: \"index\"}\n\n    def _construct_axes_dict(self, axes=None, **kwargs):\n        \"\"\"Return an axes dictionary for myself.\"\"\"\n        d = {a: self._get_axis(a) for a in (axes or self._AXIS_ORDERS)}\n        d.update(kwargs)\n        return d\n\n    @classmethod\n    def _construct_axes_from_arguments(\n        cls, args, kwargs, require_all: bool = False, sentinel=None\n    ):\n        \"\"\"\n        Construct and returns axes if supplied in args/kwargs.\n\n        If require_all, raise if all axis arguments are not supplied\n        return a tuple of (axes, kwargs).\n\n        sentinel specifies the default parameter when an axis is not\n        supplied; useful to distinguish when a user explicitly passes None\n        in scenarios where None has special meaning.\n        \"\"\"\n        # construct the args\n        args = list(args)\n        for a in cls._AXIS_ORDERS:\n\n            # look for a argument by position\n            if a not in kwargs:\n                try:\n                    kwargs[a] = args.pop(0)\n                except IndexError as err:\n                    if require_all:\n                        raise TypeError(\n                            \"not enough/duplicate arguments specified!\"\n                        ) from err\n\n        axes = {a: kwargs.pop(a, sentinel) for a in cls._AXIS_ORDERS}\n        return axes, kwargs\n\n    @classmethod\n    def _get_axis_number(cls, axis: Axis) -> int:\n        try:\n            return cls._AXIS_TO_AXIS_NUMBER[axis]\n        except KeyError:\n            raise ValueError(f\"No axis named {axis} for object type {cls.__name__}\")\n\n    @classmethod\n    def _get_axis_name(cls, axis: Axis) -> str:\n        axis_number = cls._get_axis_number(axis)\n        return cls._AXIS_ORDERS[axis_number]\n\n    def _get_axis(self, axis: Axis) -> Index:\n        axis_number = self._get_axis_number(axis)\n        assert axis_number in {0, 1}\n        return self.index if axis_number == 0 else self.columns\n\n    @classmethod\n    def _get_block_manager_axis(cls, axis: Axis) -> int:\n        \"\"\"Map the axis to the block_manager axis.\"\"\"\n        axis = cls._get_axis_number(axis)\n        if cls._AXIS_REVERSED:\n            m = cls._AXIS_LEN - 1\n            return m - axis\n        return axis\n\n    def _get_axis_resolvers(self, axis: str) -> Dict[str, Union[Series, MultiIndex]]:\n        # index or columns\n        axis_index = getattr(self, axis)\n        d = dict()\n        prefix = axis[0]\n\n        for i, name in enumerate(axis_index.names):\n            if name is not None:\n                key = level = name\n            else:\n                # prefix with 'i' or 'c' depending on the input axis\n                # e.g., you must do ilevel_0 for the 0th level of an unnamed\n                # multiiindex\n                key = f\"{prefix}level_{i}\"\n                level = i\n\n            level_values = axis_index.get_level_values(level)\n            s = level_values.to_series()\n            s.index = axis_index\n            d[key] = s\n\n        # put the index/columns itself in the dict\n        if isinstance(axis_index, MultiIndex):\n            dindex = axis_index\n        else:\n            dindex = axis_index.to_series()\n\n        d[axis] = dindex\n        return d\n\n    def _get_index_resolvers(self) -> Dict[str, Union[Series, MultiIndex]]:\n        from pandas.core.computation.parsing import clean_column_name\n\n        d: Dict[str, Union[Series, MultiIndex]] = {}\n        for axis_name in self._AXIS_ORDERS:\n            d.update(self._get_axis_resolvers(axis_name))\n\n        return {clean_column_name(k): v for k, v in d.items() if not isinstance(k, int)}\n\n    def _get_cleaned_column_resolvers(self) -> Dict[str, ABCSeries]:\n        \"\"\"\n        Return the special character free column resolvers of a dataframe.\n\n        Column names with special characters are 'cleaned up' so that they can\n        be referred to by backtick quoting.\n        Used in :meth:`DataFrame.eval`.\n        \"\"\"\n        from pandas.core.computation.parsing import clean_column_name\n\n        if isinstance(self, ABCSeries):\n            self = cast(\"Series\", self)\n            return {clean_column_name(self.name): self}\n\n        return {\n            clean_column_name(k): v for k, v in self.items() if not isinstance(k, int)\n        }\n\n    @property\n    def _info_axis(self) -> Index:\n        return getattr(self, self._info_axis_name)\n\n    @property\n    def _stat_axis(self) -> Index:\n        return getattr(self, self._stat_axis_name)\n\n    @property\n    def shape(self) -> Tuple[int, ...]:\n        \"\"\"\n        Return a tuple of axis dimensions\n        \"\"\"\n        return tuple(len(self._get_axis(a)) for a in self._AXIS_ORDERS)\n\n    @property\n    def axes(self) -> List[Index]:\n        \"\"\"\n        Return index label(s) of the internal NDFrame\n        \"\"\"\n        # we do it this way because if we have reversed axes, then\n        # the block manager shows then reversed\n        return [self._get_axis(a) for a in self._AXIS_ORDERS]\n\n    @property\n    def ndim(self) -> int:\n        \"\"\"\n        Return an int representing the number of axes / array dimensions.\n\n        Return 1 if Series. Otherwise return 2 if DataFrame.\n\n        See Also\n        --------\n        ndarray.ndim : Number of array dimensions.\n\n        Examples\n        --------\n        >>> s = pd.Series({'a': 1, 'b': 2, 'c': 3})\n        >>> s.ndim\n        1\n\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.ndim\n        2\n        \"\"\"\n        return self._mgr.ndim\n\n    @property\n    def size(self) -> int:\n        \"\"\"\n        Return an int representing the number of elements in this object.\n\n        Return the number of rows if Series. Otherwise return the number of\n        rows times number of columns if DataFrame.\n\n        See Also\n        --------\n        ndarray.size : Number of elements in the array.\n\n        Examples\n        --------\n        >>> s = pd.Series({'a': 1, 'b': 2, 'c': 3})\n        >>> s.size\n        3\n\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.size\n        4\n        \"\"\"\n        return np.prod(self.shape)\n\n    @property\n    def _selected_obj(self: FrameOrSeries) -> FrameOrSeries:\n        \"\"\" internal compat with SelectionMixin \"\"\"\n        return self\n\n    @property\n    def _obj_with_exclusions(self: FrameOrSeries) -> FrameOrSeries:\n        \"\"\" internal compat with SelectionMixin \"\"\"\n        return self\n\n    def set_axis(self, labels, axis: Axis = 0, inplace: bool = False):\n        \"\"\"\n        Assign desired index to given axis.\n\n        Indexes for%(extended_summary_sub)s row labels can be changed by assigning\n        a list-like or Index.\n\n        Parameters\n        ----------\n        labels : list-like, Index\n            The values for the new index.\n\n        axis : %(axes_single_arg)s, default 0\n            The axis to update. The value 0 identifies the rows%(axis_description_sub)s.\n\n        inplace : bool, default False\n            Whether to return a new %(klass)s instance.\n\n        Returns\n        -------\n        renamed : %(klass)s or None\n            An object of type %(klass)s or None if ``inplace=True``.\n\n        See Also\n        --------\n        %(klass)s.rename_axis : Alter the name of the index%(see_also_sub)s.\n        \"\"\"\n        self._check_inplace_and_allows_duplicate_labels(inplace)\n        return self._set_axis_nocheck(labels, axis, inplace)\n\n    def _set_axis_nocheck(self, labels, axis: Axis, inplace: bool):\n        # NDFrame.rename with inplace=False calls set_axis(inplace=True) on a copy.\n        if inplace:\n            setattr(self, self._get_axis_name(axis), labels)\n        else:\n            obj = self.copy()\n            obj.set_axis(labels, axis=axis, inplace=True)\n            return obj\n\n    def _set_axis(self, axis: int, labels: Index) -> None:\n        labels = ensure_index(labels)\n        self._mgr.set_axis(axis, labels)\n        self._clear_item_cache()\n\n    def swapaxes(self: FrameOrSeries, axis1, axis2, copy=True) -> FrameOrSeries:\n        \"\"\"\n        Interchange axes and swap values axes appropriately.\n\n        Returns\n        -------\n        y : same as input\n        \"\"\"\n        i = self._get_axis_number(axis1)\n        j = self._get_axis_number(axis2)\n\n        if i == j:\n            if copy:\n                return self.copy()\n            return self\n\n        mapping = {i: j, j: i}\n\n        new_axes = (self._get_axis(mapping.get(k, k)) for k in range(self._AXIS_LEN))\n        new_values = self.values.swapaxes(i, j)\n        if copy:\n            new_values = new_values.copy()\n\n        # ignore needed because of NDFrame constructor is different than\n        # DataFrame/Series constructors.\n        return self._constructor(\n            new_values, *new_axes  # type: ignore[arg-type]\n        ).__finalize__(self, method=\"swapaxes\")\n\n    def droplevel(self: FrameOrSeries, level, axis=0) -> FrameOrSeries:\n        \"\"\"\n        Return DataFrame with requested index / column level(s) removed.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        level : int, str, or list-like\n            If a string is given, must be the name of a level\n            If list-like, elements must be names or positional indexes\n            of levels.\n\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the level(s) is removed:\n\n            * 0 or 'index': remove level(s) in column.\n            * 1 or 'columns': remove level(s) in row.\n\n        Returns\n        -------\n        DataFrame\n            DataFrame with requested index / column level(s) removed.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([\n        ...     [1, 2, 3, 4],\n        ...     [5, 6, 7, 8],\n        ...     [9, 10, 11, 12]\n        ... ]).set_index([0, 1]).rename_axis(['a', 'b'])\n\n        >>> df.columns = pd.MultiIndex.from_tuples([\n        ...     ('c', 'e'), ('d', 'f')\n        ... ], names=['level_1', 'level_2'])\n\n        >>> df\n        level_1   c   d\n        level_2   e   f\n        a b\n        1 2      3   4\n        5 6      7   8\n        9 10    11  12\n\n        >>> df.droplevel('a')\n        level_1   c   d\n        level_2   e   f\n        b\n        2        3   4\n        6        7   8\n        10      11  12\n\n        >>> df.droplevel('level_2', axis=1)\n        level_1   c   d\n        a b\n        1 2      3   4\n        5 6      7   8\n        9 10    11  12\n        \"\"\"\n        labels = self._get_axis(axis)\n        new_labels = labels.droplevel(level)\n        result = self.set_axis(new_labels, axis=axis, inplace=False)\n        return result\n\n    def pop(self, item: Label) -> Union[Series, Any]:\n        result = self[item]\n        del self[item]\n        if self.ndim == 2:\n            result._reset_cacher()\n\n        return result\n\n    def squeeze(self, axis=None):\n        \"\"\"\n        Squeeze 1 dimensional axis objects into scalars.\n\n        Series or DataFrames with a single element are squeezed to a scalar.\n        DataFrames with a single column or a single row are squeezed to a\n        Series. Otherwise the object is unchanged.\n\n        This method is most useful when you don't know if your\n        object is a Series or DataFrame, but you do know it has just a single\n        column. In that case you can safely call `squeeze` to ensure you have a\n        Series.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns', None}, default None\n            A specific axis to squeeze. By default, all length-1 axes are\n            squeezed.\n\n        Returns\n        -------\n        DataFrame, Series, or scalar\n            The projection after squeezing `axis` or all the axes.\n\n        See Also\n        --------\n        Series.iloc : Integer-location based indexing for selecting scalars.\n        DataFrame.iloc : Integer-location based indexing for selecting Series.\n        Series.to_frame : Inverse of DataFrame.squeeze for a\n            single-column DataFrame.\n\n        Examples\n        --------\n        >>> primes = pd.Series([2, 3, 5, 7])\n\n        Slicing might produce a Series with a single value:\n\n        >>> even_primes = primes[primes % 2 == 0]\n        >>> even_primes\n        0    2\n        dtype: int64\n\n        >>> even_primes.squeeze()\n        2\n\n        Squeezing objects with more than one value in every axis does nothing:\n\n        >>> odd_primes = primes[primes % 2 == 1]\n        >>> odd_primes\n        1    3\n        2    5\n        3    7\n        dtype: int64\n\n        >>> odd_primes.squeeze()\n        1    3\n        2    5\n        3    7\n        dtype: int64\n\n        Squeezing is even more effective when used with DataFrames.\n\n        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['a', 'b'])\n        >>> df\n           a  b\n        0  1  2\n        1  3  4\n\n        Slicing a single column will produce a DataFrame with the columns\n        having only one value:\n\n        >>> df_a = df[['a']]\n        >>> df_a\n           a\n        0  1\n        1  3\n\n        So the columns can be squeezed down, resulting in a Series:\n\n        >>> df_a.squeeze('columns')\n        0    1\n        1    3\n        Name: a, dtype: int64\n\n        Slicing a single row from a single column will produce a single\n        scalar DataFrame:\n\n        >>> df_0a = df.loc[df.index < 1, ['a']]\n        >>> df_0a\n           a\n        0  1\n\n        Squeezing the rows produces a single scalar Series:\n\n        >>> df_0a.squeeze('rows')\n        a    1\n        Name: 0, dtype: int64\n\n        Squeezing all axes will project directly into a scalar:\n\n        >>> df_0a.squeeze()\n        1\n        \"\"\"\n        axis = range(self._AXIS_LEN) if axis is None else (self._get_axis_number(axis),)\n        return self.iloc[\n            tuple(\n                0 if i in axis and len(a) == 1 else slice(None)\n                for i, a in enumerate(self.axes)\n            )\n        ]\n\n    # ----------------------------------------------------------------------\n    # Rename\n\n    def rename(\n        self: FrameOrSeries,\n        mapper: Optional[Renamer] = None,\n        *,\n        index: Optional[Renamer] = None,\n        columns: Optional[Renamer] = None,\n        axis: Optional[Axis] = None,\n        copy: bool = True,\n        inplace: bool = False,\n        level: Optional[Level] = None,\n        errors: str = \"ignore\",\n    ) -> Optional[FrameOrSeries]:\n        \"\"\"\n        Alter axes input function or functions. Function / dict values must be\n        unique (1-to-1). Labels not contained in a dict / Series will be left\n        as-is. Extra labels listed don't throw an error. Alternatively, change\n        ``Series.name`` with a scalar value (Series only).\n\n        Parameters\n        ----------\n        %(axes)s : scalar, list-like, dict-like or function, optional\n            Scalar or list-like will alter the ``Series.name`` attribute,\n            and raise on DataFrame.\n            dict-like or functions are transformations to apply to\n            that axis' values\n        copy : bool, default True\n            Also copy underlying data.\n        inplace : bool, default False\n            Whether to return a new {klass}. If True then value of copy is\n            ignored.\n        level : int or level name, default None\n            In case of a MultiIndex, only rename labels in the specified\n            level.\n        errors : {'ignore', 'raise'}, default 'ignore'\n            If 'raise', raise a `KeyError` when a dict-like `mapper`, `index`,\n            or `columns` contains labels that are not present in the Index\n            being transformed.\n            If 'ignore', existing keys will be renamed and extra keys will be\n            ignored.\n\n        Returns\n        -------\n        renamed : {klass} (new object)\n\n        Raises\n        ------\n        KeyError\n            If any of the labels is not found in the selected axis and\n            \"errors='raise'\".\n\n        See Also\n        --------\n        NDFrame.rename_axis\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n        >>> s.rename(\"my_name\") # scalar, changes Series.name\n        0    1\n        1    2\n        2    3\n        Name: my_name, dtype: int64\n        >>> s.rename(lambda x: x ** 2)  # function, changes labels\n        0    1\n        1    2\n        4    3\n        dtype: int64\n        >>> s.rename({1: 3, 2: 5})  # mapping, changes labels\n        0    1\n        3    2\n        5    3\n        dtype: int64\n\n        Since ``DataFrame`` doesn't have a ``.name`` attribute,\n        only mapping-type arguments are allowed.\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n        >>> df.rename(2)\n        Traceback (most recent call last):\n        ...\n        TypeError: 'int' object is not callable\n\n        ``DataFrame.rename`` supports two calling conventions\n\n        * ``(index=index_mapper, columns=columns_mapper, ...)``\n        * ``(mapper, axis={'index', 'columns'}, ...)``\n\n        We *highly* recommend using keyword arguments to clarify your\n        intent.\n\n        >>> df.rename(index=str, columns={\"A\": \"a\", \"B\": \"c\"})\n           a  c\n        0  1  4\n        1  2  5\n        2  3  6\n\n        >>> df.rename(index=str, columns={\"A\": \"a\", \"C\": \"c\"})\n           a  B\n        0  1  4\n        1  2  5\n        2  3  6\n\n        Using axis-style parameters\n\n        >>> df.rename(str.lower, axis='columns')\n           a  b\n        0  1  4\n        1  2  5\n        2  3  6\n\n        >>> df.rename({1: 2, 2: 4}, axis='index')\n           A  B\n        0  1  4\n        2  2  5\n        4  3  6\n\n        See the :ref:`user guide <basics.rename>` for more.\n        \"\"\"\n        if mapper is None and index is None and columns is None:\n            raise TypeError(\"must pass an index to rename\")\n\n        if index is not None or columns is not None:\n            if axis is not None:\n                raise TypeError(\n                    \"Cannot specify both 'axis' and any of 'index' or 'columns'\"\n                )\n            elif mapper is not None:\n                raise TypeError(\n                    \"Cannot specify both 'mapper' and any of 'index' or 'columns'\"\n                )\n        else:\n            # use the mapper argument\n            if axis and self._get_axis_number(axis) == 1:\n                columns = mapper\n            else:\n                index = mapper\n\n        self._check_inplace_and_allows_duplicate_labels(inplace)\n        result = self if inplace else self.copy(deep=copy)\n\n        for axis_no, replacements in enumerate((index, columns)):\n            if replacements is None:\n                continue\n\n            ax = self._get_axis(axis_no)\n            f = com.get_rename_function(replacements)\n\n            if level is not None:\n                level = ax._get_level_number(level)\n\n            # GH 13473\n            if not callable(replacements):\n                indexer = ax.get_indexer_for(replacements)\n                if errors == \"raise\" and len(indexer[indexer == -1]):\n                    missing_labels = [\n                        label\n                        for index, label in enumerate(replacements)\n                        if indexer[index] == -1\n                    ]\n                    raise KeyError(f\"{missing_labels} not found in axis\")\n\n            new_index = ax._transform_index(f, level)\n            result._set_axis_nocheck(new_index, axis=axis_no, inplace=True)\n            result._clear_item_cache()\n\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result.__finalize__(self, method=\"rename\")\n\n    @rewrite_axis_style_signature(\"mapper\", [(\"copy\", True), (\"inplace\", False)])\n    def rename_axis(self, mapper=lib.no_default, **kwargs):\n        \"\"\"\n        Set the name of the axis for the index or columns.\n\n        Parameters\n        ----------\n        mapper : scalar, list-like, optional\n            Value to set the axis name attribute.\n        index, columns : scalar, list-like, dict-like or function, optional\n            A scalar, list-like, dict-like or functions transformations to\n            apply to that axis' values.\n            Note that the ``columns`` parameter is not allowed if the\n            object is a Series. This parameter only apply for DataFrame\n            type objects.\n\n            Use either ``mapper`` and ``axis`` to\n            specify the axis to target with ``mapper``, or ``index``\n            and/or ``columns``.\n\n            .. versionchanged:: 0.24.0\n\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to rename.\n        copy : bool, default True\n            Also copy underlying data.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Series\n            or DataFrame.\n\n        Returns\n        -------\n        Series, DataFrame, or None\n            The same type as the caller or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.rename : Alter Series index labels or name.\n        DataFrame.rename : Alter DataFrame index labels or name.\n        Index.rename : Set new names on index.\n\n        Notes\n        -----\n        ``DataFrame.rename_axis`` supports two calling conventions\n\n        * ``(index=index_mapper, columns=columns_mapper, ...)``\n        * ``(mapper, axis={'index', 'columns'}, ...)``\n\n        The first calling convention will only modify the names of\n        the index and/or the names of the Index object that is the columns.\n        In this case, the parameter ``copy`` is ignored.\n\n        The second calling convention will modify the names of the\n        the corresponding index if mapper is a list or a scalar.\n        However, if mapper is dict-like or a function, it will use the\n        deprecated behavior of modifying the axis *labels*.\n\n        We *highly* recommend using keyword arguments to clarify your\n        intent.\n\n        Examples\n        --------\n        **Series**\n\n        >>> s = pd.Series([\"dog\", \"cat\", \"monkey\"])\n        >>> s\n        0       dog\n        1       cat\n        2    monkey\n        dtype: object\n        >>> s.rename_axis(\"animal\")\n        animal\n        0    dog\n        1    cat\n        2    monkey\n        dtype: object\n\n        **DataFrame**\n\n        >>> df = pd.DataFrame({\"num_legs\": [4, 4, 2],\n        ...                    \"num_arms\": [0, 0, 2]},\n        ...                   [\"dog\", \"cat\", \"monkey\"])\n        >>> df\n                num_legs  num_arms\n        dog            4         0\n        cat            4         0\n        monkey         2         2\n        >>> df = df.rename_axis(\"animal\")\n        >>> df\n                num_legs  num_arms\n        animal\n        dog            4         0\n        cat            4         0\n        monkey         2         2\n        >>> df = df.rename_axis(\"limbs\", axis=\"columns\")\n        >>> df\n        limbs   num_legs  num_arms\n        animal\n        dog            4         0\n        cat            4         0\n        monkey         2         2\n\n        **MultiIndex**\n\n        >>> df.index = pd.MultiIndex.from_product([['mammal'],\n        ...                                        ['dog', 'cat', 'monkey']],\n        ...                                       names=['type', 'name'])\n        >>> df\n        limbs          num_legs  num_arms\n        type   name\n        mammal dog            4         0\n               cat            4         0\n               monkey         2         2\n\n        >>> df.rename_axis(index={'type': 'class'})\n        limbs          num_legs  num_arms\n        class  name\n        mammal dog            4         0\n               cat            4         0\n               monkey         2         2\n\n        >>> df.rename_axis(columns=str.upper)\n        LIMBS          num_legs  num_arms\n        type   name\n        mammal dog            4         0\n               cat            4         0\n               monkey         2         2\n        \"\"\"\n        axes, kwargs = self._construct_axes_from_arguments(\n            (), kwargs, sentinel=lib.no_default\n        )\n        copy = kwargs.pop(\"copy\", True)\n        inplace = kwargs.pop(\"inplace\", False)\n        axis = kwargs.pop(\"axis\", 0)\n        if axis is not None:\n            axis = self._get_axis_number(axis)\n\n        if kwargs:\n            raise TypeError(\n                \"rename_axis() got an unexpected keyword \"\n                f'argument \"{list(kwargs.keys())[0]}\"'\n            )\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        if mapper is not lib.no_default:\n            # Use v0.23 behavior if a scalar or list\n            non_mapper = is_scalar(mapper) or (\n                is_list_like(mapper) and not is_dict_like(mapper)\n            )\n            if non_mapper:\n                return self._set_axis_name(mapper, axis=axis, inplace=inplace)\n            else:\n                raise ValueError(\"Use `.rename` to alter labels with a mapper.\")\n        else:\n            # Use new behavior.  Means that index and/or columns\n            # is specified\n            result = self if inplace else self.copy(deep=copy)\n\n            for axis in range(self._AXIS_LEN):\n                v = axes.get(self._get_axis_name(axis))\n                if v is lib.no_default:\n                    continue\n                non_mapper = is_scalar(v) or (is_list_like(v) and not is_dict_like(v))\n                if non_mapper:\n                    newnames = v\n                else:\n                    f = com.get_rename_function(v)\n                    curnames = self._get_axis(axis).names\n                    newnames = [f(name) for name in curnames]\n                result._set_axis_name(newnames, axis=axis, inplace=True)\n            if not inplace:\n                return result\n\n    def _set_axis_name(self, name, axis=0, inplace=False):\n        \"\"\"\n        Set the name(s) of the axis.\n\n        Parameters\n        ----------\n        name : str or list of str\n            Name(s) to set.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to set the label. The value 0 or 'index' specifies index,\n            and the value 1 or 'columns' specifies columns.\n        inplace : bool, default False\n            If `True`, do operation inplace and return None.\n\n        Returns\n        -------\n        Series, DataFrame, or None\n            The same type as the caller or `None` if `inplace` is `True`.\n\n        See Also\n        --------\n        DataFrame.rename : Alter the axis labels of :class:`DataFrame`.\n        Series.rename : Alter the index labels or set the index name\n            of :class:`Series`.\n        Index.rename : Set the name of :class:`Index` or :class:`MultiIndex`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"num_legs\": [4, 4, 2]},\n        ...                   [\"dog\", \"cat\", \"monkey\"])\n        >>> df\n                num_legs\n        dog            4\n        cat            4\n        monkey         2\n        >>> df._set_axis_name(\"animal\")\n                num_legs\n        animal\n        dog            4\n        cat            4\n        monkey         2\n        >>> df.index = pd.MultiIndex.from_product(\n        ...                [[\"mammal\"], ['dog', 'cat', 'monkey']])\n        >>> df._set_axis_name([\"type\", \"name\"])\n                       num_legs\n        type   name\n        mammal dog        4\n               cat        4\n               monkey     2\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        idx = self._get_axis(axis).set_names(name)\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        renamed = self if inplace else self.copy()\n        renamed.set_axis(idx, axis=axis, inplace=True)\n        if not inplace:\n            return renamed\n\n    # ----------------------------------------------------------------------\n    # Comparison Methods\n\n    def _indexed_same(self, other) -> bool:\n        return all(\n            self._get_axis(a).equals(other._get_axis(a)) for a in self._AXIS_ORDERS\n        )\n\n    def equals(self, other: object) -> bool:\n        \"\"\"\n        Test whether two objects contain the same elements.\n\n        This function allows two Series or DataFrames to be compared against\n        each other to see if they have the same shape and elements. NaNs in\n        the same location are considered equal.\n\n        The row/column index do not need to have the same type, as long\n        as the values are considered equal. Corresponding columns must be of\n        the same dtype.\n\n        Parameters\n        ----------\n        other : Series or DataFrame\n            The other Series or DataFrame to be compared with the first.\n\n        Returns\n        -------\n        bool\n            True if all elements are the same in both objects, False\n            otherwise.\n\n        See Also\n        --------\n        Series.eq : Compare two Series objects of the same length\n            and return a Series where each element is True if the element\n            in each Series is equal, False otherwise.\n        DataFrame.eq : Compare two DataFrame objects of the same shape and\n            return a DataFrame where each element is True if the respective\n            element in each DataFrame is equal, False otherwise.\n        testing.assert_series_equal : Raises an AssertionError if left and\n            right are not equal. Provides an easy interface to ignore\n            inequality in dtypes, indexes and precision among others.\n        testing.assert_frame_equal : Like assert_series_equal, but targets\n            DataFrames.\n        numpy.array_equal : Return True if two arrays have the same shape\n            and elements, False otherwise.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({1: [10], 2: [20]})\n        >>> df\n            1   2\n        0  10  20\n\n        DataFrames df and exactly_equal have the same types and values for\n        their elements and column labels, which will return True.\n\n        >>> exactly_equal = pd.DataFrame({1: [10], 2: [20]})\n        >>> exactly_equal\n            1   2\n        0  10  20\n        >>> df.equals(exactly_equal)\n        True\n\n        DataFrames df and different_column_type have the same element\n        types and values, but have different types for the column labels,\n        which will still return True.\n\n        >>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]})\n        >>> different_column_type\n           1.0  2.0\n        0   10   20\n        >>> df.equals(different_column_type)\n        True\n\n        DataFrames df and different_data_type have different types for the\n        same values for their elements, and will return False even though\n        their column labels are the same values and types.\n\n        >>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]})\n        >>> different_data_type\n              1     2\n        0  10.0  20.0\n        >>> df.equals(different_data_type)\n        False\n        \"\"\"\n        if not (isinstance(other, type(self)) or isinstance(self, type(other))):\n            return False\n        other = cast(NDFrame, other)\n        return self._mgr.equals(other._mgr)\n\n    # -------------------------------------------------------------------------\n    # Unary Methods\n\n    def __neg__(self):\n        values = self._values\n        if is_bool_dtype(values):\n            arr = operator.inv(values)\n        elif (\n            is_numeric_dtype(values)\n            or is_timedelta64_dtype(values)\n            or is_object_dtype(values)\n        ):\n            arr = operator.neg(values)\n        else:\n            raise TypeError(f\"Unary negative expects numeric dtype, not {values.dtype}\")\n        return self.__array_wrap__(arr)\n\n    def __pos__(self):\n        values = self._values\n        if is_bool_dtype(values):\n            arr = values\n        elif (\n            is_numeric_dtype(values)\n            or is_timedelta64_dtype(values)\n            or is_object_dtype(values)\n        ):\n            arr = operator.pos(values)\n        else:\n            raise TypeError(\n                \"Unary plus expects bool, numeric, timedelta, \"\n                f\"or object dtype, not {values.dtype}\"\n            )\n        return self.__array_wrap__(arr)\n\n    def __invert__(self):\n        if not self.size:\n            # inv fails with 0 len\n            return self\n\n        new_data = self._mgr.apply(operator.invert)\n        result = self._constructor(new_data).__finalize__(self, method=\"__invert__\")\n        return result\n\n    def __nonzero__(self):\n        raise ValueError(\n            f\"The truth value of a {type(self).__name__} is ambiguous. \"\n            \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n        )\n\n    __bool__ = __nonzero__\n\n    def bool(self):\n        \"\"\"\n        Return the bool of a single element Series or DataFrame.\n\n        This must be a boolean scalar value, either True or False. It will raise a\n        ValueError if the Series or DataFrame does not have exactly 1 element, or that\n        element is not boolean (integer values 0 and 1 will also raise an exception).\n\n        Returns\n        -------\n        bool\n            The value in the Series or DataFrame.\n\n        See Also\n        --------\n        Series.astype : Change the data type of a Series, including to boolean.\n        DataFrame.astype : Change the data type of a DataFrame, including to boolean.\n        numpy.bool_ : NumPy boolean data type, used by pandas for boolean values.\n\n        Examples\n        --------\n        The method will only work for single element objects with a boolean value:\n\n        >>> pd.Series([True]).bool()\n        True\n        >>> pd.Series([False]).bool()\n        False\n\n        >>> pd.DataFrame({'col': [True]}).bool()\n        True\n        >>> pd.DataFrame({'col': [False]}).bool()\n        False\n        \"\"\"\n        v = self.squeeze()\n        if isinstance(v, (bool, np.bool_)):\n            return bool(v)\n        elif is_scalar(v):\n            raise ValueError(\n                \"bool cannot act on a non-boolean single element \"\n                f\"{type(self).__name__}\"\n            )\n\n        self.__nonzero__()\n\n    def __abs__(self: FrameOrSeries) -> FrameOrSeries:\n        return self.abs()\n\n    def __round__(self: FrameOrSeries, decimals: int = 0) -> FrameOrSeries:\n        return self.round(decimals)\n\n    # -------------------------------------------------------------------------\n    # Label or Level Combination Helpers\n    #\n    # A collection of helper methods for DataFrame/Series operations that\n    # accept a combination of column/index labels and levels.  All such\n    # operations should utilize/extend these methods when possible so that we\n    # have consistent precedence and validation logic throughout the library.\n\n    def _is_level_reference(self, key, axis=0):\n        \"\"\"\n        Test whether a key is a level reference for a given axis.\n\n        To be considered a level reference, `key` must be a string that:\n          - (axis=0): Matches the name of an index level and does NOT match\n            a column label.\n          - (axis=1): Matches the name of a column level and does NOT match\n            an index label.\n\n        Parameters\n        ----------\n        key : str\n            Potential level name for the given axis\n        axis : int, default 0\n            Axis that levels are associated with (0 for index, 1 for columns)\n\n        Returns\n        -------\n        is_level : bool\n        \"\"\"\n        axis = self._get_axis_number(axis)\n\n        return (\n            key is not None\n            and is_hashable(key)\n            and key in self.axes[axis].names\n            and not self._is_label_reference(key, axis=axis)\n        )\n\n    def _is_label_reference(self, key, axis=0) -> bool_t:\n        \"\"\"\n        Test whether a key is a label reference for a given axis.\n\n        To be considered a label reference, `key` must be a string that:\n          - (axis=0): Matches a column label\n          - (axis=1): Matches an index label\n\n        Parameters\n        ----------\n        key: str\n            Potential label name\n        axis: int, default 0\n            Axis perpendicular to the axis that labels are associated with\n            (0 means search for column labels, 1 means search for index labels)\n\n        Returns\n        -------\n        is_label: bool\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        other_axes = (ax for ax in range(self._AXIS_LEN) if ax != axis)\n\n        return (\n            key is not None\n            and is_hashable(key)\n            and any(key in self.axes[ax] for ax in other_axes)\n        )\n\n    def _is_label_or_level_reference(self, key: str, axis: int = 0) -> bool_t:\n        \"\"\"\n        Test whether a key is a label or level reference for a given axis.\n\n        To be considered either a label or a level reference, `key` must be a\n        string that:\n          - (axis=0): Matches a column label or an index level\n          - (axis=1): Matches an index label or a column level\n\n        Parameters\n        ----------\n        key: str\n            Potential label or level name\n        axis: int, default 0\n            Axis that levels are associated with (0 for index, 1 for columns)\n\n        Returns\n        -------\n        is_label_or_level: bool\n        \"\"\"\n        return self._is_level_reference(key, axis=axis) or self._is_label_reference(\n            key, axis=axis\n        )\n\n    def _check_label_or_level_ambiguity(self, key, axis: int = 0) -> None:\n        \"\"\"\n        Check whether `key` is ambiguous.\n\n        By ambiguous, we mean that it matches both a level of the input\n        `axis` and a label of the other axis.\n\n        Parameters\n        ----------\n        key: str or object\n            Label or level name.\n        axis: int, default 0\n            Axis that levels are associated with (0 for index, 1 for columns).\n\n        Raises\n        ------\n        ValueError: `key` is ambiguous\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        other_axes = (ax for ax in range(self._AXIS_LEN) if ax != axis)\n\n        if (\n            key is not None\n            and is_hashable(key)\n            and key in self.axes[axis].names\n            and any(key in self.axes[ax] for ax in other_axes)\n        ):\n\n            # Build an informative and grammatical warning\n            level_article, level_type = (\n                (\"an\", \"index\") if axis == 0 else (\"a\", \"column\")\n            )\n\n            label_article, label_type = (\n                (\"a\", \"column\") if axis == 0 else (\"an\", \"index\")\n            )\n\n            msg = (\n                f\"'{key}' is both {level_article} {level_type} level and \"\n                f\"{label_article} {label_type} label, which is ambiguous.\"\n            )\n            raise ValueError(msg)\n\n    def _get_label_or_level_values(self, key: str, axis: int = 0) -> np.ndarray:\n        \"\"\"\n        Return a 1-D array of values associated with `key`, a label or level\n        from the given `axis`.\n\n        Retrieval logic:\n          - (axis=0): Return column values if `key` matches a column label.\n            Otherwise return index level values if `key` matches an index\n            level.\n          - (axis=1): Return row values if `key` matches an index label.\n            Otherwise return column level values if 'key' matches a column\n            level\n\n        Parameters\n        ----------\n        key: str\n            Label or level name.\n        axis: int, default 0\n            Axis that levels are associated with (0 for index, 1 for columns)\n\n        Returns\n        -------\n        values: np.ndarray\n\n        Raises\n        ------\n        KeyError\n            if `key` matches neither a label nor a level\n        ValueError\n            if `key` matches multiple labels\n        FutureWarning\n            if `key` is ambiguous. This will become an ambiguity error in a\n            future version\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        other_axes = [ax for ax in range(self._AXIS_LEN) if ax != axis]\n\n        if self._is_label_reference(key, axis=axis):\n            self._check_label_or_level_ambiguity(key, axis=axis)\n            values = self.xs(key, axis=other_axes[0])._values\n        elif self._is_level_reference(key, axis=axis):\n            values = self.axes[axis].get_level_values(key)._values\n        else:\n            raise KeyError(key)\n\n        # Check for duplicates\n        if values.ndim > 1:\n\n            if other_axes and isinstance(self._get_axis(other_axes[0]), MultiIndex):\n                multi_message = (\n                    \"\\n\"\n                    \"For a multi-index, the label must be a \"\n                    \"tuple with elements corresponding to each level.\"\n                )\n            else:\n                multi_message = \"\"\n\n            label_axis_name = \"column\" if axis == 0 else \"index\"\n            raise ValueError(\n                f\"The {label_axis_name} label '{key}' is not unique.{multi_message}\"\n            )\n\n        return values\n\n    def _drop_labels_or_levels(self, keys, axis: int = 0):\n        \"\"\"\n        Drop labels and/or levels for the given `axis`.\n\n        For each key in `keys`:\n          - (axis=0): If key matches a column label then drop the column.\n            Otherwise if key matches an index level then drop the level.\n          - (axis=1): If key matches an index label then drop the row.\n            Otherwise if key matches a column level then drop the level.\n\n        Parameters\n        ----------\n        keys: str or list of str\n            labels or levels to drop\n        axis: int, default 0\n            Axis that levels are associated with (0 for index, 1 for columns)\n\n        Returns\n        -------\n        dropped: DataFrame\n\n        Raises\n        ------\n        ValueError\n            if any `keys` match neither a label nor a level\n        \"\"\"\n        axis = self._get_axis_number(axis)\n\n        # Validate keys\n        keys = com.maybe_make_list(keys)\n        invalid_keys = [\n            k for k in keys if not self._is_label_or_level_reference(k, axis=axis)\n        ]\n\n        if invalid_keys:\n            raise ValueError(\n                \"The following keys are not valid labels or \"\n                f\"levels for axis {axis}: {invalid_keys}\"\n            )\n\n        # Compute levels and labels to drop\n        levels_to_drop = [k for k in keys if self._is_level_reference(k, axis=axis)]\n\n        labels_to_drop = [k for k in keys if not self._is_level_reference(k, axis=axis)]\n\n        # Perform copy upfront and then use inplace operations below.\n        # This ensures that we always perform exactly one copy.\n        # ``copy`` and/or ``inplace`` options could be added in the future.\n        dropped = self.copy()\n\n        if axis == 0:\n            # Handle dropping index levels\n            if levels_to_drop:\n                dropped.reset_index(levels_to_drop, drop=True, inplace=True)\n\n            # Handle dropping columns labels\n            if labels_to_drop:\n                dropped.drop(labels_to_drop, axis=1, inplace=True)\n        else:\n            # Handle dropping column levels\n            if levels_to_drop:\n                if isinstance(dropped.columns, MultiIndex):\n                    # Drop the specified levels from the MultiIndex\n                    dropped.columns = dropped.columns.droplevel(levels_to_drop)\n                else:\n                    # Drop the last level of Index by replacing with\n                    # a RangeIndex\n                    dropped.columns = RangeIndex(dropped.columns.size)\n\n            # Handle dropping index labels\n            if labels_to_drop:\n                dropped.drop(labels_to_drop, axis=0, inplace=True)\n\n        return dropped\n\n    # ----------------------------------------------------------------------\n    # Iteration\n\n    def __hash__(self) -> int:\n        raise TypeError(\n            f\"{repr(type(self).__name__)} objects are mutable, \"\n            f\"thus they cannot be hashed\"\n        )\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over info axis.\n\n        Returns\n        -------\n        iterator\n            Info axis as iterator.\n        \"\"\"\n        return iter(self._info_axis)\n\n    # can we get a better explanation of this?\n    def keys(self):\n        \"\"\"\n        Get the 'info axis' (see Indexing for more).\n\n        This is index for Series, columns for DataFrame.\n\n        Returns\n        -------\n        Index\n            Info axis.\n        \"\"\"\n        return self._info_axis\n\n    def items(self):\n        \"\"\"\n        Iterate over (label, values) on info axis\n\n        This is index for Series and columns for DataFrame.\n\n        Returns\n        -------\n        Generator\n        \"\"\"\n        for h in self._info_axis:\n            yield h, self[h]\n\n    @doc(items)\n    def iteritems(self):\n        return self.items()\n\n    def __len__(self) -> int:\n        \"\"\"Returns length of info axis\"\"\"\n        return len(self._info_axis)\n\n    def __contains__(self, key) -> bool_t:\n        \"\"\"True if the key is in the info axis\"\"\"\n        return key in self._info_axis\n\n    @property\n    def empty(self) -> bool_t:\n        \"\"\"\n        Indicator whether DataFrame is empty.\n\n        True if DataFrame is entirely empty (no items), meaning any of the\n        axes are of length 0.\n\n        Returns\n        -------\n        bool\n            If DataFrame is empty, return True, if not return False.\n\n        See Also\n        --------\n        Series.dropna : Return series without null values.\n        DataFrame.dropna : Return DataFrame with labels on given axis omitted\n            where (all or any) data are missing.\n\n        Notes\n        -----\n        If DataFrame contains only NaNs, it is still not considered empty. See\n        the example below.\n\n        Examples\n        --------\n        An example of an actual empty DataFrame. Notice the index is empty:\n\n        >>> df_empty = pd.DataFrame({'A' : []})\n        >>> df_empty\n        Empty DataFrame\n        Columns: [A]\n        Index: []\n        >>> df_empty.empty\n        True\n\n        If we only have NaNs in our DataFrame, it is not considered empty! We\n        will need to drop the NaNs to make the DataFrame empty:\n\n        >>> df = pd.DataFrame({'A' : [np.nan]})\n        >>> df\n            A\n        0 NaN\n        >>> df.empty\n        False\n        >>> df.dropna().empty\n        True\n        \"\"\"\n        return any(len(self._get_axis(a)) == 0 for a in self._AXIS_ORDERS)\n\n    # ----------------------------------------------------------------------\n    # Array Interface\n\n    # This is also set in IndexOpsMixin\n    # GH#23114 Ensure ndarray.__op__(DataFrame) returns NotImplemented\n    __array_priority__ = 1000\n\n    def __array__(self, dtype=None) -> np.ndarray:\n        return np.asarray(self._values, dtype=dtype)\n\n    def __array_wrap__(\n        self,\n        result: np.ndarray,\n        context: Optional[Tuple[Callable, Tuple[Any, ...], int]] = None,\n    ):\n        \"\"\"\n        Gets called after a ufunc and other functions.\n\n        Parameters\n        ----------\n        result: np.ndarray\n            The result of the ufunc or other function called on the NumPy array\n            returned by __array__\n        context: tuple of (func, tuple, int)\n            This parameter is returned by ufuncs as a 3-element tuple: (name of the\n            ufunc, arguments of the ufunc, domain of the ufunc), but is not set by\n            other numpy functions.q\n\n        Notes\n        -----\n        Series implements __array_ufunc_ so this not called for ufunc on Series.\n        \"\"\"\n        result = lib.item_from_zerodim(result)\n        if is_scalar(result):\n            # e.g. we get here with np.ptp(series)\n            # ptp also requires the item_from_zerodim\n            return result\n        d = self._construct_axes_dict(self._AXIS_ORDERS, copy=False)\n        return self._constructor(result, **d).__finalize__(\n            self, method=\"__array_wrap__\"\n        )\n\n    # ideally we would define this to avoid the getattr checks, but\n    # is slower\n    # @property\n    # def __array_interface__(self):\n    #    \"\"\" provide numpy array interface method \"\"\"\n    #    values = self.values\n    #    return dict(typestr=values.dtype.str,shape=values.shape,data=values)\n\n    # ----------------------------------------------------------------------\n    # Picklability\n\n    def __getstate__(self) -> Dict[str, Any]:\n        meta = {k: getattr(self, k, None) for k in self._metadata}\n        return dict(\n            _mgr=self._mgr,\n            _typ=self._typ,\n            _metadata=self._metadata,\n            attrs=self.attrs,\n            _flags={k: self.flags[k] for k in self.flags._keys},\n            **meta,\n        )\n\n    def __setstate__(self, state):\n        if isinstance(state, BlockManager):\n            self._mgr = state\n        elif isinstance(state, dict):\n            if \"_data\" in state and \"_mgr\" not in state:\n                # compat for older pickles\n                state[\"_mgr\"] = state.pop(\"_data\")\n            typ = state.get(\"_typ\")\n            if typ is not None:\n                attrs = state.get(\"_attrs\", {})\n                object.__setattr__(self, \"_attrs\", attrs)\n                flags = state.get(\"_flags\", dict(allows_duplicate_labels=True))\n                object.__setattr__(self, \"_flags\", Flags(self, **flags))\n\n                # set in the order of internal names\n                # to avoid definitional recursion\n                # e.g. say fill_value needing _mgr to be\n                # defined\n                meta = set(self._internal_names + self._metadata)\n                for k in list(meta):\n                    if k in state and k != \"_flags\":\n                        v = state[k]\n                        object.__setattr__(self, k, v)\n\n                for k, v in state.items():\n                    if k not in meta:\n                        object.__setattr__(self, k, v)\n\n            else:\n                raise NotImplementedError(\"Pre-0.12 pickles are no longer supported\")\n        elif len(state) == 2:\n            raise NotImplementedError(\"Pre-0.12 pickles are no longer supported\")\n\n        self._item_cache = {}\n\n    # ----------------------------------------------------------------------\n    # Rendering Methods\n\n    def __repr__(self) -> str:\n        # string representation based upon iterating over self\n        # (since, by definition, `PandasContainers` are iterable)\n        prepr = f\"[{','.join(map(pprint_thing, self))}]\"\n        return f\"{type(self).__name__}({prepr})\"\n\n    def _repr_latex_(self):\n        \"\"\"\n        Returns a LaTeX representation for a particular object.\n        Mainly for use with nbconvert (jupyter notebook conversion to pdf).\n        \"\"\"\n        if config.get_option(\"display.latex.repr\"):\n            return self.to_latex()\n        else:\n            return None\n\n    def _repr_data_resource_(self):\n        \"\"\"\n        Not a real Jupyter special repr method, but we use the same\n        naming convention.\n        \"\"\"\n        if config.get_option(\"display.html.table_schema\"):\n            data = self.head(config.get_option(\"display.max_rows\"))\n\n            as_json = data.to_json(orient=\"table\")\n            as_json = cast(str, as_json)\n            payload = json.loads(as_json, object_pairs_hook=collections.OrderedDict)\n            return payload\n\n    # ----------------------------------------------------------------------\n    # I/O Methods\n\n    @doc(klass=\"object\")\n    def to_excel(\n        self,\n        excel_writer,\n        sheet_name=\"Sheet1\",\n        na_rep=\"\",\n        float_format=None,\n        columns=None,\n        header=True,\n        index=True,\n        index_label=None,\n        startrow=0,\n        startcol=0,\n        engine=None,\n        merge_cells=True,\n        encoding=None,\n        inf_rep=\"inf\",\n        verbose=True,\n        freeze_panes=None,\n    ) -> None:\n        \"\"\"\n        Write {klass} to an Excel sheet.\n\n        To write a single {klass} to an Excel .xlsx file it is only necessary to\n        specify a target file name. To write to multiple sheets it is necessary to\n        create an `ExcelWriter` object with a target file name, and specify a sheet\n        in the file to write to.\n\n        Multiple sheets may be written to by specifying unique `sheet_name`.\n        With all data written to the file it is necessary to save the changes.\n        Note that creating an `ExcelWriter` object with a file name that already\n        exists will result in the contents of the existing file being erased.\n\n        Parameters\n        ----------\n        excel_writer : str or ExcelWriter object\n            File path or existing ExcelWriter.\n        sheet_name : str, default 'Sheet1'\n            Name of sheet which will contain DataFrame.\n        na_rep : str, default ''\n            Missing data representation.\n        float_format : str, optional\n            Format string for floating point numbers. For example\n            ``float_format=\"%.2f\"`` will format 0.1234 to 0.12.\n        columns : sequence or list of str, optional\n            Columns to write.\n        header : bool or list of str, default True\n            Write out the column names. If a list of string is given it is\n            assumed to be aliases for the column names.\n        index : bool, default True\n            Write row names (index).\n        index_label : str or sequence, optional\n            Column label for index column(s) if desired. If not specified, and\n            `header` and `index` are True, then the index names are used. A\n            sequence should be given if the DataFrame uses MultiIndex.\n        startrow : int, default 0\n            Upper left cell row to dump data frame.\n        startcol : int, default 0\n            Upper left cell column to dump data frame.\n        engine : str, optional\n            Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set this\n            via the options ``io.excel.xlsx.writer``, ``io.excel.xls.writer``, and\n            ``io.excel.xlsm.writer``.\n        merge_cells : bool, default True\n            Write MultiIndex and Hierarchical Rows as merged cells.\n        encoding : str, optional\n            Encoding of the resulting excel file. Only necessary for xlwt,\n            other writers support unicode natively.\n        inf_rep : str, default 'inf'\n            Representation for infinity (there is no native representation for\n            infinity in Excel).\n        verbose : bool, default True\n            Display more information in the error logs.\n        freeze_panes : tuple of int (length 2), optional\n            Specifies the one-based bottommost row and rightmost column that\n            is to be frozen.\n\n        See Also\n        --------\n        to_csv : Write DataFrame to a comma-separated values (csv) file.\n        ExcelWriter : Class for writing DataFrame objects into excel sheets.\n        read_excel : Read an Excel file into a pandas DataFrame.\n        read_csv : Read a comma-separated values (csv) file into DataFrame.\n\n        Notes\n        -----\n        For compatibility with :meth:`~DataFrame.to_csv`,\n        to_excel serializes lists and dicts to strings before writing.\n\n        Once a workbook has been saved it is not possible write further data\n        without rewriting the whole workbook.\n\n        Examples\n        --------\n\n        Create, write to and save a workbook:\n\n        >>> df1 = pd.DataFrame([['a', 'b'], ['c', 'd']],\n        ...                    index=['row 1', 'row 2'],\n        ...                    columns=['col 1', 'col 2'])\n        >>> df1.to_excel(\"output.xlsx\")  # doctest: +SKIP\n\n        To specify the sheet name:\n\n        >>> df1.to_excel(\"output.xlsx\",\n        ...              sheet_name='Sheet_name_1')  # doctest: +SKIP\n\n        If you wish to write to more than one sheet in the workbook, it is\n        necessary to specify an ExcelWriter object:\n\n        >>> df2 = df1.copy()\n        >>> with pd.ExcelWriter('output.xlsx') as writer:  # doctest: +SKIP\n        ...     df1.to_excel(writer, sheet_name='Sheet_name_1')\n        ...     df2.to_excel(writer, sheet_name='Sheet_name_2')\n\n        ExcelWriter can also be used to append to an existing Excel file:\n\n        >>> with pd.ExcelWriter('output.xlsx',\n        ...                     mode='a') as writer:  # doctest: +SKIP\n        ...     df.to_excel(writer, sheet_name='Sheet_name_3')\n\n        To set the library that is used to write the Excel file,\n        you can pass the `engine` keyword (the default engine is\n        automatically chosen depending on the file extension):\n\n        >>> df1.to_excel('output1.xlsx', engine='xlsxwriter')  # doctest: +SKIP\n        \"\"\"\n\n        df = self if isinstance(self, ABCDataFrame) else self.to_frame()\n\n        from pandas.io.formats.excel import ExcelFormatter\n\n        formatter = ExcelFormatter(\n            df,\n            na_rep=na_rep,\n            cols=columns,\n            header=header,\n            float_format=float_format,\n            index=index,\n            index_label=index_label,\n            merge_cells=merge_cells,\n            inf_rep=inf_rep,\n        )\n        formatter.write(\n            excel_writer,\n            sheet_name=sheet_name,\n            startrow=startrow,\n            startcol=startcol,\n            freeze_panes=freeze_panes,\n            engine=engine,\n        )\n\n    def to_json(\n        self,\n        path_or_buf: Optional[FilePathOrBuffer] = None,\n        orient: Optional[str] = None,\n        date_format: Optional[str] = None,\n        double_precision: int = 10,\n        force_ascii: bool_t = True,\n        date_unit: str = \"ms\",\n        default_handler: Optional[Callable[[Any], JSONSerializable]] = None,\n        lines: bool_t = False,\n        compression: CompressionOptions = \"infer\",\n        index: bool_t = True,\n        indent: Optional[int] = None,\n        storage_options: StorageOptions = None,\n    ) -> Optional[str]:\n        \"\"\"\n        Convert the object to a JSON string.\n\n        Note NaN's and None will be converted to null and datetime objects\n        will be converted to UNIX timestamps.\n\n        Parameters\n        ----------\n        path_or_buf : str or file handle, optional\n            File path or object. If not specified, the result is returned as\n            a string.\n        orient : str\n            Indication of expected JSON string format.\n\n            * Series:\n\n                - default is 'index'\n                - allowed values are: {'split','records','index','table'}.\n\n            * DataFrame:\n\n                - default is 'columns'\n                - allowed values are: {'split', 'records', 'index', 'columns',\n                  'values', 'table'}.\n\n            * The format of the JSON string:\n\n                - 'split' : dict like {'index' -> [index], 'columns' -> [columns],\n                  'data' -> [values]}\n                - 'records' : list like [{column -> value}, ... , {column -> value}]\n                - 'index' : dict like {index -> {column -> value}}\n                - 'columns' : dict like {column -> {index -> value}}\n                - 'values' : just the values array\n                - 'table' : dict like {'schema': {schema}, 'data': {data}}\n\n                Describing the data, where data component is like ``orient='records'``.\n\n        date_format : {None, 'epoch', 'iso'}\n            Type of date conversion. 'epoch' = epoch milliseconds,\n            'iso' = ISO8601. The default depends on the `orient`. For\n            ``orient='table'``, the default is 'iso'. For all other orients,\n            the default is 'epoch'.\n        double_precision : int, default 10\n            The number of decimal places to use when encoding\n            floating point values.\n        force_ascii : bool, default True\n            Force encoded string to be ASCII.\n        date_unit : str, default 'ms' (milliseconds)\n            The time unit to encode to, governs timestamp and ISO8601\n            precision.  One of 's', 'ms', 'us', 'ns' for second, millisecond,\n            microsecond, and nanosecond respectively.\n        default_handler : callable, default None\n            Handler to call if object cannot otherwise be converted to a\n            suitable format for JSON. Should receive a single argument which is\n            the object to convert and return a serialisable object.\n        lines : bool, default False\n            If 'orient' is 'records' write out line delimited json format. Will\n            throw ValueError if incorrect 'orient' since others are not list\n            like.\n\n        compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}\n\n            A string representing the compression to use in the output file,\n            only used when the first argument is a filename. By default, the\n            compression is inferred from the filename.\n\n            .. versionchanged:: 0.24.0\n               'infer' option added and set to default\n        index : bool, default True\n            Whether to include the index values in the JSON string. Not\n            including the index (``index=False``) is only supported when\n            orient is 'split' or 'table'.\n        indent : int, optional\n           Length of whitespace used to indent each record.\n\n           .. versionadded:: 1.0.0\n\n        storage_options : dict, optional\n            Extra options that make sense for a particular storage connection, e.g.\n            host, port, username, password, etc., if using a URL that will\n            be parsed by ``fsspec``, e.g., starting \"s3://\", \"gcs://\". An error\n            will be raised if providing this argument with a local path or\n            a file-like buffer. See the fsspec and backend storage implementation\n            docs for the set of allowed keys and values.\n\n            .. versionadded:: 1.2.0\n\n        Returns\n        -------\n        None or str\n            If path_or_buf is None, returns the resulting json format as a\n            string. Otherwise returns None.\n\n        See Also\n        --------\n        read_json : Convert a JSON string to pandas object.\n\n        Notes\n        -----\n        The behavior of ``indent=0`` varies from the stdlib, which does not\n        indent the output but does insert newlines. Currently, ``indent=0``\n        and the default ``indent=None`` are equivalent in pandas, though this\n        may change in a future release.\n\n        ``orient='table'`` contains a 'pandas_version' field under 'schema'.\n        This stores the version of `pandas` used in the latest revision of the\n        schema.\n\n        Examples\n        --------\n        >>> import json\n        >>> df = pd.DataFrame(\n        ...     [[\"a\", \"b\"], [\"c\", \"d\"]],\n        ...     index=[\"row 1\", \"row 2\"],\n        ...     columns=[\"col 1\", \"col 2\"],\n        ... )\n\n        >>> result = df.to_json(orient=\"split\")\n        >>> parsed = json.loads(result)\n        >>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n        {\n            \"columns\": [\n                \"col 1\",\n                \"col 2\"\n            ],\n            \"index\": [\n                \"row 1\",\n                \"row 2\"\n            ],\n            \"data\": [\n                [\n                    \"a\",\n                    \"b\"\n                ],\n                [\n                    \"c\",\n                    \"d\"\n                ]\n            ]\n        }\n\n        Encoding/decoding a Dataframe using ``'records'`` formatted JSON.\n        Note that index labels are not preserved with this encoding.\n\n        >>> result = df.to_json(orient=\"records\")\n        >>> parsed = json.loads(result)\n        >>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n        [\n            {\n                \"col 1\": \"a\",\n                \"col 2\": \"b\"\n            },\n            {\n                \"col 1\": \"c\",\n                \"col 2\": \"d\"\n            }\n        ]\n\n        Encoding/decoding a Dataframe using ``'index'`` formatted JSON:\n\n        >>> result = df.to_json(orient=\"index\")\n        >>> parsed = json.loads(result)\n        >>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n        {\n            \"row 1\": {\n                \"col 1\": \"a\",\n                \"col 2\": \"b\"\n            },\n            \"row 2\": {\n                \"col 1\": \"c\",\n                \"col 2\": \"d\"\n            }\n        }\n\n        Encoding/decoding a Dataframe using ``'columns'`` formatted JSON:\n\n        >>> result = df.to_json(orient=\"columns\")\n        >>> parsed = json.loads(result)\n        >>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n        {\n            \"col 1\": {\n                \"row 1\": \"a\",\n                \"row 2\": \"c\"\n            },\n            \"col 2\": {\n                \"row 1\": \"b\",\n                \"row 2\": \"d\"\n            }\n        }\n\n        Encoding/decoding a Dataframe using ``'values'`` formatted JSON:\n\n        >>> result = df.to_json(orient=\"values\")\n        >>> parsed = json.loads(result)\n        >>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n        [\n            [\n                \"a\",\n                \"b\"\n            ],\n            [\n                \"c\",\n                \"d\"\n            ]\n        ]\n\n        Encoding with Table Schema:\n\n        >>> result = df.to_json(orient=\"table\")\n        >>> parsed = json.loads(result)\n        >>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n        {\n            \"schema\": {\n                \"fields\": [\n                    {\n                        \"name\": \"index\",\n                        \"type\": \"string\"\n                    },\n                    {\n                        \"name\": \"col 1\",\n                        \"type\": \"string\"\n                    },\n                    {\n                        \"name\": \"col 2\",\n                        \"type\": \"string\"\n                    }\n                ],\n                \"primaryKey\": [\n                    \"index\"\n                ],\n                \"pandas_version\": \"0.20.0\"\n            },\n            \"data\": [\n                {\n                    \"index\": \"row 1\",\n                    \"col 1\": \"a\",\n                    \"col 2\": \"b\"\n                },\n                {\n                    \"index\": \"row 2\",\n                    \"col 1\": \"c\",\n                    \"col 2\": \"d\"\n                }\n            ]\n        }\n        \"\"\"\n        from pandas.io import json\n\n        if date_format is None and orient == \"table\":\n            date_format = \"iso\"\n        elif date_format is None:\n            date_format = \"epoch\"\n\n        config.is_nonnegative_int(indent)\n        indent = indent or 0\n\n        return json.to_json(\n            path_or_buf=path_or_buf,\n            obj=self,\n            orient=orient,\n            date_format=date_format,\n            double_precision=double_precision,\n            force_ascii=force_ascii,\n            date_unit=date_unit,\n            default_handler=default_handler,\n            lines=lines,\n            compression=compression,\n            index=index,\n            indent=indent,\n            storage_options=storage_options,\n        )\n\n    def to_hdf(\n        self,\n        path_or_buf,\n        key: str,\n        mode: str = \"a\",\n        complevel: Optional[int] = None,\n        complib: Optional[str] = None,\n        append: bool_t = False,\n        format: Optional[str] = None,\n        index: bool_t = True,\n        min_itemsize: Optional[Union[int, Dict[str, int]]] = None,\n        nan_rep=None,\n        dropna: Optional[bool_t] = None,\n        data_columns: Optional[Union[bool_t, List[str]]] = None,\n        errors: str = \"strict\",\n        encoding: str = \"UTF-8\",\n    ) -> None:\n        \"\"\"\n        Write the contained data to an HDF5 file using HDFStore.\n\n        Hierarchical Data Format (HDF) is self-describing, allowing an\n        application to interpret the structure and contents of a file with\n        no outside information. One HDF file can hold a mix of related objects\n        which can be accessed as a group or as individual objects.\n\n        In order to add another DataFrame or Series to an existing HDF file\n        please use append mode and a different a key.\n\n        For more information see the :ref:`user guide <io.hdf5>`.\n\n        Parameters\n        ----------\n        path_or_buf : str or pandas.HDFStore\n            File path or HDFStore object.\n        key : str\n            Identifier for the group in the store.\n        mode : {'a', 'w', 'r+'}, default 'a'\n            Mode to open file:\n\n            - 'w': write, a new file is created (an existing file with\n              the same name would be deleted).\n            - 'a': append, an existing file is opened for reading and\n              writing, and if the file does not exist it is created.\n            - 'r+': similar to 'a', but the file must already exist.\n        complevel : {0-9}, optional\n            Specifies a compression level for data.\n            A value of 0 disables compression.\n        complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib'\n            Specifies the compression library to be used.\n            As of v0.20.2 these additional compressors for Blosc are supported\n            (default if no compressor specified: 'blosc:blosclz'):\n            {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy',\n            'blosc:zlib', 'blosc:zstd'}.\n            Specifying a compression library which is not available issues\n            a ValueError.\n        append : bool, default False\n            For Table formats, append the input data to the existing.\n        format : {'fixed', 'table', None}, default 'fixed'\n            Possible values:\n\n            - 'fixed': Fixed format. Fast writing/reading. Not-appendable,\n              nor searchable.\n            - 'table': Table format. Write as a PyTables Table structure\n              which may perform worse but allow more flexible operations\n              like searching / selecting subsets of the data.\n            - If None, pd.get_option('io.hdf.default_format') is checked,\n              followed by fallback to \"fixed\"\n        errors : str, default 'strict'\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        encoding : str, default \"UTF-8\"\n        min_itemsize : dict or int, optional\n            Map column names to minimum string sizes for columns.\n        nan_rep : Any, optional\n            How to represent null values as str.\n            Not allowed with append=True.\n        data_columns : list of columns or True, optional\n            List of columns to create as indexed data columns for on-disk\n            queries, or True to use all columns. By default only the axes\n            of the object are indexed. See :ref:`io.hdf5-query-data-columns`.\n            Applicable only to format='table'.\n\n        See Also\n        --------\n        DataFrame.read_hdf : Read from HDF file.\n        DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n        DataFrame.to_sql : Write to a sql table.\n        DataFrame.to_feather : Write out feather-format for DataFrames.\n        DataFrame.to_csv : Write out to a csv file.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n        ...                   index=['a', 'b', 'c'])\n        >>> df.to_hdf('data.h5', key='df', mode='w')\n\n        We can add another object to the same file:\n\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.to_hdf('data.h5', key='s')\n\n        Reading from HDF file:\n\n        >>> pd.read_hdf('data.h5', 'df')\n        A  B\n        a  1  4\n        b  2  5\n        c  3  6\n        >>> pd.read_hdf('data.h5', 's')\n        0    1\n        1    2\n        2    3\n        3    4\n        dtype: int64\n\n        Deleting file with data:\n\n        >>> import os\n        >>> os.remove('data.h5')\n        \"\"\"\n        from pandas.io import pytables\n\n        pytables.to_hdf(\n            path_or_buf,\n            key,\n            self,\n            mode=mode,\n            complevel=complevel,\n            complib=complib,\n            append=append,\n            format=format,\n            index=index,\n            min_itemsize=min_itemsize,\n            nan_rep=nan_rep,\n            dropna=dropna,\n            data_columns=data_columns,\n            errors=errors,\n            encoding=encoding,\n        )\n\n    def to_sql(\n        self,\n        name: str,\n        con,\n        schema=None,\n        if_exists: str = \"fail\",\n        index: bool_t = True,\n        index_label=None,\n        chunksize=None,\n        dtype=None,\n        method=None,\n    ) -> None:\n        \"\"\"\n        Write records stored in a DataFrame to a SQL database.\n\n        Databases supported by SQLAlchemy [1]_ are supported. Tables can be\n        newly created, appended to, or overwritten.\n\n        Parameters\n        ----------\n        name : str\n            Name of SQL table.\n        con : sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection\n            Using SQLAlchemy makes it possible to use any DB supported by that\n            library. Legacy support is provided for sqlite3.Connection objects. The user\n            is responsible for engine disposal and connection closure for the SQLAlchemy\n            connectable See `here \\\n                <https://docs.sqlalchemy.org/en/13/core/connections.html>`_.\n\n        schema : str, optional\n            Specify the schema (if database flavor supports this). If None, use\n            default schema.\n        if_exists : {'fail', 'replace', 'append'}, default 'fail'\n            How to behave if the table already exists.\n\n            * fail: Raise a ValueError.\n            * replace: Drop the table before inserting new values.\n            * append: Insert new values to the existing table.\n\n        index : bool, default True\n            Write DataFrame index as a column. Uses `index_label` as the column\n            name in the table.\n        index_label : str or sequence, default None\n            Column label for index column(s). If None is given (default) and\n            `index` is True, then the index names are used.\n            A sequence should be given if the DataFrame uses MultiIndex.\n        chunksize : int, optional\n            Specify the number of rows in each batch to be written at a time.\n            By default, all rows will be written at once.\n        dtype : dict or scalar, optional\n            Specifying the datatype for columns. If a dictionary is used, the\n            keys should be the column names and the values should be the\n            SQLAlchemy types or strings for the sqlite3 legacy mode. If a\n            scalar is provided, it will be applied to all columns.\n        method : {None, 'multi', callable}, optional\n            Controls the SQL insertion clause used:\n\n            * None : Uses standard SQL ``INSERT`` clause (one per row).\n            * 'multi': Pass multiple values in a single ``INSERT`` clause.\n            * callable with signature ``(pd_table, conn, keys, data_iter)``.\n\n            Details and a sample callable implementation can be found in the\n            section :ref:`insert method <io.sql.method>`.\n\n            .. versionadded:: 0.24.0\n\n        Raises\n        ------\n        ValueError\n            When the table already exists and `if_exists` is 'fail' (the\n            default).\n\n        See Also\n        --------\n        read_sql : Read a DataFrame from a table.\n\n        Notes\n        -----\n        Timezone aware datetime columns will be written as\n        ``Timestamp with timezone`` type with SQLAlchemy if supported by the\n        database. Otherwise, the datetimes will be stored as timezone unaware\n        timestamps local to the original timezone.\n\n        .. versionadded:: 0.24.0\n\n        References\n        ----------\n        .. [1] https://docs.sqlalchemy.org\n        .. [2] https://www.python.org/dev/peps/pep-0249/\n\n        Examples\n        --------\n        Create an in-memory SQLite database.\n\n        >>> from sqlalchemy import create_engine\n        >>> engine = create_engine('sqlite://', echo=False)\n\n        Create a table from scratch with 3 rows.\n\n        >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n        >>> df\n             name\n        0  User 1\n        1  User 2\n        2  User 3\n\n        >>> df.to_sql('users', con=engine)\n        >>> engine.execute(\"SELECT * FROM users\").fetchall()\n        [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]\n\n        An `sqlalchemy.engine.Connection` can also be passed to to `con`:\n\n        >>> with engine.begin() as connection:\n        ...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})\n        ...     df1.to_sql('users', con=connection, if_exists='append')\n\n        This is allowed to support operations that require that the same\n        DBAPI connection is used for the entire operation.\n\n        >>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']})\n        >>> df2.to_sql('users', con=engine, if_exists='append')\n        >>> engine.execute(\"SELECT * FROM users\").fetchall()\n        [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),\n         (0, 'User 4'), (1, 'User 5'), (0, 'User 6'),\n         (1, 'User 7')]\n\n        Overwrite the table with just ``df2``.\n\n        >>> df2.to_sql('users', con=engine, if_exists='replace',\n        ...            index_label='id')\n        >>> engine.execute(\"SELECT * FROM users\").fetchall()\n        [(0, 'User 6'), (1, 'User 7')]\n\n        Specify the dtype (especially useful for integers with missing values).\n        Notice that while pandas is forced to store the data as floating point,\n        the database supports nullable integers. When fetching the data with\n        Python, we get back integer scalars.\n\n        >>> df = pd.DataFrame({\"A\": [1, None, 2]})\n        >>> df\n             A\n        0  1.0\n        1  NaN\n        2  2.0\n\n        >>> from sqlalchemy.types import Integer\n        >>> df.to_sql('integers', con=engine, index=False,\n        ...           dtype={\"A\": Integer()})\n\n        >>> engine.execute(\"SELECT * FROM integers\").fetchall()\n        [(1,), (None,), (2,)]\n        \"\"\"\n        from pandas.io import sql\n\n        sql.to_sql(\n            self,\n            name,\n            con,\n            schema=schema,\n            if_exists=if_exists,\n            index=index,\n            index_label=index_label,\n            chunksize=chunksize,\n            dtype=dtype,\n            method=method,\n        )\n\n    def to_pickle(\n        self,\n        path,\n        compression: CompressionOptions = \"infer\",\n        protocol: int = pickle.HIGHEST_PROTOCOL,\n        storage_options: StorageOptions = None,\n    ) -> None:\n        \"\"\"\n        Pickle (serialize) object to file.\n\n        Parameters\n        ----------\n        path : str\n            File path where the pickled object will be stored.\n        compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, \\\n        default 'infer'\n            A string representing the compression to use in the output file. By\n            default, infers from the file extension in specified path.\n        protocol : int\n            Int which indicates which protocol should be used by the pickler,\n            default HIGHEST_PROTOCOL (see [1]_ paragraph 12.1.2). The possible\n            values are 0, 1, 2, 3, 4, 5. A negative value for the protocol\n            parameter is equivalent to setting its value to HIGHEST_PROTOCOL.\n\n            .. [1] https://docs.python.org/3/library/pickle.html.\n\n        storage_options : dict, optional\n            Extra options that make sense for a particular storage connection, e.g.\n            host, port, username, password, etc., if using a URL that will\n            be parsed by ``fsspec``, e.g., starting \"s3://\", \"gcs://\". An error\n            will be raised if providing this argument with a local path or\n            a file-like buffer. See the fsspec and backend storage implementation\n            docs for the set of allowed keys and values.\n\n            .. versionadded:: 1.2.0\n\n        See Also\n        --------\n        read_pickle : Load pickled pandas object (or any object) from file.\n        DataFrame.to_hdf : Write DataFrame to an HDF5 file.\n        DataFrame.to_sql : Write DataFrame to a SQL database.\n        DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n\n        Examples\n        --------\n        >>> original_df = pd.DataFrame({\"foo\": range(5), \"bar\": range(5, 10)})\n        >>> original_df\n           foo  bar\n        0    0    5\n        1    1    6\n        2    2    7\n        3    3    8\n        4    4    9\n        >>> original_df.to_pickle(\"./dummy.pkl\")\n\n        >>> unpickled_df = pd.read_pickle(\"./dummy.pkl\")\n        >>> unpickled_df\n           foo  bar\n        0    0    5\n        1    1    6\n        2    2    7\n        3    3    8\n        4    4    9\n\n        >>> import os\n        >>> os.remove(\"./dummy.pkl\")\n        \"\"\"\n        from pandas.io.pickle import to_pickle\n\n        to_pickle(\n            self,\n            path,\n            compression=compression,\n            protocol=protocol,\n            storage_options=storage_options,\n        )\n\n    def to_clipboard(\n        self, excel: bool_t = True, sep: Optional[str] = None, **kwargs\n    ) -> None:\n        r\"\"\"\n        Copy object to the system clipboard.\n\n        Write a text representation of object to the system clipboard.\n        This can be pasted into Excel, for example.\n\n        Parameters\n        ----------\n        excel : bool, default True\n            Produce output in a csv format for easy pasting into excel.\n\n            - True, use the provided separator for csv pasting.\n            - False, write a string representation of the object to the clipboard.\n\n        sep : str, default ``'\\t'``\n            Field delimiter.\n        **kwargs\n            These parameters will be passed to DataFrame.to_csv.\n\n        See Also\n        --------\n        DataFrame.to_csv : Write a DataFrame to a comma-separated values\n            (csv) file.\n        read_clipboard : Read text from clipboard and pass to read_table.\n\n        Notes\n        -----\n        Requirements for your platform.\n\n          - Linux : `xclip`, or `xsel` (with `PyQt4` modules)\n          - Windows : none\n          - OS X : none\n\n        Examples\n        --------\n        Copy the contents of a DataFrame to the clipboard.\n\n        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C'])\n\n        >>> df.to_clipboard(sep=',')  # doctest: +SKIP\n        ... # Wrote the following to the system clipboard:\n        ... # ,A,B,C\n        ... # 0,1,2,3\n        ... # 1,4,5,6\n\n        We can omit the index by passing the keyword `index` and setting\n        it to false.\n\n        >>> df.to_clipboard(sep=',', index=False)  # doctest: +SKIP\n        ... # Wrote the following to the system clipboard:\n        ... # A,B,C\n        ... # 1,2,3\n        ... # 4,5,6\n        \"\"\"\n        from pandas.io import clipboards\n\n        clipboards.to_clipboard(self, excel=excel, sep=sep, **kwargs)\n\n    def to_xarray(self):\n        \"\"\"\n        Return an xarray object from the pandas object.\n\n        Returns\n        -------\n        xarray.DataArray or xarray.Dataset\n            Data in the pandas structure converted to Dataset if the object is\n            a DataFrame, or a DataArray if the object is a Series.\n\n        See Also\n        --------\n        DataFrame.to_hdf : Write DataFrame to an HDF5 file.\n        DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n\n        Notes\n        -----\n        See the `xarray docs <https://xarray.pydata.org/en/stable/>`__\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('falcon', 'bird', 389.0, 2),\n        ...                    ('parrot', 'bird', 24.0, 2),\n        ...                    ('lion', 'mammal', 80.5, 4),\n        ...                    ('monkey', 'mammal', np.nan, 4)],\n        ...                   columns=['name', 'class', 'max_speed',\n        ...                            'num_legs'])\n        >>> df\n             name   class  max_speed  num_legs\n        0  falcon    bird      389.0         2\n        1  parrot    bird       24.0         2\n        2    lion  mammal       80.5         4\n        3  monkey  mammal        NaN         4\n\n        >>> df.to_xarray()\n        <xarray.Dataset>\n        Dimensions:    (index: 4)\n        Coordinates:\n          * index      (index) int64 0 1 2 3\n        Data variables:\n            name       (index) object 'falcon' 'parrot' 'lion' 'monkey'\n            class      (index) object 'bird' 'bird' 'mammal' 'mammal'\n            max_speed  (index) float64 389.0 24.0 80.5 nan\n            num_legs   (index) int64 2 2 4 4\n\n        >>> df['max_speed'].to_xarray()\n        <xarray.DataArray 'max_speed' (index: 4)>\n        array([389. ,  24. ,  80.5,   nan])\n        Coordinates:\n          * index    (index) int64 0 1 2 3\n\n        >>> dates = pd.to_datetime(['2018-01-01', '2018-01-01',\n        ...                         '2018-01-02', '2018-01-02'])\n        >>> df_multiindex = pd.DataFrame({'date': dates,\n        ...                               'animal': ['falcon', 'parrot',\n        ...                                          'falcon', 'parrot'],\n        ...                               'speed': [350, 18, 361, 15]})\n        >>> df_multiindex = df_multiindex.set_index(['date', 'animal'])\n\n        >>> df_multiindex\n                           speed\n        date       animal\n        2018-01-01 falcon    350\n                   parrot     18\n        2018-01-02 falcon    361\n                   parrot     15\n\n        >>> df_multiindex.to_xarray()\n        <xarray.Dataset>\n        Dimensions:  (animal: 2, date: 2)\n        Coordinates:\n          * date     (date) datetime64[ns] 2018-01-01 2018-01-02\n          * animal   (animal) object 'falcon' 'parrot'\n        Data variables:\n            speed    (date, animal) int64 350 18 361 15\n        \"\"\"\n        xarray = import_optional_dependency(\"xarray\")\n\n        if self.ndim == 1:\n            return xarray.DataArray.from_series(self)\n        else:\n            return xarray.Dataset.from_dataframe(self)\n\n    @doc(returns=fmt.return_docstring)\n    def to_latex(\n        self,\n        buf=None,\n        columns=None,\n        col_space=None,\n        header=True,\n        index=True,\n        na_rep=\"NaN\",\n        formatters=None,\n        float_format=None,\n        sparsify=None,\n        index_names=True,\n        bold_rows=False,\n        column_format=None,\n        longtable=None,\n        escape=None,\n        encoding=None,\n        decimal=\".\",\n        multicolumn=None,\n        multicolumn_format=None,\n        multirow=None,\n        caption=None,\n        label=None,\n        position=None,\n    ):\n        r\"\"\"\n        Render object to a LaTeX tabular, longtable, or nested table/tabular.\n\n        Requires ``\\usepackage{{booktabs}}``.  The output can be copy/pasted\n        into a main LaTeX document or read from an external file\n        with ``\\input{{table.tex}}``.\n\n        .. versionchanged:: 1.0.0\n           Added caption and label arguments.\n\n        .. versionchanged:: 1.2.0\n           Added position argument, changed meaning of caption argument.\n\n        Parameters\n        ----------\n        buf : str, Path or StringIO-like, optional, default None\n            Buffer to write to. If None, the output is returned as a string.\n        columns : list of label, optional\n            The subset of columns to write. Writes all columns by default.\n        col_space : int, optional\n            The minimum width of each column.\n        header : bool or list of str, default True\n            Write out the column names. If a list of strings is given,\n            it is assumed to be aliases for the column names.\n        index : bool, default True\n            Write row names (index).\n        na_rep : str, default 'NaN'\n            Missing data representation.\n        formatters : list of functions or dict of {{str: function}}, optional\n            Formatter functions to apply to columns' elements by position or\n            name. The result of each function must be a unicode string.\n            List must be of length equal to the number of columns.\n        float_format : one-parameter function or str, optional, default None\n            Formatter for floating point numbers. For example\n            ``float_format=\"%.2f\"`` and ``float_format=\"{{:0.2f}}\".format`` will\n            both result in 0.1234 being formatted as 0.12.\n        sparsify : bool, optional\n            Set to False for a DataFrame with a hierarchical index to print\n            every multiindex key at each row. By default, the value will be\n            read from the config module.\n        index_names : bool, default True\n            Prints the names of the indexes.\n        bold_rows : bool, default False\n            Make the row labels bold in the output.\n        column_format : str, optional\n            The columns format as specified in `LaTeX table format\n            <https://en.wikibooks.org/wiki/LaTeX/Tables>`__ e.g. 'rcl' for 3\n            columns. By default, 'l' will be used for all columns except\n            columns of numbers, which default to 'r'.\n        longtable : bool, optional\n            By default, the value will be read from the pandas config\n            module. Use a longtable environment instead of tabular. Requires\n            adding a \\usepackage{{longtable}} to your LaTeX preamble.\n        escape : bool, optional\n            By default, the value will be read from the pandas config\n            module. When set to False prevents from escaping latex special\n            characters in column names.\n        encoding : str, optional\n            A string representing the encoding to use in the output file,\n            defaults to 'utf-8'.\n        decimal : str, default '.'\n            Character recognized as decimal separator, e.g. ',' in Europe.\n        multicolumn : bool, default True\n            Use \\multicolumn to enhance MultiIndex columns.\n            The default will be read from the config module.\n        multicolumn_format : str, default 'l'\n            The alignment for multicolumns, similar to `column_format`\n            The default will be read from the config module.\n        multirow : bool, default False\n            Use \\multirow to enhance MultiIndex rows. Requires adding a\n            \\usepackage{{multirow}} to your LaTeX preamble. Will print\n            centered labels (instead of top-aligned) across the contained\n            rows, separating groups via clines. The default will be read\n            from the pandas config module.\n        caption : str or tuple, optional\n            Tuple (full_caption, short_caption),\n            which results in ``\\caption[short_caption]{{full_caption}}``;\n            if a single string is passed, no short caption will be set.\n\n            .. versionadded:: 1.0.0\n\n            .. versionchanged:: 1.2.0\n               Optionally allow caption to be a tuple ``(full_caption, short_caption)``.\n\n        label : str, optional\n            The LaTeX label to be placed inside ``\\label{{}}`` in the output.\n            This is used with ``\\ref{{}}`` in the main ``.tex`` file.\n\n            .. versionadded:: 1.0.0\n        position : str, optional\n            The LaTeX positional argument for tables, to be placed after\n            ``\\begin{{}}`` in the output.\n\n            .. versionadded:: 1.2.0\n        {returns}\n        See Also\n        --------\n        DataFrame.to_string : Render a DataFrame to a console-friendly\n            tabular output.\n        DataFrame.to_html : Render a DataFrame as an HTML table.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(dict(name=['Raphael', 'Donatello'],\n        ...                   mask=['red', 'purple'],\n        ...                   weapon=['sai', 'bo staff']))\n        >>> print(df.to_latex(index=False))  # doctest: +NORMALIZE_WHITESPACE\n        \\begin{{tabular}}{{lll}}\n         \\toprule\n               name &    mask &    weapon \\\\\n         \\midrule\n            Raphael &     red &       sai \\\\\n          Donatello &  purple &  bo staff \\\\\n        \\bottomrule\n        \\end{{tabular}}\n        \"\"\"\n        # Get defaults from the pandas config\n        if self.ndim == 1:\n            self = self.to_frame()\n        if longtable is None:\n            longtable = config.get_option(\"display.latex.longtable\")\n        if escape is None:\n            escape = config.get_option(\"display.latex.escape\")\n        if multicolumn is None:\n            multicolumn = config.get_option(\"display.latex.multicolumn\")\n        if multicolumn_format is None:\n            multicolumn_format = config.get_option(\"display.latex.multicolumn_format\")\n        if multirow is None:\n            multirow = config.get_option(\"display.latex.multirow\")\n\n        self = cast(\"DataFrame\", self)\n        formatter = DataFrameFormatter(\n            self,\n            columns=columns,\n            col_space=col_space,\n            na_rep=na_rep,\n            header=header,\n            index=index,\n            formatters=formatters,\n            float_format=float_format,\n            bold_rows=bold_rows,\n            sparsify=sparsify,\n            index_names=index_names,\n            escape=escape,\n            decimal=decimal,\n        )\n        return DataFrameRenderer(formatter).to_latex(\n            buf=buf,\n            column_format=column_format,\n            longtable=longtable,\n            encoding=encoding,\n            multicolumn=multicolumn,\n            multicolumn_format=multicolumn_format,\n            multirow=multirow,\n            caption=caption,\n            label=label,\n            position=position,\n        )\n\n    def to_csv(\n        self,\n        path_or_buf: Optional[FilePathOrBuffer] = None,\n        sep: str = \",\",\n        na_rep: str = \"\",\n        float_format: Optional[str] = None,\n        columns: Optional[Sequence[Label]] = None,\n        header: Union[bool_t, List[str]] = True,\n        index: bool_t = True,\n        index_label: Optional[IndexLabel] = None,\n        mode: str = \"w\",\n        encoding: Optional[str] = None,\n        compression: CompressionOptions = \"infer\",\n        quoting: Optional[int] = None,\n        quotechar: str = '\"',\n        line_terminator: Optional[str] = None,\n        chunksize: Optional[int] = None,\n        date_format: Optional[str] = None,\n        doublequote: bool_t = True,\n        escapechar: Optional[str] = None,\n        decimal: str = \".\",\n        errors: str = \"strict\",\n        storage_options: StorageOptions = None,\n    ) -> Optional[str]:\n        r\"\"\"\n        Write object to a comma-separated values (csv) file.\n\n        .. versionchanged:: 0.24.0\n            The order of arguments for Series was changed.\n\n        Parameters\n        ----------\n        path_or_buf : str or file handle, default None\n            File path or object, if None is provided the result is returned as\n            a string.  If a non-binary file object is passed, it should be opened\n            with `newline=''`, disabling universal newlines. If a binary\n            file object is passed, `mode` needs to contain a `'b'`.\n\n            .. versionchanged:: 0.24.0\n\n               Was previously named \"path\" for Series.\n\n            .. versionchanged:: 1.2.0\n\n               Support for binary file objects was introduced.\n\n        sep : str, default ','\n            String of length 1. Field delimiter for the output file.\n        na_rep : str, default ''\n            Missing data representation.\n        float_format : str, default None\n            Format string for floating point numbers.\n        columns : sequence, optional\n            Columns to write.\n        header : bool or list of str, default True\n            Write out the column names. If a list of strings is given it is\n            assumed to be aliases for the column names.\n\n            .. versionchanged:: 0.24.0\n\n               Previously defaulted to False for Series.\n\n        index : bool, default True\n            Write row names (index).\n        index_label : str or sequence, or False, default None\n            Column label for index column(s) if desired. If None is given, and\n            `header` and `index` are True, then the index names are used. A\n            sequence should be given if the object uses MultiIndex. If\n            False do not print fields for index names. Use index_label=False\n            for easier importing in R.\n        mode : str\n            Python write mode, default 'w'.\n        encoding : str, optional\n            A string representing the encoding to use in the output file,\n            defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n            is a non-binary file object.\n        compression : str or dict, default 'infer'\n            If str, represents compression mode. If dict, value at 'method' is\n            the compression mode. Compression mode may be any of the following\n            possible values: {'infer', 'gzip', 'bz2', 'zip', 'xz', None}. If\n            compression mode is 'infer' and `path_or_buf` is path-like, then\n            detect compression mode from the following extensions: '.gz',\n            '.bz2', '.zip' or '.xz'. (otherwise no compression). If dict given\n            and mode is one of {'zip', 'gzip', 'bz2'}, or inferred as\n            one of the above, other entries passed as\n            additional compression options.\n\n            .. versionchanged:: 1.0.0\n\n               May now be a dict with key 'method' as compression mode\n               and other entries as additional compression options if\n               compression mode is 'zip'.\n\n            .. versionchanged:: 1.1.0\n\n               Passing compression options as keys in dict is\n               supported for compression modes 'gzip' and 'bz2'\n               as well as 'zip'.\n\n            .. versionchanged:: 1.2.0\n\n                Compression is supported for binary file objects.\n\n            .. versionchanged:: 1.2.0\n\n                Previous versions forwarded dict entries for 'gzip' to\n                `gzip.open` instead of `gzip.GzipFile` which prevented\n                setting `mtime`.\n\n        quoting : optional constant from csv module\n            Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n            then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n            will treat them as non-numeric.\n        quotechar : str, default '\\\"'\n            String of length 1. Character used to quote fields.\n        line_terminator : str, optional\n            The newline character or character sequence to use in the output\n            file. Defaults to `os.linesep`, which depends on the OS in which\n            this method is called ('\\n' for linux, '\\r\\n' for Windows, i.e.).\n\n            .. versionchanged:: 0.24.0\n        chunksize : int or None\n            Rows to write at a time.\n        date_format : str, default None\n            Format string for datetime objects.\n        doublequote : bool, default True\n            Control quoting of `quotechar` inside a field.\n        escapechar : str, default None\n            String of length 1. Character used to escape `sep` and `quotechar`\n            when appropriate.\n        decimal : str, default '.'\n            Character recognized as decimal separator. E.g. use ',' for\n            European data.\n        errors : str, default 'strict'\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n\n            .. versionadded:: 1.1.0\n\n        storage_options : dict, optional\n            Extra options that make sense for a particular storage connection, e.g.\n            host, port, username, password, etc., if using a URL that will\n            be parsed by ``fsspec``, e.g., starting \"s3://\", \"gcs://\". An error\n            will be raised if providing this argument with a local path or\n            a file-like buffer. See the fsspec and backend storage implementation\n            docs for the set of allowed keys and values.\n\n            .. versionadded:: 1.2.0\n\n        Returns\n        -------\n        None or str\n            If path_or_buf is None, returns the resulting csv format as a\n            string. Otherwise returns None.\n\n        See Also\n        --------\n        read_csv : Load a CSV file into a DataFrame.\n        to_excel : Write DataFrame to an Excel file.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n        ...                    'mask': ['red', 'purple'],\n        ...                    'weapon': ['sai', 'bo staff']})\n        >>> df.to_csv(index=False)\n        'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n\n        Create 'out.zip' containing 'out.csv'\n\n        >>> compression_opts = dict(method='zip',\n        ...                         archive_name='out.csv')  # doctest: +SKIP\n        >>> df.to_csv('out.zip', index=False,\n        ...           compression=compression_opts)  # doctest: +SKIP\n        \"\"\"\n        df = self if isinstance(self, ABCDataFrame) else self.to_frame()\n\n        formatter = DataFrameFormatter(\n            frame=df,\n            header=header,\n            index=index,\n            na_rep=na_rep,\n            float_format=float_format,\n            decimal=decimal,\n        )\n\n        return DataFrameRenderer(formatter).to_csv(\n            path_or_buf,\n            line_terminator=line_terminator,\n            sep=sep,\n            encoding=encoding,\n            errors=errors,\n            compression=compression,\n            quoting=quoting,\n            columns=columns,\n            index_label=index_label,\n            mode=mode,\n            chunksize=chunksize,\n            quotechar=quotechar,\n            date_format=date_format,\n            doublequote=doublequote,\n            escapechar=escapechar,\n            storage_options=storage_options,\n        )\n\n    # ----------------------------------------------------------------------\n    # Lookup Caching\n\n    def _set_as_cached(self, item, cacher) -> None:\n        \"\"\"\n        Set the _cacher attribute on the calling object with a weakref to\n        cacher.\n        \"\"\"\n        self._cacher = (item, weakref.ref(cacher))\n\n    def _reset_cacher(self) -> None:\n        \"\"\"\n        Reset the cacher.\n        \"\"\"\n        if hasattr(self, \"_cacher\"):\n            del self._cacher\n\n    def _maybe_cache_changed(self, item, value) -> None:\n        \"\"\"\n        The object has called back to us saying maybe it has changed.\n        \"\"\"\n        loc = self._info_axis.get_loc(item)\n        self._mgr.iset(loc, value)\n\n    @property\n    def _is_cached(self) -> bool_t:\n        \"\"\"Return boolean indicating if self is cached or not.\"\"\"\n        return getattr(self, \"_cacher\", None) is not None\n\n    def _get_cacher(self):\n        \"\"\"return my cacher or None\"\"\"\n        cacher = getattr(self, \"_cacher\", None)\n        if cacher is not None:\n            cacher = cacher[1]()\n        return cacher\n\n    def _maybe_update_cacher(\n        self, clear: bool_t = False, verify_is_copy: bool_t = True\n    ) -> None:\n        \"\"\"\n        See if we need to update our parent cacher if clear, then clear our\n        cache.\n\n        Parameters\n        ----------\n        clear : bool, default False\n            Clear the item cache.\n        verify_is_copy : bool, default True\n            Provide is_copy checks.\n        \"\"\"\n        cacher = getattr(self, \"_cacher\", None)\n        if cacher is not None:\n            ref = cacher[1]()\n\n            # we are trying to reference a dead referent, hence\n            # a copy\n            if ref is None:\n                del self._cacher\n            else:\n                if len(self) == len(ref):\n                    # otherwise, either self or ref has swapped in new arrays\n                    ref._maybe_cache_changed(cacher[0], self)\n                else:\n                    # GH#33675 we have swapped in a new array, so parent\n                    #  reference to self is now invalid\n                    ref._item_cache.pop(cacher[0], None)\n\n        if verify_is_copy:\n            self._check_setitem_copy(stacklevel=5, t=\"referent\")\n\n        if clear:\n            self._clear_item_cache()\n\n    def _clear_item_cache(self) -> None:\n        self._item_cache.clear()\n\n    # ----------------------------------------------------------------------\n    # Indexing Methods\n\n    def take(\n        self: FrameOrSeries, indices, axis=0, is_copy: Optional[bool_t] = None, **kwargs\n    ) -> FrameOrSeries:\n        \"\"\"\n        Return the elements in the given *positional* indices along an axis.\n\n        This means that we are not indexing according to actual values in\n        the index attribute of the object. We are indexing according to the\n        actual position of the element in the object.\n\n        Parameters\n        ----------\n        indices : array-like\n            An array of ints indicating which positions to take.\n        axis : {0 or 'index', 1 or 'columns', None}, default 0\n            The axis on which to select elements. ``0`` means that we are\n            selecting rows, ``1`` means that we are selecting columns.\n        is_copy : bool\n            Before pandas 1.0, ``is_copy=False`` can be specified to ensure\n            that the return value is an actual copy. Starting with pandas 1.0,\n            ``take`` always returns a copy, and the keyword is therefore\n            deprecated.\n\n            .. deprecated:: 1.0.0\n        **kwargs\n            For compatibility with :meth:`numpy.take`. Has no effect on the\n            output.\n\n        Returns\n        -------\n        taken : same type as caller\n            An array-like containing the elements taken from the object.\n\n        See Also\n        --------\n        DataFrame.loc : Select a subset of a DataFrame by labels.\n        DataFrame.iloc : Select a subset of a DataFrame by positions.\n        numpy.take : Take elements from an array along an axis.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n        ...                    ('parrot', 'bird', 24.0),\n        ...                    ('lion', 'mammal', 80.5),\n        ...                    ('monkey', 'mammal', np.nan)],\n        ...                   columns=['name', 'class', 'max_speed'],\n        ...                   index=[0, 2, 3, 1])\n        >>> df\n             name   class  max_speed\n        0  falcon    bird      389.0\n        2  parrot    bird       24.0\n        3    lion  mammal       80.5\n        1  monkey  mammal        NaN\n\n        Take elements at positions 0 and 3 along the axis 0 (default).\n\n        Note how the actual indices selected (0 and 1) do not correspond to\n        our selected indices 0 and 3. That's because we are selecting the 0th\n        and 3rd rows, not rows whose indices equal 0 and 3.\n\n        >>> df.take([0, 3])\n             name   class  max_speed\n        0  falcon    bird      389.0\n        1  monkey  mammal        NaN\n\n        Take elements at indices 1 and 2 along the axis 1 (column selection).\n\n        >>> df.take([1, 2], axis=1)\n            class  max_speed\n        0    bird      389.0\n        2    bird       24.0\n        3  mammal       80.5\n        1  mammal        NaN\n\n        We may take elements using negative integers for positive indices,\n        starting from the end of the object, just like with Python lists.\n\n        >>> df.take([-1, -2])\n             name   class  max_speed\n        1  monkey  mammal        NaN\n        3    lion  mammal       80.5\n        \"\"\"\n        if is_copy is not None:\n            warnings.warn(\n                \"is_copy is deprecated and will be removed in a future version. \"\n                \"'take' always returns a copy, so there is no need to specify this.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n\n        nv.validate_take(tuple(), kwargs)\n\n        self._consolidate_inplace()\n\n        new_data = self._mgr.take(\n            indices, axis=self._get_block_manager_axis(axis), verify=True\n        )\n        return self._constructor(new_data).__finalize__(self, method=\"take\")\n\n    def _take_with_is_copy(self: FrameOrSeries, indices, axis=0) -> FrameOrSeries:\n        \"\"\"\n        Internal version of the `take` method that sets the `_is_copy`\n        attribute to keep track of the parent dataframe (using in indexing\n        for the SettingWithCopyWarning).\n\n        See the docstring of `take` for full explanation of the parameters.\n        \"\"\"\n        result = self.take(indices=indices, axis=axis)\n        # Maybe set copy if we didn't actually change the index.\n        if not result._get_axis(axis).equals(self._get_axis(axis)):\n            result._set_is_copy(self)\n        return result\n\n    def xs(self, key, axis=0, level=None, drop_level: bool_t = True):\n        \"\"\"\n        Return cross-section from the Series/DataFrame.\n\n        This method takes a `key` argument to select data at a particular\n        level of a MultiIndex.\n\n        Parameters\n        ----------\n        key : label or tuple of label\n            Label contained in the index, or partially in a MultiIndex.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis to retrieve cross-section on.\n        level : object, defaults to first n levels (n=1 or len(key))\n            In case of a key partially contained in a MultiIndex, indicate\n            which levels are used. Levels can be referred by label or position.\n        drop_level : bool, default True\n            If False, returns object with same levels as self.\n\n        Returns\n        -------\n        Series or DataFrame\n            Cross-section from the original Series or DataFrame\n            corresponding to the selected index levels.\n\n        See Also\n        --------\n        DataFrame.loc : Access a group of rows and columns\n            by label(s) or a boolean array.\n        DataFrame.iloc : Purely integer-location based indexing\n            for selection by position.\n\n        Notes\n        -----\n        `xs` can not be used to set values.\n\n        MultiIndex Slicers is a generic way to get/set values on\n        any level or levels.\n        It is a superset of `xs` functionality, see\n        :ref:`MultiIndex Slicers <advanced.mi_slicers>`.\n\n        Examples\n        --------\n        >>> d = {'num_legs': [4, 4, 2, 2],\n        ...      'num_wings': [0, 0, 2, 2],\n        ...      'class': ['mammal', 'mammal', 'mammal', 'bird'],\n        ...      'animal': ['cat', 'dog', 'bat', 'penguin'],\n        ...      'locomotion': ['walks', 'walks', 'flies', 'walks']}\n        >>> df = pd.DataFrame(data=d)\n        >>> df = df.set_index(['class', 'animal', 'locomotion'])\n        >>> df\n                                   num_legs  num_wings\n        class  animal  locomotion\n        mammal cat     walks              4          0\n               dog     walks              4          0\n               bat     flies              2          2\n        bird   penguin walks              2          2\n\n        Get values at specified index\n\n        >>> df.xs('mammal')\n                           num_legs  num_wings\n        animal locomotion\n        cat    walks              4          0\n        dog    walks              4          0\n        bat    flies              2          2\n\n        Get values at several indexes\n\n        >>> df.xs(('mammal', 'dog'))\n                    num_legs  num_wings\n        locomotion\n        walks              4          0\n\n        Get values at specified index and level\n\n        >>> df.xs('cat', level=1)\n                           num_legs  num_wings\n        class  locomotion\n        mammal walks              4          0\n\n        Get values at several indexes and levels\n\n        >>> df.xs(('bird', 'walks'),\n        ...       level=[0, 'locomotion'])\n                 num_legs  num_wings\n        animal\n        penguin         2          2\n\n        Get values at specified column and axis\n\n        >>> df.xs('num_wings', axis=1)\n        class   animal   locomotion\n        mammal  cat      walks         0\n                dog      walks         0\n                bat      flies         2\n        bird    penguin  walks         2\n        Name: num_wings, dtype: int64\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        labels = self._get_axis(axis)\n        if level is not None:\n            if not isinstance(labels, MultiIndex):\n                raise TypeError(\"Index must be a MultiIndex\")\n            loc, new_ax = labels.get_loc_level(key, level=level, drop_level=drop_level)\n\n            # create the tuple of the indexer\n            _indexer = [slice(None)] * self.ndim\n            _indexer[axis] = loc\n            indexer = tuple(_indexer)\n\n            result = self.iloc[indexer]\n            setattr(result, result._get_axis_name(axis), new_ax)\n            return result\n\n        if axis == 1:\n            return self[key]\n\n        index = self.index\n        if isinstance(index, MultiIndex):\n            try:\n                loc, new_index = self.index._get_loc_level(\n                    key, level=0, drop_level=drop_level\n                )\n            except TypeError as e:\n                raise TypeError(f\"Expected label or tuple of labels, got {key}\") from e\n        else:\n            loc = self.index.get_loc(key)\n\n            if isinstance(loc, np.ndarray):\n                if loc.dtype == np.bool_:\n                    (inds,) = loc.nonzero()\n                    return self._take_with_is_copy(inds, axis=axis)\n                else:\n                    return self._take_with_is_copy(loc, axis=axis)\n\n            if not is_scalar(loc):\n                new_index = self.index[loc]\n\n        if is_scalar(loc):\n            # In this case loc should be an integer\n            if self.ndim == 1:\n                # if we encounter an array-like and we only have 1 dim\n                # that means that their are list/ndarrays inside the Series!\n                # so just return them (GH 6394)\n                return self._values[loc]\n\n            new_values = self._mgr.fast_xs(loc)\n\n            result = self._constructor_sliced(\n                new_values,\n                index=self.columns,\n                name=self.index[loc],\n                dtype=new_values.dtype,\n            )\n\n        else:\n            result = self.iloc[loc]\n            result.index = new_index\n\n        # this could be a view\n        # but only in a single-dtyped view sliceable case\n        result._set_is_copy(self, copy=not result._is_view)\n        return result\n\n    def __getitem__(self, item):\n        raise AbstractMethodError(self)\n\n    def _get_item_cache(self, item):\n        \"\"\"Return the cached item, item represents a label indexer.\"\"\"\n        cache = self._item_cache\n        res = cache.get(item)\n        if res is None:\n            # All places that call _get_item_cache have unique columns,\n            #  pending resolution of GH#33047\n\n            loc = self.columns.get_loc(item)\n            values = self._mgr.iget(loc)\n            res = self._box_col_values(values, loc)\n\n            cache[item] = res\n            res._set_as_cached(item, self)\n\n            # for a chain\n            res._is_copy = self._is_copy\n        return res\n\n    def _slice(self: FrameOrSeries, slobj: slice, axis=0) -> FrameOrSeries:\n        \"\"\"\n        Construct a slice of this container.\n\n        Slicing with this method is *always* positional.\n        \"\"\"\n        assert isinstance(slobj, slice), type(slobj)\n        axis = self._get_block_manager_axis(axis)\n        result = self._constructor(self._mgr.get_slice(slobj, axis=axis))\n        result = result.__finalize__(self)\n\n        # this could be a view\n        # but only in a single-dtyped view sliceable case\n        is_copy = axis != 0 or result._is_view\n        result._set_is_copy(self, copy=is_copy)\n        return result\n\n    def _iset_item(self, loc: int, value) -> None:\n        self._mgr.iset(loc, value)\n        self._clear_item_cache()\n\n    def _set_item(self, key, value) -> None:\n        try:\n            loc = self._info_axis.get_loc(key)\n        except KeyError:\n            # This item wasn't present, just insert at end\n            self._mgr.insert(len(self._info_axis), key, value)\n            return\n\n        NDFrame._iset_item(self, loc, value)\n\n    def _set_is_copy(self, ref, copy: bool_t = True) -> None:\n        if not copy:\n            self._is_copy = None\n        else:\n            assert ref is not None\n            self._is_copy = weakref.ref(ref)\n\n    def _check_is_chained_assignment_possible(self) -> bool_t:\n        \"\"\"\n        Check if we are a view, have a cacher, and are of mixed type.\n        If so, then force a setitem_copy check.\n\n        Should be called just near setting a value\n\n        Will return a boolean if it we are a view and are cached, but a\n        single-dtype meaning that the cacher should be updated following\n        setting.\n        \"\"\"\n        if self._is_view and self._is_cached:\n            ref = self._get_cacher()\n            if ref is not None and ref._is_mixed_type:\n                self._check_setitem_copy(stacklevel=4, t=\"referent\", force=True)\n            return True\n        elif self._is_copy:\n            self._check_setitem_copy(stacklevel=4, t=\"referent\")\n        return False\n\n    def _check_setitem_copy(self, stacklevel=4, t=\"setting\", force=False):\n        \"\"\"\n\n        Parameters\n        ----------\n        stacklevel : int, default 4\n           the level to show of the stack when the error is output\n        t : str, the type of setting error\n        force : bool, default False\n           If True, then force showing an error.\n\n        validate if we are doing a setitem on a chained copy.\n\n        If you call this function, be sure to set the stacklevel such that the\n        user will see the error *at the level of setting*\n\n        It is technically possible to figure out that we are setting on\n        a copy even WITH a multi-dtyped pandas object. In other words, some\n        blocks may be views while other are not. Currently _is_view will ALWAYS\n        return False for multi-blocks to avoid having to handle this case.\n\n        df = DataFrame(np.arange(0,9), columns=['count'])\n        df['group'] = 'b'\n\n        # This technically need not raise SettingWithCopy if both are view\n        # (which is not # generally guaranteed but is usually True.  However,\n        # this is in general not a good practice and we recommend using .loc.\n        df.iloc[0:5]['group'] = 'a'\n\n        \"\"\"\n        # return early if the check is not needed\n        if not (force or self._is_copy):\n            return\n\n        value = config.get_option(\"mode.chained_assignment\")\n        if value is None:\n            return\n\n        # see if the copy is not actually referred; if so, then dissolve\n        # the copy weakref\n        if self._is_copy is not None and not isinstance(self._is_copy, str):\n            r = self._is_copy()\n            if not gc.get_referents(r) or (r is not None and r.shape == self.shape):\n                self._is_copy = None\n                return\n\n        # a custom message\n        if isinstance(self._is_copy, str):\n            t = self._is_copy\n\n        elif t == \"referent\":\n            t = (\n                \"\\n\"\n                \"A value is trying to be set on a copy of a slice from a \"\n                \"DataFrame\\n\\n\"\n                \"See the caveats in the documentation: \"\n                \"https://pandas.pydata.org/pandas-docs/stable/user_guide/\"\n                \"indexing.html#returning-a-view-versus-a-copy\"\n            )\n\n        else:\n            t = (\n                \"\\n\"\n                \"A value is trying to be set on a copy of a slice from a \"\n                \"DataFrame.\\n\"\n                \"Try using .loc[row_indexer,col_indexer] = value \"\n                \"instead\\n\\nSee the caveats in the documentation: \"\n                \"https://pandas.pydata.org/pandas-docs/stable/user_guide/\"\n                \"indexing.html#returning-a-view-versus-a-copy\"\n            )\n\n        if value == \"raise\":\n            raise com.SettingWithCopyError(t)\n        elif value == \"warn\":\n            warnings.warn(t, com.SettingWithCopyWarning, stacklevel=stacklevel)\n\n    def __delitem__(self, key) -> None:\n        \"\"\"\n        Delete item\n        \"\"\"\n        deleted = False\n\n        maybe_shortcut = False\n        if self.ndim == 2 and isinstance(self.columns, MultiIndex):\n            try:\n                maybe_shortcut = key not in self.columns._engine\n            except TypeError:\n                pass\n\n        if maybe_shortcut:\n            # Allow shorthand to delete all columns whose first len(key)\n            # elements match key:\n            if not isinstance(key, tuple):\n                key = (key,)\n            for col in self.columns:\n                if isinstance(col, tuple) and col[: len(key)] == key:\n                    del self[col]\n                    deleted = True\n        if not deleted:\n            # If the above loop ran and didn't delete anything because\n            # there was no match, this call should raise the appropriate\n            # exception:\n            loc = self.axes[-1].get_loc(key)\n            self._mgr.idelete(loc)\n\n        # delete from the caches\n        try:\n            del self._item_cache[key]\n        except KeyError:\n            pass\n\n    # ----------------------------------------------------------------------\n    # Unsorted\n\n    def _check_inplace_and_allows_duplicate_labels(self, inplace):\n        if inplace and not self.flags.allows_duplicate_labels:\n            raise ValueError(\n                \"Cannot specify 'inplace=True' when \"\n                \"'self.flags.allows_duplicate_labels' is False.\"\n            )\n\n    def get(self, key, default=None):\n        \"\"\"\n        Get item from object for given key (ex: DataFrame column).\n\n        Returns default value if not found.\n\n        Parameters\n        ----------\n        key : object\n\n        Returns\n        -------\n        value : same type as items contained in object\n        \"\"\"\n        try:\n            return self[key]\n        except (KeyError, ValueError, IndexError):\n            return default\n\n    @property\n    def _is_view(self) -> bool_t:\n        \"\"\"Return boolean indicating if self is view of another array \"\"\"\n        return self._mgr.is_view\n\n    def reindex_like(\n        self: FrameOrSeries,\n        other,\n        method: Optional[str] = None,\n        copy: bool_t = True,\n        limit=None,\n        tolerance=None,\n    ) -> FrameOrSeries:\n        \"\"\"\n        Return an object with matching indices as other object.\n\n        Conform the object to the same index on all axes. Optional\n        filling logic, placing NaN in locations having no value\n        in the previous index. A new object is produced unless the\n        new index is equivalent to the current one and copy=False.\n\n        Parameters\n        ----------\n        other : Object of the same data type\n            Its row and column indices are used to define the new indices\n            of this object.\n        method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}\n            Method to use for filling holes in reindexed DataFrame.\n            Please note: this is only applicable to DataFrames/Series with a\n            monotonically increasing/decreasing index.\n\n            * None (default): don't fill gaps\n            * pad / ffill: propagate last valid observation forward to next\n              valid\n            * backfill / bfill: use next valid observation to fill gap\n            * nearest: use nearest valid observations to fill gap.\n\n        copy : bool, default True\n            Return a new object, even if the passed indexes are the same.\n        limit : int, default None\n            Maximum number of consecutive labels to fill for inexact matches.\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n\n            Tolerance may be a scalar value, which applies the same tolerance\n            to all values, or list-like, which applies variable tolerance per\n            element. List-like includes list, tuple, array, Series, and must be\n            the same size as the index and its dtype must exactly match the\n            index's type.\n\n        Returns\n        -------\n        Series or DataFrame\n            Same type as caller, but with changed indices on each axis.\n\n        See Also\n        --------\n        DataFrame.set_index : Set row labels.\n        DataFrame.reset_index : Remove row labels or move them to new columns.\n        DataFrame.reindex : Change to new indices or expand indices.\n\n        Notes\n        -----\n        Same as calling\n        ``.reindex(index=other.index, columns=other.columns,...)``.\n\n        Examples\n        --------\n        >>> df1 = pd.DataFrame([[24.3, 75.7, 'high'],\n        ...                     [31, 87.8, 'high'],\n        ...                     [22, 71.6, 'medium'],\n        ...                     [35, 95, 'medium']],\n        ...                    columns=['temp_celsius', 'temp_fahrenheit',\n        ...                             'windspeed'],\n        ...                    index=pd.date_range(start='2014-02-12',\n        ...                                        end='2014-02-15', freq='D'))\n\n        >>> df1\n                    temp_celsius  temp_fahrenheit windspeed\n        2014-02-12          24.3             75.7      high\n        2014-02-13          31.0             87.8      high\n        2014-02-14          22.0             71.6    medium\n        2014-02-15          35.0             95.0    medium\n\n        >>> df2 = pd.DataFrame([[28, 'low'],\n        ...                     [30, 'low'],\n        ...                     [35.1, 'medium']],\n        ...                    columns=['temp_celsius', 'windspeed'],\n        ...                    index=pd.DatetimeIndex(['2014-02-12', '2014-02-13',\n        ...                                            '2014-02-15']))\n\n        >>> df2\n                    temp_celsius windspeed\n        2014-02-12          28.0       low\n        2014-02-13          30.0       low\n        2014-02-15          35.1    medium\n\n        >>> df2.reindex_like(df1)\n                    temp_celsius  temp_fahrenheit windspeed\n        2014-02-12          28.0              NaN       low\n        2014-02-13          30.0              NaN       low\n        2014-02-14           NaN              NaN       NaN\n        2014-02-15          35.1              NaN    medium\n        \"\"\"\n        d = other._construct_axes_dict(\n            axes=self._AXIS_ORDERS,\n            method=method,\n            copy=copy,\n            limit=limit,\n            tolerance=tolerance,\n        )\n\n        return self.reindex(**d)\n\n    def drop(\n        self,\n        labels=None,\n        axis=0,\n        index=None,\n        columns=None,\n        level=None,\n        inplace: bool_t = False,\n        errors: str = \"raise\",\n    ):\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        if labels is not None:\n            if index is not None or columns is not None:\n                raise ValueError(\"Cannot specify both 'labels' and 'index'/'columns'\")\n            axis_name = self._get_axis_name(axis)\n            axes = {axis_name: labels}\n        elif index is not None or columns is not None:\n            axes, _ = self._construct_axes_from_arguments((index, columns), {})\n        else:\n            raise ValueError(\n                \"Need to specify at least one of 'labels', 'index' or 'columns'\"\n            )\n\n        obj = self\n\n        for axis, labels in axes.items():\n            if labels is not None:\n                obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n\n        if inplace:\n            self._update_inplace(obj)\n        else:\n            return obj\n\n    def _drop_axis(\n        self: FrameOrSeries, labels, axis, level=None, errors: str = \"raise\"\n    ) -> FrameOrSeries:\n        \"\"\"\n        Drop labels from specified axis. Used in the ``drop`` method\n        internally.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n        axis : int or axis name\n        level : int or level name, default None\n            For MultiIndex\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and existing labels are dropped.\n\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        axis_name = self._get_axis_name(axis)\n        axis = self._get_axis(axis)\n\n        if axis.is_unique:\n            if level is not None:\n                if not isinstance(axis, MultiIndex):\n                    raise AssertionError(\"axis must be a MultiIndex\")\n                new_axis = axis.drop(labels, level=level, errors=errors)\n            else:\n                new_axis = axis.drop(labels, errors=errors)\n            result = self.reindex(**{axis_name: new_axis})\n\n        # Case for non-unique axis\n        else:\n            labels = ensure_object(com.index_labels_to_array(labels))\n            if level is not None:\n                if not isinstance(axis, MultiIndex):\n                    raise AssertionError(\"axis must be a MultiIndex\")\n                indexer = ~axis.get_level_values(level).isin(labels)\n\n                # GH 18561 MultiIndex.drop should raise if label is absent\n                if errors == \"raise\" and indexer.all():\n                    raise KeyError(f\"{labels} not found in axis\")\n            else:\n                indexer = ~axis.isin(labels)\n                # Check if label doesn't exist along axis\n                labels_missing = (axis.get_indexer_for(labels) == -1).any()\n                if errors == \"raise\" and labels_missing:\n                    raise KeyError(f\"{labels} not found in axis\")\n\n            slicer = [slice(None)] * self.ndim\n            slicer[self._get_axis_number(axis_name)] = indexer\n\n            result = self.loc[tuple(slicer)]\n\n        return result\n\n    def _update_inplace(self, result, verify_is_copy: bool_t = True) -> None:\n        \"\"\"\n        Replace self internals with result.\n\n        Parameters\n        ----------\n        result : same type as self\n        verify_is_copy : bool, default True\n            Provide is_copy checks.\n        \"\"\"\n        # NOTE: This does *not* call __finalize__ and that's an explicit\n        # decision that we may revisit in the future.\n        self._reset_cache()\n        self._clear_item_cache()\n        self._mgr = result._mgr\n        self._maybe_update_cacher(verify_is_copy=verify_is_copy)\n\n    def add_prefix(self: FrameOrSeries, prefix: str) -> FrameOrSeries:\n        \"\"\"\n        Prefix labels with string `prefix`.\n\n        For Series, the row labels are prefixed.\n        For DataFrame, the column labels are prefixed.\n\n        Parameters\n        ----------\n        prefix : str\n            The string to add before each label.\n\n        Returns\n        -------\n        Series or DataFrame\n            New Series or DataFrame with updated labels.\n\n        See Also\n        --------\n        Series.add_suffix: Suffix row labels with string `suffix`.\n        DataFrame.add_suffix: Suffix column labels with string `suffix`.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        dtype: int64\n\n        >>> s.add_prefix('item_')\n        item_0    1\n        item_1    2\n        item_2    3\n        item_3    4\n        dtype: int64\n\n        >>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n        >>> df\n           A  B\n        0  1  3\n        1  2  4\n        2  3  5\n        3  4  6\n\n        >>> df.add_prefix('col_')\n             col_A  col_B\n        0       1       3\n        1       2       4\n        2       3       5\n        3       4       6\n        \"\"\"\n        f = functools.partial(\"{prefix}{}\".format, prefix=prefix)\n\n        mapper = {self._info_axis_name: f}\n        # error: Incompatible return value type (got \"Optional[FrameOrSeries]\",\n        # expected \"FrameOrSeries\")\n        # error: Argument 1 to \"rename\" of \"NDFrame\" has incompatible type\n        # \"**Dict[str, partial[str]]\"; expected \"Union[str, int, None]\"\n        return self.rename(**mapper)  # type: ignore[return-value, arg-type]\n\n    def add_suffix(self: FrameOrSeries, suffix: str) -> FrameOrSeries:\n        \"\"\"\n        Suffix labels with string `suffix`.\n\n        For Series, the row labels are suffixed.\n        For DataFrame, the column labels are suffixed.\n\n        Parameters\n        ----------\n        suffix : str\n            The string to add after each label.\n\n        Returns\n        -------\n        Series or DataFrame\n            New Series or DataFrame with updated labels.\n\n        See Also\n        --------\n        Series.add_prefix: Prefix row labels with string `prefix`.\n        DataFrame.add_prefix: Prefix column labels with string `prefix`.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        dtype: int64\n\n        >>> s.add_suffix('_item')\n        0_item    1\n        1_item    2\n        2_item    3\n        3_item    4\n        dtype: int64\n\n        >>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n        >>> df\n           A  B\n        0  1  3\n        1  2  4\n        2  3  5\n        3  4  6\n\n        >>> df.add_suffix('_col')\n             A_col  B_col\n        0       1       3\n        1       2       4\n        2       3       5\n        3       4       6\n        \"\"\"\n        f = functools.partial(\"{}{suffix}\".format, suffix=suffix)\n\n        mapper = {self._info_axis_name: f}\n        # error: Incompatible return value type (got \"Optional[FrameOrSeries]\",\n        # expected \"FrameOrSeries\")\n        # error: Argument 1 to \"rename\" of \"NDFrame\" has incompatible type\n        # \"**Dict[str, partial[str]]\"; expected \"Union[str, int, None]\"\n        return self.rename(**mapper)  # type: ignore[return-value, arg-type]\n\n    def sort_values(\n        self,\n        axis=0,\n        ascending=True,\n        inplace: bool_t = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        ignore_index: bool_t = False,\n        key: ValueKeyFunc = None,\n    ):\n        \"\"\"\n        Sort by the values along either axis.\n\n        Parameters\n        ----------%(optional_by)s\n        axis : %(axes_single_arg)s, default 0\n             Axis to be sorted.\n        ascending : bool or list of bool, default True\n             Sort ascending vs. descending. Specify list for multiple sort\n             orders.  If this is a list of bools, must match the length of\n             the by.\n        inplace : bool, default False\n             If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort'\n             Choice of sorting algorithm. See also ndarray.np.sort for more\n             information.  `mergesort` is the only stable algorithm. For\n             DataFrames, this option is only applied when sorting on a single\n             column or label.\n        na_position : {'first', 'last'}, default 'last'\n             Puts NaNs at the beginning if `first`; `last` puts NaNs at the\n             end.\n        ignore_index : bool, default False\n             If True, the resulting axis will be labeled 0, 1, …, n - 1.\n\n             .. versionadded:: 1.0.0\n\n        key : callable, optional\n            Apply the key function to the values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect a\n            ``Series`` and return a Series with the same shape as the input.\n            It will be applied to each column in `by` independently.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with sorted values or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.sort_index : Sort a DataFrame by the index.\n        Series.sort_values : Similar method for a Series.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\n        ...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n        ...     'col2': [2, 1, 9, 8, 7, 4],\n        ...     'col3': [0, 1, 9, 4, 2, 3],\n        ...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n        ... })\n        >>> df\n          col1  col2  col3 col4\n        0    A     2     0    a\n        1    A     1     1    B\n        2    B     9     9    c\n        3  NaN     8     4    D\n        4    D     7     2    e\n        5    C     4     3    F\n\n        Sort by col1\n\n        >>> df.sort_values(by=['col1'])\n          col1  col2  col3 col4\n        0    A     2     0    a\n        1    A     1     1    B\n        2    B     9     9    c\n        5    C     4     3    F\n        4    D     7     2    e\n        3  NaN     8     4    D\n\n        Sort by multiple columns\n\n        >>> df.sort_values(by=['col1', 'col2'])\n          col1  col2  col3 col4\n        1    A     1     1    B\n        0    A     2     0    a\n        2    B     9     9    c\n        5    C     4     3    F\n        4    D     7     2    e\n        3  NaN     8     4    D\n\n        Sort Descending\n\n        >>> df.sort_values(by='col1', ascending=False)\n          col1  col2  col3 col4\n        4    D     7     2    e\n        5    C     4     3    F\n        2    B     9     9    c\n        0    A     2     0    a\n        1    A     1     1    B\n        3  NaN     8     4    D\n\n        Putting NAs first\n\n        >>> df.sort_values(by='col1', ascending=False, na_position='first')\n          col1  col2  col3 col4\n        3  NaN     8     4    D\n        4    D     7     2    e\n        5    C     4     3    F\n        2    B     9     9    c\n        0    A     2     0    a\n        1    A     1     1    B\n\n        Sorting with a key function\n\n        >>> df.sort_values(by='col4', key=lambda col: col.str.lower())\n           col1  col2  col3 col4\n        0    A     2     0    a\n        1    A     1     1    B\n        2    B     9     9    c\n        3  NaN     8     4    D\n        4    D     7     2    e\n        5    C     4     3    F\n\n        Natural sort with the key argument,\n        using the `natsort <https://github.com/SethMMorton/natsort>` package.\n\n        >>> df = pd.DataFrame({\n        ...    \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n        ...    \"value\": [10, 20, 30, 40, 50]\n        ... })\n        >>> df\n            time  value\n        0    0hr     10\n        1  128hr     20\n        2   72hr     30\n        3   48hr     40\n        4   96hr     50\n        >>> from natsort import index_natsorted\n        >>> df.sort_values(\n        ...    by=\"time\",\n        ...    key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n        ... )\n            time  value\n        0    0hr     10\n        3   48hr     40\n        2   72hr     30\n        4   96hr     50\n        1  128hr     20\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def sort_index(\n        self,\n        axis=0,\n        level=None,\n        ascending: bool_t = True,\n        inplace: bool_t = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        sort_remaining: bool_t = True,\n        ignore_index: bool_t = False,\n        key: IndexKeyFunc = None,\n    ):\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        axis = self._get_axis_number(axis)\n        target = self._get_axis(axis)\n\n        indexer = get_indexer_indexer(\n            target, level, ascending, kind, na_position, sort_remaining, key\n        )\n\n        if indexer is None:\n            if inplace:\n                return\n            else:\n                return self.copy()\n\n        baxis = self._get_block_manager_axis(axis)\n        new_data = self._mgr.take(indexer, axis=baxis, verify=False)\n\n        # reconstruct axis if needed\n        new_data.axes[baxis] = new_data.axes[baxis]._sort_levels_monotonic()\n\n        if ignore_index:\n            axis = 1 if isinstance(self, ABCDataFrame) else 0\n            new_data.axes[axis] = ibase.default_index(len(indexer))\n\n        result = self._constructor(new_data)\n\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_index\")\n\n    @doc(\n        klass=_shared_doc_kwargs[\"klass\"],\n        axes=_shared_doc_kwargs[\"axes\"],\n        optional_labels=\"\",\n        optional_axis=\"\",\n    )\n    def reindex(self: FrameOrSeries, *args, **kwargs) -> FrameOrSeries:\n        \"\"\"\n        Conform {klass} to new index with optional filling logic.\n\n        Places NA/NaN in locations having no value in the previous index. A new object\n        is produced unless the new index is equivalent to the current one and\n        ``copy=False``.\n\n        Parameters\n        ----------\n        {optional_labels}\n        {axes} : array-like, optional\n            New labels / index to conform to, should be specified using\n            keywords. Preferably an Index object to avoid duplicating data.\n        {optional_axis}\n        method : {{None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}}\n            Method to use for filling holes in reindexed DataFrame.\n            Please note: this is only applicable to DataFrames/Series with a\n            monotonically increasing/decreasing index.\n\n            * None (default): don't fill gaps\n            * pad / ffill: Propagate last valid observation forward to next\n              valid.\n            * backfill / bfill: Use next valid observation to fill gap.\n            * nearest: Use nearest valid observations to fill gap.\n\n        copy : bool, default True\n            Return a new object, even if the passed indexes are the same.\n        level : int or name\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level.\n        fill_value : scalar, default np.NaN\n            Value to use for missing values. Defaults to NaN, but can be any\n            \"compatible\" value.\n        limit : int, default None\n            Maximum number of consecutive elements to forward or backward fill.\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations most\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n\n            Tolerance may be a scalar value, which applies the same tolerance\n            to all values, or list-like, which applies variable tolerance per\n            element. List-like includes list, tuple, array, Series, and must be\n            the same size as the index and its dtype must exactly match the\n            index's type.\n\n        Returns\n        -------\n        {klass} with changed index.\n\n        See Also\n        --------\n        DataFrame.set_index : Set row labels.\n        DataFrame.reset_index : Remove row labels or move them to new columns.\n        DataFrame.reindex_like : Change to same indices as other DataFrame.\n\n        Examples\n        --------\n        ``DataFrame.reindex`` supports two calling conventions\n\n        * ``(index=index_labels, columns=column_labels, ...)``\n        * ``(labels, axis={{'index', 'columns'}}, ...)``\n\n        We *highly* recommend using keyword arguments to clarify your\n        intent.\n\n        Create a dataframe with some fictional data.\n\n        >>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']\n        >>> df = pd.DataFrame({{'http_status': [200, 200, 404, 404, 301],\n        ...                   'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]}},\n        ...                   index=index)\n        >>> df\n                   http_status  response_time\n        Firefox            200           0.04\n        Chrome             200           0.02\n        Safari             404           0.07\n        IE10               404           0.08\n        Konqueror          301           1.00\n\n        Create a new index and reindex the dataframe. By default\n        values in the new index that do not have corresponding\n        records in the dataframe are assigned ``NaN``.\n\n        >>> new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n        ...              'Chrome']\n        >>> df.reindex(new_index)\n                       http_status  response_time\n        Safari               404.0           0.07\n        Iceweasel              NaN            NaN\n        Comodo Dragon          NaN            NaN\n        IE10                 404.0           0.08\n        Chrome               200.0           0.02\n\n        We can fill in the missing values by passing a value to\n        the keyword ``fill_value``. Because the index is not monotonically\n        increasing or decreasing, we cannot use arguments to the keyword\n        ``method`` to fill the ``NaN`` values.\n\n        >>> df.reindex(new_index, fill_value=0)\n                       http_status  response_time\n        Safari                 404           0.07\n        Iceweasel                0           0.00\n        Comodo Dragon            0           0.00\n        IE10                   404           0.08\n        Chrome                 200           0.02\n\n        >>> df.reindex(new_index, fill_value='missing')\n                      http_status response_time\n        Safari                404          0.07\n        Iceweasel         missing       missing\n        Comodo Dragon     missing       missing\n        IE10                  404          0.08\n        Chrome                200          0.02\n\n        We can also reindex the columns.\n\n        >>> df.reindex(columns=['http_status', 'user_agent'])\n                   http_status  user_agent\n        Firefox            200         NaN\n        Chrome             200         NaN\n        Safari             404         NaN\n        IE10               404         NaN\n        Konqueror          301         NaN\n\n        Or we can use \"axis-style\" keyword arguments\n\n        >>> df.reindex(['http_status', 'user_agent'], axis=\"columns\")\n                   http_status  user_agent\n        Firefox            200         NaN\n        Chrome             200         NaN\n        Safari             404         NaN\n        IE10               404         NaN\n        Konqueror          301         NaN\n\n        To further illustrate the filling functionality in\n        ``reindex``, we will create a dataframe with a\n        monotonically increasing index (for example, a sequence\n        of dates).\n\n        >>> date_index = pd.date_range('1/1/2010', periods=6, freq='D')\n        >>> df2 = pd.DataFrame({{\"prices\": [100, 101, np.nan, 100, 89, 88]}},\n        ...                    index=date_index)\n        >>> df2\n                    prices\n        2010-01-01   100.0\n        2010-01-02   101.0\n        2010-01-03     NaN\n        2010-01-04   100.0\n        2010-01-05    89.0\n        2010-01-06    88.0\n\n        Suppose we decide to expand the dataframe to cover a wider\n        date range.\n\n        >>> date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')\n        >>> df2.reindex(date_index2)\n                    prices\n        2009-12-29     NaN\n        2009-12-30     NaN\n        2009-12-31     NaN\n        2010-01-01   100.0\n        2010-01-02   101.0\n        2010-01-03     NaN\n        2010-01-04   100.0\n        2010-01-05    89.0\n        2010-01-06    88.0\n        2010-01-07     NaN\n\n        The index entries that did not have a value in the original data frame\n        (for example, '2009-12-29') are by default filled with ``NaN``.\n        If desired, we can fill in the missing values using one of several\n        options.\n\n        For example, to back-propagate the last valid value to fill the ``NaN``\n        values, pass ``bfill`` as an argument to the ``method`` keyword.\n\n        >>> df2.reindex(date_index2, method='bfill')\n                    prices\n        2009-12-29   100.0\n        2009-12-30   100.0\n        2009-12-31   100.0\n        2010-01-01   100.0\n        2010-01-02   101.0\n        2010-01-03     NaN\n        2010-01-04   100.0\n        2010-01-05    89.0\n        2010-01-06    88.0\n        2010-01-07     NaN\n\n        Please note that the ``NaN`` value present in the original dataframe\n        (at index value 2010-01-03) will not be filled by any of the\n        value propagation schemes. This is because filling while reindexing\n        does not look at dataframe values, but only compares the original and\n        desired indexes. If you do want to fill in the ``NaN`` values present\n        in the original dataframe, use the ``fillna()`` method.\n\n        See the :ref:`user guide <basics.reindexing>` for more.\n        \"\"\"\n        # TODO: Decide if we care about having different examples for different\n        # kinds\n\n        # construct the args\n        axes, kwargs = self._construct_axes_from_arguments(args, kwargs)\n        method = missing.clean_reindex_fill_method(kwargs.pop(\"method\", None))\n        level = kwargs.pop(\"level\", None)\n        copy = kwargs.pop(\"copy\", True)\n        limit = kwargs.pop(\"limit\", None)\n        tolerance = kwargs.pop(\"tolerance\", None)\n        fill_value = kwargs.pop(\"fill_value\", None)\n\n        # Series.reindex doesn't use / need the axis kwarg\n        # We pop and ignore it here, to make writing Series/Frame generic code\n        # easier\n        kwargs.pop(\"axis\", None)\n\n        if kwargs:\n            raise TypeError(\n                \"reindex() got an unexpected keyword \"\n                f'argument \"{list(kwargs.keys())[0]}\"'\n            )\n\n        self._consolidate_inplace()\n\n        # if all axes that are requested to reindex are equal, then only copy\n        # if indicated must have index names equal here as well as values\n        if all(\n            self._get_axis(axis).identical(ax)\n            for axis, ax in axes.items()\n            if ax is not None\n        ):\n            if copy:\n                return self.copy()\n            return self\n\n        # check if we are a multi reindex\n        if self._needs_reindex_multi(axes, method, level):\n            return self._reindex_multi(axes, copy, fill_value)\n\n        # perform the reindex on the axes\n        return self._reindex_axes(\n            axes, level, limit, tolerance, method, fill_value, copy\n        ).__finalize__(self, method=\"reindex\")\n\n    def _reindex_axes(\n        self: FrameOrSeries, axes, level, limit, tolerance, method, fill_value, copy\n    ) -> FrameOrSeries:\n        \"\"\"Perform the reindex for all the axes.\"\"\"\n        obj = self\n        for a in self._AXIS_ORDERS:\n            labels = axes[a]\n            if labels is None:\n                continue\n\n            ax = self._get_axis(a)\n            new_index, indexer = ax.reindex(\n                labels, level=level, limit=limit, tolerance=tolerance, method=method\n            )\n\n            axis = self._get_axis_number(a)\n            obj = obj._reindex_with_indexers(\n                {axis: [new_index, indexer]},\n                fill_value=fill_value,\n                copy=copy,\n                allow_dups=False,\n            )\n\n        return obj\n\n    def _needs_reindex_multi(self, axes, method, level) -> bool_t:\n        \"\"\"Check if we do need a multi reindex.\"\"\"\n        return (\n            (com.count_not_none(*axes.values()) == self._AXIS_LEN)\n            and method is None\n            and level is None\n            and not self._is_mixed_type\n        )\n\n    def _reindex_multi(self, axes, copy, fill_value):\n        raise AbstractMethodError(self)\n\n    def _reindex_with_indexers(\n        self: FrameOrSeries,\n        reindexers,\n        fill_value=None,\n        copy: bool_t = False,\n        allow_dups: bool_t = False,\n    ) -> FrameOrSeries:\n        \"\"\"allow_dups indicates an internal call here \"\"\"\n        # reindex doing multiple operations on different axes if indicated\n        new_data = self._mgr\n        for axis in sorted(reindexers.keys()):\n            index, indexer = reindexers[axis]\n            baxis = self._get_block_manager_axis(axis)\n\n            if index is None:\n                continue\n\n            index = ensure_index(index)\n            if indexer is not None:\n                indexer = ensure_int64(indexer)\n\n            # TODO: speed up on homogeneous DataFrame objects\n            new_data = new_data.reindex_indexer(\n                index,\n                indexer,\n                axis=baxis,\n                fill_value=fill_value,\n                allow_dups=allow_dups,\n                copy=copy,\n            )\n            # If we've made a copy once, no need to make another one\n            copy = False\n\n        if copy and new_data is self._mgr:\n            new_data = new_data.copy()\n\n        return self._constructor(new_data).__finalize__(self)\n\n    def filter(\n        self: FrameOrSeries,\n        items=None,\n        like: Optional[str] = None,\n        regex: Optional[str] = None,\n        axis=None,\n    ) -> FrameOrSeries:\n        \"\"\"\n        Subset the dataframe rows or columns according to the specified index labels.\n\n        Note that this routine does not filter a dataframe on its\n        contents. The filter is applied to the labels of the index.\n\n        Parameters\n        ----------\n        items : list-like\n            Keep labels from axis which are in items.\n        like : str\n            Keep labels from axis for which \"like in label == True\".\n        regex : str (regular expression)\n            Keep labels from axis for which re.search(regex, label) == True.\n        axis : {0 or ‘index’, 1 or ‘columns’, None}, default None\n            The axis to filter on, expressed either as an index (int)\n            or axis name (str). By default this is the info axis,\n            'index' for Series, 'columns' for DataFrame.\n\n        Returns\n        -------\n        same type as input object\n\n        See Also\n        --------\n        DataFrame.loc : Access a group of rows and columns\n            by label(s) or a boolean array.\n\n        Notes\n        -----\n        The ``items``, ``like``, and ``regex`` parameters are\n        enforced to be mutually exclusive.\n\n        ``axis`` defaults to the info axis that is used when indexing\n        with ``[]``.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n        ...                   index=['mouse', 'rabbit'],\n        ...                   columns=['one', 'two', 'three'])\n        >>> df\n                one  two  three\n        mouse     1    2      3\n        rabbit    4    5      6\n\n        >>> # select columns by name\n        >>> df.filter(items=['one', 'three'])\n                 one  three\n        mouse     1      3\n        rabbit    4      6\n\n        >>> # select columns by regular expression\n        >>> df.filter(regex='e$', axis=1)\n                 one  three\n        mouse     1      3\n        rabbit    4      6\n\n        >>> # select rows containing 'bbi'\n        >>> df.filter(like='bbi', axis=0)\n                 one  two  three\n        rabbit    4    5      6\n        \"\"\"\n        nkw = com.count_not_none(items, like, regex)\n        if nkw > 1:\n            raise TypeError(\n                \"Keyword arguments `items`, `like`, or `regex` \"\n                \"are mutually exclusive\"\n            )\n\n        if axis is None:\n            axis = self._info_axis_name\n        labels = self._get_axis(axis)\n\n        if items is not None:\n            name = self._get_axis_name(axis)\n            return self.reindex(**{name: [r for r in items if r in labels]})\n        elif like:\n\n            def f(x) -> bool:\n                assert like is not None  # needed for mypy\n                return like in ensure_str(x)\n\n            values = labels.map(f)\n            return self.loc(axis=axis)[values]\n        elif regex:\n\n            def f(x) -> bool:\n                return matcher.search(ensure_str(x)) is not None\n\n            matcher = re.compile(regex)\n            values = labels.map(f)\n            return self.loc(axis=axis)[values]\n        else:\n            raise TypeError(\"Must pass either `items`, `like`, or `regex`\")\n\n    def head(self: FrameOrSeries, n: int = 5) -> FrameOrSeries:\n        \"\"\"\n        Return the first `n` rows.\n\n        This function returns the first `n` rows for the object based\n        on position. It is useful for quickly testing if your object\n        has the right type of data in it.\n\n        For negative values of `n`, this function returns all rows except\n        the last `n` rows, equivalent to ``df[:-n]``.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Number of rows to select.\n\n        Returns\n        -------\n        same type as caller\n            The first `n` rows of the caller object.\n\n        See Also\n        --------\n        DataFrame.tail: Returns the last `n` rows.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n        ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n        >>> df\n              animal\n        0  alligator\n        1        bee\n        2     falcon\n        3       lion\n        4     monkey\n        5     parrot\n        6      shark\n        7      whale\n        8      zebra\n\n        Viewing the first 5 lines\n\n        >>> df.head()\n              animal\n        0  alligator\n        1        bee\n        2     falcon\n        3       lion\n        4     monkey\n\n        Viewing the first `n` lines (three in this case)\n\n        >>> df.head(3)\n              animal\n        0  alligator\n        1        bee\n        2     falcon\n\n        For negative values of `n`\n\n        >>> df.head(-3)\n              animal\n        0  alligator\n        1        bee\n        2     falcon\n        3       lion\n        4     monkey\n        5     parrot\n        \"\"\"\n        return self.iloc[:n]\n\n    def tail(self: FrameOrSeries, n: int = 5) -> FrameOrSeries:\n        \"\"\"\n        Return the last `n` rows.\n\n        This function returns last `n` rows from the object based on\n        position. It is useful for quickly verifying data, for example,\n        after sorting or appending rows.\n\n        For negative values of `n`, this function returns all rows except\n        the first `n` rows, equivalent to ``df[n:]``.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Number of rows to select.\n\n        Returns\n        -------\n        type of caller\n            The last `n` rows of the caller object.\n\n        See Also\n        --------\n        DataFrame.head : The first `n` rows of the caller object.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n        ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n        >>> df\n              animal\n        0  alligator\n        1        bee\n        2     falcon\n        3       lion\n        4     monkey\n        5     parrot\n        6      shark\n        7      whale\n        8      zebra\n\n        Viewing the last 5 lines\n\n        >>> df.tail()\n           animal\n        4  monkey\n        5  parrot\n        6   shark\n        7   whale\n        8   zebra\n\n        Viewing the last `n` lines (three in this case)\n\n        >>> df.tail(3)\n          animal\n        6  shark\n        7  whale\n        8  zebra\n\n        For negative values of `n`\n\n        >>> df.tail(-3)\n           animal\n        3    lion\n        4  monkey\n        5  parrot\n        6   shark\n        7   whale\n        8   zebra\n        \"\"\"\n        if n == 0:\n            return self.iloc[0:0]\n        return self.iloc[-n:]\n\n    def sample(\n        self: FrameOrSeries,\n        n=None,\n        frac=None,\n        replace=False,\n        weights=None,\n        random_state=None,\n        axis=None,\n    ) -> FrameOrSeries:\n        \"\"\"\n        Return a random sample of items from an axis of object.\n\n        You can use `random_state` for reproducibility.\n\n        Parameters\n        ----------\n        n : int, optional\n            Number of items from axis to return. Cannot be used with `frac`.\n            Default = 1 if `frac` = None.\n        frac : float, optional\n            Fraction of axis items to return. Cannot be used with `n`.\n        replace : bool, default False\n            Allow or disallow sampling of the same row more than once.\n        weights : str or ndarray-like, optional\n            Default 'None' results in equal probability weighting.\n            If passed a Series, will align with target object on index. Index\n            values in weights not found in sampled object will be ignored and\n            index values in sampled object not in weights will be assigned\n            weights of zero.\n            If called on a DataFrame, will accept the name of a column\n            when axis = 0.\n            Unless weights are a Series, weights must be same length as axis\n            being sampled.\n            If weights do not sum to 1, they will be normalized to sum to 1.\n            Missing values in the weights column will be treated as zero.\n            Infinite values not allowed.\n        random_state : int, array-like, BitGenerator, np.random.RandomState, optional\n            If int, array-like, or BitGenerator (NumPy>=1.17), seed for\n            random number generator\n            If np.random.RandomState, use as numpy RandomState object.\n\n            .. versionchanged:: 1.1.0\n\n                array-like and BitGenerator (for NumPy>=1.17) object now passed to\n                np.random.RandomState() as seed\n\n        axis : {0 or ‘index’, 1 or ‘columns’, None}, default None\n            Axis to sample. Accepts axis number or name. Default is stat axis\n            for given data type (0 for Series and DataFrames).\n\n        Returns\n        -------\n        Series or DataFrame\n            A new object of same type as caller containing `n` items randomly\n            sampled from the caller object.\n\n        See Also\n        --------\n        DataFrameGroupBy.sample: Generates random samples from each group of a\n            DataFrame object.\n        SeriesGroupBy.sample: Generates random samples from each group of a\n            Series object.\n        numpy.random.choice: Generates a random sample from a given 1-D numpy\n            array.\n\n        Notes\n        -----\n        If `frac` > 1, `replacement` should be set to `True`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [2, 4, 8, 0],\n        ...                    'num_wings': [2, 0, 0, 0],\n        ...                    'num_specimen_seen': [10, 2, 1, 8]},\n        ...                   index=['falcon', 'dog', 'spider', 'fish'])\n        >>> df\n                num_legs  num_wings  num_specimen_seen\n        falcon         2          2                 10\n        dog            4          0                  2\n        spider         8          0                  1\n        fish           0          0                  8\n\n        Extract 3 random elements from the ``Series`` ``df['num_legs']``:\n        Note that we use `random_state` to ensure the reproducibility of\n        the examples.\n\n        >>> df['num_legs'].sample(n=3, random_state=1)\n        fish      0\n        spider    8\n        falcon    2\n        Name: num_legs, dtype: int64\n\n        A random 50% sample of the ``DataFrame`` with replacement:\n\n        >>> df.sample(frac=0.5, replace=True, random_state=1)\n              num_legs  num_wings  num_specimen_seen\n        dog          4          0                  2\n        fish         0          0                  8\n\n        An upsample sample of the ``DataFrame`` with replacement:\n        Note that `replace` parameter has to be `True` for `frac` parameter > 1.\n\n        >>> df.sample(frac=2, replace=True, random_state=1)\n                num_legs  num_wings  num_specimen_seen\n        dog            4          0                  2\n        fish           0          0                  8\n        falcon         2          2                 10\n        falcon         2          2                 10\n        fish           0          0                  8\n        dog            4          0                  2\n        fish           0          0                  8\n        dog            4          0                  2\n\n        Using a DataFrame column as weights. Rows with larger value in the\n        `num_specimen_seen` column are more likely to be sampled.\n\n        >>> df.sample(n=2, weights='num_specimen_seen', random_state=1)\n                num_legs  num_wings  num_specimen_seen\n        falcon         2          2                 10\n        fish           0          0                  8\n        \"\"\"\n        if axis is None:\n            axis = self._stat_axis_number\n\n        axis = self._get_axis_number(axis)\n        axis_length = self.shape[axis]\n\n        # Process random_state argument\n        rs = com.random_state(random_state)\n\n        # Check weights for compliance\n        if weights is not None:\n\n            # If a series, align with frame\n            if isinstance(weights, ABCSeries):\n                weights = weights.reindex(self.axes[axis])\n\n            # Strings acceptable if a dataframe and axis = 0\n            if isinstance(weights, str):\n                if isinstance(self, ABCDataFrame):\n                    if axis == 0:\n                        try:\n                            weights = self[weights]\n                        except KeyError as err:\n                            raise KeyError(\n                                \"String passed to weights not a valid column\"\n                            ) from err\n                    else:\n                        raise ValueError(\n                            \"Strings can only be passed to \"\n                            \"weights when sampling from rows on \"\n                            \"a DataFrame\"\n                        )\n                else:\n                    raise ValueError(\n                        \"Strings cannot be passed as weights \"\n                        \"when sampling from a Series.\"\n                    )\n\n            weights = pd.Series(weights, dtype=\"float64\")\n\n            if len(weights) != axis_length:\n                raise ValueError(\n                    \"Weights and axis to be sampled must be of same length\"\n                )\n\n            if (weights == np.inf).any() or (weights == -np.inf).any():\n                raise ValueError(\"weight vector may not include `inf` values\")\n\n            if (weights < 0).any():\n                raise ValueError(\"weight vector many not include negative values\")\n\n            # If has nan, set to zero.\n            weights = weights.fillna(0)\n\n            # Renormalize if don't sum to 1\n            if weights.sum() != 1:\n                if weights.sum() != 0:\n                    weights = weights / weights.sum()\n                else:\n                    raise ValueError(\"Invalid weights: weights sum to zero\")\n\n            weights = weights._values\n\n        # If no frac or n, default to n=1.\n        if n is None and frac is None:\n            n = 1\n        elif frac is not None and frac > 1 and not replace:\n            raise ValueError(\n                \"Replace has to be set to `True` when \"\n                \"upsampling the population `frac` > 1.\"\n            )\n        elif n is not None and frac is None and n % 1 != 0:\n            raise ValueError(\"Only integers accepted as `n` values\")\n        elif n is None and frac is not None:\n            n = int(round(frac * axis_length))\n        elif n is not None and frac is not None:\n            raise ValueError(\"Please enter a value for `frac` OR `n`, not both\")\n\n        # Check for negative sizes\n        if n < 0:\n            raise ValueError(\n                \"A negative number of rows requested. Please provide positive value.\"\n            )\n\n        locs = rs.choice(axis_length, size=n, replace=replace, p=weights)\n        return self.take(locs, axis=axis)\n\n    @doc(klass=_shared_doc_kwargs[\"klass\"])\n    def pipe(self, func, *args, **kwargs):\n        r\"\"\"\n        Apply func(self, \\*args, \\*\\*kwargs).\n\n        Parameters\n        ----------\n        func : function\n            Function to apply to the {klass}.\n            ``args``, and ``kwargs`` are passed into ``func``.\n            Alternatively a ``(callable, data_keyword)`` tuple where\n            ``data_keyword`` is a string indicating the keyword of\n            ``callable`` that expects the {klass}.\n        args : iterable, optional\n            Positional arguments passed into ``func``.\n        kwargs : mapping, optional\n            A dictionary of keyword arguments passed into ``func``.\n\n        Returns\n        -------\n        object : the return type of ``func``.\n\n        See Also\n        --------\n        DataFrame.apply : Apply a function along input axis of DataFrame.\n        DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n        Series.map : Apply a mapping correspondence on a\n            :class:`~pandas.Series`.\n\n        Notes\n        -----\n        Use ``.pipe`` when chaining together functions that expect\n        Series, DataFrames or GroupBy objects. Instead of writing\n\n        >>> func(g(h(df), arg1=a), arg2=b, arg3=c)  # doctest: +SKIP\n\n        You can write\n\n        >>> (df.pipe(h)\n        ...    .pipe(g, arg1=a)\n        ...    .pipe(func, arg2=b, arg3=c)\n        ... )  # doctest: +SKIP\n\n        If you have a function that takes the data as (say) the second\n        argument, pass a tuple indicating which keyword expects the\n        data. For example, suppose ``f`` takes its data as ``arg2``:\n\n        >>> (df.pipe(h)\n        ...    .pipe(g, arg1=a)\n        ...    .pipe((func, 'arg2'), arg1=a, arg3=c)\n        ...  )  # doctest: +SKIP\n        \"\"\"\n        return com.pipe(self, func, *args, **kwargs)\n\n    # ----------------------------------------------------------------------\n    # Attribute access\n\n    def __finalize__(\n        self: FrameOrSeries, other, method: Optional[str] = None, **kwargs\n    ) -> FrameOrSeries:\n        \"\"\"\n        Propagate metadata from other to self.\n\n        Parameters\n        ----------\n        other : the object from which to get the attributes that we are going\n            to propagate\n        method : str, optional\n            A passed method name providing context on where ``__finalize__``\n            was called.\n\n            .. warning::\n\n               The value passed as `method` are not currently considered\n               stable across pandas releases.\n        \"\"\"\n        if isinstance(other, NDFrame):\n            for name in other.attrs:\n                self.attrs[name] = other.attrs[name]\n\n            self.flags.allows_duplicate_labels = other.flags.allows_duplicate_labels\n            # For subclasses using _metadata.\n            for name in set(self._metadata) & set(other._metadata):\n                assert isinstance(name, str)\n                object.__setattr__(self, name, getattr(other, name, None))\n\n        if method == \"concat\":\n            allows_duplicate_labels = all(\n                x.flags.allows_duplicate_labels for x in other.objs\n            )\n            self.flags.allows_duplicate_labels = allows_duplicate_labels\n\n        return self\n\n    def __getattr__(self, name: str):\n        \"\"\"\n        After regular attribute access, try looking up the name\n        This allows simpler access to columns for interactive use.\n        \"\"\"\n        # Note: obj.x will always call obj.__getattribute__('x') prior to\n        # calling obj.__getattr__('x').\n        if (\n            name in self._internal_names_set\n            or name in self._metadata\n            or name in self._accessors\n        ):\n            return object.__getattribute__(self, name)\n        else:\n            if self._info_axis._can_hold_identifiers_and_holds_name(name):\n                return self[name]\n            return object.__getattribute__(self, name)\n\n    def __setattr__(self, name: str, value) -> None:\n        \"\"\"\n        After regular attribute access, try setting the name\n        This allows simpler access to columns for interactive use.\n        \"\"\"\n        # first try regular attribute access via __getattribute__, so that\n        # e.g. ``obj.x`` and ``obj.x = 4`` will always reference/modify\n        # the same attribute.\n\n        try:\n            object.__getattribute__(self, name)\n            return object.__setattr__(self, name, value)\n        except AttributeError:\n            pass\n\n        # if this fails, go on to more involved attribute setting\n        # (note that this matches __getattr__, above).\n        if name in self._internal_names_set:\n            object.__setattr__(self, name, value)\n        elif name in self._metadata:\n            object.__setattr__(self, name, value)\n        else:\n            try:\n                existing = getattr(self, name)\n                if isinstance(existing, Index):\n                    object.__setattr__(self, name, value)\n                elif name in self._info_axis:\n                    self[name] = value\n                else:\n                    object.__setattr__(self, name, value)\n            except (AttributeError, TypeError):\n                if isinstance(self, ABCDataFrame) and (is_list_like(value)):\n                    warnings.warn(\n                        \"Pandas doesn't allow columns to be \"\n                        \"created via a new attribute name - see \"\n                        \"https://pandas.pydata.org/pandas-docs/\"\n                        \"stable/indexing.html#attribute-access\",\n                        stacklevel=2,\n                    )\n                object.__setattr__(self, name, value)\n\n    def _dir_additions(self) -> Set[str]:\n        \"\"\"\n        add the string-like attributes from the info_axis.\n        If info_axis is a MultiIndex, it's first level values are used.\n        \"\"\"\n        additions = {\n            c\n            for c in self._info_axis.unique(level=0)[:100]\n            if isinstance(c, str) and c.isidentifier()\n        }\n        return super()._dir_additions().union(additions)\n\n    # ----------------------------------------------------------------------\n    # Consolidation of internals\n\n    def _protect_consolidate(self, f):\n        \"\"\"\n        Consolidate _mgr -- if the blocks have changed, then clear the\n        cache\n        \"\"\"\n        blocks_before = len(self._mgr.blocks)\n        result = f()\n        if len(self._mgr.blocks) != blocks_before:\n            self._clear_item_cache()\n        return result\n\n    def _consolidate_inplace(self) -> None:\n        \"\"\"Consolidate data in place and return None\"\"\"\n\n        def f():\n            self._mgr = self._mgr.consolidate()\n\n        self._protect_consolidate(f)\n\n    def _consolidate(self):\n        \"\"\"\n        Compute NDFrame with \"consolidated\" internals (data of each dtype\n        grouped together in a single ndarray).\n\n        Returns\n        -------\n        consolidated : same type as caller\n        \"\"\"\n        f = lambda: self._mgr.consolidate()\n        cons_data = self._protect_consolidate(f)\n        return self._constructor(cons_data).__finalize__(self)\n\n    @property\n    def _is_mixed_type(self) -> bool_t:\n        if self._mgr.is_single_block:\n            return False\n\n        if self._mgr.any_extension_types:\n            # Even if they have the same dtype, we cant consolidate them,\n            #  so we pretend this is \"mixed'\"\n            return True\n\n        return self.dtypes.nunique() > 1\n\n    def _check_inplace_setting(self, value) -> bool_t:\n        \"\"\" check whether we allow in-place setting with this type of value \"\"\"\n        if self._is_mixed_type:\n            if not self._mgr.is_numeric_mixed_type:\n\n                # allow an actual np.nan thru\n                if is_float(value) and np.isnan(value):\n                    return True\n\n                raise TypeError(\n                    \"Cannot do inplace boolean setting on \"\n                    \"mixed-types with a non np.nan value\"\n                )\n\n        return True\n\n    def _get_numeric_data(self):\n        return self._constructor(self._mgr.get_numeric_data()).__finalize__(self)\n\n    def _get_bool_data(self):\n        return self._constructor(self._mgr.get_bool_data()).__finalize__(self)\n\n    # ----------------------------------------------------------------------\n    # Internal Interface Methods\n\n    @property\n    def values(self) -> np.ndarray:\n        \"\"\"\n        Return a Numpy representation of the DataFrame.\n\n        .. warning::\n\n           We recommend using :meth:`DataFrame.to_numpy` instead.\n\n        Only the values in the DataFrame will be returned, the axes labels\n        will be removed.\n\n        Returns\n        -------\n        numpy.ndarray\n            The values of the DataFrame.\n\n        See Also\n        --------\n        DataFrame.to_numpy : Recommended alternative to this method.\n        DataFrame.index : Retrieve the index labels.\n        DataFrame.columns : Retrieving the column names.\n\n        Notes\n        -----\n        The dtype will be a lower-common-denominator dtype (implicit\n        upcasting); that is to say if the dtypes (even of numeric types)\n        are mixed, the one that accommodates all will be chosen. Use this\n        with care if you are not dealing with the blocks.\n\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\n        float32.  If dtypes are int32 and uint8, dtype will be upcast to\n        int32. By :func:`numpy.find_common_type` convention, mixing int64\n        and uint64 will result in a float64 dtype.\n\n        Examples\n        --------\n        A DataFrame where all columns are the same type (e.g., int64) results\n        in an array of the same type.\n\n        >>> df = pd.DataFrame({'age':    [ 3,  29],\n        ...                    'height': [94, 170],\n        ...                    'weight': [31, 115]})\n        >>> df\n           age  height  weight\n        0    3      94      31\n        1   29     170     115\n        >>> df.dtypes\n        age       int64\n        height    int64\n        weight    int64\n        dtype: object\n        >>> df.values\n        array([[  3,  94,  31],\n               [ 29, 170, 115]])\n\n        A DataFrame with mixed type columns(e.g., str/object, int64, float32)\n        results in an ndarray of the broadest type that accommodates these\n        mixed types (e.g., object).\n\n        >>> df2 = pd.DataFrame([('parrot',   24.0, 'second'),\n        ...                     ('lion',     80.5, 1),\n        ...                     ('monkey', np.nan, None)],\n        ...                   columns=('name', 'max_speed', 'rank'))\n        >>> df2.dtypes\n        name          object\n        max_speed    float64\n        rank          object\n        dtype: object\n        >>> df2.values\n        array([['parrot', 24.0, 'second'],\n               ['lion', 80.5, 1],\n               ['monkey', nan, None]], dtype=object)\n        \"\"\"\n        self._consolidate_inplace()\n        return self._mgr.as_array(transpose=self._AXIS_REVERSED)\n\n    @property\n    def _values(self) -> np.ndarray:\n        \"\"\"internal implementation\"\"\"\n        return self.values\n\n    @property\n    def dtypes(self):\n        \"\"\"\n        Return the dtypes in the DataFrame.\n\n        This returns a Series with the data type of each column.\n        The result's index is the original DataFrame's columns. Columns\n        with mixed types are stored with the ``object`` dtype. See\n        :ref:`the User Guide <basics.dtypes>` for more.\n\n        Returns\n        -------\n        pandas.Series\n            The data type of each column.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'float': [1.0],\n        ...                    'int': [1],\n        ...                    'datetime': [pd.Timestamp('20180310')],\n        ...                    'string': ['foo']})\n        >>> df.dtypes\n        float              float64\n        int                  int64\n        datetime    datetime64[ns]\n        string              object\n        dtype: object\n        \"\"\"\n        data = self._mgr.get_dtypes()\n        return self._constructor_sliced(data, index=self._info_axis, dtype=np.object_)\n\n    def _to_dict_of_blocks(self, copy: bool_t = True):\n        \"\"\"\n        Return a dict of dtype -> Constructor Types that\n        each is a homogeneous dtype.\n\n        Internal ONLY\n        \"\"\"\n        return {\n            k: self._constructor(v).__finalize__(self)\n            for k, v, in self._mgr.to_dict(copy=copy).items()\n        }\n\n    def astype(\n        self: FrameOrSeries, dtype, copy: bool_t = True, errors: str = \"raise\"\n    ) -> FrameOrSeries:\n        \"\"\"\n        Cast a pandas object to a specified dtype ``dtype``.\n\n        Parameters\n        ----------\n        dtype : data type, or dict of column name -> data type\n            Use a numpy.dtype or Python type to cast entire pandas object to\n            the same type. Alternatively, use {col: dtype, ...}, where col is a\n            column label and dtype is a numpy.dtype or Python type to cast one\n            or more of the DataFrame's columns to column-specific types.\n        copy : bool, default True\n            Return a copy when ``copy=True`` (be very careful setting\n            ``copy=False`` as changes to values then may propagate to other\n            pandas objects).\n        errors : {'raise', 'ignore'}, default 'raise'\n            Control raising of exceptions on invalid data for provided dtype.\n\n            - ``raise`` : allow exceptions to be raised\n            - ``ignore`` : suppress exceptions. On error return original object.\n\n        Returns\n        -------\n        casted : same type as caller\n\n        See Also\n        --------\n        to_datetime : Convert argument to datetime.\n        to_timedelta : Convert argument to timedelta.\n        to_numeric : Convert argument to a numeric type.\n        numpy.ndarray.astype : Cast a numpy array to a specified type.\n\n        Examples\n        --------\n        Create a DataFrame:\n\n        >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n        >>> df = pd.DataFrame(data=d)\n        >>> df.dtypes\n        col1    int64\n        col2    int64\n        dtype: object\n\n        Cast all columns to int32:\n\n        >>> df.astype('int32').dtypes\n        col1    int32\n        col2    int32\n        dtype: object\n\n        Cast col1 to int32 using a dictionary:\n\n        >>> df.astype({'col1': 'int32'}).dtypes\n        col1    int32\n        col2    int64\n        dtype: object\n\n        Create a series:\n\n        >>> ser = pd.Series([1, 2], dtype='int32')\n        >>> ser\n        0    1\n        1    2\n        dtype: int32\n        >>> ser.astype('int64')\n        0    1\n        1    2\n        dtype: int64\n\n        Convert to categorical type:\n\n        >>> ser.astype('category')\n        0    1\n        1    2\n        dtype: category\n        Categories (2, int64): [1, 2]\n\n        Convert to ordered categorical type with custom ordering:\n\n        >>> cat_dtype = pd.api.types.CategoricalDtype(\n        ...     categories=[2, 1], ordered=True)\n        >>> ser.astype(cat_dtype)\n        0    1\n        1    2\n        dtype: category\n        Categories (2, int64): [2 < 1]\n\n        Note that using ``copy=False`` and changing data on a new\n        pandas object may propagate changes:\n\n        >>> s1 = pd.Series([1, 2])\n        >>> s2 = s1.astype('int64', copy=False)\n        >>> s2[0] = 10\n        >>> s1  # note that s1[0] has changed too\n        0    10\n        1     2\n        dtype: int64\n\n        Create a series of dates:\n\n        >>> ser_date = pd.Series(pd.date_range('20200101', periods=3))\n        >>> ser_date\n        0   2020-01-01\n        1   2020-01-02\n        2   2020-01-03\n        dtype: datetime64[ns]\n\n        Datetimes are localized to UTC first before\n        converting to the specified timezone:\n\n        >>> ser_date.astype('datetime64[ns, US/Eastern]')\n        0   2019-12-31 19:00:00-05:00\n        1   2020-01-01 19:00:00-05:00\n        2   2020-01-02 19:00:00-05:00\n        dtype: datetime64[ns, US/Eastern]\n        \"\"\"\n        if is_dict_like(dtype):\n            if self.ndim == 1:  # i.e. Series\n                if len(dtype) > 1 or self.name not in dtype:\n                    raise KeyError(\n                        \"Only the Series name can be used for \"\n                        \"the key in Series dtype mappings.\"\n                    )\n                new_type = dtype[self.name]\n                return self.astype(new_type, copy, errors)\n\n            for col_name in dtype.keys():\n                if col_name not in self:\n                    raise KeyError(\n                        \"Only a column name can be used for the \"\n                        \"key in a dtype mappings argument.\"\n                    )\n            results = []\n            for col_name, col in self.items():\n                if col_name in dtype:\n                    results.append(\n                        col.astype(dtype=dtype[col_name], copy=copy, errors=errors)\n                    )\n                else:\n                    results.append(col.copy() if copy else col)\n\n        elif is_extension_array_dtype(dtype) and self.ndim > 1:\n            # GH 18099/22869: columnwise conversion to extension dtype\n            # GH 24704: use iloc to handle duplicate column names\n            results = [\n                self.iloc[:, i].astype(dtype, copy=copy)\n                for i in range(len(self.columns))\n            ]\n\n        else:\n            # else, only a single dtype is given\n            new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n            return self._constructor(new_data).__finalize__(self, method=\"astype\")\n\n        # GH 33113: handle empty frame or series\n        if not results:\n            return self.copy()\n\n        # GH 19920: retain column metadata after concat\n        result = pd.concat(results, axis=1, copy=False)\n        result.columns = self.columns\n        return result\n\n    def copy(self: FrameOrSeries, deep: bool_t = True) -> FrameOrSeries:\n        \"\"\"\n        Make a copy of this object's indices and data.\n\n        When ``deep=True`` (default), a new object will be created with a\n        copy of the calling object's data and indices. Modifications to\n        the data or indices of the copy will not be reflected in the\n        original object (see notes below).\n\n        When ``deep=False``, a new object will be created without copying\n        the calling object's data or index (only references to the data\n        and index are copied). Any changes to the data of the original\n        will be reflected in the shallow copy (and vice versa).\n\n        Parameters\n        ----------\n        deep : bool, default True\n            Make a deep copy, including a copy of the data and the indices.\n            With ``deep=False`` neither the indices nor the data are copied.\n\n        Returns\n        -------\n        copy : Series or DataFrame\n            Object type matches caller.\n\n        Notes\n        -----\n        When ``deep=True``, data is copied but actual Python objects\n        will not be copied recursively, only the reference to the object.\n        This is in contrast to `copy.deepcopy` in the Standard Library,\n        which recursively copies object data (see examples below).\n\n        While ``Index`` objects are copied when ``deep=True``, the underlying\n        numpy array is not copied for performance reasons. Since ``Index`` is\n        immutable, the underlying data can be safely shared and a copy\n        is not needed.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\n        >>> s\n        a    1\n        b    2\n        dtype: int64\n\n        >>> s_copy = s.copy()\n        >>> s_copy\n        a    1\n        b    2\n        dtype: int64\n\n        **Shallow copy versus default (deep) copy:**\n\n        >>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\n        >>> deep = s.copy()\n        >>> shallow = s.copy(deep=False)\n\n        Shallow copy shares data and index with original.\n\n        >>> s is shallow\n        False\n        >>> s.values is shallow.values and s.index is shallow.index\n        True\n\n        Deep copy has own copy of data and index.\n\n        >>> s is deep\n        False\n        >>> s.values is deep.values or s.index is deep.index\n        False\n\n        Updates to the data shared by shallow copy and original is reflected\n        in both; deep copy remains unchanged.\n\n        >>> s[0] = 3\n        >>> shallow[1] = 4\n        >>> s\n        a    3\n        b    4\n        dtype: int64\n        >>> shallow\n        a    3\n        b    4\n        dtype: int64\n        >>> deep\n        a    1\n        b    2\n        dtype: int64\n\n        Note that when copying an object containing Python objects, a deep copy\n        will copy the data, but will not do so recursively. Updating a nested\n        data object will be reflected in the deep copy.\n\n        >>> s = pd.Series([[1, 2], [3, 4]])\n        >>> deep = s.copy()\n        >>> s[0][0] = 10\n        >>> s\n        0    [10, 2]\n        1     [3, 4]\n        dtype: object\n        >>> deep\n        0    [10, 2]\n        1     [3, 4]\n        dtype: object\n        \"\"\"\n        data = self._mgr.copy(deep=deep)\n        self._clear_item_cache()\n        return self._constructor(data).__finalize__(self, method=\"copy\")\n\n    def __copy__(self: FrameOrSeries, deep: bool_t = True) -> FrameOrSeries:\n        return self.copy(deep=deep)\n\n    def __deepcopy__(self: FrameOrSeries, memo=None) -> FrameOrSeries:\n        \"\"\"\n        Parameters\n        ----------\n        memo, default None\n            Standard signature. Unused\n        \"\"\"\n        return self.copy(deep=True)\n\n    def _convert(\n        self: FrameOrSeries,\n        datetime: bool_t = False,\n        numeric: bool_t = False,\n        timedelta: bool_t = False,\n        coerce: bool_t = False,\n    ) -> FrameOrSeries:\n        \"\"\"\n        Attempt to infer better dtype for object columns\n\n        Parameters\n        ----------\n        datetime : bool, default False\n            If True, convert to date where possible.\n        numeric : bool, default False\n            If True, attempt to convert to numbers (including strings), with\n            unconvertible values becoming NaN.\n        timedelta : bool, default False\n            If True, convert to timedelta where possible.\n        coerce : bool, default False\n            If True, force conversion with unconvertible values converted to\n            nulls (NaN or NaT).\n\n        Returns\n        -------\n        converted : same as input object\n        \"\"\"\n        validate_bool_kwarg(datetime, \"datetime\")\n        validate_bool_kwarg(numeric, \"numeric\")\n        validate_bool_kwarg(timedelta, \"timedelta\")\n        validate_bool_kwarg(coerce, \"coerce\")\n        return self._constructor(\n            self._mgr.convert(\n                datetime=datetime,\n                numeric=numeric,\n                timedelta=timedelta,\n                coerce=coerce,\n                copy=True,\n            )\n        ).__finalize__(self)\n\n    def infer_objects(self: FrameOrSeries) -> FrameOrSeries:\n        \"\"\"\n        Attempt to infer better dtypes for object columns.\n\n        Attempts soft conversion of object-dtyped\n        columns, leaving non-object and unconvertible\n        columns unchanged. The inference rules are the\n        same as during normal Series/DataFrame construction.\n\n        Returns\n        -------\n        converted : same type as input object\n\n        See Also\n        --------\n        to_datetime : Convert argument to datetime.\n        to_timedelta : Convert argument to timedelta.\n        to_numeric : Convert argument to numeric type.\n        convert_dtypes : Convert argument to best possible dtype.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [\"a\", 1, 2, 3]})\n        >>> df = df.iloc[1:]\n        >>> df\n           A\n        1  1\n        2  2\n        3  3\n\n        >>> df.dtypes\n        A    object\n        dtype: object\n\n        >>> df.infer_objects().dtypes\n        A    int64\n        dtype: object\n        \"\"\"\n        # numeric=False necessary to only soft convert;\n        # python objects will still be converted to\n        # native numpy numeric types\n        return self._constructor(\n            self._mgr.convert(\n                datetime=True, numeric=False, timedelta=True, coerce=False, copy=True\n            )\n        ).__finalize__(self, method=\"infer_objects\")\n\n    def convert_dtypes(\n        self: FrameOrSeries,\n        infer_objects: bool_t = True,\n        convert_string: bool_t = True,\n        convert_integer: bool_t = True,\n        convert_boolean: bool_t = True,\n    ) -> FrameOrSeries:\n        \"\"\"\n        Convert columns to best possible dtypes using dtypes supporting ``pd.NA``.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        infer_objects : bool, default True\n            Whether object dtypes should be converted to the best possible types.\n        convert_string : bool, default True\n            Whether object dtypes should be converted to ``StringDtype()``.\n        convert_integer : bool, default True\n            Whether, if possible, conversion can be done to integer extension types.\n        convert_boolean : bool, defaults True\n            Whether object dtypes should be converted to ``BooleanDtypes()``.\n\n        Returns\n        -------\n        Series or DataFrame\n            Copy of input object with new dtype.\n\n        See Also\n        --------\n        infer_objects : Infer dtypes of objects.\n        to_datetime : Convert argument to datetime.\n        to_timedelta : Convert argument to timedelta.\n        to_numeric : Convert argument to a numeric type.\n\n        Notes\n        -----\n        By default, ``convert_dtypes`` will attempt to convert a Series (or each\n        Series in a DataFrame) to dtypes that support ``pd.NA``. By using the options\n        ``convert_string``, ``convert_integer``, and ``convert_boolean``, it is\n        possible to turn off individual conversions to ``StringDtype``, the integer\n        extension types or ``BooleanDtype``, respectively.\n\n        For object-dtyped columns, if ``infer_objects`` is ``True``, use the inference\n        rules as during normal Series/DataFrame construction.  Then, if possible,\n        convert to ``StringDtype``, ``BooleanDtype`` or an appropriate integer extension\n        type, otherwise leave as ``object``.\n\n        If the dtype is integer, convert to an appropriate integer extension type.\n\n        If the dtype is numeric, and consists of all integers, convert to an\n        appropriate integer extension type.\n\n        In the future, as new dtypes are added that support ``pd.NA``, the results\n        of this method will change to support those new dtypes.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\n        ...         \"a\": pd.Series([1, 2, 3], dtype=np.dtype(\"int32\")),\n        ...         \"b\": pd.Series([\"x\", \"y\", \"z\"], dtype=np.dtype(\"O\")),\n        ...         \"c\": pd.Series([True, False, np.nan], dtype=np.dtype(\"O\")),\n        ...         \"d\": pd.Series([\"h\", \"i\", np.nan], dtype=np.dtype(\"O\")),\n        ...         \"e\": pd.Series([10, np.nan, 20], dtype=np.dtype(\"float\")),\n        ...         \"f\": pd.Series([np.nan, 100.5, 200], dtype=np.dtype(\"float\")),\n        ...     }\n        ... )\n\n        Start with a DataFrame with default dtypes.\n\n        >>> df\n           a  b      c    d     e      f\n        0  1  x   True    h  10.0    NaN\n        1  2  y  False    i   NaN  100.5\n        2  3  z    NaN  NaN  20.0  200.0\n\n        >>> df.dtypes\n        a      int32\n        b     object\n        c     object\n        d     object\n        e    float64\n        f    float64\n        dtype: object\n\n        Convert the DataFrame to use best possible dtypes.\n\n        >>> dfn = df.convert_dtypes()\n        >>> dfn\n           a  b      c     d     e      f\n        0  1  x   True     h    10    NaN\n        1  2  y  False     i  <NA>  100.5\n        2  3  z   <NA>  <NA>    20  200.0\n\n        >>> dfn.dtypes\n        a      Int32\n        b     string\n        c    boolean\n        d     string\n        e      Int64\n        f    float64\n        dtype: object\n\n        Start with a Series of strings and missing data represented by ``np.nan``.\n\n        >>> s = pd.Series([\"a\", \"b\", np.nan])\n        >>> s\n        0      a\n        1      b\n        2    NaN\n        dtype: object\n\n        Obtain a Series with dtype ``StringDtype``.\n\n        >>> s.convert_dtypes()\n        0       a\n        1       b\n        2    <NA>\n        dtype: string\n        \"\"\"\n        if self.ndim == 1:\n            return self._convert_dtypes(\n                infer_objects, convert_string, convert_integer, convert_boolean\n            )\n        else:\n            results = [\n                col._convert_dtypes(\n                    infer_objects, convert_string, convert_integer, convert_boolean\n                )\n                for col_name, col in self.items()\n            ]\n            result = pd.concat(results, axis=1, copy=False)\n            return result\n\n    # ----------------------------------------------------------------------\n    # Filling NA's\n\n    @doc(**_shared_doc_kwargs)\n    def fillna(\n        self: FrameOrSeries,\n        value=None,\n        method=None,\n        axis=None,\n        inplace: bool_t = False,\n        limit=None,\n        downcast=None,\n    ) -> Optional[FrameOrSeries]:\n        \"\"\"\n        Fill NA/NaN values using the specified method.\n\n        Parameters\n        ----------\n        value : scalar, dict, Series, or DataFrame\n            Value to use to fill holes (e.g. 0), alternately a\n            dict/Series/DataFrame of values specifying which value to use for\n            each index (for a Series) or column (for a DataFrame).  Values not\n            in the dict/Series/DataFrame will not be filled. This value cannot\n            be a list.\n        method : {{'backfill', 'bfill', 'pad', 'ffill', None}}, default None\n            Method to use for filling holes in reindexed Series\n            pad / ffill: propagate last valid observation forward to next valid\n            backfill / bfill: use next valid observation to fill gap.\n        axis : {axes_single_arg}\n            Axis along which to fill missing values.\n        inplace : bool, default False\n            If True, fill in-place. Note: this will modify any\n            other views on this object (e.g., a no-copy slice for a column in a\n            DataFrame).\n        limit : int, default None\n            If method is specified, this is the maximum number of consecutive\n            NaN values to forward/backward fill. In other words, if there is\n            a gap with more than this number of consecutive NaNs, it will only\n            be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled. Must be greater than 0 if not None.\n        downcast : dict, default is None\n            A dict of item->dtype of what to downcast if possible,\n            or the string 'infer' which will try to downcast to an appropriate\n            equal type (e.g. float64 to int64 if possible).\n\n        Returns\n        -------\n        {klass} or None\n            Object with missing values filled or None if ``inplace=True``.\n\n        See Also\n        --------\n        interpolate : Fill NaN values using interpolation.\n        reindex : Conform object to new index.\n        asfreq : Convert TimeSeries to specified frequency.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n        ...                    [3, 4, np.nan, 1],\n        ...                    [np.nan, np.nan, np.nan, 5],\n        ...                    [np.nan, 3, np.nan, 4]],\n        ...                   columns=list('ABCD'))\n        >>> df\n             A    B   C  D\n        0  NaN  2.0 NaN  0\n        1  3.0  4.0 NaN  1\n        2  NaN  NaN NaN  5\n        3  NaN  3.0 NaN  4\n\n        Replace all NaN elements with 0s.\n\n        >>> df.fillna(0)\n            A   B   C   D\n        0   0.0 2.0 0.0 0\n        1   3.0 4.0 0.0 1\n        2   0.0 0.0 0.0 5\n        3   0.0 3.0 0.0 4\n\n        We can also propagate non-null values forward or backward.\n\n        >>> df.fillna(method='ffill')\n            A   B   C   D\n        0   NaN 2.0 NaN 0\n        1   3.0 4.0 NaN 1\n        2   3.0 4.0 NaN 5\n        3   3.0 3.0 NaN 4\n\n        Replace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1,\n        2, and 3 respectively.\n\n        >>> values = {{'A': 0, 'B': 1, 'C': 2, 'D': 3}}\n        >>> df.fillna(value=values)\n            A   B   C   D\n        0   0.0 2.0 2.0 0\n        1   3.0 4.0 2.0 1\n        2   0.0 1.0 2.0 5\n        3   0.0 3.0 2.0 4\n\n        Only replace the first NaN element.\n\n        >>> df.fillna(value=values, limit=1)\n            A   B   C   D\n        0   0.0 2.0 2.0 0\n        1   3.0 4.0 NaN 1\n        2   NaN 1.0 NaN 5\n        3   NaN 3.0 NaN 4\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        value, method = validate_fillna_kwargs(value, method)\n\n        # set the default here, so functions examining the signaure\n        # can detect if something was set (e.g. in groupby) (GH9221)\n        if axis is None:\n            axis = 0\n        axis = self._get_axis_number(axis)\n\n        if value is None:\n            if not self._mgr.is_single_block and axis == 1:\n                if inplace:\n                    raise NotImplementedError()\n                result = self.T.fillna(method=method, limit=limit).T\n\n                # need to downcast here because of all of the transposes\n                result._mgr = result._mgr.downcast()\n\n                return result\n\n            new_data = self._mgr.interpolate(\n                method=method,\n                axis=axis,\n                limit=limit,\n                inplace=inplace,\n                coerce=True,\n                downcast=downcast,\n            )\n        else:\n            if self.ndim == 1:\n                if isinstance(value, (dict, ABCSeries)):\n                    value = create_series_with_explicit_dtype(\n                        value, dtype_if_empty=object\n                    )\n                    value = value.reindex(self.index, copy=False)\n                    value = value._values\n                elif not is_list_like(value):\n                    pass\n                else:\n                    raise TypeError(\n                        '\"value\" parameter must be a scalar, dict '\n                        \"or Series, but you passed a \"\n                        f'\"{type(value).__name__}\"'\n                    )\n\n                new_data = self._mgr.fillna(\n                    value=value, limit=limit, inplace=inplace, downcast=downcast\n                )\n\n            elif isinstance(value, (dict, ABCSeries)):\n                if axis == 1:\n                    raise NotImplementedError(\n                        \"Currently only can fill \"\n                        \"with dict/Series column \"\n                        \"by column\"\n                    )\n\n                result = self if inplace else self.copy()\n                for k, v in value.items():\n                    if k not in result:\n                        continue\n                    obj = result[k]\n                    obj.fillna(v, limit=limit, inplace=True, downcast=downcast)\n                return result if not inplace else None\n\n            elif not is_list_like(value):\n                new_data = self._mgr.fillna(\n                    value=value, limit=limit, inplace=inplace, downcast=downcast\n                )\n            elif isinstance(value, ABCDataFrame) and self.ndim == 2:\n                new_data = self.where(self.notna(), value)._data\n            else:\n                raise ValueError(f\"invalid fill value with a {type(value)}\")\n\n        result = self._constructor(new_data)\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"fillna\")\n\n    def ffill(\n        self: FrameOrSeries,\n        axis=None,\n        inplace: bool_t = False,\n        limit=None,\n        downcast=None,\n    ) -> Optional[FrameOrSeries]:\n        \"\"\"\n        Synonym for :meth:`DataFrame.fillna` with ``method='ffill'``.\n\n        Returns\n        -------\n        {klass} or None\n            Object with missing values filled or None if ``inplace=True``.\n        \"\"\"\n        return self.fillna(\n            method=\"ffill\", axis=axis, inplace=inplace, limit=limit, downcast=downcast\n        )\n\n    pad = ffill\n\n    def bfill(\n        self: FrameOrSeries,\n        axis=None,\n        inplace: bool_t = False,\n        limit=None,\n        downcast=None,\n    ) -> Optional[FrameOrSeries]:\n        \"\"\"\n        Synonym for :meth:`DataFrame.fillna` with ``method='bfill'``.\n\n        Returns\n        -------\n        {klass} or None\n            Object with missing values filled or None if ``inplace=True``.\n        \"\"\"\n        return self.fillna(\n            method=\"bfill\", axis=axis, inplace=inplace, limit=limit, downcast=downcast\n        )\n\n    backfill = bfill\n\n    @doc(klass=_shared_doc_kwargs[\"klass\"])\n    def replace(\n        self,\n        to_replace=None,\n        value=None,\n        inplace: bool_t = False,\n        limit: Optional[int] = None,\n        regex=False,\n        method=\"pad\",\n    ):\n        \"\"\"\n        Replace values given in `to_replace` with `value`.\n\n        Values of the {klass} are replaced with other values dynamically.\n        This differs from updating with ``.loc`` or ``.iloc``, which require\n        you to specify a location to update with some value.\n\n        Parameters\n        ----------\n        to_replace : str, regex, list, dict, Series, int, float, or None\n            How to find the values that will be replaced.\n\n            * numeric, str or regex:\n\n                - numeric: numeric values equal to `to_replace` will be\n                  replaced with `value`\n                - str: string exactly matching `to_replace` will be replaced\n                  with `value`\n                - regex: regexs matching `to_replace` will be replaced with\n                  `value`\n\n            * list of str, regex, or numeric:\n\n                - First, if `to_replace` and `value` are both lists, they\n                  **must** be the same length.\n                - Second, if ``regex=True`` then all of the strings in **both**\n                  lists will be interpreted as regexs otherwise they will match\n                  directly. This doesn't matter much for `value` since there\n                  are only a few possible substitution regexes you can use.\n                - str, regex and numeric rules apply as above.\n\n            * dict:\n\n                - Dicts can be used to specify different replacement values\n                  for different existing values. For example,\n                  ``{{'a': 'b', 'y': 'z'}}`` replaces the value 'a' with 'b' and\n                  'y' with 'z'. To use a dict in this way the `value`\n                  parameter should be `None`.\n                - For a DataFrame a dict can specify that different values\n                  should be replaced in different columns. For example,\n                  ``{{'a': 1, 'b': 'z'}}`` looks for the value 1 in column 'a'\n                  and the value 'z' in column 'b' and replaces these values\n                  with whatever is specified in `value`. The `value` parameter\n                  should not be ``None`` in this case. You can treat this as a\n                  special case of passing two lists except that you are\n                  specifying the column to search in.\n                - For a DataFrame nested dictionaries, e.g.,\n                  ``{{'a': {{'b': np.nan}}}}``, are read as follows: look in column\n                  'a' for the value 'b' and replace it with NaN. The `value`\n                  parameter should be ``None`` to use a nested dict in this\n                  way. You can nest regular expressions as well. Note that\n                  column names (the top-level dictionary keys in a nested\n                  dictionary) **cannot** be regular expressions.\n\n            * None:\n\n                - This means that the `regex` argument must be a string,\n                  compiled regular expression, or list, dict, ndarray or\n                  Series of such elements. If `value` is also ``None`` then\n                  this **must** be a nested dictionary or Series.\n\n            See the examples section for examples of each of these.\n        value : scalar, dict, list, str, regex, default None\n            Value to replace any values matching `to_replace` with.\n            For a DataFrame a dict of values can be used to specify which\n            value to use for each column (columns not in the dict will not be\n            filled). Regular expressions, strings and lists or dicts of such\n            objects are also allowed.\n        inplace : bool, default False\n            If True, in place. Note: this will modify any\n            other views on this object (e.g. a column from a DataFrame).\n            Returns the caller if this is True.\n        limit : int or None, default None\n            Maximum size gap to forward or backward fill.\n        regex : bool or same types as `to_replace`, default False\n            Whether to interpret `to_replace` and/or `value` as regular\n            expressions. If this is ``True`` then `to_replace` *must* be a\n            string. Alternatively, this could be a regular expression or a\n            list, dict, or array of regular expressions in which case\n            `to_replace` must be ``None``.\n        method : {{'pad', 'ffill', 'bfill', `None`}}\n            The method to use when for replacement, when `to_replace` is a\n            scalar, list or tuple and `value` is ``None``.\n\n        Returns\n        -------\n        {klass} or None\n            Object after replacement or None if ``inplace=True``.\n\n        Raises\n        ------\n        AssertionError\n            * If `regex` is not a ``bool`` and `to_replace` is not\n              ``None``.\n\n        TypeError\n            * If `to_replace` is not a scalar, array-like, ``dict``, or ``None``\n            * If `to_replace` is a ``dict`` and `value` is not a ``list``,\n              ``dict``, ``ndarray``, or ``Series``\n            * If `to_replace` is ``None`` and `regex` is not compilable\n              into a regular expression or is a list, dict, ndarray, or\n              Series.\n            * When replacing multiple ``bool`` or ``datetime64`` objects and\n              the arguments to `to_replace` does not match the type of the\n              value being replaced\n\n        ValueError\n            * If a ``list`` or an ``ndarray`` is passed to `to_replace` and\n              `value` but they are not the same length.\n\n        See Also\n        --------\n        {klass}.fillna : Fill NA values.\n        {klass}.where : Replace values based on boolean condition.\n        Series.str.replace : Simple string replacement.\n\n        Notes\n        -----\n        * Regex substitution is performed under the hood with ``re.sub``. The\n          rules for substitution for ``re.sub`` are the same.\n        * Regular expressions will only substitute on strings, meaning you\n          cannot provide, for example, a regular expression matching floating\n          point numbers and expect the columns in your frame that have a\n          numeric dtype to be matched. However, if those floating point\n          numbers *are* strings, then you can do this.\n        * This method has *a lot* of options. You are encouraged to experiment\n          and play with this method to gain intuition about how it works.\n        * When dict is used as the `to_replace` value, it is like\n          key(s) in the dict are the to_replace part and\n          value(s) in the dict are the value parameter.\n\n        Examples\n        --------\n\n        **Scalar `to_replace` and `value`**\n\n        >>> s = pd.Series([0, 1, 2, 3, 4])\n        >>> s.replace(0, 5)\n        0    5\n        1    1\n        2    2\n        3    3\n        4    4\n        dtype: int64\n\n        >>> df = pd.DataFrame({{'A': [0, 1, 2, 3, 4],\n        ...                    'B': [5, 6, 7, 8, 9],\n        ...                    'C': ['a', 'b', 'c', 'd', 'e']}})\n        >>> df.replace(0, 5)\n           A  B  C\n        0  5  5  a\n        1  1  6  b\n        2  2  7  c\n        3  3  8  d\n        4  4  9  e\n\n        **List-like `to_replace`**\n\n        >>> df.replace([0, 1, 2, 3], 4)\n           A  B  C\n        0  4  5  a\n        1  4  6  b\n        2  4  7  c\n        3  4  8  d\n        4  4  9  e\n\n        >>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n           A  B  C\n        0  4  5  a\n        1  3  6  b\n        2  2  7  c\n        3  1  8  d\n        4  4  9  e\n\n        >>> s.replace([1, 2], method='bfill')\n        0    0\n        1    3\n        2    3\n        3    3\n        4    4\n        dtype: int64\n\n        **dict-like `to_replace`**\n\n        >>> df.replace({{0: 10, 1: 100}})\n             A  B  C\n        0   10  5  a\n        1  100  6  b\n        2    2  7  c\n        3    3  8  d\n        4    4  9  e\n\n        >>> df.replace({{'A': 0, 'B': 5}}, 100)\n             A    B  C\n        0  100  100  a\n        1    1    6  b\n        2    2    7  c\n        3    3    8  d\n        4    4    9  e\n\n        >>> df.replace({{'A': {{0: 100, 4: 400}}}})\n             A  B  C\n        0  100  5  a\n        1    1  6  b\n        2    2  7  c\n        3    3  8  d\n        4  400  9  e\n\n        **Regular expression `to_replace`**\n\n        >>> df = pd.DataFrame({{'A': ['bat', 'foo', 'bait'],\n        ...                    'B': ['abc', 'bar', 'xyz']}})\n        >>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n              A    B\n        0   new  abc\n        1   foo  new\n        2  bait  xyz\n\n        >>> df.replace({{'A': r'^ba.$'}}, {{'A': 'new'}}, regex=True)\n              A    B\n        0   new  abc\n        1   foo  bar\n        2  bait  xyz\n\n        >>> df.replace(regex=r'^ba.$', value='new')\n              A    B\n        0   new  abc\n        1   foo  new\n        2  bait  xyz\n\n        >>> df.replace(regex={{r'^ba.$': 'new', 'foo': 'xyz'}})\n              A    B\n        0   new  abc\n        1   xyz  new\n        2  bait  xyz\n\n        >>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n              A    B\n        0   new  abc\n        1   new  new\n        2  bait  xyz\n\n        Compare the behavior of ``s.replace({{'a': None}})`` and\n        ``s.replace('a', None)`` to understand the peculiarities\n        of the `to_replace` parameter:\n\n        >>> s = pd.Series([10, 'a', 'a', 'b', 'a'])\n\n        When one uses a dict as the `to_replace` value, it is like the\n        value(s) in the dict are equal to the `value` parameter.\n        ``s.replace({{'a': None}})`` is equivalent to\n        ``s.replace(to_replace={{'a': None}}, value=None, method=None)``:\n\n        >>> s.replace({{'a': None}})\n        0      10\n        1    None\n        2    None\n        3       b\n        4    None\n        dtype: object\n\n        When ``value=None`` and `to_replace` is a scalar, list or\n        tuple, `replace` uses the method parameter (default 'pad') to do the\n        replacement. So this is why the 'a' values are being replaced by 10\n        in rows 1 and 2 and 'b' in row 4 in this case.\n        The command ``s.replace('a', None)`` is actually equivalent to\n        ``s.replace(to_replace='a', value=None, method='pad')``:\n\n        >>> s.replace('a', None)\n        0    10\n        1    10\n        2    10\n        3     b\n        4     b\n        dtype: object\n        \"\"\"\n        if not (\n            is_scalar(to_replace)\n            or is_re_compilable(to_replace)\n            or is_list_like(to_replace)\n        ):\n            raise TypeError(\n                \"Expecting 'to_replace' to be either a scalar, array-like, \"\n                \"dict or None, got invalid type \"\n                f\"{repr(type(to_replace).__name__)}\"\n            )\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if not is_bool(regex) and to_replace is not None:\n            raise ValueError(\"'to_replace' must be 'None' if 'regex' is not a bool\")\n\n        if value is None:\n            # passing a single value that is scalar like\n            # when value is None (GH5319), for compat\n            if not is_dict_like(to_replace) and not is_dict_like(regex):\n                to_replace = [to_replace]\n\n            if isinstance(to_replace, (tuple, list)):\n                if isinstance(self, ABCDataFrame):\n                    return self.apply(\n                        _single_replace, args=(to_replace, method, inplace, limit)\n                    )\n                self = cast(\"Series\", self)\n                return _single_replace(self, to_replace, method, inplace, limit)\n\n            if not is_dict_like(to_replace):\n                if not is_dict_like(regex):\n                    raise TypeError(\n                        'If \"to_replace\" and \"value\" are both None '\n                        'and \"to_replace\" is not a list, then '\n                        \"regex must be a mapping\"\n                    )\n                to_replace = regex\n                regex = True\n\n            items = list(to_replace.items())\n            if items:\n                keys, values = zip(*items)\n            else:\n                keys, values = ([], [])\n\n            are_mappings = [is_dict_like(v) for v in values]\n\n            if any(are_mappings):\n                if not all(are_mappings):\n                    raise TypeError(\n                        \"If a nested mapping is passed, all values \"\n                        \"of the top level mapping must be mappings\"\n                    )\n                # passed a nested dict/Series\n                to_rep_dict = {}\n                value_dict = {}\n\n                for k, v in items:\n                    keys, values = list(zip(*v.items())) or ([], [])\n\n                    to_rep_dict[k] = list(keys)\n                    value_dict[k] = list(values)\n\n                to_replace, value = to_rep_dict, value_dict\n            else:\n                to_replace, value = keys, values\n\n            return self.replace(\n                to_replace, value, inplace=inplace, limit=limit, regex=regex\n            )\n        else:\n\n            # need a non-zero len on all axes\n            if not self.size:\n                if inplace:\n                    return\n                return self.copy()\n\n            if is_dict_like(to_replace):\n                if is_dict_like(value):  # {'A' : NA} -> {'A' : 0}\n                    # Note: Checking below for `in foo.keys()` instead of\n                    #  `in foo` is needed for when we have a Series and not dict\n                    mapping = {\n                        col: (to_replace[col], value[col])\n                        for col in to_replace.keys()\n                        if col in value.keys() and col in self\n                    }\n                    return self._replace_columnwise(mapping, inplace, regex)\n\n                # {'A': NA} -> 0\n                elif not is_list_like(value):\n                    # Operate column-wise\n                    if self.ndim == 1:\n                        raise ValueError(\n                            \"Series.replace cannot use dict-like to_replace \"\n                            \"and non-None value\"\n                        )\n                    mapping = {\n                        col: (to_rep, value) for col, to_rep in to_replace.items()\n                    }\n                    return self._replace_columnwise(mapping, inplace, regex)\n                else:\n                    raise TypeError(\"value argument must be scalar, dict, or Series\")\n\n            elif is_list_like(to_replace):  # [NA, ''] -> [0, 'missing']\n                if is_list_like(value):\n                    if len(to_replace) != len(value):\n                        raise ValueError(\n                            f\"Replacement lists must match in length. \"\n                            f\"Expecting {len(to_replace)} got {len(value)} \"\n                        )\n                    self._consolidate_inplace()\n                    new_data = self._mgr.replace_list(\n                        src_list=to_replace,\n                        dest_list=value,\n                        inplace=inplace,\n                        regex=regex,\n                    )\n\n                else:  # [NA, ''] -> 0\n                    new_data = self._mgr.replace(\n                        to_replace=to_replace, value=value, inplace=inplace, regex=regex\n                    )\n            elif to_replace is None:\n                if not (\n                    is_re_compilable(regex)\n                    or is_list_like(regex)\n                    or is_dict_like(regex)\n                ):\n                    raise TypeError(\n                        f\"'regex' must be a string or a compiled regular expression \"\n                        f\"or a list or dict of strings or regular expressions, \"\n                        f\"you passed a {repr(type(regex).__name__)}\"\n                    )\n                return self.replace(\n                    regex, value, inplace=inplace, limit=limit, regex=True\n                )\n            else:\n\n                # dest iterable dict-like\n                if is_dict_like(value):  # NA -> {'A' : 0, 'B' : -1}\n                    # Operate column-wise\n                    if self.ndim == 1:\n                        raise ValueError(\n                            \"Series.replace cannot use dict-value and \"\n                            \"non-None to_replace\"\n                        )\n                    mapping = {col: (to_replace, val) for col, val in value.items()}\n                    return self._replace_columnwise(mapping, inplace, regex)\n\n                elif not is_list_like(value):  # NA -> 0\n                    new_data = self._mgr.replace(\n                        to_replace=to_replace, value=value, inplace=inplace, regex=regex\n                    )\n                else:\n                    raise TypeError(\n                        f'Invalid \"to_replace\" type: {repr(type(to_replace).__name__)}'\n                    )\n\n        result = self._constructor(new_data)\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"replace\")\n\n    def interpolate(\n        self: FrameOrSeries,\n        method: str = \"linear\",\n        axis: Axis = 0,\n        limit: Optional[int] = None,\n        inplace: bool_t = False,\n        limit_direction: Optional[str] = None,\n        limit_area: Optional[str] = None,\n        downcast: Optional[str] = None,\n        **kwargs,\n    ) -> Optional[FrameOrSeries]:\n        \"\"\"\n        Fill NaN values using an interpolation method.\n\n        Please note that only ``method='linear'`` is supported for\n        DataFrame/Series with a MultiIndex.\n\n        Parameters\n        ----------\n        method : str, default 'linear'\n            Interpolation technique to use. One of:\n\n            * 'linear': Ignore the index and treat the values as equally\n              spaced. This is the only method supported on MultiIndexes.\n            * 'time': Works on daily and higher resolution data to interpolate\n              given length of interval.\n            * 'index', 'values': use the actual numerical values of the index.\n            * 'pad': Fill in NaNs using existing values.\n            * 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'spline',\n              'barycentric', 'polynomial': Passed to\n              `scipy.interpolate.interp1d`. These methods use the numerical\n              values of the index.  Both 'polynomial' and 'spline' require that\n              you also specify an `order` (int), e.g.\n              ``df.interpolate(method='polynomial', order=5)``.\n            * 'krogh', 'piecewise_polynomial', 'spline', 'pchip', 'akima',\n              'cubicspline': Wrappers around the SciPy interpolation methods of\n              similar names. See `Notes`.\n            * 'from_derivatives': Refers to\n              `scipy.interpolate.BPoly.from_derivatives` which\n              replaces 'piecewise_polynomial' interpolation method in\n              scipy 0.18.\n\n        axis : {{0 or 'index', 1 or 'columns', None}}, default None\n            Axis to interpolate along.\n        limit : int, optional\n            Maximum number of consecutive NaNs to fill. Must be greater than\n            0.\n        inplace : bool, default False\n            Update the data in place if possible.\n        limit_direction : {{'forward', 'backward', 'both'}}, Optional\n            Consecutive NaNs will be filled in this direction.\n\n            If limit is specified:\n                * If 'method' is 'pad' or 'ffill', 'limit_direction' must be 'forward'.\n                * If 'method' is 'backfill' or 'bfill', 'limit_direction' must be\n                  'backwards'.\n\n            If 'limit' is not specified:\n                * If 'method' is 'backfill' or 'bfill', the default is 'backward'\n                * else the default is 'forward'\n\n            .. versionchanged:: 1.1.0\n                raises ValueError if `limit_direction` is 'forward' or 'both' and\n                    method is 'backfill' or 'bfill'.\n                raises ValueError if `limit_direction` is 'backward' or 'both' and\n                    method is 'pad' or 'ffill'.\n\n        limit_area : {{`None`, 'inside', 'outside'}}, default None\n            If limit is specified, consecutive NaNs will be filled with this\n            restriction.\n\n            * ``None``: No fill restriction.\n            * 'inside': Only fill NaNs surrounded by valid values\n              (interpolate).\n            * 'outside': Only fill NaNs outside valid values (extrapolate).\n\n        downcast : optional, 'infer' or None, defaults to None\n            Downcast dtypes if possible.\n        ``**kwargs`` : optional\n            Keyword arguments to pass on to the interpolating function.\n\n        Returns\n        -------\n        Series or DataFrame or None\n            Returns the same object type as the caller, interpolated at\n            some or all ``NaN`` values or None if ``inplace=True``.\n\n        See Also\n        --------\n        fillna : Fill missing values using different methods.\n        scipy.interpolate.Akima1DInterpolator : Piecewise cubic polynomials\n            (Akima interpolator).\n        scipy.interpolate.BPoly.from_derivatives : Piecewise polynomial in the\n            Bernstein basis.\n        scipy.interpolate.interp1d : Interpolate a 1-D function.\n        scipy.interpolate.KroghInterpolator : Interpolate polynomial (Krogh\n            interpolator).\n        scipy.interpolate.PchipInterpolator : PCHIP 1-d monotonic cubic\n            interpolation.\n        scipy.interpolate.CubicSpline : Cubic spline data interpolator.\n\n        Notes\n        -----\n        The 'krogh', 'piecewise_polynomial', 'spline', 'pchip' and 'akima'\n        methods are wrappers around the respective SciPy implementations of\n        similar names. These use the actual numerical values of the index.\n        For more information on their behavior, see the\n        `SciPy documentation\n        <https://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation>`__\n        and `SciPy tutorial\n        <https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html>`__.\n\n        Examples\n        --------\n        Filling in ``NaN`` in a :class:`~pandas.Series` via linear\n        interpolation.\n\n        >>> s = pd.Series([0, 1, np.nan, 3])\n        >>> s\n        0    0.0\n        1    1.0\n        2    NaN\n        3    3.0\n        dtype: float64\n        >>> s.interpolate()\n        0    0.0\n        1    1.0\n        2    2.0\n        3    3.0\n        dtype: float64\n\n        Filling in ``NaN`` in a Series by padding, but filling at most two\n        consecutive ``NaN`` at a time.\n\n        >>> s = pd.Series([np.nan, \"single_one\", np.nan,\n        ...                \"fill_two_more\", np.nan, np.nan, np.nan,\n        ...                4.71, np.nan])\n        >>> s\n        0              NaN\n        1       single_one\n        2              NaN\n        3    fill_two_more\n        4              NaN\n        5              NaN\n        6              NaN\n        7             4.71\n        8              NaN\n        dtype: object\n        >>> s.interpolate(method='pad', limit=2)\n        0              NaN\n        1       single_one\n        2       single_one\n        3    fill_two_more\n        4    fill_two_more\n        5    fill_two_more\n        6              NaN\n        7             4.71\n        8             4.71\n        dtype: object\n\n        Filling in ``NaN`` in a Series via polynomial interpolation or splines:\n        Both 'polynomial' and 'spline' methods require that you also specify\n        an ``order`` (int).\n\n        >>> s = pd.Series([0, 2, np.nan, 8])\n        >>> s.interpolate(method='polynomial', order=2)\n        0    0.000000\n        1    2.000000\n        2    4.666667\n        3    8.000000\n        dtype: float64\n\n        Fill the DataFrame forward (that is, going down) along each column\n        using linear interpolation.\n\n        Note how the last entry in column 'a' is interpolated differently,\n        because there is no entry after it to use for interpolation.\n        Note how the first entry in column 'b' remains ``NaN``, because there\n        is no entry before it to use for interpolation.\n\n        >>> df = pd.DataFrame([(0.0, np.nan, -1.0, 1.0),\n        ...                    (np.nan, 2.0, np.nan, np.nan),\n        ...                    (2.0, 3.0, np.nan, 9.0),\n        ...                    (np.nan, 4.0, -4.0, 16.0)],\n        ...                   columns=list('abcd'))\n        >>> df\n             a    b    c     d\n        0  0.0  NaN -1.0   1.0\n        1  NaN  2.0  NaN   NaN\n        2  2.0  3.0  NaN   9.0\n        3  NaN  4.0 -4.0  16.0\n        >>> df.interpolate(method='linear', limit_direction='forward', axis=0)\n             a    b    c     d\n        0  0.0  NaN -1.0   1.0\n        1  1.0  2.0 -2.0   5.0\n        2  2.0  3.0 -3.0   9.0\n        3  2.0  4.0 -4.0  16.0\n\n        Using polynomial interpolation.\n\n        >>> df['d'].interpolate(method='polynomial', order=2)\n        0     1.0\n        1     4.0\n        2     9.0\n        3    16.0\n        Name: d, dtype: float64\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        axis = self._get_axis_number(axis)\n\n        fillna_methods = [\"ffill\", \"bfill\", \"pad\", \"backfill\"]\n        should_transpose = axis == 1 and method not in fillna_methods\n\n        obj = self.T if should_transpose else self\n\n        if obj.empty:\n            return self.copy()\n\n        if method not in fillna_methods:\n            axis = self._info_axis_number\n\n        if isinstance(obj.index, MultiIndex) and method != \"linear\":\n            raise ValueError(\n                \"Only `method=linear` interpolation is supported on MultiIndexes.\"\n            )\n\n        # Set `limit_direction` depending on `method`\n        if limit_direction is None:\n            limit_direction = (\n                \"backward\" if method in (\"backfill\", \"bfill\") else \"forward\"\n            )\n        else:\n            if method in (\"pad\", \"ffill\") and limit_direction != \"forward\":\n                raise ValueError(\n                    f\"`limit_direction` must be 'forward' for method `{method}`\"\n                )\n            if method in (\"backfill\", \"bfill\") and limit_direction != \"backward\":\n                raise ValueError(\n                    f\"`limit_direction` must be 'backward' for method `{method}`\"\n                )\n\n        if obj.ndim == 2 and np.all(obj.dtypes == np.dtype(object)):\n            raise TypeError(\n                \"Cannot interpolate with all object-dtype columns \"\n                \"in the DataFrame. Try setting at least one \"\n                \"column to a numeric dtype.\"\n            )\n\n        # create/use the index\n        if method == \"linear\":\n            # prior default\n            index = np.arange(len(obj.index))\n        else:\n            index = obj.index\n            methods = {\"index\", \"values\", \"nearest\", \"time\"}\n            is_numeric_or_datetime = (\n                is_numeric_dtype(index.dtype)\n                or is_datetime64_any_dtype(index.dtype)\n                or is_timedelta64_dtype(index.dtype)\n            )\n            if method not in methods and not is_numeric_or_datetime:\n                raise ValueError(\n                    \"Index column must be numeric or datetime type when \"\n                    f\"using {method} method other than linear. \"\n                    \"Try setting a numeric or datetime index column before \"\n                    \"interpolating.\"\n                )\n\n        if isna(index).any():\n            raise NotImplementedError(\n                \"Interpolation with NaNs in the index \"\n                \"has not been implemented. Try filling \"\n                \"those NaNs before interpolating.\"\n            )\n        new_data = obj._mgr.interpolate(\n            method=method,\n            axis=axis,\n            index=index,\n            limit=limit,\n            limit_direction=limit_direction,\n            limit_area=limit_area,\n            inplace=inplace,\n            downcast=downcast,\n            **kwargs,\n        )\n\n        result = self._constructor(new_data)\n        if should_transpose:\n            result = result.T\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"interpolate\")\n\n    # ----------------------------------------------------------------------\n    # Timeseries methods Methods\n\n    def asof(self, where, subset=None):\n        \"\"\"\n        Return the last row(s) without any NaNs before `where`.\n\n        The last row (for each element in `where`, if list) without any\n        NaN is taken.\n        In case of a :class:`~pandas.DataFrame`, the last row without NaN\n        considering only the subset of columns (if not `None`)\n\n        If there is no good value, NaN is returned for a Series or\n        a Series of NaN values for a DataFrame\n\n        Parameters\n        ----------\n        where : date or array-like of dates\n            Date(s) before which the last row(s) are returned.\n        subset : str or array-like of str, default `None`\n            For DataFrame, if not `None`, only use these columns to\n            check for NaNs.\n\n        Returns\n        -------\n        scalar, Series, or DataFrame\n\n            The return can be:\n\n            * scalar : when `self` is a Series and `where` is a scalar\n            * Series: when `self` is a Series and `where` is an array-like,\n              or when `self` is a DataFrame and `where` is a scalar\n            * DataFrame : when `self` is a DataFrame and `where` is an\n              array-like\n\n            Return scalar, Series, or DataFrame.\n\n        See Also\n        --------\n        merge_asof : Perform an asof merge. Similar to left join.\n\n        Notes\n        -----\n        Dates are assumed to be sorted. Raises if this is not the case.\n\n        Examples\n        --------\n        A Series and a scalar `where`.\n\n        >>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])\n        >>> s\n        10    1.0\n        20    2.0\n        30    NaN\n        40    4.0\n        dtype: float64\n\n        >>> s.asof(20)\n        2.0\n\n        For a sequence `where`, a Series is returned. The first value is\n        NaN, because the first element of `where` is before the first\n        index value.\n\n        >>> s.asof([5, 20])\n        5     NaN\n        20    2.0\n        dtype: float64\n\n        Missing values are not considered. The following is ``2.0``, not\n        NaN, even though NaN is at the index location for ``30``.\n\n        >>> s.asof(30)\n        2.0\n\n        Take all columns into consideration\n\n        >>> df = pd.DataFrame({'a': [10, 20, 30, 40, 50],\n        ...                    'b': [None, None, None, None, 500]},\n        ...                   index=pd.DatetimeIndex(['2018-02-27 09:01:00',\n        ...                                           '2018-02-27 09:02:00',\n        ...                                           '2018-02-27 09:03:00',\n        ...                                           '2018-02-27 09:04:00',\n        ...                                           '2018-02-27 09:05:00']))\n        >>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n        ...                           '2018-02-27 09:04:30']))\n                              a   b\n        2018-02-27 09:03:30 NaN NaN\n        2018-02-27 09:04:30 NaN NaN\n\n        Take a single column into consideration\n\n        >>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n        ...                           '2018-02-27 09:04:30']),\n        ...         subset=['a'])\n                                 a   b\n        2018-02-27 09:03:30   30.0 NaN\n        2018-02-27 09:04:30   40.0 NaN\n        \"\"\"\n        if isinstance(where, str):\n            where = Timestamp(where)\n\n        if not self.index.is_monotonic:\n            raise ValueError(\"asof requires a sorted index\")\n\n        is_series = isinstance(self, ABCSeries)\n        if is_series:\n            if subset is not None:\n                raise ValueError(\"subset is not valid for Series\")\n        else:\n            if subset is None:\n                subset = self.columns\n            if not is_list_like(subset):\n                subset = [subset]\n\n        is_list = is_list_like(where)\n        if not is_list:\n            start = self.index[0]\n            if isinstance(self.index, PeriodIndex):\n                where = Period(where, freq=self.index.freq)\n\n            if where < start:\n                if not is_series:\n                    return self._constructor_sliced(\n                        index=self.columns, name=where, dtype=np.float64\n                    )\n                return np.nan\n\n            # It's always much faster to use a *while* loop here for\n            # Series than pre-computing all the NAs. However a\n            # *while* loop is extremely expensive for DataFrame\n            # so we later pre-compute all the NAs and use the same\n            # code path whether *where* is a scalar or list.\n            # See PR: https://github.com/pandas-dev/pandas/pull/14476\n            if is_series:\n                loc = self.index.searchsorted(where, side=\"right\")\n                if loc > 0:\n                    loc -= 1\n\n                values = self._values\n                while loc > 0 and isna(values[loc]):\n                    loc -= 1\n                return values[loc]\n\n        if not isinstance(where, Index):\n            where = Index(where) if is_list else Index([where])\n\n        nulls = self.isna() if is_series else self[subset].isna().any(1)\n        if nulls.all():\n            if is_series:\n                self = cast(\"Series\", self)\n                return self._constructor(np.nan, index=where, name=self.name)\n            elif is_list:\n                self = cast(\"DataFrame\", self)\n                return self._constructor(np.nan, index=where, columns=self.columns)\n            else:\n                self = cast(\"DataFrame\", self)\n                return self._constructor_sliced(\n                    np.nan, index=self.columns, name=where[0]\n                )\n\n        locs = self.index.asof_locs(where, ~(nulls._values))\n\n        # mask the missing\n        missing = locs == -1\n        data = self.take(locs)\n        data.index = where\n        data.loc[missing] = np.nan\n        return data if is_list else data.iloc[-1]\n\n    # ----------------------------------------------------------------------\n    # Action Methods\n\n    @doc(klass=_shared_doc_kwargs[\"klass\"])\n    def isna(self: FrameOrSeries) -> FrameOrSeries:\n        \"\"\"\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as None or :attr:`numpy.NaN`, gets mapped to True\n        values.\n        Everything else gets mapped to False values. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n\n        Returns\n        -------\n        {klass}\n            Mask of bool values for each element in {klass} that\n            indicates whether an element is an NA value.\n\n        See Also\n        --------\n        {klass}.isnull : Alias of isna.\n        {klass}.notna : Boolean inverse of isna.\n        {klass}.dropna : Omit axes labels with missing values.\n        isna : Top-level isna.\n\n        Examples\n        --------\n        Show which entries in a DataFrame are NA.\n\n        >>> df = pd.DataFrame(dict(age=[5, 6, np.NaN],\n        ...                    born=[pd.NaT, pd.Timestamp('1939-05-27'),\n        ...                             pd.Timestamp('1940-04-25')],\n        ...                    name=['Alfred', 'Batman', ''],\n        ...                    toy=[None, 'Batmobile', 'Joker']))\n        >>> df\n           age       born    name        toy\n        0  5.0        NaT  Alfred       None\n        1  6.0 1939-05-27  Batman  Batmobile\n        2  NaN 1940-04-25              Joker\n\n        >>> df.isna()\n             age   born   name    toy\n        0  False   True  False   True\n        1  False  False  False  False\n        2   True  False  False  False\n\n        Show which entries in a Series are NA.\n\n        >>> ser = pd.Series([5, 6, np.NaN])\n        >>> ser\n        0    5.0\n        1    6.0\n        2    NaN\n        dtype: float64\n\n        >>> ser.isna()\n        0    False\n        1    False\n        2     True\n        dtype: bool\n        \"\"\"\n        return isna(self).__finalize__(self, method=\"isna\")\n\n    @doc(isna, klass=_shared_doc_kwargs[\"klass\"])\n    def isnull(self: FrameOrSeries) -> FrameOrSeries:\n        return isna(self).__finalize__(self, method=\"isnull\")\n\n    @doc(klass=_shared_doc_kwargs[\"klass\"])\n    def notna(self: FrameOrSeries) -> FrameOrSeries:\n        \"\"\"\n        Detect existing (non-missing) values.\n\n        Return a boolean same-sized object indicating if the values are not NA.\n        Non-missing values get mapped to True. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n        NA values, such as None or :attr:`numpy.NaN`, get mapped to False\n        values.\n\n        Returns\n        -------\n        {klass}\n            Mask of bool values for each element in {klass} that\n            indicates whether an element is not an NA value.\n\n        See Also\n        --------\n        {klass}.notnull : Alias of notna.\n        {klass}.isna : Boolean inverse of notna.\n        {klass}.dropna : Omit axes labels with missing values.\n        notna : Top-level notna.\n\n        Examples\n        --------\n        Show which entries in a DataFrame are not NA.\n\n        >>> df = pd.DataFrame(dict(age=[5, 6, np.NaN],\n        ...                    born=[pd.NaT, pd.Timestamp('1939-05-27'),\n        ...                             pd.Timestamp('1940-04-25')],\n        ...                    name=['Alfred', 'Batman', ''],\n        ...                    toy=[None, 'Batmobile', 'Joker']))\n        >>> df\n           age       born    name        toy\n        0  5.0        NaT  Alfred       None\n        1  6.0 1939-05-27  Batman  Batmobile\n        2  NaN 1940-04-25              Joker\n\n        >>> df.notna()\n             age   born  name    toy\n        0   True  False  True  False\n        1   True   True  True   True\n        2  False   True  True   True\n\n        Show which entries in a Series are not NA.\n\n        >>> ser = pd.Series([5, 6, np.NaN])\n        >>> ser\n        0    5.0\n        1    6.0\n        2    NaN\n        dtype: float64\n\n        >>> ser.notna()\n        0     True\n        1     True\n        2    False\n        dtype: bool\n        \"\"\"\n        return notna(self).__finalize__(self, method=\"notna\")\n\n    @doc(notna, klass=_shared_doc_kwargs[\"klass\"])\n    def notnull(self: FrameOrSeries) -> FrameOrSeries:\n        return notna(self).__finalize__(self, method=\"notnull\")\n\n    def _clip_with_scalar(self, lower, upper, inplace: bool_t = False):\n        if (lower is not None and np.any(isna(lower))) or (\n            upper is not None and np.any(isna(upper))\n        ):\n            raise ValueError(\"Cannot use an NA value as a clip threshold\")\n\n        result = self\n        mask = isna(self._values)\n\n        with np.errstate(all=\"ignore\"):\n            if upper is not None:\n                subset = self.to_numpy() <= upper\n                result = result.where(subset, upper, axis=None, inplace=False)\n            if lower is not None:\n                subset = self.to_numpy() >= lower\n                result = result.where(subset, lower, axis=None, inplace=False)\n\n        if np.any(mask):\n            result[mask] = np.nan\n\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result\n\n    def _clip_with_one_bound(self, threshold, method, axis, inplace):\n\n        if axis is not None:\n            axis = self._get_axis_number(axis)\n\n        # method is self.le for upper bound and self.ge for lower bound\n        if is_scalar(threshold) and is_number(threshold):\n            if method.__name__ == \"le\":\n                return self._clip_with_scalar(None, threshold, inplace=inplace)\n            return self._clip_with_scalar(threshold, None, inplace=inplace)\n\n        subset = method(threshold, axis=axis) | isna(self)\n\n        # GH #15390\n        # In order for where method to work, the threshold must\n        # be transformed to NDFrame from other array like structure.\n        if (not isinstance(threshold, ABCSeries)) and is_list_like(threshold):\n            if isinstance(self, ABCSeries):\n                threshold = self._constructor(threshold, index=self.index)\n            else:\n                threshold = align_method_FRAME(self, threshold, axis, flex=None)[1]\n        return self.where(subset, threshold, axis=axis, inplace=inplace)\n\n    def clip(\n        self: FrameOrSeries,\n        lower=None,\n        upper=None,\n        axis=None,\n        inplace: bool_t = False,\n        *args,\n        **kwargs,\n    ) -> FrameOrSeries:\n        \"\"\"\n        Trim values at input threshold(s).\n\n        Assigns values outside boundary to boundary values. Thresholds\n        can be singular values or array like, and in the latter case\n        the clipping is performed element-wise in the specified axis.\n\n        Parameters\n        ----------\n        lower : float or array_like, default None\n            Minimum threshold value. All values below this\n            threshold will be set to it.\n        upper : float or array_like, default None\n            Maximum threshold value. All values above this\n            threshold will be set to it.\n        axis : int or str axis name, optional\n            Align object with lower and upper along the given axis.\n        inplace : bool, default False\n            Whether to perform the operation in place on the data.\n        *args, **kwargs\n            Additional keywords have no effect but might be accepted\n            for compatibility with numpy.\n\n        Returns\n        -------\n        Series or DataFrame or None\n            Same type as calling object with the values outside the\n            clip boundaries replaced or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.clip : Trim values at input threshold in series.\n        DataFrame.clip : Trim values at input threshold in dataframe.\n        numpy.clip : Clip (limit) the values in an array.\n\n        Examples\n        --------\n        >>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}\n        >>> df = pd.DataFrame(data)\n        >>> df\n           col_0  col_1\n        0      9     -2\n        1     -3     -7\n        2      0      6\n        3     -1      8\n        4      5     -5\n\n        Clips per column using lower and upper thresholds:\n\n        >>> df.clip(-4, 6)\n           col_0  col_1\n        0      6     -2\n        1     -3     -4\n        2      0      6\n        3     -1      6\n        4      5     -4\n\n        Clips using specific lower and upper thresholds per column element:\n\n        >>> t = pd.Series([2, -4, -1, 6, 3])\n        >>> t\n        0    2\n        1   -4\n        2   -1\n        3    6\n        4    3\n        dtype: int64\n\n        >>> df.clip(t, t + 4, axis=0)\n           col_0  col_1\n        0      6      2\n        1     -3     -4\n        2      0      3\n        3      6      8\n        4      5      3\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        axis = nv.validate_clip_with_axis(axis, args, kwargs)\n        if axis is not None:\n            axis = self._get_axis_number(axis)\n\n        # GH 17276\n        # numpy doesn't like NaN as a clip value\n        # so ignore\n        # GH 19992\n        # numpy doesn't drop a list-like bound containing NaN\n        if not is_list_like(lower) and np.any(isna(lower)):\n            lower = None\n        if not is_list_like(upper) and np.any(isna(upper)):\n            upper = None\n\n        # GH 2747 (arguments were reversed)\n        if lower is not None and upper is not None:\n            if is_scalar(lower) and is_scalar(upper):\n                lower, upper = min(lower, upper), max(lower, upper)\n\n        # fast-path for scalars\n        if (lower is None or (is_scalar(lower) and is_number(lower))) and (\n            upper is None or (is_scalar(upper) and is_number(upper))\n        ):\n            return self._clip_with_scalar(lower, upper, inplace=inplace)\n\n        result = self\n        if lower is not None:\n            result = result._clip_with_one_bound(\n                lower, method=self.ge, axis=axis, inplace=inplace\n            )\n        if upper is not None:\n            if inplace:\n                result = self\n            result = result._clip_with_one_bound(\n                upper, method=self.le, axis=axis, inplace=inplace\n            )\n\n        return result\n\n    def asfreq(\n        self: FrameOrSeries,\n        freq,\n        method=None,\n        how: Optional[str] = None,\n        normalize: bool_t = False,\n        fill_value=None,\n    ) -> FrameOrSeries:\n        \"\"\"\n        Convert TimeSeries to specified frequency.\n\n        Optionally provide filling method to pad/backfill missing values.\n\n        Returns the original data conformed to a new index with the specified\n        frequency. ``resample`` is more appropriate if an operation, such as\n        summarization, is necessary to represent the data at the new frequency.\n\n        Parameters\n        ----------\n        freq : DateOffset or str\n            Frequency DateOffset or string.\n        method : {'backfill'/'bfill', 'pad'/'ffill'}, default None\n            Method to use for filling holes in reindexed Series (note this\n            does not fill NaNs that already were present):\n\n            * 'pad' / 'ffill': propagate last valid observation forward to next\n              valid\n            * 'backfill' / 'bfill': use NEXT valid observation to fill.\n        how : {'start', 'end'}, default end\n            For PeriodIndex only (see PeriodIndex.asfreq).\n        normalize : bool, default False\n            Whether to reset output index to midnight.\n        fill_value : scalar, optional\n            Value to use for missing values, applied during upsampling (note\n            this does not fill NaNs that already were present).\n\n        Returns\n        -------\n        Same type as caller\n            Object converted to the specified frequency.\n\n        See Also\n        --------\n        reindex : Conform DataFrame to new index with optional filling logic.\n\n        Notes\n        -----\n        To learn more about the frequency strings, please see `this link\n        <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\n        Examples\n        --------\n        Start by creating a series with 4 one minute timestamps.\n\n        >>> index = pd.date_range('1/1/2000', periods=4, freq='T')\n        >>> series = pd.Series([0.0, None, 2.0, 3.0], index=index)\n        >>> df = pd.DataFrame({'s':series})\n        >>> df\n                               s\n        2000-01-01 00:00:00    0.0\n        2000-01-01 00:01:00    NaN\n        2000-01-01 00:02:00    2.0\n        2000-01-01 00:03:00    3.0\n\n        Upsample the series into 30 second bins.\n\n        >>> df.asfreq(freq='30S')\n                               s\n        2000-01-01 00:00:00    0.0\n        2000-01-01 00:00:30    NaN\n        2000-01-01 00:01:00    NaN\n        2000-01-01 00:01:30    NaN\n        2000-01-01 00:02:00    2.0\n        2000-01-01 00:02:30    NaN\n        2000-01-01 00:03:00    3.0\n\n        Upsample again, providing a ``fill value``.\n\n        >>> df.asfreq(freq='30S', fill_value=9.0)\n                               s\n        2000-01-01 00:00:00    0.0\n        2000-01-01 00:00:30    9.0\n        2000-01-01 00:01:00    NaN\n        2000-01-01 00:01:30    9.0\n        2000-01-01 00:02:00    2.0\n        2000-01-01 00:02:30    9.0\n        2000-01-01 00:03:00    3.0\n\n        Upsample again, providing a ``method``.\n\n        >>> df.asfreq(freq='30S', method='bfill')\n                               s\n        2000-01-01 00:00:00    0.0\n        2000-01-01 00:00:30    NaN\n        2000-01-01 00:01:00    NaN\n        2000-01-01 00:01:30    2.0\n        2000-01-01 00:02:00    2.0\n        2000-01-01 00:02:30    3.0\n        2000-01-01 00:03:00    3.0\n        \"\"\"\n        from pandas.core.resample import asfreq\n\n        return asfreq(\n            self,\n            freq,\n            method=method,\n            how=how,\n            normalize=normalize,\n            fill_value=fill_value,\n        )\n\n    def at_time(\n        self: FrameOrSeries, time, asof: bool_t = False, axis=None\n    ) -> FrameOrSeries:\n        \"\"\"\n        Select values at particular time of day (e.g., 9:30AM).\n\n        Parameters\n        ----------\n        time : datetime.time or str\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        Series or DataFrame\n\n        Raises\n        ------\n        TypeError\n            If the index is not  a :class:`DatetimeIndex`\n\n        See Also\n        --------\n        between_time : Select values between particular times of the day.\n        first : Select initial periods of time series based on a date offset.\n        last : Select final periods of time series based on a date offset.\n        DatetimeIndex.indexer_at_time : Get just the index locations for\n            values at particular time of the day.\n\n        Examples\n        --------\n        >>> i = pd.date_range('2018-04-09', periods=4, freq='12H')\n        >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n        >>> ts\n                             A\n        2018-04-09 00:00:00  1\n        2018-04-09 12:00:00  2\n        2018-04-10 00:00:00  3\n        2018-04-10 12:00:00  4\n\n        >>> ts.at_time('12:00')\n                             A\n        2018-04-09 12:00:00  2\n        2018-04-10 12:00:00  4\n        \"\"\"\n        if axis is None:\n            axis = self._stat_axis_number\n        axis = self._get_axis_number(axis)\n\n        index = self._get_axis(axis)\n\n        if not isinstance(index, DatetimeIndex):\n            raise TypeError(\"Index must be DatetimeIndex\")\n\n        indexer = index.indexer_at_time(time, asof=asof)\n        return self._take_with_is_copy(indexer, axis=axis)\n\n    def between_time(\n        self: FrameOrSeries,\n        start_time,\n        end_time,\n        include_start: bool_t = True,\n        include_end: bool_t = True,\n        axis=None,\n    ) -> FrameOrSeries:\n        \"\"\"\n        Select values between particular times of the day (e.g., 9:00-9:30 AM).\n\n        By setting ``start_time`` to be later than ``end_time``,\n        you can get the times that are *not* between the two times.\n\n        Parameters\n        ----------\n        start_time : datetime.time or str\n            Initial time as a time filter limit.\n        end_time : datetime.time or str\n            End time as a time filter limit.\n        include_start : bool, default True\n            Whether the start time needs to be included in the result.\n        include_end : bool, default True\n            Whether the end time needs to be included in the result.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Determine range time on index or columns value.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        Series or DataFrame\n            Data from the original object filtered to the specified dates range.\n\n        Raises\n        ------\n        TypeError\n            If the index is not  a :class:`DatetimeIndex`\n\n        See Also\n        --------\n        at_time : Select values at a particular time of the day.\n        first : Select initial periods of time series based on a date offset.\n        last : Select final periods of time series based on a date offset.\n        DatetimeIndex.indexer_between_time : Get just the index locations for\n            values between particular times of the day.\n\n        Examples\n        --------\n        >>> i = pd.date_range('2018-04-09', periods=4, freq='1D20min')\n        >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n        >>> ts\n                             A\n        2018-04-09 00:00:00  1\n        2018-04-10 00:20:00  2\n        2018-04-11 00:40:00  3\n        2018-04-12 01:00:00  4\n\n        >>> ts.between_time('0:15', '0:45')\n                             A\n        2018-04-10 00:20:00  2\n        2018-04-11 00:40:00  3\n\n        You get the times that are *not* between two times by setting\n        ``start_time`` later than ``end_time``:\n\n        >>> ts.between_time('0:45', '0:15')\n                             A\n        2018-04-09 00:00:00  1\n        2018-04-12 01:00:00  4\n        \"\"\"\n        if axis is None:\n            axis = self._stat_axis_number\n        axis = self._get_axis_number(axis)\n\n        index = self._get_axis(axis)\n        if not isinstance(index, DatetimeIndex):\n            raise TypeError(\"Index must be DatetimeIndex\")\n\n        indexer = index.indexer_between_time(\n            start_time, end_time, include_start=include_start, include_end=include_end\n        )\n        return self._take_with_is_copy(indexer, axis=axis)\n\n    def resample(\n        self,\n        rule,\n        axis=0,\n        closed: Optional[str] = None,\n        label: Optional[str] = None,\n        convention: str = \"start\",\n        kind: Optional[str] = None,\n        loffset=None,\n        base: Optional[int] = None,\n        on=None,\n        level=None,\n        origin: Union[str, TimestampConvertibleTypes] = \"start_day\",\n        offset: Optional[TimedeltaConvertibleTypes] = None,\n    ) -> Resampler:\n        \"\"\"\n        Resample time-series data.\n\n        Convenience method for frequency conversion and resampling of time\n        series. Object must have a datetime-like index (`DatetimeIndex`,\n        `PeriodIndex`, or `TimedeltaIndex`), or pass datetime-like values\n        to the `on` or `level` keyword.\n\n        Parameters\n        ----------\n        rule : DateOffset, Timedelta or str\n            The offset string or object representing target conversion.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Which axis to use for up- or down-sampling. For `Series` this\n            will default to 0, i.e. along the rows. Must be\n            `DatetimeIndex`, `TimedeltaIndex` or `PeriodIndex`.\n        closed : {'right', 'left'}, default None\n            Which side of bin interval is closed. The default is 'left'\n            for all frequency offsets except for 'M', 'A', 'Q', 'BM',\n            'BA', 'BQ', and 'W' which all have a default of 'right'.\n        label : {'right', 'left'}, default None\n            Which bin edge label to label bucket with. The default is 'left'\n            for all frequency offsets except for 'M', 'A', 'Q', 'BM',\n            'BA', 'BQ', and 'W' which all have a default of 'right'.\n        convention : {'start', 'end', 's', 'e'}, default 'start'\n            For `PeriodIndex` only, controls whether to use the start or\n            end of `rule`.\n        kind : {'timestamp', 'period'}, optional, default None\n            Pass 'timestamp' to convert the resulting index to a\n            `DateTimeIndex` or 'period' to convert it to a `PeriodIndex`.\n            By default the input representation is retained.\n        loffset : timedelta, default None\n            Adjust the resampled time labels.\n\n            .. deprecated:: 1.1.0\n                You should add the loffset to the `df.index` after the resample.\n                See below.\n\n        base : int, default 0\n            For frequencies that evenly subdivide 1 day, the \"origin\" of the\n            aggregated intervals. For example, for '5min' frequency, base could\n            range from 0 through 4. Defaults to 0.\n\n            .. deprecated:: 1.1.0\n                The new arguments that you should use are 'offset' or 'origin'.\n\n        on : str, optional\n            For a DataFrame, column to use instead of index for resampling.\n            Column must be datetime-like.\n        level : str or int, optional\n            For a MultiIndex, level (name or number) to use for\n            resampling. `level` must be datetime-like.\n        origin : {'epoch', 'start', 'start_day'}, Timestamp or str, default 'start_day'\n            The timestamp on which to adjust the grouping. The timezone of origin\n            must match the timezone of the index.\n            If a timestamp is not used, these values are also supported:\n\n            - 'epoch': `origin` is 1970-01-01\n            - 'start': `origin` is the first value of the timeseries\n            - 'start_day': `origin` is the first day at midnight of the timeseries\n\n            .. versionadded:: 1.1.0\n\n        offset : Timedelta or str, default is None\n            An offset timedelta added to the origin.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Resampler object\n\n        See Also\n        --------\n        groupby : Group by mapping, function, label, or list of labels.\n        Series.resample : Resample a Series.\n        DataFrame.resample: Resample a DataFrame.\n\n        Notes\n        -----\n        See the `user guide\n        <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling>`_\n        for more.\n\n        To learn more about the offset strings, please see `this link\n        <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects>`__.\n\n        Examples\n        --------\n        Start by creating a series with 9 one minute timestamps.\n\n        >>> index = pd.date_range('1/1/2000', periods=9, freq='T')\n        >>> series = pd.Series(range(9), index=index)\n        >>> series\n        2000-01-01 00:00:00    0\n        2000-01-01 00:01:00    1\n        2000-01-01 00:02:00    2\n        2000-01-01 00:03:00    3\n        2000-01-01 00:04:00    4\n        2000-01-01 00:05:00    5\n        2000-01-01 00:06:00    6\n        2000-01-01 00:07:00    7\n        2000-01-01 00:08:00    8\n        Freq: T, dtype: int64\n\n        Downsample the series into 3 minute bins and sum the values\n        of the timestamps falling into a bin.\n\n        >>> series.resample('3T').sum()\n        2000-01-01 00:00:00     3\n        2000-01-01 00:03:00    12\n        2000-01-01 00:06:00    21\n        Freq: 3T, dtype: int64\n\n        Downsample the series into 3 minute bins as above, but label each\n        bin using the right edge instead of the left. Please note that the\n        value in the bucket used as the label is not included in the bucket,\n        which it labels. For example, in the original series the\n        bucket ``2000-01-01 00:03:00`` contains the value 3, but the summed\n        value in the resampled bucket with the label ``2000-01-01 00:03:00``\n        does not include 3 (if it did, the summed value would be 6, not 3).\n        To include this value close the right side of the bin interval as\n        illustrated in the example below this one.\n\n        >>> series.resample('3T', label='right').sum()\n        2000-01-01 00:03:00     3\n        2000-01-01 00:06:00    12\n        2000-01-01 00:09:00    21\n        Freq: 3T, dtype: int64\n\n        Downsample the series into 3 minute bins as above, but close the right\n        side of the bin interval.\n\n        >>> series.resample('3T', label='right', closed='right').sum()\n        2000-01-01 00:00:00     0\n        2000-01-01 00:03:00     6\n        2000-01-01 00:06:00    15\n        2000-01-01 00:09:00    15\n        Freq: 3T, dtype: int64\n\n        Upsample the series into 30 second bins.\n\n        >>> series.resample('30S').asfreq()[0:5]   # Select first 5 rows\n        2000-01-01 00:00:00   0.0\n        2000-01-01 00:00:30   NaN\n        2000-01-01 00:01:00   1.0\n        2000-01-01 00:01:30   NaN\n        2000-01-01 00:02:00   2.0\n        Freq: 30S, dtype: float64\n\n        Upsample the series into 30 second bins and fill the ``NaN``\n        values using the ``pad`` method.\n\n        >>> series.resample('30S').pad()[0:5]\n        2000-01-01 00:00:00    0\n        2000-01-01 00:00:30    0\n        2000-01-01 00:01:00    1\n        2000-01-01 00:01:30    1\n        2000-01-01 00:02:00    2\n        Freq: 30S, dtype: int64\n\n        Upsample the series into 30 second bins and fill the\n        ``NaN`` values using the ``bfill`` method.\n\n        >>> series.resample('30S').bfill()[0:5]\n        2000-01-01 00:00:00    0\n        2000-01-01 00:00:30    1\n        2000-01-01 00:01:00    1\n        2000-01-01 00:01:30    2\n        2000-01-01 00:02:00    2\n        Freq: 30S, dtype: int64\n\n        Pass a custom function via ``apply``\n\n        >>> def custom_resampler(array_like):\n        ...     return np.sum(array_like) + 5\n        ...\n        >>> series.resample('3T').apply(custom_resampler)\n        2000-01-01 00:00:00     8\n        2000-01-01 00:03:00    17\n        2000-01-01 00:06:00    26\n        Freq: 3T, dtype: int64\n\n        For a Series with a PeriodIndex, the keyword `convention` can be\n        used to control whether to use the start or end of `rule`.\n\n        Resample a year by quarter using 'start' `convention`. Values are\n        assigned to the first quarter of the period.\n\n        >>> s = pd.Series([1, 2], index=pd.period_range('2012-01-01',\n        ...                                             freq='A',\n        ...                                             periods=2))\n        >>> s\n        2012    1\n        2013    2\n        Freq: A-DEC, dtype: int64\n        >>> s.resample('Q', convention='start').asfreq()\n        2012Q1    1.0\n        2012Q2    NaN\n        2012Q3    NaN\n        2012Q4    NaN\n        2013Q1    2.0\n        2013Q2    NaN\n        2013Q3    NaN\n        2013Q4    NaN\n        Freq: Q-DEC, dtype: float64\n\n        Resample quarters by month using 'end' `convention`. Values are\n        assigned to the last month of the period.\n\n        >>> q = pd.Series([1, 2, 3, 4], index=pd.period_range('2018-01-01',\n        ...                                                   freq='Q',\n        ...                                                   periods=4))\n        >>> q\n        2018Q1    1\n        2018Q2    2\n        2018Q3    3\n        2018Q4    4\n        Freq: Q-DEC, dtype: int64\n        >>> q.resample('M', convention='end').asfreq()\n        2018-03    1.0\n        2018-04    NaN\n        2018-05    NaN\n        2018-06    2.0\n        2018-07    NaN\n        2018-08    NaN\n        2018-09    3.0\n        2018-10    NaN\n        2018-11    NaN\n        2018-12    4.0\n        Freq: M, dtype: float64\n\n        For DataFrame objects, the keyword `on` can be used to specify the\n        column instead of the index for resampling.\n\n        >>> d = dict({'price': [10, 11, 9, 13, 14, 18, 17, 19],\n        ...           'volume': [50, 60, 40, 100, 50, 100, 40, 50]})\n        >>> df = pd.DataFrame(d)\n        >>> df['week_starting'] = pd.date_range('01/01/2018',\n        ...                                     periods=8,\n        ...                                     freq='W')\n        >>> df\n           price  volume week_starting\n        0     10      50    2018-01-07\n        1     11      60    2018-01-14\n        2      9      40    2018-01-21\n        3     13     100    2018-01-28\n        4     14      50    2018-02-04\n        5     18     100    2018-02-11\n        6     17      40    2018-02-18\n        7     19      50    2018-02-25\n        >>> df.resample('M', on='week_starting').mean()\n                       price  volume\n        week_starting\n        2018-01-31     10.75    62.5\n        2018-02-28     17.00    60.0\n\n        For a DataFrame with MultiIndex, the keyword `level` can be used to\n        specify on which level the resampling needs to take place.\n\n        >>> days = pd.date_range('1/1/2000', periods=4, freq='D')\n        >>> d2 = dict({'price': [10, 11, 9, 13, 14, 18, 17, 19],\n        ...            'volume': [50, 60, 40, 100, 50, 100, 40, 50]})\n        >>> df2 = pd.DataFrame(d2,\n        ...                    index=pd.MultiIndex.from_product([days,\n        ...                                                     ['morning',\n        ...                                                      'afternoon']]\n        ...                                                     ))\n        >>> df2\n                              price  volume\n        2000-01-01 morning       10      50\n                   afternoon     11      60\n        2000-01-02 morning        9      40\n                   afternoon     13     100\n        2000-01-03 morning       14      50\n                   afternoon     18     100\n        2000-01-04 morning       17      40\n                   afternoon     19      50\n        >>> df2.resample('D', level=0).sum()\n                    price  volume\n        2000-01-01     21     110\n        2000-01-02     22     140\n        2000-01-03     32     150\n        2000-01-04     36      90\n\n        If you want to adjust the start of the bins based on a fixed timestamp:\n\n        >>> start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00'\n        >>> rng = pd.date_range(start, end, freq='7min')\n        >>> ts = pd.Series(np.arange(len(rng)) * 3, index=rng)\n        >>> ts\n        2000-10-01 23:30:00     0\n        2000-10-01 23:37:00     3\n        2000-10-01 23:44:00     6\n        2000-10-01 23:51:00     9\n        2000-10-01 23:58:00    12\n        2000-10-02 00:05:00    15\n        2000-10-02 00:12:00    18\n        2000-10-02 00:19:00    21\n        2000-10-02 00:26:00    24\n        Freq: 7T, dtype: int64\n\n        >>> ts.resample('17min').sum()\n        2000-10-01 23:14:00     0\n        2000-10-01 23:31:00     9\n        2000-10-01 23:48:00    21\n        2000-10-02 00:05:00    54\n        2000-10-02 00:22:00    24\n        Freq: 17T, dtype: int64\n\n        >>> ts.resample('17min', origin='epoch').sum()\n        2000-10-01 23:18:00     0\n        2000-10-01 23:35:00    18\n        2000-10-01 23:52:00    27\n        2000-10-02 00:09:00    39\n        2000-10-02 00:26:00    24\n        Freq: 17T, dtype: int64\n\n        >>> ts.resample('17min', origin='2000-01-01').sum()\n        2000-10-01 23:24:00     3\n        2000-10-01 23:41:00    15\n        2000-10-01 23:58:00    45\n        2000-10-02 00:15:00    45\n        Freq: 17T, dtype: int64\n\n        If you want to adjust the start of the bins with an `offset` Timedelta, the two\n        following lines are equivalent:\n\n        >>> ts.resample('17min', origin='start').sum()\n        2000-10-01 23:30:00     9\n        2000-10-01 23:47:00    21\n        2000-10-02 00:04:00    54\n        2000-10-02 00:21:00    24\n        Freq: 17T, dtype: int64\n\n        >>> ts.resample('17min', offset='23h30min').sum()\n        2000-10-01 23:30:00     9\n        2000-10-01 23:47:00    21\n        2000-10-02 00:04:00    54\n        2000-10-02 00:21:00    24\n        Freq: 17T, dtype: int64\n\n        To replace the use of the deprecated `base` argument, you can now use `offset`,\n        in this example it is equivalent to have `base=2`:\n\n        >>> ts.resample('17min', offset='2min').sum()\n        2000-10-01 23:16:00     0\n        2000-10-01 23:33:00     9\n        2000-10-01 23:50:00    36\n        2000-10-02 00:07:00    39\n        2000-10-02 00:24:00    24\n        Freq: 17T, dtype: int64\n\n        To replace the use of the deprecated `loffset` argument:\n\n        >>> from pandas.tseries.frequencies import to_offset\n        >>> loffset = '19min'\n        >>> ts_out = ts.resample('17min').sum()\n        >>> ts_out.index = ts_out.index + to_offset(loffset)\n        >>> ts_out\n        2000-10-01 23:33:00     0\n        2000-10-01 23:50:00     9\n        2000-10-02 00:07:00    21\n        2000-10-02 00:24:00    54\n        2000-10-02 00:41:00    24\n        Freq: 17T, dtype: int64\n        \"\"\"\n        from pandas.core.resample import get_resampler\n\n        axis = self._get_axis_number(axis)\n        return get_resampler(\n            self,\n            freq=rule,\n            label=label,\n            closed=closed,\n            axis=axis,\n            kind=kind,\n            loffset=loffset,\n            convention=convention,\n            base=base,\n            key=on,\n            level=level,\n            origin=origin,\n            offset=offset,\n        )\n\n    def first(self: FrameOrSeries, offset) -> FrameOrSeries:\n        \"\"\"\n        Select initial periods of time series data based on a date offset.\n\n        When having a DataFrame with dates as index, this function can\n        select the first few rows based on a date offset.\n\n        Parameters\n        ----------\n        offset : str, DateOffset or dateutil.relativedelta\n            The offset length of the data that will be selected. For instance,\n            '1M' will display all the rows having their index within the first month.\n\n        Returns\n        -------\n        Series or DataFrame\n            A subset of the caller.\n\n        Raises\n        ------\n        TypeError\n            If the index is not  a :class:`DatetimeIndex`\n\n        See Also\n        --------\n        last : Select final periods of time series based on a date offset.\n        at_time : Select values at a particular time of the day.\n        between_time : Select values between particular times of the day.\n\n        Examples\n        --------\n        >>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n        >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n        >>> ts\n                    A\n        2018-04-09  1\n        2018-04-11  2\n        2018-04-13  3\n        2018-04-15  4\n\n        Get the rows for the first 3 days:\n\n        >>> ts.first('3D')\n                    A\n        2018-04-09  1\n        2018-04-11  2\n\n        Notice the data for 3 first calendar days were returned, not the first\n        3 days observed in the dataset, and therefore data for 2018-04-13 was\n        not returned.\n        \"\"\"\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(\"'first' only supports a DatetimeIndex index\")\n\n        if len(self.index) == 0:\n            return self\n\n        offset = to_offset(offset)\n        end_date = end = self.index[0] + offset\n\n        # Tick-like, e.g. 3 weeks\n        if isinstance(offset, Tick):\n            if end_date in self.index:\n                end = self.index.searchsorted(end_date, side=\"left\")\n                return self.iloc[:end]\n\n        return self.loc[:end]\n\n    def last(self: FrameOrSeries, offset) -> FrameOrSeries:\n        \"\"\"\n        Select final periods of time series data based on a date offset.\n\n        When having a DataFrame with dates as index, this function can\n        select the last few rows based on a date offset.\n\n        Parameters\n        ----------\n        offset : str, DateOffset, dateutil.relativedelta\n            The offset length of the data that will be selected. For instance,\n            '3D' will display all the rows having their index within the last 3 days.\n\n        Returns\n        -------\n        Series or DataFrame\n            A subset of the caller.\n\n        Raises\n        ------\n        TypeError\n            If the index is not  a :class:`DatetimeIndex`\n\n        See Also\n        --------\n        first : Select initial periods of time series based on a date offset.\n        at_time : Select values at a particular time of the day.\n        between_time : Select values between particular times of the day.\n\n        Examples\n        --------\n        >>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n        >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n        >>> ts\n                    A\n        2018-04-09  1\n        2018-04-11  2\n        2018-04-13  3\n        2018-04-15  4\n\n        Get the rows for the last 3 days:\n\n        >>> ts.last('3D')\n                    A\n        2018-04-13  3\n        2018-04-15  4\n\n        Notice the data for 3 last calendar days were returned, not the last\n        3 observed days in the dataset, and therefore data for 2018-04-11 was\n        not returned.\n        \"\"\"\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(\"'last' only supports a DatetimeIndex index\")\n\n        if len(self.index) == 0:\n            return self\n\n        offset = to_offset(offset)\n\n        start_date = self.index[-1] - offset\n        start = self.index.searchsorted(start_date, side=\"right\")\n        return self.iloc[start:]\n\n    def rank(\n        self: FrameOrSeries,\n        axis=0,\n        method: str = \"average\",\n        numeric_only: Optional[bool_t] = None,\n        na_option: str = \"keep\",\n        ascending: bool_t = True,\n        pct: bool_t = False,\n    ) -> FrameOrSeries:\n        \"\"\"\n        Compute numerical data ranks (1 through n) along axis.\n\n        By default, equal values are assigned a rank that is the average of the\n        ranks of those values.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Index to direct ranking.\n        method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n            How to rank the group of records that have the same value (i.e. ties):\n\n            * average: average rank of the group\n            * min: lowest rank in the group\n            * max: highest rank in the group\n            * first: ranks assigned in order they appear in the array\n            * dense: like 'min', but rank always increases by 1 between groups.\n\n        numeric_only : bool, optional\n            For DataFrame objects, rank only numeric columns if set to True.\n        na_option : {'keep', 'top', 'bottom'}, default 'keep'\n            How to rank NaN values:\n\n            * keep: assign NaN rank to NaN values\n            * top: assign smallest rank to NaN values if ascending\n            * bottom: assign highest rank to NaN values if ascending.\n\n        ascending : bool, default True\n            Whether or not the elements should be ranked in ascending order.\n        pct : bool, default False\n            Whether or not to display the returned rankings in percentile\n            form.\n\n        Returns\n        -------\n        same type as caller\n            Return a Series or DataFrame with data ranks as values.\n\n        See Also\n        --------\n        core.groupby.GroupBy.rank : Rank of values within each group.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(data={'Animal': ['cat', 'penguin', 'dog',\n        ...                                    'spider', 'snake'],\n        ...                         'Number_legs': [4, 2, 4, 8, np.nan]})\n        >>> df\n            Animal  Number_legs\n        0      cat          4.0\n        1  penguin          2.0\n        2      dog          4.0\n        3   spider          8.0\n        4    snake          NaN\n\n        The following example shows how the method behaves with the above\n        parameters:\n\n        * default_rank: this is the default behaviour obtained without using\n          any parameter.\n        * max_rank: setting ``method = 'max'`` the records that have the\n          same values are ranked using the highest rank (e.g.: since 'cat'\n          and 'dog' are both in the 2nd and 3rd position, rank 3 is assigned.)\n        * NA_bottom: choosing ``na_option = 'bottom'``, if there are records\n          with NaN values they are placed at the bottom of the ranking.\n        * pct_rank: when setting ``pct = True``, the ranking is expressed as\n          percentile rank.\n\n        >>> df['default_rank'] = df['Number_legs'].rank()\n        >>> df['max_rank'] = df['Number_legs'].rank(method='max')\n        >>> df['NA_bottom'] = df['Number_legs'].rank(na_option='bottom')\n        >>> df['pct_rank'] = df['Number_legs'].rank(pct=True)\n        >>> df\n            Animal  Number_legs  default_rank  max_rank  NA_bottom  pct_rank\n        0      cat          4.0           2.5       3.0        2.5     0.625\n        1  penguin          2.0           1.0       1.0        1.0     0.250\n        2      dog          4.0           2.5       3.0        2.5     0.625\n        3   spider          8.0           4.0       4.0        4.0     1.000\n        4    snake          NaN           NaN       NaN        5.0       NaN\n        \"\"\"\n        axis = self._get_axis_number(axis)\n\n        if na_option not in {\"keep\", \"top\", \"bottom\"}:\n            msg = \"na_option must be one of 'keep', 'top', or 'bottom'\"\n            raise ValueError(msg)\n\n        def ranker(data):\n            ranks = algos.rank(\n                data.values,\n                axis=axis,\n                method=method,\n                ascending=ascending,\n                na_option=na_option,\n                pct=pct,\n            )\n            ranks = self._constructor(ranks, **data._construct_axes_dict())\n            return ranks.__finalize__(self, method=\"rank\")\n\n        # if numeric_only is None, and we can't get anything, we try with\n        # numeric_only=True\n        if numeric_only is None:\n            try:\n                return ranker(self)\n            except TypeError:\n                numeric_only = True\n\n        if numeric_only:\n            data = self._get_numeric_data()\n        else:\n            data = self\n\n        return ranker(data)\n\n    @Appender(_shared_docs[\"compare\"] % _shared_doc_kwargs)\n    def compare(\n        self,\n        other,\n        align_axis: Axis = 1,\n        keep_shape: bool_t = False,\n        keep_equal: bool_t = False,\n    ):\n        from pandas.core.reshape.concat import concat\n\n        if type(self) is not type(other):\n            cls_self, cls_other = type(self).__name__, type(other).__name__\n            raise TypeError(\n                f\"can only compare '{cls_self}' (not '{cls_other}') with '{cls_self}'\"\n            )\n\n        mask = ~((self == other) | (self.isna() & other.isna()))\n        keys = [\"self\", \"other\"]\n\n        if not keep_equal:\n            self = self.where(mask)\n            other = other.where(mask)\n\n        if not keep_shape:\n            if isinstance(self, ABCDataFrame):\n                cmask = mask.any()\n                rmask = mask.any(axis=1)\n                self = self.loc[rmask, cmask]\n                other = other.loc[rmask, cmask]\n            else:\n                self = self[mask]\n                other = other[mask]\n\n        if align_axis in (1, \"columns\"):  # This is needed for Series\n            axis = 1\n        else:\n            axis = self._get_axis_number(align_axis)\n\n        diff = concat([self, other], axis=axis, keys=keys)\n\n        if axis >= self.ndim:\n            # No need to reorganize data if stacking on new axis\n            # This currently applies for stacking two Series on columns\n            return diff\n\n        ax = diff._get_axis(axis)\n        ax_names = np.array(ax.names)\n\n        # set index names to positions to avoid confusion\n        ax.names = np.arange(len(ax_names))\n\n        # bring self-other to inner level\n        order = list(range(1, ax.nlevels)) + [0]\n        if isinstance(diff, ABCDataFrame):\n            diff = diff.reorder_levels(order, axis=axis)\n        else:\n            diff = diff.reorder_levels(order)\n\n        # restore the index names in order\n        diff._get_axis(axis=axis).names = ax_names[order]\n\n        # reorder axis to keep things organized\n        indices = (\n            np.arange(diff.shape[axis]).reshape([2, diff.shape[axis] // 2]).T.flatten()\n        )\n        diff = diff.take(indices, axis=axis)\n\n        return diff\n\n    @doc(**_shared_doc_kwargs)\n    def align(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy=True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n        broadcast_axis=None,\n    ):\n        \"\"\"\n        Align two objects on their axes with the specified join method.\n\n        Join method is specified for each axis Index.\n\n        Parameters\n        ----------\n        other : DataFrame or Series\n        join : {{'outer', 'inner', 'left', 'right'}}, default 'outer'\n        axis : allowed axis of the other object, default None\n            Align on index (0), columns (1), or both (None).\n        level : int or level name, default None\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level.\n        copy : bool, default True\n            Always returns new objects. If copy=False and no reindexing is\n            required then original objects are returned.\n        fill_value : scalar, default np.NaN\n            Value to use for missing values. Defaults to NaN, but can be any\n            \"compatible\" value.\n        method : {{'backfill', 'bfill', 'pad', 'ffill', None}}, default None\n            Method to use for filling holes in reindexed Series:\n\n            - pad / ffill: propagate last valid observation forward to next valid.\n            - backfill / bfill: use NEXT valid observation to fill gap.\n\n        limit : int, default None\n            If method is specified, this is the maximum number of consecutive\n            NaN values to forward/backward fill. In other words, if there is\n            a gap with more than this number of consecutive NaNs, it will only\n            be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled. Must be greater than 0 if not None.\n        fill_axis : {axes_single_arg}, default 0\n            Filling axis, method and limit.\n        broadcast_axis : {axes_single_arg}, default None\n            Broadcast values along this axis, if aligning two objects of\n            different dimensions.\n\n        Returns\n        -------\n        (left, right) : ({klass}, type of other)\n            Aligned objects.\n        \"\"\"\n\n        method = missing.clean_fill_method(method)\n\n        if broadcast_axis == 1 and self.ndim != other.ndim:\n            if isinstance(self, ABCSeries):\n                # this means other is a DataFrame, and we need to broadcast\n                # self\n                cons = self._constructor_expanddim\n                df = cons(\n                    {c: self for c in other.columns}, **other._construct_axes_dict()\n                )\n                return df._align_frame(\n                    other,\n                    join=join,\n                    axis=axis,\n                    level=level,\n                    copy=copy,\n                    fill_value=fill_value,\n                    method=method,\n                    limit=limit,\n                    fill_axis=fill_axis,\n                )\n            elif isinstance(other, ABCSeries):\n                # this means self is a DataFrame, and we need to broadcast\n                # other\n                cons = other._constructor_expanddim\n                df = cons(\n                    {c: other for c in self.columns}, **self._construct_axes_dict()\n                )\n                return self._align_frame(\n                    df,\n                    join=join,\n                    axis=axis,\n                    level=level,\n                    copy=copy,\n                    fill_value=fill_value,\n                    method=method,\n                    limit=limit,\n                    fill_axis=fill_axis,\n                )\n\n        if axis is not None:\n            axis = self._get_axis_number(axis)\n        if isinstance(other, ABCDataFrame):\n            return self._align_frame(\n                other,\n                join=join,\n                axis=axis,\n                level=level,\n                copy=copy,\n                fill_value=fill_value,\n                method=method,\n                limit=limit,\n                fill_axis=fill_axis,\n            )\n        elif isinstance(other, ABCSeries):\n            return self._align_series(\n                other,\n                join=join,\n                axis=axis,\n                level=level,\n                copy=copy,\n                fill_value=fill_value,\n                method=method,\n                limit=limit,\n                fill_axis=fill_axis,\n            )\n        else:  # pragma: no cover\n            raise TypeError(f\"unsupported type: {type(other)}\")\n\n    def _align_frame(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy: bool_t = True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n    ):\n        # defaults\n        join_index, join_columns = None, None\n        ilidx, iridx = None, None\n        clidx, cridx = None, None\n\n        is_series = isinstance(self, ABCSeries)\n\n        if axis is None or axis == 0:\n            if not self.index.equals(other.index):\n                join_index, ilidx, iridx = self.index.join(\n                    other.index, how=join, level=level, return_indexers=True\n                )\n\n        if axis is None or axis == 1:\n            if not is_series and not self.columns.equals(other.columns):\n                join_columns, clidx, cridx = self.columns.join(\n                    other.columns, how=join, level=level, return_indexers=True\n                )\n\n        if is_series:\n            reindexers = {0: [join_index, ilidx]}\n        else:\n            reindexers = {0: [join_index, ilidx], 1: [join_columns, clidx]}\n\n        left = self._reindex_with_indexers(\n            reindexers, copy=copy, fill_value=fill_value, allow_dups=True\n        )\n        # other must be always DataFrame\n        right = other._reindex_with_indexers(\n            {0: [join_index, iridx], 1: [join_columns, cridx]},\n            copy=copy,\n            fill_value=fill_value,\n            allow_dups=True,\n        )\n\n        if method is not None:\n            _left = left.fillna(method=method, axis=fill_axis, limit=limit)\n            assert _left is not None  # needed for mypy\n            left = _left\n            right = right.fillna(method=method, axis=fill_axis, limit=limit)\n\n        # if DatetimeIndex have different tz, convert to UTC\n        if is_datetime64tz_dtype(left.index.dtype):\n            if left.index.tz != right.index.tz:\n                if join_index is not None:\n                    # GH#33671 ensure we don't change the index on\n                    #  our original Series (NB: by default deep=False)\n                    left = left.copy()\n                    right = right.copy()\n                    left.index = join_index\n                    right.index = join_index\n\n        return (\n            left.__finalize__(self),\n            right.__finalize__(other),\n        )\n\n    def _align_series(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy: bool_t = True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n    ):\n\n        is_series = isinstance(self, ABCSeries)\n\n        # series/series compat, other must always be a Series\n        if is_series:\n            if axis:\n                raise ValueError(\"cannot align series to a series other than axis 0\")\n\n            # equal\n            if self.index.equals(other.index):\n                join_index, lidx, ridx = None, None, None\n            else:\n                join_index, lidx, ridx = self.index.join(\n                    other.index, how=join, level=level, return_indexers=True\n                )\n\n            left = self._reindex_indexer(join_index, lidx, copy)\n            right = other._reindex_indexer(join_index, ridx, copy)\n\n        else:\n            # one has > 1 ndim\n            fdata = self._mgr\n            if axis == 0:\n                join_index = self.index\n                lidx, ridx = None, None\n                if not self.index.equals(other.index):\n                    join_index, lidx, ridx = self.index.join(\n                        other.index, how=join, level=level, return_indexers=True\n                    )\n\n                if lidx is not None:\n                    fdata = fdata.reindex_indexer(join_index, lidx, axis=1)\n\n            elif axis == 1:\n                join_index = self.columns\n                lidx, ridx = None, None\n                if not self.columns.equals(other.index):\n                    join_index, lidx, ridx = self.columns.join(\n                        other.index, how=join, level=level, return_indexers=True\n                    )\n\n                if lidx is not None:\n                    fdata = fdata.reindex_indexer(join_index, lidx, axis=0)\n            else:\n                raise ValueError(\"Must specify axis=0 or 1\")\n\n            if copy and fdata is self._mgr:\n                fdata = fdata.copy()\n\n            left = self._constructor(fdata)\n\n            if ridx is None:\n                right = other\n            else:\n                right = other.reindex(join_index, level=level)\n\n        # fill\n        fill_na = notna(fill_value) or (method is not None)\n        if fill_na:\n            left = left.fillna(fill_value, method=method, limit=limit, axis=fill_axis)\n            right = right.fillna(fill_value, method=method, limit=limit)\n\n        # if DatetimeIndex have different tz, convert to UTC\n        if is_series or (not is_series and axis == 0):\n            if is_datetime64tz_dtype(left.index.dtype):\n                if left.index.tz != right.index.tz:\n                    if join_index is not None:\n                        # GH#33671 ensure we don't change the index on\n                        #  our original Series (NB: by default deep=False)\n                        left = left.copy()\n                        right = right.copy()\n                        left.index = join_index\n                        right.index = join_index\n\n        return (\n            left.__finalize__(self),\n            right.__finalize__(other),\n        )\n\n    def _where(\n        self,\n        cond,\n        other=np.nan,\n        inplace=False,\n        axis=None,\n        level=None,\n        errors=\"raise\",\n        try_cast=False,\n    ):\n        \"\"\"\n        Equivalent to public method `where`, except that `other` is not\n        applied as a function even if callable. Used in __setitem__.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        # align the cond to same shape as myself\n        cond = com.apply_if_callable(cond, self)\n        if isinstance(cond, NDFrame):\n            cond, _ = cond.align(self, join=\"right\", broadcast_axis=1)\n        else:\n            if not hasattr(cond, \"shape\"):\n                cond = np.asanyarray(cond)\n            if cond.shape != self.shape:\n                raise ValueError(\"Array conditional must be same shape as self\")\n            cond = self._constructor(cond, **self._construct_axes_dict())\n\n        # make sure we are boolean\n        fill_value = bool(inplace)\n        cond = cond.fillna(fill_value)\n\n        msg = \"Boolean array expected for the condition, not {dtype}\"\n\n        if not cond.empty:\n            if not isinstance(cond, ABCDataFrame):\n                # This is a single-dimensional object.\n                if not is_bool_dtype(cond):\n                    raise ValueError(msg.format(dtype=cond.dtype))\n            else:\n                for dt in cond.dtypes:\n                    if not is_bool_dtype(dt):\n                        raise ValueError(msg.format(dtype=dt))\n        else:\n            # GH#21947 we have an empty DataFrame/Series, could be object-dtype\n            cond = cond.astype(bool)\n\n        cond = -cond if inplace else cond\n\n        # try to align with other\n        try_quick = True\n        if isinstance(other, NDFrame):\n\n            # align with me\n            if other.ndim <= self.ndim:\n\n                _, other = self.align(\n                    other, join=\"left\", axis=axis, level=level, fill_value=np.nan\n                )\n\n                # if we are NOT aligned, raise as we cannot where index\n                if axis is None and not all(\n                    other._get_axis(i).equals(ax) for i, ax in enumerate(self.axes)\n                ):\n                    raise InvalidIndexError\n\n            # slice me out of the other\n            else:\n                raise NotImplementedError(\n                    \"cannot align with a higher dimensional NDFrame\"\n                )\n\n        if isinstance(other, np.ndarray):\n\n            if other.shape != self.shape:\n\n                if self.ndim == 1:\n\n                    icond = cond._values\n\n                    # GH 2745 / GH 4192\n                    # treat like a scalar\n                    if len(other) == 1:\n                        other = other[0]\n\n                    # GH 3235\n                    # match True cond to other\n                    elif len(cond[icond]) == len(other):\n\n                        # try to not change dtype at first (if try_quick)\n                        if try_quick:\n                            new_other = np.asarray(self)\n                            new_other = new_other.copy()\n                            new_other[icond] = other\n                            other = new_other\n\n                    else:\n                        raise ValueError(\n                            \"Length of replacements must equal series length\"\n                        )\n\n                else:\n                    raise ValueError(\n                        \"other must be the same shape as self when an ndarray\"\n                    )\n\n            # we are the same shape, so create an actual object for alignment\n            else:\n                other = self._constructor(other, **self._construct_axes_dict())\n\n        if axis is None:\n            axis = 0\n\n        if self.ndim == getattr(other, \"ndim\", 0):\n            align = True\n        else:\n            align = self._get_axis_number(axis) == 1\n\n        if align and isinstance(other, NDFrame):\n            other = other.reindex(self._info_axis, axis=self._info_axis_number)\n        if isinstance(cond, NDFrame):\n            cond = cond.reindex(self._info_axis, axis=self._info_axis_number)\n\n        block_axis = self._get_block_manager_axis(axis)\n\n        if inplace:\n            # we may have different type blocks come out of putmask, so\n            # reconstruct the block manager\n\n            self._check_inplace_setting(other)\n            new_data = self._mgr.putmask(\n                mask=cond, new=other, align=align, axis=block_axis\n            )\n            result = self._constructor(new_data)\n            return self._update_inplace(result)\n\n        else:\n            new_data = self._mgr.where(\n                other=other,\n                cond=cond,\n                align=align,\n                errors=errors,\n                try_cast=try_cast,\n                axis=block_axis,\n            )\n            result = self._constructor(new_data)\n            return result.__finalize__(self)\n\n    @doc(\n        klass=_shared_doc_kwargs[\"klass\"],\n        cond=\"True\",\n        cond_rev=\"False\",\n        name=\"where\",\n        name_other=\"mask\",\n    )\n    def where(\n        self,\n        cond,\n        other=np.nan,\n        inplace=False,\n        axis=None,\n        level=None,\n        errors=\"raise\",\n        try_cast=False,\n    ):\n        \"\"\"\n        Replace values where the condition is {cond_rev}.\n\n        Parameters\n        ----------\n        cond : bool {klass}, array-like, or callable\n            Where `cond` is {cond}, keep the original value. Where\n            {cond_rev}, replace with corresponding value from `other`.\n            If `cond` is callable, it is computed on the {klass} and\n            should return boolean {klass} or array. The callable must\n            not change input {klass} (though pandas doesn't check it).\n        other : scalar, {klass}, or callable\n            Entries where `cond` is {cond_rev} are replaced with\n            corresponding value from `other`.\n            If other is callable, it is computed on the {klass} and\n            should return scalar or {klass}. The callable must not\n            change input {klass} (though pandas doesn't check it).\n        inplace : bool, default False\n            Whether to perform the operation in place on the data.\n        axis : int, default None\n            Alignment axis if needed.\n        level : int, default None\n            Alignment level if needed.\n        errors : str, {{'raise', 'ignore'}}, default 'raise'\n            Note that currently this parameter won't affect\n            the results and will always coerce to a suitable dtype.\n\n            - 'raise' : allow exceptions to be raised.\n            - 'ignore' : suppress exceptions. On error return original object.\n\n        try_cast : bool, default False\n            Try to cast the result back to the input type (if possible).\n\n        Returns\n        -------\n        Same type as caller or None if ``inplace=True``.\n\n        See Also\n        --------\n        :func:`DataFrame.{name_other}` : Return an object of same shape as\n            self.\n\n        Notes\n        -----\n        The {name} method is an application of the if-then idiom. For each\n        element in the calling DataFrame, if ``cond`` is ``{cond}`` the\n        element is used; otherwise the corresponding element from the DataFrame\n        ``other`` is used.\n\n        The signature for :func:`DataFrame.where` differs from\n        :func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to\n        ``np.where(m, df1, df2)``.\n\n        For further details and examples see the ``{name}`` documentation in\n        :ref:`indexing <indexing.where_mask>`.\n\n        Examples\n        --------\n        >>> s = pd.Series(range(5))\n        >>> s.where(s > 0)\n        0    NaN\n        1    1.0\n        2    2.0\n        3    3.0\n        4    4.0\n        dtype: float64\n        >>> s.mask(s > 0)\n        0    0.0\n        1    NaN\n        2    NaN\n        3    NaN\n        4    NaN\n        dtype: float64\n\n        >>> s.where(s > 1, 10)\n        0    10\n        1    10\n        2    2\n        3    3\n        4    4\n        dtype: int64\n        >>> s.mask(s > 1, 10)\n        0     0\n        1     1\n        2    10\n        3    10\n        4    10\n        dtype: int64\n\n        >>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n        >>> df\n           A  B\n        0  0  1\n        1  2  3\n        2  4  5\n        3  6  7\n        4  8  9\n        >>> m = df % 3 == 0\n        >>> df.where(m, -df)\n           A  B\n        0  0 -1\n        1 -2  3\n        2 -4 -5\n        3  6 -7\n        4 -8  9\n        >>> df.where(m, -df) == np.where(m, df, -df)\n              A     B\n        0  True  True\n        1  True  True\n        2  True  True\n        3  True  True\n        4  True  True\n        >>> df.where(m, -df) == df.mask(~m, -df)\n              A     B\n        0  True  True\n        1  True  True\n        2  True  True\n        3  True  True\n        4  True  True\n        \"\"\"\n        other = com.apply_if_callable(other, self)\n        return self._where(\n            cond, other, inplace, axis, level, errors=errors, try_cast=try_cast\n        )\n\n    @doc(\n        where,\n        klass=_shared_doc_kwargs[\"klass\"],\n        cond=\"False\",\n        cond_rev=\"True\",\n        name=\"mask\",\n        name_other=\"where\",\n    )\n    def mask(\n        self,\n        cond,\n        other=np.nan,\n        inplace=False,\n        axis=None,\n        level=None,\n        errors=\"raise\",\n        try_cast=False,\n    ):\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        cond = com.apply_if_callable(cond, self)\n\n        # see gh-21891\n        if not hasattr(cond, \"__invert__\"):\n            cond = np.array(cond)\n\n        return self.where(\n            ~cond,\n            other=other,\n            inplace=inplace,\n            axis=axis,\n            level=level,\n            try_cast=try_cast,\n            errors=errors,\n        )\n\n    @doc(klass=_shared_doc_kwargs[\"klass\"])\n    def shift(\n        self: FrameOrSeries, periods=1, freq=None, axis=0, fill_value=None\n    ) -> FrameOrSeries:\n        \"\"\"\n        Shift index by desired number of periods with an optional time `freq`.\n\n        When `freq` is not passed, shift the index without realigning the data.\n        If `freq` is passed (in this case, the index must be date or datetime,\n        or it will raise a `NotImplementedError`), the index will be\n        increased using the periods and the `freq`. `freq` can be inferred\n        when specified as \"infer\" as long as either freq or inferred_freq\n        attribute is set in the index.\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to shift. Can be positive or negative.\n        freq : DateOffset, tseries.offsets, timedelta, or str, optional\n            Offset to use from the tseries module or time rule (e.g. 'EOM').\n            If `freq` is specified then the index values are shifted but the\n            data is not realigned. That is, use `freq` if you would like to\n            extend the index when shifting and preserve the original data.\n            If `freq` is specified as \"infer\" then it will be inferred from\n            the freq or inferred_freq attributes of the index. If neither of\n            those attributes exist, a ValueError is thrown.\n        axis : {{0 or 'index', 1 or 'columns', None}}, default None\n            Shift direction.\n        fill_value : object, optional\n            The scalar value to use for newly introduced missing values.\n            the default depends on the dtype of `self`.\n            For numeric data, ``np.nan`` is used.\n            For datetime, timedelta, or period data, etc. :attr:`NaT` is used.\n            For extension dtypes, ``self.dtype.na_value`` is used.\n\n            .. versionchanged:: 1.1.0\n\n        Returns\n        -------\n        {klass}\n            Copy of input object, shifted.\n\n        See Also\n        --------\n        Index.shift : Shift values of Index.\n        DatetimeIndex.shift : Shift values of DatetimeIndex.\n        PeriodIndex.shift : Shift values of PeriodIndex.\n        tshift : Shift the time index, using the index's frequency if\n            available.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({{\"Col1\": [10, 20, 15, 30, 45],\n        ...                    \"Col2\": [13, 23, 18, 33, 48],\n        ...                    \"Col3\": [17, 27, 22, 37, 52]}},\n        ...                   index=pd.date_range(\"2020-01-01\", \"2020-01-05\"))\n        >>> df\n                    Col1  Col2  Col3\n        2020-01-01    10    13    17\n        2020-01-02    20    23    27\n        2020-01-03    15    18    22\n        2020-01-04    30    33    37\n        2020-01-05    45    48    52\n\n        >>> df.shift(periods=3)\n                    Col1  Col2  Col3\n        2020-01-01   NaN   NaN   NaN\n        2020-01-02   NaN   NaN   NaN\n        2020-01-03   NaN   NaN   NaN\n        2020-01-04  10.0  13.0  17.0\n        2020-01-05  20.0  23.0  27.0\n\n        >>> df.shift(periods=1, axis=\"columns\")\n                    Col1  Col2  Col3\n        2020-01-01   NaN    10    13\n        2020-01-02   NaN    20    23\n        2020-01-03   NaN    15    18\n        2020-01-04   NaN    30    33\n        2020-01-05   NaN    45    48\n\n        >>> df.shift(periods=3, fill_value=0)\n                    Col1  Col2  Col3\n        2020-01-01     0     0     0\n        2020-01-02     0     0     0\n        2020-01-03     0     0     0\n        2020-01-04    10    13    17\n        2020-01-05    20    23    27\n\n        >>> df.shift(periods=3, freq=\"D\")\n                    Col1  Col2  Col3\n        2020-01-04    10    13    17\n        2020-01-05    20    23    27\n        2020-01-06    15    18    22\n        2020-01-07    30    33    37\n        2020-01-08    45    48    52\n\n        >>> df.shift(periods=3, freq=\"infer\")\n                    Col1  Col2  Col3\n        2020-01-04    10    13    17\n        2020-01-05    20    23    27\n        2020-01-06    15    18    22\n        2020-01-07    30    33    37\n        2020-01-08    45    48    52\n        \"\"\"\n        if periods == 0:\n            return self.copy()\n\n        if freq is None:\n            # when freq is None, data is shifted, index is not\n            block_axis = self._get_block_manager_axis(axis)\n            new_data = self._mgr.shift(\n                periods=periods, axis=block_axis, fill_value=fill_value\n            )\n            return self._constructor(new_data).__finalize__(self, method=\"shift\")\n\n        # when freq is given, index is shifted, data is not\n        index = self._get_axis(axis)\n\n        if freq == \"infer\":\n            freq = getattr(index, \"freq\", None)\n\n            if freq is None:\n                freq = getattr(index, \"inferred_freq\", None)\n\n            if freq is None:\n                msg = \"Freq was not set in the index hence cannot be inferred\"\n                raise ValueError(msg)\n\n        elif isinstance(freq, str):\n            freq = to_offset(freq)\n\n        if isinstance(index, PeriodIndex):\n            orig_freq = to_offset(index.freq)\n            if freq != orig_freq:\n                assert orig_freq is not None  # for mypy\n                raise ValueError(\n                    f\"Given freq {freq.rule_code} does not match \"\n                    f\"PeriodIndex freq {orig_freq.rule_code}\"\n                )\n            new_ax = index.shift(periods)\n        else:\n            new_ax = index.shift(periods, freq)\n\n        result = self.set_axis(new_ax, axis)\n        return result.__finalize__(self, method=\"shift\")\n\n    def slice_shift(self: FrameOrSeries, periods: int = 1, axis=0) -> FrameOrSeries:\n        \"\"\"\n        Equivalent to `shift` without copying data.\n\n        The shifted data will not include the dropped periods and the\n        shifted axis will be smaller than the original.\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to move, can be positive or negative.\n\n        Returns\n        -------\n        shifted : same type as caller\n\n        Notes\n        -----\n        While the `slice_shift` is faster than `shift`, you may pay for it\n        later during alignment.\n        \"\"\"\n        if periods == 0:\n            return self\n\n        if periods > 0:\n            vslicer = slice(None, -periods)\n            islicer = slice(periods, None)\n        else:\n            vslicer = slice(-periods, None)\n            islicer = slice(None, periods)\n\n        new_obj = self._slice(vslicer, axis=axis)\n        shifted_axis = self._get_axis(axis)[islicer]\n        new_obj.set_axis(shifted_axis, axis=axis, inplace=True)\n\n        return new_obj.__finalize__(self, method=\"slice_shift\")\n\n    def tshift(\n        self: FrameOrSeries, periods: int = 1, freq=None, axis: Axis = 0\n    ) -> FrameOrSeries:\n        \"\"\"\n        Shift the time index, using the index's frequency if available.\n\n        .. deprecated:: 1.1.0\n            Use `shift` instead.\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to move, can be positive or negative.\n        freq : DateOffset, timedelta, or str, default None\n            Increment to use from the tseries module\n            or time rule expressed as a string (e.g. 'EOM').\n        axis : {0 or ‘index’, 1 or ‘columns’, None}, default 0\n            Corresponds to the axis that contains the Index.\n\n        Returns\n        -------\n        shifted : Series/DataFrame\n\n        Notes\n        -----\n        If freq is not specified then tries to use the freq or inferred_freq\n        attributes of the index. If neither of those attributes exist, a\n        ValueError is thrown\n        \"\"\"\n        warnings.warn(\n            (\n                \"tshift is deprecated and will be removed in a future version. \"\n                \"Please use shift instead.\"\n            ),\n            FutureWarning,\n            stacklevel=2,\n        )\n\n        if freq is None:\n            freq = \"infer\"\n\n        return self.shift(periods, freq, axis)\n\n    def truncate(\n        self: FrameOrSeries, before=None, after=None, axis=None, copy: bool_t = True\n    ) -> FrameOrSeries:\n        \"\"\"\n        Truncate a Series or DataFrame before and after some index value.\n\n        This is a useful shorthand for boolean indexing based on index\n        values above or below certain thresholds.\n\n        Parameters\n        ----------\n        before : date, str, int\n            Truncate all rows before this index value.\n        after : date, str, int\n            Truncate all rows after this index value.\n        axis : {0 or 'index', 1 or 'columns'}, optional\n            Axis to truncate. Truncates the index (rows) by default.\n        copy : bool, default is True,\n            Return a copy of the truncated section.\n\n        Returns\n        -------\n        type of caller\n            The truncated Series or DataFrame.\n\n        See Also\n        --------\n        DataFrame.loc : Select a subset of a DataFrame by label.\n        DataFrame.iloc : Select a subset of a DataFrame by position.\n\n        Notes\n        -----\n        If the index being truncated contains only datetime values,\n        `before` and `after` may be specified as strings instead of\n        Timestamps.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c', 'd', 'e'],\n        ...                    'B': ['f', 'g', 'h', 'i', 'j'],\n        ...                    'C': ['k', 'l', 'm', 'n', 'o']},\n        ...                   index=[1, 2, 3, 4, 5])\n        >>> df\n           A  B  C\n        1  a  f  k\n        2  b  g  l\n        3  c  h  m\n        4  d  i  n\n        5  e  j  o\n\n        >>> df.truncate(before=2, after=4)\n           A  B  C\n        2  b  g  l\n        3  c  h  m\n        4  d  i  n\n\n        The columns of a DataFrame can be truncated.\n\n        >>> df.truncate(before=\"A\", after=\"B\", axis=\"columns\")\n           A  B\n        1  a  f\n        2  b  g\n        3  c  h\n        4  d  i\n        5  e  j\n\n        For Series, only rows can be truncated.\n\n        >>> df['A'].truncate(before=2, after=4)\n        2    b\n        3    c\n        4    d\n        Name: A, dtype: object\n\n        The index values in ``truncate`` can be datetimes or string\n        dates.\n\n        >>> dates = pd.date_range('2016-01-01', '2016-02-01', freq='s')\n        >>> df = pd.DataFrame(index=dates, data={'A': 1})\n        >>> df.tail()\n                             A\n        2016-01-31 23:59:56  1\n        2016-01-31 23:59:57  1\n        2016-01-31 23:59:58  1\n        2016-01-31 23:59:59  1\n        2016-02-01 00:00:00  1\n\n        >>> df.truncate(before=pd.Timestamp('2016-01-05'),\n        ...             after=pd.Timestamp('2016-01-10')).tail()\n                             A\n        2016-01-09 23:59:56  1\n        2016-01-09 23:59:57  1\n        2016-01-09 23:59:58  1\n        2016-01-09 23:59:59  1\n        2016-01-10 00:00:00  1\n\n        Because the index is a DatetimeIndex containing only dates, we can\n        specify `before` and `after` as strings. They will be coerced to\n        Timestamps before truncation.\n\n        >>> df.truncate('2016-01-05', '2016-01-10').tail()\n                             A\n        2016-01-09 23:59:56  1\n        2016-01-09 23:59:57  1\n        2016-01-09 23:59:58  1\n        2016-01-09 23:59:59  1\n        2016-01-10 00:00:00  1\n\n        Note that ``truncate`` assumes a 0 value for any unspecified time\n        component (midnight). This differs from partial string slicing, which\n        returns any partially matching dates.\n\n        >>> df.loc['2016-01-05':'2016-01-10', :].tail()\n                             A\n        2016-01-10 23:59:55  1\n        2016-01-10 23:59:56  1\n        2016-01-10 23:59:57  1\n        2016-01-10 23:59:58  1\n        2016-01-10 23:59:59  1\n        \"\"\"\n        if axis is None:\n            axis = self._stat_axis_number\n        axis = self._get_axis_number(axis)\n        ax = self._get_axis(axis)\n\n        # GH 17935\n        # Check that index is sorted\n        if not ax.is_monotonic_increasing and not ax.is_monotonic_decreasing:\n            raise ValueError(\"truncate requires a sorted index\")\n\n        # if we have a date index, convert to dates, otherwise\n        # treat like a slice\n        if ax._is_all_dates:\n            if is_object_dtype(ax.dtype):\n                warnings.warn(\n                    \"Treating object-dtype Index of date objects as DatetimeIndex \"\n                    \"is deprecated, will be removed in a future version.\",\n                    FutureWarning,\n                )\n            from pandas.core.tools.datetimes import to_datetime\n\n            before = to_datetime(before)\n            after = to_datetime(after)\n\n        if before is not None and after is not None:\n            if before > after:\n                raise ValueError(f\"Truncate: {after} must be after {before}\")\n\n        if len(ax) > 1 and ax.is_monotonic_decreasing:\n            before, after = after, before\n\n        slicer = [slice(None, None)] * self._AXIS_LEN\n        slicer[axis] = slice(before, after)\n        result = self.loc[tuple(slicer)]\n\n        if isinstance(ax, MultiIndex):\n            setattr(result, self._get_axis_name(axis), ax.truncate(before, after))\n\n        if copy:\n            result = result.copy()\n\n        return result\n\n    def tz_convert(\n        self: FrameOrSeries, tz, axis=0, level=None, copy: bool_t = True\n    ) -> FrameOrSeries:\n        \"\"\"\n        Convert tz-aware axis to target time zone.\n\n        Parameters\n        ----------\n        tz : str or tzinfo object\n        axis : the axis to convert\n        level : int, str, default None\n            If axis is a MultiIndex, convert a specific level. Otherwise\n            must be None.\n        copy : bool, default True\n            Also make a copy of the underlying data.\n\n        Returns\n        -------\n        {klass}\n            Object with time zone converted axis.\n\n        Raises\n        ------\n        TypeError\n            If the axis is tz-naive.\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        ax = self._get_axis(axis)\n\n        def _tz_convert(ax, tz):\n            if not hasattr(ax, \"tz_convert\"):\n                if len(ax) > 0:\n                    ax_name = self._get_axis_name(axis)\n                    raise TypeError(\n                        f\"{ax_name} is not a valid DatetimeIndex or PeriodIndex\"\n                    )\n                else:\n                    ax = DatetimeIndex([], tz=tz)\n            else:\n                ax = ax.tz_convert(tz)\n            return ax\n\n        # if a level is given it must be a MultiIndex level or\n        # equivalent to the axis name\n        if isinstance(ax, MultiIndex):\n            level = ax._get_level_number(level)\n            new_level = _tz_convert(ax.levels[level], tz)\n            ax = ax.set_levels(new_level, level=level)\n        else:\n            if level not in (None, 0, ax.name):\n                raise ValueError(f\"The level {level} is not valid\")\n            ax = _tz_convert(ax, tz)\n\n        result = self.copy(deep=copy)\n        result = result.set_axis(ax, axis=axis, inplace=False)\n        return result.__finalize__(self, method=\"tz_convert\")\n\n    def tz_localize(\n        self: FrameOrSeries,\n        tz,\n        axis=0,\n        level=None,\n        copy: bool_t = True,\n        ambiguous=\"raise\",\n        nonexistent: str = \"raise\",\n    ) -> FrameOrSeries:\n        \"\"\"\n        Localize tz-naive index of a Series or DataFrame to target time zone.\n\n        This operation localizes the Index. To localize the values in a\n        timezone-naive Series, use :meth:`Series.dt.tz_localize`.\n\n        Parameters\n        ----------\n        tz : str or tzinfo\n        axis : the axis to localize\n        level : int, str, default None\n            If axis ia a MultiIndex, localize a specific level. Otherwise\n            must be None.\n        copy : bool, default True\n            Also make a copy of the underlying data.\n        ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'\n            When clocks moved backward due to DST, ambiguous times may arise.\n            For example in Central European Time (UTC+01), when going from\n            03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at\n            00:30:00 UTC and at 01:30:00 UTC. In such a situation, the\n            `ambiguous` parameter dictates how ambiguous times should be\n            handled.\n\n            - 'infer' will attempt to infer fall dst-transition hours based on\n              order\n            - bool-ndarray where True signifies a DST time, False designates\n              a non-DST time (note that this flag is only applicable for\n              ambiguous times)\n            - 'NaT' will return NaT where there are ambiguous times\n            - 'raise' will raise an AmbiguousTimeError if there are ambiguous\n              times.\n        nonexistent : str, default 'raise'\n            A nonexistent time does not exist in a particular timezone\n            where clocks moved forward due to DST. Valid values are:\n\n            - 'shift_forward' will shift the nonexistent time forward to the\n              closest existing time\n            - 'shift_backward' will shift the nonexistent time backward to the\n              closest existing time\n            - 'NaT' will return NaT where there are nonexistent times\n            - timedelta objects will shift nonexistent times by the timedelta\n            - 'raise' will raise an NonExistentTimeError if there are\n              nonexistent times.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        Series or DataFrame\n            Same type as the input.\n\n        Raises\n        ------\n        TypeError\n            If the TimeSeries is tz-aware and tz is not None.\n\n        Examples\n        --------\n        Localize local times:\n\n        >>> s = pd.Series([1],\n        ...               index=pd.DatetimeIndex(['2018-09-15 01:30:00']))\n        >>> s.tz_localize('CET')\n        2018-09-15 01:30:00+02:00    1\n        dtype: int64\n\n        Be careful with DST changes. When there is sequential data, pandas\n        can infer the DST time:\n\n        >>> s = pd.Series(range(7),\n        ...               index=pd.DatetimeIndex(['2018-10-28 01:30:00',\n        ...                                       '2018-10-28 02:00:00',\n        ...                                       '2018-10-28 02:30:00',\n        ...                                       '2018-10-28 02:00:00',\n        ...                                       '2018-10-28 02:30:00',\n        ...                                       '2018-10-28 03:00:00',\n        ...                                       '2018-10-28 03:30:00']))\n        >>> s.tz_localize('CET', ambiguous='infer')\n        2018-10-28 01:30:00+02:00    0\n        2018-10-28 02:00:00+02:00    1\n        2018-10-28 02:30:00+02:00    2\n        2018-10-28 02:00:00+01:00    3\n        2018-10-28 02:30:00+01:00    4\n        2018-10-28 03:00:00+01:00    5\n        2018-10-28 03:30:00+01:00    6\n        dtype: int64\n\n        In some cases, inferring the DST is impossible. In such cases, you can\n        pass an ndarray to the ambiguous parameter to set the DST explicitly\n\n        >>> s = pd.Series(range(3),\n        ...               index=pd.DatetimeIndex(['2018-10-28 01:20:00',\n        ...                                       '2018-10-28 02:36:00',\n        ...                                       '2018-10-28 03:46:00']))\n        >>> s.tz_localize('CET', ambiguous=np.array([True, True, False]))\n        2018-10-28 01:20:00+02:00    0\n        2018-10-28 02:36:00+02:00    1\n        2018-10-28 03:46:00+01:00    2\n        dtype: int64\n\n        If the DST transition causes nonexistent times, you can shift these\n        dates forward or backward with a timedelta object or `'shift_forward'`\n        or `'shift_backward'`.\n\n        >>> s = pd.Series(range(2),\n        ...               index=pd.DatetimeIndex(['2015-03-29 02:30:00',\n        ...                                       '2015-03-29 03:30:00']))\n        >>> s.tz_localize('Europe/Warsaw', nonexistent='shift_forward')\n        2015-03-29 03:00:00+02:00    0\n        2015-03-29 03:30:00+02:00    1\n        dtype: int64\n        >>> s.tz_localize('Europe/Warsaw', nonexistent='shift_backward')\n        2015-03-29 01:59:59.999999999+01:00    0\n        2015-03-29 03:30:00+02:00              1\n        dtype: int64\n        >>> s.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H'))\n        2015-03-29 03:30:00+02:00    0\n        2015-03-29 03:30:00+02:00    1\n        dtype: int64\n        \"\"\"\n        nonexistent_options = (\"raise\", \"NaT\", \"shift_forward\", \"shift_backward\")\n        if nonexistent not in nonexistent_options and not isinstance(\n            nonexistent, timedelta\n        ):\n            raise ValueError(\n                \"The nonexistent argument must be one of 'raise', \"\n                \"'NaT', 'shift_forward', 'shift_backward' or \"\n                \"a timedelta object\"\n            )\n\n        axis = self._get_axis_number(axis)\n        ax = self._get_axis(axis)\n\n        def _tz_localize(ax, tz, ambiguous, nonexistent):\n            if not hasattr(ax, \"tz_localize\"):\n                if len(ax) > 0:\n                    ax_name = self._get_axis_name(axis)\n                    raise TypeError(\n                        f\"{ax_name} is not a valid DatetimeIndex or PeriodIndex\"\n                    )\n                else:\n                    ax = DatetimeIndex([], tz=tz)\n            else:\n                ax = ax.tz_localize(tz, ambiguous=ambiguous, nonexistent=nonexistent)\n            return ax\n\n        # if a level is given it must be a MultiIndex level or\n        # equivalent to the axis name\n        if isinstance(ax, MultiIndex):\n            level = ax._get_level_number(level)\n            new_level = _tz_localize(ax.levels[level], tz, ambiguous, nonexistent)\n            ax = ax.set_levels(new_level, level=level)\n        else:\n            if level not in (None, 0, ax.name):\n                raise ValueError(f\"The level {level} is not valid\")\n            ax = _tz_localize(ax, tz, ambiguous, nonexistent)\n\n        result = self.copy(deep=copy)\n        result = result.set_axis(ax, axis=axis, inplace=False)\n        return result.__finalize__(self, method=\"tz_localize\")\n\n    # ----------------------------------------------------------------------\n    # Numeric Methods\n\n    def abs(self: FrameOrSeries) -> FrameOrSeries:\n        \"\"\"\n        Return a Series/DataFrame with absolute numeric value of each element.\n\n        This function only applies to elements that are all numeric.\n\n        Returns\n        -------\n        abs\n            Series/DataFrame containing the absolute value of each element.\n\n        See Also\n        --------\n        numpy.absolute : Calculate the absolute value element-wise.\n\n        Notes\n        -----\n        For ``complex`` inputs, ``1.2 + 1j``, the absolute value is\n        :math:`\\\\sqrt{ a^2 + b^2 }`.\n\n        Examples\n        --------\n        Absolute numeric values in a Series.\n\n        >>> s = pd.Series([-1.10, 2, -3.33, 4])\n        >>> s.abs()\n        0    1.10\n        1    2.00\n        2    3.33\n        3    4.00\n        dtype: float64\n\n        Absolute numeric values in a Series with complex numbers.\n\n        >>> s = pd.Series([1.2 + 1j])\n        >>> s.abs()\n        0    1.56205\n        dtype: float64\n\n        Absolute numeric values in a Series with a Timedelta element.\n\n        >>> s = pd.Series([pd.Timedelta('1 days')])\n        >>> s.abs()\n        0   1 days\n        dtype: timedelta64[ns]\n\n        Select rows with data closest to certain value using argsort (from\n        `StackOverflow <https://stackoverflow.com/a/17758115>`__).\n\n        >>> df = pd.DataFrame({\n        ...     'a': [4, 5, 6, 7],\n        ...     'b': [10, 20, 30, 40],\n        ...     'c': [100, 50, -30, -50]\n        ... })\n        >>> df\n             a    b    c\n        0    4   10  100\n        1    5   20   50\n        2    6   30  -30\n        3    7   40  -50\n        >>> df.loc[(df.c - 43).abs().argsort()]\n             a    b    c\n        1    5   20   50\n        0    4   10  100\n        2    6   30  -30\n        3    7   40  -50\n        \"\"\"\n        return np.abs(self)\n\n    def describe(\n        self: FrameOrSeries,\n        percentiles=None,\n        include=None,\n        exclude=None,\n        datetime_is_numeric=False,\n    ) -> FrameOrSeries:\n        \"\"\"\n        Generate descriptive statistics.\n\n        Descriptive statistics include those that summarize the central\n        tendency, dispersion and shape of a\n        dataset's distribution, excluding ``NaN`` values.\n\n        Analyzes both numeric and object series, as well\n        as ``DataFrame`` column sets of mixed data types. The output\n        will vary depending on what is provided. Refer to the notes\n        below for more detail.\n\n        Parameters\n        ----------\n        percentiles : list-like of numbers, optional\n            The percentiles to include in the output. All should\n            fall between 0 and 1. The default is\n            ``[.25, .5, .75]``, which returns the 25th, 50th, and\n            75th percentiles.\n        include : 'all', list-like of dtypes or None (default), optional\n            A white list of data types to include in the result. Ignored\n            for ``Series``. Here are the options:\n\n            - 'all' : All columns of the input will be included in the output.\n            - A list-like of dtypes : Limits the results to the\n              provided data types.\n              To limit the result to numeric types submit\n              ``numpy.number``. To limit it instead to object columns submit\n              the ``numpy.object`` data type. Strings\n              can also be used in the style of\n              ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To\n              select pandas categorical columns, use ``'category'``\n            - None (default) : The result will include all numeric columns.\n        exclude : list-like of dtypes or None (default), optional,\n            A black list of data types to omit from the result. Ignored\n            for ``Series``. Here are the options:\n\n            - A list-like of dtypes : Excludes the provided data types\n              from the result. To exclude numeric types submit\n              ``numpy.number``. To exclude object columns submit the data\n              type ``numpy.object``. Strings can also be used in the style of\n              ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To\n              exclude pandas categorical columns, use ``'category'``\n            - None (default) : The result will exclude nothing.\n        datetime_is_numeric : bool, default False\n            Whether to treat datetime dtypes as numeric. This affects statistics\n            calculated for the column. For DataFrame input, this also\n            controls whether datetime columns are included by default.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series or DataFrame\n            Summary statistics of the Series or Dataframe provided.\n\n        See Also\n        --------\n        DataFrame.count: Count number of non-NA/null observations.\n        DataFrame.max: Maximum of the values in the object.\n        DataFrame.min: Minimum of the values in the object.\n        DataFrame.mean: Mean of the values.\n        DataFrame.std: Standard deviation of the observations.\n        DataFrame.select_dtypes: Subset of a DataFrame including/excluding\n            columns based on their dtype.\n\n        Notes\n        -----\n        For numeric data, the result's index will include ``count``,\n        ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and\n        upper percentiles. By default the lower percentile is ``25`` and the\n        upper percentile is ``75``. The ``50`` percentile is the\n        same as the median.\n\n        For object data (e.g. strings or timestamps), the result's index\n        will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``\n        is the most common value. The ``freq`` is the most common value's\n        frequency. Timestamps also include the ``first`` and ``last`` items.\n\n        If multiple object values have the highest count, then the\n        ``count`` and ``top`` results will be arbitrarily chosen from\n        among those with the highest count.\n\n        For mixed data types provided via a ``DataFrame``, the default is to\n        return only an analysis of numeric columns. If the dataframe consists\n        only of object and categorical data without any numeric columns, the\n        default is to return an analysis of both the object and categorical\n        columns. If ``include='all'`` is provided as an option, the result\n        will include a union of attributes of each type.\n\n        The `include` and `exclude` parameters can be used to limit\n        which columns in a ``DataFrame`` are analyzed for the output.\n        The parameters are ignored when analyzing a ``Series``.\n\n        Examples\n        --------\n        Describing a numeric ``Series``.\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.describe()\n        count    3.0\n        mean     2.0\n        std      1.0\n        min      1.0\n        25%      1.5\n        50%      2.0\n        75%      2.5\n        max      3.0\n        dtype: float64\n\n        Describing a categorical ``Series``.\n\n        >>> s = pd.Series(['a', 'a', 'b', 'c'])\n        >>> s.describe()\n        count     4\n        unique    3\n        top       a\n        freq      2\n        dtype: object\n\n        Describing a timestamp ``Series``.\n\n        >>> s = pd.Series([\n        ...   np.datetime64(\"2000-01-01\"),\n        ...   np.datetime64(\"2010-01-01\"),\n        ...   np.datetime64(\"2010-01-01\")\n        ... ])\n        >>> s.describe(datetime_is_numeric=True)\n        count                      3\n        mean     2006-09-01 08:00:00\n        min      2000-01-01 00:00:00\n        25%      2004-12-31 12:00:00\n        50%      2010-01-01 00:00:00\n        75%      2010-01-01 00:00:00\n        max      2010-01-01 00:00:00\n        dtype: object\n\n        Describing a ``DataFrame``. By default only numeric fields\n        are returned.\n\n        >>> df = pd.DataFrame({'categorical': pd.Categorical(['d','e','f']),\n        ...                    'numeric': [1, 2, 3],\n        ...                    'object': ['a', 'b', 'c']\n        ...                   })\n        >>> df.describe()\n               numeric\n        count      3.0\n        mean       2.0\n        std        1.0\n        min        1.0\n        25%        1.5\n        50%        2.0\n        75%        2.5\n        max        3.0\n\n        Describing all columns of a ``DataFrame`` regardless of data type.\n\n        >>> df.describe(include='all')  # doctest: +SKIP\n               categorical  numeric object\n        count            3      3.0      3\n        unique           3      NaN      3\n        top              f      NaN      a\n        freq             1      NaN      1\n        mean           NaN      2.0    NaN\n        std            NaN      1.0    NaN\n        min            NaN      1.0    NaN\n        25%            NaN      1.5    NaN\n        50%            NaN      2.0    NaN\n        75%            NaN      2.5    NaN\n        max            NaN      3.0    NaN\n\n        Describing a column from a ``DataFrame`` by accessing it as\n        an attribute.\n\n        >>> df.numeric.describe()\n        count    3.0\n        mean     2.0\n        std      1.0\n        min      1.0\n        25%      1.5\n        50%      2.0\n        75%      2.5\n        max      3.0\n        Name: numeric, dtype: float64\n\n        Including only numeric columns in a ``DataFrame`` description.\n\n        >>> df.describe(include=[np.number])\n               numeric\n        count      3.0\n        mean       2.0\n        std        1.0\n        min        1.0\n        25%        1.5\n        50%        2.0\n        75%        2.5\n        max        3.0\n\n        Including only string columns in a ``DataFrame`` description.\n\n        >>> df.describe(include=[object])  # doctest: +SKIP\n               object\n        count       3\n        unique      3\n        top         a\n        freq        1\n\n        Including only categorical columns from a ``DataFrame`` description.\n\n        >>> df.describe(include=['category'])\n               categorical\n        count            3\n        unique           3\n        top              f\n        freq             1\n\n        Excluding numeric columns from a ``DataFrame`` description.\n\n        >>> df.describe(exclude=[np.number])  # doctest: +SKIP\n               categorical object\n        count            3      3\n        unique           3      3\n        top              f      a\n        freq             1      1\n\n        Excluding object columns from a ``DataFrame`` description.\n\n        >>> df.describe(exclude=[object])  # doctest: +SKIP\n               categorical  numeric\n        count            3      3.0\n        unique           3      NaN\n        top              f      NaN\n        freq             1      NaN\n        mean           NaN      2.0\n        std            NaN      1.0\n        min            NaN      1.0\n        25%            NaN      1.5\n        50%            NaN      2.0\n        75%            NaN      2.5\n        max            NaN      3.0\n        \"\"\"\n        if self.ndim == 2 and self.columns.size == 0:\n            raise ValueError(\"Cannot describe a DataFrame without columns\")\n\n        if percentiles is not None:\n            # explicit conversion of `percentiles` to list\n            percentiles = list(percentiles)\n\n            # get them all to be in [0, 1]\n            validate_percentile(percentiles)\n\n            # median should always be included\n            if 0.5 not in percentiles:\n                percentiles.append(0.5)\n            percentiles = np.asarray(percentiles)\n        else:\n            percentiles = np.array([0.25, 0.5, 0.75])\n\n        # sort and check for duplicates\n        unique_pcts = np.unique(percentiles)\n        if len(unique_pcts) < len(percentiles):\n            raise ValueError(\"percentiles cannot contain duplicates\")\n        percentiles = unique_pcts\n\n        formatted_percentiles = format_percentiles(percentiles)\n\n        def describe_numeric_1d(series):\n            stat_index = (\n                [\"count\", \"mean\", \"std\", \"min\"] + formatted_percentiles + [\"max\"]\n            )\n            d = (\n                [series.count(), series.mean(), series.std(), series.min()]\n                + series.quantile(percentiles).tolist()\n                + [series.max()]\n            )\n            return pd.Series(d, index=stat_index, name=series.name)\n\n        def describe_categorical_1d(data):\n            names = [\"count\", \"unique\"]\n            objcounts = data.value_counts()\n            count_unique = len(objcounts[objcounts != 0])\n            result = [data.count(), count_unique]\n            dtype = None\n            if result[1] > 0:\n                top, freq = objcounts.index[0], objcounts.iloc[0]\n                if is_datetime64_any_dtype(data.dtype):\n                    if self.ndim == 1:\n                        stacklevel = 4\n                    else:\n                        stacklevel = 5\n                    warnings.warn(\n                        \"Treating datetime data as categorical rather than numeric in \"\n                        \"`.describe` is deprecated and will be removed in a future \"\n                        \"version of pandas. Specify `datetime_is_numeric=True` to \"\n                        \"silence this warning and adopt the future behavior now.\",\n                        FutureWarning,\n                        stacklevel=stacklevel,\n                    )\n                    tz = data.dt.tz\n                    asint = data.dropna().values.view(\"i8\")\n                    top = Timestamp(top)\n                    if top.tzinfo is not None and tz is not None:\n                        # Don't tz_localize(None) if key is already tz-aware\n                        top = top.tz_convert(tz)\n                    else:\n                        top = top.tz_localize(tz)\n                    names += [\"top\", \"freq\", \"first\", \"last\"]\n                    result += [\n                        top,\n                        freq,\n                        Timestamp(asint.min(), tz=tz),\n                        Timestamp(asint.max(), tz=tz),\n                    ]\n                else:\n                    names += [\"top\", \"freq\"]\n                    result += [top, freq]\n\n            # If the DataFrame is empty, set 'top' and 'freq' to None\n            # to maintain output shape consistency\n            else:\n                names += [\"top\", \"freq\"]\n                result += [np.nan, np.nan]\n                dtype = \"object\"\n\n            return pd.Series(result, index=names, name=data.name, dtype=dtype)\n\n        def describe_timestamp_1d(data):\n            # GH-30164\n            stat_index = [\"count\", \"mean\", \"min\"] + formatted_percentiles + [\"max\"]\n            d = (\n                [data.count(), data.mean(), data.min()]\n                + data.quantile(percentiles).tolist()\n                + [data.max()]\n            )\n            return pd.Series(d, index=stat_index, name=data.name)\n\n        def describe_1d(data):\n            if is_bool_dtype(data.dtype):\n                return describe_categorical_1d(data)\n            elif is_numeric_dtype(data):\n                return describe_numeric_1d(data)\n            elif is_datetime64_any_dtype(data.dtype) and datetime_is_numeric:\n                return describe_timestamp_1d(data)\n            elif is_timedelta64_dtype(data.dtype):\n                return describe_numeric_1d(data)\n            else:\n                return describe_categorical_1d(data)\n\n        if self.ndim == 1:\n            return describe_1d(self)\n        elif (include is None) and (exclude is None):\n            # when some numerics are found, keep only numerics\n            default_include = [np.number]\n            if datetime_is_numeric:\n                default_include.append(\"datetime\")\n            data = self.select_dtypes(include=default_include)\n            if len(data.columns) == 0:\n                data = self\n        elif include == \"all\":\n            if exclude is not None:\n                msg = \"exclude must be None when include is 'all'\"\n                raise ValueError(msg)\n            data = self\n        else:\n            data = self.select_dtypes(include=include, exclude=exclude)\n\n        ldesc = [describe_1d(s) for _, s in data.items()]\n        # set a convenient order for rows\n        names: List[Label] = []\n        ldesc_indexes = sorted((x.index for x in ldesc), key=len)\n        for idxnames in ldesc_indexes:\n            for name in idxnames:\n                if name not in names:\n                    names.append(name)\n\n        d = pd.concat([x.reindex(names, copy=False) for x in ldesc], axis=1, sort=False)\n        d.columns = data.columns.copy()\n        return d\n\n    def pct_change(\n        self: FrameOrSeries,\n        periods=1,\n        fill_method=\"pad\",\n        limit=None,\n        freq=None,\n        **kwargs,\n    ) -> FrameOrSeries:\n        \"\"\"\n        Percentage change between the current and a prior element.\n\n        Computes the percentage change from the immediately previous row by\n        default. This is useful in comparing the percentage of change in a time\n        series of elements.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for forming percent change.\n        fill_method : str, default 'pad'\n            How to handle NAs before computing percent changes.\n        limit : int, default None\n            The number of consecutive NAs to fill before stopping.\n        freq : DateOffset, timedelta, or str, optional\n            Increment to use from time series API (e.g. 'M' or BDay()).\n        **kwargs\n            Additional keyword arguments are passed into\n            `DataFrame.shift` or `Series.shift`.\n\n        Returns\n        -------\n        chg : Series or DataFrame\n            The same type as the calling object.\n\n        See Also\n        --------\n        Series.diff : Compute the difference of two elements in a Series.\n        DataFrame.diff : Compute the difference of two elements in a DataFrame.\n        Series.shift : Shift the index by some number of periods.\n        DataFrame.shift : Shift the index by some number of periods.\n\n        Examples\n        --------\n        **Series**\n\n        >>> s = pd.Series([90, 91, 85])\n        >>> s\n        0    90\n        1    91\n        2    85\n        dtype: int64\n\n        >>> s.pct_change()\n        0         NaN\n        1    0.011111\n        2   -0.065934\n        dtype: float64\n\n        >>> s.pct_change(periods=2)\n        0         NaN\n        1         NaN\n        2   -0.055556\n        dtype: float64\n\n        See the percentage change in a Series where filling NAs with last\n        valid observation forward to next valid.\n\n        >>> s = pd.Series([90, 91, None, 85])\n        >>> s\n        0    90.0\n        1    91.0\n        2     NaN\n        3    85.0\n        dtype: float64\n\n        >>> s.pct_change(fill_method='ffill')\n        0         NaN\n        1    0.011111\n        2    0.000000\n        3   -0.065934\n        dtype: float64\n\n        **DataFrame**\n\n        Percentage change in French franc, Deutsche Mark, and Italian lira from\n        1980-01-01 to 1980-03-01.\n\n        >>> df = pd.DataFrame(dict(\n        ...     FR=[4.0405, 4.0963, 4.3149],\n        ...     GR=[1.7246, 1.7482, 1.8519],\n        ...     IT=[804.74, 810.01, 860.13]),\n        ...     index=['1980-01-01', '1980-02-01', '1980-03-01'])\n        >>> df\n                        FR      GR      IT\n        1980-01-01  4.0405  1.7246  804.74\n        1980-02-01  4.0963  1.7482  810.01\n        1980-03-01  4.3149  1.8519  860.13\n\n        >>> df.pct_change()\n                          FR        GR        IT\n        1980-01-01       NaN       NaN       NaN\n        1980-02-01  0.013810  0.013684  0.006549\n        1980-03-01  0.053365  0.059318  0.061876\n\n        Percentage of change in GOOG and APPL stock volume. Shows computing\n        the percentage change between columns.\n\n        >>> df = pd.DataFrame(dict([\n        ...     ('2016', [1769950, 30586265]),\n        ...     ('2015', [1500923, 40912316]),\n        ...     ('2014', [1371819, 41403351])]),\n        ...     index=['GOOG', 'APPL'])\n        >>> df\n                  2016      2015      2014\n        GOOG   1769950   1500923   1371819\n        APPL  30586265  40912316  41403351\n\n        >>> df.pct_change(axis='columns')\n              2016      2015      2014\n        GOOG   NaN -0.151997 -0.086016\n        APPL   NaN  0.337604  0.012002\n        \"\"\"\n        axis = self._get_axis_number(kwargs.pop(\"axis\", self._stat_axis_name))\n        if fill_method is None:\n            data = self\n        else:\n            _data = self.fillna(method=fill_method, axis=axis, limit=limit)\n            assert _data is not None  # needed for mypy\n            data = _data\n\n        rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1\n        if freq is not None:\n            # Shift method is implemented differently when freq is not None\n            # We want to restore the original index\n            rs = rs.loc[~rs.index.duplicated()]\n            rs = rs.reindex_like(data)\n        return rs\n\n    def _agg_by_level(self, name, axis=0, level=0, skipna=True, **kwargs):\n        if axis is None:\n            raise ValueError(\"Must specify 'axis' when aggregating by level.\")\n        grouped = self.groupby(level=level, axis=axis, sort=False)\n        if hasattr(grouped, name) and skipna:\n            return getattr(grouped, name)(**kwargs)\n        axis = self._get_axis_number(axis)\n        method = getattr(type(self), name)\n        applyf = lambda x: method(x, axis=axis, skipna=skipna, **kwargs)\n        return grouped.aggregate(applyf)\n\n    def _logical_func(\n        self, name: str, func, axis=0, bool_only=None, skipna=True, level=None, **kwargs\n    ):\n        nv.validate_logical_func(tuple(), kwargs, fname=name)\n        if level is not None:\n            if bool_only is not None:\n                raise NotImplementedError(\n                    \"Option bool_only is not implemented with option level.\"\n                )\n            return self._agg_by_level(name, axis=axis, level=level, skipna=skipna)\n\n        if self.ndim > 1 and axis is None:\n            # Reduce along one dimension then the other, to simplify DataFrame._reduce\n            res = self._logical_func(\n                name, func, axis=0, bool_only=bool_only, skipna=skipna, **kwargs\n            )\n            return res._logical_func(name, func, skipna=skipna, **kwargs)\n\n        return self._reduce(\n            func,\n            name=name,\n            axis=axis,\n            skipna=skipna,\n            numeric_only=bool_only,\n            filter_type=\"bool\",\n        )\n\n    def any(self, axis=0, bool_only=None, skipna=True, level=None, **kwargs):\n        return self._logical_func(\n            \"any\", nanops.nanany, axis, bool_only, skipna, level, **kwargs\n        )\n\n    def all(self, axis=0, bool_only=None, skipna=True, level=None, **kwargs):\n        return self._logical_func(\n            \"all\", nanops.nanall, axis, bool_only, skipna, level, **kwargs\n        )\n\n    def _accum_func(self, name: str, func, axis=None, skipna=True, *args, **kwargs):\n        skipna = nv.validate_cum_func_with_skipna(skipna, args, kwargs, name)\n        if axis is None:\n            axis = self._stat_axis_number\n        else:\n            axis = self._get_axis_number(axis)\n\n        if axis == 1:\n            return self.T._accum_func(\n                name, func, axis=0, skipna=skipna, *args, **kwargs\n            ).T\n\n        def block_accum_func(blk_values):\n            values = blk_values.T if hasattr(blk_values, \"T\") else blk_values\n\n            result = nanops.na_accum_func(values, func, skipna=skipna)\n\n            result = result.T if hasattr(result, \"T\") else result\n            return result\n\n        result = self._mgr.apply(block_accum_func)\n\n        return self._constructor(result).__finalize__(self, method=name)\n\n    def cummax(self, axis=None, skipna=True, *args, **kwargs):\n        return self._accum_func(\n            \"cummax\", np.maximum.accumulate, axis, skipna, *args, **kwargs\n        )\n\n    def cummin(self, axis=None, skipna=True, *args, **kwargs):\n        return self._accum_func(\n            \"cummin\", np.minimum.accumulate, axis, skipna, *args, **kwargs\n        )\n\n    def cumsum(self, axis=None, skipna=True, *args, **kwargs):\n        return self._accum_func(\"cumsum\", np.cumsum, axis, skipna, *args, **kwargs)\n\n    def cumprod(self, axis=None, skipna=True, *args, **kwargs):\n        return self._accum_func(\"cumprod\", np.cumprod, axis, skipna, *args, **kwargs)\n\n    def _stat_function_ddof(\n        self,\n        name: str,\n        func,\n        axis=None,\n        skipna=None,\n        level=None,\n        ddof=1,\n        numeric_only=None,\n        **kwargs,\n    ):\n        nv.validate_stat_ddof_func(tuple(), kwargs, fname=name)\n        if skipna is None:\n            skipna = True\n        if axis is None:\n            axis = self._stat_axis_number\n        if level is not None:\n            return self._agg_by_level(\n                name, axis=axis, level=level, skipna=skipna, ddof=ddof\n            )\n        return self._reduce(\n            func, name, axis=axis, numeric_only=numeric_only, skipna=skipna, ddof=ddof\n        )\n\n    def sem(\n        self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs\n    ):\n        return self._stat_function_ddof(\n            \"sem\", nanops.nansem, axis, skipna, level, ddof, numeric_only, **kwargs\n        )\n\n    def var(\n        self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs\n    ):\n        return self._stat_function_ddof(\n            \"var\", nanops.nanvar, axis, skipna, level, ddof, numeric_only, **kwargs\n        )\n\n    def std(\n        self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs\n    ):\n        return self._stat_function_ddof(\n            \"std\", nanops.nanstd, axis, skipna, level, ddof, numeric_only, **kwargs\n        )\n\n    def _stat_function(\n        self,\n        name: str,\n        func,\n        axis=None,\n        skipna=None,\n        level=None,\n        numeric_only=None,\n        **kwargs,\n    ):\n        if name == \"median\":\n            nv.validate_median(tuple(), kwargs)\n        else:\n            nv.validate_stat_func(tuple(), kwargs, fname=name)\n        if skipna is None:\n            skipna = True\n        if axis is None:\n            axis = self._stat_axis_number\n        if level is not None:\n            return self._agg_by_level(name, axis=axis, level=level, skipna=skipna)\n        return self._reduce(\n            func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\n        )\n\n    def min(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\n        return self._stat_function(\n            \"min\", nanops.nanmin, axis, skipna, level, numeric_only, **kwargs\n        )\n\n    def max(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\n        return self._stat_function(\n            \"max\", nanops.nanmax, axis, skipna, level, numeric_only, **kwargs\n        )\n\n    def mean(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\n        return self._stat_function(\n            \"mean\", nanops.nanmean, axis, skipna, level, numeric_only, **kwargs\n        )\n\n    def median(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\n        return self._stat_function(\n            \"median\", nanops.nanmedian, axis, skipna, level, numeric_only, **kwargs\n        )\n\n    def skew(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\n        return self._stat_function(\n            \"skew\", nanops.nanskew, axis, skipna, level, numeric_only, **kwargs\n        )\n\n    def kurt(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\n        return self._stat_function(\n            \"kurt\", nanops.nankurt, axis, skipna, level, numeric_only, **kwargs\n        )\n\n    kurtosis = kurt\n\n    def _min_count_stat_function(\n        self,\n        name: str,\n        func,\n        axis=None,\n        skipna=None,\n        level=None,\n        numeric_only=None,\n        min_count=0,\n        **kwargs,\n    ):\n        if name == \"sum\":\n            nv.validate_sum(tuple(), kwargs)\n        elif name == \"prod\":\n            nv.validate_prod(tuple(), kwargs)\n        else:\n            nv.validate_stat_func(tuple(), kwargs, fname=name)\n        if skipna is None:\n            skipna = True\n        if axis is None:\n            axis = self._stat_axis_number\n        if level is not None:\n            return self._agg_by_level(\n                name, axis=axis, level=level, skipna=skipna, min_count=min_count\n            )\n        return self._reduce(\n            func,\n            name=name,\n            axis=axis,\n            skipna=skipna,\n            numeric_only=numeric_only,\n            min_count=min_count,\n        )\n\n    def sum(\n        self,\n        axis=None,\n        skipna=None,\n        level=None,\n        numeric_only=None,\n        min_count=0,\n        **kwargs,\n    ):\n        return self._min_count_stat_function(\n            \"sum\", nanops.nansum, axis, skipna, level, numeric_only, min_count, **kwargs\n        )\n\n    def prod(\n        self,\n        axis=None,\n        skipna=None,\n        level=None,\n        numeric_only=None,\n        min_count=0,\n        **kwargs,\n    ):\n        return self._min_count_stat_function(\n            \"prod\",\n            nanops.nanprod,\n            axis,\n            skipna,\n            level,\n            numeric_only,\n            min_count,\n            **kwargs,\n        )\n\n    product = prod\n\n    def mad(self, axis=None, skipna=None, level=None):\n        \"\"\"\n        {desc}\n\n        Parameters\n        ----------\n        axis : {axis_descr}\n            Axis for the function to be applied on.\n        skipna : bool, default None\n            Exclude NA/null values when computing the result.\n        level : int or level name, default None\n            If the axis is a MultiIndex (hierarchical), count along a\n            particular level, collapsing into a {name1}.\n\n        Returns\n        -------\n        {name1} or {name2} (if level specified)\\\n        {see_also}\\\n        {examples}\n        \"\"\"\n        if skipna is None:\n            skipna = True\n        if axis is None:\n            axis = self._stat_axis_number\n        if level is not None:\n            return self._agg_by_level(\"mad\", axis=axis, level=level, skipna=skipna)\n\n        data = self._get_numeric_data()\n        if axis == 0:\n            demeaned = data - data.mean(axis=0)\n        else:\n            demeaned = data.sub(data.mean(axis=1), axis=0)\n        return np.abs(demeaned).mean(axis=axis, skipna=skipna)\n\n    @classmethod\n    def _add_numeric_operations(cls):\n        \"\"\"\n        Add the operations to the cls; evaluate the doc strings again\n        \"\"\"\n        axis_descr, name1, name2 = _doc_parms(cls)\n\n        @doc(\n            _bool_doc,\n            desc=_any_desc,\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            see_also=_any_see_also,\n            examples=_any_examples,\n            empty_value=False,\n        )\n        def any(self, axis=0, bool_only=None, skipna=True, level=None, **kwargs):\n            return NDFrame.any(self, axis, bool_only, skipna, level, **kwargs)\n\n        cls.any = any\n\n        @doc(\n            _bool_doc,\n            desc=_all_desc,\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            see_also=_all_see_also,\n            examples=_all_examples,\n            empty_value=True,\n        )\n        def all(self, axis=0, bool_only=None, skipna=True, level=None, **kwargs):\n            return NDFrame.all(self, axis, bool_only, skipna, level, **kwargs)\n\n        cls.all = all\n\n        @doc(\n            desc=\"Return the mean absolute deviation of the values \"\n            \"over the requested axis.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            see_also=\"\",\n            examples=\"\",\n        )\n        @Appender(NDFrame.mad.__doc__)\n        def mad(self, axis=None, skipna=None, level=None):\n            return NDFrame.mad(self, axis, skipna, level)\n\n        cls.mad = mad\n\n        @doc(\n            _num_ddof_doc,\n            desc=\"Return unbiased standard error of the mean over requested \"\n            \"axis.\\n\\nNormalized by N-1 by default. This can be changed \"\n            \"using the ddof argument\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n        )\n        def sem(\n            self,\n            axis=None,\n            skipna=None,\n            level=None,\n            ddof=1,\n            numeric_only=None,\n            **kwargs,\n        ):\n            return NDFrame.sem(self, axis, skipna, level, ddof, numeric_only, **kwargs)\n\n        cls.sem = sem\n\n        @doc(\n            _num_ddof_doc,\n            desc=\"Return unbiased variance over requested axis.\\n\\nNormalized by \"\n            \"N-1 by default. This can be changed using the ddof argument\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n        )\n        def var(\n            self,\n            axis=None,\n            skipna=None,\n            level=None,\n            ddof=1,\n            numeric_only=None,\n            **kwargs,\n        ):\n            return NDFrame.var(self, axis, skipna, level, ddof, numeric_only, **kwargs)\n\n        cls.var = var\n\n        @doc(\n            _num_ddof_doc,\n            desc=\"Return sample standard deviation over requested axis.\"\n            \"\\n\\nNormalized by N-1 by default. This can be changed using the \"\n            \"ddof argument\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n        )\n        def std(\n            self,\n            axis=None,\n            skipna=None,\n            level=None,\n            ddof=1,\n            numeric_only=None,\n            **kwargs,\n        ):\n            return NDFrame.std(self, axis, skipna, level, ddof, numeric_only, **kwargs)\n\n        cls.std = std\n\n        @doc(\n            _cnum_doc,\n            desc=\"minimum\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            accum_func_name=\"min\",\n            examples=_cummin_examples,\n        )\n        def cummin(self, axis=None, skipna=True, *args, **kwargs):\n            return NDFrame.cummin(self, axis, skipna, *args, **kwargs)\n\n        cls.cummin = cummin\n\n        @doc(\n            _cnum_doc,\n            desc=\"maximum\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            accum_func_name=\"max\",\n            examples=_cummax_examples,\n        )\n        def cummax(self, axis=None, skipna=True, *args, **kwargs):\n            return NDFrame.cummax(self, axis, skipna, *args, **kwargs)\n\n        cls.cummax = cummax\n\n        @doc(\n            _cnum_doc,\n            desc=\"sum\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            accum_func_name=\"sum\",\n            examples=_cumsum_examples,\n        )\n        def cumsum(self, axis=None, skipna=True, *args, **kwargs):\n            return NDFrame.cumsum(self, axis, skipna, *args, **kwargs)\n\n        cls.cumsum = cumsum\n\n        @doc(\n            _cnum_doc,\n            desc=\"product\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            accum_func_name=\"prod\",\n            examples=_cumprod_examples,\n        )\n        def cumprod(self, axis=None, skipna=True, *args, **kwargs):\n            return NDFrame.cumprod(self, axis, skipna, *args, **kwargs)\n\n        cls.cumprod = cumprod\n\n        @doc(\n            _num_doc,\n            desc=\"Return the sum of the values over the requested axis.\\n\\n\"\n            \"This is equivalent to the method ``numpy.sum``.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=_min_count_stub,\n            see_also=_stat_func_see_also,\n            examples=_sum_examples,\n        )\n        def sum(\n            self,\n            axis=None,\n            skipna=None,\n            level=None,\n            numeric_only=None,\n            min_count=0,\n            **kwargs,\n        ):\n            return NDFrame.sum(\n                self, axis, skipna, level, numeric_only, min_count, **kwargs\n            )\n\n        cls.sum = sum\n\n        @doc(\n            _num_doc,\n            desc=\"Return the product of the values over the requested axis.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=_min_count_stub,\n            see_also=_stat_func_see_also,\n            examples=_prod_examples,\n        )\n        def prod(\n            self,\n            axis=None,\n            skipna=None,\n            level=None,\n            numeric_only=None,\n            min_count=0,\n            **kwargs,\n        ):\n            return NDFrame.prod(\n                self, axis, skipna, level, numeric_only, min_count, **kwargs\n            )\n\n        cls.prod = prod\n        cls.product = prod\n\n        @doc(\n            _num_doc,\n            desc=\"Return the mean of the values over the requested axis.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=\"\",\n            see_also=\"\",\n            examples=\"\",\n        )\n        def mean(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\n            return NDFrame.mean(self, axis, skipna, level, numeric_only, **kwargs)\n\n        cls.mean = mean\n\n        @doc(\n            _num_doc,\n            desc=\"Return unbiased skew over requested axis.\\n\\nNormalized by N-1.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=\"\",\n            see_also=\"\",\n            examples=\"\",\n        )\n        def skew(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\n            return NDFrame.skew(self, axis, skipna, level, numeric_only, **kwargs)\n\n        cls.skew = skew\n\n        @doc(\n            _num_doc,\n            desc=\"Return unbiased kurtosis over requested axis.\\n\\n\"\n            \"Kurtosis obtained using Fisher's definition of\\n\"\n            \"kurtosis (kurtosis of normal == 0.0). Normalized \"\n            \"by N-1.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=\"\",\n            see_also=\"\",\n            examples=\"\",\n        )\n        def kurt(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\n            return NDFrame.kurt(self, axis, skipna, level, numeric_only, **kwargs)\n\n        cls.kurt = kurt\n        cls.kurtosis = kurt\n\n        @doc(\n            _num_doc,\n            desc=\"Return the median of the values over the requested axis.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=\"\",\n            see_also=\"\",\n            examples=\"\",\n        )\n        def median(\n            self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs\n        ):\n            return NDFrame.median(self, axis, skipna, level, numeric_only, **kwargs)\n\n        cls.median = median\n\n        @doc(\n            _num_doc,\n            desc=\"Return the maximum of the values over the requested axis.\\n\\n\"\n            \"If you want the *index* of the maximum, use ``idxmax``. This is\"\n            \"the equivalent of the ``numpy.ndarray`` method ``argmax``.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=\"\",\n            see_also=_stat_func_see_also,\n            examples=_max_examples,\n        )\n        def max(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\n            return NDFrame.max(self, axis, skipna, level, numeric_only, **kwargs)\n\n        cls.max = max\n\n        @doc(\n            _num_doc,\n            desc=\"Return the minimum of the values over the requested axis.\\n\\n\"\n            \"If you want the *index* of the minimum, use ``idxmin``. This is\"\n            \"the equivalent of the ``numpy.ndarray`` method ``argmin``.\",\n            name1=name1,\n            name2=name2,\n            axis_descr=axis_descr,\n            min_count=\"\",\n            see_also=_stat_func_see_also,\n            examples=_min_examples,\n        )\n        def min(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\n            return NDFrame.min(self, axis, skipna, level, numeric_only, **kwargs)\n\n        cls.min = min\n\n    @doc(Rolling)\n    def rolling(\n        self,\n        window: Union[int, timedelta, BaseOffset, BaseIndexer],\n        min_periods: Optional[int] = None,\n        center: bool_t = False,\n        win_type: Optional[str] = None,\n        on: Optional[str] = None,\n        axis: Axis = 0,\n        closed: Optional[str] = None,\n    ):\n        axis = self._get_axis_number(axis)\n\n        if win_type is not None:\n            return Window(\n                self,\n                window=window,\n                min_periods=min_periods,\n                center=center,\n                win_type=win_type,\n                on=on,\n                axis=axis,\n                closed=closed,\n            )\n\n        return Rolling(\n            self,\n            window=window,\n            min_periods=min_periods,\n            center=center,\n            win_type=win_type,\n            on=on,\n            axis=axis,\n            closed=closed,\n        )\n\n    @doc(Expanding)\n    def expanding(\n        self, min_periods: int = 1, center: Optional[bool_t] = None, axis: Axis = 0\n    ) -> Expanding:\n        axis = self._get_axis_number(axis)\n        if center is not None:\n            warnings.warn(\n                \"The `center` argument on `expanding` will be removed in the future\",\n                FutureWarning,\n                stacklevel=2,\n            )\n        else:\n            center = False\n\n        return Expanding(self, min_periods=min_periods, center=center, axis=axis)\n\n    @doc(ExponentialMovingWindow)\n    def ewm(\n        self,\n        com: Optional[float] = None,\n        span: Optional[float] = None,\n        halflife: Optional[Union[float, TimedeltaConvertibleTypes]] = None,\n        alpha: Optional[float] = None,\n        min_periods: int = 0,\n        adjust: bool_t = True,\n        ignore_na: bool_t = False,\n        axis: Axis = 0,\n        times: Optional[Union[str, np.ndarray, FrameOrSeries]] = None,\n    ) -> ExponentialMovingWindow:\n        axis = self._get_axis_number(axis)\n        return ExponentialMovingWindow(\n            self,\n            com=com,\n            span=span,\n            halflife=halflife,\n            alpha=alpha,\n            min_periods=min_periods,\n            adjust=adjust,\n            ignore_na=ignore_na,\n            axis=axis,\n            times=times,\n        )\n\n    # ----------------------------------------------------------------------\n    # Arithmetic Methods\n\n    def _inplace_method(self, other, op):\n        \"\"\"\n        Wrap arithmetic method to operate inplace.\n        \"\"\"\n        result = op(self, other)\n\n        # Delete cacher\n        self._reset_cacher()\n\n        # this makes sure that we are aligned like the input\n        # we are updating inplace so we want to ignore is_copy\n        self._update_inplace(\n            result.reindex_like(self, copy=False), verify_is_copy=False\n        )\n        return self\n\n    def __iadd__(self, other):\n        return self._inplace_method(other, type(self).__add__)\n\n    def __isub__(self, other):\n        return self._inplace_method(other, type(self).__sub__)\n\n    def __imul__(self, other):\n        return self._inplace_method(other, type(self).__mul__)\n\n    def __itruediv__(self, other):\n        return self._inplace_method(other, type(self).__truediv__)\n\n    def __ifloordiv__(self, other):\n        return self._inplace_method(other, type(self).__floordiv__)\n\n    def __imod__(self, other):\n        return self._inplace_method(other, type(self).__mod__)\n\n    def __ipow__(self, other):\n        return self._inplace_method(other, type(self).__pow__)\n\n    def __iand__(self, other):\n        return self._inplace_method(other, type(self).__and__)\n\n    def __ior__(self, other):\n        return self._inplace_method(other, type(self).__or__)\n\n    def __ixor__(self, other):\n        return self._inplace_method(other, type(self).__xor__)\n\n    # ----------------------------------------------------------------------\n    # Misc methods\n\n    def _find_valid_index(self, how: str):\n        \"\"\"\n        Retrieves the index of the first valid value.\n\n        Parameters\n        ----------\n        how : {'first', 'last'}\n            Use this parameter to change between the first or last valid index.\n\n        Returns\n        -------\n        idx_first_valid : type of index\n        \"\"\"\n        idxpos = find_valid_index(self._values, how)\n        if idxpos is None:\n            return None\n        return self.index[idxpos]\n\n    @doc(position=\"first\", klass=_shared_doc_kwargs[\"klass\"])\n    def first_valid_index(self):\n        \"\"\"\n        Return index for {position} non-NA/null value.\n\n        Returns\n        -------\n        scalar : type of index\n\n        Notes\n        -----\n        If all elements are non-NA/null, returns None.\n        Also returns None for empty {klass}.\n        \"\"\"\n        return self._find_valid_index(\"first\")\n\n    @doc(first_valid_index, position=\"last\", klass=_shared_doc_kwargs[\"klass\"])\n    def last_valid_index(self):\n        return self._find_valid_index(\"last\")\n\n\ndef _doc_parms(cls):\n    \"\"\"Return a tuple of the doc parms.\"\"\"\n    axis_descr = (\n        f\"{{{', '.join(f'{a} ({i})' for i, a in enumerate(cls._AXIS_ORDERS))}}}\"\n    )\n    name = cls._constructor_sliced.__name__ if cls._AXIS_LEN > 1 else \"scalar\"\n    name2 = cls.__name__\n    return axis_descr, name, name2\n\n\n_num_doc = \"\"\"\n{desc}\n\nParameters\n----------\naxis : {axis_descr}\n    Axis for the function to be applied on.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a {name1}.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n{min_count}\\\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\n{name1} or {name2} (if level specified)\\\n{see_also}\\\n{examples}\n\"\"\"\n\n_num_ddof_doc = \"\"\"\n{desc}\n\nParameters\n----------\naxis : {axis_descr}\nskipna : bool, default True\n    Exclude NA/null values. If an entire row/column is NA, the result\n    will be NA.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a {name1}.\nddof : int, default 1\n    Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n    where N represents the number of elements.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n\nReturns\n-------\n{name1} or {name2} (if level specified)\n\nNotes\n-----\nTo have the same behaviour as `numpy.std`, use `ddof=0` (instead of the\ndefault `ddof=1`)\\n\"\"\"\n\n_bool_doc = \"\"\"\n{desc}\n\nParameters\n----------\naxis : {{0 or 'index', 1 or 'columns', None}}, default 0\n    Indicate which axis or axes should be reduced.\n\n    * 0 / 'index' : reduce the index, return a Series whose index is the\n      original column labels.\n    * 1 / 'columns' : reduce the columns, return a Series whose index is the\n      original index.\n    * None : reduce all axes, return a scalar.\n\nbool_only : bool, default None\n    Include only boolean columns. If None, will attempt to use everything,\n    then use only boolean data. Not implemented for Series.\nskipna : bool, default True\n    Exclude NA/null values. If the entire row/column is NA and skipna is\n    True, then the result will be {empty_value}, as for an empty row/column.\n    If skipna is False, then NA are treated as True, because these are not\n    equal to zero.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a {name1}.\n**kwargs : any, default None\n    Additional keywords have no effect but might be accepted for\n    compatibility with NumPy.\n\nReturns\n-------\n{name1} or {name2}\n    If level is specified, then, {name2} is returned; otherwise, {name1}\n    is returned.\n\n{see_also}\n{examples}\"\"\"\n\n_all_desc = \"\"\"\\\nReturn whether all elements are True, potentially over an axis.\n\nReturns True unless there at least one element within a series or\nalong a Dataframe axis that is False or equivalent (e.g. zero or\nempty).\"\"\"\n\n_all_examples = \"\"\"\\\nExamples\n--------\n**Series**\n\n>>> pd.Series([True, True]).all()\nTrue\n>>> pd.Series([True, False]).all()\nFalse\n>>> pd.Series([]).all()\nTrue\n>>> pd.Series([np.nan]).all()\nTrue\n>>> pd.Series([np.nan]).all(skipna=False)\nTrue\n\n**DataFrames**\n\nCreate a dataframe from a dictionary.\n\n>>> df = pd.DataFrame({'col1': [True, True], 'col2': [True, False]})\n>>> df\n   col1   col2\n0  True   True\n1  True  False\n\nDefault behaviour checks if column-wise values all return True.\n\n>>> df.all()\ncol1     True\ncol2    False\ndtype: bool\n\nSpecify ``axis='columns'`` to check if row-wise values all return True.\n\n>>> df.all(axis='columns')\n0     True\n1    False\ndtype: bool\n\nOr ``axis=None`` for whether every value is True.\n\n>>> df.all(axis=None)\nFalse\n\"\"\"\n\n_all_see_also = \"\"\"\\\nSee Also\n--------\nSeries.all : Return True if all elements are True.\nDataFrame.any : Return True if one (or more) elements are True.\n\"\"\"\n\n_cnum_doc = \"\"\"\nReturn cumulative {desc} over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative\n{desc}.\n\nParameters\n----------\naxis : {{0 or 'index', 1 or 'columns'}}, default 0\n    The index or the name of the axis. 0 is equivalent to None or 'index'.\nskipna : bool, default True\n    Exclude NA/null values. If an entire row/column is NA, the result\n    will be NA.\n*args, **kwargs\n    Additional keywords have no effect but might be accepted for\n    compatibility with NumPy.\n\nReturns\n-------\n{name1} or {name2}\n    Return cumulative {desc} of {name1} or {name2}.\n\nSee Also\n--------\ncore.window.Expanding.{accum_func_name} : Similar functionality\n    but ignores ``NaN`` values.\n{name2}.{accum_func_name} : Return the {desc} over\n    {name2} axis.\n{name2}.cummax : Return cumulative maximum over {name2} axis.\n{name2}.cummin : Return cumulative minimum over {name2} axis.\n{name2}.cumsum : Return cumulative sum over {name2} axis.\n{name2}.cumprod : Return cumulative product over {name2} axis.\n\n{examples}\"\"\"\n\n_cummin_examples = \"\"\"\\\nExamples\n--------\n**Series**\n\n>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64\n\nBy default, NA values are ignored.\n\n>>> s.cummin()\n0    2.0\n1    NaN\n2    2.0\n3   -1.0\n4   -1.0\ndtype: float64\n\nTo include NA values in the operation, use ``skipna=False``\n\n>>> s.cummin(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64\n\n**DataFrame**\n\n>>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                    columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0\n\nBy default, iterates over rows and finds the minimum\nin each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n\n>>> df.cummin()\n     A    B\n0  2.0  1.0\n1  2.0  NaN\n2  1.0  0.0\n\nTo iterate over columns and find the minimum in each row,\nuse ``axis=1``\n\n>>> df.cummin(axis=1)\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0\n\"\"\"\n\n_cumsum_examples = \"\"\"\\\nExamples\n--------\n**Series**\n\n>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64\n\nBy default, NA values are ignored.\n\n>>> s.cumsum()\n0    2.0\n1    NaN\n2    7.0\n3    6.0\n4    6.0\ndtype: float64\n\nTo include NA values in the operation, use ``skipna=False``\n\n>>> s.cumsum(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64\n\n**DataFrame**\n\n>>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                    columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0\n\nBy default, iterates over rows and finds the sum\nin each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n\n>>> df.cumsum()\n     A    B\n0  2.0  1.0\n1  5.0  NaN\n2  6.0  1.0\n\nTo iterate over columns and find the sum in each row,\nuse ``axis=1``\n\n>>> df.cumsum(axis=1)\n     A    B\n0  2.0  3.0\n1  3.0  NaN\n2  1.0  1.0\n\"\"\"\n\n_cumprod_examples = \"\"\"\\\nExamples\n--------\n**Series**\n\n>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64\n\nBy default, NA values are ignored.\n\n>>> s.cumprod()\n0     2.0\n1     NaN\n2    10.0\n3   -10.0\n4    -0.0\ndtype: float64\n\nTo include NA values in the operation, use ``skipna=False``\n\n>>> s.cumprod(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64\n\n**DataFrame**\n\n>>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                    columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0\n\nBy default, iterates over rows and finds the product\nin each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n\n>>> df.cumprod()\n     A    B\n0  2.0  1.0\n1  6.0  NaN\n2  6.0  0.0\n\nTo iterate over columns and find the product in each row,\nuse ``axis=1``\n\n>>> df.cumprod(axis=1)\n     A    B\n0  2.0  2.0\n1  3.0  NaN\n2  1.0  0.0\n\"\"\"\n\n_cummax_examples = \"\"\"\\\nExamples\n--------\n**Series**\n\n>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64\n\nBy default, NA values are ignored.\n\n>>> s.cummax()\n0    2.0\n1    NaN\n2    5.0\n3    5.0\n4    5.0\ndtype: float64\n\nTo include NA values in the operation, use ``skipna=False``\n\n>>> s.cummax(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64\n\n**DataFrame**\n\n>>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                    columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0\n\nBy default, iterates over rows and finds the maximum\nin each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n\n>>> df.cummax()\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  3.0  1.0\n\nTo iterate over columns and find the maximum in each row,\nuse ``axis=1``\n\n>>> df.cummax(axis=1)\n     A    B\n0  2.0  2.0\n1  3.0  NaN\n2  1.0  1.0\n\"\"\"\n\n_any_see_also = \"\"\"\\\nSee Also\n--------\nnumpy.any : Numpy version of this method.\nSeries.any : Return whether any element is True.\nSeries.all : Return whether all elements are True.\nDataFrame.any : Return whether any element is True over requested axis.\nDataFrame.all : Return whether all elements are True over requested axis.\n\"\"\"\n\n_any_desc = \"\"\"\\\nReturn whether any element is True, potentially over an axis.\n\nReturns False unless there at least one element within a series or\nalong a Dataframe axis that is True or equivalent (e.g. non-zero or\nnon-empty).\"\"\"\n\n_any_examples = \"\"\"\\\nExamples\n--------\n**Series**\n\nFor Series input, the output is a scalar indicating whether any element\nis True.\n\n>>> pd.Series([False, False]).any()\nFalse\n>>> pd.Series([True, False]).any()\nTrue\n>>> pd.Series([]).any()\nFalse\n>>> pd.Series([np.nan]).any()\nFalse\n>>> pd.Series([np.nan]).any(skipna=False)\nTrue\n\n**DataFrame**\n\nWhether each column contains at least one True element (the default).\n\n>>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [0, 2], \"C\": [0, 0]})\n>>> df\n   A  B  C\n0  1  0  0\n1  2  2  0\n\n>>> df.any()\nA     True\nB     True\nC    False\ndtype: bool\n\nAggregating over the columns.\n\n>>> df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 2]})\n>>> df\n       A  B\n0   True  1\n1  False  2\n\n>>> df.any(axis='columns')\n0    True\n1    True\ndtype: bool\n\n>>> df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 0]})\n>>> df\n       A  B\n0   True  1\n1  False  0\n\n>>> df.any(axis='columns')\n0    True\n1    False\ndtype: bool\n\nAggregating over the entire DataFrame with ``axis=None``.\n\n>>> df.any(axis=None)\nTrue\n\n`any` for an empty DataFrame is an empty Series.\n\n>>> pd.DataFrame([]).any()\nSeries([], dtype: bool)\n\"\"\"\n\n_shared_docs[\n    \"stat_func_example\"\n] = \"\"\"\n\nExamples\n--------\n>>> idx = pd.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64\n\n>>> s.{stat_func}()\n{default_output}\n\n{verb} using level names, as well as indices.\n\n>>> s.{stat_func}(level='blooded')\nblooded\nwarm    {level_output_0}\ncold    {level_output_1}\nName: legs, dtype: int64\n\n>>> s.{stat_func}(level=0)\nblooded\nwarm    {level_output_0}\ncold    {level_output_1}\nName: legs, dtype: int64\"\"\"\n\n_sum_examples = _shared_docs[\"stat_func_example\"].format(\n    stat_func=\"sum\", verb=\"Sum\", default_output=14, level_output_0=6, level_output_1=8\n)\n\n_sum_examples += \"\"\"\n\nBy default, the sum of an empty or all-NA Series is ``0``.\n\n>>> pd.Series([]).sum()  # min_count=0 is the default\n0.0\n\nThis can be controlled with the ``min_count`` parameter. For example, if\nyou'd like the sum of an empty series to be NaN, pass ``min_count=1``.\n\n>>> pd.Series([]).sum(min_count=1)\nnan\n\nThanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\nempty series identically.\n\n>>> pd.Series([np.nan]).sum()\n0.0\n\n>>> pd.Series([np.nan]).sum(min_count=1)\nnan\"\"\"\n\n_max_examples = _shared_docs[\"stat_func_example\"].format(\n    stat_func=\"max\", verb=\"Max\", default_output=8, level_output_0=4, level_output_1=8\n)\n\n_min_examples = _shared_docs[\"stat_func_example\"].format(\n    stat_func=\"min\", verb=\"Min\", default_output=0, level_output_0=2, level_output_1=0\n)\n\n_stat_func_see_also = \"\"\"\n\nSee Also\n--------\nSeries.sum : Return the sum.\nSeries.min : Return the minimum.\nSeries.max : Return the maximum.\nSeries.idxmin : Return the index of the minimum.\nSeries.idxmax : Return the index of the maximum.\nDataFrame.sum : Return the sum over the requested axis.\nDataFrame.min : Return the minimum over the requested axis.\nDataFrame.max : Return the maximum over the requested axis.\nDataFrame.idxmin : Return the index of the minimum over the requested axis.\nDataFrame.idxmax : Return the index of the maximum over the requested axis.\"\"\"\n\n_prod_examples = \"\"\"\n\nExamples\n--------\nBy default, the product of an empty or all-NA Series is ``1``\n\n>>> pd.Series([]).prod()\n1.0\n\nThis can be controlled with the ``min_count`` parameter\n\n>>> pd.Series([]).prod(min_count=1)\nnan\n\nThanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\nempty series identically.\n\n>>> pd.Series([np.nan]).prod()\n1.0\n\n>>> pd.Series([np.nan]).prod(min_count=1)\nnan\"\"\"\n\n_min_count_stub = \"\"\"\\\nmin_count : int, default 0\n    The required number of valid values to perform the operation. If fewer than\n    ``min_count`` non-NA values are present the result will be NA.\n\"\"\"\n"
    },
    {
      "filename": "pandas/core/groupby/groupby.py",
      "content": "\"\"\"\nProvide the groupby split-apply-combine paradigm. Define the GroupBy\nclass providing the base-class of operations.\n\nThe SeriesGroupBy and DataFrameGroupBy sub-class\n(defined in pandas.core.groupby.generic)\nexpose these user-facing objects to provide specific functionality.\n\"\"\"\n\nfrom contextlib import contextmanager\nimport datetime\nfrom functools import partial, wraps\nimport inspect\nimport re\nimport types\nfrom typing import (\n    Callable,\n    Dict,\n    FrozenSet,\n    Generic,\n    Hashable,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\n\nimport numpy as np\n\nfrom pandas._config.config import option_context\n\nfrom pandas._libs import Timestamp, lib\nimport pandas._libs.groupby as libgroupby\nfrom pandas._typing import (\n    F,\n    FrameOrSeries,\n    FrameOrSeriesUnion,\n    IndexLabel,\n    Label,\n    Scalar,\n)\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import Appender, Substitution, cache_readonly, doc\n\nfrom pandas.core.dtypes.cast import maybe_cast_result\nfrom pandas.core.dtypes.common import (\n    ensure_float,\n    is_bool_dtype,\n    is_datetime64_dtype,\n    is_extension_array_dtype,\n    is_integer_dtype,\n    is_numeric_dtype,\n    is_object_dtype,\n    is_scalar,\n)\nfrom pandas.core.dtypes.missing import isna, notna\n\nfrom pandas.core import nanops\nimport pandas.core.algorithms as algorithms\nfrom pandas.core.arrays import Categorical, DatetimeArray\nfrom pandas.core.base import DataError, PandasObject, SelectionMixin\nimport pandas.core.common as com\nfrom pandas.core.frame import DataFrame\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.groupby import base, numba_, ops\nfrom pandas.core.indexes.api import CategoricalIndex, Index, MultiIndex\nfrom pandas.core.series import Series\nfrom pandas.core.sorting import get_group_index_sorter\nfrom pandas.core.util.numba_ import NUMBA_FUNC_CACHE\n\n_common_see_also = \"\"\"\n        See Also\n        --------\n        Series.%(name)s : Apply a function %(name)s to a Series.\n        DataFrame.%(name)s : Apply a function %(name)s\n            to each row or column of a DataFrame.\n\"\"\"\n\n_apply_docs = dict(\n    template=\"\"\"\n    Apply function `func` group-wise and combine the results together.\n\n    The function passed to `apply` must take a {input} as its first\n    argument and return a DataFrame, Series or scalar. `apply` will\n    then take care of combining the results back together into a single\n    dataframe or series. `apply` is therefore a highly flexible\n    grouping method.\n\n    While `apply` is a very flexible method, its downside is that\n    using it can be quite a bit slower than using more specific methods\n    like `agg` or `transform`. Pandas offers a wide range of method that will\n    be much faster than using `apply` for their specific purposes, so try to\n    use them before reaching for `apply`.\n\n    Parameters\n    ----------\n    func : callable\n        A callable that takes a {input} as its first argument, and\n        returns a dataframe, a series or a scalar. In addition the\n        callable may take positional and keyword arguments.\n    args, kwargs : tuple and dict\n        Optional positional and keyword arguments to pass to `func`.\n\n    Returns\n    -------\n    applied : Series or DataFrame\n\n    See Also\n    --------\n    pipe : Apply function to the full GroupBy object instead of to each\n        group.\n    aggregate : Apply aggregate function to the GroupBy object.\n    transform : Apply function column-by-column to the GroupBy object.\n    Series.apply : Apply a function to a Series.\n    DataFrame.apply : Apply a function to each row or column of a DataFrame.\n    \"\"\",\n    dataframe_examples=\"\"\"\n    >>> df = pd.DataFrame({'A': 'a a b'.split(),\n                           'B': [1,2,3],\n                           'C': [4,6, 5]})\n    >>> g = df.groupby('A')\n\n    Notice that ``g`` has two groups, ``a`` and ``b``.\n    Calling `apply` in various ways, we can get different grouping results:\n\n    Example 1: below the function passed to `apply` takes a DataFrame as\n    its argument and returns a DataFrame. `apply` combines the result for\n    each group together into a new DataFrame:\n\n    >>> g[['B', 'C']].apply(lambda x: x / x.sum())\n              B    C\n    0  0.333333  0.4\n    1  0.666667  0.6\n    2  1.000000  1.0\n\n    Example 2: The function passed to `apply` takes a DataFrame as\n    its argument and returns a Series.  `apply` combines the result for\n    each group together into a new DataFrame:\n\n    >>> g[['B', 'C']].apply(lambda x: x.max() - x.min())\n       B  C\n    A\n    a  1  2\n    b  0  0\n\n    Example 3: The function passed to `apply` takes a DataFrame as\n    its argument and returns a scalar. `apply` combines the result for\n    each group together into a Series, including setting the index as\n    appropriate:\n\n    >>> g.apply(lambda x: x.C.max() - x.B.min())\n    A\n    a    5\n    b    2\n    dtype: int64\n    \"\"\",\n    series_examples=\"\"\"\n    >>> s = pd.Series([0, 1, 2], index='a a b'.split())\n    >>> g = s.groupby(s.index)\n\n    From ``s`` above we can see that ``g`` has two groups, ``a`` and ``b``.\n    Calling `apply` in various ways, we can get different grouping results:\n\n    Example 1: The function passed to `apply` takes a Series as\n    its argument and returns a Series.  `apply` combines the result for\n    each group together into a new Series:\n\n    >>> g.apply(lambda x:  x*2 if x.name == 'b' else x/2)\n    0    0.0\n    1    0.5\n    2    4.0\n    dtype: float64\n\n    Example 2: The function passed to `apply` takes a Series as\n    its argument and returns a scalar. `apply` combines the result for\n    each group together into a Series, including setting the index as\n    appropriate:\n\n    >>> g.apply(lambda x: x.max() - x.min())\n    a    1\n    b    0\n    dtype: int64\n\n    Notes\n    -----\n    In the current implementation `apply` calls `func` twice on the\n    first group to decide whether it can take a fast or slow code\n    path. This can lead to unexpected behavior if `func` has\n    side-effects, as they will take effect twice for the first\n    group.\n\n    Examples\n    --------\n    {examples}\n    \"\"\",\n)\n\n_groupby_agg_method_template = \"\"\"\nCompute {fname} of group values.\n\nParameters\n----------\nnumeric_only : bool, default {no}\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data.\nmin_count : int, default {mc}\n    The required number of valid values to perform the operation. If fewer\n    than ``min_count`` non-NA values are present the result will be NA.\n\nReturns\n-------\nSeries or DataFrame\n    Computed {fname} of values within each group.\n\"\"\"\n\n_pipe_template = \"\"\"\nApply a function `func` with arguments to this %(klass)s object and return\nthe function's result.\n\nUse `.pipe` when you want to improve readability by chaining together\nfunctions that expect Series, DataFrames, GroupBy or Resampler objects.\nInstead of writing\n\n>>> h(g(f(df.groupby('group')), arg1=a), arg2=b, arg3=c)  # doctest: +SKIP\n\nYou can write\n\n>>> (df.groupby('group')\n...    .pipe(f)\n...    .pipe(g, arg1=a)\n...    .pipe(h, arg2=b, arg3=c))  # doctest: +SKIP\n\nwhich is much more readable.\n\nParameters\n----------\nfunc : callable or tuple of (callable, str)\n    Function to apply to this %(klass)s object or, alternatively,\n    a `(callable, data_keyword)` tuple where `data_keyword` is a\n    string indicating the keyword of `callable` that expects the\n    %(klass)s object.\nargs : iterable, optional\n       Positional arguments passed into `func`.\nkwargs : dict, optional\n         A dictionary of keyword arguments passed into `func`.\n\nReturns\n-------\nobject : the return type of `func`.\n\nSee Also\n--------\nSeries.pipe : Apply a function with arguments to a series.\nDataFrame.pipe: Apply a function with arguments to a dataframe.\napply : Apply function to each group instead of to the\n    full %(klass)s object.\n\nNotes\n-----\nSee more `here\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#piping-function-calls>`_\n\nExamples\n--------\n%(examples)s\n\"\"\"\n\n_transform_template = \"\"\"\nCall function producing a like-indexed %(klass)s on each group and\nreturn a %(klass)s having the same indexes as the original object\nfilled with the transformed values\n\nParameters\n----------\nf : function\n    Function to apply to each group.\n\n    Can also accept a Numba JIT function with\n    ``engine='numba'`` specified.\n\n    If the ``'numba'`` engine is chosen, the function must be\n    a user defined function with ``values`` and ``index`` as the\n    first and second arguments respectively in the function signature.\n    Each group's index will be passed to the user defined function\n    and optionally available for use.\n\n    .. versionchanged:: 1.1.0\n*args\n    Positional arguments to pass to func.\nengine : str, default None\n    * ``'cython'`` : Runs the function through C-extensions from cython.\n    * ``'numba'`` : Runs the function through JIT compiled code from numba.\n    * ``None`` : Defaults to ``'cython'`` or globally setting ``compute.use_numba``\n\n    .. versionadded:: 1.1.0\nengine_kwargs : dict, default None\n    * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n    * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n      and ``parallel`` dictionary keys. The values must either be ``True`` or\n      ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n      ``{'nopython': True, 'nogil': False, 'parallel': False}`` and will be\n      applied to the function\n\n    .. versionadded:: 1.1.0\n**kwargs\n    Keyword arguments to be passed into func.\n\nReturns\n-------\n%(klass)s\n\nSee Also\n--------\n%(klass)s.groupby.apply : Apply function func group-wise\n    and combine the results together.\n%(klass)s.groupby.aggregate : Aggregate using one or more\n    operations over the specified axis.\n%(klass)s.transform : Transforms the Series on each group\n    based on the given function.\n\nNotes\n-----\nEach group is endowed the attribute 'name' in case you need to know\nwhich group you are working on.\n\nThe current implementation imposes three requirements on f:\n\n* f must return a value that either has the same shape as the input\n  subframe or can be broadcast to the shape of the input subframe.\n  For example, if `f` returns a scalar it will be broadcast to have the\n  same shape as the input subframe.\n* if this is a DataFrame, f must support application column-by-column\n  in the subframe. If f also supports application to the entire subframe,\n  then a fast path is used starting from the second chunk.\n* f must not mutate groups. Mutation is not supported and may\n  produce unexpected results.\n\nWhen using ``engine='numba'``, there will be no \"fall back\" behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\nExamples\n--------\n\n>>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n...                           'foo', 'bar'],\n...                    'B' : ['one', 'one', 'two', 'three',\n...                           'two', 'two'],\n...                    'C' : [1, 5, 5, 2, 5, 5],\n...                    'D' : [2.0, 5., 8., 1., 2., 9.]})\n>>> grouped = df.groupby('A')\n>>> grouped.transform(lambda x: (x - x.mean()) / x.std())\n          C         D\n0 -1.154701 -0.577350\n1  0.577350  0.000000\n2  0.577350  1.154701\n3 -1.154701 -1.000000\n4  0.577350 -0.577350\n5  0.577350  1.000000\n\nBroadcast result of the transformation\n\n>>> grouped.transform(lambda x: x.max() - x.min())\n   C    D\n0  4  6.0\n1  3  8.0\n2  4  6.0\n3  3  8.0\n4  4  6.0\n5  3  8.0\n\"\"\"\n\n_agg_template = \"\"\"\nAggregate using one or more operations over the specified axis.\n\nParameters\n----------\nfunc : function, str, list or dict\n    Function to use for aggregating the data. If a function, must either\n    work when passed a {klass} or when passed to {klass}.apply.\n\n    Accepted combinations are:\n\n    - function\n    - string function name\n    - list of functions and/or function names, e.g. ``[np.sum, 'mean']``\n    - dict of axis labels -> functions, function names or list of such.\n\n    Can also accept a Numba JIT function with\n    ``engine='numba'`` specified. Only passing a single function is supported\n    with this engine.\n\n    If the ``'numba'`` engine is chosen, the function must be\n    a user defined function with ``values`` and ``index`` as the\n    first and second arguments respectively in the function signature.\n    Each group's index will be passed to the user defined function\n    and optionally available for use.\n\n    .. versionchanged:: 1.1.0\n*args\n    Positional arguments to pass to func.\nengine : str, default None\n    * ``'cython'`` : Runs the function through C-extensions from cython.\n    * ``'numba'`` : Runs the function through JIT compiled code from numba.\n    * ``None`` : Defaults to ``'cython'`` or globally setting ``compute.use_numba``\n\n    .. versionadded:: 1.1.0\nengine_kwargs : dict, default None\n    * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n    * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n      and ``parallel`` dictionary keys. The values must either be ``True`` or\n      ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n      ``{{'nopython': True, 'nogil': False, 'parallel': False}}`` and will be\n      applied to the function\n\n    .. versionadded:: 1.1.0\n**kwargs\n    Keyword arguments to be passed into func.\n\nReturns\n-------\n{klass}\n\nSee Also\n--------\n{klass}.groupby.apply : Apply function func group-wise\n    and combine the results together.\n{klass}.groupby.transform : Aggregate using one or more\n    operations over the specified axis.\n{klass}.aggregate : Transforms the Series on each group\n    based on the given function.\n\nNotes\n-----\nWhen using ``engine='numba'``, there will be no \"fall back\" behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n{examples}\n\"\"\"\n\n\nclass GroupByPlot(PandasObject):\n    \"\"\"\n    Class implementing the .plot attribute for groupby objects.\n    \"\"\"\n\n    def __init__(self, groupby):\n        self._groupby = groupby\n\n    def __call__(self, *args, **kwargs):\n        def f(self):\n            return self.plot(*args, **kwargs)\n\n        f.__name__ = \"plot\"\n        return self._groupby.apply(f)\n\n    def __getattr__(self, name: str):\n        def attr(*args, **kwargs):\n            def f(self):\n                return getattr(self.plot, name)(*args, **kwargs)\n\n            return self._groupby.apply(f)\n\n        return attr\n\n\n@contextmanager\ndef group_selection_context(groupby: \"BaseGroupBy\") -> Iterator[\"BaseGroupBy\"]:\n    \"\"\"\n    Set / reset the group_selection_context.\n    \"\"\"\n    groupby._set_group_selection()\n    try:\n        yield groupby\n    finally:\n        groupby._reset_group_selection()\n\n\n_KeysArgType = Union[\n    Hashable,\n    List[Hashable],\n    Callable[[Hashable], Hashable],\n    List[Callable[[Hashable], Hashable]],\n    Mapping[Hashable, Hashable],\n]\n\n\nclass BaseGroupBy(PandasObject, SelectionMixin, Generic[FrameOrSeries]):\n    _group_selection: Optional[IndexLabel] = None\n    _apply_allowlist: FrozenSet[str] = frozenset()\n\n    def __init__(\n        self,\n        obj: FrameOrSeries,\n        keys: Optional[_KeysArgType] = None,\n        axis: int = 0,\n        level: Optional[IndexLabel] = None,\n        grouper: Optional[\"ops.BaseGrouper\"] = None,\n        exclusions: Optional[Set[Label]] = None,\n        selection: Optional[IndexLabel] = None,\n        as_index: bool = True,\n        sort: bool = True,\n        group_keys: bool = True,\n        squeeze: bool = False,\n        observed: bool = False,\n        mutated: bool = False,\n        dropna: bool = True,\n    ):\n\n        self._selection = selection\n\n        assert isinstance(obj, NDFrame), type(obj)\n\n        self.level = level\n\n        if not as_index:\n            if not isinstance(obj, DataFrame):\n                raise TypeError(\"as_index=False only valid with DataFrame\")\n            if axis != 0:\n                raise ValueError(\"as_index=False only valid for axis=0\")\n\n        self.as_index = as_index\n        self.keys = keys\n        self.sort = sort\n        self.group_keys = group_keys\n        self.squeeze = squeeze\n        self.observed = observed\n        self.mutated = mutated\n        self.dropna = dropna\n\n        if grouper is None:\n            from pandas.core.groupby.grouper import get_grouper\n\n            grouper, exclusions, obj = get_grouper(\n                obj,\n                keys,\n                axis=axis,\n                level=level,\n                sort=sort,\n                observed=observed,\n                mutated=self.mutated,\n                dropna=self.dropna,\n            )\n\n        self.obj = obj\n        self.axis = obj._get_axis_number(axis)\n        self.grouper = grouper\n        self.exclusions = exclusions or set()\n\n    def __len__(self) -> int:\n        return len(self.groups)\n\n    def __repr__(self) -> str:\n        # TODO: Better repr for GroupBy object\n        return object.__repr__(self)\n\n    def _assure_grouper(self) -> None:\n        \"\"\"\n        We create the grouper on instantiation sub-classes may have a\n        different policy.\n        \"\"\"\n        pass\n\n    @property\n    def groups(self) -> Dict[Hashable, np.ndarray]:\n        \"\"\"\n        Dict {group name -> group labels}.\n        \"\"\"\n        self._assure_grouper()\n        return self.grouper.groups\n\n    @property\n    def ngroups(self) -> int:\n        self._assure_grouper()\n        return self.grouper.ngroups\n\n    @property\n    def indices(self):\n        \"\"\"\n        Dict {group name -> group indices}.\n        \"\"\"\n        self._assure_grouper()\n        return self.grouper.indices\n\n    def _get_indices(self, names):\n        \"\"\"\n        Safe get multiple indices, translate keys for\n        datelike to underlying repr.\n        \"\"\"\n\n        def get_converter(s):\n            # possibly convert to the actual key types\n            # in the indices, could be a Timestamp or a np.datetime64\n            if isinstance(s, datetime.datetime):\n                return lambda key: Timestamp(key)\n            elif isinstance(s, np.datetime64):\n                return lambda key: Timestamp(key).asm8\n            else:\n                return lambda key: key\n\n        if len(names) == 0:\n            return []\n\n        if len(self.indices) > 0:\n            index_sample = next(iter(self.indices))\n        else:\n            index_sample = None  # Dummy sample\n\n        name_sample = names[0]\n        if isinstance(index_sample, tuple):\n            if not isinstance(name_sample, tuple):\n                msg = \"must supply a tuple to get_group with multiple grouping keys\"\n                raise ValueError(msg)\n            if not len(name_sample) == len(index_sample):\n                try:\n                    # If the original grouper was a tuple\n                    return [self.indices[name] for name in names]\n                except KeyError as err:\n                    # turns out it wasn't a tuple\n                    msg = (\n                        \"must supply a same-length tuple to get_group \"\n                        \"with multiple grouping keys\"\n                    )\n                    raise ValueError(msg) from err\n\n            converters = [get_converter(s) for s in index_sample]\n            names = (tuple(f(n) for f, n in zip(converters, name)) for name in names)\n\n        else:\n            converter = get_converter(index_sample)\n            names = (converter(name) for name in names)\n\n        return [self.indices.get(name, []) for name in names]\n\n    def _get_index(self, name):\n        \"\"\"\n        Safe get index, translate keys for datelike to underlying repr.\n        \"\"\"\n        return self._get_indices([name])[0]\n\n    @cache_readonly\n    def _selected_obj(self):\n        # Note: _selected_obj is always just `self.obj` for SeriesGroupBy\n\n        if self._selection is None or isinstance(self.obj, Series):\n            if self._group_selection is not None:\n                return self.obj[self._group_selection]\n            return self.obj\n        else:\n            return self.obj[self._selection]\n\n    def _reset_group_selection(self) -> None:\n        \"\"\"\n        Clear group based selection.\n\n        Used for methods needing to return info on each group regardless of\n        whether a group selection was previously set.\n        \"\"\"\n        if self._group_selection is not None:\n            # GH12839 clear cached selection too when changing group selection\n            self._group_selection = None\n            self._reset_cache(\"_selected_obj\")\n\n    def _set_group_selection(self) -> None:\n        \"\"\"\n        Create group based selection.\n\n        Used when selection is not passed directly but instead via a grouper.\n\n        NOTE: this should be paired with a call to _reset_group_selection\n        \"\"\"\n        grp = self.grouper\n        if not (\n            self.as_index\n            and getattr(grp, \"groupings\", None) is not None\n            and self.obj.ndim > 1\n            and self._group_selection is None\n        ):\n            return\n\n        groupers = [g.name for g in grp.groupings if g.level is None and g.in_axis]\n\n        if len(groupers):\n            # GH12839 clear selected obj cache when group selection changes\n            ax = self.obj._info_axis\n            self._group_selection = ax.difference(Index(groupers), sort=False).tolist()\n            self._reset_cache(\"_selected_obj\")\n\n    def _set_result_index_ordered(\n        self, result: \"OutputFrameOrSeries\"\n    ) -> \"OutputFrameOrSeries\":\n        # set the result index on the passed values object and\n        # return the new object, xref 8046\n\n        # the values/counts are repeated according to the group index\n        # shortcut if we have an already ordered grouper\n        if not self.grouper.is_monotonic:\n            index = Index(np.concatenate(self._get_indices(self.grouper.result_index)))\n            result.set_axis(index, axis=self.axis, inplace=True)\n            result = result.sort_index(axis=self.axis)\n\n        result.set_axis(self.obj._get_axis(self.axis), axis=self.axis, inplace=True)\n        return result\n\n    def _dir_additions(self) -> Set[str]:\n        return self.obj._dir_additions() | self._apply_allowlist\n\n    def __getattr__(self, attr: str):\n        if attr in self._internal_names_set:\n            return object.__getattribute__(self, attr)\n        if attr in self.obj:\n            return self[attr]\n\n        raise AttributeError(\n            f\"'{type(self).__name__}' object has no attribute '{attr}'\"\n        )\n\n    @Substitution(\n        klass=\"GroupBy\",\n        examples=\"\"\"\\\n>>> df = pd.DataFrame({'A': 'a b a b'.split(), 'B': [1, 2, 3, 4]})\n>>> df\n   A  B\n0  a  1\n1  b  2\n2  a  3\n3  b  4\n\nTo get the difference between each groups maximum and minimum value in one\npass, you can do\n\n>>> df.groupby('A').pipe(lambda x: x.max() - x.min())\n   B\nA\na  2\nb  2\"\"\",\n    )\n    @Appender(_pipe_template)\n    def pipe(self, func, *args, **kwargs):\n        return com.pipe(self, func, *args, **kwargs)\n\n    plot = property(GroupByPlot)\n\n    def _make_wrapper(self, name: str) -> Callable:\n        assert name in self._apply_allowlist\n\n        with group_selection_context(self):\n            # need to setup the selection\n            # as are not passed directly but in the grouper\n            f = getattr(self._obj_with_exclusions, name)\n            if not isinstance(f, types.MethodType):\n                return self.apply(lambda self: getattr(self, name))\n\n        f = getattr(type(self._obj_with_exclusions), name)\n        sig = inspect.signature(f)\n\n        def wrapper(*args, **kwargs):\n            # a little trickery for aggregation functions that need an axis\n            # argument\n            if \"axis\" in sig.parameters:\n                if kwargs.get(\"axis\", None) is None:\n                    kwargs[\"axis\"] = self.axis\n\n            def curried(x):\n                return f(x, *args, **kwargs)\n\n            # preserve the name so we can detect it when calling plot methods,\n            # to avoid duplicates\n            curried.__name__ = name\n\n            # special case otherwise extra plots are created when catching the\n            # exception below\n            if name in base.plotting_methods:\n                return self.apply(curried)\n\n            try:\n                return self._python_apply_general(curried, self._obj_with_exclusions)\n            except TypeError as err:\n                if not re.search(\n                    \"reduction operation '.*' not allowed for this dtype\", str(err)\n                ):\n                    # We don't have a cython implementation\n                    # TODO: is the above comment accurate?\n                    raise\n\n            if self.obj.ndim == 1:\n                # this can be called recursively, so need to raise ValueError\n                raise ValueError\n\n            # GH#3688 try to operate item-by-item\n            result = self._aggregate_item_by_item(name, *args, **kwargs)\n            return result\n\n        wrapper.__name__ = name\n        return wrapper\n\n    def get_group(self, name, obj=None):\n        \"\"\"\n        Construct DataFrame from group with provided name.\n\n        Parameters\n        ----------\n        name : object\n            The name of the group to get as a DataFrame.\n        obj : DataFrame, default None\n            The DataFrame to take the DataFrame out of.  If\n            it is None, the object groupby was called on will\n            be used.\n\n        Returns\n        -------\n        group : same type as obj\n        \"\"\"\n        if obj is None:\n            obj = self._selected_obj\n\n        inds = self._get_index(name)\n        if not len(inds):\n            raise KeyError(name)\n\n        return obj._take_with_is_copy(inds, axis=self.axis)\n\n    def __iter__(self) -> Iterator[Tuple[Label, FrameOrSeries]]:\n        \"\"\"\n        Groupby iterator.\n\n        Returns\n        -------\n        Generator yielding sequence of (name, subsetted object)\n        for each group\n        \"\"\"\n        return self.grouper.get_iterator(self.obj, axis=self.axis)\n\n    @Appender(\n        _apply_docs[\"template\"].format(\n            input=\"dataframe\", examples=_apply_docs[\"dataframe_examples\"]\n        )\n    )\n    def apply(self, func, *args, **kwargs):\n\n        func = self._is_builtin_func(func)\n\n        # this is needed so we don't try and wrap strings. If we could\n        # resolve functions to their callable functions prior, this\n        # wouldn't be needed\n        if args or kwargs:\n            if callable(func):\n\n                @wraps(func)\n                def f(g):\n                    with np.errstate(all=\"ignore\"):\n                        return func(g, *args, **kwargs)\n\n            elif hasattr(nanops, \"nan\" + func):\n                # TODO: should we wrap this in to e.g. _is_builtin_func?\n                f = getattr(nanops, \"nan\" + func)\n\n            else:\n                raise ValueError(\n                    \"func must be a callable if args or kwargs are supplied\"\n                )\n        else:\n            f = func\n\n        # ignore SettingWithCopy here in case the user mutates\n        with option_context(\"mode.chained_assignment\", None):\n            try:\n                result = self._python_apply_general(f, self._selected_obj)\n            except TypeError:\n                # gh-20949\n                # try again, with .apply acting as a filtering\n                # operation, by excluding the grouping column\n                # This would normally not be triggered\n                # except if the udf is trying an operation that\n                # fails on *some* columns, e.g. a numeric operation\n                # on a string grouper column\n\n                with group_selection_context(self):\n                    return self._python_apply_general(f, self._selected_obj)\n\n        return result\n\n    def _python_apply_general(\n        self, f: F, data: FrameOrSeriesUnion\n    ) -> FrameOrSeriesUnion:\n        \"\"\"\n        Apply function f in python space\n\n        Parameters\n        ----------\n        f : callable\n            Function to apply\n        data : Series or DataFrame\n            Data to apply f to\n\n        Returns\n        -------\n        Series or DataFrame\n            data after applying f\n        \"\"\"\n        keys, values, mutated = self.grouper.apply(f, data, self.axis)\n\n        return self._wrap_applied_output(\n            keys, values, not_indexed_same=mutated or self.mutated\n        )\n\n    def _iterate_slices(self) -> Iterable[Series]:\n        raise AbstractMethodError(self)\n\n    def transform(self, func, *args, **kwargs):\n        raise AbstractMethodError(self)\n\n    def _cumcount_array(self, ascending: bool = True):\n        \"\"\"\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Notes\n        -----\n        this is currently implementing sort=False\n        (though the default is sort=True) for groupby in general\n        \"\"\"\n        ids, _, ngroups = self.grouper.group_info\n        sorter = get_group_index_sorter(ids, ngroups)\n        ids, count = ids[sorter], len(ids)\n\n        if count == 0:\n            return np.empty(0, dtype=np.int64)\n\n        run = np.r_[True, ids[:-1] != ids[1:]]\n        rep = np.diff(np.r_[np.nonzero(run)[0], count])\n        out = (~run).cumsum()\n\n        if ascending:\n            out -= np.repeat(out[run], rep)\n        else:\n            out = np.repeat(out[np.r_[run[1:], True]], rep) - out\n\n        rev = np.empty(count, dtype=np.intp)\n        rev[sorter] = np.arange(count, dtype=np.intp)\n        return out[rev].astype(np.int64, copy=False)\n\n    def _transform_should_cast(self, func_nm: str) -> bool:\n        \"\"\"\n        Parameters\n        ----------\n        func_nm: str\n            The name of the aggregation function being performed\n\n        Returns\n        -------\n        bool\n            Whether transform should attempt to cast the result of aggregation\n        \"\"\"\n        filled_series = self.grouper.size().fillna(0)\n        assert filled_series is not None\n        return filled_series.gt(0).any() and func_nm not in base.cython_cast_blocklist\n\n    def _cython_transform(self, how: str, numeric_only: bool = True, **kwargs):\n        output: Dict[base.OutputKey, np.ndarray] = {}\n        for idx, obj in enumerate(self._iterate_slices()):\n            name = obj.name\n            is_numeric = is_numeric_dtype(obj.dtype)\n            if numeric_only and not is_numeric:\n                continue\n\n            try:\n                result, _ = self.grouper.transform(obj.values, how, **kwargs)\n            except NotImplementedError:\n                continue\n\n            if self._transform_should_cast(how):\n                result = maybe_cast_result(result, obj, how=how)\n\n            key = base.OutputKey(label=name, position=idx)\n            output[key] = result\n\n        if len(output) == 0:\n            raise DataError(\"No numeric types to aggregate\")\n\n        return self._wrap_transformed_output(output)\n\n    def _wrap_aggregated_output(\n        self, output: Mapping[base.OutputKey, np.ndarray], index: Optional[Index]\n    ):\n        raise AbstractMethodError(self)\n\n    def _wrap_transformed_output(self, output: Mapping[base.OutputKey, np.ndarray]):\n        raise AbstractMethodError(self)\n\n    def _wrap_applied_output(self, keys, values, not_indexed_same: bool = False):\n        raise AbstractMethodError(self)\n\n    def _agg_general(\n        self,\n        numeric_only: bool = True,\n        min_count: int = -1,\n        *,\n        alias: str,\n        npfunc: Callable,\n    ):\n        with group_selection_context(self):\n            # try a cython aggregation if we can\n            result = None\n            try:\n                result = self._cython_agg_general(\n                    how=alias,\n                    alt=npfunc,\n                    numeric_only=numeric_only,\n                    min_count=min_count,\n                )\n            except DataError:\n                pass\n            except NotImplementedError as err:\n                if \"function is not implemented for this dtype\" in str(\n                    err\n                ) or \"category dtype not supported\" in str(err):\n                    # raised in _get_cython_function, in some cases can\n                    #  be trimmed by implementing cython funcs for more dtypes\n                    pass\n                else:\n                    raise\n\n            # apply a non-cython aggregation\n            if result is None:\n                result = self.aggregate(lambda x: npfunc(x, axis=self.axis))\n            return result.__finalize__(self.obj, method=\"groupby\")\n\n    def _cython_agg_general(\n        self, how: str, alt=None, numeric_only: bool = True, min_count: int = -1\n    ):\n        output: Dict[base.OutputKey, Union[np.ndarray, DatetimeArray]] = {}\n        # Ideally we would be able to enumerate self._iterate_slices and use\n        # the index from enumeration as the key of output, but ohlc in particular\n        # returns a (n x 4) array. Output requires 1D ndarrays as values, so we\n        # need to slice that up into 1D arrays\n        idx = 0\n        for obj in self._iterate_slices():\n            name = obj.name\n            is_numeric = is_numeric_dtype(obj.dtype)\n            if numeric_only and not is_numeric:\n                continue\n\n            result, agg_names = self.grouper.aggregate(\n                obj._values, how, min_count=min_count\n            )\n\n            if agg_names:\n                # e.g. ohlc\n                assert len(agg_names) == result.shape[1]\n                for result_column, result_name in zip(result.T, agg_names):\n                    key = base.OutputKey(label=result_name, position=idx)\n                    output[key] = maybe_cast_result(result_column, obj, how=how)\n                    idx += 1\n            else:\n                assert result.ndim == 1\n                key = base.OutputKey(label=name, position=idx)\n                output[key] = maybe_cast_result(result, obj, how=how)\n                idx += 1\n\n        if len(output) == 0:\n            raise DataError(\"No numeric types to aggregate\")\n\n        return self._wrap_aggregated_output(output, index=self.grouper.result_index)\n\n    def _transform_with_numba(self, data, func, *args, engine_kwargs=None, **kwargs):\n        \"\"\"\n        Perform groupby transform routine with the numba engine.\n\n        This routine mimics the data splitting routine of the DataSplitter class\n        to generate the indices of each group in the sorted data and then passes the\n        data and indices into a Numba jitted function.\n        \"\"\"\n        if not callable(func):\n            raise NotImplementedError(\n                \"Numba engine can only be used with a single function.\"\n            )\n        group_keys = self.grouper._get_group_keys()\n        labels, _, n_groups = self.grouper.group_info\n        sorted_index = get_group_index_sorter(labels, n_groups)\n        sorted_labels = algorithms.take_nd(labels, sorted_index, allow_fill=False)\n        sorted_data = data.take(sorted_index, axis=self.axis).to_numpy()\n        starts, ends = lib.generate_slices(sorted_labels, n_groups)\n\n        numba_transform_func = numba_.generate_numba_transform_func(\n            tuple(args), kwargs, func, engine_kwargs\n        )\n        result = numba_transform_func(\n            sorted_data, sorted_index, starts, ends, len(group_keys), len(data.columns)\n        )\n\n        cache_key = (func, \"groupby_transform\")\n        if cache_key not in NUMBA_FUNC_CACHE:\n            NUMBA_FUNC_CACHE[cache_key] = numba_transform_func\n\n        # result values needs to be resorted to their original positions since we\n        # evaluated the data sorted by group\n        return result.take(np.argsort(sorted_index), axis=0)\n\n    def _aggregate_with_numba(self, data, func, *args, engine_kwargs=None, **kwargs):\n        \"\"\"\n        Perform groupby aggregation routine with the numba engine.\n\n        This routine mimics the data splitting routine of the DataSplitter class\n        to generate the indices of each group in the sorted data and then passes the\n        data and indices into a Numba jitted function.\n        \"\"\"\n        if not callable(func):\n            raise NotImplementedError(\n                \"Numba engine can only be used with a single function.\"\n            )\n        group_keys = self.grouper._get_group_keys()\n        labels, _, n_groups = self.grouper.group_info\n        sorted_index = get_group_index_sorter(labels, n_groups)\n        sorted_labels = algorithms.take_nd(labels, sorted_index, allow_fill=False)\n        sorted_data = data.take(sorted_index, axis=self.axis).to_numpy()\n        starts, ends = lib.generate_slices(sorted_labels, n_groups)\n\n        numba_agg_func = numba_.generate_numba_agg_func(\n            tuple(args), kwargs, func, engine_kwargs\n        )\n        result = numba_agg_func(\n            sorted_data, sorted_index, starts, ends, len(group_keys), len(data.columns)\n        )\n\n        cache_key = (func, \"groupby_agg\")\n        if cache_key not in NUMBA_FUNC_CACHE:\n            NUMBA_FUNC_CACHE[cache_key] = numba_agg_func\n\n        if self.grouper.nkeys > 1:\n            index = MultiIndex.from_tuples(group_keys, names=self.grouper.names)\n        else:\n            index = Index(group_keys, name=self.grouper.names[0])\n        return result, index\n\n    def _python_agg_general(self, func, *args, **kwargs):\n        func = self._is_builtin_func(func)\n        f = lambda x: func(x, *args, **kwargs)\n\n        # iterate through \"columns\" ex exclusions to populate output dict\n        output: Dict[base.OutputKey, np.ndarray] = {}\n\n        for idx, obj in enumerate(self._iterate_slices()):\n            name = obj.name\n            if self.grouper.ngroups == 0:\n                # agg_series below assumes ngroups > 0\n                continue\n\n            try:\n                # if this function is invalid for this dtype, we will ignore it.\n                result, counts = self.grouper.agg_series(obj, f)\n            except TypeError:\n                continue\n\n            assert result is not None\n            key = base.OutputKey(label=name, position=idx)\n            output[key] = maybe_cast_result(result, obj, numeric_only=True)\n\n        if len(output) == 0:\n            return self._python_apply_general(f, self._selected_obj)\n\n        if self.grouper._filter_empty_groups:\n\n            mask = counts.ravel() > 0\n            for key, result in output.items():\n\n                # since we are masking, make sure that we have a float object\n                values = result\n                if is_numeric_dtype(values.dtype):\n                    values = ensure_float(values)\n\n                output[key] = maybe_cast_result(values[mask], result)\n\n        return self._wrap_aggregated_output(output, index=self.grouper.result_index)\n\n    def _concat_objects(self, keys, values, not_indexed_same: bool = False):\n        from pandas.core.reshape.concat import concat\n\n        def reset_identity(values):\n            # reset the identities of the components\n            # of the values to prevent aliasing\n            for v in com.not_none(*values):\n                ax = v._get_axis(self.axis)\n                ax._reset_identity()\n            return values\n\n        if not not_indexed_same:\n            result = concat(values, axis=self.axis)\n            ax = self._selected_obj._get_axis(self.axis)\n\n            # this is a very unfortunate situation\n            # we can't use reindex to restore the original order\n            # when the ax has duplicates\n            # so we resort to this\n            # GH 14776, 30667\n            if ax.has_duplicates and not result.axes[self.axis].equals(ax):\n                indexer, _ = result.index.get_indexer_non_unique(ax.values)\n                indexer = algorithms.unique1d(indexer)\n                result = result.take(indexer, axis=self.axis)\n            else:\n                result = result.reindex(ax, axis=self.axis, copy=False)\n\n        elif self.group_keys:\n\n            values = reset_identity(values)\n            if self.as_index:\n\n                # possible MI return case\n                group_keys = keys\n                group_levels = self.grouper.levels\n                group_names = self.grouper.names\n\n                result = concat(\n                    values,\n                    axis=self.axis,\n                    keys=group_keys,\n                    levels=group_levels,\n                    names=group_names,\n                    sort=False,\n                )\n            else:\n\n                # GH5610, returns a MI, with the first level being a\n                # range index\n                keys = list(range(len(values)))\n                result = concat(values, axis=self.axis, keys=keys)\n        else:\n            values = reset_identity(values)\n            result = concat(values, axis=self.axis)\n\n        if isinstance(result, Series) and self._selection_name is not None:\n\n            result.name = self._selection_name\n\n        return result\n\n    def _apply_filter(self, indices, dropna):\n        if len(indices) == 0:\n            indices = np.array([], dtype=\"int64\")\n        else:\n            indices = np.sort(np.concatenate(indices))\n        if dropna:\n            filtered = self._selected_obj.take(indices, axis=self.axis)\n        else:\n            mask = np.empty(len(self._selected_obj.index), dtype=bool)\n            mask.fill(False)\n            mask[indices.astype(int)] = True\n            # mask fails to broadcast when passed to where; broadcast manually.\n            mask = np.tile(mask, list(self._selected_obj.shape[1:]) + [1]).T\n            filtered = self._selected_obj.where(mask)  # Fill with NaNs.\n        return filtered\n\n\n# To track operations that expand dimensions, like ohlc\nOutputFrameOrSeries = TypeVar(\"OutputFrameOrSeries\", bound=NDFrame)\n\n\nclass GroupBy(BaseGroupBy[FrameOrSeries]):\n    \"\"\"\n    Class for grouping and aggregating relational data.\n\n    See aggregate, transform, and apply functions on this object.\n\n    It's easiest to use obj.groupby(...) to use GroupBy, but you can also do:\n\n    ::\n\n        grouped = groupby(obj, ...)\n\n    Parameters\n    ----------\n    obj : pandas object\n    axis : int, default 0\n    level : int, default None\n        Level of MultiIndex\n    groupings : list of Grouping objects\n        Most users should ignore this\n    exclusions : array-like, optional\n        List of columns to exclude\n    name : str\n        Most users should ignore this\n\n    Returns\n    -------\n    **Attributes**\n    groups : dict\n        {group name -> group labels}\n    len(grouped) : int\n        Number of groups\n\n    Notes\n    -----\n    After grouping, see aggregate, apply, and transform functions. Here are\n    some other brief notes about usage. When grouping by multiple groups, the\n    result index will be a MultiIndex (hierarchical) by default.\n\n    Iteration produces (key, group) tuples, i.e. chunking the data by group. So\n    you can write code like:\n\n    ::\n\n        grouped = obj.groupby(keys, axis=axis)\n        for key, group in grouped:\n            # do something with the data\n\n    Function calls on GroupBy, if not specially implemented, \"dispatch\" to the\n    grouped data. So if you group a DataFrame and wish to invoke the std()\n    method on each group, you can simply do:\n\n    ::\n\n        df.groupby(mapper).std()\n\n    rather than\n\n    ::\n\n        df.groupby(mapper).aggregate(np.std)\n\n    You can pass arguments to these \"wrapped\" functions, too.\n\n    See the online documentation for full exposition on these topics and much\n    more\n    \"\"\"\n\n    @property\n    def _obj_1d_constructor(self) -> Type[\"Series\"]:\n        # GH28330 preserve subclassed Series/DataFrames\n        if isinstance(self.obj, DataFrame):\n            return self.obj._constructor_sliced\n        assert isinstance(self.obj, Series)\n        return self.obj._constructor\n\n    def _bool_agg(self, val_test, skipna):\n        \"\"\"\n        Shared func to call any / all Cython GroupBy implementations.\n        \"\"\"\n\n        def objs_to_bool(vals: np.ndarray) -> Tuple[np.ndarray, Type]:\n            if is_object_dtype(vals):\n                vals = np.array([bool(x) for x in vals])\n            else:\n                vals = vals.astype(bool)\n\n            return vals.view(np.uint8), bool\n\n        def result_to_bool(result: np.ndarray, inference: Type) -> np.ndarray:\n            return result.astype(inference, copy=False)\n\n        return self._get_cythonized_result(\n            \"group_any_all\",\n            aggregate=True,\n            numeric_only=False,\n            cython_dtype=np.dtype(np.uint8),\n            needs_values=True,\n            needs_mask=True,\n            pre_processing=objs_to_bool,\n            post_processing=result_to_bool,\n            val_test=val_test,\n            skipna=skipna,\n        )\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def any(self, skipna: bool = True):\n        \"\"\"\n        Return True if any value in the group is truthful, else False.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Flag to ignore nan values during truth testing.\n\n        Returns\n        -------\n        Series or DataFrame\n            DataFrame or Series of boolean values, where a value is True if any element\n            is True within its respective group, False otherwise.\n        \"\"\"\n        return self._bool_agg(\"any\", skipna)\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def all(self, skipna: bool = True):\n        \"\"\"\n        Return True if all values in the group are truthful, else False.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Flag to ignore nan values during truth testing.\n\n        Returns\n        -------\n        Series or DataFrame\n            DataFrame or Series of boolean values, where a value is True if all elements\n            are True within its respective group, False otherwise.\n        \"\"\"\n        return self._bool_agg(\"all\", skipna)\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def count(self):\n        \"\"\"\n        Compute count of group, excluding missing values.\n\n        Returns\n        -------\n        Series or DataFrame\n            Count of values within each group.\n        \"\"\"\n        # defined here for API doc\n        raise NotImplementedError\n\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def mean(self, numeric_only: bool = True):\n        \"\"\"\n        Compute mean of groups, excluding missing values.\n\n        Parameters\n        ----------\n        numeric_only : bool, default True\n            Include only float, int, boolean columns. If None, will attempt to use\n            everything, then use only numeric data.\n\n        Returns\n        -------\n        pandas.Series or pandas.DataFrame\n        %(see_also)s\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5],\n        ...                    'C': [1, 2, 1, 1, 2]}, columns=['A', 'B', 'C'])\n\n        Groupby one column and return the mean of the remaining columns in\n        each group.\n\n        >>> df.groupby('A').mean()\n             B         C\n        A\n        1  3.0  1.333333\n        2  4.0  1.500000\n\n        Groupby two columns and return the mean of the remaining column.\n\n        >>> df.groupby(['A', 'B']).mean()\n               C\n        A B\n        1 2.0  2\n          4.0  1\n        2 3.0  1\n          5.0  2\n\n        Groupby one column and return the mean of only particular column in\n        the group.\n\n        >>> df.groupby('A')['B'].mean()\n        A\n        1    3.0\n        2    4.0\n        Name: B, dtype: float64\n        \"\"\"\n        return self._cython_agg_general(\n            \"mean\",\n            alt=lambda x, axis: Series(x).mean(numeric_only=numeric_only),\n            numeric_only=numeric_only,\n        )\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def median(self, numeric_only=True):\n        \"\"\"\n        Compute median of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Parameters\n        ----------\n        numeric_only : bool, default True\n            Include only float, int, boolean columns. If None, will attempt to use\n            everything, then use only numeric data.\n\n        Returns\n        -------\n        Series or DataFrame\n            Median of values within each group.\n        \"\"\"\n        return self._cython_agg_general(\n            \"median\",\n            alt=lambda x, axis: Series(x).median(axis=axis, numeric_only=numeric_only),\n            numeric_only=numeric_only,\n        )\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def std(self, ddof: int = 1):\n        \"\"\"\n        Compute standard deviation of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        Returns\n        -------\n        Series or DataFrame\n            Standard deviation of values within each group.\n        \"\"\"\n        return self._get_cythonized_result(\n            \"group_var_float64\",\n            aggregate=True,\n            needs_counts=True,\n            needs_values=True,\n            needs_2d=True,\n            cython_dtype=np.dtype(np.float64),\n            post_processing=lambda vals, inference: np.sqrt(vals),\n            ddof=ddof,\n        )\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def var(self, ddof: int = 1):\n        \"\"\"\n        Compute variance of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        Returns\n        -------\n        Series or DataFrame\n            Variance of values within each group.\n        \"\"\"\n        if ddof == 1:\n            return self._cython_agg_general(\n                \"var\", alt=lambda x, axis: Series(x).var(ddof=ddof)\n            )\n        else:\n            func = lambda x: x.var(ddof=ddof)\n            with group_selection_context(self):\n                return self._python_agg_general(func)\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def sem(self, ddof: int = 1):\n        \"\"\"\n        Compute standard error of the mean of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        Returns\n        -------\n        Series or DataFrame\n            Standard error of the mean of values within each group.\n        \"\"\"\n        result = self.std(ddof=ddof)\n        if result.ndim == 1:\n            result /= np.sqrt(self.count())\n        else:\n            cols = result.columns.get_indexer_for(\n                result.columns.difference(self.exclusions).unique()\n            )\n            # TODO(GH-22046) - setting with iloc broken if labels are not unique\n            # .values to remove labels\n            result.iloc[:, cols] = (\n                result.iloc[:, cols].values / np.sqrt(self.count().iloc[:, cols]).values\n            )\n        return result\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def size(self) -> FrameOrSeriesUnion:\n        \"\"\"\n        Compute group sizes.\n\n        Returns\n        -------\n        DataFrame or Series\n            Number of rows in each group as a Series if as_index is True\n            or a DataFrame if as_index is False.\n        \"\"\"\n        result = self.grouper.size()\n\n        # GH28330 preserve subclassed Series/DataFrames through calls\n        if issubclass(self.obj._constructor, Series):\n            result = self._obj_1d_constructor(result, name=self.obj.name)\n        else:\n            result = self._obj_1d_constructor(result)\n\n        if not self.as_index:\n            result = result.rename(\"size\").reset_index()\n\n        return self._reindex_output(result, fill_value=0)\n\n    @doc(_groupby_agg_method_template, fname=\"sum\", no=True, mc=0)\n    def sum(self, numeric_only: bool = True, min_count: int = 0):\n\n        # If we are grouping on categoricals we want unobserved categories to\n        # return zero, rather than the default of NaN which the reindexing in\n        # _agg_general() returns. GH #31422\n        with com.temp_setattr(self, \"observed\", True):\n            result = self._agg_general(\n                numeric_only=numeric_only,\n                min_count=min_count,\n                alias=\"add\",\n                npfunc=np.sum,\n            )\n\n        return self._reindex_output(result, fill_value=0)\n\n    @doc(_groupby_agg_method_template, fname=\"prod\", no=True, mc=0)\n    def prod(self, numeric_only: bool = True, min_count: int = 0):\n        return self._agg_general(\n            numeric_only=numeric_only, min_count=min_count, alias=\"prod\", npfunc=np.prod\n        )\n\n    @doc(_groupby_agg_method_template, fname=\"min\", no=False, mc=-1)\n    def min(self, numeric_only: bool = False, min_count: int = -1):\n        return self._agg_general(\n            numeric_only=numeric_only, min_count=min_count, alias=\"min\", npfunc=np.min\n        )\n\n    @doc(_groupby_agg_method_template, fname=\"max\", no=False, mc=-1)\n    def max(self, numeric_only: bool = False, min_count: int = -1):\n        return self._agg_general(\n            numeric_only=numeric_only, min_count=min_count, alias=\"max\", npfunc=np.max\n        )\n\n    @doc(_groupby_agg_method_template, fname=\"first\", no=False, mc=-1)\n    def first(self, numeric_only: bool = False, min_count: int = -1):\n        def first_compat(obj: FrameOrSeries, axis: int = 0):\n            def first(x: Series):\n                \"\"\"Helper function for first item that isn't NA.\"\"\"\n                x = x.array[notna(x.array)]\n                if len(x) == 0:\n                    return np.nan\n                return x[0]\n\n            if isinstance(obj, DataFrame):\n                return obj.apply(first, axis=axis)\n            elif isinstance(obj, Series):\n                return first(obj)\n            else:\n                raise TypeError(type(obj))\n\n        return self._agg_general(\n            numeric_only=numeric_only,\n            min_count=min_count,\n            alias=\"first\",\n            npfunc=first_compat,\n        )\n\n    @doc(_groupby_agg_method_template, fname=\"last\", no=False, mc=-1)\n    def last(self, numeric_only: bool = False, min_count: int = -1):\n        def last_compat(obj: FrameOrSeries, axis: int = 0):\n            def last(x: Series):\n                \"\"\"Helper function for last item that isn't NA.\"\"\"\n                x = x.array[notna(x.array)]\n                if len(x) == 0:\n                    return np.nan\n                return x[-1]\n\n            if isinstance(obj, DataFrame):\n                return obj.apply(last, axis=axis)\n            elif isinstance(obj, Series):\n                return last(obj)\n            else:\n                raise TypeError(type(obj))\n\n        return self._agg_general(\n            numeric_only=numeric_only,\n            min_count=min_count,\n            alias=\"last\",\n            npfunc=last_compat,\n        )\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def ohlc(self) -> DataFrame:\n        \"\"\"\n        Compute open, high, low and close values of a group, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Returns\n        -------\n        DataFrame\n            Open, high, low and close values within each group.\n        \"\"\"\n        return self._apply_to_column_groupbys(lambda x: x._cython_agg_general(\"ohlc\"))\n\n    @doc(DataFrame.describe)\n    def describe(self, **kwargs):\n        with group_selection_context(self):\n            result = self.apply(lambda x: x.describe(**kwargs))\n            if self.axis == 1:\n                return result.T\n            return result.unstack()\n\n    def resample(self, rule, *args, **kwargs):\n        \"\"\"\n        Provide resampling when using a TimeGrouper.\n\n        Given a grouper, the function resamples it according to a string\n        \"string\" -> \"frequency\".\n\n        See the :ref:`frequency aliases <timeseries.offset_aliases>`\n        documentation for more details.\n\n        Parameters\n        ----------\n        rule : str or DateOffset\n            The offset string or object representing target grouper conversion.\n        *args, **kwargs\n            Possible arguments are `how`, `fill_method`, `limit`, `kind` and\n            `on`, and other arguments of `TimeGrouper`.\n\n        Returns\n        -------\n        Grouper\n            Return a new grouper with our resampler appended.\n\n        See Also\n        --------\n        Grouper : Specify a frequency to resample with when\n            grouping by a key.\n        DatetimeIndex.resample : Frequency conversion and resampling of\n            time series.\n\n        Examples\n        --------\n        >>> idx = pd.date_range('1/1/2000', periods=4, freq='T')\n        >>> df = pd.DataFrame(data=4 * [range(2)],\n        ...                   index=idx,\n        ...                   columns=['a', 'b'])\n        >>> df.iloc[2, 0] = 5\n        >>> df\n                            a  b\n        2000-01-01 00:00:00  0  1\n        2000-01-01 00:01:00  0  1\n        2000-01-01 00:02:00  5  1\n        2000-01-01 00:03:00  0  1\n\n        Downsample the DataFrame into 3 minute bins and sum the values of\n        the timestamps falling into a bin.\n\n        >>> df.groupby('a').resample('3T').sum()\n                                 a  b\n        a\n        0   2000-01-01 00:00:00  0  2\n            2000-01-01 00:03:00  0  1\n        5   2000-01-01 00:00:00  5  1\n\n        Upsample the series into 30 second bins.\n\n        >>> df.groupby('a').resample('30S').sum()\n                            a  b\n        a\n        0   2000-01-01 00:00:00  0  1\n            2000-01-01 00:00:30  0  0\n            2000-01-01 00:01:00  0  1\n            2000-01-01 00:01:30  0  0\n            2000-01-01 00:02:00  0  0\n            2000-01-01 00:02:30  0  0\n            2000-01-01 00:03:00  0  1\n        5   2000-01-01 00:02:00  5  1\n\n        Resample by month. Values are assigned to the month of the period.\n\n        >>> df.groupby('a').resample('M').sum()\n                    a  b\n        a\n        0   2000-01-31  0  3\n        5   2000-01-31  5  1\n\n        Downsample the series into 3 minute bins as above, but close the right\n        side of the bin interval.\n\n        >>> df.groupby('a').resample('3T', closed='right').sum()\n                                 a  b\n        a\n        0   1999-12-31 23:57:00  0  1\n            2000-01-01 00:00:00  0  2\n        5   2000-01-01 00:00:00  5  1\n\n        Downsample the series into 3 minute bins and close the right side of\n        the bin interval, but label each bin using the right edge instead of\n        the left.\n\n        >>> df.groupby('a').resample('3T', closed='right', label='right').sum()\n                                 a  b\n        a\n        0   2000-01-01 00:00:00  0  1\n            2000-01-01 00:03:00  0  2\n        5   2000-01-01 00:03:00  5  1\n        \"\"\"\n        from pandas.core.resample import get_resampler_for_grouping\n\n        return get_resampler_for_grouping(self, rule, *args, **kwargs)\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def rolling(self, *args, **kwargs):\n        \"\"\"\n        Return a rolling grouper, providing rolling functionality per group.\n        \"\"\"\n        from pandas.core.window import RollingGroupby\n\n        return RollingGroupby(self, *args, **kwargs)\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def expanding(self, *args, **kwargs):\n        \"\"\"\n        Return an expanding grouper, providing expanding\n        functionality per group.\n        \"\"\"\n        from pandas.core.window import ExpandingGroupby\n\n        return ExpandingGroupby(self, *args, **kwargs)\n\n    def _fill(self, direction, limit=None):\n        \"\"\"\n        Shared function for `pad` and `backfill` to call Cython method.\n\n        Parameters\n        ----------\n        direction : {'ffill', 'bfill'}\n            Direction passed to underlying Cython function. `bfill` will cause\n            values to be filled backwards. `ffill` and any other values will\n            default to a forward fill\n        limit : int, default None\n            Maximum number of consecutive values to fill. If `None`, this\n            method will convert to -1 prior to passing to Cython\n\n        Returns\n        -------\n        `Series` or `DataFrame` with filled values\n\n        See Also\n        --------\n        pad : Returns Series with minimum number of char in object.\n        backfill : Backward fill the missing values in the dataset.\n        \"\"\"\n        # Need int value for Cython\n        if limit is None:\n            limit = -1\n\n        return self._get_cythonized_result(\n            \"group_fillna_indexer\",\n            numeric_only=False,\n            needs_mask=True,\n            cython_dtype=np.dtype(np.int64),\n            result_is_index=True,\n            direction=direction,\n            limit=limit,\n            dropna=self.dropna,\n        )\n\n    @Substitution(name=\"groupby\")\n    def pad(self, limit=None):\n        \"\"\"\n        Forward fill the values.\n\n        Parameters\n        ----------\n        limit : int, optional\n            Limit of how many values to fill.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object with missing values filled.\n\n        See Also\n        --------\n        Series.pad: Returns Series with minimum number of char in object.\n        DataFrame.pad: Object with missing values filled or None if inplace=True.\n        Series.fillna: Fill NaN values of a Series.\n        DataFrame.fillna: Fill NaN values of a DataFrame.\n        \"\"\"\n        return self._fill(\"ffill\", limit=limit)\n\n    ffill = pad\n\n    @Substitution(name=\"groupby\")\n    def backfill(self, limit=None):\n        \"\"\"\n        Backward fill the values.\n\n        Parameters\n        ----------\n        limit : int, optional\n            Limit of how many values to fill.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object with missing values filled.\n\n        See Also\n        --------\n        Series.backfill :  Backward fill the missing values in the dataset.\n        DataFrame.backfill:  Backward fill the missing values in the dataset.\n        Series.fillna: Fill NaN values of a Series.\n        DataFrame.fillna: Fill NaN values of a DataFrame.\n        \"\"\"\n        return self._fill(\"bfill\", limit=limit)\n\n    bfill = backfill\n\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def nth(self, n: Union[int, List[int]], dropna: Optional[str] = None) -> DataFrame:\n        \"\"\"\n        Take the nth row from each group if n is an int, or a subset of rows\n        if n is a list of ints.\n\n        If dropna, will take the nth non-null row, dropna is either\n        'all' or 'any'; this is equivalent to calling dropna(how=dropna)\n        before the groupby.\n\n        Parameters\n        ----------\n        n : int or list of ints\n            A single nth value for the row or a list of nth values.\n        dropna : None or str, optional\n            Apply the specified dropna operation before counting which row is\n            the nth row. Needs to be None, 'any' or 'all'.\n\n        Returns\n        -------\n        Series or DataFrame\n            N-th value within each group.\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5]}, columns=['A', 'B'])\n        >>> g = df.groupby('A')\n        >>> g.nth(0)\n             B\n        A\n        1  NaN\n        2  3.0\n        >>> g.nth(1)\n             B\n        A\n        1  2.0\n        2  5.0\n        >>> g.nth(-1)\n             B\n        A\n        1  4.0\n        2  5.0\n        >>> g.nth([0, 1])\n             B\n        A\n        1  NaN\n        1  2.0\n        2  3.0\n        2  5.0\n\n        Specifying `dropna` allows count ignoring ``NaN``\n\n        >>> g.nth(0, dropna='any')\n             B\n        A\n        1  2.0\n        2  3.0\n\n        NaNs denote group exhausted when using dropna\n\n        >>> g.nth(3, dropna='any')\n            B\n        A\n        1 NaN\n        2 NaN\n\n        Specifying `as_index=False` in `groupby` keeps the original index.\n\n        >>> df.groupby('A', as_index=False).nth(1)\n           A    B\n        1  1  2.0\n        4  2  5.0\n        \"\"\"\n        valid_containers = (set, list, tuple)\n        if not isinstance(n, (valid_containers, int)):\n            raise TypeError(\"n needs to be an int or a list/set/tuple of ints\")\n\n        if not dropna:\n\n            if isinstance(n, int):\n                nth_values = [n]\n            elif isinstance(n, valid_containers):\n                nth_values = list(set(n))\n\n            nth_array = np.array(nth_values, dtype=np.intp)\n            with group_selection_context(self):\n\n                mask_left = np.in1d(self._cumcount_array(), nth_array)\n                mask_right = np.in1d(\n                    self._cumcount_array(ascending=False) + 1, -nth_array\n                )\n                mask = mask_left | mask_right\n\n                ids, _, _ = self.grouper.group_info\n\n                # Drop NA values in grouping\n                mask = mask & (ids != -1)\n\n                out = self._selected_obj[mask]\n                if not self.as_index:\n                    return out\n\n                result_index = self.grouper.result_index\n                out.index = result_index[ids[mask]]\n\n                if not self.observed and isinstance(result_index, CategoricalIndex):\n                    out = out.reindex(result_index)\n\n                out = self._reindex_output(out)\n                return out.sort_index() if self.sort else out\n\n        # dropna is truthy\n        if isinstance(n, valid_containers):\n            raise ValueError(\"dropna option with a list of nth values is not supported\")\n\n        if dropna not in [\"any\", \"all\"]:\n            # Note: when agg-ing picker doesn't raise this, just returns NaN\n            raise ValueError(\n                \"For a DataFrame groupby, dropna must be \"\n                \"either None, 'any' or 'all', \"\n                f\"(was passed {dropna}).\"\n            )\n\n        # old behaviour, but with all and any support for DataFrames.\n        # modified in GH 7559 to have better perf\n        max_len = n if n >= 0 else -1 - n\n        dropped = self.obj.dropna(how=dropna, axis=self.axis)\n\n        # get a new grouper for our dropped obj\n        if self.keys is None and self.level is None:\n\n            # we don't have the grouper info available\n            # (e.g. we have selected out\n            # a column that is not in the current object)\n            axis = self.grouper.axis\n            grouper = axis[axis.isin(dropped.index)]\n\n        else:\n\n            # create a grouper with the original parameters, but on dropped\n            # object\n            from pandas.core.groupby.grouper import get_grouper\n\n            grouper, _, _ = get_grouper(\n                dropped,\n                key=self.keys,\n                axis=self.axis,\n                level=self.level,\n                sort=self.sort,\n                mutated=self.mutated,\n            )\n\n        grb = dropped.groupby(grouper, as_index=self.as_index, sort=self.sort)\n        sizes, result = grb.size(), grb.nth(n)\n        mask = (sizes < max_len)._values\n\n        # set the results which don't meet the criteria\n        if len(result) and mask.any():\n            result.loc[mask] = np.nan\n\n        # reset/reindex to the original groups\n        if len(self.obj) == len(dropped) or len(result) == len(\n            self.grouper.result_index\n        ):\n            result.index = self.grouper.result_index\n        else:\n            result = result.reindex(self.grouper.result_index)\n\n        return result\n\n    def quantile(self, q=0.5, interpolation: str = \"linear\"):\n        \"\"\"\n        Return group values at the given quantile, a la numpy.percentile.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            Value(s) between 0 and 1 providing the quantile(s) to compute.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            Method to use when the desired quantile falls between two points.\n\n        Returns\n        -------\n        Series or DataFrame\n            Return type determined by caller of GroupBy object.\n\n        See Also\n        --------\n        Series.quantile : Similar method for Series.\n        DataFrame.quantile : Similar method for DataFrame.\n        numpy.percentile : NumPy method to compute qth percentile.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([\n        ...     ['a', 1], ['a', 2], ['a', 3],\n        ...     ['b', 1], ['b', 3], ['b', 5]\n        ... ], columns=['key', 'val'])\n        >>> df.groupby('key').quantile()\n            val\n        key\n        a    2.0\n        b    3.0\n        \"\"\"\n        from pandas import concat\n\n        def pre_processor(vals: np.ndarray) -> Tuple[np.ndarray, Optional[Type]]:\n            if is_object_dtype(vals):\n                raise TypeError(\n                    \"'quantile' cannot be performed against 'object' dtypes!\"\n                )\n\n            inference = None\n            if is_integer_dtype(vals.dtype):\n                if is_extension_array_dtype(vals.dtype):\n                    vals = vals.to_numpy(dtype=float, na_value=np.nan)\n                inference = np.int64\n            elif is_bool_dtype(vals.dtype) and is_extension_array_dtype(vals.dtype):\n                vals = vals.to_numpy(dtype=float, na_value=np.nan)\n            elif is_datetime64_dtype(vals.dtype):\n                inference = \"datetime64[ns]\"\n                vals = np.asarray(vals).astype(float)\n\n            return vals, inference\n\n        def post_processor(vals: np.ndarray, inference: Optional[Type]) -> np.ndarray:\n            if inference:\n                # Check for edge case\n                if not (\n                    is_integer_dtype(inference)\n                    and interpolation in {\"linear\", \"midpoint\"}\n                ):\n                    vals = vals.astype(inference)\n\n            return vals\n\n        if is_scalar(q):\n            return self._get_cythonized_result(\n                \"group_quantile\",\n                aggregate=True,\n                numeric_only=False,\n                needs_values=True,\n                needs_mask=True,\n                cython_dtype=np.dtype(np.float64),\n                pre_processing=pre_processor,\n                post_processing=post_processor,\n                q=q,\n                interpolation=interpolation,\n            )\n        else:\n            results = [\n                self._get_cythonized_result(\n                    \"group_quantile\",\n                    aggregate=True,\n                    needs_values=True,\n                    needs_mask=True,\n                    cython_dtype=np.dtype(np.float64),\n                    pre_processing=pre_processor,\n                    post_processing=post_processor,\n                    q=qi,\n                    interpolation=interpolation,\n                )\n                for qi in q\n            ]\n            result = concat(results, axis=0, keys=q)\n            # fix levels to place quantiles on the inside\n            # TODO(GH-10710): Ideally, we could write this as\n            #  >>> result.stack(0).loc[pd.IndexSlice[:, ..., q], :]\n            #  but this hits https://github.com/pandas-dev/pandas/issues/10710\n            #  which doesn't reorder the list-like `q` on the inner level.\n            order = list(range(1, result.index.nlevels)) + [0]\n\n            # temporarily saves the index names\n            index_names = np.array(result.index.names)\n\n            # set index names to positions to avoid confusion\n            result.index.names = np.arange(len(index_names))\n\n            # place quantiles on the inside\n            result = result.reorder_levels(order)\n\n            # restore the index names in order\n            result.index.names = index_names[order]\n\n            # reorder rows to keep things sorted\n            indices = np.arange(len(result)).reshape([len(q), self.ngroups]).T.flatten()\n            return result.take(indices)\n\n    @Substitution(name=\"groupby\")\n    def ngroup(self, ascending: bool = True):\n        \"\"\"\n        Number each group from 0 to the number of groups - 1.\n\n        This is the enumerative complement of cumcount.  Note that the\n        numbers given to the groups match the order in which the groups\n        would be seen when iterating over the groupby object, not the\n        order they are first observed.\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from number of group - 1 to 0.\n\n        Returns\n        -------\n        Series\n            Unique numbers for each group.\n\n        See Also\n        --------\n        .cumcount : Number the rows in each group.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": list(\"aaabba\")})\n        >>> df\n           A\n        0  a\n        1  a\n        2  a\n        3  b\n        4  b\n        5  a\n        >>> df.groupby('A').ngroup()\n        0    0\n        1    0\n        2    0\n        3    1\n        4    1\n        5    0\n        dtype: int64\n        >>> df.groupby('A').ngroup(ascending=False)\n        0    1\n        1    1\n        2    1\n        3    0\n        4    0\n        5    1\n        dtype: int64\n        >>> df.groupby([\"A\", [1,1,2,3,2,1]]).ngroup()\n        0    0\n        1    0\n        2    1\n        3    3\n        4    2\n        5    0\n        dtype: int64\n        \"\"\"\n        with group_selection_context(self):\n            index = self._selected_obj.index\n            result = self._obj_1d_constructor(self.grouper.group_info[0], index)\n            if not ascending:\n                result = self.ngroups - 1 - result\n            return result\n\n    @Substitution(name=\"groupby\")\n    def cumcount(self, ascending: bool = True):\n        \"\"\"\n        Number each item in each group from 0 to the length of that group - 1.\n\n        Essentially this is equivalent to\n\n        .. code-block:: python\n\n            self.apply(lambda x: pd.Series(np.arange(len(x)), x.index))\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Returns\n        -------\n        Series\n            Sequence number of each element within each group.\n\n        See Also\n        --------\n        .ngroup : Number the groups themselves.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([['a'], ['a'], ['a'], ['b'], ['b'], ['a']],\n        ...                   columns=['A'])\n        >>> df\n           A\n        0  a\n        1  a\n        2  a\n        3  b\n        4  b\n        5  a\n        >>> df.groupby('A').cumcount()\n        0    0\n        1    1\n        2    2\n        3    0\n        4    1\n        5    3\n        dtype: int64\n        >>> df.groupby('A').cumcount(ascending=False)\n        0    3\n        1    2\n        2    1\n        3    1\n        4    0\n        5    0\n        dtype: int64\n        \"\"\"\n        with group_selection_context(self):\n            index = self._selected_obj.index\n            cumcounts = self._cumcount_array(ascending=ascending)\n            return self._obj_1d_constructor(cumcounts, index)\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def rank(\n        self,\n        method: str = \"average\",\n        ascending: bool = True,\n        na_option: str = \"keep\",\n        pct: bool = False,\n        axis: int = 0,\n    ):\n        \"\"\"\n        Provide the rank of values within each group.\n\n        Parameters\n        ----------\n        method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n            * average: average rank of group.\n            * min: lowest rank in group.\n            * max: highest rank in group.\n            * first: ranks assigned in order they appear in the array.\n            * dense: like 'min', but rank always increases by 1 between groups.\n        ascending : bool, default True\n            False for ranks by high (1) to low (N).\n        na_option : {'keep', 'top', 'bottom'}, default 'keep'\n            * keep: leave NA values where they are.\n            * top: smallest rank if ascending.\n            * bottom: smallest rank if descending.\n        pct : bool, default False\n            Compute percentage rank of data within each group.\n        axis : int, default 0\n            The axis of the object over which to compute the rank.\n\n        Returns\n        -------\n        DataFrame with ranking of values within each group\n        \"\"\"\n        if na_option not in {\"keep\", \"top\", \"bottom\"}:\n            msg = \"na_option must be one of 'keep', 'top', or 'bottom'\"\n            raise ValueError(msg)\n        return self._cython_transform(\n            \"rank\",\n            numeric_only=False,\n            ties_method=method,\n            ascending=ascending,\n            na_option=na_option,\n            pct=pct,\n            axis=axis,\n        )\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def cumprod(self, axis=0, *args, **kwargs):\n        \"\"\"\n        Cumulative product for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        nv.validate_groupby_func(\"cumprod\", args, kwargs, [\"numeric_only\", \"skipna\"])\n        if axis != 0:\n            return self.apply(lambda x: x.cumprod(axis=axis, **kwargs))\n\n        return self._cython_transform(\"cumprod\", **kwargs)\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def cumsum(self, axis=0, *args, **kwargs):\n        \"\"\"\n        Cumulative sum for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        nv.validate_groupby_func(\"cumsum\", args, kwargs, [\"numeric_only\", \"skipna\"])\n        if axis != 0:\n            return self.apply(lambda x: x.cumsum(axis=axis, **kwargs))\n\n        return self._cython_transform(\"cumsum\", **kwargs)\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def cummin(self, axis=0, **kwargs):\n        \"\"\"\n        Cumulative min for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        if axis != 0:\n            return self.apply(lambda x: np.minimum.accumulate(x, axis))\n\n        return self._cython_transform(\"cummin\", numeric_only=False)\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def cummax(self, axis=0, **kwargs):\n        \"\"\"\n        Cumulative max for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        if axis != 0:\n            return self.apply(lambda x: np.maximum.accumulate(x, axis))\n\n        return self._cython_transform(\"cummax\", numeric_only=False)\n\n    def _get_cythonized_result(\n        self,\n        how: str,\n        cython_dtype: np.dtype,\n        aggregate: bool = False,\n        numeric_only: bool = True,\n        needs_counts: bool = False,\n        needs_values: bool = False,\n        needs_2d: bool = False,\n        min_count: Optional[int] = None,\n        needs_mask: bool = False,\n        needs_ngroups: bool = False,\n        result_is_index: bool = False,\n        pre_processing=None,\n        post_processing=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Get result for Cythonized functions.\n\n        Parameters\n        ----------\n        how : str, Cythonized function name to be called\n        cython_dtype : np.dtype\n            Type of the array that will be modified by the Cython call.\n        aggregate : bool, default False\n            Whether the result should be aggregated to match the number of\n            groups\n        numeric_only : bool, default True\n            Whether only numeric datatypes should be computed\n        needs_counts : bool, default False\n            Whether the counts should be a part of the Cython call\n        needs_values : bool, default False\n            Whether the values should be a part of the Cython call\n            signature\n        needs_2d : bool, default False\n            Whether the values and result of the Cython call signature\n            are 2-dimensional.\n        min_count : int, default None\n            When not None, min_count for the Cython call\n        needs_mask : bool, default False\n            Whether boolean mask needs to be part of the Cython call\n            signature\n        needs_ngroups : bool, default False\n            Whether number of groups is part of the Cython call signature\n        result_is_index : bool, default False\n            Whether the result of the Cython operation is an index of\n            values to be retrieved, instead of the actual values themselves\n        pre_processing : function, default None\n            Function to be applied to `values` prior to passing to Cython.\n            Function should return a tuple where the first element is the\n            values to be passed to Cython and the second element is an optional\n            type which the values should be converted to after being returned\n            by the Cython operation. This function is also responsible for\n            raising a TypeError if the values have an invalid type. Raises\n            if `needs_values` is False.\n        post_processing : function, default None\n            Function to be applied to result of Cython function. Should accept\n            an array of values as the first argument and type inferences as its\n            second argument, i.e. the signature should be\n            (ndarray, Type).\n        **kwargs : dict\n            Extra arguments to be passed back to Cython funcs\n\n        Returns\n        -------\n        `Series` or `DataFrame`  with filled values\n        \"\"\"\n        if result_is_index and aggregate:\n            raise ValueError(\"'result_is_index' and 'aggregate' cannot both be True!\")\n        if post_processing:\n            if not callable(post_processing):\n                raise ValueError(\"'post_processing' must be a callable!\")\n        if pre_processing:\n            if not callable(pre_processing):\n                raise ValueError(\"'pre_processing' must be a callable!\")\n            if not needs_values:\n                raise ValueError(\n                    \"Cannot use 'pre_processing' without specifying 'needs_values'!\"\n                )\n\n        grouper = self.grouper\n\n        labels, _, ngroups = grouper.group_info\n        output: Dict[base.OutputKey, np.ndarray] = {}\n        base_func = getattr(libgroupby, how)\n\n        error_msg = \"\"\n        for idx, obj in enumerate(self._iterate_slices()):\n            name = obj.name\n            values = obj._values\n\n            if numeric_only and not is_numeric_dtype(values):\n                continue\n\n            if aggregate:\n                result_sz = ngroups\n            else:\n                result_sz = len(values)\n\n            result = np.zeros(result_sz, dtype=cython_dtype)\n            if needs_2d:\n                result = result.reshape((-1, 1))\n            func = partial(base_func, result)\n\n            inferences = None\n\n            if needs_counts:\n                counts = np.zeros(self.ngroups, dtype=np.int64)\n                func = partial(func, counts)\n\n            if needs_values:\n                vals = values\n                if pre_processing:\n                    try:\n                        vals, inferences = pre_processing(vals)\n                    except TypeError as e:\n                        error_msg = str(e)\n                        continue\n                if needs_2d:\n                    vals = vals.reshape((-1, 1))\n                vals = vals.astype(cython_dtype, copy=False)\n                func = partial(func, vals)\n\n            func = partial(func, labels)\n\n            if min_count is not None:\n                func = partial(func, min_count)\n\n            if needs_mask:\n                mask = isna(values).view(np.uint8)\n                func = partial(func, mask)\n\n            if needs_ngroups:\n                func = partial(func, ngroups)\n\n            func(**kwargs)  # Call func to modify indexer values in place\n\n            if needs_2d:\n                result = result.reshape(-1)\n\n            if result_is_index:\n                result = algorithms.take_nd(values, result)\n\n            if post_processing:\n                result = post_processing(result, inferences)\n\n            key = base.OutputKey(label=name, position=idx)\n            output[key] = result\n\n        # error_msg is \"\" on an frame/series with no rows or columns\n        if len(output) == 0 and error_msg != \"\":\n            raise TypeError(error_msg)\n\n        if aggregate:\n            return self._wrap_aggregated_output(output, index=self.grouper.result_index)\n        else:\n            return self._wrap_transformed_output(output)\n\n    @Substitution(name=\"groupby\")\n    def shift(self, periods=1, freq=None, axis=0, fill_value=None):\n        \"\"\"\n        Shift each group by periods observations.\n\n        If freq is passed, the index will be increased using the periods and the freq.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods to shift.\n        freq : str, optional\n            Frequency string.\n        axis : axis to shift, default 0\n            Shift direction.\n        fill_value : optional\n            The scalar value to use for newly introduced missing values.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        Series or DataFrame\n            Object shifted within each group.\n\n        See Also\n        --------\n        Index.shift : Shift values of Index.\n        tshift : Shift the time index, using the index’s frequency\n            if available.\n        \"\"\"\n        if freq is not None or axis != 0 or not isna(fill_value):\n            return self.apply(lambda x: x.shift(periods, freq, axis, fill_value))\n\n        return self._get_cythonized_result(\n            \"group_shift_indexer\",\n            numeric_only=False,\n            cython_dtype=np.dtype(np.int64),\n            needs_ngroups=True,\n            result_is_index=True,\n            periods=periods,\n        )\n\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def pct_change(self, periods=1, fill_method=\"pad\", limit=None, freq=None, axis=0):\n        \"\"\"\n        Calculate pct_change of each value to previous entry in group.\n\n        Returns\n        -------\n        Series or DataFrame\n            Percentage changes within each group.\n        \"\"\"\n        if freq is not None or axis != 0:\n            return self.apply(\n                lambda x: x.pct_change(\n                    periods=periods,\n                    fill_method=fill_method,\n                    limit=limit,\n                    freq=freq,\n                    axis=axis,\n                )\n            )\n        if fill_method is None:  # GH30463\n            fill_method = \"pad\"\n            limit = 0\n        filled = getattr(self, fill_method)(limit=limit)\n        fill_grp = filled.groupby(self.grouper.codes)\n        shifted = fill_grp.shift(periods=periods, freq=freq)\n        return (filled / shifted) - 1\n\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def head(self, n=5):\n        \"\"\"\n        Return first n rows of each group.\n\n        Similar to ``.apply(lambda x: x.head(n))``, but it returns a subset of rows\n        from the original DataFrame with original index and order preserved\n        (``as_index`` flag is ignored).\n\n        Does not work for negative values of `n`.\n\n        Returns\n        -------\n        Series or DataFrame\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([[1, 2], [1, 4], [5, 6]],\n        ...                   columns=['A', 'B'])\n        >>> df.groupby('A').head(1)\n           A  B\n        0  1  2\n        2  5  6\n        >>> df.groupby('A').head(-1)\n        Empty DataFrame\n        Columns: [A, B]\n        Index: []\n        \"\"\"\n        self._reset_group_selection()\n        mask = self._cumcount_array() < n\n        return self._selected_obj[mask]\n\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def tail(self, n=5):\n        \"\"\"\n        Return last n rows of each group.\n\n        Similar to ``.apply(lambda x: x.tail(n))``, but it returns a subset of rows\n        from the original DataFrame with original index and order preserved\n        (``as_index`` flag is ignored).\n\n        Does not work for negative values of `n`.\n\n        Returns\n        -------\n        Series or DataFrame\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([['a', 1], ['a', 2], ['b', 1], ['b', 2]],\n        ...                   columns=['A', 'B'])\n        >>> df.groupby('A').tail(1)\n           A  B\n        1  a  2\n        3  b  2\n        >>> df.groupby('A').tail(-1)\n        Empty DataFrame\n        Columns: [A, B]\n        Index: []\n        \"\"\"\n        self._reset_group_selection()\n        mask = self._cumcount_array(ascending=False) < n\n        return self._selected_obj[mask]\n\n    def _reindex_output(\n        self, output: OutputFrameOrSeries, fill_value: Scalar = np.NaN\n    ) -> OutputFrameOrSeries:\n        \"\"\"\n        If we have categorical groupers, then we might want to make sure that\n        we have a fully re-indexed output to the levels. This means expanding\n        the output space to accommodate all values in the cartesian product of\n        our groups, regardless of whether they were observed in the data or\n        not. This will expand the output space if there are missing groups.\n\n        The method returns early without modifying the input if the number of\n        groupings is less than 2, self.observed == True or none of the groupers\n        are categorical.\n\n        Parameters\n        ----------\n        output : Series or DataFrame\n            Object resulting from grouping and applying an operation.\n        fill_value : scalar, default np.NaN\n            Value to use for unobserved categories if self.observed is False.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object (potentially) re-indexed to include all possible groups.\n        \"\"\"\n        groupings = self.grouper.groupings\n        if groupings is None:\n            return output\n        elif len(groupings) == 1:\n            return output\n\n        # if we only care about the observed values\n        # we are done\n        elif self.observed:\n            return output\n\n        # reindexing only applies to a Categorical grouper\n        elif not any(\n            isinstance(ping.grouper, (Categorical, CategoricalIndex))\n            for ping in groupings\n        ):\n            return output\n\n        levels_list = [ping.group_index for ping in groupings]\n        index, _ = MultiIndex.from_product(\n            levels_list, names=self.grouper.names\n        ).sortlevel()\n\n        if self.as_index:\n            d = {\n                self.obj._get_axis_name(self.axis): index,\n                \"copy\": False,\n                \"fill_value\": fill_value,\n            }\n            return output.reindex(**d)\n\n        # GH 13204\n        # Here, the categorical in-axis groupers, which need to be fully\n        # expanded, are columns in `output`. An idea is to do:\n        # output = output.set_index(self.grouper.names)\n        #                .reindex(index).reset_index()\n        # but special care has to be taken because of possible not-in-axis\n        # groupers.\n        # So, we manually select and drop the in-axis grouper columns,\n        # reindex `output`, and then reset the in-axis grouper columns.\n\n        # Select in-axis groupers\n        in_axis_grps = (\n            (i, ping.name) for (i, ping) in enumerate(groupings) if ping.in_axis\n        )\n        g_nums, g_names = zip(*in_axis_grps)\n\n        output = output.drop(labels=list(g_names), axis=1)\n\n        # Set a temp index and reindex (possibly expanding)\n        output = output.set_index(self.grouper.result_index).reindex(\n            index, copy=False, fill_value=fill_value\n        )\n\n        # Reset in-axis grouper columns\n        # (using level numbers `g_nums` because level names may not be unique)\n        output = output.reset_index(level=g_nums)\n\n        return output.reset_index(drop=True)\n\n    def sample(\n        self,\n        n: Optional[int] = None,\n        frac: Optional[float] = None,\n        replace: bool = False,\n        weights: Optional[Union[Sequence, Series]] = None,\n        random_state=None,\n    ):\n        \"\"\"\n        Return a random sample of items from each group.\n\n        You can use `random_state` for reproducibility.\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        n : int, optional\n            Number of items to return for each group. Cannot be used with\n            `frac` and must be no larger than the smallest group unless\n            `replace` is True. Default is one if `frac` is None.\n        frac : float, optional\n            Fraction of items to return. Cannot be used with `n`.\n        replace : bool, default False\n            Allow or disallow sampling of the same row more than once.\n        weights : list-like, optional\n            Default None results in equal probability weighting.\n            If passed a list-like then values must have the same length as\n            the underlying DataFrame or Series object and will be used as\n            sampling probabilities after normalization within each group.\n            Values must be non-negative with at least one positive element\n            within each group.\n        random_state : int, array-like, BitGenerator, np.random.RandomState, optional\n            If int, array-like, or BitGenerator (NumPy>=1.17), seed for\n            random number generator\n            If np.random.RandomState, use as numpy RandomState object.\n\n        Returns\n        -------\n        Series or DataFrame\n            A new object of same type as caller containing items randomly\n            sampled within each group from the caller object.\n\n        See Also\n        --------\n        DataFrame.sample: Generate random samples from a DataFrame object.\n        numpy.random.choice: Generate a random sample from a given 1-D numpy\n            array.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"a\": [\"red\"] * 2 + [\"blue\"] * 2 + [\"black\"] * 2, \"b\": range(6)}\n        ... )\n        >>> df\n               a  b\n        0    red  0\n        1    red  1\n        2   blue  2\n        3   blue  3\n        4  black  4\n        5  black  5\n\n        Select one row at random for each distinct value in column a. The\n        `random_state` argument can be used to guarantee reproducibility:\n\n        >>> df.groupby(\"a\").sample(n=1, random_state=1)\n               a  b\n        4  black  4\n        2   blue  2\n        1    red  1\n\n        Set `frac` to sample fixed proportions rather than counts:\n\n        >>> df.groupby(\"a\")[\"b\"].sample(frac=0.5, random_state=2)\n        5    5\n        2    2\n        0    0\n        Name: b, dtype: int64\n\n        Control sample probabilities within groups by setting weights:\n\n        >>> df.groupby(\"a\").sample(\n        ...     n=1,\n        ...     weights=[1, 1, 1, 0, 0, 1],\n        ...     random_state=1,\n        ... )\n               a  b\n        5  black  5\n        2   blue  2\n        0    red  0\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        if weights is not None:\n            weights = Series(weights, index=self._selected_obj.index)\n            ws = [weights[idx] for idx in self.indices.values()]\n        else:\n            ws = [None] * self.ngroups\n\n        if random_state is not None:\n            random_state = com.random_state(random_state)\n\n        samples = [\n            obj.sample(\n                n=n, frac=frac, replace=replace, weights=w, random_state=random_state\n            )\n            for (_, obj), w in zip(self, ws)\n        ]\n\n        return concat(samples, axis=self.axis)\n\n\n@doc(GroupBy)\ndef get_groupby(\n    obj: NDFrame,\n    by: Optional[_KeysArgType] = None,\n    axis: int = 0,\n    level=None,\n    grouper: \"Optional[ops.BaseGrouper]\" = None,\n    exclusions=None,\n    selection=None,\n    as_index: bool = True,\n    sort: bool = True,\n    group_keys: bool = True,\n    squeeze: bool = False,\n    observed: bool = False,\n    mutated: bool = False,\n    dropna: bool = True,\n) -> GroupBy:\n\n    klass: Type[GroupBy]\n    if isinstance(obj, Series):\n        from pandas.core.groupby.generic import SeriesGroupBy\n\n        klass = SeriesGroupBy\n    elif isinstance(obj, DataFrame):\n        from pandas.core.groupby.generic import DataFrameGroupBy\n\n        klass = DataFrameGroupBy\n    else:\n        raise TypeError(f\"invalid type: {obj}\")\n\n    return klass(\n        obj=obj,\n        keys=by,\n        axis=axis,\n        level=level,\n        grouper=grouper,\n        exclusions=exclusions,\n        selection=selection,\n        as_index=as_index,\n        sort=sort,\n        group_keys=group_keys,\n        squeeze=squeeze,\n        observed=observed,\n        mutated=mutated,\n        dropna=dropna,\n    )\n"
    },
    {
      "filename": "pandas/core/indexes/accessors.py",
      "content": "\"\"\"\ndatetimelike delegation\n\"\"\"\nfrom typing import TYPE_CHECKING\nimport warnings\n\nimport numpy as np\n\nfrom pandas.core.dtypes.common import (\n    is_categorical_dtype,\n    is_datetime64_dtype,\n    is_datetime64tz_dtype,\n    is_integer_dtype,\n    is_list_like,\n    is_period_dtype,\n    is_timedelta64_dtype,\n)\nfrom pandas.core.dtypes.generic import ABCSeries\n\nfrom pandas.core.accessor import PandasDelegate, delegate_names\nfrom pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray\nfrom pandas.core.base import NoNewAttributesMixin, PandasObject\nfrom pandas.core.indexes.datetimes import DatetimeIndex\nfrom pandas.core.indexes.timedeltas import TimedeltaIndex\n\nif TYPE_CHECKING:\n    from pandas import Series\n\n\nclass Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n    def __init__(self, data: \"Series\", orig):\n        if not isinstance(data, ABCSeries):\n            raise TypeError(\n                f\"cannot convert an object of type {type(data)} to a datetimelike index\"\n            )\n\n        self._parent = data\n        self.orig = orig\n        self.name = getattr(data, \"name\", None)\n        self._freeze()\n\n    def _get_values(self):\n        data = self._parent\n        if is_datetime64_dtype(data.dtype):\n            return DatetimeIndex(data, copy=False, name=self.name)\n\n        elif is_datetime64tz_dtype(data.dtype):\n            return DatetimeIndex(data, copy=False, name=self.name)\n\n        elif is_timedelta64_dtype(data.dtype):\n            return TimedeltaIndex(data, copy=False, name=self.name)\n\n        elif is_period_dtype(data.dtype):\n            return PeriodArray(data, copy=False)\n\n        raise TypeError(\n            f\"cannot convert an object of type {type(data)} to a datetimelike index\"\n        )\n\n    def _delegate_property_get(self, name):\n        from pandas import Series\n\n        values = self._get_values()\n\n        result = getattr(values, name)\n\n        # maybe need to upcast (ints)\n        if isinstance(result, np.ndarray):\n            if is_integer_dtype(result):\n                result = result.astype(\"int64\")\n        elif not is_list_like(result):\n            return result\n\n        result = np.asarray(result)\n\n        if self.orig is not None:\n            index = self.orig.index\n        else:\n            index = self._parent.index\n        # return the result as a Series, which is by definition a copy\n        result = Series(result, index=index, name=self.name).__finalize__(self._parent)\n\n        # setting this object will show a SettingWithCopyWarning/Error\n        result._is_copy = (\n            \"modifications to a property of a datetimelike \"\n            \"object are not supported and are discarded. \"\n            \"Change values on the original.\"\n        )\n\n        return result\n\n    def _delegate_property_set(self, name, value, *args, **kwargs):\n        raise ValueError(\n            \"modifications to a property of a datetimelike object are not supported. \"\n            \"Change values on the original.\"\n        )\n\n    def _delegate_method(self, name, *args, **kwargs):\n        from pandas import Series\n\n        values = self._get_values()\n\n        method = getattr(values, name)\n        result = method(*args, **kwargs)\n\n        if not is_list_like(result):\n            return result\n\n        result = Series(result, index=self._parent.index, name=self.name).__finalize__(\n            self._parent\n        )\n\n        # setting this object will show a SettingWithCopyWarning/Error\n        result._is_copy = (\n            \"modifications to a method of a datetimelike \"\n            \"object are not supported and are discarded. \"\n            \"Change values on the original.\"\n        )\n\n        return result\n\n\n@delegate_names(\n    delegate=DatetimeArray, accessors=DatetimeArray._datetimelike_ops, typ=\"property\"\n)\n@delegate_names(\n    delegate=DatetimeArray, accessors=DatetimeArray._datetimelike_methods, typ=\"method\"\n)\nclass DatetimeProperties(Properties):\n    \"\"\"\n    Accessor object for datetimelike properties of the Series values.\n\n    Examples\n    --------\n    >>> seconds_series = pd.Series(pd.date_range(\"2000-01-01\", periods=3, freq=\"s\"))\n    >>> seconds_series\n    0   2000-01-01 00:00:00\n    1   2000-01-01 00:00:01\n    2   2000-01-01 00:00:02\n    dtype: datetime64[ns]\n    >>> seconds_series.dt.second\n    0    0\n    1    1\n    2    2\n    dtype: int64\n\n    >>> hours_series = pd.Series(pd.date_range(\"2000-01-01\", periods=3, freq=\"h\"))\n    >>> hours_series\n    0   2000-01-01 00:00:00\n    1   2000-01-01 01:00:00\n    2   2000-01-01 02:00:00\n    dtype: datetime64[ns]\n    >>> hours_series.dt.hour\n    0    0\n    1    1\n    2    2\n    dtype: int64\n\n    >>> quarters_series = pd.Series(pd.date_range(\"2000-01-01\", periods=3, freq=\"q\"))\n    >>> quarters_series\n    0   2000-03-31\n    1   2000-06-30\n    2   2000-09-30\n    dtype: datetime64[ns]\n    >>> quarters_series.dt.quarter\n    0    1\n    1    2\n    2    3\n    dtype: int64\n\n    Returns a Series indexed like the original Series.\n    Raises TypeError if the Series does not contain datetimelike values.\n    \"\"\"\n\n    def to_pydatetime(self) -> np.ndarray:\n        \"\"\"\n        Return the data as an array of native Python datetime objects.\n\n        Timezone information is retained if present.\n\n        .. warning::\n\n           Python's datetime uses microsecond resolution, which is lower than\n           pandas (nanosecond). The values are truncated.\n\n        Returns\n        -------\n        numpy.ndarray\n            Object dtype array containing native Python datetime objects.\n\n        See Also\n        --------\n        datetime.datetime : Standard library value for a datetime.\n\n        Examples\n        --------\n        >>> s = pd.Series(pd.date_range('20180310', periods=2))\n        >>> s\n        0   2018-03-10\n        1   2018-03-11\n        dtype: datetime64[ns]\n\n        >>> s.dt.to_pydatetime()\n        array([datetime.datetime(2018, 3, 10, 0, 0),\n               datetime.datetime(2018, 3, 11, 0, 0)], dtype=object)\n\n        pandas' nanosecond precision is truncated to microseconds.\n\n        >>> s = pd.Series(pd.date_range('20180310', periods=2, freq='ns'))\n        >>> s\n        0   2018-03-10 00:00:00.000000000\n        1   2018-03-10 00:00:00.000000001\n        dtype: datetime64[ns]\n\n        >>> s.dt.to_pydatetime()\n        array([datetime.datetime(2018, 3, 10, 0, 0),\n               datetime.datetime(2018, 3, 10, 0, 0)], dtype=object)\n        \"\"\"\n        return self._get_values().to_pydatetime()\n\n    @property\n    def freq(self):\n        return self._get_values().inferred_freq\n\n    def isocalendar(self):\n        \"\"\"\n        Returns a DataFrame with the year, week, and day calculated according to\n        the ISO 8601 standard.\n\n        .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        DataFrame\n            with columns year, week and day\n\n        See Also\n        --------\n        Timestamp.isocalendar : Function return a 3-tuple containing ISO year,\n            week number, and weekday for the given Timestamp object.\n        datetime.date.isocalendar : Return a named tuple object with\n            three components: year, week and weekday.\n\n        Examples\n        --------\n        >>> ser = pd.to_datetime(pd.Series([\"2010-01-01\", pd.NaT]))\n        >>> ser.dt.isocalendar()\n           year  week  day\n        0  2009    53     5\n        1  <NA>  <NA>  <NA>\n        >>> ser.dt.isocalendar().week\n        0      53\n        1    <NA>\n        Name: week, dtype: UInt32\n        \"\"\"\n        return self._get_values().isocalendar().set_index(self._parent.index)\n\n    @property\n    def weekofyear(self):\n        \"\"\"\n        The week ordinal of the year.\n\n        .. deprecated:: 1.1.0\n\n        Series.dt.weekofyear and Series.dt.week have been deprecated.\n        Please use Series.dt.isocalendar().week instead.\n        \"\"\"\n        warnings.warn(\n            \"Series.dt.weekofyear and Series.dt.week have been deprecated.  \"\n            \"Please use Series.dt.isocalendar().week instead.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        week_series = self.isocalendar().week\n        week_series.name = self.name\n        if week_series.hasnans:\n            return week_series.astype(\"float64\")\n        return week_series.astype(\"int64\")\n\n    week = weekofyear\n\n\n@delegate_names(\n    delegate=TimedeltaArray, accessors=TimedeltaArray._datetimelike_ops, typ=\"property\"\n)\n@delegate_names(\n    delegate=TimedeltaArray,\n    accessors=TimedeltaArray._datetimelike_methods,\n    typ=\"method\",\n)\nclass TimedeltaProperties(Properties):\n    \"\"\"\n    Accessor object for datetimelike properties of the Series values.\n\n    Returns a Series indexed like the original Series.\n    Raises TypeError if the Series does not contain datetimelike values.\n\n    Examples\n    --------\n    >>> seconds_series = pd.Series(\n    ...     pd.timedelta_range(start=\"1 second\", periods=3, freq=\"S\")\n    ... )\n    >>> seconds_series\n    0   0 days 00:00:01\n    1   0 days 00:00:02\n    2   0 days 00:00:03\n    dtype: timedelta64[ns]\n    >>> seconds_series.dt.seconds\n    0    1\n    1    2\n    2    3\n    dtype: int64\n    \"\"\"\n\n    def to_pytimedelta(self) -> np.ndarray:\n        \"\"\"\n        Return an array of native `datetime.timedelta` objects.\n\n        Python's standard `datetime` library uses a different representation\n        timedelta's. This method converts a Series of pandas Timedeltas\n        to `datetime.timedelta` format with the same length as the original\n        Series.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of 1D containing data with `datetime.timedelta` type.\n\n        See Also\n        --------\n        datetime.timedelta : A duration expressing the difference\n            between two date, time, or datetime.\n\n        Examples\n        --------\n        >>> s = pd.Series(pd.to_timedelta(np.arange(5), unit=\"d\"))\n        >>> s\n        0   0 days\n        1   1 days\n        2   2 days\n        3   3 days\n        4   4 days\n        dtype: timedelta64[ns]\n\n        >>> s.dt.to_pytimedelta()\n        array([datetime.timedelta(0), datetime.timedelta(days=1),\n        datetime.timedelta(days=2), datetime.timedelta(days=3),\n        datetime.timedelta(days=4)], dtype=object)\n        \"\"\"\n        return self._get_values().to_pytimedelta()\n\n    @property\n    def components(self):\n        \"\"\"\n        Return a Dataframe of the components of the Timedeltas.\n\n        Returns\n        -------\n        DataFrame\n\n        Examples\n        --------\n        >>> s = pd.Series(pd.to_timedelta(np.arange(5), unit='s'))\n        >>> s\n        0   0 days 00:00:00\n        1   0 days 00:00:01\n        2   0 days 00:00:02\n        3   0 days 00:00:03\n        4   0 days 00:00:04\n        dtype: timedelta64[ns]\n        >>> s.dt.components\n           days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds\n        0     0      0        0        0             0             0            0\n        1     0      0        0        1             0             0            0\n        2     0      0        0        2             0             0            0\n        3     0      0        0        3             0             0            0\n        4     0      0        0        4             0             0            0\n        \"\"\"\n        return (\n            self._get_values()\n            .components.set_index(self._parent.index)\n            .__finalize__(self._parent)\n        )\n\n    @property\n    def freq(self):\n        return self._get_values().inferred_freq\n\n\n@delegate_names(\n    delegate=PeriodArray, accessors=PeriodArray._datetimelike_ops, typ=\"property\"\n)\n@delegate_names(\n    delegate=PeriodArray, accessors=PeriodArray._datetimelike_methods, typ=\"method\"\n)\nclass PeriodProperties(Properties):\n    \"\"\"\n    Accessor object for datetimelike properties of the Series values.\n\n    Returns a Series indexed like the original Series.\n    Raises TypeError if the Series does not contain datetimelike values.\n\n    Examples\n    --------\n    >>> seconds_series = pd.Series(\n    ...     pd.period_range(\n    ...         start=\"2000-01-01 00:00:00\", end=\"2000-01-01 00:00:03\", freq=\"s\"\n    ...     )\n    ... )\n    >>> seconds_series\n    0    2000-01-01 00:00:00\n    1    2000-01-01 00:00:01\n    2    2000-01-01 00:00:02\n    3    2000-01-01 00:00:03\n    dtype: period[S]\n    >>> seconds_series.dt.second\n    0    0\n    1    1\n    2    2\n    3    3\n    dtype: int64\n\n    >>> hours_series = pd.Series(\n    ...     pd.period_range(start=\"2000-01-01 00:00\", end=\"2000-01-01 03:00\", freq=\"h\")\n    ... )\n    >>> hours_series\n    0    2000-01-01 00:00\n    1    2000-01-01 01:00\n    2    2000-01-01 02:00\n    3    2000-01-01 03:00\n    dtype: period[H]\n    >>> hours_series.dt.hour\n    0    0\n    1    1\n    2    2\n    3    3\n    dtype: int64\n\n    >>> quarters_series = pd.Series(\n    ...     pd.period_range(start=\"2000-01-01\", end=\"2000-12-31\", freq=\"Q-DEC\")\n    ... )\n    >>> quarters_series\n    0    2000Q1\n    1    2000Q2\n    2    2000Q3\n    3    2000Q4\n    dtype: period[Q-DEC]\n    >>> quarters_series.dt.quarter\n    0    1\n    1    2\n    2    3\n    3    4\n    dtype: int64\n    \"\"\"\n\n\nclass CombinedDatetimelikeProperties(\n    DatetimeProperties, TimedeltaProperties, PeriodProperties\n):\n    def __new__(cls, data: \"Series\"):\n        # CombinedDatetimelikeProperties isn't really instantiated. Instead\n        # we need to choose which parent (datetime or timedelta) is\n        # appropriate. Since we're checking the dtypes anyway, we'll just\n        # do all the validation here.\n\n        if not isinstance(data, ABCSeries):\n            raise TypeError(\n                f\"cannot convert an object of type {type(data)} to a datetimelike index\"\n            )\n\n        orig = data if is_categorical_dtype(data.dtype) else None\n        if orig is not None:\n            data = data._constructor(\n                orig.array,\n                name=orig.name,\n                copy=False,\n                dtype=orig._values.categories.dtype,\n            )\n\n        if is_datetime64_dtype(data.dtype):\n            return DatetimeProperties(data, orig)\n        elif is_datetime64tz_dtype(data.dtype):\n            return DatetimeProperties(data, orig)\n        elif is_timedelta64_dtype(data.dtype):\n            return TimedeltaProperties(data, orig)\n        elif is_period_dtype(data.dtype):\n            return PeriodProperties(data, orig)\n\n        raise AttributeError(\"Can only use .dt accessor with datetimelike values\")\n"
    },
    {
      "filename": "pandas/core/indexes/base.py",
      "content": "from copy import copy as copy_func\nfrom datetime import datetime\nfrom itertools import zip_longest\nimport operator\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    FrozenSet,\n    Hashable,\n    List,\n    NewType,\n    Optional,\n    Sequence,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import algos as libalgos, index as libindex, lib\nimport pandas._libs.join as libjoin\nfrom pandas._libs.lib import is_datetime_array, no_default\nfrom pandas._libs.tslibs import IncompatibleFrequency, OutOfBoundsDatetime, Timestamp\nfrom pandas._libs.tslibs.timezones import tz_compare\nfrom pandas._typing import AnyArrayLike, Dtype, DtypeObj, Label\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import DuplicateLabelError, InvalidIndexError\nfrom pandas.util._decorators import Appender, cache_readonly, doc\n\nfrom pandas.core.dtypes.cast import (\n    maybe_cast_to_integer_array,\n    validate_numeric_casting,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_int64,\n    ensure_object,\n    ensure_platform_int,\n    is_bool,\n    is_bool_dtype,\n    is_categorical_dtype,\n    is_datetime64_any_dtype,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float,\n    is_float_dtype,\n    is_hashable,\n    is_integer,\n    is_integer_dtype,\n    is_interval_dtype,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_period_dtype,\n    is_scalar,\n    is_signed_integer_dtype,\n    is_timedelta64_dtype,\n    is_unsigned_integer_dtype,\n    needs_i8_conversion,\n    pandas_dtype,\n    validate_all_hashable,\n)\nfrom pandas.core.dtypes.concat import concat_compat\nfrom pandas.core.dtypes.generic import (\n    ABCDatetimeIndex,\n    ABCMultiIndex,\n    ABCPandasArray,\n    ABCPeriodIndex,\n    ABCRangeIndex,\n    ABCSeries,\n    ABCTimedeltaIndex,\n)\nfrom pandas.core.dtypes.missing import array_equivalent, isna\n\nfrom pandas.core import missing, ops\nfrom pandas.core.accessor import CachedAccessor\nimport pandas.core.algorithms as algos\nfrom pandas.core.arrays import Categorical, ExtensionArray\nfrom pandas.core.arrays.datetimes import tz_to_dtype, validate_tz_from_dtype\nfrom pandas.core.base import IndexOpsMixin, PandasObject\nimport pandas.core.common as com\nfrom pandas.core.construction import extract_array\nfrom pandas.core.indexers import deprecate_ndim_indexing\nfrom pandas.core.indexes.frozen import FrozenList\nfrom pandas.core.ops import get_op_result_name\nfrom pandas.core.ops.invalid import make_invalid_op\nfrom pandas.core.sorting import ensure_key_mapped, nargsort\nfrom pandas.core.strings import StringMethods\n\nfrom pandas.io.formats.printing import (\n    PrettyDict,\n    default_pprint,\n    format_object_attrs,\n    format_object_summary,\n    pprint_thing,\n)\n\nif TYPE_CHECKING:\n    from pandas import MultiIndex, RangeIndex, Series\n\n\n__all__ = [\"Index\"]\n\n_unsortable_types = frozenset((\"mixed\", \"mixed-integer\"))\n\n_index_doc_kwargs = dict(\n    klass=\"Index\",\n    inplace=\"\",\n    target_klass=\"Index\",\n    raises_section=\"\",\n    unique=\"Index\",\n    duplicated=\"np.ndarray\",\n)\n_index_shared_docs = dict()\nstr_t = str\n\n\n_o_dtype = np.dtype(object)\n\n\n_Identity = NewType(\"_Identity\", object)\n\n\ndef _new_Index(cls, d):\n    \"\"\"\n    This is called upon unpickling, rather than the default which doesn't\n    have arguments and breaks __new__.\n    \"\"\"\n    # required for backward compat, because PI can't be instantiated with\n    # ordinals through __new__ GH #13277\n    if issubclass(cls, ABCPeriodIndex):\n        from pandas.core.indexes.period import _new_PeriodIndex\n\n        return _new_PeriodIndex(cls, **d)\n\n    if issubclass(cls, ABCMultiIndex):\n        if \"labels\" in d and \"codes\" not in d:\n            # GH#23752 \"labels\" kwarg has been replaced with \"codes\"\n            d[\"codes\"] = d.pop(\"labels\")\n\n    return cls.__new__(cls, **d)\n\n\n_IndexT = TypeVar(\"_IndexT\", bound=\"Index\")\n\n\nclass Index(IndexOpsMixin, PandasObject):\n    \"\"\"\n    Immutable sequence used for indexing and alignment. The basic object\n    storing axis labels for all pandas objects.\n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n    dtype : NumPy dtype (default: object)\n        If dtype is None, we find the dtype that best fits the data.\n        If an actual dtype is provided, we coerce to that dtype if it's safe.\n        Otherwise, an error will be raised.\n    copy : bool\n        Make a copy of input ndarray.\n    name : object\n        Name to be stored in the index.\n    tupleize_cols : bool (default: True)\n        When True, attempt to create a MultiIndex if possible.\n\n    See Also\n    --------\n    RangeIndex : Index implementing a monotonic integer range.\n    CategoricalIndex : Index of :class:`Categorical` s.\n    MultiIndex : A multi-level, or hierarchical Index.\n    IntervalIndex : An Index of :class:`Interval` s.\n    DatetimeIndex : Index of datetime64 data.\n    TimedeltaIndex : Index of timedelta64 data.\n    PeriodIndex : Index of Period data.\n    Int64Index : A special case of :class:`Index` with purely integer labels.\n    UInt64Index : A special case of :class:`Index` with purely unsigned integer labels.\n    Float64Index : A special case of :class:`Index` with purely float labels.\n\n    Notes\n    -----\n    An Index instance can **only** contain hashable objects\n\n    Examples\n    --------\n    >>> pd.Index([1, 2, 3])\n    Int64Index([1, 2, 3], dtype='int64')\n\n    >>> pd.Index(list('abc'))\n    Index(['a', 'b', 'c'], dtype='object')\n    \"\"\"\n\n    # tolist is not actually deprecated, just suppressed in the __dir__\n    _deprecations: FrozenSet[str] = (\n        PandasObject._deprecations\n        | IndexOpsMixin._deprecations\n        | frozenset([\"contains\", \"set_value\"])\n    )\n\n    # To hand over control to subclasses\n    _join_precedence = 1\n\n    # Cython methods; see github.com/cython/cython/issues/2647\n    #  for why we need to wrap these instead of making them class attributes\n    # Moreover, cython will choose the appropriate-dtyped sub-function\n    #  given the dtypes of the passed arguments\n    def _left_indexer_unique(self, left, right):\n        return libjoin.left_join_indexer_unique(left, right)\n\n    def _left_indexer(self, left, right):\n        return libjoin.left_join_indexer(left, right)\n\n    def _inner_indexer(self, left, right):\n        return libjoin.inner_join_indexer(left, right)\n\n    def _outer_indexer(self, left, right):\n        return libjoin.outer_join_indexer(left, right)\n\n    _typ = \"index\"\n    _data: Union[ExtensionArray, np.ndarray]\n    _id: Optional[_Identity] = None\n    _name: Label = None\n    # MultiIndex.levels previously allowed setting the index name. We\n    # don't allow this anymore, and raise if it happens rather than\n    # failing silently.\n    _no_setting_name: bool = False\n    _comparables = [\"name\"]\n    _attributes = [\"name\"]\n    _is_numeric_dtype = False\n    _can_hold_na = True\n\n    # would we like our indexing holder to defer to us\n    _defer_to_indexing = False\n\n    _engine_type = libindex.ObjectEngine\n    # whether we support partial string indexing. Overridden\n    # in DatetimeIndex and PeriodIndex\n    _supports_partial_string_indexing = False\n\n    _accessors = {\"str\"}\n\n    str = CachedAccessor(\"str\", StringMethods)\n\n    # --------------------------------------------------------------------\n    # Constructors\n\n    def __new__(\n        cls, data=None, dtype=None, copy=False, name=None, tupleize_cols=True, **kwargs\n    ) -> \"Index\":\n\n        from pandas.core.indexes.range import RangeIndex\n\n        name = maybe_extract_name(name, data, cls)\n\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n        if \"tz\" in kwargs:\n            tz = kwargs.pop(\"tz\")\n            validate_tz_from_dtype(dtype, tz)\n            dtype = tz_to_dtype(tz)\n\n        if isinstance(data, ABCPandasArray):\n            # ensure users don't accidentally put a PandasArray in an index.\n            data = data.to_numpy()\n\n        data_dtype = getattr(data, \"dtype\", None)\n\n        # range\n        if isinstance(data, RangeIndex):\n            return RangeIndex(start=data, copy=copy, dtype=dtype, name=name)\n        elif isinstance(data, range):\n            return RangeIndex.from_range(data, dtype=dtype, name=name)\n\n        # categorical\n        elif is_categorical_dtype(data_dtype) or is_categorical_dtype(dtype):\n            # Delay import for perf. https://github.com/pandas-dev/pandas/pull/31423\n            from pandas.core.indexes.category import CategoricalIndex\n\n            return _maybe_asobject(dtype, CategoricalIndex, data, copy, name, **kwargs)\n\n        # interval\n        elif is_interval_dtype(data_dtype) or is_interval_dtype(dtype):\n            # Delay import for perf. https://github.com/pandas-dev/pandas/pull/31423\n            from pandas.core.indexes.interval import IntervalIndex\n\n            return _maybe_asobject(dtype, IntervalIndex, data, copy, name, **kwargs)\n\n        elif is_datetime64_any_dtype(data_dtype) or is_datetime64_any_dtype(dtype):\n            # Delay import for perf. https://github.com/pandas-dev/pandas/pull/31423\n            from pandas import DatetimeIndex\n\n            return _maybe_asobject(dtype, DatetimeIndex, data, copy, name, **kwargs)\n\n        elif is_timedelta64_dtype(data_dtype) or is_timedelta64_dtype(dtype):\n            # Delay import for perf. https://github.com/pandas-dev/pandas/pull/31423\n            from pandas import TimedeltaIndex\n\n            return _maybe_asobject(dtype, TimedeltaIndex, data, copy, name, **kwargs)\n\n        elif is_period_dtype(data_dtype) or is_period_dtype(dtype):\n            # Delay import for perf. https://github.com/pandas-dev/pandas/pull/31423\n            from pandas import PeriodIndex\n\n            return _maybe_asobject(dtype, PeriodIndex, data, copy, name, **kwargs)\n\n        # extension dtype\n        elif is_extension_array_dtype(data_dtype) or is_extension_array_dtype(dtype):\n            if not (dtype is None or is_object_dtype(dtype)):\n                # coerce to the provided dtype\n                ea_cls = dtype.construct_array_type()\n                data = ea_cls._from_sequence(data, dtype=dtype, copy=False)\n            else:\n                data = np.asarray(data, dtype=object)\n\n            # coerce to the object dtype\n            data = data.astype(object)\n            return Index(data, dtype=object, copy=copy, name=name, **kwargs)\n\n        # index-like\n        elif isinstance(data, (np.ndarray, Index, ABCSeries)):\n            # Delay import for perf. https://github.com/pandas-dev/pandas/pull/31423\n            from pandas.core.indexes.numeric import (\n                Float64Index,\n                Int64Index,\n                UInt64Index,\n            )\n\n            if dtype is not None:\n                # we need to avoid having numpy coerce\n                # things that look like ints/floats to ints unless\n                # they are actually ints, e.g. '0' and 0.0\n                # should not be coerced\n                # GH 11836\n                data = _maybe_cast_with_dtype(data, dtype, copy)\n                dtype = data.dtype  # TODO: maybe not for object?\n\n            # maybe coerce to a sub-class\n            if is_signed_integer_dtype(data.dtype):\n                return Int64Index(data, copy=copy, dtype=dtype, name=name)\n            elif is_unsigned_integer_dtype(data.dtype):\n                return UInt64Index(data, copy=copy, dtype=dtype, name=name)\n            elif is_float_dtype(data.dtype):\n                return Float64Index(data, copy=copy, dtype=dtype, name=name)\n            elif issubclass(data.dtype.type, bool) or is_bool_dtype(data):\n                subarr = data.astype(\"object\")\n            else:\n                subarr = com.asarray_tuplesafe(data, dtype=object)\n\n            # asarray_tuplesafe does not always copy underlying data,\n            # so need to make sure that this happens\n            if copy:\n                subarr = subarr.copy()\n\n            if dtype is None:\n                new_data, new_dtype = _maybe_cast_data_without_dtype(subarr)\n                if new_dtype is not None:\n                    return cls(\n                        new_data, dtype=new_dtype, copy=False, name=name, **kwargs\n                    )\n\n            if kwargs:\n                raise TypeError(f\"Unexpected keyword arguments {repr(set(kwargs))}\")\n            if subarr.ndim > 1:\n                # GH#13601, GH#20285, GH#27125\n                raise ValueError(\"Index data must be 1-dimensional\")\n            return cls._simple_new(subarr, name)\n\n        elif data is None or is_scalar(data):\n            raise cls._scalar_data_error(data)\n        elif hasattr(data, \"__array__\"):\n            return Index(np.asarray(data), dtype=dtype, copy=copy, name=name, **kwargs)\n        else:\n            if tupleize_cols and is_list_like(data):\n                # GH21470: convert iterable to list before determining if empty\n                if is_iterator(data):\n                    data = list(data)\n\n                if data and all(isinstance(e, tuple) for e in data):\n                    # we must be all tuples, otherwise don't construct\n                    # 10697\n                    from pandas.core.indexes.multi import MultiIndex\n\n                    return MultiIndex.from_tuples(\n                        data, names=name or kwargs.get(\"names\")\n                    )\n            # other iterable of some kind\n            subarr = com.asarray_tuplesafe(data, dtype=object)\n            return Index(subarr, dtype=dtype, copy=copy, name=name, **kwargs)\n\n    \"\"\"\n    NOTE for new Index creation:\n\n    - _simple_new: It returns new Index with the same type as the caller.\n      All metadata (such as name) must be provided by caller's responsibility.\n      Using _shallow_copy is recommended because it fills these metadata\n      otherwise specified.\n\n    - _shallow_copy: It returns new Index with the same type (using\n      _simple_new), but fills caller's metadata otherwise specified. Passed\n      kwargs will overwrite corresponding metadata.\n\n    See each method's docstring.\n    \"\"\"\n\n    @property\n    def asi8(self):\n        \"\"\"\n        Integer representation of the values.\n\n        Returns\n        -------\n        ndarray\n            An ndarray with int64 dtype.\n        \"\"\"\n        return None\n\n    @classmethod\n    def _simple_new(cls, values, name: Label = None):\n        \"\"\"\n        We require that we have a dtype compat for the values. If we are passed\n        a non-dtype compat, then coerce using the constructor.\n\n        Must be careful not to recurse.\n        \"\"\"\n        assert isinstance(values, np.ndarray), type(values)\n\n        result = object.__new__(cls)\n        result._data = values\n        # _index_data is a (temporary?) fix to ensure that the direct data\n        # manipulation we do in `_libs/reduction.pyx` continues to work.\n        # We need access to the actual ndarray, since we're messing with\n        # data buffers and strides.\n        result._index_data = values\n        result._name = name\n        result._cache = {}\n        result._reset_identity()\n\n        return result\n\n    @cache_readonly\n    def _constructor(self):\n        return type(self)\n\n    def _maybe_check_unique(self):\n        \"\"\"\n        Check that an Index has no duplicates.\n\n        This is typically only called via\n        `NDFrame.flags.allows_duplicate_labels.setter` when it's set to\n        True (duplicates aren't allowed).\n\n        Raises\n        ------\n        DuplicateLabelError\n            When the index is not unique.\n        \"\"\"\n        if not self.is_unique:\n            msg = \"\"\"Index has duplicates.\"\"\"\n            duplicates = self._format_duplicate_message()\n            msg += f\"\\n{duplicates}\"\n\n            raise DuplicateLabelError(msg)\n\n    def _format_duplicate_message(self):\n        \"\"\"\n        Construct the DataFrame for a DuplicateLabelError.\n\n        This returns a DataFrame indicating the labels and positions\n        of duplicates in an index. This should only be called when it's\n        already known that duplicates are present.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['a', 'b', 'a'])\n        >>> idx._format_duplicate_message()\n            positions\n        label\n        a        [0, 2]\n        \"\"\"\n        from pandas import Series\n\n        duplicates = self[self.duplicated(keep=\"first\")].unique()\n        assert len(duplicates)\n\n        out = Series(np.arange(len(self))).groupby(self).agg(list)[duplicates]\n        if self.nlevels == 1:\n            out = out.rename_axis(\"label\")\n        return out.to_frame(name=\"positions\")\n\n    # --------------------------------------------------------------------\n    # Index Internals Methods\n\n    def _get_attributes_dict(self):\n        \"\"\"\n        Return an attributes dict for my class.\n        \"\"\"\n        return {k: getattr(self, k, None) for k in self._attributes}\n\n    def _shallow_copy(self, values=None, name: Label = no_default):\n        \"\"\"\n        Create a new Index with the same class as the caller, don't copy the\n        data, use the same object attributes with passed in attributes taking\n        precedence.\n\n        *this is an internal non-public method*\n\n        Parameters\n        ----------\n        values : the values to create the new Index, optional\n        name : Label, defaults to self.name\n        \"\"\"\n        name = self.name if name is no_default else name\n\n        if values is not None:\n            return self._simple_new(values, name=name)\n\n        result = self._simple_new(self._values, name=name)\n        result._cache = self._cache\n        return result\n\n    def is_(self, other) -> bool:\n        \"\"\"\n        More flexible, faster check like ``is`` but that works through views.\n\n        Note: this is *not* the same as ``Index.identical()``, which checks\n        that metadata is also the same.\n\n        Parameters\n        ----------\n        other : object\n            Other object to compare against.\n\n        Returns\n        -------\n        bool\n            True if both have same underlying data, False otherwise.\n\n        See Also\n        --------\n        Index.identical : Works like ``Index.is_`` but also checks metadata.\n        \"\"\"\n        if self is other:\n            return True\n        elif not hasattr(other, \"_id\"):\n            return False\n        elif com.any_none(self._id, other._id):\n            return False\n        else:\n            return self._id is other._id\n\n    def _reset_identity(self) -> None:\n        \"\"\"\n        Initializes or resets ``_id`` attribute with new object.\n        \"\"\"\n        self._id = _Identity(object())\n\n    def _cleanup(self):\n        self._engine.clear_mapping()\n\n    @cache_readonly\n    def _engine(self):\n        # property, for now, slow to look up\n\n        # to avoid a reference cycle, bind `target_values` to a local variable, so\n        # `self` is not passed into the lambda.\n        target_values = self._get_engine_target()\n        return self._engine_type(lambda: target_values, len(self))\n\n    # --------------------------------------------------------------------\n    # Array-Like Methods\n\n    # ndarray compat\n    def __len__(self) -> int:\n        \"\"\"\n        Return the length of the Index.\n        \"\"\"\n        return len(self._data)\n\n    def __array__(self, dtype=None) -> np.ndarray:\n        \"\"\"\n        The array interface, return my values.\n        \"\"\"\n        return np.asarray(self._data, dtype=dtype)\n\n    def __array_wrap__(self, result, context=None):\n        \"\"\"\n        Gets called after a ufunc and other functions.\n        \"\"\"\n        result = lib.item_from_zerodim(result)\n        if is_bool_dtype(result) or lib.is_scalar(result) or np.ndim(result) > 1:\n            return result\n\n        attrs = self._get_attributes_dict()\n        return Index(result, **attrs)\n\n    @cache_readonly\n    def dtype(self):\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._data.dtype\n\n    def ravel(self, order=\"C\"):\n        \"\"\"\n        Return an ndarray of the flattened values of the underlying data.\n\n        Returns\n        -------\n        numpy.ndarray\n            Flattened array.\n\n        See Also\n        --------\n        numpy.ndarray.ravel : Return a flattened array.\n        \"\"\"\n        warnings.warn(\n            \"Index.ravel returning ndarray is deprecated; in a future version \"\n            \"this will return a view on self.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        values = self._get_engine_target()\n        return values.ravel(order=order)\n\n    def view(self, cls=None):\n\n        # we need to see if we are subclassing an\n        # index type here\n        if cls is not None and not hasattr(cls, \"_typ\"):\n            result = self._data.view(cls)\n        else:\n            result = self._shallow_copy()\n        if isinstance(result, Index):\n            result._id = self._id\n        return result\n\n    def astype(self, dtype, copy=True):\n        \"\"\"\n        Create an Index with values cast to dtypes.\n\n        The class of a new Index is determined by dtype. When conversion is\n        impossible, a TypeError exception is raised.\n\n        Parameters\n        ----------\n        dtype : numpy dtype or pandas type\n            Note that any signed integer `dtype` is treated as ``'int64'``,\n            and any unsigned integer `dtype` is treated as ``'uint64'``,\n            regardless of the size.\n        copy : bool, default True\n            By default, astype always returns a newly allocated object.\n            If copy is set to False and internal requirements on dtype are\n            satisfied, the original data is used to create a new Index\n            or the original Index is returned.\n\n        Returns\n        -------\n        Index\n            Index with values cast to specified dtype.\n        \"\"\"\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n\n        if is_dtype_equal(self.dtype, dtype):\n            return self.copy() if copy else self\n\n        elif is_categorical_dtype(dtype):\n            from pandas.core.indexes.category import CategoricalIndex\n\n            return CategoricalIndex(\n                self._values, name=self.name, dtype=dtype, copy=copy\n            )\n\n        elif is_extension_array_dtype(dtype):\n            return Index(np.asarray(self), name=self.name, dtype=dtype, copy=copy)\n\n        try:\n            casted = self._values.astype(dtype, copy=copy)\n        except (TypeError, ValueError) as err:\n            raise TypeError(\n                f\"Cannot cast {type(self).__name__} to dtype {dtype}\"\n            ) from err\n        return Index(casted, name=self.name, dtype=dtype)\n\n    _index_shared_docs[\n        \"take\"\n    ] = \"\"\"\n        Return a new %(klass)s of the values selected by the indices.\n\n        For internal compatibility with numpy arrays.\n\n        Parameters\n        ----------\n        indices : list\n            Indices to be taken.\n        axis : int, optional\n            The axis over which to select values, always 0.\n        allow_fill : bool, default True\n        fill_value : bool, default None\n            If allow_fill=True and fill_value is not None, indices specified by\n            -1 is regarded as NA. If Index doesn't hold NA, raise ValueError.\n\n        Returns\n        -------\n        numpy.ndarray\n            Elements of given indices.\n\n        See Also\n        --------\n        numpy.ndarray.take: Return an array formed from the\n            elements of a at the given indices.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"take\"] % _index_doc_kwargs)\n    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):\n        if kwargs:\n            nv.validate_take(tuple(), kwargs)\n        indices = ensure_platform_int(indices)\n        if self._can_hold_na:\n            taken = self._assert_take_fillable(\n                self._values,\n                indices,\n                allow_fill=allow_fill,\n                fill_value=fill_value,\n                na_value=self._na_value,\n            )\n        else:\n            if allow_fill and fill_value is not None:\n                cls_name = type(self).__name__\n                raise ValueError(\n                    f\"Unable to fill values because {cls_name} cannot contain NA\"\n                )\n            taken = self._values.take(indices)\n        return self._shallow_copy(taken)\n\n    def _assert_take_fillable(\n        self, values, indices, allow_fill=True, fill_value=None, na_value=np.nan\n    ):\n        \"\"\"\n        Internal method to handle NA filling of take.\n        \"\"\"\n        indices = ensure_platform_int(indices)\n\n        # only fill if we are passing a non-None fill_value\n        if allow_fill and fill_value is not None:\n            if (indices < -1).any():\n                raise ValueError(\n                    \"When allow_fill=True and fill_value is not None, \"\n                    \"all indices must be >= -1\"\n                )\n            taken = algos.take(\n                values, indices, allow_fill=allow_fill, fill_value=na_value\n            )\n        else:\n            taken = values.take(indices)\n        return taken\n\n    _index_shared_docs[\n        \"repeat\"\n    ] = \"\"\"\n        Repeat elements of a %(klass)s.\n\n        Returns a new %(klass)s where each element of the current %(klass)s\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            %(klass)s.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        repeated_index : %(klass)s\n            Newly created %(klass)s with repeated elements.\n\n        See Also\n        --------\n        Series.repeat : Equivalent function for Series.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['a', 'b', 'c'])\n        >>> idx\n        Index(['a', 'b', 'c'], dtype='object')\n        >>> idx.repeat(2)\n        Index(['a', 'a', 'b', 'b', 'c', 'c'], dtype='object')\n        >>> idx.repeat([1, 2, 3])\n        Index(['a', 'b', 'b', 'c', 'c', 'c'], dtype='object')\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n    def repeat(self, repeats, axis=None):\n        repeats = ensure_platform_int(repeats)\n        nv.validate_repeat(tuple(), dict(axis=axis))\n        return self._shallow_copy(self._values.repeat(repeats))\n\n    # --------------------------------------------------------------------\n    # Copying Methods\n\n    def copy(\n        self: _IndexT,\n        name: Optional[Label] = None,\n        deep: bool = False,\n        dtype: Optional[Dtype] = None,\n        names: Optional[Sequence[Label]] = None,\n    ) -> _IndexT:\n        \"\"\"\n        Make a copy of this object.\n\n        Name and dtype sets those attributes on the new object.\n\n        Parameters\n        ----------\n        name : Label, optional\n            Set name for new object.\n        deep : bool, default False\n        dtype : numpy dtype or pandas type, optional\n            Set dtype for new object.\n\n            .. deprecated:: 1.2.0\n                use ``astype`` method instead.\n        names : list-like, optional\n            Kept for compatibility with MultiIndex. Should not be used.\n\n        Returns\n        -------\n        Index\n            Index refer to new object which is a copy of this object.\n\n        Notes\n        -----\n        In most cases, there should be no functional difference from using\n        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n        \"\"\"\n        name = self._validate_names(name=name, names=names, deep=deep)[0]\n        if deep:\n            new_index = self._shallow_copy(self._data.copy(), name=name)\n        else:\n            new_index = self._shallow_copy(name=name)\n\n        if dtype:\n            warnings.warn(\n                \"parameter dtype is deprecated and will be removed in a future \"\n                \"version. Use the astype method instead.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n            new_index = new_index.astype(dtype)\n        return new_index\n\n    def __copy__(self, **kwargs):\n        return self.copy(**kwargs)\n\n    def __deepcopy__(self, memo=None):\n        \"\"\"\n        Parameters\n        ----------\n        memo, default None\n            Standard signature. Unused\n        \"\"\"\n        return self.copy(deep=True)\n\n    # --------------------------------------------------------------------\n    # Rendering Methods\n\n    def __repr__(self) -> str_t:\n        \"\"\"\n        Return a string representation for this object.\n        \"\"\"\n        klass_name = type(self).__name__\n        data = self._format_data()\n        attrs = self._format_attrs()\n        space = self._format_space()\n        attrs_str = [f\"{k}={v}\" for k, v in attrs]\n        prepr = f\",{space}\".join(attrs_str)\n\n        # no data provided, just attributes\n        if data is None:\n            data = \"\"\n\n        res = f\"{klass_name}({data}{prepr})\"\n\n        return res\n\n    def _format_space(self) -> str_t:\n\n        # using space here controls if the attributes\n        # are line separated or not (the default)\n\n        # max_seq_items = get_option('display.max_seq_items')\n        # if len(self) > max_seq_items:\n        #    space = \"\\n%s\" % (' ' * (len(klass) + 1))\n        return \" \"\n\n    @property\n    def _formatter_func(self):\n        \"\"\"\n        Return the formatter function.\n        \"\"\"\n        return default_pprint\n\n    def _format_data(self, name=None) -> str_t:\n        \"\"\"\n        Return the formatted data as a unicode string.\n        \"\"\"\n        # do we want to justify (only do so for non-objects)\n        is_justify = True\n\n        if self.inferred_type == \"string\":\n            is_justify = False\n        elif self.inferred_type == \"categorical\":\n            # error: \"Index\" has no attribute \"categories\"\n            if is_object_dtype(self.categories):  # type: ignore[attr-defined]\n                is_justify = False\n\n        return format_object_summary(\n            self, self._formatter_func, is_justify=is_justify, name=name\n        )\n\n    def _format_attrs(self):\n        \"\"\"\n        Return a list of tuples of the (attr,formatted_value).\n        \"\"\"\n        return format_object_attrs(self)\n\n    def _mpl_repr(self):\n        # how to represent ourselves to matplotlib\n        return self.values\n\n    def format(\n        self,\n        name: bool = False,\n        formatter: Optional[Callable] = None,\n        na_rep: str_t = \"NaN\",\n    ) -> List[str_t]:\n        \"\"\"\n        Render a string representation of the Index.\n        \"\"\"\n        header = []\n        if name:\n            header.append(\n                pprint_thing(self.name, escape_chars=(\"\\t\", \"\\r\", \"\\n\"))\n                if self.name is not None\n                else \"\"\n            )\n\n        if formatter is not None:\n            return header + list(self.map(formatter))\n\n        return self._format_with_header(header, na_rep=na_rep)\n\n    def _format_with_header(\n        self, header: List[str_t], na_rep: str_t = \"NaN\"\n    ) -> List[str_t]:\n        from pandas.io.formats.format import format_array\n\n        values = self._values\n\n        if is_object_dtype(values.dtype):\n            values = lib.maybe_convert_objects(values, safe=1)\n\n        if is_object_dtype(values.dtype):\n            result = [pprint_thing(x, escape_chars=(\"\\t\", \"\\r\", \"\\n\")) for x in values]\n\n            # could have nans\n            mask = isna(values)\n            if mask.any():\n                result_arr = np.array(result)\n                result_arr[mask] = na_rep\n                result = result_arr.tolist()\n        else:\n            result = trim_front(format_array(values, None, justify=\"left\"))\n        return header + result\n\n    def to_native_types(self, slicer=None, **kwargs):\n        \"\"\"\n        Format specified values of `self` and return them.\n\n        .. deprecated:: 1.2.0\n\n        Parameters\n        ----------\n        slicer : int, array-like\n            An indexer into `self` that specifies which values\n            are used in the formatting process.\n        kwargs : dict\n            Options for specifying how the values should be formatted.\n            These options include the following:\n\n            1) na_rep : str\n                The value that serves as a placeholder for NULL values\n            2) quoting : bool or None\n                Whether or not there are quoted values in `self`\n            3) date_format : str\n                The format used to represent date-like values.\n\n        Returns\n        -------\n        numpy.ndarray\n            Formatted values.\n        \"\"\"\n        warnings.warn(\n            \"The 'to_native_types' method is deprecated and will be removed in \"\n            \"a future version. Use 'astype(str)' instead.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        values = self\n        if slicer is not None:\n            values = values[slicer]\n        return values._format_native_types(**kwargs)\n\n    def _format_native_types(self, na_rep=\"\", quoting=None, **kwargs):\n        \"\"\"\n        Actually format specific types of the index.\n        \"\"\"\n        mask = isna(self)\n        if not self.is_object() and not quoting:\n            values = np.asarray(self).astype(str)\n        else:\n            values = np.array(self, dtype=object, copy=True)\n\n        values[mask] = na_rep\n        return values\n\n    def _summary(self, name=None) -> str_t:\n        \"\"\"\n        Return a summarized representation.\n\n        Parameters\n        ----------\n        name : str\n            name to use in the summary representation\n\n        Returns\n        -------\n        String with a summarized representation of the index\n        \"\"\"\n        if len(self) > 0:\n            head = self[0]\n            if hasattr(head, \"format\") and not isinstance(head, str):\n                head = head.format()\n            tail = self[-1]\n            if hasattr(tail, \"format\") and not isinstance(tail, str):\n                tail = tail.format()\n            index_summary = f\", {head} to {tail}\"\n        else:\n            index_summary = \"\"\n\n        if name is None:\n            name = type(self).__name__\n        return f\"{name}: {len(self)} entries{index_summary}\"\n\n    # --------------------------------------------------------------------\n    # Conversion Methods\n\n    def to_flat_index(self):\n        \"\"\"\n        Identity method.\n\n        .. versionadded:: 0.24.0\n\n        This is implemented for compatibility with subclass implementations\n        when chaining.\n\n        Returns\n        -------\n        pd.Index\n            Caller.\n\n        See Also\n        --------\n        MultiIndex.to_flat_index : Subclass implementation.\n        \"\"\"\n        return self\n\n    def to_series(self, index=None, name=None):\n        \"\"\"\n        Create a Series with both index and values equal to the index keys.\n\n        Useful with map for returning an indexer based on an index.\n\n        Parameters\n        ----------\n        index : Index, optional\n            Index of resulting Series. If None, defaults to original index.\n        name : str, optional\n            Name of resulting Series. If None, defaults to name of original\n            index.\n\n        Returns\n        -------\n        Series\n            The dtype will be based on the type of the Index values.\n\n        See Also\n        --------\n        Index.to_frame : Convert an Index to a DataFrame.\n        Series.to_frame : Convert Series to DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n\n        By default, the original Index and original name is reused.\n\n        >>> idx.to_series()\n        animal\n        Ant      Ant\n        Bear    Bear\n        Cow      Cow\n        Name: animal, dtype: object\n\n        To enforce a new Index, specify new labels to ``index``:\n\n        >>> idx.to_series(index=[0, 1, 2])\n        0     Ant\n        1    Bear\n        2     Cow\n        Name: animal, dtype: object\n\n        To override the name of the resulting column, specify `name`:\n\n        >>> idx.to_series(name='zoo')\n        animal\n        Ant      Ant\n        Bear    Bear\n        Cow      Cow\n        Name: zoo, dtype: object\n        \"\"\"\n        from pandas import Series\n\n        if index is None:\n            index = self._shallow_copy()\n        if name is None:\n            name = self.name\n\n        return Series(self.values.copy(), index=index, name=name)\n\n    def to_frame(self, index: bool = True, name=None):\n        \"\"\"\n        Create a DataFrame with a column containing the Index.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        index : bool, default True\n            Set the index of the returned DataFrame as the original Index.\n\n        name : object, default None\n            The passed name should substitute for the index name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame containing the original Index data.\n\n        See Also\n        --------\n        Index.to_series : Convert an Index to a Series.\n        Series.to_frame : Convert Series to DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n        >>> idx.to_frame()\n               animal\n        animal\n        Ant       Ant\n        Bear     Bear\n        Cow       Cow\n\n        By default, the original Index is reused. To enforce a new Index:\n\n        >>> idx.to_frame(index=False)\n            animal\n        0   Ant\n        1  Bear\n        2   Cow\n\n        To override the name of the resulting column, specify `name`:\n\n        >>> idx.to_frame(index=False, name='zoo')\n            zoo\n        0   Ant\n        1  Bear\n        2   Cow\n        \"\"\"\n        from pandas import DataFrame\n\n        if name is None:\n            name = self.name or 0\n        result = DataFrame({name: self._values.copy()})\n\n        if index:\n            result.index = self\n        return result\n\n    # --------------------------------------------------------------------\n    # Name-Centric Methods\n\n    @property\n    def name(self):\n        \"\"\"\n        Return Index or MultiIndex name.\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        if self._no_setting_name:\n            # Used in MultiIndex.levels to avoid silently ignoring name updates.\n            raise RuntimeError(\n                \"Cannot set name on a level of a MultiIndex. Use \"\n                \"'MultiIndex.set_names' instead.\"\n            )\n        maybe_extract_name(value, None, type(self))\n        self._name = value\n\n    def _validate_names(self, name=None, names=None, deep: bool = False) -> List[Label]:\n        \"\"\"\n        Handles the quirks of having a singular 'name' parameter for general\n        Index and plural 'names' parameter for MultiIndex.\n        \"\"\"\n        from copy import deepcopy\n\n        if names is not None and name is not None:\n            raise TypeError(\"Can only provide one of `names` and `name`\")\n        elif names is None and name is None:\n            new_names = deepcopy(self.names) if deep else self.names\n        elif names is not None:\n            if not is_list_like(names):\n                raise TypeError(\"Must pass list-like as `names`.\")\n            new_names = names\n        elif not is_list_like(name):\n            new_names = [name]\n        else:\n            new_names = name\n\n        if len(new_names) != len(self.names):\n            raise ValueError(\n                f\"Length of new names must be {len(self.names)}, got {len(new_names)}\"\n            )\n\n        # All items in 'new_names' need to be hashable\n        validate_all_hashable(*new_names, error_name=f\"{type(self).__name__}.name\")\n\n        return new_names\n\n    def _get_names(self):\n        return FrozenList((self.name,))\n\n    def _set_names(self, values, level=None):\n        \"\"\"\n        Set new names on index. Each name has to be a hashable type.\n\n        Parameters\n        ----------\n        values : str or sequence\n            name(s) to set\n        level : int, level name, or sequence of int/level names (default None)\n            If the index is a MultiIndex (hierarchical), level(s) to set (None\n            for all levels).  Otherwise level must be None\n\n        Raises\n        ------\n        TypeError if each name is not hashable.\n        \"\"\"\n        if not is_list_like(values):\n            raise ValueError(\"Names must be a list-like\")\n        if len(values) != 1:\n            raise ValueError(f\"Length of new names must be 1, got {len(values)}\")\n\n        # GH 20527\n        # All items in 'name' need to be hashable:\n        validate_all_hashable(*values, error_name=f\"{type(self).__name__}.name\")\n\n        self._name = values[0]\n\n    names = property(fset=_set_names, fget=_get_names)\n\n    def set_names(self, names, level=None, inplace: bool = False):\n        \"\"\"\n        Set Index or MultiIndex name.\n\n        Able to set new names partially and by level.\n\n        Parameters\n        ----------\n        names : label or list of label\n            Name(s) to set.\n        level : int, label or list of int or label, optional\n            If the index is a MultiIndex, level(s) to set (None for all\n            levels). Otherwise level must be None.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index or None\n            The same type as the caller or None if ``inplace=True``.\n\n        See Also\n        --------\n        Index.rename : Able to set new names without level.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx\n        Int64Index([1, 2, 3, 4], dtype='int64')\n        >>> idx.set_names('quarter')\n        Int64Index([1, 2, 3, 4], dtype='int64', name='quarter')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]])\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   )\n        >>> idx.set_names(['kind', 'year'], inplace=True)\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['kind', 'year'])\n        >>> idx.set_names('species', level=0)\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['species', 'year'])\n        \"\"\"\n        if level is not None and not isinstance(self, ABCMultiIndex):\n            raise ValueError(\"Level must be None for non-MultiIndex\")\n\n        if level is not None and not is_list_like(level) and is_list_like(names):\n            raise TypeError(\"Names must be a string when a single level is provided.\")\n\n        if not is_list_like(names) and level is None and self.nlevels > 1:\n            raise TypeError(\"Must pass list-like as `names`.\")\n\n        if not is_list_like(names):\n            names = [names]\n        if level is not None and not is_list_like(level):\n            level = [level]\n\n        if inplace:\n            idx = self\n        else:\n            idx = self._shallow_copy()\n        idx._set_names(names, level=level)\n        if not inplace:\n            return idx\n\n    def rename(self, name, inplace=False):\n        \"\"\"\n        Alter Index or MultiIndex name.\n\n        Able to set new names without level. Defaults to returning new index.\n        Length of names must match number of levels in MultiIndex.\n\n        Parameters\n        ----------\n        name : label or list of labels\n            Name(s) to set.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index or None\n            The same type as the caller or None if ``inplace=True``.\n\n        See Also\n        --------\n        Index.set_names : Able to set new names partially and by level.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['A', 'C', 'A', 'B'], name='score')\n        >>> idx.rename('grade')\n        Index(['A', 'C', 'A', 'B'], dtype='object', name='grade')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]],\n        ...                                   names=['kind', 'year'])\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['kind', 'year'])\n        >>> idx.rename(['species', 'year'])\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['species', 'year'])\n        >>> idx.rename('species')\n        Traceback (most recent call last):\n        TypeError: Must pass list-like as `names`.\n        \"\"\"\n        return self.set_names([name], inplace=inplace)\n\n    # --------------------------------------------------------------------\n    # Level-Centric Methods\n\n    @property\n    def nlevels(self) -> int:\n        \"\"\"\n        Number of levels.\n        \"\"\"\n        return 1\n\n    def _sort_levels_monotonic(self):\n        \"\"\"\n        Compat with MultiIndex.\n        \"\"\"\n        return self\n\n    def _validate_index_level(self, level):\n        \"\"\"\n        Validate index level.\n\n        For single-level Index getting level number is a no-op, but some\n        verification must be done like in MultiIndex.\n\n        \"\"\"\n        if isinstance(level, int):\n            if level < 0 and level != -1:\n                raise IndexError(\n                    \"Too many levels: Index has only 1 level, \"\n                    f\"{level} is not a valid level number\"\n                )\n            elif level > 0:\n                raise IndexError(\n                    f\"Too many levels: Index has only 1 level, not {level + 1}\"\n                )\n        elif level != self.name:\n            raise KeyError(\n                f\"Requested level ({level}) does not match index name ({self.name})\"\n            )\n\n    def _get_level_number(self, level) -> int:\n        self._validate_index_level(level)\n        return 0\n\n    def sortlevel(self, level=None, ascending=True, sort_remaining=None):\n        \"\"\"\n        For internal compatibility with with the Index API.\n\n        Sort the Index. This is for compat with MultiIndex\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            False to sort in descending order\n\n        level, sort_remaining are compat parameters\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        if not isinstance(ascending, (list, bool)):\n            raise TypeError(\n                \"ascending must be a single bool value or\"\n                \"a list of bool values of length 1\"\n            )\n\n        if isinstance(ascending, list):\n            if len(ascending) != 1:\n                raise TypeError(\"ascending must be a list of bool values of length 1\")\n            ascending = ascending[0]\n\n        if not isinstance(ascending, bool):\n            raise TypeError(\"ascending must be a bool value\")\n\n        return self.sort_values(return_indexer=True, ascending=ascending)\n\n    def _get_level_values(self, level):\n        \"\"\"\n        Return an Index of values for requested level.\n\n        This is primarily useful to get an individual level of values from a\n        MultiIndex, but is provided on Index as well for compatibility.\n\n        Parameters\n        ----------\n        level : int or str\n            It is either the integer position or the name of the level.\n\n        Returns\n        -------\n        Index\n            Calling object, as there is only one level in the Index.\n\n        See Also\n        --------\n        MultiIndex.get_level_values : Get values for a level of a MultiIndex.\n\n        Notes\n        -----\n        For Index, level should be 0, since there are no multiple levels.\n\n        Examples\n        --------\n        >>> idx = pd.Index(list('abc'))\n        >>> idx\n        Index(['a', 'b', 'c'], dtype='object')\n\n        Get level values by supplying `level` as integer:\n\n        >>> idx.get_level_values(0)\n        Index(['a', 'b', 'c'], dtype='object')\n        \"\"\"\n        self._validate_index_level(level)\n        return self\n\n    get_level_values = _get_level_values\n\n    def droplevel(self, level=0):\n        \"\"\"\n        Return index with requested level(s) removed.\n\n        If resulting index has only 1 level left, the result will be\n        of Index type, not MultiIndex.\n\n        Parameters\n        ----------\n        level : int, str, or list-like, default 0\n            If a string is given, must be the name of a level\n            If list-like, elements must be names or indexes of levels.\n\n        Returns\n        -------\n        Index or MultiIndex\n        \"\"\"\n        if not isinstance(level, (tuple, list)):\n            level = [level]\n\n        levnums = sorted(self._get_level_number(lev) for lev in level)[::-1]\n\n        return self._drop_level_numbers(levnums)\n\n    def _drop_level_numbers(self, levnums: List[int]):\n        \"\"\"\n        Drop MultiIndex levels by level _number_, not name.\n        \"\"\"\n\n        if len(levnums) == 0:\n            return self\n        if len(levnums) >= self.nlevels:\n            raise ValueError(\n                f\"Cannot remove {len(levnums)} levels from an index with \"\n                f\"{self.nlevels} levels: at least one level must be left.\"\n            )\n        # The two checks above guarantee that here self is a MultiIndex\n        self = cast(\"MultiIndex\", self)\n\n        new_levels = list(self.levels)\n        new_codes = list(self.codes)\n        new_names = list(self.names)\n\n        for i in levnums:\n            new_levels.pop(i)\n            new_codes.pop(i)\n            new_names.pop(i)\n\n        if len(new_levels) == 1:\n\n            # set nan if needed\n            mask = new_codes[0] == -1\n            result = new_levels[0].take(new_codes[0])\n            if mask.any():\n                result = result.putmask(mask, np.nan)\n\n            result._name = new_names[0]\n            return result\n        else:\n            from pandas.core.indexes.multi import MultiIndex\n\n            return MultiIndex(\n                levels=new_levels,\n                codes=new_codes,\n                names=new_names,\n                verify_integrity=False,\n            )\n\n    def _get_grouper_for_level(self, mapper, level=None):\n        \"\"\"\n        Get index grouper corresponding to an index level\n\n        Parameters\n        ----------\n        mapper: Group mapping function or None\n            Function mapping index values to groups\n        level : int or None\n            Index level\n\n        Returns\n        -------\n        grouper : Index\n            Index of values to group on.\n        labels : ndarray of int or None\n            Array of locations in level_index.\n        uniques : Index or None\n            Index of unique values for level.\n        \"\"\"\n        assert level is None or level == 0\n        if mapper is None:\n            grouper = self\n        else:\n            grouper = self.map(mapper)\n\n        return grouper, None, None\n\n    # --------------------------------------------------------------------\n    # Introspection Methods\n\n    @property\n    def is_monotonic(self) -> bool:\n        \"\"\"\n        Alias for is_monotonic_increasing.\n        \"\"\"\n        return self.is_monotonic_increasing\n\n    @property\n    def is_monotonic_increasing(self) -> bool:\n        \"\"\"\n        Return if the index is monotonic increasing (only equal or\n        increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3]).is_monotonic_increasing\n        True\n        >>> Index([1, 2, 2]).is_monotonic_increasing\n        True\n        >>> Index([1, 3, 2]).is_monotonic_increasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_increasing\n\n    @property\n    def is_monotonic_decreasing(self) -> bool:\n        \"\"\"\n        Return if the index is monotonic decreasing (only equal or\n        decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1]).is_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2]).is_monotonic_decreasing\n        True\n        >>> Index([3, 1, 2]).is_monotonic_decreasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_decreasing\n\n    @property\n    def _is_strictly_monotonic_increasing(self) -> bool:\n        \"\"\"\n        Return if the index is strictly monotonic increasing\n        (only increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3])._is_strictly_monotonic_increasing\n        True\n        >>> Index([1, 2, 2])._is_strictly_monotonic_increasing\n        False\n        >>> Index([1, 3, 2])._is_strictly_monotonic_increasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_increasing\n\n    @property\n    def _is_strictly_monotonic_decreasing(self) -> bool:\n        \"\"\"\n        Return if the index is strictly monotonic decreasing\n        (only decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1])._is_strictly_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2])._is_strictly_monotonic_decreasing\n        False\n        >>> Index([3, 1, 2])._is_strictly_monotonic_decreasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_decreasing\n\n    @cache_readonly\n    def is_unique(self) -> bool:\n        \"\"\"\n        Return if the index has unique values.\n        \"\"\"\n        return self._engine.is_unique\n\n    @property\n    def has_duplicates(self) -> bool:\n        \"\"\"\n        Check if the Index has duplicate values.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index has duplicate values.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 5, 7, 7])\n        >>> idx.has_duplicates\n        True\n\n        >>> idx = pd.Index([1, 5, 7])\n        >>> idx.has_duplicates\n        False\n\n        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.has_duplicates\n        True\n\n        >>> idx = pd.Index([\"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.has_duplicates\n        False\n        \"\"\"\n        return not self.is_unique\n\n    def is_boolean(self) -> bool:\n        \"\"\"\n        Check if the Index only consists of booleans.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index only consists of booleans.\n\n        See Also\n        --------\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([True, False, True])\n        >>> idx.is_boolean()\n        True\n\n        >>> idx = pd.Index([\"True\", \"False\", \"True\"])\n        >>> idx.is_boolean()\n        False\n\n        >>> idx = pd.Index([True, False, \"True\"])\n        >>> idx.is_boolean()\n        False\n        \"\"\"\n        return self.inferred_type in [\"boolean\"]\n\n    def is_integer(self) -> bool:\n        \"\"\"\n        Check if the Index only consists of integers.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index only consists of integers.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx.is_integer()\n        True\n\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n        >>> idx.is_integer()\n        False\n\n        >>> idx = pd.Index([\"Apple\", \"Mango\", \"Watermelon\"])\n        >>> idx.is_integer()\n        False\n        \"\"\"\n        return self.inferred_type in [\"integer\"]\n\n    def is_floating(self) -> bool:\n        \"\"\"\n        Check if the Index is a floating type.\n\n        The Index may consist of only floats, NaNs, or a mix of floats,\n        integers, or NaNs.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index only consists of only consists of floats, NaNs, or\n            a mix of floats, integers, or NaNs.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n        >>> idx.is_floating()\n        True\n\n        >>> idx = pd.Index([1.0, 2.0, np.nan, 4.0])\n        >>> idx.is_floating()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4, np.nan])\n        >>> idx.is_floating()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx.is_floating()\n        False\n        \"\"\"\n        return self.inferred_type in [\"floating\", \"mixed-integer-float\", \"integer-na\"]\n\n    def is_numeric(self) -> bool:\n        \"\"\"\n        Check if the Index only consists of numeric data.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index only consists of numeric data.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n        >>> idx.is_numeric()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4.0])\n        >>> idx.is_numeric()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx.is_numeric()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4.0, np.nan])\n        >>> idx.is_numeric()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4.0, np.nan, \"Apple\"])\n        >>> idx.is_numeric()\n        False\n        \"\"\"\n        return self.inferred_type in [\"integer\", \"floating\"]\n\n    def is_object(self) -> bool:\n        \"\"\"\n        Check if the Index is of the object dtype.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index is of the object dtype.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([\"Apple\", \"Mango\", \"Watermelon\"])\n        >>> idx.is_object()\n        True\n\n        >>> idx = pd.Index([\"Apple\", \"Mango\", 2.0])\n        >>> idx.is_object()\n        True\n\n        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.is_object()\n        False\n\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n        >>> idx.is_object()\n        False\n        \"\"\"\n        return is_object_dtype(self.dtype)\n\n    def is_categorical(self) -> bool:\n        \"\"\"\n        Check if the Index holds categorical data.\n\n        Returns\n        -------\n        bool\n            True if the Index is categorical.\n\n        See Also\n        --------\n        CategoricalIndex : Index for categorical data.\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.is_categorical()\n        True\n\n        >>> idx = pd.Index([1, 3, 5, 7])\n        >>> idx.is_categorical()\n        False\n\n        >>> s = pd.Series([\"Peter\", \"Victor\", \"Elisabeth\", \"Mar\"])\n        >>> s\n        0        Peter\n        1       Victor\n        2    Elisabeth\n        3          Mar\n        dtype: object\n        >>> s.index.is_categorical()\n        False\n        \"\"\"\n        return self.inferred_type in [\"categorical\"]\n\n    def is_interval(self) -> bool:\n        \"\"\"\n        Check if the Index holds Interval objects.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index holds Interval objects.\n\n        See Also\n        --------\n        IntervalIndex : Index for Interval objects.\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([pd.Interval(left=0, right=5),\n        ...                 pd.Interval(left=5, right=10)])\n        >>> idx.is_interval()\n        True\n\n        >>> idx = pd.Index([1, 3, 5, 7])\n        >>> idx.is_interval()\n        False\n        \"\"\"\n        return self.inferred_type in [\"interval\"]\n\n    def is_mixed(self) -> bool:\n        \"\"\"\n        Check if the Index holds data with mixed data types.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index holds data with mixed data types.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['a', np.nan, 'b'])\n        >>> idx.is_mixed()\n        True\n\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 5.0])\n        >>> idx.is_mixed()\n        False\n        \"\"\"\n        warnings.warn(\n            \"Index.is_mixed is deprecated and will be removed in a future version. \"\n            \"Check index.inferred_type directly instead.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return self.inferred_type in [\"mixed\"]\n\n    def holds_integer(self) -> bool:\n        \"\"\"\n        Whether the type is an integer type.\n        \"\"\"\n        return self.inferred_type in [\"integer\", \"mixed-integer\"]\n\n    @cache_readonly\n    def inferred_type(self) -> str_t:\n        \"\"\"\n        Return a string of the type inferred from the values.\n        \"\"\"\n        return lib.infer_dtype(self._values, skipna=False)\n\n    @cache_readonly\n    def _is_all_dates(self) -> bool:\n        \"\"\"\n        Whether or not the index values only consist of dates.\n        \"\"\"\n        return is_datetime_array(ensure_object(self._values))\n\n    @cache_readonly\n    def is_all_dates(self):\n        \"\"\"\n        Whether or not the index values only consist of dates.\n        \"\"\"\n        warnings.warn(\n            \"Index.is_all_dates is deprecated, will be removed in a future version.  \"\n            \"check index.inferred_type instead\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return self._is_all_dates\n\n    # --------------------------------------------------------------------\n    # Pickle Methods\n\n    def __reduce__(self):\n        d = dict(data=self._data)\n        d.update(self._get_attributes_dict())\n        return _new_Index, (type(self), d), None\n\n    # --------------------------------------------------------------------\n    # Null Handling Methods\n\n    _na_value = np.nan\n    \"\"\"The expected NA value to use with this index.\"\"\"\n\n    @cache_readonly\n    def _isnan(self):\n        \"\"\"\n        Return if each value is NaN.\n        \"\"\"\n        if self._can_hold_na:\n            return isna(self)\n        else:\n            # shouldn't reach to this condition by checking hasnans beforehand\n            values = np.empty(len(self), dtype=np.bool_)\n            values.fill(False)\n            return values\n\n    @cache_readonly\n    def _nan_idxs(self):\n        if self._can_hold_na:\n            return self._isnan.nonzero()[0]\n        else:\n            return np.array([], dtype=np.int64)\n\n    @cache_readonly\n    def hasnans(self) -> bool:\n        \"\"\"\n        Return if I have any nans; enables various perf speedups.\n        \"\"\"\n        if self._can_hold_na:\n            return bool(self._isnan.any())\n        else:\n            return False\n\n    def isna(self):\n        \"\"\"\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`pd.NaT`, get\n        mapped to ``True`` values.\n        Everything else get mapped to ``False`` values. Characters such as\n        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n\n        Returns\n        -------\n        numpy.ndarray\n            A boolean array of whether my values are NA.\n\n        See Also\n        --------\n        Index.notna : Boolean inverse of isna.\n        Index.dropna : Omit entries with missing values.\n        isna : Top-level isna.\n        Series.isna : Detect missing values in Series object.\n\n        Examples\n        --------\n        Show which entries in a pandas.Index are NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.isna()\n        array([False, False,  True])\n\n        Empty strings are not considered NA values. None is considered an NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.isna()\n        array([False, False, False,  True])\n\n        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n\n        >>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n        ...                         pd.Timestamp(''), None, pd.NaT])\n        >>> idx\n        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n                      dtype='datetime64[ns]', freq=None)\n        >>> idx.isna()\n        array([False,  True,  True,  True])\n        \"\"\"\n        return self._isnan\n\n    isnull = isna\n\n    def notna(self):\n        \"\"\"\n        Detect existing (non-missing) values.\n\n        Return a boolean same-sized object indicating if the values are not NA.\n        Non-missing values get mapped to ``True``. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n        NA values, such as None or :attr:`numpy.NaN`, get mapped to ``False``\n        values.\n\n        Returns\n        -------\n        numpy.ndarray\n            Boolean array to indicate which entries are not NA.\n\n        See Also\n        --------\n        Index.notnull : Alias of notna.\n        Index.isna: Inverse of notna.\n        notna : Top-level notna.\n\n        Examples\n        --------\n        Show which entries in an Index are not NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.notna()\n        array([ True,  True, False])\n\n        Empty strings are not considered NA values. None is considered a NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.notna()\n        array([ True,  True,  True, False])\n        \"\"\"\n        return ~self.isna()\n\n    notnull = notna\n\n    def fillna(self, value=None, downcast=None):\n        \"\"\"\n        Fill NA/NaN values with the specified value.\n\n        Parameters\n        ----------\n        value : scalar\n            Scalar value to use to fill holes (e.g. 0).\n            This value cannot be a list-likes.\n        downcast : dict, default is None\n            A dict of item->dtype of what to downcast if possible,\n            or the string 'infer' which will try to downcast to an appropriate\n            equal type (e.g. float64 to int64 if possible).\n\n        Returns\n        -------\n        Index\n\n        See Also\n        --------\n        DataFrame.fillna : Fill NaN values of a DataFrame.\n        Series.fillna : Fill NaN Values of a Series.\n        \"\"\"\n        value = self._validate_scalar(value)\n        if self.hasnans:\n            result = self.putmask(self._isnan, value)\n            if downcast is None:\n                # no need to care metadata other than name\n                # because it can't have freq if\n                return Index(result, name=self.name)\n        return self._shallow_copy()\n\n    def dropna(self, how=\"any\"):\n        \"\"\"\n        Return Index without NA/NaN values.\n\n        Parameters\n        ----------\n        how : {'any', 'all'}, default 'any'\n            If the Index is a MultiIndex, drop the value when any or all levels\n            are NaN.\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        if how not in (\"any\", \"all\"):\n            raise ValueError(f\"invalid how option: {how}\")\n\n        if self.hasnans:\n            return self._shallow_copy(self._values[~self._isnan])\n        return self._shallow_copy()\n\n    # --------------------------------------------------------------------\n    # Uniqueness Methods\n\n    def unique(self, level=None):\n        \"\"\"\n        Return unique values in the index.\n\n        Unique values are returned in order of appearance, this does NOT sort.\n\n        Parameters\n        ----------\n        level : int or str, optional, default None\n            Only return values from specified level (for MultiIndex).\n\n        Returns\n        -------\n        Index without duplicates\n\n        See Also\n        --------\n        unique : Numpy array of unique values in that column.\n        Series.unique : Return unique values of Series object.\n        \"\"\"\n        if level is not None:\n            self._validate_index_level(level)\n        result = super().unique()\n        return self._shallow_copy(result)\n\n    def drop_duplicates(self, keep=\"first\"):\n        \"\"\"\n        Return Index with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        Returns\n        -------\n        deduplicated : Index\n\n        See Also\n        --------\n        Series.drop_duplicates : Equivalent method on Series.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Index.duplicated : Related method on Index, indicating duplicate\n            Index values.\n\n        Examples\n        --------\n        Generate an pandas.Index with duplicate values.\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])\n\n        The `keep` parameter controls  which duplicate values are removed.\n        The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> idx.drop_duplicates(keep='first')\n        Index(['lama', 'cow', 'beetle', 'hippo'], dtype='object')\n\n        The value 'last' keeps the last occurrence for each set of duplicated\n        entries.\n\n        >>> idx.drop_duplicates(keep='last')\n        Index(['cow', 'beetle', 'lama', 'hippo'], dtype='object')\n\n        The value ``False`` discards all sets of duplicated entries.\n\n        >>> idx.drop_duplicates(keep=False)\n        Index(['cow', 'beetle', 'hippo'], dtype='object')\n        \"\"\"\n        if self.is_unique:\n            return self._shallow_copy()\n\n        return super().drop_duplicates(keep=keep)\n\n    def duplicated(self, keep=\"first\"):\n        \"\"\"\n        Indicate duplicate index values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        array. Either all duplicates, all except the first, or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            The value or values in a set of duplicates to mark as missing.\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        numpy.ndarray\n\n        See Also\n        --------\n        Series.duplicated : Equivalent method on pandas.Series.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Index.drop_duplicates : Remove duplicate values from Index.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set to False and all others to True:\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> idx.duplicated()\n        array([False, False,  True, False,  True])\n\n        which is equivalent to\n\n        >>> idx.duplicated(keep='first')\n        array([False, False,  True, False,  True])\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> idx.duplicated(keep='last')\n        array([ True, False,  True, False, False])\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> idx.duplicated(keep=False)\n        array([ True, False,  True, False,  True])\n        \"\"\"\n        if self.is_unique:\n            # fastpath available bc we are immutable\n            return np.zeros(len(self), dtype=bool)\n        return super().duplicated(keep=keep)\n\n    def _get_unique_index(self, dropna: bool = False):\n        \"\"\"\n        Returns an index containing unique values.\n\n        Parameters\n        ----------\n        dropna : bool, default False\n            If True, NaN values are dropped.\n\n        Returns\n        -------\n        uniques : index\n        \"\"\"\n        if self.is_unique and not dropna:\n            return self\n\n        if not self.is_unique:\n            values = self.unique()\n            if not isinstance(self, ABCMultiIndex):\n                # extract an array to pass to _shallow_copy\n                values = values._data\n        else:\n            values = self._values\n\n        if dropna:\n            try:\n                if self.hasnans:\n                    values = values[~isna(values)]\n            except NotImplementedError:\n                pass\n\n        return self._shallow_copy(values)\n\n    # --------------------------------------------------------------------\n    # Arithmetic & Logical Methods\n\n    def __iadd__(self, other):\n        # alias for __add__\n        return self + other\n\n    def __and__(self, other):\n        return self.intersection(other)\n\n    def __or__(self, other):\n        return self.union(other)\n\n    def __xor__(self, other):\n        return self.symmetric_difference(other)\n\n    def __nonzero__(self):\n        raise ValueError(\n            f\"The truth value of a {type(self).__name__} is ambiguous. \"\n            \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n        )\n\n    __bool__ = __nonzero__\n\n    # --------------------------------------------------------------------\n    # Set Operation Methods\n\n    def _get_reconciled_name_object(self, other):\n        \"\"\"\n        If the result of a set operation will be self,\n        return self, unless the name changes, in which\n        case make a shallow copy of self.\n        \"\"\"\n        name = get_op_result_name(self, other)\n        if self.name != name:\n            return self.rename(name)\n        return self\n\n    def _union_incompatible_dtypes(self, other, sort):\n        \"\"\"\n        Casts this and other index to object dtype to allow the formation\n        of a union between incompatible types.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        this = self.astype(object, copy=False)\n        # cast to Index for when `other` is list-like\n        other = Index(other).astype(object, copy=False)\n        return Index.union(this, other, sort=sort).astype(object, copy=False)\n\n    def _can_union_without_object_cast(self, other) -> bool:\n        \"\"\"\n        Check whether this and the other dtype are compatible with each other.\n        Meaning a union can be formed between them without needing to be cast\n        to dtype object.\n\n        Parameters\n        ----------\n        other : Index or array-like\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return type(self) is type(other) and is_dtype_equal(self.dtype, other.dtype)\n\n    def _validate_sort_keyword(self, sort):\n        if sort not in [None, False]:\n            raise ValueError(\n                \"The 'sort' keyword only takes the values of \"\n                f\"None or False; {sort} was passed.\"\n            )\n\n    def union(self, other, sort=None):\n        \"\"\"\n        Form the union of two Index objects.\n\n        If the Index objects are incompatible, both Index objects will be\n        cast to dtype('object') first.\n\n            .. versionchanged:: 0.25.0\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : bool or None, default None\n            Whether to sort the resulting Index.\n\n            * None : Sort the result, except when\n\n              1. `self` and `other` are equal.\n              2. `self` or `other` has length 0.\n              3. Some values in `self` or `other` cannot be compared.\n                 A RuntimeWarning is issued in this case.\n\n            * False : do not sort the result.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        union : Index\n\n        Examples\n        --------\n        Union matching dtypes\n\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.union(idx2)\n        Int64Index([1, 2, 3, 4, 5, 6], dtype='int64')\n\n        Union mismatched dtypes\n\n        >>> idx1 = pd.Index(['a', 'b', 'c', 'd'])\n        >>> idx2 = pd.Index([1, 2, 3, 4])\n        >>> idx1.union(idx2)\n        Index(['a', 'b', 'c', 'd', 1, 2, 3, 4], dtype='object')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other = ensure_index(other)\n\n        if not self._can_union_without_object_cast(other):\n            return self._union_incompatible_dtypes(other, sort=sort)\n\n        result = self._union(other, sort=sort)\n\n        return self._wrap_setop_result(other, result)\n\n    def _union(self, other, sort):\n        \"\"\"\n        Specific union logic should go here. In subclasses, union behavior\n        should be overwritten here rather than in `self.union`.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        if not len(other) or self.equals(other):\n            return self\n\n        if not len(self):\n            return other\n\n        # TODO(EA): setops-refactor, clean all this up\n        lvals = self._values\n        rvals = other._values\n\n        if sort is None and self.is_monotonic and other.is_monotonic:\n            try:\n                result = self._outer_indexer(lvals, rvals)[0]\n            except TypeError:\n                # incomparable objects\n                result = list(lvals)\n\n                # worth making this faster? a very unusual case\n                value_set = set(lvals)\n                result.extend([x for x in rvals if x not in value_set])\n                result = Index(result)._values  # do type inference here\n        else:\n            # find indexes of things in \"other\" that are not in \"self\"\n            if self.is_unique:\n                indexer = self.get_indexer(other)\n                indexer = (indexer == -1).nonzero()[0]\n            else:\n                indexer = algos.unique1d(self.get_indexer_non_unique(other)[1])\n\n            if len(indexer) > 0:\n                other_diff = algos.take_nd(rvals, indexer, allow_fill=False)\n                result = concat_compat((lvals, other_diff))\n\n            else:\n                result = lvals\n\n            if sort is None:\n                try:\n                    result = algos.safe_sort(result)\n                except TypeError as err:\n                    warnings.warn(\n                        f\"{err}, sort order is undefined for incomparable objects\",\n                        RuntimeWarning,\n                        stacklevel=3,\n                    )\n\n        return self._shallow_copy(result)\n\n    def _wrap_setop_result(self, other, result):\n        if isinstance(self, (ABCDatetimeIndex, ABCTimedeltaIndex)) and isinstance(\n            result, np.ndarray\n        ):\n            result = type(self._data)._simple_new(result, dtype=self.dtype)\n\n        name = get_op_result_name(self, other)\n        if isinstance(result, Index):\n            if result.name != name:\n                return result.rename(name)\n            return result\n        else:\n            return self._shallow_copy(result, name=name)\n\n    # TODO: standardize return type of non-union setops type(self vs other)\n    def intersection(self, other, sort=False):\n        \"\"\"\n        Form the intersection of two Index objects.\n\n        This returns a new Index with elements common to the index and `other`.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default from ``True`` to ``False``, to match\n               the behaviour of 0.23.4 and earlier.\n\n        Returns\n        -------\n        intersection : Index\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.intersection(idx2)\n        Int64Index([3, 4], dtype='int64')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other = ensure_index(other)\n\n        if self.equals(other):\n            return self._get_reconciled_name_object(other)\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            this = self.astype(\"O\")\n            other = other.astype(\"O\")\n            return this.intersection(other, sort=sort)\n\n        # TODO(EA): setops-refactor, clean all this up\n        lvals = self._values\n        rvals = other._values\n\n        if self.is_monotonic and other.is_monotonic:\n            try:\n                result = self._inner_indexer(lvals, rvals)[0]\n            except TypeError:\n                pass\n            else:\n                return self._wrap_setop_result(other, result)\n\n        try:\n            indexer = Index(rvals).get_indexer(lvals)\n            indexer = indexer.take((indexer != -1).nonzero()[0])\n        except (InvalidIndexError, IncompatibleFrequency):\n            # InvalidIndexError raised by get_indexer if non-unique\n            # IncompatibleFrequency raised by PeriodIndex.get_indexer\n            indexer = algos.unique1d(Index(rvals).get_indexer_non_unique(lvals)[0])\n            indexer = indexer[indexer != -1]\n\n        result = other.take(indexer)._values\n\n        if sort is None:\n            result = algos.safe_sort(result)\n\n        return self._wrap_setop_result(other, result)\n\n    def difference(self, other, sort=None):\n        \"\"\"\n        Return a new Index with elements of index not in `other`.\n\n        This is the set difference of two Index objects.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default None\n            Whether to sort the resulting index. By default, the\n            values are attempted to be sorted, but any TypeError from\n            incomparable elements is caught by pandas.\n\n            * None : Attempt to sort the result, but catch any TypeErrors\n              from comparing incomparable elements.\n            * False : Do not sort the result.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        difference : Index\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([2, 1, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.difference(idx2)\n        Int64Index([1, 2], dtype='int64')\n        >>> idx1.difference(idx2, sort=False)\n        Int64Index([2, 1], dtype='int64')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n\n        if self.equals(other):\n            # pass an empty np.ndarray with the appropriate dtype\n            return self._shallow_copy(self._data[:0])\n\n        other, result_name = self._convert_can_do_setop(other)\n\n        this = self._get_unique_index()\n\n        indexer = this.get_indexer(other)\n        indexer = indexer.take((indexer != -1).nonzero()[0])\n\n        label_diff = np.setdiff1d(np.arange(this.size), indexer, assume_unique=True)\n        the_diff = this._values.take(label_diff)\n        if sort is None:\n            try:\n                the_diff = algos.safe_sort(the_diff)\n            except TypeError:\n                pass\n\n        return this._shallow_copy(the_diff, name=result_name)\n\n    def symmetric_difference(self, other, result_name=None, sort=None):\n        \"\"\"\n        Compute the symmetric difference of two Index objects.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        result_name : str\n        sort : False or None, default None\n            Whether to sort the resulting index. By default, the\n            values are attempted to be sorted, but any TypeError from\n            incomparable elements is caught by pandas.\n\n            * None : Attempt to sort the result, but catch any TypeErrors\n              from comparing incomparable elements.\n            * False : Do not sort the result.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        symmetric_difference : Index\n\n        Notes\n        -----\n        ``symmetric_difference`` contains elements that appear in either\n        ``idx1`` or ``idx2`` but not both. Equivalent to the Index created by\n        ``idx1.difference(idx2) | idx2.difference(idx1)`` with duplicates\n        dropped.\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([2, 3, 4, 5])\n        >>> idx1.symmetric_difference(idx2)\n        Int64Index([1, 5], dtype='int64')\n\n        You can also use the ``^`` operator:\n\n        >>> idx1 ^ idx2\n        Int64Index([1, 5], dtype='int64')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, result_name_update = self._convert_can_do_setop(other)\n        if result_name is None:\n            result_name = result_name_update\n\n        this = self._get_unique_index()\n        other = other._get_unique_index()\n        indexer = this.get_indexer(other)\n\n        # {this} minus {other}\n        common_indexer = indexer.take((indexer != -1).nonzero()[0])\n        left_indexer = np.setdiff1d(\n            np.arange(this.size), common_indexer, assume_unique=True\n        )\n        left_diff = this._values.take(left_indexer)\n\n        # {other} minus {this}\n        right_indexer = (indexer == -1).nonzero()[0]\n        right_diff = other._values.take(right_indexer)\n\n        the_diff = concat_compat([left_diff, right_diff])\n        if sort is None:\n            try:\n                the_diff = algos.safe_sort(the_diff)\n            except TypeError:\n                pass\n\n        return Index(the_diff, dtype=self.dtype, name=result_name)\n\n    def _assert_can_do_setop(self, other):\n        if not is_list_like(other):\n            raise TypeError(\"Input must be Index or array-like\")\n        return True\n\n    def _convert_can_do_setop(self, other):\n        if not isinstance(other, Index):\n            other = Index(other, name=self.name)\n            result_name = self.name\n        else:\n            result_name = get_op_result_name(self, other)\n        return other, result_name\n\n    # --------------------------------------------------------------------\n    # Indexing Methods\n\n    def get_loc(self, key, method=None, tolerance=None):\n        \"\"\"\n        Get integer location, slice or boolean mask for requested label.\n\n        Parameters\n        ----------\n        key : label\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        tolerance : int or float, optional\n            Maximum distance from index value for inexact matches. The value of\n            the index at the matching location must satisfy the equation\n            ``abs(index[loc] - key) <= tolerance``.\n\n        Returns\n        -------\n        loc : int if unique index, slice if monotonic index, else mask\n\n        Examples\n        --------\n        >>> unique_index = pd.Index(list('abc'))\n        >>> unique_index.get_loc('b')\n        1\n\n        >>> monotonic_index = pd.Index(list('abbc'))\n        >>> monotonic_index.get_loc('b')\n        slice(1, 3, None)\n\n        >>> non_monotonic_index = pd.Index(list('abcb'))\n        >>> non_monotonic_index.get_loc('b')\n        array([False,  True, False,  True])\n        \"\"\"\n        if method is None:\n            if tolerance is not None:\n                raise ValueError(\n                    \"tolerance argument only valid if using pad, \"\n                    \"backfill or nearest lookups\"\n                )\n            casted_key = self._maybe_cast_indexer(key)\n            try:\n                return self._engine.get_loc(casted_key)\n            except KeyError as err:\n                raise KeyError(key) from err\n\n        if tolerance is not None:\n            tolerance = self._convert_tolerance(tolerance, np.asarray(key))\n\n        indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n        if indexer.ndim > 1 or indexer.size > 1:\n            raise TypeError(\"get_loc requires scalar valued input\")\n        loc = indexer.item()\n        if loc == -1:\n            raise KeyError(key)\n        return loc\n\n    _index_shared_docs[\n        \"get_indexer\"\n    ] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        limit : int, optional\n            Maximum number of consecutive labels in ``target`` to match for\n            inexact matches.\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n\n            Tolerance may be a scalar value, which applies the same tolerance\n            to all values, or list-like, which applies variable tolerance per\n            element. List-like includes list, tuple, array, Series, and must be\n            the same size as the index and its dtype must exactly match the\n            index's type.\n\n        Returns\n        -------\n        indexer : ndarray of int\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n        %(raises_section)s\n        Examples\n        --------\n        >>> index = pd.Index(['c', 'a', 'b'])\n        >>> index.get_indexer(['a', 'b', 'x'])\n        array([ 1,  2, -1])\n\n        Notice that the return value is an array of locations in ``index``\n        and ``x`` is marked by -1, as it is not in ``index``.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"get_indexer\"] % _index_doc_kwargs)\n    def get_indexer(\n        self, target, method=None, limit=None, tolerance=None\n    ) -> np.ndarray:\n        method = missing.clean_reindex_fill_method(method)\n        target = ensure_index(target)\n        if tolerance is not None:\n            tolerance = self._convert_tolerance(tolerance, target)\n\n        # Treat boolean labels passed to a numeric index as not found. Without\n        # this fix False and True would be treated as 0 and 1 respectively.\n        # (GH #16877)\n        if target.is_boolean() and self.is_numeric():\n            return ensure_platform_int(np.repeat(-1, target.size))\n\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer(\n                ptarget, method=method, limit=limit, tolerance=tolerance\n            )\n\n        if not is_dtype_equal(self.dtype, target.dtype):\n            this = self.astype(object)\n            target = target.astype(object)\n            return this.get_indexer(\n                target, method=method, limit=limit, tolerance=tolerance\n            )\n\n        if not self.is_unique:\n            raise InvalidIndexError(\n                \"Reindexing only valid with uniquely valued Index objects\"\n            )\n\n        if method == \"pad\" or method == \"backfill\":\n            indexer = self._get_fill_indexer(target, method, limit, tolerance)\n        elif method == \"nearest\":\n            indexer = self._get_nearest_indexer(target, limit, tolerance)\n        else:\n            if tolerance is not None:\n                raise ValueError(\n                    \"tolerance argument only valid if doing pad, \"\n                    \"backfill or nearest reindexing\"\n                )\n            if limit is not None:\n                raise ValueError(\n                    \"limit argument only valid if doing pad, \"\n                    \"backfill or nearest reindexing\"\n                )\n\n            indexer = self._engine.get_indexer(target._get_engine_target())\n\n        return ensure_platform_int(indexer)\n\n    def _convert_tolerance(self, tolerance, target):\n        # override this method on subclasses\n        tolerance = np.asarray(tolerance)\n        if target.size != tolerance.size and tolerance.size > 1:\n            raise ValueError(\"list-like tolerance size must match target index size\")\n        return tolerance\n\n    def _get_fill_indexer(\n        self, target: \"Index\", method: str_t, limit=None, tolerance=None\n    ) -> np.ndarray:\n\n        target_values = target._get_engine_target()\n\n        if self.is_monotonic_increasing and target.is_monotonic_increasing:\n            engine_method = (\n                self._engine.get_pad_indexer\n                if method == \"pad\"\n                else self._engine.get_backfill_indexer\n            )\n            indexer = engine_method(target_values, limit)\n        else:\n            indexer = self._get_fill_indexer_searchsorted(target, method, limit)\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target_values, indexer, tolerance)\n        return indexer\n\n    def _get_fill_indexer_searchsorted(\n        self, target: \"Index\", method: str_t, limit=None\n    ) -> np.ndarray:\n        \"\"\"\n        Fallback pad/backfill get_indexer that works for monotonic decreasing\n        indexes and non-monotonic targets.\n        \"\"\"\n        if limit is not None:\n            raise ValueError(\n                f\"limit argument for {repr(method)} method only well-defined \"\n                \"if index and target are monotonic\"\n            )\n\n        side = \"left\" if method == \"pad\" else \"right\"\n\n        # find exact matches first (this simplifies the algorithm)\n        indexer = self.get_indexer(target)\n        nonexact = indexer == -1\n        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact], side)\n        if side == \"left\":\n            # searchsorted returns \"indices into a sorted array such that,\n            # if the corresponding elements in v were inserted before the\n            # indices, the order of a would be preserved\".\n            # Thus, we need to subtract 1 to find values to the left.\n            indexer[nonexact] -= 1\n            # This also mapped not found values (values of 0 from\n            # np.searchsorted) to -1, which conveniently is also our\n            # sentinel for missing values\n        else:\n            # Mark indices to the right of the largest value as not found\n            indexer[indexer == len(self)] = -1\n        return indexer\n\n    def _get_nearest_indexer(self, target: \"Index\", limit, tolerance) -> np.ndarray:\n        \"\"\"\n        Get the indexer for the nearest index labels; requires an index with\n        values that can be subtracted from each other (e.g., not strings or\n        tuples).\n        \"\"\"\n        left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n        right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n\n        target_values = target._values\n        left_distances = np.abs(self._values[left_indexer] - target_values)\n        right_distances = np.abs(self._values[right_indexer] - target_values)\n\n        op = operator.lt if self.is_monotonic_increasing else operator.le\n        indexer = np.where(\n            op(left_distances, right_distances) | (right_indexer == -1),\n            left_indexer,\n            right_indexer,\n        )\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target_values, indexer, tolerance)\n        return indexer\n\n    def _filter_indexer_tolerance(\n        self,\n        target: Union[\"Index\", np.ndarray, ExtensionArray],\n        indexer: np.ndarray,\n        tolerance,\n    ) -> np.ndarray:\n        distance = abs(self._values[indexer] - target)\n        indexer = np.where(distance <= tolerance, indexer, -1)\n        return indexer\n\n    # --------------------------------------------------------------------\n    # Indexer Conversion Methods\n\n    def _get_partial_string_timestamp_match_key(self, key):\n        \"\"\"\n        Translate any partial string timestamp matches in key, returning the\n        new key.\n\n        Only relevant for MultiIndex.\n        \"\"\"\n        # GH#10331\n        return key\n\n    def _validate_positional_slice(self, key: slice):\n        \"\"\"\n        For positional indexing, a slice must have either int or None\n        for each of start, stop, and step.\n        \"\"\"\n        self._validate_indexer(\"positional\", key.start, \"iloc\")\n        self._validate_indexer(\"positional\", key.stop, \"iloc\")\n        self._validate_indexer(\"positional\", key.step, \"iloc\")\n\n    def _convert_slice_indexer(self, key: slice, kind: str_t):\n        \"\"\"\n        Convert a slice indexer.\n\n        By definition, these are labels unless 'iloc' is passed in.\n        Floats are not allowed as the start, step, or stop of the slice.\n\n        Parameters\n        ----------\n        key : label of the slice bound\n        kind : {'loc', 'getitem'}\n        \"\"\"\n        assert kind in [\"loc\", \"getitem\"], kind\n\n        # potentially cast the bounds to integers\n        start, stop, step = key.start, key.stop, key.step\n\n        # figure out if this is a positional indexer\n        def is_int(v):\n            return v is None or is_integer(v)\n\n        is_index_slice = is_int(start) and is_int(stop) and is_int(step)\n        is_positional = is_index_slice and not (\n            self.is_integer() or self.is_categorical()\n        )\n\n        if kind == \"getitem\":\n            \"\"\"\n            called from the getitem slicers, validate that we are in fact\n            integers\n            \"\"\"\n            if self.is_integer() or is_index_slice:\n                self._validate_indexer(\"slice\", key.start, \"getitem\")\n                self._validate_indexer(\"slice\", key.stop, \"getitem\")\n                self._validate_indexer(\"slice\", key.step, \"getitem\")\n                return key\n\n        # convert the slice to an indexer here\n\n        # if we are mixed and have integers\n        if is_positional:\n            try:\n                # Validate start & stop\n                if start is not None:\n                    self.get_loc(start)\n                if stop is not None:\n                    self.get_loc(stop)\n                is_positional = False\n            except KeyError:\n                pass\n\n        if com.is_null_slice(key):\n            # It doesn't matter if we are positional or label based\n            indexer = key\n        elif is_positional:\n            if kind == \"loc\":\n                # GH#16121, GH#24612, GH#31810\n                warnings.warn(\n                    \"Slicing a positional slice with .loc is not supported, \"\n                    \"and will raise TypeError in a future version.  \"\n                    \"Use .loc with labels or .iloc with positions instead.\",\n                    FutureWarning,\n                    stacklevel=6,\n                )\n            indexer = key\n        else:\n            indexer = self.slice_indexer(start, stop, step, kind=kind)\n\n        return indexer\n\n    def _convert_listlike_indexer(self, keyarr):\n        \"\"\"\n        Parameters\n        ----------\n        keyarr : list-like\n            Indexer to convert.\n\n        Returns\n        -------\n        indexer : numpy.ndarray or None\n            Return an ndarray or None if cannot convert.\n        keyarr : numpy.ndarray\n            Return tuple-safe keys.\n        \"\"\"\n        if isinstance(keyarr, Index):\n            pass\n        else:\n            keyarr = self._convert_arr_indexer(keyarr)\n\n        indexer = self._convert_list_indexer(keyarr)\n        return indexer, keyarr\n\n    def _convert_arr_indexer(self, keyarr):\n        \"\"\"\n        Convert an array-like indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : array-like\n            Indexer to convert.\n\n        Returns\n        -------\n        converted_keyarr : array-like\n        \"\"\"\n        keyarr = com.asarray_tuplesafe(keyarr)\n        return keyarr\n\n    def _convert_list_indexer(self, keyarr):\n        \"\"\"\n        Convert a list-like indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : Index (or sub-class)\n            Indexer to convert.\n        kind : iloc, loc, optional\n\n        Returns\n        -------\n        positional indexer or None\n        \"\"\"\n        return None\n\n    def _invalid_indexer(self, form: str_t, key):\n        \"\"\"\n        Consistent invalid indexer message.\n        \"\"\"\n        raise TypeError(\n            f\"cannot do {form} indexing on {type(self).__name__} with these \"\n            f\"indexers [{key}] of type {type(key).__name__}\"\n        )\n\n    # --------------------------------------------------------------------\n    # Reindex Methods\n\n    def _can_reindex(self, indexer):\n        \"\"\"\n        Check if we are allowing reindexing with this particular indexer.\n\n        Parameters\n        ----------\n        indexer : an integer indexer\n\n        Raises\n        ------\n        ValueError if its a duplicate axis\n        \"\"\"\n        # trying to reindex on an axis with duplicates\n        if not self._index_as_unique and len(indexer):\n            raise ValueError(\"cannot reindex from a duplicate axis\")\n\n    def reindex(self, target, method=None, level=None, limit=None, tolerance=None):\n        \"\"\"\n        Create index with target's values.\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index.\n        indexer : np.ndarray or None\n            Indices of output values in original index.\n        \"\"\"\n        # GH6552: preserve names when reindexing to non-named target\n        # (i.e. neither Index nor Series).\n        preserve_names = not hasattr(target, \"name\")\n\n        # GH7774: preserve dtype/tz if target is empty and not an Index.\n        target = ensure_has_len(target)  # target may be an iterator\n\n        if not isinstance(target, Index) and len(target) == 0:\n            if isinstance(self, ABCRangeIndex):\n                values = range(0)\n            else:\n                values = self._data[:0]  # appropriately-dtyped empty array\n            target = self._simple_new(values, name=self.name)\n        else:\n            target = ensure_index(target)\n\n        if level is not None:\n            if method is not None:\n                raise TypeError(\"Fill method not supported if level passed\")\n            _, indexer, _ = self._join_level(\n                target, level, how=\"right\", return_indexers=True\n            )\n        else:\n            if self.equals(target):\n                indexer = None\n            else:\n                if self._index_as_unique:\n                    indexer = self.get_indexer(\n                        target, method=method, limit=limit, tolerance=tolerance\n                    )\n                else:\n                    if method is not None or limit is not None:\n                        raise ValueError(\n                            \"cannot reindex a non-unique index \"\n                            \"with a method or limit\"\n                        )\n                    indexer, missing = self.get_indexer_non_unique(target)\n\n        if preserve_names and target.nlevels == 1 and target.name != self.name:\n            target = target.copy()\n            target.name = self.name\n\n        return target, indexer\n\n    def _reindex_non_unique(self, target):\n        \"\"\"\n        Create a new index with target's values (move/add/delete values as\n        necessary) use with non-unique Index and a possibly non-unique target.\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index.\n        indexer : np.ndarray or None\n            Indices of output values in original index.\n\n        \"\"\"\n        target = ensure_index(target)\n        if len(target) == 0:\n            # GH#13691\n            return self[:0], np.array([], dtype=np.intp), None\n\n        indexer, missing = self.get_indexer_non_unique(target)\n        check = indexer != -1\n        new_labels = self.take(indexer[check])\n        new_indexer = None\n\n        if len(missing):\n            length = np.arange(len(indexer))\n\n            missing = ensure_platform_int(missing)\n            missing_labels = target.take(missing)\n            missing_indexer = ensure_int64(length[~check])\n            cur_labels = self.take(indexer[check]).values\n            cur_indexer = ensure_int64(length[check])\n\n            new_labels = np.empty(tuple([len(indexer)]), dtype=object)\n            new_labels[cur_indexer] = cur_labels\n            new_labels[missing_indexer] = missing_labels\n\n            # a unique indexer\n            if target.is_unique:\n\n                # see GH5553, make sure we use the right indexer\n                new_indexer = np.arange(len(indexer))\n                new_indexer[cur_indexer] = np.arange(len(cur_labels))\n                new_indexer[missing_indexer] = -1\n\n            # we have a non_unique selector, need to use the original\n            # indexer here\n            else:\n\n                # need to retake to have the same size as the indexer\n                indexer[~check] = -1\n\n                # reset the new indexer to account for the new size\n                new_indexer = np.arange(len(self.take(indexer)))\n                new_indexer[~check] = -1\n\n        if isinstance(self, ABCMultiIndex):\n            new_index = type(self).from_tuples(new_labels, names=self.names)\n        else:\n            new_index = Index(new_labels, name=self.name)\n        return new_index, indexer, new_indexer\n\n    # --------------------------------------------------------------------\n    # Join Methods\n\n    def join(self, other, how=\"left\", level=None, return_indexers=False, sort=False):\n        \"\"\"\n        Compute join_index and indexers to conform data\n        structures to the new index.\n\n        Parameters\n        ----------\n        other : Index\n        how : {'left', 'right', 'inner', 'outer'}\n        level : int or level name, default None\n        return_indexers : bool, default False\n        sort : bool, default False\n            Sort the join keys lexicographically in the result Index. If False,\n            the order of the join keys depends on the join type (how keyword).\n\n        Returns\n        -------\n        join_index, (left_indexer, right_indexer)\n        \"\"\"\n        other = ensure_index(other)\n        self_is_mi = isinstance(self, ABCMultiIndex)\n        other_is_mi = isinstance(other, ABCMultiIndex)\n\n        # try to figure out the join level\n        # GH3662\n        if level is None and (self_is_mi or other_is_mi):\n\n            # have the same levels/names so a simple join\n            if self.names == other.names:\n                pass\n            else:\n                return self._join_multi(other, how=how, return_indexers=return_indexers)\n\n        # join on the level\n        if level is not None and (self_is_mi or other_is_mi):\n            return self._join_level(\n                other, level, how=how, return_indexers=return_indexers\n            )\n\n        if len(other) == 0 and how in (\"left\", \"outer\"):\n            join_index = self._shallow_copy()\n            if return_indexers:\n                rindexer = np.repeat(-1, len(join_index))\n                return join_index, None, rindexer\n            else:\n                return join_index\n\n        if len(self) == 0 and how in (\"right\", \"outer\"):\n            join_index = other._shallow_copy()\n            if return_indexers:\n                lindexer = np.repeat(-1, len(join_index))\n                return join_index, lindexer, None\n            else:\n                return join_index\n\n        if self._join_precedence < other._join_precedence:\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n            result = other.join(\n                self, how=how, level=level, return_indexers=return_indexers\n            )\n            if return_indexers:\n                x, y, z = result\n                result = x, z, y\n            return result\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            this = self.astype(\"O\")\n            other = other.astype(\"O\")\n            return this.join(other, how=how, return_indexers=return_indexers)\n\n        _validate_join_method(how)\n\n        if not self.is_unique and not other.is_unique:\n            return self._join_non_unique(\n                other, how=how, return_indexers=return_indexers\n            )\n        elif not self.is_unique or not other.is_unique:\n            if self.is_monotonic and other.is_monotonic:\n                return self._join_monotonic(\n                    other, how=how, return_indexers=return_indexers\n                )\n            else:\n                return self._join_non_unique(\n                    other, how=how, return_indexers=return_indexers\n                )\n        elif self.is_monotonic and other.is_monotonic:\n            try:\n                return self._join_monotonic(\n                    other, how=how, return_indexers=return_indexers\n                )\n            except TypeError:\n                pass\n\n        if how == \"left\":\n            join_index = self\n        elif how == \"right\":\n            join_index = other\n        elif how == \"inner\":\n            # TODO: sort=False here for backwards compat. It may\n            # be better to use the sort parameter passed into join\n            join_index = self.intersection(other, sort=False)\n        elif how == \"outer\":\n            # TODO: sort=True here for backwards compat. It may\n            # be better to use the sort parameter passed into join\n            join_index = self.union(other)\n\n        if sort:\n            join_index = join_index.sort_values()\n\n        if return_indexers:\n            if join_index is self:\n                lindexer = None\n            else:\n                lindexer = self.get_indexer(join_index)\n            if join_index is other:\n                rindexer = None\n            else:\n                rindexer = other.get_indexer(join_index)\n            return join_index, lindexer, rindexer\n        else:\n            return join_index\n\n    def _join_multi(self, other, how, return_indexers=True):\n        from pandas.core.indexes.multi import MultiIndex\n        from pandas.core.reshape.merge import restore_dropped_levels_multijoin\n\n        # figure out join names\n        self_names_list = list(com.not_none(*self.names))\n        other_names_list = list(com.not_none(*other.names))\n        self_names_order = self_names_list.index\n        other_names_order = other_names_list.index\n        self_names = set(self_names_list)\n        other_names = set(other_names_list)\n        overlap = self_names & other_names\n\n        # need at least 1 in common\n        if not overlap:\n            raise ValueError(\"cannot join with no overlapping index names\")\n\n        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n\n            # Drop the non-matching levels from left and right respectively\n            ldrop_names = sorted(self_names - overlap, key=self_names_order)\n            rdrop_names = sorted(other_names - overlap, key=other_names_order)\n\n            # if only the order differs\n            if not len(ldrop_names + rdrop_names):\n                self_jnlevels = self\n                other_jnlevels = other.reorder_levels(self.names)\n            else:\n                self_jnlevels = self.droplevel(ldrop_names)\n                other_jnlevels = other.droplevel(rdrop_names)\n\n            # Join left and right\n            # Join on same leveled multi-index frames is supported\n            join_idx, lidx, ridx = self_jnlevels.join(\n                other_jnlevels, how, return_indexers=True\n            )\n\n            # Restore the dropped levels\n            # Returned index level order is\n            # common levels, ldrop_names, rdrop_names\n            dropped_names = ldrop_names + rdrop_names\n\n            levels, codes, names = restore_dropped_levels_multijoin(\n                self, other, dropped_names, join_idx, lidx, ridx\n            )\n\n            # Re-create the multi-index\n            multi_join_idx = MultiIndex(\n                levels=levels, codes=codes, names=names, verify_integrity=False\n            )\n\n            multi_join_idx = multi_join_idx.remove_unused_levels()\n\n            if return_indexers:\n                return multi_join_idx, lidx, ridx\n            else:\n                return multi_join_idx\n\n        jl = list(overlap)[0]\n\n        # Case where only one index is multi\n        # make the indices into mi's that match\n        flip_order = False\n        if isinstance(self, MultiIndex):\n            self, other = other, self\n            flip_order = True\n            # flip if join method is right or left\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n\n        level = other.names.index(jl)\n        result = self._join_level(\n            other, level, how=how, return_indexers=return_indexers\n        )\n\n        if flip_order:\n            if isinstance(result, tuple):\n                return result[0], result[2], result[1]\n        return result\n\n    def _join_non_unique(self, other, how=\"left\", return_indexers=False):\n        from pandas.core.reshape.merge import get_join_indexers\n\n        # We only get here if dtypes match\n        assert self.dtype == other.dtype\n\n        lvalues = self._get_engine_target()\n        rvalues = other._get_engine_target()\n\n        left_idx, right_idx = get_join_indexers(\n            [lvalues], [rvalues], how=how, sort=True\n        )\n\n        left_idx = ensure_platform_int(left_idx)\n        right_idx = ensure_platform_int(right_idx)\n\n        join_index = np.asarray(lvalues.take(left_idx))\n        mask = left_idx == -1\n        np.putmask(join_index, mask, rvalues.take(right_idx))\n\n        join_index = self._wrap_joined_index(join_index, other)\n\n        if return_indexers:\n            return join_index, left_idx, right_idx\n        else:\n            return join_index\n\n    def _join_level(\n        self, other, level, how=\"left\", return_indexers=False, keep_order=True\n    ):\n        \"\"\"\n        The join method *only* affects the level of the resulting\n        MultiIndex. Otherwise it just exactly aligns the Index data to the\n        labels of the level in the MultiIndex.\n\n        If ```keep_order == True```, the order of the data indexed by the\n        MultiIndex will not be changed; otherwise, it will tie out\n        with `other`.\n        \"\"\"\n        from pandas.core.indexes.multi import MultiIndex\n\n        def _get_leaf_sorter(labels):\n            \"\"\"\n            Returns sorter for the inner most level while preserving the\n            order of higher levels.\n            \"\"\"\n            if labels[0].size == 0:\n                return np.empty(0, dtype=\"int64\")\n\n            if len(labels) == 1:\n                lab = ensure_int64(labels[0])\n                sorter, _ = libalgos.groupsort_indexer(lab, 1 + lab.max())\n                return sorter\n\n            # find indexers of beginning of each set of\n            # same-key labels w.r.t all but last level\n            tic = labels[0][:-1] != labels[0][1:]\n            for lab in labels[1:-1]:\n                tic |= lab[:-1] != lab[1:]\n\n            starts = np.hstack(([True], tic, [True])).nonzero()[0]\n            lab = ensure_int64(labels[-1])\n            return lib.get_level_sorter(lab, ensure_int64(starts))\n\n        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n            raise TypeError(\"Join on level between two MultiIndex objects is ambiguous\")\n\n        left, right = self, other\n\n        flip_order = not isinstance(self, MultiIndex)\n        if flip_order:\n            left, right = right, left\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n\n        assert isinstance(left, MultiIndex)\n\n        level = left._get_level_number(level)\n        old_level = left.levels[level]\n\n        if not right.is_unique:\n            raise NotImplementedError(\n                \"Index._join_level on non-unique index is not implemented\"\n            )\n\n        new_level, left_lev_indexer, right_lev_indexer = old_level.join(\n            right, how=how, return_indexers=True\n        )\n\n        if left_lev_indexer is None:\n            if keep_order or len(left) == 0:\n                left_indexer = None\n                join_index = left\n            else:  # sort the leaves\n                left_indexer = _get_leaf_sorter(left.codes[: level + 1])\n                join_index = left[left_indexer]\n\n        else:\n            left_lev_indexer = ensure_int64(left_lev_indexer)\n            rev_indexer = lib.get_reverse_indexer(left_lev_indexer, len(old_level))\n\n            new_lev_codes = algos.take_nd(\n                rev_indexer, left.codes[level], allow_fill=False\n            )\n\n            new_codes = list(left.codes)\n            new_codes[level] = new_lev_codes\n\n            new_levels = list(left.levels)\n            new_levels[level] = new_level\n\n            if keep_order:  # just drop missing values. o.w. keep order\n                left_indexer = np.arange(len(left), dtype=np.intp)\n                mask = new_lev_codes != -1\n                if not mask.all():\n                    new_codes = [lab[mask] for lab in new_codes]\n                    left_indexer = left_indexer[mask]\n\n            else:  # tie out the order with other\n                if level == 0:  # outer most level, take the fast route\n                    ngroups = 1 + new_lev_codes.max()\n                    left_indexer, counts = libalgos.groupsort_indexer(\n                        new_lev_codes, ngroups\n                    )\n\n                    # missing values are placed first; drop them!\n                    left_indexer = left_indexer[counts[0] :]\n                    new_codes = [lab[left_indexer] for lab in new_codes]\n\n                else:  # sort the leaves\n                    mask = new_lev_codes != -1\n                    mask_all = mask.all()\n                    if not mask_all:\n                        new_codes = [lab[mask] for lab in new_codes]\n\n                    left_indexer = _get_leaf_sorter(new_codes[: level + 1])\n                    new_codes = [lab[left_indexer] for lab in new_codes]\n\n                    # left_indexers are w.r.t masked frame.\n                    # reverse to original frame!\n                    if not mask_all:\n                        left_indexer = mask.nonzero()[0][left_indexer]\n\n            join_index = MultiIndex(\n                levels=new_levels,\n                codes=new_codes,\n                names=left.names,\n                verify_integrity=False,\n            )\n\n        if right_lev_indexer is not None:\n            right_indexer = algos.take_nd(\n                right_lev_indexer, join_index.codes[level], allow_fill=False\n            )\n        else:\n            right_indexer = join_index.codes[level]\n\n        if flip_order:\n            left_indexer, right_indexer = right_indexer, left_indexer\n\n        if return_indexers:\n            left_indexer = (\n                None if left_indexer is None else ensure_platform_int(left_indexer)\n            )\n            right_indexer = (\n                None if right_indexer is None else ensure_platform_int(right_indexer)\n            )\n            return join_index, left_indexer, right_indexer\n        else:\n            return join_index\n\n    def _join_monotonic(self, other, how=\"left\", return_indexers=False):\n        # We only get here with matching dtypes\n        assert other.dtype == self.dtype\n\n        if self.equals(other):\n            ret_index = other if how == \"right\" else self\n            if return_indexers:\n                return ret_index, None, None\n            else:\n                return ret_index\n\n        sv = self._get_engine_target()\n        ov = other._get_engine_target()\n\n        if self.is_unique and other.is_unique:\n            # We can perform much better than the general case\n            if how == \"left\":\n                join_index = self\n                lidx = None\n                ridx = self._left_indexer_unique(sv, ov)\n            elif how == \"right\":\n                join_index = other\n                lidx = self._left_indexer_unique(ov, sv)\n                ridx = None\n            elif how == \"inner\":\n                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n                join_index = self._wrap_joined_index(join_index, other)\n            elif how == \"outer\":\n                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n                join_index = self._wrap_joined_index(join_index, other)\n        else:\n            if how == \"left\":\n                join_index, lidx, ridx = self._left_indexer(sv, ov)\n            elif how == \"right\":\n                join_index, ridx, lidx = self._left_indexer(ov, sv)\n            elif how == \"inner\":\n                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n            elif how == \"outer\":\n                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n            join_index = self._wrap_joined_index(join_index, other)\n\n        if return_indexers:\n            lidx = None if lidx is None else ensure_platform_int(lidx)\n            ridx = None if ridx is None else ensure_platform_int(ridx)\n            return join_index, lidx, ridx\n        else:\n            return join_index\n\n    def _wrap_joined_index(self, joined, other):\n        if isinstance(self, ABCMultiIndex):\n            name = self.names if self.names == other.names else None\n        else:\n            name = get_op_result_name(self, other)\n        return self._constructor(joined, name=name)\n\n    # --------------------------------------------------------------------\n    # Uncategorized Methods\n\n    @property\n    def values(self) -> np.ndarray:\n        \"\"\"\n        Return an array representing the data in the Index.\n\n        .. warning::\n\n           We recommend using :attr:`Index.array` or\n           :meth:`Index.to_numpy`, depending on whether you need\n           a reference to the underlying data or a NumPy array.\n\n        Returns\n        -------\n        array: numpy.ndarray or ExtensionArray\n\n        See Also\n        --------\n        Index.array : Reference to the underlying data.\n        Index.to_numpy : A NumPy array representing the underlying data.\n        \"\"\"\n        return self._data.view(np.ndarray)\n\n    @cache_readonly\n    @doc(IndexOpsMixin.array)\n    def array(self) -> ExtensionArray:\n        array = self._data\n        if isinstance(array, np.ndarray):\n            from pandas.core.arrays.numpy_ import PandasArray\n\n            array = PandasArray(array)\n        return array\n\n    @property\n    def _values(self) -> Union[ExtensionArray, np.ndarray]:\n        \"\"\"\n        The best array representation.\n\n        This is an ndarray or ExtensionArray.\n\n        ``_values`` are consistent between``Series`` and ``Index``.\n\n        It may differ from the public '.values' method.\n\n        index             | values          | _values       |\n        ----------------- | --------------- | ------------- |\n        Index             | ndarray         | ndarray       |\n        CategoricalIndex  | Categorical     | Categorical   |\n        DatetimeIndex     | ndarray[M8ns]   | DatetimeArray |\n        DatetimeIndex[tz] | ndarray[M8ns]   | DatetimeArray |\n        PeriodIndex       | ndarray[object] | PeriodArray   |\n        IntervalIndex     | IntervalArray   | IntervalArray |\n\n        See Also\n        --------\n        values : Values\n        \"\"\"\n        return self._data\n\n    def _get_engine_target(self) -> np.ndarray:\n        \"\"\"\n        Get the ndarray that we can pass to the IndexEngine constructor.\n        \"\"\"\n        return self._values\n\n    @doc(IndexOpsMixin.memory_usage)\n    def memory_usage(self, deep: bool = False) -> int:\n        result = super().memory_usage(deep=deep)\n\n        # include our engine hashtable\n        result += self._engine.sizeof(deep=deep)\n        return result\n\n    def where(self, cond, other=None):\n        \"\"\"\n        Replace values where the condition is False.\n\n        The replacement is taken from other.\n\n        Parameters\n        ----------\n        cond : bool array-like with the same length as self\n            Condition to select the values on.\n        other : scalar, or array-like, default None\n            Replacement if the condition is False.\n\n        Returns\n        -------\n        pandas.Index\n            A copy of self with values replaced from other\n            where the condition is False.\n\n        See Also\n        --------\n        Series.where : Same method for Series.\n        DataFrame.where : Same method for DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['car', 'bike', 'train', 'tractor'])\n        >>> idx\n        Index(['car', 'bike', 'train', 'tractor'], dtype='object')\n        >>> idx.where(idx.isin(['car', 'train']), 'other')\n        Index(['car', 'other', 'train', 'other'], dtype='object')\n        \"\"\"\n        if other is None:\n            other = self._na_value\n\n        dtype = self.dtype\n        values = self.values\n\n        if is_bool(other) or is_bool_dtype(other):\n\n            # bools force casting\n            values = values.astype(object)\n            dtype = None\n\n        values = np.where(cond, values, other)\n\n        if self._is_numeric_dtype and np.any(isna(values)):\n            # We can't coerce to the numeric dtype of \"self\" (unless\n            # it's float) if there are NaN values in our output.\n            dtype = None\n\n        return Index(values, dtype=dtype, name=self.name)\n\n    # construction helpers\n    @classmethod\n    def _scalar_data_error(cls, data):\n        # We return the TypeError so that we can raise it from the constructor\n        #  in order to keep mypy happy\n        return TypeError(\n            f\"{cls.__name__}(...) must be called with a collection of some \"\n            f\"kind, {repr(data)} was passed\"\n        )\n\n    @classmethod\n    def _string_data_error(cls, data):\n        raise TypeError(\n            \"String dtype not supported, you may need \"\n            \"to explicitly cast to a numeric type\"\n        )\n\n    def _coerce_scalar_to_index(self, item):\n        \"\"\"\n        We need to coerce a scalar to a compat for our index type.\n\n        Parameters\n        ----------\n        item : scalar item to coerce\n        \"\"\"\n        dtype = self.dtype\n\n        if self._is_numeric_dtype and isna(item):\n            # We can't coerce to the numeric dtype of \"self\" (unless\n            # it's float) if there are NaN values in our output.\n            dtype = None\n\n        return Index([item], dtype=dtype, **self._get_attributes_dict())\n\n    def _to_safe_for_reshape(self):\n        \"\"\"\n        Convert to object if we are a categorical.\n        \"\"\"\n        return self\n\n    def _validate_fill_value(self, value):\n        \"\"\"\n        Check if the value can be inserted into our array, and convert\n        it to an appropriate native type if necessary.\n        \"\"\"\n        return value\n\n    def _validate_scalar(self, value):\n        \"\"\"\n        Check that this is a scalar value that we can use for setitem-like\n        operations without changing dtype.\n        \"\"\"\n        if not is_scalar(value):\n            raise TypeError(f\"'value' must be a scalar, passed: {type(value).__name__}\")\n        return value\n\n    @property\n    def _has_complex_internals(self) -> bool:\n        \"\"\"\n        Indicates if an index is not directly backed by a numpy array\n        \"\"\"\n        # used to avoid libreduction code paths, which raise or require conversion\n        return False\n\n    def _is_memory_usage_qualified(self) -> bool:\n        \"\"\"\n        Return a boolean if we need a qualified .info display.\n        \"\"\"\n        return self.is_object()\n\n    def is_type_compatible(self, kind) -> bool:\n        \"\"\"\n        Whether the index type is compatible with the provided type.\n        \"\"\"\n        return kind == self.inferred_type\n\n    def __contains__(self, key: Any) -> bool:\n        \"\"\"\n        Return a boolean indicating whether the provided key is in the index.\n\n        Parameters\n        ----------\n        key : label\n            The key to check if it is present in the index.\n\n        Returns\n        -------\n        bool\n            Whether the key search is in the index.\n\n        Raises\n        ------\n        TypeError\n            If the key is not hashable.\n\n        See Also\n        --------\n        Index.isin : Returns an ndarray of boolean dtype indicating whether the\n            list-like key is in the index.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx\n        Int64Index([1, 2, 3, 4], dtype='int64')\n\n        >>> 2 in idx\n        True\n        >>> 6 in idx\n        False\n        \"\"\"\n        hash(key)\n        try:\n            return key in self._engine\n        except (OverflowError, TypeError, ValueError):\n            return False\n\n    def __hash__(self):\n        raise TypeError(f\"unhashable type: {repr(type(self).__name__)}\")\n\n    def __setitem__(self, key, value):\n        raise TypeError(\"Index does not support mutable operations\")\n\n    def __getitem__(self, key):\n        \"\"\"\n        Override numpy.ndarray's __getitem__ method to work as desired.\n\n        This function adds lists and Series as valid boolean indexers\n        (ndarrays only supports ndarray with dtype=bool).\n\n        If resulting ndim != 1, plain ndarray is returned instead of\n        corresponding `Index` subclass.\n\n        \"\"\"\n        # There's no custom logic to be implemented in __getslice__, so it's\n        # not overloaded intentionally.\n        getitem = self._data.__getitem__\n        promote = self._shallow_copy\n\n        if is_scalar(key):\n            key = com.cast_scalar_indexer(key, warn_float=True)\n            return getitem(key)\n\n        if isinstance(key, slice):\n            # This case is separated from the conditional above to avoid\n            # pessimization of basic indexing.\n            return promote(getitem(key))\n\n        if com.is_bool_indexer(key):\n            key = np.asarray(key, dtype=bool)\n\n        result = getitem(key)\n        if not is_scalar(result):\n            if np.ndim(result) > 1:\n                deprecate_ndim_indexing(result)\n                return result\n            return promote(result)\n        else:\n            return result\n\n    def _can_hold_identifiers_and_holds_name(self, name) -> bool:\n        \"\"\"\n        Faster check for ``name in self`` when we know `name` is a Python\n        identifier (e.g. in NDFrame.__getattr__, which hits this to support\n        . key lookup). For indexes that can't hold identifiers (everything\n        but object & categorical) we just return False.\n\n        https://github.com/pandas-dev/pandas/issues/19764\n        \"\"\"\n        if self.is_object() or self.is_categorical():\n            return name in self\n        return False\n\n    def append(self, other):\n        \"\"\"\n        Append a collection of Index options together.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        appended : Index\n        \"\"\"\n        to_concat = [self]\n\n        if isinstance(other, (list, tuple)):\n            to_concat = to_concat + list(other)\n        else:\n            to_concat.append(other)\n\n        for obj in to_concat:\n            if not isinstance(obj, Index):\n                raise TypeError(\"all inputs must be Index\")\n\n        names = {obj.name for obj in to_concat}\n        name = None if len(names) > 1 else self.name\n\n        return self._concat(to_concat, name)\n\n    def _concat(self, to_concat, name):\n        \"\"\"\n        Concatenate multiple Index objects.\n        \"\"\"\n        to_concat = [x._values if isinstance(x, Index) else x for x in to_concat]\n\n        result = concat_compat(to_concat)\n        return Index(result, name=name)\n\n    def putmask(self, mask, value):\n        \"\"\"\n        Return a new Index of the values set with the mask.\n\n        Returns\n        -------\n        Index\n\n        See Also\n        --------\n        numpy.ndarray.putmask : Changes elements of an array\n            based on conditional and input values.\n        \"\"\"\n        values = self.values.copy()\n        try:\n            converted = self._validate_fill_value(value)\n            np.putmask(values, mask, converted)\n            return self._shallow_copy(values)\n        except (ValueError, TypeError) as err:\n            if is_object_dtype(self):\n                raise err\n\n            # coerces to object\n            return self.astype(object).putmask(mask, value)\n\n    def equals(self, other: object) -> bool:\n        \"\"\"\n        Determine if two Index object are equal.\n\n        The things that are being compared are:\n\n        * The elements inside the Index object.\n        * The order of the elements inside the Index object.\n\n        Parameters\n        ----------\n        other : Any\n            The other object to compare against.\n\n        Returns\n        -------\n        bool\n            True if \"other\" is an Index and it has the same elements and order\n            as the calling index; False otherwise.\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([1, 2, 3])\n        >>> idx1\n        Int64Index([1, 2, 3], dtype='int64')\n        >>> idx1.equals(pd.Index([1, 2, 3]))\n        True\n\n        The elements inside are compared\n\n        >>> idx2 = pd.Index([\"1\", \"2\", \"3\"])\n        >>> idx2\n        Index(['1', '2', '3'], dtype='object')\n\n        >>> idx1.equals(idx2)\n        False\n\n        The order is compared\n\n        >>> ascending_idx = pd.Index([1, 2, 3])\n        >>> ascending_idx\n        Int64Index([1, 2, 3], dtype='int64')\n        >>> descending_idx = pd.Index([3, 2, 1])\n        >>> descending_idx\n        Int64Index([3, 2, 1], dtype='int64')\n        >>> ascending_idx.equals(descending_idx)\n        False\n\n        The dtype is *not* compared\n\n        >>> int64_idx = pd.Int64Index([1, 2, 3])\n        >>> int64_idx\n        Int64Index([1, 2, 3], dtype='int64')\n        >>> uint64_idx = pd.UInt64Index([1, 2, 3])\n        >>> uint64_idx\n        UInt64Index([1, 2, 3], dtype='uint64')\n        >>> int64_idx.equals(uint64_idx)\n        True\n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n\n        # If other is a subclass of self and defines it's own equals method, we\n        # dispatch to the subclass method. For instance for a MultiIndex,\n        # a d-level MultiIndex can equal d-tuple Index.\n        # Note: All EA-backed Index subclasses override equals\n        if (\n            isinstance(other, type(self))\n            and type(other) is not type(self)\n            and other.equals is not self.equals\n        ):\n            return other.equals(self)\n\n        return array_equivalent(self._values, other._values)\n\n    def identical(self, other) -> bool:\n        \"\"\"\n        Similar to equals, but checks that object attributes and types are also equal.\n\n        Returns\n        -------\n        bool\n            If two Index objects have equal elements and same type True,\n            otherwise False.\n        \"\"\"\n        return (\n            self.equals(other)\n            and all(\n                getattr(self, c, None) == getattr(other, c, None)\n                for c in self._comparables\n            )\n            and type(self) == type(other)\n        )\n\n    def asof(self, label):\n        \"\"\"\n        Return the label from the index, or, if not present, the previous one.\n\n        Assuming that the index is sorted, return the passed index label if it\n        is in the index, or return the previous index label if the passed one\n        is not in the index.\n\n        Parameters\n        ----------\n        label : object\n            The label up to which the method returns the latest index label.\n\n        Returns\n        -------\n        object\n            The passed label if it is in the index. The previous label if the\n            passed label is not in the sorted index or `NaN` if there is no\n            such label.\n\n        See Also\n        --------\n        Series.asof : Return the latest value in a Series up to the\n            passed index.\n        merge_asof : Perform an asof merge (similar to left join but it\n            matches on nearest key rather than equal key).\n        Index.get_loc : An `asof` is a thin wrapper around `get_loc`\n            with method='pad'.\n\n        Examples\n        --------\n        `Index.asof` returns the latest index label up to the passed label.\n\n        >>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])\n        >>> idx.asof('2014-01-01')\n        '2013-12-31'\n\n        If the label is in the index, the method returns the passed label.\n\n        >>> idx.asof('2014-01-02')\n        '2014-01-02'\n\n        If all of the labels in the index are later than the passed label,\n        NaN is returned.\n\n        >>> idx.asof('1999-01-02')\n        nan\n\n        If the index is not sorted, an error is raised.\n\n        >>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',\n        ...                            '2014-01-03'])\n        >>> idx_not_sorted.asof('2013-12-31')\n        Traceback (most recent call last):\n        ValueError: index must be monotonic increasing or decreasing\n        \"\"\"\n        try:\n            loc = self.get_loc(label, method=\"pad\")\n        except KeyError:\n            return self._na_value\n        else:\n            if isinstance(loc, slice):\n                loc = loc.indices(len(self))[-1]\n            return self[loc]\n\n    def asof_locs(self, where, mask):\n        \"\"\"\n        Return the locations (indices) of labels in the index.\n\n        As in the `asof` function, if the label (a particular entry in\n        `where`) is not in the index, the latest index label up to the\n        passed label is chosen and its index returned.\n\n        If all of the labels in the index are later than a label in `where`,\n        -1 is returned.\n\n        `mask` is used to ignore NA values in the index during calculation.\n\n        Parameters\n        ----------\n        where : Index\n            An Index consisting of an array of timestamps.\n        mask : array-like\n            Array of booleans denoting where values in the original\n            data are not NA.\n\n        Returns\n        -------\n        numpy.ndarray\n            An array of locations (indices) of the labels from the Index\n            which correspond to the return values of the `asof` function\n            for every element in `where`.\n        \"\"\"\n        locs = self.values[mask].searchsorted(where.values, side=\"right\")\n        locs = np.where(locs > 0, locs - 1, 0)\n\n        result = np.arange(len(self))[mask].take(locs)\n\n        first = mask.argmax()\n        result[(locs == 0) & (where.values < self.values[first])] = -1\n\n        return result\n\n    def sort_values(\n        self,\n        return_indexer: bool = False,\n        ascending: bool = True,\n        na_position: str_t = \"last\",\n        key: Optional[Callable] = None,\n    ):\n        \"\"\"\n        Return a sorted copy of the index.\n\n        Return a sorted copy of the index, and optionally return the indices\n        that sorted the index itself.\n\n        Parameters\n        ----------\n        return_indexer : bool, default False\n            Should the indices that would sort the index be returned.\n        ascending : bool, default True\n            Should the index values be sorted in an ascending order.\n        na_position : {'first' or 'last'}, default 'last'\n            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n            the end.\n\n            .. versionadded:: 1.2.0\n\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        sorted_index : pandas.Index\n            Sorted copy of the index.\n        indexer : numpy.ndarray, optional\n            The indices that the index itself was sorted by.\n\n        See Also\n        --------\n        Series.sort_values : Sort values of a Series.\n        DataFrame.sort_values : Sort values in a DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index([10, 100, 1, 1000])\n        >>> idx\n        Int64Index([10, 100, 1, 1000], dtype='int64')\n\n        Sort values in ascending order (default behavior).\n\n        >>> idx.sort_values()\n        Int64Index([1, 10, 100, 1000], dtype='int64')\n\n        Sort values in descending order, and also get the indices `idx` was\n        sorted by.\n\n        >>> idx.sort_values(ascending=False, return_indexer=True)\n        (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\n        \"\"\"\n        idx = ensure_key_mapped(self, key)\n\n        # GH 35584. Sort missing values according to na_position kwarg\n        # ignore na_position for MultiIndex\n        if not isinstance(\n            self, (ABCMultiIndex, ABCDatetimeIndex, ABCTimedeltaIndex, ABCPeriodIndex)\n        ):\n            _as = nargsort(\n                items=idx, ascending=ascending, na_position=na_position, key=key\n            )\n        else:\n            _as = idx.argsort()\n            if not ascending:\n                _as = _as[::-1]\n\n        sorted_index = self.take(_as)\n\n        if return_indexer:\n            return sorted_index, _as\n        else:\n            return sorted_index\n\n    def sort(self, *args, **kwargs):\n        \"\"\"\n        Use sort_values instead.\n        \"\"\"\n        raise TypeError(\"cannot sort an Index object in-place, use sort_values instead\")\n\n    def shift(self, periods=1, freq=None):\n        \"\"\"\n        Shift index by desired number of time frequency increments.\n\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n        freq : pandas.DateOffset, pandas.Timedelta or str, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        pandas.Index\n            Shifted index.\n\n        See Also\n        --------\n        Series.shift : Shift values of Series.\n\n        Notes\n        -----\n        This method is only implemented for datetime-like index classes,\n        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n\n        Examples\n        --------\n        Put the first 5 month starts of 2011 into an index.\n\n        >>> month_starts = pd.date_range('1/1/2011', periods=5, freq='MS')\n        >>> month_starts\n        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n                       '2011-05-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Shift the index by 10 days.\n\n        >>> month_starts.shift(10, freq='D')\n        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n                       '2011-05-11'],\n                      dtype='datetime64[ns]', freq=None)\n\n        The default value of `freq` is the `freq` attribute of the index,\n        which is 'MS' (month start) in this example.\n\n        >>> month_starts.shift(10)\n        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n                       '2012-03-01'],\n                      dtype='datetime64[ns]', freq='MS')\n        \"\"\"\n        raise NotImplementedError(f\"Not supported for type {type(self).__name__}\")\n\n    def argsort(self, *args, **kwargs) -> np.ndarray:\n        \"\"\"\n        Return the integer indices that would sort the index.\n\n        Parameters\n        ----------\n        *args\n            Passed to `numpy.ndarray.argsort`.\n        **kwargs\n            Passed to `numpy.ndarray.argsort`.\n\n        Returns\n        -------\n        numpy.ndarray\n            Integer indices that would sort the index if used as\n            an indexer.\n\n        See Also\n        --------\n        numpy.argsort : Similar method for NumPy arrays.\n        Index.sort_values : Return sorted copy of Index.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['b', 'a', 'd', 'c'])\n        >>> idx\n        Index(['b', 'a', 'd', 'c'], dtype='object')\n\n        >>> order = idx.argsort()\n        >>> order\n        array([1, 0, 3, 2])\n\n        >>> idx[order]\n        Index(['a', 'b', 'c', 'd'], dtype='object')\n        \"\"\"\n        result = self.asi8\n\n        if result is None:\n            result = np.array(self)\n\n        return result.argsort(*args, **kwargs)\n\n    def get_value(self, series: \"Series\", key):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray.\n\n        Only use this if you know what you're doing.\n\n        Returns\n        -------\n        scalar or Series\n        \"\"\"\n        warnings.warn(\n            \"get_value is deprecated and will be removed in a future version. \"\n            \"Use Series[key] instead\",\n            FutureWarning,\n            stacklevel=2,\n        )\n\n        self._check_indexing_error(key)\n\n        try:\n            # GH 20882, 21257\n            # First try to convert the key to a location\n            # If that fails, raise a KeyError if an integer\n            # index, otherwise, see if key is an integer, and\n            # try that\n            loc = self.get_loc(key)\n        except KeyError:\n            if not self._should_fallback_to_positional():\n                raise\n            elif is_integer(key):\n                # If the Index cannot hold integer, then this is unambiguously\n                #  a locational lookup.\n                loc = key\n            else:\n                raise\n\n        return self._get_values_for_loc(series, loc, key)\n\n    def _check_indexing_error(self, key):\n        if not is_scalar(key):\n            # if key is not a scalar, directly raise an error (the code below\n            # would convert to numpy arrays and raise later any way) - GH29926\n            raise InvalidIndexError(key)\n\n    def _should_fallback_to_positional(self) -> bool:\n        \"\"\"\n        Should an integer key be treated as positional?\n        \"\"\"\n        if self.holds_integer() or self.is_boolean():\n            return False\n        return True\n\n    def _get_values_for_loc(self, series: \"Series\", loc, key):\n        \"\"\"\n        Do a positional lookup on the given Series, returning either a scalar\n        or a Series.\n\n        Assumes that `series.index is self`\n\n        key is included for MultiIndex compat.\n        \"\"\"\n        if is_integer(loc):\n            return series._values[loc]\n\n        return series.iloc[loc]\n\n    def set_value(self, arr, key, value):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray.\n\n        .. deprecated:: 1.0\n\n        Notes\n        -----\n        Only use this if you know what you're doing.\n        \"\"\"\n        warnings.warn(\n            (\n                \"The 'set_value' method is deprecated, and \"\n                \"will be removed in a future version.\"\n            ),\n            FutureWarning,\n            stacklevel=2,\n        )\n        loc = self._engine.get_loc(key)\n        validate_numeric_casting(arr.dtype, value)\n        arr[loc] = value\n\n    _index_shared_docs[\n        \"get_indexer_non_unique\"\n    ] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n\n        Returns\n        -------\n        indexer : ndarray of int\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n        missing : ndarray of int\n            An indexer into the target of the values not found.\n            These correspond to the -1 in the indexer array.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"get_indexer_non_unique\"] % _index_doc_kwargs)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n\n        if is_categorical_dtype(target.dtype):\n            tgt_values = np.asarray(target)\n        else:\n            tgt_values = target._get_engine_target()\n\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n\n    def get_indexer_for(self, target, **kwargs):\n        \"\"\"\n        Guaranteed return of an indexer even when non-unique.\n\n        This dispatches to get_indexer or get_indexer_non_unique\n        as appropriate.\n\n        Returns\n        -------\n        numpy.ndarray\n            List of indices.\n        \"\"\"\n        if self._index_as_unique:\n            return self.get_indexer(target, **kwargs)\n        indexer, _ = self.get_indexer_non_unique(target)\n        return indexer\n\n    @property\n    def _index_as_unique(self):\n        \"\"\"\n        Whether we should treat this as unique for the sake of\n        get_indexer vs get_indexer_non_unique.\n\n        For IntervalIndex compat.\n        \"\"\"\n        return self.is_unique\n\n    def _maybe_promote(self, other: \"Index\"):\n        \"\"\"\n        When dealing with an object-dtype Index and a non-object Index, see\n        if we can upcast the object-dtype one to improve performance.\n        \"\"\"\n\n        if self.inferred_type == \"date\" and isinstance(other, ABCDatetimeIndex):\n            try:\n                return type(other)(self), other\n            except OutOfBoundsDatetime:\n                return self, other\n        elif self.inferred_type == \"timedelta\" and isinstance(other, ABCTimedeltaIndex):\n            # TODO: we dont have tests that get here\n            return type(other)(self), other\n        elif self.inferred_type == \"boolean\":\n            if not is_object_dtype(self.dtype):\n                return self.astype(\"object\"), other.astype(\"object\")\n\n        if not is_object_dtype(self.dtype) and is_object_dtype(other.dtype):\n            # Reverse op so we dont need to re-implement on the subclasses\n            other, self = other._maybe_promote(self)\n\n        return self, other\n\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        \"\"\"\n        Can we compare values of the given dtype to our own?\n        \"\"\"\n        return True\n\n    def groupby(self, values) -> PrettyDict[Hashable, np.ndarray]:\n        \"\"\"\n        Group the index labels by a given array of values.\n\n        Parameters\n        ----------\n        values : array\n            Values used to determine the groups.\n\n        Returns\n        -------\n        dict\n            {group name -> group labels}\n        \"\"\"\n        # TODO: if we are a MultiIndex, we can do better\n        # that converting to tuples\n        if isinstance(values, ABCMultiIndex):\n            values = values._values\n        values = Categorical(values)\n        result = values._reverse_indexer()\n\n        # map to the label\n        result = {k: self.take(v) for k, v in result.items()}\n\n        return PrettyDict(result)\n\n    def map(self, mapper, na_action=None):\n        \"\"\"\n        Map values using input correspondence (a dict, Series, or function).\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}\n            If 'ignore', propagate NA values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        applied : Union[Index, MultiIndex], inferred\n            The output of the mapping function applied to the index.\n            If the function returns a tuple with more than one element\n            a MultiIndex will be returned.\n        \"\"\"\n        from pandas.core.indexes.multi import MultiIndex\n\n        new_values = super()._map_values(mapper, na_action=na_action)\n\n        attributes = self._get_attributes_dict()\n\n        # we can return a MultiIndex\n        if new_values.size and isinstance(new_values[0], tuple):\n            if isinstance(self, MultiIndex):\n                names = self.names\n            elif attributes.get(\"name\"):\n                names = [attributes.get(\"name\")] * len(new_values[0])\n            else:\n                names = None\n            return MultiIndex.from_tuples(new_values, names=names)\n\n        attributes[\"copy\"] = False\n        if not new_values.size:\n            # empty\n            attributes[\"dtype\"] = self.dtype\n\n        return Index(new_values, **attributes)\n\n    # TODO: De-duplicate with map, xref GH#32349\n    def _transform_index(self, func, level=None) -> \"Index\":\n        \"\"\"\n        Apply function to all values found in index.\n\n        This includes transforming multiindex entries separately.\n        Only apply function to one level of the MultiIndex if level is specified.\n        \"\"\"\n        if isinstance(self, ABCMultiIndex):\n            if level is not None:\n                items = [\n                    tuple(func(y) if i == level else y for i, y in enumerate(x))\n                    for x in self\n                ]\n            else:\n                items = [tuple(func(y) for y in x) for x in self]\n            return type(self).from_tuples(items, names=self.names)\n        else:\n            items = [func(x) for x in self]\n            return Index(items, name=self.name, tupleize_cols=False)\n\n    def isin(self, values, level=None):\n        \"\"\"\n        Return a boolean array where the index values are in `values`.\n\n        Compute boolean array of whether each index value is found in the\n        passed set of values. The length of the returned boolean array matches\n        the length of the index.\n\n        Parameters\n        ----------\n        values : set or list-like\n            Sought values.\n        level : str or int, optional\n            Name or position of the index level to use (if the index is a\n            `MultiIndex`).\n\n        Returns\n        -------\n        is_contained : ndarray\n            NumPy array of boolean values.\n\n        See Also\n        --------\n        Series.isin : Same for Series.\n        DataFrame.isin : Same method for DataFrames.\n\n        Notes\n        -----\n        In the case of `MultiIndex` you must either specify `values` as a\n        list-like object containing tuples that are the same length as the\n        number of levels, or specify `level`. Otherwise it will raise a\n        ``ValueError``.\n\n        If `level` is specified:\n\n        - if it is the name of one *and only one* index level, use that level;\n        - otherwise it should be a number indicating level position.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1,2,3])\n        >>> idx\n        Int64Index([1, 2, 3], dtype='int64')\n\n        Check whether each index value in a list of values.\n\n        >>> idx.isin([1, 4])\n        array([ True, False, False])\n\n        >>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n        ...                                  ['red', 'blue', 'green']],\n        ...                                  names=('number', 'color'))\n        >>> midx\n        MultiIndex([(1,   'red'),\n                    (2,  'blue'),\n                    (3, 'green')],\n                   names=['number', 'color'])\n\n        Check whether the strings in the 'color' level of the MultiIndex\n        are in a list of colors.\n\n        >>> midx.isin(['red', 'orange', 'yellow'], level='color')\n        array([ True, False, False])\n\n        To check across the levels of a MultiIndex, pass a list of tuples:\n\n        >>> midx.isin([(1, 'red'), (3, 'red')])\n        array([ True, False, False])\n\n        For a DatetimeIndex, string values in `values` are converted to\n        Timestamps.\n\n        >>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\n        >>> dti = pd.to_datetime(dates)\n        >>> dti\n        DatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\n        dtype='datetime64[ns]', freq=None)\n\n        >>> dti.isin(['2000-03-11'])\n        array([ True, False, False])\n        \"\"\"\n        if level is not None:\n            self._validate_index_level(level)\n        return algos.isin(self, values)\n\n    def _get_string_slice(self, key: str_t):\n        # this is for partial string indexing,\n        # overridden in DatetimeIndex, TimedeltaIndex and PeriodIndex\n        raise NotImplementedError\n\n    def slice_indexer(\n        self,\n        start: Optional[Label] = None,\n        end: Optional[Label] = None,\n        step: Optional[int] = None,\n        kind: Optional[str_t] = None,\n    ) -> slice:\n        \"\"\"\n        Compute the slice indexer for input labels and step.\n\n        Index needs to be ordered and unique.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning.\n        end : label, default None\n            If None, defaults to the end.\n        step : int, default None\n        kind : str, default None\n\n        Returns\n        -------\n        indexer : slice\n\n        Raises\n        ------\n        KeyError : If key does not exist, or key is not unique and index is\n            not ordered.\n\n        Notes\n        -----\n        This function assumes that the data is sorted, so use at your own peril\n\n        Examples\n        --------\n        This is a method on all index types. For example you can do:\n\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_indexer(start='b', end='c')\n        slice(1, 3, None)\n\n        >>> idx = pd.MultiIndex.from_arrays([list('abcd'), list('efgh')])\n        >>> idx.slice_indexer(start='b', end=('c', 'g'))\n        slice(1, 3, None)\n        \"\"\"\n        start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind)\n\n        # return a slice\n        if not is_scalar(start_slice):\n            raise AssertionError(\"Start slice bound is non-scalar\")\n        if not is_scalar(end_slice):\n            raise AssertionError(\"End slice bound is non-scalar\")\n\n        return slice(start_slice, end_slice, step)\n\n    def _maybe_cast_indexer(self, key):\n        \"\"\"\n        If we have a float key and are not a floating index, then try to cast\n        to an int if equivalent.\n        \"\"\"\n        if not self.is_floating():\n            return com.cast_scalar_indexer(key)\n        return key\n\n    def _validate_indexer(self, form: str_t, key, kind: str_t):\n        \"\"\"\n        If we are positional indexer, validate that we have appropriate\n        typed bounds must be an integer.\n        \"\"\"\n        assert kind in [\"getitem\", \"iloc\"]\n\n        if key is None:\n            pass\n        elif is_integer(key):\n            pass\n        else:\n            self._invalid_indexer(form, key)\n\n    def _maybe_cast_slice_bound(self, label, side: str_t, kind):\n        \"\"\"\n        This function should be overloaded in subclasses that allow non-trivial\n        casting on label-slice bounds, e.g. datetime-like indices allowing\n        strings containing formatted datetimes.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'loc', 'getitem'} or None\n\n        Returns\n        -------\n        label : object\n\n        Notes\n        -----\n        Value of `side` parameter should be validated in caller.\n        \"\"\"\n        assert kind in [\"loc\", \"getitem\", None]\n\n        # We are a plain index here (sub-class override this method if they\n        # wish to have special treatment for floats/ints, e.g. Float64Index and\n        # datetimelike Indexes\n        # reject them\n        if is_float(label):\n            self._invalid_indexer(\"slice\", label)\n\n        # we are trying to find integer bounds on a non-integer based index\n        # this is rejected (generally .loc gets you here)\n        elif is_integer(label):\n            self._invalid_indexer(\"slice\", label)\n\n        return label\n\n    def _searchsorted_monotonic(self, label, side=\"left\"):\n        if self.is_monotonic_increasing:\n            return self.searchsorted(label, side=side)\n        elif self.is_monotonic_decreasing:\n            # np.searchsorted expects ascending sort order, have to reverse\n            # everything for it to work (element ordering, search side and\n            # resulting value).\n            pos = self[::-1].searchsorted(\n                label, side=\"right\" if side == \"left\" else \"left\"\n            )\n            return len(self) - pos\n\n        raise ValueError(\"index must be monotonic increasing or decreasing\")\n\n    def get_slice_bound(self, label, side: str_t, kind) -> int:\n        \"\"\"\n        Calculate slice bound that corresponds to given label.\n\n        Returns leftmost (one-past-the-rightmost if ``side=='right'``) position\n        of given label.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'loc', 'getitem'} or None\n\n        Returns\n        -------\n        int\n            Index of label.\n        \"\"\"\n        assert kind in [\"loc\", \"getitem\", None]\n\n        if side not in (\"left\", \"right\"):\n            raise ValueError(\n                \"Invalid value for side kwarg, must be either \"\n                f\"'left' or 'right': {side}\"\n            )\n\n        original_label = label\n\n        # For datetime indices label may be a string that has to be converted\n        # to datetime boundary according to its resolution.\n        label = self._maybe_cast_slice_bound(label, side, kind)\n\n        # we need to look up the label\n        try:\n            slc = self.get_loc(label)\n        except KeyError as err:\n            try:\n                return self._searchsorted_monotonic(label, side)\n            except ValueError:\n                # raise the original KeyError\n                raise err\n\n        if isinstance(slc, np.ndarray):\n            # get_loc may return a boolean array or an array of indices, which\n            # is OK as long as they are representable by a slice.\n            if is_bool_dtype(slc):\n                slc = lib.maybe_booleans_to_slice(slc.view(\"u1\"))\n            else:\n                slc = lib.maybe_indices_to_slice(slc.astype(\"i8\"), len(self))\n            if isinstance(slc, np.ndarray):\n                raise KeyError(\n                    f\"Cannot get {side} slice bound for non-unique \"\n                    f\"label: {repr(original_label)}\"\n                )\n\n        if isinstance(slc, slice):\n            if side == \"left\":\n                return slc.start\n            else:\n                return slc.stop\n        else:\n            if side == \"right\":\n                return slc + 1\n            else:\n                return slc\n\n    def slice_locs(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        Compute slice locations for input labels.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning.\n        end : label, default None\n            If None, defaults to the end.\n        step : int, defaults None\n            If None, defaults to 1.\n        kind : {'loc', 'getitem'} or None\n\n        Returns\n        -------\n        start, end : int\n\n        See Also\n        --------\n        Index.get_loc : Get location for a single label.\n\n        Notes\n        -----\n        This method only works if the index is monotonic or unique.\n\n        Examples\n        --------\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_locs(start='b', end='c')\n        (1, 3)\n        \"\"\"\n        inc = step is None or step >= 0\n\n        if not inc:\n            # If it's a reverse slice, temporarily swap bounds.\n            start, end = end, start\n\n        # GH 16785: If start and end happen to be date strings with UTC offsets\n        # attempt to parse and check that the offsets are the same\n        if isinstance(start, (str, datetime)) and isinstance(end, (str, datetime)):\n            try:\n                ts_start = Timestamp(start)\n                ts_end = Timestamp(end)\n            except (ValueError, TypeError):\n                pass\n            else:\n                if not tz_compare(ts_start.tzinfo, ts_end.tzinfo):\n                    raise ValueError(\"Both dates must have the same UTC offset\")\n\n        start_slice = None\n        if start is not None:\n            start_slice = self.get_slice_bound(start, \"left\", kind)\n        if start_slice is None:\n            start_slice = 0\n\n        end_slice = None\n        if end is not None:\n            end_slice = self.get_slice_bound(end, \"right\", kind)\n        if end_slice is None:\n            end_slice = len(self)\n\n        if not inc:\n            # Bounds at this moment are swapped, swap them back and shift by 1.\n            #\n            # slice_locs('B', 'A', step=-1): s='B', e='A'\n            #\n            #              s='A'                 e='B'\n            # AFTER SWAP:    |                     |\n            #                v ------------------> V\n            #           -----------------------------------\n            #           | | |A|A|A|A| | | | | |B|B| | | | |\n            #           -----------------------------------\n            #              ^ <------------------ ^\n            # SHOULD BE:   |                     |\n            #           end=s-1              start=e-1\n            #\n            end_slice, start_slice = start_slice - 1, end_slice - 1\n\n            # i == -1 triggers ``len(self) + i`` selection that points to the\n            # last element, not before-the-first one, subtracting len(self)\n            # compensates that.\n            if end_slice == -1:\n                end_slice -= len(self)\n            if start_slice == -1:\n                start_slice -= len(self)\n\n        return start_slice, end_slice\n\n    def delete(self, loc):\n        \"\"\"\n        Make new Index with passed location(-s) deleted.\n\n        Parameters\n        ----------\n        loc : int or list of int\n            Location of item(-s) which will be deleted.\n            Use a list of locations to delete more than one value at the same time.\n\n        Returns\n        -------\n        Index\n            New Index with passed location(-s) deleted.\n\n        See Also\n        --------\n        numpy.delete : Delete any rows and column from NumPy array (ndarray).\n\n        Examples\n        --------\n        >>> idx = pd.Index(['a', 'b', 'c'])\n        >>> idx.delete(1)\n        Index(['a', 'c'], dtype='object')\n\n        >>> idx = pd.Index(['a', 'b', 'c'])\n        >>> idx.delete([0, 2])\n        Index(['b'], dtype='object')\n        \"\"\"\n        return self._shallow_copy(np.delete(self._data, loc))\n\n    def insert(self, loc: int, item):\n        \"\"\"\n        Make new Index inserting new item at location.\n\n        Follows Python list.append semantics for negative values.\n\n        Parameters\n        ----------\n        loc : int\n        item : object\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        # Note: this method is overridden by all ExtensionIndex subclasses,\n        #  so self is never backed by an EA.\n        arr = np.asarray(self)\n        item = self._coerce_scalar_to_index(item)._values\n        idx = np.concatenate((arr[:loc], item, arr[loc:]))\n        return Index(idx, name=self.name)\n\n    def drop(self, labels, errors: str_t = \"raise\"):\n        \"\"\"\n        Make new Index with passed list of labels deleted.\n\n        Parameters\n        ----------\n        labels : array-like\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and existing labels are dropped.\n\n        Returns\n        -------\n        dropped : Index\n\n        Raises\n        ------\n        KeyError\n            If not all of the labels are found in the selected axis\n        \"\"\"\n        arr_dtype = \"object\" if self.dtype == \"object\" else None\n        labels = com.index_labels_to_array(labels, dtype=arr_dtype)\n        indexer = self.get_indexer(labels)\n        mask = indexer == -1\n        if mask.any():\n            if errors != \"ignore\":\n                raise KeyError(f\"{labels[mask]} not found in axis\")\n            indexer = indexer[~mask]\n        return self.delete(indexer)\n\n    # --------------------------------------------------------------------\n    # Generated Arithmetic, Comparison, and Unary Methods\n\n    def _cmp_method(self, other, op):\n        \"\"\"\n        Wrapper used to dispatch comparison operations.\n        \"\"\"\n        if isinstance(other, (np.ndarray, Index, ABCSeries, ExtensionArray)):\n            if len(self) != len(other):\n                raise ValueError(\"Lengths must match to compare\")\n\n        if not isinstance(other, ABCMultiIndex):\n            other = extract_array(other, extract_numpy=True)\n        else:\n            other = np.asarray(other)\n\n        if is_object_dtype(self.dtype) and isinstance(other, ExtensionArray):\n            # e.g. PeriodArray, Categorical\n            with np.errstate(all=\"ignore\"):\n                result = op(self._values, other)\n\n        elif is_object_dtype(self.dtype) and not isinstance(self, ABCMultiIndex):\n            # don't pass MultiIndex\n            with np.errstate(all=\"ignore\"):\n                result = ops.comp_method_OBJECT_ARRAY(op, self._values, other)\n\n        elif is_interval_dtype(self.dtype):\n            with np.errstate(all=\"ignore\"):\n                result = op(self._values, np.asarray(other))\n\n        else:\n            with np.errstate(all=\"ignore\"):\n                result = ops.comparison_op(self._values, other, op)\n\n        return result\n\n    def _arith_method(self, other, op):\n        \"\"\"\n        Wrapper used to dispatch arithmetic operations.\n        \"\"\"\n\n        from pandas import Series\n\n        result = op(Series(self), other)\n        if isinstance(result, tuple):\n            return (Index(result[0]), Index(result[1]))\n        return Index(result)\n\n    def _unary_method(self, op):\n        result = op(self._values)\n        return Index(result, name=self.name)\n\n    def __abs__(self):\n        return self._unary_method(operator.abs)\n\n    def __neg__(self):\n        return self._unary_method(operator.neg)\n\n    def __pos__(self):\n        return self._unary_method(operator.pos)\n\n    def __inv__(self):\n        # TODO: why not operator.inv?\n        # TODO: __inv__ vs __invert__?\n        return self._unary_method(lambda x: -x)\n\n    def any(self, *args, **kwargs):\n        \"\"\"\n        Return whether any element is Truthy.\n\n        Parameters\n        ----------\n        *args\n            These parameters will be passed to numpy.any.\n        **kwargs\n            These parameters will be passed to numpy.any.\n\n        Returns\n        -------\n        any : bool or array_like (if axis is specified)\n            A single element array_like may be converted to bool.\n\n        See Also\n        --------\n        Index.all : Return whether all elements are True.\n        Series.all : Return whether all elements are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        >>> index = pd.Index([0, 1, 2])\n        >>> index.any()\n        True\n\n        >>> index = pd.Index([0, 0, 0])\n        >>> index.any()\n        False\n        \"\"\"\n        # FIXME: docstr inaccurate, args/kwargs not passed\n        self._maybe_disable_logical_methods(\"any\")\n        return np.any(self.values)\n\n    def all(self):\n        \"\"\"\n        Return whether all elements are Truthy.\n\n        Parameters\n        ----------\n        *args\n            These parameters will be passed to numpy.all.\n        **kwargs\n            These parameters will be passed to numpy.all.\n\n        Returns\n        -------\n        all : bool or array_like (if axis is specified)\n            A single element array_like may be converted to bool.\n\n        See Also\n        --------\n        Index.any : Return whether any element in an Index is True.\n        Series.any : Return whether any element in a Series is True.\n        Series.all : Return whether all elements in a Series are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        **all**\n\n        True, because nonzero integers are considered True.\n\n        >>> pd.Index([1, 2, 3]).all()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 1, 2]).all()\n        False\n\n        **any**\n\n        True, because ``1`` is considered True.\n\n        >>> pd.Index([0, 0, 1]).any()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 0, 0]).any()\n        False\n        \"\"\"\n        # FIXME: docstr inaccurate, args/kwargs not passed\n\n        self._maybe_disable_logical_methods(\"all\")\n        return np.all(self.values)\n\n    def _maybe_disable_logical_methods(self, opname: str_t):\n        \"\"\"\n        raise if this Index subclass does not support any or all.\n        \"\"\"\n        if (\n            isinstance(self, ABCMultiIndex)\n            or needs_i8_conversion(self.dtype)\n            or is_interval_dtype(self.dtype)\n            or is_categorical_dtype(self.dtype)\n            or is_float_dtype(self.dtype)\n        ):\n            # This call will raise\n            make_invalid_op(opname)(self)\n\n    @property\n    def shape(self):\n        \"\"\"\n        Return a tuple of the shape of the underlying data.\n        \"\"\"\n        # not using \"(len(self), )\" to return \"correct\" shape if the values\n        # consists of a >1 D array (see GH-27775)\n        # overridden in MultiIndex.shape to avoid materializing the values\n        return self._values.shape\n\n\ndef ensure_index_from_sequences(sequences, names=None):\n    \"\"\"\n    Construct an index from sequences of data.\n\n    A single sequence returns an Index. Many sequences returns a\n    MultiIndex.\n\n    Parameters\n    ----------\n    sequences : sequence of sequences\n    names : sequence of str\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    Examples\n    --------\n    >>> ensure_index_from_sequences([[1, 2, 3]], names=[\"name\"])\n    Int64Index([1, 2, 3], dtype='int64', name='name')\n\n    >>> ensure_index_from_sequences([[\"a\", \"a\"], [\"a\", \"b\"]], names=[\"L1\", \"L2\"])\n    MultiIndex([('a', 'a'),\n                ('a', 'b')],\n               names=['L1', 'L2'])\n\n    See Also\n    --------\n    ensure_index\n    \"\"\"\n    from pandas.core.indexes.multi import MultiIndex\n\n    if len(sequences) == 1:\n        if names is not None:\n            names = names[0]\n        return Index(sequences[0], name=names)\n    else:\n        return MultiIndex.from_arrays(sequences, names=names)\n\n\ndef ensure_index(\n    index_like: Union[AnyArrayLike, Sequence], copy: bool = False\n) -> Index:\n    \"\"\"\n    Ensure that we have an index from some index-like object.\n\n    Parameters\n    ----------\n    index_like : sequence\n        An Index or other sequence\n    copy : bool, default False\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    See Also\n    --------\n    ensure_index_from_sequences\n\n    Examples\n    --------\n    >>> ensure_index(['a', 'b'])\n    Index(['a', 'b'], dtype='object')\n\n    >>> ensure_index([('a', 'a'),  ('b', 'c')])\n    Index([('a', 'a'), ('b', 'c')], dtype='object')\n\n    >>> ensure_index([['a', 'a'], ['b', 'c']])\n    MultiIndex([('a', 'b'),\n            ('a', 'c')],\n           )\n    \"\"\"\n    if isinstance(index_like, Index):\n        if copy:\n            index_like = index_like.copy()\n        return index_like\n    if hasattr(index_like, \"name\"):\n        # https://github.com/python/mypy/issues/1424\n        # error: Item \"ExtensionArray\" of \"Union[ExtensionArray,\n        # Sequence[Any]]\" has no attribute \"name\"  [union-attr]\n        # error: Item \"Sequence[Any]\" of \"Union[ExtensionArray, Sequence[Any]]\"\n        # has no attribute \"name\"  [union-attr]\n        # error: \"Sequence[Any]\" has no attribute \"name\"  [attr-defined]\n        # error: Item \"Sequence[Any]\" of \"Union[Series, Sequence[Any]]\" has no\n        # attribute \"name\"  [union-attr]\n        # error: Item \"Sequence[Any]\" of \"Union[Any, Sequence[Any]]\" has no\n        # attribute \"name\"  [union-attr]\n        name = index_like.name  # type: ignore[union-attr, attr-defined]\n        return Index(index_like, name=name, copy=copy)\n\n    if is_iterator(index_like):\n        index_like = list(index_like)\n\n    # must check for exactly list here because of strict type\n    # check in clean_index_list\n    if isinstance(index_like, list):\n        if type(index_like) != list:\n            index_like = list(index_like)\n\n        converted, all_arrays = lib.clean_index_list(index_like)\n\n        if len(converted) > 0 and all_arrays:\n            from pandas.core.indexes.multi import MultiIndex\n\n            return MultiIndex.from_arrays(converted)\n        else:\n            if isinstance(converted, np.ndarray) and converted.dtype == np.int64:\n                # Check for overflows if we should actually be uint64\n                # xref GH#35481\n                alt = np.asarray(index_like)\n                if alt.dtype == np.uint64:\n                    converted = alt\n\n            index_like = converted\n    else:\n        # clean_index_list does the equivalent of copying\n        # so only need to do this if not list instance\n        if copy:\n            index_like = copy_func(index_like)\n\n    return Index(index_like)\n\n\ndef ensure_has_len(seq):\n    \"\"\"\n    If seq is an iterator, put its values into a list.\n    \"\"\"\n    try:\n        len(seq)\n    except TypeError:\n        return list(seq)\n    else:\n        return seq\n\n\ndef trim_front(strings: List[str]) -> List[str]:\n    \"\"\"\n    Trims zeros and decimal points.\n    \"\"\"\n    trimmed = strings\n    while len(strings) > 0 and all(x[0] == \" \" for x in trimmed):\n        trimmed = [x[1:] for x in trimmed]\n    return trimmed\n\n\ndef _validate_join_method(method: str):\n    if method not in [\"left\", \"right\", \"inner\", \"outer\"]:\n        raise ValueError(f\"do not recognize join method {method}\")\n\n\ndef default_index(n: int) -> \"RangeIndex\":\n    from pandas.core.indexes.range import RangeIndex\n\n    return RangeIndex(0, n, name=None)\n\n\ndef maybe_extract_name(name, obj, cls) -> Label:\n    \"\"\"\n    If no name is passed, then extract it from data, validating hashability.\n    \"\"\"\n    if name is None and isinstance(obj, (Index, ABCSeries)):\n        # Note we don't just check for \"name\" attribute since that would\n        #  pick up e.g. dtype.name\n        name = obj.name\n\n    # GH#29069\n    if not is_hashable(name):\n        raise TypeError(f\"{cls.__name__}.name must be a hashable type\")\n\n    return name\n\n\ndef _maybe_cast_with_dtype(data: np.ndarray, dtype: np.dtype, copy: bool) -> np.ndarray:\n    \"\"\"\n    If a dtype is passed, cast to the closest matching dtype that is supported\n    by Index.\n\n    Parameters\n    ----------\n    data : np.ndarray\n    dtype : np.dtype\n    copy : bool\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n    # we need to avoid having numpy coerce\n    # things that look like ints/floats to ints unless\n    # they are actually ints, e.g. '0' and 0.0\n    # should not be coerced\n    # GH 11836\n    if is_integer_dtype(dtype):\n        inferred = lib.infer_dtype(data, skipna=False)\n        if inferred == \"integer\":\n            data = maybe_cast_to_integer_array(data, dtype, copy=copy)\n        elif inferred in [\"floating\", \"mixed-integer-float\"]:\n            if isna(data).any():\n                raise ValueError(\"cannot convert float NaN to integer\")\n\n            if inferred == \"mixed-integer-float\":\n                data = maybe_cast_to_integer_array(data, dtype)\n\n            # If we are actually all equal to integers,\n            # then coerce to integer.\n            try:\n                data = _try_convert_to_int_array(data, copy, dtype)\n            except ValueError:\n                data = np.array(data, dtype=np.float64, copy=copy)\n\n        elif inferred == \"string\":\n            pass\n        else:\n            data = data.astype(dtype)\n    elif is_float_dtype(dtype):\n        inferred = lib.infer_dtype(data, skipna=False)\n        if inferred == \"string\":\n            pass\n        else:\n            data = data.astype(dtype)\n    else:\n        data = np.array(data, dtype=dtype, copy=copy)\n\n    return data\n\n\ndef _maybe_cast_data_without_dtype(subarr):\n    \"\"\"\n    If we have an arraylike input but no passed dtype, try to infer\n    a supported dtype.\n\n    Parameters\n    ----------\n    subarr : np.ndarray, Index, or Series\n\n    Returns\n    -------\n    converted : np.ndarray or ExtensionArray\n    dtype : np.dtype or ExtensionDtype\n    \"\"\"\n    # Runtime import needed bc IntervalArray imports Index\n    from pandas.core.arrays import (\n        DatetimeArray,\n        IntervalArray,\n        PeriodArray,\n        TimedeltaArray,\n    )\n\n    inferred = lib.infer_dtype(subarr, skipna=False)\n\n    if inferred == \"integer\":\n        try:\n            data = _try_convert_to_int_array(subarr, False, None)\n            return data, data.dtype\n        except ValueError:\n            pass\n\n        return subarr, object\n\n    elif inferred in [\"floating\", \"mixed-integer-float\", \"integer-na\"]:\n        # TODO: Returns IntegerArray for integer-na case in the future\n        return subarr, np.float64\n\n    elif inferred == \"interval\":\n        try:\n            data = IntervalArray._from_sequence(subarr, copy=False)\n            return data, data.dtype\n        except ValueError:\n            # GH27172: mixed closed Intervals --> object dtype\n            pass\n    elif inferred == \"boolean\":\n        # don't support boolean explicitly ATM\n        pass\n    elif inferred != \"string\":\n        if inferred.startswith(\"datetime\"):\n            try:\n                data = DatetimeArray._from_sequence(subarr, copy=False)\n                return data, data.dtype\n            except (ValueError, OutOfBoundsDatetime):\n                # GH 27011\n                # If we have mixed timezones, just send it\n                # down the base constructor\n                pass\n\n        elif inferred.startswith(\"timedelta\"):\n            data = TimedeltaArray._from_sequence(subarr, copy=False)\n            return data, data.dtype\n        elif inferred == \"period\":\n            try:\n                data = PeriodArray._from_sequence(subarr)\n                return data, data.dtype\n            except IncompatibleFrequency:\n                pass\n\n    return subarr, subarr.dtype\n\n\ndef _try_convert_to_int_array(\n    data: np.ndarray, copy: bool, dtype: np.dtype\n) -> np.ndarray:\n    \"\"\"\n    Attempt to convert an array of data into an integer array.\n\n    Parameters\n    ----------\n    data : The data to convert.\n    copy : bool\n        Whether to copy the data or not.\n    dtype : np.dtype\n\n    Returns\n    -------\n    int_array : data converted to either an ndarray[int64] or ndarray[uint64]\n\n    Raises\n    ------\n    ValueError if the conversion was not successful.\n    \"\"\"\n    if not is_unsigned_integer_dtype(dtype):\n        # skip int64 conversion attempt if uint-like dtype is passed, as\n        # this could return Int64Index when UInt64Index is what's desired\n        try:\n            res = data.astype(\"i8\", copy=False)\n            if (res == data).all():\n                return res  # TODO: might still need to copy\n        except (OverflowError, TypeError, ValueError):\n            pass\n\n    # Conversion to int64 failed (possibly due to overflow) or was skipped,\n    # so let's try now with uint64.\n    try:\n        res = data.astype(\"u8\", copy=False)\n        if (res == data).all():\n            return res  # TODO: might still need to copy\n    except (OverflowError, TypeError, ValueError):\n        pass\n\n    raise ValueError\n\n\ndef _maybe_asobject(dtype, klass, data, copy: bool, name: Label, **kwargs):\n    \"\"\"\n    If an object dtype was specified, create the non-object Index\n    and then convert it to object.\n\n    Parameters\n    ----------\n    dtype : np.dtype, ExtensionDtype, str\n    klass : Index subclass\n    data : list-like\n    copy : bool\n    name : hashable\n    **kwargs\n\n    Returns\n    -------\n    Index\n\n    Notes\n    -----\n    We assume that calling .astype(object) on this klass will make a copy.\n    \"\"\"\n\n    # GH#23524 passing `dtype=object` to DatetimeIndex is invalid,\n    #  will raise in the where `data` is already tz-aware.  So\n    #  we leave it out of this step and cast to object-dtype after\n    #  the DatetimeIndex construction.\n\n    if is_dtype_equal(_o_dtype, dtype):\n        # Note we can pass copy=False because the .astype below\n        #  will always make a copy\n        index = klass(data, copy=False, name=name, **kwargs)\n        return index.astype(object)\n\n    return klass(data, dtype=dtype, copy=copy, name=name, **kwargs)\n\n\ndef get_unanimous_names(*indexes: Index) -> Tuple[Label, ...]:\n    \"\"\"\n    Return common name if all indices agree, otherwise None (level-by-level).\n\n    Parameters\n    ----------\n    indexes : list of Index objects\n\n    Returns\n    -------\n    list\n        A list representing the unanimous 'names' found.\n    \"\"\"\n    name_tups = [tuple(i.names) for i in indexes]\n    name_sets = [{*ns} for ns in zip_longest(*name_tups)]\n    names = tuple(ns.pop() if len(ns) == 1 else None for ns in name_sets)\n    return names\n"
    },
    {
      "filename": "pandas/core/resample.py",
      "content": "import copy\nfrom datetime import timedelta\nfrom textwrap import dedent\nfrom typing import Dict, Optional, Union, no_type_check\n\nimport numpy as np\n\nfrom pandas._libs import lib\nfrom pandas._libs.tslibs import (\n    IncompatibleFrequency,\n    NaT,\n    Period,\n    Timedelta,\n    Timestamp,\n    to_offset,\n)\nfrom pandas._typing import TimedeltaConvertibleTypes, TimestampConvertibleTypes\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import Appender, Substitution, doc\n\nfrom pandas.core.dtypes.generic import ABCDataFrame, ABCSeries\n\nfrom pandas.core.aggregation import aggregate\nimport pandas.core.algorithms as algos\nfrom pandas.core.base import DataError\nfrom pandas.core.generic import NDFrame, _shared_docs\nfrom pandas.core.groupby.base import GotItemMixin, ShallowMixin\nfrom pandas.core.groupby.generic import SeriesGroupBy\nfrom pandas.core.groupby.groupby import (\n    BaseGroupBy,\n    GroupBy,\n    _pipe_template,\n    get_groupby,\n)\nfrom pandas.core.groupby.grouper import Grouper\nfrom pandas.core.groupby.ops import BinGrouper\nfrom pandas.core.indexes.api import Index\nfrom pandas.core.indexes.datetimes import DatetimeIndex, date_range\nfrom pandas.core.indexes.period import PeriodIndex, period_range\nfrom pandas.core.indexes.timedeltas import TimedeltaIndex, timedelta_range\n\nfrom pandas.tseries.frequencies import is_subperiod, is_superperiod\nfrom pandas.tseries.offsets import DateOffset, Day, Nano, Tick\n\n_shared_docs_kwargs: Dict[str, str] = dict()\n\n\nclass Resampler(BaseGroupBy, ShallowMixin):\n    \"\"\"\n    Class for resampling datetimelike data, a groupby-like operation.\n    See aggregate, transform, and apply functions on this object.\n\n    It's easiest to use obj.resample(...) to use Resampler.\n\n    Parameters\n    ----------\n    obj : pandas object\n    groupby : a TimeGrouper object\n    axis : int, default 0\n    kind : str or None\n        'period', 'timestamp' to override default index treatment\n\n    Returns\n    -------\n    a Resampler of the appropriate type\n\n    Notes\n    -----\n    After resampling, see aggregate, apply, and transform functions.\n    \"\"\"\n\n    # to the groupby descriptor\n    _attributes = [\n        \"freq\",\n        \"axis\",\n        \"closed\",\n        \"label\",\n        \"convention\",\n        \"loffset\",\n        \"kind\",\n        \"origin\",\n        \"offset\",\n    ]\n\n    def __init__(self, obj, groupby=None, axis=0, kind=None, **kwargs):\n        self.groupby = groupby\n        self.keys = None\n        self.sort = True\n        self.axis = axis\n        self.kind = kind\n        self.squeeze = False\n        self.group_keys = True\n        self.as_index = True\n        self.exclusions = set()\n        self.binner = None\n        self.grouper = None\n\n        if self.groupby is not None:\n            self.groupby._set_grouper(self._convert_obj(obj), sort=True)\n\n    def __str__(self) -> str:\n        \"\"\"\n        Provide a nice str repr of our rolling object.\n        \"\"\"\n        attrs = (\n            f\"{k}={getattr(self.groupby, k)}\"\n            for k in self._attributes\n            if getattr(self.groupby, k, None) is not None\n        )\n        return f\"{type(self).__name__} [{', '.join(attrs)}]\"\n\n    def __getattr__(self, attr: str):\n        if attr in self._internal_names_set:\n            return object.__getattribute__(self, attr)\n        if attr in self._attributes:\n            return getattr(self.groupby, attr)\n        if attr in self.obj:\n            return self[attr]\n\n        return object.__getattribute__(self, attr)\n\n    def __iter__(self):\n        \"\"\"\n        Resampler iterator.\n\n        Returns\n        -------\n        Generator yielding sequence of (name, subsetted object)\n        for each group.\n\n        See Also\n        --------\n        GroupBy.__iter__ : Generator yielding sequence for each group.\n        \"\"\"\n        self._set_binner()\n        return super().__iter__()\n\n    @property\n    def obj(self):\n        return self.groupby.obj\n\n    @property\n    def ax(self):\n        return self.groupby.ax\n\n    @property\n    def _typ(self) -> str:\n        \"\"\"\n        Masquerade for compat as a Series or a DataFrame.\n        \"\"\"\n        if isinstance(self._selected_obj, ABCSeries):\n            return \"series\"\n        return \"dataframe\"\n\n    @property\n    def _from_selection(self) -> bool:\n        \"\"\"\n        Is the resampling from a DataFrame column or MultiIndex level.\n        \"\"\"\n        # upsampling and PeriodIndex resampling do not work\n        # with selection, this state used to catch and raise an error\n        return self.groupby is not None and (\n            self.groupby.key is not None or self.groupby.level is not None\n        )\n\n    def _convert_obj(self, obj):\n        \"\"\"\n        Provide any conversions for the object in order to correctly handle.\n\n        Parameters\n        ----------\n        obj : the object to be resampled\n\n        Returns\n        -------\n        obj : converted object\n        \"\"\"\n        obj = obj._consolidate()\n        return obj\n\n    def _get_binner_for_time(self):\n        raise AbstractMethodError(self)\n\n    def _set_binner(self):\n        \"\"\"\n        Setup our binners.\n\n        Cache these as we are an immutable object\n        \"\"\"\n        if self.binner is None:\n            self.binner, self.grouper = self._get_binner()\n\n    def _get_binner(self):\n        \"\"\"\n        Create the BinGrouper, assume that self.set_grouper(obj)\n        has already been called.\n        \"\"\"\n        binner, bins, binlabels = self._get_binner_for_time()\n        assert len(bins) == len(binlabels)\n        bin_grouper = BinGrouper(bins, binlabels, indexer=self.groupby.indexer)\n        return binner, bin_grouper\n\n    def _assure_grouper(self):\n        \"\"\"\n        Make sure that we are creating our binner & grouper.\n        \"\"\"\n        self._set_binner()\n\n    @Substitution(\n        klass=\"Resampler\",\n        examples=\"\"\"\n    >>> df = pd.DataFrame({'A': [1, 2, 3, 4]},\n    ...                   index=pd.date_range('2012-08-02', periods=4))\n    >>> df\n                A\n    2012-08-02  1\n    2012-08-03  2\n    2012-08-04  3\n    2012-08-05  4\n\n    To get the difference between each 2-day period's maximum and minimum\n    value in one pass, you can do\n\n    >>> df.resample('2D').pipe(lambda x: x.max() - x.min())\n                A\n    2012-08-02  1\n    2012-08-04  1\"\"\",\n    )\n    @Appender(_pipe_template)\n    def pipe(self, func, *args, **kwargs):\n        return super().pipe(func, *args, **kwargs)\n\n    _agg_see_also_doc = dedent(\n        \"\"\"\n    See Also\n    --------\n    DataFrame.groupby.aggregate : Aggregate using callable, string, dict,\n        or list of string/callables.\n    DataFrame.resample.transform : Transforms the Series on each group\n        based on the given function.\n    DataFrame.aggregate: Aggregate using one or more\n        operations over the specified axis.\n    \"\"\"\n    )\n\n    _agg_examples_doc = dedent(\n        \"\"\"\n    Examples\n    --------\n    >>> s = pd.Series([1,2,3,4,5],\n                      index=pd.date_range('20130101', periods=5,freq='s'))\n    2013-01-01 00:00:00    1\n    2013-01-01 00:00:01    2\n    2013-01-01 00:00:02    3\n    2013-01-01 00:00:03    4\n    2013-01-01 00:00:04    5\n    Freq: S, dtype: int64\n\n    >>> r = s.resample('2s')\n    DatetimeIndexResampler [freq=<2 * Seconds>, axis=0, closed=left,\n                            label=left, convention=start]\n\n    >>> r.agg(np.sum)\n    2013-01-01 00:00:00    3\n    2013-01-01 00:00:02    7\n    2013-01-01 00:00:04    5\n    Freq: 2S, dtype: int64\n\n    >>> r.agg(['sum','mean','max'])\n                         sum  mean  max\n    2013-01-01 00:00:00    3   1.5    2\n    2013-01-01 00:00:02    7   3.5    4\n    2013-01-01 00:00:04    5   5.0    5\n\n    >>> r.agg({'result' : lambda x: x.mean() / x.std(),\n               'total' : np.sum})\n                         total    result\n    2013-01-01 00:00:00      3  2.121320\n    2013-01-01 00:00:02      7  4.949747\n    2013-01-01 00:00:04      5       NaN\n    \"\"\"\n    )\n\n    @doc(\n        _shared_docs[\"aggregate\"],\n        see_also=_agg_see_also_doc,\n        examples=_agg_examples_doc,\n        klass=\"DataFrame\",\n        axis=\"\",\n    )\n    def aggregate(self, func, *args, **kwargs):\n\n        self._set_binner()\n        result, how = aggregate(self, func, *args, **kwargs)\n        if result is None:\n            how = func\n            grouper = None\n            result = self._groupby_and_aggregate(how, grouper, *args, **kwargs)\n\n        result = self._apply_loffset(result)\n        return result\n\n    agg = aggregate\n    apply = aggregate\n\n    def transform(self, arg, *args, **kwargs):\n        \"\"\"\n        Call function producing a like-indexed Series on each group and return\n        a Series with the transformed values.\n\n        Parameters\n        ----------\n        arg : function\n            To apply to each group. Should return a Series with the same index.\n\n        Returns\n        -------\n        transformed : Series\n\n        Examples\n        --------\n        >>> resampled.transform(lambda x: (x - x.mean()) / x.std())\n        \"\"\"\n        return self._selected_obj.groupby(self.groupby).transform(arg, *args, **kwargs)\n\n    def _downsample(self, f):\n        raise AbstractMethodError(self)\n\n    def _upsample(self, f, limit=None, fill_value=None):\n        raise AbstractMethodError(self)\n\n    def _gotitem(self, key, ndim: int, subset=None):\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : {1, 2}\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n        self._set_binner()\n        grouper = self.grouper\n        if subset is None:\n            subset = self.obj\n        grouped = get_groupby(subset, by=None, grouper=grouper, axis=self.axis)\n\n        # try the key selection\n        try:\n            return grouped[key]\n        except KeyError:\n            return grouped\n\n    def _groupby_and_aggregate(self, how, grouper=None, *args, **kwargs):\n        \"\"\"\n        Re-evaluate the obj with a groupby aggregation.\n        \"\"\"\n        if grouper is None:\n            self._set_binner()\n            grouper = self.grouper\n\n        obj = self._selected_obj\n\n        grouped = get_groupby(obj, by=None, grouper=grouper, axis=self.axis)\n\n        try:\n            if isinstance(obj, ABCDataFrame) and callable(how):\n                # Check if the function is reducing or not.\n                result = grouped._aggregate_item_by_item(how, *args, **kwargs)\n            else:\n                result = grouped.aggregate(how, *args, **kwargs)\n        except (DataError, AttributeError, KeyError):\n            # we have a non-reducing function; try to evaluate\n            # alternatively we want to evaluate only a column of the input\n            result = grouped.apply(how, *args, **kwargs)\n        except ValueError as err:\n            if \"Must produce aggregated value\" in str(err):\n                # raised in _aggregate_named\n                pass\n            elif \"len(index) != len(labels)\" in str(err):\n                # raised in libgroupby validation\n                pass\n            elif \"No objects to concatenate\" in str(err):\n                # raised in concat call\n                #  In tests this is reached via either\n                #  _apply_to_column_groupbys (ohlc) or DataFrameGroupBy.nunique\n                pass\n            else:\n                raise\n\n            # we have a non-reducing function\n            # try to evaluate\n            result = grouped.apply(how, *args, **kwargs)\n\n        result = self._apply_loffset(result)\n        return self._wrap_result(result)\n\n    def _apply_loffset(self, result):\n        \"\"\"\n        If loffset is set, offset the result index.\n\n        This is NOT an idempotent routine, it will be applied\n        exactly once to the result.\n\n        Parameters\n        ----------\n        result : Series or DataFrame\n            the result of resample\n        \"\"\"\n        needs_offset = (\n            isinstance(self.loffset, (DateOffset, timedelta, np.timedelta64))\n            and isinstance(result.index, DatetimeIndex)\n            and len(result.index) > 0\n        )\n\n        if needs_offset:\n            result.index = result.index + self.loffset\n\n        self.loffset = None\n        return result\n\n    def _get_resampler_for_grouping(self, groupby, **kwargs):\n        \"\"\"\n        Return the correct class for resampling with groupby.\n        \"\"\"\n        return self._resampler_for_grouping(self, groupby=groupby, **kwargs)\n\n    def _wrap_result(self, result):\n        \"\"\"\n        Potentially wrap any results.\n        \"\"\"\n        if isinstance(result, ABCSeries) and self._selection is not None:\n            result.name = self._selection\n\n        if isinstance(result, ABCSeries) and result.empty:\n            obj = self.obj\n            result.index = _asfreq_compat(obj.index, freq=self.freq)\n            result.name = getattr(obj, \"name\", None)\n\n        return result\n\n    def pad(self, limit=None):\n        \"\"\"\n        Forward fill the values.\n\n        Parameters\n        ----------\n        limit : int, optional\n            Limit of how many values to fill.\n\n        Returns\n        -------\n        An upsampled Series.\n\n        See Also\n        --------\n        Series.fillna: Fill NA/NaN values using the specified method.\n        DataFrame.fillna: Fill NA/NaN values using the specified method.\n        \"\"\"\n        return self._upsample(\"pad\", limit=limit)\n\n    ffill = pad\n\n    def nearest(self, limit=None):\n        \"\"\"\n        Resample by using the nearest value.\n\n        When resampling data, missing values may appear (e.g., when the\n        resampling frequency is higher than the original frequency).\n        The `nearest` method will replace ``NaN`` values that appeared in\n        the resampled data with the value from the nearest member of the\n        sequence, based on the index value.\n        Missing values that existed in the original data will not be modified.\n        If `limit` is given, fill only this many values in each direction for\n        each of the original values.\n\n        Parameters\n        ----------\n        limit : int, optional\n            Limit of how many values to fill.\n\n        Returns\n        -------\n        Series or DataFrame\n            An upsampled Series or DataFrame with ``NaN`` values filled with\n            their nearest value.\n\n        See Also\n        --------\n        backfill : Backward fill the new missing values in the resampled data.\n        pad : Forward fill ``NaN`` values.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2],\n        ...               index=pd.date_range('20180101',\n        ...                                   periods=2,\n        ...                                   freq='1h'))\n        >>> s\n        2018-01-01 00:00:00    1\n        2018-01-01 01:00:00    2\n        Freq: H, dtype: int64\n\n        >>> s.resample('15min').nearest()\n        2018-01-01 00:00:00    1\n        2018-01-01 00:15:00    1\n        2018-01-01 00:30:00    2\n        2018-01-01 00:45:00    2\n        2018-01-01 01:00:00    2\n        Freq: 15T, dtype: int64\n\n        Limit the number of upsampled values imputed by the nearest:\n\n        >>> s.resample('15min').nearest(limit=1)\n        2018-01-01 00:00:00    1.0\n        2018-01-01 00:15:00    1.0\n        2018-01-01 00:30:00    NaN\n        2018-01-01 00:45:00    2.0\n        2018-01-01 01:00:00    2.0\n        Freq: 15T, dtype: float64\n        \"\"\"\n        return self._upsample(\"nearest\", limit=limit)\n\n    def backfill(self, limit=None):\n        \"\"\"\n        Backward fill the new missing values in the resampled data.\n\n        In statistics, imputation is the process of replacing missing data with\n        substituted values [1]_. When resampling data, missing values may\n        appear (e.g., when the resampling frequency is higher than the original\n        frequency). The backward fill will replace NaN values that appeared in\n        the resampled data with the next value in the original sequence.\n        Missing values that existed in the original data will not be modified.\n\n        Parameters\n        ----------\n        limit : int, optional\n            Limit of how many values to fill.\n\n        Returns\n        -------\n        Series, DataFrame\n            An upsampled Series or DataFrame with backward filled NaN values.\n\n        See Also\n        --------\n        bfill : Alias of backfill.\n        fillna : Fill NaN values using the specified method, which can be\n            'backfill'.\n        nearest : Fill NaN values with nearest neighbor starting from center.\n        pad : Forward fill NaN values.\n        Series.fillna : Fill NaN values in the Series using the\n            specified method, which can be 'backfill'.\n        DataFrame.fillna : Fill NaN values in the DataFrame using the\n            specified method, which can be 'backfill'.\n\n        References\n        ----------\n        .. [1] https://en.wikipedia.org/wiki/Imputation_(statistics)\n\n        Examples\n        --------\n        Resampling a Series:\n\n        >>> s = pd.Series([1, 2, 3],\n        ...               index=pd.date_range('20180101', periods=3, freq='h'))\n        >>> s\n        2018-01-01 00:00:00    1\n        2018-01-01 01:00:00    2\n        2018-01-01 02:00:00    3\n        Freq: H, dtype: int64\n\n        >>> s.resample('30min').backfill()\n        2018-01-01 00:00:00    1\n        2018-01-01 00:30:00    2\n        2018-01-01 01:00:00    2\n        2018-01-01 01:30:00    3\n        2018-01-01 02:00:00    3\n        Freq: 30T, dtype: int64\n\n        >>> s.resample('15min').backfill(limit=2)\n        2018-01-01 00:00:00    1.0\n        2018-01-01 00:15:00    NaN\n        2018-01-01 00:30:00    2.0\n        2018-01-01 00:45:00    2.0\n        2018-01-01 01:00:00    2.0\n        2018-01-01 01:15:00    NaN\n        2018-01-01 01:30:00    3.0\n        2018-01-01 01:45:00    3.0\n        2018-01-01 02:00:00    3.0\n        Freq: 15T, dtype: float64\n\n        Resampling a DataFrame that has missing values:\n\n        >>> df = pd.DataFrame({'a': [2, np.nan, 6], 'b': [1, 3, 5]},\n        ...                   index=pd.date_range('20180101', periods=3,\n        ...                                       freq='h'))\n        >>> df\n                               a  b\n        2018-01-01 00:00:00  2.0  1\n        2018-01-01 01:00:00  NaN  3\n        2018-01-01 02:00:00  6.0  5\n\n        >>> df.resample('30min').backfill()\n                               a  b\n        2018-01-01 00:00:00  2.0  1\n        2018-01-01 00:30:00  NaN  3\n        2018-01-01 01:00:00  NaN  3\n        2018-01-01 01:30:00  6.0  5\n        2018-01-01 02:00:00  6.0  5\n\n        >>> df.resample('15min').backfill(limit=2)\n                               a    b\n        2018-01-01 00:00:00  2.0  1.0\n        2018-01-01 00:15:00  NaN  NaN\n        2018-01-01 00:30:00  NaN  3.0\n        2018-01-01 00:45:00  NaN  3.0\n        2018-01-01 01:00:00  NaN  3.0\n        2018-01-01 01:15:00  NaN  NaN\n        2018-01-01 01:30:00  6.0  5.0\n        2018-01-01 01:45:00  6.0  5.0\n        2018-01-01 02:00:00  6.0  5.0\n        \"\"\"\n        return self._upsample(\"backfill\", limit=limit)\n\n    bfill = backfill\n\n    def fillna(self, method, limit=None):\n        \"\"\"\n        Fill missing values introduced by upsampling.\n\n        In statistics, imputation is the process of replacing missing data with\n        substituted values [1]_. When resampling data, missing values may\n        appear (e.g., when the resampling frequency is higher than the original\n        frequency).\n\n        Missing values that existed in the original data will\n        not be modified.\n\n        Parameters\n        ----------\n        method : {'pad', 'backfill', 'ffill', 'bfill', 'nearest'}\n            Method to use for filling holes in resampled data\n\n            * 'pad' or 'ffill': use previous valid observation to fill gap\n              (forward fill).\n            * 'backfill' or 'bfill': use next valid observation to fill gap.\n            * 'nearest': use nearest valid observation to fill gap.\n\n        limit : int, optional\n            Limit of how many consecutive missing values to fill.\n\n        Returns\n        -------\n        Series or DataFrame\n            An upsampled Series or DataFrame with missing values filled.\n\n        See Also\n        --------\n        backfill : Backward fill NaN values in the resampled data.\n        pad : Forward fill NaN values in the resampled data.\n        nearest : Fill NaN values in the resampled data\n            with nearest neighbor starting from center.\n        interpolate : Fill NaN values using interpolation.\n        Series.fillna : Fill NaN values in the Series using the\n            specified method, which can be 'bfill' and 'ffill'.\n        DataFrame.fillna : Fill NaN values in the DataFrame using the\n            specified method, which can be 'bfill' and 'ffill'.\n\n        References\n        ----------\n        .. [1] https://en.wikipedia.org/wiki/Imputation_(statistics)\n\n        Examples\n        --------\n        Resampling a Series:\n\n        >>> s = pd.Series([1, 2, 3],\n        ...               index=pd.date_range('20180101', periods=3, freq='h'))\n        >>> s\n        2018-01-01 00:00:00    1\n        2018-01-01 01:00:00    2\n        2018-01-01 02:00:00    3\n        Freq: H, dtype: int64\n\n        Without filling the missing values you get:\n\n        >>> s.resample(\"30min\").asfreq()\n        2018-01-01 00:00:00    1.0\n        2018-01-01 00:30:00    NaN\n        2018-01-01 01:00:00    2.0\n        2018-01-01 01:30:00    NaN\n        2018-01-01 02:00:00    3.0\n        Freq: 30T, dtype: float64\n\n        >>> s.resample('30min').fillna(\"backfill\")\n        2018-01-01 00:00:00    1\n        2018-01-01 00:30:00    2\n        2018-01-01 01:00:00    2\n        2018-01-01 01:30:00    3\n        2018-01-01 02:00:00    3\n        Freq: 30T, dtype: int64\n\n        >>> s.resample('15min').fillna(\"backfill\", limit=2)\n        2018-01-01 00:00:00    1.0\n        2018-01-01 00:15:00    NaN\n        2018-01-01 00:30:00    2.0\n        2018-01-01 00:45:00    2.0\n        2018-01-01 01:00:00    2.0\n        2018-01-01 01:15:00    NaN\n        2018-01-01 01:30:00    3.0\n        2018-01-01 01:45:00    3.0\n        2018-01-01 02:00:00    3.0\n        Freq: 15T, dtype: float64\n\n        >>> s.resample('30min').fillna(\"pad\")\n        2018-01-01 00:00:00    1\n        2018-01-01 00:30:00    1\n        2018-01-01 01:00:00    2\n        2018-01-01 01:30:00    2\n        2018-01-01 02:00:00    3\n        Freq: 30T, dtype: int64\n\n        >>> s.resample('30min').fillna(\"nearest\")\n        2018-01-01 00:00:00    1\n        2018-01-01 00:30:00    2\n        2018-01-01 01:00:00    2\n        2018-01-01 01:30:00    3\n        2018-01-01 02:00:00    3\n        Freq: 30T, dtype: int64\n\n        Missing values present before the upsampling are not affected.\n\n        >>> sm = pd.Series([1, None, 3],\n        ...               index=pd.date_range('20180101', periods=3, freq='h'))\n        >>> sm\n        2018-01-01 00:00:00    1.0\n        2018-01-01 01:00:00    NaN\n        2018-01-01 02:00:00    3.0\n        Freq: H, dtype: float64\n\n        >>> sm.resample('30min').fillna('backfill')\n        2018-01-01 00:00:00    1.0\n        2018-01-01 00:30:00    NaN\n        2018-01-01 01:00:00    NaN\n        2018-01-01 01:30:00    3.0\n        2018-01-01 02:00:00    3.0\n        Freq: 30T, dtype: float64\n\n        >>> sm.resample('30min').fillna('pad')\n        2018-01-01 00:00:00    1.0\n        2018-01-01 00:30:00    1.0\n        2018-01-01 01:00:00    NaN\n        2018-01-01 01:30:00    NaN\n        2018-01-01 02:00:00    3.0\n        Freq: 30T, dtype: float64\n\n        >>> sm.resample('30min').fillna('nearest')\n        2018-01-01 00:00:00    1.0\n        2018-01-01 00:30:00    NaN\n        2018-01-01 01:00:00    NaN\n        2018-01-01 01:30:00    3.0\n        2018-01-01 02:00:00    3.0\n        Freq: 30T, dtype: float64\n\n        DataFrame resampling is done column-wise. All the same options are\n        available.\n\n        >>> df = pd.DataFrame({'a': [2, np.nan, 6], 'b': [1, 3, 5]},\n        ...                   index=pd.date_range('20180101', periods=3,\n        ...                                       freq='h'))\n        >>> df\n                               a  b\n        2018-01-01 00:00:00  2.0  1\n        2018-01-01 01:00:00  NaN  3\n        2018-01-01 02:00:00  6.0  5\n\n        >>> df.resample('30min').fillna(\"bfill\")\n                               a  b\n        2018-01-01 00:00:00  2.0  1\n        2018-01-01 00:30:00  NaN  3\n        2018-01-01 01:00:00  NaN  3\n        2018-01-01 01:30:00  6.0  5\n        2018-01-01 02:00:00  6.0  5\n        \"\"\"\n        return self._upsample(method, limit=limit)\n\n    @doc(NDFrame.interpolate, **_shared_docs_kwargs)\n    def interpolate(\n        self,\n        method=\"linear\",\n        axis=0,\n        limit=None,\n        inplace=False,\n        limit_direction=\"forward\",\n        limit_area=None,\n        downcast=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Interpolate values according to different methods.\n        \"\"\"\n        result = self._upsample(\"asfreq\")\n        return result.interpolate(\n            method=method,\n            axis=axis,\n            limit=limit,\n            inplace=inplace,\n            limit_direction=limit_direction,\n            limit_area=limit_area,\n            downcast=downcast,\n            **kwargs,\n        )\n\n    def asfreq(self, fill_value=None):\n        \"\"\"\n        Return the values at the new freq, essentially a reindex.\n\n        Parameters\n        ----------\n        fill_value : scalar, optional\n            Value to use for missing values, applied during upsampling (note\n            this does not fill NaNs that already were present).\n\n        Returns\n        -------\n        DataFrame or Series\n            Values at the specified freq.\n\n        See Also\n        --------\n        Series.asfreq: Convert TimeSeries to specified frequency.\n        DataFrame.asfreq: Convert TimeSeries to specified frequency.\n        \"\"\"\n        return self._upsample(\"asfreq\", fill_value=fill_value)\n\n    def std(self, ddof=1, *args, **kwargs):\n        \"\"\"\n        Compute standard deviation of groups, excluding missing values.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        Returns\n        -------\n        DataFrame or Series\n            Standard deviation of values within each group.\n        \"\"\"\n        nv.validate_resampler_func(\"std\", args, kwargs)\n        return self._downsample(\"std\", ddof=ddof)\n\n    def var(self, ddof=1, *args, **kwargs):\n        \"\"\"\n        Compute variance of groups, excluding missing values.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        Returns\n        -------\n        DataFrame or Series\n            Variance of values within each group.\n        \"\"\"\n        nv.validate_resampler_func(\"var\", args, kwargs)\n        return self._downsample(\"var\", ddof=ddof)\n\n    @doc(GroupBy.size)\n    def size(self):\n        result = self._downsample(\"size\")\n        if not len(self.ax):\n            from pandas import Series\n\n            if self._selected_obj.ndim == 1:\n                name = self._selected_obj.name\n            else:\n                name = None\n            result = Series([], index=result.index, dtype=\"int64\", name=name)\n        return result\n\n    @doc(GroupBy.count)\n    def count(self):\n        result = self._downsample(\"count\")\n        if not len(self.ax):\n            if self._selected_obj.ndim == 1:\n                result = type(self._selected_obj)(\n                    [], index=result.index, dtype=\"int64\", name=self._selected_obj.name\n                )\n            else:\n                from pandas import DataFrame\n\n                result = DataFrame(\n                    [], index=result.index, columns=result.columns, dtype=\"int64\"\n                )\n\n        return result\n\n    def quantile(self, q=0.5, **kwargs):\n        \"\"\"\n        Return value at the given quantile.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n\n        Returns\n        -------\n        DataFrame or Series\n            Quantile of values within each group.\n\n        See Also\n        --------\n        Series.quantile\n            Return a series, where the index is q and the values are the quantiles.\n        DataFrame.quantile\n            Return a DataFrame, where the columns are the columns of self,\n            and the values are the quantiles.\n        DataFrameGroupBy.quantile\n            Return a DataFrame, where the coulmns are groupby columns,\n            and the values are its quantiles.\n        \"\"\"\n        return self._downsample(\"quantile\", q=q, **kwargs)\n\n\n# downsample methods\nfor method in [\"sum\", \"prod\"]:\n\n    def f(self, _method=method, min_count=0, *args, **kwargs):\n        nv.validate_resampler_func(_method, args, kwargs)\n        return self._downsample(_method, min_count=min_count)\n\n    f.__doc__ = getattr(GroupBy, method).__doc__\n    setattr(Resampler, method, f)\n\n\n# downsample methods\nfor method in [\"min\", \"max\", \"first\", \"last\", \"mean\", \"sem\", \"median\", \"ohlc\"]:\n\n    def g(self, _method=method, *args, **kwargs):\n        nv.validate_resampler_func(_method, args, kwargs)\n        return self._downsample(_method)\n\n    g.__doc__ = getattr(GroupBy, method).__doc__\n    setattr(Resampler, method, g)\n\n\n# series only methods\nfor method in [\"nunique\"]:\n\n    def h(self, _method=method):\n        return self._downsample(_method)\n\n    h.__doc__ = getattr(SeriesGroupBy, method).__doc__\n    setattr(Resampler, method, h)\n\n\nclass _GroupByMixin(GotItemMixin):\n    \"\"\"\n    Provide the groupby facilities.\n    \"\"\"\n\n    def __init__(self, obj, *args, **kwargs):\n\n        parent = kwargs.pop(\"parent\", None)\n        groupby = kwargs.pop(\"groupby\", None)\n        if parent is None:\n            parent = obj\n\n        # initialize our GroupByMixin object with\n        # the resampler attributes\n        for attr in self._attributes:\n            setattr(self, attr, kwargs.get(attr, getattr(parent, attr)))\n\n        super().__init__(None)\n        self._groupby = groupby\n        self._groupby.mutated = True\n        self._groupby.grouper.mutated = True\n        self.groupby = copy.copy(parent.groupby)\n\n    @no_type_check\n    def _apply(self, f, grouper=None, *args, **kwargs):\n        \"\"\"\n        Dispatch to _upsample; we are stripping all of the _upsample kwargs and\n        performing the original function call on the grouped object.\n        \"\"\"\n\n        def func(x):\n            x = self._shallow_copy(x, groupby=self.groupby)\n\n            if isinstance(f, str):\n                return getattr(x, f)(**kwargs)\n\n            return x.apply(f, *args, **kwargs)\n\n        result = self._groupby.apply(func)\n        return self._wrap_result(result)\n\n    _upsample = _apply\n    _downsample = _apply\n    _groupby_and_aggregate = _apply\n\n\nclass DatetimeIndexResampler(Resampler):\n    @property\n    def _resampler_for_grouping(self):\n        return DatetimeIndexResamplerGroupby\n\n    def _get_binner_for_time(self):\n\n        # this is how we are actually creating the bins\n        if self.kind == \"period\":\n            return self.groupby._get_time_period_bins(self.ax)\n        return self.groupby._get_time_bins(self.ax)\n\n    def _downsample(self, how, **kwargs):\n        \"\"\"\n        Downsample the cython defined function.\n\n        Parameters\n        ----------\n        how : string / cython mapped function\n        **kwargs : kw args passed to how function\n        \"\"\"\n        self._set_binner()\n        how = self._get_cython_func(how) or how\n        ax = self.ax\n        obj = self._selected_obj\n\n        if not len(ax):\n            # reset to the new freq\n            obj = obj.copy()\n            obj.index = obj.index._with_freq(self.freq)\n            assert obj.index.freq == self.freq, (obj.index.freq, self.freq)\n            return obj\n\n        # do we have a regular frequency\n        if ax.freq is not None or ax.inferred_freq is not None:\n\n            if len(self.grouper.binlabels) > len(ax) and how is None:\n\n                # let's do an asfreq\n                return self.asfreq()\n\n        # we are downsampling\n        # we want to call the actual grouper method here\n        result = obj.groupby(self.grouper, axis=self.axis).aggregate(how, **kwargs)\n\n        result = self._apply_loffset(result)\n        return self._wrap_result(result)\n\n    def _adjust_binner_for_upsample(self, binner):\n        \"\"\"\n        Adjust our binner when upsampling.\n\n        The range of a new index should not be outside specified range\n        \"\"\"\n        if self.closed == \"right\":\n            binner = binner[1:]\n        else:\n            binner = binner[:-1]\n        return binner\n\n    def _upsample(self, method, limit=None, fill_value=None):\n        \"\"\"\n        Parameters\n        ----------\n        method : string {'backfill', 'bfill', 'pad',\n            'ffill', 'asfreq'} method for upsampling\n        limit : int, default None\n            Maximum size gap to fill when reindexing\n        fill_value : scalar, default None\n            Value to use for missing values\n\n        See Also\n        --------\n        .fillna: Fill NA/NaN values using the specified method.\n\n        \"\"\"\n        self._set_binner()\n        if self.axis:\n            raise AssertionError(\"axis must be 0\")\n        if self._from_selection:\n            raise ValueError(\n                \"Upsampling from level= or on= selection \"\n                \"is not supported, use .set_index(...) \"\n                \"to explicitly set index to datetime-like\"\n            )\n\n        ax = self.ax\n        obj = self._selected_obj\n        binner = self.binner\n        res_index = self._adjust_binner_for_upsample(binner)\n\n        # if we have the same frequency as our axis, then we are equal sampling\n        if (\n            limit is None\n            and to_offset(ax.inferred_freq) == self.freq\n            and len(obj) == len(res_index)\n        ):\n            result = obj.copy()\n            result.index = res_index\n        else:\n            result = obj.reindex(\n                res_index, method=method, limit=limit, fill_value=fill_value\n            )\n\n        result = self._apply_loffset(result)\n        return self._wrap_result(result)\n\n    def _wrap_result(self, result):\n        result = super()._wrap_result(result)\n\n        # we may have a different kind that we were asked originally\n        # convert if needed\n        if self.kind == \"period\" and not isinstance(result.index, PeriodIndex):\n            result.index = result.index.to_period(self.freq)\n        return result\n\n\nclass DatetimeIndexResamplerGroupby(_GroupByMixin, DatetimeIndexResampler):\n    \"\"\"\n    Provides a resample of a groupby implementation\n    \"\"\"\n\n    @property\n    def _constructor(self):\n        return DatetimeIndexResampler\n\n\nclass PeriodIndexResampler(DatetimeIndexResampler):\n    @property\n    def _resampler_for_grouping(self):\n        return PeriodIndexResamplerGroupby\n\n    def _get_binner_for_time(self):\n        if self.kind == \"timestamp\":\n            return super()._get_binner_for_time()\n        return self.groupby._get_period_bins(self.ax)\n\n    def _convert_obj(self, obj):\n        obj = super()._convert_obj(obj)\n\n        if self._from_selection:\n            # see GH 14008, GH 12871\n            msg = (\n                \"Resampling from level= or on= selection \"\n                \"with a PeriodIndex is not currently supported, \"\n                \"use .set_index(...) to explicitly set index\"\n            )\n            raise NotImplementedError(msg)\n\n        if self.loffset is not None:\n            # Cannot apply loffset/timedelta to PeriodIndex -> convert to\n            # timestamps\n            self.kind = \"timestamp\"\n\n        # convert to timestamp\n        if self.kind == \"timestamp\":\n            obj = obj.to_timestamp(how=self.convention)\n\n        return obj\n\n    def _downsample(self, how, **kwargs):\n        \"\"\"\n        Downsample the cython defined function.\n\n        Parameters\n        ----------\n        how : string / cython mapped function\n        **kwargs : kw args passed to how function\n        \"\"\"\n        # we may need to actually resample as if we are timestamps\n        if self.kind == \"timestamp\":\n            return super()._downsample(how, **kwargs)\n\n        how = self._get_cython_func(how) or how\n        ax = self.ax\n\n        if is_subperiod(ax.freq, self.freq):\n            # Downsampling\n            return self._groupby_and_aggregate(how, grouper=self.grouper, **kwargs)\n        elif is_superperiod(ax.freq, self.freq):\n            if how == \"ohlc\":\n                # GH #13083\n                # upsampling to subperiods is handled as an asfreq, which works\n                # for pure aggregating/reducing methods\n                # OHLC reduces along the time dimension, but creates multiple\n                # values for each period -> handle by _groupby_and_aggregate()\n                return self._groupby_and_aggregate(how, grouper=self.grouper)\n            return self.asfreq()\n        elif ax.freq == self.freq:\n            return self.asfreq()\n\n        raise IncompatibleFrequency(\n            f\"Frequency {ax.freq} cannot be resampled to {self.freq}, \"\n            \"as they are not sub or super periods\"\n        )\n\n    def _upsample(self, method, limit=None, fill_value=None):\n        \"\"\"\n        Parameters\n        ----------\n        method : string {'backfill', 'bfill', 'pad', 'ffill'}\n            Method for upsampling.\n        limit : int, default None\n            Maximum size gap to fill when reindexing.\n        fill_value : scalar, default None\n            Value to use for missing values.\n\n        See Also\n        --------\n        .fillna: Fill NA/NaN values using the specified method.\n\n        \"\"\"\n        # we may need to actually resample as if we are timestamps\n        if self.kind == \"timestamp\":\n            return super()._upsample(method, limit=limit, fill_value=fill_value)\n\n        self._set_binner()\n        ax = self.ax\n        obj = self.obj\n        new_index = self.binner\n\n        # Start vs. end of period\n        memb = ax.asfreq(self.freq, how=self.convention)\n\n        # Get the fill indexer\n        indexer = memb.get_indexer(new_index, method=method, limit=limit)\n        return self._wrap_result(\n            _take_new_index(obj, indexer, new_index, axis=self.axis)\n        )\n\n\nclass PeriodIndexResamplerGroupby(_GroupByMixin, PeriodIndexResampler):\n    \"\"\"\n    Provides a resample of a groupby implementation.\n    \"\"\"\n\n    @property\n    def _constructor(self):\n        return PeriodIndexResampler\n\n\nclass TimedeltaIndexResampler(DatetimeIndexResampler):\n    @property\n    def _resampler_for_grouping(self):\n        return TimedeltaIndexResamplerGroupby\n\n    def _get_binner_for_time(self):\n        return self.groupby._get_time_delta_bins(self.ax)\n\n    def _adjust_binner_for_upsample(self, binner):\n        \"\"\"\n        Adjust our binner when upsampling.\n\n        The range of a new index is allowed to be greater than original range\n        so we don't need to change the length of a binner, GH 13022\n        \"\"\"\n        return binner\n\n\nclass TimedeltaIndexResamplerGroupby(_GroupByMixin, TimedeltaIndexResampler):\n    \"\"\"\n    Provides a resample of a groupby implementation.\n    \"\"\"\n\n    @property\n    def _constructor(self):\n        return TimedeltaIndexResampler\n\n\ndef get_resampler(obj, kind=None, **kwds):\n    \"\"\"\n    Create a TimeGrouper and return our resampler.\n    \"\"\"\n    tg = TimeGrouper(**kwds)\n    return tg._get_resampler(obj, kind=kind)\n\n\nget_resampler.__doc__ = Resampler.__doc__\n\n\ndef get_resampler_for_grouping(\n    groupby, rule, how=None, fill_method=None, limit=None, kind=None, **kwargs\n):\n    \"\"\"\n    Return our appropriate resampler when grouping as well.\n    \"\"\"\n    # .resample uses 'on' similar to how .groupby uses 'key'\n    kwargs[\"key\"] = kwargs.pop(\"on\", None)\n\n    tg = TimeGrouper(freq=rule, **kwargs)\n    resampler = tg._get_resampler(groupby.obj, kind=kind)\n    return resampler._get_resampler_for_grouping(groupby=groupby)\n\n\nclass TimeGrouper(Grouper):\n    \"\"\"\n    Custom groupby class for time-interval grouping.\n\n    Parameters\n    ----------\n    freq : pandas date offset or offset alias for identifying bin edges\n    closed : closed end of interval; 'left' or 'right'\n    label : interval boundary to use for labeling; 'left' or 'right'\n    convention : {'start', 'end', 'e', 's'}\n        If axis is PeriodIndex\n    \"\"\"\n\n    _attributes = Grouper._attributes + (\n        \"closed\",\n        \"label\",\n        \"how\",\n        \"loffset\",\n        \"kind\",\n        \"convention\",\n        \"origin\",\n        \"offset\",\n    )\n\n    def __init__(\n        self,\n        freq=\"Min\",\n        closed: Optional[str] = None,\n        label: Optional[str] = None,\n        how=\"mean\",\n        axis=0,\n        fill_method=None,\n        limit=None,\n        loffset=None,\n        kind: Optional[str] = None,\n        convention: Optional[str] = None,\n        base: Optional[int] = None,\n        origin: Union[str, TimestampConvertibleTypes] = \"start_day\",\n        offset: Optional[TimedeltaConvertibleTypes] = None,\n        **kwargs,\n    ):\n        # Check for correctness of the keyword arguments which would\n        # otherwise silently use the default if misspelled\n        if label not in {None, \"left\", \"right\"}:\n            raise ValueError(f\"Unsupported value {label} for `label`\")\n        if closed not in {None, \"left\", \"right\"}:\n            raise ValueError(f\"Unsupported value {closed} for `closed`\")\n        if convention not in {None, \"start\", \"end\", \"e\", \"s\"}:\n            raise ValueError(f\"Unsupported value {convention} for `convention`\")\n\n        freq = to_offset(freq)\n\n        end_types = {\"M\", \"A\", \"Q\", \"BM\", \"BA\", \"BQ\", \"W\"}\n        rule = freq.rule_code\n        if rule in end_types or (\"-\" in rule and rule[: rule.find(\"-\")] in end_types):\n            if closed is None:\n                closed = \"right\"\n            if label is None:\n                label = \"right\"\n        else:\n            if closed is None:\n                closed = \"left\"\n            if label is None:\n                label = \"left\"\n\n        self.closed = closed\n        self.label = label\n        self.kind = kind\n\n        self.convention = convention or \"E\"\n        self.convention = self.convention.lower()\n\n        self.how = how\n        self.fill_method = fill_method\n        self.limit = limit\n\n        if origin in (\"epoch\", \"start\", \"start_day\"):\n            self.origin = origin\n        else:\n            try:\n                self.origin = Timestamp(origin)\n            except Exception as e:\n                raise ValueError(\n                    \"'origin' should be equal to 'epoch', 'start', 'start_day' or \"\n                    f\"should be a Timestamp convertible type. Got '{origin}' instead.\"\n                ) from e\n\n        try:\n            self.offset = Timedelta(offset) if offset is not None else None\n        except Exception as e:\n            raise ValueError(\n                \"'offset' should be a Timedelta convertible type. \"\n                f\"Got '{offset}' instead.\"\n            ) from e\n\n        # always sort time groupers\n        kwargs[\"sort\"] = True\n\n        # Handle deprecated arguments since v1.1.0 of `base` and `loffset` (GH #31809)\n        if base is not None and offset is not None:\n            raise ValueError(\"'offset' and 'base' cannot be present at the same time\")\n\n        if base and isinstance(freq, Tick):\n            # this conversion handle the default behavior of base and the\n            # special case of GH #10530. Indeed in case when dealing with\n            # a TimedeltaIndex base was treated as a 'pure' offset even though\n            # the default behavior of base was equivalent of a modulo on\n            # freq_nanos.\n            self.offset = Timedelta(base * freq.nanos // freq.n)\n\n        if isinstance(loffset, str):\n            loffset = to_offset(loffset)\n        self.loffset = loffset\n\n        super().__init__(freq=freq, axis=axis, **kwargs)\n\n    def _get_resampler(self, obj, kind=None):\n        \"\"\"\n        Return my resampler or raise if we have an invalid axis.\n\n        Parameters\n        ----------\n        obj : input object\n        kind : string, optional\n            'period','timestamp','timedelta' are valid\n\n        Returns\n        -------\n        a Resampler\n\n        Raises\n        ------\n        TypeError if incompatible axis\n\n        \"\"\"\n        self._set_grouper(obj)\n\n        ax = self.ax\n        if isinstance(ax, DatetimeIndex):\n            return DatetimeIndexResampler(obj, groupby=self, kind=kind, axis=self.axis)\n        elif isinstance(ax, PeriodIndex) or kind == \"period\":\n            return PeriodIndexResampler(obj, groupby=self, kind=kind, axis=self.axis)\n        elif isinstance(ax, TimedeltaIndex):\n            return TimedeltaIndexResampler(obj, groupby=self, axis=self.axis)\n\n        raise TypeError(\n            \"Only valid with DatetimeIndex, \"\n            \"TimedeltaIndex or PeriodIndex, \"\n            f\"but got an instance of '{type(ax).__name__}'\"\n        )\n\n    def _get_grouper(self, obj, validate: bool = True):\n        # create the resampler and return our binner\n        r = self._get_resampler(obj)\n        r._set_binner()\n        return r.binner, r.grouper, r.obj\n\n    def _get_time_bins(self, ax):\n        if not isinstance(ax, DatetimeIndex):\n            raise TypeError(\n                \"axis must be a DatetimeIndex, but got \"\n                f\"an instance of {type(ax).__name__}\"\n            )\n\n        if len(ax) == 0:\n            binner = labels = DatetimeIndex(data=[], freq=self.freq, name=ax.name)\n            return binner, [], labels\n\n        first, last = _get_timestamp_range_edges(\n            ax.min(),\n            ax.max(),\n            self.freq,\n            closed=self.closed,\n            origin=self.origin,\n            offset=self.offset,\n        )\n        # GH #12037\n        # use first/last directly instead of call replace() on them\n        # because replace() will swallow the nanosecond part\n        # thus last bin maybe slightly before the end if the end contains\n        # nanosecond part and lead to `Values falls after last bin` error\n        # GH 25758: If DST lands at midnight (e.g. 'America/Havana'), user feedback\n        # has noted that ambiguous=True provides the most sensible result\n        binner = labels = date_range(\n            freq=self.freq,\n            start=first,\n            end=last,\n            tz=ax.tz,\n            name=ax.name,\n            ambiguous=True,\n            nonexistent=\"shift_forward\",\n        )\n\n        ax_values = ax.asi8\n        binner, bin_edges = self._adjust_bin_edges(binner, ax_values)\n\n        # general version, knowing nothing about relative frequencies\n        bins = lib.generate_bins_dt64(\n            ax_values, bin_edges, self.closed, hasnans=ax.hasnans\n        )\n\n        if self.closed == \"right\":\n            labels = binner\n            if self.label == \"right\":\n                labels = labels[1:]\n        elif self.label == \"right\":\n            labels = labels[1:]\n\n        if ax.hasnans:\n            binner = binner.insert(0, NaT)\n            labels = labels.insert(0, NaT)\n\n        # if we end up with more labels than bins\n        # adjust the labels\n        # GH4076\n        if len(bins) < len(labels):\n            labels = labels[: len(bins)]\n\n        return binner, bins, labels\n\n    def _adjust_bin_edges(self, binner, ax_values):\n        # Some hacks for > daily data, see #1471, #1458, #1483\n\n        if self.freq != \"D\" and is_superperiod(self.freq, \"D\"):\n            if self.closed == \"right\":\n                # GH 21459, GH 9119: Adjust the bins relative to the wall time\n                bin_edges = binner.tz_localize(None)\n                bin_edges = bin_edges + timedelta(1) - Nano(1)\n                bin_edges = bin_edges.tz_localize(binner.tz).asi8\n            else:\n                bin_edges = binner.asi8\n\n            # intraday values on last day\n            if bin_edges[-2] > ax_values.max():\n                bin_edges = bin_edges[:-1]\n                binner = binner[:-1]\n        else:\n            bin_edges = binner.asi8\n        return binner, bin_edges\n\n    def _get_time_delta_bins(self, ax):\n        if not isinstance(ax, TimedeltaIndex):\n            raise TypeError(\n                \"axis must be a TimedeltaIndex, but got \"\n                f\"an instance of {type(ax).__name__}\"\n            )\n\n        if not len(ax):\n            binner = labels = TimedeltaIndex(data=[], freq=self.freq, name=ax.name)\n            return binner, [], labels\n\n        start, end = ax.min(), ax.max()\n        labels = binner = timedelta_range(\n            start=start, end=end, freq=self.freq, name=ax.name\n        )\n\n        end_stamps = labels + self.freq\n        bins = ax.searchsorted(end_stamps, side=\"left\")\n\n        if self.offset:\n            # GH 10530 & 31809\n            labels += self.offset\n        if self.loffset:\n            # GH 33498\n            labels += self.loffset\n\n        return binner, bins, labels\n\n    def _get_time_period_bins(self, ax: DatetimeIndex):\n        if not isinstance(ax, DatetimeIndex):\n            raise TypeError(\n                \"axis must be a DatetimeIndex, but got \"\n                f\"an instance of {type(ax).__name__}\"\n            )\n\n        freq = self.freq\n\n        if not len(ax):\n            binner = labels = PeriodIndex(data=[], freq=freq, name=ax.name)\n            return binner, [], labels\n\n        labels = binner = period_range(start=ax[0], end=ax[-1], freq=freq, name=ax.name)\n\n        end_stamps = (labels + freq).asfreq(freq, \"s\").to_timestamp()\n        if ax.tz:\n            end_stamps = end_stamps.tz_localize(ax.tz)\n        bins = ax.searchsorted(end_stamps, side=\"left\")\n\n        return binner, bins, labels\n\n    def _get_period_bins(self, ax: PeriodIndex):\n        if not isinstance(ax, PeriodIndex):\n            raise TypeError(\n                \"axis must be a PeriodIndex, but got \"\n                f\"an instance of {type(ax).__name__}\"\n            )\n\n        memb = ax.asfreq(self.freq, how=self.convention)\n\n        # NaT handling as in pandas._lib.lib.generate_bins_dt64()\n        nat_count = 0\n        if memb.hasnans:\n            nat_count = np.sum(memb._isnan)\n            memb = memb[~memb._isnan]\n\n        # if index contains no valid (non-NaT) values, return empty index\n        if not len(memb):\n            binner = labels = PeriodIndex(data=[], freq=self.freq, name=ax.name)\n            return binner, [], labels\n\n        freq_mult = self.freq.n\n\n        start = ax.min().asfreq(self.freq, how=self.convention)\n        end = ax.max().asfreq(self.freq, how=\"end\")\n        bin_shift = 0\n\n        if isinstance(self.freq, Tick):\n            # GH 23882 & 31809: get adjusted bin edge labels with 'origin'\n            # and 'origin' support. This call only makes sense if the freq is a\n            # Tick since offset and origin are only used in those cases.\n            # Not doing this check could create an extra empty bin.\n            p_start, end = _get_period_range_edges(\n                start,\n                end,\n                self.freq,\n                closed=self.closed,\n                origin=self.origin,\n                offset=self.offset,\n            )\n\n            # Get offset for bin edge (not label edge) adjustment\n            start_offset = Period(start, self.freq) - Period(p_start, self.freq)\n            bin_shift = start_offset.n % freq_mult\n            start = p_start\n\n        labels = binner = period_range(\n            start=start, end=end, freq=self.freq, name=ax.name\n        )\n\n        i8 = memb.asi8\n\n        # when upsampling to subperiods, we need to generate enough bins\n        expected_bins_count = len(binner) * freq_mult\n        i8_extend = expected_bins_count - (i8[-1] - i8[0])\n        rng = np.arange(i8[0], i8[-1] + i8_extend, freq_mult)\n        rng += freq_mult\n        # adjust bin edge indexes to account for base\n        rng -= bin_shift\n\n        # Wrap in PeriodArray for PeriodArray.searchsorted\n        prng = type(memb._data)(rng, dtype=memb.dtype)\n        bins = memb.searchsorted(prng, side=\"left\")\n\n        if nat_count > 0:\n            # NaT handling as in pandas._lib.lib.generate_bins_dt64()\n            # shift bins by the number of NaT\n            bins += nat_count\n            bins = np.insert(bins, 0, nat_count)\n            binner = binner.insert(0, NaT)\n            labels = labels.insert(0, NaT)\n\n        return binner, bins, labels\n\n\ndef _take_new_index(obj, indexer, new_index, axis=0):\n\n    if isinstance(obj, ABCSeries):\n        new_values = algos.take_1d(obj._values, indexer)\n        return obj._constructor(new_values, index=new_index, name=obj.name)\n    elif isinstance(obj, ABCDataFrame):\n        if axis == 1:\n            raise NotImplementedError(\"axis 1 is not supported\")\n        return obj._constructor(\n            obj._mgr.reindex_indexer(new_axis=new_index, indexer=indexer, axis=1)\n        )\n    else:\n        raise ValueError(\"'obj' should be either a Series or a DataFrame\")\n\n\ndef _get_timestamp_range_edges(\n    first, last, freq, closed=\"left\", origin=\"start_day\", offset=None\n):\n    \"\"\"\n    Adjust the `first` Timestamp to the preceding Timestamp that resides on\n    the provided offset. Adjust the `last` Timestamp to the following\n    Timestamp that resides on the provided offset. Input Timestamps that\n    already reside on the offset will be adjusted depending on the type of\n    offset and the `closed` parameter.\n\n    Parameters\n    ----------\n    first : pd.Timestamp\n        The beginning Timestamp of the range to be adjusted.\n    last : pd.Timestamp\n        The ending Timestamp of the range to be adjusted.\n    freq : pd.DateOffset\n        The dateoffset to which the Timestamps will be adjusted.\n    closed : {'right', 'left'}, default None\n        Which side of bin interval is closed.\n    origin : {'epoch', 'start', 'start_day'} or Timestamp, default 'start_day'\n        The timestamp on which to adjust the grouping. The timezone of origin must\n        match the timezone of the index.\n        If a timestamp is not used, these values are also supported:\n\n        - 'epoch': `origin` is 1970-01-01\n        - 'start': `origin` is the first value of the timeseries\n        - 'start_day': `origin` is the first day at midnight of the timeseries\n    offset : pd.Timedelta, default is None\n        An offset timedelta added to the origin.\n\n    Returns\n    -------\n    A tuple of length 2, containing the adjusted pd.Timestamp objects.\n    \"\"\"\n    if isinstance(freq, Tick):\n        index_tz = first.tz\n        if isinstance(origin, Timestamp) and (origin.tz is None) != (index_tz is None):\n            raise ValueError(\"The origin must have the same timezone as the index.\")\n        elif origin == \"epoch\":\n            # set the epoch based on the timezone to have similar bins results when\n            # resampling on the same kind of indexes on different timezones\n            origin = Timestamp(\"1970-01-01\", tz=index_tz)\n\n        if isinstance(freq, Day):\n            # _adjust_dates_anchored assumes 'D' means 24H, but first/last\n            # might contain a DST transition (23H, 24H, or 25H).\n            # So \"pretend\" the dates are naive when adjusting the endpoints\n            first = first.tz_localize(None)\n            last = last.tz_localize(None)\n            if isinstance(origin, Timestamp):\n                origin = origin.tz_localize(None)\n\n        first, last = _adjust_dates_anchored(\n            first, last, freq, closed=closed, origin=origin, offset=offset\n        )\n        if isinstance(freq, Day):\n            first = first.tz_localize(index_tz)\n            last = last.tz_localize(index_tz)\n    else:\n        first = first.normalize()\n        last = last.normalize()\n\n        if closed == \"left\":\n            first = Timestamp(freq.rollback(first))\n        else:\n            first = Timestamp(first - freq)\n\n        last = Timestamp(last + freq)\n\n    return first, last\n\n\ndef _get_period_range_edges(\n    first, last, freq, closed=\"left\", origin=\"start_day\", offset=None\n):\n    \"\"\"\n    Adjust the provided `first` and `last` Periods to the respective Period of\n    the given offset that encompasses them.\n\n    Parameters\n    ----------\n    first : pd.Period\n        The beginning Period of the range to be adjusted.\n    last : pd.Period\n        The ending Period of the range to be adjusted.\n    freq : pd.DateOffset\n        The freq to which the Periods will be adjusted.\n    closed : {'right', 'left'}, default None\n        Which side of bin interval is closed.\n    origin : {'epoch', 'start', 'start_day'}, Timestamp, default 'start_day'\n        The timestamp on which to adjust the grouping. The timezone of origin must\n        match the timezone of the index.\n\n        If a timestamp is not used, these values are also supported:\n\n        - 'epoch': `origin` is 1970-01-01\n        - 'start': `origin` is the first value of the timeseries\n        - 'start_day': `origin` is the first day at midnight of the timeseries\n    offset : pd.Timedelta, default is None\n        An offset timedelta added to the origin.\n\n    Returns\n    -------\n    A tuple of length 2, containing the adjusted pd.Period objects.\n    \"\"\"\n    if not all(isinstance(obj, Period) for obj in [first, last]):\n        raise TypeError(\"'first' and 'last' must be instances of type Period\")\n\n    # GH 23882\n    first = first.to_timestamp()\n    last = last.to_timestamp()\n    adjust_first = not freq.is_on_offset(first)\n    adjust_last = freq.is_on_offset(last)\n\n    first, last = _get_timestamp_range_edges(\n        first, last, freq, closed=closed, origin=origin, offset=offset\n    )\n\n    first = (first + int(adjust_first) * freq).to_period(freq)\n    last = (last - int(adjust_last) * freq).to_period(freq)\n    return first, last\n\n\ndef _adjust_dates_anchored(\n    first, last, freq, closed=\"right\", origin=\"start_day\", offset=None\n):\n    # First and last offsets should be calculated from the start day to fix an\n    # error cause by resampling across multiple days when a one day period is\n    # not a multiple of the frequency. See GH 8683\n    # To handle frequencies that are not multiple or divisible by a day we let\n    # the possibility to define a fixed origin timestamp. See GH 31809\n    origin_nanos = 0  # origin == \"epoch\"\n    if origin == \"start_day\":\n        origin_nanos = first.normalize().value\n    elif origin == \"start\":\n        origin_nanos = first.value\n    elif isinstance(origin, Timestamp):\n        origin_nanos = origin.value\n    origin_nanos += offset.value if offset else 0\n\n    # GH 10117 & GH 19375. If first and last contain timezone information,\n    # Perform the calculation in UTC in order to avoid localizing on an\n    # Ambiguous or Nonexistent time.\n    first_tzinfo = first.tzinfo\n    last_tzinfo = last.tzinfo\n    if first_tzinfo is not None:\n        first = first.tz_convert(\"UTC\")\n    if last_tzinfo is not None:\n        last = last.tz_convert(\"UTC\")\n\n    foffset = (first.value - origin_nanos) % freq.nanos\n    loffset = (last.value - origin_nanos) % freq.nanos\n\n    if closed == \"right\":\n        if foffset > 0:\n            # roll back\n            fresult = first.value - foffset\n        else:\n            fresult = first.value - freq.nanos\n\n        if loffset > 0:\n            # roll forward\n            lresult = last.value + (freq.nanos - loffset)\n        else:\n            # already the end of the road\n            lresult = last.value\n    else:  # closed == 'left'\n        if foffset > 0:\n            fresult = first.value - foffset\n        else:\n            # start of the road\n            fresult = first.value\n\n        if loffset > 0:\n            # roll forward\n            lresult = last.value + (freq.nanos - loffset)\n        else:\n            lresult = last.value + freq.nanos\n    fresult = Timestamp(fresult)\n    lresult = Timestamp(lresult)\n    if first_tzinfo is not None:\n        fresult = fresult.tz_localize(\"UTC\").tz_convert(first_tzinfo)\n    if last_tzinfo is not None:\n        lresult = lresult.tz_localize(\"UTC\").tz_convert(last_tzinfo)\n    return fresult, lresult\n\n\ndef asfreq(obj, freq, method=None, how=None, normalize=False, fill_value=None):\n    \"\"\"\n    Utility frequency conversion method for Series/DataFrame.\n    \"\"\"\n    if isinstance(obj.index, PeriodIndex):\n        if method is not None:\n            raise NotImplementedError(\"'method' argument is not supported\")\n\n        if how is None:\n            how = \"E\"\n\n        new_obj = obj.copy()\n        new_obj.index = obj.index.asfreq(freq, how=how)\n\n    elif len(obj.index) == 0:\n        new_obj = obj.copy()\n\n        new_obj.index = _asfreq_compat(obj.index, freq)\n    else:\n        dti = date_range(obj.index[0], obj.index[-1], freq=freq)\n        dti.name = obj.index.name\n        new_obj = obj.reindex(dti, method=method, fill_value=fill_value)\n        if normalize:\n            new_obj.index = new_obj.index.normalize()\n\n    return new_obj\n\n\ndef _asfreq_compat(index, freq):\n    \"\"\"\n    Helper to mimic asfreq on (empty) DatetimeIndex and TimedeltaIndex.\n\n    Parameters\n    ----------\n    index : PeriodIndex, DatetimeIndex, or TimedeltaIndex\n    freq : DateOffset\n\n    Returns\n    -------\n    same type as index\n    \"\"\"\n    if len(index) != 0:\n        # This should never be reached, always checked by the caller\n        raise ValueError(\n            \"Can only set arbitrary freq for empty DatetimeIndex or TimedeltaIndex\"\n        )\n    new_index: Index\n    if isinstance(index, PeriodIndex):\n        new_index = index.asfreq(freq=freq)\n    else:\n        new_index = Index([], dtype=index.dtype, freq=freq, name=index.name)\n    return new_index\n"
    },
    {
      "filename": "pandas/io/formats/style.py",
      "content": "\"\"\"\nModule for applying conditional formatting to DataFrames and Series.\n\"\"\"\n\nfrom collections import defaultdict\nfrom contextlib import contextmanager\nimport copy\nfrom functools import partial\nfrom itertools import product\nfrom typing import (\n    Any,\n    Callable,\n    DefaultDict,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n)\nfrom uuid import uuid4\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import lib\nfrom pandas._typing import Axis, FrameOrSeries, FrameOrSeriesUnion, Label\nfrom pandas.compat._optional import import_optional_dependency\nfrom pandas.util._decorators import doc\n\nfrom pandas.core.dtypes.common import is_float\n\nimport pandas as pd\nfrom pandas.api.types import is_dict_like, is_list_like\nimport pandas.core.common as com\nfrom pandas.core.frame import DataFrame\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.indexing import maybe_numeric_slice, non_reducing_slice\n\njinja2 = import_optional_dependency(\"jinja2\", extra=\"DataFrame.style requires jinja2.\")\n\n\ntry:\n    from matplotlib import colors\n    import matplotlib.pyplot as plt\n\n    has_mpl = True\nexcept ImportError:\n    has_mpl = False\n    no_mpl_message = \"{0} requires matplotlib.\"\n\n\n@contextmanager\ndef _mpl(func: Callable):\n    if has_mpl:\n        yield plt, colors\n    else:\n        raise ImportError(no_mpl_message.format(func.__name__))\n\n\nclass Styler:\n    \"\"\"\n    Helps style a DataFrame or Series according to the data with HTML and CSS.\n\n    Parameters\n    ----------\n    data : Series or DataFrame\n        Data to be styled - either a Series or DataFrame.\n    precision : int\n        Precision to round floats to, defaults to pd.options.display.precision.\n    table_styles : list-like, default None\n        List of {selector: (attr, value)} dicts; see Notes.\n    uuid : str, default None\n        A unique identifier to avoid CSS collisions; generated automatically.\n    caption : str, default None\n        Caption to attach to the table.\n    table_attributes : str, default None\n        Items that show up in the opening ``<table>`` tag\n        in addition to automatic (by default) id.\n    cell_ids : bool, default True\n        If True, each cell will have an ``id`` attribute in their HTML tag.\n        The ``id`` takes the form ``T_<uuid>_row<num_row>_col<num_col>``\n        where ``<uuid>`` is the unique identifier, ``<num_row>`` is the row\n        number and ``<num_col>`` is the column number.\n    na_rep : str, optional\n        Representation for missing values.\n        If ``na_rep`` is None, no special formatting is applied.\n\n        .. versionadded:: 1.0.0\n\n    uuid_len : int, default 5\n        If ``uuid`` is not specified, the length of the ``uuid`` to randomly generate\n        expressed in hex characters, in range [0, 32].\n\n        .. versionadded:: 1.2.0\n\n    Attributes\n    ----------\n    env : Jinja2 jinja2.Environment\n    template : Jinja2 Template\n    loader : Jinja2 Loader\n\n    See Also\n    --------\n    DataFrame.style : Return a Styler object containing methods for building\n        a styled HTML representation for the DataFrame.\n\n    Notes\n    -----\n    Most styling will be done by passing style functions into\n    ``Styler.apply`` or ``Styler.applymap``. Style functions should\n    return values with strings containing CSS ``'attr: value'`` that will\n    be applied to the indicated cells.\n\n    If using in the Jupyter notebook, Styler has defined a ``_repr_html_``\n    to automatically render itself. Otherwise call Styler.render to get\n    the generated HTML.\n\n    CSS classes are attached to the generated HTML\n\n    * Index and Column names include ``index_name`` and ``level<k>``\n      where `k` is its level in a MultiIndex\n    * Index label cells include\n\n      * ``row_heading``\n      * ``row<n>`` where `n` is the numeric position of the row\n      * ``level<k>`` where `k` is the level in a MultiIndex\n\n    * Column label cells include\n      * ``col_heading``\n      * ``col<n>`` where `n` is the numeric position of the column\n      * ``level<k>`` where `k` is the level in a MultiIndex\n\n    * Blank cells include ``blank``\n    * Data cells include ``data``\n    \"\"\"\n\n    loader = jinja2.PackageLoader(\"pandas\", \"io/formats/templates\")\n    env = jinja2.Environment(loader=loader, trim_blocks=True)\n    template = env.get_template(\"html.tpl\")\n\n    def __init__(\n        self,\n        data: FrameOrSeriesUnion,\n        precision: Optional[int] = None,\n        table_styles: Optional[List[Dict[str, List[Tuple[str, str]]]]] = None,\n        uuid: Optional[str] = None,\n        caption: Optional[str] = None,\n        table_attributes: Optional[str] = None,\n        cell_ids: bool = True,\n        na_rep: Optional[str] = None,\n        uuid_len: int = 5,\n    ):\n        self.ctx: DefaultDict[Tuple[int, int], List[str]] = defaultdict(list)\n        self._todo: List[Tuple[Callable, Tuple, Dict]] = []\n\n        if not isinstance(data, (pd.Series, pd.DataFrame)):\n            raise TypeError(\"``data`` must be a Series or DataFrame\")\n        if data.ndim == 1:\n            data = data.to_frame()\n        if not data.index.is_unique or not data.columns.is_unique:\n            raise ValueError(\"style is not supported for non-unique indices.\")\n\n        self.data = data\n        self.index = data.index\n        self.columns = data.columns\n\n        if not isinstance(uuid_len, int) or not uuid_len >= 0:\n            raise TypeError(\"``uuid_len`` must be an integer in range [0, 32].\")\n        self.uuid_len = min(32, uuid_len)\n        self.uuid = (uuid or uuid4().hex[: self.uuid_len]) + \"_\"\n        self.table_styles = table_styles\n        self.caption = caption\n        if precision is None:\n            precision = get_option(\"display.precision\")\n        self.precision = precision\n        self.table_attributes = table_attributes\n        self.hidden_index = False\n        self.hidden_columns: Sequence[int] = []\n        self.cell_ids = cell_ids\n        self.na_rep = na_rep\n\n        self.cell_context: Dict[str, Any] = {}\n\n        # display_funcs maps (row, col) -> formatting function\n\n        def default_display_func(x):\n            if self.na_rep is not None and pd.isna(x):\n                return self.na_rep\n            elif is_float(x):\n                display_format = f\"{x:.{self.precision}f}\"\n                return display_format\n            else:\n                return x\n\n        self._display_funcs: DefaultDict[\n            Tuple[int, int], Callable[[Any], str]\n        ] = defaultdict(lambda: default_display_func)\n\n    def _repr_html_(self) -> str:\n        \"\"\"\n        Hooks into Jupyter notebook rich display system.\n        \"\"\"\n        return self.render()\n\n    @doc(NDFrame.to_excel, klass=\"Styler\")\n    def to_excel(\n        self,\n        excel_writer,\n        sheet_name: str = \"Sheet1\",\n        na_rep: str = \"\",\n        float_format: Optional[str] = None,\n        columns: Optional[Sequence[Label]] = None,\n        header: Union[Sequence[Label], bool] = True,\n        index: bool = True,\n        index_label: Optional[Union[Label, Sequence[Label]]] = None,\n        startrow: int = 0,\n        startcol: int = 0,\n        engine: Optional[str] = None,\n        merge_cells: bool = True,\n        encoding: Optional[str] = None,\n        inf_rep: str = \"inf\",\n        verbose: bool = True,\n        freeze_panes: Optional[Tuple[int, int]] = None,\n    ) -> None:\n\n        from pandas.io.formats.excel import ExcelFormatter\n\n        formatter = ExcelFormatter(\n            self,\n            na_rep=na_rep,\n            cols=columns,\n            header=header,\n            float_format=float_format,\n            index=index,\n            index_label=index_label,\n            merge_cells=merge_cells,\n            inf_rep=inf_rep,\n        )\n        formatter.write(\n            excel_writer,\n            sheet_name=sheet_name,\n            startrow=startrow,\n            startcol=startcol,\n            freeze_panes=freeze_panes,\n            engine=engine,\n        )\n\n    def _translate(self):\n        \"\"\"\n        Convert the DataFrame in `self.data` and the attrs from `_build_styles`\n        into a dictionary of {head, body, uuid, cellstyle}.\n        \"\"\"\n        table_styles = self.table_styles or []\n        caption = self.caption\n        ctx = self.ctx\n        precision = self.precision\n        hidden_index = self.hidden_index\n        hidden_columns = self.hidden_columns\n        uuid = self.uuid\n        ROW_HEADING_CLASS = \"row_heading\"\n        COL_HEADING_CLASS = \"col_heading\"\n        INDEX_NAME_CLASS = \"index_name\"\n\n        DATA_CLASS = \"data\"\n        BLANK_CLASS = \"blank\"\n        BLANK_VALUE = \"\"\n\n        def format_attr(pair):\n            return f\"{pair['key']}={pair['value']}\"\n\n        # for sparsifying a MultiIndex\n        idx_lengths = _get_level_lengths(self.index)\n        col_lengths = _get_level_lengths(self.columns, hidden_columns)\n\n        cell_context = self.cell_context\n\n        n_rlvls = self.data.index.nlevels\n        n_clvls = self.data.columns.nlevels\n        rlabels = self.data.index.tolist()\n        clabels = self.data.columns.tolist()\n\n        if n_rlvls == 1:\n            rlabels = [[x] for x in rlabels]\n        if n_clvls == 1:\n            clabels = [[x] for x in clabels]\n        clabels = list(zip(*clabels))\n\n        cellstyle_map = defaultdict(list)\n        head = []\n\n        for r in range(n_clvls):\n            # Blank for Index columns...\n            row_es = [\n                {\n                    \"type\": \"th\",\n                    \"value\": BLANK_VALUE,\n                    \"display_value\": BLANK_VALUE,\n                    \"is_visible\": not hidden_index,\n                    \"class\": \" \".join([BLANK_CLASS]),\n                }\n            ] * (n_rlvls - 1)\n\n            # ... except maybe the last for columns.names\n            name = self.data.columns.names[r]\n            cs = [\n                BLANK_CLASS if name is None else INDEX_NAME_CLASS,\n                f\"level{r}\",\n            ]\n            name = BLANK_VALUE if name is None else name\n            row_es.append(\n                {\n                    \"type\": \"th\",\n                    \"value\": name,\n                    \"display_value\": name,\n                    \"class\": \" \".join(cs),\n                    \"is_visible\": not hidden_index,\n                }\n            )\n\n            if clabels:\n                for c, value in enumerate(clabels[r]):\n                    cs = [\n                        COL_HEADING_CLASS,\n                        f\"level{r}\",\n                        f\"col{c}\",\n                    ]\n                    cs.extend(\n                        cell_context.get(\"col_headings\", {}).get(r, {}).get(c, [])\n                    )\n                    es = {\n                        \"type\": \"th\",\n                        \"value\": value,\n                        \"display_value\": value,\n                        \"class\": \" \".join(cs),\n                        \"is_visible\": _is_visible(c, r, col_lengths),\n                    }\n                    colspan = col_lengths.get((r, c), 0)\n                    if colspan > 1:\n                        es[\"attributes\"] = [\n                            format_attr({\"key\": \"colspan\", \"value\": f'\"{colspan}\"'})\n                        ]\n                    row_es.append(es)\n                head.append(row_es)\n\n        if (\n            self.data.index.names\n            and com.any_not_none(*self.data.index.names)\n            and not hidden_index\n        ):\n            index_header_row = []\n\n            for c, name in enumerate(self.data.index.names):\n                cs = [INDEX_NAME_CLASS, f\"level{c}\"]\n                name = \"\" if name is None else name\n                index_header_row.append(\n                    {\"type\": \"th\", \"value\": name, \"class\": \" \".join(cs)}\n                )\n\n            index_header_row.extend(\n                [{\"type\": \"th\", \"value\": BLANK_VALUE, \"class\": \" \".join([BLANK_CLASS])}]\n                * (len(clabels[0]) - len(hidden_columns))\n            )\n\n            head.append(index_header_row)\n\n        body = []\n        for r, idx in enumerate(self.data.index):\n            row_es = []\n            for c, value in enumerate(rlabels[r]):\n                rid = [\n                    ROW_HEADING_CLASS,\n                    f\"level{c}\",\n                    f\"row{r}\",\n                ]\n                es = {\n                    \"type\": \"th\",\n                    \"is_visible\": (_is_visible(r, c, idx_lengths) and not hidden_index),\n                    \"value\": value,\n                    \"display_value\": value,\n                    \"id\": \"_\".join(rid[1:]),\n                    \"class\": \" \".join(rid),\n                }\n                rowspan = idx_lengths.get((c, r), 0)\n                if rowspan > 1:\n                    es[\"attributes\"] = [\n                        format_attr({\"key\": \"rowspan\", \"value\": rowspan})\n                    ]\n                row_es.append(es)\n\n            for c, col in enumerate(self.data.columns):\n                cs = [DATA_CLASS, f\"row{r}\", f\"col{c}\"]\n                cs.extend(cell_context.get(\"data\", {}).get(r, {}).get(c, []))\n                formatter = self._display_funcs[(r, c)]\n                value = self.data.iloc[r, c]\n                row_dict = {\n                    \"type\": \"td\",\n                    \"value\": value,\n                    \"class\": \" \".join(cs),\n                    \"display_value\": formatter(value),\n                    \"is_visible\": (c not in hidden_columns),\n                }\n                # only add an id if the cell has a style\n                props = []\n                if self.cell_ids or (r, c) in ctx:\n                    row_dict[\"id\"] = \"_\".join(cs[1:])\n                    for x in ctx[r, c]:\n                        # have to handle empty styles like ['']\n                        if x.count(\":\"):\n                            props.append(tuple(x.split(\":\")))\n                        else:\n                            props.append((\"\", \"\"))\n                row_es.append(row_dict)\n                cellstyle_map[tuple(props)].append(f\"row{r}_col{c}\")\n            body.append(row_es)\n\n        cellstyle = [\n            {\"props\": list(props), \"selectors\": selectors}\n            for props, selectors in cellstyle_map.items()\n        ]\n\n        table_attr = self.table_attributes\n        use_mathjax = get_option(\"display.html.use_mathjax\")\n        if not use_mathjax:\n            table_attr = table_attr or \"\"\n            if 'class=\"' in table_attr:\n                table_attr = table_attr.replace('class=\"', 'class=\"tex2jax_ignore ')\n            else:\n                table_attr += ' class=\"tex2jax_ignore\"'\n\n        return dict(\n            head=head,\n            cellstyle=cellstyle,\n            body=body,\n            uuid=uuid,\n            precision=precision,\n            table_styles=table_styles,\n            caption=caption,\n            table_attributes=table_attr,\n        )\n\n    def format(self, formatter, subset=None, na_rep: Optional[str] = None) -> \"Styler\":\n        \"\"\"\n        Format the text display value of cells.\n\n        Parameters\n        ----------\n        formatter : str, callable, dict or None\n            If ``formatter`` is None, the default formatter is used.\n        subset : IndexSlice\n            An argument to ``DataFrame.loc`` that restricts which elements\n            ``formatter`` is applied to.\n        na_rep : str, optional\n            Representation for missing values.\n            If ``na_rep`` is None, no special formatting is applied.\n\n            .. versionadded:: 1.0.0\n\n        Returns\n        -------\n        self : Styler\n\n        Notes\n        -----\n        ``formatter`` is either an ``a`` or a dict ``{column name: a}`` where\n        ``a`` is one of\n\n        - str: this will be wrapped in: ``a.format(x)``\n        - callable: called with the value of an individual cell\n\n        The default display value for numeric values is the \"general\" (``g``)\n        format with ``pd.options.display.precision`` precision.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.random.randn(4, 2), columns=['a', 'b'])\n        >>> df.style.format(\"{:.2%}\")\n        >>> df['c'] = ['a', 'b', 'c', 'd']\n        >>> df.style.format({'c': str.upper})\n        \"\"\"\n        if formatter is None:\n            assert self._display_funcs.default_factory is not None\n            formatter = self._display_funcs.default_factory()\n\n        if subset is None:\n            row_locs = range(len(self.data))\n            col_locs = range(len(self.data.columns))\n        else:\n            subset = non_reducing_slice(subset)\n            if len(subset) == 1:\n                subset = subset, self.data.columns\n\n            sub_df = self.data.loc[subset]\n            row_locs = self.data.index.get_indexer_for(sub_df.index)\n            col_locs = self.data.columns.get_indexer_for(sub_df.columns)\n\n        if is_dict_like(formatter):\n            for col, col_formatter in formatter.items():\n                # formatter must be callable, so '{}' are converted to lambdas\n                col_formatter = _maybe_wrap_formatter(col_formatter, na_rep)\n                col_num = self.data.columns.get_indexer_for([col])[0]\n\n                for row_num in row_locs:\n                    self._display_funcs[(row_num, col_num)] = col_formatter\n        else:\n            # single scalar to format all cells with\n            formatter = _maybe_wrap_formatter(formatter, na_rep)\n            locs = product(*(row_locs, col_locs))\n            for i, j in locs:\n                self._display_funcs[(i, j)] = formatter\n        return self\n\n    def set_td_classes(self, classes: DataFrame) -> \"Styler\":\n        \"\"\"\n        Add string based CSS class names to data cells that will appear within the\n        `Styler` HTML result. These classes are added within specified `<td>` elements.\n\n        Parameters\n        ----------\n        classes : DataFrame\n            DataFrame containing strings that will be translated to CSS classes,\n            mapped by identical column and index values that must exist on the\n            underlying `Styler` data. None, NaN values, and empty strings will\n            be ignored and not affect the rendered HTML.\n\n        Returns\n        -------\n        self : Styler\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(data=[[1, 2, 3], [4, 5, 6]], columns=[\"A\", \"B\", \"C\"])\n        >>> classes = pd.DataFrame([\n        ...     [\"min-val red\", \"\", \"blue\"],\n        ...     [\"red\", None, \"blue max-val\"]\n        ... ], index=df.index, columns=df.columns)\n        >>> df.style.set_td_classes(classes)\n\n        Using `MultiIndex` columns and a `classes` `DataFrame` as a subset of the\n        underlying,\n\n        >>> df = pd.DataFrame([[1,2],[3,4]], index=[\"a\", \"b\"],\n        ...     columns=[[\"level0\", \"level0\"], [\"level1a\", \"level1b\"]])\n        >>> classes = pd.DataFrame([\"min-val\"], index=[\"a\"],\n        ...     columns=[[\"level0\"],[\"level1a\"]])\n        >>> df.style.set_td_classes(classes)\n\n        Form of the output with new additional css classes,\n\n        >>> df = pd.DataFrame([[1]])\n        >>> css = pd.DataFrame([\"other-class\"])\n        >>> s = Styler(df, uuid=\"_\", cell_ids=False).set_td_classes(css)\n        >>> s.hide_index().render()\n        '<style  type=\"text/css\" ></style>'\n        '<table id=\"T__\" >'\n        '  <thead>'\n        '    <tr><th class=\"col_heading level0 col0\" >0</th></tr>'\n        '  </thead>'\n        '  <tbody>'\n        '    <tr><td  class=\"data row0 col0 other-class\" >1</td></tr>'\n        '  </tbody>'\n        '</table>'\n\n        \"\"\"\n        classes = classes.reindex_like(self.data)\n\n        mask = (classes.isna()) | (classes.eq(\"\"))\n        self.cell_context[\"data\"] = {\n            r: {c: [str(classes.iloc[r, c])]}\n            for r, rn in enumerate(classes.index)\n            for c, cn in enumerate(classes.columns)\n            if not mask.iloc[r, c]\n        }\n\n        return self\n\n    def render(self, **kwargs) -> str:\n        \"\"\"\n        Render the built up styles to HTML.\n\n        Parameters\n        ----------\n        **kwargs\n            Any additional keyword arguments are passed\n            through to ``self.template.render``.\n            This is useful when you need to provide\n            additional variables for a custom template.\n\n        Returns\n        -------\n        rendered : str\n            The rendered HTML.\n\n        Notes\n        -----\n        ``Styler`` objects have defined the ``_repr_html_`` method\n        which automatically calls ``self.render()`` when it's the\n        last item in a Notebook cell. When calling ``Styler.render()``\n        directly, wrap the result in ``IPython.display.HTML`` to view\n        the rendered HTML in the notebook.\n\n        Pandas uses the following keys in render. Arguments passed\n        in ``**kwargs`` take precedence, so think carefully if you want\n        to override them:\n\n        * head\n        * cellstyle\n        * body\n        * uuid\n        * precision\n        * table_styles\n        * caption\n        * table_attributes\n        \"\"\"\n        self._compute()\n        # TODO: namespace all the pandas keys\n        d = self._translate()\n        # filter out empty styles, every cell will have a class\n        # but the list of props may just be [['', '']].\n        # so we have the nested anys below\n        trimmed = [x for x in d[\"cellstyle\"] if any(any(y) for y in x[\"props\"])]\n        d[\"cellstyle\"] = trimmed\n        d.update(kwargs)\n        return self.template.render(**d)\n\n    def _update_ctx(self, attrs: DataFrame) -> None:\n        \"\"\"\n        Update the state of the Styler.\n\n        Collects a mapping of {index_label: ['<property>: <value>']}.\n\n        Parameters\n        ----------\n        attrs : DataFrame\n            should contain strings of '<property>: <value>;<prop2>: <val2>'\n            Whitespace shouldn't matter and the final trailing ';' shouldn't\n            matter.\n        \"\"\"\n        coli = {k: i for i, k in enumerate(self.columns)}\n        rowi = {k: i for i, k in enumerate(self.index)}\n        for jj in range(len(attrs.columns)):\n            cn = attrs.columns[jj]\n            j = coli[cn]\n            for rn, c in attrs[[cn]].itertuples():\n                if not c:\n                    continue\n                c = c.rstrip(\";\")\n                if not c:\n                    continue\n                i = rowi[rn]\n                for pair in c.split(\";\"):\n                    self.ctx[(i, j)].append(pair)\n\n    def _copy(self, deepcopy: bool = False) -> \"Styler\":\n        styler = Styler(\n            self.data,\n            precision=self.precision,\n            caption=self.caption,\n            uuid=self.uuid,\n            table_styles=self.table_styles,\n            na_rep=self.na_rep,\n        )\n        if deepcopy:\n            styler.ctx = copy.deepcopy(self.ctx)\n            styler._todo = copy.deepcopy(self._todo)\n        else:\n            styler.ctx = self.ctx\n            styler._todo = self._todo\n        return styler\n\n    def __copy__(self) -> \"Styler\":\n        \"\"\"\n        Deep copy by default.\n        \"\"\"\n        return self._copy(deepcopy=False)\n\n    def __deepcopy__(self, memo) -> \"Styler\":\n        return self._copy(deepcopy=True)\n\n    def clear(self) -> None:\n        \"\"\"\n        Reset the styler, removing any previously applied styles.\n\n        Returns None.\n        \"\"\"\n        self.ctx.clear()\n        self.cell_context = {}\n        self._todo = []\n\n    def _compute(self):\n        \"\"\"\n        Execute the style functions built up in `self._todo`.\n\n        Relies on the conventions that all style functions go through\n        .apply or .applymap. The append styles to apply as tuples of\n\n        (application method, *args, **kwargs)\n        \"\"\"\n        r = self\n        for func, args, kwargs in self._todo:\n            r = func(self)(*args, **kwargs)\n        return r\n\n    def _apply(\n        self,\n        func: Callable[..., \"Styler\"],\n        axis: Optional[Axis] = 0,\n        subset=None,\n        **kwargs,\n    ) -> \"Styler\":\n        subset = slice(None) if subset is None else subset\n        subset = non_reducing_slice(subset)\n        data = self.data.loc[subset]\n        if axis is not None:\n            result = data.apply(func, axis=axis, result_type=\"expand\", **kwargs)\n            result.columns = data.columns\n        else:\n            result = func(data, **kwargs)\n            if not isinstance(result, pd.DataFrame):\n                raise TypeError(\n                    f\"Function {repr(func)} must return a DataFrame when \"\n                    f\"passed to `Styler.apply` with axis=None\"\n                )\n            if not (\n                result.index.equals(data.index) and result.columns.equals(data.columns)\n            ):\n                raise ValueError(\n                    f\"Result of {repr(func)} must have identical \"\n                    f\"index and columns as the input\"\n                )\n\n        result_shape = result.shape\n        expected_shape = self.data.loc[subset].shape\n        if result_shape != expected_shape:\n            raise ValueError(\n                f\"Function {repr(func)} returned the wrong shape.\\n\"\n                f\"Result has shape: {result.shape}\\n\"\n                f\"Expected shape:   {expected_shape}\"\n            )\n        self._update_ctx(result)\n        return self\n\n    def apply(\n        self,\n        func: Callable[..., \"Styler\"],\n        axis: Optional[Axis] = 0,\n        subset=None,\n        **kwargs,\n    ) -> \"Styler\":\n        \"\"\"\n        Apply a function column-wise, row-wise, or table-wise.\n\n        Updates the HTML representation with the result.\n\n        Parameters\n        ----------\n        func : function\n            ``func`` should take a Series or DataFrame (depending\n            on ``axis``), and return an object with the same shape.\n            Must return a DataFrame with identical index and\n            column labels when ``axis=None``.\n        axis : {0 or 'index', 1 or 'columns', None}, default 0\n            Apply to each column (``axis=0`` or ``'index'``), to each row\n            (``axis=1`` or ``'columns'``), or to the entire DataFrame at once\n            with ``axis=None``.\n        subset : IndexSlice\n            A valid indexer to limit ``data`` to *before* applying the\n            function. Consider using a pandas.IndexSlice.\n        **kwargs : dict\n            Pass along to ``func``.\n\n        Returns\n        -------\n        self : Styler\n\n        Notes\n        -----\n        The output shape of ``func`` should match the input, i.e. if\n        ``x`` is the input row, column, or table (depending on ``axis``),\n        then ``func(x).shape == x.shape`` should be true.\n\n        This is similar to ``DataFrame.apply``, except that ``axis=None``\n        applies the function to the entire DataFrame at once,\n        rather than column-wise or row-wise.\n\n        Examples\n        --------\n        >>> def highlight_max(x):\n        ...     return ['background-color: yellow' if v == x.max() else ''\n                        for v in x]\n        ...\n        >>> df = pd.DataFrame(np.random.randn(5, 2))\n        >>> df.style.apply(highlight_max)\n        \"\"\"\n        self._todo.append(\n            (lambda instance: getattr(instance, \"_apply\"), (func, axis, subset), kwargs)\n        )\n        return self\n\n    def _applymap(self, func: Callable, subset=None, **kwargs) -> \"Styler\":\n        func = partial(func, **kwargs)  # applymap doesn't take kwargs?\n        if subset is None:\n            subset = pd.IndexSlice[:]\n        subset = non_reducing_slice(subset)\n        result = self.data.loc[subset].applymap(func)\n        self._update_ctx(result)\n        return self\n\n    def applymap(self, func: Callable, subset=None, **kwargs) -> \"Styler\":\n        \"\"\"\n        Apply a function elementwise.\n\n        Updates the HTML representation with the result.\n\n        Parameters\n        ----------\n        func : function\n            ``func`` should take a scalar and return a scalar.\n        subset : IndexSlice\n            A valid indexer to limit ``data`` to *before* applying the\n            function. Consider using a pandas.IndexSlice.\n        **kwargs : dict\n            Pass along to ``func``.\n\n        Returns\n        -------\n        self : Styler\n\n        See Also\n        --------\n        Styler.where: Updates the HTML representation with a style which is\n            selected in accordance with the return value of a function.\n        \"\"\"\n        self._todo.append(\n            (lambda instance: getattr(instance, \"_applymap\"), (func, subset), kwargs)\n        )\n        return self\n\n    def where(\n        self,\n        cond: Callable,\n        value: str,\n        other: Optional[str] = None,\n        subset=None,\n        **kwargs,\n    ) -> \"Styler\":\n        \"\"\"\n        Apply a function elementwise.\n\n        Updates the HTML representation with a style which is\n        selected in accordance with the return value of a function.\n\n        Parameters\n        ----------\n        cond : callable\n            ``cond`` should take a scalar and return a boolean.\n        value : str\n            Applied when ``cond`` returns true.\n        other : str\n            Applied when ``cond`` returns false.\n        subset : IndexSlice\n            A valid indexer to limit ``data`` to *before* applying the\n            function. Consider using a pandas.IndexSlice.\n        **kwargs : dict\n            Pass along to ``cond``.\n\n        Returns\n        -------\n        self : Styler\n\n        See Also\n        --------\n        Styler.applymap: Updates the HTML representation with the result.\n        \"\"\"\n        if other is None:\n            other = \"\"\n\n        return self.applymap(\n            lambda val: value if cond(val) else other, subset=subset, **kwargs\n        )\n\n    def set_precision(self, precision: int) -> \"Styler\":\n        \"\"\"\n        Set the precision used to render.\n\n        Parameters\n        ----------\n        precision : int\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        self.precision = precision\n        return self\n\n    def set_table_attributes(self, attributes: str) -> \"Styler\":\n        \"\"\"\n        Set the table attributes.\n\n        These are the items that show up in the opening ``<table>`` tag\n        in addition to to automatic (by default) id.\n\n        Parameters\n        ----------\n        attributes : str\n\n        Returns\n        -------\n        self : Styler\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.random.randn(10, 4))\n        >>> df.style.set_table_attributes('class=\"pure-table\"')\n        # ... <table class=\"pure-table\"> ...\n        \"\"\"\n        self.table_attributes = attributes\n        return self\n\n    def export(self) -> List[Tuple[Callable, Tuple, Dict]]:\n        \"\"\"\n        Export the styles to applied to the current Styler.\n\n        Can be applied to a second style with ``Styler.use``.\n\n        Returns\n        -------\n        styles : list\n\n        See Also\n        --------\n        Styler.use: Set the styles on the current Styler.\n        \"\"\"\n        return self._todo\n\n    def use(self, styles: List[Tuple[Callable, Tuple, Dict]]) -> \"Styler\":\n        \"\"\"\n        Set the styles on the current Styler.\n\n        Possibly uses styles from ``Styler.export``.\n\n        Parameters\n        ----------\n        styles : list\n            List of style functions.\n\n        Returns\n        -------\n        self : Styler\n\n        See Also\n        --------\n        Styler.export : Export the styles to applied to the current Styler.\n        \"\"\"\n        self._todo.extend(styles)\n        return self\n\n    def set_uuid(self, uuid: str) -> \"Styler\":\n        \"\"\"\n        Set the uuid for a Styler.\n\n        Parameters\n        ----------\n        uuid : str\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        self.uuid = uuid\n        return self\n\n    def set_caption(self, caption: str) -> \"Styler\":\n        \"\"\"\n        Set the caption on a Styler.\n\n        Parameters\n        ----------\n        caption : str\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        self.caption = caption\n        return self\n\n    def set_table_styles(self, table_styles) -> \"Styler\":\n        \"\"\"\n        Set the table styles on a Styler.\n\n        These are placed in a ``<style>`` tag before the generated HTML table.\n\n        Parameters\n        ----------\n        table_styles : list\n            Each individual table_style should be a dictionary with\n            ``selector`` and ``props`` keys. ``selector`` should be a CSS\n            selector that the style will be applied to (automatically\n            prefixed by the table's UUID) and ``props`` should be a list of\n            tuples with ``(attribute, value)``.\n\n        Returns\n        -------\n        self : Styler\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.random.randn(10, 4))\n        >>> df.style.set_table_styles(\n        ...     [{'selector': 'tr:hover',\n        ...       'props': [('background-color', 'yellow')]}]\n        ... )\n        \"\"\"\n        self.table_styles = table_styles\n        return self\n\n    def set_na_rep(self, na_rep: str) -> \"Styler\":\n        \"\"\"\n        Set the missing data representation on a Styler.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        na_rep : str\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        self.na_rep = na_rep\n        return self\n\n    def hide_index(self) -> \"Styler\":\n        \"\"\"\n        Hide any indices from rendering.\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        self.hidden_index = True\n        return self\n\n    def hide_columns(self, subset) -> \"Styler\":\n        \"\"\"\n        Hide columns from rendering.\n\n        Parameters\n        ----------\n        subset : IndexSlice\n            An argument to ``DataFrame.loc`` that identifies which columns\n            are hidden.\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        subset = non_reducing_slice(subset)\n        hidden_df = self.data.loc[subset]\n        self.hidden_columns = self.columns.get_indexer_for(hidden_df.columns)\n        return self\n\n    # -----------------------------------------------------------------------\n    # A collection of \"builtin\" styles\n    # -----------------------------------------------------------------------\n\n    @staticmethod\n    def _highlight_null(v, null_color: str) -> str:\n        return f\"background-color: {null_color}\" if pd.isna(v) else \"\"\n\n    def highlight_null(\n        self,\n        null_color: str = \"red\",\n        subset: Optional[Union[Label, Sequence[Label]]] = None,\n    ) -> \"Styler\":\n        \"\"\"\n        Shade the background ``null_color`` for missing values.\n\n        Parameters\n        ----------\n        null_color : str, default 'red'\n        subset : label or list of labels, default None\n            A valid slice for ``data`` to limit the style application to.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        self.applymap(self._highlight_null, null_color=null_color, subset=subset)\n        return self\n\n    def background_gradient(\n        self,\n        cmap=\"PuBu\",\n        low: float = 0,\n        high: float = 0,\n        axis: Optional[Axis] = 0,\n        subset=None,\n        text_color_threshold: float = 0.408,\n        vmin: Optional[float] = None,\n        vmax: Optional[float] = None,\n    ) -> \"Styler\":\n        \"\"\"\n        Color the background in a gradient style.\n\n        The background color is determined according\n        to the data in each column (optionally row). Requires matplotlib.\n\n        Parameters\n        ----------\n        cmap : str or colormap\n            Matplotlib colormap.\n        low : float\n            Compress the range by the low.\n        high : float\n            Compress the range by the high.\n        axis : {0 or 'index', 1 or 'columns', None}, default 0\n            Apply to each column (``axis=0`` or ``'index'``), to each row\n            (``axis=1`` or ``'columns'``), or to the entire DataFrame at once\n            with ``axis=None``.\n        subset : IndexSlice\n            A valid slice for ``data`` to limit the style application to.\n        text_color_threshold : float or int\n            Luminance threshold for determining text color. Facilitates text\n            visibility across varying background colors. From 0 to 1.\n            0 = all text is dark colored, 1 = all text is light colored.\n\n            .. versionadded:: 0.24.0\n\n        vmin : float, optional\n            Minimum data value that corresponds to colormap minimum value.\n            When None (default): the minimum value of the data will be used.\n\n            .. versionadded:: 1.0.0\n\n        vmax : float, optional\n            Maximum data value that corresponds to colormap maximum value.\n            When None (default): the maximum value of the data will be used.\n\n            .. versionadded:: 1.0.0\n\n        Returns\n        -------\n        self : Styler\n\n        Raises\n        ------\n        ValueError\n            If ``text_color_threshold`` is not a value from 0 to 1.\n\n        Notes\n        -----\n        Set ``text_color_threshold`` or tune ``low`` and ``high`` to keep the\n        text legible by not using the entire range of the color map. The range\n        of the data is extended by ``low * (x.max() - x.min())`` and ``high *\n        (x.max() - x.min())`` before normalizing.\n        \"\"\"\n        subset = maybe_numeric_slice(self.data, subset)\n        subset = non_reducing_slice(subset)\n        self.apply(\n            self._background_gradient,\n            cmap=cmap,\n            subset=subset,\n            axis=axis,\n            low=low,\n            high=high,\n            text_color_threshold=text_color_threshold,\n            vmin=vmin,\n            vmax=vmax,\n        )\n        return self\n\n    @staticmethod\n    def _background_gradient(\n        s,\n        cmap=\"PuBu\",\n        low: float = 0,\n        high: float = 0,\n        text_color_threshold: float = 0.408,\n        vmin: Optional[float] = None,\n        vmax: Optional[float] = None,\n    ):\n        \"\"\"\n        Color background in a range according to the data.\n        \"\"\"\n        if (\n            not isinstance(text_color_threshold, (float, int))\n            or not 0 <= text_color_threshold <= 1\n        ):\n            msg = \"`text_color_threshold` must be a value from 0 to 1.\"\n            raise ValueError(msg)\n\n        with _mpl(Styler.background_gradient) as (plt, colors):\n            smin = np.nanmin(s.to_numpy()) if vmin is None else vmin\n            smax = np.nanmax(s.to_numpy()) if vmax is None else vmax\n            rng = smax - smin\n            # extend lower / upper bounds, compresses color range\n            norm = colors.Normalize(smin - (rng * low), smax + (rng * high))\n            # matplotlib colors.Normalize modifies inplace?\n            # https://github.com/matplotlib/matplotlib/issues/5427\n            rgbas = plt.cm.get_cmap(cmap)(norm(s.to_numpy(dtype=float)))\n\n            def relative_luminance(rgba) -> float:\n                \"\"\"\n                Calculate relative luminance of a color.\n\n                The calculation adheres to the W3C standards\n                (https://www.w3.org/WAI/GL/wiki/Relative_luminance)\n\n                Parameters\n                ----------\n                color : rgb or rgba tuple\n\n                Returns\n                -------\n                float\n                    The relative luminance as a value from 0 to 1\n                \"\"\"\n                r, g, b = (\n                    x / 12.92 if x <= 0.03928 else ((x + 0.055) / 1.055 ** 2.4)\n                    for x in rgba[:3]\n                )\n                return 0.2126 * r + 0.7152 * g + 0.0722 * b\n\n            def css(rgba) -> str:\n                dark = relative_luminance(rgba) < text_color_threshold\n                text_color = \"#f1f1f1\" if dark else \"#000000\"\n                return f\"background-color: {colors.rgb2hex(rgba)};color: {text_color};\"\n\n            if s.ndim == 1:\n                return [css(rgba) for rgba in rgbas]\n            else:\n                return pd.DataFrame(\n                    [[css(rgba) for rgba in row] for row in rgbas],\n                    index=s.index,\n                    columns=s.columns,\n                )\n\n    def set_properties(self, subset=None, **kwargs) -> \"Styler\":\n        \"\"\"\n        Method to set one or more non-data dependent properties or each cell.\n\n        Parameters\n        ----------\n        subset : IndexSlice\n            A valid slice for ``data`` to limit the style application to.\n        **kwargs : dict\n            A dictionary of property, value pairs to be set for each cell.\n\n        Returns\n        -------\n        self : Styler\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.random.randn(10, 4))\n        >>> df.style.set_properties(color=\"white\", align=\"right\")\n        >>> df.style.set_properties(**{'background-color': 'yellow'})\n        \"\"\"\n        values = \";\".join(f\"{p}: {v}\" for p, v in kwargs.items())\n        f = lambda x: values\n        return self.applymap(f, subset=subset)\n\n    @staticmethod\n    def _bar(\n        s,\n        align: str,\n        colors: List[str],\n        width: float = 100,\n        vmin: Optional[float] = None,\n        vmax: Optional[float] = None,\n    ):\n        \"\"\"\n        Draw bar chart in dataframe cells.\n        \"\"\"\n        # Get input value range.\n        smin = np.nanmin(s.to_numpy()) if vmin is None else vmin\n        smax = np.nanmax(s.to_numpy()) if vmax is None else vmax\n        if align == \"mid\":\n            smin = min(0, smin)\n            smax = max(0, smax)\n        elif align == \"zero\":\n            # For \"zero\" mode, we want the range to be symmetrical around zero.\n            smax = max(abs(smin), abs(smax))\n            smin = -smax\n        # Transform to percent-range of linear-gradient\n        normed = width * (s.to_numpy(dtype=float) - smin) / (smax - smin + 1e-12)\n        zero = -width * smin / (smax - smin + 1e-12)\n\n        def css_bar(start: float, end: float, color: str) -> str:\n            \"\"\"\n            Generate CSS code to draw a bar from start to end.\n            \"\"\"\n            css = \"width: 10em; height: 80%;\"\n            if end > start:\n                css += \"background: linear-gradient(90deg,\"\n                if start > 0:\n                    css += f\" transparent {start:.1f}%, {color} {start:.1f}%, \"\n                e = min(end, width)\n                css += f\"{color} {e:.1f}%, transparent {e:.1f}%)\"\n            return css\n\n        def css(x):\n            if pd.isna(x):\n                return \"\"\n\n            # avoid deprecated indexing `colors[x > zero]`\n            color = colors[1] if x > zero else colors[0]\n\n            if align == \"left\":\n                return css_bar(0, x, color)\n            else:\n                return css_bar(min(x, zero), max(x, zero), color)\n\n        if s.ndim == 1:\n            return [css(x) for x in normed]\n        else:\n            return pd.DataFrame(\n                [[css(x) for x in row] for row in normed],\n                index=s.index,\n                columns=s.columns,\n            )\n\n    def bar(\n        self,\n        subset=None,\n        axis: Optional[Axis] = 0,\n        color=\"#d65f5f\",\n        width: float = 100,\n        align: str = \"left\",\n        vmin: Optional[float] = None,\n        vmax: Optional[float] = None,\n    ) -> \"Styler\":\n        \"\"\"\n        Draw bar chart in the cell backgrounds.\n\n        Parameters\n        ----------\n        subset : IndexSlice, optional\n            A valid slice for `data` to limit the style application to.\n        axis : {0 or 'index', 1 or 'columns', None}, default 0\n            Apply to each column (``axis=0`` or ``'index'``), to each row\n            (``axis=1`` or ``'columns'``), or to the entire DataFrame at once\n            with ``axis=None``.\n        color : str or 2-tuple/list\n            If a str is passed, the color is the same for both\n            negative and positive numbers. If 2-tuple/list is used, the\n            first element is the color_negative and the second is the\n            color_positive (eg: ['#d65f5f', '#5fba7d']).\n        width : float, default 100\n            A number between 0 or 100. The largest value will cover `width`\n            percent of the cell's width.\n        align : {'left', 'zero',' mid'}, default 'left'\n            How to align the bars with the cells.\n\n            - 'left' : the min value starts at the left of the cell.\n            - 'zero' : a value of zero is located at the center of the cell.\n            - 'mid' : the center of the cell is at (max-min)/2, or\n              if values are all negative (positive) the zero is aligned\n              at the right (left) of the cell.\n        vmin : float, optional\n            Minimum bar value, defining the left hand limit\n            of the bar drawing range, lower values are clipped to `vmin`.\n            When None (default): the minimum value of the data will be used.\n\n            .. versionadded:: 0.24.0\n\n        vmax : float, optional\n            Maximum bar value, defining the right hand limit\n            of the bar drawing range, higher values are clipped to `vmax`.\n            When None (default): the maximum value of the data will be used.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        if align not in (\"left\", \"zero\", \"mid\"):\n            raise ValueError(\"`align` must be one of {'left', 'zero',' mid'}\")\n\n        if not (is_list_like(color)):\n            color = [color, color]\n        elif len(color) == 1:\n            color = [color[0], color[0]]\n        elif len(color) > 2:\n            raise ValueError(\n                \"`color` must be string or a list-like \"\n                \"of length 2: [`color_neg`, `color_pos`] \"\n                \"(eg: color=['#d65f5f', '#5fba7d'])\"\n            )\n\n        subset = maybe_numeric_slice(self.data, subset)\n        subset = non_reducing_slice(subset)\n        self.apply(\n            self._bar,\n            subset=subset,\n            axis=axis,\n            align=align,\n            colors=color,\n            width=width,\n            vmin=vmin,\n            vmax=vmax,\n        )\n\n        return self\n\n    def highlight_max(\n        self, subset=None, color: str = \"yellow\", axis: Optional[Axis] = 0\n    ) -> \"Styler\":\n        \"\"\"\n        Highlight the maximum by shading the background.\n\n        Parameters\n        ----------\n        subset : IndexSlice, default None\n            A valid slice for ``data`` to limit the style application to.\n        color : str, default 'yellow'\n        axis : {0 or 'index', 1 or 'columns', None}, default 0\n            Apply to each column (``axis=0`` or ``'index'``), to each row\n            (``axis=1`` or ``'columns'``), or to the entire DataFrame at once\n            with ``axis=None``.\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        return self._highlight_handler(subset=subset, color=color, axis=axis, max_=True)\n\n    def highlight_min(\n        self, subset=None, color: str = \"yellow\", axis: Optional[Axis] = 0\n    ) -> \"Styler\":\n        \"\"\"\n        Highlight the minimum by shading the background.\n\n        Parameters\n        ----------\n        subset : IndexSlice, default None\n            A valid slice for ``data`` to limit the style application to.\n        color : str, default 'yellow'\n        axis : {0 or 'index', 1 or 'columns', None}, default 0\n            Apply to each column (``axis=0`` or ``'index'``), to each row\n            (``axis=1`` or ``'columns'``), or to the entire DataFrame at once\n            with ``axis=None``.\n\n        Returns\n        -------\n        self : Styler\n        \"\"\"\n        return self._highlight_handler(\n            subset=subset, color=color, axis=axis, max_=False\n        )\n\n    def _highlight_handler(\n        self,\n        subset=None,\n        color: str = \"yellow\",\n        axis: Optional[Axis] = None,\n        max_: bool = True,\n    ) -> \"Styler\":\n        subset = non_reducing_slice(maybe_numeric_slice(self.data, subset))\n        self.apply(\n            self._highlight_extrema, color=color, axis=axis, subset=subset, max_=max_\n        )\n        return self\n\n    @staticmethod\n    def _highlight_extrema(\n        data: FrameOrSeries, color: str = \"yellow\", max_: bool = True\n    ):\n        \"\"\"\n        Highlight the min or max in a Series or DataFrame.\n        \"\"\"\n        attr = f\"background-color: {color}\"\n\n        if max_:\n            extrema = data == np.nanmax(data.to_numpy())\n        else:\n            extrema = data == np.nanmin(data.to_numpy())\n\n        if data.ndim == 1:  # Series from .apply\n            return [attr if v else \"\" for v in extrema]\n        else:  # DataFrame from .tee\n            return pd.DataFrame(\n                np.where(extrema, attr, \"\"), index=data.index, columns=data.columns\n            )\n\n    @classmethod\n    def from_custom_template(cls, searchpath, name):\n        \"\"\"\n        Factory function for creating a subclass of ``Styler``.\n\n        Uses a custom template and Jinja environment.\n\n        Parameters\n        ----------\n        searchpath : str or list\n            Path or paths of directories containing the templates.\n        name : str\n            Name of your custom template to use for rendering.\n\n        Returns\n        -------\n        MyStyler : subclass of Styler\n            Has the correct ``env`` and ``template`` class attributes set.\n        \"\"\"\n        loader = jinja2.ChoiceLoader([jinja2.FileSystemLoader(searchpath), cls.loader])\n\n        # mypy doesnt like dynamically-defined class\n        # error: Variable \"cls\" is not valid as a type  [valid-type]\n        # error: Invalid base class \"cls\"  [misc]\n        class MyStyler(cls):  # type:ignore[valid-type,misc]\n            env = jinja2.Environment(loader=loader)\n            template = env.get_template(name)\n\n        return MyStyler\n\n    def pipe(self, func: Callable, *args, **kwargs):\n        \"\"\"\n        Apply ``func(self, *args, **kwargs)``, and return the result.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        func : function\n            Function to apply to the Styler.  Alternatively, a\n            ``(callable, keyword)`` tuple where ``keyword`` is a string\n            indicating the keyword of ``callable`` that expects the Styler.\n        *args : optional\n            Arguments passed to `func`.\n        **kwargs : optional\n            A dictionary of keyword arguments passed into ``func``.\n\n        Returns\n        -------\n        object :\n            The value returned by ``func``.\n\n        See Also\n        --------\n        DataFrame.pipe : Analogous method for DataFrame.\n        Styler.apply : Apply a function row-wise, column-wise, or table-wise to\n            modify the dataframe's styling.\n\n        Notes\n        -----\n        Like :meth:`DataFrame.pipe`, this method can simplify the\n        application of several user-defined functions to a styler.  Instead\n        of writing:\n\n        .. code-block:: python\n\n            f(g(df.style.set_precision(3), arg1=a), arg2=b, arg3=c)\n\n        users can write:\n\n        .. code-block:: python\n\n            (df.style.set_precision(3)\n               .pipe(g, arg1=a)\n               .pipe(f, arg2=b, arg3=c))\n\n        In particular, this allows users to define functions that take a\n        styler object, along with other parameters, and return the styler after\n        making styling changes (such as calling :meth:`Styler.apply` or\n        :meth:`Styler.set_properties`).  Using ``.pipe``, these user-defined\n        style \"transformations\" can be interleaved with calls to the built-in\n        Styler interface.\n\n        Examples\n        --------\n        >>> def format_conversion(styler):\n        ...     return (styler.set_properties(**{'text-align': 'right'})\n        ...                   .format({'conversion': '{:.1%}'}))\n\n        The user-defined ``format_conversion`` function above can be called\n        within a sequence of other style modifications:\n\n        >>> df = pd.DataFrame({'trial': list(range(5)),\n        ...                    'conversion': [0.75, 0.85, np.nan, 0.7, 0.72]})\n        >>> (df.style\n        ...    .highlight_min(subset=['conversion'], color='yellow')\n        ...    .pipe(format_conversion)\n        ...    .set_caption(\"Results with minimum conversion highlighted.\"))\n        \"\"\"\n        return com.pipe(self, func, *args, **kwargs)\n\n\ndef _is_visible(idx_row, idx_col, lengths) -> bool:\n    \"\"\"\n    Index -> {(idx_row, idx_col): bool}).\n    \"\"\"\n    return (idx_col, idx_row) in lengths\n\n\ndef _get_level_lengths(index, hidden_elements=None):\n    \"\"\"\n    Given an index, find the level length for each element.\n\n    Optional argument is a list of index positions which\n    should not be visible.\n\n    Result is a dictionary of (level, initial_position): span\n    \"\"\"\n    if isinstance(index, pd.MultiIndex):\n        levels = index.format(sparsify=lib.no_default, adjoin=False)\n    else:\n        levels = index.format()\n\n    if hidden_elements is None:\n        hidden_elements = []\n\n    lengths = {}\n    if index.nlevels == 1:\n        for i, value in enumerate(levels):\n            if i not in hidden_elements:\n                lengths[(0, i)] = 1\n        return lengths\n\n    for i, lvl in enumerate(levels):\n        for j, row in enumerate(lvl):\n            if not get_option(\"display.multi_sparse\"):\n                lengths[(i, j)] = 1\n            elif (row is not lib.no_default) and (j not in hidden_elements):\n                last_label = j\n                lengths[(i, last_label)] = 1\n            elif row is not lib.no_default:\n                # even if its hidden, keep track of it in case\n                # length >1 and later elements are visible\n                last_label = j\n                lengths[(i, last_label)] = 0\n            elif j not in hidden_elements:\n                lengths[(i, last_label)] += 1\n\n    non_zero_lengths = {\n        element: length for element, length in lengths.items() if length >= 1\n    }\n\n    return non_zero_lengths\n\n\ndef _maybe_wrap_formatter(\n    formatter: Union[Callable, str], na_rep: Optional[str]\n) -> Callable:\n    if isinstance(formatter, str):\n        formatter_func = lambda x: formatter.format(x)\n    elif callable(formatter):\n        formatter_func = formatter\n    else:\n        msg = f\"Expected a template string or callable, got {formatter} instead\"\n        raise TypeError(msg)\n\n    if na_rep is None:\n        return formatter_func\n    elif isinstance(na_rep, str):\n        return lambda x: na_rep if pd.isna(x) else formatter_func(x)\n    else:\n        msg = f\"Expected a string, got {na_rep} instead\"\n        raise TypeError(msg)\n"
    }
  ]
}