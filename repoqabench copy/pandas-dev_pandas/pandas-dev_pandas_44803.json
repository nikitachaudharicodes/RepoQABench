{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "44803",
  "issue_description": "# BUG: `groupby.apply()` inconsistently indexes its return value with the groupby-cols\n\n### \r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the master branch of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(dict(\r\n    name=[\"Alice\", \"Bob\", \"Carl\", \"Dan\", \"Eve\"],\r\n    age=[20, 21, 20, 21, 20],\r\n    height=[165, 175, 189, 182, 157],\r\n    weight=[65, 72, 95, 83, 58],\r\n)).set_index(\"name\")\r\n\r\nindexed_by_age = df.groupby([\"age\"]).apply(lambda group: group.copy())\r\nprint(indexed_by_age)\r\n\r\nprint()\r\nprint(\"=\" * 30)\r\nprint()\r\n\r\nindexed_only_by_name = df.groupby([\"age\"]).apply(lambda group: group)\r\nprint(indexed_only_by_name)\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nThe above snippet produces the following output:\r\n\r\n```\r\n           age  height  weight\r\nage name\r\n20  Alice   20     165      65\r\n    Carl    20     189      95\r\n    Eve     20     157      58\r\n21  Bob     21     175      72\r\n    Dan     21     182      83\r\n\r\n==============================\r\n\r\n       age  height  weight\r\nname\r\nAlice   20     165      65\r\nBob     21     175      72\r\nCarl    20     189      95\r\nDan     21     182      83\r\nEve     20     157      58\r\n```\r\n\r\nWhere we can see that despite the return values of the `apply-func` being semantically identical (`group` vs. `group.copy()`), one ends up being indexed with `age` and `name` while the other only gets `name`.\r\n\r\n### Expected Behavior\r\n\r\nI would have expected the two returned dataframes to be the same. I have a personal preference for the first option, with the `age` appearing in the index, but I think I would be able to work with both.\r\n\r\n-----\r\n\r\nI know other inconsistencies have been reported:\r\n- #22545\r\n- #22546\r\n\r\nBut IMHO never as clearly as this. I'm hoping this reproducer helps diagnose the issue more easily.\r\nInterestingly, the difference only creeps up when `grouping_keys` is set to `True`. Otherwise, we get the second output twice.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : 945c9ed766a61c7d2c0a7cbb251b6edebf9cb7d5\r\npython           : 3.9.9.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 5.15.6-arch2-1\r\nVersion          : #1 SMP PREEMPT Thu, 02 Dec 2021 15:47:09 +0000\r\nmachine          : x86_64\r\nprocessor        :\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 1.3.4\r\nnumpy            : 1.21.4\r\npytz             : 2021.3\r\ndateutil         : 2.8.2\r\npip              : 21.3.1\r\nsetuptools       : 58.5.3\r\nCython           : None\r\npytest           : None\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : None\r\nIPython          : None\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfsspec           : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nmatplotlib       : None\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : None\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : None\r\nsqlalchemy       : None\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nnumba            : None\r\n\r\n</details>",
  "issue_comments": [
    {
      "id": 997407220,
      "user": "v-liuwei",
      "body": "I just met the same problem, and did some test on the latest released version v1.3.5. \r\n\r\nIt seems that the `fast_apply` set the `mutated` to `True` in the `.copy()` case while set it to `False` in the other case. \r\n\r\nhttps://github.com/pandas-dev/pandas/blob/66e3805b8cabe977f40c05259cc3fcf7ead5687d/pandas/core/groupby/ops.py#L813-L820\r\n\r\nAnd the truth value of `mutated` (the same as `not_indexed_same` here)will affect the concatenation of applied results:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/66e3805b8cabe977f40c05259cc3fcf7ead5687d/pandas/core/groupby/groupby.py#L1003-L1067\r\n\r\nI tried to disable the `fast_apply` （manually raise `IndexError` before it）, then it went to the slow apply in python code:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/66e3805b8cabe977f40c05259cc3fcf7ead5687d/pandas/core/groupby/ops.py#L847-L855\r\n\r\nand the value of `mutated` and the printed results of the two cases became consistent. \r\n\r\n### Summary\r\nI think there is a bug that **the mutate checking of the fast apply**(supported by `apply_frame_axis0` from _libs.reduction.pyx) **and the slow apply are inconsistent.** "
    },
    {
      "id": 997862795,
      "user": "ODemidenko",
      "body": "I encounter a similar issue. \r\nIn this case  whether group columns are added to the index depends on the amount fo rows returned by apply:\r\nwhen it changes - group columns are added to the index. When it doesn't change - group columns aren't added.\r\nThe problem is, that the amount of rows may or may not change after applying exactly same code.\r\n```\r\nfull_time_index = pd.Index(\r\n        pd.to_datetime(['2020-01-01', '2020-01-03', '2020-01-10', '2020-01-15']),\r\n        name='timestamp'\r\n    )\r\ngroup_cols = ['B']\r\ndf = pd.DataFrame({'A': [3, 1],\r\n                   'B': ['foo', 'foo'],\r\n                   'timestamp': full_time_index[::2]\r\n                   }).set_index('timestamp')\r\n\r\ndef reindex_by_group(df, group_cols=group_cols, new_index=full_time_index):\r\n    result = df.groupby(group_cols).apply(lambda d: d.reindex(new_index, fill_value=0)).drop(columns=group_cols)\r\n    print(result.index.names)\r\n    assert set(result.index.names) == set(['timestamp']+group_cols)\r\n\r\n#this adds group column to index (desired behavior):\r\nresult = reindex_by_group(df, new_index=full_time_index)\r\nresult = reindex_by_group(df, new_index=full_time_index[:1])\r\nresult = reindex_by_group(df.iloc[:1], new_index=full_time_index)\r\n\r\n#this fail:\r\nresult = reindex_by_group(df.iloc[:1], new_index=full_time_index[:1])\r\nresult = reindex_by_group(df, new_index=full_time_index[::2])\r\n```\r\nIMO, the most intuitive behavior is always adding group columns to index. The fact that 'transform' operations don't do it - was a major source of confustion for me and my colleagues.\r\nAnd in the case above, in would be much more difficult to implement groupby with reindex, unless group columns are added to the index.\r\n"
    },
    {
      "id": 1001740400,
      "user": "mroeschke",
      "body": "This is consistent on the master branch now (which will be reflected in the 1.4 release). This was probably most likely made consistent by https://github.com/pandas-dev/pandas/pull/42992\r\n\r\n```\r\nIn [1]: import pandas as pd\r\n   ...:\r\n   ...: df = pd.DataFrame(dict(\r\n   ...:     name=[\"Alice\", \"Bob\", \"Carl\", \"Dan\", \"Eve\"],\r\n   ...:     age=[20, 21, 20, 21, 20],\r\n   ...:     height=[165, 175, 189, 182, 157],\r\n   ...:     weight=[65, 72, 95, 83, 58],\r\n   ...: )).set_index(\"name\")\r\n   ...:\r\n   ...: indexed_by_age = df.groupby([\"age\"]).apply(lambda group: group.copy())\r\n   ...: print(indexed_by_age)\r\n   ...:\r\n   ...: print()\r\n   ...: print(\"=\" * 30)\r\n   ...: print()\r\n   ...:\r\n   ...: indexed_only_by_name = df.groupby([\"age\"]).apply(lambda group: group)\r\n   ...: print(indexed_only_by_name)\r\n       age  height  weight\r\nname\r\nAlice   20     165      65\r\nBob     21     175      72\r\nCarl    20     189      95\r\nDan     21     182      83\r\nEve     20     157      58\r\n\r\n==============================\r\n\r\n       age  height  weight\r\nname\r\nAlice   20     165      65\r\nBob     21     175      72\r\nCarl    20     189      95\r\nDan     21     182      83\r\nEve     20     157      58\r\n```\r\n\r\nSuppose could use a unit test to prevent a regression."
    },
    {
      "id": 1001769560,
      "user": "aragard",
      "body": "take"
    },
    {
      "id": 1008310835,
      "user": "pratimugale",
      "body": "May I work on this?\r\n"
    },
    {
      "id": 1013957696,
      "user": "aragard",
      "body": "Yes!"
    },
    {
      "id": 1015845512,
      "user": "NumberPiOso",
      "body": "@pratimugale  are you working on this? If not, I would like to take it"
    },
    {
      "id": 1016047276,
      "user": "pratimugale",
      "body": "@NumberPiOso you can take it"
    },
    {
      "id": 1017847720,
      "user": "NumberPiOso",
      "body": "take"
    },
    {
      "id": 1017850959,
      "user": "NumberPiOso",
      "body": "I did not understand if the output shown is the expected behaviour (#45476) . However, I will do the test to keep outputs similar, regardless of the results\r\n\r\n> This is consistent on the master branch now (which will be reflected in the 1.4 release). This was probably most likely made consistent by #42992\r\n> \r\n> ```\r\n> In [1]: import pandas as pd\r\n>    ...:\r\n>    ...: df = pd.DataFrame(dict(\r\n>    ...:     name=[\"Alice\", \"Bob\", \"Carl\", \"Dan\", \"Eve\"],\r\n>    ...:     age=[20, 21, 20, 21, 20],\r\n>    ...:     height=[165, 175, 189, 182, 157],\r\n>    ...:     weight=[65, 72, 95, 83, 58],\r\n>    ...: )).set_index(\"name\")\r\n>    ...:\r\n>    ...: indexed_by_age = df.groupby([\"age\"]).apply(lambda group: group.copy())\r\n>    ...: print(indexed_by_age)\r\n>    ...:\r\n>    ...: print()\r\n>    ...: print(\"=\" * 30)\r\n>    ...: print()\r\n>    ...:\r\n>    ...: indexed_only_by_name = df.groupby([\"age\"]).apply(lambda group: group)\r\n>    ...: print(indexed_only_by_name)\r\n>        age  height  weight\r\n> name\r\n> Alice   20     165      65\r\n> Bob     21     175      72\r\n> Carl    20     189      95\r\n> Dan     21     182      83\r\n> Eve     20     157      58\r\n> \r\n> ==============================\r\n> \r\n>        age  height  weight\r\n> name\r\n> Alice   20     165      65\r\n> Bob     21     175      72\r\n> Carl    20     189      95\r\n> Dan     21     182      83\r\n> Eve     20     157      58\r\n> ```\r\n> \r\n> Suppose could use a unit test to prevent a regression.\r\n\r\n"
    }
  ],
  "text_context": "# BUG: `groupby.apply()` inconsistently indexes its return value with the groupby-cols\n\n### \r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the master branch of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(dict(\r\n    name=[\"Alice\", \"Bob\", \"Carl\", \"Dan\", \"Eve\"],\r\n    age=[20, 21, 20, 21, 20],\r\n    height=[165, 175, 189, 182, 157],\r\n    weight=[65, 72, 95, 83, 58],\r\n)).set_index(\"name\")\r\n\r\nindexed_by_age = df.groupby([\"age\"]).apply(lambda group: group.copy())\r\nprint(indexed_by_age)\r\n\r\nprint()\r\nprint(\"=\" * 30)\r\nprint()\r\n\r\nindexed_only_by_name = df.groupby([\"age\"]).apply(lambda group: group)\r\nprint(indexed_only_by_name)\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nThe above snippet produces the following output:\r\n\r\n```\r\n           age  height  weight\r\nage name\r\n20  Alice   20     165      65\r\n    Carl    20     189      95\r\n    Eve     20     157      58\r\n21  Bob     21     175      72\r\n    Dan     21     182      83\r\n\r\n==============================\r\n\r\n       age  height  weight\r\nname\r\nAlice   20     165      65\r\nBob     21     175      72\r\nCarl    20     189      95\r\nDan     21     182      83\r\nEve     20     157      58\r\n```\r\n\r\nWhere we can see that despite the return values of the `apply-func` being semantically identical (`group` vs. `group.copy()`), one ends up being indexed with `age` and `name` while the other only gets `name`.\r\n\r\n### Expected Behavior\r\n\r\nI would have expected the two returned dataframes to be the same. I have a personal preference for the first option, with the `age` appearing in the index, but I think I would be able to work with both.\r\n\r\n-----\r\n\r\nI know other inconsistencies have been reported:\r\n- #22545\r\n- #22546\r\n\r\nBut IMHO never as clearly as this. I'm hoping this reproducer helps diagnose the issue more easily.\r\nInterestingly, the difference only creeps up when `grouping_keys` is set to `True`. Otherwise, we get the second output twice.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : 945c9ed766a61c7d2c0a7cbb251b6edebf9cb7d5\r\npython           : 3.9.9.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 5.15.6-arch2-1\r\nVersion          : #1 SMP PREEMPT Thu, 02 Dec 2021 15:47:09 +0000\r\nmachine          : x86_64\r\nprocessor        :\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 1.3.4\r\nnumpy            : 1.21.4\r\npytz             : 2021.3\r\ndateutil         : 2.8.2\r\npip              : 21.3.1\r\nsetuptools       : 58.5.3\r\nCython           : None\r\npytest           : None\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : None\r\nIPython          : None\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfsspec           : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nmatplotlib       : None\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : None\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : None\r\nsqlalchemy       : None\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nnumba            : None\r\n\r\n</details>\n\nI just met the same problem, and did some test on the latest released version v1.3.5. \r\n\r\nIt seems that the `fast_apply` set the `mutated` to `True` in the `.copy()` case while set it to `False` in the other case. \r\n\r\nhttps://github.com/pandas-dev/pandas/blob/66e3805b8cabe977f40c05259cc3fcf7ead5687d/pandas/core/groupby/ops.py#L813-L820\r\n\r\nAnd the truth value of `mutated` (the same as `not_indexed_same` here)will affect the concatenation of applied results:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/66e3805b8cabe977f40c05259cc3fcf7ead5687d/pandas/core/groupby/groupby.py#L1003-L1067\r\n\r\nI tried to disable the `fast_apply` （manually raise `IndexError` before it）, then it went to the slow apply in python code:\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/66e3805b8cabe977f40c05259cc3fcf7ead5687d/pandas/core/groupby/ops.py#L847-L855\r\n\r\nand the value of `mutated` and the printed results of the two cases became consistent. \r\n\r\n### Summary\r\nI think there is a bug that **the mutate checking of the fast apply**(supported by `apply_frame_axis0` from _libs.reduction.pyx) **and the slow apply are inconsistent.** \n\nI encounter a similar issue. \r\nIn this case  whether group columns are added to the index depends on the amount fo rows returned by apply:\r\nwhen it changes - group columns are added to the index. When it doesn't change - group columns aren't added.\r\nThe problem is, that the amount of rows may or may not change after applying exactly same code.\r\n```\r\nfull_time_index = pd.Index(\r\n        pd.to_datetime(['2020-01-01', '2020-01-03', '2020-01-10', '2020-01-15']),\r\n        name='timestamp'\r\n    )\r\ngroup_cols = ['B']\r\ndf = pd.DataFrame({'A': [3, 1],\r\n                   'B': ['foo', 'foo'],\r\n                   'timestamp': full_time_index[::2]\r\n                   }).set_index('timestamp')\r\n\r\ndef reindex_by_group(df, group_cols=group_cols, new_index=full_time_index):\r\n    result = df.groupby(group_cols).apply(lambda d: d.reindex(new_index, fill_value=0)).drop(columns=group_cols)\r\n    print(result.index.names)\r\n    assert set(result.index.names) == set(['timestamp']+group_cols)\r\n\r\n#this adds group column to index (desired behavior):\r\nresult = reindex_by_group(df, new_index=full_time_index)\r\nresult = reindex_by_group(df, new_index=full_time_index[:1])\r\nresult = reindex_by_group(df.iloc[:1], new_index=full_time_index)\r\n\r\n#this fail:\r\nresult = reindex_by_group(df.iloc[:1], new_index=full_time_index[:1])\r\nresult = reindex_by_group(df, new_index=full_time_index[::2])\r\n```\r\nIMO, the most intuitive behavior is always adding group columns to index. The fact that 'transform' operations don't do it - was a major source of confustion for me and my colleagues.\r\nAnd in the case above, in would be much more difficult to implement groupby with reindex, unless group columns are added to the index.\r\n\n\nThis is consistent on the master branch now (which will be reflected in the 1.4 release). This was probably most likely made consistent by https://github.com/pandas-dev/pandas/pull/42992\r\n\r\n```\r\nIn [1]: import pandas as pd\r\n   ...:\r\n   ...: df = pd.DataFrame(dict(\r\n   ...:     name=[\"Alice\", \"Bob\", \"Carl\", \"Dan\", \"Eve\"],\r\n   ...:     age=[20, 21, 20, 21, 20],\r\n   ...:     height=[165, 175, 189, 182, 157],\r\n   ...:     weight=[65, 72, 95, 83, 58],\r\n   ...: )).set_index(\"name\")\r\n   ...:\r\n   ...: indexed_by_age = df.groupby([\"age\"]).apply(lambda group: group.copy())\r\n   ...: print(indexed_by_age)\r\n   ...:\r\n   ...: print()\r\n   ...: print(\"=\" * 30)\r\n   ...: print()\r\n   ...:\r\n   ...: indexed_only_by_name = df.groupby([\"age\"]).apply(lambda group: group)\r\n   ...: print(indexed_only_by_name)\r\n       age  height  weight\r\nname\r\nAlice   20     165      65\r\nBob     21     175      72\r\nCarl    20     189      95\r\nDan     21     182      83\r\nEve     20     157      58\r\n\r\n==============================\r\n\r\n       age  height  weight\r\nname\r\nAlice   20     165      65\r\nBob     21     175      72\r\nCarl    20     189      95\r\nDan     21     182      83\r\nEve     20     157      58\r\n```\r\n\r\nSuppose could use a unit test to prevent a regression.\n\ntake\n\nMay I work on this?\r\n\n\nYes!\n\n@pratimugale  are you working on this? If not, I would like to take it\n\n@NumberPiOso you can take it\n\ntake\n\nI did not understand if the output shown is the expected behaviour (#45476) . However, I will do the test to keep outputs similar, regardless of the results\r\n\r\n> This is consistent on the master branch now (which will be reflected in the 1.4 release). This was probably most likely made consistent by #42992\r\n> \r\n> ```\r\n> In [1]: import pandas as pd\r\n>    ...:\r\n>    ...: df = pd.DataFrame(dict(\r\n>    ...:     name=[\"Alice\", \"Bob\", \"Carl\", \"Dan\", \"Eve\"],\r\n>    ...:     age=[20, 21, 20, 21, 20],\r\n>    ...:     height=[165, 175, 189, 182, 157],\r\n>    ...:     weight=[65, 72, 95, 83, 58],\r\n>    ...: )).set_index(\"name\")\r\n>    ...:\r\n>    ...: indexed_by_age = df.groupby([\"age\"]).apply(lambda group: group.copy())\r\n>    ...: print(indexed_by_age)\r\n>    ...:\r\n>    ...: print()\r\n>    ...: print(\"=\" * 30)\r\n>    ...: print()\r\n>    ...:\r\n>    ...: indexed_only_by_name = df.groupby([\"age\"]).apply(lambda group: group)\r\n>    ...: print(indexed_only_by_name)\r\n>        age  height  weight\r\n> name\r\n> Alice   20     165      65\r\n> Bob     21     175      72\r\n> Carl    20     189      95\r\n> Dan     21     182      83\r\n> Eve     20     157      58\r\n> \r\n> ==============================\r\n> \r\n>        age  height  weight\r\n> name\r\n> Alice   20     165      65\r\n> Bob     21     175      72\r\n> Carl    20     189      95\r\n> Dan     21     182      83\r\n> Eve     20     157      58\r\n> ```\r\n> \r\n> Suppose could use a unit test to prevent a regression.\r\n\r\n",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/42992",
  "code_context": [
    {
      "filename": "pandas/core/groupby/ops.py",
      "content": "\"\"\"\nProvide classes to perform the groupby aggregate operations.\n\nThese are not exposed to the user and provide implementations of the grouping\noperations, primarily in cython. These classes (BaseGrouper and BinGrouper)\nare contained *in* the SeriesGroupBy and DataFrameGroupBy objects.\n\"\"\"\nfrom __future__ import annotations\n\nimport collections\nimport functools\nfrom typing import (\n    Generic,\n    Hashable,\n    Iterator,\n    Sequence,\n    final,\n    overload,\n)\n\nimport numpy as np\n\nfrom pandas._libs import (\n    NaT,\n    lib,\n)\nimport pandas._libs.groupby as libgroupby\nimport pandas._libs.reduction as libreduction\nfrom pandas._typing import (\n    ArrayLike,\n    DtypeObj,\n    F,\n    FrameOrSeries,\n    Shape,\n    npt,\n)\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import cache_readonly\n\nfrom pandas.core.dtypes.cast import (\n    maybe_cast_pointwise_result,\n    maybe_downcast_to_dtype,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_float64,\n    ensure_int64,\n    ensure_platform_int,\n    is_1d_only_ea_obj,\n    is_bool_dtype,\n    is_categorical_dtype,\n    is_complex_dtype,\n    is_datetime64_any_dtype,\n    is_float_dtype,\n    is_integer_dtype,\n    is_numeric_dtype,\n    is_sparse,\n    is_timedelta64_dtype,\n    needs_i8_conversion,\n)\nfrom pandas.core.dtypes.dtypes import ExtensionDtype\nfrom pandas.core.dtypes.missing import (\n    isna,\n    maybe_fill,\n)\n\nfrom pandas.core.arrays import (\n    DatetimeArray,\n    ExtensionArray,\n    PeriodArray,\n    TimedeltaArray,\n)\nfrom pandas.core.arrays.boolean import BooleanDtype\nfrom pandas.core.arrays.floating import (\n    Float64Dtype,\n    FloatingDtype,\n)\nfrom pandas.core.arrays.integer import (\n    Int64Dtype,\n    _IntegerDtype,\n)\nfrom pandas.core.arrays.masked import (\n    BaseMaskedArray,\n    BaseMaskedDtype,\n)\nimport pandas.core.common as com\nfrom pandas.core.frame import DataFrame\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.groupby import grouper\nfrom pandas.core.indexes.api import (\n    CategoricalIndex,\n    Index,\n    MultiIndex,\n    ensure_index,\n)\nfrom pandas.core.series import Series\nfrom pandas.core.sorting import (\n    compress_group_index,\n    decons_obs_group_ids,\n    get_flattened_list,\n    get_group_index,\n    get_group_index_sorter,\n    get_indexer_dict,\n)\n\n\nclass WrappedCythonOp:\n    \"\"\"\n    Dispatch logic for functions defined in _libs.groupby\n    \"\"\"\n\n    # Functions for which we do _not_ attempt to cast the cython result\n    #  back to the original dtype.\n    cast_blocklist = frozenset([\"rank\", \"count\", \"size\", \"idxmin\", \"idxmax\"])\n\n    def __init__(self, kind: str, how: str):\n        self.kind = kind\n        self.how = how\n\n    _CYTHON_FUNCTIONS = {\n        \"aggregate\": {\n            \"add\": \"group_add\",\n            \"prod\": \"group_prod\",\n            \"min\": \"group_min\",\n            \"max\": \"group_max\",\n            \"mean\": \"group_mean\",\n            \"median\": \"group_median\",\n            \"var\": \"group_var\",\n            \"first\": \"group_nth\",\n            \"last\": \"group_last\",\n            \"ohlc\": \"group_ohlc\",\n        },\n        \"transform\": {\n            \"cumprod\": \"group_cumprod\",\n            \"cumsum\": \"group_cumsum\",\n            \"cummin\": \"group_cummin\",\n            \"cummax\": \"group_cummax\",\n            \"rank\": \"group_rank\",\n        },\n    }\n\n    _MASKED_CYTHON_FUNCTIONS = {\"cummin\", \"cummax\"}\n\n    _cython_arity = {\"ohlc\": 4}  # OHLC\n\n    # Note: we make this a classmethod and pass kind+how so that caching\n    #  works at the class level and not the instance level\n    @classmethod\n    @functools.lru_cache(maxsize=None)\n    def _get_cython_function(\n        cls, kind: str, how: str, dtype: np.dtype, is_numeric: bool\n    ):\n\n        dtype_str = dtype.name\n        ftype = cls._CYTHON_FUNCTIONS[kind][how]\n\n        # see if there is a fused-type version of function\n        # only valid for numeric\n        f = getattr(libgroupby, ftype)\n        if is_numeric:\n            return f\n        # error: Non-overlapping equality check (left operand type: \"dtype[Any]\", right\n        # operand type: \"Literal['object']\")\n        elif dtype == object:  # type: ignore[comparison-overlap]\n            if \"object\" not in f.__signatures__:\n                # raise NotImplementedError here rather than TypeError later\n                raise NotImplementedError(\n                    f\"function is not implemented for this dtype: \"\n                    f\"[how->{how},dtype->{dtype_str}]\"\n                )\n            return f\n\n    def get_cython_func_and_vals(self, values: np.ndarray, is_numeric: bool):\n        \"\"\"\n        Find the appropriate cython function, casting if necessary.\n\n        Parameters\n        ----------\n        values : np.ndarray\n        is_numeric : bool\n\n        Returns\n        -------\n        func : callable\n        values : np.ndarray\n        \"\"\"\n        how = self.how\n        kind = self.kind\n\n        if how in [\"median\", \"cumprod\"]:\n            # these two only have float64 implementations\n            if is_numeric:\n                values = ensure_float64(values)\n            else:\n                raise NotImplementedError(\n                    f\"function is not implemented for this dtype: \"\n                    f\"[how->{how},dtype->{values.dtype.name}]\"\n                )\n            func = getattr(libgroupby, f\"group_{how}_float64\")\n            return func, values\n\n        func = self._get_cython_function(kind, how, values.dtype, is_numeric)\n\n        if values.dtype.kind in [\"i\", \"u\"]:\n            if how in [\"add\", \"var\", \"prod\", \"mean\", \"ohlc\"]:\n                # result may still include NaN, so we have to cast\n                values = ensure_float64(values)\n\n        return func, values\n\n    def _disallow_invalid_ops(self, dtype: DtypeObj, is_numeric: bool = False):\n        \"\"\"\n        Check if we can do this operation with our cython functions.\n\n        Raises\n        ------\n        NotImplementedError\n            This is either not a valid function for this dtype, or\n            valid but not implemented in cython.\n        \"\"\"\n        how = self.how\n\n        if is_numeric:\n            # never an invalid op for those dtypes, so return early as fastpath\n            return\n\n        if is_categorical_dtype(dtype):\n            # NotImplementedError for methods that can fall back to a\n            #  non-cython implementation.\n            if how in [\"add\", \"prod\", \"cumsum\", \"cumprod\"]:\n                raise TypeError(f\"{dtype} type does not support {how} operations\")\n            raise NotImplementedError(f\"{dtype} dtype not supported\")\n\n        elif is_sparse(dtype):\n            # categoricals are only 1d, so we\n            #  are not setup for dim transforming\n            raise NotImplementedError(f\"{dtype} dtype not supported\")\n        elif is_datetime64_any_dtype(dtype):\n            # we raise NotImplemented if this is an invalid operation\n            #  entirely, e.g. adding datetimes\n            if how in [\"add\", \"prod\", \"cumsum\", \"cumprod\"]:\n                raise TypeError(f\"datetime64 type does not support {how} operations\")\n        elif is_timedelta64_dtype(dtype):\n            if how in [\"prod\", \"cumprod\"]:\n                raise TypeError(f\"timedelta64 type does not support {how} operations\")\n\n    def _get_output_shape(self, ngroups: int, values: np.ndarray) -> Shape:\n        how = self.how\n        kind = self.kind\n\n        arity = self._cython_arity.get(how, 1)\n\n        out_shape: Shape\n        if how == \"ohlc\":\n            out_shape = (ngroups, 4)\n        elif arity > 1:\n            raise NotImplementedError(\n                \"arity of more than 1 is not supported for the 'how' argument\"\n            )\n        elif kind == \"transform\":\n            out_shape = values.shape\n        else:\n            out_shape = (ngroups,) + values.shape[1:]\n        return out_shape\n\n    def get_out_dtype(self, dtype: np.dtype) -> np.dtype:\n        how = self.how\n\n        if how == \"rank\":\n            out_dtype = \"float64\"\n        else:\n            if is_numeric_dtype(dtype):\n                out_dtype = f\"{dtype.kind}{dtype.itemsize}\"\n            else:\n                out_dtype = \"object\"\n        return np.dtype(out_dtype)\n\n    @overload\n    def _get_result_dtype(self, dtype: np.dtype) -> np.dtype:\n        ...  # pragma: no cover\n\n    @overload\n    def _get_result_dtype(self, dtype: ExtensionDtype) -> ExtensionDtype:\n        ...  # pragma: no cover\n\n    def _get_result_dtype(self, dtype: DtypeObj) -> DtypeObj:\n        \"\"\"\n        Get the desired dtype of a result based on the\n        input dtype and how it was computed.\n\n        Parameters\n        ----------\n        dtype : np.dtype or ExtensionDtype\n            Input dtype.\n\n        Returns\n        -------\n        np.dtype or ExtensionDtype\n            The desired dtype of the result.\n        \"\"\"\n        how = self.how\n\n        if how in [\"add\", \"cumsum\", \"sum\", \"prod\"]:\n            if dtype == np.dtype(bool):\n                return np.dtype(np.int64)\n            elif isinstance(dtype, (BooleanDtype, _IntegerDtype)):\n                return Int64Dtype()\n        elif how in [\"mean\", \"median\", \"var\"]:\n            if isinstance(dtype, (BooleanDtype, _IntegerDtype)):\n                return Float64Dtype()\n            elif is_float_dtype(dtype):\n                return dtype\n            elif is_numeric_dtype(dtype):\n                return np.dtype(np.float64)\n        return dtype\n\n    def uses_mask(self) -> bool:\n        return self.how in self._MASKED_CYTHON_FUNCTIONS\n\n    @final\n    def _ea_wrap_cython_operation(\n        self,\n        values: ExtensionArray,\n        min_count: int,\n        ngroups: int,\n        comp_ids: np.ndarray,\n        **kwargs,\n    ) -> ArrayLike:\n        \"\"\"\n        If we have an ExtensionArray, unwrap, call _cython_operation, and\n        re-wrap if appropriate.\n        \"\"\"\n        # TODO: general case implementation overridable by EAs.\n        if isinstance(values, BaseMaskedArray) and self.uses_mask():\n            return self._masked_ea_wrap_cython_operation(\n                values,\n                min_count=min_count,\n                ngroups=ngroups,\n                comp_ids=comp_ids,\n                **kwargs,\n            )\n\n        if isinstance(values, (DatetimeArray, PeriodArray, TimedeltaArray)):\n            # All of the functions implemented here are ordinal, so we can\n            #  operate on the tz-naive equivalents\n            npvalues = values._ndarray.view(\"M8[ns]\")\n        elif isinstance(values.dtype, (BooleanDtype, _IntegerDtype)):\n            # IntegerArray or BooleanArray\n            npvalues = values.to_numpy(\"float64\", na_value=np.nan)\n        elif isinstance(values.dtype, FloatingDtype):\n            # FloatingArray\n            npvalues = values.to_numpy(values.dtype.numpy_dtype, na_value=np.nan)\n        else:\n            raise NotImplementedError(\n                f\"function is not implemented for this dtype: {values.dtype}\"\n            )\n\n        res_values = self._cython_op_ndim_compat(\n            npvalues,\n            min_count=min_count,\n            ngroups=ngroups,\n            comp_ids=comp_ids,\n            mask=None,\n            **kwargs,\n        )\n\n        if self.how in [\"rank\"]:\n            # i.e. how in WrappedCythonOp.cast_blocklist, since\n            #  other cast_blocklist methods dont go through cython_operation\n            return res_values\n\n        return self._reconstruct_ea_result(values, res_values)\n\n    def _reconstruct_ea_result(self, values, res_values):\n        \"\"\"\n        Construct an ExtensionArray result from an ndarray result.\n        \"\"\"\n        # TODO: allow EAs to override this logic\n\n        if isinstance(values.dtype, (BooleanDtype, _IntegerDtype, FloatingDtype)):\n            dtype = self._get_result_dtype(values.dtype)\n            cls = dtype.construct_array_type()\n            return cls._from_sequence(res_values, dtype=dtype)\n\n        elif needs_i8_conversion(values.dtype):\n            i8values = res_values.view(\"i8\")\n            return type(values)(i8values, dtype=values.dtype)\n\n        raise NotImplementedError\n\n    @final\n    def _masked_ea_wrap_cython_operation(\n        self,\n        values: BaseMaskedArray,\n        min_count: int,\n        ngroups: int,\n        comp_ids: np.ndarray,\n        **kwargs,\n    ) -> BaseMaskedArray:\n        \"\"\"\n        Equivalent of `_ea_wrap_cython_operation`, but optimized for masked EA's\n        and cython algorithms which accept a mask.\n        \"\"\"\n        orig_values = values\n\n        # Copy to ensure input and result masks don't end up shared\n        mask = values._mask.copy()\n        arr = values._data\n\n        res_values = self._cython_op_ndim_compat(\n            arr,\n            min_count=min_count,\n            ngroups=ngroups,\n            comp_ids=comp_ids,\n            mask=mask,\n            **kwargs,\n        )\n        dtype = self._get_result_dtype(orig_values.dtype)\n        assert isinstance(dtype, BaseMaskedDtype)\n        cls = dtype.construct_array_type()\n\n        return cls(res_values.astype(dtype.type, copy=False), mask)\n\n    @final\n    def _cython_op_ndim_compat(\n        self,\n        values: np.ndarray,\n        *,\n        min_count: int,\n        ngroups: int,\n        comp_ids: np.ndarray,\n        mask: np.ndarray | None,\n        **kwargs,\n    ) -> np.ndarray:\n        if values.ndim == 1:\n            # expand to 2d, dispatch, then squeeze if appropriate\n            values2d = values[None, :]\n            if mask is not None:\n                mask = mask[None, :]\n            res = self._call_cython_op(\n                values2d,\n                min_count=min_count,\n                ngroups=ngroups,\n                comp_ids=comp_ids,\n                mask=mask,\n                **kwargs,\n            )\n            if res.shape[0] == 1:\n                return res[0]\n\n            # otherwise we have OHLC\n            return res.T\n\n        return self._call_cython_op(\n            values,\n            min_count=min_count,\n            ngroups=ngroups,\n            comp_ids=comp_ids,\n            mask=mask,\n            **kwargs,\n        )\n\n    @final\n    def _call_cython_op(\n        self,\n        values: np.ndarray,  # np.ndarray[ndim=2]\n        *,\n        min_count: int,\n        ngroups: int,\n        comp_ids: np.ndarray,\n        mask: np.ndarray | None,\n        **kwargs,\n    ) -> np.ndarray:  # np.ndarray[ndim=2]\n        orig_values = values\n\n        dtype = values.dtype\n        is_numeric = is_numeric_dtype(dtype)\n\n        is_datetimelike = needs_i8_conversion(dtype)\n\n        if is_datetimelike:\n            values = values.view(\"int64\")\n            is_numeric = True\n        elif is_bool_dtype(dtype):\n            values = values.astype(\"int64\")\n        elif is_integer_dtype(dtype):\n            # e.g. uint8 -> uint64, int16 -> int64\n            dtype_str = dtype.kind + \"8\"\n            values = values.astype(dtype_str, copy=False)\n        elif is_numeric:\n            if not is_complex_dtype(dtype):\n                values = ensure_float64(values)\n\n        values = values.T\n        if mask is not None:\n            mask = mask.T\n\n        out_shape = self._get_output_shape(ngroups, values)\n        func, values = self.get_cython_func_and_vals(values, is_numeric)\n        out_dtype = self.get_out_dtype(values.dtype)\n\n        result = maybe_fill(np.empty(out_shape, dtype=out_dtype))\n        if self.kind == \"aggregate\":\n            counts = np.zeros(ngroups, dtype=np.int64)\n            if self.how in [\"min\", \"max\"]:\n                func(\n                    result,\n                    counts,\n                    values,\n                    comp_ids,\n                    min_count,\n                    is_datetimelike=is_datetimelike,\n                )\n            else:\n                func(result, counts, values, comp_ids, min_count)\n        else:\n            # TODO: min_count\n            if self.uses_mask():\n                func(\n                    result,\n                    values,\n                    comp_ids,\n                    ngroups,\n                    is_datetimelike,\n                    mask=mask,\n                    **kwargs,\n                )\n            else:\n                func(result, values, comp_ids, ngroups, is_datetimelike, **kwargs)\n\n        if self.kind == \"aggregate\":\n            # i.e. counts is defined.  Locations where count<min_count\n            # need to have the result set to np.nan, which may require casting,\n            # see GH#40767\n            if is_integer_dtype(result.dtype) and not is_datetimelike:\n                cutoff = max(1, min_count)\n                empty_groups = counts < cutoff\n                if empty_groups.any():\n                    # Note: this conversion could be lossy, see GH#40767\n                    result = result.astype(\"float64\")\n                    result[empty_groups] = np.nan\n\n        result = result.T\n\n        if self.how not in self.cast_blocklist:\n            # e.g. if we are int64 and need to restore to datetime64/timedelta64\n            # \"rank\" is the only member of cast_blocklist we get here\n            res_dtype = self._get_result_dtype(orig_values.dtype)\n            op_result = maybe_downcast_to_dtype(result, res_dtype)\n        else:\n            op_result = result\n\n        # error: Incompatible return value type (got \"Union[ExtensionArray, ndarray]\",\n        # expected \"ndarray\")\n        return op_result  # type: ignore[return-value]\n\n    @final\n    def cython_operation(\n        self,\n        *,\n        values: ArrayLike,\n        axis: int,\n        min_count: int = -1,\n        comp_ids: np.ndarray,\n        ngroups: int,\n        **kwargs,\n    ) -> ArrayLike:\n        \"\"\"\n        Call our cython function, with appropriate pre- and post- processing.\n        \"\"\"\n        if values.ndim > 2:\n            raise NotImplementedError(\"number of dimensions is currently limited to 2\")\n        elif values.ndim == 2:\n            assert axis == 1, axis\n        elif not is_1d_only_ea_obj(values):\n            # Note: it is *not* the case that axis is always 0 for 1-dim values,\n            #  as we can have 1D ExtensionArrays that we need to treat as 2D\n            assert axis == 0\n\n        dtype = values.dtype\n        is_numeric = is_numeric_dtype(dtype)\n\n        # can we do this operation with our cython functions\n        # if not raise NotImplementedError\n        self._disallow_invalid_ops(dtype, is_numeric)\n\n        if not isinstance(values, np.ndarray):\n            # i.e. ExtensionArray\n            return self._ea_wrap_cython_operation(\n                values,\n                min_count=min_count,\n                ngroups=ngroups,\n                comp_ids=comp_ids,\n                **kwargs,\n            )\n\n        return self._cython_op_ndim_compat(\n            values,\n            min_count=min_count,\n            ngroups=ngroups,\n            comp_ids=comp_ids,\n            mask=None,\n            **kwargs,\n        )\n\n\nclass BaseGrouper:\n    \"\"\"\n    This is an internal Grouper class, which actually holds\n    the generated groups\n\n    Parameters\n    ----------\n    axis : Index\n    groupings : Sequence[Grouping]\n        all the grouping instances to handle in this grouper\n        for example for grouper list to groupby, need to pass the list\n    sort : bool, default True\n        whether this grouper will give sorted result or not\n    group_keys : bool, default True\n    mutated : bool, default False\n    indexer : np.ndarray[np.intp], optional\n        the indexer created by Grouper\n        some groupers (TimeGrouper) will sort its axis and its\n        group_info is also sorted, so need the indexer to reorder\n\n    \"\"\"\n\n    axis: Index\n\n    def __init__(\n        self,\n        axis: Index,\n        groupings: Sequence[grouper.Grouping],\n        sort: bool = True,\n        group_keys: bool = True,\n        mutated: bool = False,\n        indexer: npt.NDArray[np.intp] | None = None,\n        dropna: bool = True,\n    ):\n        assert isinstance(axis, Index), axis\n\n        self.axis = axis\n        self._groupings: list[grouper.Grouping] = list(groupings)\n        self._sort = sort\n        self.group_keys = group_keys\n        self.mutated = mutated\n        self.indexer = indexer\n        self.dropna = dropna\n\n    @property\n    def groupings(self) -> list[grouper.Grouping]:\n        return self._groupings\n\n    @property\n    def shape(self) -> Shape:\n        return tuple(ping.ngroups for ping in self.groupings)\n\n    def __iter__(self):\n        return iter(self.indices)\n\n    @property\n    def nkeys(self) -> int:\n        return len(self.groupings)\n\n    def get_iterator(\n        self, data: FrameOrSeries, axis: int = 0\n    ) -> Iterator[tuple[Hashable, FrameOrSeries]]:\n        \"\"\"\n        Groupby iterator\n\n        Returns\n        -------\n        Generator yielding sequence of (name, subsetted object)\n        for each group\n        \"\"\"\n        splitter = self._get_splitter(data, axis=axis)\n        keys = self._get_group_keys()\n        for key, group in zip(keys, splitter):\n            yield key, group.__finalize__(data, method=\"groupby\")\n\n    @final\n    def _get_splitter(self, data: FrameOrSeries, axis: int = 0) -> DataSplitter:\n        \"\"\"\n        Returns\n        -------\n        Generator yielding subsetted objects\n\n        __finalize__ has not been called for the subsetted objects returned.\n        \"\"\"\n        ids, _, ngroups = self.group_info\n        return get_splitter(data, ids, ngroups, axis=axis)\n\n    def _get_grouper(self):\n        \"\"\"\n        We are a grouper as part of another's groupings.\n\n        We have a specific method of grouping, so cannot\n        convert to a Index for our grouper.\n        \"\"\"\n        return self.groupings[0].grouping_vector\n\n    @final\n    def _get_group_keys(self):\n        if len(self.groupings) == 1:\n            return self.levels[0]\n        else:\n            ids, _, ngroups = self.group_info\n\n            # provide \"flattened\" iterator for multi-group setting\n            return get_flattened_list(ids, ngroups, self.levels, self.codes)\n\n    @final\n    def apply(self, f: F, data: FrameOrSeries, axis: int = 0):\n        mutated = self.mutated\n        splitter = self._get_splitter(data, axis=axis)\n        group_keys = self._get_group_keys()\n        result_values = []\n\n        # This calls DataSplitter.__iter__\n        zipped = zip(group_keys, splitter)\n\n        for key, group in zipped:\n            object.__setattr__(group, \"name\", key)\n\n            # group might be modified\n            group_axes = group.axes\n            res = f(group)\n            if not _is_indexed_like(res, group_axes, axis):\n                mutated = True\n            result_values.append(res)\n\n        return group_keys, result_values, mutated\n\n    @cache_readonly\n    def indices(self):\n        \"\"\"dict {group name -> group indices}\"\"\"\n        if len(self.groupings) == 1 and isinstance(self.result_index, CategoricalIndex):\n            # This shows unused categories in indices GH#38642\n            return self.groupings[0].indices\n        codes_list = [ping.codes for ping in self.groupings]\n        keys = [ping.group_index for ping in self.groupings]\n        return get_indexer_dict(codes_list, keys)\n\n    @property\n    def codes(self) -> list[np.ndarray]:\n        return [ping.codes for ping in self.groupings]\n\n    @property\n    def levels(self) -> list[Index]:\n        return [ping.group_index for ping in self.groupings]\n\n    @property\n    def names(self) -> list[Hashable]:\n        return [ping.name for ping in self.groupings]\n\n    @final\n    def size(self) -> Series:\n        \"\"\"\n        Compute group sizes.\n        \"\"\"\n        ids, _, ngroups = self.group_info\n        if ngroups:\n            out = np.bincount(ids[ids != -1], minlength=ngroups)\n        else:\n            out = []\n        return Series(out, index=self.result_index, dtype=\"int64\")\n\n    @cache_readonly\n    def groups(self) -> dict[Hashable, np.ndarray]:\n        \"\"\"dict {group name -> group labels}\"\"\"\n        if len(self.groupings) == 1:\n            return self.groupings[0].groups\n        else:\n            to_groupby = zip(*(ping.grouping_vector for ping in self.groupings))\n            index = Index(to_groupby)\n            return self.axis.groupby(index)\n\n    @final\n    @cache_readonly\n    def is_monotonic(self) -> bool:\n        # return if my group orderings are monotonic\n        return Index(self.group_info[0]).is_monotonic\n\n    @cache_readonly\n    def group_info(self):\n        comp_ids, obs_group_ids = self._get_compressed_codes()\n\n        ngroups = len(obs_group_ids)\n        comp_ids = ensure_platform_int(comp_ids)\n\n        return comp_ids, obs_group_ids, ngroups\n\n    @final\n    @cache_readonly\n    def codes_info(self) -> np.ndarray:\n        # return the codes of items in original grouped axis\n        ids, _, _ = self.group_info\n        if self.indexer is not None:\n            sorter = np.lexsort((ids, self.indexer))\n            ids = ids[sorter]\n        return ids\n\n    @final\n    def _get_compressed_codes(self) -> tuple[np.ndarray, np.ndarray]:\n        if len(self.groupings) > 1:\n            group_index = get_group_index(self.codes, self.shape, sort=True, xnull=True)\n            return compress_group_index(group_index, sort=self._sort)\n\n        ping = self.groupings[0]\n        return ping.codes, np.arange(len(ping.group_index))\n\n    @final\n    @cache_readonly\n    def ngroups(self) -> int:\n        return len(self.result_index)\n\n    @property\n    def reconstructed_codes(self) -> list[np.ndarray]:\n        codes = self.codes\n        ids, obs_ids, _ = self.group_info\n        return decons_obs_group_ids(ids, obs_ids, self.shape, codes, xnull=True)\n\n    @cache_readonly\n    def result_arraylike(self) -> ArrayLike:\n        \"\"\"\n        Analogous to result_index, but returning an ndarray/ExtensionArray\n        allowing us to retain ExtensionDtypes not supported by Index.\n        \"\"\"\n        # TODO: once Index supports arbitrary EAs, this can be removed in favor\n        #  of result_index\n        if len(self.groupings) == 1:\n            return self.groupings[0].group_arraylike\n\n        # result_index is MultiIndex\n        return self.result_index._values\n\n    @cache_readonly\n    def result_index(self) -> Index:\n        if len(self.groupings) == 1:\n            return self.groupings[0].result_index.rename(self.names[0])\n\n        codes = self.reconstructed_codes\n        levels = [ping.result_index for ping in self.groupings]\n        return MultiIndex(\n            levels=levels, codes=codes, verify_integrity=False, names=self.names\n        )\n\n    @final\n    def get_group_levels(self) -> list[ArrayLike]:\n        # Note: only called from _insert_inaxis_grouper_inplace, which\n        #  is only called for BaseGrouper, never for BinGrouper\n        if len(self.groupings) == 1:\n            return [self.groupings[0].group_arraylike]\n\n        name_list = []\n        for ping, codes in zip(self.groupings, self.reconstructed_codes):\n            codes = ensure_platform_int(codes)\n            levels = ping.group_arraylike.take(codes)\n\n            name_list.append(levels)\n\n        return name_list\n\n    # ------------------------------------------------------------\n    # Aggregation functions\n\n    @final\n    def _cython_operation(\n        self,\n        kind: str,\n        values,\n        how: str,\n        axis: int,\n        min_count: int = -1,\n        **kwargs,\n    ) -> ArrayLike:\n        \"\"\"\n        Returns the values of a cython operation.\n        \"\"\"\n        assert kind in [\"transform\", \"aggregate\"]\n\n        cy_op = WrappedCythonOp(kind=kind, how=how)\n\n        ids, _, _ = self.group_info\n        ngroups = self.ngroups\n        return cy_op.cython_operation(\n            values=values,\n            axis=axis,\n            min_count=min_count,\n            comp_ids=ids,\n            ngroups=ngroups,\n            **kwargs,\n        )\n\n    @final\n    def agg_series(\n        self, obj: Series, func: F, preserve_dtype: bool = False\n    ) -> ArrayLike:\n        \"\"\"\n        Parameters\n        ----------\n        obj : Series\n        func : function taking a Series and returning a scalar-like\n        preserve_dtype : bool\n            Whether the aggregation is known to be dtype-preserving.\n\n        Returns\n        -------\n        np.ndarray or ExtensionArray\n        \"\"\"\n        # test_groupby_empty_with_category gets here with self.ngroups == 0\n        #  and len(obj) > 0\n\n        if len(obj) == 0:\n            # SeriesGrouper would raise if we were to call _aggregate_series_fast\n            result = self._aggregate_series_pure_python(obj, func)\n\n        elif not isinstance(obj._values, np.ndarray):\n            # _aggregate_series_fast would raise TypeError when\n            #  calling libreduction.Slider\n            # In the datetime64tz case it would incorrectly cast to tz-naive\n            # TODO: can we get a performant workaround for EAs backed by ndarray?\n            result = self._aggregate_series_pure_python(obj, func)\n\n            # we can preserve a little bit more aggressively with EA dtype\n            #  because maybe_cast_pointwise_result will do a try/except\n            #  with _from_sequence.  NB we are assuming here that _from_sequence\n            #  is sufficiently strict that it casts appropriately.\n            preserve_dtype = True\n\n        elif obj.index._has_complex_internals:\n            # Preempt TypeError in _aggregate_series_fast\n            result = self._aggregate_series_pure_python(obj, func)\n\n        else:\n            result = self._aggregate_series_fast(obj, func)\n\n        npvalues = lib.maybe_convert_objects(result, try_float=False)\n        if preserve_dtype:\n            out = maybe_cast_pointwise_result(npvalues, obj.dtype, numeric_only=True)\n        else:\n            out = npvalues\n        return out\n\n    def _aggregate_series_fast(self, obj: Series, func: F) -> np.ndarray:\n        # -> np.ndarray[object]\n\n        # At this point we have already checked that\n        #  - obj.index is not a MultiIndex\n        #  - obj is backed by an ndarray, not ExtensionArray\n        #  - len(obj) > 0\n        func = com.is_builtin_func(func)\n\n        ids, _, ngroups = self.group_info\n\n        # avoids object / Series creation overhead\n        indexer = get_group_index_sorter(ids, ngroups)\n        obj = obj.take(indexer)\n        ids = ids.take(indexer)\n        sgrouper = libreduction.SeriesGrouper(obj, func, ids, ngroups)\n        result, _ = sgrouper.get_result()\n        return result\n\n    @final\n    def _aggregate_series_pure_python(self, obj: Series, func: F) -> np.ndarray:\n        # -> np.ndarray[object]\n        ids, _, ngroups = self.group_info\n\n        counts = np.zeros(ngroups, dtype=int)\n        result = np.empty(ngroups, dtype=\"O\")\n        initialized = False\n\n        # equiv: splitter = self._get_splitter(obj, axis=0)\n        splitter = get_splitter(obj, ids, ngroups, axis=0)\n\n        for i, group in enumerate(splitter):\n\n            # Each step of this loop corresponds to\n            #  libreduction._BaseGrouper._apply_to_group\n            res = func(group)\n            res = libreduction.extract_result(res)\n\n            if not initialized:\n                # We only do this validation on the first iteration\n                libreduction.check_result_array(res, group.dtype)\n                initialized = True\n\n            counts[i] = group.shape[0]\n            result[i] = res\n\n        return result\n\n\nclass BinGrouper(BaseGrouper):\n    \"\"\"\n    This is an internal Grouper class\n\n    Parameters\n    ----------\n    bins : the split index of binlabels to group the item of axis\n    binlabels : the label list\n    mutated : bool, default False\n    indexer : np.ndarray[np.intp]\n\n    Examples\n    --------\n    bins: [2, 4, 6, 8, 10]\n    binlabels: DatetimeIndex(['2005-01-01', '2005-01-03',\n        '2005-01-05', '2005-01-07', '2005-01-09'],\n        dtype='datetime64[ns]', freq='2D')\n\n    the group_info, which contains the label of each item in grouped\n    axis, the index of label in label list, group number, is\n\n    (array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4]), array([0, 1, 2, 3, 4]), 5)\n\n    means that, the grouped axis has 10 items, can be grouped into 5\n    labels, the first and second items belong to the first label, the\n    third and forth items belong to the second label, and so on\n\n    \"\"\"\n\n    bins: np.ndarray  # np.ndarray[np.int64]\n    binlabels: Index\n    mutated: bool\n\n    def __init__(\n        self,\n        bins,\n        binlabels,\n        mutated: bool = False,\n        indexer=None,\n    ):\n        self.bins = ensure_int64(bins)\n        self.binlabels = ensure_index(binlabels)\n        self.mutated = mutated\n        self.indexer = indexer\n\n        # These lengths must match, otherwise we could call agg_series\n        #  with empty self.bins, which would raise in libreduction.\n        assert len(self.binlabels) == len(self.bins)\n\n    @cache_readonly\n    def groups(self):\n        \"\"\"dict {group name -> group labels}\"\"\"\n        # this is mainly for compat\n        # GH 3881\n        result = {\n            key: value\n            for key, value in zip(self.binlabels, self.bins)\n            if key is not NaT\n        }\n        return result\n\n    @property\n    def nkeys(self) -> int:\n        # still matches len(self.groupings), but we can hard-code\n        return 1\n\n    def _get_grouper(self):\n        \"\"\"\n        We are a grouper as part of another's groupings.\n\n        We have a specific method of grouping, so cannot\n        convert to a Index for our grouper.\n        \"\"\"\n        return self\n\n    def get_iterator(self, data: FrameOrSeries, axis: int = 0):\n        \"\"\"\n        Groupby iterator\n\n        Returns\n        -------\n        Generator yielding sequence of (name, subsetted object)\n        for each group\n        \"\"\"\n        if axis == 0:\n            slicer = lambda start, edge: data.iloc[start:edge]\n        else:\n            slicer = lambda start, edge: data.iloc[:, start:edge]\n\n        length = len(data.axes[axis])\n\n        start = 0\n        for edge, label in zip(self.bins, self.binlabels):\n            if label is not NaT:\n                yield label, slicer(start, edge)\n            start = edge\n\n        if start < length:\n            yield self.binlabels[-1], slicer(start, None)\n\n    @cache_readonly\n    def indices(self):\n        indices = collections.defaultdict(list)\n\n        i = 0\n        for label, bin in zip(self.binlabels, self.bins):\n            if i < bin:\n                if label is not NaT:\n                    indices[label] = list(range(i, bin))\n                i = bin\n        return indices\n\n    @cache_readonly\n    def group_info(self):\n        ngroups = self.ngroups\n        obs_group_ids = np.arange(ngroups, dtype=np.int64)\n        rep = np.diff(np.r_[0, self.bins])\n\n        rep = ensure_platform_int(rep)\n        if ngroups == len(self.bins):\n            comp_ids = np.repeat(np.arange(ngroups), rep)\n        else:\n            comp_ids = np.repeat(np.r_[-1, np.arange(ngroups)], rep)\n\n        return (\n            ensure_platform_int(comp_ids),\n            obs_group_ids,\n            ngroups,\n        )\n\n    @cache_readonly\n    def reconstructed_codes(self) -> list[np.ndarray]:\n        # get unique result indices, and prepend 0 as groupby starts from the first\n        return [np.r_[0, np.flatnonzero(self.bins[1:] != self.bins[:-1]) + 1]]\n\n    @cache_readonly\n    def result_index(self):\n        if len(self.binlabels) != 0 and isna(self.binlabels[0]):\n            return self.binlabels[1:]\n\n        return self.binlabels\n\n    @property\n    def levels(self) -> list[Index]:\n        return [self.binlabels]\n\n    @property\n    def names(self) -> list[Hashable]:\n        return [self.binlabels.name]\n\n    @property\n    def groupings(self) -> list[grouper.Grouping]:\n        lev = self.binlabels\n        ping = grouper.Grouping(lev, lev, in_axis=False, level=None)\n        return [ping]\n\n    def _aggregate_series_fast(self, obj: Series, func: F) -> np.ndarray:\n        # -> np.ndarray[object]\n\n        # At this point we have already checked that\n        #  - obj.index is not a MultiIndex\n        #  - obj is backed by an ndarray, not ExtensionArray\n        #  - ngroups != 0\n        #  - len(self.bins) > 0\n        sbg = libreduction.SeriesBinGrouper(obj, func, self.bins)\n        result, _ = sbg.get_result()\n        return result\n\n\ndef _is_indexed_like(obj, axes, axis: int) -> bool:\n    if isinstance(obj, Series):\n        if len(axes) > 1:\n            return False\n        return obj.axes[axis].equals(axes[axis])\n    elif isinstance(obj, DataFrame):\n        return obj.axes[axis].equals(axes[axis])\n\n    return False\n\n\n# ----------------------------------------------------------------------\n# Splitting / application\n\n\nclass DataSplitter(Generic[FrameOrSeries]):\n    def __init__(\n        self,\n        data: FrameOrSeries,\n        labels: npt.NDArray[np.intp],\n        ngroups: int,\n        axis: int = 0,\n    ):\n        self.data = data\n        self.labels = ensure_platform_int(labels)  # _should_ already be np.intp\n        self.ngroups = ngroups\n\n        self.axis = axis\n        assert isinstance(axis, int), axis\n\n    @cache_readonly\n    def slabels(self) -> np.ndarray:  # np.ndarray[np.intp]\n        # Sorted labels\n        return self.labels.take(self._sort_idx)\n\n    @cache_readonly\n    def _sort_idx(self) -> np.ndarray:  # np.ndarray[np.intp]\n        # Counting sort indexer\n        return get_group_index_sorter(self.labels, self.ngroups)\n\n    def __iter__(self):\n        sdata = self.sorted_data\n\n        if self.ngroups == 0:\n            # we are inside a generator, rather than raise StopIteration\n            # we merely return signal the end\n            return\n\n        starts, ends = lib.generate_slices(self.slabels, self.ngroups)\n\n        for start, end in zip(starts, ends):\n            yield self._chop(sdata, slice(start, end))\n\n    @cache_readonly\n    def sorted_data(self) -> FrameOrSeries:\n        return self.data.take(self._sort_idx, axis=self.axis)\n\n    def _chop(self, sdata, slice_obj: slice) -> NDFrame:\n        raise AbstractMethodError(self)\n\n\nclass SeriesSplitter(DataSplitter):\n    def _chop(self, sdata: Series, slice_obj: slice) -> Series:\n        # fastpath equivalent to `sdata.iloc[slice_obj]`\n        mgr = sdata._mgr.get_slice(slice_obj)\n        # __finalize__ not called here, must be applied by caller if applicable\n\n        # fastpath equivalent to:\n        # `return sdata._constructor(mgr, name=sdata.name, fastpath=True)`\n        obj = type(sdata)._from_mgr(mgr)\n        object.__setattr__(obj, \"_flags\", sdata._flags)\n        object.__setattr__(obj, \"_name\", sdata._name)\n        return obj\n\n\nclass FrameSplitter(DataSplitter):\n    def _chop(self, sdata: DataFrame, slice_obj: slice) -> DataFrame:\n        # Fastpath equivalent to:\n        # if self.axis == 0:\n        #     return sdata.iloc[slice_obj]\n        # else:\n        #     return sdata.iloc[:, slice_obj]\n        mgr = sdata._mgr.get_slice(slice_obj, axis=1 - self.axis)\n        # __finalize__ not called here, must be applied by caller if applicable\n\n        # fastpath equivalent to `return sdata._constructor(mgr)`\n        obj = type(sdata)._from_mgr(mgr)\n        object.__setattr__(obj, \"_flags\", sdata._flags)\n        return obj\n\n\ndef get_splitter(\n    data: FrameOrSeries, labels: np.ndarray, ngroups: int, axis: int = 0\n) -> DataSplitter:\n    if isinstance(data, Series):\n        klass: type[DataSplitter] = SeriesSplitter\n    else:\n        # i.e. DataFrame\n        klass = FrameSplitter\n\n    return klass(data, labels, ngroups, axis)\n"
    },
    {
      "filename": "pandas/tests/groupby/test_apply.py",
      "content": "from datetime import (\n    date,\n    datetime,\n)\nfrom io import StringIO\n\nimport numpy as np\nimport pytest\n\nimport pandas as pd\nfrom pandas import (\n    DataFrame,\n    Index,\n    MultiIndex,\n    Series,\n    bdate_range,\n)\nimport pandas._testing as tm\n\n\ndef test_apply_issues():\n    # GH 5788\n\n    s = \"\"\"2011.05.16,00:00,1.40893\n2011.05.16,01:00,1.40760\n2011.05.16,02:00,1.40750\n2011.05.16,03:00,1.40649\n2011.05.17,02:00,1.40893\n2011.05.17,03:00,1.40760\n2011.05.17,04:00,1.40750\n2011.05.17,05:00,1.40649\n2011.05.18,02:00,1.40893\n2011.05.18,03:00,1.40760\n2011.05.18,04:00,1.40750\n2011.05.18,05:00,1.40649\"\"\"\n\n    df = pd.read_csv(\n        StringIO(s),\n        header=None,\n        names=[\"date\", \"time\", \"value\"],\n        parse_dates=[[\"date\", \"time\"]],\n    )\n    df = df.set_index(\"date_time\")\n\n    expected = df.groupby(df.index.date).idxmax()\n    result = df.groupby(df.index.date).apply(lambda x: x.idxmax())\n    tm.assert_frame_equal(result, expected)\n\n    # GH 5789\n    # don't auto coerce dates\n    df = pd.read_csv(StringIO(s), header=None, names=[\"date\", \"time\", \"value\"])\n    exp_idx = Index(\n        [\"2011.05.16\", \"2011.05.17\", \"2011.05.18\"], dtype=object, name=\"date\"\n    )\n    expected = Series([\"00:00\", \"02:00\", \"02:00\"], index=exp_idx)\n    result = df.groupby(\"date\").apply(lambda x: x[\"time\"][x[\"value\"].idxmax()])\n    tm.assert_series_equal(result, expected)\n\n\ndef test_apply_trivial():\n    # GH 20066\n    # trivial apply: ignore input and return a constant dataframe.\n    df = DataFrame(\n        {\"key\": [\"a\", \"a\", \"b\", \"b\", \"a\"], \"data\": [1.0, 2.0, 3.0, 4.0, 5.0]},\n        columns=[\"key\", \"data\"],\n    )\n    expected = pd.concat([df.iloc[1:], df.iloc[1:]], axis=1, keys=[\"float64\", \"object\"])\n    result = df.groupby([str(x) for x in df.dtypes], axis=1).apply(\n        lambda x: df.iloc[1:]\n    )\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_apply_trivial_fail():\n    # GH 20066\n    df = DataFrame(\n        {\"key\": [\"a\", \"a\", \"b\", \"b\", \"a\"], \"data\": [1.0, 2.0, 3.0, 4.0, 5.0]},\n        columns=[\"key\", \"data\"],\n    )\n    expected = pd.concat([df, df], axis=1, keys=[\"float64\", \"object\"])\n    result = df.groupby([str(x) for x in df.dtypes], axis=1).apply(lambda x: df)\n\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"df, group_names\",\n    [\n        (DataFrame({\"a\": [1, 1, 1, 2, 3], \"b\": [\"a\", \"a\", \"a\", \"b\", \"c\"]}), [1, 2, 3]),\n        (DataFrame({\"a\": [0, 0, 1, 1], \"b\": [0, 1, 0, 1]}), [0, 1]),\n        (DataFrame({\"a\": [1]}), [1]),\n        (DataFrame({\"a\": [1, 1, 1, 2, 2, 1, 1, 2], \"b\": range(8)}), [1, 2]),\n        (DataFrame({\"a\": [1, 2, 3, 1, 2, 3], \"two\": [4, 5, 6, 7, 8, 9]}), [1, 2, 3]),\n        (\n            DataFrame(\n                {\n                    \"a\": list(\"aaabbbcccc\"),\n                    \"B\": [3, 4, 3, 6, 5, 2, 1, 9, 5, 4],\n                    \"C\": [4, 0, 2, 2, 2, 7, 8, 6, 2, 8],\n                }\n            ),\n            [\"a\", \"b\", \"c\"],\n        ),\n        (DataFrame([[1, 2, 3], [2, 2, 3]], columns=[\"a\", \"b\", \"c\"]), [1, 2]),\n    ],\n    ids=[\n        \"GH2936\",\n        \"GH7739 & GH10519\",\n        \"GH10519\",\n        \"GH2656\",\n        \"GH12155\",\n        \"GH20084\",\n        \"GH21417\",\n    ],\n)\ndef test_group_apply_once_per_group(df, group_names):\n    # GH2936, GH7739, GH10519, GH2656, GH12155, GH20084, GH21417\n\n    # This test should ensure that a function is only evaluated\n    # once per group. Previously the function has been evaluated twice\n    # on the first group to check if the Cython index slider is safe to use\n    # This test ensures that the side effect (append to list) is only triggered\n    # once per group\n\n    names = []\n    # cannot parameterize over the functions since they need external\n    # `names` to detect side effects\n\n    def f_copy(group):\n        # this takes the fast apply path\n        names.append(group.name)\n        return group.copy()\n\n    def f_nocopy(group):\n        # this takes the slow apply path\n        names.append(group.name)\n        return group\n\n    def f_scalar(group):\n        # GH7739, GH2656\n        names.append(group.name)\n        return 0\n\n    def f_none(group):\n        # GH10519, GH12155, GH21417\n        names.append(group.name)\n        return None\n\n    def f_constant_df(group):\n        # GH2936, GH20084\n        names.append(group.name)\n        return DataFrame({\"a\": [1], \"b\": [1]})\n\n    for func in [f_copy, f_nocopy, f_scalar, f_none, f_constant_df]:\n        del names[:]\n\n        df.groupby(\"a\").apply(func)\n        assert names == group_names\n\n\ndef test_group_apply_once_per_group2(capsys):\n    # GH: 31111\n    # groupby-apply need to execute len(set(group_by_columns)) times\n\n    expected = 2  # Number of times `apply` should call a function for the current test\n\n    df = DataFrame(\n        {\n            \"group_by_column\": [0, 0, 0, 0, 1, 1, 1, 1],\n            \"test_column\": [\"0\", \"2\", \"4\", \"6\", \"8\", \"10\", \"12\", \"14\"],\n        },\n        index=[\"0\", \"2\", \"4\", \"6\", \"8\", \"10\", \"12\", \"14\"],\n    )\n\n    df.groupby(\"group_by_column\").apply(lambda df: print(\"function_called\"))\n\n    result = capsys.readouterr().out.count(\"function_called\")\n    # If `groupby` behaves unexpectedly, this test will break\n    assert result == expected\n\n\ndef test_apply_fast_slow_identical():\n    # GH 31613\n\n    df = DataFrame({\"A\": [0, 0, 1], \"b\": range(3)})\n\n    # For simple index structures we check for fast/slow apply using\n    # an identity check on in/output\n    def slow(group):\n        return group\n\n    def fast(group):\n        return group.copy()\n\n    fast_df = df.groupby(\"A\").apply(fast)\n    slow_df = df.groupby(\"A\").apply(slow)\n\n    tm.assert_frame_equal(fast_df, slow_df)\n\n\n@pytest.mark.parametrize(\n    \"func\",\n    [\n        lambda x: x,\n        lambda x: x[:],\n        lambda x: x.copy(deep=False),\n        lambda x: x.copy(deep=True),\n    ],\n)\ndef test_groupby_apply_identity_maybecopy_index_identical(func):\n    # GH 14927\n    # Whether the function returns a copy of the input data or not should not\n    # have an impact on the index structure of the result since this is not\n    # transparent to the user\n\n    df = DataFrame({\"g\": [1, 2, 2, 2], \"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8]})\n\n    result = df.groupby(\"g\").apply(func)\n    tm.assert_frame_equal(result, df)\n\n\ndef test_apply_with_mixed_dtype():\n    # GH3480, apply with mixed dtype on axis=1 breaks in 0.11\n    df = DataFrame(\n        {\n            \"foo1\": np.random.randn(6),\n            \"foo2\": [\"one\", \"two\", \"two\", \"three\", \"one\", \"two\"],\n        }\n    )\n    result = df.apply(lambda x: x, axis=1).dtypes\n    expected = df.dtypes\n    tm.assert_series_equal(result, expected)\n\n    # GH 3610 incorrect dtype conversion with as_index=False\n    df = DataFrame({\"c1\": [1, 2, 6, 6, 8]})\n    df[\"c2\"] = df.c1 / 2.0\n    result1 = df.groupby(\"c2\").mean().reset_index().c2\n    result2 = df.groupby(\"c2\", as_index=False).mean().c2\n    tm.assert_series_equal(result1, result2)\n\n\ndef test_groupby_as_index_apply(df):\n    # GH #4648 and #3417\n    df = DataFrame(\n        {\n            \"item_id\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"b\"],\n            \"user_id\": [1, 2, 1, 1, 3, 1],\n            \"time\": range(6),\n        }\n    )\n\n    g_as = df.groupby(\"user_id\", as_index=True)\n    g_not_as = df.groupby(\"user_id\", as_index=False)\n\n    res_as = g_as.head(2).index\n    res_not_as = g_not_as.head(2).index\n    exp = Index([0, 1, 2, 4])\n    tm.assert_index_equal(res_as, exp)\n    tm.assert_index_equal(res_not_as, exp)\n\n    res_as_apply = g_as.apply(lambda x: x.head(2)).index\n    res_not_as_apply = g_not_as.apply(lambda x: x.head(2)).index\n\n    # apply doesn't maintain the original ordering\n    # changed in GH5610 as the as_index=False returns a MI here\n    exp_not_as_apply = MultiIndex.from_tuples([(0, 0), (0, 2), (1, 1), (2, 4)])\n    tp = [(1, 0), (1, 2), (2, 1), (3, 4)]\n    exp_as_apply = MultiIndex.from_tuples(tp, names=[\"user_id\", None])\n\n    tm.assert_index_equal(res_as_apply, exp_as_apply)\n    tm.assert_index_equal(res_not_as_apply, exp_not_as_apply)\n\n    ind = Index(list(\"abcde\"))\n    df = DataFrame([[1, 2], [2, 3], [1, 4], [1, 5], [2, 6]], index=ind)\n    res = df.groupby(0, as_index=False).apply(lambda x: x).index\n    tm.assert_index_equal(res, ind)\n\n\ndef test_apply_concat_preserve_names(three_group):\n    grouped = three_group.groupby([\"A\", \"B\"])\n\n    def desc(group):\n        result = group.describe()\n        result.index.name = \"stat\"\n        return result\n\n    def desc2(group):\n        result = group.describe()\n        result.index.name = \"stat\"\n        result = result[: len(group)]\n        # weirdo\n        return result\n\n    def desc3(group):\n        result = group.describe()\n\n        # names are different\n        result.index.name = f\"stat_{len(group):d}\"\n\n        result = result[: len(group)]\n        # weirdo\n        return result\n\n    result = grouped.apply(desc)\n    assert result.index.names == (\"A\", \"B\", \"stat\")\n\n    result2 = grouped.apply(desc2)\n    assert result2.index.names == (\"A\", \"B\", \"stat\")\n\n    result3 = grouped.apply(desc3)\n    assert result3.index.names == (\"A\", \"B\", None)\n\n\ndef test_apply_series_to_frame():\n    def f(piece):\n        with np.errstate(invalid=\"ignore\"):\n            logged = np.log(piece)\n        return DataFrame(\n            {\"value\": piece, \"demeaned\": piece - piece.mean(), \"logged\": logged}\n        )\n\n    dr = bdate_range(\"1/1/2000\", periods=100)\n    ts = Series(np.random.randn(100), index=dr)\n\n    grouped = ts.groupby(lambda x: x.month)\n    result = grouped.apply(f)\n\n    assert isinstance(result, DataFrame)\n    tm.assert_index_equal(result.index, ts.index)\n\n\ndef test_apply_series_yield_constant(df):\n    result = df.groupby([\"A\", \"B\"])[\"C\"].apply(len)\n    assert result.index.names[:2] == (\"A\", \"B\")\n\n\ndef test_apply_frame_yield_constant(df):\n    # GH13568\n    result = df.groupby([\"A\", \"B\"]).apply(len)\n    assert isinstance(result, Series)\n    assert result.name is None\n\n    result = df.groupby([\"A\", \"B\"])[[\"C\", \"D\"]].apply(len)\n    assert isinstance(result, Series)\n    assert result.name is None\n\n\ndef test_apply_frame_to_series(df):\n    grouped = df.groupby([\"A\", \"B\"])\n    result = grouped.apply(len)\n    expected = grouped.count()[\"C\"]\n    tm.assert_index_equal(result.index, expected.index)\n    tm.assert_numpy_array_equal(result.values, expected.values)\n\n\ndef test_apply_frame_not_as_index_column_name(df):\n    # GH 35964 - path within _wrap_applied_output not hit by a test\n    grouped = df.groupby([\"A\", \"B\"], as_index=False)\n    result = grouped.apply(len)\n    expected = grouped.count().rename(columns={\"C\": np.nan}).drop(columns=\"D\")\n    # TODO: Use assert_frame_equal when column name is not np.nan (GH 36306)\n    tm.assert_index_equal(result.index, expected.index)\n    tm.assert_numpy_array_equal(result.values, expected.values)\n\n\ndef test_apply_frame_concat_series():\n    def trans(group):\n        return group.groupby(\"B\")[\"C\"].sum().sort_values()[:2]\n\n    def trans2(group):\n        grouped = group.groupby(df.reindex(group.index)[\"B\"])\n        return grouped.sum().sort_values()[:2]\n\n    df = DataFrame(\n        {\n            \"A\": np.random.randint(0, 5, 1000),\n            \"B\": np.random.randint(0, 5, 1000),\n            \"C\": np.random.randn(1000),\n        }\n    )\n\n    result = df.groupby(\"A\").apply(trans)\n    exp = df.groupby(\"A\")[\"C\"].apply(trans2)\n    tm.assert_series_equal(result, exp, check_names=False)\n    assert result.name == \"C\"\n\n\ndef test_apply_transform(ts):\n    grouped = ts.groupby(lambda x: x.month)\n    result = grouped.apply(lambda x: x * 2)\n    expected = grouped.transform(lambda x: x * 2)\n    tm.assert_series_equal(result, expected)\n\n\ndef test_apply_multikey_corner(tsframe):\n    grouped = tsframe.groupby([lambda x: x.year, lambda x: x.month])\n\n    def f(group):\n        return group.sort_values(\"A\")[-5:]\n\n    result = grouped.apply(f)\n    for key, group in grouped:\n        tm.assert_frame_equal(result.loc[key], f(group))\n\n\ndef test_apply_chunk_view():\n    # Low level tinkering could be unsafe, make sure not\n    df = DataFrame({\"key\": [1, 1, 1, 2, 2, 2, 3, 3, 3], \"value\": range(9)})\n\n    result = df.groupby(\"key\", group_keys=False).apply(lambda x: x[:2])\n    expected = df.take([0, 1, 3, 4, 6, 7])\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_apply_no_name_column_conflict():\n    df = DataFrame(\n        {\n            \"name\": [1, 1, 1, 1, 1, 1, 2, 2, 2, 2],\n            \"name2\": [0, 0, 0, 1, 1, 1, 0, 0, 1, 1],\n            \"value\": range(9, -1, -1),\n        }\n    )\n\n    # it works! #2605\n    grouped = df.groupby([\"name\", \"name2\"])\n    grouped.apply(lambda x: x.sort_values(\"value\", inplace=True))\n\n\ndef test_apply_typecast_fail():\n    df = DataFrame(\n        {\n            \"d\": [1.0, 1.0, 1.0, 2.0, 2.0, 2.0],\n            \"c\": np.tile([\"a\", \"b\", \"c\"], 2),\n            \"v\": np.arange(1.0, 7.0),\n        }\n    )\n\n    def f(group):\n        v = group[\"v\"]\n        group[\"v2\"] = (v - v.min()) / (v.max() - v.min())\n        return group\n\n    result = df.groupby(\"d\").apply(f)\n\n    expected = df.copy()\n    expected[\"v2\"] = np.tile([0.0, 0.5, 1], 2)\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_apply_multiindex_fail():\n    index = MultiIndex.from_arrays([[0, 0, 0, 1, 1, 1], [1, 2, 3, 1, 2, 3]])\n    df = DataFrame(\n        {\n            \"d\": [1.0, 1.0, 1.0, 2.0, 2.0, 2.0],\n            \"c\": np.tile([\"a\", \"b\", \"c\"], 2),\n            \"v\": np.arange(1.0, 7.0),\n        },\n        index=index,\n    )\n\n    def f(group):\n        v = group[\"v\"]\n        group[\"v2\"] = (v - v.min()) / (v.max() - v.min())\n        return group\n\n    result = df.groupby(\"d\").apply(f)\n\n    expected = df.copy()\n    expected[\"v2\"] = np.tile([0.0, 0.5, 1], 2)\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_apply_corner(tsframe):\n    result = tsframe.groupby(lambda x: x.year).apply(lambda x: x * 2)\n    expected = tsframe * 2\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_apply_without_copy():\n    # GH 5545\n    # returning a non-copy in an applied function fails\n\n    data = DataFrame(\n        {\n            \"id_field\": [100, 100, 200, 300],\n            \"category\": [\"a\", \"b\", \"c\", \"c\"],\n            \"value\": [1, 2, 3, 4],\n        }\n    )\n\n    def filt1(x):\n        if x.shape[0] == 1:\n            return x.copy()\n        else:\n            return x[x.category == \"c\"]\n\n    def filt2(x):\n        if x.shape[0] == 1:\n            return x\n        else:\n            return x[x.category == \"c\"]\n\n    expected = data.groupby(\"id_field\").apply(filt1)\n    result = data.groupby(\"id_field\").apply(filt2)\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"test_series\", [True, False])\ndef test_apply_with_duplicated_non_sorted_axis(test_series):\n    # GH 30667\n    df = DataFrame(\n        [[\"x\", \"p\"], [\"x\", \"p\"], [\"x\", \"o\"]], columns=[\"X\", \"Y\"], index=[1, 2, 2]\n    )\n    if test_series:\n        ser = df.set_index(\"Y\")[\"X\"]\n        result = ser.groupby(level=0).apply(lambda x: x)\n\n        # not expecting the order to remain the same for duplicated axis\n        result = result.sort_index()\n        expected = ser.sort_index()\n        tm.assert_series_equal(result, expected)\n    else:\n        result = df.groupby(\"Y\").apply(lambda x: x)\n\n        # not expecting the order to remain the same for duplicated axis\n        result = result.sort_values(\"Y\")\n        expected = df.sort_values(\"Y\")\n        tm.assert_frame_equal(result, expected)\n\n\ndef test_apply_reindex_values():\n    # GH: 26209\n    # reindexing from a single column of a groupby object with duplicate indices caused\n    # a ValueError (cannot reindex from duplicate axis) in 0.24.2, the problem was\n    # solved in #30679\n    values = [1, 2, 3, 4]\n    indices = [1, 1, 2, 2]\n    df = DataFrame({\"group\": [\"Group1\", \"Group2\"] * 2, \"value\": values}, index=indices)\n    expected = Series(values, index=indices, name=\"value\")\n\n    def reindex_helper(x):\n        return x.reindex(np.arange(x.index.min(), x.index.max() + 1))\n\n    # the following group by raised a ValueError\n    result = df.groupby(\"group\").value.apply(reindex_helper)\n    tm.assert_series_equal(expected, result)\n\n\ndef test_apply_corner_cases():\n    # #535, can't use sliding iterator\n\n    N = 1000\n    labels = np.random.randint(0, 100, size=N)\n    df = DataFrame(\n        {\n            \"key\": labels,\n            \"value1\": np.random.randn(N),\n            \"value2\": [\"foo\", \"bar\", \"baz\", \"qux\"] * (N // 4),\n        }\n    )\n\n    grouped = df.groupby(\"key\")\n\n    def f(g):\n        g[\"value3\"] = g[\"value1\"] * 2\n        return g\n\n    result = grouped.apply(f)\n    assert \"value3\" in result\n\n\ndef test_apply_numeric_coercion_when_datetime():\n    # In the past, group-by/apply operations have been over-eager\n    # in converting dtypes to numeric, in the presence of datetime\n    # columns.  Various GH issues were filed, the reproductions\n    # for which are here.\n\n    # GH 15670\n    df = DataFrame(\n        {\"Number\": [1, 2], \"Date\": [\"2017-03-02\"] * 2, \"Str\": [\"foo\", \"inf\"]}\n    )\n    expected = df.groupby([\"Number\"]).apply(lambda x: x.iloc[0])\n    df.Date = pd.to_datetime(df.Date)\n    result = df.groupby([\"Number\"]).apply(lambda x: x.iloc[0])\n    tm.assert_series_equal(result[\"Str\"], expected[\"Str\"])\n\n    # GH 15421\n    df = DataFrame(\n        {\"A\": [10, 20, 30], \"B\": [\"foo\", \"3\", \"4\"], \"T\": [pd.Timestamp(\"12:31:22\")] * 3}\n    )\n\n    def get_B(g):\n        return g.iloc[0][[\"B\"]]\n\n    result = df.groupby(\"A\").apply(get_B)[\"B\"]\n    expected = df.B\n    expected.index = df.A\n    tm.assert_series_equal(result, expected)\n\n    # GH 14423\n    def predictions(tool):\n        out = Series(index=[\"p1\", \"p2\", \"useTime\"], dtype=object)\n        if \"step1\" in list(tool.State):\n            out[\"p1\"] = str(tool[tool.State == \"step1\"].Machine.values[0])\n        if \"step2\" in list(tool.State):\n            out[\"p2\"] = str(tool[tool.State == \"step2\"].Machine.values[0])\n            out[\"useTime\"] = str(tool[tool.State == \"step2\"].oTime.values[0])\n        return out\n\n    df1 = DataFrame(\n        {\n            \"Key\": [\"B\", \"B\", \"A\", \"A\"],\n            \"State\": [\"step1\", \"step2\", \"step1\", \"step2\"],\n            \"oTime\": [\"\", \"2016-09-19 05:24:33\", \"\", \"2016-09-19 23:59:04\"],\n            \"Machine\": [\"23\", \"36L\", \"36R\", \"36R\"],\n        }\n    )\n    df2 = df1.copy()\n    df2.oTime = pd.to_datetime(df2.oTime)\n    expected = df1.groupby(\"Key\").apply(predictions).p1\n    result = df2.groupby(\"Key\").apply(predictions).p1\n    tm.assert_series_equal(expected, result)\n\n\ndef test_apply_aggregating_timedelta_and_datetime():\n    # Regression test for GH 15562\n    # The following groupby caused ValueErrors and IndexErrors pre 0.20.0\n\n    df = DataFrame(\n        {\n            \"clientid\": [\"A\", \"B\", \"C\"],\n            \"datetime\": [np.datetime64(\"2017-02-01 00:00:00\")] * 3,\n        }\n    )\n    df[\"time_delta_zero\"] = df.datetime - df.datetime\n    result = df.groupby(\"clientid\").apply(\n        lambda ddf: Series(\n            {\"clientid_age\": ddf.time_delta_zero.min(), \"date\": ddf.datetime.min()}\n        )\n    )\n    expected = DataFrame(\n        {\n            \"clientid\": [\"A\", \"B\", \"C\"],\n            \"clientid_age\": [np.timedelta64(0, \"D\")] * 3,\n            \"date\": [np.datetime64(\"2017-02-01 00:00:00\")] * 3,\n        }\n    ).set_index(\"clientid\")\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_apply_groupby_datetimeindex():\n    # GH 26182\n    # groupby apply failed on dataframe with DatetimeIndex\n\n    data = [[\"A\", 10], [\"B\", 20], [\"B\", 30], [\"C\", 40], [\"C\", 50]]\n    df = DataFrame(\n        data, columns=[\"Name\", \"Value\"], index=pd.date_range(\"2020-09-01\", \"2020-09-05\")\n    )\n\n    result = df.groupby(\"Name\").sum()\n\n    expected = DataFrame({\"Name\": [\"A\", \"B\", \"C\"], \"Value\": [10, 50, 90]})\n    expected.set_index(\"Name\", inplace=True)\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_time_field_bug():\n    # Test a fix for the following error related to GH issue 11324 When\n    # non-key fields in a group-by dataframe contained time-based fields\n    # that were not returned by the apply function, an exception would be\n    # raised.\n\n    df = DataFrame({\"a\": 1, \"b\": [datetime.now() for nn in range(10)]})\n\n    def func_with_no_date(batch):\n        return Series({\"c\": 2})\n\n    def func_with_date(batch):\n        return Series({\"b\": datetime(2015, 1, 1), \"c\": 2})\n\n    dfg_no_conversion = df.groupby(by=[\"a\"]).apply(func_with_no_date)\n    dfg_no_conversion_expected = DataFrame({\"c\": 2}, index=[1])\n    dfg_no_conversion_expected.index.name = \"a\"\n\n    dfg_conversion = df.groupby(by=[\"a\"]).apply(func_with_date)\n    dfg_conversion_expected = DataFrame({\"b\": datetime(2015, 1, 1), \"c\": 2}, index=[1])\n    dfg_conversion_expected.index.name = \"a\"\n\n    tm.assert_frame_equal(dfg_no_conversion, dfg_no_conversion_expected)\n    tm.assert_frame_equal(dfg_conversion, dfg_conversion_expected)\n\n\ndef test_gb_apply_list_of_unequal_len_arrays():\n\n    # GH1738\n    df = DataFrame(\n        {\n            \"group1\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\"],\n            \"group2\": [\"c\", \"c\", \"d\", \"d\", \"d\", \"e\", \"c\", \"c\", \"d\", \"d\", \"d\", \"e\"],\n            \"weight\": [1.1, 2, 3, 4, 5, 6, 2, 4, 6, 8, 1, 2],\n            \"value\": [7.1, 8, 9, 10, 11, 12, 8, 7, 6, 5, 4, 3],\n        }\n    )\n    df = df.set_index([\"group1\", \"group2\"])\n    df_grouped = df.groupby(level=[\"group1\", \"group2\"], sort=True)\n\n    def noddy(value, weight):\n        out = np.array(value * weight).repeat(3)\n        return out\n\n    # the kernel function returns arrays of unequal length\n    # pandas sniffs the first one, sees it's an array and not\n    # a list, and assumed the rest are of equal length\n    # and so tries a vstack\n\n    # don't die\n    df_grouped.apply(lambda x: noddy(x.value, x.weight))\n\n\ndef test_groupby_apply_all_none():\n    # Tests to make sure no errors if apply function returns all None\n    # values. Issue 9684.\n    test_df = DataFrame({\"groups\": [0, 0, 1, 1], \"random_vars\": [8, 7, 4, 5]})\n\n    def test_func(x):\n        pass\n\n    result = test_df.groupby(\"groups\").apply(test_func)\n    expected = DataFrame()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_apply_none_first():\n    # GH 12824. Tests if apply returns None first.\n    test_df1 = DataFrame({\"groups\": [1, 1, 1, 2], \"vars\": [0, 1, 2, 3]})\n    test_df2 = DataFrame({\"groups\": [1, 2, 2, 2], \"vars\": [0, 1, 2, 3]})\n\n    def test_func(x):\n        if x.shape[0] < 2:\n            return None\n        return x.iloc[[0, -1]]\n\n    result1 = test_df1.groupby(\"groups\").apply(test_func)\n    result2 = test_df2.groupby(\"groups\").apply(test_func)\n    index1 = MultiIndex.from_arrays([[1, 1], [0, 2]], names=[\"groups\", None])\n    index2 = MultiIndex.from_arrays([[2, 2], [1, 3]], names=[\"groups\", None])\n    expected1 = DataFrame({\"groups\": [1, 1], \"vars\": [0, 2]}, index=index1)\n    expected2 = DataFrame({\"groups\": [2, 2], \"vars\": [1, 3]}, index=index2)\n    tm.assert_frame_equal(result1, expected1)\n    tm.assert_frame_equal(result2, expected2)\n\n\ndef test_groupby_apply_return_empty_chunk():\n    # GH 22221: apply filter which returns some empty groups\n    df = DataFrame({\"value\": [0, 1], \"group\": [\"filled\", \"empty\"]})\n    groups = df.groupby(\"group\")\n    result = groups.apply(lambda group: group[group.value != 1][\"value\"])\n    expected = Series(\n        [0],\n        name=\"value\",\n        index=MultiIndex.from_product(\n            [[\"empty\", \"filled\"], [0]], names=[\"group\", None]\n        ).drop(\"empty\"),\n    )\n    tm.assert_series_equal(result, expected)\n\n\ndef test_apply_with_mixed_types():\n    # gh-20949\n    df = DataFrame({\"A\": \"a a b\".split(), \"B\": [1, 2, 3], \"C\": [4, 6, 5]})\n    g = df.groupby(\"A\")\n\n    result = g.transform(lambda x: x / x.sum())\n    expected = DataFrame({\"B\": [1 / 3.0, 2 / 3.0, 1], \"C\": [0.4, 0.6, 1.0]})\n    tm.assert_frame_equal(result, expected)\n\n    result = g.apply(lambda x: x / x.sum())\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_func_returns_object():\n    # GH 28652\n    df = DataFrame({\"a\": [1, 2]}, index=pd.Int64Index([1, 2]))\n    result = df.groupby(\"a\").apply(lambda g: g.index)\n    expected = Series(\n        [pd.Int64Index([1]), pd.Int64Index([2])], index=pd.Int64Index([1, 2], name=\"a\")\n    )\n\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"group_column_dtlike\",\n    [datetime.today(), datetime.today().date(), datetime.today().time()],\n)\ndef test_apply_datetime_issue(group_column_dtlike):\n    # GH-28247\n    # groupby-apply throws an error if one of the columns in the DataFrame\n    #   is a datetime object and the column labels are different from\n    #   standard int values in range(len(num_columns))\n\n    df = DataFrame({\"a\": [\"foo\"], \"b\": [group_column_dtlike]})\n    result = df.groupby(\"a\").apply(lambda x: Series([\"spam\"], index=[42]))\n\n    expected = DataFrame(\n        [\"spam\"], Index([\"foo\"], dtype=\"object\", name=\"a\"), columns=[42]\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_apply_series_return_dataframe_groups():\n    # GH 10078\n    tdf = DataFrame(\n        {\n            \"day\": {\n                0: pd.Timestamp(\"2015-02-24 00:00:00\"),\n                1: pd.Timestamp(\"2015-02-24 00:00:00\"),\n                2: pd.Timestamp(\"2015-02-24 00:00:00\"),\n                3: pd.Timestamp(\"2015-02-24 00:00:00\"),\n                4: pd.Timestamp(\"2015-02-24 00:00:00\"),\n            },\n            \"userAgent\": {\n                0: \"some UA string\",\n                1: \"some UA string\",\n                2: \"some UA string\",\n                3: \"another UA string\",\n                4: \"some UA string\",\n            },\n            \"userId\": {\n                0: \"17661101\",\n                1: \"17661101\",\n                2: \"17661101\",\n                3: \"17661101\",\n                4: \"17661101\",\n            },\n        }\n    )\n\n    def most_common_values(df):\n        return Series({c: s.value_counts().index[0] for c, s in df.iteritems()})\n\n    result = tdf.groupby(\"day\").apply(most_common_values)[\"userId\"]\n    expected = Series(\n        [\"17661101\"], index=pd.DatetimeIndex([\"2015-02-24\"], name=\"day\"), name=\"userId\"\n    )\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"category\", [False, True])\ndef test_apply_multi_level_name(category):\n    # https://github.com/pandas-dev/pandas/issues/31068\n    b = [1, 2] * 5\n    if category:\n        b = pd.Categorical(b, categories=[1, 2, 3])\n        expected_index = pd.CategoricalIndex([1, 2], categories=[1, 2, 3], name=\"B\")\n    else:\n        expected_index = Index([1, 2], name=\"B\")\n    df = DataFrame(\n        {\"A\": np.arange(10), \"B\": b, \"C\": list(range(10)), \"D\": list(range(10))}\n    ).set_index([\"A\", \"B\"])\n    result = df.groupby(\"B\").apply(lambda x: x.sum())\n    expected = DataFrame({\"C\": [20, 25], \"D\": [20, 25]}, index=expected_index)\n    tm.assert_frame_equal(result, expected)\n    assert df.index.names == [\"A\", \"B\"]\n\n\ndef test_groupby_apply_datetime_result_dtypes():\n    # GH 14849\n    data = DataFrame.from_records(\n        [\n            (pd.Timestamp(2016, 1, 1), \"red\", \"dark\", 1, \"8\"),\n            (pd.Timestamp(2015, 1, 1), \"green\", \"stormy\", 2, \"9\"),\n            (pd.Timestamp(2014, 1, 1), \"blue\", \"bright\", 3, \"10\"),\n            (pd.Timestamp(2013, 1, 1), \"blue\", \"calm\", 4, \"potato\"),\n        ],\n        columns=[\"observation\", \"color\", \"mood\", \"intensity\", \"score\"],\n    )\n    result = data.groupby(\"color\").apply(lambda g: g.iloc[0]).dtypes\n    expected = Series(\n        [np.dtype(\"datetime64[ns]\"), object, object, np.int64, object],\n        index=[\"observation\", \"color\", \"mood\", \"intensity\", \"score\"],\n    )\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"index\",\n    [\n        pd.CategoricalIndex(list(\"abc\")),\n        pd.interval_range(0, 3),\n        pd.period_range(\"2020\", periods=3, freq=\"D\"),\n        MultiIndex.from_tuples([(\"a\", 0), (\"a\", 1), (\"b\", 0)]),\n    ],\n)\ndef test_apply_index_has_complex_internals(index):\n    # GH 31248\n    df = DataFrame({\"group\": [1, 1, 2], \"value\": [0, 1, 0]}, index=index)\n    result = df.groupby(\"group\").apply(lambda x: x)\n    tm.assert_frame_equal(result, df)\n\n\n@pytest.mark.parametrize(\n    \"function, expected_values\",\n    [\n        (lambda x: x.index.to_list(), [[0, 1], [2, 3]]),\n        (lambda x: set(x.index.to_list()), [{0, 1}, {2, 3}]),\n        (lambda x: tuple(x.index.to_list()), [(0, 1), (2, 3)]),\n        (\n            lambda x: {n: i for (n, i) in enumerate(x.index.to_list())},\n            [{0: 0, 1: 1}, {0: 2, 1: 3}],\n        ),\n        (\n            lambda x: [{n: i} for (n, i) in enumerate(x.index.to_list())],\n            [[{0: 0}, {1: 1}], [{0: 2}, {1: 3}]],\n        ),\n    ],\n)\ndef test_apply_function_returns_non_pandas_non_scalar(function, expected_values):\n    # GH 31441\n    df = DataFrame([\"A\", \"A\", \"B\", \"B\"], columns=[\"groups\"])\n    result = df.groupby(\"groups\").apply(function)\n    expected = Series(expected_values, index=Index([\"A\", \"B\"], name=\"groups\"))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_apply_function_returns_numpy_array():\n    # GH 31605\n    def fct(group):\n        return group[\"B\"].values.flatten()\n\n    df = DataFrame({\"A\": [\"a\", \"a\", \"b\", \"none\"], \"B\": [1, 2, 3, np.nan]})\n\n    result = df.groupby(\"A\").apply(fct)\n    expected = Series(\n        [[1.0, 2.0], [3.0], [np.nan]], index=Index([\"a\", \"b\", \"none\"], name=\"A\")\n    )\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"function\", [lambda gr: gr.index, lambda gr: gr.index + 1 - 1])\ndef test_apply_function_index_return(function):\n    # GH: 22541\n    df = DataFrame([1, 2, 2, 2, 1, 2, 3, 1, 3, 1], columns=[\"id\"])\n    result = df.groupby(\"id\").apply(function)\n    expected = Series(\n        [Index([0, 4, 7, 9]), Index([1, 2, 3, 5]), Index([6, 8])],\n        index=Index([1, 2, 3], name=\"id\"),\n    )\n    tm.assert_series_equal(result, expected)\n\n\ndef test_apply_function_with_indexing_return_column():\n    # GH: 7002\n    df = DataFrame(\n        {\n            \"foo1\": [\"one\", \"two\", \"two\", \"three\", \"one\", \"two\"],\n            \"foo2\": [1, 2, 4, 4, 5, 6],\n        }\n    )\n    with tm.assert_produces_warning(FutureWarning, match=\"Select only valid\"):\n        result = df.groupby(\"foo1\", as_index=False).apply(lambda x: x.mean())\n    expected = DataFrame({\"foo1\": [\"one\", \"three\", \"two\"], \"foo2\": [3.0, 4.0, 4.0]})\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_apply_with_timezones_aware():\n    # GH: 27212\n    dates = [\"2001-01-01\"] * 2 + [\"2001-01-02\"] * 2 + [\"2001-01-03\"] * 2\n    index_no_tz = pd.DatetimeIndex(dates)\n    index_tz = pd.DatetimeIndex(dates, tz=\"UTC\")\n    df1 = DataFrame({\"x\": list(range(2)) * 3, \"y\": range(6), \"t\": index_no_tz})\n    df2 = DataFrame({\"x\": list(range(2)) * 3, \"y\": range(6), \"t\": index_tz})\n\n    result1 = df1.groupby(\"x\", group_keys=False).apply(lambda df: df[[\"x\", \"y\"]].copy())\n    result2 = df2.groupby(\"x\", group_keys=False).apply(lambda df: df[[\"x\", \"y\"]].copy())\n\n    tm.assert_frame_equal(result1, result2)\n\n\ndef test_apply_is_unchanged_when_other_methods_are_called_first(reduction_func):\n    # GH #34656\n    # GH #34271\n    df = DataFrame(\n        {\n            \"a\": [99, 99, 99, 88, 88, 88],\n            \"b\": [1, 2, 3, 4, 5, 6],\n            \"c\": [10, 20, 30, 40, 50, 60],\n        }\n    )\n\n    expected = DataFrame(\n        {\"a\": [264, 297], \"b\": [15, 6], \"c\": [150, 60]},\n        index=Index([88, 99], name=\"a\"),\n    )\n\n    # Check output when no other methods are called before .apply()\n    grp = df.groupby(by=\"a\")\n    result = grp.apply(sum)\n    tm.assert_frame_equal(result, expected)\n\n    # Check output when another method is called before .apply()\n    grp = df.groupby(by=\"a\")\n    args = {\"nth\": [0], \"corrwith\": [df]}.get(reduction_func, [])\n    _ = getattr(grp, reduction_func)(*args)\n    result = grp.apply(sum)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_apply_with_date_in_multiindex_does_not_convert_to_timestamp():\n    # GH 29617\n\n    df = DataFrame(\n        {\n            \"A\": [\"a\", \"a\", \"a\", \"b\"],\n            \"B\": [\n                date(2020, 1, 10),\n                date(2020, 1, 10),\n                date(2020, 2, 10),\n                date(2020, 2, 10),\n            ],\n            \"C\": [1, 2, 3, 4],\n        },\n        index=Index([100, 101, 102, 103], name=\"idx\"),\n    )\n\n    grp = df.groupby([\"A\", \"B\"])\n    result = grp.apply(lambda x: x.head(1))\n\n    expected = df.iloc[[0, 2, 3]]\n    expected = expected.reset_index()\n    expected.index = MultiIndex.from_frame(expected[[\"A\", \"B\", \"idx\"]])\n    expected = expected.drop(columns=\"idx\")\n\n    tm.assert_frame_equal(result, expected)\n    for val in result.index.levels[1]:\n        assert type(val) is date\n\n\ndef test_apply_by_cols_equals_apply_by_rows_transposed():\n    # GH 16646\n    # Operating on the columns, or transposing and operating on the rows\n    # should give the same result. There was previously a bug where the\n    # by_rows operation would work fine, but by_cols would throw a ValueError\n\n    df = DataFrame(\n        np.random.random([6, 4]),\n        columns=MultiIndex.from_product([[\"A\", \"B\"], [1, 2]]),\n    )\n\n    by_rows = df.T.groupby(axis=0, level=0).apply(\n        lambda x: x.droplevel(axis=0, level=0)\n    )\n    by_cols = df.groupby(axis=1, level=0).apply(lambda x: x.droplevel(axis=1, level=0))\n\n    tm.assert_frame_equal(by_cols, by_rows.T)\n    tm.assert_frame_equal(by_cols, df)\n\n\ndef test_apply_dropna_with_indexed_same():\n    # GH 38227\n\n    df = DataFrame(\n        {\n            \"col\": [1, 2, 3, 4, 5],\n            \"group\": [\"a\", np.nan, np.nan, \"b\", \"b\"],\n        },\n        index=list(\"xxyxz\"),\n    )\n    result = df.groupby(\"group\").apply(lambda x: x)\n    expected = DataFrame(\n        {\n            \"col\": [1, 4, 5],\n            \"group\": [\"a\", \"b\", \"b\"],\n        },\n        index=list(\"xxz\"),\n    )\n\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"as_index, expected\",\n    [\n        [\n            False,\n            DataFrame(\n                [[1, 1, 1], [2, 2, 1]], columns=Index([\"a\", \"b\", None], dtype=object)\n            ),\n        ],\n        [\n            True,\n            Series(\n                [1, 1], index=MultiIndex.from_tuples([(1, 1), (2, 2)], names=[\"a\", \"b\"])\n            ),\n        ],\n    ],\n)\ndef test_apply_as_index_constant_lambda(as_index, expected):\n    # GH 13217\n    df = DataFrame({\"a\": [1, 1, 2, 2], \"b\": [1, 1, 2, 2], \"c\": [1, 1, 1, 1]})\n    result = df.groupby([\"a\", \"b\"], as_index=as_index).apply(lambda x: 1)\n    tm.assert_equal(result, expected)\n\n\ndef test_sort_index_groups():\n    # GH 20420\n    df = DataFrame(\n        {\"A\": [1, 2, 3, 4, 5], \"B\": [6, 7, 8, 9, 0], \"C\": [1, 1, 1, 2, 2]},\n        index=range(5),\n    )\n    result = df.groupby(\"C\").apply(lambda x: x.A.sort_index())\n    expected = Series(\n        range(1, 6),\n        index=MultiIndex.from_tuples(\n            [(1, 0), (1, 1), (1, 2), (2, 3), (2, 4)], names=[\"C\", None]\n        ),\n        name=\"A\",\n    )\n    tm.assert_series_equal(result, expected)\n\n\ndef test_positional_slice_groups_datetimelike():\n    # GH 21651\n    expected = DataFrame(\n        {\n            \"date\": pd.date_range(\"2010-01-01\", freq=\"12H\", periods=5),\n            \"vals\": range(5),\n            \"let\": list(\"abcde\"),\n        }\n    )\n    result = expected.groupby([expected.let, expected.date.dt.date]).apply(\n        lambda x: x.iloc[0:]\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_doctest_example2():\n    # GH#42702 this fails if we cache_readonly Block.shape\n    # TODO: more informative name\n    df = DataFrame({\"A\": [\"a\", \"a\", \"b\"], \"B\": [1, 2, 3], \"C\": [4, 6, 5]})\n    gb = df.groupby(\"A\")\n    result = gb[[\"B\", \"C\"]].apply(lambda x: x.astype(float).max() - x.min())\n\n    expected = DataFrame(\n        {\"B\": [1.0, 0.0], \"C\": [2.0, 0.0]}, index=Index([\"a\", \"b\"], name=\"A\")\n    )\n    tm.assert_frame_equal(result, expected)\n"
    }
  ]
}