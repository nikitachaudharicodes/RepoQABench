{
  "repo_name": "scrapy_scrapy",
  "issue_id": "3331",
  "issue_description": "# Documentation example fails with `proxy URL with no authority`\n\nRunning the [example](https://doc.scrapy.org/en/1.5/intro/overview.html#walk-through-of-an-example-spider) from the documentation yields this:\r\n```\r\n10:11 $ scrapy runspider quotes.py \r\n2018-07-11 10:12:04 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-07-11 10:12:04 [scrapy.utils.log] INFO: Versions: lxml 3.5.0.0, libxml2 2.9.3, cssselect 0.9.1, parsel 1.5.0, w3lib 1.19.0, Twisted 16.0.0, Python 2.7.12 (default, Dec  4 2017, 14:50:18) - [GCC 5.4.0 20160609], pyOpenSSL 0.15.1 (OpenSSL 1.0.2g  1 Mar 2016), cryptography 1.2.3, Platform Linux-4.4.0-130-generic-x86_64-with-Ubuntu-16.04-xenial\r\n2018-07-11 10:12:04 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}\r\n2018-07-11 10:12:04 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.corestats.CoreStats']\r\nUnhandled error in Deferred:\r\n2018-07-11 10:12:04 [twisted] CRITICAL: Unhandled error in Deferred:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/runspider.py\", line 88, in run\r\n    self.crawler_process.crawl(spidercls, **opts.spargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 171, in crawl\r\n    return self._crawl(crawler, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 175, in _crawl\r\n    d = crawler.crawl(*args, **kwargs)\r\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1274, in unwindGenerator\r\n    return _inlineCallbacks(None, gen, Deferred())\r\n--- <exception caught here> ---\r\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1128, in _inlineCallbacks\r\n    result = g.send(result)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 98, in crawl\r\n    six.reraise(*exc_info)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 80, in crawl\r\n    self.engine = self._create_engine()\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 105, in _create_engine\r\n    return ExecutionEngine(self, lambda _: self.stop())\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 69, in __init__\r\n    self.downloader = downloader_cls(crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/downloader/__init__.py\", line 88, in __init__\r\n    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py\", line 58, in from_crawler\r\n    return cls.from_settings(crawler.settings, crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py\", line 36, in from_settings\r\n    mw = mwcls.from_crawler(crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 29, in from_crawler\r\n    return cls(auth_encoding)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 22, in __init__\r\n    self.proxies[type] = self._get_proxy(url, type)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 39, in _get_proxy\r\n    proxy_type, user, password, hostport = _parse_proxy(url)\r\n  File \"/usr/lib/python2.7/urllib2.py\", line 721, in _parse_proxy\r\n    raise ValueError(\"proxy URL with no authority: %r\" % proxy)\r\nexceptions.ValueError: proxy URL with no authority: '/var/run/docker.sock'\r\n2018-07-11 10:12:04 [twisted] CRITICAL:\r\n```\r\nLooks like proxy code does not handle `no_proxy` correctly.",
  "issue_comments": [
    {
      "id": 404096282,
      "user": "grammy-jiang",
      "body": "Hi, @a-palchikov \r\n\r\nThis exception is reported by Python standard lib, not Scrapy.\r\n\r\nWould you mind to post your start_urls, and the environment variables about PROXY here?"
    },
    {
      "id": 519477571,
      "user": "Gallaecio",
      "body": "Closing due to lack of feedback from the author."
    },
    {
      "id": 654712494,
      "user": "otakutyrant",
      "body": "I have encountered this issue too. My relative proxy environment variable is `no_proxy=/var/run/docker.sock`, and after unsetting this one the issue is solved. So as the poster said, looks like proxy code does not handle `no_proxy` correctly."
    },
    {
      "id": 655334270,
      "user": "Gallaecio",
      "body": "The code probably does not expect a file path in that variable. I guess we should silently ignore those."
    },
    {
      "id": 678216640,
      "user": "kartecianos",
      "body": "Hi, I am a newcomer and I would like to take this issue"
    },
    {
      "id": 678240598,
      "user": "Gallaecio",
      "body": "No need to ask for permission :slightly_smiling_face: "
    },
    {
      "id": 678693819,
      "user": "drs-11",
      "body": "Also looks like ` _get_proxy` here doesn't handle multiple addresses in the `no_proxy` env variable well.\r\nFor eg: \r\nIf the `no_proxy` env var is set to: `no_proxy=\"127.0.0.1,localhost,localdomain.com\"`\r\nThen `self.proxies` in `HttpProxyMiddleware` will be set as:\r\n`{'no': (None, 'no://127.0.0.1,localhost,localdomain.com')}`\r\n\r\nThat's not how it should be, right?"
    },
    {
      "id": 680035295,
      "user": "Gallaecio",
      "body": "Probably no, indeed."
    },
    {
      "id": 687309743,
      "user": "drs-11",
      "body": "I'm not sure what could be a solution to this issue.\r\n`/var/run/docker.sock` seems the only exception for having a socket file in a `no_proxy` env variable. So either the socket file be ignored and not added to the list of proxies or maybe add it without passing the socket file path to `_get_proxy` method which is causing the error?\r\n\r\nBut the second option will cause further errors when the proxy is parsed in other modules.\r\nSo I think ignoring the socket file will be the best option? Also I can't find any other cases where a socket file is used in `no_proxy`.\r\nThoughts?\r\n\r\n"
    },
    {
      "id": 687323045,
      "user": "a-palchikov",
      "body": "I guess NO_PROXY handling is very open to specific interpretations and is not standardized. Docker client describes the uses of NO_PROXY for its purposes [here](https://github.com/moby/moby/pull/10192/files?short_path=c13a5c5#diff-c13a5c583fae16a859a034cdd06c7c58) while scrapy can just ignore the proxy that the `urllib2.parse_proxy` fails to parse."
    }
  ],
  "text_context": "# Documentation example fails with `proxy URL with no authority`\n\nRunning the [example](https://doc.scrapy.org/en/1.5/intro/overview.html#walk-through-of-an-example-spider) from the documentation yields this:\r\n```\r\n10:11 $ scrapy runspider quotes.py \r\n2018-07-11 10:12:04 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-07-11 10:12:04 [scrapy.utils.log] INFO: Versions: lxml 3.5.0.0, libxml2 2.9.3, cssselect 0.9.1, parsel 1.5.0, w3lib 1.19.0, Twisted 16.0.0, Python 2.7.12 (default, Dec  4 2017, 14:50:18) - [GCC 5.4.0 20160609], pyOpenSSL 0.15.1 (OpenSSL 1.0.2g  1 Mar 2016), cryptography 1.2.3, Platform Linux-4.4.0-130-generic-x86_64-with-Ubuntu-16.04-xenial\r\n2018-07-11 10:12:04 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}\r\n2018-07-11 10:12:04 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.corestats.CoreStats']\r\nUnhandled error in Deferred:\r\n2018-07-11 10:12:04 [twisted] CRITICAL: Unhandled error in Deferred:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/runspider.py\", line 88, in run\r\n    self.crawler_process.crawl(spidercls, **opts.spargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 171, in crawl\r\n    return self._crawl(crawler, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 175, in _crawl\r\n    d = crawler.crawl(*args, **kwargs)\r\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1274, in unwindGenerator\r\n    return _inlineCallbacks(None, gen, Deferred())\r\n--- <exception caught here> ---\r\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1128, in _inlineCallbacks\r\n    result = g.send(result)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 98, in crawl\r\n    six.reraise(*exc_info)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 80, in crawl\r\n    self.engine = self._create_engine()\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 105, in _create_engine\r\n    return ExecutionEngine(self, lambda _: self.stop())\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 69, in __init__\r\n    self.downloader = downloader_cls(crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/downloader/__init__.py\", line 88, in __init__\r\n    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py\", line 58, in from_crawler\r\n    return cls.from_settings(crawler.settings, crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py\", line 36, in from_settings\r\n    mw = mwcls.from_crawler(crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 29, in from_crawler\r\n    return cls(auth_encoding)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 22, in __init__\r\n    self.proxies[type] = self._get_proxy(url, type)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 39, in _get_proxy\r\n    proxy_type, user, password, hostport = _parse_proxy(url)\r\n  File \"/usr/lib/python2.7/urllib2.py\", line 721, in _parse_proxy\r\n    raise ValueError(\"proxy URL with no authority: %r\" % proxy)\r\nexceptions.ValueError: proxy URL with no authority: '/var/run/docker.sock'\r\n2018-07-11 10:12:04 [twisted] CRITICAL:\r\n```\r\nLooks like proxy code does not handle `no_proxy` correctly.\n\nHi, @a-palchikov \r\n\r\nThis exception is reported by Python standard lib, not Scrapy.\r\n\r\nWould you mind to post your start_urls, and the environment variables about PROXY here?\n\nClosing due to lack of feedback from the author.\n\nI have encountered this issue too. My relative proxy environment variable is `no_proxy=/var/run/docker.sock`, and after unsetting this one the issue is solved. So as the poster said, looks like proxy code does not handle `no_proxy` correctly.\n\nThe code probably does not expect a file path in that variable. I guess we should silently ignore those.\n\nHi, I am a newcomer and I would like to take this issue\n\nNo need to ask for permission :slightly_smiling_face: \n\nAlso looks like ` _get_proxy` here doesn't handle multiple addresses in the `no_proxy` env variable well.\r\nFor eg: \r\nIf the `no_proxy` env var is set to: `no_proxy=\"127.0.0.1,localhost,localdomain.com\"`\r\nThen `self.proxies` in `HttpProxyMiddleware` will be set as:\r\n`{'no': (None, 'no://127.0.0.1,localhost,localdomain.com')}`\r\n\r\nThat's not how it should be, right?\n\nProbably no, indeed.\n\nI'm not sure what could be a solution to this issue.\r\n`/var/run/docker.sock` seems the only exception for having a socket file in a `no_proxy` env variable. So either the socket file be ignored and not added to the list of proxies or maybe add it without passing the socket file path to `_get_proxy` method which is causing the error?\r\n\r\nBut the second option will cause further errors when the proxy is parsed in other modules.\r\nSo I think ignoring the socket file will be the best option? Also I can't find any other cases where a socket file is used in `no_proxy`.\r\nThoughts?\r\n\r\n\n\nI guess NO_PROXY handling is very open to specific interpretations and is not standardized. Docker client describes the uses of NO_PROXY for its purposes [here](https://github.com/moby/moby/pull/10192/files?short_path=c13a5c5#diff-c13a5c583fae16a859a034cdd06c7c58) while scrapy can just ignore the proxy that the `urllib2.parse_proxy` fails to parse.",
  "pr_link": "https://github.com/moby/moby/pull/10192",
  "code_context": []
}