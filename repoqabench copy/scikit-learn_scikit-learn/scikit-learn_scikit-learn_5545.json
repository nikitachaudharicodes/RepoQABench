{
  "repo_name": "scikit-learn_scikit-learn",
  "issue_id": "5545",
  "issue_description": "# ENH: make consistent interface for scipy's eigs, eigsh and svds with random start\n\nRecent PRs #5012 #5243 introduce random seeding for those scipy calls to ARPACK, that are otherwise non-deterministic. We should make them consistent across the code base. Somewhere in `sklearn.utils` is the place.\n\nThe interface should also be extended to every call we make to those functions in sklearn.\n",
  "issue_comments": [
    {
      "id": 150499244,
      "user": "giorgiop",
      "body": "Ping @yanlend in case you would like to take care of this. #5243 is yet to be merged though.\nPing @ogrisel for discussion.\n"
    },
    {
      "id": 402638445,
      "user": "ogrisel",
      "body": "Sounds like a good idea and potentially a good task for a first-time contributor to scikit-learn.\r\n\r\nIf you are interested, please first try to fully read the diffs and the discussions in the #5012 PR and the scipy documentation on `eigs`, `eighs` and `svds` to understand the context and get familiar with the relevant part of the code base.\r\n\r\nThen use commands such as `git grep eigs`, `git grep eigsh`, `git grep svds` and `git grep \"v0=\"` to find all the parts of the scikit-learn code base that could benefit from factorizing that initialization strategy in a utility function such as `sklearn.utils._init_arpack_v0(size, random_state)` that could be used to generate the `v0` vector correctly whenever we use the `eigsh` and `svds` solvers in scikit-learn.\r\n\r\nIt's important to also list the uses of `eigs`, `eigsh` and `svds` that currently do not use deterministic seeding by passing the `v0` option."
    },
    {
      "id": 402692836,
      "user": "ogrisel",
      "body": "Also, it might be interesting to also contact discuss with upstream scipy on whether or not they would like to add an optional parameter named `seed` or `random_state`  that would be used to initialize `v0` when `v0` is not explicitly passed by the caller."
    },
    {
      "id": 402699944,
      "user": "summer-bebop",
      "body": "Hi. Since it's a good task for a first timer I would be glad to follow the guidelines you provided and tackle this issue. Would it be ok ?"
    },
    {
      "id": 402720279,
      "user": "ogrisel",
      "body": "Sure: make sure you understand everything I mentioned above, no need to ask for permission."
    },
    {
      "id": 402732956,
      "user": "ogrisel",
      "body": "Also read about: http://scikit-learn.org/dev/developers/contributing.html#random-numbers"
    },
    {
      "id": 559665927,
      "user": "imnotaqtpie",
      "body": "hi @ogrisel this seems like a good place to get started and since its been over a year since the last activity, is it ok for me to take this up?"
    },
    {
      "id": 559823474,
      "user": "jeremiedbb",
      "body": "Most of the work has been done in #11524 besides a couple comments. You can take over his work (I can speak for him :) ). I advise you to read the discussion in #11524 to see why some choices have been made and how to resolve the comments that have not been addressed."
    },
    {
      "id": 559823842,
      "user": "summer-bebop",
      "body": "Indeed don't hesitate, my schedule is painful and this should be merged for\nages... Thanks for your help\n\nLe ven. 29 nov. 2019 à 16:27, Jérémie du Boisberranger <\nnotifications@github.com> a écrit :\n\n> Most of the work has been done in #11524\n> <https://github.com/scikit-learn/scikit-learn/pull/11524> besides a\n> couple comments. You can take over his work (I can speak for him :) ). I\n> advise you to read the discussion in #11524\n> <https://github.com/scikit-learn/scikit-learn/pull/11524> to see why some\n> choices have been made and how to resolve the comments that have not been\n> addressed.\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/5545?email_source=notifications&email_token=AFITJHGOXL2AHTYYH7QEBPDQWEYFTA5CNFSM4BSRLAKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEFPDU4Q#issuecomment-559823474>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFITJHAEGGTCY2NJ6PQQYA3QWEYFTANCNFSM4BSRLAKA>\n> .\n>\n"
    },
    {
      "id": 560111836,
      "user": "imnotaqtpie",
      "body": "sure thing, ill take a look at the comments and finish up what i can. \r\nthanks"
    },
    {
      "id": 660296721,
      "user": "gauravkdesai",
      "body": "@ogrisel @jeremiedbb @FollowKenny does this issue needs any help? I would like this to be my first contribution if possible."
    },
    {
      "id": 661055477,
      "user": "jeremiedbb",
      "body": "Thanks @gauravkdesai. You can take over #11524 which is stalled but almost ready. You'd probably have to fix some merge conflicts. There are also a few comments left to address."
    },
    {
      "id": 661065099,
      "user": "gauravkdesai",
      "body": "@jeremiedbb thanks. "
    },
    {
      "id": 683387742,
      "user": "gauravkdesai",
      "body": "take"
    },
    {
      "id": 734931029,
      "user": "giorgiop",
      "body": "WOW!"
    }
  ],
  "text_context": "# ENH: make consistent interface for scipy's eigs, eigsh and svds with random start\n\nRecent PRs #5012 #5243 introduce random seeding for those scipy calls to ARPACK, that are otherwise non-deterministic. We should make them consistent across the code base. Somewhere in `sklearn.utils` is the place.\n\nThe interface should also be extended to every call we make to those functions in sklearn.\n\n\nPing @yanlend in case you would like to take care of this. #5243 is yet to be merged though.\nPing @ogrisel for discussion.\n\n\nSounds like a good idea and potentially a good task for a first-time contributor to scikit-learn.\r\n\r\nIf you are interested, please first try to fully read the diffs and the discussions in the #5012 PR and the scipy documentation on `eigs`, `eighs` and `svds` to understand the context and get familiar with the relevant part of the code base.\r\n\r\nThen use commands such as `git grep eigs`, `git grep eigsh`, `git grep svds` and `git grep \"v0=\"` to find all the parts of the scikit-learn code base that could benefit from factorizing that initialization strategy in a utility function such as `sklearn.utils._init_arpack_v0(size, random_state)` that could be used to generate the `v0` vector correctly whenever we use the `eigsh` and `svds` solvers in scikit-learn.\r\n\r\nIt's important to also list the uses of `eigs`, `eigsh` and `svds` that currently do not use deterministic seeding by passing the `v0` option.\n\nAlso, it might be interesting to also contact discuss with upstream scipy on whether or not they would like to add an optional parameter named `seed` or `random_state`  that would be used to initialize `v0` when `v0` is not explicitly passed by the caller.\n\nHi. Since it's a good task for a first timer I would be glad to follow the guidelines you provided and tackle this issue. Would it be ok ?\n\nSure: make sure you understand everything I mentioned above, no need to ask for permission.\n\nAlso read about: http://scikit-learn.org/dev/developers/contributing.html#random-numbers\n\nhi @ogrisel this seems like a good place to get started and since its been over a year since the last activity, is it ok for me to take this up?\n\nMost of the work has been done in #11524 besides a couple comments. You can take over his work (I can speak for him :) ). I advise you to read the discussion in #11524 to see why some choices have been made and how to resolve the comments that have not been addressed.\n\nIndeed don't hesitate, my schedule is painful and this should be merged for\nages... Thanks for your help\n\nLe ven. 29 nov. 2019 à 16:27, Jérémie du Boisberranger <\nnotifications@github.com> a écrit :\n\n> Most of the work has been done in #11524\n> <https://github.com/scikit-learn/scikit-learn/pull/11524> besides a\n> couple comments. You can take over his work (I can speak for him :) ). I\n> advise you to read the discussion in #11524\n> <https://github.com/scikit-learn/scikit-learn/pull/11524> to see why some\n> choices have been made and how to resolve the comments that have not been\n> addressed.\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/5545?email_source=notifications&email_token=AFITJHGOXL2AHTYYH7QEBPDQWEYFTA5CNFSM4BSRLAKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEFPDU4Q#issuecomment-559823474>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFITJHAEGGTCY2NJ6PQQYA3QWEYFTANCNFSM4BSRLAKA>\n> .\n>\n\n\nsure thing, ill take a look at the comments and finish up what i can. \r\nthanks\n\n@ogrisel @jeremiedbb @FollowKenny does this issue needs any help? I would like this to be my first contribution if possible.\n\nThanks @gauravkdesai. You can take over #11524 which is stalled but almost ready. You'd probably have to fix some merge conflicts. There are also a few comments left to address.\n\n@jeremiedbb thanks. \n\ntake\n\nWOW!",
  "pr_link": "https://github.com/scikit-learn/scikit-learn/pull/11524",
  "code_context": [
    {
      "filename": "benchmarks/bench_plot_randomized_svd.py",
      "content": "\"\"\"\nBenchmarks on the power iterations phase in randomized SVD.\n\nWe test on various synthetic and real datasets the effect of increasing\nthe number of power iterations in terms of quality of approximation\nand running time. A number greater than 0 should help with noisy matrices,\nwhich are characterized by a slow spectral decay.\n\nWe test several policy for normalizing the power iterations. Normalization\nis crucial to avoid numerical issues.\n\nThe quality of the approximation is measured by the spectral norm discrepancy\nbetween the original input matrix and the reconstructed one (by multiplying\nthe randomized_svd's outputs). The spectral norm is always equivalent to the\nlargest singular value of a matrix. (3) justifies this choice. However, one can\nnotice in these experiments that Frobenius and spectral norms behave\nvery similarly in a qualitative sense. Therefore, we suggest to run these\nbenchmarks with `enable_spectral_norm = False`, as Frobenius' is MUCH faster to\ncompute.\n\nThe benchmarks follow.\n\n(a) plot: time vs norm, varying number of power iterations\n    data: many datasets\n    goal: compare normalization policies and study how the number of power\n    iterations affect time and norm\n\n(b) plot: n_iter vs norm, varying rank of data and number of components for\n    randomized_SVD\n    data: low-rank matrices on which we control the rank\n    goal: study whether the rank of the matrix and the number of components\n    extracted by randomized SVD affect \"the optimal\" number of power iterations\n\n(c) plot: time vs norm, varying datasets\n    data: many datasets\n    goal: compare default configurations\n\nWe compare the following algorithms:\n-   randomized_svd(..., power_iteration_normalizer='none')\n-   randomized_svd(..., power_iteration_normalizer='LU')\n-   randomized_svd(..., power_iteration_normalizer='QR')\n-   randomized_svd(..., power_iteration_normalizer='auto')\n-   fbpca.pca() from https://github.com/facebook/fbpca (if installed)\n\nConclusion\n----------\n- n_iter=2 appears to be a good default value\n- power_iteration_normalizer='none' is OK if n_iter is small, otherwise LU\n  gives similar errors to QR but is cheaper. That's what 'auto' implements.\n\nReferences\n----------\n(1) Finding structure with randomness: Stochastic algorithms for constructing\n    approximate matrix decompositions\n    Halko, et al., 2009 https://arxiv.org/abs/0909.4061\n\n(2) A randomized algorithm for the decomposition of matrices\n    Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert\n\n(3) An implementation of a randomized algorithm for principal component\n    analysis\n    A. Szlam et al. 2014\n\"\"\"\n\n# Author: Giorgio Patrini\n\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\n\nimport gc\nimport pickle\nfrom time import time\nfrom collections import defaultdict\nimport os.path\n\nfrom sklearn.utils import gen_batches, _init_arpack_v0\nfrom sklearn.utils.validation import check_random_state\nfrom sklearn.utils.extmath import randomized_svd\nfrom sklearn.datasets.samples_generator import (make_low_rank_matrix,\n                                                make_sparse_uncorrelated)\nfrom sklearn.datasets import (fetch_lfw_people,\n                              fetch_mldata,\n                              fetch_20newsgroups_vectorized,\n                              fetch_olivetti_faces,\n                              fetch_rcv1)\n\ntry:\n    import fbpca\n    fbpca_available = True\nexcept ImportError:\n    fbpca_available = False\n\n# If this is enabled, tests are much slower and will crash with the large data\nenable_spectral_norm = False\n\n# TODO: compute approximate spectral norms with the power method as in\n# Estimating the largest eigenvalues by the power and Lanczos methods with\n# a random start, Jacek Kuczynski and Henryk Wozniakowski, SIAM Journal on\n# Matrix Analysis and Applications, 13 (4): 1094-1122, 1992.\n# This approximation is a very fast estimate of the spectral norm, but depends\n# on starting random vectors.\n\n# Determine when to switch to batch computation for matrix norms,\n# in case the reconstructed (dense) matrix is too large\nMAX_MEMORY = np.int(2e9)\n\n# The following datasets can be dowloaded manually from:\n# CIFAR 10: https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n# SVHN: http://ufldl.stanford.edu/housenumbers/train_32x32.mat\nCIFAR_FOLDER = \"./cifar-10-batches-py/\"\nSVHN_FOLDER = \"./SVHN/\"\n\ndatasets = ['low rank matrix', 'lfw_people', 'olivetti_faces', '20newsgroups',\n            'MNIST original', 'CIFAR', 'a1a', 'SVHN', 'uncorrelated matrix']\n\nbig_sparse_datasets = ['big sparse matrix', 'rcv1']\n\n\ndef unpickle(file_name):\n    with open(file_name, 'rb') as fo:\n        return pickle.load(fo, encoding='latin1')[\"data\"]\n\n\ndef handle_missing_dataset(file_folder):\n    if not os.path.isdir(file_folder):\n        print(\"%s file folder not found. Test skipped.\" % file_folder)\n        return 0\n\n\ndef get_data(dataset_name):\n    print(\"Getting dataset: %s\" % dataset_name)\n\n    if dataset_name == 'lfw_people':\n        X = fetch_lfw_people().data\n    elif dataset_name == '20newsgroups':\n        X = fetch_20newsgroups_vectorized().data[:, :100000]\n    elif dataset_name == 'olivetti_faces':\n        X = fetch_olivetti_faces().data\n    elif dataset_name == 'rcv1':\n        X = fetch_rcv1().data\n    elif dataset_name == 'CIFAR':\n        if handle_missing_dataset(CIFAR_FOLDER) == \"skip\":\n            return\n        X1 = [unpickle(\"%sdata_batch_%d\" % (CIFAR_FOLDER, i + 1))\n              for i in range(5)]\n        X = np.vstack(X1)\n        del X1\n    elif dataset_name == 'SVHN':\n        if handle_missing_dataset(SVHN_FOLDER) == 0:\n            return\n        X1 = sp.io.loadmat(\"%strain_32x32.mat\" % SVHN_FOLDER)['X']\n        X2 = [X1[:, :, :, i].reshape(32 * 32 * 3) for i in range(X1.shape[3])]\n        X = np.vstack(X2)\n        del X1\n        del X2\n    elif dataset_name == 'low rank matrix':\n        X = make_low_rank_matrix(n_samples=500, n_features=np.int(1e4),\n                                 effective_rank=100, tail_strength=.5,\n                                 random_state=random_state)\n    elif dataset_name == 'uncorrelated matrix':\n        X, _ = make_sparse_uncorrelated(n_samples=500, n_features=10000,\n                                        random_state=random_state)\n    elif dataset_name == 'big sparse matrix':\n        sparsity = np.int(1e6)\n        size = np.int(1e6)\n        small_size = np.int(1e4)\n        data = np.random.normal(0, 1, np.int(sparsity/10))\n        data = np.repeat(data, 10)\n        row = np.random.uniform(0, small_size, sparsity)\n        col = np.random.uniform(0, small_size, sparsity)\n        X = sp.sparse.csr_matrix((data, (row, col)), shape=(size, small_size))\n        del data\n        del row\n        del col\n    else:\n        X = fetch_mldata(dataset_name).data\n    return X\n\n\ndef plot_time_vs_s(time, norm, point_labels, title):\n    plt.figure()\n    colors = ['g', 'b', 'y']\n    for i, l in enumerate(sorted(norm.keys())):\n        if l != \"fbpca\":\n            plt.plot(time[l], norm[l], label=l, marker='o', c=colors.pop())\n        else:\n            plt.plot(time[l], norm[l], label=l, marker='^', c='red')\n\n        for label, x, y in zip(point_labels, list(time[l]), list(norm[l])):\n            plt.annotate(label, xy=(x, y), xytext=(0, -20),\n                         textcoords='offset points', ha='right', va='bottom')\n    plt.legend(loc=\"upper right\")\n    plt.suptitle(title)\n    plt.ylabel(\"norm discrepancy\")\n    plt.xlabel(\"running time [s]\")\n\n\ndef scatter_time_vs_s(time, norm, point_labels, title):\n    plt.figure()\n    size = 100\n    for i, l in enumerate(sorted(norm.keys())):\n        if l != \"fbpca\":\n            plt.scatter(time[l], norm[l], label=l, marker='o', c='b', s=size)\n            for label, x, y in zip(point_labels, list(time[l]), list(norm[l])):\n                plt.annotate(label, xy=(x, y), xytext=(0, -80),\n                             textcoords='offset points', ha='right',\n                             arrowprops=dict(arrowstyle=\"->\",\n                                             connectionstyle=\"arc3\"),\n                             va='bottom', size=11, rotation=90)\n        else:\n            plt.scatter(time[l], norm[l], label=l, marker='^', c='red', s=size)\n            for label, x, y in zip(point_labels, list(time[l]), list(norm[l])):\n                plt.annotate(label, xy=(x, y), xytext=(0, 30),\n                             textcoords='offset points', ha='right',\n                             arrowprops=dict(arrowstyle=\"->\",\n                                             connectionstyle=\"arc3\"),\n                             va='bottom', size=11, rotation=90)\n\n    plt.legend(loc=\"best\")\n    plt.suptitle(title)\n    plt.ylabel(\"norm discrepancy\")\n    plt.xlabel(\"running time [s]\")\n\n\ndef plot_power_iter_vs_s(power_iter, s, title):\n    plt.figure()\n    for l in sorted(s.keys()):\n        plt.plot(power_iter, s[l], label=l, marker='o')\n    plt.legend(loc=\"lower right\", prop={'size': 10})\n    plt.suptitle(title)\n    plt.ylabel(\"norm discrepancy\")\n    plt.xlabel(\"n_iter\")\n\n\ndef svd_timing(X, n_comps, n_iter, n_oversamples,\n               power_iteration_normalizer='auto', method=None):\n    \"\"\"\n    Measure time for decomposition\n    \"\"\"\n    print(\"... running SVD ...\")\n    if method is not 'fbpca':\n        gc.collect()\n        t0 = time()\n        U, mu, V = randomized_svd(X, n_comps, n_oversamples, n_iter,\n                                  power_iteration_normalizer,\n                                  random_state=random_state, transpose=False)\n        call_time = time() - t0\n    else:\n        gc.collect()\n        t0 = time()\n        # There is a different convention for l here\n        U, mu, V = fbpca.pca(X, n_comps, raw=True, n_iter=n_iter,\n                             l=n_oversamples+n_comps)\n        call_time = time() - t0\n\n    return U, mu, V, call_time\n\n\ndef norm_diff(A, norm=2, msg=True, random_state=None):\n    \"\"\"\n    Compute the norm diff with the original matrix, when randomized\n    SVD is called with *params.\n\n    norm: 2 => spectral; 'fro' => Frobenius\n    \"\"\"\n\n    if msg:\n        print(\"... computing %s norm ...\" % norm)\n    if norm == 2:\n        # s = sp.linalg.norm(A, ord=2)  # slow\n        v0 = _init_arpack_v0(min(A.shape), random_state)\n        value = sp.sparse.linalg.svds(A,\n                                      k=1,\n                                      return_singular_vectors=False,\n                                      v0=v0)\n    else:\n        if sp.sparse.issparse(A):\n            value = sp.sparse.linalg.norm(A, ord=norm)\n        else:\n            value = sp.linalg.norm(A, ord=norm)\n    return value\n\n\ndef scalable_frobenius_norm_discrepancy(X, U, s, V):\n    # if the input is not too big, just call scipy\n    if X.shape[0] * X.shape[1] < MAX_MEMORY:\n        A = X - U.dot(np.diag(s).dot(V))\n        return norm_diff(A, norm='fro')\n\n    print(\"... computing fro norm by batches...\")\n    batch_size = 1000\n    Vhat = np.diag(s).dot(V)\n    cum_norm = .0\n    for batch in gen_batches(X.shape[0], batch_size):\n        M = X[batch, :] - U[batch, :].dot(Vhat)\n        cum_norm += norm_diff(M, norm='fro', msg=False)\n    return np.sqrt(cum_norm)\n\n\ndef bench_a(X, dataset_name, power_iter, n_oversamples, n_comps):\n\n    all_time = defaultdict(list)\n    if enable_spectral_norm:\n        all_spectral = defaultdict(list)\n        X_spectral_norm = norm_diff(X, norm=2, msg=False)\n    all_frobenius = defaultdict(list)\n    X_fro_norm = norm_diff(X, norm='fro', msg=False)\n\n    for pi in power_iter:\n        for pm in ['none', 'LU', 'QR']:\n            print(\"n_iter = %d on sklearn - %s\" % (pi, pm))\n            U, s, V, time = svd_timing(X, n_comps, n_iter=pi,\n                                       power_iteration_normalizer=pm,\n                                       n_oversamples=n_oversamples)\n            label = \"sklearn - %s\" % pm\n            all_time[label].append(time)\n            if enable_spectral_norm:\n                A = U.dot(np.diag(s).dot(V))\n                all_spectral[label].append(norm_diff(X - A, norm=2) /\n                                           X_spectral_norm)\n            f = scalable_frobenius_norm_discrepancy(X, U, s, V)\n            all_frobenius[label].append(f / X_fro_norm)\n\n        if fbpca_available:\n            print(\"n_iter = %d on fbca\" % (pi))\n            U, s, V, time = svd_timing(X, n_comps, n_iter=pi,\n                                       power_iteration_normalizer=pm,\n                                       n_oversamples=n_oversamples,\n                                       method='fbpca')\n            label = \"fbpca\"\n            all_time[label].append(time)\n            if enable_spectral_norm:\n                A = U.dot(np.diag(s).dot(V))\n                all_spectral[label].append(norm_diff(X - A, norm=2) /\n                                           X_spectral_norm)\n            f = scalable_frobenius_norm_discrepancy(X, U, s, V)\n            all_frobenius[label].append(f / X_fro_norm)\n\n    if enable_spectral_norm:\n        title = \"%s: spectral norm diff vs running time\" % (dataset_name)\n        plot_time_vs_s(all_time, all_spectral, power_iter, title)\n    title = \"%s: Frobenius norm diff vs running time\" % (dataset_name)\n    plot_time_vs_s(all_time, all_frobenius, power_iter, title)\n\n\ndef bench_b(power_list):\n\n    n_samples, n_features = 1000, 10000\n    data_params = {'n_samples': n_samples, 'n_features': n_features,\n                   'tail_strength': .7, 'random_state': random_state}\n    dataset_name = \"low rank matrix %d x %d\" % (n_samples, n_features)\n    ranks = [10, 50, 100]\n\n    if enable_spectral_norm:\n        all_spectral = defaultdict(list)\n    all_frobenius = defaultdict(list)\n    for rank in ranks:\n        X = make_low_rank_matrix(effective_rank=rank, **data_params)\n        if enable_spectral_norm:\n            X_spectral_norm = norm_diff(X, norm=2, msg=False)\n        X_fro_norm = norm_diff(X, norm='fro', msg=False)\n\n        for n_comp in [np.int(rank/2), rank, rank*2]:\n            label = \"rank=%d, n_comp=%d\" % (rank, n_comp)\n            print(label)\n            for pi in power_list:\n                U, s, V, _ = svd_timing(X, n_comp, n_iter=pi, n_oversamples=2,\n                                        power_iteration_normalizer='LU')\n                if enable_spectral_norm:\n                    A = U.dot(np.diag(s).dot(V))\n                    all_spectral[label].append(norm_diff(X - A, norm=2) /\n                                               X_spectral_norm)\n                f = scalable_frobenius_norm_discrepancy(X, U, s, V)\n                all_frobenius[label].append(f / X_fro_norm)\n\n    if enable_spectral_norm:\n        title = \"%s: spectral norm diff vs n power iteration\" % (dataset_name)\n        plot_power_iter_vs_s(power_iter, all_spectral, title)\n    title = \"%s: Frobenius norm diff vs n power iteration\" % (dataset_name)\n    plot_power_iter_vs_s(power_iter, all_frobenius, title)\n\n\ndef bench_c(datasets, n_comps):\n    all_time = defaultdict(list)\n    if enable_spectral_norm:\n        all_spectral = defaultdict(list)\n    all_frobenius = defaultdict(list)\n\n    for dataset_name in datasets:\n        X = get_data(dataset_name)\n        if X is None:\n            continue\n\n        if enable_spectral_norm:\n            X_spectral_norm = norm_diff(X, norm=2, msg=False)\n        X_fro_norm = norm_diff(X, norm='fro', msg=False)\n        n_comps = np.minimum(n_comps, np.min(X.shape))\n\n        label = \"sklearn\"\n        print(\"%s %d x %d - %s\" %\n              (dataset_name, X.shape[0], X.shape[1], label))\n        U, s, V, time = svd_timing(X, n_comps, n_iter=2, n_oversamples=10,\n                                   method=label)\n\n        all_time[label].append(time)\n        if enable_spectral_norm:\n            A = U.dot(np.diag(s).dot(V))\n            all_spectral[label].append(norm_diff(X - A, norm=2) /\n                                       X_spectral_norm)\n        f = scalable_frobenius_norm_discrepancy(X, U, s, V)\n        all_frobenius[label].append(f / X_fro_norm)\n\n        if fbpca_available:\n            label = \"fbpca\"\n            print(\"%s %d x %d - %s\" %\n                  (dataset_name, X.shape[0], X.shape[1], label))\n            U, s, V, time = svd_timing(X, n_comps, n_iter=2, n_oversamples=2,\n                                       method=label)\n            all_time[label].append(time)\n            if enable_spectral_norm:\n                A = U.dot(np.diag(s).dot(V))\n                all_spectral[label].append(norm_diff(X - A, norm=2) /\n                                           X_spectral_norm)\n            f = scalable_frobenius_norm_discrepancy(X, U, s, V)\n            all_frobenius[label].append(f / X_fro_norm)\n\n    if len(all_time) == 0:\n        raise ValueError(\"No tests ran. Aborting.\")\n\n    if enable_spectral_norm:\n        title = \"normalized spectral norm diff vs running time\"\n        scatter_time_vs_s(all_time, all_spectral, datasets, title)\n    title = \"normalized Frobenius norm diff vs running time\"\n    scatter_time_vs_s(all_time, all_frobenius, datasets, title)\n\n\nif __name__ == '__main__':\n    random_state = check_random_state(1234)\n\n    power_iter = np.linspace(0, 6, 7, dtype=int)\n    n_comps = 50\n\n    for dataset_name in datasets:\n        X = get_data(dataset_name)\n        if X is None:\n            continue\n        print(\" >>>>>> Benching sklearn and fbpca on %s %d x %d\" %\n              (dataset_name, X.shape[0], X.shape[1]))\n        bench_a(X, dataset_name, power_iter, n_oversamples=2,\n                n_comps=np.minimum(n_comps, np.min(X.shape)))\n\n    print(\" >>>>>> Benching on simulated low rank matrix with variable rank\")\n    bench_b(power_iter)\n\n    print(\" >>>>>> Benching sklearn and fbpca default configurations\")\n    bench_c(datasets + big_sparse_datasets, n_comps)\n\n    plt.show()\n"
    },
    {
      "filename": "sklearn/cluster/bicluster.py",
      "content": "\"\"\"Spectral biclustering algorithms.\n\nAuthors : Kemal Eren\nLicense: BSD 3 clause\n\n\"\"\"\nfrom abc import ABCMeta, abstractmethod\n\nimport numpy as np\n\nfrom scipy.linalg import norm\nfrom scipy.sparse import dia_matrix, issparse\nfrom scipy.sparse.linalg import eigsh, svds\n\nfrom . import KMeans, MiniBatchKMeans\nfrom ..base import BaseEstimator, BiclusterMixin\nfrom ..externals import six\nfrom ..utils.arpack import _init_arpack_v0\n\nfrom ..utils.extmath import (make_nonnegative, randomized_svd,\n                             safe_sparse_dot)\n\nfrom ..utils.validation import assert_all_finite, check_array\n\n\n__all__ = ['SpectralCoclustering',\n           'SpectralBiclustering']\n\n\ndef _scale_normalize(X):\n    \"\"\"Normalize ``X`` by scaling rows and columns independently.\n\n    Returns the normalized matrix and the row and column scaling\n    factors.\n\n    \"\"\"\n    X = make_nonnegative(X)\n    row_diag = np.asarray(1.0 / np.sqrt(X.sum(axis=1))).squeeze()\n    col_diag = np.asarray(1.0 / np.sqrt(X.sum(axis=0))).squeeze()\n    row_diag = np.where(np.isnan(row_diag), 0, row_diag)\n    col_diag = np.where(np.isnan(col_diag), 0, col_diag)\n    if issparse(X):\n        n_rows, n_cols = X.shape\n        r = dia_matrix((row_diag, [0]), shape=(n_rows, n_rows))\n        c = dia_matrix((col_diag, [0]), shape=(n_cols, n_cols))\n        an = r * X * c\n    else:\n        an = row_diag[:, np.newaxis] * X * col_diag\n    return an, row_diag, col_diag\n\n\ndef _bistochastic_normalize(X, max_iter=1000, tol=1e-5):\n    \"\"\"Normalize rows and columns of ``X`` simultaneously so that all\n    rows sum to one constant and all columns sum to a different\n    constant.\n\n    \"\"\"\n    # According to paper, this can also be done more efficiently with\n    # deviation reduction and balancing algorithms.\n    X = make_nonnegative(X)\n    X_scaled = X\n    for _ in range(max_iter):\n        X_new, _, _ = _scale_normalize(X_scaled)\n        if issparse(X):\n            dist = norm(X_scaled.data - X.data)\n        else:\n            dist = norm(X_scaled - X_new)\n        X_scaled = X_new\n        if dist is not None and dist < tol:\n            break\n    return X_scaled\n\n\ndef _log_normalize(X):\n    \"\"\"Normalize ``X`` according to Kluger's log-interactions scheme.\"\"\"\n    X = make_nonnegative(X, min_value=1)\n    if issparse(X):\n        raise ValueError(\"Cannot compute log of a sparse matrix,\"\n                         \" because log(x) diverges to -infinity as x\"\n                         \" goes to 0.\")\n    L = np.log(X)\n    row_avg = L.mean(axis=1)[:, np.newaxis]\n    col_avg = L.mean(axis=0)\n    avg = L.mean()\n    return L - row_avg - col_avg + avg\n\n\nclass BaseSpectral(six.with_metaclass(ABCMeta, BaseEstimator,\n                                      BiclusterMixin)):\n    \"\"\"Base class for spectral biclustering.\"\"\"\n\n    @abstractmethod\n    def __init__(self, n_clusters=3, svd_method=\"randomized\",\n                 n_svd_vecs=None, mini_batch=False, init=\"k-means++\",\n                 n_init=10, n_jobs=None, random_state=None):\n        self.n_clusters = n_clusters\n        self.svd_method = svd_method\n        self.n_svd_vecs = n_svd_vecs\n        self.mini_batch = mini_batch\n        self.init = init\n        self.n_init = n_init\n        self.n_jobs = n_jobs\n        self.random_state = random_state\n\n    def _check_parameters(self):\n        legal_svd_methods = ('randomized', 'arpack')\n        if self.svd_method not in legal_svd_methods:\n            raise ValueError(\"Unknown SVD method: '{0}'. svd_method must be\"\n                             \" one of {1}.\".format(self.svd_method,\n                                                   legal_svd_methods))\n\n    def fit(self, X, y=None):\n        \"\"\"Creates a biclustering for X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        y : Ignored\n\n        \"\"\"\n        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n        self._check_parameters()\n        self._fit(X)\n        return self\n\n    def _svd(self, array, n_components, n_discard):\n        \"\"\"Returns first `n_components` left and right singular\n        vectors u and v, discarding the first `n_discard`.\n\n        \"\"\"\n        if self.svd_method == 'randomized':\n            kwargs = {}\n            if self.n_svd_vecs is not None:\n                kwargs['n_oversamples'] = self.n_svd_vecs\n            u, _, vt = randomized_svd(array, n_components,\n                                      random_state=self.random_state,\n                                      **kwargs)\n\n        elif self.svd_method == 'arpack':\n            v0 = _init_arpack_v0(min(array.shape), self.random_state)\n            u, _, vt = svds(array, k=n_components, ncv=self.n_svd_vecs, v0=v0)\n            if np.any(np.isnan(vt)):\n                # some eigenvalues of A * A.T are negative, causing\n                # sqrt() to be np.nan. This causes some vectors in vt\n                # to be np.nan.\n                A = safe_sparse_dot(array.T, array)\n                # We have to renitialize v0 differently because the shape\n                # here can be different from the previous init\n                v0 = _init_arpack_v0(A.shape[0], self.random_state)\n                _, v = eigsh(A, ncv=self.n_svd_vecs, v0=v0)\n                vt = v.T\n            if np.any(np.isnan(u)):\n                A = safe_sparse_dot(array, array.T)\n                v0 = _init_arpack_v0(A.shape[0], self.random_state)\n                _, u = eigsh(A, ncv=self.n_svd_vecs, v0=v0)\n\n        assert_all_finite(u)\n        assert_all_finite(vt)\n        u = u[:, n_discard:]\n        vt = vt[n_discard:]\n        return u, vt.T\n\n    def _k_means(self, data, n_clusters):\n        if self.mini_batch:\n            model = MiniBatchKMeans(n_clusters,\n                                    init=self.init,\n                                    n_init=self.n_init,\n                                    random_state=self.random_state)\n        else:\n            model = KMeans(n_clusters, init=self.init,\n                           n_init=self.n_init, n_jobs=self.n_jobs,\n                           random_state=self.random_state)\n        model.fit(data)\n        centroid = model.cluster_centers_\n        labels = model.labels_\n        return centroid, labels\n\n\nclass SpectralCoclustering(BaseSpectral):\n    \"\"\"Spectral Co-Clustering algorithm (Dhillon, 2001).\n\n    Clusters rows and columns of an array `X` to solve the relaxed\n    normalized cut of the bipartite graph created from `X` as follows:\n    the edge between row vertex `i` and column vertex `j` has weight\n    `X[i, j]`.\n\n    The resulting bicluster structure is block-diagonal, since each\n    row and each column belongs to exactly one bicluster.\n\n    Supports sparse matrices, as long as they are nonnegative.\n\n    Read more in the :ref:`User Guide <spectral_coclustering>`.\n\n    Parameters\n    ----------\n    n_clusters : integer, optional, default: 3\n        The number of biclusters to find.\n\n    svd_method : string, optional, default: 'randomized'\n        Selects the algorithm for finding singular vectors. May be\n        'randomized' or 'arpack'. If 'randomized', use\n        :func:`sklearn.utils.extmath.randomized_svd`, which may be faster\n        for large matrices. If 'arpack', use\n        :func:`scipy.sparse.linalg.svds`, which is more accurate, but\n        possibly slower in some cases.\n\n    n_svd_vecs : int, optional, default: None\n        Number of vectors to use in calculating the SVD. Corresponds\n        to `ncv` when `svd_method=arpack` and `n_oversamples` when\n        `svd_method` is 'randomized`.\n\n    mini_batch : bool, optional, default: False\n        Whether to use mini-batch k-means, which is faster but may get\n        different results.\n\n    init : {'k-means++', 'random' or an ndarray}\n         Method for initialization of k-means algorithm; defaults to\n         'k-means++'.\n\n    n_init : int, optional, default: 10\n        Number of random initializations that are tried with the\n        k-means algorithm.\n\n        If mini-batch k-means is used, the best initialization is\n        chosen and the algorithm runs once. Otherwise, the algorithm\n        is run for each initialization and the best solution chosen.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation. This works by breaking\n        down the pairwise matrix into n_jobs even slices and computing them in\n        parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance or None (default)\n        Used for randomizing the singular value decomposition and the k-means\n        initialization. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    rows_ : array-like, shape (n_row_clusters, n_rows)\n        Results of the clustering. `rows[i, r]` is True if\n        cluster `i` contains row `r`. Available only after calling ``fit``.\n\n    columns_ : array-like, shape (n_column_clusters, n_columns)\n        Results of the clustering, like `rows`.\n\n    row_labels_ : array-like, shape (n_rows,)\n        The bicluster label of each row.\n\n    column_labels_ : array-like, shape (n_cols,)\n        The bicluster label of each column.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralCoclustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n    >>> clustering.row_labels_\n    array([0, 1, 1, 0, 0, 0], dtype=int32)\n    >>> clustering.column_labels_\n    array([0, 0], dtype=int32)\n    >>> clustering # doctest: +NORMALIZE_WHITESPACE\n    SpectralCoclustering(init='k-means++', mini_batch=False, n_clusters=2,\n               n_init=10, n_jobs=None, n_svd_vecs=None, random_state=0,\n               svd_method='randomized')\n\n    References\n    ----------\n\n    * Dhillon, Inderjit S, 2001. `Co-clustering documents and words using\n      bipartite spectral graph partitioning\n      <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.3011>`__.\n\n    \"\"\"\n    def __init__(self, n_clusters=3, svd_method='randomized',\n                 n_svd_vecs=None, mini_batch=False, init='k-means++',\n                 n_init=10, n_jobs=None, random_state=None):\n        super(SpectralCoclustering, self).__init__(n_clusters,\n                                                   svd_method,\n                                                   n_svd_vecs,\n                                                   mini_batch,\n                                                   init,\n                                                   n_init,\n                                                   n_jobs,\n                                                   random_state)\n\n    def _fit(self, X):\n        normalized_data, row_diag, col_diag = _scale_normalize(X)\n        n_sv = 1 + int(np.ceil(np.log2(self.n_clusters)))\n        u, v = self._svd(normalized_data, n_sv, n_discard=1)\n        z = np.vstack((row_diag[:, np.newaxis] * u,\n                       col_diag[:, np.newaxis] * v))\n\n        _, labels = self._k_means(z, self.n_clusters)\n\n        n_rows = X.shape[0]\n        self.row_labels_ = labels[:n_rows]\n        self.column_labels_ = labels[n_rows:]\n\n        self.rows_ = np.vstack([self.row_labels_ == c\n                                for c in range(self.n_clusters)])\n        self.columns_ = np.vstack([self.column_labels_ == c\n                                   for c in range(self.n_clusters)])\n\n\nclass SpectralBiclustering(BaseSpectral):\n    \"\"\"Spectral biclustering (Kluger, 2003).\n\n    Partitions rows and columns under the assumption that the data has\n    an underlying checkerboard structure. For instance, if there are\n    two row partitions and three column partitions, each row will\n    belong to three biclusters, and each column will belong to two\n    biclusters. The outer product of the corresponding row and column\n    label vectors gives this checkerboard structure.\n\n    Read more in the :ref:`User Guide <spectral_biclustering>`.\n\n    Parameters\n    ----------\n    n_clusters : integer or tuple (n_row_clusters, n_column_clusters)\n        The number of row and column clusters in the checkerboard\n        structure.\n\n    method : string, optional, default: 'bistochastic'\n        Method of normalizing and converting singular vectors into\n        biclusters. May be one of 'scale', 'bistochastic', or 'log'.\n        The authors recommend using 'log'. If the data is sparse,\n        however, log normalization will not work, which is why the\n        default is 'bistochastic'. CAUTION: if `method='log'`, the\n        data must not be sparse.\n\n    n_components : integer, optional, default: 6\n        Number of singular vectors to check.\n\n    n_best : integer, optional, default: 3\n        Number of best singular vectors to which to project the data\n        for clustering.\n\n    svd_method : string, optional, default: 'randomized'\n        Selects the algorithm for finding singular vectors. May be\n        'randomized' or 'arpack'. If 'randomized', uses\n        `sklearn.utils.extmath.randomized_svd`, which may be faster\n        for large matrices. If 'arpack', uses\n        `scipy.sparse.linalg.svds`, which is more accurate, but\n        possibly slower in some cases.\n\n    n_svd_vecs : int, optional, default: None\n        Number of vectors to use in calculating the SVD. Corresponds\n        to `ncv` when `svd_method=arpack` and `n_oversamples` when\n        `svd_method` is 'randomized`.\n\n    mini_batch : bool, optional, default: False\n        Whether to use mini-batch k-means, which is faster but may get\n        different results.\n\n    init : {'k-means++', 'random' or an ndarray}\n         Method for initialization of k-means algorithm; defaults to\n         'k-means++'.\n\n    n_init : int, optional, default: 10\n        Number of random initializations that are tried with the\n        k-means algorithm.\n\n        If mini-batch k-means is used, the best initialization is\n        chosen and the algorithm runs once. Otherwise, the algorithm\n        is run for each initialization and the best solution chosen.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation. This works by breaking\n        down the pairwise matrix into n_jobs even slices and computing them in\n        parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance or None (default)\n        Used for randomizing the singular value decomposition and the k-means\n        initialization. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    rows_ : array-like, shape (n_row_clusters, n_rows)\n        Results of the clustering. `rows[i, r]` is True if\n        cluster `i` contains row `r`. Available only after calling ``fit``.\n\n    columns_ : array-like, shape (n_column_clusters, n_columns)\n        Results of the clustering, like `rows`.\n\n    row_labels_ : array-like, shape (n_rows,)\n        Row partition labels.\n\n    column_labels_ : array-like, shape (n_cols,)\n        Column partition labels.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralBiclustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)\n    >>> clustering.row_labels_\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> clustering.column_labels_\n    array([0, 1], dtype=int32)\n    >>> clustering # doctest: +NORMALIZE_WHITESPACE\n    SpectralBiclustering(init='k-means++', method='bistochastic',\n               mini_batch=False, n_best=3, n_clusters=2, n_components=6,\n               n_init=10, n_jobs=None, n_svd_vecs=None, random_state=0,\n               svd_method='randomized')\n\n    References\n    ----------\n\n    * Kluger, Yuval, et. al., 2003. `Spectral biclustering of microarray\n      data: coclustering genes and conditions\n      <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608>`__.\n\n    \"\"\"\n    def __init__(self, n_clusters=3, method='bistochastic',\n                 n_components=6, n_best=3, svd_method='randomized',\n                 n_svd_vecs=None, mini_batch=False, init='k-means++',\n                 n_init=10, n_jobs=None, random_state=None):\n        super(SpectralBiclustering, self).__init__(n_clusters,\n                                                   svd_method,\n                                                   n_svd_vecs,\n                                                   mini_batch,\n                                                   init,\n                                                   n_init,\n                                                   n_jobs,\n                                                   random_state)\n        self.method = method\n        self.n_components = n_components\n        self.n_best = n_best\n\n    def _check_parameters(self):\n        super(SpectralBiclustering, self)._check_parameters()\n        legal_methods = ('bistochastic', 'scale', 'log')\n        if self.method not in legal_methods:\n            raise ValueError(\"Unknown method: '{0}'. method must be\"\n                             \" one of {1}.\".format(self.method, legal_methods))\n        try:\n            int(self.n_clusters)\n        except TypeError:\n            try:\n                r, c = self.n_clusters\n                int(r)\n                int(c)\n            except (ValueError, TypeError):\n                raise ValueError(\"Incorrect parameter n_clusters has value:\"\n                                 \" {}. It should either be a single integer\"\n                                 \" or an iterable with two integers:\"\n                                 \" (n_row_clusters, n_column_clusters)\")\n        if self.n_components < 1:\n            raise ValueError(\"Parameter n_components must be greater than 0,\"\n                             \" but its value is {}\".format(self.n_components))\n        if self.n_best < 1:\n            raise ValueError(\"Parameter n_best must be greater than 0,\"\n                             \" but its value is {}\".format(self.n_best))\n        if self.n_best > self.n_components:\n            raise ValueError(\"n_best cannot be larger than\"\n                             \" n_components, but {} >  {}\"\n                             \"\".format(self.n_best, self.n_components))\n\n    def _fit(self, X):\n        n_sv = self.n_components\n        if self.method == 'bistochastic':\n            normalized_data = _bistochastic_normalize(X)\n            n_sv += 1\n        elif self.method == 'scale':\n            normalized_data, _, _ = _scale_normalize(X)\n            n_sv += 1\n        elif self.method == 'log':\n            normalized_data = _log_normalize(X)\n        n_discard = 0 if self.method == 'log' else 1\n        u, v = self._svd(normalized_data, n_sv, n_discard)\n        ut = u.T\n        vt = v.T\n\n        try:\n            n_row_clusters, n_col_clusters = self.n_clusters\n        except TypeError:\n            n_row_clusters = n_col_clusters = self.n_clusters\n\n        best_ut = self._fit_best_piecewise(ut, self.n_best,\n                                           n_row_clusters)\n\n        best_vt = self._fit_best_piecewise(vt, self.n_best,\n                                           n_col_clusters)\n\n        self.row_labels_ = self._project_and_cluster(X, best_vt.T,\n                                                     n_row_clusters)\n\n        self.column_labels_ = self._project_and_cluster(X.T, best_ut.T,\n                                                        n_col_clusters)\n\n        self.rows_ = np.vstack([self.row_labels_ == label\n                                for label in range(n_row_clusters)\n                                for _ in range(n_col_clusters)])\n        self.columns_ = np.vstack([self.column_labels_ == label\n                                   for _ in range(n_row_clusters)\n                                   for label in range(n_col_clusters)])\n\n    def _fit_best_piecewise(self, vectors, n_best, n_clusters):\n        \"\"\"Find the ``n_best`` vectors that are best approximated by piecewise\n        constant vectors.\n\n        The piecewise vectors are found by k-means; the best is chosen\n        according to Euclidean distance.\n\n        \"\"\"\n        def make_piecewise(v):\n            centroid, labels = self._k_means(v.reshape(-1, 1), n_clusters)\n            return centroid[labels].ravel()\n        piecewise_vectors = np.apply_along_axis(make_piecewise,\n                                                axis=1, arr=vectors)\n        dists = np.apply_along_axis(norm, axis=1,\n                                    arr=(vectors - piecewise_vectors))\n        result = vectors[np.argsort(dists)[:n_best]]\n        return result\n\n    def _project_and_cluster(self, data, vectors, n_clusters):\n        \"\"\"Project ``data`` to ``vectors`` and cluster the result.\"\"\"\n        projected = safe_sparse_dot(data, vectors)\n        _, labels = self._k_means(projected, n_clusters)\n        return labels\n"
    },
    {
      "filename": "sklearn/cross_decomposition/pls_.py",
      "content": "\"\"\"\nThe :mod:`sklearn.pls` module implements Partial Least Squares (PLS).\n\"\"\"\n\n# Author: Edouard Duchesnay <edouard.duchesnay@cea.fr>\n# License: BSD 3 clause\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\n\nimport numpy as np\nfrom scipy.linalg import pinv2, svd\nfrom scipy.sparse.linalg import svds\n\nfrom ..base import BaseEstimator, RegressorMixin, TransformerMixin\nfrom ..utils import check_array, check_consistent_length\nfrom ..utils.arpack import _init_arpack_v0\nfrom ..utils.extmath import svd_flip\nfrom ..utils.validation import check_is_fitted, FLOAT_DTYPES\nfrom ..exceptions import ConvergenceWarning\nfrom ..externals import six\n\n__all__ = ['PLSCanonical', 'PLSRegression', 'PLSSVD']\n\n\ndef _nipals_twoblocks_inner_loop(X, Y, mode=\"A\", max_iter=500, tol=1e-06,\n                                 norm_y_weights=False):\n    \"\"\"Inner loop of the iterative NIPALS algorithm.\n\n    Provides an alternative to the svd(X'Y); returns the first left and right\n    singular vectors of X'Y.  See PLS for the meaning of the parameters.  It is\n    similar to the Power method for determining the eigenvectors and\n    eigenvalues of a X'Y.\n    \"\"\"\n    y_score = Y[:, [0]]\n    x_weights_old = 0\n    ite = 1\n    X_pinv = Y_pinv = None\n    eps = np.finfo(X.dtype).eps\n    # Inner loop of the Wold algo.\n    while True:\n        # 1.1 Update u: the X weights\n        if mode == \"B\":\n            if X_pinv is None:\n                # We use slower pinv2 (same as np.linalg.pinv) for stability\n                # reasons\n                X_pinv = pinv2(X, check_finite=False)\n            x_weights = np.dot(X_pinv, y_score)\n        else:  # mode A\n            # Mode A regress each X column on y_score\n            x_weights = np.dot(X.T, y_score) / np.dot(y_score.T, y_score)\n        # If y_score only has zeros x_weights will only have zeros. In\n        # this case add an epsilon to converge to a more acceptable\n        # solution\n        if np.dot(x_weights.T, x_weights) < eps:\n            x_weights += eps\n        # 1.2 Normalize u\n        x_weights /= np.sqrt(np.dot(x_weights.T, x_weights)) + eps\n        # 1.3 Update x_score: the X latent scores\n        x_score = np.dot(X, x_weights)\n        # 2.1 Update y_weights\n        if mode == \"B\":\n            if Y_pinv is None:\n                Y_pinv = pinv2(Y, check_finite=False)  # compute once pinv(Y)\n            y_weights = np.dot(Y_pinv, x_score)\n        else:\n            # Mode A regress each Y column on x_score\n            y_weights = np.dot(Y.T, x_score) / np.dot(x_score.T, x_score)\n        # 2.2 Normalize y_weights\n        if norm_y_weights:\n            y_weights /= np.sqrt(np.dot(y_weights.T, y_weights)) + eps\n        # 2.3 Update y_score: the Y latent scores\n        y_score = np.dot(Y, y_weights) / (np.dot(y_weights.T, y_weights) + eps)\n        # y_score = np.dot(Y, y_weights) / np.dot(y_score.T, y_score) ## BUG\n        x_weights_diff = x_weights - x_weights_old\n        if np.dot(x_weights_diff.T, x_weights_diff) < tol or Y.shape[1] == 1:\n            break\n        if ite == max_iter:\n            warnings.warn('Maximum number of iterations reached',\n                          ConvergenceWarning)\n            break\n        x_weights_old = x_weights\n        ite += 1\n    return x_weights, y_weights, ite\n\n\ndef _svd_cross_product(X, Y):\n    C = np.dot(X.T, Y)\n    U, s, Vh = svd(C, full_matrices=False)\n    u = U[:, [0]]\n    v = Vh.T[:, [0]]\n    return u, v\n\n\ndef _center_scale_xy(X, Y, scale=True):\n    \"\"\" Center X, Y and scale if the scale parameter==True\n\n    Returns\n    -------\n        X, Y, x_mean, y_mean, x_std, y_std\n    \"\"\"\n    # center\n    x_mean = X.mean(axis=0)\n    X -= x_mean\n    y_mean = Y.mean(axis=0)\n    Y -= y_mean\n    # scale\n    if scale:\n        x_std = X.std(axis=0, ddof=1)\n        x_std[x_std == 0.0] = 1.0\n        X /= x_std\n        y_std = Y.std(axis=0, ddof=1)\n        y_std[y_std == 0.0] = 1.0\n        Y /= y_std\n    else:\n        x_std = np.ones(X.shape[1])\n        y_std = np.ones(Y.shape[1])\n    return X, Y, x_mean, y_mean, x_std, y_std\n\n\nclass _PLS(six.with_metaclass(ABCMeta), BaseEstimator, TransformerMixin,\n           RegressorMixin):\n    \"\"\"Partial Least Squares (PLS)\n\n    This class implements the generic PLS algorithm, constructors' parameters\n    allow to obtain a specific implementation such as:\n\n    - PLS2 regression, i.e., PLS 2 blocks, mode A, with asymmetric deflation\n      and unnormalized y weights such as defined by [Tenenhaus 1998] p. 132.\n      With univariate response it implements PLS1.\n\n    - PLS canonical, i.e., PLS 2 blocks, mode A, with symmetric deflation and\n      normalized y weights such as defined by [Tenenhaus 1998] (p. 132) and\n      [Wegelin et al. 2000]. This parametrization implements the original Wold\n      algorithm.\n\n    We use the terminology defined by [Wegelin et al. 2000].\n    This implementation uses the PLS Wold 2 blocks algorithm based on two\n    nested loops:\n        (i) The outer loop iterate over components.\n        (ii) The inner loop estimates the weights vectors. This can be done\n        with two algo. (a) the inner loop of the original NIPALS algo. or (b) a\n        SVD on residuals cross-covariance matrices.\n\n    n_components : int, number of components to keep. (default 2).\n\n    scale : boolean, scale data? (default True)\n\n    deflation_mode : str, \"canonical\" or \"regression\". See notes.\n\n    mode : \"A\" classical PLS and \"B\" CCA. See notes.\n\n    norm_y_weights : boolean, normalize Y weights to one? (default False)\n\n    algorithm : string, \"nipals\" or \"svd\"\n        The algorithm used to estimate the weights. It will be called\n        n_components times, i.e. once for each iteration of the outer loop.\n\n    max_iter : int (default 500)\n        The maximum number of iterations\n        of the NIPALS inner loop (used only if algorithm=\"nipals\")\n\n    tol : non-negative real, default 1e-06\n        The tolerance used in the iterative algorithm.\n\n    copy : boolean, default True\n        Whether the deflation should be done on a copy. Let the default\n        value to True unless you don't care about side effects.\n\n    Attributes\n    ----------\n    x_weights_ : array, [p, n_components]\n        X block weights vectors.\n\n    y_weights_ : array, [q, n_components]\n        Y block weights vectors.\n\n    x_loadings_ : array, [p, n_components]\n        X block loadings vectors.\n\n    y_loadings_ : array, [q, n_components]\n        Y block loadings vectors.\n\n    x_scores_ : array, [n_samples, n_components]\n        X scores.\n\n    y_scores_ : array, [n_samples, n_components]\n        Y scores.\n\n    x_rotations_ : array, [p, n_components]\n        X block to latents rotations.\n\n    y_rotations_ : array, [q, n_components]\n        Y block to latents rotations.\n\n    coef_ : array, [p, q]\n        The coefficients of the linear model: ``Y = X coef_ + Err``\n\n    n_iter_ : array-like\n        Number of iterations of the NIPALS inner loop for each\n        component. Not useful if the algorithm given is \"svd\".\n\n    References\n    ----------\n\n    Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with\n    emphasis on the two-block case. Technical Report 371, Department of\n    Statistics, University of Washington, Seattle, 2000.\n\n    In French but still a reference:\n    Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\n    Editions Technic.\n\n    See also\n    --------\n    PLSCanonical\n    PLSRegression\n    CCA\n    PLS_SVD\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self, n_components=2, scale=True, deflation_mode=\"regression\",\n                 mode=\"A\", algorithm=\"nipals\", norm_y_weights=False,\n                 max_iter=500, tol=1e-06, copy=True):\n        self.n_components = n_components\n        self.deflation_mode = deflation_mode\n        self.mode = mode\n        self.norm_y_weights = norm_y_weights\n        self.scale = scale\n        self.algorithm = algorithm\n        self.max_iter = max_iter\n        self.tol = tol\n        self.copy = copy\n\n    def fit(self, X, Y):\n        \"\"\"Fit model to data.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of predictors.\n\n        Y : array-like, shape = [n_samples, n_targets]\n            Target vectors, where n_samples is the number of samples and\n            n_targets is the number of response variables.\n        \"\"\"\n\n        # copy since this will contains the residuals (deflated) matrices\n        check_consistent_length(X, Y)\n        X = check_array(X, dtype=np.float64, copy=self.copy,\n                        ensure_min_samples=2)\n        Y = check_array(Y, dtype=np.float64, copy=self.copy, ensure_2d=False)\n        if Y.ndim == 1:\n            Y = Y.reshape(-1, 1)\n\n        n = X.shape[0]\n        p = X.shape[1]\n        q = Y.shape[1]\n\n        if self.n_components < 1 or self.n_components > p:\n            raise ValueError('Invalid number of components: %d' %\n                             self.n_components)\n        if self.algorithm not in (\"svd\", \"nipals\"):\n            raise ValueError(\"Got algorithm %s when only 'svd' \"\n                             \"and 'nipals' are known\" % self.algorithm)\n        if self.algorithm == \"svd\" and self.mode == \"B\":\n            raise ValueError('Incompatible configuration: mode B is not '\n                             'implemented with svd algorithm')\n        if self.deflation_mode not in [\"canonical\", \"regression\"]:\n            raise ValueError('The deflation mode is unknown')\n        # Scale (in place)\n        X, Y, self.x_mean_, self.y_mean_, self.x_std_, self.y_std_ = (\n            _center_scale_xy(X, Y, self.scale))\n        # Residuals (deflated) matrices\n        Xk = X\n        Yk = Y\n        # Results matrices\n        self.x_scores_ = np.zeros((n, self.n_components))\n        self.y_scores_ = np.zeros((n, self.n_components))\n        self.x_weights_ = np.zeros((p, self.n_components))\n        self.y_weights_ = np.zeros((q, self.n_components))\n        self.x_loadings_ = np.zeros((p, self.n_components))\n        self.y_loadings_ = np.zeros((q, self.n_components))\n        self.n_iter_ = []\n\n        # NIPALS algo: outer loop, over components\n        for k in range(self.n_components):\n            if np.all(np.dot(Yk.T, Yk) < np.finfo(np.double).eps):\n                # Yk constant\n                warnings.warn('Y residual constant at iteration %s' % k)\n                break\n            # 1) weights estimation (inner loop)\n            # -----------------------------------\n            if self.algorithm == \"nipals\":\n                x_weights, y_weights, n_iter_ = \\\n                    _nipals_twoblocks_inner_loop(\n                        X=Xk, Y=Yk, mode=self.mode, max_iter=self.max_iter,\n                        tol=self.tol, norm_y_weights=self.norm_y_weights)\n                self.n_iter_.append(n_iter_)\n            elif self.algorithm == \"svd\":\n                x_weights, y_weights = _svd_cross_product(X=Xk, Y=Yk)\n            # Forces sign stability of x_weights and y_weights\n            # Sign undeterminacy issue from svd if algorithm == \"svd\"\n            # and from platform dependent computation if algorithm == 'nipals'\n            x_weights, y_weights = svd_flip(x_weights, y_weights.T)\n            y_weights = y_weights.T\n            # compute scores\n            x_scores = np.dot(Xk, x_weights)\n            if self.norm_y_weights:\n                y_ss = 1\n            else:\n                y_ss = np.dot(y_weights.T, y_weights)\n            y_scores = np.dot(Yk, y_weights) / y_ss\n            # test for null variance\n            if np.dot(x_scores.T, x_scores) < np.finfo(np.double).eps:\n                warnings.warn('X scores are null at iteration %s' % k)\n                break\n            # 2) Deflation (in place)\n            # ----------------------\n            # Possible memory footprint reduction may done here: in order to\n            # avoid the allocation of a data chunk for the rank-one\n            # approximations matrix which is then subtracted to Xk, we suggest\n            # to perform a column-wise deflation.\n            #\n            # - regress Xk's on x_score\n            x_loadings = np.dot(Xk.T, x_scores) / np.dot(x_scores.T, x_scores)\n            # - subtract rank-one approximations to obtain remainder matrix\n            Xk -= np.dot(x_scores, x_loadings.T)\n            if self.deflation_mode == \"canonical\":\n                # - regress Yk's on y_score, then subtract rank-one approx.\n                y_loadings = (np.dot(Yk.T, y_scores)\n                              / np.dot(y_scores.T, y_scores))\n                Yk -= np.dot(y_scores, y_loadings.T)\n            if self.deflation_mode == \"regression\":\n                # - regress Yk's on x_score, then subtract rank-one approx.\n                y_loadings = (np.dot(Yk.T, x_scores)\n                              / np.dot(x_scores.T, x_scores))\n                Yk -= np.dot(x_scores, y_loadings.T)\n            # 3) Store weights, scores and loadings # Notation:\n            self.x_scores_[:, k] = x_scores.ravel()  # T\n            self.y_scores_[:, k] = y_scores.ravel()  # U\n            self.x_weights_[:, k] = x_weights.ravel()  # W\n            self.y_weights_[:, k] = y_weights.ravel()  # C\n            self.x_loadings_[:, k] = x_loadings.ravel()  # P\n            self.y_loadings_[:, k] = y_loadings.ravel()  # Q\n        # Such that: X = TP' + Err and Y = UQ' + Err\n\n        # 4) rotations from input space to transformed space (scores)\n        # T = X W(P'W)^-1 = XW* (W* : p x k matrix)\n        # U = Y C(Q'C)^-1 = YC* (W* : q x k matrix)\n        self.x_rotations_ = np.dot(\n            self.x_weights_,\n            pinv2(np.dot(self.x_loadings_.T, self.x_weights_),\n                  check_finite=False))\n        if Y.shape[1] > 1:\n            self.y_rotations_ = np.dot(\n                self.y_weights_,\n                pinv2(np.dot(self.y_loadings_.T, self.y_weights_),\n                      check_finite=False))\n        else:\n            self.y_rotations_ = np.ones(1)\n\n        if True or self.deflation_mode == \"regression\":\n            # FIXME what's with the if?\n            # Estimate regression coefficient\n            # Regress Y on T\n            # Y = TQ' + Err,\n            # Then express in function of X\n            # Y = X W(P'W)^-1Q' + Err = XB + Err\n            # => B = W*Q' (p x q)\n            self.coef_ = np.dot(self.x_rotations_, self.y_loadings_.T)\n            self.coef_ = self.coef_ * self.y_std_\n        return self\n\n    def transform(self, X, Y=None, copy=True):\n        \"\"\"Apply the dimension reduction learned on the train data.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of predictors.\n\n        Y : array-like, shape = [n_samples, n_targets]\n            Target vectors, where n_samples is the number of samples and\n            n_targets is the number of response variables.\n\n        copy : boolean, default True\n            Whether to copy X and Y, or perform in-place normalization.\n\n        Returns\n        -------\n        x_scores if Y is not given, (x_scores, y_scores) otherwise.\n        \"\"\"\n        check_is_fitted(self, 'x_mean_')\n        X = check_array(X, copy=copy, dtype=FLOAT_DTYPES)\n        # Normalize\n        X -= self.x_mean_\n        X /= self.x_std_\n        # Apply rotation\n        x_scores = np.dot(X, self.x_rotations_)\n        if Y is not None:\n            Y = check_array(Y, ensure_2d=False, copy=copy, dtype=FLOAT_DTYPES)\n            if Y.ndim == 1:\n                Y = Y.reshape(-1, 1)\n            Y -= self.y_mean_\n            Y /= self.y_std_\n            y_scores = np.dot(Y, self.y_rotations_)\n            return x_scores, y_scores\n\n        return x_scores\n\n    def predict(self, X, copy=True):\n        \"\"\"Apply the dimension reduction learned on the train data.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of predictors.\n\n        copy : boolean, default True\n            Whether to copy X and Y, or perform in-place normalization.\n\n        Notes\n        -----\n        This call requires the estimation of a p x q matrix, which may\n        be an issue in high dimensional space.\n        \"\"\"\n        check_is_fitted(self, 'x_mean_')\n        X = check_array(X, copy=copy, dtype=FLOAT_DTYPES)\n        # Normalize\n        X -= self.x_mean_\n        X /= self.x_std_\n        Ypred = np.dot(X, self.coef_)\n        return Ypred + self.y_mean_\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Learn and apply the dimension reduction on the train data.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of predictors.\n\n        y : array-like, shape = [n_samples, n_targets]\n            Target vectors, where n_samples is the number of samples and\n            n_targets is the number of response variables.\n\n        Returns\n        -------\n        x_scores if Y is not given, (x_scores, y_scores) otherwise.\n        \"\"\"\n        return self.fit(X, y).transform(X, y)\n\n\nclass PLSRegression(_PLS):\n    \"\"\"PLS regression\n\n    PLSRegression implements the PLS 2 blocks regression known as PLS2 or PLS1\n    in case of one dimensional response.\n    This class inherits from _PLS with mode=\"A\", deflation_mode=\"regression\",\n    norm_y_weights=False and algorithm=\"nipals\".\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    Parameters\n    ----------\n    n_components : int, (default 2)\n        Number of components to keep.\n\n    scale : boolean, (default True)\n        whether to scale the data\n\n    max_iter : an integer, (default 500)\n        the maximum number of iterations of the NIPALS inner loop (used\n        only if algorithm=\"nipals\")\n\n    tol : non-negative real\n        Tolerance used in the iterative algorithm default 1e-06.\n\n    copy : boolean, default True\n        Whether the deflation should be done on a copy. Let the default\n        value to True unless you don't care about side effect\n\n    Attributes\n    ----------\n    x_weights_ : array, [p, n_components]\n        X block weights vectors.\n\n    y_weights_ : array, [q, n_components]\n        Y block weights vectors.\n\n    x_loadings_ : array, [p, n_components]\n        X block loadings vectors.\n\n    y_loadings_ : array, [q, n_components]\n        Y block loadings vectors.\n\n    x_scores_ : array, [n_samples, n_components]\n        X scores.\n\n    y_scores_ : array, [n_samples, n_components]\n        Y scores.\n\n    x_rotations_ : array, [p, n_components]\n        X block to latents rotations.\n\n    y_rotations_ : array, [q, n_components]\n        Y block to latents rotations.\n\n    coef_ : array, [p, q]\n        The coefficients of the linear model: ``Y = X coef_ + Err``\n\n    n_iter_ : array-like\n        Number of iterations of the NIPALS inner loop for each\n        component.\n\n    Notes\n    -----\n    Matrices::\n\n        T: x_scores_\n        U: y_scores_\n        W: x_weights_\n        C: y_weights_\n        P: x_loadings_\n        Q: y_loadings__\n\n    Are computed such that::\n\n        X = T P.T + Err and Y = U Q.T + Err\n        T[:, k] = Xk W[:, k] for k in range(n_components)\n        U[:, k] = Yk C[:, k] for k in range(n_components)\n        x_rotations_ = W (P.T W)^(-1)\n        y_rotations_ = C (Q.T C)^(-1)\n\n    where Xk and Yk are residual matrices at iteration k.\n\n    `Slides explaining\n    PLS <http://www.eigenvector.com/Docs/Wise_pls_properties.pdf>`_\n\n\n    For each component k, find weights u, v that optimizes:\n    ``max corr(Xk u, Yk v) * std(Xk u) std(Yk u)``, such that ``|u| = 1``\n\n    Note that it maximizes both the correlations between the scores and the\n    intra-block variances.\n\n    The residual matrix of X (Xk+1) block is obtained by the deflation on\n    the current X score: x_score.\n\n    The residual matrix of Y (Yk+1) block is obtained by deflation on the\n    current X score. This performs the PLS regression known as PLS2. This\n    mode is prediction oriented.\n\n    This implementation provides the same results that 3 PLS packages\n    provided in the R language (R-project):\n\n        - \"mixOmics\" with function pls(X, Y, mode = \"regression\")\n        - \"plspm \" with function plsreg2(X, Y)\n        - \"pls\" with function oscorespls.fit(X, Y)\n\n    Examples\n    --------\n    >>> from sklearn.cross_decomposition import PLSRegression\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    >>> pls2 = PLSRegression(n_components=2)\n    >>> pls2.fit(X, Y)\n    ... # doctest: +NORMALIZE_WHITESPACE\n    PLSRegression(copy=True, max_iter=500, n_components=2, scale=True,\n            tol=1e-06)\n    >>> Y_pred = pls2.predict(X)\n\n    References\n    ----------\n\n    Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with\n    emphasis on the two-block case. Technical Report 371, Department of\n    Statistics, University of Washington, Seattle, 2000.\n\n    In french but still a reference:\n    Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\n    Editions Technic.\n    \"\"\"\n\n    def __init__(self, n_components=2, scale=True,\n                 max_iter=500, tol=1e-06, copy=True):\n        super(PLSRegression, self).__init__(\n            n_components=n_components, scale=scale,\n            deflation_mode=\"regression\", mode=\"A\",\n            norm_y_weights=False, max_iter=max_iter, tol=tol,\n            copy=copy)\n\n\nclass PLSCanonical(_PLS):\n    \"\"\" PLSCanonical implements the 2 blocks canonical PLS of the original Wold\n    algorithm [Tenenhaus 1998] p.204, referred as PLS-C2A in [Wegelin 2000].\n\n    This class inherits from PLS with mode=\"A\" and deflation_mode=\"canonical\",\n    norm_y_weights=True and algorithm=\"nipals\", but svd should provide similar\n    results up to numerical errors.\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    Parameters\n    ----------\n    n_components : int, (default 2).\n        Number of components to keep\n\n    scale : boolean, (default True)\n        Option to scale data\n\n    algorithm : string, \"nipals\" or \"svd\"\n        The algorithm used to estimate the weights. It will be called\n        n_components times, i.e. once for each iteration of the outer loop.\n\n    max_iter : an integer, (default 500)\n        the maximum number of iterations of the NIPALS inner loop (used\n        only if algorithm=\"nipals\")\n\n    tol : non-negative real, default 1e-06\n        the tolerance used in the iterative algorithm\n\n    copy : boolean, default True\n        Whether the deflation should be done on a copy. Let the default\n        value to True unless you don't care about side effect\n\n    Attributes\n    ----------\n    x_weights_ : array, shape = [p, n_components]\n        X block weights vectors.\n\n    y_weights_ : array, shape = [q, n_components]\n        Y block weights vectors.\n\n    x_loadings_ : array, shape = [p, n_components]\n        X block loadings vectors.\n\n    y_loadings_ : array, shape = [q, n_components]\n        Y block loadings vectors.\n\n    x_scores_ : array, shape = [n_samples, n_components]\n        X scores.\n\n    y_scores_ : array, shape = [n_samples, n_components]\n        Y scores.\n\n    x_rotations_ : array, shape = [p, n_components]\n        X block to latents rotations.\n\n    y_rotations_ : array, shape = [q, n_components]\n        Y block to latents rotations.\n\n    n_iter_ : array-like\n        Number of iterations of the NIPALS inner loop for each\n        component. Not useful if the algorithm provided is \"svd\".\n\n    Notes\n    -----\n    Matrices::\n\n        T: x_scores_\n        U: y_scores_\n        W: x_weights_\n        C: y_weights_\n        P: x_loadings_\n        Q: y_loadings__\n\n    Are computed such that::\n\n        X = T P.T + Err and Y = U Q.T + Err\n        T[:, k] = Xk W[:, k] for k in range(n_components)\n        U[:, k] = Yk C[:, k] for k in range(n_components)\n        x_rotations_ = W (P.T W)^(-1)\n        y_rotations_ = C (Q.T C)^(-1)\n\n    where Xk and Yk are residual matrices at iteration k.\n\n    `Slides explaining PLS\n    <http://www.eigenvector.com/Docs/Wise_pls_properties.pdf>`_\n\n    For each component k, find weights u, v that optimize::\n\n        max corr(Xk u, Yk v) * std(Xk u) std(Yk u), such that ``|u| = |v| = 1``\n\n    Note that it maximizes both the correlations between the scores and the\n    intra-block variances.\n\n    The residual matrix of X (Xk+1) block is obtained by the deflation on the\n    current X score: x_score.\n\n    The residual matrix of Y (Yk+1) block is obtained by deflation on the\n    current Y score. This performs a canonical symmetric version of the PLS\n    regression. But slightly different than the CCA. This is mostly used\n    for modeling.\n\n    This implementation provides the same results that the \"plspm\" package\n    provided in the R language (R-project), using the function plsca(X, Y).\n    Results are equal or collinear with the function\n    ``pls(..., mode = \"canonical\")`` of the \"mixOmics\" package. The difference\n    relies in the fact that mixOmics implementation does not exactly implement\n    the Wold algorithm since it does not normalize y_weights to one.\n\n    Examples\n    --------\n    >>> from sklearn.cross_decomposition import PLSCanonical\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    >>> plsca = PLSCanonical(n_components=2)\n    >>> plsca.fit(X, Y)\n    ... # doctest: +NORMALIZE_WHITESPACE\n    PLSCanonical(algorithm='nipals', copy=True, max_iter=500, n_components=2,\n                 scale=True, tol=1e-06)\n    >>> X_c, Y_c = plsca.transform(X, Y)\n\n    References\n    ----------\n\n    Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with\n    emphasis on the two-block case. Technical Report 371, Department of\n    Statistics, University of Washington, Seattle, 2000.\n\n    Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\n    Editions Technic.\n\n    See also\n    --------\n    CCA\n    PLSSVD\n    \"\"\"\n\n    def __init__(self, n_components=2, scale=True, algorithm=\"nipals\",\n                 max_iter=500, tol=1e-06, copy=True):\n        super(PLSCanonical, self).__init__(\n            n_components=n_components, scale=scale,\n            deflation_mode=\"canonical\", mode=\"A\",\n            norm_y_weights=True, algorithm=algorithm,\n            max_iter=max_iter, tol=tol, copy=copy)\n\n\nclass PLSSVD(BaseEstimator, TransformerMixin):\n    \"\"\"Partial Least Square SVD\n\n    Simply perform a svd on the crosscovariance matrix: X'Y\n    There are no iterative deflation here.\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    Parameters\n    ----------\n    n_components : int, default 2\n        Number of components to keep.\n\n    scale : boolean, default True\n        Whether to scale X and Y.\n\n    copy : boolean, default True\n        Whether to copy X and Y, or perform in-place computations.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`.\n\n        .. versionadded:: 0.21\n\n    Attributes\n    ----------\n    x_weights_ : array, [p, n_components]\n        X block weights vectors.\n\n    y_weights_ : array, [q, n_components]\n        Y block weights vectors.\n\n    x_scores_ : array, [n_samples, n_components]\n        X scores.\n\n    y_scores_ : array, [n_samples, n_components]\n        Y scores.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cross_decomposition import PLSSVD\n    >>> X = np.array([[0., 0., 1.],\n    ...     [1.,0.,0.],\n    ...     [2.,2.,2.],\n    ...     [2.,5.,4.]])\n    >>> Y = np.array([[0.1, -0.2],\n    ...     [0.9, 1.1],\n    ...     [6.2, 5.9],\n    ...     [11.9, 12.3]])\n    >>> plsca = PLSSVD(n_components=2)\n    >>> plsca.fit(X, Y)\n    PLSSVD(copy=True, n_components=2, random_state=None, scale=True)\n    >>> X_c, Y_c = plsca.transform(X, Y)\n    >>> X_c.shape, Y_c.shape\n    ((4, 2), (4, 2))\n\n    See also\n    --------\n    PLSCanonical\n    CCA\n    \"\"\"\n\n    def __init__(self, n_components=2, scale=True,\n                 copy=True, random_state=None):\n        self.n_components = n_components\n        self.scale = scale\n        self.copy = copy\n        self.random_state = random_state\n\n    def fit(self, X, Y):\n        \"\"\"Fit model to data.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of predictors.\n\n        Y : array-like, shape = [n_samples, n_targets]\n            Target vectors, where n_samples is the number of samples and\n            n_targets is the number of response variables.\n        \"\"\"\n        # copy since this will contains the centered data\n        check_consistent_length(X, Y)\n        X = check_array(X, dtype=np.float64, copy=self.copy,\n                        ensure_min_samples=2)\n        Y = check_array(Y, dtype=np.float64, copy=self.copy, ensure_2d=False)\n        if Y.ndim == 1:\n            Y = Y.reshape(-1, 1)\n\n        if self.n_components > max(Y.shape[1], X.shape[1]):\n            raise ValueError(\"Invalid number of components n_components=%d\"\n                             \" with X of shape %s and Y of shape %s.\"\n                             % (self.n_components, str(X.shape), str(Y.shape)))\n\n        # Scale (in place)\n        X, Y, self.x_mean_, self.y_mean_, self.x_std_, self.y_std_ = (\n            _center_scale_xy(X, Y, self.scale))\n        # svd(X'Y)\n        C = np.dot(X.T, Y)\n\n        # The arpack svds solver only works if the number of extracted\n        # components is smaller than rank(X) - 1. Hence, if we want to extract\n        # all the components (C.shape[1]), we have to use another one. Else,\n        # let's use arpacks to compute only the interesting components.\n        if self.n_components >= np.min(C.shape):\n            U, s, V = svd(C, full_matrices=False)\n        else:\n            v0 = _init_arpack_v0(min(C.shape), self.random_state)\n            U, s, V = svds(C, k=self.n_components, v0=v0)\n        # Deterministic output\n        U, V = svd_flip(U, V)\n        V = V.T\n        self.x_scores_ = np.dot(X, U)\n        self.y_scores_ = np.dot(Y, V)\n        self.x_weights_ = U\n        self.y_weights_ = V\n        return self\n\n    def transform(self, X, Y=None):\n        \"\"\"\n        Apply the dimension reduction learned on the train data.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of predictors.\n\n        Y : array-like, shape = [n_samples, n_targets]\n            Target vectors, where n_samples is the number of samples and\n            n_targets is the number of response variables.\n        \"\"\"\n        check_is_fitted(self, 'x_mean_')\n        X = check_array(X, dtype=np.float64)\n        Xr = (X - self.x_mean_) / self.x_std_\n        x_scores = np.dot(Xr, self.x_weights_)\n        if Y is not None:\n            if Y.ndim == 1:\n                Y = Y.reshape(-1, 1)\n            Yr = (Y - self.y_mean_) / self.y_std_\n            y_scores = np.dot(Yr, self.y_weights_)\n            return x_scores, y_scores\n        return x_scores\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Learn and apply the dimension reduction on the train data.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of predictors.\n\n        y : array-like, shape = [n_samples, n_targets]\n            Target vectors, where n_samples is the number of samples and\n            n_targets is the number of response variables.\n\n        Returns\n        -------\n        x_scores if Y is not given, (x_scores, y_scores) otherwise.\n        \"\"\"\n        return self.fit(X, y).transform(X, y)\n"
    },
    {
      "filename": "sklearn/decomposition/kernel_pca.py",
      "content": "\"\"\"Kernel Principal Components Analysis\"\"\"\n\n# Author: Mathieu Blondel <mathieu@mblondel.org>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.sparse.linalg import eigsh\n\nfrom ..utils.arpack import _init_arpack_v0\nfrom ..utils.validation import check_is_fitted, check_array\nfrom ..exceptions import NotFittedError\nfrom ..base import BaseEstimator, TransformerMixin\nfrom ..preprocessing import KernelCenterer\nfrom ..metrics.pairwise import pairwise_kernels\n\n\nclass KernelPCA(BaseEstimator, TransformerMixin):\n    \"\"\"Kernel Principal component analysis (KPCA)\n\n    Non-linear dimensionality reduction through the use of kernels (see\n    :ref:`metrics`).\n\n    Read more in the :ref:`User Guide <kernel_PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components. If None, all non-zero components are kept.\n\n    kernel : \"linear\" | \"poly\" | \"rbf\" | \"sigmoid\" | \"cosine\" | \"precomputed\"\n        Kernel. Default=\"linear\".\n\n    gamma : float, default=1/n_features\n        Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other\n        kernels.\n\n    degree : int, default=3\n        Degree for poly kernels. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Independent term in poly and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : mapping of string to any, default=None\n        Parameters (keyword arguments) and values for kernel passed as\n        callable object. Ignored by other kernels.\n\n    alpha : int, default=1.0\n        Hyperparameter of the ridge regression that learns the\n        inverse transform (when fit_inverse_transform=True).\n\n    fit_inverse_transform : bool, default=False\n        Learn the inverse transform for non-precomputed kernels.\n        (i.e. learn to find the pre-image of a point)\n\n    eigen_solver : string ['auto'|'dense'|'arpack'], default='auto'\n        Select eigensolver to use. If n_components is much less than\n        the number of training samples, arpack may be more efficient\n        than the dense eigensolver.\n\n    tol : float, default=0\n        Convergence tolerance for arpack.\n        If 0, optimal value will be chosen by arpack.\n\n    max_iter : int, default=None\n        Maximum number of iterations for arpack.\n        If None, optimal value will be chosen by arpack.\n\n    remove_zero_eig : boolean, default=False\n        If True, then all components with zero eigenvalues are removed, so\n        that the number of components in the output may be < n_components\n        (and sometimes even zero due to numerical instability).\n        When n_components is None, this parameter is ignored and components\n        with zero eigenvalues are removed regardless.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``eigen_solver`` == 'arpack'.\n\n        .. versionadded:: 0.18\n\n    copy_X : boolean, default=True\n        If True, input X is copied and stored by the model in the `X_fit_`\n        attribute. If no further changes will be done to X, setting\n        `copy_X=False` saves memory by storing a reference.\n\n        .. versionadded:: 0.18\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    Attributes\n    ----------\n    lambdas_ : array, (n_components,)\n        Eigenvalues of the centered kernel matrix in decreasing order.\n        If `n_components` and `remove_zero_eig` are not set,\n        then all values are stored.\n\n    alphas_ : array, (n_samples, n_components)\n        Eigenvectors of the centered kernel matrix. If `n_components` and\n        `remove_zero_eig` are not set, then all components are stored.\n\n    dual_coef_ : array, (n_samples, n_features)\n        Inverse transform matrix. Only available when\n        ``fit_inverse_transform`` is True.\n\n    X_transformed_fit_ : array, (n_samples, n_components)\n        Projection of the fitted data on the kernel principal components.\n        Only available when ``fit_inverse_transform`` is True.\n\n    X_fit_ : (n_samples, n_features)\n        The data used to fit the model. If `copy_X=False`, then `X_fit_` is\n        a reference. This attribute is used for the calls to transform.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import KernelPCA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = KernelPCA(n_components=7, kernel='linear')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n\n    References\n    ----------\n    Kernel PCA was introduced in:\n        Bernhard Schoelkopf, Alexander J. Smola,\n        and Klaus-Robert Mueller. 1999. Kernel principal\n        component analysis. In Advances in kernel methods,\n        MIT Press, Cambridge, MA, USA 327-352.\n    \"\"\"\n\n    def __init__(self, n_components=None, kernel=\"linear\",\n                 gamma=None, degree=3, coef0=1, kernel_params=None,\n                 alpha=1.0, fit_inverse_transform=False, eigen_solver='auto',\n                 tol=0, max_iter=None, remove_zero_eig=False,\n                 random_state=None, copy_X=True, n_jobs=None):\n        if fit_inverse_transform and kernel == 'precomputed':\n            raise ValueError(\n                \"Cannot fit_inverse_transform with a precomputed kernel.\")\n        self.n_components = n_components\n        self.kernel = kernel\n        self.kernel_params = kernel_params\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.alpha = alpha\n        self.fit_inverse_transform = fit_inverse_transform\n        self.eigen_solver = eigen_solver\n        self.remove_zero_eig = remove_zero_eig\n        self.tol = tol\n        self.max_iter = max_iter\n        self.random_state = random_state\n        self.n_jobs = n_jobs\n        self.copy_X = copy_X\n\n    @property\n    def _pairwise(self):\n        return self.kernel == \"precomputed\"\n\n    def _get_kernel(self, X, Y=None):\n        if callable(self.kernel):\n            params = self.kernel_params or {}\n        else:\n            params = {\"gamma\": self.gamma,\n                      \"degree\": self.degree,\n                      \"coef0\": self.coef0}\n        return pairwise_kernels(X, Y, metric=self.kernel,\n                                filter_params=True, n_jobs=self.n_jobs,\n                                **params)\n\n    def _fit_transform(self, K):\n        \"\"\" Fit's using kernel K\"\"\"\n        # center kernel\n        K = self._centerer.fit_transform(K)\n\n        if self.n_components is None:\n            n_components = K.shape[0]\n        else:\n            n_components = min(K.shape[0], self.n_components)\n\n        # compute eigenvectors\n        if self.eigen_solver == 'auto':\n            if K.shape[0] > 200 and n_components < 10:\n                eigen_solver = 'arpack'\n            else:\n                eigen_solver = 'dense'\n        else:\n            eigen_solver = self.eigen_solver\n\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            v0 = _init_arpack_v0(K.shape[0], self.random_state)\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n\n        return K\n\n    def _fit_inverse_transform(self, X_transformed, X):\n        if hasattr(X, \"tocsr\"):\n            raise NotImplementedError(\"Inverse transform not implemented for \"\n                                      \"sparse matrices!\")\n\n        n_samples = X_transformed.shape[0]\n        K = self._get_kernel(X_transformed)\n        K.flat[::n_samples + 1] += self.alpha\n        self.dual_coef_ = linalg.solve(K, X, sym_pos=True, overwrite_a=True)\n        self.X_transformed_fit_ = X_transformed\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        X = check_array(X, accept_sparse='csr', copy=self.copy_X)\n        self._centerer = KernelCenterer()\n        K = self._get_kernel(X)\n        self._fit_transform(K)\n\n        if self.fit_inverse_transform:\n            sqrt_lambdas = np.diag(np.sqrt(self.lambdas_))\n            X_transformed = np.dot(self.alphas_, sqrt_lambdas)\n            self._fit_inverse_transform(X_transformed, X)\n\n        self.X_fit_ = X\n        return self\n\n    def fit_transform(self, X, y=None, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        self.fit(X, **params)\n\n        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n\n        return X_transformed\n\n    def transform(self, X):\n        \"\"\"Transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        check_is_fitted(self, 'X_fit_')\n\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n\n    def inverse_transform(self, X):\n        \"\"\"Transform X back to original space.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_components)\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_features)\n\n        References\n        ----------\n        \"Learning to Find Pre-Images\", G BakIr et al, 2004.\n        \"\"\"\n        if not self.fit_inverse_transform:\n            raise NotFittedError(\"The fit_inverse_transform parameter was not\"\n                                 \" set to True when instantiating and hence \"\n                                 \"the inverse transform is not available.\")\n\n        K = self._get_kernel(X, self.X_transformed_fit_)\n\n        return np.dot(K, self.dual_coef_)\n"
    },
    {
      "filename": "sklearn/decomposition/pca.py",
      "content": "\"\"\" Principal Component Analysis\n\"\"\"\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#         Olivier Grisel <olivier.grisel@ensta.org>\n#         Mathieu Blondel <mathieu@mblondel.org>\n#         Denis A. Engemann <denis-alexander.engemann@inria.fr>\n#         Michael Eickenberg <michael.eickenberg@inria.fr>\n#         Giorgio Patrini <giorgio.patrini@anu.edu.au>\n#\n# License: BSD 3 clause\n\nfrom math import log, sqrt\nimport numbers\n\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.special import gammaln\nfrom scipy.sparse import issparse\nfrom scipy.sparse.linalg import svds\n\nfrom ..externals import six\n\nfrom .base import _BasePCA\nfrom ..utils import check_random_state\nfrom ..utils import check_array\nfrom ..utils.arpack import _init_arpack_v0\nfrom ..utils.extmath import fast_logdet, randomized_svd, svd_flip\nfrom ..utils.extmath import stable_cumsum\nfrom ..utils.validation import check_is_fitted\n\n\ndef _assess_dimension_(spectrum, rank, n_samples, n_features):\n    \"\"\"Compute the likelihood of a rank ``rank`` dataset\n\n    The dataset is assumed to be embedded in gaussian noise of shape(n,\n    dimf) having spectrum ``spectrum``.\n\n    Parameters\n    ----------\n    spectrum : array of shape (n)\n        Data spectrum.\n    rank : int\n        Tested rank value.\n    n_samples : int\n        Number of samples.\n    n_features : int\n        Number of features.\n\n    Returns\n    -------\n    ll : float,\n        The log-likelihood\n\n    Notes\n    -----\n    This implements the method of `Thomas P. Minka:\n    Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604`\n    \"\"\"\n    if rank > len(spectrum):\n        raise ValueError(\"The tested rank cannot exceed the rank of the\"\n                         \" dataset\")\n\n    pu = -rank * log(2.)\n    for i in range(rank):\n        pu += (gammaln((n_features - i) / 2.) -\n               log(np.pi) * (n_features - i) / 2.)\n\n    pl = np.sum(np.log(spectrum[:rank]))\n    pl = -pl * n_samples / 2.\n\n    if rank == n_features:\n        pv = 0\n        v = 1\n    else:\n        v = np.sum(spectrum[rank:]) / (n_features - rank)\n        pv = -np.log(v) * n_samples * (n_features - rank) / 2.\n\n    m = n_features * rank - rank * (rank + 1.) / 2.\n    pp = log(2. * np.pi) * (m + rank + 1.) / 2.\n\n    pa = 0.\n    spectrum_ = spectrum.copy()\n    spectrum_[rank:n_features] = v\n    for i in range(rank):\n        for j in range(i + 1, len(spectrum)):\n            pa += log((spectrum[i] - spectrum[j]) *\n                      (1. / spectrum_[j] - 1. / spectrum_[i])) + log(n_samples)\n\n    ll = pu + pl + pv + pp - pa / 2. - rank * log(n_samples) / 2.\n\n    return ll\n\n\ndef _infer_dimension_(spectrum, n_samples, n_features):\n    \"\"\"Infers the dimension of a dataset of shape (n_samples, n_features)\n\n    The dataset is described by its spectrum `spectrum`.\n    \"\"\"\n    n_spectrum = len(spectrum)\n    ll = np.empty(n_spectrum)\n    for rank in range(n_spectrum):\n        ll[rank] = _assess_dimension_(spectrum, rank, n_samples, n_features)\n    return ll.argmax()\n\n\nclass PCA(_BasePCA):\n    \"\"\"Principal component analysis (PCA)\n\n    Linear dimensionality reduction using Singular Value Decomposition of the\n    data to project it to a lower dimensional space.\n\n    It uses the LAPACK implementation of the full SVD or a randomized truncated\n    SVD by the method of Halko et al. 2009, depending on the shape of the input\n    data and the number of components to extract.\n\n    It can also use the scipy.sparse.linalg ARPACK implementation of the\n    truncated SVD.\n\n    Notice that this class does not support sparse input. See\n    :class:`TruncatedSVD` for an alternative with sparse data.\n\n    Read more in the :ref:`User Guide <PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, float, None or string\n        Number of components to keep.\n        if n_components is not set all components are kept::\n\n            n_components == min(n_samples, n_features)\n\n        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n        MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n        will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n        If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n        number of components such that the amount of variance that needs to be\n        explained is greater than the percentage specified by n_components.\n\n        If ``svd_solver == 'arpack'``, the number of components must be\n        strictly less than the minimum of n_features and n_samples.\n\n        Hence, the None case results in::\n\n            n_components == min(n_samples, n_features) - 1\n\n    copy : bool (default True)\n        If False, data passed to fit are overwritten and running\n        fit(X).transform(X) will not yield the expected results,\n        use fit_transform(X) instead.\n\n    whiten : bool, optional (default False)\n        When True (False by default) the `components_` vectors are multiplied\n        by the square root of n_samples and then divided by the singular values\n        to ensure uncorrelated outputs with unit component-wise variances.\n\n        Whitening will remove some information from the transformed signal\n        (the relative variance scales of the components) but can sometime\n        improve the predictive accuracy of the downstream estimators by\n        making their data respect some hard-wired assumptions.\n\n    svd_solver : string {'auto', 'full', 'arpack', 'randomized'}\n        auto :\n            the solver is selected by a default policy based on `X.shape` and\n            `n_components`: if the input data is larger than 500x500 and the\n            number of components to extract is lower than 80% of the smallest\n            dimension of the data, then the more efficient 'randomized'\n            method is enabled. Otherwise the exact full SVD is computed and\n            optionally truncated afterwards.\n        full :\n            run exact full SVD calling the standard LAPACK solver via\n            `scipy.linalg.svd` and select the components by postprocessing\n        arpack :\n            run SVD truncated to n_components calling ARPACK solver via\n            `scipy.sparse.linalg.svds`. It requires strictly\n            0 < n_components < min(X.shape)\n        randomized :\n            run randomized SVD by the method of Halko et al.\n\n        .. versionadded:: 0.18.0\n\n    tol : float >= 0, optional (default .0)\n        Tolerance for singular values computed by svd_solver == 'arpack'.\n\n        .. versionadded:: 0.18.0\n\n    iterated_power : int >= 0, or 'auto', (default 'auto')\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'.\n\n        .. versionadded:: 0.18.0\n\n    random_state : int, RandomState instance or None, optional (default None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``svd_solver`` == 'arpack' or 'randomized'.\n\n        .. versionadded:: 0.18.0\n\n    Attributes\n    ----------\n    components_ : array, shape (n_components, n_features)\n        Principal axes in feature space, representing the directions of\n        maximum variance in the data. The components are sorted by\n        ``explained_variance_``.\n\n    explained_variance_ : array, shape (n_components,)\n        The amount of variance explained by each of the selected components.\n\n        Equal to n_components largest eigenvalues\n        of the covariance matrix of X.\n\n        .. versionadded:: 0.18\n\n    explained_variance_ratio_ : array, shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n        If ``n_components`` is not set then all components are stored and the\n        sum of the ratios is equal to 1.0.\n\n    singular_values_ : array, shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    mean_ : array, shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n        Equal to `X.mean(axis=0)`.\n\n    n_components_ : int\n        The estimated number of components. When n_components is set\n        to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n        number is estimated from input data. Otherwise it equals the parameter\n        n_components, or the lesser value of n_features and n_samples\n        if n_components is None.\n\n    noise_variance_ : float\n        The estimated noise covariance following the Probabilistic PCA model\n        from Tipping and Bishop 1999. See \"Pattern Recognition and\n        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n        http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n        compute the estimated data covariance and score samples.\n\n        Equal to the average of (min(n_features, n_samples) - n_components)\n        smallest eigenvalues of the covariance matrix of X.\n\n    References\n    ----------\n    For n_components == 'mle', this class uses the method of `Minka, T. P.\n    \"Automatic choice of dimensionality for PCA\". In NIPS, pp. 598-604`\n\n    Implements the probabilistic PCA model from:\n    `Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n    component analysis\". Journal of the Royal Statistical Society:\n    Series B (Statistical Methodology), 61(3), 611-622.\n    via the score and score_samples methods.\n    See http://www.miketipping.com/papers/met-mppca.pdf\n\n    For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\n    For svd_solver == 'randomized', see:\n    `Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n    \"Finding structure with randomness: Probabilistic algorithms for\n    constructing approximate matrix decompositions\".\n    SIAM review, 53(2), 217-288.` and also\n    `Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n    \"A randomized algorithm for the decomposition of matrices\".\n    Applied and Computational Harmonic Analysis, 30(1), 47-68.`\n\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import PCA\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> pca = PCA(n_components=2)\n    >>> pca.fit(X)\n    PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n      svd_solver='auto', tol=0.0, whiten=False)\n    >>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n    [0.9924... 0.0075...]\n    >>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=2, svd_solver='full')\n    >>> pca.fit(X)                 # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n    PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n      svd_solver='full', tol=0.0, whiten=False)\n    >>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n    [0.9924... 0.00755...]\n    >>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=1, svd_solver='arpack')\n    >>> pca.fit(X)\n    PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,\n      svd_solver='arpack', tol=0.0, whiten=False)\n    >>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n    [0.99244...]\n    >>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n    [6.30061...]\n\n    See also\n    --------\n    KernelPCA\n    SparsePCA\n    TruncatedSVD\n    IncrementalPCA\n    \"\"\"\n\n    def __init__(self, n_components=None, copy=True, whiten=False,\n                 svd_solver='auto', tol=0.0, iterated_power='auto',\n                 random_state=None):\n        self.n_components = n_components\n        self.copy = copy\n        self.whiten = whiten\n        self.svd_solver = svd_solver\n        self.tol = tol\n        self.iterated_power = iterated_power\n        self.random_state = random_state\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the model with X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        self._fit(X)\n        return self\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Fit the model with X and apply the dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n\n        \"\"\"\n        U, S, V = self._fit(X)\n        U = U[:, :self.n_components_]\n\n        if self.whiten:\n            # X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\n            U *= sqrt(X.shape[0] - 1)\n        else:\n            # X_new = X * V = U * S * V^T * V = U * S\n            U *= S[:self.n_components_]\n\n        return U\n\n    def _fit(self, X):\n        \"\"\"Dispatch to the right submethod depending on the chosen solver.\"\"\"\n\n        # Raise an error for sparse input.\n        # This is more informative than the generic one raised by check_array.\n        if issparse(X):\n            raise TypeError('PCA does not support sparse input. See '\n                            'TruncatedSVD for a possible alternative.')\n\n        X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,\n                        copy=self.copy)\n\n        # Handle n_components==None\n        if self.n_components is None:\n            if self.svd_solver != 'arpack':\n                n_components = min(X.shape)\n            else:\n                n_components = min(X.shape) - 1\n        else:\n            n_components = self.n_components\n\n        # Handle svd_solver\n        self._fit_svd_solver = self.svd_solver\n        if self._fit_svd_solver == 'auto':\n            # Small problem or n_components == 'mle', just call full PCA\n            if max(X.shape) <= 500 or n_components == 'mle':\n                self._fit_svd_solver = 'full'\n            elif n_components >= 1 and n_components < .8 * min(X.shape):\n                self._fit_svd_solver = 'randomized'\n            # This is also the case of n_components in (0,1)\n            else:\n                self._fit_svd_solver = 'full'\n\n        # Call different fits for either full or truncated SVD\n        if self._fit_svd_solver == 'full':\n            return self._fit_full(X, n_components)\n        elif self._fit_svd_solver in ['arpack', 'randomized']:\n            return self._fit_truncated(X, n_components, self._fit_svd_solver)\n        else:\n            raise ValueError(\"Unrecognized svd_solver='{0}'\"\n                             \"\".format(self._fit_svd_solver))\n\n    def _fit_full(self, X, n_components):\n        \"\"\"Fit the model by computing full SVD on X\"\"\"\n        n_samples, n_features = X.shape\n\n        if n_components == 'mle':\n            if n_samples < n_features:\n                raise ValueError(\"n_components='mle' is only supported \"\n                                 \"if n_samples >= n_features\")\n        elif not 0 <= n_components <= min(n_samples, n_features):\n            raise ValueError(\"n_components=%r must be between 0 and \"\n                             \"min(n_samples, n_features)=%r with \"\n                             \"svd_solver='full'\"\n                             % (n_components, min(n_samples, n_features)))\n        elif n_components >= 1:\n            if not isinstance(n_components, (numbers.Integral, np.integer)):\n                raise ValueError(\"n_components=%r must be of type int \"\n                                 \"when greater than or equal to 1, \"\n                                 \"was of type=%r\"\n                                 % (n_components, type(n_components)))\n\n        # Center data\n        self.mean_ = np.mean(X, axis=0)\n        X -= self.mean_\n\n        U, S, V = linalg.svd(X, full_matrices=False)\n        # flip eigenvectors' sign to enforce deterministic output\n        U, V = svd_flip(U, V)\n\n        components_ = V\n\n        # Get variance explained by singular values\n        explained_variance_ = (S ** 2) / (n_samples - 1)\n        total_var = explained_variance_.sum()\n        explained_variance_ratio_ = explained_variance_ / total_var\n        singular_values_ = S.copy()  # Store the singular values.\n\n        # Postprocess the number of components required\n        if n_components == 'mle':\n            n_components = \\\n                _infer_dimension_(explained_variance_, n_samples, n_features)\n        elif 0 < n_components < 1.0:\n            # number of components for which the cumulated explained\n            # variance percentage is superior to the desired threshold\n            ratio_cumsum = stable_cumsum(explained_variance_ratio_)\n            n_components = np.searchsorted(ratio_cumsum, n_components) + 1\n\n        # Compute noise covariance using Probabilistic PCA model\n        # The sigma2 maximum likelihood (cf. eq. 12.46)\n        if n_components < min(n_features, n_samples):\n            self.noise_variance_ = explained_variance_[n_components:].mean()\n        else:\n            self.noise_variance_ = 0.\n\n        self.n_samples_, self.n_features_ = n_samples, n_features\n        self.components_ = components_[:n_components]\n        self.n_components_ = n_components\n        self.explained_variance_ = explained_variance_[:n_components]\n        self.explained_variance_ratio_ = \\\n            explained_variance_ratio_[:n_components]\n        self.singular_values_ = singular_values_[:n_components]\n\n        return U, S, V\n\n    def _fit_truncated(self, X, n_components, svd_solver):\n        \"\"\"Fit the model by computing truncated SVD (by ARPACK or randomized)\n        on X\n        \"\"\"\n        n_samples, n_features = X.shape\n\n        if isinstance(n_components, six.string_types):\n            raise ValueError(\"n_components=%r cannot be a string \"\n                             \"with svd_solver='%s'\"\n                             % (n_components, svd_solver))\n        elif not 1 <= n_components <= min(n_samples, n_features):\n            raise ValueError(\"n_components=%r must be between 1 and \"\n                             \"min(n_samples, n_features)=%r with \"\n                             \"svd_solver='%s'\"\n                             % (n_components, min(n_samples, n_features),\n                                svd_solver))\n        elif not isinstance(n_components, (numbers.Integral, np.integer)):\n            raise ValueError(\"n_components=%r must be of type int \"\n                             \"when greater than or equal to 1, was of type=%r\"\n                             % (n_components, type(n_components)))\n        elif svd_solver == 'arpack' and n_components == min(n_samples,\n                                                            n_features):\n            raise ValueError(\"n_components=%r must be strictly less than \"\n                             \"min(n_samples, n_features)=%r with \"\n                             \"svd_solver='%s'\"\n                             % (n_components, min(n_samples, n_features),\n                                svd_solver))\n\n        random_state = check_random_state(self.random_state)\n\n        # Center data\n        self.mean_ = np.mean(X, axis=0)\n        X -= self.mean_\n\n        if svd_solver == 'arpack':\n            v0 = _init_arpack_v0(min(X.shape), self.random_state)\n            U, S, V = svds(X, k=n_components, tol=self.tol, v0=v0)\n            # svds doesn't abide by scipy.linalg.svd/randomized_svd\n            # conventions, so reverse its outputs.\n            S = S[::-1]\n            # flip eigenvectors' sign to enforce deterministic output\n            U, V = svd_flip(U[:, ::-1], V[::-1])\n\n        elif svd_solver == 'randomized':\n            # sign flipping is done inside\n            U, S, V = randomized_svd(X, n_components=n_components,\n                                     n_iter=self.iterated_power,\n                                     flip_sign=True,\n                                     random_state=random_state)\n\n        self.n_samples_, self.n_features_ = n_samples, n_features\n        self.components_ = V\n        self.n_components_ = n_components\n\n        # Get variance explained by singular values\n        self.explained_variance_ = (S ** 2) / (n_samples - 1)\n        total_var = np.var(X, ddof=1, axis=0)\n        self.explained_variance_ratio_ = \\\n            self.explained_variance_ / total_var.sum()\n        self.singular_values_ = S.copy()  # Store the singular values.\n\n        if self.n_components_ < min(n_features, n_samples):\n            self.noise_variance_ = (total_var.sum() -\n                                    self.explained_variance_.sum())\n            self.noise_variance_ /= min(n_features, n_samples) - n_components\n        else:\n            self.noise_variance_ = 0.\n\n        return U, S, V\n\n    def score_samples(self, X):\n        \"\"\"Return the log-likelihood of each sample.\n\n        See. \"Pattern Recognition and Machine Learning\"\n        by C. Bishop, 12.2.1 p. 574\n        or http://www.miketipping.com/papers/met-mppca.pdf\n\n        Parameters\n        ----------\n        X : array, shape(n_samples, n_features)\n            The data.\n\n        Returns\n        -------\n        ll : array, shape (n_samples,)\n            Log-likelihood of each sample under the current model\n        \"\"\"\n        check_is_fitted(self, 'mean_')\n\n        X = check_array(X)\n        Xr = X - self.mean_\n        n_features = X.shape[1]\n        precision = self.get_precision()\n        log_like = -.5 * (Xr * (np.dot(Xr, precision))).sum(axis=1)\n        log_like -= .5 * (n_features * log(2. * np.pi) -\n                          fast_logdet(precision))\n        return log_like\n\n    def score(self, X, y=None):\n        \"\"\"Return the average log-likelihood of all samples.\n\n        See. \"Pattern Recognition and Machine Learning\"\n        by C. Bishop, 12.2.1 p. 574\n        or http://www.miketipping.com/papers/met-mppca.pdf\n\n        Parameters\n        ----------\n        X : array, shape(n_samples, n_features)\n            The data.\n\n        y : Ignored\n\n        Returns\n        -------\n        ll : float\n            Average log-likelihood of the samples under the current model\n        \"\"\"\n        return np.mean(self.score_samples(X))\n"
    },
    {
      "filename": "sklearn/decomposition/truncated_svd.py",
      "content": "\"\"\"Truncated SVD for sparse matrices, aka latent semantic analysis (LSA).\n\"\"\"\n\n# Author: Lars Buitinck\n#         Olivier Grisel <olivier.grisel@ensta.org>\n#         Michael Becker <mike@beckerfuffle.com>\n# License: 3-clause BSD.\n\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import svds\n\nfrom ..base import BaseEstimator, TransformerMixin\nfrom ..utils import check_array, check_random_state\nfrom ..utils.arpack import _init_arpack_v0\nfrom ..utils.extmath import randomized_svd, safe_sparse_dot, svd_flip\nfrom ..utils.sparsefuncs import mean_variance_axis\n\n__all__ = [\"TruncatedSVD\"]\n\n\nclass TruncatedSVD(BaseEstimator, TransformerMixin):\n    \"\"\"Dimensionality reduction using truncated SVD (aka LSA).\n\n    This transformer performs linear dimensionality reduction by means of\n    truncated singular value decomposition (SVD). Contrary to PCA, this\n    estimator does not center the data before computing the singular value\n    decomposition. This means it can work with scipy.sparse matrices\n    efficiently.\n\n    In particular, truncated SVD works on term count/tf-idf matrices as\n    returned by the vectorizers in sklearn.feature_extraction.text. In that\n    context, it is known as latent semantic analysis (LSA).\n\n    This estimator supports two algorithms: a fast randomized SVD solver, and\n    a \"naive\" algorithm that uses ARPACK as an eigensolver on (X * X.T) or\n    (X.T * X), whichever is more efficient.\n\n    Read more in the :ref:`User Guide <LSA>`.\n\n    Parameters\n    ----------\n    n_components : int, default = 2\n        Desired dimensionality of output data.\n        Must be strictly less than the number of features.\n        The default value is useful for visualisation. For LSA, a value of\n        100 is recommended.\n\n    algorithm : string, default = \"randomized\"\n        SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy\n        (scipy.sparse.linalg.svds), or \"randomized\" for the randomized\n        algorithm due to Halko (2009).\n\n    n_iter : int, optional (default 5)\n        Number of iterations for randomized SVD solver. Not used by ARPACK.\n        The default is larger than the default in `randomized_svd` to handle\n        sparse matrices that may have large slowly decaying spectrum.\n\n    random_state : int, RandomState instance or None, optional, default = None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    tol : float, optional\n        Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\n        SVD solver.\n\n    Attributes\n    ----------\n    components_ : array, shape (n_components, n_features)\n\n    explained_variance_ : array, shape (n_components,)\n        The variance of the training samples transformed by a projection to\n        each component.\n\n    explained_variance_ratio_ : array, shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n    singular_values_ : array, shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import TruncatedSVD\n    >>> from sklearn.random_projection import sparse_random_matrix\n    >>> X = sparse_random_matrix(100, 100, density=0.01, random_state=42)\n    >>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> svd.fit(X)  # doctest: +NORMALIZE_WHITESPACE\n    TruncatedSVD(algorithm='randomized', n_components=5, n_iter=7,\n            random_state=42, tol=0.0)\n    >>> print(svd.explained_variance_ratio_)  # doctest: +ELLIPSIS\n    [0.0606... 0.0584... 0.0497... 0.0434... 0.0372...]\n    >>> print(svd.explained_variance_ratio_.sum())  # doctest: +ELLIPSIS\n    0.249...\n    >>> print(svd.singular_values_)  # doctest: +ELLIPSIS\n    [2.5841... 2.5245... 2.3201... 2.1753... 2.0443...]\n\n    See also\n    --------\n    PCA\n\n    References\n    ----------\n    Finding structure with randomness: Stochastic algorithms for constructing\n    approximate matrix decompositions\n    Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf\n\n    Notes\n    -----\n    SVD suffers from a problem called \"sign indeterminacy\", which means the\n    sign of the ``components_`` and the output from transform depend on the\n    algorithm and random state. To work around this, fit instances of this\n    class to data once, then keep the instance around to do transformations.\n\n    \"\"\"\n    def __init__(self, n_components=2, algorithm=\"randomized\", n_iter=5,\n                 random_state=None, tol=0.):\n        self.algorithm = algorithm\n        self.n_components = n_components\n        self.n_iter = n_iter\n        self.random_state = random_state\n        self.tol = tol\n\n    def fit(self, X, y=None):\n        \"\"\"Fit LSI model on training data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the transformer object.\n        \"\"\"\n        self.fit_transform(X)\n        return self\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Fit LSI model to X and perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : array, shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.\n        \"\"\"\n        X = check_array(X, accept_sparse=['csr', 'csc'])\n        random_state = check_random_state(self.random_state)\n\n        if self.algorithm == \"arpack\":\n            v0 = _init_arpack_v0(min(X.shape), self.random_state)\n            U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol, v0=v0)\n            # svds doesn't abide by scipy.linalg.svd/randomized_svd\n            # conventions, so reverse its outputs.\n            Sigma = Sigma[::-1]\n            U, VT = svd_flip(U[:, ::-1], VT[::-1])\n\n        elif self.algorithm == \"randomized\":\n            k = self.n_components\n            n_features = X.shape[1]\n            if k >= n_features:\n                raise ValueError(\"n_components must be < n_features;\"\n                                 \" got %d >= %d\" % (k, n_features))\n            U, Sigma, VT = randomized_svd(X, self.n_components,\n                                          n_iter=self.n_iter,\n                                          random_state=random_state)\n        else:\n            raise ValueError(\"unknown algorithm %r\" % self.algorithm)\n\n        self.components_ = VT\n\n        # Calculate explained variance & explained variance ratio\n        X_transformed = U * Sigma\n        self.explained_variance_ = exp_var = np.var(X_transformed, axis=0)\n        if sp.issparse(X):\n            _, full_var = mean_variance_axis(X, axis=0)\n            full_var = full_var.sum()\n        else:\n            full_var = np.var(X, axis=0).sum()\n        self.explained_variance_ratio_ = exp_var / full_var\n        self.singular_values_ = Sigma  # Store the singular values.\n\n        return X_transformed\n\n    def transform(self, X):\n        \"\"\"Perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            New data.\n\n        Returns\n        -------\n        X_new : array, shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.\n        \"\"\"\n        X = check_array(X, accept_sparse='csr')\n        return safe_sparse_dot(X, self.components_.T)\n\n    def inverse_transform(self, X):\n        \"\"\"Transform X back to its original space.\n\n        Returns an array X_original whose transform would be X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_components)\n            New data.\n\n        Returns\n        -------\n        X_original : array, shape (n_samples, n_features)\n            Note that this is always a dense array.\n        \"\"\"\n        X = check_array(X)\n        return np.dot(X, self.components_)\n"
    },
    {
      "filename": "sklearn/manifold/locally_linear.py",
      "content": "\"\"\"Locally Linear Embedding\"\"\"\n\n# Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>\n#         Jake Vanderplas  -- <vanderplas@astro.washington.edu>\n# License: BSD 3 clause (C) INRIA 2011\n\nimport numpy as np\nfrom scipy.linalg import eigh, svd, qr, solve\nfrom scipy.sparse import eye, csr_matrix\nfrom scipy.sparse.linalg import eigsh\n\nfrom ..base import BaseEstimator, TransformerMixin\nfrom ..utils import check_random_state, check_array\nfrom ..utils.arpack import _init_arpack_v0\nfrom ..utils.extmath import stable_cumsum\nfrom ..utils.validation import check_is_fitted\nfrom ..utils.validation import FLOAT_DTYPES\nfrom ..neighbors import NearestNeighbors\n\n\ndef barycenter_weights(X, Z, reg=1e-3):\n    \"\"\"Compute barycenter weights of X from Y along the first axis\n\n    We estimate the weights to assign to each point in Y[i] to recover\n    the point X[i]. The barycenter weights sum to 1.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_dim)\n\n    Z : array-like, shape (n_samples, n_neighbors, n_dim)\n\n    reg : float, optional\n        amount of regularization to add for the problem to be\n        well-posed in the case of n_neighbors > n_dim\n\n    Returns\n    -------\n    B : array-like, shape (n_samples, n_neighbors)\n\n    Notes\n    -----\n    See developers note for more information.\n    \"\"\"\n    X = check_array(X, dtype=FLOAT_DTYPES)\n    Z = check_array(Z, dtype=FLOAT_DTYPES, allow_nd=True)\n\n    n_samples, n_neighbors = X.shape[0], Z.shape[1]\n    B = np.empty((n_samples, n_neighbors), dtype=X.dtype)\n    v = np.ones(n_neighbors, dtype=X.dtype)\n\n    # this might raise a LinalgError if G is singular and has trace\n    # zero\n    for i, A in enumerate(Z.transpose(0, 2, 1)):\n        C = A.T - X[i]  # broadcasting\n        G = np.dot(C, C.T)\n        trace = np.trace(G)\n        if trace > 0:\n            R = reg * trace\n        else:\n            R = reg\n        G.flat[::Z.shape[1] + 1] += R\n        w = solve(G, v, sym_pos=True)\n        B[i, :] = w / np.sum(w)\n    return B\n\n\ndef barycenter_kneighbors_graph(X, n_neighbors, reg=1e-3, n_jobs=None):\n    \"\"\"Computes the barycenter weighted graph of k-Neighbors for points in X\n\n    Parameters\n    ----------\n    X : {array-like, NearestNeighbors}\n        Sample data, shape = (n_samples, n_features), in the form of a\n        numpy array or a NearestNeighbors object.\n\n    n_neighbors : int\n        Number of neighbors for each sample.\n\n    reg : float, optional\n        Amount of regularization when solving the least-squares\n        problem. Only relevant if mode='barycenter'. If None, use the\n        default.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Returns\n    -------\n    A : sparse matrix in CSR format, shape = [n_samples, n_samples]\n        A[i, j] is assigned the weight of edge that connects i to j.\n\n    See also\n    --------\n    sklearn.neighbors.kneighbors_graph\n    sklearn.neighbors.radius_neighbors_graph\n    \"\"\"\n    knn = NearestNeighbors(n_neighbors + 1, n_jobs=n_jobs).fit(X)\n    X = knn._fit_X\n    n_samples = X.shape[0]\n    ind = knn.kneighbors(X, return_distance=False)[:, 1:]\n    data = barycenter_weights(X, X[ind], reg=reg)\n    indptr = np.arange(0, n_samples * n_neighbors + 1, n_neighbors)\n    return csr_matrix((data.ravel(), ind.ravel(), indptr),\n                      shape=(n_samples, n_samples))\n\n\ndef null_space(M, k, k_skip=1, eigen_solver='arpack', tol=1E-6, max_iter=100,\n               random_state=None):\n    \"\"\"\n    Find the null space of a matrix M.\n\n    Parameters\n    ----------\n    M : {array, matrix, sparse matrix, LinearOperator}\n        Input covariance matrix: should be symmetric positive semi-definite\n\n    k : integer\n        Number of eigenvalues/vectors to return\n\n    k_skip : integer, optional\n        Number of low eigenvalues to skip.\n\n    eigen_solver : string, {'auto', 'arpack', 'dense'}\n        auto : algorithm will attempt to choose the best method for input data\n        arpack : use arnoldi iteration in shift-invert mode.\n                    For this method, M may be a dense matrix, sparse matrix,\n                    or general linear operator.\n                    Warning: ARPACK can be unstable for some problems.  It is\n                    best to try several random seeds in order to check results.\n        dense  : use standard dense matrix operations for the eigenvalue\n                    decomposition.  For this method, M must be an array\n                    or matrix type.  This method should be avoided for\n                    large problems.\n\n    tol : float, optional\n        Tolerance for 'arpack' method.\n        Not used if eigen_solver=='dense'.\n\n    max_iter : int\n        Maximum number of iterations for 'arpack' method.\n        Not used if eigen_solver=='dense'\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``solver`` == 'arpack'.\n\n    \"\"\"\n    if eigen_solver == 'auto':\n        if M.shape[0] > 200 and k + k_skip < 10:\n            eigen_solver = 'arpack'\n        else:\n            eigen_solver = 'dense'\n\n    if eigen_solver == 'arpack':\n        v0 = _init_arpack_v0(M.shape[0], random_state)\n        try:\n            eigen_values, eigen_vectors = eigsh(M, k + k_skip, sigma=0.0,\n                                                tol=tol, maxiter=max_iter,\n                                                v0=v0)\n        except RuntimeError as msg:\n            raise ValueError(\"Error in determining null-space with ARPACK. \"\n                             \"Error message: '%s'. \"\n                             \"Note that method='arpack' can fail when the \"\n                             \"weight matrix is singular or otherwise \"\n                             \"ill-behaved.  method='dense' is recommended. \"\n                             \"See online documentation for more information.\"\n                             % msg)\n\n        return eigen_vectors[:, k_skip:], np.sum(eigen_values[k_skip:])\n    elif eigen_solver == 'dense':\n        if hasattr(M, 'toarray'):\n            M = M.toarray()\n        eigen_values, eigen_vectors = eigh(\n            M, eigvals=(k_skip, k + k_skip - 1), overwrite_a=True)\n        index = np.argsort(np.abs(eigen_values))\n        return eigen_vectors[:, index], np.sum(eigen_values)\n    else:\n        raise ValueError(\"Unrecognized eigen_solver '%s'\" % eigen_solver)\n\n\ndef locally_linear_embedding(\n        X, n_neighbors, n_components, reg=1e-3, eigen_solver='auto', tol=1e-6,\n        max_iter=100, method='standard', hessian_tol=1E-4, modified_tol=1E-12,\n        random_state=None, n_jobs=None):\n    \"\"\"Perform a Locally Linear Embedding analysis on the data.\n\n    Read more in the :ref:`User Guide <locally_linear_embedding>`.\n\n    Parameters\n    ----------\n    X : {array-like, NearestNeighbors}\n        Sample data, shape = (n_samples, n_features), in the form of a\n        numpy array or a NearestNeighbors object.\n\n    n_neighbors : integer\n        number of neighbors to consider for each point.\n\n    n_components : integer\n        number of coordinates for the manifold.\n\n    reg : float\n        regularization constant, multiplies the trace of the local covariance\n        matrix of the distances.\n\n    eigen_solver : string, {'auto', 'arpack', 'dense'}\n        auto : algorithm will attempt to choose the best method for input data\n\n        arpack : use arnoldi iteration in shift-invert mode.\n                    For this method, M may be a dense matrix, sparse matrix,\n                    or general linear operator.\n                    Warning: ARPACK can be unstable for some problems.  It is\n                    best to try several random seeds in order to check results.\n\n        dense  : use standard dense matrix operations for the eigenvalue\n                    decomposition.  For this method, M must be an array\n                    or matrix type.  This method should be avoided for\n                    large problems.\n\n    tol : float, optional\n        Tolerance for 'arpack' method\n        Not used if eigen_solver=='dense'.\n\n    max_iter : integer\n        maximum number of iterations for the arpack solver.\n\n    method : {'standard', 'hessian', 'modified', 'ltsa'}\n        standard : use the standard locally linear embedding algorithm.\n                   see reference [1]_\n        hessian  : use the Hessian eigenmap method.  This method requires\n                   n_neighbors > n_components * (1 + (n_components + 1) / 2.\n                   see reference [2]_\n        modified : use the modified locally linear embedding algorithm.\n                   see reference [3]_\n        ltsa     : use local tangent space alignment algorithm\n                   see reference [4]_\n\n    hessian_tol : float, optional\n        Tolerance for Hessian eigenmapping method.\n        Only used if method == 'hessian'\n\n    modified_tol : float, optional\n        Tolerance for modified LLE method.\n        Only used if method == 'modified'\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``solver`` == 'arpack'.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Returns\n    -------\n    Y : array-like, shape [n_samples, n_components]\n        Embedding vectors.\n\n    squared_error : float\n        Reconstruction error for the embedding vectors. Equivalent to\n        ``norm(Y - W Y, 'fro')**2``, where W are the reconstruction weights.\n\n    References\n    ----------\n\n    .. [1] `Roweis, S. & Saul, L. Nonlinear dimensionality reduction\n        by locally linear embedding.  Science 290:2323 (2000).`\n    .. [2] `Donoho, D. & Grimes, C. Hessian eigenmaps: Locally\n        linear embedding techniques for high-dimensional data.\n        Proc Natl Acad Sci U S A.  100:5591 (2003).`\n    .. [3] `Zhang, Z. & Wang, J. MLLE: Modified Locally Linear\n        Embedding Using Multiple Weights.`\n        http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382\n    .. [4] `Zhang, Z. & Zha, H. Principal manifolds and nonlinear\n        dimensionality reduction via tangent space alignment.\n        Journal of Shanghai Univ.  8:406 (2004)`\n    \"\"\"\n    if eigen_solver not in ('auto', 'arpack', 'dense'):\n        raise ValueError(\"unrecognized eigen_solver '%s'\" % eigen_solver)\n\n    if method not in ('standard', 'hessian', 'modified', 'ltsa'):\n        raise ValueError(\"unrecognized method '%s'\" % method)\n\n    nbrs = NearestNeighbors(n_neighbors=n_neighbors + 1, n_jobs=n_jobs)\n    nbrs.fit(X)\n    X = nbrs._fit_X\n\n    N, d_in = X.shape\n\n    if n_components > d_in:\n        raise ValueError(\"output dimension must be less than or equal \"\n                         \"to input dimension\")\n    if n_neighbors >= N:\n        raise ValueError(\n            \"Expected n_neighbors <= n_samples, \"\n            \" but n_samples = %d, n_neighbors = %d\" %\n            (N, n_neighbors)\n        )\n\n    if n_neighbors <= 0:\n        raise ValueError(\"n_neighbors must be positive\")\n\n    M_sparse = (eigen_solver != 'dense')\n\n    if method == 'standard':\n        W = barycenter_kneighbors_graph(\n            nbrs, n_neighbors=n_neighbors, reg=reg, n_jobs=n_jobs)\n\n        # we'll compute M = (I-W)'(I-W)\n        # depending on the solver, we'll do this differently\n        if M_sparse:\n            M = eye(*W.shape, format=W.format) - W\n            M = (M.T * M).tocsr()\n        else:\n            M = (W.T * W - W.T - W).toarray()\n            M.flat[::M.shape[0] + 1] += 1  # W = W - I = W - I\n\n    elif method == 'hessian':\n        dp = n_components * (n_components + 1) // 2\n\n        if n_neighbors <= n_components + dp:\n            raise ValueError(\"for method='hessian', n_neighbors must be \"\n                             \"greater than \"\n                             \"[n_components * (n_components + 3) / 2]\")\n\n        neighbors = nbrs.kneighbors(X, n_neighbors=n_neighbors + 1,\n                                    return_distance=False)\n        neighbors = neighbors[:, 1:]\n\n        Yi = np.empty((n_neighbors, 1 + n_components + dp), dtype=np.float64)\n        Yi[:, 0] = 1\n\n        M = np.zeros((N, N), dtype=np.float64)\n\n        use_svd = (n_neighbors > d_in)\n\n        for i in range(N):\n            Gi = X[neighbors[i]]\n            Gi -= Gi.mean(0)\n\n            # build Hessian estimator\n            if use_svd:\n                U = svd(Gi, full_matrices=0)[0]\n            else:\n                Ci = np.dot(Gi, Gi.T)\n                U = eigh(Ci)[1][:, ::-1]\n\n            Yi[:, 1:1 + n_components] = U[:, :n_components]\n\n            j = 1 + n_components\n            for k in range(n_components):\n                Yi[:, j:j + n_components - k] = (U[:, k:k + 1] *\n                                                 U[:, k:n_components])\n                j += n_components - k\n\n            Q, R = qr(Yi)\n\n            w = Q[:, n_components + 1:]\n            S = w.sum(0)\n\n            S[np.where(abs(S) < hessian_tol)] = 1\n            w /= S\n\n            nbrs_x, nbrs_y = np.meshgrid(neighbors[i], neighbors[i])\n            M[nbrs_x, nbrs_y] += np.dot(w, w.T)\n\n        if M_sparse:\n            M = csr_matrix(M)\n\n    elif method == 'modified':\n        if n_neighbors < n_components:\n            raise ValueError(\"modified LLE requires \"\n                             \"n_neighbors >= n_components\")\n\n        neighbors = nbrs.kneighbors(X, n_neighbors=n_neighbors + 1,\n                                    return_distance=False)\n        neighbors = neighbors[:, 1:]\n\n        # find the eigenvectors and eigenvalues of each local covariance\n        # matrix. We want V[i] to be a [n_neighbors x n_neighbors] matrix,\n        # where the columns are eigenvectors\n        V = np.zeros((N, n_neighbors, n_neighbors))\n        nev = min(d_in, n_neighbors)\n        evals = np.zeros([N, nev])\n\n        # choose the most efficient way to find the eigenvectors\n        use_svd = (n_neighbors > d_in)\n\n        if use_svd:\n            for i in range(N):\n                X_nbrs = X[neighbors[i]] - X[i]\n                V[i], evals[i], _ = svd(X_nbrs,\n                                        full_matrices=True)\n            evals **= 2\n        else:\n            for i in range(N):\n                X_nbrs = X[neighbors[i]] - X[i]\n                C_nbrs = np.dot(X_nbrs, X_nbrs.T)\n                evi, vi = eigh(C_nbrs)\n                evals[i] = evi[::-1]\n                V[i] = vi[:, ::-1]\n\n        # find regularized weights: this is like normal LLE.\n        # because we've already computed the SVD of each covariance matrix,\n        # it's faster to use this rather than np.linalg.solve\n        reg = 1E-3 * evals.sum(1)\n\n        tmp = np.dot(V.transpose(0, 2, 1), np.ones(n_neighbors))\n        tmp[:, :nev] /= evals + reg[:, None]\n        tmp[:, nev:] /= reg[:, None]\n\n        w_reg = np.zeros((N, n_neighbors))\n        for i in range(N):\n            w_reg[i] = np.dot(V[i], tmp[i])\n        w_reg /= w_reg.sum(1)[:, None]\n\n        # calculate eta: the median of the ratio of small to large eigenvalues\n        # across the points.  This is used to determine s_i, below\n        rho = evals[:, n_components:].sum(1) / evals[:, :n_components].sum(1)\n        eta = np.median(rho)\n\n        # find s_i, the size of the \"almost null space\" for each point:\n        # this is the size of the largest set of eigenvalues\n        # such that Sum[v; v in set]/Sum[v; v not in set] < eta\n        s_range = np.zeros(N, dtype=int)\n        evals_cumsum = stable_cumsum(evals, 1)\n        eta_range = evals_cumsum[:, -1:] / evals_cumsum[:, :-1] - 1\n        for i in range(N):\n            s_range[i] = np.searchsorted(eta_range[i, ::-1], eta)\n        s_range += n_neighbors - nev  # number of zero eigenvalues\n\n        # Now calculate M.\n        # This is the [N x N] matrix whose null space is the desired embedding\n        M = np.zeros((N, N), dtype=np.float64)\n        for i in range(N):\n            s_i = s_range[i]\n\n            # select bottom s_i eigenvectors and calculate alpha\n            Vi = V[i, :, n_neighbors - s_i:]\n            alpha_i = np.linalg.norm(Vi.sum(0)) / np.sqrt(s_i)\n\n            # compute Householder matrix which satisfies\n            #  Hi*Vi.T*ones(n_neighbors) = alpha_i*ones(s)\n            # using prescription from paper\n            h = np.full(s_i, alpha_i) - np.dot(Vi.T, np.ones(n_neighbors))\n\n            norm_h = np.linalg.norm(h)\n            if norm_h < modified_tol:\n                h *= 0\n            else:\n                h /= norm_h\n\n            # Householder matrix is\n            #  >> Hi = np.identity(s_i) - 2*np.outer(h,h)\n            # Then the weight matrix is\n            #  >> Wi = np.dot(Vi,Hi) + (1-alpha_i) * w_reg[i,:,None]\n            # We do this much more efficiently:\n            Wi = (Vi - 2 * np.outer(np.dot(Vi, h), h) +\n                  (1 - alpha_i) * w_reg[i, :, None])\n\n            # Update M as follows:\n            # >> W_hat = np.zeros( (N,s_i) )\n            # >> W_hat[neighbors[i],:] = Wi\n            # >> W_hat[i] -= 1\n            # >> M += np.dot(W_hat,W_hat.T)\n            # We can do this much more efficiently:\n            nbrs_x, nbrs_y = np.meshgrid(neighbors[i], neighbors[i])\n            M[nbrs_x, nbrs_y] += np.dot(Wi, Wi.T)\n            Wi_sum1 = Wi.sum(1)\n            M[i, neighbors[i]] -= Wi_sum1\n            M[neighbors[i], i] -= Wi_sum1\n            M[i, i] += s_i\n\n        if M_sparse:\n            M = csr_matrix(M)\n\n    elif method == 'ltsa':\n        neighbors = nbrs.kneighbors(X, n_neighbors=n_neighbors + 1,\n                                    return_distance=False)\n        neighbors = neighbors[:, 1:]\n\n        M = np.zeros((N, N))\n\n        use_svd = (n_neighbors > d_in)\n\n        for i in range(N):\n            Xi = X[neighbors[i]]\n            Xi -= Xi.mean(0)\n\n            # compute n_components largest eigenvalues of Xi * Xi^T\n            if use_svd:\n                v = svd(Xi, full_matrices=True)[0]\n            else:\n                Ci = np.dot(Xi, Xi.T)\n                v = eigh(Ci)[1][:, ::-1]\n\n            Gi = np.zeros((n_neighbors, n_components + 1))\n            Gi[:, 1:] = v[:, :n_components]\n            Gi[:, 0] = 1. / np.sqrt(n_neighbors)\n\n            GiGiT = np.dot(Gi, Gi.T)\n\n            nbrs_x, nbrs_y = np.meshgrid(neighbors[i], neighbors[i])\n            M[nbrs_x, nbrs_y] -= GiGiT\n            M[neighbors[i], neighbors[i]] += 1\n\n    return null_space(M, n_components, k_skip=1, eigen_solver=eigen_solver,\n                      tol=tol, max_iter=max_iter, random_state=random_state)\n\n\nclass LocallyLinearEmbedding(BaseEstimator, TransformerMixin):\n    \"\"\"Locally Linear Embedding\n\n    Read more in the :ref:`User Guide <locally_linear_embedding>`.\n\n    Parameters\n    ----------\n    n_neighbors : integer\n        number of neighbors to consider for each point.\n\n    n_components : integer\n        number of coordinates for the manifold\n\n    reg : float\n        regularization constant, multiplies the trace of the local covariance\n        matrix of the distances.\n\n    eigen_solver : string, {'auto', 'arpack', 'dense'}\n        auto : algorithm will attempt to choose the best method for input data\n\n        arpack : use arnoldi iteration in shift-invert mode.\n                    For this method, M may be a dense matrix, sparse matrix,\n                    or general linear operator.\n                    Warning: ARPACK can be unstable for some problems.  It is\n                    best to try several random seeds in order to check results.\n\n        dense  : use standard dense matrix operations for the eigenvalue\n                    decomposition.  For this method, M must be an array\n                    or matrix type.  This method should be avoided for\n                    large problems.\n\n    tol : float, optional\n        Tolerance for 'arpack' method\n        Not used if eigen_solver=='dense'.\n\n    max_iter : integer\n        maximum number of iterations for the arpack solver.\n        Not used if eigen_solver=='dense'.\n\n    method : string ('standard', 'hessian', 'modified' or 'ltsa')\n        standard : use the standard locally linear embedding algorithm.  see\n                   reference [1]\n        hessian  : use the Hessian eigenmap method. This method requires\n                   ``n_neighbors > n_components * (1 + (n_components + 1) / 2``\n                   see reference [2]\n        modified : use the modified locally linear embedding algorithm.\n                   see reference [3]\n        ltsa     : use local tangent space alignment algorithm\n                   see reference [4]\n\n    hessian_tol : float, optional\n        Tolerance for Hessian eigenmapping method.\n        Only used if ``method == 'hessian'``\n\n    modified_tol : float, optional\n        Tolerance for modified LLE method.\n        Only used if ``method == 'modified'``\n\n    neighbors_algorithm : string ['auto'|'brute'|'kd_tree'|'ball_tree']\n        algorithm to use for nearest neighbors search,\n        passed to neighbors.NearestNeighbors instance\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``eigen_solver`` == 'arpack'.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    embedding_ : array-like, shape [n_samples, n_components]\n        Stores the embedding vectors\n\n    reconstruction_error_ : float\n        Reconstruction error associated with `embedding_`\n\n    nbrs_ : NearestNeighbors object\n        Stores nearest neighbors instance, including BallTree or KDtree\n        if applicable.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.manifold import LocallyLinearEmbedding\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> embedding = LocallyLinearEmbedding(n_components=2)\n    >>> X_transformed = embedding.fit_transform(X[:100])\n    >>> X_transformed.shape\n    (100, 2)\n\n    References\n    ----------\n\n    .. [1] `Roweis, S. & Saul, L. Nonlinear dimensionality reduction\n        by locally linear embedding.  Science 290:2323 (2000).`\n    .. [2] `Donoho, D. & Grimes, C. Hessian eigenmaps: Locally\n        linear embedding techniques for high-dimensional data.\n        Proc Natl Acad Sci U S A.  100:5591 (2003).`\n    .. [3] `Zhang, Z. & Wang, J. MLLE: Modified Locally Linear\n        Embedding Using Multiple Weights.`\n        http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382\n    .. [4] `Zhang, Z. & Zha, H. Principal manifolds and nonlinear\n        dimensionality reduction via tangent space alignment.\n        Journal of Shanghai Univ.  8:406 (2004)`\n    \"\"\"\n\n    def __init__(self, n_neighbors=5, n_components=2, reg=1E-3,\n                 eigen_solver='auto', tol=1E-6, max_iter=100,\n                 method='standard', hessian_tol=1E-4, modified_tol=1E-12,\n                 neighbors_algorithm='auto', random_state=None, n_jobs=None):\n        self.n_neighbors = n_neighbors\n        self.n_components = n_components\n        self.reg = reg\n        self.eigen_solver = eigen_solver\n        self.tol = tol\n        self.max_iter = max_iter\n        self.method = method\n        self.hessian_tol = hessian_tol\n        self.modified_tol = modified_tol\n        self.random_state = random_state\n        self.neighbors_algorithm = neighbors_algorithm\n        self.n_jobs = n_jobs\n\n    def _fit_transform(self, X):\n        self.nbrs_ = NearestNeighbors(self.n_neighbors,\n                                      algorithm=self.neighbors_algorithm,\n                                      n_jobs=self.n_jobs)\n\n        random_state = check_random_state(self.random_state)\n        X = check_array(X, dtype=float)\n        self.nbrs_.fit(X)\n        self.embedding_, self.reconstruction_error_ = \\\n            locally_linear_embedding(\n                self.nbrs_, self.n_neighbors, self.n_components,\n                eigen_solver=self.eigen_solver, tol=self.tol,\n                max_iter=self.max_iter, method=self.method,\n                hessian_tol=self.hessian_tol, modified_tol=self.modified_tol,\n                random_state=random_state, reg=self.reg, n_jobs=self.n_jobs)\n\n    def fit(self, X, y=None):\n        \"\"\"Compute the embedding vectors for data X\n\n        Parameters\n        ----------\n        X : array-like of shape [n_samples, n_features]\n            training set.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : returns an instance of self.\n        \"\"\"\n        self._fit_transform(X)\n        return self\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Compute the embedding vectors for data X and transform X.\n\n        Parameters\n        ----------\n        X : array-like of shape [n_samples, n_features]\n            training set.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        self._fit_transform(X)\n        return self.embedding_\n\n    def transform(self, X):\n        \"\"\"\n        Transform new points into embedding space.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n\n        Returns\n        -------\n        X_new : array, shape = [n_samples, n_components]\n\n        Notes\n        -----\n        Because of scaling performed by this method, it is discouraged to use\n        it together with methods that are not scale-invariant (like SVMs)\n        \"\"\"\n        check_is_fitted(self, \"nbrs_\")\n\n        X = check_array(X)\n        ind = self.nbrs_.kneighbors(X, n_neighbors=self.n_neighbors,\n                                    return_distance=False)\n        weights = barycenter_weights(X, self.nbrs_._fit_X[ind],\n                                     reg=self.reg)\n        X_new = np.empty((X.shape[0], self.n_components))\n        for i in range(X.shape[0]):\n            X_new[i] = np.dot(self.embedding_[ind[i]].T, weights[i])\n        return X_new\n"
    },
    {
      "filename": "sklearn/manifold/spectral_embedding_.py",
      "content": "\"\"\"Spectral Embedding\"\"\"\n\n# Author: Gael Varoquaux <gael.varoquaux@normalesup.org>\n#         Wei LI <kuantkid@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import division\n\nimport warnings\n\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.linalg import eigh\nfrom scipy.sparse.linalg import eigsh, lobpcg\nfrom scipy.sparse.csgraph import connected_components\nfrom scipy.sparse.csgraph import laplacian as csgraph_laplacian\n\nfrom ..base import BaseEstimator\nfrom ..externals import six\nfrom ..utils import (check_random_state, check_array,\n                     check_symmetric)\nfrom ..utils.arpack import _init_arpack_v0\nfrom ..utils.extmath import _deterministic_vector_sign_flip\nfrom ..metrics.pairwise import rbf_kernel\nfrom ..neighbors import kneighbors_graph\n\n\ndef _graph_connected_component(graph, node_id):\n    \"\"\"Find the largest graph connected components that contains one\n    given node\n\n    Parameters\n    ----------\n    graph : array-like, shape: (n_samples, n_samples)\n        adjacency matrix of the graph, non-zero weight means an edge\n        between the nodes\n\n    node_id : int\n        The index of the query node of the graph\n\n    Returns\n    -------\n    connected_components_matrix : array-like, shape: (n_samples,)\n        An array of bool value indicating the indexes of the nodes\n        belonging to the largest connected components of the given query\n        node\n    \"\"\"\n    n_node = graph.shape[0]\n    if sparse.issparse(graph):\n        # speed up row-wise access to boolean connection mask\n        graph = graph.tocsr()\n    connected_nodes = np.zeros(n_node, dtype=np.bool)\n    nodes_to_explore = np.zeros(n_node, dtype=np.bool)\n    nodes_to_explore[node_id] = True\n    for _ in range(n_node):\n        last_num_component = connected_nodes.sum()\n        np.logical_or(connected_nodes, nodes_to_explore, out=connected_nodes)\n        if last_num_component >= connected_nodes.sum():\n            break\n        indices = np.where(nodes_to_explore)[0]\n        nodes_to_explore.fill(False)\n        for i in indices:\n            if sparse.issparse(graph):\n                neighbors = graph[i].toarray().ravel()\n            else:\n                neighbors = graph[i]\n            np.logical_or(nodes_to_explore, neighbors, out=nodes_to_explore)\n    return connected_nodes\n\n\ndef _graph_is_connected(graph):\n    \"\"\" Return whether the graph is connected (True) or Not (False)\n\n    Parameters\n    ----------\n    graph : array-like or sparse matrix, shape: (n_samples, n_samples)\n        adjacency matrix of the graph, non-zero weight means an edge\n        between the nodes\n\n    Returns\n    -------\n    is_connected : bool\n        True means the graph is fully connected and False means not\n    \"\"\"\n    if sparse.isspmatrix(graph):\n        # sparse graph, find all the connected components\n        n_connected_components, _ = connected_components(graph)\n        return n_connected_components == 1\n    else:\n        # dense graph, find all connected components start from node 0\n        return _graph_connected_component(graph, 0).sum() == graph.shape[0]\n\n\ndef _set_diag(laplacian, value, norm_laplacian):\n    \"\"\"Set the diagonal of the laplacian matrix and convert it to a\n    sparse format well suited for eigenvalue decomposition\n\n    Parameters\n    ----------\n    laplacian : array or sparse matrix\n        The graph laplacian\n    value : float\n        The value of the diagonal\n    norm_laplacian : bool\n        Whether the value of the diagonal should be changed or not\n\n    Returns\n    -------\n    laplacian : array or sparse matrix\n        An array of matrix in a form that is well suited to fast\n        eigenvalue decomposition, depending on the band width of the\n        matrix.\n    \"\"\"\n    n_nodes = laplacian.shape[0]\n    # We need all entries in the diagonal to values\n    if not sparse.isspmatrix(laplacian):\n        if norm_laplacian:\n            laplacian.flat[::n_nodes + 1] = value\n    else:\n        laplacian = laplacian.tocoo()\n        if norm_laplacian:\n            diag_idx = (laplacian.row == laplacian.col)\n            laplacian.data[diag_idx] = value\n        # If the matrix has a small number of diagonals (as in the\n        # case of structured matrices coming from images), the\n        # dia format might be best suited for matvec products:\n        n_diags = np.unique(laplacian.row - laplacian.col).size\n        if n_diags <= 7:\n            # 3 or less outer diagonals on each side\n            laplacian = laplacian.todia()\n        else:\n            # csr has the fastest matvec and is thus best suited to\n            # arpack\n            laplacian = laplacian.tocsr()\n    return laplacian\n\n\ndef spectral_embedding(adjacency, n_components=8, eigen_solver=None,\n                       random_state=None, eigen_tol=0.0,\n                       norm_laplacian=True, drop_first=True):\n    \"\"\"Project the sample on the first eigenvectors of the graph Laplacian.\n\n    The adjacency matrix is used to compute a normalized graph Laplacian\n    whose spectrum (especially the eigenvectors associated to the\n    smallest eigenvalues) has an interpretation in terms of minimal\n    number of cuts necessary to split the graph into comparably sized\n    components.\n\n    This embedding can also 'work' even if the ``adjacency`` variable is\n    not strictly the adjacency matrix of a graph but more generally\n    an affinity or similarity matrix between samples (for instance the\n    heat kernel of a euclidean distance matrix or a k-NN matrix).\n\n    However care must taken to always make the affinity matrix symmetric\n    so that the eigenvector decomposition works as expected.\n\n    Note : Laplacian Eigenmaps is the actual algorithm implemented here.\n\n    Read more in the :ref:`User Guide <spectral_embedding>`.\n\n    Parameters\n    ----------\n    adjacency : array-like or sparse matrix, shape: (n_samples, n_samples)\n        The adjacency matrix of the graph to embed.\n\n    n_components : integer, optional, default 8\n        The dimension of the projection subspace.\n\n    eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}, default None\n        The eigenvalue decomposition strategy to use. AMG requires pyamg\n        to be installed. It can be faster on very large, sparse problems,\n        but may also lead to instabilities.\n\n    random_state : int, RandomState instance or None, optional, default: None\n        A pseudo random number generator used for the initialization of the\n        lobpcg eigenvectors decomposition.  If int, random_state is the seed\n        used by the random number generator; If RandomState instance,\n        random_state is the random number generator; If None, the random number\n        generator is the RandomState instance used by `np.random`. Used when\n        ``solver`` == 'amg'.\n\n    eigen_tol : float, optional, default=0.0\n        Stopping criterion for eigendecomposition of the Laplacian matrix\n        when using arpack eigen_solver.\n\n    norm_laplacian : bool, optional, default=True\n        If True, then compute normalized Laplacian.\n\n    drop_first : bool, optional, default=True\n        Whether to drop the first eigenvector. For spectral embedding, this\n        should be True as the first eigenvector should be constant vector for\n        connected graph, but for spectral clustering, this should be kept as\n        False to retain the first eigenvector.\n\n    Returns\n    -------\n    embedding : array, shape=(n_samples, n_components)\n        The reduced samples.\n\n    Notes\n    -----\n    Spectral Embedding (Laplacian Eigenmaps) is most useful when the graph\n    has one connected component. If there graph has many components, the first\n    few eigenvectors will simply uncover the connected components of the graph.\n\n    References\n    ----------\n    * https://en.wikipedia.org/wiki/LOBPCG\n\n    * Toward the Optimal Preconditioned Eigensolver: Locally Optimal\n      Block Preconditioned Conjugate Gradient Method\n      Andrew V. Knyazev\n      https://doi.org/10.1137%2FS1064827500366124\n    \"\"\"\n    adjacency = check_symmetric(adjacency)\n\n    try:\n        from pyamg import smoothed_aggregation_solver\n    except ImportError:\n        if eigen_solver == \"amg\":\n            raise ValueError(\"The eigen_solver was set to 'amg', but pyamg is \"\n                             \"not available.\")\n\n    if eigen_solver is None:\n        eigen_solver = 'arpack'\n    elif eigen_solver not in ('arpack', 'lobpcg', 'amg'):\n        raise ValueError(\"Unknown value for eigen_solver: '%s'.\"\n                         \"Should be 'amg', 'arpack', or 'lobpcg'\"\n                         % eigen_solver)\n\n    random_state = check_random_state(random_state)\n\n    n_nodes = adjacency.shape[0]\n    # Whether to drop the first eigenvector\n    if drop_first:\n        n_components = n_components + 1\n\n    if not _graph_is_connected(adjacency):\n        warnings.warn(\"Graph is not fully connected, spectral embedding\"\n                      \" may not work as expected.\")\n\n    laplacian, dd = csgraph_laplacian(adjacency, normed=norm_laplacian,\n                                      return_diag=True)\n    if (eigen_solver == 'arpack' or eigen_solver != 'lobpcg' and\n       (not sparse.isspmatrix(laplacian) or n_nodes < 5 * n_components)):\n        # lobpcg used with eigen_solver='amg' has bugs for low number of nodes\n        # for details see the source code in scipy:\n        # https://github.com/scipy/scipy/blob/v0.11.0/scipy/sparse/linalg/eigen\n        # /lobpcg/lobpcg.py#L237\n        # or matlab:\n        # https://www.mathworks.com/matlabcentral/fileexchange/48-lobpcg-m\n        laplacian = _set_diag(laplacian, 1, norm_laplacian)\n\n        # Here we'll use shift-invert mode for fast eigenvalues\n        # (see https://docs.scipy.org/doc/scipy/reference/tutorial/arpack.html\n        #  for a short explanation of what this means)\n        # Because the normalized Laplacian has eigenvalues between 0 and 2,\n        # I - L has eigenvalues between -1 and 1.  ARPACK is most efficient\n        # when finding eigenvalues of largest magnitude (keyword which='LM')\n        # and when these eigenvalues are very large compared to the rest.\n        # For very large, very sparse graphs, I - L can have many, many\n        # eigenvalues very near 1.0.  This leads to slow convergence.  So\n        # instead, we'll use ARPACK's shift-invert mode, asking for the\n        # eigenvalues near 1.0.  This effectively spreads-out the spectrum\n        # near 1.0 and leads to much faster convergence: potentially an\n        # orders-of-magnitude speedup over simply using keyword which='LA'\n        # in standard mode.\n        try:\n            # We are computing the opposite of the laplacian inplace so as\n            # to spare a memory allocation of a possibly very large array\n            laplacian *= -1\n            v0 = _init_arpack_v0(laplacian.shape[0], random_state)\n            lambdas, diffusion_map = eigsh(laplacian, k=n_components,\n                                           sigma=1.0, which='LM',\n                                           tol=eigen_tol, v0=v0)\n            embedding = diffusion_map.T[n_components::-1]\n            if norm_laplacian:\n                embedding = embedding / dd\n        except RuntimeError:\n            # When submatrices are exactly singular, an LU decomposition\n            # in arpack fails. We fallback to lobpcg\n            eigen_solver = \"lobpcg\"\n            # Revert the laplacian to its opposite to have lobpcg work\n            laplacian *= -1\n\n    if eigen_solver == 'amg':\n        # Use AMG to get a preconditioner and speed up the eigenvalue\n        # problem.\n        if not sparse.issparse(laplacian):\n            warnings.warn(\"AMG works better for sparse matrices\")\n        # lobpcg needs double precision floats\n        laplacian = check_array(laplacian, dtype=np.float64,\n                                accept_sparse=True)\n        laplacian = _set_diag(laplacian, 1, norm_laplacian)\n        ml = smoothed_aggregation_solver(check_array(laplacian, 'csr'))\n        M = ml.aspreconditioner()\n        X = random_state.rand(laplacian.shape[0], n_components + 1)\n        X[:, 0] = dd.ravel()\n        lambdas, diffusion_map = lobpcg(laplacian, X, M=M, tol=1.e-12,\n                                        largest=False)\n        embedding = diffusion_map.T\n        if norm_laplacian:\n            embedding = embedding / dd\n        if embedding.shape[0] == 1:\n            raise ValueError\n\n    elif eigen_solver == \"lobpcg\":\n        # lobpcg needs double precision floats\n        laplacian = check_array(laplacian, dtype=np.float64,\n                                accept_sparse=True)\n        if n_nodes < 5 * n_components + 1:\n            # see note above under arpack why lobpcg has problems with small\n            # number of nodes\n            # lobpcg will fallback to eigh, so we short circuit it\n            if sparse.isspmatrix(laplacian):\n                laplacian = laplacian.toarray()\n            lambdas, diffusion_map = eigh(laplacian)\n            embedding = diffusion_map.T[:n_components]\n            if norm_laplacian:\n                embedding = embedding / dd\n        else:\n            laplacian = _set_diag(laplacian, 1, norm_laplacian)\n            # We increase the number of eigenvectors requested, as lobpcg\n            # doesn't behave well in low dimension\n            X = random_state.rand(laplacian.shape[0], n_components + 1)\n            X[:, 0] = dd.ravel()\n            lambdas, diffusion_map = lobpcg(laplacian, X, tol=1e-15,\n                                            largest=False, maxiter=2000)\n            embedding = diffusion_map.T[:n_components]\n            if norm_laplacian:\n                embedding = embedding / dd\n            if embedding.shape[0] == 1:\n                raise ValueError\n\n    embedding = _deterministic_vector_sign_flip(embedding)\n    if drop_first:\n        return embedding[1:n_components].T\n    else:\n        return embedding[:n_components].T\n\n\nclass SpectralEmbedding(BaseEstimator):\n    \"\"\"Spectral embedding for non-linear dimensionality reduction.\n\n    Forms an affinity matrix given by the specified function and\n    applies spectral decomposition to the corresponding graph laplacian.\n    The resulting transformation is given by the value of the\n    eigenvectors for each data point.\n\n    Note : Laplacian Eigenmaps is the actual algorithm implemented here.\n\n    Read more in the :ref:`User Guide <spectral_embedding>`.\n\n    Parameters\n    -----------\n    n_components : integer, default: 2\n        The dimension of the projected subspace.\n\n    affinity : string or callable, default : \"nearest_neighbors\"\n        How to construct the affinity matrix.\n         - 'nearest_neighbors' : construct affinity matrix by knn graph\n         - 'rbf' : construct affinity matrix by rbf kernel\n         - 'precomputed' : interpret X as precomputed affinity matrix\n         - callable : use passed in function as affinity\n           the function takes in data matrix (n_samples, n_features)\n           and return affinity matrix (n_samples, n_samples).\n\n    gamma : float, optional, default : 1/n_features\n        Kernel coefficient for rbf kernel.\n\n    random_state : int, RandomState instance or None, optional, default: None\n        A pseudo random number generator used for the initialization of the\n        lobpcg eigenvectors.  If int, random_state is the seed used by the\n        random number generator; If RandomState instance, random_state is the\n        random number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`. Used when ``solver`` ==\n        'amg'.\n\n    eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}\n        The eigenvalue decomposition strategy to use. AMG requires pyamg\n        to be installed. It can be faster on very large, sparse problems,\n        but may also lead to instabilities.\n\n    n_neighbors : int, default : max(n_samples/10 , 1)\n        Number of nearest neighbors for nearest_neighbors graph building.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n\n    embedding_ : array, shape = (n_samples, n_components)\n        Spectral embedding of the training matrix.\n\n    affinity_matrix_ : array, shape = (n_samples, n_samples)\n        Affinity_matrix constructed from samples or precomputed.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.manifold import SpectralEmbedding\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> embedding = SpectralEmbedding(n_components=2)\n    >>> X_transformed = embedding.fit_transform(X[:100])\n    >>> X_transformed.shape\n    (100, 2)\n\n    References\n    ----------\n\n    - A Tutorial on Spectral Clustering, 2007\n      Ulrike von Luxburg\n      http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n    - On Spectral Clustering: Analysis and an algorithm, 2001\n      Andrew Y. Ng, Michael I. Jordan, Yair Weiss\n      http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100\n\n    - Normalized cuts and image segmentation, 2000\n      Jianbo Shi, Jitendra Malik\n      http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n    \"\"\"\n\n    def __init__(self, n_components=2, affinity=\"nearest_neighbors\",\n                 gamma=None, random_state=None, eigen_solver=None,\n                 n_neighbors=None, n_jobs=None):\n        self.n_components = n_components\n        self.affinity = affinity\n        self.gamma = gamma\n        self.random_state = random_state\n        self.eigen_solver = eigen_solver\n        self.n_neighbors = n_neighbors\n        self.n_jobs = n_jobs\n\n    @property\n    def _pairwise(self):\n        return self.affinity == \"precomputed\"\n\n    def _get_affinity_matrix(self, X, Y=None):\n        \"\"\"Calculate the affinity matrix from data\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples\n            and n_features is the number of features.\n\n            If affinity is \"precomputed\"\n            X : array-like, shape (n_samples, n_samples),\n            Interpret X as precomputed adjacency graph computed from\n            samples.\n\n        Y: Ignored\n\n        Returns\n        -------\n        affinity_matrix, shape (n_samples, n_samples)\n        \"\"\"\n        if self.affinity == 'precomputed':\n            self.affinity_matrix_ = X\n            return self.affinity_matrix_\n        if self.affinity == 'nearest_neighbors':\n            if sparse.issparse(X):\n                warnings.warn(\"Nearest neighbors affinity currently does \"\n                              \"not support sparse input, falling back to \"\n                              \"rbf affinity\")\n                self.affinity = \"rbf\"\n            else:\n                self.n_neighbors_ = (self.n_neighbors\n                                     if self.n_neighbors is not None\n                                     else max(int(X.shape[0] / 10), 1))\n                self.affinity_matrix_ = kneighbors_graph(X, self.n_neighbors_,\n                                                         include_self=True,\n                                                         n_jobs=self.n_jobs)\n                # currently only symmetric affinity_matrix supported\n                self.affinity_matrix_ = 0.5 * (self.affinity_matrix_ +\n                                               self.affinity_matrix_.T)\n                return self.affinity_matrix_\n        if self.affinity == 'rbf':\n            self.gamma_ = (self.gamma\n                           if self.gamma is not None else 1.0 / X.shape[1])\n            self.affinity_matrix_ = rbf_kernel(X, gamma=self.gamma_)\n            return self.affinity_matrix_\n        self.affinity_matrix_ = self.affinity(X)\n        return self.affinity_matrix_\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples\n            and n_features is the number of features.\n\n            If affinity is \"precomputed\"\n            X : array-like, shape (n_samples, n_samples),\n            Interpret X as precomputed adjacency graph computed from\n            samples.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n\n        X = check_array(X, ensure_min_samples=2, estimator=self)\n\n        random_state = check_random_state(self.random_state)\n        if isinstance(self.affinity, six.string_types):\n            if self.affinity not in set((\"nearest_neighbors\", \"rbf\",\n                                         \"precomputed\")):\n                raise ValueError((\"%s is not a valid affinity. Expected \"\n                                  \"'precomputed', 'rbf', 'nearest_neighbors' \"\n                                  \"or a callable.\") % self.affinity)\n        elif not callable(self.affinity):\n            raise ValueError((\"'affinity' is expected to be an affinity \"\n                              \"name or a callable. Got: %s\") % self.affinity)\n\n        affinity_matrix = self._get_affinity_matrix(X)\n        self.embedding_ = spectral_embedding(affinity_matrix,\n                                             n_components=self.n_components,\n                                             eigen_solver=self.eigen_solver,\n                                             random_state=random_state)\n        return self\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples\n            and n_features is the number of features.\n\n            If affinity is \"precomputed\"\n            X : array-like, shape (n_samples, n_samples),\n            Interpret X as precomputed adjacency graph computed from\n            samples.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        self.fit(X)\n        return self.embedding_\n"
    },
    {
      "filename": "sklearn/utils/arpack.py",
      "content": "from .validation import check_random_state\n\n\ndef _init_arpack_v0(size, random_state):\n    \"\"\"Initialize the starting vector for iteration in ARPACK functions\n\n    Initialize a ndarray with values sampled from the uniform distribution\n    on [-1, 1]. This initialization model has been chosen to be\n    consistent with the ARPACK one as another initialization can lead to\n    convergence issues.\n\n    Parameters\n    ----------\n    size : int\n        the size of the eigenvalue vector to be initialized\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`.\n\n    Returns\n    -------\n    v0 : array of shape (size,)\n        the initialized vector\n    \"\"\"\n\n    random_state = check_random_state(random_state)\n    v0 = random_state.uniform(-1, 1, size)\n    return v0\n"
    },
    {
      "filename": "sklearn/utils/tests/test_arpack.py",
      "content": "import numpy as np\n\nfrom sklearn.utils.arpack import _init_arpack_v0\n\n\ndef test_init_arpack_v0():\n    v0s = []\n    for i in range(100):\n        v0s.append(_init_arpack_v0(1000, i))\n        if i > 0:\n            assert not any(np.equal(v0s[i], v0s[i-1]))\n\n    v0 = np.concatenate(v0s)\n    assert np.allclose(np.mean(v0), 0, atol=1e-2)\n    assert np.allclose(np.std(v0), 1/np.sqrt(3), atol=1e-3)\n"
    }
  ]
}