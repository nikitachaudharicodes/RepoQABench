{
  "repo_name": "scikit-learn_scikit-learn",
  "issue_id": "10734",
  "issue_description": "# return_X_y should be available on more dataset loaders/fetchers\n\nVersion 0.18 added a `return_X_y` option to `load_iris` et al., but not to, for example, `fetch_kddcup99`.\r\n\r\nAll dataset loaders that currently return Bunches should also be able to return (X, y).",
  "issue_comments": [
    {
      "id": 369448829,
      "user": "ccatalfo",
      "body": "Looks like a doable first issue - may I take it on?"
    },
    {
      "id": 369458904,
      "user": "jnothman",
      "body": "Sure.\n\nOn 1 March 2018 at 12:59, Chris Catalfo <notifications@github.com> wrote:\n\n> Looks like a doable first issue - may I take it on?\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/10734#issuecomment-369448829>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz6wB-I558FOQMikXvOGJLH12xAcD7ks5tZ1XygaJpZM4SXlSa>\n> .\n>\n"
    },
    {
      "id": 369458950,
      "user": "jnothman",
      "body": "Please refer to the implementation and testing of load_iris's similar\nfeature.\n"
    },
    {
      "id": 369460843,
      "user": "ccatalfo",
      "body": "Thanks - will do."
    },
    {
      "id": 371085775,
      "user": "satishjasthi",
      "body": "@jmontoyam @ccatalfo \r\nI wanna take up this commit, but before that, I wanna know whether the issue is solved. It would be great if anyone of you can inform me the status of this issue.\r\nThank you"
    },
    {
      "id": 371104098,
      "user": "jnothman",
      "body": "It looks like @ccatalfo is working on this but has not yet opened a PR"
    },
    {
      "id": 371118815,
      "user": "satishjasthi",
      "body": "@jnothman Thank you"
    },
    {
      "id": 371118935,
      "user": "satishjasthi",
      "body": "@ccatalfo can request a PR for your changes so that this issue can be closed\r\n"
    },
    {
      "id": 371128897,
      "user": "ccatalfo",
      "body": "@satishjasthi @jnothman Thanks yes I will get my changes together and submit PR later."
    },
    {
      "id": 371361025,
      "user": "ccatalfo",
      "body": "I made a PR for this: https://github.com/scikit-learn/scikit-learn/pull/10774 which adds return_X_y to datasets/rcv1, datasets/twenty_newgroups and datasets/kddcup99.\r\n\r\nI was a little unsure about whether to add return_X_y to a couple of the datasets (e.g. mldata).  I can go back and do so if that's desired."
    }
  ],
  "text_context": "# return_X_y should be available on more dataset loaders/fetchers\n\nVersion 0.18 added a `return_X_y` option to `load_iris` et al., but not to, for example, `fetch_kddcup99`.\r\n\r\nAll dataset loaders that currently return Bunches should also be able to return (X, y).\n\nLooks like a doable first issue - may I take it on?\n\nSure.\n\nOn 1 March 2018 at 12:59, Chris Catalfo <notifications@github.com> wrote:\n\n> Looks like a doable first issue - may I take it on?\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/10734#issuecomment-369448829>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz6wB-I558FOQMikXvOGJLH12xAcD7ks5tZ1XygaJpZM4SXlSa>\n> .\n>\n\n\nPlease refer to the implementation and testing of load_iris's similar\nfeature.\n\n\nThanks - will do.\n\n@jmontoyam @ccatalfo \r\nI wanna take up this commit, but before that, I wanna know whether the issue is solved. It would be great if anyone of you can inform me the status of this issue.\r\nThank you\n\nIt looks like @ccatalfo is working on this but has not yet opened a PR\n\n@jnothman Thank you\n\n@ccatalfo can request a PR for your changes so that this issue can be closed\r\n\n\n@satishjasthi @jnothman Thanks yes I will get my changes together and submit PR later.\n\nI made a PR for this: https://github.com/scikit-learn/scikit-learn/pull/10774 which adds return_X_y to datasets/rcv1, datasets/twenty_newgroups and datasets/kddcup99.\r\n\r\nI was a little unsure about whether to add return_X_y to a couple of the datasets (e.g. mldata).  I can go back and do so if that's desired.",
  "pr_link": "https://github.com/scikit-learn/scikit-learn/pull/10774",
  "code_context": [
    {
      "filename": "sklearn/datasets/california_housing.py",
      "content": "\"\"\"California housing dataset.\n\nThe original database is available from StatLib\n\n    http://lib.stat.cmu.edu/datasets/\n\nThe data contains 20,640 observations on 9 variables.\n\nThis dataset contains the average house value as target variable\nand the following input variables (features): average income,\nhousing average age, average rooms, average bedrooms, population,\naverage occupation, latitude, and longitude in that order.\n\nReferences\n----------\n\nPace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\nStatistics and Probability Letters, 33 (1997) 291-297.\n\n\"\"\"\n# Authors: Peter Prettenhofer\n# License: BSD 3 clause\n\nfrom os.path import exists\nfrom os import makedirs, remove\nimport tarfile\n\nimport numpy as np\nimport logging\n\nfrom .base import get_data_home\nfrom .base import _fetch_remote\nfrom .base import _pkl_filepath\nfrom .base import RemoteFileMetadata\nfrom ..utils import Bunch\nfrom ..externals import joblib\n\n# The original data can be found at:\n# http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz\nARCHIVE = RemoteFileMetadata(\n    filename='cal_housing.tgz',\n    url='https://ndownloader.figshare.com/files/5976036',\n    checksum=('aaa5c9a6afe2225cc2aed2723682ae40'\n              '3280c4a3695a2ddda4ffb5d8215ea681'))\n\n# Grab the module-level docstring to use as a description of the\n# dataset\nMODULE_DOCS = __doc__\n\nlogger = logging.getLogger(__name__)\n\n\ndef fetch_california_housing(data_home=None, download_if_missing=True,\n                             return_X_y=False):\n    \"\"\"Loader for the California housing dataset from StatLib.\n\n    Read more in the :ref:`User Guide <datasets>`.\n\n    Parameters\n    ----------\n    data_home : optional, default: None\n        Specify another download and cache folder for the datasets. By default\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    download_if_missing : optional, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n\n    return_X_y : boolean, default=False. If True, returns ``(data.data,\n    data.target)`` instead of a Bunch object.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    dataset : dict-like object with the following attributes:\n\n    dataset.data : ndarray, shape [20640, 8]\n        Each row corresponding to the 8 feature values in order.\n\n    dataset.target : numpy array of shape (20640,)\n        Each value corresponds to the average house value in units of 100,000.\n\n    dataset.feature_names : array of length 8\n        Array of ordered feature names used in the dataset.\n\n    dataset.DESCR : string\n        Description of the California housing dataset.\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.20\n\n    Notes\n    ------\n\n    This dataset consists of 20,640 samples and 9 features.\n    \"\"\"\n    data_home = get_data_home(data_home=data_home)\n    if not exists(data_home):\n        makedirs(data_home)\n\n    filepath = _pkl_filepath(data_home, 'cal_housing.pkz')\n    if not exists(filepath):\n        if not download_if_missing:\n            raise IOError(\"Data not found and `download_if_missing` is False\")\n\n        logger.info('Downloading Cal. housing from {} to {}'.format(\n            ARCHIVE.url, data_home))\n\n        archive_path = _fetch_remote(ARCHIVE, dirname=data_home)\n\n        with tarfile.open(mode=\"r:gz\", name=archive_path) as f:\n            cal_housing = np.loadtxt(\n                f.extractfile('CaliforniaHousing/cal_housing.data'),\n                delimiter=',')\n            # Columns are not in the same order compared to the previous\n            # URL resource on lib.stat.cmu.edu\n            columns_index = [8, 7, 2, 3, 4, 5, 6, 1, 0]\n            cal_housing = cal_housing[:, columns_index]\n\n            joblib.dump(cal_housing, filepath, compress=6)\n        remove(archive_path)\n\n    else:\n        cal_housing = joblib.load(filepath)\n\n    feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                     \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n\n    target, data = cal_housing[:, 0], cal_housing[:, 1:]\n\n    # avg rooms = total rooms / households\n    data[:, 2] /= data[:, 5]\n\n    # avg bed rooms = total bed rooms / households\n    data[:, 3] /= data[:, 5]\n\n    # avg occupancy = population / households\n    data[:, 5] = data[:, 4] / data[:, 5]\n\n    # target in units of 100,000\n    target = target / 100000.0\n\n    if return_X_y:\n        return data, target\n\n    return Bunch(data=data,\n                 target=target,\n                 feature_names=feature_names,\n                 DESCR=MODULE_DOCS)\n"
    },
    {
      "filename": "sklearn/datasets/covtype.py",
      "content": "\"\"\"Forest covertype dataset.\n\nA classic dataset for classification benchmarks, featuring categorical and\nreal-valued features.\n\nThe dataset page is available from UCI Machine Learning Repository\n\n    http://archive.ics.uci.edu/ml/datasets/Covertype\n\nCourtesy of Jock A. Blackard and Colorado State University.\n\"\"\"\n\n# Author: Lars Buitinck\n#         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n# License: BSD 3 clause\n\nfrom gzip import GzipFile\nimport logging\nfrom os.path import exists, join\nfrom os import remove\n\nimport numpy as np\n\nfrom .base import get_data_home\nfrom .base import _fetch_remote\nfrom .base import RemoteFileMetadata\nfrom ..utils import Bunch\nfrom .base import _pkl_filepath\nfrom ..utils.fixes import makedirs\nfrom ..externals import joblib\nfrom ..utils import check_random_state\n\n# The original data can be found in:\n# http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\nARCHIVE = RemoteFileMetadata(\n    filename='covtype.data.gz',\n    url='https://ndownloader.figshare.com/files/5976039',\n    checksum=('614360d0257557dd1792834a85a1cdeb'\n              'fadc3c4f30b011d56afee7ffb5b15771'))\n\nlogger = logging.getLogger(__name__)\n\n\ndef fetch_covtype(data_home=None, download_if_missing=True,\n                  random_state=None, shuffle=False, return_X_y=False):\n    \"\"\"Load the covertype dataset, downloading it if necessary.\n\n    Read more in the :ref:`User Guide <datasets>`.\n\n    Parameters\n    ----------\n    data_home : string, optional\n        Specify another download and cache folder for the datasets. By default\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    download_if_missing : boolean, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Random state for shuffling the dataset.\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    shuffle : bool, default=False\n        Whether to shuffle dataset.\n\n    return_X_y : boolean, default=False. If True, returns ``(data.data,\n    data.target)`` instead of a Bunch object.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    dataset : dict-like object with the following attributes:\n\n    dataset.data : numpy array of shape (581012, 54)\n        Each row corresponds to the 54 features in the dataset.\n\n    dataset.target : numpy array of shape (581012,)\n        Each value corresponds to one of the 7 forest covertypes with values\n        ranging between 1 to 7.\n\n    dataset.DESCR : string\n        Description of the forest covertype dataset.\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.20\n    \"\"\"\n\n    data_home = get_data_home(data_home=data_home)\n    covtype_dir = join(data_home, \"covertype\")\n    samples_path = _pkl_filepath(covtype_dir, \"samples\")\n    targets_path = _pkl_filepath(covtype_dir, \"targets\")\n    available = exists(samples_path)\n\n    if download_if_missing and not available:\n        if not exists(covtype_dir):\n            makedirs(covtype_dir)\n        logger.info(\"Downloading %s\" % ARCHIVE.url)\n\n        archive_path = _fetch_remote(ARCHIVE, dirname=covtype_dir)\n        Xy = np.genfromtxt(GzipFile(filename=archive_path), delimiter=',')\n        # delete archive\n        remove(archive_path)\n\n        X = Xy[:, :-1]\n        y = Xy[:, -1].astype(np.int32)\n\n        joblib.dump(X, samples_path, compress=9)\n        joblib.dump(y, targets_path, compress=9)\n\n    elif not available and not download_if_missing:\n        raise IOError(\"Data not found and `download_if_missing` is False\")\n    try:\n        X, y\n    except NameError:\n        X = joblib.load(samples_path)\n        y = joblib.load(targets_path)\n\n    if shuffle:\n        ind = np.arange(X.shape[0])\n        rng = check_random_state(random_state)\n        rng.shuffle(ind)\n        X = X[ind]\n        y = y[ind]\n\n    if return_X_y:\n        return X, y\n\n    return Bunch(data=X, target=y, DESCR=__doc__)\n"
    },
    {
      "filename": "sklearn/datasets/kddcup99.py",
      "content": "\"\"\"KDDCUP 99 dataset.\n\nA classic dataset for anomaly detection.\n\nThe dataset page is available from UCI Machine Learning Repository\n\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/kddcup.data.gz\n\n\"\"\"\n\nimport sys\nimport errno\nfrom gzip import GzipFile\nimport logging\nimport os\nfrom os.path import exists, join\n\nimport numpy as np\n\n\nfrom .base import _fetch_remote\nfrom .base import get_data_home\nfrom .base import RemoteFileMetadata\nfrom ..utils import Bunch\nfrom ..externals import joblib, six\nfrom ..utils import check_random_state\nfrom ..utils import shuffle as shuffle_method\n\n# The original data can be found at:\n# http://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/kddcup.data.gz\nARCHIVE = RemoteFileMetadata(\n    filename='kddcup99_data',\n    url='https://ndownloader.figshare.com/files/5976045',\n    checksum=('3b6c942aa0356c0ca35b7b595a26c89d'\n              '343652c9db428893e7494f837b274292'))\n\n# The original data can be found at:\n# http://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/kddcup.data_10_percent.gz\nARCHIVE_10_PERCENT = RemoteFileMetadata(\n    filename='kddcup99_10_data',\n    url='https://ndownloader.figshare.com/files/5976042',\n    checksum=('8045aca0d84e70e622d1148d7df78249'\n              '6f6333bf6eb979a1b0837c42a9fd9561'))\n\nlogger = logging.getLogger(__name__)\n\n\ndef fetch_kddcup99(subset=None, data_home=None, shuffle=False,\n                   random_state=None,\n                   percent10=True, download_if_missing=True, return_X_y=False):\n    \"\"\"Load and return the kddcup 99 dataset (classification).\n\n    The KDD Cup '99 dataset was created by processing the tcpdump portions\n    of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,\n    created by MIT Lincoln Lab [1]. The artificial data was generated using\n    a closed network and hand-injected attacks to produce a large number of\n    different types of attack with normal activity in the background.\n    As the initial goal was to produce a large training set for supervised\n    learning algorithms, there is a large proportion (80.1%) of abnormal\n    data which is unrealistic in real world, and inappropriate for unsupervised\n    anomaly detection which aims at detecting 'abnormal' data, ie\n\n    1) qualitatively different from normal data.\n\n    2) in large minority among the observations.\n\n    We thus transform the KDD Data set into two different data sets: SA and SF.\n\n    - SA is obtained by simply selecting all the normal data, and a small\n      proportion of abnormal data to gives an anomaly proportion of 1%.\n\n    - SF is obtained as in [2]\n      by simply picking up the data whose attribute logged_in is positive, thus\n      focusing on the intrusion attack, which gives a proportion of 0.3% of\n      attack.\n\n    - http and smtp are two subsets of SF corresponding with third feature\n      equal to 'http' (resp. to 'smtp')\n\n\n    General KDD structure :\n\n    ================      ==========================================\n    Samples total         4898431\n    Dimensionality        41\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    SA structure :\n\n    ================      ==========================================\n    Samples total         976158\n    Dimensionality        41\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    SF structure :\n\n    ================      ==========================================\n    Samples total         699691\n    Dimensionality        4\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    http structure :\n\n    ================      ==========================================\n    Samples total         619052\n    Dimensionality        3\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    smtp structure :\n\n    ================      ==========================================\n    Samples total         95373\n    Dimensionality        3\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    subset : None, 'SA', 'SF', 'http', 'smtp'\n        To return the corresponding classical subsets of kddcup 99.\n        If None, return the entire kddcup 99 dataset.\n\n    data_home : string, optional\n        Specify another download and cache folder for the datasets. By default\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n        .. versionadded:: 0.19\n\n    shuffle : bool, default=False\n        Whether to shuffle dataset.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Random state for shuffling the dataset. If subset='SA', this random\n        state is also used to randomly select the small proportion of abnormal\n        samples.\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    percent10 : bool, default=True\n        Whether to load only 10 percent of the data.\n\n    download_if_missing : bool, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    return_X_y : boolean, default=False.\n        If True, returns ``(data, target)`` instead of a Bunch object. See\n        below for more information about the `data` and `target` object.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    data : Bunch\n        Dictionary-like object, the interesting attributes are:\n        'data', the data to learn and 'target', the regression target for each\n        sample.\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.20\n\n    References\n    ----------\n    .. [1] Analysis and Results of the 1999 DARPA Off-Line Intrusion\n           Detection Evaluation Richard Lippmann, Joshua W. Haines,\n           David J. Fried, Jonathan Korba, Kumar Das\n\n    .. [2] K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online\n           unsupervised outlier detection using finite mixtures with\n           discounting learning algorithms. In Proceedings of the sixth\n           ACM SIGKDD international conference on Knowledge discovery\n           and data mining, pages 320-324. ACM Press, 2000.\n\n    \"\"\"\n    data_home = get_data_home(data_home=data_home)\n    kddcup99 = _fetch_brute_kddcup99(data_home=data_home,\n                                     percent10=percent10,\n                                     download_if_missing=download_if_missing)\n\n    data = kddcup99.data\n    target = kddcup99.target\n\n    if subset == 'SA':\n        s = target == b'normal.'\n        t = np.logical_not(s)\n        normal_samples = data[s, :]\n        normal_targets = target[s]\n        abnormal_samples = data[t, :]\n        abnormal_targets = target[t]\n\n        n_samples_abnormal = abnormal_samples.shape[0]\n        # selected abnormal samples:\n        random_state = check_random_state(random_state)\n        r = random_state.randint(0, n_samples_abnormal, 3377)\n        abnormal_samples = abnormal_samples[r]\n        abnormal_targets = abnormal_targets[r]\n\n        data = np.r_[normal_samples, abnormal_samples]\n        target = np.r_[normal_targets, abnormal_targets]\n\n    if subset == 'SF' or subset == 'http' or subset == 'smtp':\n        # select all samples with positive logged_in attribute:\n        s = data[:, 11] == 1\n        data = np.c_[data[s, :11], data[s, 12:]]\n        target = target[s]\n\n        data[:, 0] = np.log((data[:, 0] + 0.1).astype(float))\n        data[:, 4] = np.log((data[:, 4] + 0.1).astype(float))\n        data[:, 5] = np.log((data[:, 5] + 0.1).astype(float))\n\n        if subset == 'http':\n            s = data[:, 2] == b'http'\n            data = data[s]\n            target = target[s]\n            data = np.c_[data[:, 0], data[:, 4], data[:, 5]]\n\n        if subset == 'smtp':\n            s = data[:, 2] == b'smtp'\n            data = data[s]\n            target = target[s]\n            data = np.c_[data[:, 0], data[:, 4], data[:, 5]]\n\n        if subset == 'SF':\n            data = np.c_[data[:, 0], data[:, 2], data[:, 4], data[:, 5]]\n\n    if shuffle:\n        data, target = shuffle_method(data, target, random_state=random_state)\n\n    if return_X_y:\n        return data, target\n\n    return Bunch(data=data, target=target)\n\n\ndef _fetch_brute_kddcup99(data_home=None,\n                          download_if_missing=True, percent10=True):\n\n    \"\"\"Load the kddcup99 dataset, downloading it if necessary.\n\n    Parameters\n    ----------\n    data_home : string, optional\n        Specify another download and cache folder for the datasets. By default\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    download_if_missing : boolean, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    percent10 : bool, default=True\n        Whether to load only 10 percent of the data.\n\n    Returns\n    -------\n    dataset : dict-like object with the following attributes:\n        dataset.data : numpy array of shape (494021, 41)\n            Each row corresponds to the 41 features in the dataset.\n        dataset.target : numpy array of shape (494021,)\n            Each value corresponds to one of the 21 attack types or to the\n            label 'normal.'.\n        dataset.DESCR : string\n            Description of the kddcup99 dataset.\n\n    \"\"\"\n\n    data_home = get_data_home(data_home=data_home)\n    if sys.version_info[0] == 3:\n        # The zlib compression format use by joblib is not compatible when\n        # switching from Python 2 to Python 3, let us use a separate folder\n        # under Python 3:\n        dir_suffix = \"-py3\"\n    else:\n        # Backward compat for Python 2 users\n        dir_suffix = \"\"\n\n    if percent10:\n        kddcup_dir = join(data_home, \"kddcup99_10\" + dir_suffix)\n        archive = ARCHIVE_10_PERCENT\n    else:\n        kddcup_dir = join(data_home, \"kddcup99\" + dir_suffix)\n        archive = ARCHIVE\n\n    samples_path = join(kddcup_dir, \"samples\")\n    targets_path = join(kddcup_dir, \"targets\")\n    available = exists(samples_path)\n\n    if download_if_missing and not available:\n        _mkdirp(kddcup_dir)\n        logger.info(\"Downloading %s\" % archive.url)\n        _fetch_remote(archive, dirname=kddcup_dir)\n        dt = [('duration', int),\n              ('protocol_type', 'S4'),\n              ('service', 'S11'),\n              ('flag', 'S6'),\n              ('src_bytes', int),\n              ('dst_bytes', int),\n              ('land', int),\n              ('wrong_fragment', int),\n              ('urgent', int),\n              ('hot', int),\n              ('num_failed_logins', int),\n              ('logged_in', int),\n              ('num_compromised', int),\n              ('root_shell', int),\n              ('su_attempted', int),\n              ('num_root', int),\n              ('num_file_creations', int),\n              ('num_shells', int),\n              ('num_access_files', int),\n              ('num_outbound_cmds', int),\n              ('is_host_login', int),\n              ('is_guest_login', int),\n              ('count', int),\n              ('srv_count', int),\n              ('serror_rate', float),\n              ('srv_serror_rate', float),\n              ('rerror_rate', float),\n              ('srv_rerror_rate', float),\n              ('same_srv_rate', float),\n              ('diff_srv_rate', float),\n              ('srv_diff_host_rate', float),\n              ('dst_host_count', int),\n              ('dst_host_srv_count', int),\n              ('dst_host_same_srv_rate', float),\n              ('dst_host_diff_srv_rate', float),\n              ('dst_host_same_src_port_rate', float),\n              ('dst_host_srv_diff_host_rate', float),\n              ('dst_host_serror_rate', float),\n              ('dst_host_srv_serror_rate', float),\n              ('dst_host_rerror_rate', float),\n              ('dst_host_srv_rerror_rate', float),\n              ('labels', 'S16')]\n        DT = np.dtype(dt)\n        logger.debug(\"extracting archive\")\n        archive_path = join(kddcup_dir, archive.filename)\n        file_ = GzipFile(filename=archive_path, mode='r')\n        Xy = []\n        for line in file_.readlines():\n            if six.PY3:\n                line = line.decode()\n            Xy.append(line.replace('\\n', '').split(','))\n        file_.close()\n        logger.debug('extraction done')\n        os.remove(archive_path)\n\n        Xy = np.asarray(Xy, dtype=object)\n        for j in range(42):\n            Xy[:, j] = Xy[:, j].astype(DT[j])\n\n        X = Xy[:, :-1]\n        y = Xy[:, -1]\n        # XXX bug when compress!=0:\n        # (error: 'Incorrect data length while decompressing[...] the file\n        #  could be corrupted.')\n\n        joblib.dump(X, samples_path, compress=0)\n        joblib.dump(y, targets_path, compress=0)\n    elif not available:\n        if not download_if_missing:\n            raise IOError(\"Data not found and `download_if_missing` is False\")\n\n    try:\n        X, y\n    except NameError:\n        X = joblib.load(samples_path)\n        y = joblib.load(targets_path)\n\n    return Bunch(data=X, target=y, DESCR=__doc__)\n\n\ndef _mkdirp(d):\n    \"\"\"Ensure directory d exists (like mkdir -p on Unix)\n    No guarantee that the directory is writable.\n    \"\"\"\n    try:\n        os.makedirs(d)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n"
    },
    {
      "filename": "sklearn/datasets/lfw.py",
      "content": "\"\"\"Loader for the Labeled Faces in the Wild (LFW) dataset\n\nThis dataset is a collection of JPEG pictures of famous people collected\nover the internet, all details are available on the official website:\n\n    http://vis-www.cs.umass.edu/lfw/\n\nEach picture is centered on a single face. The typical task is called\nFace Verification: given a pair of two pictures, a binary classifier\nmust predict whether the two images are from the same person.\n\nAn alternative task, Face Recognition or Face Identification is:\ngiven the picture of the face of an unknown person, identify the name\nof the person by referring to a gallery of previously seen pictures of\nidentified persons.\n\nBoth Face Verification and Face Recognition are tasks that are typically\nperformed on the output of a model trained to perform Face Detection. The\nmost popular model for Face Detection is called Viola-Johns and is\nimplemented in the OpenCV library. The LFW faces were extracted by this face\ndetector from various online websites.\n\"\"\"\n# Copyright (c) 2011 Olivier Grisel <olivier.grisel@ensta.org>\n# License: BSD 3 clause\n\nfrom os import listdir, makedirs, remove\nfrom os.path import join, exists, isdir\n\nimport logging\nimport numpy as np\n\nfrom .base import get_data_home, _fetch_remote, RemoteFileMetadata\nfrom ..utils import Bunch\nfrom ..externals.joblib import Memory\nfrom ..externals.six import b\n\nlogger = logging.getLogger(__name__)\n\n# The original data can be found in:\n# http://vis-www.cs.umass.edu/lfw/lfw.tgz\nARCHIVE = RemoteFileMetadata(\n    filename='lfw.tgz',\n    url='https://ndownloader.figshare.com/files/5976018',\n    checksum=('055f7d9c632d7370e6fb4afc7468d40f'\n              '970c34a80d4c6f50ffec63f5a8d536c0'))\n\n# The original funneled data can be found in:\n# http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz\nFUNNELED_ARCHIVE = RemoteFileMetadata(\n    filename='lfw-funneled.tgz',\n    url='https://ndownloader.figshare.com/files/5976015',\n    checksum=('b47c8422c8cded889dc5a13418c4bc2a'\n              'bbda121092b3533a83306f90d900100a'))\n\n# The original target data can be found in:\n# http://vis-www.cs.umass.edu/lfw/pairsDevTrain.txt',\n# http://vis-www.cs.umass.edu/lfw/pairsDevTest.txt',\n# http://vis-www.cs.umass.edu/lfw/pairs.txt',\nTARGETS = (\n    RemoteFileMetadata(\n        filename='pairsDevTrain.txt',\n        url='https://ndownloader.figshare.com/files/5976012',\n        checksum=('1d454dada7dfeca0e7eab6f65dc4e97a'\n                  '6312d44cf142207be28d688be92aabfa')),\n\n    RemoteFileMetadata(\n        filename='pairsDevTest.txt',\n        url='https://ndownloader.figshare.com/files/5976009',\n        checksum=('7cb06600ea8b2814ac26e946201cdb30'\n                  '4296262aad67d046a16a7ec85d0ff87c')),\n\n    RemoteFileMetadata(\n        filename='pairs.txt',\n        url='https://ndownloader.figshare.com/files/5976006',\n        checksum=('ea42330c62c92989f9d7c03237ed5d59'\n                  '1365e89b3e649747777b70e692dc1592')),\n)\n\n\ndef scale_face(face):\n    \"\"\"Scale back to 0-1 range in case of normalization for plotting\"\"\"\n    scaled = face - face.min()\n    scaled /= scaled.max()\n    return scaled\n\n\n#\n# Common private utilities for data fetching from the original LFW website\n# local disk caching, and image decoding.\n#\n\n\ndef check_fetch_lfw(data_home=None, funneled=True, download_if_missing=True):\n    \"\"\"Helper function to download any missing LFW data\"\"\"\n\n    data_home = get_data_home(data_home=data_home)\n    lfw_home = join(data_home, \"lfw_home\")\n\n    if not exists(lfw_home):\n        makedirs(lfw_home)\n\n    for target in TARGETS:\n        target_filepath = join(lfw_home, target.filename)\n        if not exists(target_filepath):\n            if download_if_missing:\n                logger.info(\"Downloading LFW metadata: %s\", target.url)\n                _fetch_remote(target, dirname=lfw_home)\n            else:\n                raise IOError(\"%s is missing\" % target_filepath)\n\n    if funneled:\n        data_folder_path = join(lfw_home, \"lfw_funneled\")\n        archive = FUNNELED_ARCHIVE\n    else:\n        data_folder_path = join(lfw_home, \"lfw\")\n        archive = ARCHIVE\n\n    if not exists(data_folder_path):\n        archive_path = join(lfw_home, archive.filename)\n        if not exists(archive_path):\n            if download_if_missing:\n                logger.info(\"Downloading LFW data (~200MB): %s\",\n                            archive.url)\n                _fetch_remote(archive, dirname=lfw_home)\n            else:\n                raise IOError(\"%s is missing\" % archive_path)\n\n        import tarfile\n        logger.debug(\"Decompressing the data archive to %s\", data_folder_path)\n        tarfile.open(archive_path, \"r:gz\").extractall(path=lfw_home)\n        remove(archive_path)\n\n    return lfw_home, data_folder_path\n\n\ndef _load_imgs(file_paths, slice_, color, resize):\n    \"\"\"Internally used to load images\"\"\"\n    # import PIL only when needed\n    from ..externals._pilutil import imread, imresize\n\n    # compute the portion of the images to load to respect the slice_ parameter\n    # given by the caller\n    default_slice = (slice(0, 250), slice(0, 250))\n    if slice_ is None:\n        slice_ = default_slice\n    else:\n        slice_ = tuple(s or ds for s, ds in zip(slice_, default_slice))\n\n    h_slice, w_slice = slice_\n    h = (h_slice.stop - h_slice.start) // (h_slice.step or 1)\n    w = (w_slice.stop - w_slice.start) // (w_slice.step or 1)\n\n    if resize is not None:\n        resize = float(resize)\n        h = int(resize * h)\n        w = int(resize * w)\n\n    # allocate some contiguous memory to host the decoded image slices\n    n_faces = len(file_paths)\n    if not color:\n        faces = np.zeros((n_faces, h, w), dtype=np.float32)\n    else:\n        faces = np.zeros((n_faces, h, w, 3), dtype=np.float32)\n\n    # iterate over the collected file path to load the jpeg files as numpy\n    # arrays\n    for i, file_path in enumerate(file_paths):\n        if i % 1000 == 0:\n            logger.debug(\"Loading face #%05d / %05d\", i + 1, n_faces)\n\n        # Checks if jpeg reading worked. Refer to issue #3594 for more\n        # details.\n        img = imread(file_path)\n        if img.ndim is 0:\n            raise RuntimeError(\"Failed to read the image file %s, \"\n                               \"Please make sure that libjpeg is installed\"\n                               % file_path)\n\n        face = np.asarray(img[slice_], dtype=np.float32)\n        face /= 255.0  # scale uint8 coded colors to the [0.0, 1.0] floats\n        if resize is not None:\n            face = imresize(face, resize)\n        if not color:\n            # average the color channels to compute a gray levels\n            # representation\n            face = face.mean(axis=2)\n\n        faces[i, ...] = face\n\n    return faces\n\n\n#\n# Task #1:  Face Identification on picture with names\n#\n\ndef _fetch_lfw_people(data_folder_path, slice_=None, color=False, resize=None,\n                      min_faces_per_person=0):\n    \"\"\"Perform the actual data loading for the lfw people dataset\n\n    This operation is meant to be cached by a joblib wrapper.\n    \"\"\"\n    # scan the data folder content to retain people with more that\n    # `min_faces_per_person` face pictures\n    person_names, file_paths = [], []\n    for person_name in sorted(listdir(data_folder_path)):\n        folder_path = join(data_folder_path, person_name)\n        if not isdir(folder_path):\n            continue\n        paths = [join(folder_path, f) for f in sorted(listdir(folder_path))]\n        n_pictures = len(paths)\n        if n_pictures >= min_faces_per_person:\n            person_name = person_name.replace('_', ' ')\n            person_names.extend([person_name] * n_pictures)\n            file_paths.extend(paths)\n\n    n_faces = len(file_paths)\n    if n_faces == 0:\n        raise ValueError(\"min_faces_per_person=%d is too restrictive\" %\n                         min_faces_per_person)\n\n    target_names = np.unique(person_names)\n    target = np.searchsorted(target_names, person_names)\n\n    faces = _load_imgs(file_paths, slice_, color, resize)\n\n    # shuffle the faces with a deterministic RNG scheme to avoid having\n    # all faces of the same person in a row, as it would break some\n    # cross validation and learning algorithms such as SGD and online\n    # k-means that make an IID assumption\n\n    indices = np.arange(n_faces)\n    np.random.RandomState(42).shuffle(indices)\n    faces, target = faces[indices], target[indices]\n    return faces, target, target_names\n\n\ndef fetch_lfw_people(data_home=None, funneled=True, resize=0.5,\n                     min_faces_per_person=0, color=False,\n                     slice_=(slice(70, 195), slice(78, 172)),\n                     download_if_missing=True, return_X_y=False):\n    \"\"\"Loader for the Labeled Faces in the Wild (LFW) people dataset\n\n    This dataset is a collection of JPEG pictures of famous people\n    collected on the internet, all details are available on the\n    official website:\n\n        http://vis-www.cs.umass.edu/lfw/\n\n    Each picture is centered on a single face. Each pixel of each channel\n    (color in RGB) is encoded by a float in range 0.0 - 1.0.\n\n    The task is called Face Recognition (or Identification): given the\n    picture of a face, find the name of the person given a training set\n    (gallery).\n\n    The original images are 250 x 250 pixels, but the default slice and resize\n    arguments reduce them to 62 x 47.\n\n    Parameters\n    ----------\n    data_home : optional, default: None\n        Specify another download and cache folder for the datasets. By default\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    funneled : boolean, optional, default: True\n        Download and use the funneled variant of the dataset.\n\n    resize : float, optional, default 0.5\n        Ratio used to resize the each face picture.\n\n    min_faces_per_person : int, optional, default None\n        The extracted dataset will only retain pictures of people that have at\n        least `min_faces_per_person` different pictures.\n\n    color : boolean, optional, default False\n        Keep the 3 RGB channels instead of averaging them to a single\n        gray level channel. If color is True the shape of the data has\n        one more dimension than the shape with color = False.\n\n    slice_ : optional\n        Provide a custom 2D slice (height, width) to extract the\n        'interesting' part of the jpeg files and avoid use statistical\n        correlation from the background\n\n    download_if_missing : optional, True by default\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    return_X_y : boolean, default=False. If True, returns ``(dataset.data,\n    dataset.target)`` instead of a Bunch object. See below for more\n    information about the `dataset.data` and `dataset.target` object.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    dataset : dict-like object with the following attributes:\n\n    dataset.data : numpy array of shape (13233, 2914)\n        Each row corresponds to a ravelled face image of original size 62 x 47\n        pixels. Changing the ``slice_`` or resize parameters will change the\n        shape of the output.\n\n    dataset.images : numpy array of shape (13233, 62, 47)\n        Each row is a face image corresponding to one of the 5749 people in\n        the dataset. Changing the ``slice_`` or resize parameters will change\n        the shape of the output.\n\n    dataset.target : numpy array of shape (13233,)\n        Labels associated to each face image. Those labels range from 0-5748\n        and correspond to the person IDs.\n\n    dataset.DESCR : string\n        Description of the Labeled Faces in the Wild (LFW) dataset.\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.20\n\n    \"\"\"\n    lfw_home, data_folder_path = check_fetch_lfw(\n        data_home=data_home, funneled=funneled,\n        download_if_missing=download_if_missing)\n    logger.debug('Loading LFW people faces from %s', lfw_home)\n\n    # wrap the loader in a memoizing function that will return memmaped data\n    # arrays for optimal memory usage\n    m = Memory(cachedir=lfw_home, compress=6, verbose=0)\n    load_func = m.cache(_fetch_lfw_people)\n\n    # load and memoize the pairs as np arrays\n    faces, target, target_names = load_func(\n        data_folder_path, resize=resize,\n        min_faces_per_person=min_faces_per_person, color=color, slice_=slice_)\n\n    X = faces.reshape(len(faces), -1)\n\n    if return_X_y:\n        return X, target\n\n    # pack the results as a Bunch instance\n    return Bunch(data=X, images=faces,\n                 target=target, target_names=target_names,\n                 DESCR=\"LFW faces dataset\")\n\n\n#\n# Task #2:  Face Verification on pairs of face pictures\n#\n\n\ndef _fetch_lfw_pairs(index_file_path, data_folder_path, slice_=None,\n                     color=False, resize=None):\n    \"\"\"Perform the actual data loading for the LFW pairs dataset\n\n    This operation is meant to be cached by a joblib wrapper.\n    \"\"\"\n    # parse the index file to find the number of pairs to be able to allocate\n    # the right amount of memory before starting to decode the jpeg files\n    with open(index_file_path, 'rb') as index_file:\n        split_lines = [ln.strip().split(b('\\t')) for ln in index_file]\n    pair_specs = [sl for sl in split_lines if len(sl) > 2]\n    n_pairs = len(pair_specs)\n\n    # iterating over the metadata lines for each pair to find the filename to\n    # decode and load in memory\n    target = np.zeros(n_pairs, dtype=np.int)\n    file_paths = list()\n    for i, components in enumerate(pair_specs):\n        if len(components) == 3:\n            target[i] = 1\n            pair = (\n                (components[0], int(components[1]) - 1),\n                (components[0], int(components[2]) - 1),\n            )\n        elif len(components) == 4:\n            target[i] = 0\n            pair = (\n                (components[0], int(components[1]) - 1),\n                (components[2], int(components[3]) - 1),\n            )\n        else:\n            raise ValueError(\"invalid line %d: %r\" % (i + 1, components))\n        for j, (name, idx) in enumerate(pair):\n            try:\n                person_folder = join(data_folder_path, name)\n            except TypeError:\n                person_folder = join(data_folder_path, str(name, 'UTF-8'))\n            filenames = list(sorted(listdir(person_folder)))\n            file_path = join(person_folder, filenames[idx])\n            file_paths.append(file_path)\n\n    pairs = _load_imgs(file_paths, slice_, color, resize)\n    shape = list(pairs.shape)\n    n_faces = shape.pop(0)\n    shape.insert(0, 2)\n    shape.insert(0, n_faces // 2)\n    pairs.shape = shape\n\n    return pairs, target, np.array(['Different persons', 'Same person'])\n\n\ndef fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5,\n                    color=False, slice_=(slice(70, 195), slice(78, 172)),\n                    download_if_missing=True):\n    \"\"\"Loader for the Labeled Faces in the Wild (LFW) pairs dataset\n\n    This dataset is a collection of JPEG pictures of famous people\n    collected on the internet, all details are available on the\n    official website:\n\n        http://vis-www.cs.umass.edu/lfw/\n\n    Each picture is centered on a single face. Each pixel of each channel\n    (color in RGB) is encoded by a float in range 0.0 - 1.0.\n\n    The task is called Face Verification: given a pair of two pictures,\n    a binary classifier must predict whether the two images are from\n    the same person.\n\n    In the official `README.txt`_ this task is described as the\n    \"Restricted\" task.  As I am not sure as to implement the\n    \"Unrestricted\" variant correctly, I left it as unsupported for now.\n\n      .. _`README.txt`: http://vis-www.cs.umass.edu/lfw/README.txt\n\n    The original images are 250 x 250 pixels, but the default slice and resize\n    arguments reduce them to 62 x 47.\n\n    Read more in the :ref:`User Guide <labeled_faces_in_the_wild>`.\n\n    Parameters\n    ----------\n    subset : optional, default: 'train'\n        Select the dataset to load: 'train' for the development training\n        set, 'test' for the development test set, and '10_folds' for the\n        official evaluation set that is meant to be used with a 10-folds\n        cross validation.\n\n    data_home : optional, default: None\n        Specify another download and cache folder for the datasets. By\n        default all scikit-learn data is stored in '~/scikit_learn_data'\n        subfolders.\n\n    funneled : boolean, optional, default: True\n        Download and use the funneled variant of the dataset.\n\n    resize : float, optional, default 0.5\n        Ratio used to resize the each face picture.\n\n    color : boolean, optional, default False\n        Keep the 3 RGB channels instead of averaging them to a single\n        gray level channel. If color is True the shape of the data has\n        one more dimension than the shape with color = False.\n\n    slice_ : optional\n        Provide a custom 2D slice (height, width) to extract the\n        'interesting' part of the jpeg files and avoid use statistical\n        correlation from the background\n\n    download_if_missing : optional, True by default\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    Returns\n    -------\n    The data is returned as a Bunch object with the following attributes:\n\n    data : numpy array of shape (2200, 5828). Shape depends on ``subset``.\n        Each row corresponds to 2 ravel'd face images of original size 62 x 47\n        pixels. Changing the ``slice_``, ``resize`` or ``subset`` parameters\n        will change the shape of the output.\n\n    pairs : numpy array of shape (2200, 2, 62, 47). Shape depends on\n            ``subset``.\n        Each row has 2 face images corresponding to same or different person\n        from the dataset containing 5749 people. Changing the ``slice_``,\n        ``resize`` or ``subset`` parameters will change the shape of the\n        output.\n\n    target : numpy array of shape (2200,). Shape depends on ``subset``.\n        Labels associated to each pair of images. The two label values being\n        different persons or the same person.\n\n    DESCR : string\n        Description of the Labeled Faces in the Wild (LFW) dataset.\n\n    \"\"\"\n    lfw_home, data_folder_path = check_fetch_lfw(\n        data_home=data_home, funneled=funneled,\n        download_if_missing=download_if_missing)\n    logger.debug('Loading %s LFW pairs from %s', subset, lfw_home)\n\n    # wrap the loader in a memoizing function that will return memmaped data\n    # arrays for optimal memory usage\n    m = Memory(cachedir=lfw_home, compress=6, verbose=0)\n    load_func = m.cache(_fetch_lfw_pairs)\n\n    # select the right metadata file according to the requested subset\n    label_filenames = {\n        'train': 'pairsDevTrain.txt',\n        'test': 'pairsDevTest.txt',\n        '10_folds': 'pairs.txt',\n    }\n    if subset not in label_filenames:\n        raise ValueError(\"subset='%s' is invalid: should be one of %r\" % (\n            subset, list(sorted(label_filenames.keys()))))\n    index_file_path = join(lfw_home, label_filenames[subset])\n\n    # load and memoize the pairs as np arrays\n    pairs, target, target_names = load_func(\n        index_file_path, data_folder_path, resize=resize, color=color,\n        slice_=slice_)\n\n    # pack the results as a Bunch instance\n    return Bunch(data=pairs.reshape(len(pairs), -1), pairs=pairs,\n                 target=target, target_names=target_names,\n                 DESCR=\"'%s' segment of the LFW pairs dataset\" % subset)\n"
    },
    {
      "filename": "sklearn/datasets/rcv1.py",
      "content": "\"\"\"RCV1 dataset.\n\"\"\"\n\n# Author: Tom Dupre la Tour\n# License: BSD 3 clause\n\nimport logging\n\nfrom os import remove\nfrom os.path import exists, join\nfrom gzip import GzipFile\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom .base import get_data_home\nfrom .base import _pkl_filepath\nfrom .base import _fetch_remote\nfrom .base import RemoteFileMetadata\nfrom ..utils.fixes import makedirs\nfrom ..externals import joblib\nfrom .svmlight_format import load_svmlight_files\nfrom ..utils import shuffle as shuffle_\nfrom ..utils import Bunch\n\n\n# The original data can be found at:\n# http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt0.dat.gz\n# http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt1.dat.gz\n# http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt2.dat.gz\n# http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt3.dat.gz\n# http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_train.dat.gz\nXY_METADATA = (\n    RemoteFileMetadata(\n        url='https://ndownloader.figshare.com/files/5976069',\n        checksum=('ed40f7e418d10484091b059703eeb95a'\n                  'e3199fe042891dcec4be6696b9968374'),\n        filename='lyrl2004_vectors_test_pt0.dat.gz'),\n    RemoteFileMetadata(\n        url='https://ndownloader.figshare.com/files/5976066',\n        checksum=('87700668ae45d45d5ca1ef6ae9bd81ab'\n                  '0f5ec88cc95dcef9ae7838f727a13aa6'),\n        filename='lyrl2004_vectors_test_pt1.dat.gz'),\n    RemoteFileMetadata(\n        url='https://ndownloader.figshare.com/files/5976063',\n        checksum=('48143ac703cbe33299f7ae9f4995db4'\n                  '9a258690f60e5debbff8995c34841c7f5'),\n        filename='lyrl2004_vectors_test_pt2.dat.gz'),\n    RemoteFileMetadata(\n        url='https://ndownloader.figshare.com/files/5976060',\n        checksum=('dfcb0d658311481523c6e6ca0c3f5a3'\n                  'e1d3d12cde5d7a8ce629a9006ec7dbb39'),\n        filename='lyrl2004_vectors_test_pt3.dat.gz'),\n    RemoteFileMetadata(\n        url='https://ndownloader.figshare.com/files/5976057',\n        checksum=('5468f656d0ba7a83afc7ad44841cf9a5'\n                  '3048a5c083eedc005dcdb5cc768924ae'),\n        filename='lyrl2004_vectors_train.dat.gz')\n)\n\n# The original data can be found at:\n# http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a08-topic-qrels/rcv1-v2.topics.qrels.gz\nTOPICS_METADATA = RemoteFileMetadata(\n    url='https://ndownloader.figshare.com/files/5976048',\n    checksum=('2a98e5e5d8b770bded93afc8930d882'\n              '99474317fe14181aee1466cc754d0d1c1'),\n    filename='rcv1v2.topics.qrels.gz')\n\nlogger = logging.getLogger(__name__)\n\n\ndef fetch_rcv1(data_home=None, subset='all', download_if_missing=True,\n               random_state=None, shuffle=False, return_X_y=False):\n    \"\"\"Load the RCV1 multilabel dataset, downloading it if necessary.\n\n    Version: RCV1-v2, vectors, full sets, topics multilabels.\n\n    ==============     =====================\n    Classes                              103\n    Samples total                     804414\n    Dimensionality                     47236\n    Features           real, between 0 and 1\n    ==============     =====================\n\n    Read more in the :ref:`User Guide <datasets>`.\n\n    .. versionadded:: 0.17\n\n    Parameters\n    ----------\n    data_home : string, optional\n        Specify another download and cache folder for the datasets. By default\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    subset : string, 'train', 'test', or 'all', default='all'\n        Select the dataset to load: 'train' for the training set\n        (23149 samples), 'test' for the test set (781265 samples),\n        'all' for both, with the training samples first if shuffle is False.\n        This follows the official LYRL2004 chronological split.\n\n    download_if_missing : boolean, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Random state for shuffling the dataset.\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    shuffle : bool, default=False\n        Whether to shuffle dataset.\n\n    return_X_y : boolean, default=False. If True, returns ``(dataset.data,\n    dataset.target)`` instead of a Bunch object. See below for more\n    information about the `dataset.data` and `dataset.target` object.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    dataset : dict-like object with the following attributes:\n\n    dataset.data : scipy csr array, dtype np.float64, shape (804414, 47236)\n        The array has 0.16% of non zero values.\n\n    dataset.target : scipy csr array, dtype np.uint8, shape (804414, 103)\n        Each sample has a value of 1 in its categories, and 0 in others.\n        The array has 3.15% of non zero values.\n\n    dataset.sample_id : numpy array, dtype np.uint32, shape (804414,)\n        Identification number of each sample, as ordered in dataset.data.\n\n    dataset.target_names : numpy array, dtype object, length (103)\n        Names of each target (RCV1 topics), as ordered in dataset.target.\n\n    dataset.DESCR : string\n        Description of the RCV1 dataset.\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.20\n\n    References\n    ----------\n    Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new\n    benchmark collection for text categorization research. The Journal of\n    Machine Learning Research, 5, 361-397.\n\n    \"\"\"\n    N_SAMPLES = 804414\n    N_FEATURES = 47236\n    N_CATEGORIES = 103\n    N_TRAIN = 23149\n\n    data_home = get_data_home(data_home=data_home)\n    rcv1_dir = join(data_home, \"RCV1\")\n    if download_if_missing:\n        if not exists(rcv1_dir):\n            makedirs(rcv1_dir)\n\n    samples_path = _pkl_filepath(rcv1_dir, \"samples.pkl\")\n    sample_id_path = _pkl_filepath(rcv1_dir, \"sample_id.pkl\")\n    sample_topics_path = _pkl_filepath(rcv1_dir, \"sample_topics.pkl\")\n    topics_path = _pkl_filepath(rcv1_dir, \"topics_names.pkl\")\n\n    # load data (X) and sample_id\n    if download_if_missing and (not exists(samples_path) or\n                                not exists(sample_id_path)):\n        files = []\n        for each in XY_METADATA:\n            logger.info(\"Downloading %s\" % each.url)\n            file_path = _fetch_remote(each, dirname=rcv1_dir)\n            files.append(GzipFile(filename=file_path))\n\n        Xy = load_svmlight_files(files, n_features=N_FEATURES)\n\n        # Training data is before testing data\n        X = sp.vstack([Xy[8], Xy[0], Xy[2], Xy[4], Xy[6]]).tocsr()\n        sample_id = np.hstack((Xy[9], Xy[1], Xy[3], Xy[5], Xy[7]))\n        sample_id = sample_id.astype(np.uint32)\n\n        joblib.dump(X, samples_path, compress=9)\n        joblib.dump(sample_id, sample_id_path, compress=9)\n\n        # delete archives\n        for f in files:\n            f.close()\n            remove(f.name)\n    else:\n        X = joblib.load(samples_path)\n        sample_id = joblib.load(sample_id_path)\n\n\n    # load target (y), categories, and sample_id_bis\n    if download_if_missing and (not exists(sample_topics_path) or\n                                not exists(topics_path)):\n        logger.info(\"Downloading %s\" % TOPICS_METADATA.url)\n        topics_archive_path = _fetch_remote(TOPICS_METADATA,\n                                            dirname=rcv1_dir)\n\n        # parse the target file\n        n_cat = -1\n        n_doc = -1\n        doc_previous = -1\n        y = np.zeros((N_SAMPLES, N_CATEGORIES), dtype=np.uint8)\n        sample_id_bis = np.zeros(N_SAMPLES, dtype=np.int32)\n        category_names = {}\n        with GzipFile(filename=topics_archive_path, mode='rb') as f:\n            for line in f:\n                line_components = line.decode(\"ascii\").split(u\" \")\n                if len(line_components) == 3:\n                    cat, doc, _ = line_components\n                    if cat not in category_names:\n                        n_cat += 1\n                        category_names[cat] = n_cat\n\n                    doc = int(doc)\n                    if doc != doc_previous:\n                        doc_previous = doc\n                        n_doc += 1\n                        sample_id_bis[n_doc] = doc\n                    y[n_doc, category_names[cat]] = 1\n\n        # delete archive\n        remove(topics_archive_path)\n\n        # Samples in X are ordered with sample_id,\n        # whereas in y, they are ordered with sample_id_bis.\n        permutation = _find_permutation(sample_id_bis, sample_id)\n        y = y[permutation, :]\n\n        # save category names in a list, with same order than y\n        categories = np.empty(N_CATEGORIES, dtype=object)\n        for k in category_names.keys():\n            categories[category_names[k]] = k\n\n        # reorder categories in lexicographic order\n        order = np.argsort(categories)\n        categories = categories[order]\n        y = sp.csr_matrix(y[:, order])\n\n        joblib.dump(y, sample_topics_path, compress=9)\n        joblib.dump(categories, topics_path, compress=9)\n    else:\n        y = joblib.load(sample_topics_path)\n        categories = joblib.load(topics_path)\n\n    if subset == 'all':\n        pass\n    elif subset == 'train':\n        X = X[:N_TRAIN, :]\n        y = y[:N_TRAIN, :]\n        sample_id = sample_id[:N_TRAIN]\n    elif subset == 'test':\n        X = X[N_TRAIN:, :]\n        y = y[N_TRAIN:, :]\n        sample_id = sample_id[N_TRAIN:]\n    else:\n        raise ValueError(\"Unknown subset parameter. Got '%s' instead of one\"\n                         \" of ('all', 'train', test')\" % subset)\n\n    if shuffle:\n        X, y, sample_id = shuffle_(X, y, sample_id, random_state=random_state)\n\n    if return_X_y:\n        return X, y\n\n    return Bunch(data=X, target=y, sample_id=sample_id,\n                 target_names=categories, DESCR=__doc__)\n\n\ndef _inverse_permutation(p):\n    \"\"\"inverse permutation p\"\"\"\n    n = p.size\n    s = np.zeros(n, dtype=np.int32)\n    i = np.arange(n, dtype=np.int32)\n    np.put(s, p, i)  # s[p] = i\n    return s\n\n\ndef _find_permutation(a, b):\n    \"\"\"find the permutation from a to b\"\"\"\n    t = np.argsort(a)\n    u = np.argsort(b)\n    u_ = _inverse_permutation(u)\n    return t[u_]\n"
    },
    {
      "filename": "sklearn/datasets/tests/test_20news.py",
      "content": "\"\"\"Test the 20news downloader, if the data is available.\"\"\"\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom sklearn.utils.testing import assert_equal\nfrom sklearn.utils.testing import assert_true\nfrom sklearn.utils.testing import SkipTest\nfrom sklearn.datasets.tests.test_common import check_return_X_y\nfrom functools import partial\n\nfrom sklearn import datasets\n\n\ndef test_20news():\n    try:\n        data = datasets.fetch_20newsgroups(\n            subset='all', download_if_missing=False, shuffle=False)\n    except IOError:\n        raise SkipTest(\"Download 20 newsgroups to run this test\")\n\n    # Extract a reduced dataset\n    data2cats = datasets.fetch_20newsgroups(\n        subset='all', categories=data.target_names[-1:-3:-1], shuffle=False)\n    # Check that the ordering of the target_names is the same\n    # as the ordering in the full dataset\n    assert_equal(data2cats.target_names,\n                 data.target_names[-2:])\n    # Assert that we have only 0 and 1 as labels\n    assert_equal(np.unique(data2cats.target).tolist(), [0, 1])\n\n    # Check that the number of filenames is consistent with data/target\n    assert_equal(len(data2cats.filenames), len(data2cats.target))\n    assert_equal(len(data2cats.filenames), len(data2cats.data))\n\n    # Check that the first entry of the reduced dataset corresponds to\n    # the first entry of the corresponding category in the full dataset\n    entry1 = data2cats.data[0]\n    category = data2cats.target_names[data2cats.target[0]]\n    label = data.target_names.index(category)\n    entry2 = data.data[np.where(data.target == label)[0][0]]\n    assert_equal(entry1, entry2)\n\n\ndef test_20news_length_consistency():\n    \"\"\"Checks the length consistencies within the bunch\n\n    This is a non-regression test for a bug present in 0.16.1.\n    \"\"\"\n    try:\n        data = datasets.fetch_20newsgroups(\n            subset='all', download_if_missing=False, shuffle=False)\n    except IOError:\n        raise SkipTest(\"Download 20 newsgroups to run this test\")\n    # Extract the full dataset\n    data = datasets.fetch_20newsgroups(subset='all')\n    assert_equal(len(data['data']), len(data.data))\n    assert_equal(len(data['target']), len(data.target))\n    assert_equal(len(data['filenames']), len(data.filenames))\n\n\ndef test_20news_vectorized():\n    try:\n        datasets.fetch_20newsgroups(subset='all',\n                                    download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"Download 20 newsgroups to run this test\")\n\n    # test subset = train\n    bunch = datasets.fetch_20newsgroups_vectorized(subset=\"train\")\n    assert_true(sp.isspmatrix_csr(bunch.data))\n    assert_equal(bunch.data.shape, (11314, 130107))\n    assert_equal(bunch.target.shape[0], 11314)\n    assert_equal(bunch.data.dtype, np.float64)\n\n    # test subset = test\n    bunch = datasets.fetch_20newsgroups_vectorized(subset=\"test\")\n    assert_true(sp.isspmatrix_csr(bunch.data))\n    assert_equal(bunch.data.shape, (7532, 130107))\n    assert_equal(bunch.target.shape[0], 7532)\n    assert_equal(bunch.data.dtype, np.float64)\n\n    # test return_X_y option\n    fetch_func = partial(datasets.fetch_20newsgroups_vectorized, subset='test')\n    check_return_X_y(bunch, fetch_func)\n\n    # test subset = all\n    bunch = datasets.fetch_20newsgroups_vectorized(subset='all')\n    assert_true(sp.isspmatrix_csr(bunch.data))\n    assert_equal(bunch.data.shape, (11314 + 7532, 130107))\n    assert_equal(bunch.target.shape[0], 11314 + 7532)\n    assert_equal(bunch.data.dtype, np.float64)\n"
    },
    {
      "filename": "sklearn/datasets/tests/test_base.py",
      "content": "import os\nimport shutil\nimport tempfile\nimport warnings\nimport numpy\nfrom pickle import loads\nfrom pickle import dumps\nfrom functools import partial\n\nfrom sklearn.datasets import get_data_home\nfrom sklearn.datasets import clear_data_home\nfrom sklearn.datasets import load_files\nfrom sklearn.datasets import load_sample_images\nfrom sklearn.datasets import load_sample_image\nfrom sklearn.datasets import load_digits\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.datasets import load_linnerud\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.datasets import load_boston\nfrom sklearn.datasets import load_wine\nfrom sklearn.datasets.base import Bunch\nfrom sklearn.datasets.tests.test_common import check_return_X_y\n\nfrom sklearn.externals.six import b, u\nfrom sklearn.externals._pilutil import pillow_installed\n\nfrom sklearn.utils.testing import assert_false\nfrom sklearn.utils.testing import assert_true\nfrom sklearn.utils.testing import assert_equal\nfrom sklearn.utils.testing import assert_raises\n\n\nDATA_HOME = tempfile.mkdtemp(prefix=\"scikit_learn_data_home_test_\")\nLOAD_FILES_ROOT = tempfile.mkdtemp(prefix=\"scikit_learn_load_files_test_\")\nTEST_CATEGORY_DIR1 = \"\"\nTEST_CATEGORY_DIR2 = \"\"\n\n\ndef _remove_dir(path):\n    if os.path.isdir(path):\n        shutil.rmtree(path)\n\n\ndef teardown_module():\n    \"\"\"Test fixture (clean up) run once after all tests of this module\"\"\"\n    for path in [DATA_HOME, LOAD_FILES_ROOT]:\n        _remove_dir(path)\n\n\ndef setup_load_files():\n    global TEST_CATEGORY_DIR1\n    global TEST_CATEGORY_DIR2\n    TEST_CATEGORY_DIR1 = tempfile.mkdtemp(dir=LOAD_FILES_ROOT)\n    TEST_CATEGORY_DIR2 = tempfile.mkdtemp(dir=LOAD_FILES_ROOT)\n    sample_file = tempfile.NamedTemporaryFile(dir=TEST_CATEGORY_DIR1,\n                                              delete=False)\n    sample_file.write(b(\"Hello World!\\n\"))\n    sample_file.close()\n\n\ndef teardown_load_files():\n    _remove_dir(TEST_CATEGORY_DIR1)\n    _remove_dir(TEST_CATEGORY_DIR2)\n\n\ndef test_data_home():\n    # get_data_home will point to a pre-existing folder\n    data_home = get_data_home(data_home=DATA_HOME)\n    assert_equal(data_home, DATA_HOME)\n    assert_true(os.path.exists(data_home))\n\n    # clear_data_home will delete both the content and the folder it-self\n    clear_data_home(data_home=data_home)\n    assert_false(os.path.exists(data_home))\n\n    # if the folder is missing it will be created again\n    data_home = get_data_home(data_home=DATA_HOME)\n    assert_true(os.path.exists(data_home))\n\n\ndef test_default_empty_load_files():\n    res = load_files(LOAD_FILES_ROOT)\n    assert_equal(len(res.filenames), 0)\n    assert_equal(len(res.target_names), 0)\n    assert_equal(res.DESCR, None)\n\n\ndef test_default_load_files():\n    try:\n        setup_load_files()\n        res = load_files(LOAD_FILES_ROOT)\n        assert_equal(len(res.filenames), 1)\n        assert_equal(len(res.target_names), 2)\n        assert_equal(res.DESCR, None)\n        assert_equal(res.data, [b(\"Hello World!\\n\")])\n    finally:\n        teardown_load_files()\n\n\ndef test_load_files_w_categories_desc_and_encoding():\n    try:\n        setup_load_files()\n        category = os.path.abspath(TEST_CATEGORY_DIR1).split('/').pop()\n        res = load_files(LOAD_FILES_ROOT, description=\"test\",\n                         categories=category, encoding=\"utf-8\")\n        assert_equal(len(res.filenames), 1)\n        assert_equal(len(res.target_names), 1)\n        assert_equal(res.DESCR, \"test\")\n        assert_equal(res.data, [u(\"Hello World!\\n\")])\n    finally:\n        teardown_load_files()\n\n\ndef test_load_files_wo_load_content():\n    try:\n        setup_load_files()\n        res = load_files(LOAD_FILES_ROOT, load_content=False)\n        assert_equal(len(res.filenames), 1)\n        assert_equal(len(res.target_names), 2)\n        assert_equal(res.DESCR, None)\n        assert_equal(res.get('data'), None)\n    finally:\n        teardown_load_files()\n\n\ndef test_load_sample_images():\n    try:\n        res = load_sample_images()\n        assert_equal(len(res.images), 2)\n        assert_equal(len(res.filenames), 2)\n        assert_true(res.DESCR)\n    except ImportError:\n        warnings.warn(\"Could not load sample images, PIL is not available.\")\n\n\ndef test_load_digits():\n    digits = load_digits()\n    assert_equal(digits.data.shape, (1797, 64))\n    assert_equal(numpy.unique(digits.target).size, 10)\n\n    # test return_X_y option\n    check_return_X_y(digits, partial(load_digits))\n\n\ndef test_load_digits_n_class_lt_10():\n    digits = load_digits(9)\n    assert_equal(digits.data.shape, (1617, 64))\n    assert_equal(numpy.unique(digits.target).size, 9)\n\n\ndef test_load_sample_image():\n    try:\n        china = load_sample_image('china.jpg')\n        assert_equal(china.dtype, 'uint8')\n        assert_equal(china.shape, (427, 640, 3))\n    except ImportError:\n        warnings.warn(\"Could not load sample images, PIL is not available.\")\n\n\ndef test_load_missing_sample_image_error():\n    if pillow_installed:\n        assert_raises(AttributeError, load_sample_image,\n                      'blop.jpg')\n    else:\n        warnings.warn(\"Could not load sample images, PIL is not available.\")\n\n\ndef test_load_diabetes():\n    res = load_diabetes()\n    assert_equal(res.data.shape, (442, 10))\n    assert_true(res.target.size, 442)\n    assert_equal(len(res.feature_names), 10)\n    assert_true(res.DESCR)\n\n    # test return_X_y option\n    check_return_X_y(res, partial(load_diabetes))\n\n\ndef test_load_linnerud():\n    res = load_linnerud()\n    assert_equal(res.data.shape, (20, 3))\n    assert_equal(res.target.shape, (20, 3))\n    assert_equal(len(res.target_names), 3)\n    assert_true(res.DESCR)\n    assert_true(os.path.exists(res.data_filename))\n    assert_true(os.path.exists(res.target_filename))\n\n    # test return_X_y option\n    check_return_X_y(res, partial(load_linnerud))\n\n\ndef test_load_iris():\n    res = load_iris()\n    assert_equal(res.data.shape, (150, 4))\n    assert_equal(res.target.size, 150)\n    assert_equal(res.target_names.size, 3)\n    assert_true(res.DESCR)\n    assert_true(os.path.exists(res.filename))\n\n    # test return_X_y option\n    check_return_X_y(res, partial(load_iris))\n\n\ndef test_load_wine():\n    res = load_wine()\n    assert_equal(res.data.shape, (178, 13))\n    assert_equal(res.target.size, 178)\n    assert_equal(res.target_names.size, 3)\n    assert_true(res.DESCR)\n\n    # test return_X_y option\n    check_return_X_y(res, partial(load_wine))\n\n\ndef test_load_breast_cancer():\n    res = load_breast_cancer()\n    assert_equal(res.data.shape, (569, 30))\n    assert_equal(res.target.size, 569)\n    assert_equal(res.target_names.size, 2)\n    assert_true(res.DESCR)\n    assert_true(os.path.exists(res.filename))\n\n    # test return_X_y option\n    check_return_X_y(res, partial(load_breast_cancer))\n\n\ndef test_load_boston():\n    res = load_boston()\n    assert_equal(res.data.shape, (506, 13))\n    assert_equal(res.target.size, 506)\n    assert_equal(res.feature_names.size, 13)\n    assert_true(res.DESCR)\n    assert_true(os.path.exists(res.filename))\n\n    # test return_X_y option\n    check_return_X_y(res, partial(load_boston))\n\n\ndef test_loads_dumps_bunch():\n    bunch = Bunch(x=\"x\")\n    bunch_from_pkl = loads(dumps(bunch))\n    bunch_from_pkl.x = \"y\"\n    assert_equal(bunch_from_pkl['x'], bunch_from_pkl.x)\n\n\ndef test_bunch_pickle_generated_with_0_16_and_read_with_0_17():\n    bunch = Bunch(key='original')\n    # This reproduces a problem when Bunch pickles have been created\n    # with scikit-learn 0.16 and are read with 0.17. Basically there\n    # is a suprising behaviour because reading bunch.key uses\n    # bunch.__dict__ (which is non empty for 0.16 Bunch objects)\n    # whereas assigning into bunch.key uses bunch.__setattr__. See\n    # https://github.com/scikit-learn/scikit-learn/issues/6196 for\n    # more details\n    bunch.__dict__['key'] = 'set from __dict__'\n    bunch_from_pkl = loads(dumps(bunch))\n    # After loading from pickle the __dict__ should have been ignored\n    assert_equal(bunch_from_pkl.key, 'original')\n    assert_equal(bunch_from_pkl['key'], 'original')\n    # Making sure that changing the attr does change the value\n    # associated with __getitem__ as well\n    bunch_from_pkl.key = 'changed'\n    assert_equal(bunch_from_pkl.key, 'changed')\n    assert_equal(bunch_from_pkl['key'], 'changed')\n\n\ndef test_bunch_dir():\n    # check that dir (important for autocomplete) shows attributes\n    data = load_iris()\n    assert_true(\"data\" in dir(data))\n"
    },
    {
      "filename": "sklearn/datasets/tests/test_california_housing.py",
      "content": "\"\"\"Test the california_housing loader.\n\nSkipped if california_housing is not already downloaded to data_home.\n\"\"\"\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.utils.testing import SkipTest\nfrom sklearn.datasets.tests.test_common import check_return_X_y\nfrom functools import partial\n\n\ndef fetch(*args, **kwargs):\n    return fetch_california_housing(*args, download_if_missing=False, **kwargs)\n\n\ndef test_fetch():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    assert((20640, 8) == data.data.shape)\n    assert((20640, ) == data.target.shape)\n\n    # test return_X_y option\n    fetch_func = partial(fetch)\n    check_return_X_y(data, fetch_func)\n"
    },
    {
      "filename": "sklearn/datasets/tests/test_common.py",
      "content": "\"\"\"Test loaders for common functionality.\n\"\"\"\n\n\ndef check_return_X_y(bunch, fetch_func_partial):\n    X_y_tuple = fetch_func_partial(return_X_y=True)\n    assert(isinstance(X_y_tuple, tuple))\n    assert(X_y_tuple[0].shape == bunch.data.shape)\n    assert(X_y_tuple[1].shape == bunch.target.shape)\n"
    },
    {
      "filename": "sklearn/datasets/tests/test_covtype.py",
      "content": "\"\"\"Test the covtype loader.\n\nSkipped if covtype is not already downloaded to data_home.\n\"\"\"\n\nfrom sklearn.datasets import fetch_covtype\nfrom sklearn.utils.testing import assert_equal, SkipTest\nfrom sklearn.datasets.tests.test_common import check_return_X_y\nfrom functools import partial\n\n\ndef fetch(*args, **kwargs):\n    return fetch_covtype(*args, download_if_missing=False, **kwargs)\n\n\ndef test_fetch():\n    try:\n        data1 = fetch(shuffle=True, random_state=42)\n    except IOError:\n        raise SkipTest(\"Covertype dataset can not be loaded.\")\n\n    data2 = fetch(shuffle=True, random_state=37)\n\n    X1, X2 = data1['data'], data2['data']\n    assert_equal((581012, 54), X1.shape)\n    assert_equal(X1.shape, X2.shape)\n\n    assert_equal(X1.sum(), X2.sum())\n\n    y1, y2 = data1['target'], data2['target']\n    assert_equal((X1.shape[0],), y1.shape)\n    assert_equal((X1.shape[0],), y2.shape)\n\n    # test return_X_y option\n    fetch_func = partial(fetch)\n    check_return_X_y(data1, fetch_func)\n"
    },
    {
      "filename": "sklearn/datasets/tests/test_kddcup99.py",
      "content": "\"\"\"Test  kddcup99 loader. Only 'percent10' mode is tested, as the full data\nis too big to use in unit-testing.\n\nThe test is skipped if the data wasn't previously fetched and saved to\nscikit-learn data folder.\n\"\"\"\n\nfrom sklearn.datasets import fetch_kddcup99\nfrom sklearn.datasets.tests.test_common import check_return_X_y\nfrom sklearn.utils.testing import assert_equal, SkipTest\nfrom functools import partial\n\n\n\ndef test_percent10():\n    try:\n        data = fetch_kddcup99(download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert_equal(data.data.shape, (494021, 41))\n    assert_equal(data.target.shape, (494021,))\n\n    data_shuffled = fetch_kddcup99(shuffle=True, random_state=0)\n    assert_equal(data.data.shape, data_shuffled.data.shape)\n    assert_equal(data.target.shape, data_shuffled.target.shape)\n\n    data = fetch_kddcup99('SA')\n    assert_equal(data.data.shape, (100655, 41))\n    assert_equal(data.target.shape, (100655,))\n\n    data = fetch_kddcup99('SF')\n    assert_equal(data.data.shape, (73237, 4))\n    assert_equal(data.target.shape, (73237,))\n\n    data = fetch_kddcup99('http')\n    assert_equal(data.data.shape, (58725, 3))\n    assert_equal(data.target.shape, (58725,))\n\n    data = fetch_kddcup99('smtp')\n    assert_equal(data.data.shape, (9571, 3))\n    assert_equal(data.target.shape, (9571,))\n\n    fetch_func = partial(fetch_kddcup99, 'smtp')\n    check_return_X_y(data, fetch_func)\n\n\ndef test_shuffle():\n    try:\n        dataset = fetch_kddcup99(random_state=0, subset='SA', shuffle=True,\n                                 percent10=True, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert(any(dataset.target[-100:] == b'normal.'))\n"
    },
    {
      "filename": "sklearn/datasets/tests/test_lfw.py",
      "content": "\"\"\"This test for the LFW require medium-size data downloading and processing\n\nIf the data has not been already downloaded by running the examples,\nthe tests won't run (skipped).\n\nIf the test are run, the first execution will be long (typically a bit\nmore than a couple of minutes) but as the dataset loader is leveraging\njoblib, successive runs will be fast (less than 200ms).\n\"\"\"\n\nimport random\nimport os\nimport shutil\nimport tempfile\nimport numpy as np\nfrom functools import partial\nfrom sklearn.externals import six\nfrom sklearn.externals._pilutil import pillow_installed, imsave\nfrom sklearn.datasets import fetch_lfw_pairs\nfrom sklearn.datasets import fetch_lfw_people\n\nfrom sklearn.utils.testing import assert_array_equal\nfrom sklearn.utils.testing import assert_equal\nfrom sklearn.utils.testing import SkipTest\nfrom sklearn.utils.testing import assert_raises\nfrom sklearn.datasets.tests.test_common import check_return_X_y\n\n\nSCIKIT_LEARN_DATA = tempfile.mkdtemp(prefix=\"scikit_learn_lfw_test_\")\nSCIKIT_LEARN_EMPTY_DATA = tempfile.mkdtemp(prefix=\"scikit_learn_empty_test_\")\n\nLFW_HOME = os.path.join(SCIKIT_LEARN_DATA, 'lfw_home')\nFAKE_NAMES = [\n    'Abdelatif_Smith',\n    'Abhati_Kepler',\n    'Camara_Alvaro',\n    'Chen_Dupont',\n    'John_Lee',\n    'Lin_Bauman',\n    'Onur_Lopez',\n]\n\n\ndef setup_module():\n    \"\"\"Test fixture run once and common to all tests of this module\"\"\"\n    if not pillow_installed:\n        raise SkipTest(\"PIL not installed.\")\n\n    if not os.path.exists(LFW_HOME):\n        os.makedirs(LFW_HOME)\n\n    random_state = random.Random(42)\n    np_rng = np.random.RandomState(42)\n\n    # generate some random jpeg files for each person\n    counts = {}\n    for name in FAKE_NAMES:\n        folder_name = os.path.join(LFW_HOME, 'lfw_funneled', name)\n        if not os.path.exists(folder_name):\n            os.makedirs(folder_name)\n\n        n_faces = np_rng.randint(1, 5)\n        counts[name] = n_faces\n        for i in range(n_faces):\n            file_path = os.path.join(folder_name, name + '_%04d.jpg' % i)\n            uniface = np_rng.randint(0, 255, size=(250, 250, 3))\n            try:\n                imsave(file_path, uniface)\n            except ImportError:\n                raise SkipTest(\"PIL not installed\")\n\n    # add some random file pollution to test robustness\n    with open(os.path.join(LFW_HOME, 'lfw_funneled', '.test.swp'), 'wb') as f:\n        f.write(six.b('Text file to be ignored by the dataset loader.'))\n\n    # generate some pairing metadata files using the same format as LFW\n    with open(os.path.join(LFW_HOME, 'pairsDevTrain.txt'), 'wb') as f:\n        f.write(six.b(\"10\\n\"))\n        more_than_two = [name for name, count in six.iteritems(counts)\n                         if count >= 2]\n        for i in range(5):\n            name = random_state.choice(more_than_two)\n            first, second = random_state.sample(range(counts[name]), 2)\n            f.write(six.b('%s\\t%d\\t%d\\n' % (name, first, second)))\n\n        for i in range(5):\n            first_name, second_name = random_state.sample(FAKE_NAMES, 2)\n            first_index = random_state.choice(np.arange(counts[first_name]))\n            second_index = random_state.choice(np.arange(counts[second_name]))\n            f.write(six.b('%s\\t%d\\t%s\\t%d\\n' % (first_name, first_index,\n                                                second_name, second_index)))\n\n    with open(os.path.join(LFW_HOME, 'pairsDevTest.txt'), 'wb') as f:\n        f.write(six.b(\"Fake place holder that won't be tested\"))\n\n    with open(os.path.join(LFW_HOME, 'pairs.txt'), 'wb') as f:\n        f.write(six.b(\"Fake place holder that won't be tested\"))\n\n\ndef teardown_module():\n    \"\"\"Test fixture (clean up) run once after all tests of this module\"\"\"\n    if os.path.isdir(SCIKIT_LEARN_DATA):\n        shutil.rmtree(SCIKIT_LEARN_DATA)\n    if os.path.isdir(SCIKIT_LEARN_EMPTY_DATA):\n        shutil.rmtree(SCIKIT_LEARN_EMPTY_DATA)\n\n\ndef test_load_empty_lfw_people():\n    assert_raises(IOError, fetch_lfw_people, data_home=SCIKIT_LEARN_EMPTY_DATA,\n                  download_if_missing=False)\n\n\ndef test_load_fake_lfw_people():\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA,\n                                  min_faces_per_person=3,\n                                  download_if_missing=False)\n\n    # The data is croped around the center as a rectangular bounding box\n    # around the face. Colors are converted to gray levels:\n    assert_equal(lfw_people.images.shape, (10, 62, 47))\n    assert_equal(lfw_people.data.shape, (10, 2914))\n\n    # the target is array of person integer ids\n    assert_array_equal(lfw_people.target, [2, 0, 1, 0, 2, 0, 2, 1, 1, 2])\n\n    # names of the persons can be found using the target_names array\n    expected_classes = ['Abdelatif Smith', 'Abhati Kepler', 'Onur Lopez']\n    assert_array_equal(lfw_people.target_names, expected_classes)\n\n    # It is possible to ask for the original data without any croping or color\n    # conversion and not limit on the number of picture per person\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, resize=None,\n                                  slice_=None, color=True,\n                                  download_if_missing=False)\n    assert_equal(lfw_people.images.shape, (17, 250, 250, 3))\n\n    # the ids and class names are the same as previously\n    assert_array_equal(lfw_people.target,\n                       [0, 0, 1, 6, 5, 6, 3, 6, 0, 3, 6, 1, 2, 4, 5, 1, 2])\n    assert_array_equal(lfw_people.target_names,\n                       ['Abdelatif Smith', 'Abhati Kepler', 'Camara Alvaro',\n                        'Chen Dupont', 'John Lee', 'Lin Bauman', 'Onur Lopez'])\n\n    # test return_X_y option\n    fetch_func = partial(fetch_lfw_people, data_home=SCIKIT_LEARN_DATA,\n                         resize=None,\n                         slice_=None, color=True,\n                         download_if_missing=False)\n    check_return_X_y(lfw_people, fetch_func)\n\n\ndef test_load_fake_lfw_people_too_restrictive():\n    assert_raises(ValueError, fetch_lfw_people, data_home=SCIKIT_LEARN_DATA,\n                  min_faces_per_person=100, download_if_missing=False)\n\n\ndef test_load_empty_lfw_pairs():\n    assert_raises(IOError, fetch_lfw_pairs,\n                  data_home=SCIKIT_LEARN_EMPTY_DATA,\n                  download_if_missing=False)\n\n\ndef test_load_fake_lfw_pairs():\n    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                      download_if_missing=False)\n\n    # The data is croped around the center as a rectangular bounding box\n    # around the face. Colors are converted to gray levels:\n    assert_equal(lfw_pairs_train.pairs.shape, (10, 2, 62, 47))\n\n    # the target is whether the person is the same or not\n    assert_array_equal(lfw_pairs_train.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n\n    # names of the persons can be found using the target_names array\n    expected_classes = ['Different persons', 'Same person']\n    assert_array_equal(lfw_pairs_train.target_names, expected_classes)\n\n    # It is possible to ask for the original data without any croping or color\n    # conversion\n    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, resize=None,\n                                      slice_=None, color=True,\n                                      download_if_missing=False)\n    assert_equal(lfw_pairs_train.pairs.shape, (10, 2, 250, 250, 3))\n\n    # the ids and class names are the same as previously\n    assert_array_equal(lfw_pairs_train.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n    assert_array_equal(lfw_pairs_train.target_names, expected_classes)\n"
    },
    {
      "filename": "sklearn/datasets/tests/test_rcv1.py",
      "content": "\"\"\"Test the rcv1 loader.\n\nSkipped if rcv1 is not already downloaded to data_home.\n\"\"\"\n\nimport errno\nimport scipy.sparse as sp\nimport numpy as np\nfrom functools import partial\nfrom sklearn.datasets import fetch_rcv1\nfrom sklearn.datasets.tests.test_common import check_return_X_y\nfrom sklearn.utils.testing import assert_almost_equal\nfrom sklearn.utils.testing import assert_array_equal\nfrom sklearn.utils.testing import assert_equal\nfrom sklearn.utils.testing import assert_true\nfrom sklearn.utils.testing import SkipTest\n\n\ndef test_fetch_rcv1():\n    try:\n        data1 = fetch_rcv1(shuffle=False, download_if_missing=False)\n    except IOError as e:\n        if e.errno == errno.ENOENT:\n            raise SkipTest(\"Download RCV1 dataset to run this test.\")\n\n    X1, Y1 = data1.data, data1.target\n    cat_list, s1 = data1.target_names.tolist(), data1.sample_id\n\n    # test sparsity\n    assert_true(sp.issparse(X1))\n    assert_true(sp.issparse(Y1))\n    assert_equal(60915113, X1.data.size)\n    assert_equal(2606875, Y1.data.size)\n\n    # test shapes\n    assert_equal((804414, 47236), X1.shape)\n    assert_equal((804414, 103), Y1.shape)\n    assert_equal((804414,), s1.shape)\n    assert_equal(103, len(cat_list))\n\n    # test ordering of categories\n    first_categories = [u'C11', u'C12', u'C13', u'C14', u'C15', u'C151']\n    assert_array_equal(first_categories, cat_list[:6])\n\n    # test number of sample for some categories\n    some_categories = ('GMIL', 'E143', 'CCAT')\n    number_non_zero_in_cat = (5, 1206, 381327)\n    for num, cat in zip(number_non_zero_in_cat, some_categories):\n        j = cat_list.index(cat)\n        assert_equal(num, Y1[:, j].data.size)\n\n    # test shuffling and subset\n    data2 = fetch_rcv1(shuffle=True, subset='train', random_state=77,\n                       download_if_missing=False)\n    X2, Y2 = data2.data, data2.target\n    s2 = data2.sample_id\n\n    # test return_X_y option\n    fetch_func = partial(fetch_rcv1, shuffle=False, subset='train',\n                         download_if_missing=False)\n    check_return_X_y(data2, fetch_func)\n\n    # The first 23149 samples are the training samples\n    assert_array_equal(np.sort(s1[:23149]), np.sort(s2))\n\n    # test some precise values\n    some_sample_ids = (2286, 3274, 14042)\n    for sample_id in some_sample_ids:\n        idx1 = s1.tolist().index(sample_id)\n        idx2 = s2.tolist().index(sample_id)\n\n        feature_values_1 = X1[idx1, :].toarray()\n        feature_values_2 = X2[idx2, :].toarray()\n        assert_almost_equal(feature_values_1, feature_values_2)\n\n        target_values_1 = Y1[idx1, :].toarray()\n        target_values_2 = Y2[idx2, :].toarray()\n        assert_almost_equal(target_values_1, target_values_2)\n"
    },
    {
      "filename": "sklearn/datasets/twenty_newsgroups.py",
      "content": "\"\"\"Caching loader for the 20 newsgroups text classification dataset\n\n\nThe description of the dataset is available on the official website at:\n\n    http://people.csail.mit.edu/jrennie/20Newsgroups/\n\nQuoting the introduction:\n\n    The 20 Newsgroups data set is a collection of approximately 20,000\n    newsgroup documents, partitioned (nearly) evenly across 20 different\n    newsgroups. To the best of my knowledge, it was originally collected\n    by Ken Lang, probably for his Newsweeder: Learning to filter netnews\n    paper, though he does not explicitly mention this collection. The 20\n    newsgroups collection has become a popular data set for experiments\n    in text applications of machine learning techniques, such as text\n    classification and text clustering.\n\nThis dataset loader will download the recommended \"by date\" variant of the\ndataset and which features a point in time split between the train and\ntest sets. The compressed dataset size is around 14 Mb compressed. Once\nuncompressed the train set is 52 MB and the test set is 34 MB.\n\nThe data is downloaded, extracted and cached in the '~/scikit_learn_data'\nfolder.\n\nThe `fetch_20newsgroups` function will not vectorize the data into numpy\narrays but the dataset lists the filenames of the posts and their categories\nas target labels.\n\nThe `fetch_20newsgroups_vectorized` function will in addition do a simple\ntf-idf vectorization step.\n\n\"\"\"\n# Copyright (c) 2011 Olivier Grisel <olivier.grisel@ensta.org>\n# License: BSD 3 clause\n\nimport os\nimport logging\nimport tarfile\nimport pickle\nimport shutil\nimport re\nimport codecs\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom .base import get_data_home\nfrom .base import load_files\nfrom .base import _pkl_filepath\nfrom .base import _fetch_remote\nfrom .base import RemoteFileMetadata\nfrom ..utils import check_random_state, Bunch\nfrom ..feature_extraction.text import CountVectorizer\nfrom ..preprocessing import normalize\nfrom ..externals import joblib\n\nlogger = logging.getLogger(__name__)\n\n# The original data can be found at:\n# http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz\nARCHIVE = RemoteFileMetadata(\n    filename='20news-bydate.tar.gz',\n    url='https://ndownloader.figshare.com/files/5975967',\n    checksum=('8f1b2514ca22a5ade8fbb9cfa5727df9'\n              '5fa587f4c87b786e15c759fa66d95610'))\n\nCACHE_NAME = \"20news-bydate.pkz\"\nTRAIN_FOLDER = \"20news-bydate-train\"\nTEST_FOLDER = \"20news-bydate-test\"\n\n\ndef download_20newsgroups(target_dir, cache_path):\n    \"\"\"Download the 20 newsgroups data and stored it as a zipped pickle.\"\"\"\n    train_path = os.path.join(target_dir, TRAIN_FOLDER)\n    test_path = os.path.join(target_dir, TEST_FOLDER)\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    logger.info(\"Downloading dataset from %s (14 MB)\", ARCHIVE.url)\n    archive_path = _fetch_remote(ARCHIVE, dirname=target_dir)\n\n    logger.debug(\"Decompressing %s\", archive_path)\n    tarfile.open(archive_path, \"r:gz\").extractall(path=target_dir)\n    os.remove(archive_path)\n\n    # Store a zipped pickle\n    cache = dict(train=load_files(train_path, encoding='latin1'),\n                 test=load_files(test_path, encoding='latin1'))\n    compressed_content = codecs.encode(pickle.dumps(cache), 'zlib_codec')\n    with open(cache_path, 'wb') as f:\n        f.write(compressed_content)\n\n    shutil.rmtree(target_dir)\n    return cache\n\n\ndef strip_newsgroup_header(text):\n    \"\"\"\n    Given text in \"news\" format, strip the headers, by removing everything\n    before the first blank line.\n    \"\"\"\n    _before, _blankline, after = text.partition('\\n\\n')\n    return after\n\n\n_QUOTE_RE = re.compile(r'(writes in|writes:|wrote:|says:|said:'\n                       r'|^In article|^Quoted from|^\\||^>)')\n\n\ndef strip_newsgroup_quoting(text):\n    \"\"\"\n    Given text in \"news\" format, strip lines beginning with the quote\n    characters > or |, plus lines that often introduce a quoted section\n    (for example, because they contain the string 'writes:'.)\n    \"\"\"\n    good_lines = [line for line in text.split('\\n')\n                  if not _QUOTE_RE.search(line)]\n    return '\\n'.join(good_lines)\n\n\ndef strip_newsgroup_footer(text):\n    \"\"\"\n    Given text in \"news\" format, attempt to remove a signature block.\n\n    As a rough heuristic, we assume that signatures are set apart by either\n    a blank line or a line made of hyphens, and that it is the last such line\n    in the file (disregarding blank lines at the end).\n    \"\"\"\n    lines = text.strip().split('\\n')\n    for line_num in range(len(lines) - 1, -1, -1):\n        line = lines[line_num]\n        if line.strip().strip('-') == '':\n            break\n\n    if line_num > 0:\n        return '\\n'.join(lines[:line_num])\n    else:\n        return text\n\n\ndef fetch_20newsgroups(data_home=None, subset='train', categories=None,\n                       shuffle=True, random_state=42,\n                       remove=(),\n                       download_if_missing=True):\n    \"\"\"Load the filenames and data from the 20 newsgroups dataset.\n\n    Read more in the :ref:`User Guide <20newsgroups>`.\n\n    Parameters\n    ----------\n    data_home : optional, default: None\n        Specify a download and cache folder for the datasets. If None,\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    subset : 'train' or 'test', 'all', optional\n        Select the dataset to load: 'train' for the training set, 'test'\n        for the test set, 'all' for both, with shuffled ordering.\n\n    categories : None or collection of string or unicode\n        If None (default), load all the categories.\n        If not None, list of category names to load (other categories\n        ignored).\n\n    shuffle : bool, optional\n        Whether or not to shuffle the data: might be important for models that\n        make the assumption that the samples are independent and identically\n        distributed (i.i.d.), such as stochastic gradient descent.\n\n    random_state : numpy random number generator or seed integer\n        Used to shuffle the dataset.\n\n    remove : tuple\n        May contain any subset of ('headers', 'footers', 'quotes'). Each of\n        these are kinds of text that will be detected and removed from the\n        newsgroup posts, preventing classifiers from overfitting on\n        metadata.\n\n        'headers' removes newsgroup headers, 'footers' removes blocks at the\n        ends of posts that look like signatures, and 'quotes' removes lines\n        that appear to be quoting another post.\n\n        'headers' follows an exact standard; the other filters are not always\n        correct.\n\n    download_if_missing : optional, True by default\n        If False, raise an IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n    \"\"\"\n\n    data_home = get_data_home(data_home=data_home)\n    cache_path = _pkl_filepath(data_home, CACHE_NAME)\n    twenty_home = os.path.join(data_home, \"20news_home\")\n    cache = None\n    if os.path.exists(cache_path):\n        try:\n            with open(cache_path, 'rb') as f:\n                compressed_content = f.read()\n            uncompressed_content = codecs.decode(\n                compressed_content, 'zlib_codec')\n            cache = pickle.loads(uncompressed_content)\n        except Exception as e:\n            print(80 * '_')\n            print('Cache loading failed')\n            print(80 * '_')\n            print(e)\n\n    if cache is None:\n        if download_if_missing:\n            logger.info(\"Downloading 20news dataset. \"\n                        \"This may take a few minutes.\")\n            cache = download_20newsgroups(target_dir=twenty_home,\n                                          cache_path=cache_path)\n        else:\n            raise IOError('20Newsgroups dataset not found')\n\n    if subset in ('train', 'test'):\n        data = cache[subset]\n    elif subset == 'all':\n        data_lst = list()\n        target = list()\n        filenames = list()\n        for subset in ('train', 'test'):\n            data = cache[subset]\n            data_lst.extend(data.data)\n            target.extend(data.target)\n            filenames.extend(data.filenames)\n\n        data.data = data_lst\n        data.target = np.array(target)\n        data.filenames = np.array(filenames)\n    else:\n        raise ValueError(\n            \"subset can only be 'train', 'test' or 'all', got '%s'\" % subset)\n\n    data.description = 'the 20 newsgroups by date dataset'\n\n    if 'headers' in remove:\n        data.data = [strip_newsgroup_header(text) for text in data.data]\n    if 'footers' in remove:\n        data.data = [strip_newsgroup_footer(text) for text in data.data]\n    if 'quotes' in remove:\n        data.data = [strip_newsgroup_quoting(text) for text in data.data]\n\n    if categories is not None:\n        labels = [(data.target_names.index(cat), cat) for cat in categories]\n        # Sort the categories to have the ordering of the labels\n        labels.sort()\n        labels, categories = zip(*labels)\n        mask = np.in1d(data.target, labels)\n        data.filenames = data.filenames[mask]\n        data.target = data.target[mask]\n        # searchsorted to have continuous labels\n        data.target = np.searchsorted(labels, data.target)\n        data.target_names = list(categories)\n        # Use an object array to shuffle: avoids memory copy\n        data_lst = np.array(data.data, dtype=object)\n        data_lst = data_lst[mask]\n        data.data = data_lst.tolist()\n\n    if shuffle:\n        random_state = check_random_state(random_state)\n        indices = np.arange(data.target.shape[0])\n        random_state.shuffle(indices)\n        data.filenames = data.filenames[indices]\n        data.target = data.target[indices]\n        # Use an object array to shuffle: avoids memory copy\n        data_lst = np.array(data.data, dtype=object)\n        data_lst = data_lst[indices]\n        data.data = data_lst.tolist()\n\n    return data\n\n\ndef fetch_20newsgroups_vectorized(subset=\"train\", remove=(), data_home=None,\n                                  download_if_missing=True, return_X_y=False):\n    \"\"\"Load the 20 newsgroups dataset and transform it into tf-idf vectors.\n\n    This is a convenience function; the tf-idf transformation is done using the\n    default settings for `sklearn.feature_extraction.text.Vectorizer`. For more\n    advanced usage (stopword filtering, n-gram extraction, etc.), combine\n    fetch_20newsgroups with a custom `Vectorizer` or `CountVectorizer`.\n\n    Read more in the :ref:`User Guide <20newsgroups>`.\n\n    Parameters\n    ----------\n    subset : 'train' or 'test', 'all', optional\n        Select the dataset to load: 'train' for the training set, 'test'\n        for the test set, 'all' for both, with shuffled ordering.\n\n    remove : tuple\n        May contain any subset of ('headers', 'footers', 'quotes'). Each of\n        these are kinds of text that will be detected and removed from the\n        newsgroup posts, preventing classifiers from overfitting on\n        metadata.\n\n        'headers' removes newsgroup headers, 'footers' removes blocks at the\n        ends of posts that look like signatures, and 'quotes' removes lines\n        that appear to be quoting another post.\n\n    data_home : optional, default: None\n        Specify an download and cache folder for the datasets. If None,\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    download_if_missing : optional, True by default\n        If False, raise an IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    return_X_y : boolean, default=False. If True, returns ``(data.data,\n    data.target)`` instead of a Bunch object.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    bunch : Bunch object\n        bunch.data: sparse matrix, shape [n_samples, n_features]\n        bunch.target: array, shape [n_samples]\n        bunch.target_names: list, length [n_classes]\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.20\n    \"\"\"\n    data_home = get_data_home(data_home=data_home)\n    filebase = '20newsgroup_vectorized'\n    if remove:\n        filebase += 'remove-' + ('-'.join(remove))\n    target_file = _pkl_filepath(data_home, filebase + \".pkl\")\n\n    # we shuffle but use a fixed seed for the memoization\n    data_train = fetch_20newsgroups(data_home=data_home,\n                                    subset='train',\n                                    categories=None,\n                                    shuffle=True,\n                                    random_state=12,\n                                    remove=remove,\n                                    download_if_missing=download_if_missing)\n\n    data_test = fetch_20newsgroups(data_home=data_home,\n                                   subset='test',\n                                   categories=None,\n                                   shuffle=True,\n                                   random_state=12,\n                                   remove=remove,\n                                   download_if_missing=download_if_missing)\n\n    if os.path.exists(target_file):\n        X_train, X_test = joblib.load(target_file)\n    else:\n        vectorizer = CountVectorizer(dtype=np.int16)\n        X_train = vectorizer.fit_transform(data_train.data).tocsr()\n        X_test = vectorizer.transform(data_test.data).tocsr()\n        joblib.dump((X_train, X_test), target_file, compress=9)\n\n    # the data is stored as int16 for compactness\n    # but normalize needs floats\n    X_train = X_train.astype(np.float64)\n    X_test = X_test.astype(np.float64)\n    normalize(X_train, copy=False)\n    normalize(X_test, copy=False)\n\n    target_names = data_train.target_names\n\n    if subset == \"train\":\n        data = X_train\n        target = data_train.target\n    elif subset == \"test\":\n        data = X_test\n        target = data_test.target\n    elif subset == \"all\":\n        data = sp.vstack((X_train, X_test)).tocsr()\n        target = np.concatenate((data_train.target, data_test.target))\n    else:\n        raise ValueError(\"%r is not a valid subset: should be one of \"\n                         \"['train', 'test', 'all']\" % subset)\n\n    if return_X_y:\n        return data, target\n\n    return Bunch(data=data, target=target, target_names=target_names)\n"
    }
  ]
}