{
  "repo_name": "scrapy_scrapy",
  "issue_id": "3928",
  "issue_description": "# Save images on remote server\n\nI want to save images on a remote server via (FTP?). Is this possible with scrapy? \r\n(I know about S3 or Google but i mean in a custom FTP server).",
  "issue_comments": [
    {
      "id": 518192557,
      "user": "Gallaecio",
      "body": "https://docs.scrapy.org/en/latest/search.html?q=ftp&check_keywords=yes&area=default"
    },
    {
      "id": 518196116,
      "user": "bezkos",
      "body": "I dont think scrapy support saving files, images to FTP servers except S3 or Google.\r\nTo achieve that, i patched files.py, images.py and pipelines.py and created a new class FTPStore."
    },
    {
      "id": 518198331,
      "user": "Gallaecio",
      "body": "Sorry, I missed the â€˜imagesâ€™ bit :man_facepalming: "
    },
    {
      "id": 518645800,
      "user": "webtekindo",
      "body": "Hi @bezkos, we are also looking for the same kind of solution, do you mind sharing your class FTPStore ? That will be very helpfull ðŸ‘ \r\n\r\n"
    },
    {
      "id": 520389476,
      "user": "bezkos",
      "body": "```python\r\nclass FTPStore(object):\r\n\r\n    FTP_USERNAME = None\r\n    FTP_PASSWORD = None    \r\n\r\n    def __init__(self, uri):\r\n        assert uri.startswith('ftp://')\r\n        serverip = uri.split('://', 1)[1]\r\n        self.ftp = FTP()\r\n        # self.ftp.set_debuglevel(2)\r\n        self.ftp.connect(serverip, 21)\r\n        self.ftp.login(self.FTP_USERNAME, self.FTP_PASSWORD)        \r\n\r\n    def persist_file(self, path, buf, info, meta=None, headers=None):        \r\n        buf.seek(0)\r\n        self.ftp.storbinary('STOR %s' %path, buf)\t\r\n    \r\n    def stat_file(self, path, info):\r\n        Custom code here cause i needed to download images only 1 time without expiration.\r\n\r\n        return {'last_modified': last_modified, 'checksum': path}\r\n```\r\n\r\nHope I helped...\r\n"
    },
    {
      "id": 521642862,
      "user": "OmarFarrag",
      "body": "Working on that "
    },
    {
      "id": 533793569,
      "user": "faizan2700",
      "body": "Kindly let me know if someone is still working on this if not please assign this to me. If someone is working on this can you tell me which \"good first issues\" (or easy ones ) are unclaimed yet I have tried to get one other before and that was already claimed by someone. Thank you in advance."
    },
    {
      "id": 534594505,
      "user": "Gallaecio",
      "body": "> Kindly let me know if someone is still working on this\r\n\r\nSee #3961\r\n\r\n"
    },
    {
      "id": 534600156,
      "user": "faizan2700",
      "body": "I will study codebase according to that @Gallaecio "
    },
    {
      "id": 536974330,
      "user": "hk1997",
      "body": "Is someone still working on this?\r\nI would love to contribute"
    },
    {
      "id": 536979975,
      "user": "faizan2700",
      "body": "I looked at the data base and tried to understand but for now I am not working on this issue and @Gallaecio will tell you if someone else is workkng on this issue"
    },
    {
      "id": 536996944,
      "user": "Gallaecio",
      "body": "@hk1997 You can see â€œMay be fixed by #3961â€ at the top of this issue, right under the issue title."
    }
  ],
  "text_context": "# Save images on remote server\n\nI want to save images on a remote server via (FTP?). Is this possible with scrapy? \r\n(I know about S3 or Google but i mean in a custom FTP server).\n\nhttps://docs.scrapy.org/en/latest/search.html?q=ftp&check_keywords=yes&area=default\n\nI dont think scrapy support saving files, images to FTP servers except S3 or Google.\r\nTo achieve that, i patched files.py, images.py and pipelines.py and created a new class FTPStore.\n\nSorry, I missed the â€˜imagesâ€™ bit :man_facepalming: \n\nHi @bezkos, we are also looking for the same kind of solution, do you mind sharing your class FTPStore ? That will be very helpfull ðŸ‘ \r\n\r\n\n\n```python\r\nclass FTPStore(object):\r\n\r\n    FTP_USERNAME = None\r\n    FTP_PASSWORD = None    \r\n\r\n    def __init__(self, uri):\r\n        assert uri.startswith('ftp://')\r\n        serverip = uri.split('://', 1)[1]\r\n        self.ftp = FTP()\r\n        # self.ftp.set_debuglevel(2)\r\n        self.ftp.connect(serverip, 21)\r\n        self.ftp.login(self.FTP_USERNAME, self.FTP_PASSWORD)        \r\n\r\n    def persist_file(self, path, buf, info, meta=None, headers=None):        \r\n        buf.seek(0)\r\n        self.ftp.storbinary('STOR %s' %path, buf)\t\r\n    \r\n    def stat_file(self, path, info):\r\n        Custom code here cause i needed to download images only 1 time without expiration.\r\n\r\n        return {'last_modified': last_modified, 'checksum': path}\r\n```\r\n\r\nHope I helped...\r\n\n\nWorking on that \n\nKindly let me know if someone is still working on this if not please assign this to me. If someone is working on this can you tell me which \"good first issues\" (or easy ones ) are unclaimed yet I have tried to get one other before and that was already claimed by someone. Thank you in advance.\n\n> Kindly let me know if someone is still working on this\r\n\r\nSee #3961\r\n\r\n\n\nI will study codebase according to that @Gallaecio \n\nIs someone still working on this?\r\nI would love to contribute\n\nI looked at the data base and tried to understand but for now I am not working on this issue and @Gallaecio will tell you if someone else is workkng on this issue\n\n@hk1997 You can see â€œMay be fixed by #3961â€ at the top of this issue, right under the issue title.",
  "pr_link": "https://github.com/scrapy/scrapy/pull/3961",
  "code_context": [
    {
      "filename": "scrapy/extensions/feedexport.py",
      "content": "\"\"\"\nFeed Exports extension\n\nSee documentation in docs/topics/feed-exports.rst\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom tempfile import NamedTemporaryFile\nfrom datetime import datetime\nfrom urllib.parse import urlparse, unquote\n\nfrom zope.interface import Interface, implementer\nfrom twisted.internet import defer, threads\nfrom w3lib.url import file_uri_to_path\n\nfrom scrapy import signals\nfrom scrapy.utils.ftp import ftp_store_file\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.misc import create_instance, load_object\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.python import without_none_values\nfrom scrapy.utils.boto import is_botocore\n\nlogger = logging.getLogger(__name__)\n\n\nclass IFeedStorage(Interface):\n    \"\"\"Interface that all Feed Storages must implement\"\"\"\n\n    def __init__(uri):\n        \"\"\"Initialize the storage with the parameters given in the URI\"\"\"\n\n    def open(spider):\n        \"\"\"Open the storage for the given spider. It must return a file-like\n        object that will be used for the exporters\"\"\"\n\n    def store(file):\n        \"\"\"Store the given file stream\"\"\"\n\n\n@implementer(IFeedStorage)\nclass BlockingFeedStorage(object):\n\n    def open(self, spider):\n        path = spider.crawler.settings['FEED_TEMPDIR']\n        if path and not os.path.isdir(path):\n            raise OSError('Not a Directory: ' + str(path))\n\n        return NamedTemporaryFile(prefix='feed-', dir=path)\n\n    def store(self, file):\n        return threads.deferToThread(self._store_in_thread, file)\n\n    def _store_in_thread(self, file):\n        raise NotImplementedError\n\n\n@implementer(IFeedStorage)\nclass StdoutFeedStorage(object):\n\n    def __init__(self, uri, _stdout=None):\n        if not _stdout:\n            _stdout = sys.stdout.buffer\n        self._stdout = _stdout\n\n    def open(self, spider):\n        return self._stdout\n\n    def store(self, file):\n        pass\n\n\n@implementer(IFeedStorage)\nclass FileFeedStorage(object):\n\n    def __init__(self, uri):\n        self.path = file_uri_to_path(uri)\n\n    def open(self, spider):\n        dirname = os.path.dirname(self.path)\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname)\n        return open(self.path, 'ab')\n\n    def store(self, file):\n        file.close()\n\n\nclass S3FeedStorage(BlockingFeedStorage):\n\n    def __init__(self, uri, access_key=None, secret_key=None, acl=None):\n        # BEGIN Backward compatibility for initialising without keys (and\n        # without using from_crawler)\n        no_defaults = access_key is None and secret_key is None\n        if no_defaults:\n            from scrapy.utils.project import get_project_settings\n            settings = get_project_settings()\n            if 'AWS_ACCESS_KEY_ID' in settings or 'AWS_SECRET_ACCESS_KEY' in settings:\n                import warnings\n                from scrapy.exceptions import ScrapyDeprecationWarning\n                warnings.warn(\n                    \"Initialising `scrapy.extensions.feedexport.S3FeedStorage` \"\n                    \"without AWS keys is deprecated. Please supply credentials or \"\n                    \"use the `from_crawler()` constructor.\",\n                    category=ScrapyDeprecationWarning,\n                    stacklevel=2\n                )\n                access_key = settings['AWS_ACCESS_KEY_ID']\n                secret_key = settings['AWS_SECRET_ACCESS_KEY']\n        # END Backward compatibility\n        u = urlparse(uri)\n        self.bucketname = u.hostname\n        self.access_key = u.username or access_key\n        self.secret_key = u.password or secret_key\n        self.is_botocore = is_botocore()\n        self.keyname = u.path[1:]  # remove first \"/\"\n        self.acl = acl\n        if self.is_botocore:\n            import botocore.session\n            session = botocore.session.get_session()\n            self.s3_client = session.create_client(\n                's3', aws_access_key_id=self.access_key,\n                aws_secret_access_key=self.secret_key)\n        else:\n            import boto\n            self.connect_s3 = boto.connect_s3\n\n    @classmethod\n    def from_crawler(cls, crawler, uri):\n        return cls(\n            uri=uri,\n            access_key=crawler.settings['AWS_ACCESS_KEY_ID'],\n            secret_key=crawler.settings['AWS_SECRET_ACCESS_KEY'],\n            acl=crawler.settings['FEED_STORAGE_S3_ACL'] or None\n        )\n\n    def _store_in_thread(self, file):\n        file.seek(0)\n        if self.is_botocore:\n            kwargs = {'ACL': self.acl} if self.acl else {}\n            self.s3_client.put_object(\n                Bucket=self.bucketname, Key=self.keyname, Body=file,\n                **kwargs)\n        else:\n            conn = self.connect_s3(self.access_key, self.secret_key)\n            bucket = conn.get_bucket(self.bucketname, validate=False)\n            key = bucket.new_key(self.keyname)\n            kwargs = {'policy': self.acl} if self.acl else {}\n            key.set_contents_from_file(file, **kwargs)\n            key.close()\n\n\nclass FTPFeedStorage(BlockingFeedStorage):\n\n    def __init__(self, uri, use_active_mode=False):\n        u = urlparse(uri)\n        self.host = u.hostname\n        self.port = int(u.port or '21')\n        self.username = u.username\n        self.password = unquote(u.password)\n        self.path = u.path\n        self.use_active_mode = use_active_mode\n\n    @classmethod\n    def from_crawler(cls, crawler, uri):\n        return cls(\n            uri=uri,\n            use_active_mode=crawler.settings.getbool('FEED_STORAGE_FTP_ACTIVE')\n        )\n\n    def _store_in_thread(self, file):\n        ftp_store_file(\n            path=self.path, file=file, host=self.host,\n            port=self.port, username=self.username,\n            password=self.password, use_active_mode=self.use_active_mode\n        )\n\n\nclass SpiderSlot(object):\n    def __init__(self, file, exporter, storage, uri):\n        self.file = file\n        self.exporter = exporter\n        self.storage = storage\n        self.uri = uri\n        self.itemcount = 0\n\n\nclass FeedExporter(object):\n\n    def __init__(self, settings):\n        self.settings = settings\n        if not settings['FEED_URI']:\n            raise NotConfigured\n        self.urifmt = str(settings['FEED_URI'])\n        self.format = settings['FEED_FORMAT'].lower()\n        self.export_encoding = settings['FEED_EXPORT_ENCODING']\n        self.storages = self._load_components('FEED_STORAGES')\n        self.exporters = self._load_components('FEED_EXPORTERS')\n        if not self._storage_supported(self.urifmt):\n            raise NotConfigured\n        if not self._exporter_supported(self.format):\n            raise NotConfigured\n        self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n        self._exporting = False\n        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None\n        self.indent = None\n        if settings.get('FEED_EXPORT_INDENT') is not None:\n            self.indent = settings.getint('FEED_EXPORT_INDENT')\n        uripar = settings['FEED_URI_PARAMS']\n        self._uripar = load_object(uripar) if uripar else lambda x, y: None\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        o = cls(crawler.settings)\n        o.crawler = crawler\n        crawler.signals.connect(o.open_spider, signals.spider_opened)\n        crawler.signals.connect(o.close_spider, signals.spider_closed)\n        crawler.signals.connect(o.item_scraped, signals.item_scraped)\n        return o\n\n    def open_spider(self, spider):\n        uri = self.urifmt % self._get_uri_params(spider)\n        storage = self._get_storage(uri)\n        file = storage.open(spider)\n        exporter = self._get_exporter(file, fields_to_export=self.export_fields,\n            encoding=self.export_encoding, indent=self.indent)\n        if self.store_empty:\n            exporter.start_exporting()\n            self._exporting = True\n        self.slot = SpiderSlot(file, exporter, storage, uri)\n\n    def close_spider(self, spider):\n        slot = self.slot\n        if not slot.itemcount and not self.store_empty:\n            # We need to call slot.storage.store nonetheless to get the file\n            # properly closed.\n            return defer.maybeDeferred(slot.storage.store, slot.file)\n        if self._exporting:\n            slot.exporter.finish_exporting()\n            self._exporting = False\n        logfmt = \"%s %%(format)s feed (%%(itemcount)d items) in: %%(uri)s\"\n        log_args = {'format': self.format,\n                    'itemcount': slot.itemcount,\n                    'uri': slot.uri}\n        d = defer.maybeDeferred(slot.storage.store, slot.file)\n        d.addCallback(lambda _: logger.info(logfmt % \"Stored\", log_args,\n                                            extra={'spider': spider}))\n        d.addErrback(lambda f: logger.error(logfmt % \"Error storing\", log_args,\n                                            exc_info=failure_to_exc_info(f),\n                                            extra={'spider': spider}))\n        return d\n\n    def item_scraped(self, item, spider):\n        slot = self.slot\n        if not self._exporting:\n            slot.exporter.start_exporting()\n            self._exporting = True\n        slot.exporter.export_item(item)\n        slot.itemcount += 1\n        return item\n\n    def _load_components(self, setting_prefix):\n        conf = without_none_values(self.settings.getwithbase(setting_prefix))\n        d = {}\n        for k, v in conf.items():\n            try:\n                d[k] = load_object(v)\n            except NotConfigured:\n                pass\n        return d\n\n    def _exporter_supported(self, format):\n        if format in self.exporters:\n            return True\n        logger.error(\"Unknown feed format: %(format)s\", {'format': format})\n\n    def _storage_supported(self, uri):\n        scheme = urlparse(uri).scheme\n        if scheme in self.storages:\n            try:\n                self._get_storage(uri)\n                return True\n            except NotConfigured as e:\n                logger.error(\"Disabled feed storage scheme: %(scheme)s. \"\n                             \"Reason: %(reason)s\",\n                             {'scheme': scheme, 'reason': str(e)})\n        else:\n            logger.error(\"Unknown feed storage scheme: %(scheme)s\",\n                         {'scheme': scheme})\n\n    def _get_instance(self, objcls, *args, **kwargs):\n        return create_instance(\n            objcls, self.settings, getattr(self, 'crawler', None),\n            *args, **kwargs)\n\n    def _get_exporter(self, *args, **kwargs):\n        return self._get_instance(self.exporters[self.format], *args, **kwargs)\n\n    def _get_storage(self, uri):\n        return self._get_instance(self.storages[urlparse(uri).scheme], uri)\n\n    def _get_uri_params(self, spider):\n        params = {}\n        for k in dir(spider):\n            params[k] = getattr(spider, k)\n        ts = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')\n        params['time'] = ts\n        self._uripar(params, spider)\n        return params\n"
    },
    {
      "filename": "scrapy/pipelines/files.py",
      "content": "\"\"\"\nFiles Pipeline\n\nSee documentation in topics/media-pipeline.rst\n\"\"\"\nimport functools\nimport hashlib\nimport logging\nimport mimetypes\nimport os\nimport time\nfrom collections import defaultdict\nfrom email.utils import parsedate_tz, mktime_tz\nfrom ftplib import FTP\nfrom io import BytesIO\nfrom urllib.parse import urlparse\n\nfrom twisted.internet import defer, threads\n\nfrom scrapy.pipelines.media import MediaPipeline\nfrom scrapy.settings import Settings\nfrom scrapy.exceptions import NotConfigured, IgnoreRequest\nfrom scrapy.http import Request\nfrom scrapy.utils.misc import md5sum\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.request import referer_str\nfrom scrapy.utils.boto import is_botocore\nfrom scrapy.utils.datatypes import CaselessDict\nfrom scrapy.utils.ftp import ftp_store_file\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass FileException(Exception):\n    \"\"\"General media error exception\"\"\"\n\n\nclass FSFilesStore(object):\n    def __init__(self, basedir):\n        if '://' in basedir:\n            basedir = basedir.split('://', 1)[1]\n        self.basedir = basedir\n        self._mkdir(self.basedir)\n        self.created_directories = defaultdict(set)\n\n    def persist_file(self, path, buf, info, meta=None, headers=None):\n        absolute_path = self._get_filesystem_path(path)\n        self._mkdir(os.path.dirname(absolute_path), info)\n        with open(absolute_path, 'wb') as f:\n            f.write(buf.getvalue())\n\n    def stat_file(self, path, info):\n        absolute_path = self._get_filesystem_path(path)\n        try:\n            last_modified = os.path.getmtime(absolute_path)\n        except os.error:\n            return {}\n\n        with open(absolute_path, 'rb') as f:\n            checksum = md5sum(f)\n\n        return {'last_modified': last_modified, 'checksum': checksum}\n\n    def _get_filesystem_path(self, path):\n        path_comps = path.split('/')\n        return os.path.join(self.basedir, *path_comps)\n\n    def _mkdir(self, dirname, domain=None):\n        seen = self.created_directories[domain] if domain else set()\n        if dirname not in seen:\n            if not os.path.exists(dirname):\n                os.makedirs(dirname)\n            seen.add(dirname)\n\n\nclass S3FilesStore(object):\n    AWS_ACCESS_KEY_ID = None\n    AWS_SECRET_ACCESS_KEY = None\n    AWS_ENDPOINT_URL = None\n    AWS_REGION_NAME = None\n    AWS_USE_SSL = None\n    AWS_VERIFY = None\n\n    POLICY = 'private'  # Overriden from settings.FILES_STORE_S3_ACL in\n                        # FilesPipeline.from_settings.\n    HEADERS = {\n        'Cache-Control': 'max-age=172800',\n    }\n\n    def __init__(self, uri):\n        self.is_botocore = is_botocore()\n        if self.is_botocore:\n            import botocore.session\n            session = botocore.session.get_session()\n            self.s3_client = session.create_client(\n                's3',\n                aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n                aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n                endpoint_url=self.AWS_ENDPOINT_URL,\n                region_name=self.AWS_REGION_NAME,\n                use_ssl=self.AWS_USE_SSL,\n                verify=self.AWS_VERIFY\n            )\n        else:\n            from boto.s3.connection import S3Connection\n            self.S3Connection = S3Connection\n        assert uri.startswith('s3://')\n        self.bucket, self.prefix = uri[5:].split('/', 1)\n\n    def stat_file(self, path, info):\n        def _onsuccess(boto_key):\n            if self.is_botocore:\n                checksum = boto_key['ETag'].strip('\"')\n                last_modified = boto_key['LastModified']\n                modified_stamp = time.mktime(last_modified.timetuple())\n            else:\n                checksum = boto_key.etag.strip('\"')\n                last_modified = boto_key.last_modified\n                modified_tuple = parsedate_tz(last_modified)\n                modified_stamp = int(mktime_tz(modified_tuple))\n            return {'checksum': checksum, 'last_modified': modified_stamp}\n\n        return self._get_boto_key(path).addCallback(_onsuccess)\n\n    def _get_boto_bucket(self):\n        # disable ssl (is_secure=False) because of this python bug:\n        # https://bugs.python.org/issue5103\n        c = self.S3Connection(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, is_secure=False)\n        return c.get_bucket(self.bucket, validate=False)\n\n    def _get_boto_key(self, path):\n        key_name = '%s%s' % (self.prefix, path)\n        if self.is_botocore:\n            return threads.deferToThread(\n                self.s3_client.head_object,\n                Bucket=self.bucket,\n                Key=key_name)\n        else:\n            b = self._get_boto_bucket()\n            return threads.deferToThread(b.get_key, key_name)\n\n    def persist_file(self, path, buf, info, meta=None, headers=None):\n        \"\"\"Upload file to S3 storage\"\"\"\n        key_name = '%s%s' % (self.prefix, path)\n        buf.seek(0)\n        if self.is_botocore:\n            extra = self._headers_to_botocore_kwargs(self.HEADERS)\n            if headers:\n                extra.update(self._headers_to_botocore_kwargs(headers))\n            return threads.deferToThread(\n                self.s3_client.put_object,\n                Bucket=self.bucket,\n                Key=key_name,\n                Body=buf,\n                Metadata={k: str(v) for k, v in (meta or {}).items()},\n                ACL=self.POLICY,\n                **extra)\n        else:\n            b = self._get_boto_bucket()\n            k = b.new_key(key_name)\n            if meta:\n                for metakey, metavalue in meta.items():\n                    k.set_metadata(metakey, str(metavalue))\n            h = self.HEADERS.copy()\n            if headers:\n                h.update(headers)\n            return threads.deferToThread(\n                k.set_contents_from_string, buf.getvalue(),\n                headers=h, policy=self.POLICY)\n\n    def _headers_to_botocore_kwargs(self, headers):\n        \"\"\" Convert headers to botocore keyword agruments.\n        \"\"\"\n        # This is required while we need to support both boto and botocore.\n        mapping = CaselessDict({\n            'Content-Type': 'ContentType',\n            'Cache-Control': 'CacheControl',\n            'Content-Disposition': 'ContentDisposition',\n            'Content-Encoding': 'ContentEncoding',\n            'Content-Language': 'ContentLanguage',\n            'Content-Length': 'ContentLength',\n            'Content-MD5': 'ContentMD5',\n            'Expires': 'Expires',\n            'X-Amz-Grant-Full-Control': 'GrantFullControl',\n            'X-Amz-Grant-Read': 'GrantRead',\n            'X-Amz-Grant-Read-ACP': 'GrantReadACP',\n            'X-Amz-Grant-Write-ACP': 'GrantWriteACP',\n            'X-Amz-Object-Lock-Legal-Hold': 'ObjectLockLegalHoldStatus',\n            'X-Amz-Object-Lock-Mode': 'ObjectLockMode',\n            'X-Amz-Object-Lock-Retain-Until-Date': 'ObjectLockRetainUntilDate',\n            'X-Amz-Request-Payer': 'RequestPayer',\n            'X-Amz-Server-Side-Encryption': 'ServerSideEncryption',\n            'X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id': 'SSEKMSKeyId',\n            'X-Amz-Server-Side-Encryption-Context': 'SSEKMSEncryptionContext',\n            'X-Amz-Server-Side-Encryption-Customer-Algorithm': 'SSECustomerAlgorithm',\n            'X-Amz-Server-Side-Encryption-Customer-Key': 'SSECustomerKey',\n            'X-Amz-Server-Side-Encryption-Customer-Key-Md5': 'SSECustomerKeyMD5',\n            'X-Amz-Storage-Class': 'StorageClass',\n            'X-Amz-Tagging': 'Tagging',\n            'X-Amz-Website-Redirect-Location': 'WebsiteRedirectLocation',\n        })\n        extra = {}\n        for key, value in headers.items():\n            try:\n                kwarg = mapping[key]\n            except KeyError:\n                raise TypeError(\n                    'Header \"%s\" is not supported by botocore' % key)\n            else:\n                extra[kwarg] = value\n        return extra\n\n\nclass GCSFilesStore(object):\n\n    GCS_PROJECT_ID = None\n\n    CACHE_CONTROL = 'max-age=172800'\n\n    # The bucket's default object ACL will be applied to the object.\n    # Overriden from settings.FILES_STORE_GCS_ACL in FilesPipeline.from_settings.\n    POLICY = None\n\n    def __init__(self, uri):\n        from google.cloud import storage\n        client = storage.Client(project=self.GCS_PROJECT_ID)\n        bucket, prefix = uri[5:].split('/', 1)\n        self.bucket = client.bucket(bucket)\n        self.prefix = prefix\n\n    def stat_file(self, path, info):\n        def _onsuccess(blob):\n            if blob:\n                checksum = blob.md5_hash\n                last_modified = time.mktime(blob.updated.timetuple())\n                return {'checksum': checksum, 'last_modified': last_modified}\n            else:\n                return {}\n\n        return threads.deferToThread(self.bucket.get_blob, path).addCallback(_onsuccess)\n\n    def _get_content_type(self, headers):\n        if headers and 'Content-Type' in headers:\n            return headers['Content-Type']\n        else:\n            return 'application/octet-stream'\n\n    def persist_file(self, path, buf, info, meta=None, headers=None):\n        blob = self.bucket.blob(self.prefix + path)\n        blob.cache_control = self.CACHE_CONTROL\n        blob.metadata = {k: str(v) for k, v in (meta or {}).items()}\n        return threads.deferToThread(\n            blob.upload_from_string,\n            data=buf.getvalue(),\n            content_type=self._get_content_type(headers),\n            predefined_acl=self.POLICY\n        )\n\n\nclass FTPFilesStore(object):\n\n    FTP_USERNAME = None\n    FTP_PASSWORD = None\n    USE_ACTIVE_MODE = None\n\n    def __init__(self, uri):\n        assert uri.startswith('ftp://')\n        u = urlparse(uri)\n        self.port = u.port\n        self.host = u.hostname\n        self.port = int(u.port or 21)\n        self.username = u.username or self.FTP_USERNAME\n        self.password = u.password or self.FTP_PASSWORD\n        self.basedir = u.path.rstrip('/')\n\n    def persist_file(self, path, buf, info, meta=None, headers=None):\n        path = '%s/%s' % (self.basedir, path)\n        return threads.deferToThread(\n            ftp_store_file, path=path, file=buf,\n            host=self.host, port=self.port, username=self.username,\n            password=self.password, use_active_mode=self.USE_ACTIVE_MODE\n        )\n\n    def stat_file(self, path, info):\n        def _stat_file(path):\n            try:\n                ftp = FTP()\n                ftp.connect(self.host, self.port)\n                ftp.login(self.username, self.password)\n                if self.USE_ACTIVE_MODE:\n                    ftp.set_pasv(False)\n                file_path = \"%s/%s\" % (self.basedir, path)\n                last_modified = float(ftp.voidcmd(\"MDTM %s\" % file_path)[4:].strip())\n                m = hashlib.md5()\n                ftp.retrbinary('RETR %s' % file_path, m.update)\n                return {'last_modified': last_modified, 'checksum': m.hexdigest()}\n            # The file doesn't exist\n            except Exception:\n                return {}\n        return threads.deferToThread(_stat_file, path)\n\n\nclass FilesPipeline(MediaPipeline):\n    \"\"\"Abstract pipeline that implement the file downloading\n\n    This pipeline tries to minimize network transfers and file processing,\n    doing stat of the files and determining if file is new, uptodate or\n    expired.\n\n    ``new`` files are those that pipeline never processed and needs to be\n        downloaded from supplier site the first time.\n\n    ``uptodate`` files are the ones that the pipeline processed and are still\n        valid files.\n\n    ``expired`` files are those that pipeline already processed but the last\n        modification was made long time ago, so a reprocessing is recommended to\n        refresh it in case of change.\n\n    \"\"\"\n\n    MEDIA_NAME = \"file\"\n    EXPIRES = 90\n    STORE_SCHEMES = {\n        '': FSFilesStore,\n        'file': FSFilesStore,\n        's3': S3FilesStore,\n        'gs': GCSFilesStore,\n        'ftp': FTPFilesStore\n    }\n    DEFAULT_FILES_URLS_FIELD = 'file_urls'\n    DEFAULT_FILES_RESULT_FIELD = 'files'\n\n    def __init__(self, store_uri, download_func=None, settings=None):\n        if not store_uri:\n            raise NotConfigured\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        cls_name = \"FilesPipeline\"\n        self.store = self._get_store(store_uri)\n        resolve = functools.partial(self._key_for_pipe,\n                                    base_class_name=cls_name,\n                                    settings=settings)\n        self.expires = settings.getint(\n            resolve('FILES_EXPIRES'), self.EXPIRES\n        )\n        if not hasattr(self, \"FILES_URLS_FIELD\"):\n            self.FILES_URLS_FIELD = self.DEFAULT_FILES_URLS_FIELD\n        if not hasattr(self, \"FILES_RESULT_FIELD\"):\n            self.FILES_RESULT_FIELD = self.DEFAULT_FILES_RESULT_FIELD\n        self.files_urls_field = settings.get(\n            resolve('FILES_URLS_FIELD'), self.FILES_URLS_FIELD\n        )\n        self.files_result_field = settings.get(\n            resolve('FILES_RESULT_FIELD'), self.FILES_RESULT_FIELD\n        )\n\n        super(FilesPipeline, self).__init__(download_func=download_func, settings=settings)\n\n    @classmethod\n    def from_settings(cls, settings):\n        s3store = cls.STORE_SCHEMES['s3']\n        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n        s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n        s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n        s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n        s3store.AWS_VERIFY = settings['AWS_VERIFY']\n        s3store.POLICY = settings['FILES_STORE_S3_ACL']\n\n        gcs_store = cls.STORE_SCHEMES['gs']\n        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n        gcs_store.POLICY = settings['FILES_STORE_GCS_ACL'] or None\n\n        ftp_store = cls.STORE_SCHEMES['ftp']\n        ftp_store.FTP_USERNAME = settings['FTP_USER']\n        ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']\n        ftp_store.USE_ACTIVE_MODE = settings.getbool('FEED_STORAGE_FTP_ACTIVE')\n\n        store_uri = settings['FILES_STORE']\n        return cls(store_uri, settings=settings)\n\n    def _get_store(self, uri):\n        if os.path.isabs(uri):  # to support win32 paths like: C:\\\\some\\dir\n            scheme = 'file'\n        else:\n            scheme = urlparse(uri).scheme\n        store_cls = self.STORE_SCHEMES[scheme]\n        return store_cls(uri)\n\n    def media_to_download(self, request, info):\n        def _onsuccess(result):\n            if not result:\n                return  # returning None force download\n\n            last_modified = result.get('last_modified', None)\n            if not last_modified:\n                return  # returning None force download\n\n            age_seconds = time.time() - last_modified\n            age_days = age_seconds / 60 / 60 / 24\n            if age_days > self.expires:\n                return  # returning None force download\n\n            referer = referer_str(request)\n            logger.debug(\n                'File (uptodate): Downloaded %(medianame)s from %(request)s '\n                'referred in <%(referer)s>',\n                {'medianame': self.MEDIA_NAME, 'request': request,\n                 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            self.inc_stats(info.spider, 'uptodate')\n\n            checksum = result.get('checksum', None)\n            return {'url': request.url, 'path': path, 'checksum': checksum}\n\n        path = self.file_path(request, info=info)\n        dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n        dfd.addCallbacks(_onsuccess, lambda _: None)\n        dfd.addErrback(\n            lambda f:\n            logger.error(self.__class__.__name__ + '.store.stat_file',\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\n        )\n        return dfd\n\n    def media_failed(self, failure, request, info):\n        if not isinstance(failure.value, IgnoreRequest):\n            referer = referer_str(request)\n            logger.warning(\n                'File (unknown-error): Error downloading %(medianame)s from '\n                '%(request)s referred in <%(referer)s>: %(exception)s',\n                {'medianame': self.MEDIA_NAME, 'request': request,\n                 'referer': referer, 'exception': failure.value},\n                extra={'spider': info.spider}\n            )\n\n        raise FileException\n\n    def media_downloaded(self, response, request, info):\n        referer = referer_str(request)\n\n        if response.status != 200:\n            logger.warning(\n                'File (code: %(status)s): Error downloading file from '\n                '%(request)s referred in <%(referer)s>',\n                {'status': response.status,\n                 'request': request, 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            raise FileException('download-error')\n\n        if not response.body:\n            logger.warning(\n                'File (empty-content): Empty file from %(request)s referred '\n                'in <%(referer)s>: no-content',\n                {'request': request, 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            raise FileException('empty-content')\n\n        status = 'cached' if 'cached' in response.flags else 'downloaded'\n        logger.debug(\n            'File (%(status)s): Downloaded file from %(request)s referred in '\n            '<%(referer)s>',\n            {'status': status, 'request': request, 'referer': referer},\n            extra={'spider': info.spider}\n        )\n        self.inc_stats(info.spider, status)\n\n        try:\n            path = self.file_path(request, response=response, info=info)\n            checksum = self.file_downloaded(response, request, info)\n        except FileException as exc:\n            logger.warning(\n                'File (error): Error processing file from %(request)s '\n                'referred in <%(referer)s>: %(errormsg)s',\n                {'request': request, 'referer': referer, 'errormsg': str(exc)},\n                extra={'spider': info.spider}, exc_info=True\n            )\n            raise\n        except Exception as exc:\n            logger.error(\n                'File (unknown-error): Error processing file from %(request)s '\n                'referred in <%(referer)s>',\n                {'request': request, 'referer': referer},\n                exc_info=True, extra={'spider': info.spider}\n            )\n            raise FileException(str(exc))\n\n        return {'url': request.url, 'path': path, 'checksum': checksum}\n\n    def inc_stats(self, spider, status):\n        spider.crawler.stats.inc_value('file_count', spider=spider)\n        spider.crawler.stats.inc_value('file_status_count/%s' % status, spider=spider)\n\n    ### Overridable Interface\n    def get_media_requests(self, item, info):\n        return [Request(x) for x in item.get(self.files_urls_field, [])]\n\n    def file_downloaded(self, response, request, info):\n        path = self.file_path(request, response=response, info=info)\n        buf = BytesIO(response.body)\n        checksum = md5sum(buf)\n        buf.seek(0)\n        self.store.persist_file(path, buf, info)\n        return checksum\n\n    def item_completed(self, results, item, info):\n        if isinstance(item, dict) or self.files_result_field in item.fields:\n            item[self.files_result_field] = [x for ok, x in results if ok]\n        return item\n\n    def file_path(self, request, response=None, info=None):\n        media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n        media_ext = os.path.splitext(request.url)[1]\n        # Handles empty and wild extensions by trying to guess the\n        # mime type then extension or default to empty string otherwise\n        if media_ext not in mimetypes.types_map:\n            media_ext = ''\n            media_type = mimetypes.guess_type(request.url)[0]\n            if media_type:\n                media_ext = mimetypes.guess_extension(media_type)\n        return 'full/%s%s' % (media_guid, media_ext)\n"
    },
    {
      "filename": "scrapy/pipelines/images.py",
      "content": "\"\"\"\nImages Pipeline\n\nSee documentation in topics/media-pipeline.rst\n\"\"\"\nimport functools\nimport hashlib\nfrom io import BytesIO\n\nfrom PIL import Image\n\nfrom scrapy.utils.misc import md5sum\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.http import Request\nfrom scrapy.settings import Settings\nfrom scrapy.exceptions import DropItem\n#TODO: from scrapy.pipelines.media import MediaPipeline\nfrom scrapy.pipelines.files import FileException, FilesPipeline\n\n\nclass NoimagesDrop(DropItem):\n    \"\"\"Product with no images exception\"\"\"\n\n\nclass ImageException(FileException):\n    \"\"\"General image error exception\"\"\"\n\n\nclass ImagesPipeline(FilesPipeline):\n    \"\"\"Abstract pipeline that implement the image thumbnail generation logic\n\n    \"\"\"\n\n    MEDIA_NAME = 'image'\n\n    # Uppercase attributes kept for backward compatibility with code that subclasses\n    # ImagesPipeline. They may be overridden by settings.\n    MIN_WIDTH = 0\n    MIN_HEIGHT = 0\n    EXPIRES = 90\n    THUMBS = {}\n    DEFAULT_IMAGES_URLS_FIELD = 'image_urls'\n    DEFAULT_IMAGES_RESULT_FIELD = 'images'\n\n    def __init__(self, store_uri, download_func=None, settings=None):\n        super(ImagesPipeline, self).__init__(store_uri, settings=settings,\n                                             download_func=download_func)\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        resolve = functools.partial(self._key_for_pipe,\n                                    base_class_name=\"ImagesPipeline\",\n                                    settings=settings)\n        self.expires = settings.getint(\n            resolve(\"IMAGES_EXPIRES\"), self.EXPIRES\n        )\n\n        if not hasattr(self, \"IMAGES_RESULT_FIELD\"):\n            self.IMAGES_RESULT_FIELD = self.DEFAULT_IMAGES_RESULT_FIELD\n        if not hasattr(self, \"IMAGES_URLS_FIELD\"):\n            self.IMAGES_URLS_FIELD = self.DEFAULT_IMAGES_URLS_FIELD\n\n        self.images_urls_field = settings.get(\n            resolve('IMAGES_URLS_FIELD'),\n            self.IMAGES_URLS_FIELD\n        )\n        self.images_result_field = settings.get(\n            resolve('IMAGES_RESULT_FIELD'),\n            self.IMAGES_RESULT_FIELD\n        )\n        self.min_width = settings.getint(\n            resolve('IMAGES_MIN_WIDTH'), self.MIN_WIDTH\n        )\n        self.min_height = settings.getint(\n            resolve('IMAGES_MIN_HEIGHT'), self.MIN_HEIGHT\n        )\n        self.thumbs = settings.get(\n            resolve('IMAGES_THUMBS'), self.THUMBS\n        )\n\n    @classmethod\n    def from_settings(cls, settings):\n        s3store = cls.STORE_SCHEMES['s3']\n        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n        s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n        s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n        s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n        s3store.AWS_VERIFY = settings['AWS_VERIFY']\n        s3store.POLICY = settings['IMAGES_STORE_S3_ACL']\n\n        gcs_store = cls.STORE_SCHEMES['gs']\n        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n        gcs_store.POLICY = settings['IMAGES_STORE_GCS_ACL'] or None\n\n        ftp_store = cls.STORE_SCHEMES['ftp']\n        ftp_store.FTP_USERNAME = settings['FTP_USER']\n        ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']\n        ftp_store.USE_ACTIVE_MODE = settings.getbool('FEED_STORAGE_FTP_ACTIVE')\n\n        store_uri = settings['IMAGES_STORE']\n        return cls(store_uri, settings=settings)\n\n    def file_downloaded(self, response, request, info):\n        return self.image_downloaded(response, request, info)\n\n    def image_downloaded(self, response, request, info):\n        checksum = None\n        for path, image, buf in self.get_images(response, request, info):\n            if checksum is None:\n                buf.seek(0)\n                checksum = md5sum(buf)\n            width, height = image.size\n            self.store.persist_file(\n                path, buf, info,\n                meta={'width': width, 'height': height},\n                headers={'Content-Type': 'image/jpeg'})\n        return checksum\n\n    def get_images(self, response, request, info):\n        path = self.file_path(request, response=response, info=info)\n        orig_image = Image.open(BytesIO(response.body))\n\n        width, height = orig_image.size\n        if width < self.min_width or height < self.min_height:\n            raise ImageException(\"Image too small (%dx%d < %dx%d)\" %\n                                 (width, height, self.min_width, self.min_height))\n\n        image, buf = self.convert_image(orig_image)\n        yield path, image, buf\n\n        for thumb_id, size in self.thumbs.items():\n            thumb_path = self.thumb_path(request, thumb_id, response=response, info=info)\n            thumb_image, thumb_buf = self.convert_image(image, size)\n            yield thumb_path, thumb_image, thumb_buf\n\n    def convert_image(self, image, size=None):\n        if image.format == 'PNG' and image.mode == 'RGBA':\n            background = Image.new('RGBA', image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert('RGB')\n        elif image.mode == 'P':\n            image = image.convert(\"RGBA\")\n            background = Image.new('RGBA', image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert('RGB')\n        elif image.mode != 'RGB':\n            image = image.convert('RGB')\n\n        if size:\n            image = image.copy()\n            image.thumbnail(size, Image.ANTIALIAS)\n\n        buf = BytesIO()\n        image.save(buf, 'JPEG')\n        return image, buf\n\n    def get_media_requests(self, item, info):\n        return [Request(x) for x in item.get(self.images_urls_field, [])]\n\n    def item_completed(self, results, item, info):\n        if isinstance(item, dict) or self.images_result_field in item.fields:\n            item[self.images_result_field] = [x for ok, x in results if ok]\n        return item\n\n    def file_path(self, request, response=None, info=None):\n        image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n        return 'full/%s.jpg' % (image_guid)\n\n    def thumb_path(self, request, thumb_id, response=None, info=None):\n        thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n        return 'thumbs/%s/%s.jpg' % (thumb_id, thumb_guid)\n"
    },
    {
      "filename": "scrapy/utils/ftp.py",
      "content": "import posixpath\n\nfrom ftplib import error_perm, FTP\nfrom posixpath import dirname\n\n\ndef ftp_makedirs_cwd(ftp, path, first_call=True):\n    \"\"\"Set the current directory of the FTP connection given in the ``ftp``\n    argument (as a ftplib.FTP object), creating all parent directories if they\n    don't exist. The ftplib.FTP object must be already connected and logged in.\n    \"\"\"\n    try:\n        ftp.cwd(path)\n    except error_perm:\n        ftp_makedirs_cwd(ftp, dirname(path), False)\n        ftp.mkd(path)\n        if first_call:\n            ftp.cwd(path)\n\n\ndef ftp_store_file(\n        *, path, file, host, port,\n        username, password, use_active_mode=False):\n    \"\"\"Opens a FTP connection with passed credentials,sets current directory\n    to the directory extracted from given path, then uploads the file to server\n    \"\"\"\n    with FTP() as ftp:\n        ftp.connect(host, port)\n        ftp.login(username, password)\n        if use_active_mode:\n            ftp.set_pasv(False)\n        file.seek(0)\n        dirname, filename = posixpath.split(path)\n        ftp_makedirs_cwd(ftp, dirname)\n        ftp.storbinary('STOR %s' % filename, file)\n"
    },
    {
      "filename": "scrapy/utils/test.py",
      "content": "\"\"\"\nThis module contains some assorted functions used in tests\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom posixpath import split\nimport asyncio\nimport os\n\nfrom importlib import import_module\nfrom twisted.trial.unittest import SkipTest\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.boto import is_botocore\n\n\ndef assert_aws_environ():\n    \"\"\"Asserts the current environment is suitable for running AWS testsi.\n    Raises SkipTest with the reason if it's not.\n    \"\"\"\n    skip_if_no_boto()\n    if 'AWS_ACCESS_KEY_ID' not in os.environ:\n        raise SkipTest(\"AWS keys not found\")\n\n\ndef assert_gcs_environ():\n    if 'GCS_PROJECT_ID' not in os.environ:\n        raise SkipTest(\"GCS_PROJECT_ID not found\")\n\n\ndef skip_if_no_boto():\n    try:\n        is_botocore()\n    except NotConfigured as e:\n        raise SkipTest(e)\n\n\ndef get_s3_content_and_delete(bucket, path, with_key=False):\n    \"\"\" Get content from s3 key, and delete key afterwards.\n    \"\"\"\n    if is_botocore():\n        import botocore.session\n        session = botocore.session.get_session()\n        client = session.create_client('s3')\n        key = client.get_object(Bucket=bucket, Key=path)\n        content = key['Body'].read()\n        client.delete_object(Bucket=bucket, Key=path)\n    else:\n        import boto\n        # assuming boto=2.2.2\n        bucket = boto.connect_s3().get_bucket(bucket, validate=False)\n        key = bucket.get_key(path)\n        content = key.get_contents_as_string()\n        bucket.delete_key(path)\n    return (content, key) if with_key else content\n\n\ndef get_gcs_content_and_delete(bucket, path):\n    from google.cloud import storage\n    client = storage.Client(project=os.environ.get('GCS_PROJECT_ID'))\n    bucket = client.get_bucket(bucket)\n    blob = bucket.get_blob(path)\n    content = blob.download_as_string()\n    acl = list(blob.acl)  # loads acl before it will be deleted\n    bucket.delete_blob(path)\n    return content, acl, blob\n\n\ndef get_ftp_content_and_delete(\n        path, host, port, username,\n        password, use_active_mode=False):\n    from ftplib import FTP\n    ftp = FTP()\n    ftp.connect(host, port)\n    ftp.login(username, password)\n    if use_active_mode:\n        ftp.set_pasv(False)\n    ftp_data = []\n\n    def buffer_data(data):\n        ftp_data.append(data)\n    ftp.retrbinary('RETR %s' % path, buffer_data)\n    dirname, filename = split(path)\n    ftp.cwd(dirname)\n    ftp.delete(filename)\n    return \"\".join(ftp_data)\n\n\ndef get_crawler(spidercls=None, settings_dict=None):\n    \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n    will be used to populate the crawler settings with a project level\n    priority.\n    \"\"\"\n    from scrapy.crawler import CrawlerRunner\n    from scrapy.spiders import Spider\n\n    runner = CrawlerRunner(settings_dict)\n    return runner.create_crawler(spidercls or Spider)\n\n\ndef get_pythonpath():\n    \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n    installation of Scrapy\"\"\"\n    scrapy_path = import_module('scrapy').__path__[0]\n    return os.path.dirname(scrapy_path) + os.pathsep + os.environ.get('PYTHONPATH', '')\n\n\ndef get_testenv():\n    \"\"\"Return a OS environment dict suitable to fork processes that need to import\n    this installation of Scrapy, instead of a system installed one.\n    \"\"\"\n    env = os.environ.copy()\n    env['PYTHONPATH'] = get_pythonpath()\n    return env\n\n\ndef assert_samelines(testcase, text1, text2, msg=None):\n    \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n    line endings between platforms\n    \"\"\"\n    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)\n\n\ndef get_from_asyncio_queue(value):\n    q = asyncio.Queue()\n    getter = q.get()\n    q.put_nowait(value)\n    return getter\n"
    },
    {
      "filename": "tests/test_pipeline_files.py",
      "content": "import os\nimport random\nimport time\nfrom io import BytesIO\nfrom tempfile import mkdtemp\nfrom shutil import rmtree\nfrom unittest import mock\nfrom urllib.parse import urlparse\n\nfrom twisted.trial import unittest\nfrom twisted.internet import defer\n\nfrom scrapy.pipelines.files import FilesPipeline, FSFilesStore, S3FilesStore, GCSFilesStore, FTPFilesStore\nfrom scrapy.item import Item, Field\nfrom scrapy.http import Request, Response\nfrom scrapy.settings import Settings\nfrom scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete\nfrom scrapy.utils.test import assert_gcs_environ, get_gcs_content_and_delete\nfrom scrapy.utils.test import get_ftp_content_and_delete\nfrom scrapy.utils.boto import is_botocore\n\n\ndef _mocked_download_func(request, info):\n    response = request.meta.get('response')\n    return response() if callable(response) else response\n\n\nclass FilesPipelineTestCase(unittest.TestCase):\n\n    def setUp(self):\n        self.tempdir = mkdtemp()\n        self.pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': self.tempdir}))\n        self.pipeline.download_func = _mocked_download_func\n        self.pipeline.open_spider(None)\n\n    def tearDown(self):\n        rmtree(self.tempdir)\n\n    def test_file_path(self):\n        file_path = self.pipeline.file_path\n        self.assertEqual(file_path(Request(\"https://dev.mydeco.com/mydeco.pdf\")),\n                         'full/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf')\n        self.assertEqual(file_path(Request(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt\")),\n                         'full/4ce274dd83db0368bafd7e406f382ae088e39219.txt')\n        self.assertEqual(file_path(Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc\")),\n                         'full/94ccc495a17b9ac5d40e3eabf3afcb8c2c9b9e1a.doc')\n        self.assertEqual(file_path(Request(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\")),\n                         'full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg')\n        self.assertEqual(file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532/\")),\n                         'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2')\n        self.assertEqual(file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\")),\n                         'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1')\n        self.assertEqual(file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                                   response=Response(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                                   info=object()),\n                         'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1')\n        self.assertEqual(file_path(Request(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg.bohaha\")),\n                         'full/76c00cef2ef669ae65052661f68d451162829507')\n        self.assertEqual(file_path(Request(\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAR0AAACxCAMAAADOHZloAAACClBMVEX/\\\n                                    //+F0tzCwMK76ZKQ21AMqr7oAAC96JvD5aWM2kvZ78J0N7fmAAC46Y4Ap7y\")),\n                         'full/178059cbeba2e34120a67f2dc1afc3ecc09b61cb.png')\n\n    def test_fs_store(self):\n        assert isinstance(self.pipeline.store, FSFilesStore)\n        self.assertEqual(self.pipeline.store.basedir, self.tempdir)\n\n        path = 'some/image/key.jpg'\n        fullpath = os.path.join(self.tempdir, 'some', 'image', 'key.jpg')\n        self.assertEqual(self.pipeline.store._get_filesystem_path(path), fullpath)\n\n    @defer.inlineCallbacks\n    def test_file_not_expired(self):\n        item_url = \"http://example.com/file.pdf\"\n        item = _create_item_with_files(item_url)\n        patchers = [\n            mock.patch.object(FilesPipeline, 'inc_stats', return_value=True),\n            mock.patch.object(FSFilesStore, 'stat_file', return_value={\n                'checksum': 'abc', 'last_modified': time.time()}),\n            mock.patch.object(FilesPipeline, 'get_media_requests',\n                              return_value=[_prepare_request_object(item_url)])\n        ]\n        for p in patchers:\n            p.start()\n\n        result = yield self.pipeline.process_item(item, None)\n        self.assertEqual(result['files'][0]['checksum'], 'abc')\n\n        for p in patchers:\n            p.stop()\n\n    @defer.inlineCallbacks\n    def test_file_expired(self):\n        item_url = \"http://example.com/file2.pdf\"\n        item = _create_item_with_files(item_url)\n        patchers = [\n            mock.patch.object(FSFilesStore, 'stat_file', return_value={\n                'checksum': 'abc',\n                'last_modified': time.time() - (self.pipeline.expires * 60 * 60 * 24 * 2)}),\n            mock.patch.object(FilesPipeline, 'get_media_requests',\n                              return_value=[_prepare_request_object(item_url)]),\n            mock.patch.object(FilesPipeline, 'inc_stats', return_value=True)\n        ]\n        for p in patchers:\n            p.start()\n\n        result = yield self.pipeline.process_item(item, None)\n        self.assertNotEqual(result['files'][0]['checksum'], 'abc')\n\n        for p in patchers:\n            p.stop()\n\n\nclass FilesPipelineTestCaseFields(unittest.TestCase):\n\n    def test_item_fields_default(self):\n        class TestItem(Item):\n            name = Field()\n            file_urls = Field()\n            files = Field()\n\n        for cls in TestItem, dict:\n            url = 'http://www.example.com/files/1.txt'\n            item = cls({'name': 'item1', 'file_urls': [url]})\n            pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': 's3://example/files/'}))\n            requests = list(pipeline.get_media_requests(item, None))\n            self.assertEqual(requests[0].url, url)\n            results = [(True, {'url': url})]\n            pipeline.item_completed(results, item, None)\n            self.assertEqual(item['files'], [results[0][1]])\n\n    def test_item_fields_override_settings(self):\n        class TestItem(Item):\n            name = Field()\n            files = Field()\n            stored_file = Field()\n\n        for cls in TestItem, dict:\n            url = 'http://www.example.com/files/1.txt'\n            item = cls({'name': 'item1', 'files': [url]})\n            pipeline = FilesPipeline.from_settings(Settings({\n                'FILES_STORE': 's3://example/files/',\n                'FILES_URLS_FIELD': 'files',\n                'FILES_RESULT_FIELD': 'stored_file'\n            }))\n            requests = list(pipeline.get_media_requests(item, None))\n            self.assertEqual(requests[0].url, url)\n            results = [(True, {'url': url})]\n            pipeline.item_completed(results, item, None)\n            self.assertEqual(item['stored_file'], [results[0][1]])\n\n\nclass FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n    default_cls_settings = {\n        \"EXPIRES\": 90,\n        \"FILES_URLS_FIELD\": \"file_urls\",\n        \"FILES_RESULT_FIELD\": \"files\"\n    }\n    file_cls_attr_settings_map = {\n        (\"EXPIRES\", \"FILES_EXPIRES\", \"expires\"),\n        (\"FILES_URLS_FIELD\", \"FILES_URLS_FIELD\", \"files_urls_field\"),\n        (\"FILES_RESULT_FIELD\", \"FILES_RESULT_FIELD\", \"files_result_field\")\n    }\n\n    def setUp(self):\n        self.tempdir = mkdtemp()\n\n    def tearDown(self):\n        rmtree(self.tempdir)\n\n    def _generate_fake_settings(self, prefix=None):\n\n        def random_string():\n            return \"\".join([chr(random.randint(97, 123)) for _ in range(10)])\n\n        settings = {\n            \"FILES_EXPIRES\": random.randint(100, 1000),\n            \"FILES_URLS_FIELD\": random_string(),\n            \"FILES_RESULT_FIELD\": random_string(),\n            \"FILES_STORE\": self.tempdir\n        }\n        if not prefix:\n            return settings\n\n        return {prefix.upper() + \"_\" + k if k != \"FILES_STORE\" else k: v for k, v in settings.items()}\n\n    def _generate_fake_pipeline(self):\n\n        class UserDefinedFilePipeline(FilesPipeline):\n            EXPIRES = 1001\n            FILES_URLS_FIELD = \"alfa\"\n            FILES_RESULT_FIELD = \"beta\"\n\n        return UserDefinedFilePipeline\n\n    def test_different_settings_for_different_instances(self):\n        \"\"\"\n        If there are different instances with different settings they should keep\n        different settings.\n        \"\"\"\n        custom_settings = self._generate_fake_settings()\n        another_pipeline = FilesPipeline.from_settings(Settings(custom_settings))\n        one_pipeline = FilesPipeline(self.tempdir)\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            default_value = self.default_cls_settings[pipe_attr]\n            self.assertEqual(getattr(one_pipeline, pipe_attr), default_value)\n            custom_value = custom_settings[settings_attr]\n            self.assertNotEqual(default_value, custom_value)\n            self.assertEqual(getattr(another_pipeline, pipe_ins_attr), custom_value)\n\n    def test_subclass_attributes_preserved_if_no_settings(self):\n        \"\"\"\n        If subclasses override class attributes and there are no special settings those values should be kept.\n        \"\"\"\n        pipe_cls = self._generate_fake_pipeline()\n        pipe = pipe_cls.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            custom_value = getattr(pipe, pipe_ins_attr)\n            self.assertNotEqual(custom_value, self.default_cls_settings[pipe_attr])\n            self.assertEqual(getattr(pipe, pipe_ins_attr), getattr(pipe, pipe_attr))\n\n    def test_subclass_attrs_preserved_custom_settings(self):\n        \"\"\"\n        If file settings are defined but they are not defined for subclass\n        settings should be preserved.\n        \"\"\"\n        pipeline_cls = self._generate_fake_pipeline()\n        settings = self._generate_fake_settings()\n        pipeline = pipeline_cls.from_settings(Settings(settings))\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            value = getattr(pipeline, pipe_ins_attr)\n            setting_value = settings.get(settings_attr)\n            self.assertNotEqual(value, self.default_cls_settings[pipe_attr])\n            self.assertEqual(value, setting_value)\n\n    def test_no_custom_settings_for_subclasses(self):\n        \"\"\"\n        If there are no settings for subclass and no subclass attributes, pipeline should use\n        attributes of base class.\n        \"\"\"\n        class UserDefinedFilesPipeline(FilesPipeline):\n            pass\n\n        user_pipeline = UserDefinedFilesPipeline.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            # Values from settings for custom pipeline should be set on pipeline instance.\n            custom_value = self.default_cls_settings.get(pipe_attr.upper())\n            self.assertEqual(getattr(user_pipeline, pipe_ins_attr), custom_value)\n\n    def test_custom_settings_for_subclasses(self):\n        \"\"\"\n        If there are custom settings for subclass and NO class attributes, pipeline should use custom\n        settings.\n        \"\"\"\n        class UserDefinedFilesPipeline(FilesPipeline):\n            pass\n\n        prefix = UserDefinedFilesPipeline.__name__.upper()\n        settings = self._generate_fake_settings(prefix=prefix)\n        user_pipeline = UserDefinedFilesPipeline.from_settings(Settings(settings))\n        for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n            # Values from settings for custom pipeline should be set on pipeline instance.\n            custom_value = settings.get(prefix + \"_\" + settings_attr)\n            self.assertNotEqual(custom_value, self.default_cls_settings[pipe_attr])\n            self.assertEqual(getattr(user_pipeline, pipe_inst_attr), custom_value)\n\n    def test_custom_settings_and_class_attrs_for_subclasses(self):\n        \"\"\"\n        If there are custom settings for subclass AND class attributes\n        setting keys are preferred and override attributes.\n        \"\"\"\n        pipeline_cls = self._generate_fake_pipeline()\n        prefix = pipeline_cls.__name__.upper()\n        settings = self._generate_fake_settings(prefix=prefix)\n        user_pipeline = pipeline_cls.from_settings(Settings(settings))\n        for pipe_cls_attr, settings_attr, pipe_inst_attr  in self.file_cls_attr_settings_map:\n            custom_value = settings.get(prefix + \"_\" + settings_attr)\n            self.assertNotEqual(custom_value, self.default_cls_settings[pipe_cls_attr])\n            self.assertEqual(getattr(user_pipeline, pipe_inst_attr), custom_value)\n\n    def test_cls_attrs_with_DEFAULT_prefix(self):\n        class UserDefinedFilesPipeline(FilesPipeline):\n            DEFAULT_FILES_RESULT_FIELD = \"this\"\n            DEFAULT_FILES_URLS_FIELD = \"that\"\n\n        pipeline = UserDefinedFilesPipeline.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n        self.assertEqual(pipeline.files_result_field, \"this\")\n        self.assertEqual(pipeline.files_urls_field, \"that\")\n\n\n    def test_user_defined_subclass_default_key_names(self):\n        \"\"\"Test situation when user defines subclass of FilesPipeline,\n        but uses attribute names for default pipeline (without prefixing\n        them with pipeline class name).\n        \"\"\"\n        settings = self._generate_fake_settings()\n\n        class UserPipe(FilesPipeline):\n            pass\n\n        pipeline_cls = UserPipe.from_settings(Settings(settings))\n\n        for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n            expected_value = settings.get(settings_attr)\n            self.assertEqual(getattr(pipeline_cls, pipe_inst_attr),\n                             expected_value)\n\n\nclass TestS3FilesStore(unittest.TestCase):\n    @defer.inlineCallbacks\n    def test_persist(self):\n        assert_aws_environ()\n        uri = os.environ.get('S3_TEST_FILE_URI')\n        if not uri:\n            raise unittest.SkipTest(\"No S3 URI available for testing\")\n        data = b\"TestS3FilesStore: \\xe2\\x98\\x83\"\n        buf = BytesIO(data)\n        meta = {'foo': 'bar'}\n        path = ''\n        store = S3FilesStore(uri)\n        yield store.persist_file(\n            path, buf, info=None, meta=meta,\n            headers={'Content-Type': 'image/png'})\n        s = yield store.stat_file(path, info=None)\n        self.assertIn('last_modified', s)\n        self.assertIn('checksum', s)\n        self.assertEqual(s['checksum'], '3187896a9657a28163abb31667df64c8')\n        u = urlparse(uri)\n        content, key = get_s3_content_and_delete(\n            u.hostname, u.path[1:], with_key=True)\n        self.assertEqual(content, data)\n        if is_botocore():\n            self.assertEqual(key['Metadata'], {'foo': 'bar'})\n            self.assertEqual(\n                key['CacheControl'], S3FilesStore.HEADERS['Cache-Control'])\n            self.assertEqual(key['ContentType'], 'image/png')\n        else:\n            self.assertEqual(key.metadata, {'foo': 'bar'})\n            self.assertEqual(\n                key.cache_control, S3FilesStore.HEADERS['Cache-Control'])\n            self.assertEqual(key.content_type, 'image/png')\n\n\nclass TestGCSFilesStore(unittest.TestCase):\n    @defer.inlineCallbacks\n    def test_persist(self):\n        assert_gcs_environ()\n        uri = os.environ.get('GCS_TEST_FILE_URI')\n        if not uri:\n            raise unittest.SkipTest(\"No GCS URI available for testing\")\n        data = b\"TestGCSFilesStore: \\xe2\\x98\\x83\"\n        buf = BytesIO(data)\n        meta = {'foo': 'bar'}\n        path = 'full/filename'\n        store = GCSFilesStore(uri)\n        store.POLICY = 'authenticatedRead'\n        expected_policy = {'role': 'READER', 'entity': 'allAuthenticatedUsers'}\n        yield store.persist_file(path, buf, info=None, meta=meta, headers=None)\n        s = yield store.stat_file(path, info=None)\n        self.assertIn('last_modified', s)\n        self.assertIn('checksum', s)\n        self.assertEqual(s['checksum'], 'zc2oVgXkbQr2EQdSdw3OPA==')\n        u = urlparse(uri)\n        content, acl, blob = get_gcs_content_and_delete(u.hostname, u.path[1:]+path)\n        self.assertEqual(content, data)\n        self.assertEqual(blob.metadata, {'foo': 'bar'})\n        self.assertEqual(blob.cache_control, GCSFilesStore.CACHE_CONTROL)\n        self.assertEqual(blob.content_type, 'application/octet-stream')\n        self.assertIn(expected_policy, acl)\n\n\nclass TestFTPFileStore(unittest.TestCase):\n    @defer.inlineCallbacks\n    def test_persist(self):\n        uri = os.environ.get('FTP_TEST_FILE_URI')\n        if not uri:\n            raise unittest.SkipTest(\"No FTP URI available for testing\")\n        data = b\"TestFTPFilesStore: \\xe2\\x98\\x83\"\n        buf = BytesIO(data)\n        meta = {'foo': 'bar'}\n        path = 'full/filename'\n        store = FTPFilesStore(uri)\n        empty_dict = yield store.stat_file(path, info=None)\n        self.assertEqual(empty_dict, {})\n        yield store.persist_file(path, buf, info=None, meta=meta, headers=None)\n        stat = yield store.stat_file(path, info=None)\n        self.assertIn('last_modified', stat)\n        self.assertIn('checksum', stat)\n        self.assertEqual(stat['checksum'], 'd113d66b2ec7258724a268bd88eef6b6')\n        path = '%s/%s' % (store.basedir, path)\n        content = get_ftp_content_and_delete(\n            path, store.host, store.port,\n            store.username, store.password, store.USE_ACTIVE_MODE)\n        self.assertEqual(data.decode(), content)\n\n\nclass ItemWithFiles(Item):\n    file_urls = Field()\n    files = Field()\n\n\ndef _create_item_with_files(*files):\n    item = ItemWithFiles()\n    item['file_urls'] = files\n    return item\n\n\ndef _prepare_request_object(item_url):\n    return Request(\n        item_url,\n        meta={'response': Response(item_url, status=200, body=b'data')})\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"
    }
  ],
  "questions": [
    "https://docs.scrapy.org/en/latest/search.html?q=ftp&check_keywords=yes&area=default",
    "I want to save images on a remote server via (FTP?). Is this possible with scrapy? \r\n(I know about S3 or Google but i mean in a custom FTP server)."
  ],
  "golden_answers": [
    "I dont think scrapy support saving files, images to FTP servers except S3 or Google.\r\nTo achieve that, i patched files.py, images.py and pipelines.py and created a new class FTPStore.",
    "I dont think scrapy support saving files, images to FTP servers except S3 or Google.\r\nTo achieve that, i patched files.py, images.py and pipelines.py and created a new class FTPStore."
  ],
  "questions_generated": [
    "How can images be saved to a custom FTP server using Scrapy, given the repository's current capabilities?",
    "What is the role of the `FTPStore` class, and how does it integrate with Scrapy's existing infrastructure?",
    "What changes would be necessary in `scrapy/extensions/feedexport.py` to support FTP storage for feed exports?",
    "What are the potential challenges or limitations of implementing FTP storage in Scrapy's pipeline or feed export systems?",
    "How does the `BlockingFeedStorage` class differ from a potential FTP-based storage class in terms of implementation and functionality?"
  ],
  "golden_answers_generated": [
    "Scrapy does not natively support saving images to a custom FTP server. However, a custom solution can be implemented by creating a new class, such as `FTPStore`, which manages FTP connections and file storage operations. This involves patching existing files like `files.py`, `images.py`, and `pipelines.py` to integrate the custom FTP functionality.",
    "The `FTPStore` class is a custom implementation designed to save files to an FTP server. It establishes an FTP connection using provided credentials and implements methods like `persist_file` to upload files. This class would integrate with Scrapy by modifying or extending the pipeline components to utilize this FTP storage mechanism instead of the default file storage methods.",
    "To support FTP storage for feed exports, a new class implementing the `IFeedStorage` interface could be created. This class would handle FTP connections and file transfers, similar to `FTPStore`. The `store()` method would need to use FTP commands to transfer the feed file to the remote server, and the `open()` method might involve creating a temporary file that gets uploaded upon completion.",
    "Implementing FTP storage in Scrapy's systems might present challenges such as managing FTP connection errors, ensuring secure transfer of files (e.g., via FTP over TLS/SSL), handling different FTP server configurations, and maintaining performance efficiency. Additionally, integrating FTP storage seamlessly with Scrapy's asynchronous processing model could require careful handling of threads or deferred operations.",
    "The `BlockingFeedStorage` class is designed to use a temporary file for storing feed data, which is later processed in a blocking manner using threads. In contrast, a potential FTP-based storage class would need to manage FTP connections and perform network operations to upload files to a remote server. The FTP storage class would involve more complex error handling, connection management, and possibly different threading or asynchronous strategies to accommodate network latency and ensure reliable file transfers."
  ]
}