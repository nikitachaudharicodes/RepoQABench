{
  "repo_name": "scrapy_scrapy",
  "issue_id": "5874",
  "issue_description": "# Scrapy does not decode base64 MD5 checksum from GCS\n\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nIncorrect GCS Checksum processing\r\n\r\n### Steps to Reproduce\r\n\r\n1. Obtain the checksum for an up-to-date file.\r\n\r\n**Expected behavior:** [What you expect to happen]\r\nmatches the checksum of the file downloaded\r\n**Actual behavior:** [What actually happens]\r\nNOT matches the checksum of the file downloaded\r\n**Reproduces how often:** [What percentage of the time does it reproduce?]\r\nAlways\r\n### Versions\r\ncurrent\r\n\r\n### Additional context\r\n\r\nhttps://cloud.google.com/storage/docs/json_api/v1/objects\r\n> MD5 hash of the data, encoded using [base64](https://datatracker.ietf.org/doc/html/rfc4648#section-4).\r\n\r\nBut, Scrapy dose not decode MD5 from GCS.\r\n",
  "issue_comments": [
    {
      "id": 1484763054,
      "user": "wRAR",
      "body": "Hi! Can you please provide more info? "
    },
    {
      "id": 1484790987,
      "user": "namelessGonbai",
      "body": "For instance, I downloaded and saved a file from Google Cloud Storage using Scrapy, and its MD5 is \"42bff117f2040df9c0fbf24752da0023\". However, when I attempted to scrap the file again, the Scrapy output showed \"checksum\": \"Qr/xF/IEDfnA+/JHUtoAIw==\", \"status\": \"uptodate\". This is because, as indicated in the URL I provided earlier, Google's API (and the client) encodes the MD5 in base64 for some unknown reason. By decoding \"Qr/xF/IEDfnA+/JHUtoAIw==\" with base64 and using byte.hex(), the original MD5 can be obtained."
    },
    {
      "id": 1484836755,
      "user": "wRAR",
      "body": "I see. The relevant code seems to be https://github.com/scrapy/scrapy/blob/d60b4edd11436e61284615ec7ce89f8ac7e46d9a/scrapy/pipelines/files.py#L231"
    },
    {
      "id": 1486021439,
      "user": "jmannoop",
      "body": "me and @tstauder can work on this issue."
    },
    {
      "id": 1487440964,
      "user": "tstauder",
      "body": "\r\nIs it possible to have it assigned to one of us? "
    },
    {
      "id": 1487468585,
      "user": "wRAR",
      "body": "You don't really need that to work on it or provide a PR."
    },
    {
      "id": 1503237578,
      "user": "ghost",
      "body": "Hi @Gallaecio , from what I understand from the discussion above and the source code, it seems that all that is required to fix this is to change `checksum = blob.md5_hash` to `checksum = base64.b64decode(blob.md5_hash).hex()`. Let me know if there's anything more to it, I'll be more than happy to raise a PR"
    },
    {
      "id": 1503279296,
      "user": "Gallaecio",
      "body": "@heppymxm I believe that’s it. And adding a test for it, which is probably harder than the change itself :sweat_smile: "
    },
    {
      "id": 1503320862,
      "user": "ghost",
      "body": "This [line](https://github.com/scrapy/scrapy/blob/c7730627a0f99afccca11437b0775757c50ca9e8/tests/test_pipeline_files.py#L604) is comparing the checksum to a b64 encoded string. I'm curious where this even came from? And if this is valid, can we not simply extract out the md5 hash from this and replace it here?\r\nThanks!"
    },
    {
      "id": 1503346306,
      "user": "Gallaecio",
      "body": "Oh, updating that value to the right expectation should be test enough, nice! I assume the current base is the base64-encoded MD5 of https://github.com/scrapy/scrapy/blob/c7730627a0f99afccca11437b0775757c50ca9e8/tests/test_pipeline_files.py#L593"
    },
    {
      "id": 1503347857,
      "user": "ghost",
      "body": "Shall I raise a PR with these changes?"
    },
    {
      "id": 1503355235,
      "user": "Gallaecio",
      "body": "That would be awesome!"
    }
  ],
  "text_context": "# Scrapy does not decode base64 MD5 checksum from GCS\n\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nIncorrect GCS Checksum processing\r\n\r\n### Steps to Reproduce\r\n\r\n1. Obtain the checksum for an up-to-date file.\r\n\r\n**Expected behavior:** [What you expect to happen]\r\nmatches the checksum of the file downloaded\r\n**Actual behavior:** [What actually happens]\r\nNOT matches the checksum of the file downloaded\r\n**Reproduces how often:** [What percentage of the time does it reproduce?]\r\nAlways\r\n### Versions\r\ncurrent\r\n\r\n### Additional context\r\n\r\nhttps://cloud.google.com/storage/docs/json_api/v1/objects\r\n> MD5 hash of the data, encoded using [base64](https://datatracker.ietf.org/doc/html/rfc4648#section-4).\r\n\r\nBut, Scrapy dose not decode MD5 from GCS.\r\n\n\nHi! Can you please provide more info? \n\nFor instance, I downloaded and saved a file from Google Cloud Storage using Scrapy, and its MD5 is \"42bff117f2040df9c0fbf24752da0023\". However, when I attempted to scrap the file again, the Scrapy output showed \"checksum\": \"Qr/xF/IEDfnA+/JHUtoAIw==\", \"status\": \"uptodate\". This is because, as indicated in the URL I provided earlier, Google's API (and the client) encodes the MD5 in base64 for some unknown reason. By decoding \"Qr/xF/IEDfnA+/JHUtoAIw==\" with base64 and using byte.hex(), the original MD5 can be obtained.\n\nI see. The relevant code seems to be https://github.com/scrapy/scrapy/blob/d60b4edd11436e61284615ec7ce89f8ac7e46d9a/scrapy/pipelines/files.py#L231\n\nme and @tstauder can work on this issue.\n\n\r\nIs it possible to have it assigned to one of us? \n\nYou don't really need that to work on it or provide a PR.\n\nHi @Gallaecio , from what I understand from the discussion above and the source code, it seems that all that is required to fix this is to change `checksum = blob.md5_hash` to `checksum = base64.b64decode(blob.md5_hash).hex()`. Let me know if there's anything more to it, I'll be more than happy to raise a PR\n\n@heppymxm I believe that’s it. And adding a test for it, which is probably harder than the change itself :sweat_smile: \n\nThis [line](https://github.com/scrapy/scrapy/blob/c7730627a0f99afccca11437b0775757c50ca9e8/tests/test_pipeline_files.py#L604) is comparing the checksum to a b64 encoded string. I'm curious where this even came from? And if this is valid, can we not simply extract out the md5 hash from this and replace it here?\r\nThanks!\n\nOh, updating that value to the right expectation should be test enough, nice! I assume the current base is the base64-encoded MD5 of https://github.com/scrapy/scrapy/blob/c7730627a0f99afccca11437b0775757c50ca9e8/tests/test_pipeline_files.py#L593\n\nShall I raise a PR with these changes?\n\nThat would be awesome!",
  "pr_link": "https://github.com/scrapy/scrapy/pull/5891",
  "code_context": [
    {
      "filename": "scrapy/pipelines/files.py",
      "content": "\"\"\"\nFiles Pipeline\n\nSee documentation in topics/media-pipeline.rst\n\"\"\"\nimport base64\nimport functools\nimport hashlib\nimport logging\nimport mimetypes\nimport os\nimport time\nfrom collections import defaultdict\nfrom contextlib import suppress\nfrom ftplib import FTP\nfrom io import BytesIO\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import DefaultDict, Optional, Set, Union\nfrom urllib.parse import urlparse\n\nfrom itemadapter import ItemAdapter\nfrom twisted.internet import defer, threads\n\nfrom scrapy.exceptions import IgnoreRequest, NotConfigured\nfrom scrapy.http import Request\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.pipelines.media import MediaPipeline\nfrom scrapy.settings import Settings\nfrom scrapy.utils.boto import is_botocore_available\nfrom scrapy.utils.datatypes import CaselessDict\nfrom scrapy.utils.ftp import ftp_store_file\nfrom scrapy.utils.log import failure_to_exc_info\nfrom scrapy.utils.misc import md5sum\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.request import referer_str\n\nlogger = logging.getLogger(__name__)\n\n\ndef _to_string(path: Union[str, PathLike]) -> str:\n    return str(path)  # convert a Path object to string\n\n\nclass FileException(Exception):\n    \"\"\"General media error exception\"\"\"\n\n\nclass FSFilesStore:\n    def __init__(self, basedir: Union[str, PathLike]):\n        basedir = _to_string(basedir)\n        if \"://\" in basedir:\n            basedir = basedir.split(\"://\", 1)[1]\n        self.basedir = basedir\n        self._mkdir(Path(self.basedir))\n        self.created_directories: DefaultDict[str, Set[str]] = defaultdict(set)\n\n    def persist_file(\n        self, path: Union[str, PathLike], buf, info, meta=None, headers=None\n    ):\n        absolute_path = self._get_filesystem_path(path)\n        self._mkdir(absolute_path.parent, info)\n        absolute_path.write_bytes(buf.getvalue())\n\n    def stat_file(self, path: Union[str, PathLike], info):\n        absolute_path = self._get_filesystem_path(path)\n        try:\n            last_modified = absolute_path.stat().st_mtime\n        except os.error:\n            return {}\n\n        with absolute_path.open(\"rb\") as f:\n            checksum = md5sum(f)\n\n        return {\"last_modified\": last_modified, \"checksum\": checksum}\n\n    def _get_filesystem_path(self, path: Union[str, PathLike]) -> Path:\n        path_comps = _to_string(path).split(\"/\")\n        return Path(self.basedir, *path_comps)\n\n    def _mkdir(self, dirname: Path, domain: Optional[str] = None):\n        seen = self.created_directories[domain] if domain else set()\n        if str(dirname) not in seen:\n            if not dirname.exists():\n                dirname.mkdir(parents=True)\n            seen.add(str(dirname))\n\n\nclass S3FilesStore:\n    AWS_ACCESS_KEY_ID = None\n    AWS_SECRET_ACCESS_KEY = None\n    AWS_SESSION_TOKEN = None\n    AWS_ENDPOINT_URL = None\n    AWS_REGION_NAME = None\n    AWS_USE_SSL = None\n    AWS_VERIFY = None\n\n    POLICY = \"private\"  # Overridden from settings.FILES_STORE_S3_ACL in FilesPipeline.from_settings\n    HEADERS = {\n        \"Cache-Control\": \"max-age=172800\",\n    }\n\n    def __init__(self, uri):\n        if not is_botocore_available():\n            raise NotConfigured(\"missing botocore library\")\n        import botocore.session\n\n        session = botocore.session.get_session()\n        self.s3_client = session.create_client(\n            \"s3\",\n            aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n            aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n            aws_session_token=self.AWS_SESSION_TOKEN,\n            endpoint_url=self.AWS_ENDPOINT_URL,\n            region_name=self.AWS_REGION_NAME,\n            use_ssl=self.AWS_USE_SSL,\n            verify=self.AWS_VERIFY,\n        )\n        if not uri.startswith(\"s3://\"):\n            raise ValueError(f\"Incorrect URI scheme in {uri}, expected 's3'\")\n        self.bucket, self.prefix = uri[5:].split(\"/\", 1)\n\n    def stat_file(self, path, info):\n        def _onsuccess(boto_key):\n            checksum = boto_key[\"ETag\"].strip('\"')\n            last_modified = boto_key[\"LastModified\"]\n            modified_stamp = time.mktime(last_modified.timetuple())\n            return {\"checksum\": checksum, \"last_modified\": modified_stamp}\n\n        return self._get_boto_key(path).addCallback(_onsuccess)\n\n    def _get_boto_key(self, path):\n        key_name = f\"{self.prefix}{path}\"\n        return threads.deferToThread(\n            self.s3_client.head_object, Bucket=self.bucket, Key=key_name\n        )\n\n    def persist_file(self, path, buf, info, meta=None, headers=None):\n        \"\"\"Upload file to S3 storage\"\"\"\n        key_name = f\"{self.prefix}{path}\"\n        buf.seek(0)\n        extra = self._headers_to_botocore_kwargs(self.HEADERS)\n        if headers:\n            extra.update(self._headers_to_botocore_kwargs(headers))\n        return threads.deferToThread(\n            self.s3_client.put_object,\n            Bucket=self.bucket,\n            Key=key_name,\n            Body=buf,\n            Metadata={k: str(v) for k, v in (meta or {}).items()},\n            ACL=self.POLICY,\n            **extra,\n        )\n\n    def _headers_to_botocore_kwargs(self, headers):\n        \"\"\"Convert headers to botocore keyword arguments.\"\"\"\n        # This is required while we need to support both boto and botocore.\n        mapping = CaselessDict(\n            {\n                \"Content-Type\": \"ContentType\",\n                \"Cache-Control\": \"CacheControl\",\n                \"Content-Disposition\": \"ContentDisposition\",\n                \"Content-Encoding\": \"ContentEncoding\",\n                \"Content-Language\": \"ContentLanguage\",\n                \"Content-Length\": \"ContentLength\",\n                \"Content-MD5\": \"ContentMD5\",\n                \"Expires\": \"Expires\",\n                \"X-Amz-Grant-Full-Control\": \"GrantFullControl\",\n                \"X-Amz-Grant-Read\": \"GrantRead\",\n                \"X-Amz-Grant-Read-ACP\": \"GrantReadACP\",\n                \"X-Amz-Grant-Write-ACP\": \"GrantWriteACP\",\n                \"X-Amz-Object-Lock-Legal-Hold\": \"ObjectLockLegalHoldStatus\",\n                \"X-Amz-Object-Lock-Mode\": \"ObjectLockMode\",\n                \"X-Amz-Object-Lock-Retain-Until-Date\": \"ObjectLockRetainUntilDate\",\n                \"X-Amz-Request-Payer\": \"RequestPayer\",\n                \"X-Amz-Server-Side-Encryption\": \"ServerSideEncryption\",\n                \"X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id\": \"SSEKMSKeyId\",\n                \"X-Amz-Server-Side-Encryption-Context\": \"SSEKMSEncryptionContext\",\n                \"X-Amz-Server-Side-Encryption-Customer-Algorithm\": \"SSECustomerAlgorithm\",\n                \"X-Amz-Server-Side-Encryption-Customer-Key\": \"SSECustomerKey\",\n                \"X-Amz-Server-Side-Encryption-Customer-Key-Md5\": \"SSECustomerKeyMD5\",\n                \"X-Amz-Storage-Class\": \"StorageClass\",\n                \"X-Amz-Tagging\": \"Tagging\",\n                \"X-Amz-Website-Redirect-Location\": \"WebsiteRedirectLocation\",\n            }\n        )\n        extra = {}\n        for key, value in headers.items():\n            try:\n                kwarg = mapping[key]\n            except KeyError:\n                raise TypeError(f'Header \"{key}\" is not supported by botocore')\n            else:\n                extra[kwarg] = value\n        return extra\n\n\nclass GCSFilesStore:\n    GCS_PROJECT_ID = None\n\n    CACHE_CONTROL = \"max-age=172800\"\n\n    # The bucket's default object ACL will be applied to the object.\n    # Overridden from settings.FILES_STORE_GCS_ACL in FilesPipeline.from_settings.\n    POLICY = None\n\n    def __init__(self, uri):\n        from google.cloud import storage\n\n        client = storage.Client(project=self.GCS_PROJECT_ID)\n        bucket, prefix = uri[5:].split(\"/\", 1)\n        self.bucket = client.bucket(bucket)\n        self.prefix = prefix\n        permissions = self.bucket.test_iam_permissions(\n            [\"storage.objects.get\", \"storage.objects.create\"]\n        )\n        if \"storage.objects.get\" not in permissions:\n            logger.warning(\n                \"No 'storage.objects.get' permission for GSC bucket %(bucket)s. \"\n                \"Checking if files are up to date will be impossible. Files will be downloaded every time.\",\n                {\"bucket\": bucket},\n            )\n        if \"storage.objects.create\" not in permissions:\n            logger.error(\n                \"No 'storage.objects.create' permission for GSC bucket %(bucket)s. Saving files will be impossible!\",\n                {\"bucket\": bucket},\n            )\n\n    def stat_file(self, path, info):\n        def _onsuccess(blob):\n            if blob:\n                checksum = base64.b64decode(blob.md5_hash).hex()\n                last_modified = time.mktime(blob.updated.timetuple())\n                return {\"checksum\": checksum, \"last_modified\": last_modified}\n            return {}\n\n        blob_path = self._get_blob_path(path)\n        return threads.deferToThread(self.bucket.get_blob, blob_path).addCallback(\n            _onsuccess\n        )\n\n    def _get_content_type(self, headers):\n        if headers and \"Content-Type\" in headers:\n            return headers[\"Content-Type\"]\n        return \"application/octet-stream\"\n\n    def _get_blob_path(self, path):\n        return self.prefix + path\n\n    def persist_file(self, path, buf, info, meta=None, headers=None):\n        blob_path = self._get_blob_path(path)\n        blob = self.bucket.blob(blob_path)\n        blob.cache_control = self.CACHE_CONTROL\n        blob.metadata = {k: str(v) for k, v in (meta or {}).items()}\n        return threads.deferToThread(\n            blob.upload_from_string,\n            data=buf.getvalue(),\n            content_type=self._get_content_type(headers),\n            predefined_acl=self.POLICY,\n        )\n\n\nclass FTPFilesStore:\n    FTP_USERNAME = None\n    FTP_PASSWORD = None\n    USE_ACTIVE_MODE = None\n\n    def __init__(self, uri):\n        if not uri.startswith(\"ftp://\"):\n            raise ValueError(f\"Incorrect URI scheme in {uri}, expected 'ftp'\")\n        u = urlparse(uri)\n        self.port = u.port\n        self.host = u.hostname\n        self.port = int(u.port or 21)\n        self.username = u.username or self.FTP_USERNAME\n        self.password = u.password or self.FTP_PASSWORD\n        self.basedir = u.path.rstrip(\"/\")\n\n    def persist_file(self, path, buf, info, meta=None, headers=None):\n        path = f\"{self.basedir}/{path}\"\n        return threads.deferToThread(\n            ftp_store_file,\n            path=path,\n            file=buf,\n            host=self.host,\n            port=self.port,\n            username=self.username,\n            password=self.password,\n            use_active_mode=self.USE_ACTIVE_MODE,\n        )\n\n    def stat_file(self, path, info):\n        def _stat_file(path):\n            try:\n                ftp = FTP()\n                ftp.connect(self.host, self.port)\n                ftp.login(self.username, self.password)\n                if self.USE_ACTIVE_MODE:\n                    ftp.set_pasv(False)\n                file_path = f\"{self.basedir}/{path}\"\n                last_modified = float(ftp.voidcmd(f\"MDTM {file_path}\")[4:].strip())\n                m = hashlib.md5()\n                ftp.retrbinary(f\"RETR {file_path}\", m.update)\n                return {\"last_modified\": last_modified, \"checksum\": m.hexdigest()}\n            # The file doesn't exist\n            except Exception:\n                return {}\n\n        return threads.deferToThread(_stat_file, path)\n\n\nclass FilesPipeline(MediaPipeline):\n    \"\"\"Abstract pipeline that implement the file downloading\n\n    This pipeline tries to minimize network transfers and file processing,\n    doing stat of the files and determining if file is new, up-to-date or\n    expired.\n\n    ``new`` files are those that pipeline never processed and needs to be\n        downloaded from supplier site the first time.\n\n    ``uptodate`` files are the ones that the pipeline processed and are still\n        valid files.\n\n    ``expired`` files are those that pipeline already processed but the last\n        modification was made long time ago, so a reprocessing is recommended to\n        refresh it in case of change.\n\n    \"\"\"\n\n    MEDIA_NAME = \"file\"\n    EXPIRES = 90\n    STORE_SCHEMES = {\n        \"\": FSFilesStore,\n        \"file\": FSFilesStore,\n        \"s3\": S3FilesStore,\n        \"gs\": GCSFilesStore,\n        \"ftp\": FTPFilesStore,\n    }\n    DEFAULT_FILES_URLS_FIELD = \"file_urls\"\n    DEFAULT_FILES_RESULT_FIELD = \"files\"\n\n    def __init__(self, store_uri, download_func=None, settings=None):\n        store_uri = _to_string(store_uri)\n        if not store_uri:\n            raise NotConfigured\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n        cls_name = \"FilesPipeline\"\n        self.store = self._get_store(store_uri)\n        resolve = functools.partial(\n            self._key_for_pipe, base_class_name=cls_name, settings=settings\n        )\n        self.expires = settings.getint(resolve(\"FILES_EXPIRES\"), self.EXPIRES)\n        if not hasattr(self, \"FILES_URLS_FIELD\"):\n            self.FILES_URLS_FIELD = self.DEFAULT_FILES_URLS_FIELD\n        if not hasattr(self, \"FILES_RESULT_FIELD\"):\n            self.FILES_RESULT_FIELD = self.DEFAULT_FILES_RESULT_FIELD\n        self.files_urls_field = settings.get(\n            resolve(\"FILES_URLS_FIELD\"), self.FILES_URLS_FIELD\n        )\n        self.files_result_field = settings.get(\n            resolve(\"FILES_RESULT_FIELD\"), self.FILES_RESULT_FIELD\n        )\n\n        super().__init__(download_func=download_func, settings=settings)\n\n    @classmethod\n    def from_settings(cls, settings):\n        s3store = cls.STORE_SCHEMES[\"s3\"]\n        s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n        s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n        s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n        s3store.AWS_ENDPOINT_URL = settings[\"AWS_ENDPOINT_URL\"]\n        s3store.AWS_REGION_NAME = settings[\"AWS_REGION_NAME\"]\n        s3store.AWS_USE_SSL = settings[\"AWS_USE_SSL\"]\n        s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n        s3store.POLICY = settings[\"FILES_STORE_S3_ACL\"]\n\n        gcs_store = cls.STORE_SCHEMES[\"gs\"]\n        gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n        gcs_store.POLICY = settings[\"FILES_STORE_GCS_ACL\"] or None\n\n        ftp_store = cls.STORE_SCHEMES[\"ftp\"]\n        ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n        ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n        ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n\n        store_uri = settings[\"FILES_STORE\"]\n        return cls(store_uri, settings=settings)\n\n    def _get_store(self, uri: str):\n        if Path(uri).is_absolute():  # to support win32 paths like: C:\\\\some\\dir\n            scheme = \"file\"\n        else:\n            scheme = urlparse(uri).scheme\n        store_cls = self.STORE_SCHEMES[scheme]\n        return store_cls(uri)\n\n    def media_to_download(self, request, info, *, item=None):\n        def _onsuccess(result):\n            if not result:\n                return  # returning None force download\n\n            last_modified = result.get(\"last_modified\", None)\n            if not last_modified:\n                return  # returning None force download\n\n            age_seconds = time.time() - last_modified\n            age_days = age_seconds / 60 / 60 / 24\n            if age_days > self.expires:\n                return  # returning None force download\n\n            referer = referer_str(request)\n            logger.debug(\n                \"File (uptodate): Downloaded %(medianame)s from %(request)s \"\n                \"referred in <%(referer)s>\",\n                {\"medianame\": self.MEDIA_NAME, \"request\": request, \"referer\": referer},\n                extra={\"spider\": info.spider},\n            )\n            self.inc_stats(info.spider, \"uptodate\")\n\n            checksum = result.get(\"checksum\", None)\n            return {\n                \"url\": request.url,\n                \"path\": path,\n                \"checksum\": checksum,\n                \"status\": \"uptodate\",\n            }\n\n        path = self.file_path(request, info=info, item=item)\n        dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n        dfd.addCallbacks(_onsuccess, lambda _: None)\n        dfd.addErrback(\n            lambda f: logger.error(\n                self.__class__.__name__ + \".store.stat_file\",\n                exc_info=failure_to_exc_info(f),\n                extra={\"spider\": info.spider},\n            )\n        )\n        return dfd\n\n    def media_failed(self, failure, request, info):\n        if not isinstance(failure.value, IgnoreRequest):\n            referer = referer_str(request)\n            logger.warning(\n                \"File (unknown-error): Error downloading %(medianame)s from \"\n                \"%(request)s referred in <%(referer)s>: %(exception)s\",\n                {\n                    \"medianame\": self.MEDIA_NAME,\n                    \"request\": request,\n                    \"referer\": referer,\n                    \"exception\": failure.value,\n                },\n                extra={\"spider\": info.spider},\n            )\n\n        raise FileException\n\n    def media_downloaded(self, response, request, info, *, item=None):\n        referer = referer_str(request)\n\n        if response.status != 200:\n            logger.warning(\n                \"File (code: %(status)s): Error downloading file from \"\n                \"%(request)s referred in <%(referer)s>\",\n                {\"status\": response.status, \"request\": request, \"referer\": referer},\n                extra={\"spider\": info.spider},\n            )\n            raise FileException(\"download-error\")\n\n        if not response.body:\n            logger.warning(\n                \"File (empty-content): Empty file from %(request)s referred \"\n                \"in <%(referer)s>: no-content\",\n                {\"request\": request, \"referer\": referer},\n                extra={\"spider\": info.spider},\n            )\n            raise FileException(\"empty-content\")\n\n        status = \"cached\" if \"cached\" in response.flags else \"downloaded\"\n        logger.debug(\n            \"File (%(status)s): Downloaded file from %(request)s referred in \"\n            \"<%(referer)s>\",\n            {\"status\": status, \"request\": request, \"referer\": referer},\n            extra={\"spider\": info.spider},\n        )\n        self.inc_stats(info.spider, status)\n\n        try:\n            path = self.file_path(request, response=response, info=info, item=item)\n            checksum = self.file_downloaded(response, request, info, item=item)\n        except FileException as exc:\n            logger.warning(\n                \"File (error): Error processing file from %(request)s \"\n                \"referred in <%(referer)s>: %(errormsg)s\",\n                {\"request\": request, \"referer\": referer, \"errormsg\": str(exc)},\n                extra={\"spider\": info.spider},\n                exc_info=True,\n            )\n            raise\n        except Exception as exc:\n            logger.error(\n                \"File (unknown-error): Error processing file from %(request)s \"\n                \"referred in <%(referer)s>\",\n                {\"request\": request, \"referer\": referer},\n                exc_info=True,\n                extra={\"spider\": info.spider},\n            )\n            raise FileException(str(exc))\n\n        return {\n            \"url\": request.url,\n            \"path\": path,\n            \"checksum\": checksum,\n            \"status\": status,\n        }\n\n    def inc_stats(self, spider, status):\n        spider.crawler.stats.inc_value(\"file_count\", spider=spider)\n        spider.crawler.stats.inc_value(f\"file_status_count/{status}\", spider=spider)\n\n    # Overridable Interface\n    def get_media_requests(self, item, info):\n        urls = ItemAdapter(item).get(self.files_urls_field, [])\n        return [Request(u, callback=NO_CALLBACK) for u in urls]\n\n    def file_downloaded(self, response, request, info, *, item=None):\n        path = self.file_path(request, response=response, info=info, item=item)\n        buf = BytesIO(response.body)\n        checksum = md5sum(buf)\n        buf.seek(0)\n        self.store.persist_file(path, buf, info)\n        return checksum\n\n    def item_completed(self, results, item, info):\n        with suppress(KeyError):\n            ItemAdapter(item)[self.files_result_field] = [x for ok, x in results if ok]\n        return item\n\n    def file_path(self, request, response=None, info=None, *, item=None):\n        media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n        media_ext = Path(request.url).suffix\n        # Handles empty and wild extensions by trying to guess the\n        # mime type then extension or default to empty string otherwise\n        if media_ext not in mimetypes.types_map:\n            media_ext = \"\"\n            media_type = mimetypes.guess_type(request.url)[0]\n            if media_type:\n                media_ext = mimetypes.guess_extension(media_type)\n        return f\"full/{media_guid}{media_ext}\"\n"
    },
    {
      "filename": "tests/test_pipeline_files.py",
      "content": "import dataclasses\nimport os\nimport random\nimport time\nfrom datetime import datetime\nfrom io import BytesIO\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom tempfile import mkdtemp\nfrom unittest import mock\nfrom urllib.parse import urlparse\n\nimport attr\nfrom itemadapter import ItemAdapter\nfrom twisted.internet import defer\nfrom twisted.trial import unittest\n\nfrom scrapy.http import Request, Response\nfrom scrapy.item import Field, Item\nfrom scrapy.pipelines.files import (\n    FilesPipeline,\n    FSFilesStore,\n    FTPFilesStore,\n    GCSFilesStore,\n    S3FilesStore,\n)\nfrom scrapy.settings import Settings\nfrom scrapy.utils.test import (\n    assert_gcs_environ,\n    get_crawler,\n    get_ftp_content_and_delete,\n    get_gcs_content_and_delete,\n    skip_if_no_boto,\n)\n\nfrom .test_pipeline_media import _mocked_download_func\n\n\nclass FilesPipelineTestCase(unittest.TestCase):\n    def setUp(self):\n        self.tempdir = mkdtemp()\n        settings_dict = {\"FILES_STORE\": self.tempdir}\n        crawler = get_crawler(spidercls=None, settings_dict=settings_dict)\n        self.pipeline = FilesPipeline.from_crawler(crawler)\n        self.pipeline.download_func = _mocked_download_func\n        self.pipeline.open_spider(None)\n\n    def tearDown(self):\n        rmtree(self.tempdir)\n\n    def test_file_path(self):\n        file_path = self.pipeline.file_path\n        self.assertEqual(\n            file_path(Request(\"https://dev.mydeco.com/mydeco.pdf\")),\n            \"full/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\n                    \"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt\"\n                )\n            ),\n            \"full/4ce274dd83db0368bafd7e406f382ae088e39219.txt\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc\")\n            ),\n            \"full/94ccc495a17b9ac5d40e3eabf3afcb8c2c9b9e1a.doc\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\n                    \"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\"\n                )\n            ),\n            \"full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg\",\n        )\n        self.assertEqual(\n            file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532/\")),\n            \"full/97ee6f8a46cbbb418ea91502fd24176865cf39b2\",\n        )\n        self.assertEqual(\n            file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\")),\n            \"full/244e0dd7d96a3b7b01f54eded250c9e272577aa1\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                response=Response(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                info=object(),\n            ),\n            \"full/244e0dd7d96a3b7b01f54eded250c9e272577aa1\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\n                    \"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg.bohaha\"\n                )\n            ),\n            \"full/76c00cef2ef669ae65052661f68d451162829507\",\n        )\n        self.assertEqual(\n            file_path(\n                Request(\n                    \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAR0AAACxCAMAAADOHZloAAACClBMVEX/\\\n                                    //+F0tzCwMK76ZKQ21AMqr7oAAC96JvD5aWM2kvZ78J0N7fmAAC46Y4Ap7y\"\n                )\n            ),\n            \"full/178059cbeba2e34120a67f2dc1afc3ecc09b61cb.png\",\n        )\n\n    def test_fs_store(self):\n        assert isinstance(self.pipeline.store, FSFilesStore)\n        self.assertEqual(self.pipeline.store.basedir, self.tempdir)\n\n        path = \"some/image/key.jpg\"\n        fullpath = Path(self.tempdir, \"some\", \"image\", \"key.jpg\")\n        self.assertEqual(self.pipeline.store._get_filesystem_path(path), fullpath)\n\n    @defer.inlineCallbacks\n    def test_file_not_expired(self):\n        item_url = \"http://example.com/file.pdf\"\n        item = _create_item_with_files(item_url)\n        patchers = [\n            mock.patch.object(FilesPipeline, \"inc_stats\", return_value=True),\n            mock.patch.object(\n                FSFilesStore,\n                \"stat_file\",\n                return_value={\"checksum\": \"abc\", \"last_modified\": time.time()},\n            ),\n            mock.patch.object(\n                FilesPipeline,\n                \"get_media_requests\",\n                return_value=[_prepare_request_object(item_url)],\n            ),\n        ]\n        for p in patchers:\n            p.start()\n\n        result = yield self.pipeline.process_item(item, None)\n        self.assertEqual(result[\"files\"][0][\"checksum\"], \"abc\")\n        self.assertEqual(result[\"files\"][0][\"status\"], \"uptodate\")\n\n        for p in patchers:\n            p.stop()\n\n    @defer.inlineCallbacks\n    def test_file_expired(self):\n        item_url = \"http://example.com/file2.pdf\"\n        item = _create_item_with_files(item_url)\n        patchers = [\n            mock.patch.object(\n                FSFilesStore,\n                \"stat_file\",\n                return_value={\n                    \"checksum\": \"abc\",\n                    \"last_modified\": time.time()\n                    - (self.pipeline.expires * 60 * 60 * 24 * 2),\n                },\n            ),\n            mock.patch.object(\n                FilesPipeline,\n                \"get_media_requests\",\n                return_value=[_prepare_request_object(item_url)],\n            ),\n            mock.patch.object(FilesPipeline, \"inc_stats\", return_value=True),\n        ]\n        for p in patchers:\n            p.start()\n\n        result = yield self.pipeline.process_item(item, None)\n        self.assertNotEqual(result[\"files\"][0][\"checksum\"], \"abc\")\n        self.assertEqual(result[\"files\"][0][\"status\"], \"downloaded\")\n\n        for p in patchers:\n            p.stop()\n\n    @defer.inlineCallbacks\n    def test_file_cached(self):\n        item_url = \"http://example.com/file3.pdf\"\n        item = _create_item_with_files(item_url)\n        patchers = [\n            mock.patch.object(FilesPipeline, \"inc_stats\", return_value=True),\n            mock.patch.object(\n                FSFilesStore,\n                \"stat_file\",\n                return_value={\n                    \"checksum\": \"abc\",\n                    \"last_modified\": time.time()\n                    - (self.pipeline.expires * 60 * 60 * 24 * 2),\n                },\n            ),\n            mock.patch.object(\n                FilesPipeline,\n                \"get_media_requests\",\n                return_value=[_prepare_request_object(item_url, flags=[\"cached\"])],\n            ),\n        ]\n        for p in patchers:\n            p.start()\n\n        result = yield self.pipeline.process_item(item, None)\n        self.assertNotEqual(result[\"files\"][0][\"checksum\"], \"abc\")\n        self.assertEqual(result[\"files\"][0][\"status\"], \"cached\")\n\n        for p in patchers:\n            p.stop()\n\n    def test_file_path_from_item(self):\n        \"\"\"\n        Custom file path based on item data, overriding default implementation\n        \"\"\"\n\n        class CustomFilesPipeline(FilesPipeline):\n            def file_path(self, request, response=None, info=None, item=None):\n                return f'full/{item.get(\"path\")}'\n\n        file_path = CustomFilesPipeline.from_settings(\n            Settings({\"FILES_STORE\": self.tempdir})\n        ).file_path\n        item = dict(path=\"path-to-store-file\")\n        request = Request(\"http://example.com\")\n        self.assertEqual(file_path(request, item=item), \"full/path-to-store-file\")\n\n\nclass FilesPipelineTestCaseFieldsMixin:\n    def test_item_fields_default(self):\n        url = \"http://www.example.com/files/1.txt\"\n        item = self.item_class(name=\"item1\", file_urls=[url])\n        pipeline = FilesPipeline.from_settings(\n            Settings({\"FILES_STORE\": \"s3://example/files/\"})\n        )\n        requests = list(pipeline.get_media_requests(item, None))\n        self.assertEqual(requests[0].url, url)\n        results = [(True, {\"url\": url})]\n        item = pipeline.item_completed(results, item, None)\n        files = ItemAdapter(item).get(\"files\")\n        self.assertEqual(files, [results[0][1]])\n        self.assertIsInstance(item, self.item_class)\n\n    def test_item_fields_override_settings(self):\n        url = \"http://www.example.com/files/1.txt\"\n        item = self.item_class(name=\"item1\", custom_file_urls=[url])\n        pipeline = FilesPipeline.from_settings(\n            Settings(\n                {\n                    \"FILES_STORE\": \"s3://example/files/\",\n                    \"FILES_URLS_FIELD\": \"custom_file_urls\",\n                    \"FILES_RESULT_FIELD\": \"custom_files\",\n                }\n            )\n        )\n        requests = list(pipeline.get_media_requests(item, None))\n        self.assertEqual(requests[0].url, url)\n        results = [(True, {\"url\": url})]\n        item = pipeline.item_completed(results, item, None)\n        custom_files = ItemAdapter(item).get(\"custom_files\")\n        self.assertEqual(custom_files, [results[0][1]])\n        self.assertIsInstance(item, self.item_class)\n\n\nclass FilesPipelineTestCaseFieldsDict(\n    FilesPipelineTestCaseFieldsMixin, unittest.TestCase\n):\n    item_class = dict\n\n\nclass FilesPipelineTestItem(Item):\n    name = Field()\n    # default fields\n    file_urls = Field()\n    files = Field()\n    # overridden fields\n    custom_file_urls = Field()\n    custom_files = Field()\n\n\nclass FilesPipelineTestCaseFieldsItem(\n    FilesPipelineTestCaseFieldsMixin, unittest.TestCase\n):\n    item_class = FilesPipelineTestItem\n\n\n@dataclasses.dataclass\nclass FilesPipelineTestDataClass:\n    name: str\n    # default fields\n    file_urls: list = dataclasses.field(default_factory=list)\n    files: list = dataclasses.field(default_factory=list)\n    # overridden fields\n    custom_file_urls: list = dataclasses.field(default_factory=list)\n    custom_files: list = dataclasses.field(default_factory=list)\n\n\nclass FilesPipelineTestCaseFieldsDataClass(\n    FilesPipelineTestCaseFieldsMixin, unittest.TestCase\n):\n    item_class = FilesPipelineTestDataClass\n\n\n@attr.s\nclass FilesPipelineTestAttrsItem:\n    name = attr.ib(default=\"\")\n    # default fields\n    file_urls = attr.ib(default=lambda: [])\n    files = attr.ib(default=lambda: [])\n    # overridden fields\n    custom_file_urls = attr.ib(default=lambda: [])\n    custom_files = attr.ib(default=lambda: [])\n\n\nclass FilesPipelineTestCaseFieldsAttrsItem(\n    FilesPipelineTestCaseFieldsMixin, unittest.TestCase\n):\n    item_class = FilesPipelineTestAttrsItem\n\n\nclass FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n    default_cls_settings = {\n        \"EXPIRES\": 90,\n        \"FILES_URLS_FIELD\": \"file_urls\",\n        \"FILES_RESULT_FIELD\": \"files\",\n    }\n    file_cls_attr_settings_map = {\n        (\"EXPIRES\", \"FILES_EXPIRES\", \"expires\"),\n        (\"FILES_URLS_FIELD\", \"FILES_URLS_FIELD\", \"files_urls_field\"),\n        (\"FILES_RESULT_FIELD\", \"FILES_RESULT_FIELD\", \"files_result_field\"),\n    }\n\n    def setUp(self):\n        self.tempdir = mkdtemp()\n\n    def tearDown(self):\n        rmtree(self.tempdir)\n\n    def _generate_fake_settings(self, prefix=None):\n        def random_string():\n            return \"\".join([chr(random.randint(97, 123)) for _ in range(10)])\n\n        settings = {\n            \"FILES_EXPIRES\": random.randint(100, 1000),\n            \"FILES_URLS_FIELD\": random_string(),\n            \"FILES_RESULT_FIELD\": random_string(),\n            \"FILES_STORE\": self.tempdir,\n        }\n        if not prefix:\n            return settings\n\n        return {\n            prefix.upper() + \"_\" + k if k != \"FILES_STORE\" else k: v\n            for k, v in settings.items()\n        }\n\n    def _generate_fake_pipeline(self):\n        class UserDefinedFilePipeline(FilesPipeline):\n            EXPIRES = 1001\n            FILES_URLS_FIELD = \"alfa\"\n            FILES_RESULT_FIELD = \"beta\"\n\n        return UserDefinedFilePipeline\n\n    def test_different_settings_for_different_instances(self):\n        \"\"\"\n        If there are different instances with different settings they should keep\n        different settings.\n        \"\"\"\n        custom_settings = self._generate_fake_settings()\n        another_pipeline = FilesPipeline.from_settings(Settings(custom_settings))\n        one_pipeline = FilesPipeline(self.tempdir)\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            default_value = self.default_cls_settings[pipe_attr]\n            self.assertEqual(getattr(one_pipeline, pipe_attr), default_value)\n            custom_value = custom_settings[settings_attr]\n            self.assertNotEqual(default_value, custom_value)\n            self.assertEqual(getattr(another_pipeline, pipe_ins_attr), custom_value)\n\n    def test_subclass_attributes_preserved_if_no_settings(self):\n        \"\"\"\n        If subclasses override class attributes and there are no special settings those values should be kept.\n        \"\"\"\n        pipe_cls = self._generate_fake_pipeline()\n        pipe = pipe_cls.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            custom_value = getattr(pipe, pipe_ins_attr)\n            self.assertNotEqual(custom_value, self.default_cls_settings[pipe_attr])\n            self.assertEqual(getattr(pipe, pipe_ins_attr), getattr(pipe, pipe_attr))\n\n    def test_subclass_attrs_preserved_custom_settings(self):\n        \"\"\"\n        If file settings are defined but they are not defined for subclass\n        settings should be preserved.\n        \"\"\"\n        pipeline_cls = self._generate_fake_pipeline()\n        settings = self._generate_fake_settings()\n        pipeline = pipeline_cls.from_settings(Settings(settings))\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            value = getattr(pipeline, pipe_ins_attr)\n            setting_value = settings.get(settings_attr)\n            self.assertNotEqual(value, self.default_cls_settings[pipe_attr])\n            self.assertEqual(value, setting_value)\n\n    def test_no_custom_settings_for_subclasses(self):\n        \"\"\"\n        If there are no settings for subclass and no subclass attributes, pipeline should use\n        attributes of base class.\n        \"\"\"\n\n        class UserDefinedFilesPipeline(FilesPipeline):\n            pass\n\n        user_pipeline = UserDefinedFilesPipeline.from_settings(\n            Settings({\"FILES_STORE\": self.tempdir})\n        )\n        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n            # Values from settings for custom pipeline should be set on pipeline instance.\n            custom_value = self.default_cls_settings.get(pipe_attr.upper())\n            self.assertEqual(getattr(user_pipeline, pipe_ins_attr), custom_value)\n\n    def test_custom_settings_for_subclasses(self):\n        \"\"\"\n        If there are custom settings for subclass and NO class attributes, pipeline should use custom\n        settings.\n        \"\"\"\n\n        class UserDefinedFilesPipeline(FilesPipeline):\n            pass\n\n        prefix = UserDefinedFilesPipeline.__name__.upper()\n        settings = self._generate_fake_settings(prefix=prefix)\n        user_pipeline = UserDefinedFilesPipeline.from_settings(Settings(settings))\n        for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n            # Values from settings for custom pipeline should be set on pipeline instance.\n            custom_value = settings.get(prefix + \"_\" + settings_attr)\n            self.assertNotEqual(custom_value, self.default_cls_settings[pipe_attr])\n            self.assertEqual(getattr(user_pipeline, pipe_inst_attr), custom_value)\n\n    def test_custom_settings_and_class_attrs_for_subclasses(self):\n        \"\"\"\n        If there are custom settings for subclass AND class attributes\n        setting keys are preferred and override attributes.\n        \"\"\"\n        pipeline_cls = self._generate_fake_pipeline()\n        prefix = pipeline_cls.__name__.upper()\n        settings = self._generate_fake_settings(prefix=prefix)\n        user_pipeline = pipeline_cls.from_settings(Settings(settings))\n        for (\n            pipe_cls_attr,\n            settings_attr,\n            pipe_inst_attr,\n        ) in self.file_cls_attr_settings_map:\n            custom_value = settings.get(prefix + \"_\" + settings_attr)\n            self.assertNotEqual(custom_value, self.default_cls_settings[pipe_cls_attr])\n            self.assertEqual(getattr(user_pipeline, pipe_inst_attr), custom_value)\n\n    def test_cls_attrs_with_DEFAULT_prefix(self):\n        class UserDefinedFilesPipeline(FilesPipeline):\n            DEFAULT_FILES_RESULT_FIELD = \"this\"\n            DEFAULT_FILES_URLS_FIELD = \"that\"\n\n        pipeline = UserDefinedFilesPipeline.from_settings(\n            Settings({\"FILES_STORE\": self.tempdir})\n        )\n        self.assertEqual(pipeline.files_result_field, \"this\")\n        self.assertEqual(pipeline.files_urls_field, \"that\")\n\n    def test_user_defined_subclass_default_key_names(self):\n        \"\"\"Test situation when user defines subclass of FilesPipeline,\n        but uses attribute names for default pipeline (without prefixing\n        them with pipeline class name).\n        \"\"\"\n        settings = self._generate_fake_settings()\n\n        class UserPipe(FilesPipeline):\n            pass\n\n        pipeline_cls = UserPipe.from_settings(Settings(settings))\n\n        for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n            expected_value = settings.get(settings_attr)\n            self.assertEqual(getattr(pipeline_cls, pipe_inst_attr), expected_value)\n\n    def test_file_pipeline_using_pathlike_objects(self):\n        class CustomFilesPipelineWithPathLikeDir(FilesPipeline):\n            def file_path(self, request, response=None, info=None, *, item=None):\n                return Path(\"subdir\") / Path(request.url).name\n\n        pipeline = CustomFilesPipelineWithPathLikeDir.from_settings(\n            Settings({\"FILES_STORE\": Path(\"./Temp\")})\n        )\n        request = Request(\"http://example.com/image01.jpg\")\n        self.assertEqual(pipeline.file_path(request), Path(\"subdir/image01.jpg\"))\n\n    def test_files_store_constructor_with_pathlike_object(self):\n        path = Path(\"./FileDir\")\n        fs_store = FSFilesStore(path)\n        self.assertEqual(fs_store.basedir, str(path))\n\n\nclass TestS3FilesStore(unittest.TestCase):\n    @defer.inlineCallbacks\n    def test_persist(self):\n        skip_if_no_boto()\n\n        bucket = \"mybucket\"\n        key = \"export.csv\"\n        uri = f\"s3://{bucket}/{key}\"\n        buffer = mock.MagicMock()\n        meta = {\"foo\": \"bar\"}\n        path = \"\"\n        content_type = \"image/png\"\n\n        store = S3FilesStore(uri)\n        from botocore.stub import Stubber\n\n        with Stubber(store.s3_client) as stub:\n            stub.add_response(\n                \"put_object\",\n                expected_params={\n                    \"ACL\": S3FilesStore.POLICY,\n                    \"Body\": buffer,\n                    \"Bucket\": bucket,\n                    \"CacheControl\": S3FilesStore.HEADERS[\"Cache-Control\"],\n                    \"ContentType\": content_type,\n                    \"Key\": key,\n                    \"Metadata\": meta,\n                },\n                service_response={},\n            )\n\n            yield store.persist_file(\n                path,\n                buffer,\n                info=None,\n                meta=meta,\n                headers={\"Content-Type\": content_type},\n            )\n\n            stub.assert_no_pending_responses()\n            self.assertEqual(\n                buffer.method_calls,\n                [\n                    mock.call.seek(0),\n                    # The call to read does not happen with Stubber\n                ],\n            )\n\n    @defer.inlineCallbacks\n    def test_stat(self):\n        skip_if_no_boto()\n\n        bucket = \"mybucket\"\n        key = \"export.csv\"\n        uri = f\"s3://{bucket}/{key}\"\n        checksum = \"3187896a9657a28163abb31667df64c8\"\n        last_modified = datetime(2019, 12, 1)\n\n        store = S3FilesStore(uri)\n        from botocore.stub import Stubber\n\n        with Stubber(store.s3_client) as stub:\n            stub.add_response(\n                \"head_object\",\n                expected_params={\n                    \"Bucket\": bucket,\n                    \"Key\": key,\n                },\n                service_response={\n                    \"ETag\": f'\"{checksum}\"',\n                    \"LastModified\": last_modified,\n                },\n            )\n\n            file_stats = yield store.stat_file(\"\", info=None)\n            self.assertEqual(\n                file_stats,\n                {\n                    \"checksum\": checksum,\n                    \"last_modified\": last_modified.timestamp(),\n                },\n            )\n\n            stub.assert_no_pending_responses()\n\n\nclass TestGCSFilesStore(unittest.TestCase):\n    @defer.inlineCallbacks\n    def test_persist(self):\n        assert_gcs_environ()\n        uri = os.environ.get(\"GCS_TEST_FILE_URI\")\n        if not uri:\n            raise unittest.SkipTest(\"No GCS URI available for testing\")\n        data = b\"TestGCSFilesStore: \\xe2\\x98\\x83\"\n        buf = BytesIO(data)\n        meta = {\"foo\": \"bar\"}\n        path = \"full/filename\"\n        store = GCSFilesStore(uri)\n        store.POLICY = \"authenticatedRead\"\n        expected_policy = {\"role\": \"READER\", \"entity\": \"allAuthenticatedUsers\"}\n        yield store.persist_file(path, buf, info=None, meta=meta, headers=None)\n        s = yield store.stat_file(path, info=None)\n        self.assertIn(\"last_modified\", s)\n        self.assertIn(\"checksum\", s)\n        self.assertEqual(s[\"checksum\"], \"cdcda85605e46d0af6110752770dce3c\")\n        u = urlparse(uri)\n        content, acl, blob = get_gcs_content_and_delete(u.hostname, u.path[1:] + path)\n        self.assertEqual(content, data)\n        self.assertEqual(blob.metadata, {\"foo\": \"bar\"})\n        self.assertEqual(blob.cache_control, GCSFilesStore.CACHE_CONTROL)\n        self.assertEqual(blob.content_type, \"application/octet-stream\")\n        self.assertIn(expected_policy, acl)\n\n    @defer.inlineCallbacks\n    def test_blob_path_consistency(self):\n        \"\"\"Test to make sure that paths used to store files is the same as the one used to get\n        already uploaded files.\n        \"\"\"\n        assert_gcs_environ()\n        try:\n            import google.cloud.storage  # noqa\n        except ModuleNotFoundError:\n            raise unittest.SkipTest(\"google-cloud-storage is not installed\")\n        else:\n            with mock.patch(\"google.cloud.storage\") as _:\n                with mock.patch(\"scrapy.pipelines.files.time\") as _:\n                    uri = \"gs://my_bucket/my_prefix/\"\n                    store = GCSFilesStore(uri)\n                    store.bucket = mock.Mock()\n                    path = \"full/my_data.txt\"\n                    yield store.persist_file(\n                        path, mock.Mock(), info=None, meta=None, headers=None\n                    )\n                    yield store.stat_file(path, info=None)\n                    expected_blob_path = store.prefix + path\n                    store.bucket.blob.assert_called_with(expected_blob_path)\n                    store.bucket.get_blob.assert_called_with(expected_blob_path)\n\n\nclass TestFTPFileStore(unittest.TestCase):\n    @defer.inlineCallbacks\n    def test_persist(self):\n        uri = os.environ.get(\"FTP_TEST_FILE_URI\")\n        if not uri:\n            raise unittest.SkipTest(\"No FTP URI available for testing\")\n        data = b\"TestFTPFilesStore: \\xe2\\x98\\x83\"\n        buf = BytesIO(data)\n        meta = {\"foo\": \"bar\"}\n        path = \"full/filename\"\n        store = FTPFilesStore(uri)\n        empty_dict = yield store.stat_file(path, info=None)\n        self.assertEqual(empty_dict, {})\n        yield store.persist_file(path, buf, info=None, meta=meta, headers=None)\n        stat = yield store.stat_file(path, info=None)\n        self.assertIn(\"last_modified\", stat)\n        self.assertIn(\"checksum\", stat)\n        self.assertEqual(stat[\"checksum\"], \"d113d66b2ec7258724a268bd88eef6b6\")\n        path = f\"{store.basedir}/{path}\"\n        content = get_ftp_content_and_delete(\n            path,\n            store.host,\n            store.port,\n            store.username,\n            store.password,\n            store.USE_ACTIVE_MODE,\n        )\n        self.assertEqual(data.decode(), content)\n\n\nclass ItemWithFiles(Item):\n    file_urls = Field()\n    files = Field()\n\n\ndef _create_item_with_files(*files):\n    item = ItemWithFiles()\n    item[\"file_urls\"] = files\n    return item\n\n\ndef _prepare_request_object(item_url, flags=None):\n    return Request(\n        item_url,\n        meta={\"response\": Response(item_url, status=200, body=b\"data\", flags=flags)},\n    )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"
    }
  ],
  "questions": [
    "This [line](https://github.com/scrapy/scrapy/blob/c7730627a0f99afccca11437b0775757c50ca9e8/tests/test_pipeline_files.py#L604) is comparing the checksum to a b64 encoded string. I'm curious where this even came from? And if this is valid, can we not simply extract out the md5 hash from this and replace it here?\r\nThanks!"
  ],
  "golden_answers": [
    "Oh, updating that value to the right expectation should be test enough, nice! I assume the current base is the base64-encoded MD5 of https://github.com/scrapy/scrapy/blob/c7730627a0f99afccca11437b0775757c50ca9e8/tests/test_pipeline_files.py#L593"
  ],
  "questions_generated": [
    "What is the functionality lacking in the Scrapy library as described in the issue?",
    "Which part of the Scrapy codebase is responsible for handling file checksums from GCS?",
    "How can the issue with base64 MD5 checksum decoding be resolved in the Scrapy codebase?",
    "What additional steps must be taken when updating the code to fix the checksum decoding issue?",
    "Why is it important to update the test comparing the checksum to a base64 encoded string in the context of this issue?"
  ],
  "golden_answers_generated": [
    "The issue describes that Scrapy does not decode base64 MD5 checksums from Google Cloud Storage (GCS). This means that when a file is downloaded from GCS, its checksum is provided in base64 format, but Scrapy does not decode it to compare it with the actual checksum of the downloaded file.",
    "The relevant code for handling file checksums from GCS is located in the `scrapy/pipelines/files.py` file. This includes the logic for obtaining and comparing checksums for downloaded files.",
    "The issue can be resolved by changing the line that sets the checksum from `checksum = blob.md5_hash` to `checksum = base64.b64decode(blob.md5_hash).hex()`. This change will decode the base64 MD5 hash to its original hexadecimal format, enabling proper checksum comparison.",
    "In addition to modifying the code to decode the base64 checksum, a test must be added to ensure that the decoding is performed correctly and that the checksum comparison works as expected. Writing and implementing a test might be more challenging than the code change itself.",
    "Updating the test is important because the current test compares the checksum to a base64 encoded string, which may not reflect the correct behavior after implementing the fix. The test needs to verify that the checksum is correctly decoded and compared in its original hexadecimal format to ensure the issue is fully resolved."
  ]
}