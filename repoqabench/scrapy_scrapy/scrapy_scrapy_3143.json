{
  "repo_name": "scrapy_scrapy",
  "issue_id": "3143",
  "issue_description": "# Python 3.7 support\n\nThe goal is to add python 3.7 to travis and pass all tests, the first beta was already released at the end of January.",
  "issue_comments": [
    {
      "id": 368559739,
      "user": "lopuhin",
      "body": "One issue was reported at https://stackoverflow.com/questions/48861287/why-am-i-getting-this-error-in-scrapy-python3-7-invalid-syntax\r\n\r\n```$ scrapy shell http://quotes.toscrape.com/random\r\n2018-02-26 19:11:02 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-02-26 19:11:02 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.3, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.7.0b1 (default, Feb 26 2018, 19:04:22) - [GCC 5.4.0 20160609], pyOpenSSL 17.5.0 (OpenSSL 1.0.2g  1 Mar 2016), cryptography 2.1.4, Platform Linux-4.4.0-116-generic-x86_64-with-debian-stretch-sid\r\n2018-02-26 19:11:02 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'EDITOR': 'vim', 'LOGSTATS_INTERVAL': 0, 'TELNETCONSOLE_ENABLED': '0'}\r\nTraceback (most recent call last):\r\n  File \"/home/kostia/tmp/py37/bin/scrapy\", line 11, in <module>\r\n    sys.exit(execute())\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/cmdline.py\", line 150, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/cmdline.py\", line 157, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/commands/shell.py\", line 65, in run\r\n    crawler = self.crawler_process._create_crawler(spidercls)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/crawler.py\", line 203, in _create_crawler\r\n    return Crawler(spidercls, self.settings)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/crawler.py\", line 55, in __init__\r\n    self.extensions = ExtensionManager.from_crawler(self)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/middleware.py\", line 58, in from_crawler\r\n    return cls.from_settings(crawler.settings, crawler)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/middleware.py\", line 34, in from_settings\r\n    mwcls = load_object(clspath)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/utils/misc.py\", line 44, in load_object\r\n    mod = import_module(module)\r\n  File \"/home/kostia/.pyenv/versions/3.7.0b1/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 723, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/extensions/telnet.py\", line 12, in <module>\r\n    from twisted.conch import manhole, telnet\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/twisted/conch/manhole.py\", line 154\r\n    def write(self, data, async=False):\r\n                              ^\r\nSyntaxError: invalid syntax\r\n```\r\n\r\nThis also happens if  ``-s TELNETCONSOLE_ENABLED=0`` is passed, because the module is still imported. \r\n\r\nTwisted issue: https://twistedmatrix.com/trac/ticket/9384#ticket and PR https://github.com/twisted/twisted/pull/966/"
    },
    {
      "id": 369914800,
      "user": "patiences",
      "body": "Is this an issue that needs someone to work on? I would love to give it a shot. :-) Looks like the PR to fix the above issue should be merged soon. "
    },
    {
      "id": 369917383,
      "user": "lopuhin",
      "body": "@patiences yes, I'm not aware of anyone working on it, so it would be awesome if you give it a shot! And feel free to submit a work in progress PR. I think first step would be adding python 3.7 to tox and travis. There might be other issues besides this syntax error in twisted."
    },
    {
      "id": 369933079,
      "user": "patiences",
      "body": "@lopuhin Great!! The WIP PR is here: https://github.com/scrapy/scrapy/pull/3150 :-) "
    },
    {
      "id": 402935022,
      "user": "qianyinghuanmie",
      "body": "I once thought it supported Python3.7，in other words，  was it support Python3.6-？\r\n"
    },
    {
      "id": 402937963,
      "user": "grammy-jiang",
      "body": "@qianyinghuanmie \r\n\r\nScrapy supports Python 3.6, and Travis has related tests."
    },
    {
      "id": 402938273,
      "user": "illgitthat",
      "body": "It seems like this issue is dependent on https://github.com/twisted/twisted/pull/966 to be merged in first."
    },
    {
      "id": 403319741,
      "user": "jstnms123",
      "body": "I have reopened this issue.  The problem persists.\r\n<<<<<<<\r\njA$ scrapy shell\r\n2018-07-08 14:42:44 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-07-08 14:42:44 [scrapy.utils.log] INFO: Versions: lxml 4.2.3.0, libxml2 2.9.2, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.7.0 (default, Jun 29 2018, 20:13:53) - [Clang 8.0.0 (clang-800.0.42.1)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.6.0-x86_64-i386-64bit\r\n2018-07-08 14:42:44 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/scrapy\", line 11, in <module>\r\n    sys.exit(execute())\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 150, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 157, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/commands/shell.py\", line 65, in run\r\n    crawler = self.crawler_process._create_crawler(spidercls)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/crawler.py\", line 203, in _create_crawler\r\n    return Crawler(spidercls, self.settings)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/crawler.py\", line 55, in __init__\r\n    self.extensions = ExtensionManager.from_crawler(self)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/middleware.py\", line 58, in from_crawler\r\n    return cls.from_settings(crawler.settings, crawler)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/middleware.py\", line 34, in from_settings\r\n    mwcls = load_object(clspath)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/utils/misc.py\", line 44, in load_object\r\n    mod = import_module(module)\r\n  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/extensions/telnet.py\", line 12, in <module>\r\n    from twisted.conch import manhole, telnet\r\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/manhole.py\", line 154\r\n    def write(self, data, async=False):\r\n                              ^\r\nSyntaxError: invalid syntax\r\n\r\n>>>>>>>"
    },
    {
      "id": 403327015,
      "user": "illgitthat",
      "body": "@jstnms123 I don't think this issue was ever closed, just the referenced issue since it was a duplicate.\r\n\r\n@lopuhin do you know of a current workaround to install the branch you created in twisted so scrapy can run with 3.7? (https://github.com/twisted/twisted/pull/966)"
    },
    {
      "id": 403408994,
      "user": "lopuhin",
      "body": "@illgitthat you can install the branch with ``pip install git+https://github.com/lopuhin/twisted.git@9384-remove-async-param``. I hope we'll be able to also provide a work-around in 1.6, and then will hopefully finish the twisted fix (it affects modules that are very rarely used, but scrapy imports then unconditionally)."
    },
    {
      "id": 403482439,
      "user": "illgitthat",
      "body": "Thank you @lopuhin! I googled around but I must have been just off on the syntax."
    },
    {
      "id": 403591570,
      "user": "jstnms123",
      "body": "@lopuhin -- I used the work around to appropriate effect. Thanks for the info.\r\n\r\njA"
    },
    {
      "id": 404067668,
      "user": "threezhang",
      "body": "thanks @lopuhin  \r\n\r\npip install git+https://github.com/lopuhin/twisted.git@9384-remove-async-param"
    },
    {
      "id": 406486529,
      "user": "illgitthat",
      "body": "pip install git+https://github.com/lopuhin/twisted.git@9384-remove-async-param fails for me on Windows but worked on Linux. I think this is related to a twisted issue not scrapy, just leaving this here in case anyone else on Windows is stuck trying to run scrapy on 3.7."
    },
    {
      "id": 406498161,
      "user": "lopuhin",
      "body": "FWIW, thanks to awesome Twisted maintainers, the fix in https://github.com/twisted/twisted/pull/966 was finished and merged into Twisted, so it should be in the next Twisted release.\r\n\r\nAlso a workaround for this issue was merged into scrapy master (and will be in the 1.6 release), so another way to enable python 3.7 support right now, is to install scrapy master and use Twisted from PyPI."
    },
    {
      "id": 406768335,
      "user": "illgitthat",
      "body": "Using `pip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade` is working for me.\r\n\r\nThanks for the help. "
    },
    {
      "id": 408581647,
      "user": "johntiger1",
      "body": "Yup @illgitthat works for me; I think @lopuhin branch might be merged in now, so it's gone "
    },
    {
      "id": 418207515,
      "user": "ghost",
      "body": "Ummm just replace the word async with isAsync the other is deprecated...."
    },
    {
      "id": 418304314,
      "user": "kmike",
      "body": "@joshspivey `async` keyword is used in Twisted, not in Scrapy. @lopuhin worked with Twisted maintainers to fix it in Twisted, so Scrapy will work with Python 3.7 after Twisted release a new version with a fix. Also, we've worked around it in Scrapy itself, so that Scrapy works with the current Twisted release (disabling manhole), this will be available in a next Scrapy release. \"Ummm just replace\" is not how to fix this issue; such comments are not helpful."
    },
    {
      "id": 420058208,
      "user": "wertartem",
      "body": "Go to Python37\\Lib\\site-packages\\twisted\\conch edit \"manhole\" file and replace the 'async' parameter by 'isAsync'."
    },
    {
      "id": 422163400,
      "user": "georgiana-gligor",
      "body": "Because I was just trying this out in a new project, @illgitthat 's solution didn't work for me.\r\nI had to follow all dependencies, so `pip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade` did the trick."
    },
    {
      "id": 422817889,
      "user": "cgironda",
      "body": "Thank you to all of you guys, the command `pip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade` worked for me too."
    },
    {
      "id": 422990661,
      "user": "clockelliptic",
      "body": "This also worked for me. `pip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade`\r\n\r\nThank you very much."
    },
    {
      "id": 423871991,
      "user": "ciehanski",
      "body": "Also adding in that the current fix for this is running `pip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade`\r\n\r\nThank you everyone!"
    },
    {
      "id": 428340825,
      "user": "AsthaSrivastava8",
      "body": "````\r\n2018-10-10 01:56:15 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-10-10 01:56:15 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0\r\n2018-10-10 01:56:15 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}\r\n2018-10-10 01:56:15 [scrapy.middleware] WARNING: Disabled TelnetConsole: TELNETCONSOLE_ENABLED setting is True but required twisted modules failed to import:\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\extensions\\telnet.py\", line 13, in <module>\r\n    from twisted.conch import manhole, telnet\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\twisted\\conch\\manhole.py\", line 154\r\n    def write(self, data, async=False):\r\n                              ^\r\nSyntaxError: invalid syntax\r\n\r\n2018-10-10 01:56:16 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.logstats.LogStats']\r\nUnhandled error in Deferred:\r\n2018-10-10 01:56:16 [twisted] CRITICAL: Unhandled error in Deferred:\r\n\r\n2018-10-10 01:56:16 [twisted] CRITICAL:\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1418, in _inlineCallbacks\r\n    result = g.send(result)\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\crawler.py\", line 80, in crawl\r\n    self.engine = self._create_engine()\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\crawler.py\", line 105, in _create_engine\r\n    return ExecutionEngine(self, lambda _: self.stop())\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\core\\engine.py\", line 69, in __init__\r\n    self.downloader = downloader_cls(crawler)\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\core\\downloader\\__init__.py\", line 88, in __init__\r\n    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\middleware.py\", line 53, in from_crawler\r\n    return cls.from_settings(crawler.settings, crawler)\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\middleware.py\", line 34, in from_settings\r\n    mwcls = load_object(clspath)\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\utils\\misc.py\", line 44, in load_object\r\n    mod = import_module(module)\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\downloadermiddlewares\\retry.py\", line 20, in <module>\r\n    from twisted.web.client import ResponseFailed\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\twisted\\web\\client.py\", line 41, in <module>\r\n    from twisted.internet.endpoints import HostnameEndpoint, wrapClientTLS\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\twisted\\internet\\endpoints.py\", line 41, in <module>\r\n    from twisted.internet.stdio import StandardIO, PipeAddress\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\twisted\\internet\\stdio.py\", line 30, in <module>\r\n    from twisted.internet import _win32stdio\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\twisted\\internet\\_win32stdio.py\", line 9, in <module>\r\n    import win32api\r\nModuleNotFoundError: No module named 'win32api'\r\n````\r\n\r\nGetting error on crawl"
    },
    {
      "id": 429246215,
      "user": "rogerlee6411",
      "body": "same issue as AsthaSrivastava8 post. any solution now ?\r\n\r\n"
    },
    {
      "id": 429534041,
      "user": "appcypher",
      "body": "Installed scrapy via `pipenv` and it installed successfully but whenever I try the `scrapy crawl` command I get a similar error as the one above.\r\n\r\nPython = 3.7.0\r\nPipenv = 2018.7.1 \r\nScrapy = 1.5.1\r\n\r\nIs there a temporary workaround? \r\n"
    },
    {
      "id": 429561657,
      "user": "ghost",
      "body": "@appcypher As far as I understood the developers: Install from the repository directly, because it seems they implemented a workaround which was not released, yet.\r\n\r\nSee above: https://github.com/scrapy/scrapy/issues/3143#issuecomment-422990661"
    },
    {
      "id": 429817922,
      "user": "wmorgue",
      "body": "@appcypher via `pipenv`:\r\n\r\n```bash\r\npipenv install scrapy==1.5.1\r\npipenv shell\r\npip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade\r\n```"
    },
    {
      "id": 432714362,
      "user": "xanderwang",
      "body": "```\r\npip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade\r\n```\r\n\r\nworked for me."
    }
  ],
  "text_context": "# Python 3.7 support\n\nThe goal is to add python 3.7 to travis and pass all tests, the first beta was already released at the end of January.\n\nOne issue was reported at https://stackoverflow.com/questions/48861287/why-am-i-getting-this-error-in-scrapy-python3-7-invalid-syntax\r\n\r\n```$ scrapy shell http://quotes.toscrape.com/random\r\n2018-02-26 19:11:02 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-02-26 19:11:02 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.3, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 3.7.0b1 (default, Feb 26 2018, 19:04:22) - [GCC 5.4.0 20160609], pyOpenSSL 17.5.0 (OpenSSL 1.0.2g  1 Mar 2016), cryptography 2.1.4, Platform Linux-4.4.0-116-generic-x86_64-with-debian-stretch-sid\r\n2018-02-26 19:11:02 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'EDITOR': 'vim', 'LOGSTATS_INTERVAL': 0, 'TELNETCONSOLE_ENABLED': '0'}\r\nTraceback (most recent call last):\r\n  File \"/home/kostia/tmp/py37/bin/scrapy\", line 11, in <module>\r\n    sys.exit(execute())\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/cmdline.py\", line 150, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/cmdline.py\", line 157, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/commands/shell.py\", line 65, in run\r\n    crawler = self.crawler_process._create_crawler(spidercls)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/crawler.py\", line 203, in _create_crawler\r\n    return Crawler(spidercls, self.settings)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/crawler.py\", line 55, in __init__\r\n    self.extensions = ExtensionManager.from_crawler(self)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/middleware.py\", line 58, in from_crawler\r\n    return cls.from_settings(crawler.settings, crawler)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/middleware.py\", line 34, in from_settings\r\n    mwcls = load_object(clspath)\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/utils/misc.py\", line 44, in load_object\r\n    mod = import_module(module)\r\n  File \"/home/kostia/.pyenv/versions/3.7.0b1/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 723, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/scrapy/extensions/telnet.py\", line 12, in <module>\r\n    from twisted.conch import manhole, telnet\r\n  File \"/home/kostia/tmp/py37/lib/python3.7/site-packages/twisted/conch/manhole.py\", line 154\r\n    def write(self, data, async=False):\r\n                              ^\r\nSyntaxError: invalid syntax\r\n```\r\n\r\nThis also happens if  ``-s TELNETCONSOLE_ENABLED=0`` is passed, because the module is still imported. \r\n\r\nTwisted issue: https://twistedmatrix.com/trac/ticket/9384#ticket and PR https://github.com/twisted/twisted/pull/966/\n\nIs this an issue that needs someone to work on? I would love to give it a shot. :-) Looks like the PR to fix the above issue should be merged soon. \n\n@patiences yes, I'm not aware of anyone working on it, so it would be awesome if you give it a shot! And feel free to submit a work in progress PR. I think first step would be adding python 3.7 to tox and travis. There might be other issues besides this syntax error in twisted.\n\n@lopuhin Great!! The WIP PR is here: https://github.com/scrapy/scrapy/pull/3150 :-) \n\nI once thought it supported Python3.7，in other words，  was it support Python3.6-？\r\n\n\n@qianyinghuanmie \r\n\r\nScrapy supports Python 3.6, and Travis has related tests.\n\nIt seems like this issue is dependent on https://github.com/twisted/twisted/pull/966 to be merged in first.\n\nI have reopened this issue.  The problem persists.\r\n<<<<<<<\r\njA$ scrapy shell\r\n2018-07-08 14:42:44 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-07-08 14:42:44 [scrapy.utils.log] INFO: Versions: lxml 4.2.3.0, libxml2 2.9.2, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.7.0 (default, Jun 29 2018, 20:13:53) - [Clang 8.0.0 (clang-800.0.42.1)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.6.0-x86_64-i386-64bit\r\n2018-07-08 14:42:44 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/scrapy\", line 11, in <module>\r\n    sys.exit(execute())\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 150, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 157, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/commands/shell.py\", line 65, in run\r\n    crawler = self.crawler_process._create_crawler(spidercls)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/crawler.py\", line 203, in _create_crawler\r\n    return Crawler(spidercls, self.settings)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/crawler.py\", line 55, in __init__\r\n    self.extensions = ExtensionManager.from_crawler(self)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/middleware.py\", line 58, in from_crawler\r\n    return cls.from_settings(crawler.settings, crawler)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/middleware.py\", line 34, in from_settings\r\n    mwcls = load_object(clspath)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/utils/misc.py\", line 44, in load_object\r\n    mod = import_module(module)\r\n  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/extensions/telnet.py\", line 12, in <module>\r\n    from twisted.conch import manhole, telnet\r\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/manhole.py\", line 154\r\n    def write(self, data, async=False):\r\n                              ^\r\nSyntaxError: invalid syntax\r\n\r\n>>>>>>>\n\n@jstnms123 I don't think this issue was ever closed, just the referenced issue since it was a duplicate.\r\n\r\n@lopuhin do you know of a current workaround to install the branch you created in twisted so scrapy can run with 3.7? (https://github.com/twisted/twisted/pull/966)\n\n@illgitthat you can install the branch with ``pip install git+https://github.com/lopuhin/twisted.git@9384-remove-async-param``. I hope we'll be able to also provide a work-around in 1.6, and then will hopefully finish the twisted fix (it affects modules that are very rarely used, but scrapy imports then unconditionally).\n\nThank you @lopuhin! I googled around but I must have been just off on the syntax.\n\n@lopuhin -- I used the work around to appropriate effect. Thanks for the info.\r\n\r\njA\n\nthanks @lopuhin  \r\n\r\npip install git+https://github.com/lopuhin/twisted.git@9384-remove-async-param\n\npip install git+https://github.com/lopuhin/twisted.git@9384-remove-async-param fails for me on Windows but worked on Linux. I think this is related to a twisted issue not scrapy, just leaving this here in case anyone else on Windows is stuck trying to run scrapy on 3.7.\n\nFWIW, thanks to awesome Twisted maintainers, the fix in https://github.com/twisted/twisted/pull/966 was finished and merged into Twisted, so it should be in the next Twisted release.\r\n\r\nAlso a workaround for this issue was merged into scrapy master (and will be in the 1.6 release), so another way to enable python 3.7 support right now, is to install scrapy master and use Twisted from PyPI.\n\nUsing `pip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade` is working for me.\r\n\r\nThanks for the help. \n\nYup @illgitthat works for me; I think @lopuhin branch might be merged in now, so it's gone \n\nUmmm just replace the word async with isAsync the other is deprecated....\n\n@joshspivey `async` keyword is used in Twisted, not in Scrapy. @lopuhin worked with Twisted maintainers to fix it in Twisted, so Scrapy will work with Python 3.7 after Twisted release a new version with a fix. Also, we've worked around it in Scrapy itself, so that Scrapy works with the current Twisted release (disabling manhole), this will be available in a next Scrapy release. \"Ummm just replace\" is not how to fix this issue; such comments are not helpful.\n\nGo to Python37\\Lib\\site-packages\\twisted\\conch edit \"manhole\" file and replace the 'async' parameter by 'isAsync'.\n\nBecause I was just trying this out in a new project, @illgitthat 's solution didn't work for me.\r\nI had to follow all dependencies, so `pip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade` did the trick.\n\nThank you to all of you guys, the command `pip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade` worked for me too.\n\nThis also worked for me. `pip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade`\r\n\r\nThank you very much.\n\nAlso adding in that the current fix for this is running `pip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade`\r\n\r\nThank you everyone!\n\n````\r\n2018-10-10 01:56:15 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-10-10 01:56:15 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0\r\n2018-10-10 01:56:15 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}\r\n2018-10-10 01:56:15 [scrapy.middleware] WARNING: Disabled TelnetConsole: TELNETCONSOLE_ENABLED setting is True but required twisted modules failed to import:\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\extensions\\telnet.py\", line 13, in <module>\r\n    from twisted.conch import manhole, telnet\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\twisted\\conch\\manhole.py\", line 154\r\n    def write(self, data, async=False):\r\n                              ^\r\nSyntaxError: invalid syntax\r\n\r\n2018-10-10 01:56:16 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.logstats.LogStats']\r\nUnhandled error in Deferred:\r\n2018-10-10 01:56:16 [twisted] CRITICAL: Unhandled error in Deferred:\r\n\r\n2018-10-10 01:56:16 [twisted] CRITICAL:\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1418, in _inlineCallbacks\r\n    result = g.send(result)\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\crawler.py\", line 80, in crawl\r\n    self.engine = self._create_engine()\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\crawler.py\", line 105, in _create_engine\r\n    return ExecutionEngine(self, lambda _: self.stop())\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\core\\engine.py\", line 69, in __init__\r\n    self.downloader = downloader_cls(crawler)\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\core\\downloader\\__init__.py\", line 88, in __init__\r\n    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\middleware.py\", line 53, in from_crawler\r\n    return cls.from_settings(crawler.settings, crawler)\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\middleware.py\", line 34, in from_settings\r\n    mwcls = load_object(clspath)\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\utils\\misc.py\", line 44, in load_object\r\n    mod = import_module(module)\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\downloadermiddlewares\\retry.py\", line 20, in <module>\r\n    from twisted.web.client import ResponseFailed\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\twisted\\web\\client.py\", line 41, in <module>\r\n    from twisted.internet.endpoints import HostnameEndpoint, wrapClientTLS\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\twisted\\internet\\endpoints.py\", line 41, in <module>\r\n    from twisted.internet.stdio import StandardIO, PipeAddress\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\twisted\\internet\\stdio.py\", line 30, in <module>\r\n    from twisted.internet import _win32stdio\r\n  File \"c:\\users\\astha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\twisted\\internet\\_win32stdio.py\", line 9, in <module>\r\n    import win32api\r\nModuleNotFoundError: No module named 'win32api'\r\n````\r\n\r\nGetting error on crawl\n\nsame issue as AsthaSrivastava8 post. any solution now ?\r\n\r\n\n\nInstalled scrapy via `pipenv` and it installed successfully but whenever I try the `scrapy crawl` command I get a similar error as the one above.\r\n\r\nPython = 3.7.0\r\nPipenv = 2018.7.1 \r\nScrapy = 1.5.1\r\n\r\nIs there a temporary workaround? \r\n\n\n@appcypher As far as I understood the developers: Install from the repository directly, because it seems they implemented a workaround which was not released, yet.\r\n\r\nSee above: https://github.com/scrapy/scrapy/issues/3143#issuecomment-422990661\n\n@appcypher via `pipenv`:\r\n\r\n```bash\r\npipenv install scrapy==1.5.1\r\npipenv shell\r\npip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade\r\n```\n\n```\r\npip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade\r\n```\r\n\r\nworked for me.",
  "pr_link": "https://github.com/scrapy/scrapy/pull/3326",
  "code_context": [
    {
      "filename": "scrapy/extensions/telnet.py",
      "content": "\"\"\"\nScrapy Telnet Console extension\n\nSee documentation in docs/topics/telnetconsole.rst\n\"\"\"\n\nimport pprint\nimport logging\nimport traceback\n\nfrom twisted.internet import protocol\ntry:\n    from twisted.conch import manhole, telnet\n    from twisted.conch.insults import insults\n    TWISTED_CONCH_AVAILABLE = True\nexcept (ImportError, SyntaxError):\n    _TWISTED_CONCH_TRACEBACK = traceback.format_exc()\n    TWISTED_CONCH_AVAILABLE = False\n\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy import signals\nfrom scrapy.utils.trackref import print_live_refs\nfrom scrapy.utils.engine import print_engine_status\nfrom scrapy.utils.reactor import listen_tcp\n\ntry:\n    import guppy\n    hpy = guppy.hpy()\nexcept ImportError:\n    hpy = None\n\nlogger = logging.getLogger(__name__)\n\n# signal to update telnet variables\n# args: telnet_vars\nupdate_telnet_vars = object()\n\n\nclass TelnetConsole(protocol.ServerFactory):\n\n    def __init__(self, crawler):\n        if not crawler.settings.getbool('TELNETCONSOLE_ENABLED'):\n            raise NotConfigured\n        if not TWISTED_CONCH_AVAILABLE:\n            raise NotConfigured(\n                'TELNETCONSOLE_ENABLED setting is True but required twisted '\n                'modules failed to import:\\n' + _TWISTED_CONCH_TRACEBACK)\n        self.crawler = crawler\n        self.noisy = False\n        self.portrange = [int(x) for x in crawler.settings.getlist('TELNETCONSOLE_PORT')]\n        self.host = crawler.settings['TELNETCONSOLE_HOST']\n        self.crawler.signals.connect(self.start_listening, signals.engine_started)\n        self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler)\n\n    def start_listening(self):\n        self.port = listen_tcp(self.portrange, self.host, self)\n        h = self.port.getHost()\n        logger.info(\"Telnet console listening on %(host)s:%(port)d\",\n                    {'host': h.host, 'port': h.port},\n                    extra={'crawler': self.crawler})\n\n    def stop_listening(self):\n        self.port.stopListening()\n\n    def protocol(self):\n        telnet_vars = self._get_telnet_vars()\n        return telnet.TelnetTransport(telnet.TelnetBootstrapProtocol,\n            insults.ServerProtocol, manhole.Manhole, telnet_vars)\n\n    def _get_telnet_vars(self):\n        # Note: if you add entries here also update topics/telnetconsole.rst\n        telnet_vars = {\n            'engine': self.crawler.engine,\n            'spider': self.crawler.engine.spider,\n            'slot': self.crawler.engine.slot,\n            'crawler': self.crawler,\n            'extensions': self.crawler.extensions,\n            'stats': self.crawler.stats,\n            'settings': self.crawler.settings,\n            'est': lambda: print_engine_status(self.crawler.engine),\n            'p': pprint.pprint,\n            'prefs': print_live_refs,\n            'hpy': hpy,\n            'help': \"This is Scrapy telnet console. For more info see: \" \\\n                \"https://doc.scrapy.org/en/latest/topics/telnetconsole.html\",\n        }\n        self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n        return telnet_vars\n"
    },
    {
      "filename": "scrapy/utils/iterators.py",
      "content": "import re\nimport csv\nimport logging\ntry:\n    from cStringIO import StringIO as BytesIO\nexcept ImportError:\n    from io import BytesIO\nfrom io import StringIO\nimport six\n\nfrom scrapy.http import TextResponse, Response\nfrom scrapy.selector import Selector\nfrom scrapy.utils.python import re_rsearch, to_unicode\n\nlogger = logging.getLogger(__name__)\n\n\ndef xmliter(obj, nodename):\n    \"\"\"Return a iterator of Selector's over all nodes of a XML document,\n       given the name of the node to iterate. Useful for parsing XML feeds.\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n    \"\"\"\n    nodename_patt = re.escape(nodename)\n\n    HEADER_START_RE = re.compile(r'^(.*?)<\\s*%s(?:\\s|>)' % nodename_patt, re.S)\n    HEADER_END_RE = re.compile(r'<\\s*/%s\\s*>' % nodename_patt, re.S)\n    text = _body_or_str(obj)\n\n    header_start = re.search(HEADER_START_RE, text)\n    header_start = header_start.group(1).strip() if header_start else ''\n    header_end = re_rsearch(HEADER_END_RE, text)\n    header_end = text[header_end[1]:].strip() if header_end else ''\n\n    r = re.compile(r'<%(np)s[\\s>].*?</%(np)s>' % {'np': nodename_patt}, re.DOTALL)\n    for match in r.finditer(text):\n        nodetext = header_start + match.group() + header_end\n        yield Selector(text=nodetext, type='xml').xpath('//' + nodename)[0]\n\n\ndef xmliter_lxml(obj, nodename, namespace=None, prefix='x'):\n    from lxml import etree\n    reader = _StreamReader(obj)\n    tag = '{%s}%s' % (namespace, nodename) if namespace else nodename\n    iterable = etree.iterparse(reader, tag=tag, encoding=reader.encoding)\n    selxpath = '//' + ('%s:%s' % (prefix, nodename) if namespace else nodename)\n    for _, node in iterable:\n        nodetext = etree.tostring(node, encoding='unicode')\n        node.clear()\n        xs = Selector(text=nodetext, type='xml')\n        if namespace:\n            xs.register_namespace(prefix, namespace)\n        yield xs.xpath(selxpath)[0]\n\n\nclass _StreamReader(object):\n\n    def __init__(self, obj):\n        self._ptr = 0\n        if isinstance(obj, Response):\n            self._text, self.encoding = obj.body, obj.encoding\n        else:\n            self._text, self.encoding = obj, 'utf-8'\n        self._is_unicode = isinstance(self._text, six.text_type)\n\n    def read(self, n=65535):\n        self.read = self._read_unicode if self._is_unicode else self._read_string\n        return self.read(n).lstrip()\n\n    def _read_string(self, n=65535):\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return self._text[s:e]\n\n    def _read_unicode(self, n=65535):\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return self._text[s:e].encode('utf-8')\n\n\ndef csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n    \"\"\" Returns an iterator of dictionaries from the given csv object\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n\n    delimiter is the character used to separate fields on the given obj.\n\n    headers is an iterable that when provided offers the keys\n    for the returned dictionaries, if not the first row is used.\n\n    quotechar is the character used to enclosure fields on the given obj.\n    \"\"\"\n\n    encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or 'utf-8'\n\n    def row_to_unicode(row_):\n        return [to_unicode(field, encoding) for field in row_]\n\n    # Python 3 csv reader input object needs to return strings\n    if six.PY3:\n        lines = StringIO(_body_or_str(obj, unicode=True))\n    else:\n        lines = BytesIO(_body_or_str(obj, unicode=False))\n\n    kwargs = {}\n    if delimiter: kwargs[\"delimiter\"] = delimiter\n    if quotechar: kwargs[\"quotechar\"] = quotechar\n    csv_r = csv.reader(lines, **kwargs)\n\n    if not headers:\n        try:\n            row = next(csv_r)\n        except StopIteration:\n            return\n        headers = row_to_unicode(row)\n\n    for row in csv_r:\n        row = row_to_unicode(row)\n        if len(row) != len(headers):\n            logger.warning(\"ignoring row %(csvlnum)d (length: %(csvrow)d, \"\n                           \"should be: %(csvheader)d)\",\n                           {'csvlnum': csv_r.line_num, 'csvrow': len(row),\n                            'csvheader': len(headers)})\n            continue\n        else:\n            yield dict(zip(headers, row))\n\n\ndef _body_or_str(obj, unicode=True):\n    expected_types = (Response, six.text_type, six.binary_type)\n    assert isinstance(obj, expected_types), \\\n        \"obj must be %s, not %s\" % (\n            \" or \".join(t.__name__ for t in expected_types),\n            type(obj).__name__)\n    if isinstance(obj, Response):\n        if not unicode:\n            return obj.body\n        elif isinstance(obj, TextResponse):\n            return obj.text\n        else:\n            return obj.body.decode('utf-8')\n    elif isinstance(obj, six.text_type):\n        return obj if unicode else obj.encode('utf-8')\n    else:\n        return obj.decode('utf-8') if unicode else obj\n"
    },
    {
      "filename": "tests/test_crawler.py",
      "content": "import logging\nimport tempfile\nimport warnings\nimport unittest\n\nfrom twisted.internet import defer\nimport twisted.trial.unittest\n\nimport scrapy\nfrom scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess\nfrom scrapy.settings import Settings, default_settings\nfrom scrapy.spiderloader import SpiderLoader\nfrom scrapy.utils.log import configure_logging, get_scrapy_root_handler\nfrom scrapy.utils.spider import DefaultSpider\nfrom scrapy.utils.misc import load_object\nfrom scrapy.extensions.throttle import AutoThrottle\nfrom scrapy.extensions import telnet\n\n\nclass BaseCrawlerTest(unittest.TestCase):\n\n    def assertOptionIsDefault(self, settings, key):\n        self.assertIsInstance(settings, Settings)\n        self.assertEqual(settings[key], getattr(default_settings, key))\n\n\nclass CrawlerTestCase(BaseCrawlerTest):\n\n    def setUp(self):\n        self.crawler = Crawler(DefaultSpider, Settings())\n\n    def test_deprecated_attribute_spiders(self):\n        with warnings.catch_warnings(record=True) as w:\n            spiders = self.crawler.spiders\n            self.assertEqual(len(w), 1)\n            self.assertIn(\"Crawler.spiders\", str(w[0].message))\n            sl_cls = load_object(self.crawler.settings['SPIDER_LOADER_CLASS'])\n            self.assertIsInstance(spiders, sl_cls)\n\n            self.crawler.spiders\n            self.assertEqual(len(w), 1, \"Warn deprecated access only once\")\n\n    def test_populate_spidercls_settings(self):\n        spider_settings = {'TEST1': 'spider', 'TEST2': 'spider'}\n        project_settings = {'TEST1': 'project', 'TEST3': 'project'}\n\n        class CustomSettingsSpider(DefaultSpider):\n            custom_settings = spider_settings\n\n        settings = Settings()\n        settings.setdict(project_settings, priority='project')\n        crawler = Crawler(CustomSettingsSpider, settings)\n\n        self.assertEqual(crawler.settings.get('TEST1'), 'spider')\n        self.assertEqual(crawler.settings.get('TEST2'), 'spider')\n        self.assertEqual(crawler.settings.get('TEST3'), 'project')\n\n        self.assertFalse(settings.frozen)\n        self.assertTrue(crawler.settings.frozen)\n\n    def test_crawler_accepts_dict(self):\n        crawler = Crawler(DefaultSpider, {'foo': 'bar'})\n        self.assertEqual(crawler.settings['foo'], 'bar')\n        self.assertOptionIsDefault(crawler.settings, 'RETRY_ENABLED')\n\n    def test_crawler_accepts_None(self):\n        crawler = Crawler(DefaultSpider)\n        self.assertOptionIsDefault(crawler.settings, 'RETRY_ENABLED')\n\n\nclass SpiderSettingsTestCase(unittest.TestCase):\n    def test_spider_custom_settings(self):\n        class MySpider(scrapy.Spider):\n            name = 'spider'\n            custom_settings = {\n                'AUTOTHROTTLE_ENABLED': True\n            }\n\n        crawler = Crawler(MySpider, {})\n        enabled_exts = [e.__class__ for e in crawler.extensions.middlewares]\n        self.assertIn(AutoThrottle, enabled_exts)\n\n\nclass CrawlerLoggingTestCase(unittest.TestCase):\n    def test_no_root_handler_installed(self):\n        handler = get_scrapy_root_handler()\n        if handler is not None:\n            logging.root.removeHandler(handler)\n\n        class MySpider(scrapy.Spider):\n            name = 'spider'\n\n        crawler = Crawler(MySpider, {})\n        assert get_scrapy_root_handler() is None\n\n    def test_spider_custom_settings_log_level(self):\n        with tempfile.NamedTemporaryFile() as log_file:\n            class MySpider(scrapy.Spider):\n                name = 'spider'\n                custom_settings = {\n                    'LOG_LEVEL': 'INFO',\n                    'LOG_FILE': log_file.name,\n                    # disable telnet if not available to avoid an extra warning\n                    'TELNETCONSOLE_ENABLED': telnet.TWISTED_CONCH_AVAILABLE,\n                }\n\n            configure_logging()\n            self.assertEqual(get_scrapy_root_handler().level, logging.DEBUG)\n            crawler = Crawler(MySpider, {})\n            self.assertEqual(get_scrapy_root_handler().level, logging.INFO)\n            info_count = crawler.stats.get_value('log_count/INFO')\n            logging.debug('debug message')\n            logging.info('info message')\n            logging.warning('warning message')\n            logging.error('error message')\n            logged = log_file.read().decode('utf8')\n        self.assertNotIn('debug message', logged)\n        self.assertIn('info message', logged)\n        self.assertIn('warning message', logged)\n        self.assertIn('error message', logged)\n        self.assertEqual(crawler.stats.get_value('log_count/ERROR'), 1)\n        self.assertEqual(crawler.stats.get_value('log_count/WARNING'), 1)\n        self.assertEqual(\n            crawler.stats.get_value('log_count/INFO') - info_count, 1)\n        self.assertEqual(crawler.stats.get_value('log_count/DEBUG', 0), 0)\n\n\nclass SpiderLoaderWithWrongInterface(object):\n\n    def unneeded_method(self):\n        pass\n\n\nclass CustomSpiderLoader(SpiderLoader):\n    pass\n\n\nclass CrawlerRunnerTestCase(BaseCrawlerTest):\n\n    def test_spider_manager_verify_interface(self):\n        settings = Settings({\n            'SPIDER_LOADER_CLASS': 'tests.test_crawler.SpiderLoaderWithWrongInterface'\n        })\n        with warnings.catch_warnings(record=True) as w, \\\n                self.assertRaises(AttributeError):\n            CrawlerRunner(settings)\n            self.assertEqual(len(w), 1)\n            self.assertIn(\"SPIDER_LOADER_CLASS\", str(w[0].message))\n            self.assertIn(\"scrapy.interfaces.ISpiderLoader\", str(w[0].message))\n\n    def test_crawler_runner_accepts_dict(self):\n        runner = CrawlerRunner({'foo': 'bar'})\n        self.assertEqual(runner.settings['foo'], 'bar')\n        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')\n\n    def test_crawler_runner_accepts_None(self):\n        runner = CrawlerRunner()\n        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')\n\n    def test_deprecated_attribute_spiders(self):\n        with warnings.catch_warnings(record=True) as w:\n            runner = CrawlerRunner(Settings())\n            spiders = runner.spiders\n            self.assertEqual(len(w), 1)\n            self.assertIn(\"CrawlerRunner.spiders\", str(w[0].message))\n            self.assertIn(\"CrawlerRunner.spider_loader\", str(w[0].message))\n            sl_cls = load_object(runner.settings['SPIDER_LOADER_CLASS'])\n            self.assertIsInstance(spiders, sl_cls)\n\n    def test_spidermanager_deprecation(self):\n        with warnings.catch_warnings(record=True) as w:\n            runner = CrawlerRunner({\n                'SPIDER_MANAGER_CLASS': 'tests.test_crawler.CustomSpiderLoader'\n            })\n            self.assertIsInstance(runner.spider_loader, CustomSpiderLoader)\n            self.assertEqual(len(w), 1)\n            self.assertIn('Please use SPIDER_LOADER_CLASS', str(w[0].message))\n\n\nclass CrawlerProcessTest(BaseCrawlerTest):\n    def test_crawler_process_accepts_dict(self):\n        runner = CrawlerProcess({'foo': 'bar'})\n        self.assertEqual(runner.settings['foo'], 'bar')\n        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')\n\n    def test_crawler_process_accepts_None(self):\n        runner = CrawlerProcess()\n        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')\n\n\nclass ExceptionSpider(scrapy.Spider):\n    name = 'exception'\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        raise ValueError('Exception in from_crawler method')\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = 'no_request'\n\n    def start_requests(self):\n        return []\n\n\nclass CrawlerRunnerHasSpider(twisted.trial.unittest.TestCase):\n\n    @defer.inlineCallbacks\n    def test_crawler_runner_bootstrap_successful(self):\n        runner = CrawlerRunner()\n        yield runner.crawl(NoRequestsSpider)\n        self.assertEqual(runner.bootstrap_failed, False)\n\n    @defer.inlineCallbacks\n    def test_crawler_runner_bootstrap_successful_for_several(self):\n        runner = CrawlerRunner()\n        yield runner.crawl(NoRequestsSpider)\n        yield runner.crawl(NoRequestsSpider)\n        self.assertEqual(runner.bootstrap_failed, False)\n\n    @defer.inlineCallbacks\n    def test_crawler_runner_bootstrap_failed(self):\n        runner = CrawlerRunner()\n\n        try:\n            yield runner.crawl(ExceptionSpider)\n        except ValueError:\n            pass\n        else:\n            self.fail('Exception should be raised from spider')\n\n        self.assertEqual(runner.bootstrap_failed, True)\n\n    @defer.inlineCallbacks\n    def test_crawler_runner_bootstrap_failed_for_several(self):\n        runner = CrawlerRunner()\n\n        try:\n            yield runner.crawl(ExceptionSpider)\n        except ValueError:\n            pass\n        else:\n            self.fail('Exception should be raised from spider')\n\n        yield runner.crawl(NoRequestsSpider)\n\n        self.assertEqual(runner.bootstrap_failed, True)\n"
    },
    {
      "filename": "tests/test_utils_log.py",
      "content": "# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport sys\nimport logging\nimport unittest\n\nfrom testfixtures import LogCapture\nfrom twisted.python.failure import Failure\n\nfrom scrapy.utils.log import (failure_to_exc_info, TopLevelFormatter,\n                              LogCounterHandler, StreamLogger)\nfrom scrapy.utils.test import get_crawler\nfrom scrapy.extensions import telnet\n\n\nclass FailureToExcInfoTest(unittest.TestCase):\n\n    def test_failure(self):\n        try:\n            0/0\n        except ZeroDivisionError:\n            exc_info = sys.exc_info()\n            failure = Failure()\n\n        self.assertTupleEqual(exc_info, failure_to_exc_info(failure))\n\n    def test_non_failure(self):\n        self.assertIsNone(failure_to_exc_info('test'))\n\n\nclass TopLevelFormatterTest(unittest.TestCase):\n\n    def setUp(self):\n        self.handler = LogCapture()\n        self.handler.addFilter(TopLevelFormatter(['test']))\n\n    def test_top_level_logger(self):\n        logger = logging.getLogger('test')\n        with self.handler as l:\n            logger.warning('test log msg')\n\n        l.check(('test', 'WARNING', 'test log msg'))\n\n    def test_children_logger(self):\n        logger = logging.getLogger('test.test1')\n        with self.handler as l:\n            logger.warning('test log msg')\n\n        l.check(('test', 'WARNING', 'test log msg'))\n\n    def test_overlapping_name_logger(self):\n        logger = logging.getLogger('test2')\n        with self.handler as l:\n            logger.warning('test log msg')\n\n        l.check(('test2', 'WARNING', 'test log msg'))\n\n    def test_different_name_logger(self):\n        logger = logging.getLogger('different')\n        with self.handler as l:\n            logger.warning('test log msg')\n\n        l.check(('different', 'WARNING', 'test log msg'))\n\n\nclass LogCounterHandlerTest(unittest.TestCase):\n\n    def setUp(self):\n        settings = {'LOG_LEVEL': 'WARNING'}\n        if not telnet.TWISTED_CONCH_AVAILABLE:\n            # disable it to avoid the extra warning\n            settings['TELNETCONSOLE_ENABLED'] = False\n        self.logger = logging.getLogger('test')\n        self.logger.setLevel(logging.NOTSET)\n        self.logger.propagate = False\n        self.crawler = get_crawler(settings_dict=settings)\n        self.handler = LogCounterHandler(self.crawler)\n        self.logger.addHandler(self.handler)\n\n    def tearDown(self):\n        self.logger.propagate = True\n        self.logger.removeHandler(self.handler)\n\n    def test_init(self):\n        self.assertIsNone(self.crawler.stats.get_value('log_count/DEBUG'))\n        self.assertIsNone(self.crawler.stats.get_value('log_count/INFO'))\n        self.assertIsNone(self.crawler.stats.get_value('log_count/WARNING'))\n        self.assertIsNone(self.crawler.stats.get_value('log_count/ERROR'))\n        self.assertIsNone(self.crawler.stats.get_value('log_count/CRITICAL'))\n\n    def test_accepted_level(self):\n        self.logger.error('test log msg')\n        self.assertEqual(self.crawler.stats.get_value('log_count/ERROR'), 1)\n\n    def test_filtered_out_level(self):\n        self.logger.debug('test log msg')\n        self.assertIsNone(self.crawler.stats.get_value('log_count/INFO'))\n\n\nclass StreamLoggerTest(unittest.TestCase):\n\n    def setUp(self):\n        self.stdout = sys.stdout\n        logger = logging.getLogger('test')\n        logger.setLevel(logging.WARNING)\n        sys.stdout = StreamLogger(logger, logging.ERROR)\n\n    def tearDown(self):\n        sys.stdout = self.stdout\n\n    def test_redirect(self):\n        with LogCapture() as l:\n            print('test log msg')\n        l.check(('test', 'ERROR', 'test log msg'))\n"
    }
  ],
  "questions": [
    "Installed scrapy via `pipenv` and it installed successfully but whenever I try the `scrapy crawl` command I get a similar error as the one above.\r\n\r\nPython = 3.7.0\r\nPipenv = 2018.7.1 \r\nScrapy = 1.5.1\r\n\r\nIs there a temporary workaround?"
  ],
  "golden_answers": [
    "@appcypher via `pipenv`:\r\n\r\n```bash\r\npipenv install scrapy==1.5.1\r\npipenv shell\r\npip install git+https://github.com/scrapy/scrapy@master --no-dependencies --upgrade\r\n```"
  ],
  "questions_generated": [
    "What is the main goal of the issue related to Python 3.7 support in the Scrapy repository?",
    "In the context of the given error log, what role does the 'TelnetConsole' class play in Scrapy?",
    "What could be a potential cause for the 'SyntaxError' when importing the 'telnet' module in the provided traceback?",
    "How does Scrapy handle the situation when Twisted's Conch module is unavailable according to the 'telnet.py' file?",
    "Why is the 'NotConfigured' exception raised in the 'TelnetConsole' class, and what conditions trigger it?"
  ],
  "golden_answers_generated": [
    "The main goal of the issue is to add support for Python 3.7 in the Scrapy repository by updating the Travis CI configuration to include Python 3.7 and ensuring all tests pass successfully.",
    "The 'TelnetConsole' class in Scrapy is responsible for setting up a Telnet console for the crawler, which allows for interactive debugging and monitoring during the crawler's execution. It is implemented as a Twisted protocol server factory and relies on the presence of Twisted's Conch module to function properly.",
    "A potential cause for the 'SyntaxError' when importing the 'telnet' module could be compatibility issues between Twisted and Python 3.7, especially if Twisted was not updated to be compatible with changes introduced in Python 3.7 at the time of the beta release.",
    "When Twisted's Conch module is unavailable, Scrapy sets the 'TWISTED_CONCH_AVAILABLE' flag to False and records the traceback of the ImportError or SyntaxError. If the Telnet console is enabled in the settings but the required modules are not available, an exception is raised indicating that the Telnet console cannot be configured.",
    "The 'NotConfigured' exception is raised in the 'TelnetConsole' class if the Telnet console feature is not enabled in the crawler's settings or if the Twisted Conch modules required for the Telnet functionality are not available. This is to prevent the configuration of a Telnet console when it is not supported by the environment or settings."
  ]
}