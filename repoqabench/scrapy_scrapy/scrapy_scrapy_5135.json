{
  "repo_name": "scrapy_scrapy",
  "issue_id": "5135",
  "issue_description": "# Remove UrlLengthMiddleware from default enabled middlewares\n\nAccording [RFC2396](http://www.faqs.org/rfcs/rfc2396.html), section 3.2.1:\r\n```\r\n   The HTTP protocol does not place any a priori limit on the length of a URI.\r\n   Servers MUST be able to handle the URI of any resource they serve, and\r\n   SHOULD be able to handle URIs of unbounded length if they provide \r\n   GET-based forms that could generate such URIs. A server SHOULD \r\n   return 414 (Request-URI Too Long) status if a URI is longer than the server\r\n   can handle (see section 10.4.15).\r\n```\r\n\r\nWe have enabled by default `scrapy.spidermiddlewares.urllength.UrlLengthMiddleware` that has a default limit defined by `URLLENGTH_LIMIT` setting (that can be modified by in project settings) set to `2083`. As [mentioned here](https://github.com/scrapy/scrapy/pull/5134), the reason for this number is related to limits of Microsoft Internet Explorer to handle URIs longer than that. \r\n\r\nThis can cause problems to spiders that will skip requests of URIs longer than that. Certainly we can change `URLLENGTH_LIMIT` on these spiders, but sometimes is not easy to set the right value and we chose to set a higher number just to make the middleware happy. This is what I am doing in a real world project, but the solution doesn't look good.\r\n\r\nI know that we can or disable the middleware, or change the length limit, but I think it is smoother for the user not to have to worry about this artificial limit we have on Scrapy. We are not using Microsoft Internet Explorer, we don't need this limit.\r\n\r\nSome alternatives that I considered:\r\n\r\n- Remove `UrlLengthMiddleware` as a default enabled middlewares, so we don't need to worry about that limit unless we really need to worry about that (I don't know the exact use-case that required this limit, so keeping the middleware available may make sense);\r\n- Change the default value to a more reasonable (difficult to find a reasonable value)\r\n- Allow `URLLENGTH_LIMIT = -1`, and in this case, ignore the limit. This seems an easier change in the settings than modifying `SPIDER_MIDDLEWARES` setting",
  "issue_comments": [
    {
      "id": 833568953,
      "user": "Gallaecio",
      "body": "I would go for `URLLENGTH_LIMIT = -1` (or `None` or `0`?) and making that the default value, unless we decide to remove the middleware altogether.\r\n\r\nAnd I think we should at least consider removing the middleware altogether unless we can come up with scenarios where this middleware can be useful, and if we do we should mention those in the documentation of the `URLLENGTH_LIMIT` setting."
    },
    {
      "id": 835863308,
      "user": "kmike",
      "body": "MSIE is not the only main reason for having url length limit. Sometimes, when you're doing broad crawls, you can have a website returning links of ever-increasing length, which usually indicates a loop (and sometimes - incorrect link extraction code); url length limit acts as a stopping condition in this case. It also puts some limits on the request size. I'm not sure, maybe that was also useful for data uris (before we had a downloader handler for them), to prevent queues from exploding.\r\n\r\nI'd still consider having some URL length limit a good practice for broad crawls.\r\n\r\n"
    },
    {
      "id": 836304816,
      "user": "Gallaecio",
      "body": "> you can have a website returning links of ever-increasing length, which usually indicates a loop\r\n\r\nDoesn’t that happen through redirects? (i.e. handled by `REDIRECT_MAX_TIMES`) Or are we talking about a website containing ever-increasing links in the HTML of their responses?"
    },
    {
      "id": 836494467,
      "user": "kmike",
      "body": "Yeah, it is about ever-increasing links in HTML responses, or links which could be incorectly built by the client code."
    },
    {
      "id": 836522161,
      "user": "Gallaecio",
      "body": "That could probably be handled by `DEPTH_LIMIT`, but since it is disabled by default, I guess it makes sense to keep `URLLENGTH_LIMIT` set by default.\r\n\r\nShall we simply allow to set `URLLENGTH_LIMIT` to a value that effectively disables the middleware? Any preference? (`-1`, `0`, `None`)."
    },
    {
      "id": 836678049,
      "user": "kmike",
      "body": "> Shall we simply allow to set URLLENGTH_LIMIT to a value that effectively disables the middleware? Any preference? (-1, 0, None).\r\n\r\nYeah, why not? I think we're using 0 for other settings as such value."
    },
    {
      "id": 838234985,
      "user": "Gallaecio",
      "body": "@rennerocha We need to add documentation and tests for it, but know that it turns out the existing code already disables the middleware if you set the setting to `0`."
    },
    {
      "id": 892846796,
      "user": "sidharthkumar2019",
      "body": "I want to contribute. Has this issue been resolved?\r\n"
    },
    {
      "id": 893332924,
      "user": "Gallaecio",
      "body": "@sidharthkumar2019 It hasn’t been resolved, it’s up for the taking.\r\n\r\nThe goal here is to update the documentation of the `URLLENGTH_LIMIT` setting to indicate how it can be disabled and to mention scenarios where it can be useful (to justify it being enabled by default)."
    },
    {
      "id": 930981763,
      "user": "iDeepverma",
      "body": "I suppose this is still open, if so I would like to add to the docs"
    },
    {
      "id": 931163050,
      "user": "Gallaecio",
      "body": "@iDeepverma Feel free! Let us know if you have any question."
    },
    {
      "id": 931394846,
      "user": "iDeepverma",
      "body": "@Gallaecio  Should I Add using ``DEPTH_LIMIT`` to some appropriate value as the recommended way of using it while disabling the ``URLLENGTH_LIMIT`` to avoid loops (which can cause URLs of increasing lengths) as discussed in the above comments  "
    }
  ],
  "text_context": "# Remove UrlLengthMiddleware from default enabled middlewares\n\nAccording [RFC2396](http://www.faqs.org/rfcs/rfc2396.html), section 3.2.1:\r\n```\r\n   The HTTP protocol does not place any a priori limit on the length of a URI.\r\n   Servers MUST be able to handle the URI of any resource they serve, and\r\n   SHOULD be able to handle URIs of unbounded length if they provide \r\n   GET-based forms that could generate such URIs. A server SHOULD \r\n   return 414 (Request-URI Too Long) status if a URI is longer than the server\r\n   can handle (see section 10.4.15).\r\n```\r\n\r\nWe have enabled by default `scrapy.spidermiddlewares.urllength.UrlLengthMiddleware` that has a default limit defined by `URLLENGTH_LIMIT` setting (that can be modified by in project settings) set to `2083`. As [mentioned here](https://github.com/scrapy/scrapy/pull/5134), the reason for this number is related to limits of Microsoft Internet Explorer to handle URIs longer than that. \r\n\r\nThis can cause problems to spiders that will skip requests of URIs longer than that. Certainly we can change `URLLENGTH_LIMIT` on these spiders, but sometimes is not easy to set the right value and we chose to set a higher number just to make the middleware happy. This is what I am doing in a real world project, but the solution doesn't look good.\r\n\r\nI know that we can or disable the middleware, or change the length limit, but I think it is smoother for the user not to have to worry about this artificial limit we have on Scrapy. We are not using Microsoft Internet Explorer, we don't need this limit.\r\n\r\nSome alternatives that I considered:\r\n\r\n- Remove `UrlLengthMiddleware` as a default enabled middlewares, so we don't need to worry about that limit unless we really need to worry about that (I don't know the exact use-case that required this limit, so keeping the middleware available may make sense);\r\n- Change the default value to a more reasonable (difficult to find a reasonable value)\r\n- Allow `URLLENGTH_LIMIT = -1`, and in this case, ignore the limit. This seems an easier change in the settings than modifying `SPIDER_MIDDLEWARES` setting\n\nI would go for `URLLENGTH_LIMIT = -1` (or `None` or `0`?) and making that the default value, unless we decide to remove the middleware altogether.\r\n\r\nAnd I think we should at least consider removing the middleware altogether unless we can come up with scenarios where this middleware can be useful, and if we do we should mention those in the documentation of the `URLLENGTH_LIMIT` setting.\n\nMSIE is not the only main reason for having url length limit. Sometimes, when you're doing broad crawls, you can have a website returning links of ever-increasing length, which usually indicates a loop (and sometimes - incorrect link extraction code); url length limit acts as a stopping condition in this case. It also puts some limits on the request size. I'm not sure, maybe that was also useful for data uris (before we had a downloader handler for them), to prevent queues from exploding.\r\n\r\nI'd still consider having some URL length limit a good practice for broad crawls.\r\n\r\n\n\n> you can have a website returning links of ever-increasing length, which usually indicates a loop\r\n\r\nDoesn’t that happen through redirects? (i.e. handled by `REDIRECT_MAX_TIMES`) Or are we talking about a website containing ever-increasing links in the HTML of their responses?\n\nYeah, it is about ever-increasing links in HTML responses, or links which could be incorectly built by the client code.\n\nThat could probably be handled by `DEPTH_LIMIT`, but since it is disabled by default, I guess it makes sense to keep `URLLENGTH_LIMIT` set by default.\r\n\r\nShall we simply allow to set `URLLENGTH_LIMIT` to a value that effectively disables the middleware? Any preference? (`-1`, `0`, `None`).\n\n> Shall we simply allow to set URLLENGTH_LIMIT to a value that effectively disables the middleware? Any preference? (-1, 0, None).\r\n\r\nYeah, why not? I think we're using 0 for other settings as such value.\n\n@rennerocha We need to add documentation and tests for it, but know that it turns out the existing code already disables the middleware if you set the setting to `0`.\n\nI want to contribute. Has this issue been resolved?\r\n\n\n@sidharthkumar2019 It hasn’t been resolved, it’s up for the taking.\r\n\r\nThe goal here is to update the documentation of the `URLLENGTH_LIMIT` setting to indicate how it can be disabled and to mention scenarios where it can be useful (to justify it being enabled by default).\n\nI suppose this is still open, if so I would like to add to the docs\n\n@iDeepverma Feel free! Let us know if you have any question.\n\n@Gallaecio  Should I Add using ``DEPTH_LIMIT`` to some appropriate value as the recommended way of using it while disabling the ``URLLENGTH_LIMIT`` to avoid loops (which can cause URLs of increasing lengths) as discussed in the above comments  ",
  "pr_link": "https://github.com/scrapy/scrapy/pull/5137",
  "code_context": [
    {
      "filename": "tests/test_spidermiddleware_urllength.py",
      "content": "from unittest import TestCase\n\nfrom testfixtures import LogCapture\n\nfrom scrapy.spidermiddlewares.urllength import UrlLengthMiddleware\nfrom scrapy.http import Response, Request\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.test import get_crawler\nfrom scrapy.settings import Settings\n\n\nclass TestUrlLengthMiddleware(TestCase):\n\n    def setUp(self):\n        self.short_url_req = Request('http://scrapytest.org/')\n        self.long_url_req = Request('http://scrapytest.org/this_is_a_long_url')\n        self.reqs = [self.short_url_req, self.long_url_req]\n        self.response = Response('http://scrapytest.org')\n\n    def generate_middleware_data(self, maxlength):\n        result = dict()\n        self.maxlength = maxlength\n        self.settings = Settings({'URLLENGTH_LIMIT': maxlength})\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider('foo')\n        self.stats = self.crawler.stats\n        self.mw = UrlLengthMiddleware.from_settings(self.settings)\n        return result\n\n    def process_spider_output(self, maxlength):\n        self.generate_middleware_data(maxlength)\n        return list(self.mw.process_spider_output(self.response, self.reqs, self.spider))\n\n    def test_middleware_works(self):\n        self.assertEqual(self.process_spider_output(25), [self.short_url_req])\n\n    def test_middleware_disabled(self):\n        self.assertRaises(NotConfigured, self.process_spider_output, 0)\n\n    def test_logging(self):\n        with LogCapture() as log:\n            self.process_spider_output(25)\n\n        ric = self.stats.get_value('urllength/request_ignored_count', spider=self.spider)\n        self.assertEqual(ric, 1)\n        self.assertIn(f'Ignoring link (url length > {self.maxlength})', str(log))\n"
    }
  ],
  "questions": [
    "I would go for `URLLENGTH_LIMIT = -1` (or `None` or `0`?) and making that the default value, unless we decide to remove the middleware altogether.\r\n\r\nAnd I think we should at least consider removing the middleware altogether unless we can come up with scenarios where this middleware can be useful, and if we do we should mention those in the documentation of the `URLLENGTH_LIMIT` setting.",
    "> you can have a website returning links of ever-increasing length, which usually indicates a loop\r\n\r\nDoesn’t that happen through redirects? (i.e. handled by `REDIRECT_MAX_TIMES`) Or are we talking about a website containing ever-increasing links in the HTML of their responses?",
    "That could probably be handled by `DEPTH_LIMIT`, but since it is disabled by default, I guess it makes sense to keep `URLLENGTH_LIMIT` set by default.\r\n\r\nShall we simply allow to set `URLLENGTH_LIMIT` to a value that effectively disables the middleware? Any preference? (`-1`, `0`, `None`).",
    "> Shall we simply allow to set URLLENGTH_LIMIT to a value that effectively disables the middleware? Any preference? (-1, 0, None).\r\n\r\nYeah, why not? I think we're using 0 for other settings as such value."
  ],
  "golden_answers": [
    "> you can have a website returning links of ever-increasing length, which usually indicates a loop\r\n\r\nDoesn’t that happen through redirects? (i.e. handled by `REDIRECT_MAX_TIMES`) Or are we talking about a website containing ever-increasing links in the HTML of their responses?",
    "That could probably be handled by `DEPTH_LIMIT`, but since it is disabled by default, I guess it makes sense to keep `URLLENGTH_LIMIT` set by default.\r\n\r\nShall we simply allow to set `URLLENGTH_LIMIT` to a value that effectively disables the middleware? Any preference? (`-1`, `0`, `None`).",
    "@rennerocha We need to add documentation and tests for it, but know that it turns out the existing code already disables the middleware if you set the setting to `0`.",
    "@rennerocha We need to add documentation and tests for it, but know that it turns out the existing code already disables the middleware if you set the setting to `0`."
  ],
  "questions_generated": [
    "What is the purpose of the UrlLengthMiddleware in the scrapy_scrapy repository, and why might it be considered for removal?",
    "How does the current implementation of UrlLengthMiddleware determine whether a request should be ignored based on URL length?",
    "What are some potential issues with using a fixed URL length limit in web scraping, as highlighted in the discussion?",
    "What alternatives are suggested to address the limitations of UrlLengthMiddleware, and what are their potential benefits?",
    "In the context of the test cases provided, how does the test_middleware_works method verify the functionality of UrlLengthMiddleware?"
  ],
  "golden_answers_generated": [
    "The UrlLengthMiddleware in the scrapy_scrapy repository is designed to limit the length of URLs processed by Scrapy spiders, with a default limit set by the URLLENGTH_LIMIT setting to 2083 characters. This limit is historically based on Microsoft Internet Explorer's URI length handling capability. However, the middleware is considered for removal because modern web practices do not require such a limit, and it can cause spiders to skip legitimate requests unnecessarily. Removing it would alleviate the need for users to adjust this artificial limit, which is not relevant for most current use cases.",
    "The UrlLengthMiddleware uses the URLLENGTH_LIMIT setting to determine if a request's URL exceeds the specified maximum length. If the URL length is greater than this limit, the middleware processes the spider's output and ignores the request by not including it in the list of requests to be processed further. This logic is implemented in the process_spider_output method, which checks each request's URL against the maxlength parameter derived from the settings.",
    "Using a fixed URL length limit can lead to several issues: 1) It may cause legitimate requests to be ignored if their URLs exceed the arbitrary limit, which is particularly problematic for modern web applications that may use longer URLs. 2) Adjusting the limit can be cumbersome and may lead to setting an unnecessarily high limit just to avoid missing requests. 3) It introduces an unnecessary constraint that users have to manage, despite it being irrelevant for most modern browsers and web practices. 4) It may prevent the detection of legitimate use cases where long URLs are needed, such as in GET-based form submissions.",
    "The suggested alternatives include: 1) Removing UrlLengthMiddleware from the default enabled middlewares, which would eliminate the need to worry about URL length limits unless explicitly needed. 2) Changing the default URLLENGTH_LIMIT to a more reasonable value, although finding a universally reasonable value is challenging. 3) Allowing URLLENGTH_LIMIT to be set to -1 to ignore the limit entirely, making it easier for users to bypass the restriction without altering middleware settings. These alternatives could provide more flexibility and reduce unnecessary constraints for users.",
    "The test_middleware_works method verifies the functionality of the UrlLengthMiddleware by setting a maxlength of 25 and then processing a list of requests with varying URL lengths using process_spider_output. It checks that only requests with URLs shorter than or equal to the specified maxlength are included in the output list, specifically asserting that a short URL request remains while a longer one is omitted. This confirms that the middleware correctly filters out requests based on the defined URL length limit."
  ]
}