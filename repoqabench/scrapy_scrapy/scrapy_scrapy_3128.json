{
  "repo_name": "scrapy_scrapy",
  "issue_id": "3128",
  "issue_description": "# [suggest ] `response.follow` should raise a exception when called on None or an empty string, instead of crawling the current page again\n\n`response.follow` will raise a exception when url='' or none in stead of crawl the (base) page itself again.\r\n\r\nnone will use follow to crawl the source(base) page again right? all parsers will be passed without warning if that way.\r\n\r\nthanks\r\n",
  "issue_comments": [
    {
      "id": 366957402,
      "user": "ghost",
      "body": "Hi @NewUserHa - I re-worded the title, to more clearly match what I _think_ you're suggesting. Can you confirm that I understood you correctly?\r\n\r\nThis does look like an odd behaviour, and I can think of plenty of ways that a `None` could creep in there in normal circumstances, sadly. I don't think this should be the expected behaviour; maybe for special pseudo-URLs like \"#\" or \".\", but not IMO for the empty string or None?"
    },
    {
      "id": 366973078,
      "user": "kmike",
      "body": "If url is '', Scrapy should follow the same page, this is an intended behavior. Doing otherwise would be non-standard, and can break a lot of things.\r\n\r\nWhen is url None? Could you please give an example?"
    },
    {
      "id": 366975046,
      "user": "ghost",
      "body": "Hi @kmike, take this contrived example:\r\n\r\n```ipython\r\nIn [1]: fetch(\"http://httpbin.org/html\")\r\n2018-02-20 13:18:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://httpbin.org/html> (referer: None)\r\n\r\nIn [2]: response.css(\"a[href]::attr(href)\").extract_first() is None\r\nOut[2]: True\r\n\r\nIn [3]: fetch(response.follow(response.css(\"a[href]::attr(href)\").extract_first()))\r\n2018-02-20 13:18:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://httpbin.org/html> (referer: None)\r\n```\r\n\r\nDupefilter should catch these cases, probably. But if Dupefilter is disabled for some reason and custom logic enabled, then it's possible for this to cause unexpected behaviour.\r\n\r\nGood to know about empty-string being expected behaviour."
    },
    {
      "id": 366980942,
      "user": "kmike",
      "body": "Right, I would expect response.follow to raise an exception if None is passed, +1 to fix that. From docs:\r\n\r\n```\r\n        Return a :class:`~.Request` instance to follow a link ``url``.\r\n        It accepts the same arguments as ``Request.__init__`` method,\r\n        but ``url`` can be not only an absolute URL, but also\r\n        \r\n        * a relative URL;\r\n        * a scrapy.link.Link object (e.g. a link extractor result);\r\n        * an attribute Selector (not SelectorList) - e.g.\r\n          ``response.css('a::attr(href)')[0]`` or\r\n          ``response.xpath('//img/@src')[0]``.\r\n        * a Selector for ``<a>`` or ``<link>`` element, e.g.\r\n          ``response.css('a.my_link')[0]``.\r\n```\r\n\r\n^^ None is not one of accepted values."
    },
    {
      "id": 367000689,
      "user": "NewUserHa",
      "body": "@cathalgarvey yeah thanks.\r\n@kmike it may be intended. but in the most situation, I suppose none intense to crawl the current page again just because of extract_first() return an empty string/link. and it's redundancy if check if it's empty every time I believe. ( even if the dupefilter works well, but it will hide the un-except result since few ppl will only check error message beside dupe message. PLUS the doc/tutorial never mention this problem)\r\nthanks if you also fix empty string or note in doc"
    },
    {
      "id": 367004168,
      "user": "ghost",
      "body": "Hey @NewUserHa - are you interested in contributing a fix for the issue? I might also mark it as \"Good First Issue\" otherwise, because I think this should be an easy enough fix:\r\n\r\n1. Add a test case for passing `None` as an argument to `Response.follow`, expect it to throw a TypeError\r\n1. Write code to check for None and throw that error when it happens in `Response.follow`. Or, better still, write a check for all expected types, and throw TypeError on any other type. e.g. `isinstance(url, (str, Selector, Link))`."
    },
    {
      "id": 367018537,
      "user": "kmike",
      "body": "@cathalgarvey currently response.follow throws ValueErrors for bad url argument; it makes sense for unsupported Selectors, though I'm not sure throwing it for SelectorList is the way to go. "
    },
    {
      "id": 367020498,
      "user": "ghost",
      "body": "I assumed from the above paste, that SelectorList is an unsupported Type, so I reckon that's a `TypeError` (even though I see how that might confuse some users because its API is so similar to Selector). But, throwing a ValueError for Selectors that can't reasonably be \"followed\" makes sense.\r\n\r\nThough, that raises questions like \"Should we allow things with `src` attributes to be followed, not just `a[href]` elements?\""
    },
    {
      "id": 367039552,
      "user": "kmike",
      "body": "> Though, that raises questions like \"Should we allow things with src attributes to be followed, not just a[href] elements?\"\r\n\r\nRight - we started conservative, only with `<a>` elements at first, then added `<link>` elements in next Scrapy version. Following anything with `src` requires more discussion, and this discussion haven't started. I can see some disadvantages, e.g. a selector doesn't select `<a>` explicitly, a website uses an image for missing links, and selector selects this image, image URL would be followed instead of a link. It can be also more common to use file or media pipeline to download content linked via src attribute. So ideally, it should start with a use case from a real project, where code can be simplified if response.follow supports selectors with src."
    },
    {
      "id": 367170867,
      "user": "NewUserHa",
      "body": "@cathalgarvey thanks, and I created two pull requests.\r\n#3131 #3132 "
    },
    {
      "id": 367360813,
      "user": "ghost",
      "body": "Hi @NewUserHa - We might as well keep the implementation and the tests together, could you merge one patch into the other patch? Thanks, will review these shortly. :)"
    },
    {
      "id": 367381745,
      "user": "NewUserHa",
      "body": "@cathalgarvey done. #3131 "
    },
    {
      "id": 367501447,
      "user": "kmike",
      "body": "Fixed by https://github.com/scrapy/scrapy/pull/3131."
    },
    {
      "id": 367633259,
      "user": "NewUserHa",
      "body": "hmm, can the empty string url be discussed again?"
    },
    {
      "id": 367635949,
      "user": "kmike",
      "body": "@NewUserHa from my point of view, it is clear that we shouldn't raise an exception for empty URLs in response.follow or response.urljoin. Clicking on a link with empty URL redirects you to the same page in browsers, and joining with an empty URL is a perfectly standard operation. Doing otherwise would be non-standard. \r\n\r\nThe only argument so far against current way of handling \"\" is that response.follow will return a request to the same page. But I see it as an expected behavior, and something which can be desired. For example, you may do a POST request to submit a form to the same page. Or a website may require to re-send a request with different cookies. Or you may be using scrapy-splash, where URL fragment matters, and \"\" is not really the same page as a current page. Or you really want to send the request twice, e.g. to check if page changes. If we start raising an error for an empty string, it'd break all these use cases."
    },
    {
      "id": 367645621,
      "user": "NewUserHa",
      "body": "well, the latter examples do make sense.\r\nalso, the None is the unexcepted behavior reason of what I wanted to say.\r\nthanks"
    },
    {
      "id": 367654288,
      "user": "ghost",
      "body": "Thanks for the report @NewUserHa and for the PR fixing the bug! Sorry that the empty-string behaviour is a problem for you. Scrapy often has to match Browser behaviour, even if that behaviour is a bit odd. I hope you find a clean way to work around the issue in your project."
    },
    {
      "id": 367666159,
      "user": "NewUserHa",
      "body": "oh, I meant \"It's None that the bad guy caused a situation what I wanted to say\"\r\nand thanks for the explanation from the rest part of your reply"
    }
  ],
  "text_context": "# [suggest ] `response.follow` should raise a exception when called on None or an empty string, instead of crawling the current page again\n\n`response.follow` will raise a exception when url='' or none in stead of crawl the (base) page itself again.\r\n\r\nnone will use follow to crawl the source(base) page again right? all parsers will be passed without warning if that way.\r\n\r\nthanks\r\n\n\nHi @NewUserHa - I re-worded the title, to more clearly match what I _think_ you're suggesting. Can you confirm that I understood you correctly?\r\n\r\nThis does look like an odd behaviour, and I can think of plenty of ways that a `None` could creep in there in normal circumstances, sadly. I don't think this should be the expected behaviour; maybe for special pseudo-URLs like \"#\" or \".\", but not IMO for the empty string or None?\n\nIf url is '', Scrapy should follow the same page, this is an intended behavior. Doing otherwise would be non-standard, and can break a lot of things.\r\n\r\nWhen is url None? Could you please give an example?\n\nHi @kmike, take this contrived example:\r\n\r\n```ipython\r\nIn [1]: fetch(\"http://httpbin.org/html\")\r\n2018-02-20 13:18:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://httpbin.org/html> (referer: None)\r\n\r\nIn [2]: response.css(\"a[href]::attr(href)\").extract_first() is None\r\nOut[2]: True\r\n\r\nIn [3]: fetch(response.follow(response.css(\"a[href]::attr(href)\").extract_first()))\r\n2018-02-20 13:18:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://httpbin.org/html> (referer: None)\r\n```\r\n\r\nDupefilter should catch these cases, probably. But if Dupefilter is disabled for some reason and custom logic enabled, then it's possible for this to cause unexpected behaviour.\r\n\r\nGood to know about empty-string being expected behaviour.\n\nRight, I would expect response.follow to raise an exception if None is passed, +1 to fix that. From docs:\r\n\r\n```\r\n        Return a :class:`~.Request` instance to follow a link ``url``.\r\n        It accepts the same arguments as ``Request.__init__`` method,\r\n        but ``url`` can be not only an absolute URL, but also\r\n        \r\n        * a relative URL;\r\n        * a scrapy.link.Link object (e.g. a link extractor result);\r\n        * an attribute Selector (not SelectorList) - e.g.\r\n          ``response.css('a::attr(href)')[0]`` or\r\n          ``response.xpath('//img/@src')[0]``.\r\n        * a Selector for ``<a>`` or ``<link>`` element, e.g.\r\n          ``response.css('a.my_link')[0]``.\r\n```\r\n\r\n^^ None is not one of accepted values.\n\n@cathalgarvey yeah thanks.\r\n@kmike it may be intended. but in the most situation, I suppose none intense to crawl the current page again just because of extract_first() return an empty string/link. and it's redundancy if check if it's empty every time I believe. ( even if the dupefilter works well, but it will hide the un-except result since few ppl will only check error message beside dupe message. PLUS the doc/tutorial never mention this problem)\r\nthanks if you also fix empty string or note in doc\n\nHey @NewUserHa - are you interested in contributing a fix for the issue? I might also mark it as \"Good First Issue\" otherwise, because I think this should be an easy enough fix:\r\n\r\n1. Add a test case for passing `None` as an argument to `Response.follow`, expect it to throw a TypeError\r\n1. Write code to check for None and throw that error when it happens in `Response.follow`. Or, better still, write a check for all expected types, and throw TypeError on any other type. e.g. `isinstance(url, (str, Selector, Link))`.\n\n@cathalgarvey currently response.follow throws ValueErrors for bad url argument; it makes sense for unsupported Selectors, though I'm not sure throwing it for SelectorList is the way to go. \n\nI assumed from the above paste, that SelectorList is an unsupported Type, so I reckon that's a `TypeError` (even though I see how that might confuse some users because its API is so similar to Selector). But, throwing a ValueError for Selectors that can't reasonably be \"followed\" makes sense.\r\n\r\nThough, that raises questions like \"Should we allow things with `src` attributes to be followed, not just `a[href]` elements?\"\n\n> Though, that raises questions like \"Should we allow things with src attributes to be followed, not just a[href] elements?\"\r\n\r\nRight - we started conservative, only with `<a>` elements at first, then added `<link>` elements in next Scrapy version. Following anything with `src` requires more discussion, and this discussion haven't started. I can see some disadvantages, e.g. a selector doesn't select `<a>` explicitly, a website uses an image for missing links, and selector selects this image, image URL would be followed instead of a link. It can be also more common to use file or media pipeline to download content linked via src attribute. So ideally, it should start with a use case from a real project, where code can be simplified if response.follow supports selectors with src.\n\n@cathalgarvey thanks, and I created two pull requests.\r\n#3131 #3132 \n\nHi @NewUserHa - We might as well keep the implementation and the tests together, could you merge one patch into the other patch? Thanks, will review these shortly. :)\n\n@cathalgarvey done. #3131 \n\nFixed by https://github.com/scrapy/scrapy/pull/3131.\n\nhmm, can the empty string url be discussed again?\n\n@NewUserHa from my point of view, it is clear that we shouldn't raise an exception for empty URLs in response.follow or response.urljoin. Clicking on a link with empty URL redirects you to the same page in browsers, and joining with an empty URL is a perfectly standard operation. Doing otherwise would be non-standard. \r\n\r\nThe only argument so far against current way of handling \"\" is that response.follow will return a request to the same page. But I see it as an expected behavior, and something which can be desired. For example, you may do a POST request to submit a form to the same page. Or a website may require to re-send a request with different cookies. Or you may be using scrapy-splash, where URL fragment matters, and \"\" is not really the same page as a current page. Or you really want to send the request twice, e.g. to check if page changes. If we start raising an error for an empty string, it'd break all these use cases.\n\nwell, the latter examples do make sense.\r\nalso, the None is the unexcepted behavior reason of what I wanted to say.\r\nthanks\n\nThanks for the report @NewUserHa and for the PR fixing the bug! Sorry that the empty-string behaviour is a problem for you. Scrapy often has to match Browser behaviour, even if that behaviour is a bit odd. I hope you find a clean way to work around the issue in your project.\n\noh, I meant \"It's None that the bad guy caused a situation what I wanted to say\"\r\nand thanks for the explanation from the rest part of your reply",
  "pr_link": "https://github.com/scrapy/scrapy/pull/3131",
  "code_context": [
    {
      "filename": "scrapy/http/response/__init__.py",
      "content": "\"\"\"\nThis module implements the Response class which is used to represent HTTP\nresponses in Scrapy.\n\nSee documentation in docs/topics/request-response.rst\n\"\"\"\nfrom six.moves.urllib.parse import urljoin\n\nfrom scrapy.http.request import Request\nfrom scrapy.http.headers import Headers\nfrom scrapy.link import Link\nfrom scrapy.utils.trackref import object_ref\nfrom scrapy.http.common import obsolete_setter\nfrom scrapy.exceptions import NotSupported\n\n\nclass Response(object_ref):\n\n    def __init__(self, url, status=200, headers=None, body=b'', flags=None, request=None):\n        self.headers = Headers(headers or {})\n        self.status = int(status)\n        self._set_body(body)\n        self._set_url(url)\n        self.request = request\n        self.flags = [] if flags is None else list(flags)\n\n    @property\n    def meta(self):\n        try:\n            return self.request.meta\n        except AttributeError:\n            raise AttributeError(\n                \"Response.meta not available, this response \"\n                \"is not tied to any request\"\n            )\n\n    def _get_url(self):\n        return self._url\n\n    def _set_url(self, url):\n        if isinstance(url, str):\n            self._url = url\n        else:\n            raise TypeError('%s url must be str, got %s:' % (type(self).__name__,\n                type(url).__name__))\n\n    url = property(_get_url, obsolete_setter(_set_url, 'url'))\n\n    def _get_body(self):\n        return self._body\n\n    def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        elif not isinstance(body, bytes):\n            raise TypeError(\n                \"Response body must be bytes. \"\n                \"If you want to pass unicode body use TextResponse \"\n                \"or HtmlResponse.\")\n        else:\n            self._body = body\n\n    body = property(_get_body, obsolete_setter(_set_body, 'body'))\n\n    def __str__(self):\n        return \"<%d %s>\" % (self.status, self.url)\n\n    __repr__ = __str__\n\n    def copy(self):\n        \"\"\"Return a copy of this Response\"\"\"\n        return self.replace()\n\n    def replace(self, *args, **kwargs):\n        \"\"\"Create a new Response with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in ['url', 'status', 'headers', 'body', 'request', 'flags']:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)\n\n    def urljoin(self, url):\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(self.url, url)\n\n    @property\n    def text(self):\n        \"\"\"For subclasses of TextResponse, this will return the body\n        as text (unicode object in Python 2 and str in Python 3)\n        \"\"\"\n        raise AttributeError(\"Response content isn't text\")\n\n    def css(self, *a, **kw):\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")\n\n    def xpath(self, *a, **kw):\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")\n\n    def follow(self, url, callback=None, method='GET', headers=None, body=None,\n               cookies=None, meta=None, encoding='utf-8', priority=0,\n               dont_filter=False, errback=None):\n        # type: (...) -> Request\n        \"\"\"\n        Return a :class:`~.Request` instance to follow a link ``url``.\n        It accepts the same arguments as ``Request.__init__`` method,\n        but ``url`` can be a relative URL or a ``scrapy.link.Link`` object,\n        not only an absolute URL.\n        \n        :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow` \n        method which supports selectors in addition to absolute/relative URLs\n        and Link objects.\n        \"\"\"\n        if isinstance(url, Link):\n            url = url.url\n        elif url is None:\n            raise ValueError(\"url can't be None\")\n        url = self.urljoin(url)\n        return Request(url, callback,\n                       method=method,\n                       headers=headers,\n                       body=body,\n                       cookies=cookies,\n                       meta=meta,\n                       encoding=encoding,\n                       priority=priority,\n                       dont_filter=dont_filter,\n                       errback=errback)\n"
    },
    {
      "filename": "tests/test_http_response.py",
      "content": "# -*- coding: utf-8 -*-\nimport unittest\n\nimport six\nfrom w3lib.encoding import resolve_encoding\n\nfrom scrapy.http import (Request, Response, TextResponse, HtmlResponse,\n                         XmlResponse, Headers)\nfrom scrapy.selector import Selector\nfrom scrapy.utils.python import to_native_str\nfrom scrapy.exceptions import NotSupported\nfrom scrapy.link import Link\nfrom tests import get_testdata\n\n\nclass BaseResponseTest(unittest.TestCase):\n\n    response_class = Response\n\n    def test_init(self):\n        # Response requires url in the consturctor\n        self.assertRaises(Exception, self.response_class)\n        self.assertTrue(isinstance(self.response_class('http://example.com/'), self.response_class))\n        if not six.PY2:\n            self.assertRaises(TypeError, self.response_class, b\"http://example.com\")\n        # body can be str or None\n        self.assertTrue(isinstance(self.response_class('http://example.com/', body=b''), self.response_class))\n        self.assertTrue(isinstance(self.response_class('http://example.com/', body=b'body'), self.response_class))\n        # test presence of all optional parameters\n        self.assertTrue(isinstance(self.response_class('http://example.com/', body=b'', headers={}, status=200), self.response_class))\n\n        r = self.response_class(\"http://www.example.com\")\n        assert isinstance(r.url, str)\n        self.assertEqual(r.url, \"http://www.example.com\")\n        self.assertEqual(r.status, 200)\n\n        assert isinstance(r.headers, Headers)\n        self.assertEqual(r.headers, {})\n\n        headers = {\"foo\": \"bar\"}\n        body = b\"a body\"\n        r = self.response_class(\"http://www.example.com\", headers=headers, body=body)\n\n        assert r.headers is not headers\n        self.assertEqual(r.headers[b\"foo\"], b\"bar\")\n\n        r = self.response_class(\"http://www.example.com\", status=301)\n        self.assertEqual(r.status, 301)\n        r = self.response_class(\"http://www.example.com\", status='301')\n        self.assertEqual(r.status, 301)\n        self.assertRaises(ValueError, self.response_class, \"http://example.com\", status='lala200')\n\n    def test_copy(self):\n        \"\"\"Test Response copy\"\"\"\n\n        r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\")\n        r1.flags.append('cached')\n        r2 = r1.copy()\n\n        self.assertEqual(r1.status, r2.status)\n        self.assertEqual(r1.body, r2.body)\n\n        # make sure flags list is shallow copied\n        assert r1.flags is not r2.flags, \"flags must be a shallow copy, not identical\"\n        self.assertEqual(r1.flags, r2.flags)\n\n        # make sure headers attribute is shallow copied\n        assert r1.headers is not r2.headers, \"headers must be a shallow copy, not identical\"\n        self.assertEqual(r1.headers, r2.headers)\n\n    def test_copy_meta(self):\n        req = Request(\"http://www.example.com\")\n        req.meta['foo'] = 'bar'\n        r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\", request=req)\n        assert r1.meta is req.meta\n\n    def test_copy_inherited_classes(self):\n        \"\"\"Test Response children copies preserve their class\"\"\"\n\n        class CustomResponse(self.response_class):\n            pass\n\n        r1 = CustomResponse('http://www.example.com')\n        r2 = r1.copy()\n\n        assert type(r2) is CustomResponse\n\n    def test_replace(self):\n        \"\"\"Test Response.replace() method\"\"\"\n        hdrs = Headers({\"key\": \"value\"})\n        r1 = self.response_class(\"http://www.example.com\")\n        r2 = r1.replace(status=301, body=b\"New body\", headers=hdrs)\n        assert r1.body == b''\n        self.assertEqual(r1.url, r2.url)\n        self.assertEqual((r1.status, r2.status), (200, 301))\n        self.assertEqual((r1.body, r2.body), (b'', b\"New body\"))\n        self.assertEqual((r1.headers, r2.headers), ({}, hdrs))\n\n        # Empty attributes (which may fail if not compared properly)\n        r3 = self.response_class(\"http://www.example.com\", flags=['cached'])\n        r4 = r3.replace(body=b'', flags=[])\n        self.assertEqual(r4.body, b'')\n        self.assertEqual(r4.flags, [])\n\n    def _assert_response_values(self, response, encoding, body):\n        if isinstance(body, six.text_type):\n            body_unicode = body\n            body_bytes = body.encode(encoding)\n        else:\n            body_unicode = body.decode(encoding)\n            body_bytes = body\n\n        assert isinstance(response.body, bytes)\n        assert isinstance(response.text, six.text_type)\n        self._assert_response_encoding(response, encoding)\n        self.assertEqual(response.body, body_bytes)\n        self.assertEqual(response.body_as_unicode(), body_unicode)\n        self.assertEqual(response.text, body_unicode)\n\n    def _assert_response_encoding(self, response, encoding):\n        self.assertEqual(response.encoding, resolve_encoding(encoding))\n\n    def test_immutable_attributes(self):\n        r = self.response_class(\"http://example.com\")\n        self.assertRaises(AttributeError, setattr, r, 'url', 'http://example2.com')\n        self.assertRaises(AttributeError, setattr, r, 'body', 'xxx')\n\n    def test_urljoin(self):\n        \"\"\"Test urljoin shortcut (only for existence, since behavior equals urljoin)\"\"\"\n        joined = self.response_class('http://www.example.com').urljoin('/test')\n        absolute = 'http://www.example.com/test'\n        self.assertEqual(joined, absolute)\n\n    def test_shortcut_attributes(self):\n        r = self.response_class(\"http://example.com\", body=b'hello')\n        if self.response_class == Response:\n            msg = \"Response content isn't text\"\n            self.assertRaisesRegexp(AttributeError, msg, getattr, r, 'text')\n            self.assertRaisesRegexp(NotSupported, msg, r.css, 'body')\n            self.assertRaisesRegexp(NotSupported, msg, r.xpath, '//body')\n        else:\n            r.text\n            r.css('body')\n            r.xpath('//body')\n\n    def test_follow_url_absolute(self):\n        self._assert_followed_url('http://foo.example.com',\n                                  'http://foo.example.com')\n\n    def test_follow_url_relative(self):\n        self._assert_followed_url('foo',\n                                  'http://example.com/foo')\n\n    def test_follow_link(self):\n        self._assert_followed_url(Link('http://example.com/foo'),\n                                  'http://example.com/foo')\n\n    def test_follow_None_url(self):\n        r = self.response_class(\"http://example.com\")\n        self.assertRaises(ValueError, r.follow, None)\n\n    def test_follow_whitespace_url(self):\n        self._assert_followed_url('foo ',\n                                  'http://example.com/foo%20')\n\n    def test_follow_whitespace_link(self):\n        self._assert_followed_url(Link('http://example.com/foo '),\n                                  'http://example.com/foo%20')\n    def _assert_followed_url(self, follow_obj, target_url, response=None):\n        if response is None:\n            response = self._links_response()\n        req = response.follow(follow_obj)\n        self.assertEqual(req.url, target_url)\n        return req\n\n    def _links_response(self):\n        body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n        resp = self.response_class('http://example.com/index', body=body)\n        return resp\n\n\nclass TextResponseTest(BaseResponseTest):\n\n    response_class = TextResponse\n\n    def test_replace(self):\n        super(TextResponseTest, self).test_replace()\n        r1 = self.response_class(\"http://www.example.com\", body=\"hello\", encoding=\"cp852\")\n        r2 = r1.replace(url=\"http://www.example.com/other\")\n        r3 = r1.replace(url=\"http://www.example.com/other\", encoding=\"latin1\")\n\n        assert isinstance(r2, self.response_class)\n        self.assertEqual(r2.url, \"http://www.example.com/other\")\n        self._assert_response_encoding(r2, \"cp852\")\n        self.assertEqual(r3.url, \"http://www.example.com/other\")\n        self.assertEqual(r3._declared_encoding(), \"latin1\")\n\n    def test_unicode_url(self):\n        # instantiate with unicode url without encoding (should set default encoding)\n        resp = self.response_class(u\"http://www.example.com/\")\n        self._assert_response_encoding(resp, self.response_class._DEFAULT_ENCODING)\n\n        # make sure urls are converted to str\n        resp = self.response_class(url=u\"http://www.example.com/\", encoding='utf-8')\n        assert isinstance(resp.url, str)\n\n        resp = self.response_class(url=u\"http://www.example.com/price/\\xa3\", encoding='utf-8')\n        self.assertEqual(resp.url, to_native_str(b'http://www.example.com/price/\\xc2\\xa3'))\n        resp = self.response_class(url=u\"http://www.example.com/price/\\xa3\", encoding='latin-1')\n        self.assertEqual(resp.url, 'http://www.example.com/price/\\xa3')\n        resp = self.response_class(u\"http://www.example.com/price/\\xa3\", headers={\"Content-type\": [\"text/html; charset=utf-8\"]})\n        self.assertEqual(resp.url, to_native_str(b'http://www.example.com/price/\\xc2\\xa3'))\n        resp = self.response_class(u\"http://www.example.com/price/\\xa3\", headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]})\n        self.assertEqual(resp.url, 'http://www.example.com/price/\\xa3')\n\n    def test_unicode_body(self):\n        unicode_string = u'\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0442\\u0435\\u043a\\u0441\\u0442'\n        self.assertRaises(TypeError, self.response_class, 'http://www.example.com', body=u'unicode body')\n\n        original_string = unicode_string.encode('cp1251')\n        r1 = self.response_class('http://www.example.com', body=original_string, encoding='cp1251')\n\n        # check body_as_unicode\n        self.assertTrue(isinstance(r1.body_as_unicode(), six.text_type))\n        self.assertEqual(r1.body_as_unicode(), unicode_string)\n\n        # check response.text\n        self.assertTrue(isinstance(r1.text, six.text_type))\n        self.assertEqual(r1.text, unicode_string)\n\n    def test_encoding(self):\n        r1 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=utf-8\"]}, body=b\"\\xc2\\xa3\")\n        r2 = self.response_class(\"http://www.example.com\", encoding='utf-8', body=u\"\\xa3\")\n        r3 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]}, body=b\"\\xa3\")\n        r4 = self.response_class(\"http://www.example.com\", body=b\"\\xa2\\xa3\")\n        r5 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=None\"]}, body=b\"\\xc2\\xa3\")\n        r6 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=gb2312\"]}, body=b\"\\xa8D\")\n        r7 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=gbk\"]}, body=b\"\\xa8D\")\n\n        self.assertEqual(r1._headers_encoding(), \"utf-8\")\n        self.assertEqual(r2._headers_encoding(), None)\n        self.assertEqual(r2._declared_encoding(), 'utf-8')\n        self._assert_response_encoding(r2, 'utf-8')\n        self.assertEqual(r3._headers_encoding(), \"cp1252\")\n        self.assertEqual(r3._declared_encoding(), \"cp1252\")\n        self.assertEqual(r4._headers_encoding(), None)\n        self.assertEqual(r5._headers_encoding(), None)\n        self._assert_response_encoding(r5, \"utf-8\")\n        assert r4._body_inferred_encoding() is not None and r4._body_inferred_encoding() != 'ascii'\n        self._assert_response_values(r1, 'utf-8', u\"\\xa3\")\n        self._assert_response_values(r2, 'utf-8', u\"\\xa3\")\n        self._assert_response_values(r3, 'iso-8859-1', u\"\\xa3\")\n        self._assert_response_values(r6, 'gb18030', u\"\\u2015\")\n        self._assert_response_values(r7, 'gb18030', u\"\\u2015\")\n\n        # TextResponse (and subclasses) must be passed a encoding when instantiating with unicode bodies\n        self.assertRaises(TypeError, self.response_class, \"http://www.example.com\", body=u\"\\xa3\")\n\n    def test_declared_encoding_invalid(self):\n        \"\"\"Check that unknown declared encodings are ignored\"\"\"\n        r = self.response_class(\"http://www.example.com\",\n                                headers={\"Content-type\": [\"text/html; charset=UKNOWN\"]},\n                                body=b\"\\xc2\\xa3\")\n        self.assertEqual(r._declared_encoding(), None)\n        self._assert_response_values(r, 'utf-8', u\"\\xa3\")\n\n    def test_utf16(self):\n        \"\"\"Test utf-16 because UnicodeDammit is known to have problems with\"\"\"\n        r = self.response_class(\"http://www.example.com\",\n                                body=b'\\xff\\xfeh\\x00i\\x00',\n                                encoding='utf-16')\n        self._assert_response_values(r, 'utf-16', u\"hi\")\n\n    def test_invalid_utf8_encoded_body_with_valid_utf8_BOM(self):\n        r6 = self.response_class(\"http://www.example.com\",\n                                 headers={\"Content-type\": [\"text/html; charset=utf-8\"]},\n                                 body=b\"\\xef\\xbb\\xbfWORD\\xe3\\xab\")\n        self.assertEqual(r6.encoding, 'utf-8')\n        self.assertIn(r6.text, {\n            u'WORD\\ufffd\\ufffd',  # w3lib < 1.19.0\n            u'WORD\\ufffd',        # w3lib >= 1.19.0\n        })\n\n    def test_bom_is_removed_from_body(self):\n        # Inferring encoding from body also cache decoded body as sideeffect,\n        # this test tries to ensure that calling response.encoding and\n        # response.text in indistint order doesn't affect final\n        # values for encoding and decoded body.\n        url = 'http://example.com'\n        body = b\"\\xef\\xbb\\xbfWORD\"\n        headers = {\"Content-type\": [\"text/html; charset=utf-8\"]}\n\n        # Test response without content-type and BOM encoding\n        response = self.response_class(url, body=body)\n        self.assertEqual(response.encoding, 'utf-8')\n        self.assertEqual(response.text, u'WORD')\n        response = self.response_class(url, body=body)\n        self.assertEqual(response.text, u'WORD')\n        self.assertEqual(response.encoding, 'utf-8')\n\n        # Body caching sideeffect isn't triggered when encoding is declared in\n        # content-type header but BOM still need to be removed from decoded\n        # body\n        response = self.response_class(url, headers=headers, body=body)\n        self.assertEqual(response.encoding, 'utf-8')\n        self.assertEqual(response.text, u'WORD')\n        response = self.response_class(url, headers=headers, body=body)\n        self.assertEqual(response.text, u'WORD')\n        self.assertEqual(response.encoding, 'utf-8')\n\n    def test_replace_wrong_encoding(self):\n        \"\"\"Test invalid chars are replaced properly\"\"\"\n        r = self.response_class(\"http://www.example.com\", encoding='utf-8', body=b'PREFIX\\xe3\\xabSUFFIX')\n        # XXX: Policy for replacing invalid chars may suffer minor variations\n        # but it should always contain the unicode replacement char (u'\\ufffd')\n        assert u'\\ufffd' in r.text, repr(r.text)\n        assert u'PREFIX' in r.text, repr(r.text)\n        assert u'SUFFIX' in r.text, repr(r.text)\n\n        # Do not destroy html tags due to encoding bugs\n        r = self.response_class(\"http://example.com\", encoding='utf-8', \\\n                body=b'\\xf0<span>value</span>')\n        assert u'<span>value</span>' in r.text, repr(r.text)\n\n        # FIXME: This test should pass once we stop using BeautifulSoup's UnicodeDammit in TextResponse\n        #r = self.response_class(\"http://www.example.com\", body=b'PREFIX\\xe3\\xabSUFFIX')\n        #assert u'\\ufffd' in r.text, repr(r.text)\n\n    def test_selector(self):\n        body = b\"<html><head><title>Some page</title><body></body></html>\"\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        self.assertIsInstance(response.selector, Selector)\n        self.assertEqual(response.selector.type, 'html')\n        self.assertIs(response.selector, response.selector)  # property is cached\n        self.assertIs(response.selector.response, response)\n\n        self.assertEqual(\n            response.selector.xpath(\"//title/text()\").extract(),\n            [u'Some page']\n        )\n        self.assertEqual(\n            response.selector.css(\"title::text\").extract(),\n            [u'Some page']\n        )\n        self.assertEqual(\n            response.selector.re(\"Some (.*)</title>\"),\n            [u'page']\n        )\n\n    def test_selector_shortcuts(self):\n        body = b\"<html><head><title>Some page</title><body></body></html>\"\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        self.assertEqual(\n            response.xpath(\"//title/text()\").extract(),\n            response.selector.xpath(\"//title/text()\").extract(),\n        )\n        self.assertEqual(\n            response.css(\"title::text\").extract(),\n            response.selector.css(\"title::text\").extract(),\n        )\n\n    def test_selector_shortcuts_kwargs(self):\n        body = b\"<html><head><title>Some page</title><body><p class=\\\"content\\\">A nice paragraph.</p></body></html>\"\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        self.assertEqual(\n            response.xpath(\"normalize-space(//p[@class=$pclass])\", pclass=\"content\").extract(),\n            response.xpath(\"normalize-space(//p[@class=\\\"content\\\"])\").extract(),\n        )\n        self.assertEqual(\n            response.xpath(\"//title[count(following::p[@class=$pclass])=$pcount]/text()\",\n                pclass=\"content\", pcount=1).extract(),\n            response.xpath(\"//title[count(following::p[@class=\\\"content\\\"])=1]/text()\").extract(),\n        )\n\n    def test_urljoin_with_base_url(self):\n        \"\"\"Test urljoin shortcut which also evaluates base-url through get_base_url().\"\"\"\n        body = b'<html><body><base href=\"https://example.net\"></body></html>'\n        joined = self.response_class('http://www.example.com', body=body).urljoin('/test')\n        absolute = 'https://example.net/test'\n        self.assertEqual(joined, absolute)\n\n        body = b'<html><body><base href=\"/elsewhere\"></body></html>'\n        joined = self.response_class('http://www.example.com', body=body).urljoin('test')\n        absolute = 'http://www.example.com/test'\n        self.assertEqual(joined, absolute)\n\n        body = b'<html><body><base href=\"/elsewhere/\"></body></html>'\n        joined = self.response_class('http://www.example.com', body=body).urljoin('test')\n        absolute = 'http://www.example.com/elsewhere/test'\n        self.assertEqual(joined, absolute)\n\n    def test_follow_selector(self):\n        resp = self._links_response()\n        urls = [\n            'http://example.com/sample2.html',\n            'http://example.com/sample3.html',\n            'http://example.com/sample3.html',\n            'http://example.com/sample3.html#foo',\n            'http://www.google.com/something',\n            'http://example.com/innertag.html'\n        ]\n\n        # select <a> elements\n        for sellist in [resp.css('a'), resp.xpath('//a')]:\n            for sel, url in zip(sellist, urls):\n                self._assert_followed_url(sel, url, response=resp)\n\n        # select <link> elements\n        self._assert_followed_url(\n            Selector(text='<link href=\"foo\"></link>').css('link')[0],\n            'http://example.com/foo',\n            response=resp\n        )\n\n        # href attributes should work\n        for sellist in [resp.css('a::attr(href)'), resp.xpath('//a/@href')]:\n            for sel, url in zip(sellist, urls):\n                self._assert_followed_url(sel, url, response=resp)\n\n        # non-a elements are not supported\n        self.assertRaises(ValueError, resp.follow, resp.css('div')[0])\n\n    def test_follow_selector_list(self):\n        resp = self._links_response()\n        self.assertRaisesRegexp(ValueError, 'SelectorList',\n                                resp.follow, resp.css('a'))\n\n    def test_follow_selector_invalid(self):\n        resp = self._links_response()\n        self.assertRaisesRegexp(ValueError, 'Unsupported',\n                                resp.follow, resp.xpath('count(//div)')[0])\n\n    def test_follow_selector_attribute(self):\n        resp = self._links_response()\n        for src in resp.css('img::attr(src)'):\n            self._assert_followed_url(src, 'http://example.com/sample2.jpg')\n\n    def test_follow_selector_no_href(self):\n        resp = self.response_class(\n            url='http://example.com',\n            body=b'<html><body><a name=123>click me</a></body></html>',\n        )\n        self.assertRaisesRegexp(ValueError, 'no href',\n                                resp.follow, resp.css('a')[0])\n\n    def test_follow_whitespace_selector(self):\n        resp = self.response_class(\n            'http://example.com',\n            body=b'''<html><body><a href=\" foo\\n\">click me</a></body></html>'''\n        )\n        self._assert_followed_url(resp.css('a')[0],\n                                 'http://example.com/foo',\n                                  response=resp)\n        self._assert_followed_url(resp.css('a::attr(href)')[0],\n                                 'http://example.com/foo',\n                                  response=resp)\n\n    def test_follow_encoding(self):\n        resp1 = self.response_class(\n            'http://example.com',\n            encoding='utf8',\n            body=u'<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode('utf8')\n        )\n        req = self._assert_followed_url(\n            resp1.css('a')[0],\n            'http://example.com/foo?%D0%BF%D1%80%D0%B8%D0%B2%D0%B5%D1%82',\n            response=resp1,\n        )\n        self.assertEqual(req.encoding, 'utf8')\n\n        resp2 = self.response_class(\n            'http://example.com',\n            encoding='cp1251',\n            body=u'<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode('cp1251')\n        )\n        req = self._assert_followed_url(\n            resp2.css('a')[0],\n            'http://example.com/foo?%EF%F0%E8%E2%E5%F2',\n            response=resp2,\n        )\n        self.assertEqual(req.encoding, 'cp1251')\n\n\nclass HtmlResponseTest(TextResponseTest):\n\n    response_class = HtmlResponse\n\n    def test_html_encoding(self):\n\n        body = b\"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">\n        </head><body>Price: \\xa3100</body></html>'\n        \"\"\"\n        r1 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r1, 'iso-8859-1', body)\n\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\n        <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\">\n        Price: \\xa3100\n        \"\"\"\n        r2 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r2, 'iso-8859-1', body)\n\n        # for conflicting declarations headers must take precedence\n        body = b\"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n        </head><body>Price: \\xa3100</body></html>'\n        \"\"\"\n        r3 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]}, body=body)\n        self._assert_response_values(r3, 'iso-8859-1', body)\n\n        # make sure replace() preserves the encoding of the original response\n        body = b\"New body \\xa3\"\n        r4 = r3.replace(body=body)\n        self._assert_response_values(r4, 'iso-8859-1', body)\n\n    def test_html5_meta_charset(self):\n        body = b\"\"\"<html><head><meta charset=\"gb2312\" /><title>Some page</title><body>bla bla</body>\"\"\"\n        r1 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r1, 'gb2312', body)\n\n\nclass XmlResponseTest(TextResponseTest):\n\n    response_class = XmlResponse\n\n    def test_xml_encoding(self):\n        body = b\"<xml></xml>\"\n        r1 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r1, self.response_class._DEFAULT_ENCODING, body)\n\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n        r2 = self.response_class(\"http://www.example.com\", body=body)\n        self._assert_response_values(r2, 'iso-8859-1', body)\n\n        # make sure replace() preserves the explicit encoding passed in the constructor\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n        r3 = self.response_class(\"http://www.example.com\", body=body, encoding='utf-8')\n        body2 = b\"New body\"\n        r4 = r3.replace(body=body2)\n        self._assert_response_values(r4, 'utf-8', body2)\n\n    def test_replace_encoding(self):\n        # make sure replace() keeps the previous encoding unless overridden explicitly\n        body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n        body2 = b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><xml></xml>\"\"\"\n        r5 = self.response_class(\"http://www.example.com\", body=body)\n        r6 = r5.replace(body=body2)\n        r7 = r5.replace(body=body2, encoding='utf-8')\n        self._assert_response_values(r5, 'iso-8859-1', body)\n        self._assert_response_values(r6, 'iso-8859-1', body2)\n        self._assert_response_values(r7, 'utf-8', body2)\n\n    def test_selector(self):\n        body = b'<?xml version=\"1.0\" encoding=\"utf-8\"?><xml><elem>value</elem></xml>'\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        self.assertIsInstance(response.selector, Selector)\n        self.assertEqual(response.selector.type, 'xml')\n        self.assertIs(response.selector, response.selector)  # property is cached\n        self.assertIs(response.selector.response, response)\n\n        self.assertEqual(\n            response.selector.xpath(\"//elem/text()\").extract(),\n            [u'value']\n        )\n\n    def test_selector_shortcuts(self):\n        body = b'<?xml version=\"1.0\" encoding=\"utf-8\"?><xml><elem>value</elem></xml>'\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        self.assertEqual(\n            response.xpath(\"//elem/text()\").extract(),\n            response.selector.xpath(\"//elem/text()\").extract(),\n        )\n\n    def test_selector_shortcuts_kwargs(self):\n        body = b'''<?xml version=\"1.0\" encoding=\"utf-8\"?>\n        <xml xmlns:somens=\"http://scrapy.org\">\n        <somens:elem>value</somens:elem>\n        </xml>'''\n        response = self.response_class(\"http://www.example.com\", body=body)\n\n        self.assertEqual(\n            response.xpath(\"//s:elem/text()\", namespaces={'s': 'http://scrapy.org'}).extract(),\n            response.selector.xpath(\"//s:elem/text()\", namespaces={'s': 'http://scrapy.org'}).extract(),\n        )\n\n        response.selector.register_namespace('s2', 'http://scrapy.org')\n        self.assertEqual(\n            response.xpath(\"//s1:elem/text()\", namespaces={'s1': 'http://scrapy.org'}).extract(),\n            response.selector.xpath(\"//s2:elem/text()\").extract(),\n        )\n"
    }
  ],
  "questions": [
    "Hi @NewUserHa - I re-worded the title, to more clearly match what I _think_ you're suggesting. Can you confirm that I understood you correctly?\r\n\r\nThis does look like an odd behaviour, and I can think of plenty of ways that a `None` could creep in there in normal circumstances, sadly. I don't think this should be the expected behaviour; maybe for special pseudo-URLs like \"#\" or \".\", but not IMO for the empty string or None?",
    "If url is '', Scrapy should follow the same page, this is an intended behavior. Doing otherwise would be non-standard, and can break a lot of things.\r\n\r\nWhen is url None? Could you please give an example?",
    "Hey @NewUserHa - are you interested in contributing a fix for the issue? I might also mark it as \"Good First Issue\" otherwise, because I think this should be an easy enough fix:\r\n\r\n1. Add a test case for passing `None` as an argument to `Response.follow`, expect it to throw a TypeError\r\n1. Write code to check for None and throw that error when it happens in `Response.follow`. Or, better still, write a check for all expected types, and throw TypeError on any other type. e.g. `isinstance(url, (str, Selector, Link))`.",
    "I assumed from the above paste, that SelectorList is an unsupported Type, so I reckon that's a `TypeError` (even though I see how that might confuse some users because its API is so similar to Selector). But, throwing a ValueError for Selectors that can't reasonably be \"followed\" makes sense.\r\n\r\nThough, that raises questions like \"Should we allow things with `src` attributes to be followed, not just `a[href]` elements?\"",
    "> Though, that raises questions like \"Should we allow things with src attributes to be followed, not just a[href] elements?\"\r\n\r\nRight - we started conservative, only with `<a>` elements at first, then added `<link>` elements in next Scrapy version. Following anything with `src` requires more discussion, and this discussion haven't started. I can see some disadvantages, e.g. a selector doesn't select `<a>` explicitly, a website uses an image for missing links, and selector selects this image, image URL would be followed instead of a link. It can be also more common to use file or media pipeline to download content linked via src attribute. So ideally, it should start with a use case from a real project, where code can be simplified if response.follow supports selectors with src.",
    "`response.follow` will raise a exception when url='' or none in stead of crawl the (base) page itself again.\r\n\r\nnone will use follow to crawl the source(base) page again right? all parsers will be passed without warning if that way.\r\n\r\nthanks"
  ],
  "golden_answers": [
    "Hi @kmike, take this contrived example:\r\n\r\n```ipython\r\nIn [1]: fetch(\"http://httpbin.org/html\")\r\n2018-02-20 13:18:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://httpbin.org/html> (referer: None)\r\n\r\nIn [2]: response.css(\"a[href]::attr(href)\").extract_first() is None\r\nOut[2]: True\r\n\r\nIn [3]: fetch(response.follow(response.css(\"a[href]::attr(href)\").extract_first()))\r\n2018-02-20 13:18:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://httpbin.org/html> (referer: None)\r\n```\r\n\r\nDupefilter should catch these cases, probably. But if Dupefilter is disabled for some reason and custom logic enabled, then it's possible for this to cause unexpected behaviour.\r\n\r\nGood to know about empty-string being expected behaviour.",
    "Hi @kmike, take this contrived example:\r\n\r\n```ipython\r\nIn [1]: fetch(\"http://httpbin.org/html\")\r\n2018-02-20 13:18:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://httpbin.org/html> (referer: None)\r\n\r\nIn [2]: response.css(\"a[href]::attr(href)\").extract_first() is None\r\nOut[2]: True\r\n\r\nIn [3]: fetch(response.follow(response.css(\"a[href]::attr(href)\").extract_first()))\r\n2018-02-20 13:18:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://httpbin.org/html> (referer: None)\r\n```\r\n\r\nDupefilter should catch these cases, probably. But if Dupefilter is disabled for some reason and custom logic enabled, then it's possible for this to cause unexpected behaviour.\r\n\r\nGood to know about empty-string being expected behaviour.",
    "I assumed from the above paste, that SelectorList is an unsupported Type, so I reckon that's a `TypeError` (even though I see how that might confuse some users because its API is so similar to Selector). But, throwing a ValueError for Selectors that can't reasonably be \"followed\" makes sense.\r\n\r\nThough, that raises questions like \"Should we allow things with `src` attributes to be followed, not just `a[href]` elements?\"",
    "> Though, that raises questions like \"Should we allow things with src attributes to be followed, not just a[href] elements?\"\r\n\r\nRight - we started conservative, only with `<a>` elements at first, then added `<link>` elements in next Scrapy version. Following anything with `src` requires more discussion, and this discussion haven't started. I can see some disadvantages, e.g. a selector doesn't select `<a>` explicitly, a website uses an image for missing links, and selector selects this image, image URL would be followed instead of a link. It can be also more common to use file or media pipeline to download content linked via src attribute. So ideally, it should start with a use case from a real project, where code can be simplified if response.follow supports selectors with src.",
    "Hi @NewUserHa - We might as well keep the implementation and the tests together, could you merge one patch into the other patch? Thanks, will review these shortly. :)",
    "Hi @NewUserHa - I re-worded the title, to more clearly match what I _think_ you're suggesting. Can you confirm that I understood you correctly?\r\n\r\nThis does look like an odd behaviour, and I can think of plenty of ways that a `None` could creep in there in normal circumstances, sadly. I don't think this should be the expected behaviour; maybe for special pseudo-URLs like \"#\" or \".\", but not IMO for the empty string or None?"
  ],
  "questions_generated": [
    "What is the expected behavior of the `response.follow` method when the URL is an empty string?",
    "Why might it be problematic for `response.follow` to handle a `None` URL by crawling the current page again?",
    "How does the `Response.__init__` method handle the `url` parameter, and what type checks are performed on it?",
    "What are some accepted types of URLs that the `response.follow` method can handle, according to the documentation?",
    "What does the `Response.meta` property do, and what exception might it raise under certain conditions?",
    "What is the community's opinion on whether `response.follow` should raise an exception for `None` URLs, and what is the rationale behind it?"
  ],
  "golden_answers_generated": [
    "When the URL is an empty string, the expected behavior of `response.follow` is to follow the same page, as this is considered standard behavior in Scrapy.",
    "Handling a `None` URL by crawling the current page again can lead to unexpected behavior, especially if `None` values are unintentionally passed in normal circumstances. This could result in unnecessary duplicate requests and potential issues if dupefilter is disabled or custom logic is implemented.",
    "The `Response.__init__` method sets the URL using the `_set_url` method, which checks if the URL is a string. If it's not a string, a TypeError is raised indicating that the URL must be of type `str`.",
    "According to the documentation, `response.follow` can handle absolute URLs, relative URLs, `scrapy.link.Link` objects, attribute Selectors, and Selectors for `<a>` or `<link>` elements. However, `None` is not an accepted value.",
    "The `Response.meta` property attempts to return the `meta` attribute from the associated request. If the response is not tied to any request, it raises an `AttributeError` indicating that `Response.meta` is not available.",
    "The community, as represented in the discussion, generally agrees that `response.follow` should raise an exception for `None` URLs. The rationale is that passing `None` is not part of the accepted URL types and handling it by crawling the current page can lead to unintended and redundant behaviors, which might not be immediately evident if dupefilter hides these cases."
  ]
}