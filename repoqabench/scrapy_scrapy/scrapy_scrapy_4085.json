{
  "repo_name": "scrapy_scrapy",
  "issue_id": "4085",
  "issue_description": "# Make Python 3.8 support official\n\nThis involves, among other things:\r\n- Enabling a Python 3.8 environment in Tox\r\n- Enabling a Python 3.8 job in the Travis CI\r\n- Updating the `setup.py` file",
  "issue_comments": [
    {
      "id": 544469286,
      "user": "wRAR",
      "body": "Does Travis already support it?"
    },
    {
      "id": 544477651,
      "user": "wRAR",
      "body": "According to https://docs.travis-ci.com/user/languages/python/ it does"
    },
    {
      "id": 544582304,
      "user": "illgitthat",
      "body": "I am trying to install scrapy for python 3.8 in a Windows environment. Are there currently any ways I can do this? Or is this blocked upstream by conda https://github.com/conda/conda/issues/9343"
    },
    {
      "id": 544584488,
      "user": "Gallaecio",
      "body": "> I am trying to install scrapy for python 3.8 in a Windows environment. Are there currently any ways I can do this? Or is this blocked upstream by conda [conda/conda#9343](https://github.com/conda/conda/issues/9343)\r\n\r\nYou can try using `pip`. If you cannot get it to work with `pip`, then you’ll need to wait for Conda to support Python 3.8."
    },
    {
      "id": 544585222,
      "user": "further-reading",
      "body": "Hey! I'll grab this one if it is okay?"
    },
    {
      "id": 544586752,
      "user": "illgitthat",
      "body": "> > I am trying to install scrapy for python 3.8 in a Windows environment. Are there currently any ways I can do this? Or is this blocked upstream by conda [conda/conda#9343](https://github.com/conda/conda/issues/9343)\r\n> \r\n> You can try using `pip`. If you cannot get it to work with `pip`, then you’ll need to wait for Conda to support Python 3.8.\r\n\r\nNo luck using just `pip install scrapy` unfortunately."
    },
    {
      "id": 546925409,
      "user": "asciidiego",
      "body": "Installing Python 3.8.0 and immediately later `scrapy` fails. \r\n\r\nHere's the commands used:\r\n\r\n```bash\r\npip install python=3.8.0\r\npip install scrapy\r\n```\r\n\r\nMy workstation: `4.19.79-1-MANJARO GNU/Linux`\r\n\r\nThe log itself: https://pastebin.com/cAawd2jB"
    },
    {
      "id": 546926901,
      "user": "wRAR",
      "body": "@diegovincent this is not related to Scrapy, the log shows a problem with compiling lxml and other libraries."
    },
    {
      "id": 546935805,
      "user": "asciidiego",
      "body": "@wRAR But why would that happen? is `lxml` not \"ready\" for 3.8? But after all, scrapy depends on the `lxml` library, so it might be a good idea to have it here?"
    },
    {
      "id": 546938087,
      "user": "wRAR",
      "body": "@diegovincent your compiler toolchain seems broken, and no, this is not a good place to discuss problems with installing third-party modules."
    },
    {
      "id": 546940199,
      "user": "asciidiego",
      "body": "I used Anaconda. \r\n\r\nAnd ok, I won't discuss it anymore here, but FYI the error seems to persist even after [`conda` officially supports 3.8.0](https://anaconda.org/anaconda/python)."
    },
    {
      "id": 546946160,
      "user": "Gallaecio",
      "body": "> I won't discuss it anymore here\r\n\r\nFeel free to create a separate issue about it.\r\n"
    }
  ],
  "text_context": "# Make Python 3.8 support official\n\nThis involves, among other things:\r\n- Enabling a Python 3.8 environment in Tox\r\n- Enabling a Python 3.8 job in the Travis CI\r\n- Updating the `setup.py` file\n\nDoes Travis already support it?\n\nAccording to https://docs.travis-ci.com/user/languages/python/ it does\n\nI am trying to install scrapy for python 3.8 in a Windows environment. Are there currently any ways I can do this? Or is this blocked upstream by conda https://github.com/conda/conda/issues/9343\n\n> I am trying to install scrapy for python 3.8 in a Windows environment. Are there currently any ways I can do this? Or is this blocked upstream by conda [conda/conda#9343](https://github.com/conda/conda/issues/9343)\r\n\r\nYou can try using `pip`. If you cannot get it to work with `pip`, then you’ll need to wait for Conda to support Python 3.8.\n\nHey! I'll grab this one if it is okay?\n\n> > I am trying to install scrapy for python 3.8 in a Windows environment. Are there currently any ways I can do this? Or is this blocked upstream by conda [conda/conda#9343](https://github.com/conda/conda/issues/9343)\r\n> \r\n> You can try using `pip`. If you cannot get it to work with `pip`, then you’ll need to wait for Conda to support Python 3.8.\r\n\r\nNo luck using just `pip install scrapy` unfortunately.\n\nInstalling Python 3.8.0 and immediately later `scrapy` fails. \r\n\r\nHere's the commands used:\r\n\r\n```bash\r\npip install python=3.8.0\r\npip install scrapy\r\n```\r\n\r\nMy workstation: `4.19.79-1-MANJARO GNU/Linux`\r\n\r\nThe log itself: https://pastebin.com/cAawd2jB\n\n@diegovincent this is not related to Scrapy, the log shows a problem with compiling lxml and other libraries.\n\n@wRAR But why would that happen? is `lxml` not \"ready\" for 3.8? But after all, scrapy depends on the `lxml` library, so it might be a good idea to have it here?\n\n@diegovincent your compiler toolchain seems broken, and no, this is not a good place to discuss problems with installing third-party modules.\n\nI used Anaconda. \r\n\r\nAnd ok, I won't discuss it anymore here, but FYI the error seems to persist even after [`conda` officially supports 3.8.0](https://anaconda.org/anaconda/python).\n\n> I won't discuss it anymore here\r\n\r\nFeel free to create a separate issue about it.\r\n",
  "pr_link": "https://github.com/scrapy/scrapy/pull/4092",
  "code_context": [
    {
      "filename": "scrapy/extensions/httpcache.py",
      "content": "from __future__ import print_function\n\nimport gzip\nimport logging\nimport os\nfrom email.utils import mktime_tz, parsedate_tz\nfrom importlib import import_module\nfrom time import time\nfrom warnings import warn\nfrom weakref import WeakKeyDictionary\n\nfrom six.moves import cPickle as pickle\nfrom w3lib.http import headers_raw_to_dict, headers_dict_to_raw\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.http import Headers, Response\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.project import data_path\nfrom scrapy.utils.python import to_bytes, to_unicode, garbage_collect\nfrom scrapy.utils.request import request_fingerprint\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass DummyPolicy(object):\n\n    def __init__(self, settings):\n        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n        self.ignore_http_codes = [int(x) for x in settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES')]\n\n    def should_cache_request(self, request):\n        return urlparse_cached(request).scheme not in self.ignore_schemes\n\n    def should_cache_response(self, response, request):\n        return response.status not in self.ignore_http_codes\n\n    def is_cached_response_fresh(self, cachedresponse, request):\n        return True\n\n    def is_cached_response_valid(self, cachedresponse, response, request):\n        return True\n\n\nclass RFC2616Policy(object):\n\n    MAXAGE = 3600 * 24 * 365  # one year\n\n    def __init__(self, settings):\n        self.always_store = settings.getbool('HTTPCACHE_ALWAYS_STORE')\n        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n        self.ignore_response_cache_controls = [to_bytes(cc) for cc in\n            settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS')]\n        self._cc_parsed = WeakKeyDictionary()\n\n    def _parse_cachecontrol(self, r):\n        if r not in self._cc_parsed:\n            cch = r.headers.get(b'Cache-Control', b'')\n            parsed = parse_cachecontrol(cch)\n            if isinstance(r, Response):\n                for key in self.ignore_response_cache_controls:\n                    parsed.pop(key, None)\n            self._cc_parsed[r] = parsed\n        return self._cc_parsed[r]\n\n    def should_cache_request(self, request):\n        if urlparse_cached(request).scheme in self.ignore_schemes:\n            return False\n        cc = self._parse_cachecontrol(request)\n        # obey user-agent directive \"Cache-Control: no-store\"\n        if b'no-store' in cc:\n            return False\n        # Any other is eligible for caching\n        return True\n\n    def should_cache_response(self, response, request):\n        # What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1\n        # Response cacheability - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4\n        # Status code 206 is not included because cache can not deal with partial contents\n        cc = self._parse_cachecontrol(response)\n        # obey directive \"Cache-Control: no-store\"\n        if b'no-store' in cc:\n            return False\n        # Never cache 304 (Not Modified) responses\n        elif response.status == 304:\n            return False\n        # Cache unconditionally if configured to do so\n        elif self.always_store:\n            return True\n        # Any hint on response expiration is good\n        elif b'max-age' in cc or b'Expires' in response.headers:\n            return True\n        # Firefox fallbacks this statuses to one year expiration if none is set\n        elif response.status in (300, 301, 308):\n            return True\n        # Other statuses without expiration requires at least one validator\n        elif response.status in (200, 203, 401):\n            return b'Last-Modified' in response.headers or b'ETag' in response.headers\n        # Any other is probably not eligible for caching\n        # Makes no sense to cache responses that does not contain expiration\n        # info and can not be revalidated\n        else:\n            return False\n\n    def is_cached_response_fresh(self, cachedresponse, request):\n        cc = self._parse_cachecontrol(cachedresponse)\n        ccreq = self._parse_cachecontrol(request)\n        if b'no-cache' in cc or b'no-cache' in ccreq:\n            return False\n\n        now = time()\n        freshnesslifetime = self._compute_freshness_lifetime(cachedresponse, request, now)\n        currentage = self._compute_current_age(cachedresponse, request, now)\n\n        reqmaxage = self._get_max_age(ccreq)\n        if reqmaxage is not None:\n            freshnesslifetime = min(freshnesslifetime, reqmaxage)\n\n        if currentage < freshnesslifetime:\n            return True\n\n        if b'max-stale' in ccreq and b'must-revalidate' not in cc:\n            # From RFC2616: \"Indicates that the client is willing to\n            # accept a response that has exceeded its expiration time.\n            # If max-stale is assigned a value, then the client is\n            # willing to accept a response that has exceeded its\n            # expiration time by no more than the specified number of\n            # seconds. If no value is assigned to max-stale, then the\n            # client is willing to accept a stale response of any age.\"\n            staleage = ccreq[b'max-stale']\n            if staleage is None:\n                return True\n\n            try:\n                if currentage < freshnesslifetime + max(0, int(staleage)):\n                    return True\n            except ValueError:\n                pass\n\n        # Cached response is stale, try to set validators if any\n        self._set_conditional_validators(request, cachedresponse)\n        return False\n\n    def is_cached_response_valid(self, cachedresponse, response, request):\n        # Use the cached response if the new response is a server error,\n        # as long as the old response didn't specify must-revalidate.\n        if response.status >= 500:\n            cc = self._parse_cachecontrol(cachedresponse)\n            if b'must-revalidate' not in cc:\n                return True\n\n        # Use the cached response if the server says it hasn't changed.\n        return response.status == 304\n\n    def _set_conditional_validators(self, request, cachedresponse):\n        if b'Last-Modified' in cachedresponse.headers:\n            request.headers[b'If-Modified-Since'] = cachedresponse.headers[b'Last-Modified']\n\n        if b'ETag' in cachedresponse.headers:\n            request.headers[b'If-None-Match'] = cachedresponse.headers[b'ETag']\n\n    def _get_max_age(self, cc):\n        try:\n            return max(0, int(cc[b'max-age']))\n        except (KeyError, ValueError):\n            return None\n\n    def _compute_freshness_lifetime(self, response, request, now):\n        # Reference nsHttpResponseHead::ComputeFreshnessLifetime\n        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#706\n        cc = self._parse_cachecontrol(response)\n        maxage = self._get_max_age(cc)\n        if maxage is not None:\n            return maxage\n\n        # Parse date header or synthesize it if none exists\n        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now\n\n        # Try HTTP/1.0 Expires header\n        if b'Expires' in response.headers:\n            expires = rfc1123_to_epoch(response.headers[b'Expires'])\n            # When parsing Expires header fails RFC 2616 section 14.21 says we\n            # should treat this as an expiration time in the past.\n            return max(0, expires - date) if expires else 0\n\n        # Fallback to heuristic using last-modified header\n        # This is not in RFC but on Firefox caching implementation\n        lastmodified = rfc1123_to_epoch(response.headers.get(b'Last-Modified'))\n        if lastmodified and lastmodified <= date:\n            return (date - lastmodified) / 10\n\n        # This request can be cached indefinitely\n        if response.status in (300, 301, 308):\n            return self.MAXAGE\n\n        # Insufficient information to compute fresshness lifetime\n        return 0\n\n    def _compute_current_age(self, response, request, now):\n        # Reference nsHttpResponseHead::ComputeCurrentAge\n        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#658\n        currentage = 0\n        # If Date header is not set we assume it is a fast connection, and\n        # clock is in sync with the server\n        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now\n        if now > date:\n            currentage = now - date\n\n        if b'Age' in response.headers:\n            try:\n                age = int(response.headers[b'Age'])\n                currentage = max(currentage, age)\n            except ValueError:\n                pass\n\n        return currentage\n\n\nclass DbmCacheStorage(object):\n\n    def __init__(self, settings):\n        self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n        self.dbmodule = import_module(settings['HTTPCACHE_DBM_MODULE'])\n        self.db = None\n\n    def open_spider(self, spider):\n        dbpath = os.path.join(self.cachedir, '%s.db' % spider.name)\n        self.db = self.dbmodule.open(dbpath, 'c')\n\n        logger.debug(\"Using DBM cache storage in %(cachepath)s\" % {'cachepath': dbpath}, extra={'spider': spider})\n\n    def close_spider(self, spider):\n        self.db.close()\n\n    def retrieve_response(self, spider, request):\n        data = self._read_data(spider, request)\n        if data is None:\n            return  # not cached\n        url = data['url']\n        status = data['status']\n        headers = Headers(data['headers'])\n        body = data['body']\n        respcls = responsetypes.from_args(headers=headers, url=url)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response\n\n    def store_response(self, spider, request, response):\n        key = self._request_key(request)\n        data = {\n            'status': response.status,\n            'url': response.url,\n            'headers': dict(response.headers),\n            'body': response.body,\n        }\n        self.db['%s_data' % key] = pickle.dumps(data, protocol=2)\n        self.db['%s_time' % key] = str(time())\n\n    def _read_data(self, spider, request):\n        key = self._request_key(request)\n        db = self.db\n        tkey = '%s_time' % key\n        if tkey not in db:\n            return  # not found\n\n        ts = db[tkey]\n        if 0 < self.expiration_secs < time() - float(ts):\n            return  # expired\n\n        return pickle.loads(db['%s_data' % key])\n\n    def _request_key(self, request):\n        return request_fingerprint(request)\n\n\nclass FilesystemCacheStorage(object):\n\n    def __init__(self, settings):\n        self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n        self.use_gzip = settings.getbool('HTTPCACHE_GZIP')\n        self._open = gzip.open if self.use_gzip else open\n\n    def open_spider(self, spider):\n        logger.debug(\"Using filesystem cache storage in %(cachedir)s\" % {'cachedir': self.cachedir},\n                     extra={'spider': spider})\n\n    def close_spider(self, spider):\n        pass\n\n    def retrieve_response(self, spider, request):\n        \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n        metadata = self._read_meta(spider, request)\n        if metadata is None:\n            return  # not cached\n        rpath = self._get_request_path(spider, request)\n        with self._open(os.path.join(rpath, 'response_body'), 'rb') as f:\n            body = f.read()\n        with self._open(os.path.join(rpath, 'response_headers'), 'rb') as f:\n            rawheaders = f.read()\n        url = metadata.get('response_url')\n        status = metadata['status']\n        headers = Headers(headers_raw_to_dict(rawheaders))\n        respcls = responsetypes.from_args(headers=headers, url=url)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response\n\n    def store_response(self, spider, request, response):\n        \"\"\"Store the given response in the cache.\"\"\"\n        rpath = self._get_request_path(spider, request)\n        if not os.path.exists(rpath):\n            os.makedirs(rpath)\n        metadata = {\n            'url': request.url,\n            'method': request.method,\n            'status': response.status,\n            'response_url': response.url,\n            'timestamp': time(),\n        }\n        with self._open(os.path.join(rpath, 'meta'), 'wb') as f:\n            f.write(to_bytes(repr(metadata)))\n        with self._open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:\n            pickle.dump(metadata, f, protocol=2)\n        with self._open(os.path.join(rpath, 'response_headers'), 'wb') as f:\n            f.write(headers_dict_to_raw(response.headers))\n        with self._open(os.path.join(rpath, 'response_body'), 'wb') as f:\n            f.write(response.body)\n        with self._open(os.path.join(rpath, 'request_headers'), 'wb') as f:\n            f.write(headers_dict_to_raw(request.headers))\n        with self._open(os.path.join(rpath, 'request_body'), 'wb') as f:\n            f.write(request.body)\n\n    def _get_request_path(self, spider, request):\n        key = request_fingerprint(request)\n        return os.path.join(self.cachedir, spider.name, key[0:2], key)\n\n    def _read_meta(self, spider, request):\n        rpath = self._get_request_path(spider, request)\n        metapath = os.path.join(rpath, 'pickled_meta')\n        if not os.path.exists(metapath):\n            return  # not found\n        mtime = os.stat(metapath).st_mtime\n        if 0 < self.expiration_secs < time() - mtime:\n            return  # expired\n        with self._open(metapath, 'rb') as f:\n            return pickle.load(f)\n\n\nclass LeveldbCacheStorage(object):\n\n    def __init__(self, settings):\n        warn(\"The LevelDB storage backend is deprecated.\",\n             ScrapyDeprecationWarning, stacklevel=2)\n        import leveldb\n        self._leveldb = leveldb\n        self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n        self.db = None\n\n    def open_spider(self, spider):\n        dbpath = os.path.join(self.cachedir, '%s.leveldb' % spider.name)\n        self.db = self._leveldb.LevelDB(dbpath)\n\n        logger.debug(\"Using LevelDB cache storage in %(cachepath)s\" % {'cachepath': dbpath}, extra={'spider': spider})\n\n    def close_spider(self, spider):\n        # Do compactation each time to save space and also recreate files to\n        # avoid them being removed in storages with timestamp-based autoremoval.\n        self.db.CompactRange()\n        del self.db\n        garbage_collect()\n\n    def retrieve_response(self, spider, request):\n        data = self._read_data(spider, request)\n        if data is None:\n            return  # not cached\n        url = data['url']\n        status = data['status']\n        headers = Headers(data['headers'])\n        body = data['body']\n        respcls = responsetypes.from_args(headers=headers, url=url)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response\n\n    def store_response(self, spider, request, response):\n        key = self._request_key(request)\n        data = {\n            'status': response.status,\n            'url': response.url,\n            'headers': dict(response.headers),\n            'body': response.body,\n        }\n        batch = self._leveldb.WriteBatch()\n        batch.Put(key + b'_data', pickle.dumps(data, protocol=2))\n        batch.Put(key + b'_time', to_bytes(str(time())))\n        self.db.Write(batch)\n\n    def _read_data(self, spider, request):\n        key = self._request_key(request)\n        try:\n            ts = self.db.Get(key + b'_time')\n        except KeyError:\n            return  # not found or invalid entry\n\n        if 0 < self.expiration_secs < time() - float(ts):\n            return  # expired\n\n        try:\n            data = self.db.Get(key + b'_data')\n        except KeyError:\n            return  # invalid entry\n        else:\n            return pickle.loads(data)\n\n    def _request_key(self, request):\n        return to_bytes(request_fingerprint(request))\n\n\n\ndef parse_cachecontrol(header):\n    \"\"\"Parse Cache-Control header\n\n    https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n\n    >>> parse_cachecontrol(b'public, max-age=3600') == {b'public': None,\n    ...                                                 b'max-age': b'3600'}\n    True\n    >>> parse_cachecontrol(b'') == {}\n    True\n\n    \"\"\"\n    directives = {}\n    for directive in header.split(b','):\n        key, sep, val = directive.strip().partition(b'=')\n        if key:\n            directives[key.lower()] = val if sep else None\n    return directives\n\n\ndef rfc1123_to_epoch(date_str):\n    try:\n        date_str = to_unicode(date_str, encoding='ascii')\n        return mktime_tz(parsedate_tz(date_str))\n    except Exception:\n        return None\n"
    },
    {
      "filename": "setup.py",
      "content": "from os.path import dirname, join\nfrom pkg_resources import parse_version\nfrom setuptools import setup, find_packages, __version__ as setuptools_version\n\n\nwith open(join(dirname(__file__), 'scrapy/VERSION'), 'rb') as f:\n    version = f.read().decode('ascii').strip()\n\n\ndef has_environment_marker_platform_impl_support():\n    \"\"\"Code extracted from 'pytest/setup.py'\n    https://github.com/pytest-dev/pytest/blob/7538680c/setup.py#L31\n\n    The first known release to support environment marker with range operators\n    it is 18.5, see:\n    https://setuptools.readthedocs.io/en/latest/history.html#id235\n    \"\"\"\n    return parse_version(setuptools_version) >= parse_version('18.5')\n\n\nextras_require = {}\n\nif has_environment_marker_platform_impl_support():\n    extras_require[':platform_python_implementation == \"PyPy\"'] = [\n        'PyPyDispatcher>=2.1.0',\n    ]\n\n\nsetup(\n    name='Scrapy',\n    version=version,\n    url='https://scrapy.org',\n    description='A high-level Web Crawling and Web Scraping framework',\n    long_description=open('README.rst').read(),\n    author='Scrapy developers',\n    maintainer='Pablo Hoffman',\n    maintainer_email='pablo@pablohoffman.com',\n    license='BSD',\n    packages=find_packages(exclude=('tests', 'tests.*')),\n    include_package_data=True,\n    zip_safe=False,\n    entry_points={\n        'console_scripts': ['scrapy = scrapy.cmdline:execute']\n    },\n    classifiers=[\n        'Framework :: Scrapy',\n        'Development Status :: 5 - Production/Stable',\n        'Environment :: Console',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: BSD License',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: Implementation :: CPython',\n        'Programming Language :: Python :: Implementation :: PyPy',\n        'Topic :: Internet :: WWW/HTTP',\n        'Topic :: Software Development :: Libraries :: Application Frameworks',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n    ],\n    python_requires='>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*',\n    install_requires=[\n        'Twisted>=16.0.0;python_version==\"2.7\"',\n        'Twisted>=17.9.0;python_version>=\"3.5\"',\n        'cryptography>=2.0',\n        'cssselect>=0.9.1',\n        'lxml>=3.5.0',\n        'parsel>=1.5.0',\n        'PyDispatcher>=2.0.5',\n        'pyOpenSSL>=16.2.0',\n        'queuelib>=1.4.2',\n        'service_identity>=16.0.0',\n        'six>=1.10.0',\n        'w3lib>=1.17.0',\n        'zope.interface>=4.1.3',\n        'protego>=0.1.15',\n    ],\n    extras_require=extras_require,\n)\n"
    },
    {
      "filename": "tests/test_downloadermiddleware_httpcache.py",
      "content": "from __future__ import print_function\nimport time\nimport tempfile\nimport shutil\nimport unittest\nimport email.utils\nfrom contextlib import contextmanager\nimport pytest\nimport sys\n\nfrom scrapy.http import Response, HtmlResponse, Request\nfrom scrapy.spiders import Spider\nfrom scrapy.settings import Settings\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.utils.test import get_crawler\nfrom scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware\n\n\nclass _BaseTest(unittest.TestCase):\n\n    storage_class = 'scrapy.extensions.httpcache.DbmCacheStorage'\n    policy_class = 'scrapy.extensions.httpcache.RFC2616Policy'\n\n    def setUp(self):\n        self.yesterday = email.utils.formatdate(time.time() - 86400)\n        self.today = email.utils.formatdate()\n        self.tomorrow = email.utils.formatdate(time.time() + 86400)\n        self.crawler = get_crawler(Spider)\n        self.spider = self.crawler._create_spider('example.com')\n        self.tmpdir = tempfile.mkdtemp()\n        self.request = Request('http://www.example.com',\n                               headers={'User-Agent': 'test'})\n        self.response = Response('http://www.example.com',\n                                 headers={'Content-Type': 'text/html'},\n                                 body=b'test body',\n                                 status=202)\n        self.crawler.stats.open_spider(self.spider)\n\n    def tearDown(self):\n        self.crawler.stats.close_spider(self.spider, '')\n        shutil.rmtree(self.tmpdir)\n\n    def _get_settings(self, **new_settings):\n        settings = {\n            'HTTPCACHE_ENABLED': True,\n            'HTTPCACHE_DIR': self.tmpdir,\n            'HTTPCACHE_EXPIRATION_SECS': 1,\n            'HTTPCACHE_IGNORE_HTTP_CODES': [],\n            'HTTPCACHE_POLICY': self.policy_class,\n            'HTTPCACHE_STORAGE': self.storage_class,\n        }\n        settings.update(new_settings)\n        return Settings(settings)\n\n    @contextmanager\n    def _storage(self, **new_settings):\n        with self._middleware(**new_settings) as mw:\n            yield mw.storage\n\n    @contextmanager\n    def _policy(self, **new_settings):\n        with self._middleware(**new_settings) as mw:\n            yield mw.policy\n\n    @contextmanager\n    def _middleware(self, **new_settings):\n        settings = self._get_settings(**new_settings)\n        mw = HttpCacheMiddleware(settings, self.crawler.stats)\n        mw.spider_opened(self.spider)\n        try:\n            yield mw\n        finally:\n            mw.spider_closed(self.spider)\n\n    def assertEqualResponse(self, response1, response2):\n        self.assertEqual(response1.url, response2.url)\n        self.assertEqual(response1.status, response2.status)\n        self.assertEqual(response1.headers, response2.headers)\n        self.assertEqual(response1.body, response2.body)\n\n    def assertEqualRequest(self, request1, request2):\n        self.assertEqual(request1.url, request2.url)\n        self.assertEqual(request1.headers, request2.headers)\n        self.assertEqual(request1.body, request2.body)\n\n    def assertEqualRequestButWithCacheValidators(self, request1, request2):\n        self.assertEqual(request1.url, request2.url)\n        assert not b'If-None-Match' in request1.headers\n        assert not b'If-Modified-Since' in request1.headers\n        assert any(h in request2.headers for h in (b'If-None-Match', b'If-Modified-Since'))\n        self.assertEqual(request1.body, request2.body)\n\n    def test_dont_cache(self):\n        with self._middleware() as mw:\n            self.request.meta['dont_cache'] = True\n            mw.process_response(self.request, self.response, self.spider)\n            self.assertEqual(mw.storage.retrieve_response(self.spider, self.request), None)\n\n        with self._middleware() as mw:\n            self.request.meta['dont_cache'] = False\n            mw.process_response(self.request, self.response, self.spider)\n            if mw.policy.should_cache_response(self.response, self.request):\n                self.assertIsInstance(mw.storage.retrieve_response(self.spider, self.request), self.response.__class__)\n\n\nclass DefaultStorageTest(_BaseTest):\n\n    def test_storage(self):\n        with self._storage() as storage:\n            request2 = self.request.copy()\n            assert storage.retrieve_response(self.spider, request2) is None\n\n            storage.store_response(self.spider, self.request, self.response)\n            response2 = storage.retrieve_response(self.spider, request2)\n            assert isinstance(response2, HtmlResponse)  # content-type header\n            self.assertEqualResponse(self.response, response2)\n\n            time.sleep(2)  # wait for cache to expire\n            assert storage.retrieve_response(self.spider, request2) is None\n\n    def test_storage_never_expire(self):\n        with self._storage(HTTPCACHE_EXPIRATION_SECS=0) as storage:\n            assert storage.retrieve_response(self.spider, self.request) is None\n            storage.store_response(self.spider, self.request, self.response)\n            time.sleep(0.5)  # give the chance to expire\n            assert storage.retrieve_response(self.spider, self.request)\n\n\nclass DbmStorageTest(DefaultStorageTest):\n\n    storage_class = 'scrapy.extensions.httpcache.DbmCacheStorage'\n\n\nclass DbmStorageWithCustomDbmModuleTest(DbmStorageTest):\n\n    dbm_module = 'tests.mocks.dummydbm'\n\n    def _get_settings(self, **new_settings):\n        new_settings.setdefault('HTTPCACHE_DBM_MODULE', self.dbm_module)\n        return super(DbmStorageWithCustomDbmModuleTest, self)._get_settings(**new_settings)\n\n    def test_custom_dbm_module_loaded(self):\n        # make sure our dbm module has been loaded\n        with self._storage() as storage:\n            self.assertEqual(storage.dbmodule.__name__, self.dbm_module)\n\n\nclass FilesystemStorageTest(DefaultStorageTest):\n\n    storage_class = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n\nclass FilesystemStorageGzipTest(FilesystemStorageTest):\n\n    def _get_settings(self, **new_settings):\n        new_settings.setdefault('HTTPCACHE_GZIP', True)\n        return super(FilesystemStorageTest, self)._get_settings(**new_settings)\n\n\nclass LeveldbStorageTest(DefaultStorageTest):\n\n    try:\n        pytest.importorskip('leveldb')\n    except SystemError:\n        pytestmark = pytest.mark.skip(\"Test module skipped - 'SystemError: bad call flags' occurs when >= Python 3.8\")\n    storage_class = 'scrapy.extensions.httpcache.LeveldbCacheStorage'\n\n\nclass DummyPolicyTest(_BaseTest):\n\n    policy_class = 'scrapy.extensions.httpcache.DummyPolicy'\n\n    def test_middleware(self):\n        with self._middleware() as mw:\n            assert mw.process_request(self.request, self.spider) is None\n            mw.process_response(self.request, self.response, self.spider)\n            response = mw.process_request(self.request, self.spider)\n            assert isinstance(response, HtmlResponse)\n            self.assertEqualResponse(self.response, response)\n            assert 'cached' in response.flags\n\n    def test_different_request_response_urls(self):\n        with self._middleware() as mw:\n            req = Request('http://host.com/path')\n            res = Response('http://host2.net/test.html')\n            assert mw.process_request(req, self.spider) is None\n            mw.process_response(req, res, self.spider)\n            cached = mw.process_request(req, self.spider)\n            assert isinstance(cached, Response)\n            self.assertEqualResponse(res, cached)\n            assert 'cached' in cached.flags\n\n    def test_middleware_ignore_missing(self):\n        with self._middleware(HTTPCACHE_IGNORE_MISSING=True) as mw:\n            self.assertRaises(IgnoreRequest, mw.process_request, self.request, self.spider)\n            mw.process_response(self.request, self.response, self.spider)\n            response = mw.process_request(self.request, self.spider)\n            assert isinstance(response, HtmlResponse)\n            self.assertEqualResponse(self.response, response)\n            assert 'cached' in response.flags\n\n    def test_middleware_ignore_schemes(self):\n        # http responses are cached by default\n        req, res = Request('http://test.com/'), Response('http://test.com/')\n        with self._middleware() as mw:\n            assert mw.process_request(req, self.spider) is None\n            mw.process_response(req, res, self.spider)\n\n            cached = mw.process_request(req, self.spider)\n            assert isinstance(cached, Response), type(cached)\n            self.assertEqualResponse(res, cached)\n            assert 'cached' in cached.flags\n\n        # file response is not cached by default\n        req, res = Request('file:///tmp/t.txt'), Response('file:///tmp/t.txt')\n        with self._middleware() as mw:\n            assert mw.process_request(req, self.spider) is None\n            mw.process_response(req, res, self.spider)\n\n            assert mw.storage.retrieve_response(self.spider, req) is None\n            assert mw.process_request(req, self.spider) is None\n\n        # s3 scheme response is cached by default\n        req, res = Request('s3://bucket/key'), Response('http://bucket/key')\n        with self._middleware() as mw:\n            assert mw.process_request(req, self.spider) is None\n            mw.process_response(req, res, self.spider)\n\n            cached = mw.process_request(req, self.spider)\n            assert isinstance(cached, Response), type(cached)\n            self.assertEqualResponse(res, cached)\n            assert 'cached' in cached.flags\n\n        # ignore s3 scheme\n        req, res = Request('s3://bucket/key2'), Response('http://bucket/key2')\n        with self._middleware(HTTPCACHE_IGNORE_SCHEMES=['s3']) as mw:\n            assert mw.process_request(req, self.spider) is None\n            mw.process_response(req, res, self.spider)\n\n            assert mw.storage.retrieve_response(self.spider, req) is None\n            assert mw.process_request(req, self.spider) is None\n\n    def test_middleware_ignore_http_codes(self):\n        # test response is not cached\n        with self._middleware(HTTPCACHE_IGNORE_HTTP_CODES=[202]) as mw:\n            assert mw.process_request(self.request, self.spider) is None\n            mw.process_response(self.request, self.response, self.spider)\n\n            assert mw.storage.retrieve_response(self.spider, self.request) is None\n            assert mw.process_request(self.request, self.spider) is None\n\n        # test response is cached\n        with self._middleware(HTTPCACHE_IGNORE_HTTP_CODES=[203]) as mw:\n            mw.process_response(self.request, self.response, self.spider)\n            response = mw.process_request(self.request, self.spider)\n            assert isinstance(response, HtmlResponse)\n            self.assertEqualResponse(self.response, response)\n            assert 'cached' in response.flags\n\n\nclass RFC2616PolicyTest(DefaultStorageTest):\n\n    policy_class = 'scrapy.extensions.httpcache.RFC2616Policy'\n\n    def _process_requestresponse(self, mw, request, response):\n        result = None\n        try:\n            result = mw.process_request(request, self.spider)\n            if result:\n                assert isinstance(result, (Request, Response))\n                return result\n            else:\n                result = mw.process_response(request, response, self.spider)\n                assert isinstance(result, Response)\n                return result\n        except Exception:\n            print('Request', request)\n            print('Response', response)\n            print('Result', result)\n            raise\n\n    def test_request_cacheability(self):\n        res0 = Response(self.request.url, status=200,\n                        headers={'Expires': self.tomorrow})\n        req0 = Request('http://example.com')\n        req1 = req0.replace(headers={'Cache-Control': 'no-store'})\n        req2 = req0.replace(headers={'Cache-Control': 'no-cache'})\n        with self._middleware() as mw:\n            # response for a request with no-store must not be cached\n            res1 = self._process_requestresponse(mw, req1, res0)\n            self.assertEqualResponse(res1, res0)\n            assert mw.storage.retrieve_response(self.spider, req1) is None\n            # Re-do request without no-store and expect it to be cached\n            res2 = self._process_requestresponse(mw, req0, res0)\n            assert 'cached' not in res2.flags\n            res3 = mw.process_request(req0, self.spider)\n            assert 'cached' in res3.flags\n            self.assertEqualResponse(res2, res3)\n            # request with no-cache directive must not return cached response\n            # but it allows new response to be stored\n            res0b = res0.replace(body=b'foo')\n            res4 = self._process_requestresponse(mw, req2, res0b)\n            self.assertEqualResponse(res4, res0b)\n            assert 'cached' not in res4.flags\n            res5 = self._process_requestresponse(mw, req0, None)\n            self.assertEqualResponse(res5, res0b)\n            assert 'cached' in res5.flags\n\n    def test_response_cacheability(self):\n        responses = [\n            # 304 is not cacheable no matter what servers sends\n            (False, 304, {}),\n            (False, 304, {'Last-Modified': self.yesterday}),\n            (False, 304, {'Expires': self.tomorrow}),\n            (False, 304, {'Etag': 'bar'}),\n            (False, 304, {'Cache-Control': 'max-age=3600'}),\n            # Always obey no-store cache control\n            (False, 200, {'Cache-Control': 'no-store'}),\n            (False, 200, {'Cache-Control': 'no-store, max-age=300'}),  # invalid\n            (False, 200, {'Cache-Control': 'no-store', 'Expires': self.tomorrow}),  # invalid\n            # Ignore responses missing expiration and/or validation headers\n            (False, 200, {}),\n            (False, 302, {}),\n            (False, 307, {}),\n            (False, 404, {}),\n            # Cache responses with expiration and/or validation headers\n            (True, 200, {'Last-Modified': self.yesterday}),\n            (True, 203, {'Last-Modified': self.yesterday}),\n            (True, 300, {'Last-Modified': self.yesterday}),\n            (True, 301, {'Last-Modified': self.yesterday}),\n            (True, 308, {'Last-Modified': self.yesterday}),\n            (True, 401, {'Last-Modified': self.yesterday}),\n            (True, 404, {'Cache-Control': 'public, max-age=600'}),\n            (True, 302, {'Expires': self.tomorrow}),\n            (True, 200, {'Etag': 'foo'}),\n        ]\n        with self._middleware() as mw:\n            for idx, (shouldcache, status, headers) in enumerate(responses):\n                req0 = Request('http://example-%d.com' % idx)\n                res0 = Response(req0.url, status=status, headers=headers)\n                res1 = self._process_requestresponse(mw, req0, res0)\n                res304 = res0.replace(status=304)\n                res2 = self._process_requestresponse(mw, req0, res304 if shouldcache else res0)\n                self.assertEqualResponse(res1, res0)\n                self.assertEqualResponse(res2, res0)\n                resc = mw.storage.retrieve_response(self.spider, req0)\n                if shouldcache:\n                    self.assertEqualResponse(resc, res1)\n                    assert 'cached' in res2.flags and res2.status != 304\n                else:\n                    self.assertFalse(resc)\n                    assert 'cached' not in res2.flags\n\n        # cache unconditionally unless response contains no-store or is a 304\n        with self._middleware(HTTPCACHE_ALWAYS_STORE=True) as mw:\n            for idx, (_, status, headers) in enumerate(responses):\n                shouldcache = 'no-store' not in headers.get('Cache-Control', '') and status != 304\n                req0 = Request('http://example2-%d.com' % idx)\n                res0 = Response(req0.url, status=status, headers=headers)\n                res1 = self._process_requestresponse(mw, req0, res0)\n                res304 = res0.replace(status=304)\n                res2 = self._process_requestresponse(mw, req0, res304 if shouldcache else res0)\n                self.assertEqualResponse(res1, res0)\n                self.assertEqualResponse(res2, res0)\n                resc = mw.storage.retrieve_response(self.spider, req0)\n                if shouldcache:\n                    self.assertEqualResponse(resc, res1)\n                    assert 'cached' in res2.flags and res2.status != 304\n                else:\n                    self.assertFalse(resc)\n                    assert 'cached' not in res2.flags\n\n    def test_cached_and_fresh(self):\n        sampledata = [\n            (200, {'Date': self.yesterday, 'Expires': self.tomorrow}),\n            (200, {'Date': self.yesterday, 'Cache-Control': 'max-age=86405'}),\n            (200, {'Age': '299', 'Cache-Control': 'max-age=300'}),\n            # Obey max-age if present over any others\n            (200, {'Date': self.today,\n                   'Age': '86405',\n                   'Cache-Control': 'max-age=' + str(86400 * 3),\n                   'Expires': self.yesterday,\n                   'Last-Modified': self.yesterday,\n                   }),\n            # obey Expires if max-age is not present\n            (200, {'Date': self.yesterday,\n                   'Age': '86400',\n                   'Cache-Control': 'public',\n                   'Expires': self.tomorrow,\n                   'Last-Modified': self.yesterday,\n                   }),\n            # Default missing Date header to right now\n            (200, {'Expires': self.tomorrow}),\n            # Firefox - Expires if age is greater than 10% of (Date - Last-Modified)\n            (200, {'Date': self.today, 'Last-Modified': self.yesterday, 'Age': str(86400 / 10 - 1)}),\n            # Firefox - Set one year maxage to permanent redirects missing expiration info\n            (300, {}), (301, {}), (308, {}),\n        ]\n        with self._middleware() as mw:\n            for idx, (status, headers) in enumerate(sampledata):\n                req0 = Request('http://example-%d.com' % idx)\n                res0 = Response(req0.url, status=status, headers=headers)\n                # cache fresh response\n                res1 = self._process_requestresponse(mw, req0, res0)\n                self.assertEqualResponse(res1, res0)\n                assert 'cached' not in res1.flags\n                # return fresh cached response without network interaction\n                res2 = self._process_requestresponse(mw, req0, None)\n                self.assertEqualResponse(res1, res2)\n                assert 'cached' in res2.flags\n                # validate cached response if request max-age set as 0\n                req1 = req0.replace(headers={'Cache-Control': 'max-age=0'})\n                res304 = res0.replace(status=304)\n                assert mw.process_request(req1, self.spider) is None\n                res3 = self._process_requestresponse(mw, req1, res304)\n                self.assertEqualResponse(res1, res3)\n                assert 'cached' in res3.flags\n\n    def test_cached_and_stale(self):\n        sampledata = [\n            (200, {'Date': self.today, 'Expires': self.yesterday}),\n            (200, {'Date': self.today, 'Expires': self.yesterday, 'Last-Modified': self.yesterday}),\n            (200, {'Expires': self.yesterday}),\n            (200, {'Expires': self.yesterday, 'ETag': 'foo'}),\n            (200, {'Expires': self.yesterday, 'Last-Modified': self.yesterday}),\n            (200, {'Expires': self.tomorrow, 'Age': '86405'}),\n            (200, {'Cache-Control': 'max-age=86400', 'Age': '86405'}),\n            # no-cache forces expiration, also revalidation if validators exists\n            (200, {'Cache-Control': 'no-cache'}),\n            (200, {'Cache-Control': 'no-cache', 'ETag': 'foo'}),\n            (200, {'Cache-Control': 'no-cache', 'Last-Modified': self.yesterday}),\n            (200, {'Cache-Control': 'no-cache,must-revalidate', 'Last-Modified': self.yesterday}),\n            (200, {'Cache-Control': 'must-revalidate', 'Expires': self.yesterday, 'Last-Modified': self.yesterday}),\n            (200, {'Cache-Control': 'max-age=86400,must-revalidate', 'Age': '86405'}),\n        ]\n        with self._middleware() as mw:\n            for idx, (status, headers) in enumerate(sampledata):\n                req0 = Request('http://example-%d.com' % idx)\n                res0a = Response(req0.url, status=status, headers=headers)\n                # cache expired response\n                res1 = self._process_requestresponse(mw, req0, res0a)\n                self.assertEqualResponse(res1, res0a)\n                assert 'cached' not in res1.flags\n                # Same request but as cached response is stale a new response must\n                # be returned\n                res0b = res0a.replace(body=b'bar')\n                res2 = self._process_requestresponse(mw, req0, res0b)\n                self.assertEqualResponse(res2, res0b)\n                assert 'cached' not in res2.flags\n                cc = headers.get('Cache-Control', '')\n                # Previous response expired too, subsequent request to same\n                # resource must revalidate and succeed on 304 if validators\n                # are present\n                if 'ETag' in headers or 'Last-Modified' in headers:\n                    res0c = res0b.replace(status=304)\n                    res3 = self._process_requestresponse(mw, req0, res0c)\n                    self.assertEqualResponse(res3, res0b)\n                    assert 'cached' in res3.flags\n                    # get cached response on server errors unless must-revalidate\n                    # in cached response\n                    res0d = res0b.replace(status=500)\n                    res4 = self._process_requestresponse(mw, req0, res0d)\n                    if 'must-revalidate' in cc:\n                        assert 'cached' not in res4.flags\n                        self.assertEqualResponse(res4, res0d)\n                    else:\n                        assert 'cached' in res4.flags\n                        self.assertEqualResponse(res4, res0b)\n                # Requests with max-stale can fetch expired cached responses\n                # unless cached response has must-revalidate\n                req1 = req0.replace(headers={'Cache-Control': 'max-stale'})\n                res5 = self._process_requestresponse(mw, req1, res0b)\n                self.assertEqualResponse(res5, res0b)\n                if 'no-cache' in cc or 'must-revalidate' in cc:\n                    assert 'cached' not in res5.flags\n                else:\n                    assert 'cached' in res5.flags\n\n    def test_process_exception(self):\n        with self._middleware() as mw:\n            res0 = Response(self.request.url, headers={'Expires': self.yesterday})\n            req0 = Request(self.request.url)\n            self._process_requestresponse(mw, req0, res0)\n            for e in mw.DOWNLOAD_EXCEPTIONS:\n                # Simulate encountering an error on download attempts\n                assert mw.process_request(req0, self.spider) is None\n                res1 = mw.process_exception(req0, e('foo'), self.spider)\n                # Use cached response as recovery\n                assert 'cached' in res1.flags\n                self.assertEqualResponse(res0, res1)\n            # Do not use cached response for unhandled exceptions\n            mw.process_request(req0, self.spider)\n            assert mw.process_exception(req0, Exception('foo'), self.spider) is None\n\n    def test_ignore_response_cache_controls(self):\n        sampledata = [\n            (200, {'Date': self.yesterday, 'Expires': self.tomorrow}),\n            (200, {'Date': self.yesterday, 'Cache-Control': 'no-store,max-age=86405'}),\n            (200, {'Age': '299', 'Cache-Control': 'max-age=300,no-cache'}),\n            (300, {'Cache-Control': 'no-cache'}),\n            (200, {'Expires': self.tomorrow, 'Cache-Control': 'no-store'}),\n        ]\n        with self._middleware(HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS=['no-cache', 'no-store']) as mw:\n            for idx, (status, headers) in enumerate(sampledata):\n                req0 = Request('http://example-%d.com' % idx)\n                res0 = Response(req0.url, status=status, headers=headers)\n                # cache fresh response\n                res1 = self._process_requestresponse(mw, req0, res0)\n                self.assertEqualResponse(res1, res0)\n                assert 'cached' not in res1.flags\n                # return fresh cached response without network interaction\n                res2 = self._process_requestresponse(mw, req0, None)\n                self.assertEqualResponse(res1, res2)\n                assert 'cached' in res2.flags\n\nif __name__ == '__main__':\n    unittest.main()\n"
    }
  ],
  "questions": [
    "> I am trying to install scrapy for python 3.8 in a Windows environment. Are there currently any ways I can do this? Or is this blocked upstream by conda [conda/conda#9343](https://github.com/conda/conda/issues/9343)\r\n\r\nYou can try using `pip`. If you cannot get it to work with `pip`, then you’ll need to wait for Conda to support Python 3.8.",
    "> > I am trying to install scrapy for python 3.8 in a Windows environment. Are there currently any ways I can do this? Or is this blocked upstream by conda [conda/conda#9343](https://github.com/conda/conda/issues/9343)\r\n> \r\n> You can try using `pip`. If you cannot get it to work with `pip`, then you’ll need to wait for Conda to support Python 3.8.\r\n\r\nNo luck using just `pip install scrapy` unfortunately.",
    "@wRAR But why would that happen? is `lxml` not \"ready\" for 3.8? But after all, scrapy depends on the `lxml` library, so it might be a good idea to have it here?"
  ],
  "golden_answers": [
    "> > I am trying to install scrapy for python 3.8 in a Windows environment. Are there currently any ways I can do this? Or is this blocked upstream by conda [conda/conda#9343](https://github.com/conda/conda/issues/9343)\r\n> \r\n> You can try using `pip`. If you cannot get it to work with `pip`, then you’ll need to wait for Conda to support Python 3.8.\r\n\r\nNo luck using just `pip install scrapy` unfortunately.",
    "Installing Python 3.8.0 and immediately later `scrapy` fails. \r\n\r\nHere's the commands used:\r\n\r\n```bash\r\npip install python=3.8.0\r\npip install scrapy\r\n```\r\n\r\nMy workstation: `4.19.79-1-MANJARO GNU/Linux`\r\n\r\nThe log itself: https://pastebin.com/cAawd2jB",
    "@diegovincent your compiler toolchain seems broken, and no, this is not a good place to discuss problems with installing third-party modules."
  ],
  "questions_generated": [
    "What are the necessary changes to officially support Python 3.8 in the scrapy_scrapy repository?",
    "Why might there be issues with installing Scrapy using Python 3.8 on certain systems?",
    "How does the DummyPolicy class in `scrapy/extensions/httpcache.py` determine if a request should be cached?",
    "What is a possible reason for a compilation error when installing Scrapy with Python 3.8 on a GNU/Linux system?",
    "Why might it be inappropriate to discuss third-party module installation issues in the context of the scrapy_scrapy repository issue?"
  ],
  "golden_answers_generated": [
    "The necessary changes include enabling a Python 3.8 environment in Tox, enabling a Python 3.8 job in Travis CI, and updating the `setup.py` file to reflect support for Python 3.8.",
    "Issues may arise due to dependencies like `lxml` that need to be compiled and may not yet fully support Python 3.8. Additionally, there could be problems related to the compiler toolchain on the user's system.",
    "The DummyPolicy class determines if a request should be cached based on the URL's scheme. It does not cache requests with schemes listed in the `HTTPCACHE_IGNORE_SCHEMES` setting.",
    "A possible reason for a compilation error could be a broken compiler toolchain, which affects the ability to compile dependencies like `lxml` that Scrapy relies on.",
    "It is inappropriate because the issue is specific to the third-party module, such as `lxml`, and not directly related to the Scrapy codebase. Discussion should focus on Scrapy's integration and support for Python 3.8, while issues with dependencies should be addressed in their respective repositories or support channels."
  ]
}