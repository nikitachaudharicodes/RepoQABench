{
  "repo_name": "scrapy_scrapy",
  "issue_id": "3331",
  "issue_description": "# Documentation example fails with `proxy URL with no authority`\n\nRunning the [example](https://doc.scrapy.org/en/1.5/intro/overview.html#walk-through-of-an-example-spider) from the documentation yields this:\r\n```\r\n10:11 $ scrapy runspider quotes.py \r\n2018-07-11 10:12:04 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-07-11 10:12:04 [scrapy.utils.log] INFO: Versions: lxml 3.5.0.0, libxml2 2.9.3, cssselect 0.9.1, parsel 1.5.0, w3lib 1.19.0, Twisted 16.0.0, Python 2.7.12 (default, Dec  4 2017, 14:50:18) - [GCC 5.4.0 20160609], pyOpenSSL 0.15.1 (OpenSSL 1.0.2g  1 Mar 2016), cryptography 1.2.3, Platform Linux-4.4.0-130-generic-x86_64-with-Ubuntu-16.04-xenial\r\n2018-07-11 10:12:04 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}\r\n2018-07-11 10:12:04 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.corestats.CoreStats']\r\nUnhandled error in Deferred:\r\n2018-07-11 10:12:04 [twisted] CRITICAL: Unhandled error in Deferred:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/runspider.py\", line 88, in run\r\n    self.crawler_process.crawl(spidercls, **opts.spargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 171, in crawl\r\n    return self._crawl(crawler, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 175, in _crawl\r\n    d = crawler.crawl(*args, **kwargs)\r\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1274, in unwindGenerator\r\n    return _inlineCallbacks(None, gen, Deferred())\r\n--- <exception caught here> ---\r\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1128, in _inlineCallbacks\r\n    result = g.send(result)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 98, in crawl\r\n    six.reraise(*exc_info)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 80, in crawl\r\n    self.engine = self._create_engine()\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 105, in _create_engine\r\n    return ExecutionEngine(self, lambda _: self.stop())\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 69, in __init__\r\n    self.downloader = downloader_cls(crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/downloader/__init__.py\", line 88, in __init__\r\n    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py\", line 58, in from_crawler\r\n    return cls.from_settings(crawler.settings, crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py\", line 36, in from_settings\r\n    mw = mwcls.from_crawler(crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 29, in from_crawler\r\n    return cls(auth_encoding)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 22, in __init__\r\n    self.proxies[type] = self._get_proxy(url, type)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 39, in _get_proxy\r\n    proxy_type, user, password, hostport = _parse_proxy(url)\r\n  File \"/usr/lib/python2.7/urllib2.py\", line 721, in _parse_proxy\r\n    raise ValueError(\"proxy URL with no authority: %r\" % proxy)\r\nexceptions.ValueError: proxy URL with no authority: '/var/run/docker.sock'\r\n2018-07-11 10:12:04 [twisted] CRITICAL:\r\n```\r\nLooks like proxy code does not handle `no_proxy` correctly.",
  "issue_comments": [
    {
      "id": 404096282,
      "user": "grammy-jiang",
      "body": "Hi, @a-palchikov \r\n\r\nThis exception is reported by Python standard lib, not Scrapy.\r\n\r\nWould you mind to post your start_urls, and the environment variables about PROXY here?"
    },
    {
      "id": 519477571,
      "user": "Gallaecio",
      "body": "Closing due to lack of feedback from the author."
    },
    {
      "id": 654712494,
      "user": "otakutyrant",
      "body": "I have encountered this issue too. My relative proxy environment variable is `no_proxy=/var/run/docker.sock`, and after unsetting this one the issue is solved. So as the poster said, looks like proxy code does not handle `no_proxy` correctly."
    },
    {
      "id": 655334270,
      "user": "Gallaecio",
      "body": "The code probably does not expect a file path in that variable. I guess we should silently ignore those."
    },
    {
      "id": 678216640,
      "user": "kartecianos",
      "body": "Hi, I am a newcomer and I would like to take this issue"
    },
    {
      "id": 678240598,
      "user": "Gallaecio",
      "body": "No need to ask for permission :slightly_smiling_face: "
    },
    {
      "id": 678693819,
      "user": "drs-11",
      "body": "Also looks like ` _get_proxy` here doesn't handle multiple addresses in the `no_proxy` env variable well.\r\nFor eg: \r\nIf the `no_proxy` env var is set to: `no_proxy=\"127.0.0.1,localhost,localdomain.com\"`\r\nThen `self.proxies` in `HttpProxyMiddleware` will be set as:\r\n`{'no': (None, 'no://127.0.0.1,localhost,localdomain.com')}`\r\n\r\nThat's not how it should be, right?"
    },
    {
      "id": 680035295,
      "user": "Gallaecio",
      "body": "Probably no, indeed."
    },
    {
      "id": 687309743,
      "user": "drs-11",
      "body": "I'm not sure what could be a solution to this issue.\r\n`/var/run/docker.sock` seems the only exception for having a socket file in a `no_proxy` env variable. So either the socket file be ignored and not added to the list of proxies or maybe add it without passing the socket file path to `_get_proxy` method which is causing the error?\r\n\r\nBut the second option will cause further errors when the proxy is parsed in other modules.\r\nSo I think ignoring the socket file will be the best option? Also I can't find any other cases where a socket file is used in `no_proxy`.\r\nThoughts?\r\n\r\n"
    },
    {
      "id": 687323045,
      "user": "a-palchikov",
      "body": "I guess NO_PROXY handling is very open to specific interpretations and is not standardized. Docker client describes the uses of NO_PROXY for its purposes [here](https://github.com/moby/moby/pull/10192/files?short_path=c13a5c5#diff-c13a5c583fae16a859a034cdd06c7c58) while scrapy can just ignore the proxy that the `urllib2.parse_proxy` fails to parse."
    }
  ],
  "text_context": "# Documentation example fails with `proxy URL with no authority`\n\nRunning the [example](https://doc.scrapy.org/en/1.5/intro/overview.html#walk-through-of-an-example-spider) from the documentation yields this:\r\n```\r\n10:11 $ scrapy runspider quotes.py \r\n2018-07-11 10:12:04 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-07-11 10:12:04 [scrapy.utils.log] INFO: Versions: lxml 3.5.0.0, libxml2 2.9.3, cssselect 0.9.1, parsel 1.5.0, w3lib 1.19.0, Twisted 16.0.0, Python 2.7.12 (default, Dec  4 2017, 14:50:18) - [GCC 5.4.0 20160609], pyOpenSSL 0.15.1 (OpenSSL 1.0.2g  1 Mar 2016), cryptography 1.2.3, Platform Linux-4.4.0-130-generic-x86_64-with-Ubuntu-16.04-xenial\r\n2018-07-11 10:12:04 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}\r\n2018-07-11 10:12:04 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.corestats.CoreStats']\r\nUnhandled error in Deferred:\r\n2018-07-11 10:12:04 [twisted] CRITICAL: Unhandled error in Deferred:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/runspider.py\", line 88, in run\r\n    self.crawler_process.crawl(spidercls, **opts.spargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 171, in crawl\r\n    return self._crawl(crawler, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 175, in _crawl\r\n    d = crawler.crawl(*args, **kwargs)\r\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1274, in unwindGenerator\r\n    return _inlineCallbacks(None, gen, Deferred())\r\n--- <exception caught here> ---\r\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1128, in _inlineCallbacks\r\n    result = g.send(result)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 98, in crawl\r\n    six.reraise(*exc_info)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 80, in crawl\r\n    self.engine = self._create_engine()\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 105, in _create_engine\r\n    return ExecutionEngine(self, lambda _: self.stop())\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 69, in __init__\r\n    self.downloader = downloader_cls(crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/downloader/__init__.py\", line 88, in __init__\r\n    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py\", line 58, in from_crawler\r\n    return cls.from_settings(crawler.settings, crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py\", line 36, in from_settings\r\n    mw = mwcls.from_crawler(crawler)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 29, in from_crawler\r\n    return cls(auth_encoding)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 22, in __init__\r\n    self.proxies[type] = self._get_proxy(url, type)\r\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 39, in _get_proxy\r\n    proxy_type, user, password, hostport = _parse_proxy(url)\r\n  File \"/usr/lib/python2.7/urllib2.py\", line 721, in _parse_proxy\r\n    raise ValueError(\"proxy URL with no authority: %r\" % proxy)\r\nexceptions.ValueError: proxy URL with no authority: '/var/run/docker.sock'\r\n2018-07-11 10:12:04 [twisted] CRITICAL:\r\n```\r\nLooks like proxy code does not handle `no_proxy` correctly.\n\nHi, @a-palchikov \r\n\r\nThis exception is reported by Python standard lib, not Scrapy.\r\n\r\nWould you mind to post your start_urls, and the environment variables about PROXY here?\n\nClosing due to lack of feedback from the author.\n\nI have encountered this issue too. My relative proxy environment variable is `no_proxy=/var/run/docker.sock`, and after unsetting this one the issue is solved. So as the poster said, looks like proxy code does not handle `no_proxy` correctly.\n\nThe code probably does not expect a file path in that variable. I guess we should silently ignore those.\n\nHi, I am a newcomer and I would like to take this issue\n\nNo need to ask for permission :slightly_smiling_face: \n\nAlso looks like ` _get_proxy` here doesn't handle multiple addresses in the `no_proxy` env variable well.\r\nFor eg: \r\nIf the `no_proxy` env var is set to: `no_proxy=\"127.0.0.1,localhost,localdomain.com\"`\r\nThen `self.proxies` in `HttpProxyMiddleware` will be set as:\r\n`{'no': (None, 'no://127.0.0.1,localhost,localdomain.com')}`\r\n\r\nThat's not how it should be, right?\n\nProbably no, indeed.\n\nI'm not sure what could be a solution to this issue.\r\n`/var/run/docker.sock` seems the only exception for having a socket file in a `no_proxy` env variable. So either the socket file be ignored and not added to the list of proxies or maybe add it without passing the socket file path to `_get_proxy` method which is causing the error?\r\n\r\nBut the second option will cause further errors when the proxy is parsed in other modules.\r\nSo I think ignoring the socket file will be the best option? Also I can't find any other cases where a socket file is used in `no_proxy`.\r\nThoughts?\r\n\r\n\n\nI guess NO_PROXY handling is very open to specific interpretations and is not standardized. Docker client describes the uses of NO_PROXY for its purposes [here](https://github.com/moby/moby/pull/10192/files?short_path=c13a5c5#diff-c13a5c583fae16a859a034cdd06c7c58) while scrapy can just ignore the proxy that the `urllib2.parse_proxy` fails to parse.",
  "pr_link": "https://github.com/moby/moby/pull/10192",
  "code_context": [],
  "questions": [
    "Hi, @a-palchikov \r\n\r\nThis exception is reported by Python standard lib, not Scrapy.\r\n\r\nWould you mind to post your start_urls, and the environment variables about PROXY here?",
    "Also looks like ` _get_proxy` here doesn't handle multiple addresses in the `no_proxy` env variable well.\r\nFor eg: \r\nIf the `no_proxy` env var is set to: `no_proxy=\"127.0.0.1,localhost,localdomain.com\"`\r\nThen `self.proxies` in `HttpProxyMiddleware` will be set as:\r\n`{'no': (None, 'no://127.0.0.1,localhost,localdomain.com')}`\r\n\r\nThat's not how it should be, right?",
    "I'm not sure what could be a solution to this issue.\r\n`/var/run/docker.sock` seems the only exception for having a socket file in a `no_proxy` env variable. So either the socket file be ignored and not added to the list of proxies or maybe add it without passing the socket file path to `_get_proxy` method which is causing the error?\r\n\r\nBut the second option will cause further errors when the proxy is parsed in other modules.\r\nSo I think ignoring the socket file will be the best option? Also I can't find any other cases where a socket file is used in `no_proxy`.\r\nThoughts?"
  ],
  "golden_answers": [
    "I have encountered this issue too. My relative proxy environment variable is `no_proxy=/var/run/docker.sock`, and after unsetting this one the issue is solved. So as the poster said, looks like proxy code does not handle `no_proxy` correctly.",
    "I'm not sure what could be a solution to this issue.\r\n`/var/run/docker.sock` seems the only exception for having a socket file in a `no_proxy` env variable. So either the socket file be ignored and not added to the list of proxies or maybe add it without passing the socket file path to `_get_proxy` method which is causing the error?\r\n\r\nBut the second option will cause further errors when the proxy is parsed in other modules.\r\nSo I think ignoring the socket file will be the best option? Also I can't find any other cases where a socket file is used in `no_proxy`.\r\nThoughts?",
    "I guess NO_PROXY handling is very open to specific interpretations and is not standardized. Docker client describes the uses of NO_PROXY for its purposes [here](https://github.com/moby/moby/pull/10192/files?short_path=c13a5c5#diff-c13a5c583fae16a859a034cdd06c7c58) while scrapy can just ignore the proxy that the `urllib2.parse_proxy` fails to parse."
  ],
  "questions_generated": [
    "What is the primary cause of the 'proxy URL with no authority' error in the Scrapy example?",
    "In the traceback provided, which Scrapy middleware class is involved in the handling of proxy settings?",
    "How does the Scrapy `DownloaderMiddlewareManager` relate to the issue encountered in the example?",
    "Which Python version is being used in the example, and why might this be relevant to debugging the issue?",
    "What role does the `ExecutionEngine` play in the Scrapy framework, and how is it linked to the error in the provided scenario?"
  ],
  "golden_answers_generated": [
    "The error is caused by the proxy code not handling the 'no_proxy' setting correctly, which results in an invalid proxy URL being generated. Specifically, the error arises when the proxy URL is parsed, and it lacks the required authority component.",
    "The Scrapy middleware class involved is `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware`. This class is responsible for managing HTTP proxy settings, and the error occurs during its initialization when it attempts to parse the proxy URL.",
    "The `DownloaderMiddlewareManager` is responsible for managing all the downloader middlewares, including the `HttpProxyMiddleware`. The issue arises when this manager attempts to initialize the proxy middleware, and an invalid proxy URL is encountered, leading to the error in the middleware setup.",
    "The example uses Python 2.7.12. This is relevant because certain libraries and modules in Python 2 might behave differently compared to Python 3, and some modern features or updates in proxy handling might not be present in this version, potentially contributing to the issue.",
    "The `ExecutionEngine` is a core component of Scrapy that manages the execution of the crawling process, coordinating the spiders, downloader, and scheduler. In the scenario, the error occurs during the creation of the `ExecutionEngine`, specifically when initializing the downloader component, which includes setting up the proxy middleware that fails due to the proxy URL issue."
  ]
}