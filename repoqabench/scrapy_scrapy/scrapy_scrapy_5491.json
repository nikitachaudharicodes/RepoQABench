{
  "repo_name": "scrapy_scrapy",
  "issue_id": "5491",
  "issue_description": "# OpenSSL unsafe legacy renegotiation disabled error\n\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nI get an SSL issue on a working [site ](https://dorotheum.com) \r\n`twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'unsafe legacy renegotiation disabled')]>]`\r\n\r\n### Steps to Reproduce\r\n\r\n1. `scrapy shell https://dorotheum.com`\r\n\r\n**Expected behavior:** HTML page\r\n\r\n**Actual behavior:** the error above\r\n\r\n**Reproduces how often:** 100%\r\n\r\n### Versions\r\n\r\n```\r\nScrapy       : 2.6.1\r\nlxml         : 4.8.0.0\r\nlibxml2      : 2.9.4\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 22.4.0\r\nPython       : 3.9.12 (main, Mar 26 2022, 15:44:31) - [Clang 13.1.6 (clang-1316.0.21.2)]\r\npyOpenSSL    : 22.0.0 (OpenSSL 3.0.3 3 May 2022)\r\ncryptography : 37.0.2\r\nPlatform     : macOS-12.2.1-arm64-arm-64bit\r\n```\r\n",
  "issue_comments": [
    {
      "id": 1119306903,
      "user": "wRAR",
      "body": "This is explicitly disabled in OpenSSL 3: https://www.openssl.org/docs/manmaster/man7/migration_guide.html\r\n\r\nWe may need to add some new OP_* flags to support connecting to these outdated servers."
    },
    {
      "id": 1119414080,
      "user": "HASKADOG",
      "body": "@wRAR Can the downgrade of OpenSSL, pyopenssl ang cryprography help in this situation?"
    },
    {
      "id": 1119440518,
      "user": "wRAR",
      "body": "You should downgrade pyOpenSSL to a version that ships 1.1, assuming it uses a bundled one in your environment (which I suspect it does)."
    },
    {
      "id": 1119488383,
      "user": "HASKADOG",
      "body": "@wRAR looks like twisted does not support OP flasgs... "
    },
    {
      "id": 1119503413,
      "user": "wRAR",
      "body": "You can usually access the underlying OpenSSL.SSL.Context object and call `set_options()` on it."
    },
    {
      "id": 1119528632,
      "user": "HASKADOG",
      "body": "@wRAR fixed by downgrading cryptography to  36.0.2\r\nHere's my current `scrapy version --verbose ` result\r\n```\r\nScrapy       : 2.6.1\r\nlxml         : 4.8.0.0\r\nlibxml2      : 2.9.4\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 22.4.0\r\nPython       : 3.9.12 (main, Mar 26 2022, 15:44:31) - [Clang 13.1.6 (clang-1316.0.21.2)]\r\npyOpenSSL    : 22.0.0 (OpenSSL 1.1.1n  15 Mar 2022)\r\ncryptography : 36.0.2\r\nPlatform     : macOS-12.3.1-arm64-arm-64bit\r\n```\r\nNow pyOpenSSL uses OpenSSL 1.1.1n\r\nEven though the problem is fixed, the flags enhancement would be great.\r\nThank you so much @wRAR !\r\n"
    },
    {
      "id": 1119537067,
      "user": "wRAR",
      "body": "Right, it's `cryptography` that bundles libssl."
    },
    {
      "id": 1121495157,
      "user": "Yoyoda75",
      "body": "I'm having the same problem when trying to scrape the Cisco website for security advisories. How can I downgrade cryptography to a specific version inside of the scrapy package ?"
    },
    {
      "id": 1121526624,
      "user": "wRAR",
      "body": "It's not \"inside of the scrapy package\", it's in your Python environment."
    },
    {
      "id": 1175601595,
      "user": "dogweather",
      "body": "I've having the same problem with Government of Ireland websites. E.g., \r\n\r\n```\r\nscrapy fetch https://www.courts.ie/\r\n```\r\n\r\nComes back with the error, \r\n\r\n```\r\n2022-07-05 17:33:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.courts.ie/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'unsafe legacy renegotiation disabled')]>]\r\n```\r\n\r\nCurl and various web browsers do fine with it. Downgrading `cryptography` works. Thank you! \r\n\r\nDoes anyone know how to determine exactly what these websites need to upgrade? I.e., if I were going to get in touch with the Government of Ireland webmaster, what would I say? It's not immediately obvious that the bug is not with `cryptography`."
    },
    {
      "id": 1175877328,
      "user": "wRAR",
      "body": "If you are going to contact the website owner just point them to https://www.ssllabs.com/ssltest/ results for their website.\r\n\r\nI mean, if the task is \"make it work with OpenSSL 3.0 clients\" then the fix is to implement  RFC 5746, but I'm not sure the website owner would really want to fix it, considering that the website works in browsers, and it may not be easy or even possible with the stack they are now using, looking at its other problems. And if the task is \"make it more secure\" then the more important problem is SSL3 not being disabled. \r\n\r\n> It's not immediately obvious that the bug is not with cryptography.\r\n\r\nI agree it's not immediately obvious and some research would be needed. I did it earlier and linked the OpenSSL 3.0 changelog which explicitly lists things that are not supported anymore."
    },
    {
      "id": 1241862323,
      "user": "jeroenvermunt",
      "body": "I want to drop this relevant link here:\r\n\r\nhttps://stackoverflow.com/questions/71603314/ssl-error-unsafe-legacy-renegotiation-disabled/72374542#72374542\r\n\r\nHere is a very useful piece of code to circumvent the issue without downgrading:\r\n\r\n````\r\nimport requests\r\nimport urllib3\r\nimport SSL\r\n\r\n\r\nclass CustomHttpAdapter (requests.adapters.HTTPAdapter):\r\n    # \"Transport adapter\" that allows us to use custom ssl_context.\r\n\r\n    def __init__(self, ssl_context=None, **kwargs):\r\n        self.ssl_context = ssl_context\r\n        super().__init__(**kwargs)\r\n\r\n    def init_poolmanager(self, connections, maxsize, block=False):\r\n        self.poolmanager = urllib3.poolmanager.PoolManager(\r\n            num_pools=connections, maxsize=maxsize,\r\n            block=block, ssl_context=self.ssl_context)\r\n\r\n\r\ndef get_legacy_session():\r\n    ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\r\n    ctx.options |= 0x4  # OP_LEGACY_SERVER_CONNECT\r\n    session = requests.session()\r\n    session.mount('https://', CustomHttpAdapter(ctx))\r\n    return session\r\n````\r\n\r\n````\r\nwith (\r\n    get_legacy_session() as s,\r\n    s.get(\"some-url\") as response\r\n):\r\n    print(response.json())\r\n````"
    },
    {
      "id": 1241905503,
      "user": "wRAR",
      "body": "(note that Scrapy doesn't use `requests`, so accessing the OpenSSL context from a spider is left as an exercise to the reader)"
    },
    {
      "id": 1273619001,
      "user": "aysegulc",
      "body": "I ran into the same problem, and built a custom context factory to solve it. I hope it will be helpful for others:\r\n\r\ncontextfactory.py (within MyProject folder)\r\n\r\n```\r\nfrom scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\r\n\r\n\r\nclass LegacyConnectContextFactory(ScrapyClientContextFactory):\r\n\r\n    def getContext(self, hostname=None, port=None):\r\n        ctx = self.getCertificateOptions().getContext()\r\n        ctx.set_options(0x4)\r\n        return ctx\r\n\r\n```\r\n\r\nwithin my_spider.py\r\n\r\n```\r\nclass MySpider(Spider):\r\n    name = \"my_spider\"\r\n\r\n    custom_settings = {\r\n        'DOWNLOADER_CLIENTCONTEXTFACTORY': 'MyProject.contextfactory.LegacyConnectContextFactory',\r\n    }\r\n```\r\n\r\n\r\n"
    },
    {
      "id": 1307743556,
      "user": "visar63",
      "body": "Thanks @aysegulc, your suggestion works."
    },
    {
      "id": 1315000203,
      "user": "paradox-lab",
      "body": "> I ran into the same problem, and built a custom context factory to solve it. I hope it will be helpful for others:\r\n> \r\n> contextfactory.py (within MyProject folder)\r\n> \r\n> ```\r\n> from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\r\n> \r\n> \r\n> class LegacyConnectContextFactory(ScrapyClientContextFactory):\r\n> \r\n>     def getContext(self, hostname=None, port=None):\r\n>         ctx = self.getCertificateOptions().getContext()\r\n>         ctx.set_options(0x4)\r\n>         return ctx\r\n\r\n\r\nthis code works too.\r\n\r\ncontextfactory.py\r\n```python\r\nfrom scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\r\nfrom cryptography.hazmat.bindings.openssl.binding import Binding\r\n\r\n\r\nclass LegacyConnectContextFactory(ScrapyClientContextFactory):\r\n\r\n    def getContext(self, hostname=None, port=None):\r\n        ctx = super(LegacyConnectContextFactory, self).getContext()\r\n        binding = Binding()\r\n        ctx.set_options(binding.lib.SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION)\r\n        return ctx\r\n\r\n```"
    },
    {
      "id": 1332770171,
      "user": "joseiram-avd",
      "body": "> I ran into the same problem, and built a custom context factory to solve it. I hope it will be helpful for others:\r\n> \r\n> contextfactory.py (within MyProject folder)\r\n> \r\n> ```\r\n> from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\r\n> \r\n> \r\n> class LegacyConnectContextFactory(ScrapyClientContextFactory):\r\n> \r\n>     def getContext(self, hostname=None, port=None):\r\n>         ctx = self.getCertificateOptions().getContext()\r\n>         ctx.set_options(0x4)\r\n>         return ctx\r\n> ```\r\n> \r\n> within my_spider.py\r\n> \r\n> ```\r\n> class MySpider(Spider):\r\n>     name = \"my_spider\"\r\n> \r\n>     custom_settings = {\r\n>         'DOWNLOADER_CLIENTCONTEXTFACTORY': 'MyProject.contextfactory.LegacyConnectContextFactory',\r\n>     }\r\n> ```\r\n\r\nthanks."
    }
  ],
  "text_context": "# OpenSSL unsafe legacy renegotiation disabled error\n\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nI get an SSL issue on a working [site ](https://dorotheum.com) \r\n`twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'unsafe legacy renegotiation disabled')]>]`\r\n\r\n### Steps to Reproduce\r\n\r\n1. `scrapy shell https://dorotheum.com`\r\n\r\n**Expected behavior:** HTML page\r\n\r\n**Actual behavior:** the error above\r\n\r\n**Reproduces how often:** 100%\r\n\r\n### Versions\r\n\r\n```\r\nScrapy       : 2.6.1\r\nlxml         : 4.8.0.0\r\nlibxml2      : 2.9.4\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 22.4.0\r\nPython       : 3.9.12 (main, Mar 26 2022, 15:44:31) - [Clang 13.1.6 (clang-1316.0.21.2)]\r\npyOpenSSL    : 22.0.0 (OpenSSL 3.0.3 3 May 2022)\r\ncryptography : 37.0.2\r\nPlatform     : macOS-12.2.1-arm64-arm-64bit\r\n```\r\n\n\nThis is explicitly disabled in OpenSSL 3: https://www.openssl.org/docs/manmaster/man7/migration_guide.html\r\n\r\nWe may need to add some new OP_* flags to support connecting to these outdated servers.\n\n@wRAR Can the downgrade of OpenSSL, pyopenssl ang cryprography help in this situation?\n\nYou should downgrade pyOpenSSL to a version that ships 1.1, assuming it uses a bundled one in your environment (which I suspect it does).\n\n@wRAR looks like twisted does not support OP flasgs... \n\nYou can usually access the underlying OpenSSL.SSL.Context object and call `set_options()` on it.\n\n@wRAR fixed by downgrading cryptography to  36.0.2\r\nHere's my current `scrapy version --verbose ` result\r\n```\r\nScrapy       : 2.6.1\r\nlxml         : 4.8.0.0\r\nlibxml2      : 2.9.4\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 22.4.0\r\nPython       : 3.9.12 (main, Mar 26 2022, 15:44:31) - [Clang 13.1.6 (clang-1316.0.21.2)]\r\npyOpenSSL    : 22.0.0 (OpenSSL 1.1.1n  15 Mar 2022)\r\ncryptography : 36.0.2\r\nPlatform     : macOS-12.3.1-arm64-arm-64bit\r\n```\r\nNow pyOpenSSL uses OpenSSL 1.1.1n\r\nEven though the problem is fixed, the flags enhancement would be great.\r\nThank you so much @wRAR !\r\n\n\nRight, it's `cryptography` that bundles libssl.\n\nI'm having the same problem when trying to scrape the Cisco website for security advisories. How can I downgrade cryptography to a specific version inside of the scrapy package ?\n\nIt's not \"inside of the scrapy package\", it's in your Python environment.\n\nI've having the same problem with Government of Ireland websites. E.g., \r\n\r\n```\r\nscrapy fetch https://www.courts.ie/\r\n```\r\n\r\nComes back with the error, \r\n\r\n```\r\n2022-07-05 17:33:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.courts.ie/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'unsafe legacy renegotiation disabled')]>]\r\n```\r\n\r\nCurl and various web browsers do fine with it. Downgrading `cryptography` works. Thank you! \r\n\r\nDoes anyone know how to determine exactly what these websites need to upgrade? I.e., if I were going to get in touch with the Government of Ireland webmaster, what would I say? It's not immediately obvious that the bug is not with `cryptography`.\n\nIf you are going to contact the website owner just point them to https://www.ssllabs.com/ssltest/ results for their website.\r\n\r\nI mean, if the task is \"make it work with OpenSSL 3.0 clients\" then the fix is to implement  RFC 5746, but I'm not sure the website owner would really want to fix it, considering that the website works in browsers, and it may not be easy or even possible with the stack they are now using, looking at its other problems. And if the task is \"make it more secure\" then the more important problem is SSL3 not being disabled. \r\n\r\n> It's not immediately obvious that the bug is not with cryptography.\r\n\r\nI agree it's not immediately obvious and some research would be needed. I did it earlier and linked the OpenSSL 3.0 changelog which explicitly lists things that are not supported anymore.\n\nI want to drop this relevant link here:\r\n\r\nhttps://stackoverflow.com/questions/71603314/ssl-error-unsafe-legacy-renegotiation-disabled/72374542#72374542\r\n\r\nHere is a very useful piece of code to circumvent the issue without downgrading:\r\n\r\n````\r\nimport requests\r\nimport urllib3\r\nimport SSL\r\n\r\n\r\nclass CustomHttpAdapter (requests.adapters.HTTPAdapter):\r\n    # \"Transport adapter\" that allows us to use custom ssl_context.\r\n\r\n    def __init__(self, ssl_context=None, **kwargs):\r\n        self.ssl_context = ssl_context\r\n        super().__init__(**kwargs)\r\n\r\n    def init_poolmanager(self, connections, maxsize, block=False):\r\n        self.poolmanager = urllib3.poolmanager.PoolManager(\r\n            num_pools=connections, maxsize=maxsize,\r\n            block=block, ssl_context=self.ssl_context)\r\n\r\n\r\ndef get_legacy_session():\r\n    ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\r\n    ctx.options |= 0x4  # OP_LEGACY_SERVER_CONNECT\r\n    session = requests.session()\r\n    session.mount('https://', CustomHttpAdapter(ctx))\r\n    return session\r\n````\r\n\r\n````\r\nwith (\r\n    get_legacy_session() as s,\r\n    s.get(\"some-url\") as response\r\n):\r\n    print(response.json())\r\n````\n\n(note that Scrapy doesn't use `requests`, so accessing the OpenSSL context from a spider is left as an exercise to the reader)\n\nI ran into the same problem, and built a custom context factory to solve it. I hope it will be helpful for others:\r\n\r\ncontextfactory.py (within MyProject folder)\r\n\r\n```\r\nfrom scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\r\n\r\n\r\nclass LegacyConnectContextFactory(ScrapyClientContextFactory):\r\n\r\n    def getContext(self, hostname=None, port=None):\r\n        ctx = self.getCertificateOptions().getContext()\r\n        ctx.set_options(0x4)\r\n        return ctx\r\n\r\n```\r\n\r\nwithin my_spider.py\r\n\r\n```\r\nclass MySpider(Spider):\r\n    name = \"my_spider\"\r\n\r\n    custom_settings = {\r\n        'DOWNLOADER_CLIENTCONTEXTFACTORY': 'MyProject.contextfactory.LegacyConnectContextFactory',\r\n    }\r\n```\r\n\r\n\r\n\n\nThanks @aysegulc, your suggestion works.\n\n> I ran into the same problem, and built a custom context factory to solve it. I hope it will be helpful for others:\r\n> \r\n> contextfactory.py (within MyProject folder)\r\n> \r\n> ```\r\n> from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\r\n> \r\n> \r\n> class LegacyConnectContextFactory(ScrapyClientContextFactory):\r\n> \r\n>     def getContext(self, hostname=None, port=None):\r\n>         ctx = self.getCertificateOptions().getContext()\r\n>         ctx.set_options(0x4)\r\n>         return ctx\r\n\r\n\r\nthis code works too.\r\n\r\ncontextfactory.py\r\n```python\r\nfrom scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\r\nfrom cryptography.hazmat.bindings.openssl.binding import Binding\r\n\r\n\r\nclass LegacyConnectContextFactory(ScrapyClientContextFactory):\r\n\r\n    def getContext(self, hostname=None, port=None):\r\n        ctx = super(LegacyConnectContextFactory, self).getContext()\r\n        binding = Binding()\r\n        ctx.set_options(binding.lib.SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION)\r\n        return ctx\r\n\r\n```\n\n> I ran into the same problem, and built a custom context factory to solve it. I hope it will be helpful for others:\r\n> \r\n> contextfactory.py (within MyProject folder)\r\n> \r\n> ```\r\n> from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\r\n> \r\n> \r\n> class LegacyConnectContextFactory(ScrapyClientContextFactory):\r\n> \r\n>     def getContext(self, hostname=None, port=None):\r\n>         ctx = self.getCertificateOptions().getContext()\r\n>         ctx.set_options(0x4)\r\n>         return ctx\r\n> ```\r\n> \r\n> within my_spider.py\r\n> \r\n> ```\r\n> class MySpider(Spider):\r\n>     name = \"my_spider\"\r\n> \r\n>     custom_settings = {\r\n>         'DOWNLOADER_CLIENTCONTEXTFACTORY': 'MyProject.contextfactory.LegacyConnectContextFactory',\r\n>     }\r\n> ```\r\n\r\nthanks.",
  "pr_link": "https://github.com/scrapy/scrapy/pull/5790",
  "code_context": [
    {
      "filename": "scrapy/core/downloader/contextfactory.py",
      "content": "import warnings\n\nfrom OpenSSL import SSL\nfrom twisted.internet._sslverify import _setAcceptableProtocols\nfrom twisted.internet.ssl import optionsForClientTLS, CertificateOptions, platformTrust, AcceptableCiphers\nfrom twisted.web.client import BrowserLikePolicyForHTTPS\nfrom twisted.web.iweb import IPolicyForHTTPS\nfrom zope.interface.declarations import implementer\nfrom zope.interface.verify import verifyObject\n\nfrom scrapy.core.downloader.tls import DEFAULT_CIPHERS, openssl_methods, ScrapyClientTLSOptions\nfrom scrapy.utils.misc import create_instance, load_object\n\n\n@implementer(IPolicyForHTTPS)\nclass ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n    \"\"\"\n    Non-peer-certificate verifying HTTPS context factory\n\n    Default OpenSSL method is TLS_METHOD (also called SSLv23_METHOD)\n    which allows TLS protocol negotiation\n\n    'A TLS/SSL connection established with [this method] may\n     understand the TLSv1, TLSv1.1 and TLSv1.2 protocols.'\n    \"\"\"\n\n    def __init__(self, method=SSL.SSLv23_METHOD, tls_verbose_logging=False, tls_ciphers=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._ssl_method = method\n        self.tls_verbose_logging = tls_verbose_logging\n        if tls_ciphers:\n            self.tls_ciphers = AcceptableCiphers.fromOpenSSLCipherString(tls_ciphers)\n        else:\n            self.tls_ciphers = DEFAULT_CIPHERS\n\n    @classmethod\n    def from_settings(cls, settings, method=SSL.SSLv23_METHOD, *args, **kwargs):\n        tls_verbose_logging = settings.getbool('DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING')\n        tls_ciphers = settings['DOWNLOADER_CLIENT_TLS_CIPHERS']\n        return cls(method=method, tls_verbose_logging=tls_verbose_logging, tls_ciphers=tls_ciphers, *args, **kwargs)\n\n    def getCertificateOptions(self):\n        # setting verify=True will require you to provide CAs\n        # to verify against; in other words: it's not that simple\n\n        # backward-compatible SSL/TLS method:\n        #\n        # * this will respect `method` attribute in often recommended\n        #   `ScrapyClientContextFactory` subclass\n        #   (https://github.com/scrapy/scrapy/issues/1429#issuecomment-131782133)\n        #\n        # * getattr() for `_ssl_method` attribute for context factories\n        #   not calling super().__init__\n        return CertificateOptions(\n            verify=False,\n            method=getattr(self, 'method', getattr(self, '_ssl_method', None)),\n            fixBrokenPeers=True,\n            acceptableCiphers=self.tls_ciphers,\n        )\n\n    # kept for old-style HTTP/1.0 downloader context twisted calls,\n    # e.g. connectSSL()\n    def getContext(self, hostname=None, port=None):\n        ctx = self.getCertificateOptions().getContext()\n        ctx.set_options(0x4)  # OP_LEGACY_SERVER_CONNECT\n        return ctx\n\n    def creatorForNetloc(self, hostname, port):\n        return ScrapyClientTLSOptions(hostname.decode(\"ascii\"), self.getContext(),\n                                      verbose_logging=self.tls_verbose_logging)\n\n\n@implementer(IPolicyForHTTPS)\nclass BrowserLikeContextFactory(ScrapyClientContextFactory):\n    \"\"\"\n    Twisted-recommended context factory for web clients.\n\n    Quoting the documentation of the :class:`~twisted.web.client.Agent` class:\n\n        The default is to use a\n        :class:`~twisted.web.client.BrowserLikePolicyForHTTPS`, so unless you\n        have special requirements you can leave this as-is.\n\n    :meth:`creatorForNetloc` is the same as\n    :class:`~twisted.web.client.BrowserLikePolicyForHTTPS` except this context\n    factory allows setting the TLS/SSL method to use.\n\n    The default OpenSSL method is ``TLS_METHOD`` (also called\n    ``SSLv23_METHOD``) which allows TLS protocol negotiation.\n    \"\"\"\n\n    def creatorForNetloc(self, hostname, port):\n        # trustRoot set to platformTrust() will use the platform's root CAs.\n        #\n        # This means that a website like https://www.cacert.org will be rejected\n        # by default, since CAcert.org CA certificate is seldom shipped.\n        return optionsForClientTLS(\n            hostname=hostname.decode(\"ascii\"),\n            trustRoot=platformTrust(),\n            extraCertificateOptions={'method': self._ssl_method},\n        )\n\n\n@implementer(IPolicyForHTTPS)\nclass AcceptableProtocolsContextFactory:\n    \"\"\"Context factory to used to override the acceptable protocols\n    to set up the [OpenSSL.SSL.Context] for doing NPN and/or ALPN\n    negotiation.\n    \"\"\"\n\n    def __init__(self, context_factory, acceptable_protocols):\n        verifyObject(IPolicyForHTTPS, context_factory)\n        self._wrapped_context_factory = context_factory\n        self._acceptable_protocols = acceptable_protocols\n\n    def creatorForNetloc(self, hostname, port):\n        options = self._wrapped_context_factory.creatorForNetloc(hostname, port)\n        _setAcceptableProtocols(options._ctx, self._acceptable_protocols)\n        return options\n\n\ndef load_context_factory_from_settings(settings, crawler):\n    ssl_method = openssl_methods[settings.get('DOWNLOADER_CLIENT_TLS_METHOD')]\n    context_factory_cls = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n    # try method-aware context factory\n    try:\n        context_factory = create_instance(\n            objcls=context_factory_cls,\n            settings=settings,\n            crawler=crawler,\n            method=ssl_method,\n        )\n    except TypeError:\n        # use context factory defaults\n        context_factory = create_instance(\n            objcls=context_factory_cls,\n            settings=settings,\n            crawler=crawler,\n        )\n        msg = (\n            f\"{settings['DOWNLOADER_CLIENTCONTEXTFACTORY']} does not accept \"\n            \"a `method` argument (type OpenSSL.SSL method, e.g. \"\n            \"OpenSSL.SSL.SSLv23_METHOD) and/or a `tls_verbose_logging` \"\n            \"argument and/or a `tls_ciphers` argument. Please, upgrade your \"\n            \"context factory class to handle them or ignore them.\"\n        )\n        warnings.warn(msg)\n\n    return context_factory\n"
    },
    {
      "filename": "scrapy/core/downloader/tls.py",
      "content": "import logging\n\nfrom OpenSSL import SSL\nfrom service_identity.exceptions import CertificateError\nfrom twisted.internet._sslverify import ClientTLSOptions, verifyHostname, VerificationError\nfrom twisted.internet.ssl import AcceptableCiphers\n\nfrom scrapy.utils.ssl import x509name_to_string, get_temp_key_info\n\nlogger = logging.getLogger(__name__)\n\n\nMETHOD_TLS = 'TLS'\nMETHOD_TLSv10 = 'TLSv1.0'\nMETHOD_TLSv11 = 'TLSv1.1'\nMETHOD_TLSv12 = 'TLSv1.2'\n\n\nopenssl_methods = {\n    METHOD_TLS: SSL.SSLv23_METHOD,      # protocol negotiation (recommended)\n    METHOD_TLSv10: SSL.TLSv1_METHOD,    # TLS 1.0 only\n    METHOD_TLSv11: SSL.TLSv1_1_METHOD,  # TLS 1.1 only\n    METHOD_TLSv12: SSL.TLSv1_2_METHOD,  # TLS 1.2 only\n}\n\n\nclass ScrapyClientTLSOptions(ClientTLSOptions):\n    \"\"\"\n    SSL Client connection creator ignoring certificate verification errors\n    (for genuinely invalid certificates or bugs in verification code).\n\n    Same as Twisted's private _sslverify.ClientTLSOptions,\n    except that VerificationError, CertificateError and ValueError\n    exceptions are caught, so that the connection is not closed, only\n    logging warnings. Also, HTTPS connection parameters logging is added.\n    \"\"\"\n\n    def __init__(self, hostname, ctx, verbose_logging=False):\n        super().__init__(hostname, ctx)\n        self.verbose_logging = verbose_logging\n\n    def _identityVerifyingInfoCallback(self, connection, where, ret):\n        if where & SSL.SSL_CB_HANDSHAKE_START:\n            connection.set_tlsext_host_name(self._hostnameBytes)\n        elif where & SSL.SSL_CB_HANDSHAKE_DONE:\n            if self.verbose_logging:\n                logger.debug('SSL connection to %s using protocol %s, cipher %s',\n                             self._hostnameASCII,\n                             connection.get_protocol_version_name(),\n                             connection.get_cipher_name(),\n                             )\n                server_cert = connection.get_peer_certificate()\n                logger.debug('SSL connection certificate: issuer \"%s\", subject \"%s\"',\n                             x509name_to_string(server_cert.get_issuer()),\n                             x509name_to_string(server_cert.get_subject()),\n                             )\n                key_info = get_temp_key_info(connection._ssl)\n                if key_info:\n                    logger.debug('SSL temp key: %s', key_info)\n\n            try:\n                verifyHostname(connection, self._hostnameASCII)\n            except (CertificateError, VerificationError) as e:\n                logger.warning(\n                    'Remote certificate is not valid for hostname \"%s\"; %s',\n                    self._hostnameASCII, e)\n\n            except ValueError as e:\n                logger.warning(\n                    'Ignoring error while verifying certificate '\n                    'from host \"%s\" (exception: %r)',\n                    self._hostnameASCII, e)\n\n\nDEFAULT_CIPHERS = AcceptableCiphers.fromOpenSSLCipherString('DEFAULT')\n"
    },
    {
      "filename": "scrapy/utils/ssl.py",
      "content": "import OpenSSL.SSL\nimport OpenSSL._util as pyOpenSSLutil\n\nfrom scrapy.utils.python import to_unicode\n\n\ndef ffi_buf_to_string(buf):\n    return to_unicode(pyOpenSSLutil.ffi.string(buf))\n\n\ndef x509name_to_string(x509name):\n    # from OpenSSL.crypto.X509Name.__repr__\n    result_buffer = pyOpenSSLutil.ffi.new(\"char[]\", 512)\n    pyOpenSSLutil.lib.X509_NAME_oneline(x509name._name, result_buffer, len(result_buffer))\n\n    return ffi_buf_to_string(result_buffer)\n\n\ndef get_temp_key_info(ssl_object):\n    # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n    temp_key_p = pyOpenSSLutil.ffi.new(\"EVP_PKEY **\")\n    if not pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p):\n        return None\n    temp_key = temp_key_p[0]\n    if temp_key == pyOpenSSLutil.ffi.NULL:\n        return None\n    temp_key = pyOpenSSLutil.ffi.gc(temp_key, pyOpenSSLutil.lib.EVP_PKEY_free)\n    key_info = []\n    key_type = pyOpenSSLutil.lib.EVP_PKEY_id(temp_key)\n    if key_type == pyOpenSSLutil.lib.EVP_PKEY_RSA:\n        key_info.append('RSA')\n    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_DH:\n        key_info.append('DH')\n    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_EC:\n        key_info.append('ECDH')\n        ec_key = pyOpenSSLutil.lib.EVP_PKEY_get1_EC_KEY(temp_key)\n        ec_key = pyOpenSSLutil.ffi.gc(ec_key, pyOpenSSLutil.lib.EC_KEY_free)\n        nid = pyOpenSSLutil.lib.EC_GROUP_get_curve_name(pyOpenSSLutil.lib.EC_KEY_get0_group(ec_key))\n        cname = pyOpenSSLutil.lib.EC_curve_nid2nist(nid)\n        if cname == pyOpenSSLutil.ffi.NULL:\n            cname = pyOpenSSLutil.lib.OBJ_nid2sn(nid)\n        key_info.append(ffi_buf_to_string(cname))\n    else:\n        key_info.append(ffi_buf_to_string(pyOpenSSLutil.lib.OBJ_nid2sn(key_type)))\n    key_info.append(f'{pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key)} bits')\n    return ', '.join(key_info)\n\n\ndef get_openssl_version():\n    system_openssl = OpenSSL.SSL.SSLeay_version(\n        OpenSSL.SSL.SSLEAY_VERSION\n    ).decode('ascii', errors='replace')\n    return f'{OpenSSL.version.__version__} ({system_openssl})'\n"
    },
    {
      "filename": "tests/mockserver.py",
      "content": "import argparse\nimport json\nimport random\nimport sys\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom subprocess import Popen, PIPE\nfrom tempfile import mkdtemp\nfrom urllib.parse import urlencode\n\nfrom OpenSSL import SSL\nfrom twisted.internet import defer, reactor, ssl\nfrom twisted.internet.task import deferLater\nfrom twisted.names import dns, error\nfrom twisted.names.server import DNSServerFactory\nfrom twisted.web import resource, server\nfrom twisted.web.server import GzipEncoderFactory, NOT_DONE_YET, Site\nfrom twisted.web.static import File\nfrom twisted.web.util import redirectTo\n\nfrom scrapy.utils.python import to_bytes, to_unicode\nfrom scrapy.utils.test import get_testenv\n\n\ndef getarg(request, name, default=None, type=None):\n    if name in request.args:\n        value = request.args[name][0]\n        if type is not None:\n            value = type(value)\n        return value\n    return default\n\n\n# most of the following resources are copied from twisted.web.test.test_webclient\nclass ForeverTakingResource(resource.Resource):\n    \"\"\"\n    L{ForeverTakingResource} is a resource which never finishes responding\n    to requests.\n    \"\"\"\n\n    def __init__(self, write=False):\n        resource.Resource.__init__(self)\n        self._write = write\n\n    def render(self, request):\n        if self._write:\n            request.write(b\"some bytes\")\n        return server.NOT_DONE_YET\n\n\nclass ErrorResource(resource.Resource):\n    def render(self, request):\n        request.setResponseCode(401)\n        if request.args.get(b\"showlength\"):\n            request.setHeader(b\"content-length\", b\"0\")\n        return b\"\"\n\n\nclass NoLengthResource(resource.Resource):\n    def render(self, request):\n        return b\"nolength\"\n\n\nclass HostHeaderResource(resource.Resource):\n    \"\"\"\n    A testing resource which renders itself as the value of the host header\n    from the request.\n    \"\"\"\n\n    def render(self, request):\n        return request.requestHeaders.getRawHeaders(b\"host\")[0]\n\n\nclass PayloadResource(resource.Resource):\n    \"\"\"\n    A testing resource which renders itself as the contents of the request body\n    as long as the request body is 100 bytes long, otherwise which renders\n    itself as C{\"ERROR\"}.\n    \"\"\"\n\n    def render(self, request):\n        data = request.content.read()\n        contentLength = request.requestHeaders.getRawHeaders(b\"content-length\")[0]\n        if len(data) != 100 or int(contentLength) != 100:\n            return b\"ERROR\"\n        return data\n\n\nclass BrokenDownloadResource(resource.Resource):\n    def render(self, request):\n        # only sends 3 bytes even though it claims to send 5\n        request.setHeader(b\"content-length\", b\"5\")\n        request.write(b\"abc\")\n        return b\"\"\n\n\nclass LeafResource(resource.Resource):\n\n    isLeaf = True\n\n    def deferRequest(self, request, delay, f, *a, **kw):\n        def _cancelrequest(_):\n            # silence CancelledError\n            d.addErrback(lambda _: None)\n            d.cancel()\n\n        d = deferLater(reactor, delay, f, *a, **kw)\n        request.notifyFinish().addErrback(_cancelrequest)\n        return d\n\n\nclass Follow(LeafResource):\n\n    def render(self, request):\n        total = getarg(request, b\"total\", 100, type=int)\n        show = getarg(request, b\"show\", 1, type=int)\n        order = getarg(request, b\"order\", b\"desc\")\n        maxlatency = getarg(request, b\"maxlatency\", 0, type=float)\n        n = getarg(request, b\"n\", total, type=int)\n        if order == b\"rand\":\n            nlist = [random.randint(1, total) for _ in range(show)]\n        else:  # order == \"desc\"\n            nlist = range(n, max(n - show, 0), -1)\n\n        lag = random.random() * maxlatency\n        self.deferRequest(request, lag, self.renderRequest, request, nlist)\n        return NOT_DONE_YET\n\n    def renderRequest(self, request, nlist):\n        s = \"\"\"<html> <head></head> <body>\"\"\"\n        args = request.args.copy()\n        for nl in nlist:\n            args[b\"n\"] = [to_bytes(str(nl))]\n            argstr = urlencode(args, doseq=True)\n            s += f\"<a href='/follow?{argstr}'>follow {nl}</a><br>\"\n        s += \"\"\"</body>\"\"\"\n        request.write(to_bytes(s))\n        request.finish()\n\n\nclass Delay(LeafResource):\n\n    def render_GET(self, request):\n        n = getarg(request, b\"n\", 1, type=float)\n        b = getarg(request, b\"b\", 1, type=int)\n        if b:\n            # send headers now and delay body\n            request.write('')\n        self.deferRequest(request, n, self._delayedRender, request, n)\n        return NOT_DONE_YET\n\n    def _delayedRender(self, request, n):\n        request.write(to_bytes(f\"Response delayed for {n:.3f} seconds\\n\"))\n        request.finish()\n\n\nclass Status(LeafResource):\n\n    def render_GET(self, request):\n        n = getarg(request, b\"n\", 200, type=int)\n        request.setResponseCode(n)\n        return b\"\"\n\n\nclass Raw(LeafResource):\n\n    def render_GET(self, request):\n        request.startedWriting = 1\n        self.deferRequest(request, 0, self._delayedRender, request)\n        return NOT_DONE_YET\n    render_POST = render_GET\n\n    def _delayedRender(self, request):\n        raw = getarg(request, b'raw', b'HTTP 1.1 200 OK\\n')\n        request.startedWriting = 1\n        request.write(raw)\n        request.channel.transport.loseConnection()\n        request.finish()\n\n\nclass Echo(LeafResource):\n\n    def render_GET(self, request):\n        output = {\n            'headers': dict(\n                (to_unicode(k), [to_unicode(v) for v in vs])\n                for k, vs in request.requestHeaders.getAllRawHeaders()),\n            'body': to_unicode(request.content.read()),\n        }\n        return to_bytes(json.dumps(output))\n    render_POST = render_GET\n\n\nclass RedirectTo(LeafResource):\n\n    def render(self, request):\n        goto = getarg(request, b'goto', b'/')\n        # we force the body content, otherwise Twisted redirectTo()\n        # returns HTML with <meta http-equiv=\"refresh\"\n        redirectTo(goto, request)\n        return b'redirecting...'\n\n\nclass Partial(LeafResource):\n\n    def render_GET(self, request):\n        request.setHeader(b\"Content-Length\", b\"1024\")\n        self.deferRequest(request, 0, self._delayedRender, request)\n        return NOT_DONE_YET\n\n    def _delayedRender(self, request):\n        request.write(b\"partial content\\n\")\n        request.finish()\n\n\nclass Drop(Partial):\n\n    def _delayedRender(self, request):\n        abort = getarg(request, b\"abort\", 0, type=int)\n        request.write(b\"this connection will be dropped\\n\")\n        tr = request.channel.transport\n        try:\n            if abort and hasattr(tr, 'abortConnection'):\n                tr.abortConnection()\n            else:\n                tr.loseConnection()\n        finally:\n            request.finish()\n\n\nclass ArbitraryLengthPayloadResource(LeafResource):\n\n    def render(self, request):\n        return request.content.read()\n\n\nclass Root(resource.Resource):\n\n    def __init__(self):\n        resource.Resource.__init__(self)\n        self.putChild(b\"status\", Status())\n        self.putChild(b\"follow\", Follow())\n        self.putChild(b\"delay\", Delay())\n        self.putChild(b\"partial\", Partial())\n        self.putChild(b\"drop\", Drop())\n        self.putChild(b\"raw\", Raw())\n        self.putChild(b\"echo\", Echo())\n        self.putChild(b\"payload\", PayloadResource())\n        self.putChild(b\"xpayload\", resource.EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))\n        self.putChild(b\"alpayload\", ArbitraryLengthPayloadResource())\n        try:\n            from tests import tests_datadir\n            self.putChild(b\"files\", File(str(Path(tests_datadir, 'test_site/files/'))))\n        except Exception:\n            pass\n        self.putChild(b\"redirect-to\", RedirectTo())\n\n    def getChild(self, name, request):\n        return self\n\n    def render(self, request):\n        return b'Scrapy mock HTTP server\\n'\n\n\nclass MockServer:\n\n    def __enter__(self):\n        self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver', '-t', 'http'],\n                          stdout=PIPE, env=get_testenv())\n        http_address = self.proc.stdout.readline().strip().decode('ascii')\n        https_address = self.proc.stdout.readline().strip().decode('ascii')\n\n        self.http_address = http_address\n        self.https_address = https_address\n\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.proc.kill()\n        self.proc.communicate()\n\n    def url(self, path, is_secure=False):\n        host = self.https_address if is_secure else self.http_address\n        host = host.replace('0.0.0.0', '127.0.0.1')\n        return host + path\n\n\nclass MockDNSResolver:\n    \"\"\"\n    Implements twisted.internet.interfaces.IResolver partially\n    \"\"\"\n\n    def _resolve(self, name):\n        record = dns.Record_A(address=b\"127.0.0.1\")\n        answer = dns.RRHeader(name=name, payload=record)\n        return [answer], [], []\n\n    def query(self, query, timeout=None):\n        if query.type == dns.A:\n            return defer.succeed(self._resolve(query.name.name))\n        return defer.fail(error.DomainError())\n\n    def lookupAllRecords(self, name, timeout=None):\n        return defer.succeed(self._resolve(name))\n\n\nclass MockDNSServer:\n\n    def __enter__(self):\n        self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver', '-t', 'dns'],\n                          stdout=PIPE, env=get_testenv())\n        self.host = '127.0.0.1'\n        self.port = int(self.proc.stdout.readline().strip().decode('ascii').split(\":\")[1])\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.proc.kill()\n        self.proc.communicate()\n\n\nclass MockFTPServer:\n    \"\"\"Creates an FTP server on port 2121 with a default passwordless user\n    (anonymous) and a temporary root path that you can read from the\n    :attr:`path` attribute.\"\"\"\n\n    def __enter__(self):\n        self.path = Path(mkdtemp())\n        self.proc = Popen([sys.executable, '-u', '-m', 'tests.ftpserver', '-d', str(self.path)],\n                          stderr=PIPE, env=get_testenv())\n        for line in self.proc.stderr:\n            if b'starting FTP server' in line:\n                break\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        rmtree(str(self.path))\n        self.proc.kill()\n        self.proc.communicate()\n\n    def url(self, path):\n        return 'ftp://127.0.0.1:2121/' + path\n\n\ndef ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.crt', cipher_string=None):\n    factory = ssl.DefaultOpenSSLContextFactory(\n        str(Path(__file__).parent / keyfile),\n        str(Path(__file__).parent / certfile),\n    )\n    if cipher_string:\n        ctx = factory.getContext()\n        # disabling TLS1.3 because it unconditionally enables some strong ciphers\n        ctx.set_options(SSL.OP_CIPHER_SERVER_PREFERENCE | SSL.OP_NO_TLSv1_3)\n        ctx.set_cipher_list(to_bytes(cipher_string))\n    return factory\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-t\", \"--type\", type=str, choices=(\"http\", \"dns\"), default=\"http\")\n    args = parser.parse_args()\n\n    if args.type == \"http\":\n        root = Root()\n        factory = Site(root)\n        httpPort = reactor.listenTCP(0, factory)\n        contextFactory = ssl_context_factory()\n        httpsPort = reactor.listenSSL(0, factory, contextFactory)\n\n        def print_listening():\n            httpHost = httpPort.getHost()\n            httpsHost = httpsPort.getHost()\n            httpAddress = f'http://{httpHost.host}:{httpHost.port}'\n            httpsAddress = f'https://{httpsHost.host}:{httpsHost.port}'\n            print(httpAddress)\n            print(httpsAddress)\n\n    elif args.type == \"dns\":\n        clients = [MockDNSResolver()]\n        factory = DNSServerFactory(clients=clients)\n        protocol = dns.DNSDatagramProtocol(controller=factory)\n        listener = reactor.listenUDP(0, protocol)\n\n        def print_listening():\n            host = listener.getHost()\n            print(f\"{host.host}:{host.port}\")\n\n    reactor.callWhenRunning(print_listening)\n    reactor.run()\n"
    }
  ],
  "questions": [
    "I'm having the same problem when trying to scrape the Cisco website for security advisories. How can I downgrade cryptography to a specific version inside of the scrapy package ?",
    "I've having the same problem with Government of Ireland websites. E.g., \r\n\r\n```\r\nscrapy fetch https://www.courts.ie/\r\n```\r\n\r\nComes back with the error, \r\n\r\n```\r\n2022-07-05 17:33:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.courts.ie/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'unsafe legacy renegotiation disabled')]>]\r\n```\r\n\r\nCurl and various web browsers do fine with it. Downgrading `cryptography` works. Thank you! \r\n\r\nDoes anyone know how to determine exactly what these websites need to upgrade? I.e., if I were going to get in touch with the Government of Ireland webmaster, what would I say? It's not immediately obvious that the bug is not with `cryptography`."
  ],
  "golden_answers": [
    "I've having the same problem with Government of Ireland websites. E.g., \r\n\r\n```\r\nscrapy fetch https://www.courts.ie/\r\n```\r\n\r\nComes back with the error, \r\n\r\n```\r\n2022-07-05 17:33:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.courts.ie/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'unsafe legacy renegotiation disabled')]>]\r\n```\r\n\r\nCurl and various web browsers do fine with it. Downgrading `cryptography` works. Thank you! \r\n\r\nDoes anyone know how to determine exactly what these websites need to upgrade? I.e., if I were going to get in touch with the Government of Ireland webmaster, what would I say? It's not immediately obvious that the bug is not with `cryptography`.",
    "I want to drop this relevant link here:\r\n\r\nhttps://stackoverflow.com/questions/71603314/ssl-error-unsafe-legacy-renegotiation-disabled/72374542#72374542\r\n\r\nHere is a very useful piece of code to circumvent the issue without downgrading:\r\n\r\n````\r\nimport requests\r\nimport urllib3\r\nimport SSL\r\n\r\n\r\nclass CustomHttpAdapter (requests.adapters.HTTPAdapter):\r\n    # \"Transport adapter\" that allows us to use custom ssl_context.\r\n\r\n    def __init__(self, ssl_context=None, **kwargs):\r\n        self.ssl_context = ssl_context\r\n        super().__init__(**kwargs)\r\n\r\n    def init_poolmanager(self, connections, maxsize, block=False):\r\n        self.poolmanager = urllib3.poolmanager.PoolManager(\r\n            num_pools=connections, maxsize=maxsize,\r\n            block=block, ssl_context=self.ssl_context)\r\n\r\n\r\ndef get_legacy_session():\r\n    ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\r\n    ctx.options |= 0x4  # OP_LEGACY_SERVER_CONNECT\r\n    session = requests.session()\r\n    session.mount('https://', CustomHttpAdapter(ctx))\r\n    return session\r\n````\r\n\r\n````\r\nwith (\r\n    get_legacy_session() as s,\r\n    s.get(\"some-url\") as response\r\n):\r\n    print(response.json())\r\n````"
  ],
  "questions_generated": [
    "What is the main cause of the 'OpenSSL unsafe legacy renegotiation disabled' error in this Scrapy project?",
    "How can you modify the ScrapyClientContextFactory to support connecting to servers requiring legacy renegotiation?",
    "What workaround was suggested to resolve the SSL error without modifying the Scrapy codebase?",
    "In the context of the issue discussed, what role does the 'cryptography' library play in the SSL/TLS handshake process?",
    "Why might downgrading 'pyOpenSSL' alone not resolve the issue with legacy renegotiation?"
  ],
  "golden_answers_generated": [
    "The error occurs because the version of OpenSSL used in the project (OpenSSL 3.0.3) has disabled unsafe legacy renegotiation by default. This change affects the ability to connect to servers that require legacy renegotiation, leading to the SSL error.",
    "To support servers requiring legacy renegotiation, you would need to modify the underlying OpenSSL.SSL.Context object within the ScrapyClientContextFactory to set appropriate SSL options using the `set_options()` method. However, this requires that the Twisted library supports such operations, which might not be straightforward.",
    "The suggested workaround was to downgrade the 'cryptography' package to a version that ships with OpenSSL 1.1.1, which still supports the legacy renegotiation feature. This was done by downgrading 'cryptography' to version 36.0.2, which resolved the issue by reverting to an older OpenSSL version.",
    "The 'cryptography' library provides bindings to OpenSSL, which is used for the SSL/TLS handshake process. It is responsible for creating and managing SSL contexts, which include protocol versions, ciphers, and other cryptographic settings necessary for establishing secure connections.",
    "Downgrading 'pyOpenSSL' alone might not resolve the issue because 'pyOpenSSL' relies on the version of OpenSSL provided by the 'cryptography' library. If 'cryptography' is still using OpenSSL 3.0.3, which disables legacy renegotiation, downgrading 'pyOpenSSL' will not change the underlying OpenSSL version being used."
  ]
}