{
  "repo_name": "mindsdb_mindsdb",
  "issue_id": "2425",
  "issue_description": "# [New Integration]: Store files in AWS S3 and Google Cloud Storage\n\n### Is there an existing integration?\n\n- [X] I have searched the existing integrations.\n\n### Use Case\n\nThis will allow to host the software on a serverless and stateless service. This will facilitate scaling because no critical data will be on the disk of the system running the application.\n\n### Motivation\n\nAllow users to save resources and money.\n\n### Implementation\n\n_No response_\n\n### Anything else?\n\n_No response_",
  "issue_comments": [
    {
      "id": 1187723500,
      "user": "MinuraPunchihewa",
      "body": "@EZFRICA and @ZoranPandovski Sorry for the trouble, but is this about pulling data from files that are stored in S3 and GCS?"
    },
    {
      "id": 1189184811,
      "user": "EZFRICA",
      "body": "No, the goal here is to use S3 and GCS as the file storage system for MindsDB. But I think it would be interesting to add your idea of extracting data from stored files."
    },
    {
      "id": 1189376632,
      "user": "MinuraPunchihewa",
      "body": "@EZFRICA Thank you for clearing that up!"
    },
    {
      "id": 1189445886,
      "user": "EZFRICA",
      "body": "You're welcome!\n\nLe mar. 19 juil. 2022 à 18:42, Minura Punchihewa ***@***.***>\na écrit :\n\n> @EZFRICA <https://github.com/EZFRICA> Thank you for clearing that up!\n>\n> —\n> Reply to this email directly, view it on GitHub\n> <https://github.com/mindsdb/mindsdb/issues/2425#issuecomment-1189376632>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIQDOZ662L62TBWTZ2RBF53VU3SJRANCNFSM52NSOY7Q>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n"
    },
    {
      "id": 1272479267,
      "user": "h1t35h",
      "body": "Hey,\r\nI'm interested in this. Can I pick it up? It would be helpful if you can add some context as well. "
    },
    {
      "id": 1272622182,
      "user": "EZFRICA",
      "body": "Hi\r\nYes, you can.\r\n\r\n**Context**\r\nIn the configuration file we have below the **storage_dir** parameter is used to configure the file storage directory for [mindsdb](https://docs.mindsdb.com/setup/self-hosted/docker/) on the host machine. \r\n\r\n1. The main objective here is to be able to store files on other spaces (AWS S3, Google Cloud Storage, ...) different from the host [mindsdb](https://docs.mindsdb.com/setup/self-hosted/docker/). This will allow to host the software on a serverless and stateless service. This will facilitate scaling because no critical data will be on the disk of the system running the application.\r\n\r\n2. The @MinuraPunchihewa proposal to extract data from files stored in AWS S3 and Google Cloud Storage.\r\n\r\n`\r\n \"config_version\":\"1.4\",\r\n \"storage_dir\": \"/root/mdb_storage\",\r\n \"log\": { \r\n     \"level\": {\r\n         \"console\": \"ERROR\",\r\n     \"file\": \"WARNING\",\r\n     \"db\": \"WARNING\"\r\n          } \r\n        },\r\n \"debug\": false,\r\n \"integrations\": {},\r\n \"api\":\r\n  {\r\n   \"http\": {\r\n       \"host\": \"0.0.0.0\",\r\n       \"port\": \"47334\"\r\n           },\r\n   \"mysql\": {\r\n       \"host\": \"0.0.0.0\",\r\n       \"password\": \"\",\r\n       \"port\": \"47335\",\r\n       \"user\": \"mindsdb\",\r\n       \"database\": \"mindsdb\",\r\n       \"ssl\": true\r\n            },\r\n   \"mongodb\": {\r\n       \"host\": \"0.0.0.0\",\r\n       \"port\": \"47336\",\r\n       \"database\": \"mindsdb\"\r\n              }\r\n  }\r\n}\r\n`"
    },
    {
      "id": 1273189564,
      "user": "h1t35h",
      "body": "Thanks that helps,\r\nI do have bunch of follow up questions though. These would help in taking a better call as to how to refactor the changes. \r\n\r\nThe current `storage_dir` seems to be passed around as root across the code base which implies there are direct indirect dependencies on assumption that bunch of things are locally available. As you are the maintainer I want to understand your view on how best to deal with the _backward compatibility_ issue. Should I just introduce a new `storage_dir_v2` and build on top of it for now?\r\n\r\nAlso, I want to understand your thoughts on the below approaches I'm thinking of: \r\n\r\n## Mount the distributed file system as local volume in docker container\r\nSolve this a docker problem we mount s3/gcs to the docker container and use it across the board. \r\n\r\n**Pros**\r\n* This solves the across the board (We no longer maintain the headache of multiple storage integration to our code everything is a local filesystem).  \r\n\r\n**Cons**\r\n* The issue with this is not every distributed file system would have a readily mountable docker support (A new library idea may be). \r\n* Any systems that have their own setup environments / scripts would not be able to use this solution.  \r\n\r\n## Implement filesystem layers in code\r\nAnother solve could be we implement FS layers like the one I was able to see for supporting DB.\r\n\r\n**Pros**\r\n* We are now truly serverless for any environment that offers compute and python support. FWIW, customers can run their DB layer on any compute platform and it will work. (Given some more investment - check-pointing, faster loads, synchronizations and some smart partitioning strategy).\r\n* Changes the dynamics and cost for the complete project.  \r\n\r\n**Cons**\r\n* There's a very high chance I won't be able to make it a clean cut solve with the current codebase. (Definitely, open to ideas if I'm misreading the way it needs to be done).  \r\n* Every new filesystem we want to update / add would be a new support implementation.\r\n\r\n\r\n"
    },
    {
      "id": 1296259741,
      "user": "EZFRICA",
      "body": "Sorry for the late reply.\r\nAlready for starters, this is an option. Google Cloud Storage and AWS S3 configuration will be options to set up the storage directory. There will be a current for each storage service. \r\n\r\nThe most important thing is that the file system is able to handle the path to the files stored on Cloud Storage and AWS S3. \r\n\r\nI think the option \"Mount the distributed file system as local volume in docker container\" is good."
    },
    {
      "id": 1323277268,
      "user": "h1t35h",
      "body": "Apologies, \r\nThis is taking longer than I anticipated.. Still on it though will keep updating on status."
    },
    {
      "id": 1557086306,
      "user": "elhe26",
      "body": "Any updates on GCS?"
    },
    {
      "id": 1564081659,
      "user": "h1t35h",
      "body": "Hey!\r\nHaven't been able to make much progress on it since. If someone else wants to pick it up they can."
    },
    {
      "id": 1619714385,
      "user": "h1t35h",
      "body": "More updates on this. I did have a successful run with GCS but the current docker configurations shares `/minds/var` across multiple containers which fails when only the main docker container is setup for using GCS. I'm currently working on fixing that. Open to ideas from folks that have setup the current docker compose. Is there a possibility to connect with them."
    },
    {
      "id": 1623443400,
      "user": "h1t35h",
      "body": "Added changes to the release and beta images. The catch is not everything can be on the cloud storage as the performance takes a massive hit for static assets etc. I've added changes to allow storage_db and storage/ to be moved to cloud and everything else logs, cache, static assets still stay on the local disk for performance reasons. The state of MindsDB changes are persisted across multiple docker containers. We can introduce and read and write instance separation if we are interested in that setup."
    },
    {
      "id": 1633998709,
      "user": "nanubau",
      "body": "@h1t35h  is this still open?"
    },
    {
      "id": 1634123599,
      "user": "h1t35h",
      "body": "Fortunately, not for too long. I've raised the PR for team to review and we should be able to close it soon."
    },
    {
      "id": 1752576289,
      "user": "Anuragwagh",
      "body": "is this still open & are you working on it?"
    },
    {
      "id": 1752579042,
      "user": "h1t35h",
      "body": "It's done already I have added the Pull request : https://github.com/mindsdb/mindsdb/pull/6782."
    }
  ],
  "text_context": "# [New Integration]: Store files in AWS S3 and Google Cloud Storage\n\n### Is there an existing integration?\n\n- [X] I have searched the existing integrations.\n\n### Use Case\n\nThis will allow to host the software on a serverless and stateless service. This will facilitate scaling because no critical data will be on the disk of the system running the application.\n\n### Motivation\n\nAllow users to save resources and money.\n\n### Implementation\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n@EZFRICA and @ZoranPandovski Sorry for the trouble, but is this about pulling data from files that are stored in S3 and GCS?\n\nNo, the goal here is to use S3 and GCS as the file storage system for MindsDB. But I think it would be interesting to add your idea of extracting data from stored files.\n\n@EZFRICA Thank you for clearing that up!\n\nYou're welcome!\n\nLe mar. 19 juil. 2022 à 18:42, Minura Punchihewa ***@***.***>\na écrit :\n\n> @EZFRICA <https://github.com/EZFRICA> Thank you for clearing that up!\n>\n> —\n> Reply to this email directly, view it on GitHub\n> <https://github.com/mindsdb/mindsdb/issues/2425#issuecomment-1189376632>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIQDOZ662L62TBWTZ2RBF53VU3SJRANCNFSM52NSOY7Q>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n\n\nHey,\r\nI'm interested in this. Can I pick it up? It would be helpful if you can add some context as well. \n\nHi\r\nYes, you can.\r\n\r\n**Context**\r\nIn the configuration file we have below the **storage_dir** parameter is used to configure the file storage directory for [mindsdb](https://docs.mindsdb.com/setup/self-hosted/docker/) on the host machine. \r\n\r\n1. The main objective here is to be able to store files on other spaces (AWS S3, Google Cloud Storage, ...) different from the host [mindsdb](https://docs.mindsdb.com/setup/self-hosted/docker/). This will allow to host the software on a serverless and stateless service. This will facilitate scaling because no critical data will be on the disk of the system running the application.\r\n\r\n2. The @MinuraPunchihewa proposal to extract data from files stored in AWS S3 and Google Cloud Storage.\r\n\r\n`\r\n \"config_version\":\"1.4\",\r\n \"storage_dir\": \"/root/mdb_storage\",\r\n \"log\": { \r\n     \"level\": {\r\n         \"console\": \"ERROR\",\r\n     \"file\": \"WARNING\",\r\n     \"db\": \"WARNING\"\r\n          } \r\n        },\r\n \"debug\": false,\r\n \"integrations\": {},\r\n \"api\":\r\n  {\r\n   \"http\": {\r\n       \"host\": \"0.0.0.0\",\r\n       \"port\": \"47334\"\r\n           },\r\n   \"mysql\": {\r\n       \"host\": \"0.0.0.0\",\r\n       \"password\": \"\",\r\n       \"port\": \"47335\",\r\n       \"user\": \"mindsdb\",\r\n       \"database\": \"mindsdb\",\r\n       \"ssl\": true\r\n            },\r\n   \"mongodb\": {\r\n       \"host\": \"0.0.0.0\",\r\n       \"port\": \"47336\",\r\n       \"database\": \"mindsdb\"\r\n              }\r\n  }\r\n}\r\n`\n\nThanks that helps,\r\nI do have bunch of follow up questions though. These would help in taking a better call as to how to refactor the changes. \r\n\r\nThe current `storage_dir` seems to be passed around as root across the code base which implies there are direct indirect dependencies on assumption that bunch of things are locally available. As you are the maintainer I want to understand your view on how best to deal with the _backward compatibility_ issue. Should I just introduce a new `storage_dir_v2` and build on top of it for now?\r\n\r\nAlso, I want to understand your thoughts on the below approaches I'm thinking of: \r\n\r\n## Mount the distributed file system as local volume in docker container\r\nSolve this a docker problem we mount s3/gcs to the docker container and use it across the board. \r\n\r\n**Pros**\r\n* This solves the across the board (We no longer maintain the headache of multiple storage integration to our code everything is a local filesystem).  \r\n\r\n**Cons**\r\n* The issue with this is not every distributed file system would have a readily mountable docker support (A new library idea may be). \r\n* Any systems that have their own setup environments / scripts would not be able to use this solution.  \r\n\r\n## Implement filesystem layers in code\r\nAnother solve could be we implement FS layers like the one I was able to see for supporting DB.\r\n\r\n**Pros**\r\n* We are now truly serverless for any environment that offers compute and python support. FWIW, customers can run their DB layer on any compute platform and it will work. (Given some more investment - check-pointing, faster loads, synchronizations and some smart partitioning strategy).\r\n* Changes the dynamics and cost for the complete project.  \r\n\r\n**Cons**\r\n* There's a very high chance I won't be able to make it a clean cut solve with the current codebase. (Definitely, open to ideas if I'm misreading the way it needs to be done).  \r\n* Every new filesystem we want to update / add would be a new support implementation.\r\n\r\n\r\n\n\nSorry for the late reply.\r\nAlready for starters, this is an option. Google Cloud Storage and AWS S3 configuration will be options to set up the storage directory. There will be a current for each storage service. \r\n\r\nThe most important thing is that the file system is able to handle the path to the files stored on Cloud Storage and AWS S3. \r\n\r\nI think the option \"Mount the distributed file system as local volume in docker container\" is good.\n\nApologies, \r\nThis is taking longer than I anticipated.. Still on it though will keep updating on status.\n\nAny updates on GCS?\n\nHey!\r\nHaven't been able to make much progress on it since. If someone else wants to pick it up they can.\n\nMore updates on this. I did have a successful run with GCS but the current docker configurations shares `/minds/var` across multiple containers which fails when only the main docker container is setup for using GCS. I'm currently working on fixing that. Open to ideas from folks that have setup the current docker compose. Is there a possibility to connect with them.\n\nAdded changes to the release and beta images. The catch is not everything can be on the cloud storage as the performance takes a massive hit for static assets etc. I've added changes to allow storage_db and storage/ to be moved to cloud and everything else logs, cache, static assets still stay on the local disk for performance reasons. The state of MindsDB changes are persisted across multiple docker containers. We can introduce and read and write instance separation if we are interested in that setup.\n\n@h1t35h  is this still open?\n\nFortunately, not for too long. I've raised the PR for team to review and we should be able to close it soon.\n\nis this still open & are you working on it?\n\nIt's done already I have added the Pull request : https://github.com/mindsdb/mindsdb/pull/6782.",
  "pr_link": "https://github.com/mindsdb/mindsdb/pull/6782",
  "code_context": [
    {
      "filename": "docker/mindsdb_launcher.sh",
      "content": "#!/bin/bash\n\nif [[ -n \"$MDB_CONFIG_CONTENT\" ]]; then\n  echo \"$MDB_CONFIG_CONTENT\" > /root/mindsdb_config.json;\nfi;\n\nbash /root/setup_cloud.sh\n\nif [[ -n \"$MDB_AUTOUPDATE\" ]]; then\n  URL=\"https://public.api.mindsdb.com/installer/$MDB_RELTYPE/docker___started___None\"\n  VERSION=$(python -c \"import urllib.request as r; print(r.urlopen('$URL').read().decode())\")\n  echo \"--- Updating to MindsDB $VERSION ---\"\n  pip install -U pip\n  pip install mindsdb==\"$VERSION\"\nfi;\n\nif [[ \"$1\" == \"start\" ]]; then\n  python -m mindsdb --config=/root/mindsdb_config.json --api=http,mysql,mongodb\nfi;\n"
    },
    {
      "filename": "docker/setup_cloud.sh",
      "content": "#!/bin/bash\n\n# Create mount directory for service\n# Currently only setting up GCS will update for multiple cloud storages.\n\nMINDSDB_CLOUD_DIR=$(jq '.\"cloud_storage_dir\" //empty' /root/mindsdb_config.json)\necho \"Cloud Mount dir configuration : $MINDSDB_CLOUD_DIR\"\nif [ -n \"$MINDSDB_CLOUD_DIR\" ] && [ -n \"$GOOGLE_APPLICATION_CREDENTIALS\" ] && [ -n \"$GCS_BUCKET\" ]\nthen\n  echo \"Mounting GCS Fuse.\"\n  echo \"Using application default target as : $GOOGLE_APPLICATION_CREDENTIALS\\n\"\n  mkdir -p $MINDSDB_CLOUD_DIR;\n  gcsfuse --debug_gcs --debug_fuse --debug_http $GCS_BUCKET $MINDSDB_CLOUD_DIR;\n  echo \"Mounting completed.\"\n  echo \"Cloud setup complete.\"\nelse\n  echo \"Cloud configuration not present using disk store.\"\nfi;\n\nexit 0"
    },
    {
      "filename": "mindsdb/utilities/config.py",
      "content": "import os\nimport json\nfrom copy import deepcopy\nfrom pathlib import Path\n\nfrom mindsdb.utilities.fs import create_directory, get_or_create_data_dir\n\n\ndef _merge_key_recursive(target_dict, source_dict, key):\n    if key not in target_dict:\n        target_dict[key] = source_dict[key]\n    elif not isinstance(target_dict[key], dict) or not isinstance(source_dict[key], dict):\n        target_dict[key] = source_dict[key]\n    else:\n        for k in list(source_dict[key].keys()):\n            _merge_key_recursive(target_dict[key], source_dict[key], k)\n\n\ndef _merge_configs(original_config, override_config):\n    original_config = deepcopy(original_config)\n    for key in list(override_config.keys()):\n        _merge_key_recursive(original_config, override_config, key)\n    return original_config\n\n\nconfig = None\nconfig_mtime = -1\n\n\nclass Config():\n    def __init__(self):\n        # initialize once\n        global config, config_mtime\n        self.config_path = os.environ.get('MINDSDB_CONFIG_PATH', 'absent')\n        self.use_docker_env = os.environ.get('MINDSDB_DOCKER_ENV', False)\n        if self.use_docker_env:\n            self.use_docker_env = True\n\n        if Path(self.config_path).is_file():\n            current_config_mtime = os.path.getmtime(self.config_path)\n            if config_mtime != current_config_mtime:\n                config = self.init_config()\n                config_mtime = current_config_mtime\n\n        if config is None:\n            config = self.init_config()\n\n        self._config = config\n\n    def init_config(self):\n        if self.config_path == 'absent':\n            self._override_config = {}\n        else:\n            with open(self.config_path, 'r') as fp:\n                self._override_config = json.load(fp)\n\n        # region define storage dir\n        if 'storage_dir' in self._override_config:\n            root_storage_dir = self._override_config['storage_dir']\n            os.environ['MINDSDB_STORAGE_DIR'] = root_storage_dir\n            os.environ['MINDSDB_CLOUD_DIR'] = root_storage_dir  # backward compatibility for cloud dir\n        elif os.environ.get('MINDSDB_STORAGE_DIR') is not None:\n            root_storage_dir = os.environ['MINDSDB_STORAGE_DIR']\n        else:\n            root_storage_dir = get_or_create_data_dir()\n            os.environ['MINDSDB_STORAGE_DIR'] = root_storage_dir\n            os.environ['MINDSDB_CLOUD_DIR'] = root_storage_dir  # backward compatibility for cloud dir\n        # endregion\n\n        # cloud storage setup\n        cloud_storage_dir = None\n        if 'cloud_storage_dir' in self._override_config:\n            cloud_storage_dir = self._override_config['cloud_storage_dir']\n            os.environ['MINDSDB_CLOUD_DIR'] = cloud_storage_dir\n        # end cloud storage setup\n\n        if os.path.isdir(root_storage_dir) is False:\n            os.makedirs(root_storage_dir)\n        if cloud_storage_dir is not None and os.path.isdir(cloud_storage_dir) is False:\n            os.makedirs(cloud_storage_dir)\n\n        if 'storage_db' in self._override_config:\n            os.environ['MINDSDB_DB_CON'] = self._override_config['storage_db']\n        elif os.environ.get('MINDSDB_DB_CON', '') == '':\n            db_storage_dir = root_storage_dir\n            # use cloud storage if configured\n            if cloud_storage_dir is not None:\n                db_storage_dir = cloud_storage_dir\n            os.environ['MINDSDB_DB_CON'] = 'sqlite:///' + os.path.join(db_storage_dir,\n                                                                       'mindsdb.sqlite3.db') + '?check_same_thread=False&timeout=30'\n\n        paths = {\n            'root': os.environ['MINDSDB_STORAGE_DIR'],\n            'cloud_root': os.environ['MINDSDB_CLOUD_DIR']\n        }\n\n        # content - temporary storage for entities\n        paths['content'] = os.path.join(paths['root'], 'content')\n        # storage - persist storage for entities\n        paths['storage'] = os.path.join(paths['cloud_root'], 'storage')\n        paths['static'] = os.path.join(paths['root'], 'static')\n        paths['tmp'] = os.path.join(paths['root'], 'tmp')\n        paths['log'] = os.path.join(paths['root'], 'log')\n        paths['cache'] = os.path.join(paths['root'], 'cache')\n\n        for path_name in paths:\n            create_directory(paths[path_name])\n\n        api_host = \"127.0.0.1\" if not self.use_docker_env else \"0.0.0.0\"\n        self._default_config = {\n            'permanent_storage': {\n                'location': 'local'\n            },\n            'storage_dir': os.environ['MINDSDB_STORAGE_DIR'],\n            'cloud_storage_dir': os.environ['MINDSDB_CLOUD_DIR'],\n            'paths': paths,\n            'auth': {\n                'http_auth_enabled': False,\n                'username': 'mindsdb',\n                'password': ''\n            },\n            \"log\": {\n                \"level\": {\n                    \"console\": \"INFO\",\n                    \"file\": \"DEBUG\",\n                    \"db\": \"WARNING\"\n                }\n            },\n            \"gui\": {\n                \"autoupdate\": True\n            },\n            \"debug\": False,\n            \"environment\": \"local\",\n            \"integrations\": {},\n            \"api\": {\n                \"http\": {\n                    \"host\": api_host,\n                    \"port\": \"47334\"\n                },\n                \"mysql\": {\n                    \"host\": api_host,\n                    \"password\": \"\",\n                    \"port\": \"47335\",\n                    \"database\": \"mindsdb\",\n                    \"ssl\": True\n                },\n                \"mongodb\": {\n                    \"host\": api_host,\n                    \"port\": \"47336\",\n                    \"database\": \"mindsdb\"\n                },\n                \"postgres\": {\n                    \"host\": api_host,\n                    \"port\": \"55432\",\n                    \"database\": \"mindsdb\"\n                }\n            },\n            \"cache\": {\n                \"type\": \"local\"\n            }\n        }\n\n        return _merge_configs(self._default_config, self._override_config)\n\n    def __getitem__(self, key):\n        return self._config[key]\n\n    def get(self, key, default=None):\n        return self._config.get(key, default)\n\n    def get_all(self):\n        return self._config\n\n    def update(self, data: dict):\n        config_path = Path(self.config_path)\n        if config_path.is_file() is False:\n            config_path.write_text('{}')\n\n        with open(self.config_path, 'r') as fp:\n            config_data = json.load(fp)\n\n        config_data = _merge_configs(config_data, data)\n\n        with open(self.config_path, 'wt') as fp:\n            fp.write(json.dumps(config_data, indent=4))\n\n        self.init_config()\n\n    @property\n    def paths(self):\n        return self._config['paths']\n"
    }
  ]
}