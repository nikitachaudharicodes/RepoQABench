{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "60695",
  "issue_description": "# BUG: Series constructor from dictionary drops key (index) levels when not all keys have same number of entries\n\n### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Out[30]: \r\n# l1    v1\r\n# l1    v2\r\n# dtype: object\r\n\r\n# the reason is that the Series constructor uses internally MultiIndex.from_tuples in the following way (note that the input is a tuple of tuples!):\r\npd.MultiIndex.from_tuples(((\"l1\",), (\"l1\",\"l2\")))\r\n# Out[32]: \r\n# MultiIndex([('l1',),\r\n#             ('l1',)],\r\n#            )\r\n\r\n# compare to the following which produces the expected result:\r\npd.MultiIndex.from_tuples([(\"l1\",), (\"l1\",\"l2\")])\r\n# Out[33]: \r\n# MultiIndex([('l1',  nan),\r\n#             ('l1', 'l2')],\r\n#            )\r\n\r\n# Note: this was tested with latest release and current master\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nWhen calling the `Series` constructor with a dict where the keys are tuples, a series with `MulitIndex` gets created. However, if the number of entries in the keys is not the same, key entries from keys with more than the minimum number get dropped. This is in several ways problematic, especially if this produces duplicated index values / keys which is not expected because it was called with a dict (which has per definition unique keys).\r\n\r\n### Expected Behavior\r\n\r\nThe `MultiIndex` of the new series has nan-padded values.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 0691c5cf90477d3503834d983f69350f250a6ff7\r\npython                : 3.10.16\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.8.0-51-generic\r\nVersion               : #52~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Dec  9 15:00:52 UTC 2\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : de_DE.UTF-8\r\nLOCALE                : de_DE.UTF-8\r\n\r\npandas                : 2.2.3\r\nnumpy                 : 2.2.1\r\npytz                  : 2024.2\r\ndateutil              : 2.9.0.post0\r\npip                   : 24.2\r\nCython                : None\r\nsphinx                : None\r\nIPython               : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nblosc                 : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\nhtml5lib              : None\r\nhypothesis            : None\r\ngcsfs                 : None\r\njinja2                : None\r\nlxml.etree            : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npsycopg2              : None\r\npymysql               : None\r\npyarrow               : None\r\npyreadstat            : None\r\npytest                : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nxlsxwriter            : None\r\nzstandard             : None\r\ntzdata                : 2024.2\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n</details>\r\n",
  "issue_comments": [
    {
      "id": 2608436726,
      "user": "rhshadrach",
      "body": "Thanks for the report! It seems to me treating tuples and lists differently is not desired here. This is due to:\n\nhttps://github.com/pandas-dev/pandas/blob/4c3b968a0a4de483c00d15bd267bc776a218337e/pandas/core/indexes/multi.py#L591\n\nand that code goes back to https://github.com/pandas-dev/pandas/commit/bc5a7451a5cfb049e3cc6c9cfc56d2c01656e327. It appears this was not intentional. I'd suggest looking into replacing the `isinstance` with `is_list_like`. Further investigations and PRs to fix are welcome!"
    },
    {
      "id": 2609745598,
      "user": "ShashwatAgrawal20",
      "body": "take"
    },
    {
      "id": 2610163984,
      "user": "ShashwatAgrawal20",
      "body": "hey @rhshadrach, \nI tried replacing the `isinstance` to use `is_list_like`, but that alone doesn't seem to fix the issue. The test case(`test_constructor_dict_tuple_indexer`) continues to fail, and I'm unsure if the problem lies with the test setup or if there's more to adjust? \n\nHere's the test result for `test_constructor_tuple_indexer`\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?><testsuites><testsuite name=\"pytest\" errors=\"0\" failures=\"1\" skipped=\"0\" tests=\"1\" time=\"0.594\" timestamp=\"2025-01-23T21:11:54.485741+05:30\" hostname=\"archlap\"><testcase classname=\"pandas.tests.series.test_constructors.TestSeriesConstructors\" name=\"test_constructor_dict_tuple_indexer\" time=\"0.008\"><failure message=\"AssertionError: Series.index level [2] are different&#10;&#10;Attribute &quot;dtype&quot; are different&#10;[left]:  object&#10;[right]: float64\">left = Index([], dtype='object'), right = Index([nan], dtype='float64'), obj = 'Series.index level [2]'\n\n    def _check_types(left, right, obj: str = \"Index\") -&gt; None:\n        if not exact:\n            return\n    \n        assert_class_equal(left, right, exact=exact, obj=obj)\n&gt;       assert_attr_equal(\"inferred_type\", left, right, obj=obj)\nE       AssertionError: Series.index level [2] are different\nE       \nE       Attribute \"inferred_type\" are different\nE       [left]:  empty\nE       [right]: floating\n\npandas/_testing/asserters.py:246: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &lt;pandas.tests.series.test_constructors.TestSeriesConstructors object at 0x72f8efd00b40&gt;\n\n    def test_constructor_dict_tuple_indexer(self):\n        # GH 12948\n        data = {(1, 1, None): -1.0}\n        result = Series(data)\n        expected = Series(\n            -1.0, index=MultiIndex(levels=[[1], [1], [np.nan]], codes=[[0], [0], [-1]])\n        )\n&gt;       tm.assert_series_equal(result, expected)\n\npandas/tests/series/test_constructors.py:1417: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nleft = Index([nan], dtype='object'), right = Index([nan], dtype='float64'), obj = 'Series.index level [2]'\n\n    def _check_types(left, right, obj: str = \"Index\") -&gt; None:\n        if not exact:\n            return\n    \n        assert_class_equal(left, right, exact=exact, obj=obj)\n        assert_attr_equal(\"inferred_type\", left, right, obj=obj)\n    \n        # Skip exact dtype checking when `check_categorical` is False\n        if isinstance(left.dtype, CategoricalDtype) and isinstance(\n            right.dtype, CategoricalDtype\n        ):\n            if check_categorical:\n                assert_attr_equal(\"dtype\", left, right, obj=obj)\n                assert_index_equal(left.categories, right.categories, exact=exact)\n            return\n    \n&gt;       assert_attr_equal(\"dtype\", left, right, obj=obj)\nE       AssertionError: Series.index level [2] are different\nE       \nE       Attribute \"dtype\" are different\nE       [left]:  object\nE       [right]: float64\n\npandas/_testing/asserters.py:257: AssertionError</failure></testcase></testsuite></testsuites>\n```"
    },
    {
      "id": 2610895248,
      "user": "siber64",
      "body": "Hi I'm new and this is first I looked at.  I know I didn't \"take\" it, but I think looking at it briefly try changing line 539 to\narrs = zip_longest(*tuples, fillvalue=np.nan)\nalso need to include\nimport from itertools zip_longest.\n\nThis will create an index with the number of dimensions of the longest iterable, even if it is not the first, for instance ((1,2), (3,), (3,4,5), (5,) ) gets us ((1, 3, 3, 5), (2, nan, 4, nan), (nan, nan, 5, nan)).\n\nOr should I take it and do it ?  Not sure of etiquette. @VishalSindham are you doing similar ?"
    },
    {
      "id": 2612092570,
      "user": "ShashwatAgrawal20",
      "body": "I've tried doing that, even when manually converting `None` values to `np.nan` doesn't resolve the issue with my test cases. ig it has something to do with how python's parsing these types. \n\nThe only solution I've found to make the tests pass is by adding `check_index_type=False` to the assertion statement in `test_constructor_dict_tuple_indexer`. "
    },
    {
      "id": 2612266565,
      "user": "VishalSindham",
      "body": "> [@VishalSindham](https://github.com/VishalSindham) are you doing similar ?\n \n  Yes @siber64. Did not start yet.  You can contribute early if you have the solution.\n"
    },
    {
      "id": 2612279661,
      "user": "siber64",
      "body": "Thanks, I can look later today, doesn't sound like Python problem"
    },
    {
      "id": 2612478727,
      "user": "siber64",
      "body": "@VishalSindham As I suspected it is just the behavior of zip, zip_longest fixes it.  I'll take and do a PR"
    },
    {
      "id": 2612478973,
      "user": "siber64",
      "body": "take"
    },
    {
      "id": 2658311708,
      "user": "JonKissil",
      "body": "Has this issue been completed? If not could I take it? "
    },
    {
      "id": 2658388026,
      "user": "siber64",
      "body": "Yes of course I've been swamped with life stuff.  So the fix is just to swap zip for zip_longest , there is no performance hit.  Some of the unit tests though fail as they expect the old behaviour.  Now I could not get a clean unit test run and didn't have the time to go through all the failures to see which was due to this change.\n"
    },
    {
      "id": 2658390692,
      "user": "JonKissil",
      "body": "Alright appreciated 👍 I’ll get to work on this ASAP"
    },
    {
      "id": 2658391088,
      "user": "JonKissil",
      "body": "take"
    },
    {
      "id": 2660884847,
      "user": "mansoor17syed",
      "body": "Hi everyone, \n\nI picked up this issue and have started working on it. I’d be happy to receive any guidance or feedback along the way. Apologies if this was already assigned to someone—please let me know if I should coordinate differently. Looking forward to contributing!\n\n"
    },
    {
      "id": 2660934242,
      "user": "JonKissil",
      "body": "Hello @mansoor17syed,\n\nI was already working on this but I am happy to collaborate! Right now I’m looking at the failing unit tests because of the new behavior of zip_longest. "
    },
    {
      "id": 2660943115,
      "user": "mansoor17syed",
      "body": "Hi @JonKissil ,\n\nCould you review the changes and let me know if there's anything I can help with? I just wanted to give it a shot, so I went ahead and pushed my changes.Appreciate your support "
    },
    {
      "id": 2661007638,
      "user": "JonKissil",
      "body": "@mansoor17syed you're more than welcome to continue working on it if you think you can come to a solution, I'm also a bit strapped for time. "
    },
    {
      "id": 2661332104,
      "user": "Anurag-Varma",
      "body": "take"
    },
    {
      "id": 2661347323,
      "user": "Anurag-Varma",
      "body": "Hi @ArneBinder @rhshadrach \n\nI made some changes to the code and would like to confirm if the expected output I mentioned below is correct for the given code.\n\n```python\nimport pandas as pd\n\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\n# Expected output:\n# l1  NaN    v1\n#      l2     v2\n# dtype: object\n```"
    },
    {
      "id": 2661474227,
      "user": "siber64",
      "body": "No, here is the unit test I added to pandas/tests/indexes/multi/test_constructors.py\r\n\r\n\r\n@pytest.mark.parametrize(\"keys, expected\", (\r\n    (((\"l1\",), (\"l1\",\"l2\")), ((\"l1\", np.nan), (\"l1\",\"l2\"))),\r\n    (((\"l1\",\"l2\",), (\"l1\",)), ((\"l1\",\"l2\"), (\"l1\", np.nan))),\r\n))\r\ndef test_from_tuples_with_various_tuple_lengths(keys, expected):\r\n    # Issue 60695\r\n    idx = MultiIndex.from_tuples(keys)\r\n    assert tuple(idx) == expected\r\n\r\n________________________________\r\nFrom: Anurag Varma ***@***.***>\r\nSent: 16 February 2025 09:43\r\nTo: pandas-dev/pandas ***@***.***>\r\nCc: Simon ***@***.***>; Assign ***@***.***>\r\nSubject: Re: [pandas-dev/pandas] BUG: Series constructor from dictionary drops key (index) levels when not all keys have same number of entries (Issue #60695)\r\n\r\n[Anurag-Varma]Anurag-Varma left a comment (pandas-dev/pandas#60695)<https://github.com/pandas-dev/pandas/issues/60695#issuecomment-2661347323>\r\n\r\nHi @ArneBinder<https://github.com/ArneBinder> @rhshadrach<https://github.com/rhshadrach>\r\n\r\nIs the expected output which i mentioned below correct for the given code ?\r\n\r\nimport pandas as pd\r\n\r\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Expected output:\r\n# l1  NaN    v1\r\n#      l2     v2\r\n# dtype: object\r\n\r\n—\r\nReply to this email directly, view it on GitHub<https://github.com/pandas-dev/pandas/issues/60695#issuecomment-2661347323>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AD3Z4PKNQHHHP67CZB7QOLT2QBMVDAVCNFSM6AAAAABVAHQGESVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMNRRGM2DOMZSGM>.\r\nYou are receiving this because you were assigned.\r\n"
    },
    {
      "id": 2661494022,
      "user": "Anurag-Varma",
      "body": "Hi @siber64 \n\nWhat you said is correct, but thats only of tuple of tuples given input to MultiIndex.from_tuples\n\nMy question is diffferent, i am using pd.Series with dictionary and tuple is keys in a dictionary.\n\nSo, I think pandas treats this as Multiindex and the first index of each tuple becomes the primary index and the second element becomes the sub-index.\n\nExample of existing behaviour:\n```python\nimport pandas as pd\npd.Series({(\"l1\",\"l3\"):\"v1\", (\"l1\",\"l2\"): \"v2\"})\n# Existing Output:\n# l1  l3    v1\n#      l2    v2\n# dtype: object\n```\n\nExample of error behaviour:\n```python\nimport pandas as pd\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\n# Error Output:\n# l1     v1\n# l1     v2\n# dtype: object\n```\n\nExample of expected behaviour:\n```python\nimport pandas as pd\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\n# Expected Output:\n# l1  NaN   v1\n#      l2        v2\n# dtype: object\n```"
    },
    {
      "id": 2661496156,
      "user": "siber64",
      "body": "Only problem I found was with tuples\r\n\r\nSent from Outlook for Android<https://aka.ms/AAb9ysg>\r\n________________________________\r\nFrom: Anurag Varma ***@***.***>\r\nSent: Sunday, February 16, 2025 3:50:18 PM\r\nTo: pandas-dev/pandas ***@***.***>\r\nCc: Simon ***@***.***>; Mention ***@***.***>\r\nSubject: Re: [pandas-dev/pandas] BUG: Series constructor from dictionary drops key (index) levels when not all keys have same number of entries (Issue #60695)\r\n\r\n\r\nHi @siber64<https://github.com/siber64>\r\n\r\nWhat you said is correct, but thats only of tuple of tuples given input to MultiIndex.from_tuples\r\n\r\nMy question is diffferent, i am using pd.Series with dictionary and tuple is keys in a dictionary.\r\n\r\nSo, I think pandas treats this as Multiindex and the first index of each tuple becomes the primary index and the second element becomes the sub-index.\r\n\r\nExample of existing behaviour:\r\n\r\nimport pandas as pd\r\npd.Series({(\"l1\",\"l3\"):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Existing Output:\r\n# l1  l3    v1\r\n#      l2    v2\r\n# dtype: object\r\n\r\nExample of error behaviour:\r\n\r\nimport pandas as pd\r\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Error Output:\r\n# l1     v1\r\n# l1     v2\r\n# dtype: object\r\n\r\nExample of expected behaviour:\r\n\r\nimport pandas as pd\r\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Expected Output:\r\n# l1  NaN   v1\r\n#      l2        v2\r\n# dtype: object\r\n\r\n—\r\nReply to this email directly, view it on GitHub<https://github.com/pandas-dev/pandas/issues/60695#issuecomment-2661494022>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AD3Z4PIYY4IUVULAJ2DAS4D2QCXTVAVCNFSM6AAAAABVAHQGESVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMNRRGQ4TIMBSGI>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n\r\n[Anurag-Varma]Anurag-Varma left a comment (pandas-dev/pandas#60695)<https://github.com/pandas-dev/pandas/issues/60695#issuecomment-2661494022>\r\n\r\nHi @siber64<https://github.com/siber64>\r\n\r\nWhat you said is correct, but thats only of tuple of tuples given input to MultiIndex.from_tuples\r\n\r\nMy question is diffferent, i am using pd.Series with dictionary and tuple is keys in a dictionary.\r\n\r\nSo, I think pandas treats this as Multiindex and the first index of each tuple becomes the primary index and the second element becomes the sub-index.\r\n\r\nExample of existing behaviour:\r\n\r\nimport pandas as pd\r\npd.Series({(\"l1\",\"l3\"):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Existing Output:\r\n# l1  l3    v1\r\n#      l2    v2\r\n# dtype: object\r\n\r\nExample of error behaviour:\r\n\r\nimport pandas as pd\r\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Error Output:\r\n# l1     v1\r\n# l1     v2\r\n# dtype: object\r\n\r\nExample of expected behaviour:\r\n\r\nimport pandas as pd\r\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Expected Output:\r\n# l1  NaN   v1\r\n#      l2        v2\r\n# dtype: object\r\n\r\n—\r\nReply to this email directly, view it on GitHub<https://github.com/pandas-dev/pandas/issues/60695#issuecomment-2661494022>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AD3Z4PIYY4IUVULAJ2DAS4D2QCXTVAVCNFSM6AAAAABVAHQGESVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMNRRGQ4TIMBSGI>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n"
    },
    {
      "id": 2662424366,
      "user": "ArneBinder",
      "body": "> Hi [@ArneBinder](https://github.com/ArneBinder) [@rhshadrach](https://github.com/rhshadrach)\n> \n> I made some changes to the code and would like to confirm if the expected output I mentioned below is correct for the given code.\n> \n> import pandas as pd\n> \n> pd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\n> # Expected output:\n> # l1  NaN    v1\n> #      l2     v2\n> # dtype: object\n\n@Anurag-Varma Yes, exactly, that's what I had in mind."
    },
    {
      "id": 2676441041,
      "user": "Anurag-Varma",
      "body": "Hi @rhshadrach \n\nI was trying to fix the bug but below test case was failing:\n\n`pandas/tests/series/methods/test_map.py::test_map_dict_with_tuple_keys`\n\nWhen I further tried to fix it, found out that the `Series.map()` is failing for multiple examples with tuples as keys in the dictionary.\n\nSo created a new issue for that: #60988 "
    },
    {
      "id": 2676441904,
      "user": "Anurag-Varma",
      "body": "> Hi [@rhshadrach](https://github.com/rhshadrach)\n> \n> I was trying to fix the bug but below test case was failing:\n> \n> `pandas/tests/series/methods/test_map.py::test_map_dict_with_tuple_keys`\n> \n> When I further tried to fix it, found out that the `Series.map()` is failing for multiple examples with tuples as keys in the dictionary.\n> \n> So created a new issue for that: [#60988](https://github.com/pandas-dev/pandas/issues/60988)\n\nI solved this current issue but the above test case is failing so unable to send a new commit in my PR #60944 \n\nShould i mark it as xfail and proceed forward ?"
    },
    {
      "id": 2676862019,
      "user": "rhshadrach",
      "body": "> I solved this current issue but the above test case is failing so unable to send a new commit in my PR [#60944](https://github.com/pandas-dev/pandas/pull/60944)\n\nIs this related to this change: https://github.com/pandas-dev/pandas/pull/60944/files#r1966545298?\n\nIf not, I do not understand your comment. It is best to discuss these things on the PR, where the discussion can happen next to the code involved."
    }
  ],
  "text_context": "# BUG: Series constructor from dictionary drops key (index) levels when not all keys have same number of entries\n\n### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Out[30]: \r\n# l1    v1\r\n# l1    v2\r\n# dtype: object\r\n\r\n# the reason is that the Series constructor uses internally MultiIndex.from_tuples in the following way (note that the input is a tuple of tuples!):\r\npd.MultiIndex.from_tuples(((\"l1\",), (\"l1\",\"l2\")))\r\n# Out[32]: \r\n# MultiIndex([('l1',),\r\n#             ('l1',)],\r\n#            )\r\n\r\n# compare to the following which produces the expected result:\r\npd.MultiIndex.from_tuples([(\"l1\",), (\"l1\",\"l2\")])\r\n# Out[33]: \r\n# MultiIndex([('l1',  nan),\r\n#             ('l1', 'l2')],\r\n#            )\r\n\r\n# Note: this was tested with latest release and current master\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nWhen calling the `Series` constructor with a dict where the keys are tuples, a series with `MulitIndex` gets created. However, if the number of entries in the keys is not the same, key entries from keys with more than the minimum number get dropped. This is in several ways problematic, especially if this produces duplicated index values / keys which is not expected because it was called with a dict (which has per definition unique keys).\r\n\r\n### Expected Behavior\r\n\r\nThe `MultiIndex` of the new series has nan-padded values.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 0691c5cf90477d3503834d983f69350f250a6ff7\r\npython                : 3.10.16\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.8.0-51-generic\r\nVersion               : #52~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Dec  9 15:00:52 UTC 2\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : de_DE.UTF-8\r\nLOCALE                : de_DE.UTF-8\r\n\r\npandas                : 2.2.3\r\nnumpy                 : 2.2.1\r\npytz                  : 2024.2\r\ndateutil              : 2.9.0.post0\r\npip                   : 24.2\r\nCython                : None\r\nsphinx                : None\r\nIPython               : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nblosc                 : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\nhtml5lib              : None\r\nhypothesis            : None\r\ngcsfs                 : None\r\njinja2                : None\r\nlxml.etree            : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npsycopg2              : None\r\npymysql               : None\r\npyarrow               : None\r\npyreadstat            : None\r\npytest                : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nxlsxwriter            : None\r\nzstandard             : None\r\ntzdata                : 2024.2\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n</details>\r\n\n\nThanks for the report! It seems to me treating tuples and lists differently is not desired here. This is due to:\n\nhttps://github.com/pandas-dev/pandas/blob/4c3b968a0a4de483c00d15bd267bc776a218337e/pandas/core/indexes/multi.py#L591\n\nand that code goes back to https://github.com/pandas-dev/pandas/commit/bc5a7451a5cfb049e3cc6c9cfc56d2c01656e327. It appears this was not intentional. I'd suggest looking into replacing the `isinstance` with `is_list_like`. Further investigations and PRs to fix are welcome!\n\ntake\n\nhey @rhshadrach, \nI tried replacing the `isinstance` to use `is_list_like`, but that alone doesn't seem to fix the issue. The test case(`test_constructor_dict_tuple_indexer`) continues to fail, and I'm unsure if the problem lies with the test setup or if there's more to adjust? \n\nHere's the test result for `test_constructor_tuple_indexer`\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?><testsuites><testsuite name=\"pytest\" errors=\"0\" failures=\"1\" skipped=\"0\" tests=\"1\" time=\"0.594\" timestamp=\"2025-01-23T21:11:54.485741+05:30\" hostname=\"archlap\"><testcase classname=\"pandas.tests.series.test_constructors.TestSeriesConstructors\" name=\"test_constructor_dict_tuple_indexer\" time=\"0.008\"><failure message=\"AssertionError: Series.index level [2] are different&#10;&#10;Attribute &quot;dtype&quot; are different&#10;[left]:  object&#10;[right]: float64\">left = Index([], dtype='object'), right = Index([nan], dtype='float64'), obj = 'Series.index level [2]'\n\n    def _check_types(left, right, obj: str = \"Index\") -&gt; None:\n        if not exact:\n            return\n    \n        assert_class_equal(left, right, exact=exact, obj=obj)\n&gt;       assert_attr_equal(\"inferred_type\", left, right, obj=obj)\nE       AssertionError: Series.index level [2] are different\nE       \nE       Attribute \"inferred_type\" are different\nE       [left]:  empty\nE       [right]: floating\n\npandas/_testing/asserters.py:246: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &lt;pandas.tests.series.test_constructors.TestSeriesConstructors object at 0x72f8efd00b40&gt;\n\n    def test_constructor_dict_tuple_indexer(self):\n        # GH 12948\n        data = {(1, 1, None): -1.0}\n        result = Series(data)\n        expected = Series(\n            -1.0, index=MultiIndex(levels=[[1], [1], [np.nan]], codes=[[0], [0], [-1]])\n        )\n&gt;       tm.assert_series_equal(result, expected)\n\npandas/tests/series/test_constructors.py:1417: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nleft = Index([nan], dtype='object'), right = Index([nan], dtype='float64'), obj = 'Series.index level [2]'\n\n    def _check_types(left, right, obj: str = \"Index\") -&gt; None:\n        if not exact:\n            return\n    \n        assert_class_equal(left, right, exact=exact, obj=obj)\n        assert_attr_equal(\"inferred_type\", left, right, obj=obj)\n    \n        # Skip exact dtype checking when `check_categorical` is False\n        if isinstance(left.dtype, CategoricalDtype) and isinstance(\n            right.dtype, CategoricalDtype\n        ):\n            if check_categorical:\n                assert_attr_equal(\"dtype\", left, right, obj=obj)\n                assert_index_equal(left.categories, right.categories, exact=exact)\n            return\n    \n&gt;       assert_attr_equal(\"dtype\", left, right, obj=obj)\nE       AssertionError: Series.index level [2] are different\nE       \nE       Attribute \"dtype\" are different\nE       [left]:  object\nE       [right]: float64\n\npandas/_testing/asserters.py:257: AssertionError</failure></testcase></testsuite></testsuites>\n```\n\nHi I'm new and this is first I looked at.  I know I didn't \"take\" it, but I think looking at it briefly try changing line 539 to\narrs = zip_longest(*tuples, fillvalue=np.nan)\nalso need to include\nimport from itertools zip_longest.\n\nThis will create an index with the number of dimensions of the longest iterable, even if it is not the first, for instance ((1,2), (3,), (3,4,5), (5,) ) gets us ((1, 3, 3, 5), (2, nan, 4, nan), (nan, nan, 5, nan)).\n\nOr should I take it and do it ?  Not sure of etiquette. @VishalSindham are you doing similar ?\n\nI've tried doing that, even when manually converting `None` values to `np.nan` doesn't resolve the issue with my test cases. ig it has something to do with how python's parsing these types. \n\nThe only solution I've found to make the tests pass is by adding `check_index_type=False` to the assertion statement in `test_constructor_dict_tuple_indexer`. \n\n> [@VishalSindham](https://github.com/VishalSindham) are you doing similar ?\n \n  Yes @siber64. Did not start yet.  You can contribute early if you have the solution.\n\n\nThanks, I can look later today, doesn't sound like Python problem\n\n@VishalSindham As I suspected it is just the behavior of zip, zip_longest fixes it.  I'll take and do a PR\n\ntake\n\nHas this issue been completed? If not could I take it? \n\nYes of course I've been swamped with life stuff.  So the fix is just to swap zip for zip_longest , there is no performance hit.  Some of the unit tests though fail as they expect the old behaviour.  Now I could not get a clean unit test run and didn't have the time to go through all the failures to see which was due to this change.\n\n\nAlright appreciated 👍 I’ll get to work on this ASAP\n\ntake\n\nHi everyone, \n\nI picked up this issue and have started working on it. I’d be happy to receive any guidance or feedback along the way. Apologies if this was already assigned to someone—please let me know if I should coordinate differently. Looking forward to contributing!\n\n\n\nHello @mansoor17syed,\n\nI was already working on this but I am happy to collaborate! Right now I’m looking at the failing unit tests because of the new behavior of zip_longest. \n\nHi @JonKissil ,\n\nCould you review the changes and let me know if there's anything I can help with? I just wanted to give it a shot, so I went ahead and pushed my changes.Appreciate your support \n\n@mansoor17syed you're more than welcome to continue working on it if you think you can come to a solution, I'm also a bit strapped for time. \n\ntake\n\nHi @ArneBinder @rhshadrach \n\nI made some changes to the code and would like to confirm if the expected output I mentioned below is correct for the given code.\n\n```python\nimport pandas as pd\n\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\n# Expected output:\n# l1  NaN    v1\n#      l2     v2\n# dtype: object\n```\n\nNo, here is the unit test I added to pandas/tests/indexes/multi/test_constructors.py\r\n\r\n\r\n@pytest.mark.parametrize(\"keys, expected\", (\r\n    (((\"l1\",), (\"l1\",\"l2\")), ((\"l1\", np.nan), (\"l1\",\"l2\"))),\r\n    (((\"l1\",\"l2\",), (\"l1\",)), ((\"l1\",\"l2\"), (\"l1\", np.nan))),\r\n))\r\ndef test_from_tuples_with_various_tuple_lengths(keys, expected):\r\n    # Issue 60695\r\n    idx = MultiIndex.from_tuples(keys)\r\n    assert tuple(idx) == expected\r\n\r\n________________________________\r\nFrom: Anurag Varma ***@***.***>\r\nSent: 16 February 2025 09:43\r\nTo: pandas-dev/pandas ***@***.***>\r\nCc: Simon ***@***.***>; Assign ***@***.***>\r\nSubject: Re: [pandas-dev/pandas] BUG: Series constructor from dictionary drops key (index) levels when not all keys have same number of entries (Issue #60695)\r\n\r\n[Anurag-Varma]Anurag-Varma left a comment (pandas-dev/pandas#60695)<https://github.com/pandas-dev/pandas/issues/60695#issuecomment-2661347323>\r\n\r\nHi @ArneBinder<https://github.com/ArneBinder> @rhshadrach<https://github.com/rhshadrach>\r\n\r\nIs the expected output which i mentioned below correct for the given code ?\r\n\r\nimport pandas as pd\r\n\r\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Expected output:\r\n# l1  NaN    v1\r\n#      l2     v2\r\n# dtype: object\r\n\r\n—\r\nReply to this email directly, view it on GitHub<https://github.com/pandas-dev/pandas/issues/60695#issuecomment-2661347323>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AD3Z4PKNQHHHP67CZB7QOLT2QBMVDAVCNFSM6AAAAABVAHQGESVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMNRRGM2DOMZSGM>.\r\nYou are receiving this because you were assigned.\r\n\n\nHi @siber64 \n\nWhat you said is correct, but thats only of tuple of tuples given input to MultiIndex.from_tuples\n\nMy question is diffferent, i am using pd.Series with dictionary and tuple is keys in a dictionary.\n\nSo, I think pandas treats this as Multiindex and the first index of each tuple becomes the primary index and the second element becomes the sub-index.\n\nExample of existing behaviour:\n```python\nimport pandas as pd\npd.Series({(\"l1\",\"l3\"):\"v1\", (\"l1\",\"l2\"): \"v2\"})\n# Existing Output:\n# l1  l3    v1\n#      l2    v2\n# dtype: object\n```\n\nExample of error behaviour:\n```python\nimport pandas as pd\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\n# Error Output:\n# l1     v1\n# l1     v2\n# dtype: object\n```\n\nExample of expected behaviour:\n```python\nimport pandas as pd\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\n# Expected Output:\n# l1  NaN   v1\n#      l2        v2\n# dtype: object\n```\n\nOnly problem I found was with tuples\r\n\r\nSent from Outlook for Android<https://aka.ms/AAb9ysg>\r\n________________________________\r\nFrom: Anurag Varma ***@***.***>\r\nSent: Sunday, February 16, 2025 3:50:18 PM\r\nTo: pandas-dev/pandas ***@***.***>\r\nCc: Simon ***@***.***>; Mention ***@***.***>\r\nSubject: Re: [pandas-dev/pandas] BUG: Series constructor from dictionary drops key (index) levels when not all keys have same number of entries (Issue #60695)\r\n\r\n\r\nHi @siber64<https://github.com/siber64>\r\n\r\nWhat you said is correct, but thats only of tuple of tuples given input to MultiIndex.from_tuples\r\n\r\nMy question is diffferent, i am using pd.Series with dictionary and tuple is keys in a dictionary.\r\n\r\nSo, I think pandas treats this as Multiindex and the first index of each tuple becomes the primary index and the second element becomes the sub-index.\r\n\r\nExample of existing behaviour:\r\n\r\nimport pandas as pd\r\npd.Series({(\"l1\",\"l3\"):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Existing Output:\r\n# l1  l3    v1\r\n#      l2    v2\r\n# dtype: object\r\n\r\nExample of error behaviour:\r\n\r\nimport pandas as pd\r\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Error Output:\r\n# l1     v1\r\n# l1     v2\r\n# dtype: object\r\n\r\nExample of expected behaviour:\r\n\r\nimport pandas as pd\r\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Expected Output:\r\n# l1  NaN   v1\r\n#      l2        v2\r\n# dtype: object\r\n\r\n—\r\nReply to this email directly, view it on GitHub<https://github.com/pandas-dev/pandas/issues/60695#issuecomment-2661494022>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AD3Z4PIYY4IUVULAJ2DAS4D2QCXTVAVCNFSM6AAAAABVAHQGESVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMNRRGQ4TIMBSGI>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n\r\n[Anurag-Varma]Anurag-Varma left a comment (pandas-dev/pandas#60695)<https://github.com/pandas-dev/pandas/issues/60695#issuecomment-2661494022>\r\n\r\nHi @siber64<https://github.com/siber64>\r\n\r\nWhat you said is correct, but thats only of tuple of tuples given input to MultiIndex.from_tuples\r\n\r\nMy question is diffferent, i am using pd.Series with dictionary and tuple is keys in a dictionary.\r\n\r\nSo, I think pandas treats this as Multiindex and the first index of each tuple becomes the primary index and the second element becomes the sub-index.\r\n\r\nExample of existing behaviour:\r\n\r\nimport pandas as pd\r\npd.Series({(\"l1\",\"l3\"):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Existing Output:\r\n# l1  l3    v1\r\n#      l2    v2\r\n# dtype: object\r\n\r\nExample of error behaviour:\r\n\r\nimport pandas as pd\r\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Error Output:\r\n# l1     v1\r\n# l1     v2\r\n# dtype: object\r\n\r\nExample of expected behaviour:\r\n\r\nimport pandas as pd\r\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Expected Output:\r\n# l1  NaN   v1\r\n#      l2        v2\r\n# dtype: object\r\n\r\n—\r\nReply to this email directly, view it on GitHub<https://github.com/pandas-dev/pandas/issues/60695#issuecomment-2661494022>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AD3Z4PIYY4IUVULAJ2DAS4D2QCXTVAVCNFSM6AAAAABVAHQGESVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMNRRGQ4TIMBSGI>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n\n\n> Hi [@ArneBinder](https://github.com/ArneBinder) [@rhshadrach](https://github.com/rhshadrach)\n> \n> I made some changes to the code and would like to confirm if the expected output I mentioned below is correct for the given code.\n> \n> import pandas as pd\n> \n> pd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\n> # Expected output:\n> # l1  NaN    v1\n> #      l2     v2\n> # dtype: object\n\n@Anurag-Varma Yes, exactly, that's what I had in mind.\n\nHi @rhshadrach \n\nI was trying to fix the bug but below test case was failing:\n\n`pandas/tests/series/methods/test_map.py::test_map_dict_with_tuple_keys`\n\nWhen I further tried to fix it, found out that the `Series.map()` is failing for multiple examples with tuples as keys in the dictionary.\n\nSo created a new issue for that: #60988 \n\n> Hi [@rhshadrach](https://github.com/rhshadrach)\n> \n> I was trying to fix the bug but below test case was failing:\n> \n> `pandas/tests/series/methods/test_map.py::test_map_dict_with_tuple_keys`\n> \n> When I further tried to fix it, found out that the `Series.map()` is failing for multiple examples with tuples as keys in the dictionary.\n> \n> So created a new issue for that: [#60988](https://github.com/pandas-dev/pandas/issues/60988)\n\nI solved this current issue but the above test case is failing so unable to send a new commit in my PR #60944 \n\nShould i mark it as xfail and proceed forward ?\n\n> I solved this current issue but the above test case is failing so unable to send a new commit in my PR [#60944](https://github.com/pandas-dev/pandas/pull/60944)\n\nIs this related to this change: https://github.com/pandas-dev/pandas/pull/60944/files#r1966545298?\n\nIf not, I do not understand your comment. It is best to discuss these things on the PR, where the discussion can happen next to the code involved.",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/60944",
  "code_context": [
    {
      "filename": "pandas/core/algorithms.py",
      "content": "\"\"\"\nGeneric data algorithms. This module is experimental at the moment and not\nintended for public consumption\n\"\"\"\n\nfrom __future__ import annotations\n\nimport decimal\nimport operator\nfrom textwrap import dedent\nfrom typing import (\n    TYPE_CHECKING,\n    Literal,\n    cast,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import (\n    algos,\n    hashtable as htable,\n    iNaT,\n    lib,\n)\nfrom pandas._libs.missing import NA\nfrom pandas._typing import (\n    AnyArrayLike,\n    ArrayLike,\n    ArrayLikeT,\n    AxisInt,\n    DtypeObj,\n    TakeIndexer,\n    npt,\n)\nfrom pandas.util._decorators import doc\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.core.dtypes.cast import (\n    construct_1d_object_array_from_listlike,\n    np_find_common_type,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_float64,\n    ensure_object,\n    ensure_platform_int,\n    is_bool_dtype,\n    is_complex_dtype,\n    is_dict_like,\n    is_extension_array_dtype,\n    is_float,\n    is_float_dtype,\n    is_integer,\n    is_integer_dtype,\n    is_list_like,\n    is_object_dtype,\n    is_signed_integer_dtype,\n    needs_i8_conversion,\n)\nfrom pandas.core.dtypes.concat import concat_compat\nfrom pandas.core.dtypes.dtypes import (\n    BaseMaskedDtype,\n    CategoricalDtype,\n    ExtensionDtype,\n    NumpyEADtype,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCDatetimeArray,\n    ABCExtensionArray,\n    ABCIndex,\n    ABCMultiIndex,\n    ABCNumpyExtensionArray,\n    ABCSeries,\n    ABCTimedeltaArray,\n)\nfrom pandas.core.dtypes.missing import (\n    isna,\n    na_value_for_dtype,\n)\n\nfrom pandas.core.array_algos.take import take_nd\nfrom pandas.core.construction import (\n    array as pd_array,\n    ensure_wrapped_if_datetimelike,\n    extract_array,\n)\nfrom pandas.core.indexers import validate_indices\n\nif TYPE_CHECKING:\n    from pandas._typing import (\n        ListLike,\n        NumpySorter,\n        NumpyValueArrayLike,\n    )\n\n    from pandas import (\n        Categorical,\n        Index,\n        Series,\n    )\n    from pandas.core.arrays import (\n        BaseMaskedArray,\n        ExtensionArray,\n    )\n\n\n# --------------- #\n# dtype access    #\n# --------------- #\ndef _ensure_data(values: ArrayLike) -> np.ndarray:\n    \"\"\"\n    routine to ensure that our data is of the correct\n    input dtype for lower-level routines\n\n    This will coerce:\n    - ints -> int64\n    - uint -> uint64\n    - bool -> uint8\n    - datetimelike -> i8\n    - datetime64tz -> i8 (in local tz)\n    - categorical -> codes\n\n    Parameters\n    ----------\n    values : np.ndarray or ExtensionArray\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n\n    if not isinstance(values, ABCMultiIndex):\n        # extract_array would raise\n        values = extract_array(values, extract_numpy=True)\n\n    if is_object_dtype(values.dtype):\n        return ensure_object(np.asarray(values))\n\n    elif isinstance(values.dtype, BaseMaskedDtype):\n        # i.e. BooleanArray, FloatingArray, IntegerArray\n        values = cast(\"BaseMaskedArray\", values)\n        if not values._hasna:\n            # No pd.NAs -> We can avoid an object-dtype cast (and copy) GH#41816\n            #  recurse to avoid re-implementing logic for eg bool->uint8\n            return _ensure_data(values._data)\n        return np.asarray(values)\n\n    elif isinstance(values.dtype, CategoricalDtype):\n        # NB: cases that go through here should NOT be using _reconstruct_data\n        #  on the back-end.\n        values = cast(\"Categorical\", values)\n        return values.codes\n\n    elif is_bool_dtype(values.dtype):\n        if isinstance(values, np.ndarray):\n            # i.e. actually dtype == np.dtype(\"bool\")\n            return np.asarray(values).view(\"uint8\")\n        else:\n            # e.g. Sparse[bool, False]  # TODO: no test cases get here\n            return np.asarray(values).astype(\"uint8\", copy=False)\n\n    elif is_integer_dtype(values.dtype):\n        return np.asarray(values)\n\n    elif is_float_dtype(values.dtype):\n        # Note: checking `values.dtype == \"float128\"` raises on Windows and 32bit\n        # error: Item \"ExtensionDtype\" of \"Union[Any, ExtensionDtype, dtype[Any]]\"\n        # has no attribute \"itemsize\"\n        if values.dtype.itemsize in [2, 12, 16]:  # type: ignore[union-attr]\n            # we dont (yet) have float128 hashtable support\n            return ensure_float64(values)\n        return np.asarray(values)\n\n    elif is_complex_dtype(values.dtype):\n        return cast(np.ndarray, values)\n\n    # datetimelike\n    elif needs_i8_conversion(values.dtype):\n        npvalues = values.view(\"i8\")\n        npvalues = cast(np.ndarray, npvalues)\n        return npvalues\n\n    # we have failed, return object\n    values = np.asarray(values, dtype=object)\n    return ensure_object(values)\n\n\ndef _reconstruct_data(\n    values: ArrayLikeT, dtype: DtypeObj, original: AnyArrayLike\n) -> ArrayLikeT:\n    \"\"\"\n    reverse of _ensure_data\n\n    Parameters\n    ----------\n    values : np.ndarray or ExtensionArray\n    dtype : np.dtype or ExtensionDtype\n    original : AnyArrayLike\n\n    Returns\n    -------\n    ExtensionArray or np.ndarray\n    \"\"\"\n    if isinstance(values, ABCExtensionArray) and values.dtype == dtype:\n        # Catch DatetimeArray/TimedeltaArray\n        return values\n\n    if not isinstance(dtype, np.dtype):\n        # i.e. ExtensionDtype; note we have ruled out above the possibility\n        #  that values.dtype == dtype\n        cls = dtype.construct_array_type()\n\n        # error: Incompatible types in assignment (expression has type\n        # \"ExtensionArray\", variable has type \"ndarray[Any, Any]\")\n        values = cls._from_sequence(values, dtype=dtype)  # type: ignore[assignment]\n\n    else:\n        values = values.astype(dtype, copy=False)\n\n    return values\n\n\ndef _ensure_arraylike(values, func_name: str) -> ArrayLike:\n    \"\"\"\n    ensure that we are arraylike if not already\n    \"\"\"\n    if not isinstance(\n        values,\n        (ABCIndex, ABCSeries, ABCExtensionArray, np.ndarray, ABCNumpyExtensionArray),\n    ):\n        # GH#52986\n        if func_name != \"isin-targets\":\n            # Make an exception for the comps argument in isin.\n            raise TypeError(\n                f\"{func_name} requires a Series, Index, \"\n                f\"ExtensionArray, np.ndarray or NumpyExtensionArray \"\n                f\"got {type(values).__name__}.\"\n            )\n\n        inferred = lib.infer_dtype(values, skipna=False)\n        if inferred in [\"mixed\", \"string\", \"mixed-integer\"]:\n            # \"mixed-integer\" to ensure we do not cast [\"ss\", 42] to str GH#22160\n            if isinstance(values, tuple):\n                values = list(values)\n            values = construct_1d_object_array_from_listlike(values)\n        else:\n            values = np.asarray(values)\n    return values\n\n\n_hashtables = {\n    \"complex128\": htable.Complex128HashTable,\n    \"complex64\": htable.Complex64HashTable,\n    \"float64\": htable.Float64HashTable,\n    \"float32\": htable.Float32HashTable,\n    \"uint64\": htable.UInt64HashTable,\n    \"uint32\": htable.UInt32HashTable,\n    \"uint16\": htable.UInt16HashTable,\n    \"uint8\": htable.UInt8HashTable,\n    \"int64\": htable.Int64HashTable,\n    \"int32\": htable.Int32HashTable,\n    \"int16\": htable.Int16HashTable,\n    \"int8\": htable.Int8HashTable,\n    \"string\": htable.StringHashTable,\n    \"object\": htable.PyObjectHashTable,\n}\n\n\ndef _get_hashtable_algo(\n    values: np.ndarray,\n) -> tuple[type[htable.HashTable], np.ndarray]:\n    \"\"\"\n    Parameters\n    ----------\n    values : np.ndarray\n\n    Returns\n    -------\n    htable : HashTable subclass\n    values : ndarray\n    \"\"\"\n    values = _ensure_data(values)\n\n    ndtype = _check_object_for_strings(values)\n    hashtable = _hashtables[ndtype]\n    return hashtable, values\n\n\ndef _check_object_for_strings(values: np.ndarray) -> str:\n    \"\"\"\n    Check if we can use string hashtable instead of object hashtable.\n\n    Parameters\n    ----------\n    values : ndarray\n\n    Returns\n    -------\n    str\n    \"\"\"\n    ndtype = values.dtype.name\n    if ndtype == \"object\":\n        # it's cheaper to use a String Hash Table than Object; we infer\n        # including nulls because that is the only difference between\n        # StringHashTable and ObjectHashtable\n        if lib.is_string_array(values, skipna=False):\n            ndtype = \"string\"\n    return ndtype\n\n\n# --------------- #\n# top-level algos #\n# --------------- #\n\n\ndef unique(values):\n    \"\"\"\n    Return unique values based on a hash table.\n\n    Uniques are returned in order of appearance. This does NOT sort.\n\n    Significantly faster than numpy.unique for long enough sequences.\n    Includes NA values.\n\n    Parameters\n    ----------\n    values : 1d array-like\n        The input array-like object containing values from which to extract\n        unique values.\n\n    Returns\n    -------\n    numpy.ndarray, ExtensionArray or NumpyExtensionArray\n\n        The return can be:\n\n        * Index : when the input is an Index\n        * Categorical : when the input is a Categorical dtype\n        * ndarray : when the input is a Series/ndarray\n\n        Return numpy.ndarray, ExtensionArray or NumpyExtensionArray.\n\n    See Also\n    --------\n    Index.unique : Return unique values from an Index.\n    Series.unique : Return unique values of Series object.\n\n    Examples\n    --------\n    >>> pd.unique(pd.Series([2, 1, 3, 3]))\n    array([2, 1, 3])\n\n    >>> pd.unique(pd.Series([2] + [1] * 5))\n    array([2, 1])\n\n    >>> pd.unique(pd.Series([pd.Timestamp(\"20160101\"), pd.Timestamp(\"20160101\")]))\n    array(['2016-01-01T00:00:00'], dtype='datetime64[s]')\n\n    >>> pd.unique(\n    ...     pd.Series(\n    ...         [\n    ...             pd.Timestamp(\"20160101\", tz=\"US/Eastern\"),\n    ...             pd.Timestamp(\"20160101\", tz=\"US/Eastern\"),\n    ...         ],\n    ...         dtype=\"M8[ns, US/Eastern]\",\n    ...     )\n    ... )\n    <DatetimeArray>\n    ['2016-01-01 00:00:00-05:00']\n    Length: 1, dtype: datetime64[ns, US/Eastern]\n\n    >>> pd.unique(\n    ...     pd.Index(\n    ...         [\n    ...             pd.Timestamp(\"20160101\", tz=\"US/Eastern\"),\n    ...             pd.Timestamp(\"20160101\", tz=\"US/Eastern\"),\n    ...         ],\n    ...         dtype=\"M8[ns, US/Eastern]\",\n    ...     )\n    ... )\n    DatetimeIndex(['2016-01-01 00:00:00-05:00'],\n            dtype='datetime64[ns, US/Eastern]',\n            freq=None)\n\n    >>> pd.unique(np.array(list(\"baabc\"), dtype=\"O\"))\n    array(['b', 'a', 'c'], dtype=object)\n\n    An unordered Categorical will return categories in the\n    order of appearance.\n\n    >>> pd.unique(pd.Series(pd.Categorical(list(\"baabc\"))))\n    ['b', 'a', 'c']\n    Categories (3, object): ['a', 'b', 'c']\n\n    >>> pd.unique(pd.Series(pd.Categorical(list(\"baabc\"), categories=list(\"abc\"))))\n    ['b', 'a', 'c']\n    Categories (3, object): ['a', 'b', 'c']\n\n    An ordered Categorical preserves the category ordering.\n\n    >>> pd.unique(\n    ...     pd.Series(\n    ...         pd.Categorical(list(\"baabc\"), categories=list(\"abc\"), ordered=True)\n    ...     )\n    ... )\n    ['b', 'a', 'c']\n    Categories (3, object): ['a' < 'b' < 'c']\n\n    An array of tuples\n\n    >>> pd.unique(pd.Series([(\"a\", \"b\"), (\"b\", \"a\"), (\"a\", \"c\"), (\"b\", \"a\")]).values)\n    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\n\n    An NumpyExtensionArray of complex\n\n    >>> pd.unique(pd.array([1 + 1j, 2, 3]))\n    <NumpyExtensionArray>\n    [(1+1j), (2+0j), (3+0j)]\n    Length: 3, dtype: complex128\n    \"\"\"\n    return unique_with_mask(values)\n\n\ndef nunique_ints(values: ArrayLike) -> int:\n    \"\"\"\n    Return the number of unique values for integer array-likes.\n\n    Significantly faster than pandas.unique for long enough sequences.\n    No checks are done to ensure input is integral.\n\n    Parameters\n    ----------\n    values : 1d array-like\n\n    Returns\n    -------\n    int : The number of unique values in ``values``\n    \"\"\"\n    if len(values) == 0:\n        return 0\n    values = _ensure_data(values)\n    # bincount requires intp\n    result = (np.bincount(values.ravel().astype(\"intp\")) != 0).sum()\n    return result\n\n\ndef unique_with_mask(values, mask: npt.NDArray[np.bool_] | None = None):\n    \"\"\"See algorithms.unique for docs. Takes a mask for masked arrays.\"\"\"\n    values = _ensure_arraylike(values, func_name=\"unique\")\n\n    if isinstance(values.dtype, ExtensionDtype):\n        # Dispatch to extension dtype's unique.\n        return values.unique()\n\n    if isinstance(values, ABCIndex):\n        # Dispatch to Index's unique.\n        return values.unique()\n\n    original = values\n    hashtable, values = _get_hashtable_algo(values)\n\n    table = hashtable(len(values))\n    if mask is None:\n        uniques = table.unique(values)\n        uniques = _reconstruct_data(uniques, original.dtype, original)\n        return uniques\n\n    else:\n        uniques, mask = table.unique(values, mask=mask)\n        uniques = _reconstruct_data(uniques, original.dtype, original)\n        assert mask is not None  # for mypy\n        return uniques, mask.astype(\"bool\")\n\n\nunique1d = unique\n\n\n_MINIMUM_COMP_ARR_LEN = 1_000_000\n\n\ndef isin(comps: ListLike, values: ListLike) -> npt.NDArray[np.bool_]:\n    \"\"\"\n    Compute the isin boolean array.\n\n    Parameters\n    ----------\n    comps : list-like\n    values : list-like\n\n    Returns\n    -------\n    ndarray[bool]\n        Same length as `comps`.\n    \"\"\"\n    if not is_list_like(comps):\n        raise TypeError(\n            \"only list-like objects are allowed to be passed \"\n            f\"to isin(), you passed a `{type(comps).__name__}`\"\n        )\n    if not is_list_like(values):\n        raise TypeError(\n            \"only list-like objects are allowed to be passed \"\n            f\"to isin(), you passed a `{type(values).__name__}`\"\n        )\n\n    if not isinstance(values, (ABCIndex, ABCSeries, ABCExtensionArray, np.ndarray)):\n        orig_values = list(values)\n        values = _ensure_arraylike(orig_values, func_name=\"isin-targets\")\n\n        if (\n            len(values) > 0\n            and values.dtype.kind in \"iufcb\"\n            and not is_signed_integer_dtype(comps)\n        ):\n            # GH#46485 Use object to avoid upcast to float64 later\n            # TODO: Share with _find_common_type_compat\n            values = construct_1d_object_array_from_listlike(orig_values)\n\n    elif isinstance(values, ABCMultiIndex):\n        # Avoid raising in extract_array\n        values = np.array(values)\n    else:\n        values = extract_array(values, extract_numpy=True, extract_range=True)\n\n    comps_array = _ensure_arraylike(comps, func_name=\"isin\")\n    comps_array = extract_array(comps_array, extract_numpy=True)\n    if not isinstance(comps_array, np.ndarray):\n        # i.e. Extension Array\n        return comps_array.isin(values)\n\n    elif needs_i8_conversion(comps_array.dtype):\n        # Dispatch to DatetimeLikeArrayMixin.isin\n        return pd_array(comps_array).isin(values)\n    elif needs_i8_conversion(values.dtype) and not is_object_dtype(comps_array.dtype):\n        # e.g. comps_array are integers and values are datetime64s\n        return np.zeros(comps_array.shape, dtype=bool)\n        # TODO: not quite right ... Sparse/Categorical\n    elif needs_i8_conversion(values.dtype):\n        return isin(comps_array, values.astype(object))\n\n    elif isinstance(values.dtype, ExtensionDtype):\n        return isin(np.asarray(comps_array), np.asarray(values))\n\n    # GH16012\n    # Ensure np.isin doesn't get object types or it *may* throw an exception\n    # Albeit hashmap has O(1) look-up (vs. O(logn) in sorted array),\n    # isin is faster for small sizes\n\n    # GH60678\n    # Ensure values don't contain <NA>, otherwise it throws exception with np.in1d\n\n    if (\n        len(comps_array) > _MINIMUM_COMP_ARR_LEN\n        and len(values) <= 26\n        and comps_array.dtype != object\n        and not any(v is NA for v in values)\n    ):\n        # If the values include nan we need to check for nan explicitly\n        # since np.nan it not equal to np.nan\n        if isna(values).any():\n\n            def f(c, v):\n                return np.logical_or(np.isin(c, v).ravel(), np.isnan(c))\n\n        else:\n            f = lambda a, b: np.isin(a, b).ravel()\n\n    else:\n        common = np_find_common_type(values.dtype, comps_array.dtype)\n        values = values.astype(common, copy=False)\n        comps_array = comps_array.astype(common, copy=False)\n        f = htable.ismember\n\n    return f(comps_array, values)\n\n\ndef factorize_array(\n    values: np.ndarray,\n    use_na_sentinel: bool = True,\n    size_hint: int | None = None,\n    na_value: object = None,\n    mask: npt.NDArray[np.bool_] | None = None,\n) -> tuple[npt.NDArray[np.intp], np.ndarray]:\n    \"\"\"\n    Factorize a numpy array to codes and uniques.\n\n    This doesn't do any coercion of types or unboxing before factorization.\n\n    Parameters\n    ----------\n    values : ndarray\n    use_na_sentinel : bool, default True\n        If True, the sentinel -1 will be used for NaN values. If False,\n        NaN values will be encoded as non-negative integers and will not drop the\n        NaN from the uniques of the values.\n    size_hint : int, optional\n        Passed through to the hashtable's 'get_labels' method\n    na_value : object, optional\n        A value in `values` to consider missing. Note: only use this\n        parameter when you know that you don't have any values pandas would\n        consider missing in the array (NaN for float data, iNaT for\n        datetimes, etc.).\n    mask : ndarray[bool], optional\n        If not None, the mask is used as indicator for missing values\n        (True = missing, False = valid) instead of `na_value` or\n        condition \"val != val\".\n\n    Returns\n    -------\n    codes : ndarray[np.intp]\n    uniques : ndarray\n    \"\"\"\n    original = values\n    if values.dtype.kind in \"mM\":\n        # _get_hashtable_algo will cast dt64/td64 to i8 via _ensure_data, so we\n        #  need to do the same to na_value. We are assuming here that the passed\n        #  na_value is an appropriately-typed NaT.\n        # e.g. test_where_datetimelike_categorical\n        na_value = iNaT\n\n    hash_klass, values = _get_hashtable_algo(values)\n\n    table = hash_klass(size_hint or len(values))\n    uniques, codes = table.factorize(\n        values,\n        na_sentinel=-1,\n        na_value=na_value,\n        mask=mask,\n        ignore_na=use_na_sentinel,\n    )\n\n    # re-cast e.g. i8->dt64/td64, uint8->bool\n    uniques = _reconstruct_data(uniques, original.dtype, original)\n\n    codes = ensure_platform_int(codes)\n    return codes, uniques\n\n\n@doc(\n    values=dedent(\n        \"\"\"\\\n    values : sequence\n        A 1-D sequence. Sequences that aren't pandas objects are\n        coerced to ndarrays before factorization.\n    \"\"\"\n    ),\n    sort=dedent(\n        \"\"\"\\\n    sort : bool, default False\n        Sort `uniques` and shuffle `codes` to maintain the\n        relationship.\n    \"\"\"\n    ),\n    size_hint=dedent(\n        \"\"\"\\\n    size_hint : int, optional\n        Hint to the hashtable sizer.\n    \"\"\"\n    ),\n)\ndef factorize(\n    values,\n    sort: bool = False,\n    use_na_sentinel: bool = True,\n    size_hint: int | None = None,\n) -> tuple[np.ndarray, np.ndarray | Index]:\n    \"\"\"\n    Encode the object as an enumerated type or categorical variable.\n\n    This method is useful for obtaining a numeric representation of an\n    array when all that matters is identifying distinct values. `factorize`\n    is available as both a top-level function :func:`pandas.factorize`,\n    and as a method :meth:`Series.factorize` and :meth:`Index.factorize`.\n\n    Parameters\n    ----------\n    {values}{sort}\n    use_na_sentinel : bool, default True\n        If True, the sentinel -1 will be used for NaN values. If False,\n        NaN values will be encoded as non-negative integers and will not drop the\n        NaN from the uniques of the values.\n\n        .. versionadded:: 1.5.0\n    {size_hint}\\\n\n    Returns\n    -------\n    codes : ndarray\n        An integer ndarray that's an indexer into `uniques`.\n        ``uniques.take(codes)`` will have the same values as `values`.\n    uniques : ndarray, Index, or Categorical\n        The unique valid values. When `values` is Categorical, `uniques`\n        is a Categorical. When `values` is some other pandas object, an\n        `Index` is returned. Otherwise, a 1-D ndarray is returned.\n\n        .. note::\n\n           Even if there's a missing value in `values`, `uniques` will\n           *not* contain an entry for it.\n\n    See Also\n    --------\n    cut : Discretize continuous-valued array.\n    unique : Find the unique value in an array.\n\n    Notes\n    -----\n    Reference :ref:`the user guide <reshaping.factorize>` for more examples.\n\n    Examples\n    --------\n    These examples all show factorize as a top-level method like\n    ``pd.factorize(values)``. The results are identical for methods like\n    :meth:`Series.factorize`.\n\n    >>> codes, uniques = pd.factorize(np.array(['b', 'b', 'a', 'c', 'b'], dtype=\"O\"))\n    >>> codes\n    array([0, 0, 1, 2, 0])\n    >>> uniques\n    array(['b', 'a', 'c'], dtype=object)\n\n    With ``sort=True``, the `uniques` will be sorted, and `codes` will be\n    shuffled so that the relationship is the maintained.\n\n    >>> codes, uniques = pd.factorize(np.array(['b', 'b', 'a', 'c', 'b'], dtype=\"O\"),\n    ...                               sort=True)\n    >>> codes\n    array([1, 1, 0, 2, 1])\n    >>> uniques\n    array(['a', 'b', 'c'], dtype=object)\n\n    When ``use_na_sentinel=True`` (the default), missing values are indicated in\n    the `codes` with the sentinel value ``-1`` and missing values are not\n    included in `uniques`.\n\n    >>> codes, uniques = pd.factorize(np.array(['b', None, 'a', 'c', 'b'], dtype=\"O\"))\n    >>> codes\n    array([ 0, -1,  1,  2,  0])\n    >>> uniques\n    array(['b', 'a', 'c'], dtype=object)\n\n    Thus far, we've only factorized lists (which are internally coerced to\n    NumPy arrays). When factorizing pandas objects, the type of `uniques`\n    will differ. For Categoricals, a `Categorical` is returned.\n\n    >>> cat = pd.Categorical(['a', 'a', 'c'], categories=['a', 'b', 'c'])\n    >>> codes, uniques = pd.factorize(cat)\n    >>> codes\n    array([0, 0, 1])\n    >>> uniques\n    ['a', 'c']\n    Categories (3, object): ['a', 'b', 'c']\n\n    Notice that ``'b'`` is in ``uniques.categories``, despite not being\n    present in ``cat.values``.\n\n    For all other pandas objects, an Index of the appropriate type is\n    returned.\n\n    >>> cat = pd.Series(['a', 'a', 'c'])\n    >>> codes, uniques = pd.factorize(cat)\n    >>> codes\n    array([0, 0, 1])\n    >>> uniques\n    Index(['a', 'c'], dtype='object')\n\n    If NaN is in the values, and we want to include NaN in the uniques of the\n    values, it can be achieved by setting ``use_na_sentinel=False``.\n\n    >>> values = np.array([1, 2, 1, np.nan])\n    >>> codes, uniques = pd.factorize(values)  # default: use_na_sentinel=True\n    >>> codes\n    array([ 0,  1,  0, -1])\n    >>> uniques\n    array([1., 2.])\n\n    >>> codes, uniques = pd.factorize(values, use_na_sentinel=False)\n    >>> codes\n    array([0, 1, 0, 2])\n    >>> uniques\n    array([ 1.,  2., nan])\n    \"\"\"\n    # Implementation notes: This method is responsible for 3 things\n    # 1.) coercing data to array-like (ndarray, Index, extension array)\n    # 2.) factorizing codes and uniques\n    # 3.) Maybe boxing the uniques in an Index\n    #\n    # Step 2 is dispatched to extension types (like Categorical). They are\n    # responsible only for factorization. All data coercion, sorting and boxing\n    # should happen here.\n    if isinstance(values, (ABCIndex, ABCSeries)):\n        return values.factorize(sort=sort, use_na_sentinel=use_na_sentinel)\n\n    values = _ensure_arraylike(values, func_name=\"factorize\")\n    original = values\n\n    if (\n        isinstance(values, (ABCDatetimeArray, ABCTimedeltaArray))\n        and values.freq is not None\n    ):\n        # The presence of 'freq' means we can fast-path sorting and know there\n        #  aren't NAs\n        codes, uniques = values.factorize(sort=sort)\n        return codes, uniques\n\n    elif not isinstance(values, np.ndarray):\n        # i.e. ExtensionArray\n        codes, uniques = values.factorize(use_na_sentinel=use_na_sentinel)\n\n    else:\n        values = np.asarray(values)  # convert DTA/TDA/MultiIndex\n\n        if not use_na_sentinel and values.dtype == object:\n            # factorize can now handle differentiating various types of null values.\n            # These can only occur when the array has object dtype.\n            # However, for backwards compatibility we only use the null for the\n            # provided dtype. This may be revisited in the future, see GH#48476.\n            null_mask = isna(values)\n            if null_mask.any():\n                na_value = na_value_for_dtype(values.dtype, compat=False)\n                # Don't modify (potentially user-provided) array\n                values = np.where(null_mask, na_value, values)\n\n        codes, uniques = factorize_array(\n            values,\n            use_na_sentinel=use_na_sentinel,\n            size_hint=size_hint,\n        )\n\n    if sort and len(uniques) > 0:\n        uniques, codes = safe_sort(\n            uniques,\n            codes,\n            use_na_sentinel=use_na_sentinel,\n            assume_unique=True,\n            verify=False,\n        )\n\n    uniques = _reconstruct_data(uniques, original.dtype, original)\n\n    return codes, uniques\n\n\ndef value_counts_internal(\n    values,\n    sort: bool = True,\n    ascending: bool = False,\n    normalize: bool = False,\n    bins=None,\n    dropna: bool = True,\n) -> Series:\n    from pandas import (\n        Index,\n        Series,\n    )\n\n    index_name = getattr(values, \"name\", None)\n    name = \"proportion\" if normalize else \"count\"\n\n    if bins is not None:\n        from pandas.core.reshape.tile import cut\n\n        if isinstance(values, Series):\n            values = values._values\n\n        try:\n            ii = cut(values, bins, include_lowest=True)\n        except TypeError as err:\n            raise TypeError(\"bins argument only works with numeric data.\") from err\n\n        # count, remove nulls (from the index), and but the bins\n        result = ii.value_counts(dropna=dropna)\n        result.name = name\n        result = result[result.index.notna()]\n        result.index = result.index.astype(\"interval\")\n        result = result.sort_index()\n\n        # if we are dropna and we have NO values\n        if dropna and (result._values == 0).all():\n            result = result.iloc[0:0]\n\n        # normalizing is by len of all (regardless of dropna)\n        counts = np.array([len(ii)])\n\n    else:\n        if is_extension_array_dtype(values):\n            # handle Categorical and sparse,\n            result = Series(values, copy=False)._values.value_counts(dropna=dropna)\n            result.name = name\n            result.index.name = index_name\n            counts = result._values\n            if not isinstance(counts, np.ndarray):\n                # e.g. ArrowExtensionArray\n                counts = np.asarray(counts)\n\n        elif isinstance(values, ABCMultiIndex):\n            # GH49558\n            levels = list(range(values.nlevels))\n            result = (\n                Series(index=values, name=name)\n                .groupby(level=levels, dropna=dropna)\n                .size()\n            )\n            result.index.names = values.names\n            counts = result._values\n\n        else:\n            values = _ensure_arraylike(values, func_name=\"value_counts\")\n            keys, counts, _ = value_counts_arraylike(values, dropna)\n            if keys.dtype == np.float16:\n                keys = keys.astype(np.float32)\n\n            # Starting in 3.0, we no longer perform dtype inference on the\n            #  Index object we construct here, xref GH#56161\n            idx = Index(keys, dtype=keys.dtype, name=index_name)\n            result = Series(counts, index=idx, name=name, copy=False)\n\n    if sort:\n        result = result.sort_values(ascending=ascending)\n\n    if normalize:\n        result = result / counts.sum()\n\n    return result\n\n\n# Called once from SparseArray, otherwise could be private\ndef value_counts_arraylike(\n    values: np.ndarray, dropna: bool, mask: npt.NDArray[np.bool_] | None = None\n) -> tuple[ArrayLike, npt.NDArray[np.int64], int]:\n    \"\"\"\n    Parameters\n    ----------\n    values : np.ndarray\n    dropna : bool\n    mask : np.ndarray[bool] or None, default None\n\n    Returns\n    -------\n    uniques : np.ndarray\n    counts : np.ndarray[np.int64]\n    \"\"\"\n    original = values\n    values = _ensure_data(values)\n\n    keys, counts, na_counter = htable.value_count(values, dropna, mask=mask)\n\n    if needs_i8_conversion(original.dtype):\n        # datetime, timedelta, or period\n\n        if dropna:\n            mask = keys != iNaT\n            keys, counts = keys[mask], counts[mask]\n\n    res_keys = _reconstruct_data(keys, original.dtype, original)\n    return res_keys, counts, na_counter\n\n\ndef duplicated(\n    values: ArrayLike,\n    keep: Literal[\"first\", \"last\", False] = \"first\",\n    mask: npt.NDArray[np.bool_] | None = None,\n) -> npt.NDArray[np.bool_]:\n    \"\"\"\n    Return boolean ndarray denoting duplicate values.\n\n    Parameters\n    ----------\n    values : np.ndarray or ExtensionArray\n        Array over which to check for duplicate values.\n    keep : {'first', 'last', False}, default 'first'\n        - ``first`` : Mark duplicates as ``True`` except for the first\n          occurrence.\n        - ``last`` : Mark duplicates as ``True`` except for the last\n          occurrence.\n        - False : Mark all duplicates as ``True``.\n    mask : ndarray[bool], optional\n        array indicating which elements to exclude from checking\n\n    Returns\n    -------\n    duplicated : ndarray[bool]\n    \"\"\"\n    values = _ensure_data(values)\n    return htable.duplicated(values, keep=keep, mask=mask)\n\n\ndef mode(\n    values: ArrayLike, dropna: bool = True, mask: npt.NDArray[np.bool_] | None = None\n) -> ArrayLike:\n    \"\"\"\n    Returns the mode(s) of an array.\n\n    Parameters\n    ----------\n    values : array-like\n        Array over which to check for duplicate values.\n    dropna : bool, default True\n        Don't consider counts of NaN/NaT.\n\n    Returns\n    -------\n    np.ndarray or ExtensionArray\n    \"\"\"\n    values = _ensure_arraylike(values, func_name=\"mode\")\n    original = values\n\n    if needs_i8_conversion(values.dtype):\n        # Got here with ndarray; dispatch to DatetimeArray/TimedeltaArray.\n        values = ensure_wrapped_if_datetimelike(values)\n        values = cast(\"ExtensionArray\", values)\n        return values._mode(dropna=dropna)\n\n    values = _ensure_data(values)\n\n    npresult, res_mask = htable.mode(values, dropna=dropna, mask=mask)\n    if res_mask is not None:\n        return npresult, res_mask  # type: ignore[return-value]\n\n    try:\n        npresult = safe_sort(npresult)\n    except TypeError as err:\n        warnings.warn(\n            f\"Unable to sort modes: {err}\",\n            stacklevel=find_stack_level(),\n        )\n\n    result = _reconstruct_data(npresult, original.dtype, original)\n    return result\n\n\ndef rank(\n    values: ArrayLike,\n    axis: AxisInt = 0,\n    method: str = \"average\",\n    na_option: str = \"keep\",\n    ascending: bool = True,\n    pct: bool = False,\n) -> npt.NDArray[np.float64]:\n    \"\"\"\n    Rank the values along a given axis.\n\n    Parameters\n    ----------\n    values : np.ndarray or ExtensionArray\n        Array whose values will be ranked. The number of dimensions in this\n        array must not exceed 2.\n    axis : int, default 0\n        Axis over which to perform rankings.\n    method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n        The method by which tiebreaks are broken during the ranking.\n    na_option : {'keep', 'top'}, default 'keep'\n        The method by which NaNs are placed in the ranking.\n        - ``keep``: rank each NaN value with a NaN ranking\n        - ``top``: replace each NaN with either +/- inf so that they\n                   there are ranked at the top\n    ascending : bool, default True\n        Whether or not the elements should be ranked in ascending order.\n    pct : bool, default False\n        Whether or not to the display the returned rankings in integer form\n        (e.g. 1, 2, 3) or in percentile form (e.g. 0.333..., 0.666..., 1).\n    \"\"\"\n    is_datetimelike = needs_i8_conversion(values.dtype)\n    values = _ensure_data(values)\n\n    if values.ndim == 1:\n        ranks = algos.rank_1d(\n            values,\n            is_datetimelike=is_datetimelike,\n            ties_method=method,\n            ascending=ascending,\n            na_option=na_option,\n            pct=pct,\n        )\n    elif values.ndim == 2:\n        ranks = algos.rank_2d(\n            values,\n            axis=axis,\n            is_datetimelike=is_datetimelike,\n            ties_method=method,\n            ascending=ascending,\n            na_option=na_option,\n            pct=pct,\n        )\n    else:\n        raise TypeError(\"Array with ndim > 2 are not supported.\")\n\n    return ranks\n\n\n# ---- #\n# take #\n# ---- #\n\n\ndef take(\n    arr,\n    indices: TakeIndexer,\n    axis: AxisInt = 0,\n    allow_fill: bool = False,\n    fill_value=None,\n):\n    \"\"\"\n    Take elements from an array.\n\n    Parameters\n    ----------\n    arr : array-like or scalar value\n        Non array-likes (sequences/scalars without a dtype) are coerced\n        to an ndarray.\n\n        .. deprecated:: 2.1.0\n            Passing an argument other than a numpy.ndarray, ExtensionArray,\n            Index, or Series is deprecated.\n\n    indices : sequence of int or one-dimensional np.ndarray of int\n        Indices to be taken.\n    axis : int, default 0\n        The axis over which to select values.\n    allow_fill : bool, default False\n        How to handle negative values in `indices`.\n\n        * False: negative values in `indices` indicate positional indices\n          from the right (the default). This is similar to :func:`numpy.take`.\n\n        * True: negative values in `indices` indicate\n          missing values. These values are set to `fill_value`. Any other\n          negative values raise a ``ValueError``.\n\n    fill_value : any, optional\n        Fill value to use for NA-indices when `allow_fill` is True.\n        This may be ``None``, in which case the default NA value for\n        the type (``self.dtype.na_value``) is used.\n\n        For multi-dimensional `arr`, each *element* is filled with\n        `fill_value`.\n\n    Returns\n    -------\n    ndarray or ExtensionArray\n        Same type as the input.\n\n    Raises\n    ------\n    IndexError\n        When `indices` is out of bounds for the array.\n    ValueError\n        When the indexer contains negative values other than ``-1``\n        and `allow_fill` is True.\n\n    Notes\n    -----\n    When `allow_fill` is False, `indices` may be whatever dimensionality\n    is accepted by NumPy for `arr`.\n\n    When `allow_fill` is True, `indices` should be 1-D.\n\n    See Also\n    --------\n    numpy.take : Take elements from an array along an axis.\n\n    Examples\n    --------\n    >>> import pandas as pd\n\n    With the default ``allow_fill=False``, negative numbers indicate\n    positional indices from the right.\n\n    >>> pd.api.extensions.take(np.array([10, 20, 30]), [0, 0, -1])\n    array([10, 10, 30])\n\n    Setting ``allow_fill=True`` will place `fill_value` in those positions.\n\n    >>> pd.api.extensions.take(np.array([10, 20, 30]), [0, 0, -1], allow_fill=True)\n    array([10., 10., nan])\n\n    >>> pd.api.extensions.take(\n    ...     np.array([10, 20, 30]), [0, 0, -1], allow_fill=True, fill_value=-10\n    ... )\n    array([ 10,  10, -10])\n    \"\"\"\n    if not isinstance(\n        arr,\n        (np.ndarray, ABCExtensionArray, ABCIndex, ABCSeries, ABCNumpyExtensionArray),\n    ):\n        # GH#52981\n        raise TypeError(\n            \"pd.api.extensions.take requires a numpy.ndarray, ExtensionArray, \"\n            f\"Index, Series, or NumpyExtensionArray got {type(arr).__name__}.\"\n        )\n\n    indices = ensure_platform_int(indices)\n\n    if allow_fill:\n        # Pandas style, -1 means NA\n        validate_indices(indices, arr.shape[axis])\n        # error: Argument 1 to \"take_nd\" has incompatible type\n        # \"ndarray[Any, Any] | ExtensionArray | Index | Series\"; expected\n        # \"ndarray[Any, Any]\"\n        result = take_nd(\n            arr,  # type: ignore[arg-type]\n            indices,\n            axis=axis,\n            allow_fill=True,\n            fill_value=fill_value,\n        )\n    else:\n        # NumPy style\n        # error: Unexpected keyword argument \"axis\" for \"take\" of \"ExtensionArray\"\n        result = arr.take(indices, axis=axis)  # type: ignore[call-arg,assignment]\n    return result\n\n\n# ------------ #\n# searchsorted #\n# ------------ #\n\n\ndef searchsorted(\n    arr: ArrayLike,\n    value: NumpyValueArrayLike | ExtensionArray,\n    side: Literal[\"left\", \"right\"] = \"left\",\n    sorter: NumpySorter | None = None,\n) -> npt.NDArray[np.intp] | np.intp:\n    \"\"\"\n    Find indices where elements should be inserted to maintain order.\n\n    Find the indices into a sorted array `arr` (a) such that, if the\n    corresponding elements in `value` were inserted before the indices,\n    the order of `arr` would be preserved.\n\n    Assuming that `arr` is sorted:\n\n    ======  ================================\n    `side`  returned index `i` satisfies\n    ======  ================================\n    left    ``arr[i-1] < value <= self[i]``\n    right   ``arr[i-1] <= value < self[i]``\n    ======  ================================\n\n    Parameters\n    ----------\n    arr: np.ndarray, ExtensionArray, Series\n        Input array. If `sorter` is None, then it must be sorted in\n        ascending order, otherwise `sorter` must be an array of indices\n        that sort it.\n    value : array-like or scalar\n        Values to insert into `arr`.\n    side : {'left', 'right'}, optional\n        If 'left', the index of the first suitable location found is given.\n        If 'right', return the last such index.  If there is no suitable\n        index, return either 0 or N (where N is the length of `self`).\n    sorter : 1-D array-like, optional\n        Optional array of integer indices that sort array a into ascending\n        order. They are typically the result of argsort.\n\n    Returns\n    -------\n    array of ints or int\n        If value is array-like, array of insertion points.\n        If value is scalar, a single integer.\n\n    See Also\n    --------\n    numpy.searchsorted : Similar method from NumPy.\n    \"\"\"\n    if sorter is not None:\n        sorter = ensure_platform_int(sorter)\n\n    if (\n        isinstance(arr, np.ndarray)\n        and arr.dtype.kind in \"iu\"\n        and (is_integer(value) or is_integer_dtype(value))\n    ):\n        # if `arr` and `value` have different dtypes, `arr` would be\n        # recast by numpy, causing a slow search.\n        # Before searching below, we therefore try to give `value` the\n        # same dtype as `arr`, while guarding against integer overflows.\n        iinfo = np.iinfo(arr.dtype.type)\n        value_arr = np.array([value]) if is_integer(value) else np.array(value)\n        if (value_arr >= iinfo.min).all() and (value_arr <= iinfo.max).all():\n            # value within bounds, so no overflow, so can convert value dtype\n            # to dtype of arr\n            dtype = arr.dtype\n        else:\n            dtype = value_arr.dtype\n\n        if is_integer(value):\n            # We know that value is int\n            value = cast(int, dtype.type(value))\n        else:\n            value = pd_array(cast(ArrayLike, value), dtype=dtype)\n    else:\n        # E.g. if `arr` is an array with dtype='datetime64[ns]'\n        # and `value` is a pd.Timestamp, we may need to convert value\n        arr = ensure_wrapped_if_datetimelike(arr)\n\n    # Argument 1 to \"searchsorted\" of \"ndarray\" has incompatible type\n    # \"Union[NumpyValueArrayLike, ExtensionArray]\"; expected \"NumpyValueArrayLike\"\n    return arr.searchsorted(value, side=side, sorter=sorter)  # type: ignore[arg-type]\n\n\n# ---- #\n# diff #\n# ---- #\n\n_diff_special = {\"float64\", \"float32\", \"int64\", \"int32\", \"int16\", \"int8\"}\n\n\ndef diff(arr, n: int, axis: AxisInt = 0):\n    \"\"\"\n    difference of n between self,\n    analogous to s-s.shift(n)\n\n    Parameters\n    ----------\n    arr : ndarray or ExtensionArray\n    n : int\n        number of periods\n    axis : {0, 1}\n        axis to shift on\n    stacklevel : int, default 3\n        The stacklevel for the lost dtype warning.\n\n    Returns\n    -------\n    shifted\n    \"\"\"\n\n    # added a check on the integer value of period\n    # see https://github.com/pandas-dev/pandas/issues/56607\n    if not lib.is_integer(n):\n        if not (is_float(n) and n.is_integer()):\n            raise ValueError(\"periods must be an integer\")\n        n = int(n)\n    na = np.nan\n    dtype = arr.dtype\n\n    is_bool = is_bool_dtype(dtype)\n    if is_bool:\n        op = operator.xor\n    else:\n        op = operator.sub\n\n    if isinstance(dtype, NumpyEADtype):\n        # NumpyExtensionArray cannot necessarily hold shifted versions of itself.\n        arr = arr.to_numpy()\n        dtype = arr.dtype\n\n    if not isinstance(arr, np.ndarray):\n        # i.e ExtensionArray\n        if hasattr(arr, f\"__{op.__name__}__\"):\n            if axis != 0:\n                raise ValueError(f\"cannot diff {type(arr).__name__} on axis={axis}\")\n            return op(arr, arr.shift(n))\n        else:\n            raise TypeError(\n                f\"{type(arr).__name__} has no 'diff' method. \"\n                \"Convert to a suitable dtype prior to calling 'diff'.\"\n            )\n\n    is_timedelta = False\n    if arr.dtype.kind in \"mM\":\n        dtype = np.int64\n        arr = arr.view(\"i8\")\n        na = iNaT\n        is_timedelta = True\n\n    elif is_bool:\n        # We have to cast in order to be able to hold np.nan\n        dtype = np.object_\n\n    elif dtype.kind in \"iu\":\n        # We have to cast in order to be able to hold np.nan\n\n        # int8, int16 are incompatible with float64,\n        # see https://github.com/cython/cython/issues/2646\n        if arr.dtype.name in [\"int8\", \"int16\"]:\n            dtype = np.float32\n        else:\n            dtype = np.float64\n\n    orig_ndim = arr.ndim\n    if orig_ndim == 1:\n        # reshape so we can always use algos.diff_2d\n        arr = arr.reshape(-1, 1)\n        # TODO: require axis == 0\n\n    dtype = np.dtype(dtype)\n    out_arr = np.empty(arr.shape, dtype=dtype)\n\n    na_indexer = [slice(None)] * 2\n    na_indexer[axis] = slice(None, n) if n >= 0 else slice(n, None)\n    out_arr[tuple(na_indexer)] = na\n\n    if arr.dtype.name in _diff_special:\n        # TODO: can diff_2d dtype specialization troubles be fixed by defining\n        #  out_arr inside diff_2d?\n        algos.diff_2d(arr, out_arr, n, axis, datetimelike=is_timedelta)\n    else:\n        # To keep mypy happy, _res_indexer is a list while res_indexer is\n        #  a tuple, ditto for lag_indexer.\n        _res_indexer = [slice(None)] * 2\n        _res_indexer[axis] = slice(n, None) if n >= 0 else slice(None, n)\n        res_indexer = tuple(_res_indexer)\n\n        _lag_indexer = [slice(None)] * 2\n        _lag_indexer[axis] = slice(None, -n) if n > 0 else slice(-n, None)\n        lag_indexer = tuple(_lag_indexer)\n\n        out_arr[res_indexer] = op(arr[res_indexer], arr[lag_indexer])\n\n    if is_timedelta:\n        out_arr = out_arr.view(\"timedelta64[ns]\")\n\n    if orig_ndim == 1:\n        out_arr = out_arr[:, 0]\n    return out_arr\n\n\n# --------------------------------------------------------------------\n# Helper functions\n\n\n# Note: safe_sort is in algorithms.py instead of sorting.py because it is\n#  low-dependency, is used in this module, and used private methods from\n#  this module.\ndef safe_sort(\n    values: Index | ArrayLike,\n    codes: npt.NDArray[np.intp] | None = None,\n    use_na_sentinel: bool = True,\n    assume_unique: bool = False,\n    verify: bool = True,\n) -> AnyArrayLike | tuple[AnyArrayLike, np.ndarray]:\n    \"\"\"\n    Sort ``values`` and reorder corresponding ``codes``.\n\n    ``values`` should be unique if ``codes`` is not None.\n    Safe for use with mixed types (int, str), orders ints before strs.\n\n    Parameters\n    ----------\n    values : list-like\n        Sequence; must be unique if ``codes`` is not None.\n    codes : np.ndarray[intp] or None, default None\n        Indices to ``values``. All out of bound indices are treated as\n        \"not found\" and will be masked with ``-1``.\n    use_na_sentinel : bool, default True\n        If True, the sentinel -1 will be used for NaN values. If False,\n        NaN values will be encoded as non-negative integers and will not drop the\n        NaN from the uniques of the values.\n    assume_unique : bool, default False\n        When True, ``values`` are assumed to be unique, which can speed up\n        the calculation. Ignored when ``codes`` is None.\n    verify : bool, default True\n        Check if codes are out of bound for the values and put out of bound\n        codes equal to ``-1``. If ``verify=False``, it is assumed there\n        are no out of bound codes. Ignored when ``codes`` is None.\n\n    Returns\n    -------\n    ordered : AnyArrayLike\n        Sorted ``values``\n    new_codes : ndarray\n        Reordered ``codes``; returned when ``codes`` is not None.\n\n    Raises\n    ------\n    TypeError\n        * If ``values`` is not list-like or if ``codes`` is neither None\n        nor list-like\n        * If ``values`` cannot be sorted\n    ValueError\n        * If ``codes`` is not None and ``values`` contain duplicates.\n    \"\"\"\n    if not isinstance(values, (np.ndarray, ABCExtensionArray, ABCIndex)):\n        raise TypeError(\n            \"Only np.ndarray, ExtensionArray, and Index objects are allowed to \"\n            \"be passed to safe_sort as values\"\n        )\n\n    sorter = None\n    ordered: AnyArrayLike\n\n    if (\n        not isinstance(values.dtype, ExtensionDtype)\n        and lib.infer_dtype(values, skipna=False) == \"mixed-integer\"\n    ):\n        ordered = _sort_mixed(values)\n    else:\n        try:\n            sorter = values.argsort()\n            ordered = values.take(sorter)\n        except (TypeError, decimal.InvalidOperation):\n            # Previous sorters failed or were not applicable, try `_sort_mixed`\n            # which would work, but which fails for special case of 1d arrays\n            # with tuples.\n            if values.size and isinstance(values[0], tuple):\n                # error: Argument 1 to \"_sort_tuples\" has incompatible type\n                # \"Union[Index, ExtensionArray, ndarray[Any, Any]]\"; expected\n                # \"ndarray[Any, Any]\"\n                ordered = _sort_tuples(values)  # type: ignore[arg-type]\n            else:\n                ordered = _sort_mixed(values)\n\n    # codes:\n\n    if codes is None:\n        return ordered\n\n    if not is_list_like(codes):\n        raise TypeError(\n            \"Only list-like objects or None are allowed to \"\n            \"be passed to safe_sort as codes\"\n        )\n    codes = ensure_platform_int(np.asarray(codes))\n\n    if not assume_unique and not len(unique(values)) == len(values):\n        raise ValueError(\"values should be unique if codes is not None\")\n\n    if sorter is None:\n        # mixed types\n        # error: Argument 1 to \"_get_hashtable_algo\" has incompatible type\n        # \"Union[Index, ExtensionArray, ndarray[Any, Any]]\"; expected\n        # \"ndarray[Any, Any]\"\n        hash_klass, values = _get_hashtable_algo(values)  # type: ignore[arg-type]\n        t = hash_klass(len(values))\n        t.map_locations(values)\n        # error: Argument 1 to \"lookup\" of \"HashTable\" has incompatible type\n        # \"ExtensionArray | ndarray[Any, Any] | Index | Series\"; expected \"ndarray\"\n        sorter = ensure_platform_int(t.lookup(ordered))  # type: ignore[arg-type]\n\n    if use_na_sentinel:\n        # take_nd is faster, but only works for na_sentinels of -1\n        order2 = sorter.argsort()\n        if verify:\n            mask = (codes < -len(values)) | (codes >= len(values))\n            codes[mask] = -1\n        new_codes = take_nd(order2, codes, fill_value=-1)\n    else:\n        reverse_indexer = np.empty(len(sorter), dtype=int)\n        reverse_indexer.put(sorter, np.arange(len(sorter)))\n        # Out of bound indices will be masked with `-1` next, so we\n        # may deal with them here without performance loss using `mode='wrap'`\n        new_codes = reverse_indexer.take(codes, mode=\"wrap\")\n\n    return ordered, ensure_platform_int(new_codes)\n\n\ndef _sort_mixed(values) -> AnyArrayLike:\n    \"\"\"order ints before strings before nulls in 1d arrays\"\"\"\n    str_pos = np.array([isinstance(x, str) for x in values], dtype=bool)\n    null_pos = np.array([isna(x) for x in values], dtype=bool)\n    num_pos = ~str_pos & ~null_pos\n    str_argsort = np.argsort(values[str_pos])\n    num_argsort = np.argsort(values[num_pos])\n    # convert boolean arrays to positional indices, then order by underlying values\n    str_locs = str_pos.nonzero()[0].take(str_argsort)\n    num_locs = num_pos.nonzero()[0].take(num_argsort)\n    null_locs = null_pos.nonzero()[0]\n    locs = np.concatenate([num_locs, str_locs, null_locs])\n    return values.take(locs)\n\n\ndef _sort_tuples(values: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Convert array of tuples (1d) to array of arrays (2d).\n    We need to keep the columns separately as they contain different types and\n    nans (can't use `np.sort` as it may fail when str and nan are mixed in a\n    column as types cannot be compared).\n    \"\"\"\n    from pandas.core.internals.construction import to_arrays\n    from pandas.core.sorting import lexsort_indexer\n\n    arrays, _ = to_arrays(values, None)\n    indexer = lexsort_indexer(arrays, orders=True)\n    return values[indexer]\n\n\ndef union_with_duplicates(\n    lvals: ArrayLike | Index, rvals: ArrayLike | Index\n) -> ArrayLike | Index:\n    \"\"\"\n    Extracts the union from lvals and rvals with respect to duplicates and nans in\n    both arrays.\n\n    Parameters\n    ----------\n    lvals: np.ndarray or ExtensionArray\n        left values which is ordered in front.\n    rvals: np.ndarray or ExtensionArray\n        right values ordered after lvals.\n\n    Returns\n    -------\n    np.ndarray or ExtensionArray\n        Containing the unsorted union of both arrays.\n\n    Notes\n    -----\n    Caller is responsible for ensuring lvals.dtype == rvals.dtype.\n    \"\"\"\n    from pandas import Series\n\n    l_count = value_counts_internal(lvals, dropna=False)\n    r_count = value_counts_internal(rvals, dropna=False)\n    l_count, r_count = l_count.align(r_count, fill_value=0)\n    final_count = np.maximum(l_count.values, r_count.values)\n    final_count = Series(final_count, index=l_count.index, dtype=\"int\", copy=False)\n    if isinstance(lvals, ABCMultiIndex) and isinstance(rvals, ABCMultiIndex):\n        unique_vals = lvals.append(rvals).unique()\n    else:\n        if isinstance(lvals, ABCIndex):\n            lvals = lvals._values\n        if isinstance(rvals, ABCIndex):\n            rvals = rvals._values\n        # error: List item 0 has incompatible type \"Union[ExtensionArray,\n        # ndarray[Any, Any], Index]\"; expected \"Union[ExtensionArray,\n        # ndarray[Any, Any]]\"\n        combined = concat_compat([lvals, rvals])  # type: ignore[list-item]\n        unique_vals = unique(combined)\n        unique_vals = ensure_wrapped_if_datetimelike(unique_vals)\n    repeats = final_count.reindex(unique_vals).values\n    return np.repeat(unique_vals, repeats)\n\n\ndef map_array(\n    arr: ArrayLike,\n    mapper,\n    na_action: Literal[\"ignore\"] | None = None,\n) -> np.ndarray | ExtensionArray | Index:\n    \"\"\"\n    Map values using an input mapping or function.\n\n    Parameters\n    ----------\n    mapper : function, dict, or Series\n        Mapping correspondence.\n    na_action : {None, 'ignore'}, default None\n        If 'ignore', propagate NA values, without passing them to the\n        mapping correspondence.\n\n    Returns\n    -------\n    Union[ndarray, Index, ExtensionArray]\n        The output of the mapping function applied to the array.\n        If the function returns a tuple with more than one element\n        a MultiIndex will be returned.\n    \"\"\"\n    from pandas import Index\n\n    if na_action not in (None, \"ignore\"):\n        msg = f\"na_action must either be 'ignore' or None, {na_action} was passed\"\n        raise ValueError(msg)\n\n    # we can fastpath dict/Series to an efficient map\n    # as we know that we are not going to have to yield\n    # python types\n    if is_dict_like(mapper):\n        if isinstance(mapper, dict) and hasattr(mapper, \"__missing__\"):\n            # If a dictionary subclass defines a default value method,\n            # convert mapper to a lookup function (GH #15999).\n            dict_with_default = mapper\n            mapper = lambda x: dict_with_default[\n                np.nan if isinstance(x, float) and np.isnan(x) else x\n            ]\n        else:\n            # Dictionary does not have a default. Thus it's safe to\n            # convert to an Series for efficiency.\n            # we specify the keys here to handle the\n            # possibility that they are tuples\n\n            # The return value of mapping with an empty mapper is\n            # expected to be pd.Series(np.nan, ...). As np.nan is\n            # of dtype float64 the return value of this method should\n            # be float64 as well\n            from pandas import Series\n\n            if len(mapper) == 0:\n                mapper = Series(mapper, dtype=np.float64)\n            elif isinstance(mapper, dict):\n                mapper = Series(\n                    mapper.values(), index=Index(mapper.keys(), tupleize_cols=False)\n                )\n            else:\n                mapper = Series(mapper)\n\n    if isinstance(mapper, ABCSeries):\n        if na_action == \"ignore\":\n            mapper = mapper[mapper.index.notna()]\n\n        # Since values were input this means we came from either\n        # a dict or a series and mapper should be an index\n        indexer = mapper.index.get_indexer(arr)\n        new_values = take_nd(mapper._values, indexer)\n\n        return new_values\n\n    if not len(arr):\n        return arr.copy()\n\n    # we must convert to python types\n    values = arr.astype(object, copy=False)\n    if na_action is None:\n        return lib.map_infer(values, mapper)\n    else:\n        return lib.map_infer_mask(values, mapper, mask=isna(values).view(np.uint8))\n"
    },
    {
      "filename": "pandas/core/indexes/multi.py",
      "content": "from __future__ import annotations\n\nfrom collections.abc import (\n    Callable,\n    Collection,\n    Generator,\n    Hashable,\n    Iterable,\n    Sequence,\n)\nfrom functools import wraps\nfrom itertools import zip_longest\nfrom sys import getsizeof\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Literal,\n    cast,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import (\n    algos as libalgos,\n    index as libindex,\n    lib,\n)\nfrom pandas._libs.hashtable import duplicated\nfrom pandas._typing import (\n    AnyAll,\n    AnyArrayLike,\n    Axis,\n    DropKeep,\n    DtypeObj,\n    F,\n    IgnoreRaise,\n    IndexLabel,\n    IndexT,\n    Scalar,\n    Self,\n    Shape,\n    npt,\n)\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import (\n    InvalidIndexError,\n    PerformanceWarning,\n    UnsortedIndexError,\n)\nfrom pandas.util._decorators import (\n    Appender,\n    cache_readonly,\n    doc,\n    set_module,\n)\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.core.dtypes.cast import coerce_indexer_dtype\nfrom pandas.core.dtypes.common import (\n    ensure_int64,\n    ensure_platform_int,\n    is_hashable,\n    is_integer,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_scalar,\n    is_string_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import (\n    CategoricalDtype,\n    ExtensionDtype,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.inference import is_array_like\nfrom pandas.core.dtypes.missing import (\n    array_equivalent,\n    isna,\n)\n\nimport pandas.core.algorithms as algos\nfrom pandas.core.array_algos.putmask import validate_putmask\nfrom pandas.core.arrays import (\n    Categorical,\n    ExtensionArray,\n)\nfrom pandas.core.arrays.categorical import (\n    factorize_from_iterables,\n    recode_for_categories,\n)\nimport pandas.core.common as com\nfrom pandas.core.construction import sanitize_array\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.base import (\n    Index,\n    _index_shared_docs,\n    ensure_index,\n    get_unanimous_names,\n)\nfrom pandas.core.indexes.frozen import FrozenList\nfrom pandas.core.ops.invalid import make_invalid_op\nfrom pandas.core.sorting import (\n    get_group_index,\n    lexsort_indexer,\n)\n\nfrom pandas.io.formats.printing import pprint_thing\n\nif TYPE_CHECKING:\n    from pandas import (\n        CategoricalIndex,\n        DataFrame,\n        Series,\n    )\n\n_index_doc_kwargs = dict(ibase._index_doc_kwargs)\n_index_doc_kwargs.update(\n    {\"klass\": \"MultiIndex\", \"target_klass\": \"MultiIndex or list of tuples\"}\n)\n\n\nclass MultiIndexUInt64Engine(libindex.BaseMultiIndexCodesEngine, libindex.UInt64Engine):\n    \"\"\"Manages a MultiIndex by mapping label combinations to positive integers.\n\n    The number of possible label combinations must not overflow the 64 bits integers.\n    \"\"\"\n\n    _base = libindex.UInt64Engine\n    _codes_dtype = \"uint64\"\n\n\nclass MultiIndexUInt32Engine(libindex.BaseMultiIndexCodesEngine, libindex.UInt32Engine):\n    \"\"\"Manages a MultiIndex by mapping label combinations to positive integers.\n\n    The number of possible label combinations must not overflow the 32 bits integers.\n    \"\"\"\n\n    _base = libindex.UInt32Engine\n    _codes_dtype = \"uint32\"\n\n\nclass MultiIndexUInt16Engine(libindex.BaseMultiIndexCodesEngine, libindex.UInt16Engine):\n    \"\"\"Manages a MultiIndex by mapping label combinations to positive integers.\n\n    The number of possible label combinations must not overflow the 16 bits integers.\n    \"\"\"\n\n    _base = libindex.UInt16Engine\n    _codes_dtype = \"uint16\"\n\n\nclass MultiIndexUInt8Engine(libindex.BaseMultiIndexCodesEngine, libindex.UInt8Engine):\n    \"\"\"Manages a MultiIndex by mapping label combinations to positive integers.\n\n    The number of possible label combinations must not overflow the 8 bits integers.\n    \"\"\"\n\n    _base = libindex.UInt8Engine\n    _codes_dtype = \"uint8\"\n\n\nclass MultiIndexPyIntEngine(libindex.BaseMultiIndexCodesEngine, libindex.ObjectEngine):\n    \"\"\"Manages a MultiIndex by mapping label combinations to positive integers.\n\n    This class manages those (extreme) cases in which the number of possible\n    label combinations overflows the 64 bits integers, and uses an ObjectEngine\n    containing Python integers.\n    \"\"\"\n\n    _base = libindex.ObjectEngine\n    _codes_dtype = \"object\"\n\n\ndef names_compat(meth: F) -> F:\n    \"\"\"\n    A decorator to allow either `name` or `names` keyword but not both.\n\n    This makes it easier to share code with base class.\n    \"\"\"\n\n    @wraps(meth)\n    def new_meth(self_or_cls, *args, **kwargs):\n        if \"name\" in kwargs and \"names\" in kwargs:\n            raise TypeError(\"Can only provide one of `names` and `name`\")\n        if \"name\" in kwargs:\n            kwargs[\"names\"] = kwargs.pop(\"name\")\n\n        return meth(self_or_cls, *args, **kwargs)\n\n    return cast(F, new_meth)\n\n\n@set_module(\"pandas\")\nclass MultiIndex(Index):\n    \"\"\"\n    A multi-level, or hierarchical, index object for pandas objects.\n\n    Parameters\n    ----------\n    levels : sequence of arrays\n        The unique labels for each level.\n    codes : sequence of arrays\n        Integers for each level designating which label at each location.\n    sortorder : optional int\n        Level of sortedness (must be lexicographically sorted by that\n        level).\n    names : optional sequence of objects\n        Names for each of the index levels. (name is accepted for compat).\n    copy : bool, default False\n        Copy the meta-data.\n    name : Label\n        Kept for compatibility with 1-dimensional Index. Should not be used.\n    verify_integrity : bool, default True\n        Check that the levels/codes are consistent and valid.\n\n    Attributes\n    ----------\n    names\n    levels\n    codes\n    nlevels\n    levshape\n    dtypes\n\n    Methods\n    -------\n    from_arrays\n    from_tuples\n    from_product\n    from_frame\n    set_levels\n    set_codes\n    to_frame\n    to_flat_index\n    sortlevel\n    droplevel\n    swaplevel\n    reorder_levels\n    remove_unused_levels\n    get_level_values\n    get_indexer\n    get_loc\n    get_locs\n    get_loc_level\n    drop\n\n    See Also\n    --------\n    MultiIndex.from_arrays  : Convert list of arrays to MultiIndex.\n    MultiIndex.from_product : Create a MultiIndex from the cartesian product\n                              of iterables.\n    MultiIndex.from_tuples  : Convert list of tuples to a MultiIndex.\n    MultiIndex.from_frame   : Make a MultiIndex from a DataFrame.\n    Index : The base pandas Index type.\n\n    Notes\n    -----\n    See the `user guide\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html>`__\n    for more.\n\n    Examples\n    --------\n    A new ``MultiIndex`` is typically constructed using one of the helper\n    methods :meth:`MultiIndex.from_arrays`, :meth:`MultiIndex.from_product`\n    and :meth:`MultiIndex.from_tuples`. For example (using ``.from_arrays``):\n\n    >>> arrays = [[1, 1, 2, 2], [\"red\", \"blue\", \"red\", \"blue\"]]\n    >>> pd.MultiIndex.from_arrays(arrays, names=(\"number\", \"color\"))\n    MultiIndex([(1,  'red'),\n                (1, 'blue'),\n                (2,  'red'),\n                (2, 'blue')],\n               names=['number', 'color'])\n\n    See further examples for how to construct a MultiIndex in the doc strings\n    of the mentioned helper methods.\n    \"\"\"\n\n    _hidden_attrs = Index._hidden_attrs | frozenset()\n\n    # initialize to zero-length tuples to make everything work\n    _typ = \"multiindex\"\n    _names: list[Hashable | None] = []\n    _levels = FrozenList()\n    _codes = FrozenList()\n    _comparables = [\"names\"]\n\n    sortorder: int | None\n\n    # --------------------------------------------------------------------\n    # Constructors\n\n    def __new__(\n        cls,\n        levels=None,\n        codes=None,\n        sortorder=None,\n        names=None,\n        copy: bool = False,\n        name=None,\n        verify_integrity: bool = True,\n    ) -> Self:\n        # compat with Index\n        if name is not None:\n            names = name\n        if levels is None or codes is None:\n            raise TypeError(\"Must pass both levels and codes\")\n        if len(levels) != len(codes):\n            raise ValueError(\"Length of levels and codes must be the same.\")\n        if len(levels) == 0:\n            raise ValueError(\"Must pass non-zero number of levels/codes\")\n\n        result = object.__new__(cls)\n        result._cache = {}\n\n        # we've already validated levels and codes, so shortcut here\n        result._set_levels(levels, copy=copy, validate=False)\n        result._set_codes(codes, copy=copy, validate=False)\n\n        result._names = [None] * len(levels)\n        if names is not None:\n            # handles name validation\n            result._set_names(names)\n\n        if sortorder is not None:\n            result.sortorder = int(sortorder)\n        else:\n            result.sortorder = sortorder\n\n        if verify_integrity:\n            new_codes = result._verify_integrity()\n            result._codes = new_codes\n\n        result._reset_identity()\n        result._references = None\n\n        return result\n\n    def _validate_codes(self, level: Index, code: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Reassign code values as -1 if their corresponding levels are NaN.\n\n        Parameters\n        ----------\n        code : Index\n            Code to reassign.\n        level : np.ndarray\n            Level to check for missing values (NaN, NaT, None).\n\n        Returns\n        -------\n        new code where code value = -1 if it corresponds\n        to a level with missing values (NaN, NaT, None).\n        \"\"\"\n        null_mask = isna(level)\n        if np.any(null_mask):\n            code = np.where(null_mask[code], -1, code)\n        return code\n\n    def _verify_integrity(\n        self,\n        codes: list | None = None,\n        levels: list | None = None,\n        levels_to_verify: list[int] | range | None = None,\n    ) -> FrozenList:\n        \"\"\"\n        Parameters\n        ----------\n        codes : optional list\n            Codes to check for validity. Defaults to current codes.\n        levels : optional list\n            Levels to check for validity. Defaults to current levels.\n        levels_to_validate: optional list\n            Specifies the levels to verify.\n\n        Raises\n        ------\n        ValueError\n            If length of levels and codes don't match, if the codes for any\n            level would exceed level bounds, or there are any duplicate levels.\n\n        Returns\n        -------\n        new codes where code value = -1 if it corresponds to a\n        NaN level.\n        \"\"\"\n        # NOTE: Currently does not check, among other things, that cached\n        # nlevels matches nor that sortorder matches actually sortorder.\n        codes = codes or self.codes\n        levels = levels or self.levels\n        if levels_to_verify is None:\n            levels_to_verify = range(len(levels))\n\n        if len(levels) != len(codes):\n            raise ValueError(\n                \"Length of levels and codes must match. NOTE: \"\n                \"this index is in an inconsistent state.\"\n            )\n        codes_length = len(codes[0])\n        for i in levels_to_verify:\n            level = levels[i]\n            level_codes = codes[i]\n\n            if len(level_codes) != codes_length:\n                raise ValueError(\n                    f\"Unequal code lengths: {[len(code_) for code_ in codes]}\"\n                )\n            if len(level_codes) and level_codes.max() >= len(level):\n                raise ValueError(\n                    f\"On level {i}, code max ({level_codes.max()}) >= length of \"\n                    f\"level ({len(level)}). NOTE: this index is in an \"\n                    \"inconsistent state\"\n                )\n            if len(level_codes) and level_codes.min() < -1:\n                raise ValueError(f\"On level {i}, code value ({level_codes.min()}) < -1\")\n            if not level.is_unique:\n                raise ValueError(\n                    f\"Level values must be unique: {list(level)} on level {i}\"\n                )\n        if self.sortorder is not None:\n            if self.sortorder > _lexsort_depth(self.codes, self.nlevels):\n                raise ValueError(\n                    \"Value for sortorder must be inferior or equal to actual \"\n                    f\"lexsort_depth: sortorder {self.sortorder} \"\n                    f\"with lexsort_depth {_lexsort_depth(self.codes, self.nlevels)}\"\n                )\n\n        result_codes = []\n        for i in range(len(levels)):\n            if i in levels_to_verify:\n                result_codes.append(self._validate_codes(levels[i], codes[i]))\n            else:\n                result_codes.append(codes[i])\n\n        new_codes = FrozenList(result_codes)\n        return new_codes\n\n    @classmethod\n    def from_arrays(\n        cls,\n        arrays,\n        sortorder: int | None = None,\n        names: Sequence[Hashable] | Hashable | lib.NoDefault = lib.no_default,\n    ) -> MultiIndex:\n        \"\"\"\n        Convert arrays to MultiIndex.\n\n        Parameters\n        ----------\n        arrays : list / sequence of array-likes\n            Each array-like gives one level's value for each data point.\n            len(arrays) is the number of levels.\n        sortorder : int or None\n            Level of sortedness (must be lexicographically sorted by that\n            level).\n        names : list / sequence of str, optional\n            Names for the levels in the index.\n\n        Returns\n        -------\n        MultiIndex\n\n        See Also\n        --------\n        MultiIndex.from_tuples : Convert list of tuples to MultiIndex.\n        MultiIndex.from_product : Make a MultiIndex from cartesian product\n                                  of iterables.\n        MultiIndex.from_frame : Make a MultiIndex from a DataFrame.\n\n        Examples\n        --------\n        >>> arrays = [[1, 1, 2, 2], [\"red\", \"blue\", \"red\", \"blue\"]]\n        >>> pd.MultiIndex.from_arrays(arrays, names=(\"number\", \"color\"))\n        MultiIndex([(1,  'red'),\n                    (1, 'blue'),\n                    (2,  'red'),\n                    (2, 'blue')],\n                   names=['number', 'color'])\n        \"\"\"\n        error_msg = \"Input must be a list / sequence of array-likes.\"\n        if not is_list_like(arrays):\n            raise TypeError(error_msg)\n        if is_iterator(arrays):\n            arrays = list(arrays)\n\n        # Check if elements of array are list-like\n        for array in arrays:\n            if not is_list_like(array):\n                raise TypeError(error_msg)\n\n        # Check if lengths of all arrays are equal or not,\n        # raise ValueError, if not\n        for i in range(1, len(arrays)):\n            if len(arrays[i]) != len(arrays[i - 1]):\n                raise ValueError(\"all arrays must be same length\")\n\n        codes, levels = factorize_from_iterables(arrays)\n        if names is lib.no_default:\n            names = [getattr(arr, \"name\", None) for arr in arrays]\n\n        return cls(\n            levels=levels,\n            codes=codes,\n            sortorder=sortorder,\n            names=names,\n            verify_integrity=False,\n        )\n\n    @classmethod\n    @names_compat\n    def from_tuples(\n        cls,\n        tuples: Iterable[tuple[Hashable, ...]],\n        sortorder: int | None = None,\n        names: Sequence[Hashable] | Hashable | None = None,\n    ) -> MultiIndex:\n        \"\"\"\n        Convert list of tuples to MultiIndex.\n\n        Parameters\n        ----------\n        tuples : list / sequence of tuple-likes\n            Each tuple is the index of one row/column.\n        sortorder : int or None\n            Level of sortedness (must be lexicographically sorted by that\n            level).\n        names : list / sequence of str, optional\n            Names for the levels in the index.\n\n        Returns\n        -------\n        MultiIndex\n\n        See Also\n        --------\n        MultiIndex.from_arrays : Convert list of arrays to MultiIndex.\n        MultiIndex.from_product : Make a MultiIndex from cartesian product\n                                  of iterables.\n        MultiIndex.from_frame : Make a MultiIndex from a DataFrame.\n\n        Examples\n        --------\n        >>> tuples = [(1, \"red\"), (1, \"blue\"), (2, \"red\"), (2, \"blue\")]\n        >>> pd.MultiIndex.from_tuples(tuples, names=(\"number\", \"color\"))\n        MultiIndex([(1,  'red'),\n                    (1, 'blue'),\n                    (2,  'red'),\n                    (2, 'blue')],\n                   names=['number', 'color'])\n        \"\"\"\n        if not is_list_like(tuples):\n            raise TypeError(\"Input must be a list / sequence of tuple-likes.\")\n        if is_iterator(tuples):\n            tuples = list(tuples)\n        tuples = cast(Collection[tuple[Hashable, ...]], tuples)\n\n        # handling the empty tuple cases\n        if len(tuples) and all(isinstance(e, tuple) and not e for e in tuples):\n            codes = [np.zeros(len(tuples))]\n            levels = [Index(com.asarray_tuplesafe(tuples, dtype=np.dtype(\"object\")))]\n            return cls(\n                levels=levels,\n                codes=codes,\n                sortorder=sortorder,\n                names=names,\n                verify_integrity=False,\n            )\n\n        arrays: list[Sequence[Hashable]]\n        if len(tuples) == 0:\n            if names is None:\n                raise TypeError(\"Cannot infer number of levels from empty list\")\n            # error: Argument 1 to \"len\" has incompatible type \"Hashable\";\n            # expected \"Sized\"\n            arrays = [[]] * len(names)  # type: ignore[arg-type]\n        elif isinstance(tuples, (np.ndarray, Index)):\n            if isinstance(tuples, Index):\n                tuples = np.asarray(tuples._values)\n\n            arrays = list(lib.tuples_to_object_array(tuples).T)\n        elif isinstance(tuples, list):\n            arrays = list(lib.to_object_array_tuples(tuples).T)\n        else:\n            arrs = zip_longest(*tuples, fillvalue=np.nan)\n            arrays = cast(list[Sequence[Hashable]], arrs)\n\n        return cls.from_arrays(arrays, sortorder=sortorder, names=names)\n\n    @classmethod\n    def from_product(\n        cls,\n        iterables: Sequence[Iterable[Hashable]],\n        sortorder: int | None = None,\n        names: Sequence[Hashable] | Hashable | lib.NoDefault = lib.no_default,\n    ) -> MultiIndex:\n        \"\"\"\n        Make a MultiIndex from the cartesian product of multiple iterables.\n\n        Parameters\n        ----------\n        iterables : list / sequence of iterables\n            Each iterable has unique labels for each level of the index.\n        sortorder : int or None\n            Level of sortedness (must be lexicographically sorted by that\n            level).\n        names : list / sequence of str, optional\n            Names for the levels in the index.\n            If not explicitly provided, names will be inferred from the\n            elements of iterables if an element has a name attribute.\n\n        Returns\n        -------\n        MultiIndex\n\n        See Also\n        --------\n        MultiIndex.from_arrays : Convert list of arrays to MultiIndex.\n        MultiIndex.from_tuples : Convert list of tuples to MultiIndex.\n        MultiIndex.from_frame : Make a MultiIndex from a DataFrame.\n\n        Examples\n        --------\n        >>> numbers = [0, 1, 2]\n        >>> colors = [\"green\", \"purple\"]\n        >>> pd.MultiIndex.from_product([numbers, colors], names=[\"number\", \"color\"])\n        MultiIndex([(0,  'green'),\n                    (0, 'purple'),\n                    (1,  'green'),\n                    (1, 'purple'),\n                    (2,  'green'),\n                    (2, 'purple')],\n                   names=['number', 'color'])\n        \"\"\"\n\n        if not is_list_like(iterables):\n            raise TypeError(\"Input must be a list / sequence of iterables.\")\n        if is_iterator(iterables):\n            iterables = list(iterables)\n\n        codes, levels = factorize_from_iterables(iterables)\n        if names is lib.no_default:\n            names = [getattr(it, \"name\", None) for it in iterables]\n\n        # codes are all ndarrays, so cartesian_product is lossless\n        codes = cartesian_product(codes)\n        return cls(levels, codes, sortorder=sortorder, names=names)\n\n    @classmethod\n    def from_frame(\n        cls,\n        df: DataFrame,\n        sortorder: int | None = None,\n        names: Sequence[Hashable] | Hashable | None = None,\n    ) -> MultiIndex:\n        \"\"\"\n        Make a MultiIndex from a DataFrame.\n\n        Parameters\n        ----------\n        df : DataFrame\n            DataFrame to be converted to MultiIndex.\n        sortorder : int, optional\n            Level of sortedness (must be lexicographically sorted by that\n            level).\n        names : list-like, optional\n            If no names are provided, use the column names, or tuple of column\n            names if the columns is a MultiIndex. If a sequence, overwrite\n            names with the given sequence.\n\n        Returns\n        -------\n        MultiIndex\n            The MultiIndex representation of the given DataFrame.\n\n        See Also\n        --------\n        MultiIndex.from_arrays : Convert list of arrays to MultiIndex.\n        MultiIndex.from_tuples : Convert list of tuples to MultiIndex.\n        MultiIndex.from_product : Make a MultiIndex from cartesian product\n                                  of iterables.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     [[\"HI\", \"Temp\"], [\"HI\", \"Precip\"], [\"NJ\", \"Temp\"], [\"NJ\", \"Precip\"]],\n        ...     columns=[\"a\", \"b\"],\n        ... )\n        >>> df\n              a       b\n        0    HI    Temp\n        1    HI  Precip\n        2    NJ    Temp\n        3    NJ  Precip\n\n        >>> pd.MultiIndex.from_frame(df)\n        MultiIndex([('HI',   'Temp'),\n                    ('HI', 'Precip'),\n                    ('NJ',   'Temp'),\n                    ('NJ', 'Precip')],\n                   names=['a', 'b'])\n\n        Using explicit names, instead of the column names\n\n        >>> pd.MultiIndex.from_frame(df, names=[\"state\", \"observation\"])\n        MultiIndex([('HI',   'Temp'),\n                    ('HI', 'Precip'),\n                    ('NJ',   'Temp'),\n                    ('NJ', 'Precip')],\n                   names=['state', 'observation'])\n        \"\"\"\n        if not isinstance(df, ABCDataFrame):\n            raise TypeError(\"Input must be a DataFrame\")\n\n        column_names, columns = zip(*df.items())\n        names = column_names if names is None else names\n        return cls.from_arrays(columns, sortorder=sortorder, names=names)\n\n    # --------------------------------------------------------------------\n\n    @cache_readonly\n    def _values(self) -> np.ndarray:\n        # We override here, since our parent uses _data, which we don't use.\n        values = []\n\n        for i in range(self.nlevels):\n            index = self.levels[i]\n            codes = self.codes[i]\n\n            vals = index\n            if isinstance(vals.dtype, CategoricalDtype):\n                vals = cast(\"CategoricalIndex\", vals)\n                vals = vals._data._internal_get_values()\n\n            if isinstance(vals.dtype, ExtensionDtype) or lib.is_np_dtype(\n                vals.dtype, \"mM\"\n            ):\n                vals = vals.astype(object)\n\n            array_vals = np.asarray(vals)\n            array_vals = algos.take_nd(array_vals, codes, fill_value=index._na_value)\n            values.append(array_vals)\n\n        arr = lib.fast_zip(values)\n        return arr\n\n    @property\n    def values(self) -> np.ndarray:\n        return self._values\n\n    @property\n    def array(self):\n        \"\"\"\n        Raises a ValueError for `MultiIndex` because there's no single\n        array backing a MultiIndex.\n\n        Raises\n        ------\n        ValueError\n        \"\"\"\n        raise ValueError(\n            \"MultiIndex has no single backing array. Use \"\n            \"'MultiIndex.to_numpy()' to get a NumPy array of tuples.\"\n        )\n\n    @cache_readonly\n    def dtypes(self) -> Series:\n        \"\"\"\n        Return the dtypes as a Series for the underlying MultiIndex.\n\n        See Also\n        --------\n        Index.dtype : Return the dtype object of the underlying data.\n        Series.dtypes : Return the data type of the underlying Series.\n\n        Examples\n        --------\n        >>> idx = pd.MultiIndex.from_product(\n        ...     [(0, 1, 2), (\"green\", \"purple\")], names=[\"number\", \"color\"]\n        ... )\n        >>> idx\n        MultiIndex([(0,  'green'),\n                    (0, 'purple'),\n                    (1,  'green'),\n                    (1, 'purple'),\n                    (2,  'green'),\n                    (2, 'purple')],\n                   names=['number', 'color'])\n        >>> idx.dtypes\n        number     int64\n        color     object\n        dtype: object\n        \"\"\"\n        from pandas import Series\n\n        names = com.fill_missing_names(self.names)\n        return Series([level.dtype for level in self.levels], index=Index(names))\n\n    def __len__(self) -> int:\n        return len(self.codes[0])\n\n    @property\n    def size(self) -> int:\n        \"\"\"\n        Return the number of elements in the underlying data.\n        \"\"\"\n        # override Index.size to avoid materializing _values\n        return len(self)\n\n    # --------------------------------------------------------------------\n    # Levels Methods\n\n    @cache_readonly\n    def levels(self) -> FrozenList:\n        \"\"\"\n        Levels of the MultiIndex.\n\n        Levels refer to the different hierarchical levels or layers in a MultiIndex.\n        In a MultiIndex, each level represents a distinct dimension or category of\n        the index.\n\n        To access the levels, you can use the levels attribute of the MultiIndex,\n        which returns a tuple of Index objects. Each Index object represents a\n        level in the MultiIndex and contains the unique values found in that\n        specific level.\n\n        If a MultiIndex is created with levels A, B, C, and the DataFrame using\n        it filters out all rows of the level C, MultiIndex.levels will still\n        return A, B, C.\n\n        See Also\n        --------\n        MultiIndex.codes : The codes of the levels in the MultiIndex.\n        MultiIndex.get_level_values : Return vector of label values for requested\n            level.\n\n        Examples\n        --------\n        >>> index = pd.MultiIndex.from_product(\n        ...     [[\"mammal\"], (\"goat\", \"human\", \"cat\", \"dog\")],\n        ...     names=[\"Category\", \"Animals\"],\n        ... )\n        >>> leg_num = pd.DataFrame(data=(4, 2, 4, 4), index=index, columns=[\"Legs\"])\n        >>> leg_num\n                          Legs\n        Category Animals\n        mammal   goat        4\n                 human       2\n                 cat         4\n                 dog         4\n\n        >>> leg_num.index.levels\n        FrozenList([['mammal'], ['cat', 'dog', 'goat', 'human']])\n\n        MultiIndex levels will not change even if the DataFrame using the MultiIndex\n        does not contain all them anymore.\n        See how \"human\" is not in the DataFrame, but it is still in levels:\n\n        >>> large_leg_num = leg_num[leg_num.Legs > 2]\n        >>> large_leg_num\n                          Legs\n        Category Animals\n        mammal   goat        4\n                 cat         4\n                 dog         4\n\n        >>> large_leg_num.index.levels\n        FrozenList([['mammal'], ['cat', 'dog', 'goat', 'human']])\n        \"\"\"\n        # Use cache_readonly to ensure that self.get_locs doesn't repeatedly\n        # create new IndexEngine\n        # https://github.com/pandas-dev/pandas/issues/31648\n        result = [x._rename(name=name) for x, name in zip(self._levels, self._names)]\n        for level in result:\n            # disallow midx.levels[0].name = \"foo\"\n            level._no_setting_name = True\n        return FrozenList(result)\n\n    def _set_levels(\n        self,\n        levels,\n        *,\n        level=None,\n        copy: bool = False,\n        validate: bool = True,\n        verify_integrity: bool = False,\n    ) -> None:\n        # This is NOT part of the levels property because it should be\n        # externally not allowed to set levels. User beware if you change\n        # _levels directly\n        if validate:\n            if len(levels) == 0:\n                raise ValueError(\"Must set non-zero number of levels.\")\n            if level is None and len(levels) != self.nlevels:\n                raise ValueError(\"Length of levels must match number of levels.\")\n            if level is not None and len(levels) != len(level):\n                raise ValueError(\"Length of levels must match length of level.\")\n\n        if level is None:\n            new_levels = FrozenList(\n                ensure_index(lev, copy=copy)._view() for lev in levels\n            )\n            level_numbers: range | list[int] = range(len(new_levels))\n        else:\n            level_numbers = [self._get_level_number(lev) for lev in level]\n            new_levels_list = list(self._levels)\n            for lev_num, lev in zip(level_numbers, levels):\n                new_levels_list[lev_num] = ensure_index(lev, copy=copy)._view()\n            new_levels = FrozenList(new_levels_list)\n\n        if verify_integrity:\n            new_codes = self._verify_integrity(\n                levels=new_levels, levels_to_verify=level_numbers\n            )\n            self._codes = new_codes\n\n        names = self.names\n        self._levels = new_levels\n        if any(names):\n            self._set_names(names)\n\n        self._reset_cache()\n\n    def set_levels(\n        self, levels, *, level=None, verify_integrity: bool = True\n    ) -> MultiIndex:\n        \"\"\"\n        Set new levels on MultiIndex. Defaults to returning new index.\n\n        The `set_levels` method provides a flexible way to change the levels of a\n        `MultiIndex`. This is particularly useful when you need to update the\n        index structure of your DataFrame without altering the data. The method\n        returns a new `MultiIndex` unless the operation is performed in-place,\n        ensuring that the original index remains unchanged unless explicitly\n        modified.\n\n        The method checks the integrity of the new levels against the existing\n        codes by default, but this can be disabled if you are confident that\n        your levels are consistent with the underlying data. This can be useful\n        when you want to perform optimizations or make specific adjustments to\n        the index levels that do not strictly adhere to the original structure.\n\n        Parameters\n        ----------\n        levels : sequence or list of sequence\n            New level(s) to apply.\n        level : int, level name, or sequence of int/level names (default None)\n            Level(s) to set (None for all levels).\n        verify_integrity : bool, default True\n            If True, checks that levels and codes are compatible.\n\n        Returns\n        -------\n        MultiIndex\n            A new `MultiIndex` with the updated levels.\n\n        See Also\n        --------\n        MultiIndex.set_codes : Set new codes on the existing `MultiIndex`.\n        MultiIndex.remove_unused_levels : Create new MultiIndex from current that\n            removes unused levels.\n        Index.set_names : Set Index or MultiIndex name.\n\n        Examples\n        --------\n        >>> idx = pd.MultiIndex.from_tuples(\n        ...     [\n        ...         (1, \"one\"),\n        ...         (1, \"two\"),\n        ...         (2, \"one\"),\n        ...         (2, \"two\"),\n        ...         (3, \"one\"),\n        ...         (3, \"two\"),\n        ...     ],\n        ...     names=[\"foo\", \"bar\"],\n        ... )\n        >>> idx\n        MultiIndex([(1, 'one'),\n            (1, 'two'),\n            (2, 'one'),\n            (2, 'two'),\n            (3, 'one'),\n            (3, 'two')],\n           names=['foo', 'bar'])\n\n        >>> idx.set_levels([[\"a\", \"b\", \"c\"], [1, 2]])\n        MultiIndex([('a', 1),\n                    ('a', 2),\n                    ('b', 1),\n                    ('b', 2),\n                    ('c', 1),\n                    ('c', 2)],\n                   names=['foo', 'bar'])\n        >>> idx.set_levels([\"a\", \"b\", \"c\"], level=0)\n        MultiIndex([('a', 'one'),\n                    ('a', 'two'),\n                    ('b', 'one'),\n                    ('b', 'two'),\n                    ('c', 'one'),\n                    ('c', 'two')],\n                   names=['foo', 'bar'])\n        >>> idx.set_levels([\"a\", \"b\"], level=\"bar\")\n        MultiIndex([(1, 'a'),\n                    (1, 'b'),\n                    (2, 'a'),\n                    (2, 'b'),\n                    (3, 'a'),\n                    (3, 'b')],\n                   names=['foo', 'bar'])\n\n        If any of the levels passed to ``set_levels()`` exceeds the\n        existing length, all of the values from that argument will\n        be stored in the MultiIndex levels, though the values will\n        be truncated in the MultiIndex output.\n\n        >>> idx.set_levels([[\"a\", \"b\", \"c\"], [1, 2, 3, 4]], level=[0, 1])\n        MultiIndex([('a', 1),\n            ('a', 2),\n            ('b', 1),\n            ('b', 2),\n            ('c', 1),\n            ('c', 2)],\n           names=['foo', 'bar'])\n        >>> idx.set_levels([[\"a\", \"b\", \"c\"], [1, 2, 3, 4]], level=[0, 1]).levels\n        FrozenList([['a', 'b', 'c'], [1, 2, 3, 4]])\n        \"\"\"\n\n        if isinstance(levels, Index):\n            pass\n        elif is_array_like(levels):\n            levels = Index(levels)\n        elif is_list_like(levels):\n            levels = list(levels)\n\n        level, levels = _require_listlike(level, levels, \"Levels\")\n        idx = self._view()\n        idx._reset_identity()\n        idx._set_levels(\n            levels, level=level, validate=True, verify_integrity=verify_integrity\n        )\n        return idx\n\n    @property\n    def nlevels(self) -> int:\n        \"\"\"\n        Integer number of levels in this MultiIndex.\n\n        See Also\n        --------\n        MultiIndex.levels : Get the levels of the MultiIndex.\n        MultiIndex.codes : Get the codes of the MultiIndex.\n        MultiIndex.from_arrays : Convert arrays to MultiIndex.\n        MultiIndex.from_tuples : Convert list of tuples to MultiIndex.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([[\"a\"], [\"b\"], [\"c\"]])\n        >>> mi\n        MultiIndex([('a', 'b', 'c')],\n                   )\n        >>> mi.nlevels\n        3\n        \"\"\"\n        return len(self._levels)\n\n    @property\n    def levshape(self) -> Shape:\n        \"\"\"\n        A tuple representing the length of each level in the MultiIndex.\n\n        In a `MultiIndex`, each level can contain multiple unique values. The\n        `levshape` property provides a quick way to assess the size of each\n        level by returning a tuple where each entry represents the number of\n        unique values in that specific level. This is particularly useful in\n        scenarios where you need to understand the structure and distribution\n        of your index levels, such as when working with multidimensional data.\n\n        See Also\n        --------\n        MultiIndex.shape : Return a tuple of the shape of the MultiIndex.\n        MultiIndex.levels : Returns the levels of the MultiIndex.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([[\"a\"], [\"b\"], [\"c\"]])\n        >>> mi\n        MultiIndex([('a', 'b', 'c')],\n                   )\n        >>> mi.levshape\n        (1, 1, 1)\n        \"\"\"\n        return tuple(len(x) for x in self.levels)\n\n    # --------------------------------------------------------------------\n    # Codes Methods\n\n    @property\n    def codes(self) -> FrozenList:\n        \"\"\"\n        Codes of the MultiIndex.\n\n        Codes are the position of the index value in the list of level values\n        for each level.\n\n        Returns\n        -------\n        tuple of numpy.ndarray\n            The codes of the MultiIndex. Each array in the tuple corresponds\n            to a level in the MultiIndex.\n\n        See Also\n        --------\n        MultiIndex.set_codes : Set new codes on MultiIndex.\n\n        Examples\n        --------\n        >>> arrays = [[1, 1, 2, 2], [\"red\", \"blue\", \"red\", \"blue\"]]\n        >>> mi = pd.MultiIndex.from_arrays(arrays, names=(\"number\", \"color\"))\n        >>> mi.codes\n        FrozenList([[0, 0, 1, 1], [1, 0, 1, 0]])\n        \"\"\"\n        return self._codes\n\n    def _set_codes(\n        self,\n        codes,\n        *,\n        level=None,\n        copy: bool = False,\n        validate: bool = True,\n        verify_integrity: bool = False,\n    ) -> None:\n        if validate:\n            if level is None and len(codes) != self.nlevels:\n                raise ValueError(\"Length of codes must match number of levels\")\n            if level is not None and len(codes) != len(level):\n                raise ValueError(\"Length of codes must match length of levels.\")\n\n        level_numbers: list[int] | range\n        if level is None:\n            new_codes = FrozenList(\n                _coerce_indexer_frozen(level_codes, lev, copy=copy).view()\n                for lev, level_codes in zip(self._levels, codes)\n            )\n            level_numbers = range(len(new_codes))\n        else:\n            level_numbers = [self._get_level_number(lev) for lev in level]\n            new_codes_list = list(self._codes)\n            for lev_num, level_codes in zip(level_numbers, codes):\n                lev = self.levels[lev_num]\n                new_codes_list[lev_num] = _coerce_indexer_frozen(\n                    level_codes, lev, copy=copy\n                )\n            new_codes = FrozenList(new_codes_list)\n\n        if verify_integrity:\n            new_codes = self._verify_integrity(\n                codes=new_codes, levels_to_verify=level_numbers\n            )\n\n        self._codes = new_codes\n\n        self._reset_cache()\n\n    def set_codes(\n        self, codes, *, level=None, verify_integrity: bool = True\n    ) -> MultiIndex:\n        \"\"\"\n        Set new codes on MultiIndex. Defaults to returning new index.\n\n        Parameters\n        ----------\n        codes : sequence or list of sequence\n            New codes to apply.\n        level : int, level name, or sequence of int/level names (default None)\n            Level(s) to set (None for all levels).\n        verify_integrity : bool, default True\n            If True, checks that levels and codes are compatible.\n\n        Returns\n        -------\n        new index (of same type and class...etc) or None\n            The same type as the caller or None if ``inplace=True``.\n\n        See Also\n        --------\n        MultiIndex.set_levels : Set new levels on MultiIndex.\n        MultiIndex.codes : Get the codes of the levels in the MultiIndex.\n        MultiIndex.levels : Get the levels of the MultiIndex.\n\n        Examples\n        --------\n        >>> idx = pd.MultiIndex.from_tuples(\n        ...     [(1, \"one\"), (1, \"two\"), (2, \"one\"), (2, \"two\")], names=[\"foo\", \"bar\"]\n        ... )\n        >>> idx\n        MultiIndex([(1, 'one'),\n            (1, 'two'),\n            (2, 'one'),\n            (2, 'two')],\n           names=['foo', 'bar'])\n\n        >>> idx.set_codes([[1, 0, 1, 0], [0, 0, 1, 1]])\n        MultiIndex([(2, 'one'),\n                    (1, 'one'),\n                    (2, 'two'),\n                    (1, 'two')],\n                   names=['foo', 'bar'])\n        >>> idx.set_codes([1, 0, 1, 0], level=0)\n        MultiIndex([(2, 'one'),\n                    (1, 'two'),\n                    (2, 'one'),\n                    (1, 'two')],\n                   names=['foo', 'bar'])\n        >>> idx.set_codes([0, 0, 1, 1], level=\"bar\")\n        MultiIndex([(1, 'one'),\n                    (1, 'one'),\n                    (2, 'two'),\n                    (2, 'two')],\n                   names=['foo', 'bar'])\n        >>> idx.set_codes([[1, 0, 1, 0], [0, 0, 1, 1]], level=[0, 1])\n        MultiIndex([(2, 'one'),\n                    (1, 'one'),\n                    (2, 'two'),\n                    (1, 'two')],\n                   names=['foo', 'bar'])\n        \"\"\"\n\n        level, codes = _require_listlike(level, codes, \"Codes\")\n        idx = self._view()\n        idx._reset_identity()\n        idx._set_codes(codes, level=level, verify_integrity=verify_integrity)\n        return idx\n\n    # --------------------------------------------------------------------\n    # Index Internals\n\n    @cache_readonly\n    def _engine(self):\n        # Calculate the number of bits needed to represent labels in each\n        # level, as log2 of their sizes:\n        # NaN values are shifted to 1 and missing values in other while\n        # calculating the indexer are shifted to 0\n        sizes = np.ceil(\n            np.log2(\n                [len(level) + libindex.multiindex_nulls_shift for level in self.levels]\n            )\n        )\n\n        # Sum bit counts, starting from the _right_....\n        lev_bits = np.cumsum(sizes[::-1])[::-1]\n\n        # ... in order to obtain offsets such that sorting the combination of\n        # shifted codes (one for each level, resulting in a unique integer) is\n        # equivalent to sorting lexicographically the codes themselves. Notice\n        # that each level needs to be shifted by the number of bits needed to\n        # represent the _previous_ ones:\n        offsets = np.concatenate([lev_bits[1:], [0]])\n        # Downcast the type if possible, to prevent upcasting when shifting codes:\n        offsets = offsets.astype(np.min_scalar_type(int(offsets[0])))\n\n        # Check the total number of bits needed for our representation:\n        if lev_bits[0] > 64:\n            # The levels would overflow a 64 bit uint - use Python integers:\n            return MultiIndexPyIntEngine(self.levels, self.codes, offsets)\n        if lev_bits[0] > 32:\n            # The levels would overflow a 32 bit uint - use uint64\n            return MultiIndexUInt64Engine(self.levels, self.codes, offsets)\n        if lev_bits[0] > 16:\n            # The levels would overflow a 16 bit uint - use uint8\n            return MultiIndexUInt32Engine(self.levels, self.codes, offsets)\n        if lev_bits[0] > 8:\n            # The levels would overflow a 8 bit uint - use uint16\n            return MultiIndexUInt16Engine(self.levels, self.codes, offsets)\n        # The levels fit in an 8 bit uint - use uint8\n        return MultiIndexUInt8Engine(self.levels, self.codes, offsets)\n\n    # Return type \"Callable[..., MultiIndex]\" of \"_constructor\" incompatible with return\n    # type \"Type[MultiIndex]\" in supertype \"Index\"\n    @property\n    def _constructor(self) -> Callable[..., MultiIndex]:  # type: ignore[override]\n        return type(self).from_tuples\n\n    @doc(Index._shallow_copy)\n    def _shallow_copy(self, values: np.ndarray, name=lib.no_default) -> MultiIndex:\n        names = name if name is not lib.no_default else self.names\n\n        return type(self).from_tuples(values, sortorder=None, names=names)\n\n    def _view(self) -> MultiIndex:\n        result = type(self)(\n            levels=self.levels,\n            codes=self.codes,\n            sortorder=self.sortorder,\n            names=self.names,\n            verify_integrity=False,\n        )\n        result._cache = self._cache.copy()\n        result._reset_cache(\"levels\")  # GH32669\n        return result\n\n    # --------------------------------------------------------------------\n\n    # error: Signature of \"copy\" incompatible with supertype \"Index\"\n    def copy(  # type: ignore[override]\n        self,\n        names=None,\n        deep: bool = False,\n        name=None,\n    ) -> Self:\n        \"\"\"\n        Make a copy of this object. Names, dtype, levels and codes can be passed and \\\n        will be set on new copy.\n\n        The `copy` method provides a mechanism to create a duplicate of an\n        existing MultiIndex object. This is particularly useful in scenarios where\n        modifications are required on an index, but the original MultiIndex should\n        remain unchanged. By specifying the `deep` parameter, users can control\n        whether the copy should be a deep or shallow copy, providing flexibility\n        depending on the size and complexity of the MultiIndex.\n\n        Parameters\n        ----------\n        names : sequence, optional\n            Names to set on the new MultiIndex object.\n        deep : bool, default False\n            If False, the new object will be a shallow copy. If True, a deep copy\n            will be attempted. Deep copying can be potentially expensive for large\n            MultiIndex objects.\n        name : Label\n            Kept for compatibility with 1-dimensional Index. Should not be used.\n\n        Returns\n        -------\n        MultiIndex\n            A new MultiIndex object with the specified modifications.\n\n        See Also\n        --------\n        MultiIndex.from_arrays : Convert arrays to MultiIndex.\n        MultiIndex.from_tuples : Convert list of tuples to MultiIndex.\n        MultiIndex.from_frame : Convert DataFrame to MultiIndex.\n\n        Notes\n        -----\n        In most cases, there should be no functional difference from using\n        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n        This could be potentially expensive on large MultiIndex objects.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([[\"a\"], [\"b\"], [\"c\"]])\n        >>> mi\n        MultiIndex([('a', 'b', 'c')],\n                   )\n        >>> mi.copy()\n        MultiIndex([('a', 'b', 'c')],\n                   )\n        \"\"\"\n        names = self._validate_names(name=name, names=names, deep=deep)\n        keep_id = not deep\n        levels, codes = None, None\n\n        if deep:\n            from copy import deepcopy\n\n            levels = deepcopy(self.levels)\n            codes = deepcopy(self.codes)\n\n        levels = levels if levels is not None else self.levels\n        codes = codes if codes is not None else self.codes\n\n        new_index = type(self)(\n            levels=levels,\n            codes=codes,\n            sortorder=self.sortorder,\n            names=names,\n            verify_integrity=False,\n        )\n        new_index._cache = self._cache.copy()\n        new_index._reset_cache(\"levels\")  # GH32669\n        if keep_id:\n            new_index._id = self._id\n        return new_index\n\n    def __array__(self, dtype=None, copy=None) -> np.ndarray:\n        \"\"\"the array interface, return my values\"\"\"\n        if copy is False:\n            # self.values is always a newly construct array, so raise.\n            raise ValueError(\n                \"Unable to avoid copy while creating an array as requested.\"\n            )\n        if copy is True:\n            # explicit np.array call to ensure a copy is made and unique objects\n            # are returned, because self.values is cached\n            return np.array(self.values, dtype=dtype)\n        return self.values\n\n    def view(self, cls=None) -> Self:\n        \"\"\"this is defined as a copy with the same identity\"\"\"\n        result = self.copy()\n        result._id = self._id\n        return result\n\n    @doc(Index.__contains__)\n    def __contains__(self, key: Any) -> bool:\n        hash(key)\n        try:\n            self.get_loc(key)\n            return True\n        except (LookupError, TypeError, ValueError):\n            return False\n\n    @cache_readonly\n    def dtype(self) -> np.dtype:\n        return np.dtype(\"O\")\n\n    @cache_readonly\n    def _is_memory_usage_qualified(self) -> bool:\n        \"\"\"return a boolean if we need a qualified .info display\"\"\"\n\n        def f(dtype) -> bool:\n            return is_object_dtype(dtype) or (\n                is_string_dtype(dtype) and dtype.storage == \"python\"\n            )\n\n        return any(f(level.dtype) for level in self.levels)\n\n    # Cannot determine type of \"memory_usage\"\n    @doc(Index.memory_usage)  # type: ignore[has-type]\n    def memory_usage(self, deep: bool = False) -> int:\n        # we are overwriting our base class to avoid\n        # computing .values here which could materialize\n        # a tuple representation unnecessarily\n        return self._nbytes(deep)\n\n    @cache_readonly\n    def nbytes(self) -> int:\n        \"\"\"return the number of bytes in the underlying data\"\"\"\n        return self._nbytes(False)\n\n    def _nbytes(self, deep: bool = False) -> int:\n        \"\"\"\n        return the number of bytes in the underlying data\n        deeply introspect the level data if deep=True\n\n        include the engine hashtable\n\n        *this is in internal routine*\n\n        \"\"\"\n        # for implementations with no useful getsizeof (PyPy)\n        objsize = 24\n\n        level_nbytes = sum(i.memory_usage(deep=deep) for i in self.levels)\n        label_nbytes = sum(i.nbytes for i in self.codes)\n        names_nbytes = sum(getsizeof(i, objsize) for i in self.names)\n        result = level_nbytes + label_nbytes + names_nbytes\n\n        # include our engine hashtable, only if it's already cached\n        if \"_engine\" in self._cache:\n            result += self._engine.sizeof(deep=deep)\n        return result\n\n    # --------------------------------------------------------------------\n    # Rendering Methods\n\n    def _formatter_func(self, tup):\n        \"\"\"\n        Formats each item in tup according to its level's formatter function.\n        \"\"\"\n        formatter_funcs = (level._formatter_func for level in self.levels)\n        return tuple(func(val) for func, val in zip(formatter_funcs, tup))\n\n    def _get_values_for_csv(\n        self, *, na_rep: str = \"nan\", **kwargs\n    ) -> npt.NDArray[np.object_]:\n        new_levels = []\n        new_codes = []\n\n        # go through the levels and format them\n        for level, level_codes in zip(self.levels, self.codes):\n            level_strs = level._get_values_for_csv(na_rep=na_rep, **kwargs)\n            # add nan values, if there are any\n            mask = level_codes == -1\n            if mask.any():\n                nan_index = len(level_strs)\n                # numpy 1.21 deprecated implicit string casting\n                level_strs = level_strs.astype(str)\n                level_strs = np.append(level_strs, na_rep)\n                assert not level_codes.flags.writeable  # i.e. copy is needed\n                level_codes = level_codes.copy()  # make writeable\n                level_codes[mask] = nan_index\n            new_levels.append(level_strs)\n            new_codes.append(level_codes)\n\n        if len(new_levels) == 1:\n            # a single-level multi-index\n            return Index(new_levels[0].take(new_codes[0]))._get_values_for_csv()\n        else:\n            # reconstruct the multi-index\n            mi = MultiIndex(\n                levels=new_levels,\n                codes=new_codes,\n                names=self.names,\n                sortorder=self.sortorder,\n                verify_integrity=False,\n            )\n            return mi._values\n\n    def _format_multi(\n        self,\n        *,\n        include_names: bool,\n        sparsify: bool | None | lib.NoDefault,\n        formatter: Callable | None = None,\n    ) -> list:\n        if len(self) == 0:\n            return []\n\n        stringified_levels = []\n        for lev, level_codes in zip(self.levels, self.codes):\n            na = _get_na_rep(lev.dtype)\n\n            if len(lev) > 0:\n                taken = formatted = lev.take(level_codes)\n                formatted = taken._format_flat(include_name=False, formatter=formatter)\n\n                # we have some NA\n                mask = level_codes == -1\n                if mask.any():\n                    formatted = np.array(formatted, dtype=object)\n                    formatted[mask] = na\n                    formatted = formatted.tolist()\n\n            else:\n                # weird all NA case\n                formatted = [\n                    pprint_thing(na if isna(x) else x, escape_chars=(\"\\t\", \"\\r\", \"\\n\"))\n                    for x in algos.take_nd(lev._values, level_codes)\n                ]\n            stringified_levels.append(formatted)\n\n        result_levels = []\n        for lev, lev_name in zip(stringified_levels, self.names):\n            level = []\n\n            if include_names:\n                level.append(\n                    pprint_thing(lev_name, escape_chars=(\"\\t\", \"\\r\", \"\\n\"))\n                    if lev_name is not None\n                    else \"\"\n                )\n\n            level.extend(np.array(lev, dtype=object))\n            result_levels.append(level)\n\n        if sparsify is None:\n            sparsify = get_option(\"display.multi_sparse\")\n\n        if sparsify:\n            sentinel: Literal[\"\"] | bool | lib.NoDefault = \"\"\n            # GH3547 use value of sparsify as sentinel if it's \"Falsey\"\n            assert isinstance(sparsify, bool) or sparsify is lib.no_default\n            if sparsify is lib.no_default:\n                sentinel = sparsify\n            # little bit of a kludge job for #1217\n            result_levels = sparsify_labels(\n                result_levels, start=int(include_names), sentinel=sentinel\n            )\n\n        return result_levels\n\n    # --------------------------------------------------------------------\n    # Names Methods\n\n    def _get_names(self) -> FrozenList:\n        return FrozenList(self._names)\n\n    def _set_names(self, names, *, level=None) -> None:\n        \"\"\"\n        Set new names on index. Each name has to be a hashable type.\n\n        Parameters\n        ----------\n        values : str or sequence\n            name(s) to set\n        level : int, level name, or sequence of int/level names (default None)\n            If the index is a MultiIndex (hierarchical), level(s) to set (None\n            for all levels).  Otherwise level must be None\n\n        Raises\n        ------\n        TypeError if each name is not hashable.\n\n        Notes\n        -----\n        sets names on levels. WARNING: mutates!\n\n        Note that you generally want to set this *after* changing levels, so\n        that it only acts on copies\n        \"\"\"\n        # GH 15110\n        # Don't allow a single string for names in a MultiIndex\n        if names is not None and not is_list_like(names):\n            raise ValueError(\"Names should be list-like for a MultiIndex\")\n        names = list(names)\n\n        if level is not None and len(names) != len(level):\n            raise ValueError(\"Length of names must match length of level.\")\n        if level is None and len(names) != self.nlevels:\n            raise ValueError(\n                \"Length of names must match number of levels in MultiIndex.\"\n            )\n\n        if level is None:\n            level = range(self.nlevels)\n        else:\n            level = (self._get_level_number(lev) for lev in level)\n\n        # set the name\n        for lev, name in zip(level, names):\n            if name is not None:\n                # GH 20527\n                # All items in 'names' need to be hashable:\n                if not is_hashable(name):\n                    raise TypeError(\n                        f\"{type(self).__name__}.name must be a hashable type\"\n                    )\n            self._names[lev] = name\n\n        # If .levels has been accessed, the .name of each level in our cache\n        # will be stale.\n        self._reset_cache(\"levels\")\n\n    names = property(\n        fset=_set_names,\n        fget=_get_names,\n        doc=\"\"\"\n        Names of levels in MultiIndex.\n\n        This attribute provides access to the names of the levels in a `MultiIndex`.\n        The names are stored as a `FrozenList`, which is an immutable list-like\n        container. Each name corresponds to a level in the `MultiIndex`, and can be\n        used to identify or manipulate the levels individually.\n\n        See Also\n        --------\n        MultiIndex.set_names : Set Index or MultiIndex name.\n        MultiIndex.rename : Rename specific levels in a MultiIndex.\n        Index.names : Get names on index.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays(\n        ...     [[1, 2], [3, 4], [5, 6]], names=['x', 'y', 'z']\n        ... )\n        >>> mi\n        MultiIndex([(1, 3, 5),\n                    (2, 4, 6)],\n                   names=['x', 'y', 'z'])\n        >>> mi.names\n        FrozenList(['x', 'y', 'z'])\n        \"\"\",\n    )\n\n    # --------------------------------------------------------------------\n\n    @cache_readonly\n    def inferred_type(self) -> str:\n        return \"mixed\"\n\n    def _get_level_number(self, level) -> int:\n        count = self.names.count(level)\n        if (count > 1) and not is_integer(level):\n            raise ValueError(\n                f\"The name {level} occurs multiple times, use a level number\"\n            )\n        try:\n            level = self.names.index(level)\n        except ValueError as err:\n            if not is_integer(level):\n                raise KeyError(f\"Level {level} not found\") from err\n            if level < 0:\n                level += self.nlevels\n                if level < 0:\n                    orig_level = level - self.nlevels\n                    raise IndexError(\n                        f\"Too many levels: Index has only {self.nlevels} levels, \"\n                        f\"{orig_level} is not a valid level number\"\n                    ) from err\n            # Note: levels are zero-based\n            elif level >= self.nlevels:\n                raise IndexError(\n                    f\"Too many levels: Index has only {self.nlevels} levels, \"\n                    f\"not {level + 1}\"\n                ) from err\n        return level\n\n    @cache_readonly\n    def is_monotonic_increasing(self) -> bool:\n        \"\"\"\n        Return a boolean if the values are equal or increasing.\n        \"\"\"\n        if any(-1 in code for code in self.codes):\n            return False\n\n        if all(level.is_monotonic_increasing for level in self.levels):\n            # If each level is sorted, we can operate on the codes directly. GH27495\n            return libalgos.is_lexsorted(\n                [x.astype(\"int64\", copy=False) for x in self.codes]\n            )\n\n        # reversed() because lexsort() wants the most significant key last.\n        values = [\n            self._get_level_values(i)._values for i in reversed(range(len(self.levels)))\n        ]\n        try:\n            # error: Argument 1 to \"lexsort\" has incompatible type\n            # \"List[Union[ExtensionArray, ndarray[Any, Any]]]\";\n            # expected \"Union[_SupportsArray[dtype[Any]],\n            # _NestedSequence[_SupportsArray[dtype[Any]]], bool,\n            # int, float, complex, str, bytes, _NestedSequence[Union\n            # [bool, int, float, complex, str, bytes]]]\"\n            sort_order = np.lexsort(values)  # type: ignore[arg-type]\n            return Index(sort_order).is_monotonic_increasing\n        except TypeError:\n            # we have mixed types and np.lexsort is not happy\n            return Index(self._values).is_monotonic_increasing\n\n    @cache_readonly\n    def is_monotonic_decreasing(self) -> bool:\n        \"\"\"\n        Return a boolean if the values are equal or decreasing.\n        \"\"\"\n        # monotonic decreasing if and only if reverse is monotonic increasing\n        return self[::-1].is_monotonic_increasing\n\n    @doc(Index.duplicated)\n    def duplicated(self, keep: DropKeep = \"first\") -> npt.NDArray[np.bool_]:\n        shape = tuple(len(lev) for lev in self.levels)\n        ids = get_group_index(self.codes, shape, sort=False, xnull=False)\n\n        return duplicated(ids, keep)\n\n    # error: Cannot override final attribute \"_duplicated\"\n    # (previously declared in base class \"IndexOpsMixin\")\n    _duplicated = duplicated  # type: ignore[misc]\n\n    def fillna(self, value):\n        \"\"\"\n        fillna is not implemented for MultiIndex\n        \"\"\"\n        raise NotImplementedError(\"fillna is not defined for MultiIndex\")\n\n    @doc(Index.dropna)\n    def dropna(self, how: AnyAll = \"any\") -> MultiIndex:\n        nans = [level_codes == -1 for level_codes in self.codes]\n        if how == \"any\":\n            indexer = np.any(nans, axis=0)\n        elif how == \"all\":\n            indexer = np.all(nans, axis=0)\n        else:\n            raise ValueError(f\"invalid how option: {how}\")\n\n        new_codes = [level_codes[~indexer] for level_codes in self.codes]\n        return self.set_codes(codes=new_codes)\n\n    def _get_level_values(self, level: int, unique: bool = False) -> Index:\n        \"\"\"\n        Return vector of label values for requested level,\n        equal to the length of the index\n\n        **this is an internal method**\n\n        Parameters\n        ----------\n        level : int\n        unique : bool, default False\n            if True, drop duplicated values\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        lev = self.levels[level]\n        level_codes = self.codes[level]\n        name = self._names[level]\n        if unique:\n            level_codes = algos.unique(level_codes)\n        filled = algos.take_nd(lev._values, level_codes, fill_value=lev._na_value)\n        return lev._shallow_copy(filled, name=name)\n\n    # error: Signature of \"get_level_values\" incompatible with supertype \"Index\"\n    def get_level_values(self, level) -> Index:  # type: ignore[override]\n        \"\"\"\n        Return vector of label values for requested level.\n\n        Length of returned vector is equal to the length of the index.\n        The `get_level_values` method is a crucial utility for extracting\n        specific level values from a `MultiIndex`. This function is particularly\n        useful when working with multi-level data, allowing you to isolate\n        and manipulate individual levels without having to deal with the\n        complexity of the entire `MultiIndex` structure. It seamlessly handles\n        both integer and string-based level access, providing flexibility in\n        how you can interact with the data. Additionally, this method ensures\n        that the returned `Index` maintains the integrity of the original data,\n        even when missing values are present, by appropriately casting the\n        result to a suitable data type.\n\n        Parameters\n        ----------\n        level : int or str\n            ``level`` is either the integer position of the level in the\n            MultiIndex, or the name of the level.\n\n        Returns\n        -------\n        Index\n            Values is a level of this MultiIndex converted to\n            a single :class:`Index` (or subclass thereof).\n\n        See Also\n        --------\n        MultiIndex : A multi-level, or hierarchical, index object for pandas objects.\n        Index : Immutable sequence used for indexing and alignment.\n        MultiIndex.remove_unused_levels : Create new MultiIndex from current that\n            removes unused levels.\n\n        Notes\n        -----\n        If the level contains missing values, the result may be casted to\n        ``float`` with missing values specified as ``NaN``. This is because\n        the level is converted to a regular ``Index``.\n\n        Examples\n        --------\n        Create a MultiIndex:\n\n        >>> mi = pd.MultiIndex.from_arrays((list(\"abc\"), list(\"def\")))\n        >>> mi.names = [\"level_1\", \"level_2\"]\n\n        Get level values by supplying level as either integer or name:\n\n        >>> mi.get_level_values(0)\n        Index(['a', 'b', 'c'], dtype='object', name='level_1')\n        >>> mi.get_level_values(\"level_2\")\n        Index(['d', 'e', 'f'], dtype='object', name='level_2')\n\n        If a level contains missing values, the return type of the level\n        may be cast to ``float``.\n\n        >>> pd.MultiIndex.from_arrays([[1, None, 2], [3, 4, 5]]).dtypes\n        level_0    int64\n        level_1    int64\n        dtype: object\n        >>> pd.MultiIndex.from_arrays([[1, None, 2], [3, 4, 5]]).get_level_values(0)\n        Index([1.0, nan, 2.0], dtype='float64')\n        \"\"\"\n        level = self._get_level_number(level)\n        values = self._get_level_values(level)\n        return values\n\n    @doc(Index.unique)\n    def unique(self, level=None):\n        if level is None:\n            return self.drop_duplicates()\n        else:\n            level = self._get_level_number(level)\n            return self._get_level_values(level=level, unique=True)\n\n    def to_frame(\n        self,\n        index: bool = True,\n        name=lib.no_default,\n        allow_duplicates: bool = False,\n    ) -> DataFrame:\n        \"\"\"\n        Create a DataFrame with the levels of the MultiIndex as columns.\n\n        Column ordering is determined by the DataFrame constructor with data as\n        a dict.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Set the index of the returned DataFrame as the original MultiIndex.\n\n        name : list / sequence of str, optional\n            The passed names should substitute index level names.\n\n        allow_duplicates : bool, optional default False\n            Allow duplicate column labels to be created.\n\n            .. versionadded:: 1.5.0\n\n        Returns\n        -------\n        DataFrame\n            DataFrame representation of the MultiIndex, with levels as columns.\n\n        See Also\n        --------\n        DataFrame : Two-dimensional, size-mutable, potentially heterogeneous\n            tabular data.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([[\"a\", \"b\"], [\"c\", \"d\"]])\n        >>> mi\n        MultiIndex([('a', 'c'),\n                    ('b', 'd')],\n                   )\n\n        >>> df = mi.to_frame()\n        >>> df\n             0  1\n        a c  a  c\n        b d  b  d\n\n        >>> df = mi.to_frame(index=False)\n        >>> df\n           0  1\n        0  a  c\n        1  b  d\n\n        >>> df = mi.to_frame(name=[\"x\", \"y\"])\n        >>> df\n             x  y\n        a c  a  c\n        b d  b  d\n        \"\"\"\n        from pandas import DataFrame\n\n        if name is not lib.no_default:\n            if not is_list_like(name):\n                raise TypeError(\"'name' must be a list / sequence of column names.\")\n\n            if len(name) != len(self.levels):\n                raise ValueError(\n                    \"'name' should have same length as number of levels on index.\"\n                )\n            idx_names = name\n        else:\n            idx_names = self._get_level_names()\n\n        if not allow_duplicates and len(set(idx_names)) != len(idx_names):\n            raise ValueError(\n                \"Cannot create duplicate column labels if allow_duplicates is False\"\n            )\n\n        # Guarantee resulting column order - PY36+ dict maintains insertion order\n        result = DataFrame(\n            {level: self._get_level_values(level) for level in range(len(self.levels))},\n            copy=False,\n        )\n        result.columns = idx_names\n\n        if index:\n            result.index = self\n        return result\n\n    # error: Return type \"Index\" of \"to_flat_index\" incompatible with return type\n    # \"MultiIndex\" in supertype \"Index\"\n    def to_flat_index(self) -> Index:  # type: ignore[override]\n        \"\"\"\n        Convert a MultiIndex to an Index of Tuples containing the level values.\n\n        Returns\n        -------\n        pd.Index\n            Index with the MultiIndex data represented in Tuples.\n\n        See Also\n        --------\n        MultiIndex.from_tuples : Convert flat index back to MultiIndex.\n\n        Notes\n        -----\n        This method will simply return the caller if called by anything other\n        than a MultiIndex.\n\n        Examples\n        --------\n        >>> index = pd.MultiIndex.from_product(\n        ...     [[\"foo\", \"bar\"], [\"baz\", \"qux\"]], names=[\"a\", \"b\"]\n        ... )\n        >>> index.to_flat_index()\n        Index([('foo', 'baz'), ('foo', 'qux'),\n               ('bar', 'baz'), ('bar', 'qux')],\n              dtype='object')\n        \"\"\"\n        return Index(self._values, tupleize_cols=False)\n\n    def _is_lexsorted(self) -> bool:\n        \"\"\"\n        Return True if the codes are lexicographically sorted.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        In the below examples, the first level of the MultiIndex is sorted because\n        a<b<c, so there is no need to look at the next level.\n\n        >>> pd.MultiIndex.from_arrays(\n        ...     [[\"a\", \"b\", \"c\"], [\"d\", \"e\", \"f\"]]\n        ... )._is_lexsorted()\n        True\n        >>> pd.MultiIndex.from_arrays(\n        ...     [[\"a\", \"b\", \"c\"], [\"d\", \"f\", \"e\"]]\n        ... )._is_lexsorted()\n        True\n\n        In case there is a tie, the lexicographical sorting looks\n        at the next level of the MultiIndex.\n\n        >>> pd.MultiIndex.from_arrays([[0, 1, 1], [\"a\", \"b\", \"c\"]])._is_lexsorted()\n        True\n        >>> pd.MultiIndex.from_arrays([[0, 1, 1], [\"a\", \"c\", \"b\"]])._is_lexsorted()\n        False\n        >>> pd.MultiIndex.from_arrays(\n        ...     [[\"a\", \"a\", \"b\", \"b\"], [\"aa\", \"bb\", \"aa\", \"bb\"]]\n        ... )._is_lexsorted()\n        True\n        >>> pd.MultiIndex.from_arrays(\n        ...     [[\"a\", \"a\", \"b\", \"b\"], [\"bb\", \"aa\", \"aa\", \"bb\"]]\n        ... )._is_lexsorted()\n        False\n        \"\"\"\n        return self._lexsort_depth == self.nlevels\n\n    @cache_readonly\n    def _lexsort_depth(self) -> int:\n        \"\"\"\n        Compute and return the lexsort_depth, the number of levels of the\n        MultiIndex that are sorted lexically\n\n        Returns\n        -------\n        int\n        \"\"\"\n        if self.sortorder is not None:\n            return self.sortorder\n        return _lexsort_depth(self.codes, self.nlevels)\n\n    def _sort_levels_monotonic(self, raise_if_incomparable: bool = False) -> MultiIndex:\n        \"\"\"\n        This is an *internal* function.\n\n        Create a new MultiIndex from the current to monotonically sorted\n        items IN the levels. This does not actually make the entire MultiIndex\n        monotonic, JUST the levels.\n\n        The resulting MultiIndex will have the same outward\n        appearance, meaning the same .values and ordering. It will also\n        be .equals() to the original.\n\n        Returns\n        -------\n        MultiIndex\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex(\n        ...     levels=[[\"a\", \"b\"], [\"bb\", \"aa\"]], codes=[[0, 0, 1, 1], [0, 1, 0, 1]]\n        ... )\n        >>> mi\n        MultiIndex([('a', 'bb'),\n                    ('a', 'aa'),\n                    ('b', 'bb'),\n                    ('b', 'aa')],\n                   )\n\n        >>> mi.sort_values()\n        MultiIndex([('a', 'aa'),\n                    ('a', 'bb'),\n                    ('b', 'aa'),\n                    ('b', 'bb')],\n                   )\n        \"\"\"\n        if self._is_lexsorted() and self.is_monotonic_increasing:\n            return self\n\n        new_levels = []\n        new_codes = []\n\n        for lev, level_codes in zip(self.levels, self.codes):\n            if not lev.is_monotonic_increasing:\n                try:\n                    # indexer to reorder the levels\n                    indexer = lev.argsort()\n                except TypeError:\n                    if raise_if_incomparable:\n                        raise\n                else:\n                    lev = lev.take(indexer)\n\n                    # indexer to reorder the level codes\n                    indexer = ensure_platform_int(indexer)\n                    ri = lib.get_reverse_indexer(indexer, len(indexer))\n                    level_codes = algos.take_nd(ri, level_codes, fill_value=-1)\n\n            new_levels.append(lev)\n            new_codes.append(level_codes)\n\n        return MultiIndex(\n            new_levels,\n            new_codes,\n            names=self.names,\n            sortorder=self.sortorder,\n            verify_integrity=False,\n        )\n\n    def remove_unused_levels(self) -> MultiIndex:\n        \"\"\"\n        Create new MultiIndex from current that removes unused levels.\n\n        Unused level(s) means levels that are not expressed in the\n        labels. The resulting MultiIndex will have the same outward\n        appearance, meaning the same .values and ordering. It will\n        also be .equals() to the original.\n\n        The `remove_unused_levels` method is useful in cases where you have a\n        MultiIndex with hierarchical levels, but some of these levels are no\n        longer needed due to filtering or subsetting operations. By removing\n        the unused levels, the resulting MultiIndex becomes more compact and\n        efficient, which can improve performance in subsequent operations.\n\n        Returns\n        -------\n        MultiIndex\n            A new MultiIndex with unused levels removed.\n\n        See Also\n        --------\n        MultiIndex.droplevel : Remove specified levels from a MultiIndex.\n        MultiIndex.reorder_levels : Rearrange levels of a MultiIndex.\n        MultiIndex.set_levels : Set new levels on a MultiIndex.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_product([range(2), list(\"ab\")])\n        >>> mi\n        MultiIndex([(0, 'a'),\n                    (0, 'b'),\n                    (1, 'a'),\n                    (1, 'b')],\n                   )\n\n        >>> mi[2:]\n        MultiIndex([(1, 'a'),\n                    (1, 'b')],\n                   )\n\n        The 0 from the first level is not represented\n        and can be removed\n\n        >>> mi2 = mi[2:].remove_unused_levels()\n        >>> mi2.levels\n        FrozenList([[1], ['a', 'b']])\n        \"\"\"\n        new_levels = []\n        new_codes = []\n\n        changed = False\n        for lev, level_codes in zip(self.levels, self.codes):\n            # Since few levels are typically unused, bincount() is more\n            # efficient than unique() - however it only accepts positive values\n            # (and drops order):\n            uniques = np.where(np.bincount(level_codes + 1) > 0)[0] - 1\n            has_na = int(len(uniques) and (uniques[0] == -1))\n\n            if len(uniques) != len(lev) + has_na:\n                if lev.isna().any() and len(uniques) == len(lev):\n                    break\n                # We have unused levels\n                changed = True\n\n                # Recalculate uniques, now preserving order.\n                # Can easily be cythonized by exploiting the already existing\n                # \"uniques\" and stop parsing \"level_codes\" when all items\n                # are found:\n                uniques = algos.unique(level_codes)\n                if has_na:\n                    na_idx = np.where(uniques == -1)[0]\n                    # Just ensure that -1 is in first position:\n                    uniques[[0, na_idx[0]]] = uniques[[na_idx[0], 0]]\n\n                # codes get mapped from uniques to 0:len(uniques)\n                # -1 (if present) is mapped to last position\n                code_mapping = np.zeros(len(lev) + has_na)\n                # ... and reassigned value -1:\n                code_mapping[uniques] = np.arange(len(uniques)) - has_na\n\n                level_codes = code_mapping[level_codes]\n\n                # new levels are simple\n                lev = lev.take(uniques[has_na:])\n\n            new_levels.append(lev)\n            new_codes.append(level_codes)\n\n        result = self.view()\n\n        if changed:\n            result._reset_identity()\n            result._set_levels(new_levels, validate=False)\n            result._set_codes(new_codes, validate=False)\n\n        return result\n\n    # --------------------------------------------------------------------\n    # Pickling Methods\n\n    def __reduce__(self):\n        \"\"\"Necessary for making this object picklable\"\"\"\n        d = {\n            \"levels\": list(self.levels),\n            \"codes\": list(self.codes),\n            \"sortorder\": self.sortorder,\n            \"names\": list(self.names),\n        }\n        return ibase._new_Index, (type(self), d), None\n\n    # --------------------------------------------------------------------\n\n    def __getitem__(self, key):\n        if is_scalar(key):\n            key = com.cast_scalar_indexer(key)\n\n            retval = []\n            for lev, level_codes in zip(self.levels, self.codes):\n                if level_codes[key] == -1:\n                    retval.append(np.nan)\n                else:\n                    retval.append(lev[level_codes[key]])\n\n            return tuple(retval)\n        else:\n            # in general cannot be sure whether the result will be sorted\n            sortorder = None\n            if com.is_bool_indexer(key):\n                key = np.asarray(key, dtype=bool)\n                sortorder = self.sortorder\n            elif isinstance(key, slice):\n                if key.step is None or key.step > 0:\n                    sortorder = self.sortorder\n            elif isinstance(key, Index):\n                key = np.asarray(key)\n\n            new_codes = [level_codes[key] for level_codes in self.codes]\n\n            return MultiIndex(\n                levels=self.levels,\n                codes=new_codes,\n                names=self.names,\n                sortorder=sortorder,\n                verify_integrity=False,\n            )\n\n    def _getitem_slice(self: MultiIndex, slobj: slice) -> MultiIndex:\n        \"\"\"\n        Fastpath for __getitem__ when we know we have a slice.\n        \"\"\"\n        sortorder = None\n        if slobj.step is None or slobj.step > 0:\n            sortorder = self.sortorder\n\n        new_codes = [level_codes[slobj] for level_codes in self.codes]\n\n        return type(self)(\n            levels=self.levels,\n            codes=new_codes,\n            names=self._names,\n            sortorder=sortorder,\n            verify_integrity=False,\n        )\n\n    @Appender(_index_shared_docs[\"take\"] % _index_doc_kwargs)\n    def take(\n        self: MultiIndex,\n        indices,\n        axis: Axis = 0,\n        allow_fill: bool = True,\n        fill_value=None,\n        **kwargs,\n    ) -> MultiIndex:\n        nv.validate_take((), kwargs)\n        indices = ensure_platform_int(indices)\n\n        # only fill if we are passing a non-None fill_value\n        allow_fill = self._maybe_disallow_fill(allow_fill, fill_value, indices)\n\n        if indices.ndim == 1 and lib.is_range_indexer(indices, len(self)):\n            return self.copy()\n\n        na_value = -1\n\n        taken = [lab.take(indices) for lab in self.codes]\n        if allow_fill:\n            mask = indices == -1\n            if mask.any():\n                masked = []\n                for new_label in taken:\n                    label_values = new_label\n                    label_values[mask] = na_value\n                    masked.append(np.asarray(label_values))\n                taken = masked\n\n        return MultiIndex(\n            levels=self.levels, codes=taken, names=self.names, verify_integrity=False\n        )\n\n    def append(self, other):\n        \"\"\"\n        Append a collection of Index options together.\n\n        The `append` method is used to combine multiple `Index` objects into a single\n        `Index`. This is particularly useful when dealing with multi-level indexing\n        (MultiIndex) where you might need to concatenate different levels of indices.\n        The method handles the alignment of the levels and codes of the indices being\n        appended to ensure consistency in the resulting `MultiIndex`.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n            Index or list/tuple of Index objects to be appended.\n\n        Returns\n        -------\n        Index\n            The combined index.\n\n        See Also\n        --------\n        MultiIndex: A multi-level, or hierarchical, index object for pandas objects.\n        Index.append : Append a collection of Index options together.\n        concat : Concatenate pandas objects along a particular axis.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([[\"a\"], [\"b\"]])\n        >>> mi\n        MultiIndex([('a', 'b')],\n                   )\n        >>> mi.append(mi)\n        MultiIndex([('a', 'b'), ('a', 'b')],\n                   )\n        \"\"\"\n        if not isinstance(other, (list, tuple)):\n            other = [other]\n\n        if all(\n            (isinstance(o, MultiIndex) and o.nlevels >= self.nlevels) for o in other\n        ):\n            codes = []\n            levels = []\n            names = []\n            for i in range(self.nlevels):\n                level_values = self.levels[i]\n                for mi in other:\n                    level_values = level_values.union(mi.levels[i])\n                level_codes = [\n                    recode_for_categories(\n                        mi.codes[i], mi.levels[i], level_values, copy=False\n                    )\n                    for mi in ([self, *other])\n                ]\n                level_name = self.names[i]\n                if any(mi.names[i] != level_name for mi in other):\n                    level_name = None\n                codes.append(np.concatenate(level_codes))\n                levels.append(level_values)\n                names.append(level_name)\n            return MultiIndex(\n                codes=codes, levels=levels, names=names, verify_integrity=False\n            )\n\n        to_concat = (self._values,) + tuple(k._values for k in other)\n        new_tuples = np.concatenate(to_concat)\n\n        # if all(isinstance(x, MultiIndex) for x in other):\n        try:\n            # We only get here if other contains at least one index with tuples,\n            # setting names to None automatically\n            return MultiIndex.from_tuples(new_tuples)\n        except (TypeError, IndexError):\n            return Index(new_tuples)\n\n    def argsort(\n        self, *args, na_position: str = \"last\", **kwargs\n    ) -> npt.NDArray[np.intp]:\n        target = self._sort_levels_monotonic(raise_if_incomparable=True)\n        keys = [lev.codes for lev in target._get_codes_for_sorting()]\n        return lexsort_indexer(keys, na_position=na_position, codes_given=True)\n\n    @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n    def repeat(self, repeats: int, axis=None) -> MultiIndex:\n        nv.validate_repeat((), {\"axis\": axis})\n        # error: Incompatible types in assignment (expression has type \"ndarray\",\n        # variable has type \"int\")\n        repeats = ensure_platform_int(repeats)  # type: ignore[assignment]\n        return MultiIndex(\n            levels=self.levels,\n            codes=[\n                level_codes.view(np.ndarray).astype(np.intp, copy=False).repeat(repeats)\n                for level_codes in self.codes\n            ],\n            names=self.names,\n            sortorder=self.sortorder,\n            verify_integrity=False,\n        )\n\n    # error: Signature of \"drop\" incompatible with supertype \"Index\"\n    def drop(  # type: ignore[override]\n        self,\n        codes,\n        level: Index | np.ndarray | Iterable[Hashable] | None = None,\n        errors: IgnoreRaise = \"raise\",\n    ) -> MultiIndex:\n        \"\"\"\n        Make a new :class:`pandas.MultiIndex` with the passed list of codes deleted.\n\n        This method allows for the removal of specified labels from a MultiIndex.\n        The labels to be removed can be provided as a list of tuples if no level\n        is specified, or as a list of labels from a specific level if the level\n        parameter is provided. This can be useful for refining the structure of a\n        MultiIndex to fit specific requirements.\n\n        Parameters\n        ----------\n        codes : array-like\n            Must be a list of tuples when ``level`` is not specified.\n        level : int or level name, default None\n            Level from which the labels will be dropped.\n        errors : str, default 'raise'\n            If 'ignore', suppress error and existing labels are dropped.\n\n        Returns\n        -------\n        MultiIndex\n            A new MultiIndex with the specified labels removed.\n\n        See Also\n        --------\n        MultiIndex.remove_unused_levels : Create new MultiIndex from current that\n            removes unused levels.\n        MultiIndex.reorder_levels : Rearrange levels using input order.\n        MultiIndex.rename : Rename levels in a MultiIndex.\n\n        Examples\n        --------\n        >>> idx = pd.MultiIndex.from_product(\n        ...     [(0, 1, 2), (\"green\", \"purple\")], names=[\"number\", \"color\"]\n        ... )\n        >>> idx\n        MultiIndex([(0,  'green'),\n                    (0, 'purple'),\n                    (1,  'green'),\n                    (1, 'purple'),\n                    (2,  'green'),\n                    (2, 'purple')],\n                   names=['number', 'color'])\n        >>> idx.drop([(1, \"green\"), (2, \"purple\")])\n        MultiIndex([(0,  'green'),\n                    (0, 'purple'),\n                    (1, 'purple'),\n                    (2,  'green')],\n                   names=['number', 'color'])\n\n        We can also drop from a specific level.\n\n        >>> idx.drop(\"green\", level=\"color\")\n        MultiIndex([(0, 'purple'),\n                    (1, 'purple'),\n                    (2, 'purple')],\n                   names=['number', 'color'])\n\n        >>> idx.drop([1, 2], level=0)\n        MultiIndex([(0,  'green'),\n                    (0, 'purple')],\n                   names=['number', 'color'])\n        \"\"\"\n        if level is not None:\n            return self._drop_from_level(codes, level, errors)\n\n        if not isinstance(codes, (np.ndarray, Index)):\n            try:\n                codes = com.index_labels_to_array(codes, dtype=np.dtype(\"object\"))\n            except ValueError:\n                pass\n\n        inds = []\n        for level_codes in codes:\n            try:\n                loc = self.get_loc(level_codes)\n                # get_loc returns either an integer, a slice, or a boolean\n                # mask\n                if isinstance(loc, int):\n                    inds.append(loc)\n                elif isinstance(loc, slice):\n                    step = loc.step if loc.step is not None else 1\n                    inds.extend(range(loc.start, loc.stop, step))\n                elif com.is_bool_indexer(loc):\n                    if get_option(\"performance_warnings\") and self._lexsort_depth == 0:\n                        warnings.warn(\n                            \"dropping on a non-lexsorted multi-index \"\n                            \"without a level parameter may impact performance.\",\n                            PerformanceWarning,\n                            stacklevel=find_stack_level(),\n                        )\n                    loc = loc.nonzero()[0]\n                    inds.extend(loc)\n                else:\n                    msg = f\"unsupported indexer of type {type(loc)}\"\n                    raise AssertionError(msg)\n            except KeyError:\n                if errors != \"ignore\":\n                    raise\n\n        return self.delete(inds)\n\n    def _drop_from_level(\n        self, codes, level, errors: IgnoreRaise = \"raise\"\n    ) -> MultiIndex:\n        codes = com.index_labels_to_array(codes)\n        i = self._get_level_number(level)\n        index = self.levels[i]\n        values = index.get_indexer(codes)\n        # If nan should be dropped it will equal -1 here. We have to check which values\n        # are not nan and equal -1, this means they are missing in the index\n        nan_codes = isna(codes)\n        values[(np.equal(nan_codes, False)) & (values == -1)] = -2\n        if index.shape[0] == self.shape[0]:\n            values[np.equal(nan_codes, True)] = -2\n\n        not_found = codes[values == -2]\n        if len(not_found) != 0 and errors != \"ignore\":\n            raise KeyError(f\"labels {not_found} not found in level\")\n        mask = ~algos.isin(self.codes[i], values)\n\n        return self[mask]\n\n    def swaplevel(self, i=-2, j=-1) -> MultiIndex:\n        \"\"\"\n        Swap level i with level j.\n\n        Calling this method does not change the ordering of the values.\n\n        Parameters\n        ----------\n        i : int, str, default -2\n            First level of index to be swapped. Can pass level name as string.\n            Type of parameters can be mixed.\n        j : int, str, default -1\n            Second level of index to be swapped. Can pass level name as string.\n            Type of parameters can be mixed.\n\n        Returns\n        -------\n        MultiIndex\n            A new MultiIndex.\n\n        See Also\n        --------\n        Series.swaplevel : Swap levels i and j in a MultiIndex.\n        DataFrame.swaplevel : Swap levels i and j in a MultiIndex on a\n            particular axis.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex(\n        ...     levels=[[\"a\", \"b\"], [\"bb\", \"aa\"]], codes=[[0, 0, 1, 1], [0, 1, 0, 1]]\n        ... )\n        >>> mi\n        MultiIndex([('a', 'bb'),\n                    ('a', 'aa'),\n                    ('b', 'bb'),\n                    ('b', 'aa')],\n                   )\n        >>> mi.swaplevel(0, 1)\n        MultiIndex([('bb', 'a'),\n                    ('aa', 'a'),\n                    ('bb', 'b'),\n                    ('aa', 'b')],\n                   )\n        \"\"\"\n        new_levels = list(self.levels)\n        new_codes = list(self.codes)\n        new_names = list(self.names)\n\n        i = self._get_level_number(i)\n        j = self._get_level_number(j)\n\n        new_levels[i], new_levels[j] = new_levels[j], new_levels[i]\n        new_codes[i], new_codes[j] = new_codes[j], new_codes[i]\n        new_names[i], new_names[j] = new_names[j], new_names[i]\n\n        return MultiIndex(\n            levels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\n        )\n\n    def reorder_levels(self, order) -> MultiIndex:\n        \"\"\"\n        Rearrange levels using input order. May not drop or duplicate levels.\n\n        `reorder_levels` is useful when you need to change the order of levels in\n        a MultiIndex, such as when reordering levels for hierarchical indexing. It\n        maintains the integrity of the MultiIndex, ensuring that all existing levels\n        are present and no levels are duplicated. This method is helpful for aligning\n        the index structure with other data structures or for optimizing the order\n        for specific data operations.\n\n        Parameters\n        ----------\n        order : list of int or list of str\n            List representing new level order. Reference level by number\n            (position) or by key (label).\n\n        Returns\n        -------\n        MultiIndex\n            A new MultiIndex with levels rearranged according to the specified order.\n\n        See Also\n        --------\n        MultiIndex.swaplevel : Swap two levels of the MultiIndex.\n        MultiIndex.set_names : Set names for the MultiIndex levels.\n        DataFrame.reorder_levels : Reorder levels in a DataFrame with a MultiIndex.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([[1, 2], [3, 4]], names=[\"x\", \"y\"])\n        >>> mi\n        MultiIndex([(1, 3),\n                    (2, 4)],\n                   names=['x', 'y'])\n\n        >>> mi.reorder_levels(order=[1, 0])\n        MultiIndex([(3, 1),\n                    (4, 2)],\n                   names=['y', 'x'])\n\n        >>> mi.reorder_levels(order=[\"y\", \"x\"])\n        MultiIndex([(3, 1),\n                    (4, 2)],\n                   names=['y', 'x'])\n        \"\"\"\n        order = [self._get_level_number(i) for i in order]\n        result = self._reorder_ilevels(order)\n        return result\n\n    def _reorder_ilevels(self, order) -> MultiIndex:\n        if len(order) != self.nlevels:\n            raise AssertionError(\n                f\"Length of order must be same as number of levels ({self.nlevels}), \"\n                f\"got {len(order)}\"\n            )\n        new_levels = [self.levels[i] for i in order]\n        new_codes = [self.codes[i] for i in order]\n        new_names = [self.names[i] for i in order]\n\n        return MultiIndex(\n            levels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\n        )\n\n    def _recode_for_new_levels(\n        self, new_levels, copy: bool = True\n    ) -> Generator[np.ndarray]:\n        if len(new_levels) > self.nlevels:\n            raise AssertionError(\n                f\"Length of new_levels ({len(new_levels)}) \"\n                f\"must be <= self.nlevels ({self.nlevels})\"\n            )\n        for i in range(len(new_levels)):\n            yield recode_for_categories(\n                self.codes[i], self.levels[i], new_levels[i], copy=copy\n            )\n\n    def _get_codes_for_sorting(self) -> list[Categorical]:\n        \"\"\"\n        we are categorizing our codes by using the\n        available categories (all, not just observed)\n        excluding any missing ones (-1); this is in preparation\n        for sorting, where we need to disambiguate that -1 is not\n        a valid valid\n        \"\"\"\n\n        def cats(level_codes: np.ndarray) -> np.ndarray:\n            return np.arange(\n                level_codes.max() + 1 if len(level_codes) else 0,\n                dtype=level_codes.dtype,\n            )\n\n        return [\n            Categorical.from_codes(level_codes, cats(level_codes), True, validate=False)\n            for level_codes in self.codes\n        ]\n\n    def sortlevel(\n        self,\n        level: IndexLabel = 0,\n        ascending: bool | list[bool] = True,\n        sort_remaining: bool = True,\n        na_position: str = \"first\",\n    ) -> tuple[MultiIndex, npt.NDArray[np.intp]]:\n        \"\"\"\n        Sort MultiIndex at the requested level.\n\n        This method is useful when dealing with MultiIndex objects, allowing for\n        sorting at a specific level of the index. The function preserves the\n        relative ordering of data within the same level while sorting\n        the overall MultiIndex. The method provides flexibility with the `ascending`\n        parameter to define the sort order and with the `sort_remaining` parameter to\n        control whether the remaining levels should also be sorted. Sorting a\n        MultiIndex can be crucial when performing operations that require ordered\n        indices, such as grouping or merging datasets. The `na_position` argument is\n        important in handling missing values consistently across different levels.\n\n        Parameters\n        ----------\n        level : list-like, int or str, default 0\n            If a string is given, must be a name of the level.\n            If list-like must be names or ints of levels.\n        ascending : bool, default True\n            False to sort in descending order.\n            Can also be a list to specify a directed ordering.\n        sort_remaining : bool, default True\n            If True, sorts by the remaining levels after sorting by the specified\n            `level`.\n        na_position : {'first' or 'last'}, default 'first'\n            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n            the end.\n\n            .. versionadded:: 2.1.0\n\n        Returns\n        -------\n        sorted_index : pd.MultiIndex\n            Resulting index.\n        indexer : np.ndarray[np.intp]\n            Indices of output values in original index.\n\n        See Also\n        --------\n        MultiIndex : A multi-level, or hierarchical, index object for pandas objects.\n        Index.sort_values : Sort Index values.\n        DataFrame.sort_index : Sort DataFrame by the index.\n        Series.sort_index : Sort Series by the index.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([[0, 0], [2, 1]])\n        >>> mi\n        MultiIndex([(0, 2),\n                    (0, 1)],\n                   )\n\n        >>> mi.sortlevel()\n        (MultiIndex([(0, 1),\n                    (0, 2)],\n                   ), array([1, 0]))\n\n        >>> mi.sortlevel(sort_remaining=False)\n        (MultiIndex([(0, 2),\n                    (0, 1)],\n                   ), array([0, 1]))\n\n        >>> mi.sortlevel(1)\n        (MultiIndex([(0, 1),\n                    (0, 2)],\n                   ), array([1, 0]))\n\n        >>> mi.sortlevel(1, ascending=False)\n        (MultiIndex([(0, 2),\n                    (0, 1)],\n                   ), array([0, 1]))\n        \"\"\"\n        if not is_list_like(level):\n            level = [level]\n        # error: Item \"Hashable\" of \"Union[Hashable, Sequence[Hashable]]\" has\n        # no attribute \"__iter__\" (not iterable)\n        level = [\n            self._get_level_number(lev)\n            for lev in level  # type: ignore[union-attr]\n        ]\n        sortorder = None\n\n        codes = [self.codes[lev] for lev in level]\n        # we have a directed ordering via ascending\n        if isinstance(ascending, list):\n            if not len(level) == len(ascending):\n                raise ValueError(\"level must have same length as ascending\")\n        elif sort_remaining:\n            codes.extend(\n                [self.codes[lev] for lev in range(len(self.levels)) if lev not in level]\n            )\n        else:\n            sortorder = level[0]\n\n        indexer = lexsort_indexer(\n            codes, orders=ascending, na_position=na_position, codes_given=True\n        )\n\n        indexer = ensure_platform_int(indexer)\n        new_codes = [level_codes.take(indexer) for level_codes in self.codes]\n\n        new_index = MultiIndex(\n            codes=new_codes,\n            levels=self.levels,\n            names=self.names,\n            sortorder=sortorder,\n            verify_integrity=False,\n        )\n\n        return new_index, indexer\n\n    def _wrap_reindex_result(self, target, indexer, preserve_names: bool):\n        if not isinstance(target, MultiIndex):\n            if indexer is None:\n                target = self\n            elif (indexer >= 0).all():\n                target = self.take(indexer)\n            else:\n                try:\n                    target = MultiIndex.from_tuples(target)\n                except TypeError:\n                    # not all tuples, see test_constructor_dict_multiindex_reindex_flat\n                    return target\n\n        target = self._maybe_preserve_names(target, preserve_names)\n        return target\n\n    def _maybe_preserve_names(self, target: IndexT, preserve_names: bool) -> IndexT:\n        if (\n            preserve_names\n            and target.nlevels == self.nlevels\n            and target.names != self.names\n        ):\n            target = target.copy(deep=False)\n            target.names = self.names\n        return target\n\n    # --------------------------------------------------------------------\n    # Indexing Methods\n\n    def _check_indexing_error(self, key) -> None:\n        if not is_hashable(key) or is_iterator(key):\n            # We allow tuples if they are hashable, whereas other Index\n            #  subclasses require scalar.\n            # We have to explicitly exclude generators, as these are hashable.\n            raise InvalidIndexError(key)\n\n    @cache_readonly\n    def _should_fallback_to_positional(self) -> bool:\n        \"\"\"\n        Should integer key(s) be treated as positional?\n        \"\"\"\n        # GH#33355\n        return self.levels[0]._should_fallback_to_positional\n\n    def _get_indexer_strict(\n        self, key, axis_name: str\n    ) -> tuple[Index, npt.NDArray[np.intp]]:\n        keyarr = key\n        if not isinstance(keyarr, Index):\n            keyarr = com.asarray_tuplesafe(keyarr)\n\n        if len(keyarr) and not isinstance(keyarr[0], tuple):\n            indexer = self._get_indexer_level_0(keyarr)\n\n            self._raise_if_missing(key, indexer, axis_name)\n            return self[indexer], indexer\n\n        return super()._get_indexer_strict(key, axis_name)\n\n    def _raise_if_missing(self, key, indexer, axis_name: str) -> None:\n        keyarr = key\n        if not isinstance(key, Index):\n            keyarr = com.asarray_tuplesafe(key)\n\n        if len(keyarr) and not isinstance(keyarr[0], tuple):\n            # i.e. same condition for special case in MultiIndex._get_indexer_strict\n\n            mask = indexer == -1\n            if mask.any():\n                check = self.levels[0].get_indexer(keyarr)\n                cmask = check == -1\n                if cmask.any():\n                    raise KeyError(f\"{keyarr[cmask]} not in index\")\n                # We get here when levels still contain values which are not\n                # actually in Index anymore\n                raise KeyError(f\"{keyarr} not in index\")\n        else:\n            return super()._raise_if_missing(key, indexer, axis_name)\n\n    def _get_indexer_level_0(self, target) -> npt.NDArray[np.intp]:\n        \"\"\"\n        Optimized equivalent to `self.get_level_values(0).get_indexer_for(target)`.\n        \"\"\"\n        lev = self.levels[0]\n        codes = self._codes[0]\n        cat = Categorical.from_codes(codes=codes, categories=lev, validate=False)\n        ci = Index(cat)\n        return ci.get_indexer_for(target)\n\n    def get_slice_bound(\n        self,\n        label: Hashable | Sequence[Hashable],\n        side: Literal[\"left\", \"right\"],\n    ) -> int:\n        \"\"\"\n        For an ordered MultiIndex, compute slice bound\n        that corresponds to given label.\n\n        Returns leftmost (one-past-the-rightmost if `side=='right') position\n        of given label.\n\n        Parameters\n        ----------\n        label : object or tuple of objects\n        side : {'left', 'right'}\n\n        Returns\n        -------\n        int\n            Index of label.\n\n        Notes\n        -----\n        This method only works if level 0 index of the MultiIndex is lexsorted.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([list(\"abbc\"), list(\"gefd\")])\n\n        Get the locations from the leftmost 'b' in the first level\n        until the end of the multiindex:\n\n        >>> mi.get_slice_bound(\"b\", side=\"left\")\n        1\n\n        Like above, but if you get the locations from the rightmost\n        'b' in the first level and 'f' in the second level:\n\n        >>> mi.get_slice_bound((\"b\", \"f\"), side=\"right\")\n        3\n\n        See Also\n        --------\n        MultiIndex.get_loc : Get location for a label or a tuple of labels.\n        MultiIndex.get_locs : Get location for a label/slice/list/mask or a\n                              sequence of such.\n        \"\"\"\n        if not isinstance(label, tuple):\n            label = (label,)\n        return self._partial_tup_index(label, side=side)\n\n    def slice_locs(self, start=None, end=None, step=None) -> tuple[int, int]:\n        \"\"\"\n        For an ordered MultiIndex, compute the slice locations for input\n        labels.\n\n        The input labels can be tuples representing partial levels, e.g. for a\n        MultiIndex with 3 levels, you can pass a single value (corresponding to\n        the first level), or a 1-, 2-, or 3-tuple.\n\n        Parameters\n        ----------\n        start : label or tuple, default None\n            If None, defaults to the beginning\n        end : label or tuple\n            If None, defaults to the end\n        step : int or None\n            Slice step\n\n        Returns\n        -------\n        (start, end) : (int, int)\n\n        Notes\n        -----\n        This method only works if the MultiIndex is properly lexsorted. So,\n        if only the first 2 levels of a 3-level MultiIndex are lexsorted,\n        you can only pass two levels to ``.slice_locs``.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays(\n        ...     [list(\"abbd\"), list(\"deff\")], names=[\"A\", \"B\"]\n        ... )\n\n        Get the slice locations from the beginning of 'b' in the first level\n        until the end of the multiindex:\n\n        >>> mi.slice_locs(start=\"b\")\n        (1, 4)\n\n        Like above, but stop at the end of 'b' in the first level and 'f' in\n        the second level:\n\n        >>> mi.slice_locs(start=\"b\", end=(\"b\", \"f\"))\n        (1, 3)\n\n        See Also\n        --------\n        MultiIndex.get_loc : Get location for a label or a tuple of labels.\n        MultiIndex.get_locs : Get location for a label/slice/list/mask or a\n                              sequence of such.\n        \"\"\"\n        # This function adds nothing to its parent implementation (the magic\n        # happens in get_slice_bound method), but it adds meaningful doc.\n        return super().slice_locs(start, end, step)\n\n    def _partial_tup_index(self, tup: tuple, side: Literal[\"left\", \"right\"] = \"left\"):\n        if len(tup) > self._lexsort_depth:\n            raise UnsortedIndexError(\n                f\"Key length ({len(tup)}) was greater than MultiIndex lexsort depth \"\n                f\"({self._lexsort_depth})\"\n            )\n\n        n = len(tup)\n        start, end = 0, len(self)\n        zipped = zip(tup, self.levels, self.codes)\n        for k, (lab, lev, level_codes) in enumerate(zipped):\n            section = level_codes[start:end]\n\n            loc: npt.NDArray[np.intp] | np.intp | int\n            if lab not in lev and not isna(lab):\n                # short circuit\n                try:\n                    loc = algos.searchsorted(lev, lab, side=side)\n                except TypeError as err:\n                    # non-comparable e.g. test_slice_locs_with_type_mismatch\n                    raise TypeError(f\"Level type mismatch: {lab}\") from err\n                if not is_integer(loc):\n                    # non-comparable level, e.g. test_groupby_example\n                    raise TypeError(f\"Level type mismatch: {lab}\")\n                if side == \"right\" and loc >= 0:\n                    loc -= 1\n                return start + algos.searchsorted(section, loc, side=side)\n\n            idx = self._get_loc_single_level_index(lev, lab)\n            if isinstance(idx, slice) and k < n - 1:\n                # Get start and end value from slice, necessary when a non-integer\n                # interval is given as input GH#37707\n                start = idx.start\n                end = idx.stop\n            elif k < n - 1:\n                # error: Incompatible types in assignment (expression has type\n                # \"Union[ndarray[Any, dtype[signedinteger[Any]]]\n                end = start + algos.searchsorted(  # type: ignore[assignment]\n                    section, idx, side=\"right\"\n                )\n                # error: Incompatible types in assignment (expression has type\n                # \"Union[ndarray[Any, dtype[signedinteger[Any]]]\n                start = start + algos.searchsorted(  # type: ignore[assignment]\n                    section, idx, side=\"left\"\n                )\n            elif isinstance(idx, slice):\n                idx = idx.start\n                return start + algos.searchsorted(section, idx, side=side)\n            else:\n                return start + algos.searchsorted(section, idx, side=side)\n\n    def _get_loc_single_level_index(self, level_index: Index, key: Hashable) -> int:\n        \"\"\"\n        If key is NA value, location of index unify as -1.\n\n        Parameters\n        ----------\n        level_index: Index\n        key : label\n\n        Returns\n        -------\n        loc : int\n            If key is NA value, loc is -1\n            Else, location of key in index.\n\n        See Also\n        --------\n        Index.get_loc : The get_loc method for (single-level) index.\n        \"\"\"\n        if is_scalar(key) and isna(key):\n            # TODO: need is_valid_na_for_dtype(key, level_index.dtype)\n            return -1\n        else:\n            return level_index.get_loc(key)\n\n    def get_loc(self, key):\n        \"\"\"\n        Get location for a label or a tuple of labels. The location is returned \\\n        as an integer/slice or boolean mask.\n\n        This method returns the integer location, slice object, or boolean mask\n        corresponding to the specified key, which can be a single label or a tuple\n        of labels. The key represents a position in the MultiIndex, and the location\n        indicates where the key is found within the index.\n\n        Parameters\n        ----------\n        key : label or tuple of labels (one for each level)\n            A label or tuple of labels that correspond to the levels of the MultiIndex.\n            The key must match the structure of the MultiIndex.\n\n        Returns\n        -------\n        int, slice object or boolean mask\n            If the key is past the lexsort depth, the return may be a\n            boolean mask array, otherwise it is always a slice or int.\n\n        See Also\n        --------\n        Index.get_loc : The get_loc method for (single-level) index.\n        MultiIndex.slice_locs : Get slice location given start label(s) and\n                                end label(s).\n        MultiIndex.get_locs : Get location for a label/slice/list/mask or a\n                              sequence of such.\n\n        Notes\n        -----\n        The key cannot be a slice, list of same-level labels, a boolean mask,\n        or a sequence of such. If you want to use those, use\n        :meth:`MultiIndex.get_locs` instead.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([list(\"abb\"), list(\"def\")])\n\n        >>> mi.get_loc(\"b\")\n        slice(1, 3, None)\n\n        >>> mi.get_loc((\"b\", \"e\"))\n        1\n        \"\"\"\n        self._check_indexing_error(key)\n\n        def _maybe_to_slice(loc):\n            \"\"\"convert integer indexer to boolean mask or slice if possible\"\"\"\n            if not isinstance(loc, np.ndarray) or loc.dtype != np.intp:\n                return loc\n\n            loc = lib.maybe_indices_to_slice(loc, len(self))\n            if isinstance(loc, slice):\n                return loc\n\n            mask = np.empty(len(self), dtype=\"bool\")\n            mask.fill(False)\n            mask[loc] = True\n            return mask\n\n        if not isinstance(key, tuple):\n            loc = self._get_level_indexer(key, level=0)\n            return _maybe_to_slice(loc)\n\n        keylen = len(key)\n        if self.nlevels < keylen:\n            raise KeyError(\n                f\"Key length ({keylen}) exceeds index depth ({self.nlevels})\"\n            )\n\n        if keylen == self.nlevels and self.is_unique:\n            # TODO: what if we have an IntervalIndex level?\n            #  i.e. do we need _index_as_unique on that level?\n            try:\n                return self._engine.get_loc(key)\n            except KeyError as err:\n                raise KeyError(key) from err\n            except TypeError:\n                # e.g. test_partial_slicing_with_multiindex partial string slicing\n                loc, _ = self.get_loc_level(key, range(self.nlevels))\n                return loc\n\n        # -- partial selection or non-unique index\n        # break the key into 2 parts based on the lexsort_depth of the index;\n        # the first part returns a continuous slice of the index; the 2nd part\n        # needs linear search within the slice\n        i = self._lexsort_depth\n        lead_key, follow_key = key[:i], key[i:]\n\n        if not lead_key:\n            start = 0\n            stop = len(self)\n        else:\n            try:\n                start, stop = self.slice_locs(lead_key, lead_key)\n            except TypeError as err:\n                # e.g. test_groupby_example key = ((0, 0, 1, 2), \"new_col\")\n                #  when self has 5 integer levels\n                raise KeyError(key) from err\n\n        if start == stop:\n            raise KeyError(key)\n\n        if not follow_key:\n            return slice(start, stop)\n\n        if get_option(\"performance_warnings\"):\n            warnings.warn(\n                \"indexing past lexsort depth may impact performance.\",\n                PerformanceWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        loc = np.arange(start, stop, dtype=np.intp)\n\n        for i, k in enumerate(follow_key, len(lead_key)):\n            mask = self.codes[i][loc] == self._get_loc_single_level_index(\n                self.levels[i], k\n            )\n            if not mask.all():\n                loc = loc[mask]\n            if not len(loc):\n                raise KeyError(key)\n\n        return _maybe_to_slice(loc) if len(loc) != stop - start else slice(start, stop)\n\n    def get_loc_level(self, key, level: IndexLabel = 0, drop_level: bool = True):\n        \"\"\"\n        Get location and sliced index for requested label(s)/level(s).\n\n        The `get_loc_level` method is a more advanced form of `get_loc`, allowing\n        users to specify not just a label or sequence of labels, but also the level(s)\n        in which to search. This method is useful when you need to isolate particular\n        sections of a MultiIndex, either for further analysis or for slicing and\n        dicing the data. The method provides flexibility in terms of maintaining\n        or dropping levels from the resulting index based on the `drop_level`\n        parameter.\n\n        Parameters\n        ----------\n        key : label or sequence of labels\n            The label(s) for which to get the location.\n        level : int/level name or list thereof, optional\n            The level(s) in the MultiIndex to consider. If not provided, defaults\n            to the first level.\n        drop_level : bool, default True\n            If ``False``, the resulting index will not drop any level.\n\n        Returns\n        -------\n        tuple\n            A 2-tuple where the elements :\n\n            Element 0: int, slice object or boolean array.\n\n            Element 1: The resulting sliced multiindex/index. If the key\n            contains all levels, this will be ``None``.\n\n        See Also\n        --------\n        MultiIndex.get_loc  : Get location for a label or a tuple of labels.\n        MultiIndex.get_locs : Get location for a label/slice/list/mask or a\n                              sequence of such.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([list(\"abb\"), list(\"def\")], names=[\"A\", \"B\"])\n\n        >>> mi.get_loc_level(\"b\")\n        (slice(1, 3, None), Index(['e', 'f'], dtype='object', name='B'))\n\n        >>> mi.get_loc_level(\"e\", level=\"B\")\n        (array([False,  True, False]), Index(['b'], dtype='object', name='A'))\n\n        >>> mi.get_loc_level([\"b\", \"e\"])\n        (1, None)\n        \"\"\"\n        if not isinstance(level, (range, list, tuple)):\n            level = self._get_level_number(level)\n        else:\n            level = [self._get_level_number(lev) for lev in level]\n\n        loc, mi = self._get_loc_level(key, level=level)\n        if not drop_level:\n            if lib.is_integer(loc):\n                # Slice index must be an integer or None\n                mi = self[loc : loc + 1]\n            else:\n                mi = self[loc]\n        return loc, mi\n\n    def _get_loc_level(self, key, level: int | list[int] = 0):\n        \"\"\"\n        get_loc_level but with `level` known to be positional, not name-based.\n        \"\"\"\n\n        # different name to distinguish from maybe_droplevels\n        def maybe_mi_droplevels(indexer, levels):\n            \"\"\"\n            If level does not exist or all levels were dropped, the exception\n            has to be handled outside.\n            \"\"\"\n            new_index = self[indexer]\n\n            for i in sorted(levels, reverse=True):\n                new_index = new_index._drop_level_numbers([i])\n\n            return new_index\n\n        if isinstance(level, (tuple, list)):\n            if len(key) != len(level):\n                raise AssertionError(\n                    \"Key for location must have same length as number of levels\"\n                )\n            result = None\n            for lev, k in zip(level, key):\n                loc, new_index = self._get_loc_level(k, level=lev)\n                if isinstance(loc, slice):\n                    mask = np.zeros(len(self), dtype=bool)\n                    mask[loc] = True\n                    loc = mask\n                result = loc if result is None else result & loc\n\n            try:\n                # FIXME: we should be only dropping levels on which we are\n                #  scalar-indexing\n                mi = maybe_mi_droplevels(result, level)\n            except ValueError:\n                # droplevel failed because we tried to drop all levels,\n                #  i.e. len(level) == self.nlevels\n                mi = self[result]\n\n            return result, mi\n\n        # kludge for #1796\n        if isinstance(key, list):\n            key = tuple(key)\n\n        if isinstance(key, tuple) and level == 0:\n            try:\n                # Check if this tuple is a single key in our first level\n                if key in self.levels[0]:\n                    indexer = self._get_level_indexer(key, level=level)\n                    new_index = maybe_mi_droplevels(indexer, [0])\n                    return indexer, new_index\n            except (TypeError, InvalidIndexError):\n                pass\n\n            if not any(isinstance(k, slice) for k in key):\n                if len(key) == self.nlevels and self.is_unique:\n                    # Complete key in unique index -> standard get_loc\n                    try:\n                        return (self._engine.get_loc(key), None)\n                    except KeyError as err:\n                        raise KeyError(key) from err\n                    except TypeError:\n                        # e.g. partial string indexing\n                        #  test_partial_string_timestamp_multiindex\n                        pass\n\n                # partial selection\n                indexer = self.get_loc(key)\n                ilevels = [i for i in range(len(key)) if key[i] != slice(None, None)]\n                if len(ilevels) == self.nlevels:\n                    if is_integer(indexer):\n                        # we are dropping all levels\n                        return indexer, None\n\n                    # TODO: in some cases we still need to drop some levels,\n                    #  e.g. test_multiindex_perf_warn\n                    # test_partial_string_timestamp_multiindex\n                    ilevels = [\n                        i\n                        for i in range(len(key))\n                        if (\n                            not isinstance(key[i], str)\n                            or not self.levels[i]._supports_partial_string_indexing\n                        )\n                        and key[i] != slice(None, None)\n                    ]\n                    if len(ilevels) == self.nlevels:\n                        # TODO: why?\n                        ilevels = []\n                return indexer, maybe_mi_droplevels(indexer, ilevels)\n\n            else:\n                indexer = None\n                for i, k in enumerate(key):\n                    if not isinstance(k, slice):\n                        loc_level = self._get_level_indexer(k, level=i)\n                        if isinstance(loc_level, slice):\n                            if com.is_null_slice(loc_level) or com.is_full_slice(\n                                loc_level, len(self)\n                            ):\n                                # everything\n                                continue\n\n                            # e.g. test_xs_IndexSlice_argument_not_implemented\n                            k_index = np.zeros(len(self), dtype=bool)\n                            k_index[loc_level] = True\n\n                        else:\n                            k_index = loc_level\n\n                    elif com.is_null_slice(k):\n                        # taking everything, does not affect `indexer` below\n                        continue\n\n                    else:\n                        # FIXME: this message can be inaccurate, e.g.\n                        #  test_series_varied_multiindex_alignment\n                        raise TypeError(f\"Expected label or tuple of labels, got {key}\")\n\n                    if indexer is None:\n                        indexer = k_index\n                    else:\n                        indexer &= k_index\n                if indexer is None:\n                    indexer = slice(None, None)\n                ilevels = [i for i in range(len(key)) if key[i] != slice(None, None)]\n                return indexer, maybe_mi_droplevels(indexer, ilevels)\n        else:\n            indexer = self._get_level_indexer(key, level=level)\n            if (\n                isinstance(key, str)\n                and self.levels[level]._supports_partial_string_indexing\n            ):\n                # check to see if we did an exact lookup vs sliced\n                check = self.levels[level].get_loc(key)\n                if not is_integer(check):\n                    # e.g. test_partial_string_timestamp_multiindex\n                    return indexer, self[indexer]\n\n            try:\n                result_index = maybe_mi_droplevels(indexer, [level])\n            except ValueError:\n                result_index = self[indexer]\n\n            return indexer, result_index\n\n    def _get_level_indexer(\n        self, key, level: int = 0, indexer: npt.NDArray[np.bool_] | None = None\n    ):\n        # `level` kwarg is _always_ positional, never name\n        # return a boolean array or slice showing where the key is\n        # in the totality of values\n        # if the indexer is provided, then use this\n\n        level_index = self.levels[level]\n        level_codes = self.codes[level]\n\n        def convert_indexer(start, stop, step, indexer=indexer, codes=level_codes):\n            # Compute a bool indexer to identify the positions to take.\n            # If we have an existing indexer, we only need to examine the\n            # subset of positions where the existing indexer is True.\n            if indexer is not None:\n                # we only need to look at the subset of codes where the\n                # existing indexer equals True\n                codes = codes[indexer]\n\n            if step is None or step == 1:\n                new_indexer = (codes >= start) & (codes < stop)\n            else:\n                r = np.arange(start, stop, step, dtype=codes.dtype)\n                new_indexer = algos.isin(codes, r)\n\n            if indexer is None:\n                return new_indexer\n\n            indexer = indexer.copy()\n            indexer[indexer] = new_indexer\n            return indexer\n\n        if isinstance(key, slice):\n            # handle a slice, returning a slice if we can\n            # otherwise a boolean indexer\n            step = key.step\n            is_negative_step = step is not None and step < 0\n\n            try:\n                if key.start is not None:\n                    start = level_index.get_loc(key.start)\n                elif is_negative_step:\n                    start = len(level_index) - 1\n                else:\n                    start = 0\n\n                if key.stop is not None:\n                    stop = level_index.get_loc(key.stop)\n                elif is_negative_step:\n                    stop = 0\n                elif isinstance(start, slice):\n                    stop = len(level_index)\n                else:\n                    stop = len(level_index) - 1\n            except KeyError:\n                # we have a partial slice (like looking up a partial date\n                # string)\n                start = stop = level_index.slice_indexer(key.start, key.stop, key.step)\n                step = start.step\n\n            if isinstance(start, slice) or isinstance(stop, slice):\n                # we have a slice for start and/or stop\n                # a partial date slicer on a DatetimeIndex generates a slice\n                # note that the stop ALREADY includes the stopped point (if\n                # it was a string sliced)\n                start = getattr(start, \"start\", start)\n                stop = getattr(stop, \"stop\", stop)\n                return convert_indexer(start, stop, step)\n\n            elif level > 0 or self._lexsort_depth == 0 or step is not None:\n                # need to have like semantics here to right\n                # searching as when we are using a slice\n                # so adjust the stop by 1 (so we include stop)\n                stop = (stop - 1) if is_negative_step else (stop + 1)\n                return convert_indexer(start, stop, step)\n            else:\n                # sorted, so can return slice object -> view\n                i = algos.searchsorted(level_codes, start, side=\"left\")\n                j = algos.searchsorted(level_codes, stop, side=\"right\")\n                return slice(i, j, step)\n\n        else:\n            idx = self._get_loc_single_level_index(level_index, key)\n\n            if level > 0 or self._lexsort_depth == 0:\n                # Desired level is not sorted\n                if isinstance(idx, slice):\n                    # test_get_loc_partial_timestamp_multiindex\n                    locs = (level_codes >= idx.start) & (level_codes < idx.stop)\n                    return locs\n\n                locs = np.asarray(level_codes == idx, dtype=bool)\n\n                if not locs.any():\n                    # The label is present in self.levels[level] but unused:\n                    raise KeyError(key)\n                return locs\n\n            if isinstance(idx, slice):\n                # e.g. test_partial_string_timestamp_multiindex\n                start = algos.searchsorted(level_codes, idx.start, side=\"left\")\n                # NB: \"left\" here bc of slice semantics\n                end = algos.searchsorted(level_codes, idx.stop, side=\"left\")\n            else:\n                start = algos.searchsorted(level_codes, idx, side=\"left\")\n                end = algos.searchsorted(level_codes, idx, side=\"right\")\n\n            if start == end:\n                # The label is present in self.levels[level] but unused:\n                raise KeyError(key)\n            return slice(start, end)\n\n    def get_locs(self, seq) -> npt.NDArray[np.intp]:\n        \"\"\"\n        Get location for a sequence of labels.\n\n        Parameters\n        ----------\n        seq : label, slice, list, mask or a sequence of such\n           You should use one of the above for each level.\n           If a level should not be used, set it to ``slice(None)``.\n\n        Returns\n        -------\n        numpy.ndarray\n            NumPy array of integers suitable for passing to iloc.\n\n        See Also\n        --------\n        MultiIndex.get_loc : Get location for a label or a tuple of labels.\n        MultiIndex.slice_locs : Get slice location given start label(s) and\n                                end label(s).\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([list(\"abb\"), list(\"def\")])\n\n        >>> mi.get_locs(\"b\")  # doctest: +SKIP\n        array([1, 2], dtype=int64)\n\n        >>> mi.get_locs([slice(None), [\"e\", \"f\"]])  # doctest: +SKIP\n        array([1, 2], dtype=int64)\n\n        >>> mi.get_locs([[True, False, True], slice(\"e\", \"f\")])  # doctest: +SKIP\n        array([2], dtype=int64)\n        \"\"\"\n\n        # must be lexsorted to at least as many levels\n        true_slices = [i for (i, s) in enumerate(com.is_true_slices(seq)) if s]\n        if true_slices and true_slices[-1] >= self._lexsort_depth:\n            raise UnsortedIndexError(\n                \"MultiIndex slicing requires the index to be lexsorted: slicing \"\n                f\"on levels {true_slices}, lexsort depth {self._lexsort_depth}\"\n            )\n\n        if any(x is Ellipsis for x in seq):\n            raise NotImplementedError(\n                \"MultiIndex does not support indexing with Ellipsis\"\n            )\n\n        n = len(self)\n\n        def _to_bool_indexer(indexer) -> npt.NDArray[np.bool_]:\n            if isinstance(indexer, slice):\n                new_indexer = np.zeros(n, dtype=np.bool_)\n                new_indexer[indexer] = True\n                return new_indexer\n            return indexer\n\n        # a bool indexer for the positions we want to take\n        indexer: npt.NDArray[np.bool_] | None = None\n\n        for i, k in enumerate(seq):\n            lvl_indexer: npt.NDArray[np.bool_] | slice | None = None\n\n            if com.is_bool_indexer(k):\n                if len(k) != n:\n                    raise ValueError(\n                        \"cannot index with a boolean indexer that \"\n                        \"is not the same length as the index\"\n                    )\n                if isinstance(k, (ABCSeries, Index)):\n                    k = k._values\n                lvl_indexer = np.asarray(k)\n                if indexer is None:\n                    lvl_indexer = lvl_indexer.copy()\n\n            elif is_list_like(k):\n                # a collection of labels to include from this level (these are or'd)\n\n                # GH#27591 check if this is a single tuple key in the level\n                try:\n                    lvl_indexer = self._get_level_indexer(k, level=i, indexer=indexer)\n                except (InvalidIndexError, TypeError, KeyError) as err:\n                    # InvalidIndexError e.g. non-hashable, fall back to treating\n                    #  this as a sequence of labels\n                    # KeyError it can be ambiguous if this is a label or sequence\n                    #  of labels\n                    #  github.com/pandas-dev/pandas/issues/39424#issuecomment-871626708\n                    for x in k:\n                        if not is_hashable(x):\n                            # e.g. slice\n                            raise err\n                        # GH 39424: Ignore not founds\n                        # GH 42351: No longer ignore not founds & enforced in 2.0\n                        # TODO: how to handle IntervalIndex level? (no test cases)\n                        item_indexer = self._get_level_indexer(\n                            x, level=i, indexer=indexer\n                        )\n                        if lvl_indexer is None:\n                            lvl_indexer = _to_bool_indexer(item_indexer)\n                        elif isinstance(item_indexer, slice):\n                            lvl_indexer[item_indexer] = True  # type: ignore[index]\n                        else:\n                            lvl_indexer |= item_indexer\n\n                if lvl_indexer is None:\n                    # no matches we are done\n                    # test_loc_getitem_duplicates_multiindex_empty_indexer\n                    return np.array([], dtype=np.intp)\n\n            elif com.is_null_slice(k):\n                # empty slice\n                if indexer is None and i == len(seq) - 1:\n                    return np.arange(n, dtype=np.intp)\n                continue\n\n            else:\n                # a slice or a single label\n                lvl_indexer = self._get_level_indexer(k, level=i, indexer=indexer)\n\n            # update indexer\n            lvl_indexer = _to_bool_indexer(lvl_indexer)\n            if indexer is None:\n                indexer = lvl_indexer\n            else:\n                indexer &= lvl_indexer\n                if not np.any(indexer) and np.any(lvl_indexer):\n                    raise KeyError(seq)\n\n        # empty indexer\n        if indexer is None:\n            return np.array([], dtype=np.intp)\n\n        pos_indexer = indexer.nonzero()[0]\n        return self._reorder_indexer(seq, pos_indexer)\n\n    # --------------------------------------------------------------------\n\n    def _reorder_indexer(\n        self,\n        seq: tuple[Scalar | Iterable | AnyArrayLike, ...],\n        indexer: npt.NDArray[np.intp],\n    ) -> npt.NDArray[np.intp]:\n        \"\"\"\n        Reorder an indexer of a MultiIndex (self) so that the labels are in the\n        same order as given in seq\n\n        Parameters\n        ----------\n        seq : label/slice/list/mask or a sequence of such\n        indexer: a position indexer of self\n\n        Returns\n        -------\n        indexer : a sorted position indexer of self ordered as seq\n        \"\"\"\n\n        # check if sorting is necessary\n        need_sort = False\n        for i, k in enumerate(seq):\n            if com.is_null_slice(k) or com.is_bool_indexer(k) or is_scalar(k):\n                pass\n            elif is_list_like(k):\n                if len(k) <= 1:  # type: ignore[arg-type]\n                    pass\n                elif self._is_lexsorted():\n                    # If the index is lexsorted and the list_like label\n                    # in seq are sorted then we do not need to sort\n                    k_codes = self.levels[i].get_indexer(k)\n                    k_codes = k_codes[k_codes >= 0]  # Filter absent keys\n                    # True if the given codes are not ordered\n                    need_sort = (k_codes[:-1] > k_codes[1:]).any()\n                else:\n                    need_sort = True\n            elif isinstance(k, slice):\n                if self._is_lexsorted():\n                    need_sort = k.step is not None and k.step < 0\n                else:\n                    need_sort = True\n            else:\n                need_sort = True\n            if need_sort:\n                break\n        if not need_sort:\n            return indexer\n\n        n = len(self)\n        keys: tuple[np.ndarray, ...] = ()\n        # For each level of the sequence in seq, map the level codes with the\n        # order they appears in a list-like sequence\n        # This mapping is then use to reorder the indexer\n        for i, k in enumerate(seq):\n            if is_scalar(k):\n                # GH#34603 we want to treat a scalar the same as an all equal list\n                k = [k]\n            if com.is_bool_indexer(k):\n                new_order = np.arange(n)[indexer]\n            elif is_list_like(k):\n                # Generate a map with all level codes as sorted initially\n                if not isinstance(k, (np.ndarray, ExtensionArray, Index, ABCSeries)):\n                    k = sanitize_array(k, None)\n                k = algos.unique(k)\n                key_order_map = np.ones(len(self.levels[i]), dtype=np.uint64) * len(\n                    self.levels[i]\n                )\n                # Set order as given in the indexer list\n                level_indexer = self.levels[i].get_indexer(k)\n                level_indexer = level_indexer[level_indexer >= 0]  # Filter absent keys\n                key_order_map[level_indexer] = np.arange(len(level_indexer))\n\n                new_order = key_order_map[self.codes[i][indexer]]\n            elif isinstance(k, slice) and k.step is not None and k.step < 0:\n                # flip order for negative step\n                new_order = np.arange(n - 1, -1, -1)[indexer]\n            elif isinstance(k, slice) and k.start is None and k.stop is None:\n                # slice(None) should not determine order GH#31330\n                new_order = np.ones((n,), dtype=np.intp)[indexer]\n            else:\n                # For all other case, use the same order as the level\n                new_order = np.arange(n)[indexer]\n            keys = (new_order,) + keys\n\n        # Find the reordering using lexsort on the keys mapping\n        ind = np.lexsort(keys)\n        return indexer[ind]\n\n    def truncate(self, before=None, after=None) -> MultiIndex:\n        \"\"\"\n        Slice index between two labels / tuples, return new MultiIndex.\n\n        Parameters\n        ----------\n        before : label or tuple, can be partial. Default None\n            None defaults to start.\n        after : label or tuple, can be partial. Default None\n            None defaults to end.\n\n        Returns\n        -------\n        MultiIndex\n            The truncated MultiIndex.\n\n        See Also\n        --------\n        DataFrame.truncate : Truncate a DataFrame before and after some index values.\n        Series.truncate : Truncate a Series before and after some index values.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([[\"a\", \"b\", \"c\"], [\"x\", \"y\", \"z\"]])\n        >>> mi\n        MultiIndex([('a', 'x'), ('b', 'y'), ('c', 'z')],\n                   )\n        >>> mi.truncate(before=\"a\", after=\"b\")\n        MultiIndex([('a', 'x'), ('b', 'y')],\n                   )\n        \"\"\"\n        if after and before and after < before:\n            raise ValueError(\"after < before\")\n\n        i, j = self.levels[0].slice_locs(before, after)\n        left, right = self.slice_locs(before, after)\n\n        new_levels = list(self.levels)\n        new_levels[0] = new_levels[0][i:j]\n\n        new_codes = [level_codes[left:right] for level_codes in self.codes]\n        new_codes[0] = new_codes[0] - i\n\n        return MultiIndex(\n            levels=new_levels,\n            codes=new_codes,\n            names=self._names,\n            verify_integrity=False,\n        )\n\n    def equals(self, other: object) -> bool:\n        \"\"\"\n        Determines if two MultiIndex objects have the same labeling information\n        (the levels themselves do not necessarily have to be the same)\n\n        See Also\n        --------\n        equal_levels\n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n\n        if len(self) != len(other):\n            return False\n\n        if not isinstance(other, MultiIndex):\n            # d-level MultiIndex can equal d-tuple Index\n            if not self._should_compare(other):\n                # object Index or Categorical[object] may contain tuples\n                return False\n            return array_equivalent(self._values, other._values)\n\n        if self.nlevels != other.nlevels:\n            return False\n\n        for i in range(self.nlevels):\n            self_codes = self.codes[i]\n            other_codes = other.codes[i]\n            self_mask = self_codes == -1\n            other_mask = other_codes == -1\n            if not np.array_equal(self_mask, other_mask):\n                return False\n            self_level = self.levels[i]\n            other_level = other.levels[i]\n            new_codes = recode_for_categories(\n                other_codes, other_level, self_level, copy=False\n            )\n            if not np.array_equal(self_codes, new_codes):\n                return False\n            if not self_level[:0].equals(other_level[:0]):\n                # e.g. Int64 != int64\n                return False\n        return True\n\n    def equal_levels(self, other: MultiIndex) -> bool:\n        \"\"\"\n        Return True if the levels of both MultiIndex objects are the same\n\n        \"\"\"\n        if self.nlevels != other.nlevels:\n            return False\n\n        for i in range(self.nlevels):\n            if not self.levels[i].equals(other.levels[i]):\n                return False\n        return True\n\n    # --------------------------------------------------------------------\n    # Set Methods\n\n    def _union(self, other, sort) -> MultiIndex:\n        other, result_names = self._convert_can_do_setop(other)\n        if other.has_duplicates:\n            # This is only necessary if other has dupes,\n            # otherwise difference is faster\n            result = super()._union(other, sort)\n\n            if isinstance(result, MultiIndex):\n                return result\n            return MultiIndex.from_arrays(\n                zip(*result), sortorder=None, names=result_names\n            )\n\n        else:\n            right_missing = other.difference(self, sort=False)\n            if len(right_missing):\n                result = self.append(right_missing)\n            else:\n                result = self._get_reconciled_name_object(other)\n\n            if sort is not False:\n                try:\n                    result = result.sort_values()\n                except TypeError:\n                    if sort is True:\n                        raise\n                    warnings.warn(\n                        \"The values in the array are unorderable. \"\n                        \"Pass `sort=False` to suppress this warning.\",\n                        RuntimeWarning,\n                        stacklevel=find_stack_level(),\n                    )\n            return result\n\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        return is_object_dtype(dtype)\n\n    def _get_reconciled_name_object(self, other) -> MultiIndex:\n        \"\"\"\n        If the result of a set operation will be self,\n        return self, unless the names change, in which\n        case make a shallow copy of self.\n        \"\"\"\n        names = self._maybe_match_names(other)\n        if self.names != names:\n            # error: Cannot determine type of \"rename\"\n            return self.rename(names)  # type: ignore[has-type]\n        return self\n\n    def _maybe_match_names(self, other):\n        \"\"\"\n        Try to find common names to attach to the result of an operation between\n        a and b. Return a consensus list of names if they match at least partly\n        or list of None if they have completely different names.\n        \"\"\"\n        if len(self.names) != len(other.names):\n            return [None] * len(self.names)\n        names = []\n        for a_name, b_name in zip(self.names, other.names):\n            if a_name == b_name:\n                names.append(a_name)\n            else:\n                # TODO: what if they both have np.nan for their names?\n                names.append(None)\n        return names\n\n    def _wrap_intersection_result(self, other, result) -> MultiIndex:\n        _, result_names = self._convert_can_do_setop(other)\n        return result.set_names(result_names)\n\n    def _wrap_difference_result(self, other, result: MultiIndex) -> MultiIndex:\n        _, result_names = self._convert_can_do_setop(other)\n\n        if len(result) == 0:\n            return result.remove_unused_levels().set_names(result_names)\n        else:\n            return result.set_names(result_names)\n\n    def _convert_can_do_setop(self, other):\n        result_names = self.names\n\n        if not isinstance(other, Index):\n            if len(other) == 0:\n                return self[:0], self.names\n            else:\n                msg = \"other must be a MultiIndex or a list of tuples\"\n                try:\n                    other = MultiIndex.from_tuples(other, names=self.names)\n                except (ValueError, TypeError) as err:\n                    # ValueError raised by tuples_to_object_array if we\n                    #  have non-object dtype\n                    raise TypeError(msg) from err\n        else:\n            result_names = get_unanimous_names(self, other)\n\n        return other, result_names\n\n    # --------------------------------------------------------------------\n\n    @doc(Index.astype)\n    def astype(self, dtype, copy: bool = True):\n        dtype = pandas_dtype(dtype)\n        if isinstance(dtype, CategoricalDtype):\n            msg = \"> 1 ndim Categorical are not supported at this time\"\n            raise NotImplementedError(msg)\n        if not is_object_dtype(dtype):\n            raise TypeError(\n                \"Setting a MultiIndex dtype to anything other than object \"\n                \"is not supported\"\n            )\n        if copy is True:\n            return self._view()\n        return self\n\n    def _validate_fill_value(self, item):\n        if isinstance(item, MultiIndex):\n            # GH#43212\n            if item.nlevels != self.nlevels:\n                raise ValueError(\"Item must have length equal to number of levels.\")\n            return item._values\n        elif not isinstance(item, tuple):\n            # Pad the key with empty strings if lower levels of the key\n            # aren't specified:\n            item = (item,) + (\"\",) * (self.nlevels - 1)\n        elif len(item) != self.nlevels:\n            raise ValueError(\"Item must have length equal to number of levels.\")\n        return item\n\n    def putmask(self, mask, value: MultiIndex) -> MultiIndex:\n        \"\"\"\n        Return a new MultiIndex of the values set with the mask.\n\n        Parameters\n        ----------\n        mask : array like\n        value : MultiIndex\n            Must either be the same length as self or length one\n\n        Returns\n        -------\n        MultiIndex\n        \"\"\"\n        mask, noop = validate_putmask(self, mask)\n        if noop:\n            return self.copy()\n\n        if len(mask) == len(value):\n            subset = value[mask].remove_unused_levels()\n        else:\n            subset = value.remove_unused_levels()\n\n        new_levels = []\n        new_codes = []\n\n        for i, (value_level, level, level_codes) in enumerate(\n            zip(subset.levels, self.levels, self.codes)\n        ):\n            new_level = level.union(value_level, sort=False)\n            value_codes = new_level.get_indexer_for(subset.get_level_values(i))\n            new_code = ensure_int64(level_codes)\n            new_code[mask] = value_codes\n            new_levels.append(new_level)\n            new_codes.append(new_code)\n\n        return MultiIndex(\n            levels=new_levels, codes=new_codes, names=self.names, verify_integrity=False\n        )\n\n    def insert(self, loc: int, item) -> MultiIndex:\n        \"\"\"\n        Make new MultiIndex inserting new item at location\n\n        Parameters\n        ----------\n        loc : int\n        item : tuple\n            Must be same length as number of levels in the MultiIndex\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        item = self._validate_fill_value(item)\n\n        new_levels = []\n        new_codes = []\n        for k, level, level_codes in zip(item, self.levels, self.codes):\n            if k not in level:\n                # have to insert into level\n                # must insert at end otherwise you have to recompute all the\n                # other codes\n                lev_loc = len(level)\n                level = level.insert(lev_loc, k)\n                if isna(level[lev_loc]):  # GH 59003, 60388\n                    lev_loc = -1\n            else:\n                lev_loc = level.get_loc(k)\n\n            new_levels.append(level)\n            new_codes.append(np.insert(ensure_int64(level_codes), loc, lev_loc))\n\n        return MultiIndex(\n            levels=new_levels, codes=new_codes, names=self.names, verify_integrity=False\n        )\n\n    def delete(self, loc) -> MultiIndex:\n        \"\"\"\n        Make new index with passed location deleted\n\n        Returns\n        -------\n        new_index : MultiIndex\n        \"\"\"\n        new_codes = [np.delete(level_codes, loc) for level_codes in self.codes]\n        return MultiIndex(\n            levels=self.levels,\n            codes=new_codes,\n            names=self.names,\n            verify_integrity=False,\n        )\n\n    @doc(Index.isin)\n    def isin(self, values, level=None) -> npt.NDArray[np.bool_]:\n        if isinstance(values, Generator):\n            values = list(values)\n\n        if level is None:\n            if len(values) == 0:\n                return np.zeros((len(self),), dtype=np.bool_)\n            if not isinstance(values, MultiIndex):\n                values = MultiIndex.from_tuples(values)\n            return values.unique().get_indexer_for(self) != -1\n        else:\n            num = self._get_level_number(level)\n            levs = self.get_level_values(num)\n\n            if levs.size == 0:\n                return np.zeros(len(levs), dtype=np.bool_)\n            return levs.isin(values)\n\n    # error: Incompatible types in assignment (expression has type overloaded function,\n    # base class \"Index\" defined the type as \"Callable[[Index, Any, bool], Any]\")\n    rename = Index.set_names  # type: ignore[assignment]\n\n    # ---------------------------------------------------------------\n    # Arithmetic/Numeric Methods - Disabled\n\n    __add__ = make_invalid_op(\"__add__\")\n    __radd__ = make_invalid_op(\"__radd__\")\n    __iadd__ = make_invalid_op(\"__iadd__\")\n    __sub__ = make_invalid_op(\"__sub__\")\n    __rsub__ = make_invalid_op(\"__rsub__\")\n    __isub__ = make_invalid_op(\"__isub__\")\n    __pow__ = make_invalid_op(\"__pow__\")\n    __rpow__ = make_invalid_op(\"__rpow__\")\n    __mul__ = make_invalid_op(\"__mul__\")\n    __rmul__ = make_invalid_op(\"__rmul__\")\n    __floordiv__ = make_invalid_op(\"__floordiv__\")\n    __rfloordiv__ = make_invalid_op(\"__rfloordiv__\")\n    __truediv__ = make_invalid_op(\"__truediv__\")\n    __rtruediv__ = make_invalid_op(\"__rtruediv__\")\n    __mod__ = make_invalid_op(\"__mod__\")\n    __rmod__ = make_invalid_op(\"__rmod__\")\n    __divmod__ = make_invalid_op(\"__divmod__\")\n    __rdivmod__ = make_invalid_op(\"__rdivmod__\")\n    # Unary methods disabled\n    __neg__ = make_invalid_op(\"__neg__\")\n    __pos__ = make_invalid_op(\"__pos__\")\n    __abs__ = make_invalid_op(\"__abs__\")\n    __invert__ = make_invalid_op(\"__invert__\")\n\n\ndef _lexsort_depth(codes: list[np.ndarray], nlevels: int) -> int:\n    \"\"\"Count depth (up to a maximum of `nlevels`) with which codes are lexsorted.\"\"\"\n    int64_codes = [ensure_int64(level_codes) for level_codes in codes]\n    for k in range(nlevels, 0, -1):\n        if libalgos.is_lexsorted(int64_codes[:k]):\n            return k\n    return 0\n\n\ndef sparsify_labels(label_list, start: int = 0, sentinel: object = \"\"):\n    pivoted = list(zip(*label_list))\n    k = len(label_list)\n\n    result = pivoted[: start + 1]\n    prev = pivoted[start]\n\n    for cur in pivoted[start + 1 :]:\n        sparse_cur = []\n\n        for i, (p, t) in enumerate(zip(prev, cur)):\n            if i == k - 1:\n                sparse_cur.append(t)\n                result.append(sparse_cur)  # type: ignore[arg-type]\n                break\n\n            if p == t:\n                sparse_cur.append(sentinel)\n            else:\n                sparse_cur.extend(cur[i:])\n                result.append(sparse_cur)  # type: ignore[arg-type]\n                break\n\n        prev = cur\n\n    return list(zip(*result))\n\n\ndef _get_na_rep(dtype: DtypeObj) -> str:\n    if isinstance(dtype, ExtensionDtype):\n        return f\"{dtype.na_value}\"\n    else:\n        dtype_type = dtype.type\n\n    return {np.datetime64: \"NaT\", np.timedelta64: \"NaT\"}.get(dtype_type, \"NaN\")\n\n\ndef maybe_droplevels(index: Index, key) -> Index:\n    \"\"\"\n    Attempt to drop level or levels from the given index.\n\n    Parameters\n    ----------\n    index: Index\n    key : scalar or tuple\n\n    Returns\n    -------\n    Index\n    \"\"\"\n    # drop levels\n    original_index = index\n    if isinstance(key, tuple):\n        # Caller is responsible for ensuring the key is not an entry in the first\n        #  level of the MultiIndex.\n        for _ in key:\n            try:\n                index = index._drop_level_numbers([0])\n            except ValueError:\n                # we have dropped too much, so back out\n                return original_index\n    else:\n        try:\n            index = index._drop_level_numbers([0])\n        except ValueError:\n            pass\n\n    return index\n\n\ndef _coerce_indexer_frozen(array_like, categories, copy: bool = False) -> np.ndarray:\n    \"\"\"\n    Coerce the array-like indexer to the smallest integer dtype that can encode all\n    of the given categories.\n\n    Parameters\n    ----------\n    array_like : array-like\n    categories : array-like\n    copy : bool\n\n    Returns\n    -------\n    np.ndarray\n        Non-writeable.\n    \"\"\"\n    array_like = coerce_indexer_dtype(array_like, categories)\n    if copy:\n        array_like = array_like.copy()\n    array_like.flags.writeable = False\n    return array_like\n\n\ndef _require_listlike(level, arr, arrname: str):\n    \"\"\"\n    Ensure that level is either None or listlike, and arr is list-of-listlike.\n    \"\"\"\n    if level is not None and not is_list_like(level):\n        if not is_list_like(arr):\n            raise TypeError(f\"{arrname} must be list-like\")\n        if len(arr) > 0 and is_list_like(arr[0]):\n            raise TypeError(f\"{arrname} must be list-like\")\n        level = [level]\n        arr = [arr]\n    elif level is None or is_list_like(level):\n        if not is_list_like(arr) or not is_list_like(arr[0]):\n            raise TypeError(f\"{arrname} must be list of lists-like\")\n    return level, arr\n\n\ndef cartesian_product(X: list[np.ndarray]) -> list[np.ndarray]:\n    \"\"\"\n    Numpy version of itertools.product.\n    Sometimes faster (for large inputs)...\n\n    Parameters\n    ----------\n    X : list-like of list-likes\n\n    Returns\n    -------\n    product : list of ndarrays\n\n    Examples\n    --------\n    >>> cartesian_product([list(\"ABC\"), [1, 2]])\n    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='<U1'), array([1, 2, 1, 2, 1, 2])]\n\n    See Also\n    --------\n    itertools.product : Cartesian product of input iterables.  Equivalent to\n        nested for-loops.\n    \"\"\"\n    msg = \"Input must be a list-like of list-likes\"\n    if not is_list_like(X):\n        raise TypeError(msg)\n    for x in X:\n        if not is_list_like(x):\n            raise TypeError(msg)\n\n    if len(X) == 0:\n        return []\n\n    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)\n    cumprodX = np.cumprod(lenX)\n\n    if np.any(cumprodX < 0):\n        raise ValueError(\"Product space too large to allocate arrays!\")\n\n    a = np.roll(cumprodX, 1)\n    a[0] = 1\n\n    if cumprodX[-1] != 0:\n        b = cumprodX[-1] / cumprodX\n    else:\n        # if any factor is empty, the cartesian product is empty\n        b = np.zeros_like(cumprodX)\n\n    return [\n        np.tile(\n            np.repeat(x, b[i]),\n            np.prod(a[i]),\n        )\n        for i, x in enumerate(X)\n    ]\n"
    },
    {
      "filename": "pandas/tests/indexes/multi/test_constructors.py",
      "content": "from datetime import (\n    date,\n    datetime,\n)\nimport itertools\n\nimport numpy as np\nimport pytest\n\nfrom pandas.core.dtypes.cast import construct_1d_object_array_from_listlike\n\nimport pandas as pd\nfrom pandas import (\n    Index,\n    MultiIndex,\n    Series,\n    Timestamp,\n    date_range,\n)\nimport pandas._testing as tm\n\n\ndef test_constructor_single_level():\n    result = MultiIndex(\n        levels=[[\"foo\", \"bar\", \"baz\", \"qux\"]], codes=[[0, 1, 2, 3]], names=[\"first\"]\n    )\n    assert isinstance(result, MultiIndex)\n    expected = Index([\"foo\", \"bar\", \"baz\", \"qux\"], name=\"first\")\n    tm.assert_index_equal(result.levels[0], expected)\n    assert result.names == [\"first\"]\n\n\ndef test_constructor_no_levels():\n    msg = \"non-zero number of levels/codes\"\n    with pytest.raises(ValueError, match=msg):\n        MultiIndex(levels=[], codes=[])\n\n    msg = \"Must pass both levels and codes\"\n    with pytest.raises(TypeError, match=msg):\n        MultiIndex(levels=[])\n    with pytest.raises(TypeError, match=msg):\n        MultiIndex(codes=[])\n\n\ndef test_constructor_nonhashable_names():\n    # GH 20527\n    levels = [[1, 2], [\"one\", \"two\"]]\n    codes = [[0, 0, 1, 1], [0, 1, 0, 1]]\n    names = ([\"foo\"], [\"bar\"])\n    msg = r\"MultiIndex\\.name must be a hashable type\"\n    with pytest.raises(TypeError, match=msg):\n        MultiIndex(levels=levels, codes=codes, names=names)\n\n    # With .rename()\n    mi = MultiIndex(\n        levels=[[1, 2], [\"one\", \"two\"]],\n        codes=[[0, 0, 1, 1], [0, 1, 0, 1]],\n        names=(\"foo\", \"bar\"),\n    )\n    renamed = [[\"fooo\"], [\"barr\"]]\n    with pytest.raises(TypeError, match=msg):\n        mi.rename(names=renamed)\n\n    # With .set_names()\n    with pytest.raises(TypeError, match=msg):\n        mi.set_names(names=renamed)\n\n\ndef test_constructor_mismatched_codes_levels(idx):\n    codes = [np.array([1]), np.array([2]), np.array([3])]\n    levels = [\"a\"]\n\n    msg = \"Length of levels and codes must be the same\"\n    with pytest.raises(ValueError, match=msg):\n        MultiIndex(levels=levels, codes=codes)\n\n    length_error = (\n        r\"On level 0, code max \\(3\\) >= length of level \\(1\\)\\. \"\n        \"NOTE: this index is in an inconsistent state\"\n    )\n    label_error = r\"Unequal code lengths: \\[4, 2\\]\"\n    code_value_error = r\"On level 0, code value \\(-2\\) < -1\"\n\n    # important to check that it's looking at the right thing.\n    with pytest.raises(ValueError, match=length_error):\n        MultiIndex(levels=[[\"a\"], [\"b\"]], codes=[[0, 1, 2, 3], [0, 3, 4, 1]])\n\n    with pytest.raises(ValueError, match=label_error):\n        MultiIndex(levels=[[\"a\"], [\"b\"]], codes=[[0, 0, 0, 0], [0, 0]])\n\n    # external API\n    with pytest.raises(ValueError, match=length_error):\n        idx.copy().set_levels([[\"a\"], [\"b\"]])\n\n    with pytest.raises(ValueError, match=label_error):\n        idx.copy().set_codes([[0, 0, 0, 0], [0, 0]])\n\n    # test set_codes with verify_integrity=False\n    # the setting should not raise any value error\n    idx.copy().set_codes(codes=[[0, 0, 0, 0], [0, 0]], verify_integrity=False)\n\n    # code value smaller than -1\n    with pytest.raises(ValueError, match=code_value_error):\n        MultiIndex(levels=[[\"a\"], [\"b\"]], codes=[[0, -2], [0, 0]])\n\n\ndef test_na_levels():\n    # GH26408\n    # test if codes are re-assigned value -1 for levels\n    # with missing values (NaN, NaT, None)\n    result = MultiIndex(\n        levels=[[np.nan, None, pd.NaT, 128, 2]], codes=[[0, -1, 1, 2, 3, 4]]\n    )\n    expected = MultiIndex(\n        levels=[[np.nan, None, pd.NaT, 128, 2]], codes=[[-1, -1, -1, -1, 3, 4]]\n    )\n    tm.assert_index_equal(result, expected)\n\n    result = MultiIndex(\n        levels=[[np.nan, \"s\", pd.NaT, 128, None]], codes=[[0, -1, 1, 2, 3, 4]]\n    )\n    expected = MultiIndex(\n        levels=[[np.nan, \"s\", pd.NaT, 128, None]], codes=[[-1, -1, 1, -1, 3, -1]]\n    )\n    tm.assert_index_equal(result, expected)\n\n    # verify set_levels and set_codes\n    result = MultiIndex(\n        levels=[[1, 2, 3, 4, 5]], codes=[[0, -1, 1, 2, 3, 4]]\n    ).set_levels([[np.nan, \"s\", pd.NaT, 128, None]])\n    tm.assert_index_equal(result, expected)\n\n    result = MultiIndex(\n        levels=[[np.nan, \"s\", pd.NaT, 128, None]], codes=[[1, 2, 2, 2, 2, 2]]\n    ).set_codes([[0, -1, 1, 2, 3, 4]])\n    tm.assert_index_equal(result, expected)\n\n\ndef test_copy_in_constructor():\n    levels = np.array([\"a\", \"b\", \"c\"])\n    codes = np.array([1, 1, 2, 0, 0, 1, 1])\n    val = codes[0]\n    mi = MultiIndex(levels=[levels, levels], codes=[codes, codes], copy=True)\n    assert mi.codes[0][0] == val\n    codes[0] = 15\n    assert mi.codes[0][0] == val\n    val = levels[0]\n    levels[0] = \"PANDA\"\n    assert mi.levels[0][0] == val\n\n\n# ----------------------------------------------------------------------------\n# from_arrays\n# ----------------------------------------------------------------------------\ndef test_from_arrays(idx):\n    arrays = [\n        np.asarray(lev).take(level_codes)\n        for lev, level_codes in zip(idx.levels, idx.codes)\n    ]\n\n    # list of arrays as input\n    result = MultiIndex.from_arrays(arrays, names=idx.names)\n    tm.assert_index_equal(result, idx)\n\n    # infer correctly\n    result = MultiIndex.from_arrays([[pd.NaT, Timestamp(\"20130101\")], [\"a\", \"b\"]])\n    assert result.levels[0].equals(Index([Timestamp(\"20130101\")]))\n    assert result.levels[1].equals(Index([\"a\", \"b\"]))\n\n\ndef test_from_arrays_iterator(idx):\n    # GH 18434\n    arrays = [\n        np.asarray(lev).take(level_codes)\n        for lev, level_codes in zip(idx.levels, idx.codes)\n    ]\n\n    # iterator as input\n    result = MultiIndex.from_arrays(iter(arrays), names=idx.names)\n    tm.assert_index_equal(result, idx)\n\n    # invalid iterator input\n    msg = \"Input must be a list / sequence of array-likes.\"\n    with pytest.raises(TypeError, match=msg):\n        MultiIndex.from_arrays(0)\n\n\ndef test_from_arrays_tuples(idx):\n    arrays = tuple(\n        tuple(np.asarray(lev).take(level_codes))\n        for lev, level_codes in zip(idx.levels, idx.codes)\n    )\n\n    # tuple of tuples as input\n    result = MultiIndex.from_arrays(arrays, names=idx.names)\n    tm.assert_index_equal(result, idx)\n\n\n@pytest.mark.parametrize(\n    (\"idx1\", \"idx2\"),\n    [\n        (\n            pd.period_range(\"2011-01-01\", freq=\"D\", periods=3),\n            pd.period_range(\"2015-01-01\", freq=\"h\", periods=3),\n        ),\n        (\n            date_range(\"2015-01-01 10:00\", freq=\"D\", periods=3, tz=\"US/Eastern\"),\n            date_range(\"2015-01-01 10:00\", freq=\"h\", periods=3, tz=\"Asia/Tokyo\"),\n        ),\n        (\n            pd.timedelta_range(\"1 days\", freq=\"D\", periods=3),\n            pd.timedelta_range(\"2 hours\", freq=\"h\", periods=3),\n        ),\n    ],\n)\ndef test_from_arrays_index_series_period_datetimetz_and_timedelta(idx1, idx2):\n    result = MultiIndex.from_arrays([idx1, idx2])\n    tm.assert_index_equal(result.get_level_values(0), idx1)\n    tm.assert_index_equal(result.get_level_values(1), idx2)\n\n    result2 = MultiIndex.from_arrays([Series(idx1), Series(idx2)])\n    tm.assert_index_equal(result2.get_level_values(0), idx1)\n    tm.assert_index_equal(result2.get_level_values(1), idx2)\n\n    tm.assert_index_equal(result, result2)\n\n\ndef test_from_arrays_index_datetimelike_mixed():\n    idx1 = date_range(\"2015-01-01 10:00\", freq=\"D\", periods=3, tz=\"US/Eastern\")\n    idx2 = date_range(\"2015-01-01 10:00\", freq=\"h\", periods=3)\n    idx3 = pd.timedelta_range(\"1 days\", freq=\"D\", periods=3)\n    idx4 = pd.period_range(\"2011-01-01\", freq=\"D\", periods=3)\n\n    result = MultiIndex.from_arrays([idx1, idx2, idx3, idx4])\n    tm.assert_index_equal(result.get_level_values(0), idx1)\n    tm.assert_index_equal(result.get_level_values(1), idx2)\n    tm.assert_index_equal(result.get_level_values(2), idx3)\n    tm.assert_index_equal(result.get_level_values(3), idx4)\n\n    result2 = MultiIndex.from_arrays(\n        [Series(idx1), Series(idx2), Series(idx3), Series(idx4)]\n    )\n    tm.assert_index_equal(result2.get_level_values(0), idx1)\n    tm.assert_index_equal(result2.get_level_values(1), idx2)\n    tm.assert_index_equal(result2.get_level_values(2), idx3)\n    tm.assert_index_equal(result2.get_level_values(3), idx4)\n\n    tm.assert_index_equal(result, result2)\n\n\ndef test_from_arrays_index_series_categorical():\n    # GH13743\n    idx1 = pd.CategoricalIndex(list(\"abcaab\"), categories=list(\"bac\"), ordered=False)\n    idx2 = pd.CategoricalIndex(list(\"abcaab\"), categories=list(\"bac\"), ordered=True)\n\n    result = MultiIndex.from_arrays([idx1, idx2])\n    tm.assert_index_equal(result.get_level_values(0), idx1)\n    tm.assert_index_equal(result.get_level_values(1), idx2)\n\n    result2 = MultiIndex.from_arrays([Series(idx1), Series(idx2)])\n    tm.assert_index_equal(result2.get_level_values(0), idx1)\n    tm.assert_index_equal(result2.get_level_values(1), idx2)\n\n    result3 = MultiIndex.from_arrays([idx1.values, idx2.values])\n    tm.assert_index_equal(result3.get_level_values(0), idx1)\n    tm.assert_index_equal(result3.get_level_values(1), idx2)\n\n\ndef test_from_arrays_empty():\n    # 0 levels\n    msg = \"Must pass non-zero number of levels/codes\"\n    with pytest.raises(ValueError, match=msg):\n        MultiIndex.from_arrays(arrays=[])\n\n    # 1 level\n    result = MultiIndex.from_arrays(arrays=[[]], names=[\"A\"])\n    assert isinstance(result, MultiIndex)\n    expected = Index([], name=\"A\")\n    tm.assert_index_equal(result.levels[0], expected)\n    assert result.names == [\"A\"]\n\n    # N levels\n    for N in [2, 3]:\n        arrays = [[]] * N\n        names = list(\"ABC\")[:N]\n        result = MultiIndex.from_arrays(arrays=arrays, names=names)\n        expected = MultiIndex(levels=[[]] * N, codes=[[]] * N, names=names)\n        tm.assert_index_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"invalid_sequence_of_arrays\",\n    [\n        1,\n        [1],\n        [1, 2],\n        [[1], 2],\n        [1, [2]],\n        \"a\",\n        [\"a\"],\n        [\"a\", \"b\"],\n        [[\"a\"], \"b\"],\n        (1,),\n        (1, 2),\n        ([1], 2),\n        (1, [2]),\n        (\"a\",),\n        (\"a\", \"b\"),\n        ([\"a\"], \"b\"),\n        [(1,), 2],\n        [1, (2,)],\n        [(\"a\",), \"b\"],\n        ((1,), 2),\n        (1, (2,)),\n        ((\"a\",), \"b\"),\n    ],\n)\ndef test_from_arrays_invalid_input(invalid_sequence_of_arrays):\n    msg = \"Input must be a list / sequence of array-likes\"\n    with pytest.raises(TypeError, match=msg):\n        MultiIndex.from_arrays(arrays=invalid_sequence_of_arrays)\n\n\n@pytest.mark.parametrize(\n    \"idx1, idx2\", [([1, 2, 3], [\"a\", \"b\"]), ([], [\"a\", \"b\"]), ([1, 2, 3], [])]\n)\ndef test_from_arrays_different_lengths(idx1, idx2):\n    # see gh-13599\n    msg = \"^all arrays must be same length$\"\n    with pytest.raises(ValueError, match=msg):\n        MultiIndex.from_arrays([idx1, idx2])\n\n\ndef test_from_arrays_respects_none_names():\n    # GH27292\n    a = Series([1, 2, 3], name=\"foo\")\n    b = Series([\"a\", \"b\", \"c\"], name=\"bar\")\n\n    result = MultiIndex.from_arrays([a, b], names=None)\n    expected = MultiIndex(\n        levels=[[1, 2, 3], [\"a\", \"b\", \"c\"]], codes=[[0, 1, 2], [0, 1, 2]], names=None\n    )\n\n    tm.assert_index_equal(result, expected)\n\n\n# ----------------------------------------------------------------------------\n# from_tuples\n# ----------------------------------------------------------------------------\ndef test_from_tuples():\n    msg = \"Cannot infer number of levels from empty list\"\n    with pytest.raises(TypeError, match=msg):\n        MultiIndex.from_tuples([])\n\n    expected = MultiIndex(\n        levels=[[1, 3], [2, 4]], codes=[[0, 1], [0, 1]], names=[\"a\", \"b\"]\n    )\n\n    # input tuples\n    result = MultiIndex.from_tuples(((1, 2), (3, 4)), names=[\"a\", \"b\"])\n    tm.assert_index_equal(result, expected)\n\n\ndef test_from_tuples_iterator():\n    # GH 18434\n    # input iterator for tuples\n    expected = MultiIndex(\n        levels=[[1, 3], [2, 4]], codes=[[0, 1], [0, 1]], names=[\"a\", \"b\"]\n    )\n\n    result = MultiIndex.from_tuples(zip([1, 3], [2, 4]), names=[\"a\", \"b\"])\n    tm.assert_index_equal(result, expected)\n\n    # input non-iterables\n    msg = \"Input must be a list / sequence of tuple-likes.\"\n    with pytest.raises(TypeError, match=msg):\n        MultiIndex.from_tuples(0)\n\n\ndef test_from_tuples_empty():\n    # GH 16777\n    result = MultiIndex.from_tuples([], names=[\"a\", \"b\"])\n    expected = MultiIndex.from_arrays(arrays=[[], []], names=[\"a\", \"b\"])\n    tm.assert_index_equal(result, expected)\n\n\ndef test_from_tuples_index_values(idx):\n    result = MultiIndex.from_tuples(idx)\n    assert (result.values == idx.values).all()\n\n\ndef test_tuples_with_name_string():\n    # GH 15110 and GH 14848\n\n    li = [(0, 0, 1), (0, 1, 0), (1, 0, 0)]\n    msg = \"Names should be list-like for a MultiIndex\"\n    with pytest.raises(ValueError, match=msg):\n        Index(li, name=\"abc\")\n    with pytest.raises(ValueError, match=msg):\n        Index(li, name=\"a\")\n\n\ndef test_from_tuples_with_tuple_label():\n    # GH 15457\n    expected = pd.DataFrame(\n        [[2, 1, 2], [4, (1, 2), 3]], columns=[\"a\", \"b\", \"c\"]\n    ).set_index([\"a\", \"b\"])\n    idx = MultiIndex.from_tuples([(2, 1), (4, (1, 2))], names=(\"a\", \"b\"))\n    result = pd.DataFrame([2, 3], columns=[\"c\"], index=idx)\n    tm.assert_frame_equal(expected, result)\n\n\n@pytest.mark.parametrize(\n    \"keys, expected\",\n    [\n        (((\"l1\",), (\"l1\", \"l2\")), ((\"l1\", np.nan), (\"l1\", \"l2\"))),\n        (((\"l1\", \"l2\"), (\"l1\",)), ((\"l1\", \"l2\"), (\"l1\", np.nan))),\n    ],\n)\ndef test_from_tuples_with_various_tuple_lengths(keys, expected):\n    # GH 60695\n    idx = MultiIndex.from_tuples(keys)\n    assert tuple(idx) == expected\n\n\n# ----------------------------------------------------------------------------\n# from_product\n# ----------------------------------------------------------------------------\ndef test_from_product_empty_zero_levels():\n    # 0 levels\n    msg = \"Must pass non-zero number of levels/codes\"\n    with pytest.raises(ValueError, match=msg):\n        MultiIndex.from_product([])\n\n\ndef test_from_product_empty_one_level():\n    result = MultiIndex.from_product([[]], names=[\"A\"])\n    expected = Index([], name=\"A\")\n    tm.assert_index_equal(result.levels[0], expected)\n    assert result.names == [\"A\"]\n\n\n@pytest.mark.parametrize(\n    \"first, second\", [([], []), ([\"foo\", \"bar\", \"baz\"], []), ([], [\"a\", \"b\", \"c\"])]\n)\ndef test_from_product_empty_two_levels(first, second):\n    names = [\"A\", \"B\"]\n    result = MultiIndex.from_product([first, second], names=names)\n    expected = MultiIndex(levels=[first, second], codes=[[], []], names=names)\n    tm.assert_index_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"N\", list(range(4)))\ndef test_from_product_empty_three_levels(N):\n    # GH12258\n    names = [\"A\", \"B\", \"C\"]\n    lvl2 = list(range(N))\n    result = MultiIndex.from_product([[], lvl2, []], names=names)\n    expected = MultiIndex(levels=[[], lvl2, []], codes=[[], [], []], names=names)\n    tm.assert_index_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"invalid_input\", [1, [1], [1, 2], [[1], 2], \"a\", [\"a\"], [\"a\", \"b\"], [[\"a\"], \"b\"]]\n)\ndef test_from_product_invalid_input(invalid_input):\n    msg = r\"Input must be a list / sequence of iterables|Input must be list-like\"\n    with pytest.raises(TypeError, match=msg):\n        MultiIndex.from_product(iterables=invalid_input)\n\n\ndef test_from_product_datetimeindex():\n    dt_index = date_range(\"2000-01-01\", periods=2)\n    mi = MultiIndex.from_product([[1, 2], dt_index])\n    etalon = construct_1d_object_array_from_listlike(\n        [\n            (1, Timestamp(\"2000-01-01\")),\n            (1, Timestamp(\"2000-01-02\")),\n            (2, Timestamp(\"2000-01-01\")),\n            (2, Timestamp(\"2000-01-02\")),\n        ]\n    )\n    tm.assert_numpy_array_equal(mi.values, etalon)\n\n\ndef test_from_product_rangeindex():\n    # RangeIndex is preserved by factorize, so preserved in levels\n    rng = Index(range(5))\n    other = [\"a\", \"b\"]\n    mi = MultiIndex.from_product([rng, other])\n    tm.assert_index_equal(mi._levels[0], rng, exact=True)\n\n\n@pytest.mark.parametrize(\"ordered\", [False, True])\n@pytest.mark.parametrize(\"f\", [lambda x: x, lambda x: Series(x), lambda x: x.values])\ndef test_from_product_index_series_categorical(ordered, f):\n    # GH13743\n    first = [\"foo\", \"bar\"]\n\n    idx = pd.CategoricalIndex(list(\"abcaab\"), categories=list(\"bac\"), ordered=ordered)\n    expected = pd.CategoricalIndex(\n        list(\"abcaab\") + list(\"abcaab\"), categories=list(\"bac\"), ordered=ordered\n    )\n\n    result = MultiIndex.from_product([first, f(idx)])\n    tm.assert_index_equal(result.get_level_values(1), expected)\n\n\ndef test_from_product():\n    first = [\"foo\", \"bar\", \"buz\"]\n    second = [\"a\", \"b\", \"c\"]\n    names = [\"first\", \"second\"]\n    result = MultiIndex.from_product([first, second], names=names)\n\n    tuples = [\n        (\"foo\", \"a\"),\n        (\"foo\", \"b\"),\n        (\"foo\", \"c\"),\n        (\"bar\", \"a\"),\n        (\"bar\", \"b\"),\n        (\"bar\", \"c\"),\n        (\"buz\", \"a\"),\n        (\"buz\", \"b\"),\n        (\"buz\", \"c\"),\n    ]\n    expected = MultiIndex.from_tuples(tuples, names=names)\n\n    tm.assert_index_equal(result, expected)\n\n\ndef test_from_product_iterator():\n    # GH 18434\n    first = [\"foo\", \"bar\", \"buz\"]\n    second = [\"a\", \"b\", \"c\"]\n    names = [\"first\", \"second\"]\n    tuples = [\n        (\"foo\", \"a\"),\n        (\"foo\", \"b\"),\n        (\"foo\", \"c\"),\n        (\"bar\", \"a\"),\n        (\"bar\", \"b\"),\n        (\"bar\", \"c\"),\n        (\"buz\", \"a\"),\n        (\"buz\", \"b\"),\n        (\"buz\", \"c\"),\n    ]\n    expected = MultiIndex.from_tuples(tuples, names=names)\n\n    # iterator as input\n    result = MultiIndex.from_product(iter([first, second]), names=names)\n    tm.assert_index_equal(result, expected)\n\n    # Invalid non-iterable input\n    msg = \"Input must be a list / sequence of iterables.\"\n    with pytest.raises(TypeError, match=msg):\n        MultiIndex.from_product(0)\n\n\n@pytest.mark.parametrize(\n    \"a, b, expected_names\",\n    [\n        (\n            Series([1, 2, 3], name=\"foo\"),\n            Series([\"a\", \"b\"], name=\"bar\"),\n            [\"foo\", \"bar\"],\n        ),\n        (Series([1, 2, 3], name=\"foo\"), [\"a\", \"b\"], [\"foo\", None]),\n        ([1, 2, 3], [\"a\", \"b\"], None),\n    ],\n)\ndef test_from_product_infer_names(a, b, expected_names):\n    # GH27292\n    result = MultiIndex.from_product([a, b])\n    expected = MultiIndex(\n        levels=[[1, 2, 3], [\"a\", \"b\"]],\n        codes=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]],\n        names=expected_names,\n    )\n    tm.assert_index_equal(result, expected)\n\n\ndef test_from_product_respects_none_names():\n    # GH27292\n    a = Series([1, 2, 3], name=\"foo\")\n    b = Series([\"a\", \"b\"], name=\"bar\")\n\n    result = MultiIndex.from_product([a, b], names=None)\n    expected = MultiIndex(\n        levels=[[1, 2, 3], [\"a\", \"b\"]],\n        codes=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]],\n        names=None,\n    )\n    tm.assert_index_equal(result, expected)\n\n\ndef test_from_product_readonly():\n    # GH#15286 passing read-only array to from_product\n    a = np.array(range(3))\n    b = [\"a\", \"b\"]\n    expected = MultiIndex.from_product([a, b])\n\n    a.setflags(write=False)\n    result = MultiIndex.from_product([a, b])\n    tm.assert_index_equal(result, expected)\n\n\ndef test_create_index_existing_name(idx):\n    # GH11193, when an existing index is passed, and a new name is not\n    # specified, the new index should inherit the previous object name\n    index = idx\n    index.names = [\"foo\", \"bar\"]\n    result = Index(index)\n    expected = Index(\n        Index(\n            [\n                (\"foo\", \"one\"),\n                (\"foo\", \"two\"),\n                (\"bar\", \"one\"),\n                (\"baz\", \"two\"),\n                (\"qux\", \"one\"),\n                (\"qux\", \"two\"),\n            ],\n            dtype=\"object\",\n        )\n    )\n    tm.assert_index_equal(result, expected)\n\n    result = Index(index, name=\"A\")\n    expected = Index(\n        Index(\n            [\n                (\"foo\", \"one\"),\n                (\"foo\", \"two\"),\n                (\"bar\", \"one\"),\n                (\"baz\", \"two\"),\n                (\"qux\", \"one\"),\n                (\"qux\", \"two\"),\n            ],\n            dtype=\"object\",\n        ),\n        name=\"A\",\n    )\n    tm.assert_index_equal(result, expected)\n\n\n# ----------------------------------------------------------------------------\n# from_frame\n# ----------------------------------------------------------------------------\ndef test_from_frame():\n    # GH 22420\n    df = pd.DataFrame(\n        [[\"a\", \"a\"], [\"a\", \"b\"], [\"b\", \"a\"], [\"b\", \"b\"]], columns=[\"L1\", \"L2\"]\n    )\n    expected = MultiIndex.from_tuples(\n        [(\"a\", \"a\"), (\"a\", \"b\"), (\"b\", \"a\"), (\"b\", \"b\")], names=[\"L1\", \"L2\"]\n    )\n    result = MultiIndex.from_frame(df)\n    tm.assert_index_equal(expected, result)\n\n\ndef test_from_frame_missing_values_multiIndex():\n    # GH 39984\n    pa = pytest.importorskip(\"pyarrow\")\n\n    df = pd.DataFrame(\n        {\n            \"a\": Series([1, 2, None], dtype=\"Int64\"),\n            \"b\": pd.Float64Dtype().__from_arrow__(pa.array([0.2, np.nan, None])),\n        }\n    )\n    multi_indexed = MultiIndex.from_frame(df)\n    expected = MultiIndex.from_arrays(\n        [\n            Series([1, 2, None]).astype(\"Int64\"),\n            pd.Float64Dtype().__from_arrow__(pa.array([0.2, np.nan, None])),\n        ],\n        names=[\"a\", \"b\"],\n    )\n    tm.assert_index_equal(multi_indexed, expected)\n\n\n@pytest.mark.parametrize(\n    \"non_frame\",\n    [\n        Series([1, 2, 3, 4]),\n        [1, 2, 3, 4],\n        [[1, 2], [3, 4], [5, 6]],\n        Index([1, 2, 3, 4]),\n        np.array([[1, 2], [3, 4], [5, 6]]),\n        27,\n    ],\n)\ndef test_from_frame_error(non_frame):\n    # GH 22420\n    with pytest.raises(TypeError, match=\"Input must be a DataFrame\"):\n        MultiIndex.from_frame(non_frame)\n\n\ndef test_from_frame_dtype_fidelity():\n    # GH 22420\n    df = pd.DataFrame(\n        {\n            \"dates\": date_range(\"19910905\", periods=6, tz=\"US/Eastern\"),\n            \"a\": [1, 1, 1, 2, 2, 2],\n            \"b\": pd.Categorical([\"a\", \"a\", \"b\", \"b\", \"c\", \"c\"], ordered=True),\n            \"c\": [\"x\", \"x\", \"y\", \"z\", \"x\", \"y\"],\n        }\n    )\n    original_dtypes = df.dtypes.to_dict()\n\n    expected_mi = MultiIndex.from_arrays(\n        [\n            date_range(\"19910905\", periods=6, tz=\"US/Eastern\"),\n            [1, 1, 1, 2, 2, 2],\n            pd.Categorical([\"a\", \"a\", \"b\", \"b\", \"c\", \"c\"], ordered=True),\n            [\"x\", \"x\", \"y\", \"z\", \"x\", \"y\"],\n        ],\n        names=[\"dates\", \"a\", \"b\", \"c\"],\n    )\n    mi = MultiIndex.from_frame(df)\n    mi_dtypes = {name: mi.levels[i].dtype for i, name in enumerate(mi.names)}\n\n    tm.assert_index_equal(expected_mi, mi)\n    assert original_dtypes == mi_dtypes\n\n\n@pytest.mark.parametrize(\n    \"names_in,names_out\", [(None, [(\"L1\", \"x\"), (\"L2\", \"y\")]), ([\"x\", \"y\"], [\"x\", \"y\"])]\n)\ndef test_from_frame_valid_names(names_in, names_out):\n    # GH 22420\n    df = pd.DataFrame(\n        [[\"a\", \"a\"], [\"a\", \"b\"], [\"b\", \"a\"], [\"b\", \"b\"]],\n        columns=MultiIndex.from_tuples([(\"L1\", \"x\"), (\"L2\", \"y\")]),\n    )\n    mi = MultiIndex.from_frame(df, names=names_in)\n    assert mi.names == names_out\n\n\n@pytest.mark.parametrize(\n    \"names,expected_error_msg\",\n    [\n        (\"bad_input\", \"Names should be list-like for a MultiIndex\"),\n        ([\"a\", \"b\", \"c\"], \"Length of names must match number of levels in MultiIndex\"),\n    ],\n)\ndef test_from_frame_invalid_names(names, expected_error_msg):\n    # GH 22420\n    df = pd.DataFrame(\n        [[\"a\", \"a\"], [\"a\", \"b\"], [\"b\", \"a\"], [\"b\", \"b\"]],\n        columns=MultiIndex.from_tuples([(\"L1\", \"x\"), (\"L2\", \"y\")]),\n    )\n    with pytest.raises(ValueError, match=expected_error_msg):\n        MultiIndex.from_frame(df, names=names)\n\n\ndef test_index_equal_empty_iterable():\n    # #16844\n    a = MultiIndex(levels=[[], []], codes=[[], []], names=[\"a\", \"b\"])\n    b = MultiIndex.from_arrays(arrays=[[], []], names=[\"a\", \"b\"])\n    tm.assert_index_equal(a, b)\n\n\ndef test_raise_invalid_sortorder():\n    # Test that the MultiIndex constructor raise when a incorrect sortorder is given\n    # GH#28518\n\n    levels = [[0, 1], [0, 1, 2]]\n\n    # Correct sortorder\n    MultiIndex(\n        levels=levels, codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]], sortorder=2\n    )\n\n    with pytest.raises(ValueError, match=r\".* sortorder 2 with lexsort_depth 1.*\"):\n        MultiIndex(\n            levels=levels, codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 2, 1]], sortorder=2\n        )\n\n    with pytest.raises(ValueError, match=r\".* sortorder 1 with lexsort_depth 0.*\"):\n        MultiIndex(\n            levels=levels, codes=[[0, 0, 1, 0, 1, 1], [0, 1, 0, 2, 2, 1]], sortorder=1\n        )\n\n\ndef test_datetimeindex():\n    idx1 = pd.DatetimeIndex(\n        [\"2013-04-01 9:00\", \"2013-04-02 9:00\", \"2013-04-03 9:00\"] * 2, tz=\"Asia/Tokyo\"\n    )\n    idx2 = date_range(\"2010/01/01\", periods=6, freq=\"ME\", tz=\"US/Eastern\")\n    idx = MultiIndex.from_arrays([idx1, idx2])\n\n    expected1 = pd.DatetimeIndex(\n        [\"2013-04-01 9:00\", \"2013-04-02 9:00\", \"2013-04-03 9:00\"], tz=\"Asia/Tokyo\"\n    )\n\n    tm.assert_index_equal(idx.levels[0], expected1)\n    tm.assert_index_equal(idx.levels[1], idx2)\n\n    # from datetime combos\n    # GH 7888\n    date1 = np.datetime64(\"today\")\n    date2 = datetime.today()\n    date3 = Timestamp.today()\n\n    for d1, d2 in itertools.product([date1, date2, date3], [date1, date2, date3]):\n        index = MultiIndex.from_product([[d1], [d2]])\n        assert isinstance(index.levels[0], pd.DatetimeIndex)\n        assert isinstance(index.levels[1], pd.DatetimeIndex)\n\n    # but NOT date objects, matching Index behavior\n    date4 = date.today()\n    index = MultiIndex.from_product([[date4], [date2]])\n    assert not isinstance(index.levels[0], pd.DatetimeIndex)\n    assert isinstance(index.levels[1], pd.DatetimeIndex)\n\n\ndef test_constructor_with_tz():\n    index = pd.DatetimeIndex(\n        [\"2013/01/01 09:00\", \"2013/01/02 09:00\"], name=\"dt1\", tz=\"US/Pacific\"\n    )\n    columns = pd.DatetimeIndex(\n        [\"2014/01/01 09:00\", \"2014/01/02 09:00\"], name=\"dt2\", tz=\"Asia/Tokyo\"\n    )\n\n    result = MultiIndex.from_arrays([index, columns])\n\n    assert result.names == [\"dt1\", \"dt2\"]\n    tm.assert_index_equal(result.levels[0], index)\n    tm.assert_index_equal(result.levels[1], columns)\n\n    result = MultiIndex.from_arrays([Series(index), Series(columns)])\n\n    assert result.names == [\"dt1\", \"dt2\"]\n    tm.assert_index_equal(result.levels[0], index)\n    tm.assert_index_equal(result.levels[1], columns)\n\n\ndef test_multiindex_inference_consistency():\n    # check that inference behavior matches the base class\n\n    v = date.today()\n\n    arr = [v, v]\n\n    idx = Index(arr)\n    assert idx.dtype == object\n\n    mi = MultiIndex.from_arrays([arr])\n    lev = mi.levels[0]\n    assert lev.dtype == object\n\n    mi = MultiIndex.from_product([arr])\n    lev = mi.levels[0]\n    assert lev.dtype == object\n\n    mi = MultiIndex.from_tuples([(x,) for x in arr])\n    lev = mi.levels[0]\n    assert lev.dtype == object\n\n\ndef test_dtype_representation(using_infer_string):\n    # GH#46900\n    pmidx = MultiIndex.from_arrays([[1], [\"a\"]], names=[(\"a\", \"b\"), (\"c\", \"d\")])\n    result = pmidx.dtypes\n    exp = \"object\" if not using_infer_string else pd.StringDtype(na_value=np.nan)\n    expected = Series(\n        [\"int64\", exp],\n        index=MultiIndex.from_tuples([(\"a\", \"b\"), (\"c\", \"d\")]),\n        dtype=object,\n    )\n    tm.assert_series_equal(result, expected)\n"
    },
    {
      "filename": "pandas/tests/series/test_constructors.py",
      "content": "from collections import OrderedDict\nfrom collections.abc import Iterator\nfrom datetime import (\n    datetime,\n    timedelta,\n)\n\nfrom dateutil.tz import tzoffset\nimport numpy as np\nfrom numpy import ma\nimport pytest\n\nfrom pandas._libs import (\n    iNaT,\n    lib,\n)\nfrom pandas.compat import HAS_PYARROW\nfrom pandas.compat.numpy import np_version_gt2\nfrom pandas.errors import IntCastingNaNError\n\nfrom pandas.core.dtypes.dtypes import CategoricalDtype\n\nimport pandas as pd\nfrom pandas import (\n    Categorical,\n    DataFrame,\n    DatetimeIndex,\n    DatetimeTZDtype,\n    Index,\n    Interval,\n    IntervalIndex,\n    MultiIndex,\n    NaT,\n    Period,\n    RangeIndex,\n    Series,\n    Timestamp,\n    date_range,\n    isna,\n    period_range,\n    timedelta_range,\n)\nimport pandas._testing as tm\nfrom pandas.core.arrays import (\n    IntegerArray,\n    IntervalArray,\n    period_array,\n)\nfrom pandas.core.internals.blocks import NumpyBlock\n\n\nclass TestSeriesConstructors:\n    def test_from_ints_with_non_nano_dt64_dtype(self, index_or_series):\n        values = np.arange(10)\n\n        res = index_or_series(values, dtype=\"M8[s]\")\n        expected = index_or_series(values.astype(\"M8[s]\"))\n        tm.assert_equal(res, expected)\n\n        res = index_or_series(list(values), dtype=\"M8[s]\")\n        tm.assert_equal(res, expected)\n\n    def test_from_na_value_and_interval_of_datetime_dtype(self):\n        # GH#41805\n        ser = Series([None], dtype=\"interval[datetime64[ns]]\")\n        assert ser.isna().all()\n        assert ser.dtype == \"interval[datetime64[ns], right]\"\n\n    def test_infer_with_date_and_datetime(self):\n        # GH#49341 pre-2.0 we inferred datetime-and-date to datetime64, which\n        #  was inconsistent with Index behavior\n        ts = Timestamp(2016, 1, 1)\n        vals = [ts.to_pydatetime(), ts.date()]\n\n        ser = Series(vals)\n        expected = Series(vals, dtype=object)\n        tm.assert_series_equal(ser, expected)\n\n        idx = Index(vals)\n        expected = Index(vals, dtype=object)\n        tm.assert_index_equal(idx, expected)\n\n    def test_unparsable_strings_with_dt64_dtype(self):\n        # pre-2.0 these would be silently ignored and come back with object dtype\n        vals = [\"aa\"]\n        msg = \"^Unknown datetime string format, unable to parse: aa$\"\n        with pytest.raises(ValueError, match=msg):\n            Series(vals, dtype=\"datetime64[ns]\")\n\n        with pytest.raises(ValueError, match=msg):\n            Series(np.array(vals, dtype=object), dtype=\"datetime64[ns]\")\n\n    def test_invalid_dtype_conversion_datetime_to_timedelta(self):\n        # GH#60728\n        vals = Series([NaT, Timestamp(2025, 1, 1)], dtype=\"datetime64[ns]\")\n        msg = r\"^Cannot cast DatetimeArray to dtype timedelta64\\[ns\\]$\"\n        with pytest.raises(TypeError, match=msg):\n            Series(vals, dtype=\"timedelta64[ns]\")\n\n    @pytest.mark.parametrize(\n        \"constructor\",\n        [\n            # NOTE: some overlap with test_constructor_empty but that test does not\n            # test for None or an empty generator.\n            # test_constructor_pass_none tests None but only with the index also\n            # passed.\n            (lambda idx: Series(index=idx)),\n            (lambda idx: Series(None, index=idx)),\n            (lambda idx: Series({}, index=idx)),\n            (lambda idx: Series((), index=idx)),\n            (lambda idx: Series([], index=idx)),\n            (lambda idx: Series((_ for _ in []), index=idx)),\n            (lambda idx: Series(data=None, index=idx)),\n            (lambda idx: Series(data={}, index=idx)),\n            (lambda idx: Series(data=(), index=idx)),\n            (lambda idx: Series(data=[], index=idx)),\n            (lambda idx: Series(data=(_ for _ in []), index=idx)),\n        ],\n    )\n    @pytest.mark.parametrize(\"empty_index\", [None, []])\n    def test_empty_constructor(self, constructor, empty_index):\n        # GH 49573 (addition of empty_index parameter)\n        expected = Series(index=empty_index)\n        result = constructor(empty_index)\n\n        assert result.dtype == object\n        assert len(result.index) == 0\n        tm.assert_series_equal(result, expected, check_index_type=True)\n\n    def test_invalid_dtype(self):\n        # GH15520\n        msg = \"not understood\"\n        invalid_list = [Timestamp, \"Timestamp\", list]\n        for dtype in invalid_list:\n            with pytest.raises(TypeError, match=msg):\n                Series([], name=\"time\", dtype=dtype)\n\n    def test_invalid_compound_dtype(self):\n        # GH#13296\n        c_dtype = np.dtype([(\"a\", \"i8\"), (\"b\", \"f4\")])\n        cdt_arr = np.array([(1, 0.4), (256, -13)], dtype=c_dtype)\n\n        with pytest.raises(ValueError, match=\"Use DataFrame instead\"):\n            Series(cdt_arr, index=[\"A\", \"B\"])\n\n    def test_scalar_conversion(self):\n        # Pass in scalar is disabled\n        scalar = Series(0.5)\n        assert not isinstance(scalar, float)\n\n    def test_scalar_extension_dtype(self, ea_scalar_and_dtype):\n        # GH 28401\n\n        ea_scalar, ea_dtype = ea_scalar_and_dtype\n\n        ser = Series(ea_scalar, index=range(3))\n        expected = Series([ea_scalar] * 3, dtype=ea_dtype)\n\n        assert ser.dtype == ea_dtype\n        tm.assert_series_equal(ser, expected)\n\n    def test_constructor(self, datetime_series, using_infer_string):\n        empty_series = Series()\n        assert datetime_series.index._is_all_dates\n\n        # Pass in Series\n        derived = Series(datetime_series)\n        assert derived.index._is_all_dates\n\n        tm.assert_index_equal(derived.index, datetime_series.index)\n        # Ensure new index is not created\n        assert id(datetime_series.index) == id(derived.index)\n\n        # Mixed type Series\n        mixed = Series([\"hello\", np.nan], index=[0, 1])\n        assert mixed.dtype == np.object_ if not using_infer_string else \"str\"\n        assert np.isnan(mixed[1])\n\n        assert not empty_series.index._is_all_dates\n        assert not Series().index._is_all_dates\n\n        # exception raised is of type ValueError GH35744\n        with pytest.raises(\n            ValueError,\n            match=r\"Data must be 1-dimensional, got ndarray of shape \\(3, 3\\) instead\",\n        ):\n            Series(np.random.default_rng(2).standard_normal((3, 3)), index=np.arange(3))\n\n        mixed.name = \"Series\"\n        rs = Series(mixed).name\n        xp = \"Series\"\n        assert rs == xp\n\n        # raise on MultiIndex GH4187\n        m = MultiIndex.from_arrays([[1, 2], [3, 4]])\n        msg = \"initializing a Series from a MultiIndex is not supported\"\n        with pytest.raises(NotImplementedError, match=msg):\n            Series(m)\n\n    def test_constructor_index_ndim_gt_1_raises(self):\n        # GH#18579\n        df = DataFrame([[1, 2], [3, 4], [5, 6]], index=[3, 6, 9])\n        with pytest.raises(ValueError, match=\"Index data must be 1-dimensional\"):\n            Series([1, 3, 2], index=df)\n\n    @pytest.mark.parametrize(\"input_class\", [list, dict, OrderedDict])\n    def test_constructor_empty(self, input_class, using_infer_string):\n        empty = Series()\n        empty2 = Series(input_class())\n\n        # these are Index() and RangeIndex() which don't compare type equal\n        # but are just .equals\n        tm.assert_series_equal(empty, empty2, check_index_type=False)\n\n        # With explicit dtype:\n        empty = Series(dtype=\"float64\")\n        empty2 = Series(input_class(), dtype=\"float64\")\n        tm.assert_series_equal(empty, empty2, check_index_type=False)\n\n        # GH 18515 : with dtype=category:\n        empty = Series(dtype=\"category\")\n        empty2 = Series(input_class(), dtype=\"category\")\n        tm.assert_series_equal(empty, empty2, check_index_type=False)\n\n        if input_class is not list:\n            # With index:\n            empty = Series(index=range(10))\n            empty2 = Series(input_class(), index=range(10))\n            tm.assert_series_equal(empty, empty2)\n\n            # With index and dtype float64:\n            empty = Series(np.nan, index=range(10))\n            empty2 = Series(input_class(), index=range(10), dtype=\"float64\")\n            tm.assert_series_equal(empty, empty2)\n\n            # GH 19853 : with empty string, index and dtype str\n            empty = Series(\"\", dtype=str, index=range(3))\n            if using_infer_string:\n                empty2 = Series(\"\", index=range(3), dtype=\"str\")\n            else:\n                empty2 = Series(\"\", index=range(3))\n            tm.assert_series_equal(empty, empty2)\n\n    @pytest.mark.parametrize(\"input_arg\", [np.nan, float(\"nan\")])\n    def test_constructor_nan(self, input_arg):\n        empty = Series(dtype=\"float64\", index=range(10))\n        empty2 = Series(input_arg, index=range(10))\n\n        tm.assert_series_equal(empty, empty2, check_index_type=False)\n\n    @pytest.mark.parametrize(\n        \"dtype\",\n        [\"f8\", \"i8\", \"M8[ns]\", \"m8[ns]\", \"category\", \"object\", \"datetime64[ns, UTC]\"],\n    )\n    @pytest.mark.parametrize(\"index\", [None, Index([])])\n    def test_constructor_dtype_only(self, dtype, index):\n        # GH-20865\n        result = Series(dtype=dtype, index=index)\n        assert result.dtype == dtype\n        assert len(result) == 0\n\n    def test_constructor_no_data_index_order(self):\n        result = Series(index=[\"b\", \"a\", \"c\"])\n        assert result.index.tolist() == [\"b\", \"a\", \"c\"]\n\n    def test_constructor_no_data_string_type(self):\n        # GH 22477\n        result = Series(index=[1], dtype=str)\n        assert np.isnan(result.iloc[0])\n\n    @pytest.mark.parametrize(\"item\", [\"entry\", \"ѐ\", 13])\n    def test_constructor_string_element_string_type(self, item):\n        # GH 22477\n        result = Series(item, index=[1], dtype=str)\n        assert result.iloc[0] == str(item)\n\n    def test_constructor_dtype_str_na_values(self, string_dtype):\n        # https://github.com/pandas-dev/pandas/issues/21083\n        ser = Series([\"x\", None], dtype=string_dtype)\n        result = ser.isna()\n        expected = Series([False, True])\n        tm.assert_series_equal(result, expected)\n        assert ser.iloc[1] is None\n\n        ser = Series([\"x\", np.nan], dtype=string_dtype)\n        assert np.isnan(ser.iloc[1])\n\n    def test_constructor_series(self):\n        index1 = [\"d\", \"b\", \"a\", \"c\"]\n        index2 = sorted(index1)\n        s1 = Series([4, 7, -5, 3], index=index1)\n        s2 = Series(s1, index=index2)\n\n        tm.assert_series_equal(s2, s1.sort_index())\n\n    def test_constructor_iterable(self):\n        # GH 21987\n        class Iter:\n            def __iter__(self) -> Iterator:\n                yield from range(10)\n\n        expected = Series(list(range(10)), dtype=\"int64\")\n        result = Series(Iter(), dtype=\"int64\")\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_sequence(self):\n        # GH 21987\n        expected = Series(list(range(10)), dtype=\"int64\")\n        result = Series(range(10), dtype=\"int64\")\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_single_str(self):\n        # GH 21987\n        expected = Series([\"abc\"])\n        result = Series(\"abc\")\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_list_like(self):\n        # make sure that we are coercing different\n        # list-likes to standard dtypes and not\n        # platform specific\n        expected = Series([1, 2, 3], dtype=\"int64\")\n        for obj in [[1, 2, 3], (1, 2, 3), np.array([1, 2, 3], dtype=\"int64\")]:\n            result = Series(obj, index=[0, 1, 2])\n            tm.assert_series_equal(result, expected)\n\n    def test_constructor_boolean_index(self):\n        # GH#18579\n        s1 = Series([1, 2, 3], index=[4, 5, 6])\n\n        index = s1 == 2\n        result = Series([1, 3, 2], index=index)\n        expected = Series([1, 3, 2], index=[False, True, False])\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"dtype\", [\"bool\", \"int32\", \"int64\", \"float64\"])\n    def test_constructor_index_dtype(self, dtype):\n        # GH 17088\n\n        s = Series(Index([0, 2, 4]), dtype=dtype)\n        assert s.dtype == dtype\n\n    @pytest.mark.parametrize(\n        \"input_vals\",\n        [\n            [1, 2],\n            [\"1\", \"2\"],\n            list(date_range(\"1/1/2011\", periods=2, freq=\"h\")),\n            list(date_range(\"1/1/2011\", periods=2, freq=\"h\", tz=\"US/Eastern\")),\n            [Interval(left=0, right=5)],\n        ],\n    )\n    def test_constructor_list_str(self, input_vals, string_dtype):\n        # GH 16605\n        # Ensure that data elements from a list are converted to strings\n        # when dtype is str, 'str', or 'U'\n        result = Series(input_vals, dtype=string_dtype)\n        expected = Series(input_vals).astype(string_dtype)\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_list_str_na(self, string_dtype):\n        result = Series([1.0, 2.0, np.nan], dtype=string_dtype)\n        expected = Series([\"1.0\", \"2.0\", np.nan], dtype=object)\n        tm.assert_series_equal(result, expected)\n        assert np.isnan(result[2])\n\n    def test_constructor_generator(self):\n        gen = (i for i in range(10))\n\n        result = Series(gen)\n        exp = Series(range(10))\n        tm.assert_series_equal(result, exp)\n\n        # same but with non-default index\n        gen = (i for i in range(10))\n        result = Series(gen, index=range(10, 20))\n        exp.index = range(10, 20)\n        tm.assert_series_equal(result, exp)\n\n    def test_constructor_map(self):\n        # GH8909\n        m = (x for x in range(10))\n\n        result = Series(m)\n        exp = Series(range(10))\n        tm.assert_series_equal(result, exp)\n\n        # same but with non-default index\n        m = (x for x in range(10))\n        result = Series(m, index=range(10, 20))\n        exp.index = range(10, 20)\n        tm.assert_series_equal(result, exp)\n\n    def test_constructor_categorical(self):\n        cat = Categorical([0, 1, 2, 0, 1, 2], [\"a\", \"b\", \"c\"])\n        res = Series(cat)\n        tm.assert_categorical_equal(res.values, cat)\n\n        # can cast to a new dtype\n        result = Series(Categorical([1, 2, 3]), dtype=\"int64\")\n        expected = Series([1, 2, 3], dtype=\"int64\")\n        tm.assert_series_equal(result, expected)\n\n    def test_construct_from_categorical_with_dtype(self):\n        # GH12574\n        ser = Series(Categorical([1, 2, 3]), dtype=\"category\")\n        assert isinstance(ser.dtype, CategoricalDtype)\n\n    def test_construct_intlist_values_category_dtype(self):\n        ser = Series([1, 2, 3], dtype=\"category\")\n        assert isinstance(ser.dtype, CategoricalDtype)\n\n    def test_constructor_categorical_with_coercion(self):\n        factor = Categorical([\"a\", \"b\", \"b\", \"a\", \"a\", \"c\", \"c\", \"c\"])\n        # test basic creation / coercion of categoricals\n        s = Series(factor, name=\"A\")\n        assert s.dtype == \"category\"\n        assert len(s) == len(factor)\n\n        # in a frame\n        df = DataFrame({\"A\": factor})\n        result = df[\"A\"]\n        tm.assert_series_equal(result, s)\n        result = df.iloc[:, 0]\n        tm.assert_series_equal(result, s)\n        assert len(df) == len(factor)\n\n        df = DataFrame({\"A\": s})\n        result = df[\"A\"]\n        tm.assert_series_equal(result, s)\n        assert len(df) == len(factor)\n\n        # multiples\n        df = DataFrame({\"A\": s, \"B\": s, \"C\": 1})\n        result1 = df[\"A\"]\n        result2 = df[\"B\"]\n        tm.assert_series_equal(result1, s)\n        tm.assert_series_equal(result2, s, check_names=False)\n        assert result2.name == \"B\"\n        assert len(df) == len(factor)\n\n    def test_constructor_categorical_with_coercion2(self):\n        # GH8623\n        x = DataFrame(\n            [[1, \"John P. Doe\"], [2, \"Jane Dove\"], [1, \"John P. Doe\"]],\n            columns=[\"person_id\", \"person_name\"],\n        )\n        x[\"person_name\"] = Categorical(x.person_name)  # doing this breaks transform\n\n        expected = x.iloc[0].person_name\n        result = x.person_name.iloc[0]\n        assert result == expected\n\n        result = x.person_name[0]\n        assert result == expected\n\n        result = x.person_name.loc[0]\n        assert result == expected\n\n    def test_constructor_series_to_categorical(self):\n        # see GH#16524: test conversion of Series to Categorical\n        series = Series([\"a\", \"b\", \"c\"])\n\n        result = Series(series, dtype=\"category\")\n        expected = Series([\"a\", \"b\", \"c\"], dtype=\"category\")\n\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_categorical_dtype(self):\n        result = Series(\n            [\"a\", \"b\"], dtype=CategoricalDtype([\"a\", \"b\", \"c\"], ordered=True)\n        )\n        assert isinstance(result.dtype, CategoricalDtype)\n        tm.assert_index_equal(result.cat.categories, Index([\"a\", \"b\", \"c\"]))\n        assert result.cat.ordered\n\n        result = Series([\"a\", \"b\"], dtype=CategoricalDtype([\"b\", \"a\"]))\n        assert isinstance(result.dtype, CategoricalDtype)\n        tm.assert_index_equal(result.cat.categories, Index([\"b\", \"a\"]))\n        assert result.cat.ordered is False\n\n        # GH 19565 - Check broadcasting of scalar with Categorical dtype\n        result = Series(\n            \"a\", index=[0, 1], dtype=CategoricalDtype([\"a\", \"b\"], ordered=True)\n        )\n        expected = Series(\n            [\"a\", \"a\"], index=[0, 1], dtype=CategoricalDtype([\"a\", \"b\"], ordered=True)\n        )\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_categorical_string(self):\n        # GH 26336: the string 'category' maintains existing CategoricalDtype\n        cdt = CategoricalDtype(categories=list(\"dabc\"), ordered=True)\n        expected = Series(list(\"abcabc\"), dtype=cdt)\n\n        # Series(Categorical, dtype='category') keeps existing dtype\n        cat = Categorical(list(\"abcabc\"), dtype=cdt)\n        result = Series(cat, dtype=\"category\")\n        tm.assert_series_equal(result, expected)\n\n        # Series(Series[Categorical], dtype='category') keeps existing dtype\n        result = Series(result, dtype=\"category\")\n        tm.assert_series_equal(result, expected)\n\n    def test_categorical_sideeffects_free(self):\n        # Passing a categorical to a Series and then changing values in either\n        # the series or the categorical should not change the values in the\n        # other one, IF you specify copy!\n        cat = Categorical([\"a\", \"b\", \"c\", \"a\"])\n        s = Series(cat, copy=True)\n        assert s.cat is not cat\n        s = s.cat.rename_categories([1, 2, 3])\n        exp_s = np.array([1, 2, 3, 1], dtype=np.int64)\n        exp_cat = np.array([\"a\", \"b\", \"c\", \"a\"], dtype=np.object_)\n        tm.assert_numpy_array_equal(s.__array__(), exp_s)\n        tm.assert_numpy_array_equal(cat.__array__(), exp_cat)\n\n        # setting\n        s[0] = 2\n        exp_s2 = np.array([2, 2, 3, 1], dtype=np.int64)\n        tm.assert_numpy_array_equal(s.__array__(), exp_s2)\n        tm.assert_numpy_array_equal(cat.__array__(), exp_cat)\n\n        # however, copy is False by default\n        # so this WILL change values\n        cat = Categorical([\"a\", \"b\", \"c\", \"a\"])\n        s = Series(cat, copy=False)\n        assert s.values is cat\n        s = s.cat.rename_categories([1, 2, 3])\n        assert s.values is not cat\n        exp_s = np.array([1, 2, 3, 1], dtype=np.int64)\n        tm.assert_numpy_array_equal(s.__array__(), exp_s)\n\n        s[0] = 2\n        exp_s2 = np.array([2, 2, 3, 1], dtype=np.int64)\n        tm.assert_numpy_array_equal(s.__array__(), exp_s2)\n\n    def test_unordered_compare_equal(self):\n        left = Series([\"a\", \"b\", \"c\"], dtype=CategoricalDtype([\"a\", \"b\"]))\n        right = Series(Categorical([\"a\", \"b\", np.nan], categories=[\"a\", \"b\"]))\n        tm.assert_series_equal(left, right)\n\n    def test_constructor_maskedarray(self):\n        data = ma.masked_all((3,), dtype=float)\n        result = Series(data)\n        expected = Series([np.nan, np.nan, np.nan])\n        tm.assert_series_equal(result, expected)\n\n        data[0] = 0.0\n        data[2] = 2.0\n        index = [\"a\", \"b\", \"c\"]\n        result = Series(data, index=index)\n        expected = Series([0.0, np.nan, 2.0], index=index)\n        tm.assert_series_equal(result, expected)\n\n        data[1] = 1.0\n        result = Series(data, index=index)\n        expected = Series([0.0, 1.0, 2.0], index=index)\n        tm.assert_series_equal(result, expected)\n\n        data = ma.masked_all((3,), dtype=int)\n        result = Series(data)\n        expected = Series([np.nan, np.nan, np.nan], dtype=float)\n        tm.assert_series_equal(result, expected)\n\n        data[0] = 0\n        data[2] = 2\n        index = [\"a\", \"b\", \"c\"]\n        result = Series(data, index=index)\n        expected = Series([0, np.nan, 2], index=index, dtype=float)\n        tm.assert_series_equal(result, expected)\n\n        data[1] = 1\n        result = Series(data, index=index)\n        expected = Series([0, 1, 2], index=index, dtype=int)\n        with pytest.raises(AssertionError, match=\"Series classes are different\"):\n            # TODO should this be raising at all?\n            # https://github.com/pandas-dev/pandas/issues/56131\n            tm.assert_series_equal(result, expected)\n\n        data = ma.masked_all((3,), dtype=bool)\n        result = Series(data)\n        expected = Series([np.nan, np.nan, np.nan], dtype=object)\n        tm.assert_series_equal(result, expected)\n\n        data[0] = True\n        data[2] = False\n        index = [\"a\", \"b\", \"c\"]\n        result = Series(data, index=index)\n        expected = Series([True, np.nan, False], index=index, dtype=object)\n        tm.assert_series_equal(result, expected)\n\n        data[1] = True\n        result = Series(data, index=index)\n        expected = Series([True, True, False], index=index, dtype=bool)\n        with pytest.raises(AssertionError, match=\"Series classes are different\"):\n            # TODO should this be raising at all?\n            # https://github.com/pandas-dev/pandas/issues/56131\n            tm.assert_series_equal(result, expected)\n\n        data = ma.masked_all((3,), dtype=\"M8[ns]\")\n        result = Series(data)\n        expected = Series([iNaT, iNaT, iNaT], dtype=\"M8[ns]\")\n        tm.assert_series_equal(result, expected)\n\n        data[0] = datetime(2001, 1, 1)\n        data[2] = datetime(2001, 1, 3)\n        index = [\"a\", \"b\", \"c\"]\n        result = Series(data, index=index)\n        expected = Series(\n            [datetime(2001, 1, 1), iNaT, datetime(2001, 1, 3)],\n            index=index,\n            dtype=\"M8[ns]\",\n        )\n        tm.assert_series_equal(result, expected)\n\n        data[1] = datetime(2001, 1, 2)\n        result = Series(data, index=index)\n        expected = Series(\n            [datetime(2001, 1, 1), datetime(2001, 1, 2), datetime(2001, 1, 3)],\n            index=index,\n            dtype=\"M8[ns]\",\n        )\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_maskedarray_hardened(self):\n        # Check numpy masked arrays with hard masks -- from GH24574\n        data = ma.masked_all((3,), dtype=float).harden_mask()\n        result = Series(data)\n        expected = Series([np.nan, np.nan, np.nan])\n        tm.assert_series_equal(result, expected)\n\n    def test_series_ctor_plus_datetimeindex(self):\n        rng = date_range(\"20090415\", \"20090519\", freq=\"B\")\n        data = {k: 1 for k in rng}\n\n        result = Series(data, index=rng)\n        assert result.index.is_(rng)\n\n    def test_constructor_default_index(self):\n        s = Series([0, 1, 2])\n        tm.assert_index_equal(s.index, Index(range(3)), exact=True)\n\n    @pytest.mark.parametrize(\n        \"input\",\n        [\n            [1, 2, 3],\n            (1, 2, 3),\n            list(range(3)),\n            Categorical([\"a\", \"b\", \"a\"]),\n            (i for i in range(3)),\n            (x for x in range(3)),\n        ],\n    )\n    def test_constructor_index_mismatch(self, input):\n        # GH 19342\n        # test that construction of a Series with an index of different length\n        # raises an error\n        msg = r\"Length of values \\(3\\) does not match length of index \\(4\\)\"\n        with pytest.raises(ValueError, match=msg):\n            Series(input, index=np.arange(4))\n\n    def test_constructor_numpy_scalar(self):\n        # GH 19342\n        # construction with a numpy scalar\n        # should not raise\n        result = Series(np.array(100), index=np.arange(4), dtype=\"int64\")\n        expected = Series(100, index=np.arange(4), dtype=\"int64\")\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_broadcast_list(self):\n        # GH 19342\n        # construction with single-element container and index\n        # should raise\n        msg = r\"Length of values \\(1\\) does not match length of index \\(3\\)\"\n        with pytest.raises(ValueError, match=msg):\n            Series([\"foo\"], index=[\"a\", \"b\", \"c\"])\n\n    def test_constructor_corner(self):\n        df = DataFrame(range(5), index=date_range(\"2020-01-01\", periods=5))\n        objs = [df, df]\n        s = Series(objs, index=[0, 1])\n        assert isinstance(s, Series)\n\n    def test_constructor_sanitize(self):\n        s = Series(np.array([1.0, 1.0, 8.0]), dtype=\"i8\")\n        assert s.dtype == np.dtype(\"i8\")\n\n        msg = r\"Cannot convert non-finite values \\(NA or inf\\) to integer\"\n        with pytest.raises(IntCastingNaNError, match=msg):\n            Series(np.array([1.0, 1.0, np.nan]), copy=True, dtype=\"i8\")\n\n    def test_constructor_copy(self):\n        # GH15125\n        # test dtype parameter has no side effects on copy=True\n        for data in [[1.0], np.array([1.0])]:\n            x = Series(data)\n            y = Series(x, copy=True, dtype=float)\n\n            # copy=True maintains original data in Series\n            tm.assert_series_equal(x, y)\n\n            # changes to origin of copy does not affect the copy\n            x[0] = 2.0\n            assert not x.equals(y)\n            assert x[0] == 2.0\n            assert y[0] == 1.0\n\n    @pytest.mark.parametrize(\n        \"index\",\n        [\n            date_range(\"20170101\", periods=3, tz=\"US/Eastern\"),\n            date_range(\"20170101\", periods=3),\n            timedelta_range(\"1 day\", periods=3),\n            period_range(\"2012Q1\", periods=3, freq=\"Q\"),\n            Index(list(\"abc\")),\n            Index([1, 2, 3]),\n            RangeIndex(0, 3),\n        ],\n        ids=lambda x: type(x).__name__,\n    )\n    def test_constructor_limit_copies(self, index):\n        # GH 17449\n        # limit copies of input\n        s = Series(index)\n\n        # we make 1 copy; this is just a smoke test here\n        assert s._mgr.blocks[0].values is not index\n\n    def test_constructor_shallow_copy(self):\n        # constructing a Series from Series with copy=False should still\n        # give a \"shallow\" copy (share data, not attributes)\n        # https://github.com/pandas-dev/pandas/issues/49523\n        s = Series([1, 2, 3])\n        s_orig = s.copy()\n        s2 = Series(s)\n        assert s2._mgr is not s._mgr\n        # Overwriting index of s2 doesn't change s\n        s2.index = [\"a\", \"b\", \"c\"]\n        tm.assert_series_equal(s, s_orig)\n\n    def test_constructor_pass_none(self):\n        s = Series(None, index=range(5))\n        assert s.dtype == np.float64\n\n        s = Series(None, index=range(5), dtype=object)\n        assert s.dtype == np.object_\n\n        # GH 7431\n        # inference on the index\n        s = Series(index=np.array([None]))\n        expected = Series(index=Index([None]))\n        tm.assert_series_equal(s, expected)\n\n    def test_constructor_pass_nan_nat(self):\n        # GH 13467\n        exp = Series([np.nan, np.nan], dtype=np.float64)\n        assert exp.dtype == np.float64\n        tm.assert_series_equal(Series([np.nan, np.nan]), exp)\n        tm.assert_series_equal(Series(np.array([np.nan, np.nan])), exp)\n\n        exp = Series([NaT, NaT])\n        assert exp.dtype == \"datetime64[s]\"\n        tm.assert_series_equal(Series([NaT, NaT]), exp)\n        tm.assert_series_equal(Series(np.array([NaT, NaT])), exp)\n\n        tm.assert_series_equal(Series([NaT, np.nan]), exp)\n        tm.assert_series_equal(Series(np.array([NaT, np.nan])), exp)\n\n        tm.assert_series_equal(Series([np.nan, NaT]), exp)\n        tm.assert_series_equal(Series(np.array([np.nan, NaT])), exp)\n\n    def test_constructor_cast(self):\n        msg = \"could not convert string to float\"\n        with pytest.raises(ValueError, match=msg):\n            Series([\"a\", \"b\", \"c\"], dtype=float)\n\n    def test_constructor_signed_int_overflow_raises(self):\n        # GH#41734 disallow silent overflow, enforced in 2.0\n        if np_version_gt2:\n            msg = \"The elements provided in the data cannot all be casted to the dtype\"\n            err = OverflowError\n        else:\n            msg = \"Values are too large to be losslessly converted\"\n            err = ValueError\n        with pytest.raises(err, match=msg):\n            Series([1, 200, 923442], dtype=\"int8\")\n\n        with pytest.raises(err, match=msg):\n            Series([1, 200, 923442], dtype=\"uint8\")\n\n    @pytest.mark.parametrize(\n        \"values\",\n        [\n            np.array([1], dtype=np.uint16),\n            np.array([1], dtype=np.uint32),\n            np.array([1], dtype=np.uint64),\n            [np.uint16(1)],\n            [np.uint32(1)],\n            [np.uint64(1)],\n        ],\n    )\n    def test_constructor_numpy_uints(self, values):\n        # GH#47294\n        value = values[0]\n        result = Series(values)\n\n        assert result[0].dtype == value.dtype\n        assert result[0] == value\n\n    def test_constructor_unsigned_dtype_overflow(self, any_unsigned_int_numpy_dtype):\n        # see gh-15832\n        if np_version_gt2:\n            msg = (\n                f\"The elements provided in the data cannot \"\n                f\"all be casted to the dtype {any_unsigned_int_numpy_dtype}\"\n            )\n        else:\n            msg = \"Trying to coerce negative values to unsigned integers\"\n        with pytest.raises(OverflowError, match=msg):\n            Series([-1], dtype=any_unsigned_int_numpy_dtype)\n\n    def test_constructor_floating_data_int_dtype(self, frame_or_series):\n        # GH#40110\n        arr = np.random.default_rng(2).standard_normal(2)\n\n        # Long-standing behavior (for Series, new in 2.0 for DataFrame)\n        #  has been to ignore the dtype on these;\n        #  not clear if this is what we want long-term\n        # expected = frame_or_series(arr)\n\n        # GH#49599 as of 2.0 we raise instead of silently retaining float dtype\n        msg = \"Trying to coerce float values to integer\"\n        with pytest.raises(ValueError, match=msg):\n            frame_or_series(arr, dtype=\"i8\")\n\n        with pytest.raises(ValueError, match=msg):\n            frame_or_series(list(arr), dtype=\"i8\")\n\n        # pre-2.0, when we had NaNs, we silently ignored the integer dtype\n        arr[0] = np.nan\n        # expected = frame_or_series(arr)\n\n        msg = r\"Cannot convert non-finite values \\(NA or inf\\) to integer\"\n        with pytest.raises(IntCastingNaNError, match=msg):\n            frame_or_series(arr, dtype=\"i8\")\n\n        exc = IntCastingNaNError\n        if frame_or_series is Series:\n            # TODO: try to align these\n            exc = ValueError\n            msg = \"cannot convert float NaN to integer\"\n        with pytest.raises(exc, match=msg):\n            # same behavior if we pass list instead of the ndarray\n            frame_or_series(list(arr), dtype=\"i8\")\n\n        # float array that can be losslessly cast to integers\n        arr = np.array([1.0, 2.0], dtype=\"float64\")\n        expected = frame_or_series(arr.astype(\"i8\"))\n\n        obj = frame_or_series(arr, dtype=\"i8\")\n        tm.assert_equal(obj, expected)\n\n        obj = frame_or_series(list(arr), dtype=\"i8\")\n        tm.assert_equal(obj, expected)\n\n    def test_constructor_coerce_float_fail(self, any_int_numpy_dtype):\n        # see gh-15832\n        # Updated: make sure we treat this list the same as we would treat\n        #  the equivalent ndarray\n        # GH#49599 pre-2.0 we silently retained float dtype, in 2.0 we raise\n        vals = [1, 2, 3.5]\n\n        msg = \"Trying to coerce float values to integer\"\n        with pytest.raises(ValueError, match=msg):\n            Series(vals, dtype=any_int_numpy_dtype)\n        with pytest.raises(ValueError, match=msg):\n            Series(np.array(vals), dtype=any_int_numpy_dtype)\n\n    def test_constructor_coerce_float_valid(self, float_numpy_dtype):\n        s = Series([1, 2, 3.5], dtype=float_numpy_dtype)\n        expected = Series([1, 2, 3.5]).astype(float_numpy_dtype)\n        tm.assert_series_equal(s, expected)\n\n    def test_constructor_invalid_coerce_ints_with_float_nan(self, any_int_numpy_dtype):\n        # GH 22585\n        # Updated: make sure we treat this list the same as we would treat the\n        # equivalent ndarray\n        vals = [1, 2, np.nan]\n        # pre-2.0 this would return with a float dtype, in 2.0 we raise\n\n        msg = \"cannot convert float NaN to integer\"\n        with pytest.raises(ValueError, match=msg):\n            Series(vals, dtype=any_int_numpy_dtype)\n        msg = r\"Cannot convert non-finite values \\(NA or inf\\) to integer\"\n        with pytest.raises(IntCastingNaNError, match=msg):\n            Series(np.array(vals), dtype=any_int_numpy_dtype)\n\n    def test_constructor_dtype_no_cast(self):\n        # see gh-1572\n        s = Series([1, 2, 3])\n        s2 = Series(s, dtype=np.int64)\n\n        s2[1] = 5\n        assert s[1] == 2\n\n    def test_constructor_datelike_coercion(self):\n        # GH 9477\n        # incorrectly inferring on dateimelike looking when object dtype is\n        # specified\n        s = Series([Timestamp(\"20130101\"), \"NOV\"], dtype=object)\n        assert s.iloc[0] == Timestamp(\"20130101\")\n        assert s.iloc[1] == \"NOV\"\n        assert s.dtype == object\n\n    def test_constructor_datelike_coercion2(self):\n        # the dtype was being reset on the slicing and re-inferred to datetime\n        # even thought the blocks are mixed\n        belly = \"216 3T19\".split()\n        wing1 = \"2T15 4H19\".split()\n        wing2 = \"416 4T20\".split()\n        mat = pd.to_datetime(\"2016-01-22 2019-09-07\".split())\n        df = DataFrame({\"wing1\": wing1, \"wing2\": wing2, \"mat\": mat}, index=belly)\n\n        result = df.loc[\"3T19\"]\n        assert result.dtype == object\n        result = df.loc[\"216\"]\n        assert result.dtype == object\n\n    def test_constructor_mixed_int_and_timestamp(self, frame_or_series):\n        # specifically Timestamp with nanos, not datetimes\n        objs = [Timestamp(9), 10, NaT._value]\n        result = frame_or_series(objs, dtype=\"M8[ns]\")\n\n        expected = frame_or_series([Timestamp(9), Timestamp(10), NaT])\n        tm.assert_equal(result, expected)\n\n    def test_constructor_datetimes_with_nulls(self):\n        # gh-15869\n        for arr in [\n            np.array([None, None, None, None, datetime.now(), None]),\n            np.array([None, None, datetime.now(), None]),\n        ]:\n            result = Series(arr)\n            assert result.dtype == \"M8[us]\"\n\n    def test_constructor_dtype_datetime64(self):\n        s = Series(iNaT, dtype=\"M8[ns]\", index=range(5))\n        assert isna(s).all()\n\n        # in theory this should be all nulls, but since\n        # we are not specifying a dtype is ambiguous\n        s = Series(iNaT, index=range(5))\n        assert not isna(s).all()\n\n        s = Series(np.nan, dtype=\"M8[ns]\", index=range(5))\n        assert isna(s).all()\n\n        s = Series([datetime(2001, 1, 2, 0, 0), iNaT], dtype=\"M8[ns]\")\n        assert isna(s[1])\n        assert s.dtype == \"M8[ns]\"\n\n        s = Series([datetime(2001, 1, 2, 0, 0), np.nan], dtype=\"M8[ns]\")\n        assert isna(s[1])\n        assert s.dtype == \"M8[ns]\"\n\n    def test_constructor_dtype_datetime64_10(self):\n        # GH3416\n        pydates = [datetime(2013, 1, 1), datetime(2013, 1, 2), datetime(2013, 1, 3)]\n        dates = [np.datetime64(x) for x in pydates]\n\n        ser = Series(dates)\n        assert ser.dtype == \"M8[us]\"\n\n        ser.iloc[0] = np.nan\n        assert ser.dtype == \"M8[us]\"\n\n        # GH3414 related\n        expected = Series(pydates, dtype=\"datetime64[ms]\")\n\n        result = Series(Series(dates).astype(np.int64) / 1000, dtype=\"M8[ms]\")\n        tm.assert_series_equal(result, expected)\n\n        result = Series(dates, dtype=\"datetime64[ms]\")\n        tm.assert_series_equal(result, expected)\n\n        expected = Series(\n            [NaT, datetime(2013, 1, 2), datetime(2013, 1, 3)], dtype=\"datetime64[ns]\"\n        )\n        result = Series([np.nan] + dates[1:], dtype=\"datetime64[ns]\")\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_dtype_datetime64_11(self):\n        pydates = [datetime(2013, 1, 1), datetime(2013, 1, 2), datetime(2013, 1, 3)]\n        dates = [np.datetime64(x) for x in pydates]\n\n        dts = Series(dates, dtype=\"datetime64[ns]\")\n\n        # valid astype\n        dts.astype(\"int64\")\n\n        # invalid casting\n        msg = r\"Converting from datetime64\\[ns\\] to int32 is not supported\"\n        with pytest.raises(TypeError, match=msg):\n            dts.astype(\"int32\")\n\n        # ints are ok\n        # we test with np.int64 to get similar results on\n        # windows / 32-bit platforms\n        result = Series(dts, dtype=np.int64)\n        expected = Series(dts.astype(np.int64))\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_dtype_datetime64_9(self):\n        # invalid dates can be help as object\n        result = Series([datetime(2, 1, 1)])\n        assert result[0] == datetime(2, 1, 1, 0, 0)\n\n        result = Series([datetime(3000, 1, 1)])\n        assert result[0] == datetime(3000, 1, 1, 0, 0)\n\n    def test_constructor_dtype_datetime64_8(self):\n        # don't mix types\n        result = Series([Timestamp(\"20130101\"), 1], index=[\"a\", \"b\"])\n        assert result[\"a\"] == Timestamp(\"20130101\")\n        assert result[\"b\"] == 1\n\n    def test_constructor_dtype_datetime64_7(self):\n        # GH6529\n        # coerce datetime64 non-ns properly\n        dates = date_range(\"01-Jan-2015\", \"01-Dec-2015\", freq=\"ME\")\n        values2 = dates.view(np.ndarray).astype(\"datetime64[ns]\")\n        expected = Series(values2, index=dates)\n\n        for unit in [\"s\", \"D\", \"ms\", \"us\", \"ns\"]:\n            dtype = np.dtype(f\"M8[{unit}]\")\n            values1 = dates.view(np.ndarray).astype(dtype)\n            result = Series(values1, dates)\n            if unit == \"D\":\n                # for unit=\"D\" we cast to nearest-supported reso, i.e. \"s\"\n                dtype = np.dtype(\"M8[s]\")\n            assert result.dtype == dtype\n            tm.assert_series_equal(result, expected.astype(dtype))\n\n        # GH 13876\n        # coerce to non-ns to object properly\n        expected = Series(values2, index=dates, dtype=object)\n        for dtype in [\"s\", \"D\", \"ms\", \"us\", \"ns\"]:\n            values1 = dates.view(np.ndarray).astype(f\"M8[{dtype}]\")\n            result = Series(values1, index=dates, dtype=object)\n            tm.assert_series_equal(result, expected)\n\n        # leave datetime.date alone\n        dates2 = np.array([d.date() for d in dates.to_pydatetime()], dtype=object)\n        series1 = Series(dates2, dates)\n        tm.assert_numpy_array_equal(series1.values, dates2)\n        assert series1.dtype == object\n\n    def test_constructor_dtype_datetime64_6(self):\n        # as of 2.0, these no longer infer datetime64 based on the strings,\n        #  matching the Index behavior\n\n        ser = Series([None, NaT, \"2013-08-05 15:30:00.000001\"])\n        assert ser.dtype == object\n\n        ser = Series([np.nan, NaT, \"2013-08-05 15:30:00.000001\"])\n        assert ser.dtype == object\n\n        ser = Series([NaT, None, \"2013-08-05 15:30:00.000001\"])\n        assert ser.dtype == object\n\n        ser = Series([NaT, np.nan, \"2013-08-05 15:30:00.000001\"])\n        assert ser.dtype == object\n\n    def test_constructor_dtype_datetime64_5(self):\n        # tz-aware (UTC and other tz's)\n        # GH 8411\n        dr = date_range(\"20130101\", periods=3)\n        assert Series(dr).iloc[0].tz is None\n        dr = date_range(\"20130101\", periods=3, tz=\"UTC\")\n        assert str(Series(dr).iloc[0].tz) == \"UTC\"\n        dr = date_range(\"20130101\", periods=3, tz=\"US/Eastern\")\n        assert str(Series(dr).iloc[0].tz) == \"US/Eastern\"\n\n    def test_constructor_dtype_datetime64_4(self):\n        # non-convertible\n        ser = Series([1479596223000, -1479590, NaT])\n        assert ser.dtype == \"object\"\n        assert ser[2] is NaT\n        assert \"NaT\" in str(ser)\n\n    def test_constructor_dtype_datetime64_3(self):\n        # if we passed a NaT it remains\n        ser = Series([datetime(2010, 1, 1), datetime(2, 1, 1), NaT])\n        assert ser.dtype == \"M8[us]\"\n        assert ser[2] is NaT\n        assert \"NaT\" in str(ser)\n\n    def test_constructor_dtype_datetime64_2(self):\n        # if we passed a nan it remains\n        ser = Series([datetime(2010, 1, 1), datetime(2, 1, 1), np.nan])\n        assert ser.dtype == \"M8[us]\"\n        assert ser[2] is NaT\n        assert \"NaT\" in str(ser)\n\n    def test_constructor_with_datetime_tz(self):\n        # 8260\n        # support datetime64 with tz\n\n        dr = date_range(\"20130101\", periods=3, tz=\"US/Eastern\")\n        s = Series(dr)\n        assert s.dtype.name == \"datetime64[ns, US/Eastern]\"\n        assert s.dtype == \"datetime64[ns, US/Eastern]\"\n        assert isinstance(s.dtype, DatetimeTZDtype)\n        assert \"datetime64[ns, US/Eastern]\" in str(s)\n\n        # export\n        result = s.values\n        assert isinstance(result, np.ndarray)\n        assert result.dtype == \"datetime64[ns]\"\n\n        exp = DatetimeIndex(result)\n        exp = exp.tz_localize(\"UTC\").tz_convert(tz=s.dt.tz)\n        tm.assert_index_equal(dr, exp)\n\n        # indexing\n        result = s.iloc[0]\n        assert result == Timestamp(\"2013-01-01 00:00:00-0500\", tz=\"US/Eastern\")\n        result = s[0]\n        assert result == Timestamp(\"2013-01-01 00:00:00-0500\", tz=\"US/Eastern\")\n\n        result = s[Series([True, True, False], index=s.index)]\n        tm.assert_series_equal(result, s[0:2])\n\n        result = s.iloc[0:1]\n        tm.assert_series_equal(result, Series(dr[0:1]))\n\n        # concat\n        result = pd.concat([s.iloc[0:1], s.iloc[1:]])\n        tm.assert_series_equal(result, s)\n\n        # short str\n        assert \"datetime64[ns, US/Eastern]\" in str(s)\n\n        # formatting with NaT\n        result = s.shift()\n        assert \"datetime64[ns, US/Eastern]\" in str(result)\n        assert \"NaT\" in str(result)\n\n        result = DatetimeIndex(s, freq=\"infer\")\n        tm.assert_index_equal(result, dr)\n\n    def test_constructor_with_datetime_tz5(self):\n        # long str\n        ser = Series(date_range(\"20130101\", periods=1000, tz=\"US/Eastern\"))\n        assert \"datetime64[ns, US/Eastern]\" in str(ser)\n\n    def test_constructor_with_datetime_tz4(self):\n        # inference\n        ser = Series(\n            [\n                Timestamp(\"2013-01-01 13:00:00-0800\", tz=\"US/Pacific\"),\n                Timestamp(\"2013-01-02 14:00:00-0800\", tz=\"US/Pacific\"),\n            ]\n        )\n        assert ser.dtype == \"datetime64[s, US/Pacific]\"\n        assert lib.infer_dtype(ser, skipna=True) == \"datetime64\"\n\n    def test_constructor_with_datetime_tz3(self):\n        ser = Series(\n            [\n                Timestamp(\"2013-01-01 13:00:00-0800\", tz=\"US/Pacific\"),\n                Timestamp(\"2013-01-02 14:00:00-0800\", tz=\"US/Eastern\"),\n            ]\n        )\n        assert ser.dtype == \"object\"\n        assert lib.infer_dtype(ser, skipna=True) == \"datetime\"\n\n    def test_constructor_with_datetime_tz2(self):\n        # with all NaT\n        ser = Series(NaT, index=[0, 1], dtype=\"datetime64[ns, US/Eastern]\")\n        dti = DatetimeIndex([\"NaT\", \"NaT\"], tz=\"US/Eastern\").as_unit(\"ns\")\n        expected = Series(dti)\n        tm.assert_series_equal(ser, expected)\n\n    def test_constructor_no_partial_datetime_casting(self):\n        # GH#40111\n        vals = [\n            \"nan\",\n            Timestamp(\"1990-01-01\"),\n            \"2015-03-14T16:15:14.123-08:00\",\n            \"2019-03-04T21:56:32.620-07:00\",\n            None,\n        ]\n        ser = Series(vals)\n        assert all(ser[i] is vals[i] for i in range(len(vals)))\n\n    @pytest.mark.parametrize(\"arr_dtype\", [np.int64, np.float64])\n    @pytest.mark.parametrize(\"kind\", [\"M\", \"m\"])\n    @pytest.mark.parametrize(\"unit\", [\"ns\", \"us\", \"ms\", \"s\", \"h\", \"m\", \"D\"])\n    def test_construction_to_datetimelike_unit(self, arr_dtype, kind, unit):\n        # tests all units\n        # gh-19223\n        # TODO: GH#19223 was about .astype, doesn't belong here\n        dtype = f\"{kind}8[{unit}]\"\n        arr = np.array([1, 2, 3], dtype=arr_dtype)\n        ser = Series(arr)\n        result = ser.astype(dtype)\n\n        expected = Series(arr.astype(dtype))\n\n        if unit in [\"ns\", \"us\", \"ms\", \"s\"]:\n            assert result.dtype == dtype\n            assert expected.dtype == dtype\n        else:\n            # Otherwise we cast to nearest-supported unit, i.e. seconds\n            assert result.dtype == f\"{kind}8[s]\"\n            assert expected.dtype == f\"{kind}8[s]\"\n\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"arg\", [\"2013-01-01 00:00:00\", NaT, np.nan, None])\n    def test_constructor_with_naive_string_and_datetimetz_dtype(self, arg):\n        # GH 17415: With naive string\n        result = Series([arg], dtype=\"datetime64[ns, CET]\")\n        expected = Series([Timestamp(arg)], dtype=\"M8[ns]\").dt.tz_localize(\"CET\")\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_datetime64_bigendian(self):\n        # GH#30976\n        ms = np.datetime64(1, \"ms\")\n        arr = np.array([np.datetime64(1, \"ms\")], dtype=\">M8[ms]\")\n\n        result = Series(arr)\n        expected = Series([Timestamp(ms)]).astype(\"M8[ms]\")\n        assert expected.dtype == \"M8[ms]\"\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"interval_constructor\", [IntervalIndex, IntervalArray])\n    def test_construction_interval(self, interval_constructor):\n        # construction from interval & array of intervals\n        intervals = interval_constructor.from_breaks(np.arange(3), closed=\"right\")\n        result = Series(intervals)\n        assert result.dtype == \"interval[int64, right]\"\n        tm.assert_index_equal(Index(result.values), Index(intervals))\n\n    @pytest.mark.parametrize(\n        \"data_constructor\", [list, np.array], ids=[\"list\", \"ndarray[object]\"]\n    )\n    def test_constructor_infer_interval(self, data_constructor):\n        # GH 23563: consistent closed results in interval dtype\n        data = [Interval(0, 1), Interval(0, 2), None]\n        result = Series(data_constructor(data))\n        expected = Series(IntervalArray(data))\n        assert result.dtype == \"interval[float64, right]\"\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"data_constructor\", [list, np.array], ids=[\"list\", \"ndarray[object]\"]\n    )\n    def test_constructor_interval_mixed_closed(self, data_constructor):\n        # GH 23563: mixed closed results in object dtype (not interval dtype)\n        data = [Interval(0, 1, closed=\"both\"), Interval(0, 2, closed=\"neither\")]\n        result = Series(data_constructor(data))\n        assert result.dtype == object\n        assert result.tolist() == data\n\n    def test_construction_consistency(self):\n        # make sure that we are not re-localizing upon construction\n        # GH 14928\n        ser = Series(date_range(\"20130101\", periods=3, tz=\"US/Eastern\"))\n\n        result = Series(ser, dtype=ser.dtype)\n        tm.assert_series_equal(result, ser)\n\n        result = Series(ser.dt.tz_convert(\"UTC\"), dtype=ser.dtype)\n        tm.assert_series_equal(result, ser)\n\n        # Pre-2.0 dt64 values were treated as utc, which was inconsistent\n        #  with DatetimeIndex, which treats them as wall times, see GH#33401\n        result = Series(ser.values, dtype=ser.dtype)\n        expected = Series(ser.values).dt.tz_localize(ser.dtype.tz)\n        tm.assert_series_equal(result, expected)\n\n        with tm.assert_produces_warning(None):\n            # one suggested alternative to the deprecated (changed in 2.0) usage\n            middle = Series(ser.values).dt.tz_localize(\"UTC\")\n            result = middle.dt.tz_convert(ser.dtype.tz)\n        tm.assert_series_equal(result, ser)\n\n        with tm.assert_produces_warning(None):\n            # the other suggested alternative to the deprecated usage\n            result = Series(ser.values.view(\"int64\"), dtype=ser.dtype)\n        tm.assert_series_equal(result, ser)\n\n    @pytest.mark.parametrize(\n        \"data_constructor\", [list, np.array], ids=[\"list\", \"ndarray[object]\"]\n    )\n    def test_constructor_infer_period(self, data_constructor):\n        data = [Period(\"2000\", \"D\"), Period(\"2001\", \"D\"), None]\n        result = Series(data_constructor(data))\n        expected = Series(period_array(data))\n        tm.assert_series_equal(result, expected)\n        assert result.dtype == \"Period[D]\"\n\n    @pytest.mark.xfail(reason=\"PeriodDtype Series not supported yet\")\n    def test_construct_from_ints_including_iNaT_scalar_period_dtype(self):\n        series = Series([0, 1000, 2000, pd._libs.iNaT], dtype=\"period[D]\")\n\n        val = series[3]\n        assert isna(val)\n\n        series[2] = val\n        assert isna(series[2])\n\n    def test_constructor_period_incompatible_frequency(self):\n        data = [Period(\"2000\", \"D\"), Period(\"2001\", \"Y\")]\n        result = Series(data)\n        assert result.dtype == object\n        assert result.tolist() == data\n\n    def test_constructor_periodindex(self):\n        # GH7932\n        # converting a PeriodIndex when put in a Series\n\n        pi = period_range(\"20130101\", periods=5, freq=\"D\")\n        s = Series(pi)\n        assert s.dtype == \"Period[D]\"\n        expected = Series(pi.astype(object))\n        assert expected.dtype == object\n\n    def test_constructor_dict(self):\n        d = {\"a\": 0.0, \"b\": 1.0, \"c\": 2.0}\n\n        result = Series(d)\n        expected = Series(d, index=sorted(d.keys()))\n        tm.assert_series_equal(result, expected)\n\n        result = Series(d, index=[\"b\", \"c\", \"d\", \"a\"])\n        expected = Series([1, 2, np.nan, 0], index=[\"b\", \"c\", \"d\", \"a\"])\n        tm.assert_series_equal(result, expected)\n\n        pidx = period_range(\"2020-01-01\", periods=10, freq=\"D\")\n        d = {pidx[0]: 0, pidx[1]: 1}\n        result = Series(d, index=pidx)\n        expected = Series(np.nan, pidx, dtype=np.float64)\n        expected.iloc[0] = 0\n        expected.iloc[1] = 1\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_dict_list_value_explicit_dtype(self):\n        # GH 18625\n        d = {\"a\": [[2], [3], [4]]}\n        result = Series(d, index=[\"a\"], dtype=\"object\")\n        expected = Series(d, index=[\"a\"])\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_dict_order(self):\n        # GH19018\n        # initialization ordering: by insertion order\n        d = {\"b\": 1, \"a\": 0, \"c\": 2}\n        result = Series(d)\n        expected = Series([1, 0, 2], index=list(\"bac\"))\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_dict_extension(self, ea_scalar_and_dtype):\n        ea_scalar, ea_dtype = ea_scalar_and_dtype\n        d = {\"a\": ea_scalar}\n        result = Series(d, index=[\"a\"])\n        expected = Series(ea_scalar, index=[\"a\"], dtype=ea_dtype)\n\n        assert result.dtype == ea_dtype\n\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"value\", [2, np.nan, None, float(\"nan\")])\n    def test_constructor_dict_nan_key(self, value):\n        # GH 18480\n        d = {1: \"a\", value: \"b\", float(\"nan\"): \"c\", 4: \"d\"}\n        result = Series(d).sort_values()\n        expected = Series([\"a\", \"b\", \"c\", \"d\"], index=[1, value, np.nan, 4])\n        tm.assert_series_equal(result, expected)\n\n        # MultiIndex:\n        d = {(1, 1): \"a\", (2, np.nan): \"b\", (3, value): \"c\"}\n        result = Series(d).sort_values()\n        expected = Series(\n            [\"a\", \"b\", \"c\"], index=Index([(1, 1), (2, np.nan), (3, value)])\n        )\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_dict_datetime64_index(self):\n        # GH 9456\n\n        dates_as_str = [\"1984-02-19\", \"1988-11-06\", \"1989-12-03\", \"1990-03-15\"]\n        values = [42544017.198965244, 1234565, 40512335.181958228, -1]\n\n        def create_data(constructor):\n            return dict(zip((constructor(x) for x in dates_as_str), values))\n\n        data_datetime64 = create_data(np.datetime64)\n        data_datetime = create_data(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n        data_Timestamp = create_data(Timestamp)\n\n        expected = Series(values, (Timestamp(x) for x in dates_as_str))\n\n        result_datetime64 = Series(data_datetime64)\n        result_datetime = Series(data_datetime)\n        result_Timestamp = Series(data_Timestamp)\n\n        tm.assert_series_equal(result_datetime64, expected)\n        tm.assert_series_equal(\n            result_datetime, expected.set_axis(expected.index.as_unit(\"us\"))\n        )\n        tm.assert_series_equal(result_Timestamp, expected)\n\n    def test_constructor_dict_tuple_indexer(self):\n        # GH 12948\n        data = {(1, 1, None): -1.0}\n        result = Series(data)\n        expected = Series(\n            -1.0, index=MultiIndex(levels=[[1], [1], [np.nan]], codes=[[0], [0], [-1]])\n        )\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_mapping(self, non_dict_mapping_subclass):\n        # GH 29788\n        ndm = non_dict_mapping_subclass({3: \"three\"})\n        result = Series(ndm)\n        expected = Series([\"three\"], index=[3])\n\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_list_of_tuples(self):\n        data = [(1, 1), (2, 2), (2, 3)]\n        s = Series(data)\n        assert list(s) == data\n\n    def test_constructor_tuple_of_tuples(self):\n        data = ((1, 1), (2, 2), (2, 3))\n        s = Series(data)\n        assert tuple(s) == data\n\n    @pytest.mark.parametrize(\n        \"data, expected_values, expected_index\",\n        [\n            ({(1, 2): 3, (None, 5): 6}, [3, 6], [(1, 2), (None, 5)]),\n            ({(1,): 3, (4, 5): 6}, [3, 6], [(1, None), (4, 5)]),\n        ],\n    )\n    def test_constructor_dict_of_tuples(self, data, expected_values, expected_index):\n        # GH 60695\n        result = Series(data).sort_values()\n        expected = Series(expected_values, index=MultiIndex.from_tuples(expected_index))\n        tm.assert_series_equal(result, expected)\n\n    # https://github.com/pandas-dev/pandas/issues/22698\n    @pytest.mark.filterwarnings(\"ignore:elementwise comparison:FutureWarning\")\n    def test_fromDict(self, using_infer_string):\n        data = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3}\n\n        series = Series(data)\n        tm.assert_is_sorted(series.index)\n\n        data = {\"a\": 0, \"b\": \"1\", \"c\": \"2\", \"d\": datetime.now()}\n        series = Series(data)\n        assert series.dtype == np.object_\n\n        data = {\"a\": 0, \"b\": \"1\", \"c\": \"2\", \"d\": \"3\"}\n        series = Series(data)\n        assert series.dtype == np.object_ if not using_infer_string else \"str\"\n\n        data = {\"a\": \"0\", \"b\": \"1\"}\n        series = Series(data, dtype=float)\n        assert series.dtype == np.float64\n\n    def test_fromValue(self, datetime_series, using_infer_string):\n        nans = Series(np.nan, index=datetime_series.index, dtype=np.float64)\n        assert nans.dtype == np.float64\n        assert len(nans) == len(datetime_series)\n\n        strings = Series(\"foo\", index=datetime_series.index)\n        assert strings.dtype == np.object_ if not using_infer_string else \"str\"\n        assert len(strings) == len(datetime_series)\n\n        d = datetime.now()\n        dates = Series(d, index=datetime_series.index)\n        assert dates.dtype == \"M8[us]\"\n        assert len(dates) == len(datetime_series)\n\n        # GH12336\n        # Test construction of categorical series from value\n        categorical = Series(0, index=datetime_series.index, dtype=\"category\")\n        expected = Series(0, index=datetime_series.index).astype(\"category\")\n        assert categorical.dtype == \"category\"\n        assert len(categorical) == len(datetime_series)\n        tm.assert_series_equal(categorical, expected)\n\n    def test_constructor_dtype_timedelta64(self):\n        # basic\n        td = Series([timedelta(days=i) for i in range(3)])\n        assert td.dtype == \"timedelta64[ns]\"\n\n        td = Series([timedelta(days=1)])\n        assert td.dtype == \"timedelta64[ns]\"\n\n        td = Series([timedelta(days=1), timedelta(days=2), np.timedelta64(1, \"s\")])\n\n        assert td.dtype == \"timedelta64[ns]\"\n\n        # mixed with NaT\n        td = Series([timedelta(days=1), NaT], dtype=\"m8[ns]\")\n        assert td.dtype == \"timedelta64[ns]\"\n\n        td = Series([timedelta(days=1), np.nan], dtype=\"m8[ns]\")\n        assert td.dtype == \"timedelta64[ns]\"\n\n        td = Series([np.timedelta64(300000000), NaT], dtype=\"m8[ns]\")\n        assert td.dtype == \"timedelta64[ns]\"\n\n        # improved inference\n        # GH5689\n        td = Series([np.timedelta64(300000000), NaT])\n        assert td.dtype == \"timedelta64[ns]\"\n\n        # because iNaT is int, not coerced to timedelta\n        td = Series([np.timedelta64(300000000), iNaT])\n        assert td.dtype == \"object\"\n\n        td = Series([np.timedelta64(300000000), np.nan])\n        assert td.dtype == \"timedelta64[ns]\"\n\n        td = Series([NaT, np.timedelta64(300000000)])\n        assert td.dtype == \"timedelta64[ns]\"\n\n        td = Series([np.timedelta64(1, \"s\")])\n        assert td.dtype == \"timedelta64[ns]\"\n\n        # valid astype\n        td.astype(\"int64\")\n\n        # invalid casting\n        msg = r\"Converting from timedelta64\\[ns\\] to int32 is not supported\"\n        with pytest.raises(TypeError, match=msg):\n            td.astype(\"int32\")\n\n        # this is an invalid casting\n        msg = \"|\".join(\n            [\n                \"Could not convert object to NumPy timedelta\",\n                \"Could not convert 'foo' to NumPy timedelta\",\n            ]\n        )\n        with pytest.raises(ValueError, match=msg):\n            Series([timedelta(days=1), \"foo\"], dtype=\"m8[ns]\")\n\n        # leave as object here\n        td = Series([timedelta(days=i) for i in range(3)] + [\"foo\"])\n        assert td.dtype == \"object\"\n\n        # as of 2.0, these no longer infer timedelta64 based on the strings,\n        #  matching Index behavior\n        ser = Series([None, NaT, \"1 Day\"])\n        assert ser.dtype == object\n\n        ser = Series([np.nan, NaT, \"1 Day\"])\n        assert ser.dtype == object\n\n        ser = Series([NaT, None, \"1 Day\"])\n        assert ser.dtype == object\n\n        ser = Series([NaT, np.nan, \"1 Day\"])\n        assert ser.dtype == object\n\n    # GH 16406\n    def test_constructor_mixed_tz(self):\n        s = Series([Timestamp(\"20130101\"), Timestamp(\"20130101\", tz=\"US/Eastern\")])\n        expected = Series(\n            [Timestamp(\"20130101\"), Timestamp(\"20130101\", tz=\"US/Eastern\")],\n            dtype=\"object\",\n        )\n        tm.assert_series_equal(s, expected)\n\n    def test_NaT_scalar(self):\n        series = Series([0, 1000, 2000, iNaT], dtype=\"M8[ns]\")\n\n        val = series[3]\n        assert isna(val)\n\n        series[2] = val\n        assert isna(series[2])\n\n    def test_NaT_cast(self):\n        # GH10747\n        result = Series([np.nan]).astype(\"M8[ns]\")\n        expected = Series([NaT], dtype=\"M8[ns]\")\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_name_hashable(self):\n        for n in [777, 777.0, \"name\", datetime(2001, 11, 11), (1,), \"\\u05d0\"]:\n            for data in [[1, 2, 3], np.ones(3), {\"a\": 0, \"b\": 1}]:\n                s = Series(data, name=n)\n                assert s.name == n\n\n    def test_constructor_name_unhashable(self):\n        msg = r\"Series\\.name must be a hashable type\"\n        for n in [[\"name_list\"], np.ones(2), {1: 2}]:\n            for data in [[\"name_list\"], np.ones(2), {1: 2}]:\n                with pytest.raises(TypeError, match=msg):\n                    Series(data, name=n)\n\n    def test_auto_conversion(self):\n        series = Series(list(date_range(\"1/1/2000\", periods=10)))\n        assert series.dtype == \"M8[ns]\"\n\n    def test_convert_non_ns(self):\n        # convert from a numpy array of non-ns timedelta64\n        arr = np.array([1, 2, 3], dtype=\"timedelta64[s]\")\n        ser = Series(arr)\n        assert ser.dtype == arr.dtype\n\n        tdi = timedelta_range(\"00:00:01\", periods=3, freq=\"s\").as_unit(\"s\")\n        expected = Series(tdi)\n        assert expected.dtype == arr.dtype\n        tm.assert_series_equal(ser, expected)\n\n        # convert from a numpy array of non-ns datetime64\n        arr = np.array(\n            [\"2013-01-01\", \"2013-01-02\", \"2013-01-03\"], dtype=\"datetime64[D]\"\n        )\n        ser = Series(arr)\n        expected = Series(date_range(\"20130101\", periods=3, freq=\"D\"), dtype=\"M8[s]\")\n        assert expected.dtype == \"M8[s]\"\n        tm.assert_series_equal(ser, expected)\n\n        arr = np.array(\n            [\"2013-01-01 00:00:01\", \"2013-01-01 00:00:02\", \"2013-01-01 00:00:03\"],\n            dtype=\"datetime64[s]\",\n        )\n        ser = Series(arr)\n        expected = Series(\n            date_range(\"20130101 00:00:01\", periods=3, freq=\"s\"), dtype=\"M8[s]\"\n        )\n        assert expected.dtype == \"M8[s]\"\n        tm.assert_series_equal(ser, expected)\n\n    @pytest.mark.parametrize(\n        \"index\",\n        [\n            date_range(\"1/1/2000\", periods=10),\n            timedelta_range(\"1 day\", periods=10),\n            period_range(\"2000-Q1\", periods=10, freq=\"Q\"),\n        ],\n        ids=lambda x: type(x).__name__,\n    )\n    def test_constructor_cant_cast_datetimelike(self, index):\n        # floats are not ok\n        # strip Index to convert PeriodIndex -> Period\n        # We don't care whether the error message says\n        # PeriodIndex or PeriodArray\n        msg = f\"Cannot cast {type(index).__name__.rstrip('Index')}.*? to \"\n\n        with pytest.raises(TypeError, match=msg):\n            Series(index, dtype=float)\n\n        # ints are ok\n        # we test with np.int64 to get similar results on\n        # windows / 32-bit platforms\n        result = Series(index, dtype=np.int64)\n        expected = Series(index.astype(np.int64))\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"index\",\n        [\n            date_range(\"1/1/2000\", periods=10),\n            timedelta_range(\"1 day\", periods=10),\n            period_range(\"2000-Q1\", periods=10, freq=\"Q\"),\n        ],\n        ids=lambda x: type(x).__name__,\n    )\n    def test_constructor_cast_object(self, index):\n        s = Series(index, dtype=object)\n        exp = Series(index).astype(object)\n        tm.assert_series_equal(s, exp)\n\n        s = Series(Index(index, dtype=object), dtype=object)\n        exp = Series(index).astype(object)\n        tm.assert_series_equal(s, exp)\n\n        s = Series(index.astype(object), dtype=object)\n        exp = Series(index).astype(object)\n        tm.assert_series_equal(s, exp)\n\n    @pytest.mark.parametrize(\"dtype\", [np.datetime64, np.timedelta64])\n    def test_constructor_generic_timestamp_no_frequency(self, dtype, request):\n        # see gh-15524, gh-15987\n        msg = \"dtype has no unit. Please pass in\"\n\n        if np.dtype(dtype).name not in [\"timedelta64\", \"datetime64\"]:\n            mark = pytest.mark.xfail(reason=\"GH#33890 Is assigned ns unit\")\n            request.applymarker(mark)\n\n        with pytest.raises(ValueError, match=msg):\n            Series([], dtype=dtype)\n\n    @pytest.mark.parametrize(\"unit\", [\"ps\", \"as\", \"fs\", \"Y\", \"M\", \"W\", \"D\", \"h\", \"m\"])\n    @pytest.mark.parametrize(\"kind\", [\"m\", \"M\"])\n    def test_constructor_generic_timestamp_bad_frequency(self, kind, unit):\n        # see gh-15524, gh-15987\n        # as of 2.0 we raise on any non-supported unit rather than silently\n        #  cast to nanos; previously we only raised for frequencies higher\n        #  than ns\n        dtype = f\"{kind}8[{unit}]\"\n\n        msg = \"dtype=.* is not supported. Supported resolutions are\"\n        with pytest.raises(TypeError, match=msg):\n            Series([], dtype=dtype)\n\n        with pytest.raises(TypeError, match=msg):\n            # pre-2.0 the DataFrame cast raised but the Series case did not\n            DataFrame([[0]], dtype=dtype)\n\n    @pytest.mark.parametrize(\"dtype\", [None, \"uint8\", \"category\"])\n    def test_constructor_range_dtype(self, dtype):\n        # GH 16804\n        expected = Series([0, 1, 2, 3, 4], dtype=dtype or \"int64\")\n        result = Series(range(5), dtype=dtype)\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_range_overflows(self):\n        # GH#30173 range objects that overflow int64\n        rng = range(2**63, 2**63 + 4)\n        ser = Series(rng)\n        expected = Series(list(rng))\n        tm.assert_series_equal(ser, expected)\n        assert list(ser) == list(rng)\n        assert ser.dtype == np.uint64\n\n        rng2 = range(2**63 + 4, 2**63, -1)\n        ser2 = Series(rng2)\n        expected2 = Series(list(rng2))\n        tm.assert_series_equal(ser2, expected2)\n        assert list(ser2) == list(rng2)\n        assert ser2.dtype == np.uint64\n\n        rng3 = range(-(2**63), -(2**63) - 4, -1)\n        ser3 = Series(rng3)\n        expected3 = Series(list(rng3))\n        tm.assert_series_equal(ser3, expected3)\n        assert list(ser3) == list(rng3)\n        assert ser3.dtype == object\n\n        rng4 = range(2**73, 2**73 + 4)\n        ser4 = Series(rng4)\n        expected4 = Series(list(rng4))\n        tm.assert_series_equal(ser4, expected4)\n        assert list(ser4) == list(rng4)\n        assert ser4.dtype == object\n\n    def test_constructor_tz_mixed_data(self):\n        # GH 13051\n        dt_list = [\n            Timestamp(\"2016-05-01 02:03:37\"),\n            Timestamp(\"2016-04-30 19:03:37-0700\", tz=\"US/Pacific\"),\n        ]\n        result = Series(dt_list)\n        expected = Series(dt_list, dtype=object)\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"pydt\", [True, False])\n    def test_constructor_data_aware_dtype_naive(self, tz_aware_fixture, pydt):\n        # GH#25843, GH#41555, GH#33401\n        tz = tz_aware_fixture\n        ts = Timestamp(\"2019\", tz=tz)\n        if pydt:\n            ts = ts.to_pydatetime()\n\n        msg = (\n            \"Cannot convert timezone-aware data to timezone-naive dtype. \"\n            r\"Use pd.Series\\(values\\).dt.tz_localize\\(None\\) instead.\"\n        )\n        with pytest.raises(ValueError, match=msg):\n            Series([ts], dtype=\"datetime64[ns]\")\n\n        with pytest.raises(ValueError, match=msg):\n            Series(np.array([ts], dtype=object), dtype=\"datetime64[ns]\")\n\n        with pytest.raises(ValueError, match=msg):\n            Series({0: ts}, dtype=\"datetime64[ns]\")\n\n        msg = \"Cannot unbox tzaware Timestamp to tznaive dtype\"\n        with pytest.raises(TypeError, match=msg):\n            Series(ts, index=[0], dtype=\"datetime64[ns]\")\n\n    def test_constructor_datetime64(self):\n        rng = date_range(\"1/1/2000 00:00:00\", \"1/1/2000 1:59:50\", freq=\"10s\")\n        dates = np.asarray(rng)\n\n        series = Series(dates)\n        assert np.issubdtype(series.dtype, np.dtype(\"M8[ns]\"))\n\n    def test_constructor_datetimelike_scalar_to_string_dtype(\n        self, nullable_string_dtype\n    ):\n        # https://github.com/pandas-dev/pandas/pull/33846\n        result = Series(\"M\", index=[1, 2, 3], dtype=nullable_string_dtype)\n        expected = Series([\"M\", \"M\", \"M\"], index=[1, 2, 3], dtype=nullable_string_dtype)\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"box\", [lambda x: x, np.datetime64])\n    def test_constructor_sparse_datetime64(self, box):\n        # https://github.com/pandas-dev/pandas/issues/35762\n        values = [box(\"2012-01-01\"), box(\"2013-01-01\")]\n        dtype = pd.SparseDtype(\"datetime64[ns]\")\n        result = Series(values, dtype=dtype)\n        arr = pd.arrays.SparseArray(values, dtype=dtype)\n        expected = Series(arr)\n        tm.assert_series_equal(result, expected)\n\n    def test_construction_from_ordered_collection(self):\n        # https://github.com/pandas-dev/pandas/issues/36044\n        result = Series({\"a\": 1, \"b\": 2}.keys())\n        expected = Series([\"a\", \"b\"])\n        tm.assert_series_equal(result, expected)\n\n        result = Series({\"a\": 1, \"b\": 2}.values())\n        expected = Series([1, 2])\n        tm.assert_series_equal(result, expected)\n\n    def test_construction_from_large_int_scalar_no_overflow(self):\n        # https://github.com/pandas-dev/pandas/issues/36291\n        n = 1_000_000_000_000_000_000_000\n        result = Series(n, index=[0])\n        expected = Series(n)\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_list_of_periods_infers_period_dtype(self):\n        series = Series(list(period_range(\"2000-01-01\", periods=10, freq=\"D\")))\n        assert series.dtype == \"Period[D]\"\n\n        series = Series(\n            [Period(\"2011-01-01\", freq=\"D\"), Period(\"2011-02-01\", freq=\"D\")]\n        )\n        assert series.dtype == \"Period[D]\"\n\n    def test_constructor_subclass_dict(self, dict_subclass):\n        data = dict_subclass((x, 10.0 * x) for x in range(10))\n        series = Series(data)\n        expected = Series(dict(data.items()))\n        tm.assert_series_equal(series, expected)\n\n    def test_constructor_ordereddict(self):\n        # GH3283\n        data = OrderedDict(\n            (f\"col{i}\", np.random.default_rng(2).random()) for i in range(12)\n        )\n\n        series = Series(data)\n        expected = Series(list(data.values()), list(data.keys()))\n        tm.assert_series_equal(series, expected)\n\n        # Test with subclass\n        class A(OrderedDict):\n            pass\n\n        series = Series(A(data))\n        tm.assert_series_equal(series, expected)\n\n    @pytest.mark.parametrize(\n        \"data, expected_index_multi\",\n        [\n            ({(\"a\", \"a\"): 0.0, (\"b\", \"a\"): 1.0, (\"b\", \"c\"): 2.0}, True),\n            ({(\"a\",): 0.0, (\"a\", \"b\"): 1.0}, True),\n            ({\"z\": 111.0, (\"a\", \"a\"): 0.0, (\"b\", \"a\"): 1.0, (\"b\", \"c\"): 2.0}, False),\n        ],\n    )\n    def test_constructor_dict_multiindex(self, data, expected_index_multi):\n        # GH#60695\n        result = Series(data)\n\n        if expected_index_multi:\n            expected = Series(\n                list(data.values()),\n                index=MultiIndex.from_tuples(list(data.keys())),\n            )\n            tm.assert_series_equal(result, expected)\n        else:\n            expected = Series(\n                list(data.values()),\n                index=Index(list(data.keys())),\n            )\n            tm.assert_series_equal(result, expected)\n\n    def test_constructor_dict_multiindex_reindex_flat(self):\n        # construction involves reindexing with a MultiIndex corner case\n        data = {(\"i\", \"i\"): 0, (\"i\", \"j\"): 1, (\"j\", \"i\"): 2, \"j\": np.nan}\n        expected = Series(data)\n\n        result = Series(expected[:-1].to_dict(), index=expected.index)\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_dict_timedelta_index(self):\n        # GH #12169 : Resample category data with timedelta index\n        # construct Series from dict as data and TimedeltaIndex as index\n        # will result NaN in result Series data\n        expected = Series(\n            data=[\"A\", \"B\", \"C\"], index=pd.to_timedelta([0, 10, 20], unit=\"s\")\n        )\n\n        result = Series(\n            data={\n                pd.to_timedelta(0, unit=\"s\"): \"A\",\n                pd.to_timedelta(10, unit=\"s\"): \"B\",\n                pd.to_timedelta(20, unit=\"s\"): \"C\",\n            },\n            index=pd.to_timedelta([0, 10, 20], unit=\"s\"),\n        )\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_infer_index_tz(self):\n        values = [188.5, 328.25]\n        tzinfo = tzoffset(None, 7200)\n        index = [\n            datetime(2012, 5, 11, 11, tzinfo=tzinfo),\n            datetime(2012, 5, 11, 12, tzinfo=tzinfo),\n        ]\n        series = Series(data=values, index=index)\n\n        assert series.index.tz == tzinfo\n\n        # it works! GH#2443\n        repr(series.index[0])\n\n    def test_constructor_with_pandas_dtype(self):\n        # going through 2D->1D path\n        vals = [(1,), (2,), (3,)]\n        ser = Series(vals)\n        dtype = ser.array.dtype  # NumpyEADtype\n        ser2 = Series(vals, dtype=dtype)\n        tm.assert_series_equal(ser, ser2)\n\n    def test_constructor_int_dtype_missing_values(self):\n        # GH#43017\n        result = Series(index=[0], dtype=\"int64\")\n        expected = Series(np.nan, index=[0], dtype=\"float64\")\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_bool_dtype_missing_values(self):\n        # GH#43018\n        result = Series(index=[0], dtype=\"bool\")\n        expected = Series(True, index=[0], dtype=\"bool\")\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_int64_dtype(self, any_int_dtype):\n        # GH#44923\n        result = Series([\"0\", \"1\", \"2\"], dtype=any_int_dtype)\n        expected = Series([0, 1, 2], dtype=any_int_dtype)\n        tm.assert_series_equal(result, expected)\n\n    def test_constructor_raise_on_lossy_conversion_of_strings(self):\n        # GH#44923\n        if not np_version_gt2:\n            raises = pytest.raises(\n                ValueError, match=\"string values cannot be losslessly cast to int8\"\n            )\n        else:\n            raises = pytest.raises(\n                OverflowError, match=\"The elements provided in the data\"\n            )\n        with raises:\n            Series([\"128\"], dtype=\"int8\")\n\n    def test_constructor_dtype_timedelta_alternative_construct(self):\n        # GH#35465\n        result = Series([1000000, 200000, 3000000], dtype=\"timedelta64[ns]\")\n        expected = Series(pd.to_timedelta([1000000, 200000, 3000000], unit=\"ns\"))\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.xfail(\n        reason=\"Not clear what the correct expected behavior should be with \"\n        \"integers now that we support non-nano. ATM (2022-10-08) we treat ints \"\n        \"as nanoseconds, then cast to the requested dtype. xref #48312\"\n    )\n    def test_constructor_dtype_timedelta_ns_s(self):\n        # GH#35465\n        result = Series([1000000, 200000, 3000000], dtype=\"timedelta64[ns]\")\n        expected = Series([1000000, 200000, 3000000], dtype=\"timedelta64[s]\")\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.xfail(\n        reason=\"Not clear what the correct expected behavior should be with \"\n        \"integers now that we support non-nano. ATM (2022-10-08) we treat ints \"\n        \"as nanoseconds, then cast to the requested dtype. xref #48312\"\n    )\n    def test_constructor_dtype_timedelta_ns_s_astype_int64(self):\n        # GH#35465\n        result = Series([1000000, 200000, 3000000], dtype=\"timedelta64[ns]\").astype(\n            \"int64\"\n        )\n        expected = Series([1000000, 200000, 3000000], dtype=\"timedelta64[s]\").astype(\n            \"int64\"\n        )\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.filterwarnings(\n        \"ignore:elementwise comparison failed:DeprecationWarning\"\n    )\n    @pytest.mark.parametrize(\"func\", [Series, DataFrame, Index, pd.array])\n    def test_constructor_mismatched_null_nullable_dtype(\n        self, func, any_numeric_ea_dtype\n    ):\n        # GH#44514\n        msg = \"|\".join(\n            [\n                \"cannot safely cast non-equivalent object\",\n                r\"int\\(\\) argument must be a string, a bytes-like object \"\n                \"or a (real )?number\",\n                r\"Cannot cast array data from dtype\\('O'\\) to dtype\\('float64'\\) \"\n                \"according to the rule 'safe'\",\n                \"object cannot be converted to a FloatingDtype\",\n                \"'values' contains non-numeric NA\",\n            ]\n        )\n\n        for null in tm.NP_NAT_OBJECTS + [NaT]:\n            with pytest.raises(TypeError, match=msg):\n                func([null, 1.0, 3.0], dtype=any_numeric_ea_dtype)\n\n    def test_series_constructor_ea_int_from_bool(self):\n        # GH#42137\n        result = Series([True, False, True, pd.NA], dtype=\"Int64\")\n        expected = Series([1, 0, 1, pd.NA], dtype=\"Int64\")\n        tm.assert_series_equal(result, expected)\n\n        result = Series([True, False, True], dtype=\"Int64\")\n        expected = Series([1, 0, 1], dtype=\"Int64\")\n        tm.assert_series_equal(result, expected)\n\n    def test_series_constructor_ea_int_from_string_bool(self):\n        # GH#42137\n        with pytest.raises(ValueError, match=\"invalid literal\"):\n            Series([\"True\", \"False\", \"True\", pd.NA], dtype=\"Int64\")\n\n    @pytest.mark.parametrize(\"val\", [1, 1.0])\n    def test_series_constructor_overflow_uint_ea(self, val):\n        # GH#38798\n        max_val = np.iinfo(np.uint64).max - 1\n        result = Series([max_val, val], dtype=\"UInt64\")\n        expected = Series(np.array([max_val, 1], dtype=\"uint64\"), dtype=\"UInt64\")\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"val\", [1, 1.0])\n    def test_series_constructor_overflow_uint_ea_with_na(self, val):\n        # GH#38798\n        max_val = np.iinfo(np.uint64).max - 1\n        result = Series([max_val, val, pd.NA], dtype=\"UInt64\")\n        expected = Series(\n            IntegerArray(\n                np.array([max_val, 1, 0], dtype=\"uint64\"),\n                np.array([0, 0, 1], dtype=np.bool_),\n            )\n        )\n        tm.assert_series_equal(result, expected)\n\n    def test_series_constructor_overflow_uint_with_nan(self):\n        # GH#38798\n        max_val = np.iinfo(np.uint64).max - 1\n        result = Series([max_val, np.nan], dtype=\"UInt64\")\n        expected = Series(\n            IntegerArray(\n                np.array([max_val, 1], dtype=\"uint64\"),\n                np.array([0, 1], dtype=np.bool_),\n            )\n        )\n        tm.assert_series_equal(result, expected)\n\n    def test_series_constructor_ea_all_na(self):\n        # GH#38798\n        result = Series([np.nan, np.nan], dtype=\"UInt64\")\n        expected = Series(\n            IntegerArray(\n                np.array([1, 1], dtype=\"uint64\"),\n                np.array([1, 1], dtype=np.bool_),\n            )\n        )\n        tm.assert_series_equal(result, expected)\n\n    def test_series_from_index_dtype_equal_does_not_copy(self):\n        # GH#52008\n        idx = Index([1, 2, 3])\n        expected = idx.copy(deep=True)\n        ser = Series(idx, dtype=\"int64\")\n        ser.iloc[0] = 100\n        tm.assert_index_equal(idx, expected)\n\n    def test_series_string_inference(self):\n        # GH#54430\n        with pd.option_context(\"future.infer_string\", True):\n            ser = Series([\"a\", \"b\"])\n        dtype = pd.StringDtype(\"pyarrow\" if HAS_PYARROW else \"python\", na_value=np.nan)\n        expected = Series([\"a\", \"b\"], dtype=dtype)\n        tm.assert_series_equal(ser, expected)\n\n        expected = Series([\"a\", 1], dtype=\"object\")\n        with pd.option_context(\"future.infer_string\", True):\n            ser = Series([\"a\", 1])\n        tm.assert_series_equal(ser, expected)\n\n    @pytest.mark.parametrize(\"na_value\", [None, np.nan, pd.NA])\n    def test_series_string_with_na_inference(self, na_value):\n        # GH#54430\n        with pd.option_context(\"future.infer_string\", True):\n            ser = Series([\"a\", na_value])\n        dtype = pd.StringDtype(\"pyarrow\" if HAS_PYARROW else \"python\", na_value=np.nan)\n        expected = Series([\"a\", None], dtype=dtype)\n        tm.assert_series_equal(ser, expected)\n\n    def test_series_string_inference_scalar(self):\n        # GH#54430\n        with pd.option_context(\"future.infer_string\", True):\n            ser = Series(\"a\", index=[1])\n        dtype = pd.StringDtype(\"pyarrow\" if HAS_PYARROW else \"python\", na_value=np.nan)\n        expected = Series(\"a\", index=[1], dtype=dtype)\n        tm.assert_series_equal(ser, expected)\n\n    def test_series_string_inference_array_string_dtype(self):\n        # GH#54496\n        with pd.option_context(\"future.infer_string\", True):\n            ser = Series(np.array([\"a\", \"b\"]))\n        dtype = pd.StringDtype(\"pyarrow\" if HAS_PYARROW else \"python\", na_value=np.nan)\n        expected = Series([\"a\", \"b\"], dtype=dtype)\n        tm.assert_series_equal(ser, expected)\n\n    def test_series_string_inference_storage_definition(self):\n        # https://github.com/pandas-dev/pandas/issues/54793\n        # but after PDEP-14 (string dtype), it was decided to keep dtype=\"string\"\n        # returning the NA string dtype, so expected is changed from\n        # \"string[pyarrow_numpy]\" to \"string[python]\"\n        expected = Series([\"a\", \"b\"], dtype=\"string[python]\")\n        with pd.option_context(\"future.infer_string\", True):\n            result = Series([\"a\", \"b\"], dtype=\"string\")\n        tm.assert_series_equal(result, expected)\n\n        expected = Series([\"a\", \"b\"], dtype=pd.StringDtype(na_value=np.nan))\n        with pd.option_context(\"future.infer_string\", True):\n            result = Series([\"a\", \"b\"], dtype=\"str\")\n        tm.assert_series_equal(result, expected)\n\n    def test_series_constructor_infer_string_scalar(self):\n        # GH#55537\n        with pd.option_context(\"future.infer_string\", True):\n            ser = Series(\"a\", index=[1, 2], dtype=\"string[python]\")\n        expected = Series([\"a\", \"a\"], index=[1, 2], dtype=\"string[python]\")\n        tm.assert_series_equal(ser, expected)\n        assert ser.dtype.storage == \"python\"\n\n    def test_series_string_inference_na_first(self):\n        # GH#55655\n        with pd.option_context(\"future.infer_string\", True):\n            result = Series([pd.NA, \"b\"])\n        dtype = pd.StringDtype(\"pyarrow\" if HAS_PYARROW else \"python\", na_value=np.nan)\n        expected = Series([None, \"b\"], dtype=dtype)\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"klass\", [Series, Index])\n    def test_inference_on_pandas_objects(self, klass):\n        # GH#56012\n        obj = klass([Timestamp(\"2019-12-31\")], dtype=object)\n        # This doesn't do inference\n        result = Series(obj)\n        assert result.dtype == np.object_\n\n\nclass TestSeriesConstructorIndexCoercion:\n    def test_series_constructor_datetimelike_index_coercion(self):\n        idx = date_range(\"2020-01-01\", periods=5)\n        ser = Series(\n            np.random.default_rng(2).standard_normal(len(idx)), idx.astype(object)\n        )\n        # as of 2.0, we no longer silently cast the object-dtype index\n        #  to DatetimeIndex GH#39307, GH#23598\n        assert not isinstance(ser.index, DatetimeIndex)\n\n    @pytest.mark.parametrize(\"container\", [None, np.array, Series, Index])\n    @pytest.mark.parametrize(\"data\", [1.0, range(4)])\n    def test_series_constructor_infer_multiindex(self, container, data):\n        indexes = [[\"a\", \"a\", \"b\", \"b\"], [\"x\", \"y\", \"x\", \"y\"]]\n        if container is not None:\n            indexes = [container(ind) for ind in indexes]\n\n        multi = Series(data, index=indexes)\n        assert isinstance(multi.index, MultiIndex)\n\n    # TODO: make this not cast to object in pandas 3.0\n    @pytest.mark.skipif(\n        not np_version_gt2, reason=\"StringDType only available in numpy 2 and above\"\n    )\n    @pytest.mark.parametrize(\n        \"data\",\n        [\n            [\"a\", \"b\", \"c\"],\n            [\"a\", \"b\", np.nan],\n        ],\n    )\n    def test_np_string_array_object_cast(self, data):\n        from numpy.dtypes import StringDType\n\n        arr = np.array(data, dtype=StringDType())\n        res = Series(arr)\n        assert res.dtype == np.object_\n        assert (res == data).all()\n\n\nclass TestSeriesConstructorInternals:\n    def test_constructor_no_pandas_array(self):\n        ser = Series([1, 2, 3])\n        result = Series(ser.array)\n        tm.assert_series_equal(ser, result)\n        assert isinstance(result._mgr.blocks[0], NumpyBlock)\n        assert result._mgr.blocks[0].is_numeric\n\n    def test_from_array(self):\n        result = Series(pd.array([\"1h\", \"2h\"], dtype=\"timedelta64[ns]\"))\n        assert result._mgr.blocks[0].is_extension is False\n\n        result = Series(pd.array([\"2015\"], dtype=\"datetime64[ns]\"))\n        assert result._mgr.blocks[0].is_extension is False\n\n    def test_from_list_dtype(self):\n        result = Series([\"1h\", \"2h\"], dtype=\"timedelta64[ns]\")\n        assert result._mgr.blocks[0].is_extension is False\n\n        result = Series([\"2015\"], dtype=\"datetime64[ns]\")\n        assert result._mgr.blocks[0].is_extension is False\n\n\ndef test_constructor(rand_series_with_duplicate_datetimeindex):\n    dups = rand_series_with_duplicate_datetimeindex\n    assert isinstance(dups, Series)\n    assert isinstance(dups.index, DatetimeIndex)\n\n\n@pytest.mark.parametrize(\n    \"input_dict,expected\",\n    [\n        ({0: 0}, np.array([[0]], dtype=np.int64)),\n        ({\"a\": \"a\"}, np.array([[\"a\"]], dtype=object)),\n        ({1: 1}, np.array([[1]], dtype=np.int64)),\n    ],\n)\ndef test_numpy_array(input_dict, expected):\n    result = np.array([Series(input_dict)])\n    tm.assert_numpy_array_equal(result, expected)\n\n\ndef test_index_ordered_dict_keys():\n    # GH 22077\n\n    param_index = OrderedDict(\n        [\n            (((\"a\", \"b\"), (\"c\", \"d\")), 1),\n            (((\"a\", None), (\"c\", \"d\")), 2),\n        ]\n    )\n    series = Series([1, 2], index=param_index.keys())\n    expected = Series(\n        [1, 2],\n        index=MultiIndex.from_tuples(\n            [((\"a\", \"b\"), (\"c\", \"d\")), ((\"a\", None), (\"c\", \"d\"))]\n        ),\n    )\n    tm.assert_series_equal(series, expected)\n\n\n@pytest.mark.parametrize(\n    \"input_list\",\n    [\n        [1, complex(\"nan\"), 2],\n        [1 + 1j, complex(\"nan\"), 2 + 2j],\n    ],\n)\ndef test_series_with_complex_nan(input_list):\n    # GH#53627\n    ser = Series(input_list)\n    result = Series(ser.array)\n    assert ser.dtype == \"complex128\"\n    tm.assert_series_equal(ser, result)\n\n\ndef test_dict_keys_rangeindex():\n    result = Series({0: 1, 1: 2})\n    expected = Series([1, 2], index=RangeIndex(2))\n    tm.assert_series_equal(result, expected, check_index_type=True)\n"
    }
  ],
  "questions": [
    "hey @rhshadrach, \nI tried replacing the `isinstance` to use `is_list_like`, but that alone doesn't seem to fix the issue. The test case(`test_constructor_dict_tuple_indexer`) continues to fail, and I'm unsure if the problem lies with the test setup or if there's more to adjust? \n\nHere's the test result for `test_constructor_tuple_indexer`\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?><testsuites><testsuite name=\"pytest\" errors=\"0\" failures=\"1\" skipped=\"0\" tests=\"1\" time=\"0.594\" timestamp=\"2025-01-23T21:11:54.485741+05:30\" hostname=\"archlap\"><testcase classname=\"pandas.tests.series.test_constructors.TestSeriesConstructors\" name=\"test_constructor_dict_tuple_indexer\" time=\"0.008\"><failure message=\"AssertionError: Series.index level [2] are different&#10;&#10;Attribute &quot;dtype&quot; are different&#10;[left]:  object&#10;[right]: float64\">left = Index([], dtype='object'), right = Index([nan], dtype='float64'), obj = 'Series.index level [2]'\n\n    def _check_types(left, right, obj: str = \"Index\") -&gt; None:\n        if not exact:\n            return\n    \n        assert_class_equal(left, right, exact=exact, obj=obj)\n&gt;       assert_attr_equal(\"inferred_type\", left, right, obj=obj)\nE       AssertionError: Series.index level [2] are different\nE       \nE       Attribute \"inferred_type\" are different\nE       [left]:  empty\nE       [right]: floating\n\npandas/_testing/asserters.py:246: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &lt;pandas.tests.series.test_constructors.TestSeriesConstructors object at 0x72f8efd00b40&gt;\n\n    def test_constructor_dict_tuple_indexer(self):\n        # GH 12948\n        data = {(1, 1, None): -1.0}\n        result = Series(data)\n        expected = Series(\n            -1.0, index=MultiIndex(levels=[[1], [1], [np.nan]], codes=[[0], [0], [-1]])\n        )\n&gt;       tm.assert_series_equal(result, expected)\n\npandas/tests/series/test_constructors.py:1417: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nleft = Index([nan], dtype='object'), right = Index([nan], dtype='float64'), obj = 'Series.index level [2]'\n\n    def _check_types(left, right, obj: str = \"Index\") -&gt; None:\n        if not exact:\n            return\n    \n        assert_class_equal(left, right, exact=exact, obj=obj)\n        assert_attr_equal(\"inferred_type\", left, right, obj=obj)\n    \n        # Skip exact dtype checking when `check_categorical` is False\n        if isinstance(left.dtype, CategoricalDtype) and isinstance(\n            right.dtype, CategoricalDtype\n        ):\n            if check_categorical:\n                assert_attr_equal(\"dtype\", left, right, obj=obj)\n                assert_index_equal(left.categories, right.categories, exact=exact)\n            return\n    \n&gt;       assert_attr_equal(\"dtype\", left, right, obj=obj)\nE       AssertionError: Series.index level [2] are different\nE       \nE       Attribute \"dtype\" are different\nE       [left]:  object\nE       [right]: float64\n\npandas/_testing/asserters.py:257: AssertionError</failure></testcase></testsuite></testsuites>\n```",
    "Hi I'm new and this is first I looked at.  I know I didn't \"take\" it, but I think looking at it briefly try changing line 539 to\narrs = zip_longest(*tuples, fillvalue=np.nan)\nalso need to include\nimport from itertools zip_longest.\n\nThis will create an index with the number of dimensions of the longest iterable, even if it is not the first, for instance ((1,2), (3,), (3,4,5), (5,) ) gets us ((1, 3, 3, 5), (2, nan, 4, nan), (nan, nan, 5, nan)).\n\nOr should I take it and do it ?  Not sure of etiquette. @VishalSindham are you doing similar ?",
    "No, here is the unit test I added to pandas/tests/indexes/multi/test_constructors.py\r\n\r\n\r\n@pytest.mark.parametrize(\"keys, expected\", (\r\n    (((\"l1\",), (\"l1\",\"l2\")), ((\"l1\", np.nan), (\"l1\",\"l2\"))),\r\n    (((\"l1\",\"l2\",), (\"l1\",)), ((\"l1\",\"l2\"), (\"l1\", np.nan))),\r\n))\r\ndef test_from_tuples_with_various_tuple_lengths(keys, expected):\r\n    # Issue 60695\r\n    idx = MultiIndex.from_tuples(keys)\r\n    assert tuple(idx) == expected\r\n\r\n________________________________\r\nFrom: Anurag Varma ***@***.***>\r\nSent: 16 February 2025 09:43\r\nTo: pandas-dev/pandas ***@***.***>\r\nCc: Simon ***@***.***>; Assign ***@***.***>\r\nSubject: Re: [pandas-dev/pandas] BUG: Series constructor from dictionary drops key (index) levels when not all keys have same number of entries (Issue #60695)\r\n\r\n[Anurag-Varma]Anurag-Varma left a comment (pandas-dev/pandas#60695)<https://github.com/pandas-dev/pandas/issues/60695#issuecomment-2661347323>\r\n\r\nHi @ArneBinder<https://github.com/ArneBinder> @rhshadrach<https://github.com/rhshadrach>\r\n\r\nIs the expected output which i mentioned below correct for the given code ?\r\n\r\nimport pandas as pd\r\n\r\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\r\n# Expected output:\r\n# l1  NaN    v1\r\n#      l2     v2\r\n# dtype: object\r\n\r\n—\r\nReply to this email directly, view it on GitHub<https://github.com/pandas-dev/pandas/issues/60695#issuecomment-2661347323>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AD3Z4PKNQHHHP67CZB7QOLT2QBMVDAVCNFSM6AAAAABVAHQGESVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMNRRGM2DOMZSGM>.\r\nYou are receiving this because you were assigned.",
    "> Hi [@rhshadrach](https://github.com/rhshadrach)\n> \n> I was trying to fix the bug but below test case was failing:\n> \n> `pandas/tests/series/methods/test_map.py::test_map_dict_with_tuple_keys`\n> \n> When I further tried to fix it, found out that the `Series.map()` is failing for multiple examples with tuples as keys in the dictionary.\n> \n> So created a new issue for that: [#60988](https://github.com/pandas-dev/pandas/issues/60988)\n\nI solved this current issue but the above test case is failing so unable to send a new commit in my PR #60944 \n\nShould i mark it as xfail and proceed forward ?"
  ],
  "golden_answers": [
    "Hi I'm new and this is first I looked at.  I know I didn't \"take\" it, but I think looking at it briefly try changing line 539 to\narrs = zip_longest(*tuples, fillvalue=np.nan)\nalso need to include\nimport from itertools zip_longest.\n\nThis will create an index with the number of dimensions of the longest iterable, even if it is not the first, for instance ((1,2), (3,), (3,4,5), (5,) ) gets us ((1, 3, 3, 5), (2, nan, 4, nan), (nan, nan, 5, nan)).\n\nOr should I take it and do it ?  Not sure of etiquette. @VishalSindham are you doing similar ?",
    "I've tried doing that, even when manually converting `None` values to `np.nan` doesn't resolve the issue with my test cases. ig it has something to do with how python's parsing these types. \n\nThe only solution I've found to make the tests pass is by adding `check_index_type=False` to the assertion statement in `test_constructor_dict_tuple_indexer`.",
    "Hi @siber64 \n\nWhat you said is correct, but thats only of tuple of tuples given input to MultiIndex.from_tuples\n\nMy question is diffferent, i am using pd.Series with dictionary and tuple is keys in a dictionary.\n\nSo, I think pandas treats this as Multiindex and the first index of each tuple becomes the primary index and the second element becomes the sub-index.\n\nExample of existing behaviour:\n```python\nimport pandas as pd\npd.Series({(\"l1\",\"l3\"):\"v1\", (\"l1\",\"l2\"): \"v2\"})\n# Existing Output:\n# l1  l3    v1\n#      l2    v2\n# dtype: object\n```\n\nExample of error behaviour:\n```python\nimport pandas as pd\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\n# Error Output:\n# l1     v1\n# l1     v2\n# dtype: object\n```\n\nExample of expected behaviour:\n```python\nimport pandas as pd\npd.Series({(\"l1\",):\"v1\", (\"l1\",\"l2\"): \"v2\"})\n# Expected Output:\n# l1  NaN   v1\n#      l2        v2\n# dtype: object\n```",
    "> I solved this current issue but the above test case is failing so unable to send a new commit in my PR [#60944](https://github.com/pandas-dev/pandas/pull/60944)\n\nIs this related to this change: https://github.com/pandas-dev/pandas/pull/60944/files#r1966545298?\n\nIf not, I do not understand your comment. It is best to discuss these things on the PR, where the discussion can happen next to the code involved."
  ],
  "questions_generated": [
    "What is the root cause of the bug when constructing a Series from a dictionary with tuple keys of different lengths?",
    "How does the current implementation of MultiIndex.from_tuples handle tuples with varying lengths, and why is this problematic?",
    "In the provided code context, which imports and utilities from pandas are relevant to understanding the Series constructor behavior?",
    "What is the expected behavior of the Series constructor when dealing with dictionaries that have tuple keys of different lengths?",
    "How can the issue with the Series constructor be potentially resolved within the pandas codebase?"
  ],
  "golden_answers_generated": [
    "The bug arises because the Series constructor uses MultiIndex.from_tuples internally with a tuple of tuples, which results in the dropping of key levels for tuples with fewer elements. This causes the MultiIndex to not pad shorter tuples with NaN values, leading to duplicated index values.",
    "The current implementation of MultiIndex.from_tuples creates a MultiIndex by taking a tuple of tuples as input, which causes it to truncate longer tuples to match the length of the shortest tuple. This is problematic because it leads to the loss of key information and can result in duplicated index values, which are not expected with a dictionary that inherently has unique keys.",
    "Relevant imports and utilities include MultiIndex from pandas.core.dtypes.generic, which is used to construct the index from tuples, and ensure_object from pandas.core.dtypes.common, which ensures the data type handling within the construction process. Understanding these components is crucial for diagnosing and resolving the discrepancy in how tuple keys are handled.",
    "The expected behavior is that the Series constructor should create a MultiIndex where the shorter tuples are padded with NaN values to match the length of the longest tuple. This ensures that all key levels are preserved and no information is lost, maintaining the uniqueness of dictionary keys in the index.",
    "To resolve the issue, the Series constructor or the MultiIndex.from_tuples function could be modified to handle lists of tuples directly, rather than tuples of tuples. This would involve ensuring that all tuples are padded with NaNs to the length of the longest tuple before creating the MultiIndex, preserving all key levels and preventing index duplication."
  ]
}