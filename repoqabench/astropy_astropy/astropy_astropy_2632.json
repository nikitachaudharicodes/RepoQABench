{
  "repo_name": "astropy_astropy",
  "issue_id": "2632",
  "issue_description": "# Save multiple Table objects to one FITS file\n\nI have multiple `astropy.table.Table` objects that I'd like to save to one FITS file.\n\nFrom the docs (see [here](http://astropy.readthedocs.org/en/latest/io/unified.html#fits)) I couldn't figure out how to do this ... for now I create `BinTableHDU` objects by initialising from `table._data` and put those in a `HDUList` ... but there must be a better way for this common task!?\n\n@taldcroft @embray Can you implement and / or document how to do this? (Apologies if I missed it in the docs.)\n",
  "issue_comments": [
    {
      "id": 46211445,
      "user": "embray",
      "body": "I think this is a kind of high-level example of the sort of thing that will be easier (and should be possible) once the FITS table overhaul is underway.  The plan is to make Astropy `Table`s (or some specialized subclass thereof) the main data structure used to set/get data in a `BINTABLE` HDU.  In the meantime I don't know that there's a better way, given that the original classes from PyFITS don't immediately understand what to do with a `Table` object.\n"
    },
    {
      "id": 46212273,
      "user": "embray",
      "body": "Although I should add--ISTM it should be straightforward to add an `append=True` to the FITS-Table connector similar to what we have for the HDF5 connector.  This would allow writing tables to an existing FITS file by simply appending a new HDU.  Then it would be a simple matter of:\n\n```\nfor table in tables:\n    table.write('blah.fits', append=True)\n```\n\nAfter all, we want to move people in the direction of using such a high level interface over using the low-level FITS objects.  The one downside to this is that there are some I/O-related implementation details in the fits module that might make this a little tricky to implement efficiently, but I'm working on fixing that...\n"
    },
    {
      "id": 46315767,
      "user": "cdeil",
      "body": "@embray Is there an issue / timescale for the \"FITS table overhaul\"?\nI guess this could take a while ... so :+1: to adding an `append=True` option now.\n"
    },
    {
      "id": 46316663,
      "user": "cdeil",
      "body": "Another thing I noticed is that column `description` is not stored in FITS files ... is this another aspect of \"FITS Table integration is unfinished\" or a bug?\n\n``` python\n>>> from astropy.table import Table, Column\n>>> table = Table()\n>>> table['distance'] = Column([1, 2, 3], unit='m', description='Walking distance')\n>>> table['distance']\n<Column name='distance' unit=u'm' format=None description='Walking distance'>\narray([1, 2, 3])\n>>> table.write('table.fits', overwrite=True)\n>>> table2 = Table.read('table.fits')\n>>> table2['distance']\n<Column name='distance' unit=u'm' format=None description=None>\narray([1, 2, 3])\n```\n"
    },
    {
      "id": 46316950,
      "user": "astrofrog",
      "body": "@cdeil - there isn't really a standard for storing the description as far as I understand, hence why it wasn't done. But we could try and think of a way to do it.\n"
    },
    {
      "id": 46318511,
      "user": "cdeil",
      "body": "@astrofrog I think you're right: #1768 .... too bad this isn't supported in FITS.\n"
    },
    {
      "id": 46335337,
      "user": "embray",
      "body": "There's been a timetable for that for like a year--but its coordinates keep being translated along the time axis :)\n"
    },
    {
      "id": 46335638,
      "user": "embray",
      "body": "As for the description, I've had a request in the past for supporting arbitrary table column attributes by mapping some header keyword like TDESCRn to the appropriate columns.  I think that might be one approach, but it wouldn't be built-in to work automatically since, as @astrofrog says, this isn't standard and there may be many conventions for capturing this type of information (if at all) that have nothing to do with Txxxxx keywords (for example, writing it COMMENT keywords).\n"
    },
    {
      "id": 46335754,
      "user": "embray",
      "body": "I suppose one other possibility would be to allow supplying an arbitrary function that does the mapping of Table metadata to wherever that metadata should go in the FITS header.\n"
    },
    {
      "id": 70064958,
      "user": "astrofrog",
      "body": "This is non-trivial, so removing the milestone since it won't get done for 1.0\n"
    },
    {
      "id": 70273720,
      "user": "astrofrog",
      "body": "Just for anyone looking at this, there are several things we can do here:\n- Make it so that `Table.write(HDU)` works\n- Make it so that e.g. `Table.write(HDUList, append=True)` works\n"
    },
    {
      "id": 70281392,
      "user": "cdeil",
      "body": "A related issue I ran into again today is that writing all-ascii string columns to FITS in Python 3 silently encodes the characters with zero-padded 4 bytes, i.e. `abc` is stored as `a\\x00\\x00\\x00b\\x00\\x00\\x00c\\x00\\x00\\x00`.\n\nI still don't understand why, but writing the tables to separate FITS files worked:\n\n``` python\ntable1.write('data1.fits')\ntable2.write('data2.fits')\n```\n\nand instead writing the tables to one FITS file didn't work:\n\n``` python\nhdu_list = fits.HDUList()\nhdu_list.append(fits.BinTableHDU(table1.as_array(), name='data1'))\nhdu_list.append(fits.BinTableHDU(table2.as_array(), name='data2'))\nhdu_list.writeto('data.fits', clobber=True)\n```\n\nI didn't manage to reduce this to a useful example / test case, but the crux of the issue is this:\n\n``` python\nwith open('/tmp/data.csv', 'w') as fh:\n    fh.write('name\\n')\n    fh.write('abc\\n')\n\ntable1 = Table.read('/tmp/data.csv')\nprint(table1.dtype)\ntable1.write('/tmp/data.fits', overwrite=True)\ntable2 = Table.read('/tmp/data.fits')\nprint(table2.dtype)\n```\n\n```\n[('name', '<U3')]\n[('name', '<U12')]\n```\n\nIn the I remembered that I had to call this on all tables before converting them to HDUs and everything works:\n\n``` python\ntable.convert_unicode_to_bytestring(python3_only=True)\n```\n\nI thought I'd mention this here in case someone else runs into similar issues or it might even be useful to call `convert_unicode_to_bytestring` by default when converting `Table` to `HDU` or to issue a warning if there are unicode string columns?\n"
    },
    {
      "id": 70298393,
      "user": "embray",
      "body": "@cdeil Could you open a separate issue for this, since I don't think it's directly related to this.\n"
    },
    {
      "id": 70299542,
      "user": "taldcroft",
      "body": "@cdeil - if you open a new issue you could reference this discussion of a real fix for the unicode problem in numpy.  So far this has generated a couple of long discussion but no agreement or action.  Sigh.\n\nhttp://mail.scipy.org/pipermail/numpy-discussion/2014-July/070586.html\n"
    },
    {
      "id": 70300044,
      "user": "embray",
      "body": "Although every time that discussion comes up I point out [this solution](https://github.com/embray/PyFITS/blob/c11abd331cff7e177f0fc551188347e529cde92d/lib/pyfits/fitsrec.py#L999) for which I still haven't heard too many downsides.  It's not 100% perfect, but it gets most of the way there for most cases anyone cares about (which is reading and writing some arrays of strings).  (There are also still some uncertainties about how that implementation should present itself to the user.  For example should it lie about its dtype, etc?)\n"
    },
    {
      "id": 70318335,
      "user": "cdeil",
      "body": "@embray OK, I've tried to file a separate issue about the text column to FITS issue: #3311.\n"
    },
    {
      "id": 90738084,
      "user": "cdeil",
      "body": "I've run into a performance issue [here](https://github.com/gammapy/gammapy/pull/249/files#diff-25f266495d68c172624588a69347a42cR299).\n\nI have two `Table` objects that I want to store in one FITS file.\n\nThe code I use is roughly:\n\n``` python\ntable1 = astropy.table.Table(...)\ntable2 = astropy.table.Table(...)\n\nhdu_list = fits.HDUList()\n\ndata = table1.as_array()\nheader = fits.Header()\nheader.update(table1.meta)\nhdu_list.append(fits.BinTableHDU(data=data, header=header))\n\ndata = table2.as_array()\nheader = fits.Header()\nheader.update(table1.meta)\nhdu_list.append(fits.BinTableHDU(data=data, header=header))\n```\n\nIs there a way to store a `Table` in FITS, but avoid the copy that `as_array` is doing?\n(that's the performance issue I have, one of my tables is several GB in size.)\n\nIs there a simpler way to convert `table.meta` to a `header`? I was looking through the convenience functions / methods in `fits` and `table`, but couldn't find something. There should be way to make a header from a dict in one line, if not directly in the `BinTableHDU` constructor, no?\n"
    },
    {
      "id": 90965002,
      "user": "embray",
      "body": "Since the main topic of this issue here seems to be to extend the `io.fits`-`Table` connector to allow saving multiple tables, maybe theses questions (really two separate questions) would be best addressed in the mailing list or a different issue since it's straying off-topic (I get how it's related to this issue, but I don't want things to get too noisy).\n\n(On the second question, `Header(meta.items())` would do it.)\n"
    },
    {
      "id": 90979304,
      "user": "taldcroft",
      "body": "Sorry to continue the OT question, but just wondering if it you can avoid the memory duplication using the direct FITS table creation interface described at  http://astropy.readthedocs.org/en/latest/io/fits/index.html#creating-a-new-table-file.\n\nIn other words if you assemble the `fits.Column` objects from the individual table `Column` objects, does that avoid the copy?  I think that in using the higher-level `fits.BinTableHDU` interface there is no way around creating a copy of all the individual data columns because that routine needs a numpy array (as far as I can tell).\n"
    },
    {
      "id": 91662833,
      "user": "embray",
      "body": "@taldcroft has the gist of it.  It would be pretty nice if the `from_columns` method worked on `astropy.table.Column` objects too.\n"
    },
    {
      "id": 91663046,
      "user": "embray",
      "body": "Though it still ends up making a copy, because it has to create a record array to then stuff all the column data into.  But I think this way might result in one fewer copy?  Not sure...\n"
    },
    {
      "id": 740536020,
      "user": "stvguest",
      "body": "I'm looking at this as part of the SKA/astropy hackathon. I agree with the suggestion to add an append flag that can be used from the table interface. It is indeed straightforward to add it to write_table_fits. Using the append function, performance is similar to writing the same file multiple times:\r\n\r\ntimeit.timeit(append, number=100)\r\n6.515496399999961\r\ntimeit.timeit(write, number=100)\r\n5.795052499999997"
    }
  ],
  "text_context": "# Save multiple Table objects to one FITS file\n\nI have multiple `astropy.table.Table` objects that I'd like to save to one FITS file.\n\nFrom the docs (see [here](http://astropy.readthedocs.org/en/latest/io/unified.html#fits)) I couldn't figure out how to do this ... for now I create `BinTableHDU` objects by initialising from `table._data` and put those in a `HDUList` ... but there must be a better way for this common task!?\n\n@taldcroft @embray Can you implement and / or document how to do this? (Apologies if I missed it in the docs.)\n\n\nI think this is a kind of high-level example of the sort of thing that will be easier (and should be possible) once the FITS table overhaul is underway.  The plan is to make Astropy `Table`s (or some specialized subclass thereof) the main data structure used to set/get data in a `BINTABLE` HDU.  In the meantime I don't know that there's a better way, given that the original classes from PyFITS don't immediately understand what to do with a `Table` object.\n\n\nAlthough I should add--ISTM it should be straightforward to add an `append=True` to the FITS-Table connector similar to what we have for the HDF5 connector.  This would allow writing tables to an existing FITS file by simply appending a new HDU.  Then it would be a simple matter of:\n\n```\nfor table in tables:\n    table.write('blah.fits', append=True)\n```\n\nAfter all, we want to move people in the direction of using such a high level interface over using the low-level FITS objects.  The one downside to this is that there are some I/O-related implementation details in the fits module that might make this a little tricky to implement efficiently, but I'm working on fixing that...\n\n\n@embray Is there an issue / timescale for the \"FITS table overhaul\"?\nI guess this could take a while ... so :+1: to adding an `append=True` option now.\n\n\nAnother thing I noticed is that column `description` is not stored in FITS files ... is this another aspect of \"FITS Table integration is unfinished\" or a bug?\n\n``` python\n>>> from astropy.table import Table, Column\n>>> table = Table()\n>>> table['distance'] = Column([1, 2, 3], unit='m', description='Walking distance')\n>>> table['distance']\n<Column name='distance' unit=u'm' format=None description='Walking distance'>\narray([1, 2, 3])\n>>> table.write('table.fits', overwrite=True)\n>>> table2 = Table.read('table.fits')\n>>> table2['distance']\n<Column name='distance' unit=u'm' format=None description=None>\narray([1, 2, 3])\n```\n\n\n@cdeil - there isn't really a standard for storing the description as far as I understand, hence why it wasn't done. But we could try and think of a way to do it.\n\n\n@astrofrog I think you're right: #1768 .... too bad this isn't supported in FITS.\n\n\nThere's been a timetable for that for like a year--but its coordinates keep being translated along the time axis :)\n\n\nAs for the description, I've had a request in the past for supporting arbitrary table column attributes by mapping some header keyword like TDESCRn to the appropriate columns.  I think that might be one approach, but it wouldn't be built-in to work automatically since, as @astrofrog says, this isn't standard and there may be many conventions for capturing this type of information (if at all) that have nothing to do with Txxxxx keywords (for example, writing it COMMENT keywords).\n\n\nI suppose one other possibility would be to allow supplying an arbitrary function that does the mapping of Table metadata to wherever that metadata should go in the FITS header.\n\n\nThis is non-trivial, so removing the milestone since it won't get done for 1.0\n\n\nJust for anyone looking at this, there are several things we can do here:\n- Make it so that `Table.write(HDU)` works\n- Make it so that e.g. `Table.write(HDUList, append=True)` works\n\n\nA related issue I ran into again today is that writing all-ascii string columns to FITS in Python 3 silently encodes the characters with zero-padded 4 bytes, i.e. `abc` is stored as `a\\x00\\x00\\x00b\\x00\\x00\\x00c\\x00\\x00\\x00`.\n\nI still don't understand why, but writing the tables to separate FITS files worked:\n\n``` python\ntable1.write('data1.fits')\ntable2.write('data2.fits')\n```\n\nand instead writing the tables to one FITS file didn't work:\n\n``` python\nhdu_list = fits.HDUList()\nhdu_list.append(fits.BinTableHDU(table1.as_array(), name='data1'))\nhdu_list.append(fits.BinTableHDU(table2.as_array(), name='data2'))\nhdu_list.writeto('data.fits', clobber=True)\n```\n\nI didn't manage to reduce this to a useful example / test case, but the crux of the issue is this:\n\n``` python\nwith open('/tmp/data.csv', 'w') as fh:\n    fh.write('name\\n')\n    fh.write('abc\\n')\n\ntable1 = Table.read('/tmp/data.csv')\nprint(table1.dtype)\ntable1.write('/tmp/data.fits', overwrite=True)\ntable2 = Table.read('/tmp/data.fits')\nprint(table2.dtype)\n```\n\n```\n[('name', '<U3')]\n[('name', '<U12')]\n```\n\nIn the I remembered that I had to call this on all tables before converting them to HDUs and everything works:\n\n``` python\ntable.convert_unicode_to_bytestring(python3_only=True)\n```\n\nI thought I'd mention this here in case someone else runs into similar issues or it might even be useful to call `convert_unicode_to_bytestring` by default when converting `Table` to `HDU` or to issue a warning if there are unicode string columns?\n\n\n@cdeil Could you open a separate issue for this, since I don't think it's directly related to this.\n\n\n@cdeil - if you open a new issue you could reference this discussion of a real fix for the unicode problem in numpy.  So far this has generated a couple of long discussion but no agreement or action.  Sigh.\n\nhttp://mail.scipy.org/pipermail/numpy-discussion/2014-July/070586.html\n\n\nAlthough every time that discussion comes up I point out [this solution](https://github.com/embray/PyFITS/blob/c11abd331cff7e177f0fc551188347e529cde92d/lib/pyfits/fitsrec.py#L999) for which I still haven't heard too many downsides.  It's not 100% perfect, but it gets most of the way there for most cases anyone cares about (which is reading and writing some arrays of strings).  (There are also still some uncertainties about how that implementation should present itself to the user.  For example should it lie about its dtype, etc?)\n\n\n@embray OK, I've tried to file a separate issue about the text column to FITS issue: #3311.\n\n\nI've run into a performance issue [here](https://github.com/gammapy/gammapy/pull/249/files#diff-25f266495d68c172624588a69347a42cR299).\n\nI have two `Table` objects that I want to store in one FITS file.\n\nThe code I use is roughly:\n\n``` python\ntable1 = astropy.table.Table(...)\ntable2 = astropy.table.Table(...)\n\nhdu_list = fits.HDUList()\n\ndata = table1.as_array()\nheader = fits.Header()\nheader.update(table1.meta)\nhdu_list.append(fits.BinTableHDU(data=data, header=header))\n\ndata = table2.as_array()\nheader = fits.Header()\nheader.update(table1.meta)\nhdu_list.append(fits.BinTableHDU(data=data, header=header))\n```\n\nIs there a way to store a `Table` in FITS, but avoid the copy that `as_array` is doing?\n(that's the performance issue I have, one of my tables is several GB in size.)\n\nIs there a simpler way to convert `table.meta` to a `header`? I was looking through the convenience functions / methods in `fits` and `table`, but couldn't find something. There should be way to make a header from a dict in one line, if not directly in the `BinTableHDU` constructor, no?\n\n\nSince the main topic of this issue here seems to be to extend the `io.fits`-`Table` connector to allow saving multiple tables, maybe theses questions (really two separate questions) would be best addressed in the mailing list or a different issue since it's straying off-topic (I get how it's related to this issue, but I don't want things to get too noisy).\n\n(On the second question, `Header(meta.items())` would do it.)\n\n\nSorry to continue the OT question, but just wondering if it you can avoid the memory duplication using the direct FITS table creation interface described at  http://astropy.readthedocs.org/en/latest/io/fits/index.html#creating-a-new-table-file.\n\nIn other words if you assemble the `fits.Column` objects from the individual table `Column` objects, does that avoid the copy?  I think that in using the higher-level `fits.BinTableHDU` interface there is no way around creating a copy of all the individual data columns because that routine needs a numpy array (as far as I can tell).\n\n\n@taldcroft has the gist of it.  It would be pretty nice if the `from_columns` method worked on `astropy.table.Column` objects too.\n\n\nThough it still ends up making a copy, because it has to create a record array to then stuff all the column data into.  But I think this way might result in one fewer copy?  Not sure...\n\n\nI'm looking at this as part of the SKA/astropy hackathon. I agree with the suggestion to add an append flag that can be used from the table interface. It is indeed straightforward to add it to write_table_fits. Using the append function, performance is similar to writing the same file multiple times:\r\n\r\ntimeit.timeit(append, number=100)\r\n6.515496399999961\r\ntimeit.timeit(write, number=100)\r\n5.795052499999997",
  "pr_link": "https://github.com/gammapy/gammapy/pull/249",
  "code_context": [
    {
      "filename": "gammapy/background/tests/test_reflected.py",
      "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom __future__ import print_function, division\nimport unittest\nfrom astropy.tests.helper import pytest\nfrom ...background import Maps, ReflectedRegionMaker\n\n\n@pytest.mark.xfail\nclass TestReflectedBgMaker(unittest.TestCase):\n\n    def test_analysis(self):\n        runs = 'TODO'\n        maps = Maps('maps.fits')\n        reflected_bg_maker = ReflectedRegionMaker(runs, maps, psi=2, theta=0.1)\n        total_maps = Maps('total_maps.fits')\n        for run in runs:\n            run_map = total_maps.cutout(run)\n            reflected_bg_maker.make_n_reflected_map(run, run_map)\n            total_maps.add(run_map)\n        total_maps.save('n_reflected.fits')\n"
    },
    {
      "filename": "gammapy/catalog/utils.py",
      "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"Catalog utility functions / classes.\"\"\"\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\nimport numpy as np\nfrom astropy.coordinates import Angle, SkyCoord\n\n\n__all__ = ['coordinate_iau_format',\n           'ra_iau_format',\n           'dec_iau_format',\n           'skycoord_from_table',\n           'select_sky_box',\n           'to_ds9_region',\n           'get_source_by_name'\n           ]\n\n\ndef coordinate_iau_format(coordinate, ra_digits, dec_digits=None,\n                          prefix=''):\n    \"\"\"Coordinate format as an IAU source designation.\n\n    Reference: http://cdsweb.u-strasbg.fr/Dic/iau-spec.html\n\n    Parameters\n    ----------\n    coordinate : `~astropy.coordinates.SkyCoord`\n        Source coordinate\n    ra_digits : int (>=2)\n        Number of digits for the Right Ascension part\n    dec_digits : int (>=2) or None\n        Number of digits for the declination part\n        Default is ``dec_digits`` = None, meaning ``dec_digits`` = ``ra_digits`` - 1\n    prefix : str\n        Prefix to put before the coordinate string, e.g. \"SDSS J\".\n\n    Returns\n    -------\n    strrepr : str or list of strings\n        IAU format string representation of the coordinate.\n        If this input coordinate is an array, the output is a list of strings.\n\n    Examples\n    --------\n    >>> from astropy.coordinates import SkyCoord\n    >>> from gammapy.catalog import coordinate_iau_format\n\n    Example position from IAU specification\n\n    >>> coordinate = SkyCoord('00h51m09.38s -42d26m33.8s', frame='icrs')\n    >>> designation = 'QSO J' + coordinate_iau_format(coordinate, ra_digits=6)\n    >>> print(designation)\n    QSO J005109-4226.5\n    >>> coordinate = coordinate.transform_to('fk4')\n    >>> designation = 'QSO B' + coordinate_iau_format(coordinate, ra_digits=6)\n    >>> print(designation)\n    QSO B004848-4242.8\n\n    Crab pulsar position (positive declination)\n\n    >>> coordinate = SkyCoord('05h34m31.93830s +22d00m52.1758s', frame='icrs')\n    >>> designation = 'HESS J' + coordinate_iau_format(coordinate, ra_digits=4)\n    >>> print(designation)\n    HESS J0534+220\n\n    PKS 2155-304 AGN position (negative declination)\n\n    >>> coordinate = SkyCoord('21h58m52.06511s -30d13m32.1182s', frame='icrs')\n    >>> designation = '2FGL J' + coordinate_iau_format(coordinate, ra_digits=5)\n    >>> print(designation)\n    2FGL J2158.8-3013\n\n    Coordinate array inputs result in list of string output.\n\n    >>> coordinates = SkyCoord(ra=[10.68458, 83.82208],\n    ...                        dec=[41.26917, -5.39111],\n    ...                        unit=('deg', 'deg'), frame='icrs')\n    >>> designations = coordinate_iau_format(coordinates, ra_digits=5, prefix='HESS J')\n    >>> print(designations)\n    ['HESS J0042.7+4116', 'HESS J0535.2-0523']\n    \"\"\"\n    if coordinate.frame.name == 'galactic':\n        coordinate = coordinate.transform_to('icrs')\n\n    if dec_digits is None:\n        dec_digits = max(2, ra_digits - 1)\n\n    ra_str = ra_iau_format(coordinate.ra, ra_digits)\n    dec_str = dec_iau_format(coordinate.dec, dec_digits)\n\n    if coordinate.isscalar:\n        out = prefix + ra_str + dec_str\n    else:\n        out = [prefix + r + d for (r, d) in zip(ra_str, dec_str)]\n\n    return out\n\n\ndef ra_iau_format(ra, digits):\n    \"\"\"Right Ascension part of an IAU source designation.\n\n    Reference: http://cdsweb.u-strasbg.fr/Dic/iau-spec.html\n\n    ====== ========\n    digits format\n    ====== ========\n    2      HH\n    3      HHh\n    4      HHMM\n    5      HHMM.m\n    6      HHMMSS\n    7      HHMMSS.s\n    ====== ========\n\n    Parameters\n    ----------\n    ra : `~astropy.coordinates.Longitude`\n        Right ascension\n    digits : int (>=2)\n        Number of digits\n\n    Returns\n    -------\n    strrepr : str\n        IAU format string representation of the angle\n    \"\"\"\n    if (not isinstance(digits, int)) and (digits >= 2):\n        raise ValueError('Invalid digits: {0}. Valid options: int >= 2'.format(digits))\n\n    if ra.isscalar:\n        out = _ra_iau_format_scalar(ra, digits)\n    else:\n        out = [_ra_iau_format_scalar(_, digits) for _ in ra]\n\n    return out\n\n\ndef _ra_iau_format_scalar(ra, digits):\n    \"\"\"Format a single Right Ascension.\"\"\"\n    # Note that Python string formatting always rounds the last digit,\n    # but the IAU spec requires to truncate instead.\n    # That's why integers with the correct digits are computed and formatted\n    # instead of formatting floats directly\n    ra_h = int(ra.hms[0])\n    ra_m = int(ra.hms[1])\n    ra_s = ra.hms[2]\n\n    if digits == 2:  # format: HH\n        ra_str = '{0:02d}'.format(ra_h)\n    elif digits == 3:  # format: HHh\n        ra_str = '{0:03d}'.format(int(10 * ra.hour))\n    elif digits == 4:  # format: HHMM\n        ra_str = '{0:02d}{1:02d}'.format(ra_h, ra_m)\n    elif digits == 5:  # format : HHMM.m\n        ra_str = '{0:02d}{1:02d}.{2:01d}'.format(ra_h, ra_m, int(ra_s / 6))\n    elif digits == 6:  # format: HHMMSS\n        ra_str = '{0:02d}{1:02d}{2:02d}'.format(ra_h, ra_m, int(ra_s))\n    else:  # format: HHMMSS.s\n        SS = int(ra_s)\n        s_digits = digits - 6\n        s = int(10 ** s_digits * (ra_s - SS))\n        fmt = '{0:02d}{1:02d}{2:02d}.{3:0' + str(s_digits) + 'd}'\n        ra_str = fmt.format(ra_h, ra_m, SS, s)\n\n    return ra_str\n\n\ndef dec_iau_format(dec, digits):\n    \"\"\"Declination part of an IAU source designation.\n\n    Reference: http://cdsweb.u-strasbg.fr/Dic/iau-spec.html\n\n    ====== =========\n    digits format\n    ====== =========\n    2      +DD\n    3      +DDd\n    4      +DDMM\n    5      +DDMM.m\n    6      +DDMMSS\n    7      +DDMMSS.s\n    ====== =========\n\n    Parameters\n    ----------\n    dec : `~astropy.coordinates.Latitude`\n        Declination\n    digits : int (>=2)\n        Number of digits\n\n    Returns\n    -------\n    strrepr : str\n        IAU format string representation of the angle\n    \"\"\"\n    if not isinstance(digits, int) and digits >= 2:\n        raise ValueError('Invalid digits: {0}. Valid options: int >= 2'.format(digits))\n\n    if dec.isscalar:\n        out = _dec_iau_format_scalar(dec, digits)\n    else:\n        out = [_dec_iau_format_scalar(_, digits) for _ in dec]\n\n    return out\n\n\ndef _dec_iau_format_scalar(dec, digits):\n    \"\"\"Format a single declination.\"\"\"\n    # Note that Python string formatting always rounds the last digit,\n    # but the IAU spec requires to truncate instead.\n    # That's why integers with the correct digits are computed and formatted\n    # instead of formatting floats directly\n    dec_sign = '+' if dec.deg >= 0 else '-'\n    dec_d = int(abs(dec.dms[0]))\n    dec_m = int(abs(dec.dms[1]))\n    dec_s = abs(dec.dms[2])\n\n    if digits == 2:  # format: +DD\n        dec_str = '{0}{1:02d}'.format(dec_sign, dec_d)\n    elif digits == 3:  # format: +DDd\n        dec_str = '{0:+04d}'.format(int(10 * dec.deg))\n    elif digits == 4:  # format : +DDMM\n        dec_str = '{0}{1:02d}{2:02d}'.format(dec_sign, dec_d, dec_m)\n    elif digits == 5:  # format: +DDMM.m\n        dec_str = '{0}{1:02d}{2:02d}.{3:01d}'.format(dec_sign, dec_d, dec_m, int(dec_s / 6))\n    elif digits == 6:  # format: +DDMMSS\n        dec_str = '{0}{1:02d}{2:02d}.{3:02d}'.format(dec_sign, dec_d, dec_m, int(dec_s))\n    else:  # format: +DDMMSS.s\n        SS = int(dec_s)\n        s_digits = digits - 6\n        s = int(10 ** s_digits * (dec_s - SS))\n        fmt = '{0}{1:02d}{2:02d}{3:02d}.{4:0' + str(s_digits) + 'd}'\n        dec_str = fmt.format(dec_sign, dec_d, dec_m, SS, s)\n\n    return dec_str\n\n\ndef skycoord_from_table(table):\n    \"\"\"Make `~astropy.coordinates.SkyCoord` from lon, lat columns in `~astropy.table.Table`.\n\n    This is a convenience function similar to `~astropy.coordinates.SkyCoord.guess_from_table`,\n    but with the column names we usually use.\n\n    TODO: I'm not sure if it's a good idea to use this because it's not always clear\n    which positions are taken.\n    \"\"\"\n\n    if set(['RAJ2000', 'DEJ2000']).issubset(table.colnames):\n        lon, lat, frame = 'RAJ2000', 'DEJ2000', 'icrs'\n    elif set(['RA', 'DEC']).issubset(table.colnames):\n        lon, lat, frame = 'RA', 'DEC', 'icrs'\n    elif set(['GLON', 'GLAT']).issubset(table.colnames):\n        lon, lat, frame = 'GLON', 'GLAT', 'galactic'\n    else:\n        raise KeyError('No column GLON / GLAT or RA / DEC or RAJ2000 / DEJ2000 found.')\n\n    unit = table[lon].unit if table[lon].unit else 'deg'\n\n    skycoord = SkyCoord(table[lon], table[lat], unit=unit, frame=frame)\n\n    return skycoord\n\n\ndef select_sky_box(table, lon_lim, lat_lim, frame):\n    \"\"\"Select sky positions in a box.\n\n    This function can be applied e.g. to event lists of source catalogs.\n\n    Note: if useful we can add a function that returns the mask\n    or indices instead of applying the selection directly\n\n    Parameters\n    ----------\n    table : `~astropy.table.Table`\n        Table with sky coordinate columns\n    lon_lim, lat_lim : `~astropy.coordinates.Angle`\n        Box limits (each should be a min, max tuple)\n    frame : str\n        Frame in which to apply the box cut.\n        Built-in Astropy coordinate frames are supported, e.g.\n        'icrs', 'fk5' or 'galactic'.\n\n    Returns\n    -------\n    table : `~astropy.table.Table`\n        Copy of input table with box cut applied\n\n    Examples\n    --------\n    TODO\n    \"\"\"\n    skycoord = skycoord_from_table(table)\n    skycoord = skycoord.transform_to(frame)\n    lon = skycoord.data.lon.wrap_at(Angle(180, 'deg'))\n    lat = skycoord.data.lat\n\n    lon_mask = (lon_lim[0] < lon) & (lon < lon_lim[1])\n    lat_mask = (lat_lim[0] < lat) & (lat < lat_lim[1])\n    mask = lon_mask & lat_mask\n\n    return table[mask]\n\n\ndef get_source_by_name(source, catalog, id_column='Source_Name'):\n    \"\"\"\n    Get source catalog entry by source name.\n\n    Parameters\n    ----------\n    source : string\n        Source name.\n    catalog : `~astropy.table.Table`\n        Catalog table.\n    id_column : str (default = 'Source_Name')\n        Column name of the source names.\n\n    Returns\n    -------\n    source : `~astropy.table.Row`\n        Entry for the given source.\n    \"\"\"\n    index = np.where(catalog[id_column] == source)[0]\n    if len(index) == 0:\n        raise ValueError('Source {0} not found in catalog!'.format(source))\n    else:\n        return catalog[index[0]]\n\n\ndef to_ds9_region(catalog, radius=None, color='green', glon='GLON', unc_glon=None,\n                  glat='GLAT', unc_glat=None, label=None, label_position='top',\n                  label_additional_info=None, label_distance=1.2, marker='diamond',\n                  dashed=False, width=3):\n    \"\"\"\n    Write ds9 region file from catalog.\n\n    Currently works only for galactic coordinates.\n\n\n    Parameters\n    ----------\n    catalog : `~astropy.table.Table`\n        Catalog with position and size information.\n    glon : str ('GLON')\n        Catalog column containing galactic longitude position.\n    glat : str ('GLAT')\n        Catalog column containing galactic latitude position.\n    unc_glon : str (None)\n        Catalog column containing galactic longitude position uncertainty.\n    unc_glat : str (None)\n        Catalog column containing galactic latitude position uncertainty.\n    radius : str (None)\n        Catalog column containing extension information.\n    color : str ('green')\n        Valid ds9 color.\n    label : str (None)\n        Catalog column to use for the label.\n    label_position: str ('top')\n        Position of the region label. Either 'top' or 'bottom'.\n    label_distance: float (1.2)\n        Distance of the label from the regions center. Given\n        in multiple of the region's radius.\n    label_additional_info: dict\n        Additional info to be shown in the region label.\n    dashed : bool (False)\n        Dashed region line.\n    width : int\n        Width of the region line.\n\n    Examples\n    --------\n    This example reads Greens catalog from `gammapy.datasets` and writes it to a\n    ds9 region file.\n\n    >>> from gammapy.datasets import load_catalog_green\n    >>> green = load_catalog_green()\n    >>> green['MeanDiam'] /= 120.\n    >>> green_str = to_ds9_region(green, radius='MeanDiam',\n    >>>                           label='Source_Name', color='red')\n    >>> with open(\"region_green.reg\", \"w\") as region_file:\n    >>>     region_file.write(green_str)\n\n    Returns\n    -------\n    region : str\n        DS9 region string.\n    \"\"\"\n    region_string = ''\n\n    dash = '1' if dashed else '0'\n\n    format_ = ' color = {color} width = {width} dash = {dash}\\n'.format(**locals())\n    if radius is not None:\n        shape = 'galactic;circle({0:.5f},{1:.5f},{2:.5f}) #'\n    else:\n        shape = 'galactic;point({0:.5f},{1:.5f})' + ' # point = {0}'.format(marker)\n    shape += format_\n\n    text = 'galactic;text({0:5f},{1:5f}) # text = {{{2}}} '\n    text += 'color = {0}\\n'.format(color)\n\n    for row in catalog:\n        label_ = row[label] if label is not None else ''\n        if label_additional_info is not None:\n            text_add_ = ', '.join([__.format(row[_]) for __, _\n                                   in label_additional_info.items()])\n            label_ += '(' + text_add_ + ')'\n        if radius is not None:\n            shape_ = shape.format(row[glon], row[glat], row[radius])\n            text_ = text.format(row[glon], row[glat] + row[radius]\n                                * label_distance, label_)\n        else:\n            shape_ = shape.format(row[glon], row[glat])\n            text_ = text.format(row[glon], row[glat] + 0.05, label_)\n\n        region_string += shape_ + text_\n\n        if unc_glat and unc_glat is not None:\n            cross_ = _get_cross_ds9_region(row[glon], row[glat], row[unc_glon],\n                                           row[unc_glat], color, width)\n            region_string += cross_\n    return region_string\n\n\ndef _get_cross_ds9_region(x, y, unc_x, unc_y, color, width, endbar=0.01):\n    \"\"\"\n    Get ds9 region string for a cross that represents position uncertainties.\n\n    Parameters\n    ----------\n    x : float\n        Position in x direction.\n    y : float\n        Position in y direction.\n    unc_x : float\n        Uncertainty of the position in x direction.\n    unc_y : float\n        Uncertainty of the position in y direction.\n    color : str ('green')\n        Valid ds9 color.\n    width : float\n        Linewidth of the cross.\n    endbar : float (default = 0.01)\n        Length of the endbar in deg.\n    \"\"\"\n    format_line = ' color = {0} width = {1}\\n'.format(color, width)\n    line_ = ''\n    line = 'galactic;line({0:.5f},{1:.5f},{2:.5f},{3:.5f}) #'\n    line_ += line.format(x - unc_x, y, x + unc_x, y) + format_line\n    line_ += line.format(x, y - unc_y, x, y + unc_y) + format_line\n    line_ += line.format(x - unc_x, y - endbar, x - unc_x, y + endbar) + format_line\n    line_ += line.format(x + unc_x, y - endbar, x + unc_x, y + endbar) + format_line\n    line_ += line.format(x - endbar, y + unc_y, x + endbar, y + unc_y) + format_line\n    line_ += line.format(x - endbar, y - unc_y, x + endbar, y - unc_y) + format_line\n    return line_\n"
    },
    {
      "filename": "gammapy/data/event_list.py",
      "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"Event list: table of LON, LAT, ENERGY, TIME\n\"\"\"\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\nimport logging\nfrom collections import OrderedDict\nimport os\nimport numpy as np\nfrom astropy.io import fits\nfrom astropy.units import Quantity\nfrom astropy.time import Time\nfrom astropy.coordinates import SkyCoord, Angle, AltAz\nfrom astropy.table import Table\nfrom ..data import GoodTimeIntervals, TelescopeArray\nfrom ..data import InvalidDataError\nfrom . import utils\n\n__all__ = ['EventList',\n           'EventListDataset',\n           'EventListDatasetChecker',\n           'event_lists_to_counts_image',\n           ]\n\n\nclass EventList(Table):\n    \"\"\"Event list `~astropy.table.Table`.\n\n    The most important reconstructed event parameters\n    are available as the following columns:\n\n    - ``TIME`` - Mission elapsed time (sec)\n    - ``RA``, ``DEC`` - FK5 J2000 (or ICRS?) position (deg)\n    - ``ENERGY`` - Energy (usually MeV for Fermi and TeV for IACTs)\n\n    Other optional (columns) that are sometimes useful for high-level analysis:\n\n    - ``GLON``, ``GLAT`` - Galactic coordinates (deg)\n    - ``DETX``, ``DETY`` - Field of view coordinates (radian?)\n\n    Note that when reading data for analysis you shouldn't use those\n    values directly, but access them via properties which create objects\n    of the appropriate class and convert to 64 bit:\n\n    - `time` for ``TIME``\n    - `radec` for ``RA``, ``DEC``\n    - `energy` for ``ENERGY``\n    - `galactic` for ``GLON``, ``GLAT``\n    \"\"\"\n    @property\n    def info(self):\n        \"\"\"Summary info string.\"\"\"\n        s = '---> Event list info:\\n'\n        # TODO: Which telescope?\n\n        # When and how long was the observation?\n        s += '- Observation duration: {}\\n'.format(self.observation_time_duration)\n        s += '- Dead-time fraction: {:5.3f} %\\n'.format(100 * self.observation_dead_time_fraction)\n\n        # TODO: Which target was observed?\n\n        s += '-- Event info:\\n'\n        s += '- Number of events: {}\\n'.format(len(self))\n        # TODO: add time, RA, DEC and if present GLON, GLAT info ...\n        s += '- Median energy: {}\\n'.format(np.median(self.energy))\n        # TODO: azimuth should be circular median\n        s += '- Median azimuth: {}\\n'.format(np.median(self['AZ']))\n        s += '- Median altitude: {}\\n'.format(np.median(self['ALT']))\n\n        return s\n\n    @property\n    def time(self):\n        \"\"\"Event times (`~astropy.time.Time`)\"\"\"\n        met_ref = utils._time_ref_from_dict(self.meta)\n        met = Quantity(self['TIME'].astype('f64'), 'second')\n        return met_ref + met\n\n    @property\n    def radec(self):\n        \"\"\"Event RA / DEC sky coordinates (`~astropy.coordinates.SkyCoord`)\"\"\"\n        lon = self['RA'].astype('f64')\n        lat = self['DEC'].astype('f64')\n        return SkyCoord(lon, lat, unit='deg', frame='fk5')\n\n    @property\n    def galactic(self):\n        \"\"\"Event Galactic sky coordinates (`~astropy.coordinates.SkyCoord`).\n\n        Note: uses the ``GLON`` and ``GLAT`` columns.\n        If only ``RA`` and ``DEC`` are present use the explicit\n        ``event_list.radec.to('galactic')`` instead.\n        \"\"\"\n        lon = self['GLON'].astype('f64')\n        lat = self['GLAT'].astype('f64')\n        return SkyCoord(lon, lat, unit='deg', frame='galactic')\n\n    @property\n    def altaz(self):\n        \"\"\"Event horizontal sky coordinates (`~astropy.coordinates.SkyCoord`)\"\"\"\n        lon = self['AZ'].astype('f64')\n        lat = self['ALT'].astype('f64')\n        time = self.time\n        location = self.observatory_earth_location\n        altaz_frame = AltAz(obstime=time, location=location)\n        return SkyCoord(lon, lat, unit='deg', frame=altaz_frame)\n\n    @property\n    def energy(self):\n        \"\"\"Event energies (`~astropy.units.Quantity`).\"\"\"\n        energy = self['ENERGY'].astype('f64')\n        return Quantity(energy, self.meta['EUNIT'])\n\n    @property\n    def observatory_earth_location(self):\n        \"\"\"Observatory location (`~astropy.coordinates.EarthLocation`)\"\"\"\n        return utils._earth_location_from_dict(self.meta)\n\n    # TODO: I'm not sure how to best exposure header data\n    # as quantities ... maybe expose them on `meta` or\n    # a completely separate namespace?\n    # For now I'm taking very verbose names ...\n\n    @property\n    def observation_time_duration(self):\n        \"\"\"Observation time duration in seconds (`~astropy.units.Quantity`).\n\n        The wall time, including dead-time.\n        \"\"\"\n        return Quantity(self.meta['ONTIME'], 'second')\n\n    @property\n    def observation_live_time_duration(self):\n        \"\"\"Live-time duration in seconds (`~astropy.units.Quantity`).\n\n        The dead-time-corrected observation time.\n\n        Computed as ``t_live = t_observation * (1 - f_dead)``\n        where ``f_dead`` is the dead-time fraction.\n        \"\"\"\n        return Quantity(self.meta['LIVETIME'], 'second')\n\n    @property\n    def observation_dead_time_fraction(self):\n        \"\"\"Dead-time fraction.\n\n        Defined as dead-time over observation time.\n\n        Dead-time is defined as the time during the observation\n        where the detector didn't record events:\n        http://en.wikipedia.org/wiki/Dead_time\n        http://adsabs.harvard.edu/abs/2004APh....22..285F\n\n        The dead-time fraction is used in the live-time computation,\n        which in turn is used in the exposure and flux computation.\n        \"\"\"\n        return 1 - self.meta['DEADC']\n\n\nclass EventListDataset(object):\n    \"\"\"Event list dataset (event list plus some extra info).\n\n    TODO: I'm not sure if IRFs should be included in this\n    class or if an extra container class should be added.\n\n    Parameters\n    ----------\n    event_list : `~gammapy.data.EventList`\n        Event list table\n    telescope_array : `~gammapy.data.TelescopeArray`\n        Telescope array info\n    good_time_intervals : `~gammapy.data.GoodTimeIntervals`\n        Observation time interval info\n    \"\"\"\n    def __init__(self, event_list,\n                 telescope_array=None,\n                 good_time_intervals=None):\n        self.event_list = event_list\n        self.telescope_array = telescope_array\n        self.good_time_intervals = good_time_intervals\n\n    @staticmethod\n    def from_hdu_list(hdu_list):\n        \"\"\"Create `EventList` from a `~astropy.io.fits.HDUList`.\n        \"\"\"\n        # TODO: This doesn't work because FITS / Table is not integrated.\n        # Maybe the easiest solution for now it to write the hdu_list\n        # to an in-memory buffer with StringIO and then read it\n        # back using Table.read()?\n        raise NotImplementedError\n        event_list = EventList.from_hdu(hdu_list['EVENTS'])\n        telescope_array = TelescopeArray.from_hdu(hdu_list['TELARRAY'])\n        good_time_intervals = GoodTimeIntervals.from_hdu(hdu_list['GTI'])\n\n        return EventListDataset(event_list, telescope_array, good_time_intervals)\n\n    @staticmethod\n    def read(filename):\n        \"\"\"Read event list from FITS file.\n        \"\"\"\n        # return EventList.from_hdu_list(fits.open(filename))\n        event_list = EventList.read(filename, hdu='EVENTS')\n        try:\n            telescope_array = TelescopeArray.read(filename, hdu='TELARRAY')\n        except KeyError:\n            telescope_array = None\n            # self.logger.debug('No TELARRAY extension')\n\n        try:\n            good_time_intervals = GoodTimeIntervals.read(filename, hdu='GTI')\n        except KeyError:\n            good_time_intervals = None\n\n        return EventListDataset(event_list, telescope_array, good_time_intervals)\n\n    @staticmethod\n    def vstack_from_files(filenames, logger=None):\n        \"\"\"Stack event lists vertically (combine events and GTIs).\n\n        This function stacks (a.k.a. concatenates) event lists.\n        E.g. if you have one event list with 100 events (i.e. 100 rows)\n        and another with 42 events, the output event list will have 142 events.\n\n        It also stacks the GTIs so that exposure computations are still\n        possible using the stacked event list.\n\n\n        At the moment this can require a lot of memory.\n        All event lists are loaded into memory at the same time.\n\n        TODO: implement and benchmark different a more efficient method:\n        Get number of rows from headers, pre-allocate a large table,\n        open files one by one and fill correct rows.\n\n        TODO: handle header keywords \"correctly\".\n        At the moment the output event list header keywords are copies of\n        the values from the first observation, i.e. meaningless.\n        Here's a (probably incomplete) list of values we should handle\n        (usually by computing the min, max or mean or removing it):\n        - OBS_ID\n        - DATE_OBS, DATE_END\n        - TIME_OBS, TIME_END\n        - TSTART, TSTOP\n        - LIVETIME, DEADC\n        - RA_PNT, DEC_PNT\n        - ALT_PNT, AZ_PNT\n\n\n        Parameters\n        ----------\n        filenames : list of str\n            List of event list filenames\n\n        Returns\n        -------\n        event_list_dataset : `~gammapy.data.EventListDataset`\n\n        \"\"\"\n        total_filesize = 0\n        for filename in filenames:\n            total_filesize += os.path.getsize(filename)\n\n        if logger:\n            logger.info('Number of files to stack: {}'.format(len(filenames)))\n            logger.info('Total filesize: {:.2f} MB'.format(total_filesize / 1024. ** 2))\n            logger.info('Reading event list files ...')\n\n        event_lists = []\n        gtis = []\n        from astropy.utils.console import ProgressBar\n        for filename in ProgressBar(filenames):\n            # logger.info('Reading {}'.format(filename))\n            event_list = Table.read(filename, hdu='EVENTS')\n\n            # TODO: Remove and modify header keywords for stacked event list\n            meta_del = ['OBS_ID', 'OBJECT']\n            meta_mod = ['DATE_OBS', 'DATE_END', 'TIME_OBS', 'TIME_END']\n\n            gti = Table.read(filename, hdu='GTI')\n            event_lists.append(event_list)\n            gtis.append(gti)\n\n        from astropy.table import vstack as vstack_tables\n        total_event_list = vstack_tables(event_lists, metadata_conflicts='silent')\n        total_gti = vstack_tables(gtis, metadata_conflicts='silent')\n\n        total_event_list.meta['EVTSTACK'] = 'yes'\n        total_gti.meta['EVTSTACK'] = 'yes'\n\n        return EventListDataset(event_list=total_event_list, good_time_intervals=total_gti)\n\n    def write(self, *args, **kwargs):\n        \"\"\"Write to FITS file.\n\n        Calls `~astropy.io.fits.HDUList.writeto`, forwarding all arguments.\n        \"\"\"\n        self.to_fits().writeto(*args, **kwargs)\n\n    def to_fits(self):\n        \"\"\"Convert to FITS HDU list format.\n\n        Returns\n        -------\n        hdu_list : `~astropy.io.fits.HDUList`\n            HDU list with EVENTS and GTI extension.\n        \"\"\"\n        # TODO: simplify when Table / FITS integration improves:\n        # https://github.com/astropy/astropy/issues/2632#issuecomment-70281392\n        # TODO: I think this makes an in-memory copy, i.e. is inefficient.\n        # Can we avoid this?\n        hdu_list = fits.HDUList()\n\n\n        # TODO:\n        del self.event_list['TELMASK']\n\n        data = self.event_list.as_array()\n        header = fits.Header()\n        header.update(self.event_list.meta)\n        hdu_list.append(fits.BinTableHDU(data=data, header=header, name='EVENTS'))\n\n        data = self.good_time_intervals.as_array()\n        header = fits.Header()\n        header.update(self.good_time_intervals.meta)\n        hdu_list.append(fits.BinTableHDU(data, header=header, name='GTI'))\n\n        return hdu_list\n\n    @property\n    def info(self):\n        \"\"\"Summary info string.\"\"\"\n        s = '===> Event list dataset information:\\n'\n        s += self.event_list.info\n        s += self.telescope_array.info\n        s += self.good_time_intervals.info\n        s += '- telescopes: {}\\n'.format(len(self.telescope_array))\n        s += '- good time intervals: {}\\n'.format(len(self.good_time_intervals))\n        return s\n\n    def check(self, checks='all'):\n        \"\"\"Check if format and content is ok.\n\n        This is a convenience method that instantiates\n        and runs a `~gammapy.data.EventListDatasetChecker` ...\n        if you want more options use this way to use it:\n\n        >>> from gammapy.data import EventListDatasetChecker\n        >>> checker = EventListDatasetChecker(event_list, ...)\n        >>> checker.run(which, ...)  #\n\n        Parameters\n        ----------\n        checks : list of str or 'all'\n            Which checks to run (see list in\n            `~gammapy.data.EventListDatasetChecker.run` docstring).\n\n        Returns\n        -------\n        ok : bool\n            Everything ok?\n        \"\"\"\n        checker = EventListDatasetChecker(self)\n        return checker.run(checks)\n\n\nclass EventListDatasetChecker(object):\n    \"\"\"Event list dataset checker.\n\n    TODO: link to defining standard documents,\n     especially the CTA event list spec.\n\n    Having such a checker is useful at the moment because\n    the CTA data formats are quickly evolving and there's\n    various sources of event list data, e.g. exporters are\n    being written for the existing IACTs and simulators\n    are being written for CTA.\n\n    Parameters\n    ----------\n    event_list_dataset : `~gammapy.data.EventListDataset`\n        Event list dataset\n    logger : `logging.Logger` or None\n        Logger to use\n    \"\"\"\n    _AVAILABLE_CHECKS = OrderedDict(\n        misc='check_misc',\n        times='check_times',\n        coordinates='check_coordinates',\n    )\n\n    accuracy = OrderedDict(\n        angle=Angle('1 arcsec'),\n        time=Quantity(1, 'microsecond'),\n\n    )\n\n    def __init__(self, event_list_dataset, logger=None):\n        self.dset = event_list_dataset\n        if logger:\n            self.logger = logger\n        else:\n            self.logger = logging.getLogger('EventListDatasetChecker')\n\n    def run(self, checks='all'):\n        \"\"\"Run checks.\n\n        Available checks: {...}\n\n        Parameters\n        ----------\n        checks : list of str or \"all\"\n            Which checks to run\n\n        Returns\n        -------\n        ok : bool\n            Everything ok?\n        \"\"\"\n        if checks == 'all':\n            checks = self._AVAILABLE_CHECKS.keys()\n\n        unknown_checks = set(checks).difference(self._AVAILABLE_CHECKS.keys())\n        if unknown_checks:\n            raise ValueError('Unknown checks: {}'.format(unknown_checks))\n\n        ok = True\n        for check in checks:\n            check_method = getattr(self, self._AVAILABLE_CHECKS[check])\n            ok &= check_method()\n\n        return ok\n\n    def check_misc(self):\n        \"\"\"Check misc basic stuff.\"\"\"\n        ok = True\n\n        required_meta = ['TELESCOP', 'OBS_ID']\n        missing_meta = set(required_meta) - set(self.dset.event_list.meta)\n        if missing_meta:\n            ok = False\n            logging.error('Missing meta info: {}'.format(missing_meta))\n\n        # TODO: implement more basic checks that all required info is present.\n\n        return ok\n\n    def _check_times_gtis(self):\n        \"\"\"Check GTI info\"\"\"\n        # TODO:\n        # Check that required info is there\n        for colname in ['START', 'STOP']:\n            if colname not in self.colnames:\n                raise InvalidDataError('GTI missing column: {}'.format(colname))\n\n        for key in ['TSTART', 'TSTOP', 'MJDREFI', 'MJDREFF']:\n            if key not in self.meta:\n                raise InvalidDataError('GTI missing header keyword: {}'.format(key))\n\n        # TODO: Check that header keywords agree with table entries\n        # TSTART, TSTOP, MJDREFI, MJDREFF\n\n        # Check that START and STOP times are consecutive\n        times = np.ravel(self['START'], self['STOP'])\n        # TODO: not sure this is correct ... add test with a multi-gti table from Fermi.\n        if not np.all(np.diff(times) >= 0):\n            raise InvalidDataError('GTIs are not consecutive or sorted.')\n\n    def check_times(self):\n        \"\"\"Check if various times are consistent.\n\n        The headers and tables of the FITS EVENTS and GTI extension\n        contain various observation and event time information.\n        \"\"\"\n        ok = True\n\n        # http://fermi.gsfc.nasa.gov/ssc/data/analysis/documentation/Cicerone/Cicerone_Data/Time_in_ScienceTools.html\n        telescope_met_refs = OrderedDict(\n            FERMI=Time('2001-01-01 00:00:00', scale='utc'),\n            HESS=Time('2000-01-01 12:00:00.000', scale='utc'),\n            # TODO: Once CTA has specified their MET reference add check here\n        )\n\n        telescope = self.dset.event_list.meta['TELESCOP']\n        met_ref = utils._time_ref_from_dict(self.dset.event_list.meta)\n\n        if telescope in telescope_met_refs.keys():\n            dt = (met_ref - telescope_met_refs[telescope])\n            if dt > self.accuracy['time']:\n                ok = False\n                logging.error('MET reference is incorrect.')\n        else:\n            logging.debug('Skipping MET reference check ... not known for this telescope.')\n\n        # TODO: check latest CTA spec to see which info is required / optional\n        # EVENTS header keywords:\n        # 'DATE_OBS': '2004-10-14'\n        # 'TIME_OBS': '00:08:27'\n        # 'DATE_END': '2004-10-14'\n        # 'TIME_END': '00:34:44'\n        # 'TSTART': 150984507.0\n        # 'TSTOP': 150986084.0\n        # 'MJDREFI': 51544\n        # 'MJDREFF': 0.5\n        # 'TIMEUNIT': 's'\n        # 'TIMESYS': 'TT'\n        # 'TIMEREF': 'local'\n        # 'TASSIGN': 'Namibia'\n        # 'TELAPSE': 0\n        # 'ONTIME': 1577.0\n        # 'LIVETIME': 1510.95910644531\n        # 'DEADC': 0.964236799627542\n\n        return ok\n\n    def check_coordinates(self):\n        \"\"\"Check if various event list coordinates are consistent.\n\n        Parameters\n        ----------\n        event_list_dataset : `~gammapy.data.EventListDataset`\n            Event list dataset\n        accuracy : `~astropy.coordinates.Angle`\n            Required accuracy.\n\n        Returns\n        -------\n        status : bool\n            All coordinates consistent?\n        \"\"\"\n        ok = True\n        ok &= self._check_coordinates_header()\n        ok &= self._check_coordinates_galactic()\n        ok &= self._check_coordinates_altaz()\n        ok &= self._check_coordinates_field_of_view()\n        return ok\n\n    def _check_coordinates_header(self):\n        \"\"\"Check TODO\"\"\"\n        # TODO: implement\n        return True\n\n    def _check_coordinates_galactic(self):\n        \"\"\"Check if RA / DEC matches GLON / GLAT.\"\"\"\n        event_list = self.dset.event_list\n\n        for colname in ['RA', 'DEC', 'GLON', 'GLAT']:\n            if colname not in event_list.colnames:\n                # GLON / GLAT columns are optional ...\n                # so it's OK if they are not present ... just move on ...\n                self.logger.info('Skipping Galactic coordinate check. '\n                                 'Missing column: \"{}\".'.format(colname))\n                return True\n\n        radec = event_list.radec\n        galactic = event_list.galactic\n        separation = radec.separation(galactic).to('arcsec')\n        return self._check_separation(separation, 'GLON / GLAT', 'RA / DEC')\n\n    def _check_coordinates_altaz(self):\n        \"\"\"Check if ALT / AZ matches RA / DEC.\"\"\"\n        event_list = self.dset.event_list\n        telescope_array = self.dset.telescope_array\n\n        for colname in ['RA', 'DEC', 'AZ', 'ALT']:\n            if colname not in event_list.colnames:\n                # AZ / ALT columns are optional ...\n                # so it's OK if they are not present ... just move on ...\n                self.logger.info('Skipping AltAz coordinate check. '\n                                 'Missing column: \"{}\".'.format(colname))\n                return True\n\n        radec = event_list.radec\n        altaz_expected = event_list.altaz\n        altaz_actual = radec.transform_to(altaz_expected)\n        separation = altaz_actual.separation(altaz_expected).to('arcsec')\n        return self._check_separation(separation, 'ALT / AZ', 'RA / DEC')\n\n    def _check_coordinates_field_of_view(self):\n        \"\"\"Check if DETX / DETY matches ALT / AZ\"\"\"\n        # TODO: implement\n        return True\n\n    def _check_separation(self, separation, tag1, tag2):\n        max_separation = separation.max()\n\n        if max_separation > self.accuracy['angle']:\n            # TODO: probably we need to print run number and / or other\n            # things for this to be useful in a pipeline ...\n            fmt = '{0} not consistent with {1}. Max separation: {2}'\n            args = [tag1, tag2, max_separation]\n            self.logger.warning(fmt.format(*args))\n            return False\n        else:\n            return True\n\n\ndef event_lists_to_counts_image(header, table_of_files, logger=None):\n    \"\"\"Make count image from event lists (like gtbin).\n\n    TODO: what's a good API and location for this?\n\n    Parameters\n    ----------\n    header : `~astropy.io.fits.Header`\n        FITS header\n    table_of_files : `~astropy.table.Table`\n        Table of event list filenames\n    logger : `logging.Logger` or None\n        Logger to use\n\n    Returns\n    -------\n    image : `~astropy.io.fits.ImageHDU`\n        Count image\n    \"\"\"\n    shape = (header['NAXIS2'], header['NAXIS1'])\n    data = np.zeros(shape, dtype='int')\n\n    for row in table_of_files:\n        if row['filetype'] != 'events':\n            continue\n        ds = EventListDataset.read(row['filename'])\n        if logger:\n            logger.info('Processing OBS_ID = {:06d} with {:6d} events.'\n                        ''.format(row['OBS_ID'], len(ds.event_list)))\n        # TODO: fill events in image.\n\n    return fits.ImageHDU(data=data, header=header)\n"
    },
    {
      "filename": "gammapy/irf/psf_table.py",
      "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom __future__ import print_function, division\nimport numpy as np\nfrom astropy.io import fits\nfrom astropy.units import Quantity\nfrom astropy.coordinates import Angle, SkyCoord\n#from astropy.convolution import discretize_model\nfrom astropy.convolution.utils import discretize_oversample_2D\nfrom ..morphology import Gauss2DPDF\nfrom ..utils.array import array_stats_str\n\n__all__ = ['TablePSF',\n           'EnergyDependentTablePSF',\n           ]\n\n# Default PSF spline keyword arguments\n# TODO: test and document\nDEFAULT_PSF_SPLINE_KWARGS = dict(k=1, s=0)\n\n\nclass TablePSF(object):\n    r\"\"\"Radially-symmetric table PSF.\n\n    This PSF represents a :math:`PSF(\\theta)=dP / d\\Omega(\\theta)`\n    spline interpolation curve for a given set of offset :math:`\\theta`\n    and :math:`PSF` points.\n\n    Uses `scipy.interpolate.UnivariateSpline`.\n\n    Parameters\n    ----------\n    offset : `~astropy.coordinates.Angle`\n        Offset angle array\n    dp_domega : `~astropy.units.Quantity`\n        PSF value array\n    spline_kwargs : dict\n        Keyword arguments passed to `~scipy.interpolate.UnivariateSpline`\n\n    Notes\n    -----\n    * This PSF class works well for model PSFs of arbitrary shape (represented by a table),\n      but might give unstable results if the PSF has noise.\n      E.g. if ``dp_domega`` was estimated from histograms of real or simulated event data\n      with finite statistics, it will have noise and it is your responsibility\n      to check that the interpolating spline is reasonable.\n    * To customize the spline, pass keyword arguments to `~scipy.interpolate.UnivariateSpline`\n      in ``spline_kwargs``. E.g. passing ``dict(k=1)`` changes from the default cubic to\n      linear interpolation.\n    * TODO: evaluate spline for ``(log(offset), log(PSF))`` for numerical stability?\n    * TODO: merge morphology.theta class functionality with this class.\n    * TODO: add FITS I/O methods\n    * TODO: add ``normalize`` argument to ``__init__`` with default ``True``?\n    * TODO: ``__call__`` doesn't show up in the html API docs, but it should:\n      https://github.com/astropy/astropy/pull/2135\n    \"\"\"\n    def __init__(self, offset, dp_domega, spline_kwargs=DEFAULT_PSF_SPLINE_KWARGS):\n\n        if not isinstance(offset, Angle):\n            raise ValueError(\"offset must be an Angle object.\")\n        if not isinstance(dp_domega, Quantity):\n            raise ValueError(\"dp_domega must be a Quantity object.\")\n\n        assert offset.ndim == dp_domega.ndim == 1\n        assert offset.shape == dp_domega.shape\n\n        # Store input arrays as quantities in default internal units\n        self._offset = offset.to('radian')\n        self._dp_domega = dp_domega.to('sr^-1')\n        self._dp_dtheta = (2 * np.pi * self._offset * self._dp_domega).to('radian^-1')\n        self._spline_kwargs = spline_kwargs\n\n        self._compute_splines(spline_kwargs)\n\n    @staticmethod\n    def from_shape(shape, width, offset):\n        \"\"\"Make TablePSF objects with commonly used shapes.\n\n        This function is mostly useful for examples and testing. \n\n        Parameters\n        ----------\n        shape : {'disk', 'gauss'}\n            PSF shape.\n        width : `~astropy.coordinates.Angle`\n            PSF width angle (radius for disk, sigma for Gauss).\n        offset : `~astropy.coordinates.Angle`\n            Offset angle\n\n        Returns\n        -------\n        psf : `TablePSF`\n            Table PSF\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> from astropy.coordinates import Angle\n        >>> from gammapy.irf import make_table_psf\n        >>> make_table_psf(shape='gauss', width=Angle(0.2, 'degree'),\n        ...                offset=Angle(np.linspace(0, 0.7, 100), 'degree'))\n        \"\"\"\n        if not isinstance(width, Angle):\n            raise ValueError(\"width must be an Angle object.\")\n        if not isinstance(offset, Angle):\n            raise ValueError(\"offset must be an Angle object.\")\n\n        if shape == 'disk':\n            amplitude = 1 / (np.pi * width.radian ** 2)\n            psf_value = np.where(offset < width, amplitude, 0)\n        elif shape == 'gauss':\n            gauss2d_pdf = Gauss2DPDF(sigma=width.radian)\n            psf_value = gauss2d_pdf(offset.radian)\n\n        psf_value = Quantity(psf_value, 'sr^-1')\n\n        return TablePSF(offset, psf_value)\n\n    def info(self):\n        \"\"\"Print basic info.\"\"\"\n        ss = array_stats_str(self._offset.degree, 'offset')\n        ss += 'integral = {0}\\n'.format(self.integral())\n\n        for containment in [50, 68, 80, 95]:\n            radius = self.containment_radius(0.01 * containment)\n            ss += ('containment radius {0} deg for {1}%\\n'\n                   .format(radius.degree, containment))\n\n        return ss\n\n    # TODO: remove because it's not flexible enough?\n    def __call__(self, lon, lat):\n        \"\"\"Evaluate PSF at a 2D position.\n\n        The PSF is centered on ``(0, 0)``.\n\n        Parameters\n        ----------\n        lon, lat : `~astropy.coordinates.Angle`\n            Longitude / latitude position\n\n        Returns\n        -------\n        psf_value : `~astropy.units.Quantity`\n            PSF value\n        \"\"\"\n        center = SkyCoord(0, 0, unit='radian')\n        point = SkyCoord(lon, lat)\n        offset = center.separation(point)\n        return self.evaluate(offset)\n\n    def kernel(self, pixel_size, offset_max=None, normalize=True,\n               discretize_model_kwargs=dict(factor=10)):\n        \"\"\"Make a 2-dimensional kernel image.\n\n        The kernel image is evaluated on a cartesian\n        grid with ``pixel_size`` spacing, not on the sphere.\n\n        Calls `astropy.convolution.discretize_model`,\n        allowing for accurate discretization.\n\n        Parameters\n        ----------\n        pixel_size : `~astropy.coordinates.Angle`\n            Kernel pixel size\n        discretize_model_kwargs : dict\n            Keyword arguments passed to\n            `astropy.convolution.discretize_model`\n\n        Returns\n        -------\n        kernel : `~astropy.units.Quantity`\n            Kernel 2D image of Quantities\n\n        Notes\n        -----\n        * In the future, `astropy.modeling.Fittable2DModel` and\n          `astropy.convolution.Model2DKernel` could be used to construct\n          the kernel.\n        \"\"\"\n        if not isinstance(pixel_size, Angle):\n            raise ValueError(\"pixel_size must be an Angle object.\")\n\n        if offset_max is None:\n            offset_max = self._offset.max()\n\n        def _model(x, y):\n            \"\"\"Model in the appropriate format for discretize_model.\"\"\"\n            offset = np.sqrt(x * x + y * y) * pixel_size\n            return self.evaluate(offset)\n\n        npix = int(offset_max.radian / pixel_size.radian)\n        pix_range = (-npix, npix + 1)\n\n        # FIXME: Using `discretize_model` is currently very cumbersome due to these issue:\n        # https://github.com/astropy/astropy/issues/2274\n        # https://github.com/astropy/astropy/issues/1763#issuecomment-39552900\n        #from astropy.modeling import Fittable2DModel\n        #\n        #class TempModel(Fittable2DModel):\n        #    @staticmethod\n        #    def evaluate(x, y):\n        #        return 42 temp_model_function(x, y)\n        #\n        #temp_model = TempModel()\n\n        #import IPython; IPython.embed()\n        array = discretize_oversample_2D(_model,\n                                         x_range=pix_range, y_range=pix_range,\n                                         **discretize_model_kwargs)\n        if normalize:\n            return array / array.value.sum()\n        else:\n            return array\n\n    def evaluate(self, offset, quantity='dp_domega'):\n        r\"\"\"Evaluate PSF.\n\n        The following PSF quantities are available:\n\n        * 'dp_domega': PDF per 2-dim solid angle :math:`\\Omega` in sr^-1\n\n            .. math:: \\frac{dP}{d\\Omega}\n\n        * 'dp_dtheta': PDF per 1-dim offset :math:`\\theta` in radian^-1\n\n            .. math:: \\frac{dP}{d\\theta} = 2 \\pi \\theta \\frac{dP}{d\\Omega}\n\n        Parameters\n        ----------\n        offset : `~astropy.coordinates.Angle`\n            Offset angle\n        quantity : {'dp_domega', 'dp_dtheta'}\n            Which PSF quantity?\n\n        Returns\n        -------\n        psf_value : `~astropy.units.Quantity`\n            PSF value\n        \"\"\"\n        if not isinstance(offset, Angle):\n            raise ValueError(\"offset must be an Angle object.\")\n\n        shape = offset.shape\n        x = np.array(offset.radian).flat\n\n        if quantity == 'dp_domega':\n            y = self._dp_domega_spline(x)\n            return Quantity(y, 'sr^-1').reshape(shape)\n        elif quantity == 'dp_dtheta':\n            y = self._dp_dtheta_spline(x)\n            return Quantity(y, 'radian^-1').reshape(shape)\n        else:\n            ss = 'Invalid quantity: {0}\\n'.format(quantity)\n            ss += \"Choose one of: 'dp_domega', 'dp_dtheta'\"\n            raise ValueError(ss)\n\n    def integral(self, offset_min=None, offset_max=None):\n        \"\"\"Compute PSF integral, aka containment fraction.\n\n        Parameters\n        ----------\n        offset_min, offset_max : `~astropy.coordinates.Angle`\n            Offset angle range\n\n        Returns\n        -------\n        integral : float\n            PSF integral\n        \"\"\"\n        if offset_min is None:\n            offset_min = self._offset[0]\n        else:\n            if not isinstance(offset_min, Angle):\n                raise ValueError(\"offset_min must be an Angle object.\")\n\n        if offset_max is None:\n            offset_max = self._offset[-1]\n        else:\n            if not isinstance(offset_max, Angle):\n                raise ValueError(\"offset_max must be an Angle object.\")\n\n        offset_min = self._offset_clip(offset_min)\n        offset_max = self._offset_clip(offset_max)\n\n        cdf_min = self._cdf_spline(offset_min)\n        cdf_max = self._cdf_spline(offset_max)\n\n        return cdf_max - cdf_min\n\n    def containment_radius(self, fraction):\n        \"\"\"Containment radius.\n\n        Parameters\n        ----------\n        fraction : array_like\n            Containment fraction (range 0 .. 1)\n\n        Returns\n        -------\n        radius : `~astropy.coordinates.Angle`\n            Containment radius angle\n        \"\"\"\n        radius = self._ppf_spline(fraction)\n        return Angle(radius, 'radian').to('degree')\n\n    def normalize(self):\n        \"\"\"Normalize PSF to unit integral.\n\n        Computes the total PSF integral via the :math:`dP / d\\theta` spline\n        and then divides the :math:`dP / d\\theta` array.\n        \"\"\"\n        integral = self.integral()\n\n        self._dp_dtheta /= integral\n\n        # Don't divide by 0\n        EPS = 1e-6\n        offset = np.clip(self._offset.radian, EPS, None)\n        offset = Quantity(offset, 'radian')\n        self._dp_domega = self._dp_dtheta / (2 * np.pi * offset)\n        self._compute_splines(self._spline_kwargs)\n\n    def broaden(self, factor, normalize=True):\n        r\"\"\"Broaden PSF by scaling the offset array.\n\n        For a broadening factor :math:`f` and the offset\n        array :math:`\\theta`, the offset array scaled\n        in the following way:\n\n        .. math::\n            \\theta_{new} = f \\times \\theta_{old}\n            \\frac{dP}{d\\theta}(\\theta_{new}) = \\frac{dP}{d\\theta}(\\theta_{old})\n\n        Parameters\n        ----------\n        factor : float\n            Broadening factor\n        normalize : bool\n            Normalize PSF after broadening\n        \"\"\"\n        self._offset *= factor\n        # We define broadening such that self._dp_domega remains the same\n        # so we only have to re-compute self._dp_dtheta and the slines here.\n        self._dp_dtheta = (2 * np.pi * self._offset * self._dp_domega).to('radian^-1')\n        self._compute_splines(self._spline_kwargs)\n\n        if normalize:\n            self.normalize()\n\n    def plot_psf_vs_theta(self, quantity='dp_domega'):\n        \"\"\"Plot PSF vs offset.\n\n        TODO: describe PSF ``quantity`` argument in a central place and link to it from here.\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        x = self._offset.to('degree')\n        y = self.evaluate(self._offset, quantity)\n\n        plt.plot(x.value, y.value, lw=2)\n        plt.semilogy()\n        plt.loglog()\n        plt.xlabel('Offset ({0})'.format(x.unit))\n        plt.ylabel('PSF ({0})'.format(y.unit))\n\n    def _compute_splines(self, spline_kwargs=DEFAULT_PSF_SPLINE_KWARGS):\n        \"\"\"Compute two splines representing the PSF.\n\n        * `_dp_domega_spline` is used to evaluate the 2D PSF.\n        * `_dp_dtheta_spline` is not really needed for most applications,\n          but is available via `eval`.\n        * `_cdf_spline` is used to compute integral and for normalisation.\n        * `_ppf_spline` is used to compute containment radii.\n        \"\"\"\n        from scipy.interpolate import UnivariateSpline\n\n        # Compute spline and normalize.\n        x, y = self._offset.value, self._dp_domega.value\n        self._dp_domega_spline = UnivariateSpline(x, y, **spline_kwargs)\n\n        x, y = self._offset.value, self._dp_dtheta.value \n        self._dp_dtheta_spline = UnivariateSpline(x, y, **spline_kwargs)\n\n        # We use the terminology for scipy.stats distributions\n        # http://docs.scipy.org/doc/scipy/reference/tutorial/stats.html#common-methods\n\n        # cdf = \"cumulative distribution function\"\n        self._cdf_spline = self._dp_dtheta_spline.antiderivative()\n\n        # ppf = \"percent point function\" (inverse of cdf)\n        # Here's a discussion on methods to compute the ppf\n        # http://mail.scipy.org/pipermail/scipy-user/2010-May/025237.html\n        x = self._offset.value\n        y = self._cdf_spline(x)\n        self._ppf_spline = UnivariateSpline(y, x, **spline_kwargs)\n\n    def _offset_clip(self, offset):\n        \"\"\"Clip to offset support range, because spline extrapolation is unstable.\"\"\" \n        offset = Angle(offset, 'radian').radian\n        offset = np.clip(offset, 0, self._offset[-1].radian)\n        return offset\n\n\nclass EnergyDependentTablePSF(object):\n    \"\"\"Energy-dependent radially-symmetric table PSF (``gtpsf`` format).\n\n    TODO: add references and explanations.\n\n    Parameters\n    ----------\n    energy : `~astropy.units.Quantity`\n        Energy (1-dim)\n    offset : `~astropy.coordinates.Angle`\n        Offset angle (1-dim)\n    exposure : `~astropy.units.Quantity`\n        Exposure (1-dim)\n    psf_value : `~astropy.units.Quantity`\n        PSF (2-dim with axes: psf[energy_index, offset_index]\n    \"\"\"\n    def __init__(self, energy, offset, exposure, psf_value):\n        if not isinstance(energy, Quantity):\n            raise ValueError(\"energy must be a Quantity object.\")\n        if not isinstance(offset, Angle):\n            raise ValueError(\"offset must be an Angle object.\")\n        if not isinstance(exposure, Quantity):\n            raise ValueError(\"exposure must be a Quantity object.\")\n        if not isinstance(psf_value, Quantity):\n            raise ValueError(\"psf_value must be a Quantity object.\")\n\n        self.energy = energy.to('GeV')\n        self.offset = offset.to('radian')\n        self.exposure = exposure.to('cm^2 s')\n        self.psf_value = psf_value.to('sr^-1')\n\n        # Cache for TablePSF at each energy ... only computed when needed\n        self._table_psf_cache = [None] * len(self.energy)\n\n    @staticmethod\n    def from_fits(hdu_list):\n        \"\"\"Create EnergyDependentTablePSF from ``gtpsf`` format HDU list.\n\n        Parameters\n        ----------\n        hdu_list : `~astropy.io.fits.HDUList`\n            HDU list with ``THETA`` and ``PSF`` extensions.\n\n        Returns\n        -------\n        psf : `EnergyDependentTablePSF`\n            PSF object.\n        \"\"\"\n        offset = Angle(hdu_list['THETA'].data['Theta'], 'degree')\n        energy = Quantity(hdu_list['PSF'].data['Energy'], 'MeV')\n        exposure = Quantity(hdu_list['PSF'].data['Exposure'], 'cm^2 s')\n        psf_value = Quantity(hdu_list['PSF'].data['PSF'], 'sr^-1')\n\n        return EnergyDependentTablePSF(energy, offset, exposure, psf_value)\n\n    def to_fits(self):\n        \"\"\"Convert to FITS HDU list format.\n\n        Returns\n        -------\n        hdu_list : `~astropy.io.fits.HDUList`\n            PSF in HDU list format.\n        \"\"\"\n        # TODO: write HEADER keywords as gtpsf\n\n        data = self.offset\n        theta_hdu = fits.BinTableHDU(data=data, name='Theta')\n\n        data = [self.energy, self.exposure, self.psf_value]\n        psf_hdu = fits.BinTableHDU(data=data, name='PSF')\n\n        hdu_list = fits.HDUList([theta_hdu, psf_hdu])\n        return hdu_list\n\n    @staticmethod\n    def read(filename):\n        \"\"\"Read FITS format PSF file (``gtpsf`` output).\n\n        Parameters\n        ----------\n        filename : str\n            File name\n\n        Returns\n        -------\n        psf : `EnergyDependentTablePSF`\n            PSF object.\n        \"\"\"\n        hdu_list = fits.open(filename)\n        return EnergyDependentTablePSF.from_fits(hdu_list)\n\n    def write(self, *args, **kwargs):\n        \"\"\"Write to FITS file.\n\n        Calls `~astropy.io.fits.HDUList.writeto`, forwarding all arguments.\n        \"\"\"\n        self.to_fits().writeto(*args, **kwargs)\n\n    def table_psf_at_energy(self, energy, **kwargs):\n        \"\"\"TablePSF at a given energy.\n\n        Extra ``kwargs`` are passed to the `~gammapy.irf.TablePSF` constructor.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy\n\n        Returns\n        -------\n        psf : `~gammapy.irf.TablePSF`\n            PSF\n        \"\"\"\n        if not isinstance(energy, Quantity):\n            raise ValueError(\"energy must be a Quantity object.\")\n\n        energy_index = self._energy_index(energy)\n        return self._get_1d_table_psf(energy_index, **kwargs)\n\n    def table_psf_in_energy_band(self, energy_band, spectral_index=2, spectrum=None, **kwargs):\n        \"\"\"Average PSF in a given energy band.\n\n        Expected counts in sub energy bands given the given exposure\n        and spectrum are used as weights.\n\n        Parameters\n        ----------\n        energy_band : `~astropy.units.Quantity`\n            Energy band\n        spectral_index : float\n            Power law spectral index (used if spectrum=None).\n        spectrum : callable\n            Spectrum (callable with energy as parameter).\n\n        Returns\n        -------\n        psf : `TablePSF`\n            Table PSF\n        \"\"\"\n        if spectrum is None:\n            def spectrum(energy):\n                return (energy / energy_band[0]) ** (-spectral_index)\n\n        # TODO: warn if `energy_band` is outside available data.\n        energy_idx_min, energy_idx_max = self._energy_index(energy_band)\n\n        # TODO: extract this into a utility function `npred_weighted_mean()`\n\n        # Compute weights for energy bins\n        weights = np.zeros_like(self.energy.value, dtype=np.float64)\n        for idx in range(energy_idx_min, energy_idx_max - 1):\n            energy_min = self.energy[idx]\n            energy_max = self.energy[idx + 1]\n            exposure = self.exposure[idx]\n\n            flux = spectrum(energy_min)\n            weights[idx] = (exposure * flux * (energy_max - energy_min)).value\n\n        # Normalize weights to sum to 1\n        weights = weights / weights.sum()\n\n        # Compute weighted PSF value array\n        total_psf_value = np.zeros_like(self._get_1d_psf_values(0), dtype=np.float64)\n        for idx in range(energy_idx_min, energy_idx_max - 1):\n            psf_value = self._get_1d_psf_values(idx)\n            total_psf_value += weights[idx] * psf_value\n\n        # TODO: add version that returns `total_psf_value` without\n        # making a `TablePSF`.\n        return TablePSF(self.offset, total_psf_value, **kwargs)\n\n    def containment_radius(self, energy, fraction):\n        \"\"\"Containment radius.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy\n        fraction : float\n            Containment fraction in %\n\n        Returns\n        -------\n        radius : `~astropy.units.Quantity`\n            Containment radius in deg\n        \"\"\"\n        # TODO: useless at the moment ... support array inputs or remove!\n        psf = self.table_psf_at_energy(energy)\n        return psf.containment_radius(fraction)\n\n    def integral(self, energy, offset_min, offset_max):\n        \"\"\"Containment fraction.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy\n        offset_min, offset_max : `~astropy.coordinates.Angle`\n            Offset\n\n        Returns\n        -------\n        fraction : array_like\n            Containment fraction (in range 0 .. 1)\n        \"\"\"\n        # TODO: useless at the moment ... support array inputs or remove!\n        psf = self.table_psf_at_energy(energy)\n        return psf.integral(offset_min, offset_max)\n\n    def info(self):\n        \"\"\"Print basic info.\"\"\"\n        # Summarise data members\n        ss = array_stats_str(self.offset.to('degree'), 'offset')\n        ss += array_stats_str(self.energy, 'energy')\n        ss += array_stats_str(self.exposure, 'exposure')\n\n        #ss += 'integral = {0}\\n'.format(self.integral())\n\n        # Print some example containment radii\n        fractions = [0.68, 0.95]\n        energies = Quantity([10, 100], 'GeV')\n        for energy in energies:\n            for fraction in fractions:\n                radius = self.containment_radius(energy=energy, fraction=fraction)\n                ss += '{0}% containment radius at {1}: {2}\\n'.format(100 * fraction, energy, radius)\n        return ss\n\n    def plot_psf_vs_theta(self, filename=None, energies=[1e4, 1e5, 1e6]):\n        \"\"\"Plot PSF vs theta.\n\n        Parameters\n        ----------\n        TODO\n        \"\"\"\n        import matplotlib.pyplot as plt\n        plt.figure(figsize=(6, 4))\n\n        for energy in energies:\n            energy_index = self._energy_index(energy)\n            psf = self.psf_value[energy_index, :]\n            label = '{0} GeV'.format(1e-3 * energy)\n            x = np.hstack([-self.theta[::-1], self.theta])\n            y = 1e-6 * np.hstack([psf[::-1], psf])\n            plt.plot(x, y, lw=2, label=label)\n        # plt.semilogy()\n        # plt.loglog()\n        plt.legend()\n        plt.xlim(-0.2, 0.5)\n        plt.xlabel('Offset (deg)')\n        plt.ylabel('PSF (1e-6 sr^-1)')\n        plt.tight_layout()\n\n        if filename != None:\n            plt.savefig(filename)\n\n    def plot_containment_vs_energy(self, filename=None):\n        \"\"\"Plot containment versus energy.\"\"\"\n        raise NotImplementedError\n        import matplotlib.pyplot as plt\n        plt.clf()\n\n        if filename != None:\n            plt.savefig(filename)\n\n    def plot_exposure_vs_energy(self, filename=None):\n        \"\"\"Plot exposure versus energy.\"\"\"\n        import matplotlib.pyplot as plt\n        plt.figure(figsize=(4, 3))\n        plt.plot(self.energy, self.exposure, color='black', lw=3)\n        plt.semilogx()\n        plt.xlabel('Energy (MeV)')\n        plt.ylabel('Exposure (cm^2 s)')\n        plt.xlim(1e4 / 1.3, 1.3 * 1e6)\n        plt.ylim(0, 1.5e11)\n        plt.tight_layout()\n\n        if filename != None:\n            plt.savefig(filename)\n\n    def _energy_index(self, energy):\n        \"\"\"Find energy array index.\n        \"\"\"\n        # TODO: test with array input\n        return np.searchsorted(self.energy, energy)\n\n    def _get_1d_psf_values(self, energy_index):\n        \"\"\"Get 1-dim PSF value array.\n\n        Parameters\n        ----------\n        energy_index : int\n            Energy index\n\n        Returns\n        -------\n        psf_values : `~astropy.units.Quantity`\n            PSF value array\n        \"\"\"\n        psf_values = self.psf_value[energy_index, :].flatten().copy()\n        return psf_values\n\n    def _get_1d_table_psf(self, energy_index, **kwargs):\n        \"\"\"Get 1-dim TablePSF (cached).\n\n        Parameters\n        ----------\n        energy_index : int\n            Energy index\n\n        Returns\n        -------\n        table_psf : `TablePSF`\n            Table PSF\n        \"\"\"\n        # TODO: support array_like `energy_index` here?\n        if self._table_psf_cache[energy_index] is None:\n            psf_value = self._get_1d_psf_values(energy_index)\n            table_psf = TablePSF(self.offset, psf_value, **kwargs)\n            self._table_psf_cache[energy_index] = table_psf\n\n        return self._table_psf_cache[energy_index]\n"
    },
    {
      "filename": "gammapy/obs/__init__.py",
      "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"\nObservation utility functions and classes\n\"\"\"\nfrom .observers import *\nfrom .observation import *\nfrom .datastore import *\n"
    },
    {
      "filename": "gammapy/obs/datastore.py",
      "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\nimport os\nimport numpy as np\nfrom astropy.table import Table\nfrom astropy.coordinates import SkyCoord, Angle\nfrom ..catalog import select_sky_box, skycoord_from_table\nfrom ..obs import ObservationTable\n\n__all__ = ['DataStore',\n           'DataStoreIndexTable',\n           ]\n\n\ndef _make_filename_hess_scheme(obs_id, filetype='events'):\n    \"\"\"Make filename string for the HESS storage scheme.\n\n    Parameters\n    ----------\n    obs_id : int\n        Observation ID\n    filetype : {'events', 'effective area', 'psf', 'background'}\n        Type of file\n\n    Examples\n    --------\n    >>> _make_filename_hess_scheme(obs_id=89565, filetype='effective area')\n    'run089400-089599/run089565/hess_aeff_2d_089565.fits.gz'\n    \"\"\"\n    obs_id_min = obs_id - (obs_id % 200)\n    obs_id_max = obs_id_min + 199\n    group_folder = 'run{:06d}-{:06d}'.format(obs_id_min, obs_id_max)\n    obs_folder = 'run{:06d}'.format(obs_id)\n\n    if filetype == 'events':\n        label = 'events'\n    elif filetype == 'psf':\n        label = 'psf_king'\n    elif filetype == 'effective area':\n        label = 'aeff_2d'\n    elif filetype == 'background':\n        label = 'bkg_offruns'\n    else:\n        raise ValueError('Unknown filetype: {}'.format(filetype))\n\n    filename = 'hess_{}_{:06d}.fits.gz'.format(label, obs_id)\n\n    return os.path.join(group_folder, obs_folder, filename)\n\n\nclass DataStoreIndexTable(Table):\n    \"\"\"Data store index table.\n\n    The index table is a FITS file that stores which observations\n    are available and what their most important parameters are.\n\n    This makes it possible to select observation of interest and find out\n    what data is available without opening up thousands of FITS files\n    that contain the event list and IRFs and have similar information in the\n    FITS header.\n\n    TODO: how is this different from an `gammapy.obs.ObservationTable`?\n    Can they be combined or code be shared?\n    (I think we want both, but the `DataStoreIndexTable` contains more info\n    like event list and IRF file names and basic info\n    ... maybe it should be a sub-class of `gammapy.obs.ObservationTable`?)\n\n    \"\"\"\n    # For now I've decided to not do the cleanup in `__init__`,\n    # but instead in `read`.\n    # See https://groups.google.com/d/msg/astropy-dev/0EaOw9peWSk/MSjH7q_7htoJ\n    # def __init__(self, *args, **kwargs):\n    #     super(DataStoreIndexTable, self).__init__(*args, **kwargs)\n    #     self._fixes()\n\n    @staticmethod\n    def read(*args, **kwargs):\n        \"\"\"Read from FITS file. See `~astropy.table.Table.read`.\"\"\"\n        table = Table.read(*args, **kwargs)\n        table = DataStoreIndexTable(table)\n        table._init_cleanup()\n        return table\n\n    def _init_cleanup(self):\n        # Rename to canonical column names\n        renames = [('RA_PNT', 'RA'),\n                   ('DEC_PNT', 'DEC')\n                   ]\n        for name, new_name in renames:\n            self.rename_column(name, new_name)\n\n        # Add useful extra columns\n        if not set(['GLON', 'GLAT']).issubset(self.colnames):\n            skycoord = skycoord_from_table(self).galactic\n            self['GLON'] = skycoord.l.degree\n            self['GLAT'] = skycoord.b.degree\n\n    def info(self):\n        ss = 'Data store index table summary:\\n'\n        ss += 'Number of observations: {}\\n'.format(len(self))\n        obs_id = self['OBS_ID']\n        ss += 'Observation IDs: {} to {}\\n'.format(obs_id.min(), obs_id.max())\n        ss += 'Dates: {} to {}\\n'.format('TODO', 'TODO')\n        return ss\n\n    @property\n    def radec(self):\n        \"\"\"ICRS sky coordinates (`~astropy.coordinates.SkyCoord`)\"\"\"\n        return SkyCoord(self['RA'], self['DEC'], unit='deg', frame='icrs')\n\n    @property\n    def galactic(self):\n        \"\"\"Galactic sky coordinates (`~astropy.coordinates.SkyCoord`)\"\"\"\n        return SkyCoord(self['GLON'], self['GLAT'], unit='deg', frame='galactic')\n\n\nclass DataStore(object):\n    \"\"\"Data store - convenient way to access and select data.\n\n    This is an ad-hoc prototype implementation for HESS of what will be the \"archive\"\n    and \"archive interface\" for CTA.\n\n    TODO: add methods to sync with remote datastore...\n\n    Parameters\n    ----------\n    dir : `str`\n        Data store directory on user machine\n    scheme : {'hess'}\n        Scheme of organising and naming the files.\n    \"\"\"\n\n    def __init__(self, dir, scheme='hess'):\n        self.dir = dir\n        self.index_table_filename = 'runinfo.fits'\n        filename = os.path.join(dir, self.index_table_filename)\n        print('Reading {}'.format(filename))\n        self.index_table = DataStoreIndexTable.read(filename)\n        self.scheme = scheme\n\n    def info(self):\n        \"\"\"Summary info string.\"\"\"\n        ss = 'Data store summary info:\\n'\n        ss += 'Directory: {}\\n'.format(self.dir)\n        ss += 'Index table: {}\\n'.format(self.index_table_filename)\n        ss += 'Scheme: {}\\n'.format(self.scheme)\n        ss += self.index_table.info()\n        return ss\n\n    def check_integrity(self, logger):\n        \"\"\"Check integrity, i.e. whether index table and files match.\n        \"\"\"\n        logger.info('Checking event list files')\n        available = self.check_available_event_lists(logger)\n        if np.any(~available):\n            logger.warning('Number of missing event list files: {}'.format(np.invert(available).sum()))\n\n        # TODO: implement better, more complete integrity checks.\n\n    def make_table_of_files(self, observation_table=None, filetypes=['events']):\n        \"\"\"Make list of files in the datastore directory.\n\n        Parameters\n        ----------\n        observation_table : `~gammapy.obs.ObservationTable` or None\n            Observation table (``None`` means select all observations)\n        filetypes : list of str\n            File types (TODO: document in a central location and reference from here)\n\n        Returns\n        -------\n        table : `~astropy.table.Table`\n            Table summarising info about files.\n        \"\"\"\n        if observation_table is None:\n            observation_table = ObservationTable(self.index_table)\n\n        data = []\n        for observation in observation_table:\n            for filetype in filetypes:\n                row = dict()\n                row['OBS_ID'] = observation['OBS_ID']\n                row['filetype'] = filetype\n                filename = self.filename(observation['OBS_ID'], filetype=filetype, abspath=True)\n                row['filename'] = filename\n                data.append(row)\n\n        return Table(data=data, names=['OBS_ID', 'filetype', 'filename'])\n\n    def make_summary_plots(self):\n        \"\"\"Make some plots summarising the available observations.\n\n        E.g. histograms of time, run duration, muon efficiency, zenith angle, ...\n        Aitoff plot showing run locations.\n        \"\"\"\n        raise NotImplementedError\n\n    def filename(self, obs_id, filetype='events', abspath=True):\n        \"\"\"File name (relative to datastore `dir`).\n\n        Parameters\n        ----------\n        obs_id : int\n            Observation ID\n        filetype : {'events', 'effective area', 'psf', 'background'}\n            Type of file\n        abspath : bool\n            Absolute path (including data store dir)?\n\n        Returns\n        -------\n        filename : str\n            Filename (including the directory path).\n        \"\"\"\n        scheme = self.scheme\n\n        if scheme == 'hess':\n            filename = _make_filename_hess_scheme(obs_id, filetype)\n        else:\n            raise ValueError('Invalid scheme: {}'.format(scheme))\n\n        if abspath:\n            return os.path.join(self.dir, filename)\n        else:\n            return filename\n\n    def make_observation_table(self, selection=None):\n        \"\"\"Make an observation table, applying some selection.\n\n        TODO: implement a more flexible scheme to make box cuts\n        on any fields (including e.g. OBSID or TIME\n        Not sure what a simple yet powerful method to implement this is!?\n\n        Parameters\n        ----------\n        selection : TODO\n            TODO: describe me\n\n        Returns\n        -------\n        table : `~gammapy.obs.ObservationTable`\n            Observation table\n\n        Example\n        -------\n        >>> selection = dict(shape='box', frame='galactic',\n        ...                  lon=(-100, 50), lat=(-5, 5), border=2)\n        >>> run_list = data_store.make_observation_table(selection)\n        \"\"\"\n        table = self.index_table\n\n        if selection:\n            selection_region_shape = selection['shape']\n\n            if selection_region_shape == 'box':\n                lon = selection['lon']\n                lat = selection['lat']\n                border = selection['border']\n                lon = Angle([lon[0] - border, lon[1] + border], 'deg')\n                lat = Angle([lat[0] - border, lat[1] + border], 'deg')\n                # print(lon, lat)\n                table = select_sky_box(table,\n                                       lon_lim=lon, lat_lim=lat,\n                                       frame=selection['frame'])\n            else:\n                raise ValueError('Invalid selection type: {}'.format(selection_region_shape))\n\n        return ObservationTable(table)\n\n    def check_available_event_lists(self, logger=None):\n        \"\"\"Check if all event lists are available.\n\n        TODO: extend this function, or combine e.g. with ``make_table_of_files``.\n\n        Returns\n        -------\n        file_available : `~numpy.ndarray`\n            Boolean mask which files are available.\n        \"\"\"\n        observation_table = self.index_table\n        file_available = np.ones(len(observation_table), dtype='bool')\n        for ii in range(len(observation_table)):\n            obs_id = observation_table['OBS_ID'][ii]\n            filename = self.filename(obs_id)\n            if not os.path.isfile(filename):\n                file_available[ii] = False\n                if logger:\n                    logger.warning('For OBS_ID = {:06d} the event list file is missing: {}'\n                                   ''.format(obs_id, filename))\n\n        return file_available\n"
    },
    {
      "filename": "gammapy/obs/observation.py",
      "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\nimport numpy as np\nfrom astropy.table import Table\n\n__all__ = ['Observation', 'ObservationTable']\n\n\nclass Observation(object):\n    \"\"\"Observation (a.k.a. Run).\n\n    TODO: not clear if this class is useful.\n\n    Parameters\n    ----------\n    TODO\n    \"\"\"\n    def __init__(self, GLON, GLAT, livetime=1800,\n                 eff_area=1e12, background=0):\n        self.GLON = GLON\n        self.GLAT = GLAT\n        self.livetime = livetime\n\n    def wcs_header(self, system='FOV'):\n        \"\"\"Create a WCS FITS header for an per-run image.\n\n        The image is centered on the run position in one of these systems:\n        FOV, Galactic, Equatorial\n        \"\"\"\n        raise NotImplementedError\n\n\nclass ObservationTable(Table):\n    \"\"\"Observation table (a.k.a. run list).\n\n    This is an `~astropy.table.Table` sub-class, with a few\n    convenience methods and the following columns:\n\n    * ``OBS_ID``\n    * ``ONTIME``\n    * ``LIVETIME``\n    * ...\n\n    \"\"\"\n\n    def info(self):\n        ss = 'Observation table:\\n'\n        ss += 'Number of observations: {}\\n'.format(len(self))\n        ontime = self['ONTIME'].sum()\n        ss += 'Total observation time: {}\\n'.format(ontime)\n        livetime = self['LIVETIME'].sum()\n        ss += 'Total live time: {}\\n'.format(livetime)\n        dtf = 100. * (1 - livetime / ontime)\n        ss += 'Average dead time fraction: {:5.2f}%'.format(dtf)\n        return ss\n\n    def select_linspace_subset(self, num):\n        \"\"\"Select subset of observations.\n\n        This is mostly useful for testing, if you want to make\n        the analysis run faster.\n\n        TODO: implement more methods to subset and split observation lists\n        as well as functions to summarise differences between\n        observation lists and e.g. select the common subset.\n        \"\"\"\n        indices = np.linspace(start=0, stop=len(self), num=num, endpoint=False)\n        # Round down to nearest integer\n        indices = indices.astype('int')\n        return self[indices]\n\n"
    },
    {
      "filename": "gammapy/obs/tests/test_datastore.py",
      "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom __future__ import print_function, division\nfrom ...obs import DataStore\n\n\ndef test_DataStore():\n    # data_store = DataStore(dir='TODO')\n    # TODO: add tests\n    assert True\n"
    },
    {
      "filename": "gammapy/obs/tests/test_observation.py",
      "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom __future__ import print_function, division\nfrom ...obs import Observation, ObservationTable\n\n\ndef test_Observation():\n    Observation(GLON=42, GLAT=43)\n\n\ndef test_ObservationTable():\n    ObservationTable()\n"
    },
    {
      "filename": "gammapy/scripts/__init__.py",
      "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\"\"\"Gammapy command line tools.\n\nTODO: document how this works and how users should write their own\ncommand line tools.\n\"\"\"\n\nfrom .background_cube import *\nfrom .bin_cube import *\nfrom .bin_image import *\nfrom .check import *\nfrom .coordinate_images import *\nfrom .cwt import *\nfrom .derived_images import *\nfrom .detect import *\nfrom .find_obs import *\nfrom .image_decompose_a_trous import *\nfrom .info import *\nfrom .irf_info import *\nfrom .irf_root_to_fits import *\nfrom .iterative_source_detect import *\nfrom .look_up_image import *\nfrom .model_image import *\nfrom .pfmap import *\nfrom .pfsim import *\nfrom .pfspec import *\nfrom .reflected_regions import *\nfrom .residual_images import *\nfrom .root_to_fits import *\nfrom .sherpa_like import *\nfrom .sherpa_model_image import *\nfrom .significance_image import *\nfrom .simulate_source_catalog import *\nfrom .ts_image import *\nfrom .xspec import *\n"
    },
    {
      "filename": "gammapy/scripts/find_obs.py",
      "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\nimport sys\nimport argparse\nfrom ..utils.scripts import get_parser\n\n__all__ = ['find_obs']\n\n\ndef main(args=None):\n    parser = get_parser(find_obs)\n    parser.add_argument('infile', type=argparse.FileType('r'),\n                        help='Input run list file name')\n    parser.add_argument('outfile', nargs='?', type=argparse.FileType('w'),\n                        default=sys.stdout,\n                        help='Output run list file name (default: stdout)')\n    parser.add_argument('x', type=float,\n                        help='x coordinate (deg)')\n    parser.add_argument('y', type=float,\n                        help='y coordinate (deg)')\n    parser.add_argument('--pix', action='store_true',\n                        help='Input coordinates are pixels '\n                        '(default is world coordinates)')\n    parser.add_argument('--overwrite', action='store_true',\n                        help='Overwrite existing output file?')\n    args = parser.parse_args(args)\n    find_obs(**vars(args))\n\n\ndef find_obs(infile,\n             outfile,\n             x,\n             y,\n             pix,\n             overwrite):\n    \"\"\"Select a subset of observations from a given observation list.\n\n    TODO: explain.\n    \"\"\"\n    import logging\n    logging.basicConfig(level=logging.DEBUG, format='%(levelname)s - %(message)s')\n    # from gammapy.obs import RunList\n\n    # TODO: implement\n    raise NotImplementedError"
    },
    {
      "filename": "setup.py",
      "content": "#!/usr/bin/env python\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n\nimport glob\nimport os\nimport sys\n\nimport ah_bootstrap\nfrom setuptools import setup\n\n# A dirty hack to get around some early import/configurations ambiguities\nif sys.version_info[0] >= 3:\n    import builtins\nelse:\n    import __builtin__ as builtins\nbuiltins._ASTROPY_SETUP_ = True\n\nfrom astropy_helpers.setup_helpers import (\n    register_commands, adjust_compiler, get_debug_option, get_package_info)\nfrom astropy_helpers.git_helpers import get_git_devstr\nfrom astropy_helpers.version_helpers import generate_version_py\n\n# Get some values from the setup.cfg\nfrom distutils import config\nconf = config.ConfigParser()\nconf.read(['setup.cfg'])\nmetadata = dict(conf.items('metadata'))\n\nPACKAGENAME = metadata.get('package_name', 'packagename')\nDESCRIPTION = metadata.get('description', 'Astropy affiliated package')\nAUTHOR = metadata.get('author', '')\nAUTHOR_EMAIL = metadata.get('author_email', '')\nLICENSE = metadata.get('license', 'unknown')\nURL = metadata.get('url', 'http://astropy.org')\n\n# Get the long description from the package's docstring\n#__import__(PACKAGENAME)\n#package = sys.modules[PACKAGENAME]\n#LONG_DESCRIPTION = package.__doc__\nLONG_DESCRIPTION = open('LONG_DESCRIPTION.rst').read()\n\n\n# Store the package name in a built-in variable so it's easy\n# to get from other parts of the setup infrastructure\nbuiltins._ASTROPY_PACKAGE_NAME_ = PACKAGENAME\n\n# VERSION should be PEP386 compatible (http://www.python.org/dev/peps/pep-0386)\n# We use the format is `x.y` or `x.y.z` or `x.y.dev`\nVERSION = '0.2.dev'\n\n# Indicates if this version is a release version\nRELEASE = 'dev' not in VERSION\n\nif not RELEASE:\n    VERSION += get_git_devstr(False)\n\n# Populate the dict of setup command overrides; this should be done before\n# invoking any other functionality from distutils since it can potentially\n# modify distutils' behavior.\ncmdclassd = register_commands(PACKAGENAME, VERSION, RELEASE)\n\n# Adjust the compiler in case the default on this platform is to use a\n# broken one.\nadjust_compiler(PACKAGENAME)\n\n# Freeze build information in version.py\ngenerate_version_py(PACKAGENAME, VERSION, RELEASE,\n                    get_debug_option(PACKAGENAME))\n\n# Get configuration information from all of the various subpackages.\n# See the docstring for setup_helpers.update_package_files for more\n# details.\npackage_info = get_package_info()\n\n# Add the project-global data\npackage_info['package_data'].setdefault(PACKAGENAME, [])\n\n# Command-line scripts\n# Please keep the list in alphabetical order\nentry_points = {}\nentry_points['console_scripts'] = [\n    'gammapy-background-cube = gammapy.scripts.background_cube:main',\n    'gammapy-bin-cube = gammapy.scripts.bin_cube:main',\n    'gammapy-bin-image = gammapy.scripts.bin_image:main',\n    'gammapy-coordinate-images = gammapy.scripts.coordinate_images:main',\n    'gammapy-cwt = gammapy.scripts.cwt:main',\n    'gammapy-derived-images = gammapy.scripts.derived_images:main',\n    'gammapy-detect = gammapy.scripts.detect:main',\n    'gammapy-find-obs = gammapy.scripts.find_obs:main',\n    'gammapy-image-decompose-a-trous = gammapy.scripts.image_decompose_a_trous:main',\n    'gammapy-info = gammapy.scripts.info:main',\n    'gammapy-irf-info = gammapy.scripts.irf_info:main',\n    'gammapy-irf-root-to-fits = gammapy.scripts.irf_root_to_fits:main',\n    'gammapy-iterative-source-detect = gammapy.scripts.iterative_source_detect:main',\n    'gammapy-look-up-image = gammapy.scripts.look_up_image:main',\n    'gammapy-model-image = gammapy.scripts.model_image:main',\n    'gammapy-pfmap = gammapy.scripts.pfmap:main',\n    'gammapy-pfsim = gammapy.scripts.pfsim:main',\n    'gammapy-pfspec = gammapy.scripts.pfspec:main',\n    'gammapy-reflected-regions = gammapy.scripts.reflected_regions:main',\n    'gammapy-residual-images = gammapy.scripts.residual_images:main',\n    'gammapy-root-to-fits = gammapy.scripts.root_to_fits:main',\n    'gammapy-sherpa-like = gammapy.scripts.sherpa_like:main',\n    'gammapy-sherpa-model-image = gammapy.scripts.sherpa_model_image:main',\n    'gammapy-significance-image = gammapy.scripts.significance_image:main',\n    'gammapy-simulate-source-catalog = gammapy.scripts.simulate_source_catalog:main',\n    'gammapy-test = gammapy.scripts.check:main',\n    'gammapy-ts-image = gammapy.scripts.ts_image:main',\n    'gammapy-xspec = gammapy.scripts.xpsec:main',\n]\n\n# Note: usually the `affiliated_package/data` folder is used for data files.\n# In Gammapy we use `gammapy/data` as a sub-package.\n# Uncommenting the following line was needed to avoid an error during\n# the `python setup.py build` phase\n# package_info['package_data'][PACKAGENAME].append('data/*')\n\n# Include all .c files, recursively, including those generated by\n# Cython, since we can not do this in MANIFEST.in with a \"dynamic\"\n# directory name.\nc_files = []\nfor root, dirs, files in os.walk(PACKAGENAME):\n    for filename in files:\n        if filename.endswith('.c'):\n            c_files.append(\n                os.path.join(\n                    os.path.relpath(root, PACKAGENAME), filename))\npackage_info['package_data'][PACKAGENAME].extend(c_files)\n\nsetup(name=PACKAGENAME,\n      version=VERSION,\n      description=DESCRIPTION,\n      # Note: these are the versions we test.\n      # Older versions could work, but are unsupported.\n      # To find out if everything works run the Gammapy tests.\n      install_requires=['numpy>=1.6', 'astropy'],\n      extras_require=dict(\n          plotting=['matplotlib>=1.4', 'wcsaxes>=0.3', 'aplpy>=0.9'],\n          analysis=['scipy>=0.14', 'scikit-image>=0.10', 'photutils>=0.1',\n                    'reproject', 'uncertainties>=2.4', 'naima']\n      ),\n      provides=[PACKAGENAME],\n      author=AUTHOR,\n      author_email=AUTHOR_EMAIL,\n      license=LICENSE,\n      url=URL,\n      long_description=LONG_DESCRIPTION,\n      classifiers=[\n          'Intended Audience :: Science/Research',\n          'License :: OSI Approved :: BSD License',\n          'Operating System :: OS Independent',\n          'Programming Language :: C',\n          'Programming Language :: Cython',\n          'Programming Language :: Python :: 2',\n          'Programming Language :: Python :: 2.7',\n          'Programming Language :: Python :: 3',\n          'Programming Language :: Python :: 3.3',\n          'Programming Language :: Python :: 3.4',\n          'Programming Language :: Python :: Implementation :: CPython',\n          'Topic :: Scientific/Engineering :: Astronomy',\n          'Development Status :: 3 - Alpha',\n      ],\n      cmdclass=cmdclassd,\n      zip_safe=False,\n      use_2to3=False,\n      entry_points=entry_points,\n      **package_info\n)\n"
    }
  ]
}