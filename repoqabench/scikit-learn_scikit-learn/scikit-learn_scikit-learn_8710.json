{
  "repo_name": "scikit-learn_scikit-learn",
  "issue_id": "8710",
  "issue_description": "# CalibratedClassifierCV doesn't interact properly with Pipeline estimators \n\nHi, \r\n\r\nI'm trying to use CalibratedClassifierCV to calibrate the probabilities from a Gradient Boosted Tree model. The GBM is wrapped in a Pipeline estimator, where the initial stages of the Pipeline convert categoricals (using DictVectorizer) prior to the GBM being fit. The issue is that when I try to similarly use CalibratedClassifierCV, with a prefit estimator, it fails when I pass in the data. Here's a small example: \r\n\r\n```py\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.feature_extraction import DictVectorizer\r\nfrom sklearn.calibration import CalibratedClassifierCV, _CalibratedClassifier\r\nfrom sklearn.pipeline import Pipeline\r\n\r\nfake_features = [\r\n    {'state':'NY','age':'adult'},\r\n    {'state':'TX','age':'adult'},\r\n    {'state':'VT','age':'child'}\r\n]\r\n\r\nlabels = [1,0,1]\r\n\r\npipeline = Pipeline([\r\n            ('vectorizer',DictVectorizer()),\r\n            ('clf',RandomForestClassifier())\r\n    ])\r\n\r\npipeline.fit(fake_features, labels)\r\n\r\nclf_isotonic = CalibratedClassifierCV(base_estimator=pipeline, cv='prefit', method='isotonic')\r\nclf_isotonic.fit(fake_features, labels)\r\n\r\n```\r\n\r\nWhen running that, I get the following error on the last line:\r\n```\r\nTypeError: float() argument must be a string or a number, not 'dict'\r\n```\r\n\r\n\r\nOn the other hand, if I replace the last two lines with the following, things work fine: \r\n```\r\nclf_isotonic = _CalibratedClassifier(base_estimator=pipeline, method='isotonic')\r\nclf_isotonic.fit(fake_features, labels)\r\n```\r\n\r\n\r\nIt seems that CalibratedClassifierCV checks to see if the X data is valid prior to invoking anything about the base estimator (https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/calibration.py#L126). In my case, this logic seems slightly off since I'm using the pipeline to convert the data into the proper form before feeding it into estimator. \r\n\r\nOn the other hand, _CalibratedClassifier doesn't make this check first, so the code works (i.e. the data is fed into the pipeline, the model is fit, and then probabilities are calibrated appropriately). \r\n\r\nMy use case (which is not reflected in the example) is to use the initial stages of the pipeline to select columns from a dataframe, encode the categoricals, and then fit the model. I then pickle the fitted pipeline (after using GridSearchCV to select hyperparameters). Later on, I can load the model and use it to predict on new data, while abstracting away from what needs to be transformed in the raw data. I now want to calibrate the model after fitting it but ran into this problem. \r\n\r\n\r\n\r\nFor reference, here's all my system info: \r\n```\r\nLinux-3.10.0-514.2.2.el7.x86_64-x86_64-with-redhat-7.3-Maipo\r\nPython 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 12:22:00) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\r\nNumPy 1.12.0\r\nSciPy 0.18.1\r\nScikit-Learn 0.18.1\r\n```\r\n\r\nThanks for reading (and for all of your hard work on scikit-learn!). ",
  "issue_comments": [
    {
      "id": 292049025,
      "user": "amueller",
      "body": "Hm I get\r\n```\r\nValueError: Got X with X.ndim=1. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\r\n\r\n```\r\nbut I'm on master. Either way, I agree that we don't need to do input validation here, it should be in in the base estimator."
    },
    {
      "id": 292049400,
      "user": "stoddardg",
      "body": "I should have noted that in my actual use case, I get a different error\r\n```\r\nValueError: could not convert string to float:\r\n```\r\nwhich is caused by a column of strings in the DataFrame that I pass. In both cases, it is an error about the input data. "
    },
    {
      "id": 292054409,
      "user": "jnothman",
      "body": "I agree, we should not be validating X beyond the needs of safe_indexing or\nsimilar. PR fixing it welcome.\n\nOn 6 April 2017 at 12:12, Greg Stoddard <notifications@github.com> wrote:\n\n> I should have noted that in my actual use case, I get a different error\n>\n> ValueError: could not convert string to float:\n>\n> which is caused by a column of strings in the DataFrame that I pass.\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/8710#issuecomment-292049400>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz673y6pYFFDv8WKCfeBaEO2ie8DMOks5rtEnzgaJpZM4M1BdZ>\n> .\n>\n"
    },
    {
      "id": 292054776,
      "user": "amueller",
      "body": "FYI theoretically I would prefer ``make_pipeline(DictVectorizer(), CallibratedClassifierCV(RandomForestClassifier))`` because you don't need to cross-validate the DictVectorizer part (though you're prefitting anyhow I guess....\r\nBut that also doesn't work because #6451 (at least if the pipeline clones). I just recently realized that that will become a problem with pipeline in the future...."
    },
    {
      "id": 292056064,
      "user": "stoddardg",
      "body": "So this wasn't in my example above, but the pipeline gets put into GridSearchCV and then I want to calibrate the model chosen from GridSearchCV. How does GridSearchCV interact with CalibratedClassifierCV? \r\n\r\nFrom my limited understanding, CalibratedClassifierCV produces K models, where each model is constructed with a K-1 folds, and then averages predictions from each of the K models to make a prediction. This seems semantically different from standard CV where you select the model with the best performance over the K folds but then construct a single model using the entire training data. I'm not sure how to include the CalibrationCV into that procedure.\r\n\r\nThe thing that made sense to me with CalibratedClassifierCV was to prefit the model from GridSearchCV and then calibrate with a different set of data (as the per the docs recommendation).  "
    },
    {
      "id": 292059778,
      "user": "amueller",
      "body": "> How does GridSearchCV interact with CalibratedClassifierCV?\r\n\r\nWithout prefit it's fine, with prefit it won't work right now :-/\r\n\r\nYou could just nest the GridSearchCV with the cross-validation of the CalibratedClassifierCV, thought that would be a bit computationally expensive."
    },
    {
      "id": 299042057,
      "user": "baharian",
      "body": "I am also interested in knowing the status of interplay between `RandomizedSearchCV` (or `GridSearchCV`) and `CalibratedClassifierCV`. I currently do hyper-parameter optimization with `RandomizedSearchCV` (which by default fits the best model to the entire dataset it was given) and then calibrate this model with `CalibratedClassifierCV` using `cv = 'prefit'` argument."
    },
    {
      "id": 499113383,
      "user": "henningsway",
      "body": "I'm currently running into the same issue.\r\n\r\nIs this clearly an input-validation-issue, so that the encoding of categorical data won't be attempted before an error is thrown? Is there a way to disable this input-validation locally?\r\n\r\n(I am also not familiar with `_CalibratedClassifier` and how to use it)"
    },
    {
      "id": 499130802,
      "user": "amueller",
      "body": "Hm I'm a bit surprised by the outcome of #13077 but I guess using dicts wasn't anticipated. I only skimmed the discussion there but I feel like we should skip the input validation even more."
    },
    {
      "id": 501755741,
      "user": "henningsway",
      "body": "I currently use `_CalibratedClassifier` to calibrate a Pipeline-Model (optimised by `GridSearchCV`) on some extra validation data to workaround the too restrictive input validation of `CalibratedClassifierCV`.\r\n\r\nIs there anything I loose with this approach compared to `CalibratedClassifierCV` with `prefit=True`?\r\n\r\nIdeally, I would be able to calibrate my model in the Pipeline/Gridsearch already, but I have trouble to pass the parameters from the Pipeline-Gridsearch to the calibrated model.\r\n\r\nOthers seem to have similar problems: https://stackoverflow.com/a/49833102/1392529\r\nAny suggestions here?"
    },
    {
      "id": 503003987,
      "user": "jnothman",
      "body": "I think the stackoverflow you cite is mostly a misunderstanding of syntax.\nIf you want us to properly understand your issue here, please provide code\nto explain what you're doing / trying.\n"
    },
    {
      "id": 515646288,
      "user": "jsadloske",
      "body": "I think they are asking how to do this scenario.\r\n\r\n```\r\nfrom sklearn.datasets import make_moons\r\nfrom sklearn.impute import SimpleImputer\r\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.model_selection import RandomizedSearchCV\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\n\r\nX, y = make_moons()\r\n\r\nmy_pipeline = Pipeline([\r\n    ('imputer', SimpleImputer()),\r\n    ('model', CalibratedClassifierCV(RandomForestClassifier(n_estimators=10), cv=5))\r\n])\r\n\r\nparam_grid = {\r\n    'model__max_depth': list(range(2, 10))\r\n}\r\n\r\nsearch = RandomizedSearchCV(my_pipeline, param_grid, cv=3, n_iter=4, iid=False)\r\nsearch.fit(X, y)\r\n```\r\n\r\nHow do I modify `param_grid` so `max_depth` gets passed to `RandomForestClassifier`?\r\n\r\nThis is similar to how `VotingClassifier` works, but there the `estimators` parameter takes a tuple so you can give `RandomForestClassifier` a name and reference it in  `param_grid`.\r\n\r\n```\r\nrf = RandomForestClassifier(n_estimators=10)\r\n\r\nmy_pipeline = Pipeline([\r\n    ('imputer', SimpleImputer()),\r\n    ('model', VotingClassifier([('rf', rf)]))  # we can name it 'rf'\r\n])\r\n\r\nparam_grid = {\r\n    'model__rf__max_depth': list(range(2, 10))\r\n}\r\n\r\nsearch = RandomizedSearchCV(my_pipeline, param_grid, cv=3, n_iter=4, iid=False)\r\nsearch.fit(X, y)\r\nsearch.best_params_\r\n```\r\n"
    },
    {
      "id": 516165146,
      "user": "amueller",
      "body": "``model__estimator__max_depth`` is the answer. Where in the docs should we add this to be easy to find?"
    },
    {
      "id": 516639293,
      "user": "jsadloske",
      "body": "For anyone else reading this, I got it to work with `model__base_estimator__max_depth` as I guess the parameter is called base_estimator here. \r\n\r\nIt doesn't seem to work with `fit_params` though. \r\n```\r\nfit_params = {\r\n    'model__base_estimator__sample_weight': np.random.random(size=X.shape[0])\r\n}\r\n\r\nsearch = RandomizedSearchCV(my_pipeline, param_grid, cv=3, n_iter=4, iid=False)\r\nsearch.fit(X, y, **fit_params)\r\n```\r\n\r\n`TypeError: fit() got an unexpected keyword argument 'base_estimator__sample_weight`\r\n\r\nI guess it would go in this section  [https://scikit-learn.org/stable/modules/compose.html#nested-parameters](https://scikit-learn.org/stable/modules/compose.html#nested-parameters)\r\n\r\n"
    },
    {
      "id": 517419702,
      "user": "amueller",
      "body": "The ``CallibratedClassifierCV`` doesn't have ``fit_params`` but has ``sample_weight`` and passes it through correctly, so\r\n```python\r\nfit_params = {\r\n    'model__sample_weight': np.random.random(size=X.shape[0])\r\n}\r\n\r\nsearch = RandomizedSearchCV(my_pipeline, param_grid, cv=3, n_iter=4, iid=False)\r\nsearch.fit(X, y, **fit_params)\r\n```\r\nworks.\r\nYes, that's inconsistent, and we're looking for a fix.\r\nIt's a bit tricky to figure out where in the process you want to apply sample weights and where not, see #4497.\r\nBasically, you could use the sample weights for fitting both the forest and the calibration, for just the calibration, or for just the forest. And we haven't decided on a syntax to choose between the options.\r\nRight now, ``CalibratedClassifierCV`` only allows you to do both and not separately. What we do with the parameters is slightly different because they only ever apply to a single estimator. But you might have a long pipeline and use sample_weights in each step, but you (maybe) don't want to specify ``fit_params`` for each step in the pipeline.\r\nAlso see https://github.com/scikit-learn/enhancement_proposals/pull/16\r\n\r\n"
    },
    {
      "id": 517420890,
      "user": "amueller",
      "body": "Hm actually I think it should go into the GridSearchCV documentation. You pointed towards the ``pipeline`` documentation, but it's actually somewhat unrelated to pipeline.\r\nOne could also argue it's unrelated to ``GridSearchCV`` and just concerns ``set_params`` but I think ``GridSearchCV`` is where it's most likely to matter."
    },
    {
      "id": 517421389,
      "user": "amueller",
      "body": "I'm pretty sure this issue now contains (at least) three completely separate issues and we should probably separate them."
    },
    {
      "id": 517429324,
      "user": "amueller",
      "body": "@jsadloske I tried to add an example here based on your use-case: #14548\r\n\r\nThe inconsistencies between ``fit_params`` and ``set_params`` will probably take a bit longer to fix."
    },
    {
      "id": 596622944,
      "user": "ODemidenko",
      "body": "I found another case where current input validation in the `CalibratedClassifier`  class does a bad service:\r\nI use it to calibrate XGBoost model. Through its scikit-learn api. And while usually XGBoost works fine, but here is a problem, that I trained it on pandas.DF, and xgboost, by default, expects for the scoring pandas.DF with the same column names. \r\nI do provide to the CalibratedClassifier.fit() dataframe with the same columns, but its data validation at some point extracts only numpy array from the DF and passes it to xgboost.\r\nAnd I am unable to switch-off xgboost validation, as it is set by a param provided to the .predict_proba() method.\r\n\r\nLuckily, `_CalibratedClassifier` workaround works here as well."
    },
    {
      "id": 617744817,
      "user": "Rosevear",
      "body": "I'm getting the same validation error as the original post when I try to use a pipeline (which contains the categorical encoding step) as the base estimator. This is on version 0.22.1\r\n\r\nTraceback (most recent call last):\r\n  File \"exp.py\", line 514, in <module>\r\n    cur_pipe_calibrator.fit(X_calibration_train, y_calibration_train)\r\n  File \"C:\\Users\\cr89536\\.conda\\envs\\VEE\\lib\\site-packages\\sklearn\\calibration.py\", line 135, in fit\r\n    force_all_finite=False, allow_nd=True)\r\n  File \"C:\\Users\\cr89536\\.conda\\envs\\VEE\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 755, in check_X_y\r\n    estimator=estimator)\r\n  File \"C:\\Users\\cr89536\\.conda\\envs\\VEE\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 531, in check_array\r\n    array = np.asarray(array, order=order, dtype=dtype)\r\n  File \"C:\\Users\\cr89536\\.conda\\envs\\VEE\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 85, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\nValueError: could not convert string to float: 'COMM'"
    },
    {
      "id": 750871036,
      "user": "ftrojan",
      "body": "I tried the code example in the original post on my Mac with Python 3.7.3, Numpy 1.16.4 and Scikit-Learn 0.24.0 and everything runs smoothly without any error. I would propose to either post a new reproducible example or close this issue."
    },
    {
      "id": 766331930,
      "user": "odedbd",
      "body": "@ftrojan - I ran into a similar issue with my own code, which uses CountVectorizer as part of a pipeline to classify text data. Adding the CalibratedClassiferCV as part of a pipeline fit with GridSearchCV (not using cv='prefit'), my code gets an error due to data validation in CalibratedClassfierCV (self._validate_data). I verified that commenting out the data validation removes the error. \r\n\r\nI cannot share my code (due to company policy and also it's quite involved with other complications). I have modified the OP sample to use cross validation rather than 'prefit'. If you run the code below you will get the exception from validate_data as I do in my own code. I tried running this code with the validate_data line commented out, but then I get a \"TypeError: only integer scalar arrays can be converted to a scalar index\" error which I wasn't able to overcome.\r\n\r\n```python\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.feature_extraction import DictVectorizer\r\nfrom sklearn.calibration import CalibratedClassifierCV, _CalibratedClassifier\r\nfrom sklearn.pipeline import Pipeline\r\n\r\nfake_features = [\r\n    {'state':'NY','age':'adult'},\r\n    {'state':'TX','age':'adult'},\r\n    {'state':'VT','age':'child'},\r\n    {'state':'NY','age':'adult'},\r\n    {'state':'TX','age':'adult'},\r\n    {'state':'VT','age':'child'}\r\n]\r\n\r\nlabels = [1,0,1,1,0,1]\r\n\r\npipeline = Pipeline([\r\n            ('vectorizer',DictVectorizer()),\r\n            ('clf',RandomForestClassifier())\r\n    ])\r\n\r\n# pipeline.fit(fake_features, labels)\r\n\r\nclf_isotonic = CalibratedClassifierCV(base_estimator=pipeline, cv=2, method='isotonic')\r\nclf_isotonic.fit(fake_features, labels)\r\n```"
    },
    {
      "id": 767451162,
      "user": "odedbd",
      "body": "Also, while the fit works, running prediction fails, due to a similar check in predict_proba. Removing the check does not work for this use case, because the code of predict_proba assumes that X has a .shape property, which obviously fails for this list of dicts. For my own use case I was able to hack my way around this, by overloading predict_proba, since my X input is a pandas DataFrame which fortunately has the .shape property"
    },
    {
      "id": 786169688,
      "user": "ogrisel",
      "body": "@odedbd the prediction should be fixed in #19555 and your minimal reproducer passes on this branch. Let me know if that's enough for you."
    },
    {
      "id": 795212945,
      "user": "ogrisel",
      "body": "Fixed by #19641."
    },
    {
      "id": 796556834,
      "user": "odedbd",
      "body": "@ogrisel Thanks, if the reproducer code works it should be fine. The merged changes are on the scikit-learn:main branch, right? Until now I only worked off the released versions, should I be able to pip install directly from the github-repo in order to test this on my own code, or do I need to clone the repo and build? I apologize if this is not the right place to ask this."
    },
    {
      "id": 815535902,
      "user": "ogrisel",
      "body": "@odedbd sorry I had not seen your reply. You can test that the fix solve your problem using the nightly build if you do not want to build from source with your own compilers:\r\n\r\nhttps://scikit-learn.org/stable/developers/advanced_installation.html"
    }
  ],
  "text_context": "# CalibratedClassifierCV doesn't interact properly with Pipeline estimators \n\nHi, \r\n\r\nI'm trying to use CalibratedClassifierCV to calibrate the probabilities from a Gradient Boosted Tree model. The GBM is wrapped in a Pipeline estimator, where the initial stages of the Pipeline convert categoricals (using DictVectorizer) prior to the GBM being fit. The issue is that when I try to similarly use CalibratedClassifierCV, with a prefit estimator, it fails when I pass in the data. Here's a small example: \r\n\r\n```py\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.feature_extraction import DictVectorizer\r\nfrom sklearn.calibration import CalibratedClassifierCV, _CalibratedClassifier\r\nfrom sklearn.pipeline import Pipeline\r\n\r\nfake_features = [\r\n    {'state':'NY','age':'adult'},\r\n    {'state':'TX','age':'adult'},\r\n    {'state':'VT','age':'child'}\r\n]\r\n\r\nlabels = [1,0,1]\r\n\r\npipeline = Pipeline([\r\n            ('vectorizer',DictVectorizer()),\r\n            ('clf',RandomForestClassifier())\r\n    ])\r\n\r\npipeline.fit(fake_features, labels)\r\n\r\nclf_isotonic = CalibratedClassifierCV(base_estimator=pipeline, cv='prefit', method='isotonic')\r\nclf_isotonic.fit(fake_features, labels)\r\n\r\n```\r\n\r\nWhen running that, I get the following error on the last line:\r\n```\r\nTypeError: float() argument must be a string or a number, not 'dict'\r\n```\r\n\r\n\r\nOn the other hand, if I replace the last two lines with the following, things work fine: \r\n```\r\nclf_isotonic = _CalibratedClassifier(base_estimator=pipeline, method='isotonic')\r\nclf_isotonic.fit(fake_features, labels)\r\n```\r\n\r\n\r\nIt seems that CalibratedClassifierCV checks to see if the X data is valid prior to invoking anything about the base estimator (https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/calibration.py#L126). In my case, this logic seems slightly off since I'm using the pipeline to convert the data into the proper form before feeding it into estimator. \r\n\r\nOn the other hand, _CalibratedClassifier doesn't make this check first, so the code works (i.e. the data is fed into the pipeline, the model is fit, and then probabilities are calibrated appropriately). \r\n\r\nMy use case (which is not reflected in the example) is to use the initial stages of the pipeline to select columns from a dataframe, encode the categoricals, and then fit the model. I then pickle the fitted pipeline (after using GridSearchCV to select hyperparameters). Later on, I can load the model and use it to predict on new data, while abstracting away from what needs to be transformed in the raw data. I now want to calibrate the model after fitting it but ran into this problem. \r\n\r\n\r\n\r\nFor reference, here's all my system info: \r\n```\r\nLinux-3.10.0-514.2.2.el7.x86_64-x86_64-with-redhat-7.3-Maipo\r\nPython 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 12:22:00) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\r\nNumPy 1.12.0\r\nSciPy 0.18.1\r\nScikit-Learn 0.18.1\r\n```\r\n\r\nThanks for reading (and for all of your hard work on scikit-learn!). \n\nHm I get\r\n```\r\nValueError: Got X with X.ndim=1. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\r\n\r\n```\r\nbut I'm on master. Either way, I agree that we don't need to do input validation here, it should be in in the base estimator.\n\nI should have noted that in my actual use case, I get a different error\r\n```\r\nValueError: could not convert string to float:\r\n```\r\nwhich is caused by a column of strings in the DataFrame that I pass. In both cases, it is an error about the input data. \n\nI agree, we should not be validating X beyond the needs of safe_indexing or\nsimilar. PR fixing it welcome.\n\nOn 6 April 2017 at 12:12, Greg Stoddard <notifications@github.com> wrote:\n\n> I should have noted that in my actual use case, I get a different error\n>\n> ValueError: could not convert string to float:\n>\n> which is caused by a column of strings in the DataFrame that I pass.\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/8710#issuecomment-292049400>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz673y6pYFFDv8WKCfeBaEO2ie8DMOks5rtEnzgaJpZM4M1BdZ>\n> .\n>\n\n\nFYI theoretically I would prefer ``make_pipeline(DictVectorizer(), CallibratedClassifierCV(RandomForestClassifier))`` because you don't need to cross-validate the DictVectorizer part (though you're prefitting anyhow I guess....\r\nBut that also doesn't work because #6451 (at least if the pipeline clones). I just recently realized that that will become a problem with pipeline in the future....\n\nSo this wasn't in my example above, but the pipeline gets put into GridSearchCV and then I want to calibrate the model chosen from GridSearchCV. How does GridSearchCV interact with CalibratedClassifierCV? \r\n\r\nFrom my limited understanding, CalibratedClassifierCV produces K models, where each model is constructed with a K-1 folds, and then averages predictions from each of the K models to make a prediction. This seems semantically different from standard CV where you select the model with the best performance over the K folds but then construct a single model using the entire training data. I'm not sure how to include the CalibrationCV into that procedure.\r\n\r\nThe thing that made sense to me with CalibratedClassifierCV was to prefit the model from GridSearchCV and then calibrate with a different set of data (as the per the docs recommendation).  \n\n> How does GridSearchCV interact with CalibratedClassifierCV?\r\n\r\nWithout prefit it's fine, with prefit it won't work right now :-/\r\n\r\nYou could just nest the GridSearchCV with the cross-validation of the CalibratedClassifierCV, thought that would be a bit computationally expensive.\n\nI am also interested in knowing the status of interplay between `RandomizedSearchCV` (or `GridSearchCV`) and `CalibratedClassifierCV`. I currently do hyper-parameter optimization with `RandomizedSearchCV` (which by default fits the best model to the entire dataset it was given) and then calibrate this model with `CalibratedClassifierCV` using `cv = 'prefit'` argument.\n\nI'm currently running into the same issue.\r\n\r\nIs this clearly an input-validation-issue, so that the encoding of categorical data won't be attempted before an error is thrown? Is there a way to disable this input-validation locally?\r\n\r\n(I am also not familiar with `_CalibratedClassifier` and how to use it)\n\nHm I'm a bit surprised by the outcome of #13077 but I guess using dicts wasn't anticipated. I only skimmed the discussion there but I feel like we should skip the input validation even more.\n\nI currently use `_CalibratedClassifier` to calibrate a Pipeline-Model (optimised by `GridSearchCV`) on some extra validation data to workaround the too restrictive input validation of `CalibratedClassifierCV`.\r\n\r\nIs there anything I loose with this approach compared to `CalibratedClassifierCV` with `prefit=True`?\r\n\r\nIdeally, I would be able to calibrate my model in the Pipeline/Gridsearch already, but I have trouble to pass the parameters from the Pipeline-Gridsearch to the calibrated model.\r\n\r\nOthers seem to have similar problems: https://stackoverflow.com/a/49833102/1392529\r\nAny suggestions here?\n\nI think the stackoverflow you cite is mostly a misunderstanding of syntax.\nIf you want us to properly understand your issue here, please provide code\nto explain what you're doing / trying.\n\n\nI think they are asking how to do this scenario.\r\n\r\n```\r\nfrom sklearn.datasets import make_moons\r\nfrom sklearn.impute import SimpleImputer\r\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.model_selection import RandomizedSearchCV\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\n\r\nX, y = make_moons()\r\n\r\nmy_pipeline = Pipeline([\r\n    ('imputer', SimpleImputer()),\r\n    ('model', CalibratedClassifierCV(RandomForestClassifier(n_estimators=10), cv=5))\r\n])\r\n\r\nparam_grid = {\r\n    'model__max_depth': list(range(2, 10))\r\n}\r\n\r\nsearch = RandomizedSearchCV(my_pipeline, param_grid, cv=3, n_iter=4, iid=False)\r\nsearch.fit(X, y)\r\n```\r\n\r\nHow do I modify `param_grid` so `max_depth` gets passed to `RandomForestClassifier`?\r\n\r\nThis is similar to how `VotingClassifier` works, but there the `estimators` parameter takes a tuple so you can give `RandomForestClassifier` a name and reference it in  `param_grid`.\r\n\r\n```\r\nrf = RandomForestClassifier(n_estimators=10)\r\n\r\nmy_pipeline = Pipeline([\r\n    ('imputer', SimpleImputer()),\r\n    ('model', VotingClassifier([('rf', rf)]))  # we can name it 'rf'\r\n])\r\n\r\nparam_grid = {\r\n    'model__rf__max_depth': list(range(2, 10))\r\n}\r\n\r\nsearch = RandomizedSearchCV(my_pipeline, param_grid, cv=3, n_iter=4, iid=False)\r\nsearch.fit(X, y)\r\nsearch.best_params_\r\n```\r\n\n\n``model__estimator__max_depth`` is the answer. Where in the docs should we add this to be easy to find?\n\nFor anyone else reading this, I got it to work with `model__base_estimator__max_depth` as I guess the parameter is called base_estimator here. \r\n\r\nIt doesn't seem to work with `fit_params` though. \r\n```\r\nfit_params = {\r\n    'model__base_estimator__sample_weight': np.random.random(size=X.shape[0])\r\n}\r\n\r\nsearch = RandomizedSearchCV(my_pipeline, param_grid, cv=3, n_iter=4, iid=False)\r\nsearch.fit(X, y, **fit_params)\r\n```\r\n\r\n`TypeError: fit() got an unexpected keyword argument 'base_estimator__sample_weight`\r\n\r\nI guess it would go in this section  [https://scikit-learn.org/stable/modules/compose.html#nested-parameters](https://scikit-learn.org/stable/modules/compose.html#nested-parameters)\r\n\r\n\n\nThe ``CallibratedClassifierCV`` doesn't have ``fit_params`` but has ``sample_weight`` and passes it through correctly, so\r\n```python\r\nfit_params = {\r\n    'model__sample_weight': np.random.random(size=X.shape[0])\r\n}\r\n\r\nsearch = RandomizedSearchCV(my_pipeline, param_grid, cv=3, n_iter=4, iid=False)\r\nsearch.fit(X, y, **fit_params)\r\n```\r\nworks.\r\nYes, that's inconsistent, and we're looking for a fix.\r\nIt's a bit tricky to figure out where in the process you want to apply sample weights and where not, see #4497.\r\nBasically, you could use the sample weights for fitting both the forest and the calibration, for just the calibration, or for just the forest. And we haven't decided on a syntax to choose between the options.\r\nRight now, ``CalibratedClassifierCV`` only allows you to do both and not separately. What we do with the parameters is slightly different because they only ever apply to a single estimator. But you might have a long pipeline and use sample_weights in each step, but you (maybe) don't want to specify ``fit_params`` for each step in the pipeline.\r\nAlso see https://github.com/scikit-learn/enhancement_proposals/pull/16\r\n\r\n\n\nHm actually I think it should go into the GridSearchCV documentation. You pointed towards the ``pipeline`` documentation, but it's actually somewhat unrelated to pipeline.\r\nOne could also argue it's unrelated to ``GridSearchCV`` and just concerns ``set_params`` but I think ``GridSearchCV`` is where it's most likely to matter.\n\nI'm pretty sure this issue now contains (at least) three completely separate issues and we should probably separate them.\n\n@jsadloske I tried to add an example here based on your use-case: #14548\r\n\r\nThe inconsistencies between ``fit_params`` and ``set_params`` will probably take a bit longer to fix.\n\nI found another case where current input validation in the `CalibratedClassifier`  class does a bad service:\r\nI use it to calibrate XGBoost model. Through its scikit-learn api. And while usually XGBoost works fine, but here is a problem, that I trained it on pandas.DF, and xgboost, by default, expects for the scoring pandas.DF with the same column names. \r\nI do provide to the CalibratedClassifier.fit() dataframe with the same columns, but its data validation at some point extracts only numpy array from the DF and passes it to xgboost.\r\nAnd I am unable to switch-off xgboost validation, as it is set by a param provided to the .predict_proba() method.\r\n\r\nLuckily, `_CalibratedClassifier` workaround works here as well.\n\nI'm getting the same validation error as the original post when I try to use a pipeline (which contains the categorical encoding step) as the base estimator. This is on version 0.22.1\r\n\r\nTraceback (most recent call last):\r\n  File \"exp.py\", line 514, in <module>\r\n    cur_pipe_calibrator.fit(X_calibration_train, y_calibration_train)\r\n  File \"C:\\Users\\cr89536\\.conda\\envs\\VEE\\lib\\site-packages\\sklearn\\calibration.py\", line 135, in fit\r\n    force_all_finite=False, allow_nd=True)\r\n  File \"C:\\Users\\cr89536\\.conda\\envs\\VEE\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 755, in check_X_y\r\n    estimator=estimator)\r\n  File \"C:\\Users\\cr89536\\.conda\\envs\\VEE\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 531, in check_array\r\n    array = np.asarray(array, order=order, dtype=dtype)\r\n  File \"C:\\Users\\cr89536\\.conda\\envs\\VEE\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 85, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\nValueError: could not convert string to float: 'COMM'\n\nI tried the code example in the original post on my Mac with Python 3.7.3, Numpy 1.16.4 and Scikit-Learn 0.24.0 and everything runs smoothly without any error. I would propose to either post a new reproducible example or close this issue.\n\n@ftrojan - I ran into a similar issue with my own code, which uses CountVectorizer as part of a pipeline to classify text data. Adding the CalibratedClassiferCV as part of a pipeline fit with GridSearchCV (not using cv='prefit'), my code gets an error due to data validation in CalibratedClassfierCV (self._validate_data). I verified that commenting out the data validation removes the error. \r\n\r\nI cannot share my code (due to company policy and also it's quite involved with other complications). I have modified the OP sample to use cross validation rather than 'prefit'. If you run the code below you will get the exception from validate_data as I do in my own code. I tried running this code with the validate_data line commented out, but then I get a \"TypeError: only integer scalar arrays can be converted to a scalar index\" error which I wasn't able to overcome.\r\n\r\n```python\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.feature_extraction import DictVectorizer\r\nfrom sklearn.calibration import CalibratedClassifierCV, _CalibratedClassifier\r\nfrom sklearn.pipeline import Pipeline\r\n\r\nfake_features = [\r\n    {'state':'NY','age':'adult'},\r\n    {'state':'TX','age':'adult'},\r\n    {'state':'VT','age':'child'},\r\n    {'state':'NY','age':'adult'},\r\n    {'state':'TX','age':'adult'},\r\n    {'state':'VT','age':'child'}\r\n]\r\n\r\nlabels = [1,0,1,1,0,1]\r\n\r\npipeline = Pipeline([\r\n            ('vectorizer',DictVectorizer()),\r\n            ('clf',RandomForestClassifier())\r\n    ])\r\n\r\n# pipeline.fit(fake_features, labels)\r\n\r\nclf_isotonic = CalibratedClassifierCV(base_estimator=pipeline, cv=2, method='isotonic')\r\nclf_isotonic.fit(fake_features, labels)\r\n```\n\nAlso, while the fit works, running prediction fails, due to a similar check in predict_proba. Removing the check does not work for this use case, because the code of predict_proba assumes that X has a .shape property, which obviously fails for this list of dicts. For my own use case I was able to hack my way around this, by overloading predict_proba, since my X input is a pandas DataFrame which fortunately has the .shape property\n\n@odedbd the prediction should be fixed in #19555 and your minimal reproducer passes on this branch. Let me know if that's enough for you.\n\nFixed by #19641.\n\n@ogrisel Thanks, if the reproducer code works it should be fine. The merged changes are on the scikit-learn:main branch, right? Until now I only worked off the released versions, should I be able to pip install directly from the github-repo in order to test this on my own code, or do I need to clone the repo and build? I apologize if this is not the right place to ask this.\n\n@odedbd sorry I had not seen your reply. You can test that the fix solve your problem using the nightly build if you do not want to build from source with your own compilers:\r\n\r\nhttps://scikit-learn.org/stable/developers/advanced_installation.html",
  "pr_link": "https://github.com/scikit-learn/enhancement_proposals/pull/16",
  "code_context": [
    {
      "filename": "conf.py",
      "content": "# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath('.'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = 'Scikit-learn enhancement proposals'\ncopyright = '2018, scikit-learn community'\nauthor = 'scikit-learn community'\n\n# The short X.Y version\nversion = ''\n# The full version, including alpha/beta/rc tags\nrelease = ''\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.mathjax',\n    'sphinx.ext.viewcode',\n    'sphinx_issues',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = ['.rst', '.md']\nsource_suffix = '.rst'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = []\n\ndefault_role = 'any'\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'sphinx_rtd_theme'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don't match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',\n# 'searchbox.html']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'Scikit-learnenhancementproposalsdoc'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n\n    # Latex figure (float) alignment\n    #\n    # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, 'Scikit-learnenhancementproposals.tex', 'Scikit-learn enhancement proposals Documentation',\n     'scikit-learn community', 'manual'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, 'scikit-learnenhancementproposals', 'Scikit-learn enhancement proposals Documentation',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, 'Scikit-learnenhancementproposals', 'Scikit-learn enhancement proposals Documentation',\n     author, 'Scikit-learnenhancementproposals', 'One line description of project.',\n     'Miscellaneous'),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for intersphinx extension ---------------------------------------\n\nintersphinx_mapping = {'sklearn': ('http://scikit-learn.org/stable', None)}\n\n# -- Sphinx-Issues configuration --\n\nissues_github_path = \"scikit-learn/scikit-learn\"\n"
    },
    {
      "filename": "slep006/cases_opt0a.py",
      "content": "from defs import (accuracy, group_cv, make_scorer, SelectKBest,\n                  LogisticRegressionCV, cross_validate,\n                  make_pipeline, X, y, my_groups, my_weights,\n                  my_other_weights)\n\n# TODO\n"
    },
    {
      "filename": "slep006/cases_opt0b.py",
      "content": "import pandas as pd\nfrom defs import (accuracy, group_cv, make_scorer, SelectKBest,\n                  LogisticRegressionCV, cross_validate,\n                  make_pipeline, X, y, my_groups, my_weights,\n                  my_other_weights)\n\n# TODO\n"
    },
    {
      "filename": "slep006/cases_opt1.py",
      "content": "from defs import (accuracy, group_cv, make_scorer, SelectKBest,\n                  LogisticRegressionCV, cross_validate, make_pipeline, X, y,\n                  my_groups, my_weights, my_other_weights)\n\n# %%\n# Case A: weighted scoring and fitting\n\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring='accuracy',\n)\ncross_validate(lr, X, y, cv=group_cv,\n               props={'sample_weight': my_weights, 'groups': my_groups},\n               scoring='accuracy')\n\n# Error handling: if props={'sample_eight': my_weights, ...} was passed\n# instead, the estimator would fit and score without weight, silently failing.\n\n# %%\n# Case B: weighted scoring and unweighted fitting\n\n\nclass MyLogisticRegressionCV(LogisticRegressionCV):\n    def fit(self, X, y, props=None):\n        props = props.copy()\n        props.pop('sample_weight', None)\n        super().fit(X, y, props=props)\n\n\n# %%\n# Case C: unweighted feature selection\n\n# Currently feature selection does not handle sample_weight, and as long as\n# that remains the case, it will simply ignore the prop passed to it. Hence:\n\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring='accuracy',\n)\nsel = SelectKBest()\npipe = make_pipeline(sel, lr)\ncross_validate(pipe, X, y, cv=group_cv,\n               props={'sample_weight': my_weights, 'groups': my_groups},\n               scoring='accuracy')\n\n# %%\n# Case D: different scoring and fitting weights\n\nweighted_acc = make_scorer(accuracy)\n\n\ndef specially_weighted_acc(est, X, y, props):\n    props = props.copy()\n    props['sample_weight'] = 'scoring_weight'\n    return weighted_acc(est, X, y, props)\n\n\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring=specially_weighted_acc,\n)\ncross_validate(lr, X, y, cv=group_cv,\n               props={\n                    'scoring_weight': my_weights,\n                    'sample_weight': my_other_weights,\n                    'groups': my_groups,\n               },\n               scoring=specially_weighted_acc)\n"
    },
    {
      "filename": "slep006/cases_opt2.py",
      "content": "from defs import (group_cv, SelectKBest, LogisticRegressionCV,\n                  cross_validate, make_pipeline, X, y, my_groups,\n                  my_weights, my_other_weights)\n\n# %%\n# Case A: weighted scoring and fitting\n\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring='accuracy',\n)\nprops = {'cv__groups': my_groups,\n         'estimator__cv__groups': my_groups,\n         'estimator__sample_weight': my_weights,\n         'scoring__sample_weight': my_weights,\n         'estimator__scoring__sample_weight': my_weights}\ncross_validate(lr, X, y, cv=group_cv,\n               props=props,\n               scoring='accuracy')\n\n# error handling: if props={'estimator__sample_eight': my_weights, ...} was\n# passed instead, the estimator would raise an error.\n\n# %%\n# Case B: weighted scoring and unweighted fitting\n\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring='accuracy',\n)\nprops = {'cv__groups': my_groups,\n         'estimator__cv__groups': my_groups,\n         'scoring__sample_weight': my_weights,\n         'estimator__scoring__sample_weight': my_weights}\ncross_validate(lr, X, y, cv=group_cv,\n               props=props,\n               scoring='accuracy')\n\n# %%\n# Case C: unweighted feature selection\n\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring='accuracy',\n)\npipe = make_pipeline(SelectKBest(), lr)\nprops = {'cv__groups': my_groups,\n         'estimator__logisticregressioncv__cv__groups': my_groups,\n         'estimator__logisticregressioncv__sample_weight': my_weights,\n         'scoring__sample_weight': my_weights,\n         'estimator__scoring__sample_weight': my_weights}\ncross_validate(pipe, X, y, cv=group_cv,\n               props=props,\n               scoring='accuracy')\n\n# %%\n# Case D: different scoring and fitting weights\n\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring='accuracy',\n)\nprops = {'cv__groups': my_groups,\n         'estimator__cv__groups': my_groups,\n         'estimator__sample_weight': my_other_weights,\n         'scoring__sample_weight': my_weights,\n         'estimator__scoring__sample_weight': my_weights}\ncross_validate(lr, X, y, cv=group_cv,\n               props=props,\n               scoring='accuracy')\n"
    },
    {
      "filename": "slep006/cases_opt3.py",
      "content": "from defs import (accuracy, make_scorer, SelectKBest, LogisticRegressionCV,\n                  group_cv, cross_validate, make_pipeline, X, y, my_groups,\n                  my_weights, my_other_weights)\n\n# %%\n# Case A: weighted scoring and fitting\n\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring='accuracy',\n    prop_routing={'cv': ['groups'],\n                  'scoring': ['sample_weight'],\n                  }\n    # one question here is whether we need to explicitly route sample_weight\n    # to LogisticRegressionCV's fitting...\n)\n\n# Alternative syntax, which assumes cv receives 'groups' by default, and that a\n# method-based API is provided on meta-estimators:\n#   lr = LogisticRegressionCV(\n#       cv=group_cv,\n#       scoring='accuracy',\n#   ).add_prop_route(scoring='sample_weight')\n\ncross_validate(lr, X, y, cv=group_cv,\n               props={'sample_weight': my_weights, 'groups': my_groups},\n               scoring='accuracy',\n               prop_routing={'estimator': '*',  # pass all props\n                             'cv': ['groups'],\n                             'scoring': ['sample_weight'],\n                             })\n\n# Error handling: if props={'sample_eight': my_weights, ...} was passed\n# instead, LogisticRegressionCV would have to identify that a key was passed\n# that could not be routed nor used, in order to raise an error.\n\n# %%\n# Case B: weighted scoring and unweighted fitting\n\n# Here we rename the sample_weight prop so that we can specify that it only\n# applies to scoring.\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring='accuracy',\n    prop_routing={'cv': ['groups'],\n                  # read the following as \"scoring should consume\n                  # 'scoring_weight' as if it were 'sample_weight'.\"\n                  'scoring': {'sample_weight': 'scoring_weight'},\n                  },\n)\ncross_validate(lr, X, y, cv=group_cv,\n               props={'scoring_weight': my_weights, 'groups': my_groups},\n               scoring='accuracy',\n               prop_routing={'estimator': '*',\n                             'cv': ['groups'],\n                             'scoring': {'sample_weight': 'scoring_weight'},\n                             })\n\n# %%\n# Case C: unweighted feature selection\n\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring='accuracy',\n    prop_routing={'cv': ['groups'],\n                  'scoring': ['sample_weight'],\n                  })\npipe = make_pipeline(SelectKBest(), lr,\n                     prop_routing={'logisticregressioncv': ['sample_weight',\n                                                            'groups']})\ncross_validate(lr, X, y, cv=group_cv,\n               props={'sample_weight': my_weights, 'groups': my_groups},\n               scoring='accuracy',\n               prop_routing={'estimator': '*',\n                             'cv': ['groups'],\n                             'scoring': ['sample_weight'],\n                             })\n\n# %%\n# Case D: different scoring and fitting weights\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring='accuracy',\n    prop_routing={'cv': ['groups'],\n                  # read the following as \"scoring should consume\n                  # 'scoring_weight' as if it were 'sample_weight'.\"\n                  'scoring': {'sample_weight': 'scoring_weight'},\n                  },\n)\ncross_validate(lr, X, y, cv=group_cv,\n               props={'scoring_weight': my_weights, 'groups': my_groups,\n                      'fitting_weight': my_other_weights},\n               scoring='accuracy',\n               prop_routing={'estimator': {'sample_weight': 'fitting_weight',\n                                           'scoring_weight': 'scoring_weight',\n                                           'groups': 'groups'},\n                             'cv': ['groups'],\n                             'scoring': {'sample_weight': 'scoring_weight'},\n                             })\n"
    },
    {
      "filename": "slep006/cases_opt4.py",
      "content": "from defs import (accuracy, group_cv, make_scorer, SelectKBest,\n                  LogisticRegressionCV, cross_validate,\n                  make_pipeline, X, y, my_groups, my_weights,\n                  my_other_weights)\n\n# %%\n# Case A: weighted scoring and fitting\n\n# Here we presume that GroupKFold requests `groups` by default.\n# We need to explicitly request weights in make_scorer and for\n# LogisticRegressionCV. Both of these consumers understand the meaning\n# of the key \"sample_weight\".\n\nweighted_acc = make_scorer(accuracy, request_props=['sample_weight'])\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring=weighted_acc,\n).set_props_request(['sample_weight'])\ncross_validate(lr, X, y, cv=group_cv,\n               props={'sample_weight': my_weights, 'groups': my_groups},\n               scoring=weighted_acc)\n\n# Error handling: if props={'sample_eight': my_weights, ...} was passed,\n# cross_validate would raise an error, since 'sample_eight' was not requested\n# by any of its children.\n\n# %%\n# Case B: weighted scoring and unweighted fitting\n\n# Since LogisticRegressionCV requires that weights explicitly be requested,\n# removing that request means the fitting is unweighted.\n\nweighted_acc = make_scorer(accuracy, request_props=['sample_weight'])\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring=weighted_acc,\n)\ncross_validate(lr, X, y, cv=group_cv,\n               props={'sample_weight': my_weights, 'groups': my_groups},\n               scoring=weighted_acc)\n\n# %%\n# Case C: unweighted feature selection\n\n# Like LogisticRegressionCV, SelectKBest needs to request weights explicitly.\n# Here it does not request them.\n\nweighted_acc = make_scorer(accuracy, request_props=['sample_weight'])\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring=weighted_acc,\n).set_props_request(['sample_weight'])\nsel = SelectKBest()\npipe = make_pipeline(sel, lr)\ncross_validate(pipe, X, y, cv=group_cv,\n               props={'sample_weight': my_weights, 'groups': my_groups},\n               scoring=weighted_acc)\n\n# %%\n# Case D: different scoring and fitting weights\n\n# Despite make_scorer and LogisticRegressionCV both expecting a key\n# sample_weight, we can use aliases to pass different weights to different\n# consumers.\n\nweighted_acc = make_scorer(accuracy,\n                           request_props={'scoring_weight': 'sample_weight'})\nlr = LogisticRegressionCV(\n    cv=group_cv,\n    scoring=weighted_acc,\n).set_props_request({'fitting_weight': \"sample_weight\"})\ncross_validate(lr, X, y, cv=group_cv,\n               props={\n                    'scoring_weight': my_weights,\n                    'fitting_weight': my_other_weights,\n                    'groups': my_groups,\n               },\n               scoring=weighted_acc)\n"
    },
    {
      "filename": "slep006/defs.py",
      "content": "import numpy as np\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import accuracy\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import GroupKFold, cross_validate\nfrom sklearn.pipeline import make_pipeline\n\nN, M = 100, 4\nX = np.random.rand(N, M)\ny = np.random.randint(0, 1, size=N)\nmy_groups = np.random.randint(0, 10, size=N)\nmy_weights = np.random.rand(N)\nmy_other_weights = np.random.rand(N)\n"
    }
  ]
}