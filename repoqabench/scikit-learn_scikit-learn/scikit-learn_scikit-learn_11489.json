{
  "repo_name": "scikit-learn_scikit-learn",
  "issue_id": "11489",
  "issue_description": "# KBinsDiscretizer : Support inverse_transform for encode other than ordinal\n\nCurrently, we only support ``encode='ordinal'`` in ``inverse_transform``. Since we've supported ``inverse_transform`` in ``OnehotEncoder``, it's natural to support ``inverse_transform`` for ``encode='onehot'`` and ``encode='onehot-dense'``. Store the fitted encoder will be a reasonable solution here. (See https://github.com/scikit-learn/scikit-learn/pull/11467#issuecomment-403995635)",
  "issue_comments": [
    {
      "id": 404442571,
      "user": "ggc87",
      "body": "Hi, can I try this? "
    },
    {
      "id": 404449108,
      "user": "qinhanmin2014",
      "body": "welcome to contribute :)"
    },
    {
      "id": 404667945,
      "user": "ggc87",
      "body": "I'm having a small problem with the test  test_non_meta_estimators. It fail an assertion due to the fact that on the fit process I'm storing the OHE  and therfore `__dict__` change.\r\nCan someone help me with this? Should I do it in another way?\r\n```\r\n    @pytest.mark.parametrize(\r\n            \"name, Estimator, check\",\r\n            _generate_checks_per_estimator(_yield_all_checks,\r\n                                           _tested_non_meta_estimators()),\r\n            ids=_rename_partial\r\n    )\r\n    def test_non_meta_estimators(name, Estimator, check):\r\n        # Common tests for non-meta estimators\r\n        with ignore_warnings(category=(DeprecationWarning, ConvergenceWarning,\r\n                                       UserWarning, FutureWarning)):\r\n            estimator = Estimator()\r\n            set_checking_parameters(estimator)\r\n>           check(name, estimator)\r\n\r\nEstimator  = <class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>\r\ncheck      = <function check_dict_unchanged at 0x7f8287cf5620>\r\nestimator  = KBinsDiscretizer(encode='onehot', n_bins=5, strategy='quantile')\r\nname       = 'KBinsDiscretizer'\r\n\r\nsklearn/tests/test_common.py:99: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nsklearn/utils/testing.py:326: in wrapper\r\n    return fn(*args, **kwargs)\r\nsklearn/utils/estimator_checks.py:632: in check_dict_unchanged\r\n    'Estimator changes __dict__ during %s' % method)\r\n/usr/lib/python3.6/unittest/case.py:1121: in assertDictEqual\r\n    self.fail(self._formatMessage(msg, standardMsg))\r\n```"
    },
    {
      "id": 404682418,
      "user": "qinhanmin2014",
      "body": "It's not uncommon to set attributes in fit? I don't think it will cause a test failure. Maybe you can submit a PR for us to review."
    },
    {
      "id": 405006842,
      "user": "divayjindal95",
      "body": "Is the issue still open ? Like can i try this?"
    },
    {
      "id": 405006955,
      "user": "qinhanmin2014",
      "body": "@divayjindal95 @ggc87 has submitted a PR. Please try other issues without an active PR."
    },
    {
      "id": 405007116,
      "user": "divayjindal95",
      "body": "@qinhanmin2014  Sure thanks :)"
    },
    {
      "id": 405008461,
      "user": "divayjindal95",
      "body": "@qinhanmin2014  , sorry to ask such a silly question, but i am unable to find any good first issue with no active pr, is there any particular way i could find that out ?"
    },
    {
      "id": 405008898,
      "user": "qinhanmin2014",
      "body": "@divayjindal95 Try searching for ``help wanted`` label and ``easy`` label, or simply go through the issue tracker :)"
    },
    {
      "id": 405127271,
      "user": "jnothman",
      "body": "But no, it's not easy to find that out. GitHub doesn't provide a API for\ntracking whether an issue is referenced by a PR, and we don't have the\npersonnel availability to track this with manual labels. One useful manual\nlabel is \"stalled\" on a pull request, which might not need much work to\ncomplete.\n"
    },
    {
      "id": 406768546,
      "user": "qinhanmin2014",
      "body": "Resolved in #11489"
    }
  ],
  "text_context": "# KBinsDiscretizer : Support inverse_transform for encode other than ordinal\n\nCurrently, we only support ``encode='ordinal'`` in ``inverse_transform``. Since we've supported ``inverse_transform`` in ``OnehotEncoder``, it's natural to support ``inverse_transform`` for ``encode='onehot'`` and ``encode='onehot-dense'``. Store the fitted encoder will be a reasonable solution here. (See https://github.com/scikit-learn/scikit-learn/pull/11467#issuecomment-403995635)\n\nHi, can I try this? \n\nwelcome to contribute :)\n\nI'm having a small problem with the test  test_non_meta_estimators. It fail an assertion due to the fact that on the fit process I'm storing the OHE  and therfore `__dict__` change.\r\nCan someone help me with this? Should I do it in another way?\r\n```\r\n    @pytest.mark.parametrize(\r\n            \"name, Estimator, check\",\r\n            _generate_checks_per_estimator(_yield_all_checks,\r\n                                           _tested_non_meta_estimators()),\r\n            ids=_rename_partial\r\n    )\r\n    def test_non_meta_estimators(name, Estimator, check):\r\n        # Common tests for non-meta estimators\r\n        with ignore_warnings(category=(DeprecationWarning, ConvergenceWarning,\r\n                                       UserWarning, FutureWarning)):\r\n            estimator = Estimator()\r\n            set_checking_parameters(estimator)\r\n>           check(name, estimator)\r\n\r\nEstimator  = <class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>\r\ncheck      = <function check_dict_unchanged at 0x7f8287cf5620>\r\nestimator  = KBinsDiscretizer(encode='onehot', n_bins=5, strategy='quantile')\r\nname       = 'KBinsDiscretizer'\r\n\r\nsklearn/tests/test_common.py:99: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nsklearn/utils/testing.py:326: in wrapper\r\n    return fn(*args, **kwargs)\r\nsklearn/utils/estimator_checks.py:632: in check_dict_unchanged\r\n    'Estimator changes __dict__ during %s' % method)\r\n/usr/lib/python3.6/unittest/case.py:1121: in assertDictEqual\r\n    self.fail(self._formatMessage(msg, standardMsg))\r\n```\n\nIt's not uncommon to set attributes in fit? I don't think it will cause a test failure. Maybe you can submit a PR for us to review.\n\nIs the issue still open ? Like can i try this?\n\n@divayjindal95 @ggc87 has submitted a PR. Please try other issues without an active PR.\n\n@qinhanmin2014  Sure thanks :)\n\n@qinhanmin2014  , sorry to ask such a silly question, but i am unable to find any good first issue with no active pr, is there any particular way i could find that out ?\n\n@divayjindal95 Try searching for ``help wanted`` label and ``easy`` label, or simply go through the issue tracker :)\n\nBut no, it's not easy to find that out. GitHub doesn't provide a API for\ntracking whether an issue is referenced by a PR, and we don't have the\npersonnel availability to track this with manual labels. One useful manual\nlabel is \"stalled\" on a pull request, which might not need much work to\ncomplete.\n\n\nResolved in #11489",
  "pr_link": "https://github.com/scikit-learn/scikit-learn/pull/11467",
  "code_context": [
    {
      "filename": "sklearn/preprocessing/_discretization.py",
      "content": "# -*- coding: utf-8 -*-\n\n# Author: Henry Lin <hlin117@gmail.com>\n#         Tom Dupr√© la Tour\n\n# License: BSD\n\nfrom __future__ import division, absolute_import\n\nimport numbers\nimport numpy as np\nimport warnings\n\nfrom . import OneHotEncoder\n\nfrom ..base import BaseEstimator, TransformerMixin\nfrom ..utils.validation import check_array\nfrom ..utils.validation import check_is_fitted\nfrom ..utils.validation import FLOAT_DTYPES\nfrom ..utils.fixes import np_version\n\n\nclass KBinsDiscretizer(BaseEstimator, TransformerMixin):\n    \"\"\"Bin continuous data into intervals.\n\n    Read more in the :ref:`User Guide <preprocessing_discretization>`.\n\n    Parameters\n    ----------\n    n_bins : int or array-like, shape (n_features,) (default=5)\n        The number of bins to produce. The intervals for the bins are\n        determined by the minimum and maximum of the input data.\n        Raises ValueError if ``n_bins < 2``.\n\n        If ``n_bins`` is an array, and there is an ignored feature at\n        index ``i``, ``n_bins[i]`` will be ignored.\n\n    encode : {'onehot', 'onehot-dense', 'ordinal'}, (default='onehot')\n        Method used to encode the transformed result.\n\n        onehot\n            Encode the transformed result with one-hot encoding\n            and return a sparse matrix. Ignored features are always\n            stacked to the right.\n        onehot-dense\n            Encode the transformed result with one-hot encoding\n            and return a dense array. Ignored features are always\n            stacked to the right.\n        ordinal\n            Return the bin identifier encoded as an integer value.\n\n    strategy : {'uniform', 'quantile', 'kmeans'}, (default='quantile')\n        Strategy used to define the widths of the bins.\n\n        uniform\n            All bins in each feature have identical widths.\n        quantile\n            All bins in each feature have the same number of points.\n        kmeans\n            Values in each bin have the same nearest center of a 1D k-means\n            cluster.\n\n    Attributes\n    ----------\n    n_bins_ : int array, shape (n_features,)\n        Number of bins per feature. An ignored feature at index ``i``\n        will have ``n_bins_[i] == 0``.\n\n    bin_edges_ : array of arrays, shape (n_features, )\n        The edges of each bin. Contain arrays of varying shapes (n_bins_, ).\n        Ignored features will have empty arrays.\n\n    Examples\n    --------\n    >>> X = [[-2, 1, -4,   -1],\n    ...      [-1, 2, -3, -0.5],\n    ...      [ 0, 3, -2,  0.5],\n    ...      [ 1, 4, -1,    2]]\n    >>> est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    >>> est.fit(X)  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n    KBinsDiscretizer(...)\n    >>> Xt = est.transform(X)\n    >>> Xt  # doctest: +SKIP\n    array([[ 0., 0., 0., 0.],\n           [ 1., 1., 1., 0.],\n           [ 2., 2., 2., 1.],\n           [ 2., 2., 2., 2.]])\n\n    Sometimes it may be useful to convert the data back into the original\n    feature space. The ``inverse_transform`` function converts the binned\n    data into the original feature space. Each value will be equal to the mean\n    of the two bin edges.\n\n    >>> est.bin_edges_[0]\n    array([-2., -1.,  0.,  1.])\n    >>> est.inverse_transform(Xt)\n    array([[-1.5,  1.5, -3.5, -0.5],\n           [-0.5,  2.5, -2.5, -0.5],\n           [ 0.5,  3.5, -1.5,  0.5],\n           [ 0.5,  3.5, -1.5,  1.5]])\n\n    Notes\n    -----\n    In bin edges for feature ``i``, the first and last values are used only for\n    ``inverse_transform``. During transform, bin edges are extended to::\n\n      np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf])\n\n    You can combine ``KBinsDiscretizer`` with\n    :class:`sklearn.compose.ColumnTransformer` if you only want to preprocess\n    part of the features.\n\n    See also\n    --------\n     sklearn.preprocessing.Binarizer : class used to bin values as ``0`` or\n        ``1`` based on a parameter ``threshold``.\n    \"\"\"\n\n    def __init__(self, n_bins=5, encode='onehot', strategy='quantile'):\n        self.n_bins = n_bins\n        self.encode = encode\n        self.strategy = strategy\n\n    def fit(self, X, y=None):\n        \"\"\"Fits the estimator.\n\n        Parameters\n        ----------\n        X : numeric array-like, shape (n_samples, n_features)\n            Data to be discretized.\n\n        y : ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = check_array(X, dtype='numeric')\n\n        valid_encode = ('onehot', 'onehot-dense', 'ordinal')\n        if self.encode not in valid_encode:\n            raise ValueError(\"Valid options for 'encode' are {}. \"\n                             \"Got encode={!r} instead.\"\n                             .format(valid_encode, self.encode))\n        valid_strategy = ('uniform', 'quantile', 'kmeans')\n        if self.strategy not in valid_strategy:\n            raise ValueError(\"Valid options for 'strategy' are {}. \"\n                             \"Got strategy={!r} instead.\"\n                             .format(valid_strategy, self.strategy))\n\n        n_features = X.shape[1]\n        n_bins = self._validate_n_bins(n_features)\n\n        bin_edges = np.zeros(n_features, dtype=object)\n        for jj in range(n_features):\n            column = X[:, jj]\n            col_min, col_max = column.min(), column.max()\n\n            if col_min == col_max:\n                warnings.warn(\"Feature %d is constant and will be \"\n                              \"replaced with 0.\" % jj)\n                n_bins[jj] = 1\n                bin_edges[jj] = np.array([-np.inf, np.inf])\n                continue\n\n            if self.strategy == 'uniform':\n                bin_edges[jj] = np.linspace(col_min, col_max, n_bins[jj] + 1)\n\n            elif self.strategy == 'quantile':\n                quantiles = np.linspace(0, 100, n_bins[jj] + 1)\n                if np_version < (1, 9):\n                    quantiles = list(quantiles)\n                bin_edges[jj] = np.asarray(np.percentile(column, quantiles))\n\n            elif self.strategy == 'kmeans':\n                from ..cluster import KMeans  # fixes import loops\n\n                # Deterministic initialization with uniform spacing\n                uniform_edges = np.linspace(col_min, col_max, n_bins[jj] + 1)\n                init = (uniform_edges[1:] + uniform_edges[:-1])[:, None] * 0.5\n\n                # 1D k-means procedure\n                km = KMeans(n_clusters=n_bins[jj], init=init, n_init=1)\n                centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n                bin_edges[jj] = (centers[1:] + centers[:-1]) * 0.5\n                bin_edges[jj] = np.r_[col_min, bin_edges[jj], col_max]\n\n        self.bin_edges_ = bin_edges\n        self.n_bins_ = n_bins\n\n        return self\n\n    def _validate_n_bins(self, n_features):\n        \"\"\"Returns n_bins_, the number of bins per feature.\n\n        Also ensures that ignored bins are zero.\n        \"\"\"\n        orig_bins = self.n_bins\n        if isinstance(orig_bins, numbers.Number):\n            if not isinstance(orig_bins, (numbers.Integral, np.integer)):\n                raise ValueError(\"{} received an invalid n_bins type. \"\n                                 \"Received {}, expected int.\"\n                                 .format(KBinsDiscretizer.__name__,\n                                         type(orig_bins).__name__))\n            if orig_bins < 2:\n                raise ValueError(\"{} received an invalid number \"\n                                 \"of bins. Received {}, expected at least 2.\"\n                                 .format(KBinsDiscretizer.__name__, orig_bins))\n            return np.ones(n_features, dtype=np.int) * orig_bins\n\n        n_bins = check_array(orig_bins, dtype=np.int, copy=True,\n                             ensure_2d=False)\n\n        if n_bins.ndim > 1 or n_bins.shape[0] != n_features:\n            raise ValueError(\"n_bins must be a scalar or array \"\n                             \"of shape (n_features,).\")\n\n        bad_nbins_value = (n_bins < 2) | (n_bins != orig_bins)\n\n        violating_indices = np.where(bad_nbins_value)[0]\n        if violating_indices.shape[0] > 0:\n            indices = \", \".join(str(i) for i in violating_indices)\n            raise ValueError(\"{} received an invalid number \"\n                             \"of bins at indices {}. Number of bins \"\n                             \"must be at least 2, and must be an int.\"\n                             .format(KBinsDiscretizer.__name__, indices))\n        return n_bins\n\n    def transform(self, X):\n        \"\"\"Discretizes the data.\n\n        Parameters\n        ----------\n        X : numeric array-like, shape (n_samples, n_features)\n            Data to be discretized.\n\n        Returns\n        -------\n        Xt : numeric array-like or sparse matrix\n            Data in the binned space.\n        \"\"\"\n        check_is_fitted(self, [\"bin_edges_\"])\n\n        Xt = check_array(X, copy=True, dtype=FLOAT_DTYPES)\n        n_features = self.n_bins_.shape[0]\n        if Xt.shape[1] != n_features:\n            raise ValueError(\"Incorrect number of features. Expecting {}, \"\n                             \"received {}.\".format(n_features, Xt.shape[1]))\n\n        bin_edges = self.bin_edges_\n        for jj in range(Xt.shape[1]):\n            # Values which are close to a bin edge are susceptible to numeric\n            # instability. Add eps to X so these values are binned correctly\n            # with respect to their decimal truncation. See documentation of\n            # numpy.isclose for an explanation of ``rtol`` and ``atol``.\n            rtol = 1.e-5\n            atol = 1.e-8\n            eps = atol + rtol * np.abs(Xt[:, jj])\n            Xt[:, jj] = np.digitize(Xt[:, jj] + eps, bin_edges[jj][1:])\n        np.clip(Xt, 0, self.n_bins_ - 1, out=Xt)\n\n        if self.encode == 'ordinal':\n            return Xt\n\n        encode_sparse = self.encode == 'onehot'\n        return OneHotEncoder(categories=[np.arange(i) for i in self.n_bins_],\n                             sparse=encode_sparse).fit_transform(Xt)\n\n    def inverse_transform(self, Xt):\n        \"\"\"Transforms discretized data back to original feature space.\n\n        Note that this function does not regenerate the original data\n        due to discretization rounding.\n\n        Parameters\n        ----------\n        Xt : numeric array-like, shape (n_sample, n_features)\n            Transformed data in the binned space.\n\n        Returns\n        -------\n        Xinv : numeric array-like\n            Data in the original feature space.\n        \"\"\"\n        check_is_fitted(self, [\"bin_edges_\"])\n\n        if self.encode != 'ordinal':\n            raise ValueError(\"inverse_transform only supports \"\n                             \"'encode = ordinal'. Got encode={!r} instead.\"\n                             .format(self.encode))\n\n        Xinv = check_array(Xt, copy=True, dtype=FLOAT_DTYPES)\n        n_features = self.n_bins_.shape[0]\n        if Xinv.shape[1] != n_features:\n            raise ValueError(\"Incorrect number of features. Expecting {}, \"\n                             \"received {}.\".format(n_features, Xinv.shape[1]))\n\n        for jj in range(n_features):\n            bin_edges = self.bin_edges_[jj]\n            bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5\n            Xinv[:, jj] = bin_centers[np.int_(Xinv[:, jj])]\n\n        return Xinv\n"
    },
    {
      "filename": "sklearn/preprocessing/tests/test_discretization.py",
      "content": "from __future__ import absolute_import\n\nimport pytest\nimport numpy as np\nimport scipy.sparse as sp\nimport warnings\n\nfrom sklearn.externals.six.moves import xrange as range\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.utils.testing import (\n    assert_array_equal,\n    assert_raises,\n    assert_raise_message,\n    assert_warns_message\n)\n\nX = [[-2, 1.5, -4, -1],\n     [-1, 2.5, -3, -0.5],\n     [0, 3.5, -2, 0.5],\n     [1, 4.5, -1, 2]]\n\n\n@pytest.mark.parametrize(\n    'strategy, expected',\n    [('uniform', [[0, 0, 0, 0], [1, 1, 1, 0], [2, 2, 2, 1], [2, 2, 2, 2]]),\n     ('kmeans', [[0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 1], [2, 2, 2, 2]]),\n     ('quantile', [[0, 0, 0, 0], [1, 1, 1, 1], [2, 2, 2, 2], [2, 2, 2, 2]])])\ndef test_fit_transform(strategy, expected):\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy=strategy)\n    est.fit(X)\n    assert_array_equal(expected, est.transform(X))\n\n\ndef test_valid_n_bins():\n    KBinsDiscretizer(n_bins=2).fit_transform(X)\n    KBinsDiscretizer(n_bins=np.array([2])[0]).fit_transform(X)\n    assert KBinsDiscretizer(n_bins=2).fit(X).n_bins_.dtype == np.dtype(np.int)\n\n\ndef test_invalid_n_bins():\n    est = KBinsDiscretizer(n_bins=1)\n    assert_raise_message(ValueError, \"KBinsDiscretizer received an invalid \"\n                         \"number of bins. Received 1, expected at least 2.\",\n                         est.fit_transform, X)\n\n    est = KBinsDiscretizer(n_bins=1.1)\n    assert_raise_message(ValueError, \"KBinsDiscretizer received an invalid \"\n                         \"n_bins type. Received float, expected int.\",\n                         est.fit_transform, X)\n\n\ndef test_invalid_n_bins_array():\n    # Bad shape\n    n_bins = np.ones((2, 4)) * 2\n    est = KBinsDiscretizer(n_bins=n_bins)\n    assert_raise_message(ValueError,\n                         \"n_bins must be a scalar or array of shape \"\n                         \"(n_features,).\", est.fit_transform, X)\n\n    # Incorrect number of features\n    n_bins = [1, 2, 2]\n    est = KBinsDiscretizer(n_bins=n_bins)\n    assert_raise_message(ValueError,\n                         \"n_bins must be a scalar or array of shape \"\n                         \"(n_features,).\", est.fit_transform, X)\n\n    # Bad bin values\n    n_bins = [1, 2, 2, 1]\n    est = KBinsDiscretizer(n_bins=n_bins)\n    assert_raise_message(ValueError,\n                         \"KBinsDiscretizer received an invalid number of bins \"\n                         \"at indices 0, 3. Number of bins must be at least 2, \"\n                         \"and must be an int.\",\n                         est.fit_transform, X)\n\n    # Float bin values\n    n_bins = [2.1, 2, 2.1, 2]\n    est = KBinsDiscretizer(n_bins=n_bins)\n    assert_raise_message(ValueError,\n                         \"KBinsDiscretizer received an invalid number of bins \"\n                         \"at indices 0, 2. Number of bins must be at least 2, \"\n                         \"and must be an int.\",\n                         est.fit_transform, X)\n\n\n@pytest.mark.parametrize(\n    'strategy, expected',\n    [('uniform', [[0, 0, 0, 0], [0, 1, 1, 0], [1, 2, 2, 1], [1, 2, 2, 2]]),\n     ('kmeans', [[0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 1], [1, 2, 2, 2]]),\n     ('quantile', [[0, 0, 0, 0], [0, 1, 1, 1], [1, 2, 2, 2], [1, 2, 2, 2]])])\ndef test_fit_transform_n_bins_array(strategy, expected):\n    est = KBinsDiscretizer(n_bins=[2, 3, 3, 3], encode='ordinal',\n                           strategy=strategy).fit(X)\n    assert_array_equal(expected, est.transform(X))\n\n    # test the shape of bin_edges_\n    n_features = np.array(X).shape[1]\n    assert est.bin_edges_.shape == (n_features, )\n    for bin_edges, n_bins in zip(est.bin_edges_, est.n_bins_):\n        assert bin_edges.shape == (n_bins + 1, )\n\n\ndef test_invalid_n_features():\n    est = KBinsDiscretizer(n_bins=3).fit(X)\n    bad_X = np.arange(25).reshape(5, -1)\n    assert_raise_message(ValueError,\n                         \"Incorrect number of features. Expecting 4, \"\n                         \"received 5\", est.transform, bad_X)\n\n\n@pytest.mark.parametrize('strategy', ['uniform', 'kmeans', 'quantile'])\ndef test_same_min_max(strategy):\n    warnings.simplefilter(\"always\")\n    X = np.array([[1, -2],\n                  [1, -1],\n                  [1, 0],\n                  [1, 1]])\n    est = KBinsDiscretizer(strategy=strategy, n_bins=3, encode='ordinal')\n    assert_warns_message(UserWarning,\n                         \"Feature 0 is constant and will be replaced \"\n                         \"with 0.\", est.fit, X)\n    assert est.n_bins_[0] == 1\n    # replace the feature with zeros\n    Xt = est.transform(X)\n    assert_array_equal(Xt[:, 0], np.zeros(X.shape[0]))\n\n\ndef test_transform_1d_behavior():\n    X = np.arange(4)\n    est = KBinsDiscretizer(n_bins=2)\n    assert_raises(ValueError, est.fit, X)\n\n    est = KBinsDiscretizer(n_bins=2)\n    est.fit(X.reshape(-1, 1))\n    assert_raises(ValueError, est.transform, X)\n\n\ndef test_numeric_stability():\n    X_init = np.array([2., 4., 6., 8., 10.]).reshape(-1, 1)\n    Xt_expected = np.array([0, 0, 1, 1, 1]).reshape(-1, 1)\n\n    # Test up to discretizing nano units\n    for i in range(1, 9):\n        X = X_init / 10**i\n        Xt = KBinsDiscretizer(n_bins=2, encode='ordinal').fit_transform(X)\n        assert_array_equal(Xt_expected, Xt)\n\n\ndef test_invalid_encode_option():\n    est = KBinsDiscretizer(n_bins=[2, 3, 3, 3], encode='invalid-encode')\n    assert_raise_message(ValueError, \"Valid options for 'encode' are \"\n                         \"('onehot', 'onehot-dense', 'ordinal'). \"\n                         \"Got encode='invalid-encode' instead.\",\n                         est.fit, X)\n\n\ndef test_encode_options():\n    est = KBinsDiscretizer(n_bins=[2, 3, 3, 3],\n                           encode='ordinal').fit(X)\n    Xt_1 = est.transform(X)\n    est = KBinsDiscretizer(n_bins=[2, 3, 3, 3],\n                           encode='onehot-dense').fit(X)\n    Xt_2 = est.transform(X)\n    assert not sp.issparse(Xt_2)\n    assert_array_equal(OneHotEncoder(\n                           categories=[np.arange(i) for i in [2, 3, 3, 3]],\n                           sparse=False)\n                       .fit_transform(Xt_1), Xt_2)\n    assert_raise_message(ValueError, \"inverse_transform only supports \"\n                         \"'encode = ordinal'. Got encode='onehot-dense' \"\n                         \"instead.\", est.inverse_transform, Xt_2)\n    est = KBinsDiscretizer(n_bins=[2, 3, 3, 3],\n                           encode='onehot').fit(X)\n    Xt_3 = est.transform(X)\n    assert sp.issparse(Xt_3)\n    assert_array_equal(OneHotEncoder(\n                           categories=[np.arange(i) for i in [2, 3, 3, 3]],\n                           sparse=True)\n                       .fit_transform(Xt_1).toarray(),\n                       Xt_3.toarray())\n    assert_raise_message(ValueError, \"inverse_transform only supports \"\n                         \"'encode = ordinal'. Got encode='onehot' \"\n                         \"instead.\", est.inverse_transform, Xt_2)\n\n\ndef test_invalid_strategy_option():\n    est = KBinsDiscretizer(n_bins=[2, 3, 3, 3], strategy='invalid-strategy')\n    assert_raise_message(ValueError, \"Valid options for 'strategy' are \"\n                         \"('uniform', 'quantile', 'kmeans'). \"\n                         \"Got strategy='invalid-strategy' instead.\",\n                         est.fit, X)\n\n\n@pytest.mark.parametrize(\n    'strategy, expected_2bins, expected_3bins',\n    [('uniform', [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 2, 2]),\n     ('kmeans', [0, 0, 0, 0, 1, 1], [0, 1, 1, 1, 2, 2]),\n     ('quantile', [0, 0, 0, 1, 1, 1], [0, 0, 1, 1, 2, 2])])\ndef test_nonuniform_strategies(strategy, expected_2bins, expected_3bins):\n    X = np.array([0, 1, 2, 3, 9, 10]).reshape(-1, 1)\n\n    # with 2 bins\n    est = KBinsDiscretizer(n_bins=2, strategy=strategy, encode='ordinal')\n    Xt = est.fit_transform(X)\n    assert_array_equal(expected_2bins, Xt.ravel())\n\n    # with 3 bins\n    est = KBinsDiscretizer(n_bins=3, strategy=strategy, encode='ordinal')\n    Xt = est.fit_transform(X)\n    assert_array_equal(expected_3bins, Xt.ravel())\n\n\n@pytest.mark.parametrize('strategy', ['uniform', 'kmeans', 'quantile'])\ndef test_inverse_transform(strategy):\n    X = np.random.RandomState(0).randn(100, 3)\n    kbd = KBinsDiscretizer(n_bins=3, strategy=strategy, encode='ordinal')\n    Xt = kbd.fit_transform(X)\n    assert_array_equal(Xt.max(axis=0) + 1, kbd.n_bins_)\n\n    X2 = kbd.inverse_transform(Xt)\n    X2t = kbd.fit_transform(X2)\n    assert_array_equal(X2t.max(axis=0) + 1, kbd.n_bins_)\n    assert_array_equal(Xt, X2t)\n\n\n@pytest.mark.parametrize('strategy', ['uniform', 'kmeans', 'quantile'])\ndef test_transform_outside_fit_range(strategy):\n    X = np.array([0, 1, 2, 3])[:, None]\n    kbd = KBinsDiscretizer(n_bins=4, strategy=strategy, encode='ordinal')\n    kbd.fit(X)\n\n    X2 = np.array([-2, 5])[:, None]\n    X2t = kbd.transform(X2)\n    assert_array_equal(X2t.max(axis=0) + 1, kbd.n_bins_)\n    assert_array_equal(X2t.min(axis=0), [0])\n\n\ndef test_overwrite():\n    X = np.array([0, 1, 2, 3])[:, None]\n    X_before = X.copy()\n\n    est = KBinsDiscretizer(n_bins=3, encode=\"ordinal\")\n    Xt = est.fit_transform(X)\n    assert_array_equal(X, X_before)\n\n    Xt_before = Xt.copy()\n    Xinv = est.inverse_transform(Xt)\n    assert_array_equal(Xt, Xt_before)\n    assert_array_equal(Xinv, np.array([[0.5], [1.5], [2.5], [2.5]]))\n"
    },
    {
      "filename": "sklearn/utils/testing.py",
      "content": "\"\"\"Testing utilities.\"\"\"\n\n# Copyright (c) 2011, 2012\n# Authors: Pietro Berkes,\n#          Andreas Muller\n#          Mathieu Blondel\n#          Olivier Grisel\n#          Arnaud Joly\n#          Denis Engemann\n#          Giorgio Patrini\n#          Thierry Guillemot\n# License: BSD 3 clause\nimport os\nimport inspect\nimport pkgutil\nimport warnings\nimport sys\nimport struct\nimport functools\n\nimport scipy as sp\nimport scipy.io\nfrom functools import wraps\nfrom operator import itemgetter\ntry:\n    # Python 2\n    from urllib2 import urlopen\n    from urllib2 import HTTPError\nexcept ImportError:\n    # Python 3+\n    from urllib.request import urlopen\n    from urllib.error import HTTPError\n\nimport tempfile\nimport shutil\nimport os.path as op\nimport atexit\nimport unittest\n\n# WindowsError only exist on Windows\ntry:\n    WindowsError\nexcept NameError:\n    WindowsError = None\n\nimport sklearn\nfrom sklearn.base import BaseEstimator\nfrom sklearn.externals import joblib\nfrom sklearn.utils.fixes import signature\nfrom sklearn.utils import deprecated\n\n\nadditional_names_in_all = []\ntry:\n    from nose.tools import raises as _nose_raises\n    deprecation_message = (\n        'sklearn.utils.testing.raises has been deprecated in version 0.20 '\n        'and will be removed in 0.22. Please use '\n        'sklearn.utils.testing.assert_raises instead.')\n    raises = deprecated(deprecation_message)(_nose_raises)\n    additional_names_in_all.append('raises')\nexcept ImportError:\n    pass\n\ntry:\n    from nose.tools import with_setup as _with_setup\n    deprecation_message = (\n        'sklearn.utils.testing.with_setup has been deprecated in version 0.20 '\n        'and will be removed in 0.22.'\n        'If your code relies on with_setup, please use'\n        ' nose.tools.with_setup instead.')\n    with_setup = deprecated(deprecation_message)(_with_setup)\n    additional_names_in_all.append('with_setup')\nexcept ImportError:\n    pass\n\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_array_equal\nfrom numpy.testing import assert_array_almost_equal\nfrom numpy.testing import assert_array_less\nfrom numpy.testing import assert_approx_equal\nimport numpy as np\n\nfrom sklearn.base import (ClassifierMixin, RegressorMixin, TransformerMixin,\n                          ClusterMixin)\nfrom sklearn.utils._unittest_backport import TestCase\n\n__all__ = [\"assert_equal\", \"assert_not_equal\", \"assert_raises\",\n           \"assert_raises_regexp\", \"assert_true\",\n           \"assert_false\", \"assert_almost_equal\", \"assert_array_equal\",\n           \"assert_array_almost_equal\", \"assert_array_less\",\n           \"assert_less\", \"assert_less_equal\",\n           \"assert_greater\", \"assert_greater_equal\",\n           \"assert_approx_equal\", \"SkipTest\"]\n__all__.extend(additional_names_in_all)\n\n_dummy = TestCase('__init__')\nassert_equal = _dummy.assertEqual\nassert_not_equal = _dummy.assertNotEqual\nassert_true = _dummy.assertTrue\nassert_false = _dummy.assertFalse\nassert_raises = _dummy.assertRaises\nSkipTest = unittest.case.SkipTest\nassert_dict_equal = _dummy.assertDictEqual\nassert_in = _dummy.assertIn\nassert_not_in = _dummy.assertNotIn\nassert_less = _dummy.assertLess\nassert_greater = _dummy.assertGreater\nassert_less_equal = _dummy.assertLessEqual\nassert_greater_equal = _dummy.assertGreaterEqual\n\nassert_raises_regex = _dummy.assertRaisesRegex\n# assert_raises_regexp is deprecated in Python 3.4 in favor of\n# assert_raises_regex but lets keep the backward compat in scikit-learn with\n# the old name for now\nassert_raises_regexp = assert_raises_regex\n\n\ndef assert_warns(warning_class, func, *args, **kw):\n    \"\"\"Test that a certain warning occurs.\n\n    Parameters\n    ----------\n    warning_class : the warning class\n        The class to test for, e.g. UserWarning.\n\n    func : callable\n        Callable object to trigger warnings.\n\n    *args : the positional arguments to `func`.\n\n    **kw : the keyword arguments to `func`\n\n    Returns\n    -------\n\n    result : the return value of `func`\n\n    \"\"\"\n    clean_warning_registry()\n    with warnings.catch_warnings(record=True) as w:\n        # Cause all warnings to always be triggered.\n        warnings.simplefilter(\"always\")\n        # Trigger a warning.\n        result = func(*args, **kw)\n        if hasattr(np, 'VisibleDeprecationWarning'):\n            # Filter out numpy-specific warnings in numpy >= 1.9\n            w = [e for e in w\n                 if e.category is not np.VisibleDeprecationWarning]\n\n        # Verify some things\n        if not len(w) > 0:\n            raise AssertionError(\"No warning raised when calling %s\"\n                                 % func.__name__)\n\n        found = any(warning.category is warning_class for warning in w)\n        if not found:\n            raise AssertionError(\"%s did not give warning: %s( is %s)\"\n                                 % (func.__name__, warning_class, w))\n    return result\n\n\ndef assert_warns_message(warning_class, message, func, *args, **kw):\n    # very important to avoid uncontrolled state propagation\n    \"\"\"Test that a certain warning occurs and with a certain message.\n\n    Parameters\n    ----------\n    warning_class : the warning class\n        The class to test for, e.g. UserWarning.\n\n    message : str | callable\n        The message or a substring of the message to test for. If callable,\n        it takes a string as the argument and will trigger an AssertionError\n        if the callable returns `False`.\n\n    func : callable\n        Callable object to trigger warnings.\n\n    *args : the positional arguments to `func`.\n\n    **kw : the keyword arguments to `func`.\n\n    Returns\n    -------\n    result : the return value of `func`\n\n    \"\"\"\n    clean_warning_registry()\n    with warnings.catch_warnings(record=True) as w:\n        # Cause all warnings to always be triggered.\n        warnings.simplefilter(\"always\")\n        if hasattr(np, 'VisibleDeprecationWarning'):\n            # Let's not catch the numpy internal DeprecationWarnings\n            warnings.simplefilter('ignore', np.VisibleDeprecationWarning)\n        # Trigger a warning.\n        result = func(*args, **kw)\n        # Verify some things\n        if not len(w) > 0:\n            raise AssertionError(\"No warning raised when calling %s\"\n                                 % func.__name__)\n\n        found = [issubclass(warning.category, warning_class) for warning in w]\n        if not any(found):\n            raise AssertionError(\"No warning raised for %s with class \"\n                                 \"%s\"\n                                 % (func.__name__, warning_class))\n\n        message_found = False\n        # Checks the message of all warnings belong to warning_class\n        for index in [i for i, x in enumerate(found) if x]:\n            # substring will match, the entire message with typo won't\n            msg = w[index].message  # For Python 3 compatibility\n            msg = str(msg.args[0] if hasattr(msg, 'args') else msg)\n            if callable(message):  # add support for certain tests\n                check_in_message = message\n            else:\n                check_in_message = lambda msg: message in msg\n\n            if check_in_message(msg):\n                message_found = True\n                break\n\n        if not message_found:\n            raise AssertionError(\"Did not receive the message you expected \"\n                                 \"('%s') for <%s>, got: '%s'\"\n                                 % (message, func.__name__, msg))\n\n    return result\n\n\ndef assert_warns_div0(func, *args, **kw):\n    \"\"\"Assume that numpy's warning for divide by zero is raised\n\n    Handles the case of platforms that do not support warning on divide by zero\n    \"\"\"\n\n    with np.errstate(divide='warn', invalid='warn'):\n        try:\n            assert_warns(RuntimeWarning, np.divide, 1, np.zeros(1))\n        except AssertionError:\n            # This platform does not report numpy divide by zeros\n            return func(*args, **kw)\n        return assert_warns_message(RuntimeWarning,\n                                    'invalid value encountered',\n                                    func, *args, **kw)\n\n\n# To remove when we support numpy 1.7\ndef assert_no_warnings(func, *args, **kw):\n    # very important to avoid uncontrolled state propagation\n    clean_warning_registry()\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n\n        result = func(*args, **kw)\n        if hasattr(np, 'VisibleDeprecationWarning'):\n            # Filter out numpy-specific warnings in numpy >= 1.9\n            w = [e for e in w\n                 if e.category is not np.VisibleDeprecationWarning]\n\n        if len(w) > 0:\n            raise AssertionError(\"Got warnings when calling %s: [%s]\"\n                                 % (func.__name__,\n                                    ', '.join(str(warning) for warning in w)))\n    return result\n\n\ndef ignore_warnings(obj=None, category=Warning):\n    \"\"\"Context manager and decorator to ignore warnings.\n\n    Note: Using this (in both variants) will clear all warnings\n    from all python modules loaded. In case you need to test\n    cross-module-warning-logging, this is not your tool of choice.\n\n    Parameters\n    ----------\n    category : warning class, defaults to Warning.\n        The category to filter. If Warning, all categories will be muted.\n\n    Examples\n    --------\n    >>> with ignore_warnings():\n    ...     warnings.warn('buhuhuhu')\n\n    >>> def nasty_warn():\n    ...    warnings.warn('buhuhuhu')\n    ...    print(42)\n\n    >>> ignore_warnings(nasty_warn)()\n    42\n    \"\"\"\n    if callable(obj):\n        return _IgnoreWarnings(category=category)(obj)\n    else:\n        return _IgnoreWarnings(category=category)\n\n\nclass _IgnoreWarnings(object):\n    \"\"\"Improved and simplified Python warnings context manager and decorator.\n\n    This class allows the user to ignore the warnings raised by a function.\n    Copied from Python 2.7.5 and modified as required.\n\n    Parameters\n    ----------\n    category : tuple of warning class, default to Warning\n        The category to filter. By default, all the categories will be muted.\n\n    \"\"\"\n\n    def __init__(self, category):\n        self._record = True\n        self._module = sys.modules['warnings']\n        self._entered = False\n        self.log = []\n        self.category = category\n\n    def __call__(self, fn):\n        \"\"\"Decorator to catch and hide warnings without visual nesting.\"\"\"\n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n            clean_warning_registry()\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", self.category)\n                return fn(*args, **kwargs)\n\n        return wrapper\n\n    def __repr__(self):\n        args = []\n        if self._record:\n            args.append(\"record=True\")\n        if self._module is not sys.modules['warnings']:\n            args.append(\"module=%r\" % self._module)\n        name = type(self).__name__\n        return \"%s(%s)\" % (name, \", \".join(args))\n\n    def __enter__(self):\n        if self._entered:\n            raise RuntimeError(\"Cannot enter %r twice\" % self)\n        self._entered = True\n        self._filters = self._module.filters\n        self._module.filters = self._filters[:]\n        self._showwarning = self._module.showwarning\n        clean_warning_registry()\n        warnings.simplefilter(\"ignore\", self.category)\n\n    def __exit__(self, *exc_info):\n        if not self._entered:\n            raise RuntimeError(\"Cannot exit %r without entering first\" % self)\n        self._module.filters = self._filters\n        self._module.showwarning = self._showwarning\n        self.log[:] = []\n        clean_warning_registry()\n\n\nassert_less = _dummy.assertLess\nassert_greater = _dummy.assertGreater\n\nassert_allclose = np.testing.assert_allclose\n\ndef assert_raise_message(exceptions, message, function, *args, **kwargs):\n    \"\"\"Helper function to test the message raised in an exception.\n\n    Given an exception, a callable to raise the exception, and\n    a message string, tests that the correct exception is raised and\n    that the message is a substring of the error thrown. Used to test\n    that the specific message thrown during an exception is correct.\n\n    Parameters\n    ----------\n    exceptions : exception or tuple of exception\n        An Exception object.\n\n    message : str\n        The error message or a substring of the error message.\n\n    function : callable\n        Callable object to raise error.\n\n    *args : the positional arguments to `function`.\n\n    **kwargs : the keyword arguments to `function`.\n    \"\"\"\n    try:\n        function(*args, **kwargs)\n    except exceptions as e:\n        error_message = str(e)\n        if message not in error_message:\n            raise AssertionError(\"Error message does not include the expected\"\n                                 \" string: %r. Observed error message: %r\" %\n                                 (message, error_message))\n    else:\n        # concatenate exception names\n        if isinstance(exceptions, tuple):\n            names = \" or \".join(e.__name__ for e in exceptions)\n        else:\n            names = exceptions.__name__\n\n        raise AssertionError(\"%s not raised by %s\" %\n                             (names, function.__name__))\n\n\ndef assert_allclose_dense_sparse(x, y, rtol=1e-07, atol=1e-9, err_msg=''):\n    \"\"\"Assert allclose for sparse and dense data.\n\n    Both x and y need to be either sparse or dense, they\n    can't be mixed.\n\n    Parameters\n    ----------\n    x : array-like or sparse matrix\n        First array to compare.\n\n    y : array-like or sparse matrix\n        Second array to compare.\n\n    rtol : float, optional\n        relative tolerance; see numpy.allclose\n\n    atol : float, optional\n        absolute tolerance; see numpy.allclose. Note that the default here is\n        more tolerant than the default for numpy.testing.assert_allclose, where\n        atol=0.\n\n    err_msg : string, default=''\n        Error message to raise.\n    \"\"\"\n    if sp.sparse.issparse(x) and sp.sparse.issparse(y):\n        x = x.tocsr()\n        y = y.tocsr()\n        x.sum_duplicates()\n        y.sum_duplicates()\n        assert_array_equal(x.indices, y.indices, err_msg=err_msg)\n        assert_array_equal(x.indptr, y.indptr, err_msg=err_msg)\n        assert_allclose(x.data, y.data, rtol=rtol, atol=atol, err_msg=err_msg)\n    elif not sp.sparse.issparse(x) and not sp.sparse.issparse(y):\n        # both dense\n        assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)\n    else:\n        raise ValueError(\"Can only compare two sparse matrices,\"\n                         \" not a sparse matrix and an array.\")\n\n\ndef fake_mldata(columns_dict, dataname, matfile, ordering=None):\n    \"\"\"Create a fake mldata data set.\n\n    Parameters\n    ----------\n    columns_dict : dict, keys=str, values=ndarray\n        Contains data as columns_dict[column_name] = array of data.\n\n    dataname : string\n        Name of data set.\n\n    matfile : string or file object\n        The file name string or the file-like object of the output file.\n\n    ordering : list, default None\n        List of column_names, determines the ordering in the data set.\n\n    Notes\n    -----\n    This function transposes all arrays, while fetch_mldata only transposes\n    'data', keep that into account in the tests.\n    \"\"\"\n    datasets = dict(columns_dict)\n\n    # transpose all variables\n    for name in datasets:\n        datasets[name] = datasets[name].T\n\n    if ordering is None:\n        ordering = sorted(list(datasets.keys()))\n    # NOTE: setting up this array is tricky, because of the way Matlab\n    # re-packages 1D arrays\n    datasets['mldata_descr_ordering'] = sp.empty((1, len(ordering)),\n                                                 dtype='object')\n    for i, name in enumerate(ordering):\n        datasets['mldata_descr_ordering'][0, i] = name\n\n    scipy.io.savemat(matfile, datasets, oned_as='column')\n\n\nclass mock_mldata_urlopen(object):\n    \"\"\"Object that mocks the urlopen function to fake requests to mldata.\n\n    When requesting a dataset with a name that is in mock_datasets, this object\n    creates a fake dataset in a StringIO object and returns it. Otherwise, it\n    raises an HTTPError.\n\n    Parameters\n    ----------\n    mock_datasets : dict\n        A dictionary of {dataset_name: data_dict}, or\n        {dataset_name: (data_dict, ordering). `data_dict` itself is a\n        dictionary of {column_name: data_array}, and `ordering` is a list of\n        column_names to determine the ordering in the data set (see\n        :func:`fake_mldata` for details).\n    \"\"\"\n    def __init__(self, mock_datasets):\n        self.mock_datasets = mock_datasets\n\n    def __call__(self, urlname):\n        dataset_name = urlname.split('/')[-1]\n        if dataset_name in self.mock_datasets:\n            resource_name = '_' + dataset_name\n            from io import BytesIO\n            matfile = BytesIO()\n\n            dataset = self.mock_datasets[dataset_name]\n            ordering = None\n            if isinstance(dataset, tuple):\n                dataset, ordering = dataset\n            fake_mldata(dataset, resource_name, matfile, ordering)\n\n            matfile.seek(0)\n            return matfile\n        else:\n            raise HTTPError(urlname, 404, dataset_name + \" is not available\",\n                            [], None)\n\n\ndef install_mldata_mock(mock_datasets):\n    # Lazy import to avoid mutually recursive imports\n    from sklearn import datasets\n    datasets.mldata.urlopen = mock_mldata_urlopen(mock_datasets)\n\n\ndef uninstall_mldata_mock():\n    # Lazy import to avoid mutually recursive imports\n    from sklearn import datasets\n    datasets.mldata.urlopen = urlopen\n\n\n# Meta estimators need another estimator to be instantiated.\nMETA_ESTIMATORS = [\"OneVsOneClassifier\", \"MultiOutputEstimator\",\n                   \"MultiOutputRegressor\", \"MultiOutputClassifier\",\n                   \"OutputCodeClassifier\", \"OneVsRestClassifier\",\n                   \"RFE\", \"RFECV\", \"BaseEnsemble\", \"ClassifierChain\",\n                   \"RegressorChain\"]\n# estimators that there is no way to default-construct sensibly\nOTHER = [\"Pipeline\", \"FeatureUnion\", \"GridSearchCV\", \"RandomizedSearchCV\",\n         \"SelectFromModel\", \"ColumnTransformer\"]\n\n# some strange ones\nDONT_TEST = ['SparseCoder', 'DictVectorizer',\n             'LabelBinarizer', 'LabelEncoder',\n             'MultiLabelBinarizer', 'TfidfTransformer',\n             'TfidfVectorizer', 'IsotonicRegression',\n             'OneHotEncoder', 'RandomTreesEmbedding', 'OrdinalEncoder',\n             'FeatureHasher', 'DummyClassifier', 'DummyRegressor',\n             'TruncatedSVD', 'PolynomialFeatures',\n             'GaussianRandomProjectionHash', 'HashingVectorizer',\n             'CheckingClassifier', 'PatchExtractor', 'CountVectorizer',\n             # GradientBoosting base estimators, maybe should\n             # exclude them in another way\n             'ZeroEstimator', 'ScaledLogOddsEstimator',\n             'QuantileEstimator', 'MeanEstimator',\n             'LogOddsEstimator', 'PriorProbabilityEstimator',\n             '_SigmoidCalibration', 'VotingClassifier']\n\n\ndef all_estimators(include_meta_estimators=False,\n                   include_other=False, type_filter=None,\n                   include_dont_test=False):\n    \"\"\"Get a list of all estimators from sklearn.\n\n    This function crawls the module and gets all classes that inherit\n    from BaseEstimator. Classes that are defined in test-modules are not\n    included.\n    By default meta_estimators such as GridSearchCV are also not included.\n\n    Parameters\n    ----------\n    include_meta_estimators : boolean, default=False\n        Whether to include meta-estimators that can be constructed using\n        an estimator as their first argument. These are currently\n        BaseEnsemble, OneVsOneClassifier, OutputCodeClassifier,\n        OneVsRestClassifier, RFE, RFECV.\n\n    include_other : boolean, default=False\n        Wether to include meta-estimators that are somehow special and can\n        not be default-constructed sensibly. These are currently\n        Pipeline, FeatureUnion and GridSearchCV\n\n    include_dont_test : boolean, default=False\n        Whether to include \"special\" label estimator or test processors.\n\n    type_filter : string, list of string,  or None, default=None\n        Which kind of estimators should be returned. If None, no filter is\n        applied and all estimators are returned.  Possible values are\n        'classifier', 'regressor', 'cluster' and 'transformer' to get\n        estimators only of these specific types, or a list of these to\n        get the estimators that fit at least one of the types.\n\n    Returns\n    -------\n    estimators : list of tuples\n        List of (name, class), where ``name`` is the class name as string\n        and ``class`` is the actuall type of the class.\n    \"\"\"\n    def is_abstract(c):\n        if not(hasattr(c, '__abstractmethods__')):\n            return False\n        if not len(c.__abstractmethods__):\n            return False\n        return True\n\n    all_classes = []\n    # get parent folder\n    path = sklearn.__path__\n    for importer, modname, ispkg in pkgutil.walk_packages(\n            path=path, prefix='sklearn.', onerror=lambda x: None):\n        if (\".tests.\" in modname):\n            continue\n        module = __import__(modname, fromlist=\"dummy\")\n        classes = inspect.getmembers(module, inspect.isclass)\n        all_classes.extend(classes)\n\n    all_classes = set(all_classes)\n\n    estimators = [c for c in all_classes\n                  if (issubclass(c[1], BaseEstimator) and\n                      c[0] != 'BaseEstimator')]\n    # get rid of abstract base classes\n    estimators = [c for c in estimators if not is_abstract(c[1])]\n\n    if not include_dont_test:\n        estimators = [c for c in estimators if not c[0] in DONT_TEST]\n\n    if not include_other:\n        estimators = [c for c in estimators if not c[0] in OTHER]\n    # possibly get rid of meta estimators\n    if not include_meta_estimators:\n        estimators = [c for c in estimators if not c[0] in META_ESTIMATORS]\n    if type_filter is not None:\n        if not isinstance(type_filter, list):\n            type_filter = [type_filter]\n        else:\n            type_filter = list(type_filter)  # copy\n        filtered_estimators = []\n        filters = {'classifier': ClassifierMixin,\n                   'regressor': RegressorMixin,\n                   'transformer': TransformerMixin,\n                   'cluster': ClusterMixin}\n        for name, mixin in filters.items():\n            if name in type_filter:\n                type_filter.remove(name)\n                filtered_estimators.extend([est for est in estimators\n                                            if issubclass(est[1], mixin)])\n        estimators = filtered_estimators\n        if type_filter:\n            raise ValueError(\"Parameter type_filter must be 'classifier', \"\n                             \"'regressor', 'transformer', 'cluster' or \"\n                             \"None, got\"\n                             \" %s.\" % repr(type_filter))\n\n    # drop duplicates, sort for reproducibility\n    # itemgetter is used to ensure the sort does not extend to the 2nd item of\n    # the tuple\n    return sorted(set(estimators), key=itemgetter(0))\n\n\ndef set_random_state(estimator, random_state=0):\n    \"\"\"Set random state of an estimator if it has the `random_state` param.\n    \"\"\"\n    if \"random_state\" in estimator.get_params():\n        estimator.set_params(random_state=random_state)\n\n\ndef if_matplotlib(func):\n    \"\"\"Test decorator that skips test if matplotlib not installed.\"\"\"\n    @wraps(func)\n    def run_test(*args, **kwargs):\n        try:\n            import matplotlib\n            matplotlib.use('Agg', warn=False)\n            # this fails if no $DISPLAY specified\n            import matplotlib.pyplot as plt\n            plt.figure()\n        except ImportError:\n            raise SkipTest('Matplotlib not available.')\n        else:\n            return func(*args, **kwargs)\n    return run_test\n\n\ntry:\n    import pytest\n\n    skip_if_32bit = pytest.mark.skipif(8 * struct.calcsize(\"P\") == 32,\n                                       reason='skipped on 32bit platforms')\n    skip_travis = pytest.mark.skipif(os.environ.get('TRAVIS') == 'true',\n                                     reason='skip on travis')\n\n    #  Decorator for tests involving both BLAS calls and multiprocessing.\n    #\n    #  Under POSIX (e.g. Linux or OSX), using multiprocessing in conjunction\n    #  with some implementation of BLAS (or other libraries that manage an\n    #  internal posix thread pool) can cause a crash or a freeze of the Python\n    #  process.\n    #\n    #  In practice all known packaged distributions (from Linux distros or\n    #  Anaconda) of BLAS under Linux seems to be safe. So we this problem seems\n    #  to only impact OSX users.\n    #\n    #  This wrapper makes it possible to skip tests that can possibly cause\n    #  this crash under OS X with.\n    #\n    #  Under Python 3.4+ it is possible to use the `forkserver` start method\n    #  for multiprocessing to avoid this issue. However it can cause pickling\n    #  errors on interactively defined functions. It therefore not enabled by\n    #  default.\n\n    if_safe_multiprocessing_with_blas = pytest.mark.skipif(\n            sys.platform == 'darwin',\n            reason=\"Possible multi-process bug with some BLAS\")\nexcept ImportError:\n    pass\n\n\ndef clean_warning_registry():\n    \"\"\"Clean Python warning registry for easier testing of warning messages.\n\n    We may not need to do this any more when getting rid of Python 2, not\n    entirely sure. See https://bugs.python.org/issue4180 and\n    https://bugs.python.org/issue21724 for more details.\n\n    \"\"\"\n    reg = \"__warningregistry__\"\n    for mod_name, mod in list(sys.modules.items()):\n        if 'six.moves' in mod_name:\n            continue\n        if hasattr(mod, reg):\n            getattr(mod, reg).clear()\n\n\ndef check_skip_network():\n    if int(os.environ.get('SKLEARN_SKIP_NETWORK_TESTS', 0)):\n        raise SkipTest(\"Text tutorial requires large dataset download\")\n\n\ndef _delete_folder(folder_path, warn=False):\n    \"\"\"Utility function to cleanup a temporary folder if still existing.\n\n    Copy from joblib.pool (for independence).\n    \"\"\"\n    try:\n        if os.path.exists(folder_path):\n            # This can fail under windows,\n            #  but will succeed when called by atexit\n            shutil.rmtree(folder_path)\n    except WindowsError:\n        if warn:\n            warnings.warn(\"Could not delete temporary folder %s\" % folder_path)\n\n\nclass TempMemmap(object):\n    def __init__(self, data, mmap_mode='r'):\n        self.mmap_mode = mmap_mode\n        self.data = data\n\n    def __enter__(self):\n        data_read_only, self.temp_folder = create_memmap_backed_data(\n            self.data, mmap_mode=self.mmap_mode, return_folder=True)\n        return data_read_only\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        _delete_folder(self.temp_folder)\n\n\ndef create_memmap_backed_data(data, mmap_mode='r', return_folder=False):\n    temp_folder = tempfile.mkdtemp(prefix='sklearn_testing_')\n    atexit.register(functools.partial(_delete_folder, temp_folder, warn=True))\n    filename = op.join(temp_folder, 'data.pkl')\n    joblib.dump(data, filename)\n    memmap_backed_data = joblib.load(filename, mmap_mode=mmap_mode)\n    result = (memmap_backed_data if not return_folder\n              else (memmap_backed_data, temp_folder))\n    return result\n\n\n# Utils to test docstrings\n\n\ndef _get_args(function, varargs=False):\n    \"\"\"Helper to get function arguments\"\"\"\n\n    try:\n        params = signature(function).parameters\n    except ValueError:\n        # Error on builtin C function\n        return []\n    args = [key for key, param in params.items()\n            if param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)]\n    if varargs:\n        varargs = [param.name for param in params.values()\n                   if param.kind == param.VAR_POSITIONAL]\n        if len(varargs) == 0:\n            varargs = None\n        return args, varargs\n    else:\n        return args\n\n\ndef _get_func_name(func, class_name=None):\n    \"\"\"Get function full name\n\n    Parameters\n    ----------\n    func : callable\n        The function object.\n    class_name : string, optional (default: None)\n       If ``func`` is a class method and the class name is known specify\n       class_name for the error message.\n\n    Returns\n    -------\n    name : str\n        The function name.\n    \"\"\"\n    parts = []\n    module = inspect.getmodule(func)\n    if module:\n        parts.append(module.__name__)\n    if class_name is not None:\n        parts.append(class_name)\n    elif hasattr(func, 'im_class'):\n        parts.append(func.im_class.__name__)\n\n    parts.append(func.__name__)\n    return '.'.join(parts)\n\n\ndef check_docstring_parameters(func, doc=None, ignore=None, class_name=None):\n    \"\"\"Helper to check docstring\n\n    Parameters\n    ----------\n    func : callable\n        The function object to test.\n    doc : str, optional (default: None)\n        Docstring if it is passed manually to the test.\n    ignore : None | list\n        Parameters to ignore.\n    class_name : string, optional (default: None)\n       If ``func`` is a class method and the class name is known specify\n       class_name for the error message.\n\n    Returns\n    -------\n    incorrect : list\n        A list of string describing the incorrect results.\n    \"\"\"\n    from numpydoc import docscrape\n    incorrect = []\n    ignore = [] if ignore is None else ignore\n\n    func_name = _get_func_name(func, class_name=class_name)\n    if (not func_name.startswith('sklearn.') or\n            func_name.startswith('sklearn.externals')):\n        return incorrect\n    # Don't check docstring for property-functions\n    if inspect.isdatadescriptor(func):\n        return incorrect\n    args = list(filter(lambda x: x not in ignore, _get_args(func)))\n    # drop self\n    if len(args) > 0 and args[0] == 'self':\n        args.remove('self')\n\n    if doc is None:\n        with warnings.catch_warnings(record=True) as w:\n            try:\n                doc = docscrape.FunctionDoc(func)\n            except Exception as exp:\n                incorrect += [func_name + ' parsing error: ' + str(exp)]\n                return incorrect\n        if len(w):\n            raise RuntimeError('Error for %s:\\n%s' % (func_name, w[0]))\n\n    param_names = []\n    for name, type_definition, param_doc in doc['Parameters']:\n        if not type_definition.strip():\n            if ':' in name and name[:name.index(':')][-1:].strip():\n                incorrect += [func_name +\n                              ' There was no space between the param name and '\n                              'colon (%r)' % name]\n            elif name.rstrip().endswith(':'):\n                incorrect += [func_name +\n                              ' Parameter %r has an empty type spec. '\n                              'Remove the colon' % (name.lstrip())]\n\n        if '*' not in name:\n            param_names.append(name.split(':')[0].strip('` '))\n\n    param_names = list(filter(lambda x: x not in ignore, param_names))\n\n    if len(param_names) != len(args):\n        bad = str(sorted(list(set(param_names) ^ set(args))))\n        incorrect += [func_name + ' arg mismatch: ' + bad]\n    else:\n        for n1, n2 in zip(param_names, args):\n            if n1 != n2:\n                incorrect += [func_name + ' ' + n1 + ' != ' + n2]\n    return incorrect\n"
    }
  ],
  "questions": [
    "Hi, can I try this?"
  ],
  "golden_answers": [
    "I'm having a small problem with the test  test_non_meta_estimators. It fail an assertion due to the fact that on the fit process I'm storing the OHE  and therfore `__dict__` change.\r\nCan someone help me with this? Should I do it in another way?\r\n```\r\n    @pytest.mark.parametrize(\r\n            \"name, Estimator, check\",\r\n            _generate_checks_per_estimator(_yield_all_checks,\r\n                                           _tested_non_meta_estimators()),\r\n            ids=_rename_partial\r\n    )\r\n    def test_non_meta_estimators(name, Estimator, check):\r\n        # Common tests for non-meta estimators\r\n        with ignore_warnings(category=(DeprecationWarning, ConvergenceWarning,\r\n                                       UserWarning, FutureWarning)):\r\n            estimator = Estimator()\r\n            set_checking_parameters(estimator)\r\n>           check(name, estimator)\r\n\r\nEstimator  = <class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>\r\ncheck      = <function check_dict_unchanged at 0x7f8287cf5620>\r\nestimator  = KBinsDiscretizer(encode='onehot', n_bins=5, strategy='quantile')\r\nname       = 'KBinsDiscretizer'\r\n\r\nsklearn/tests/test_common.py:99: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nsklearn/utils/testing.py:326: in wrapper\r\n    return fn(*args, **kwargs)\r\nsklearn/utils/estimator_checks.py:632: in check_dict_unchanged\r\n    'Estimator changes __dict__ during %s' % method)\r\n/usr/lib/python3.6/unittest/case.py:1121: in assertDictEqual\r\n    self.fail(self._formatMessage(msg, standardMsg))\r\n```"
  ],
  "questions_generated": [
    "What is the primary objective of the proposed change in the KBinsDiscretizer regarding the 'inverse_transform' method?",
    "Why does the KBinsDiscretizer implementation cause a test failure in 'test_non_meta_estimators' when storing the fitted encoder?",
    "What potential solutions could be considered to address the '__dict__' change in KBinsDiscretizer without causing test failures?",
    "How does the encoding strategy affect the transformation process in KBinsDiscretizer, and why is it important to support multiple encoding types?",
    "What are the challenges in implementing 'inverse_transform' for 'onehot' and 'onehot-dense' encoding in KBinsDiscretizer, and how might they be addressed?"
  ],
  "golden_answers_generated": [
    "The primary objective is to extend the 'inverse_transform' functionality in KBinsDiscretizer to support 'encode' options other than 'ordinal', specifically 'onehot' and 'onehot-dense'. This is consistent with the support provided in OneHotEncoder and would involve storing the fitted encoder as part of the solution.",
    "The test failure occurs because storing the fitted encoder changes the estimator's '__dict__', which leads to a failure in the 'check_dict_unchanged' test. This test ensures that fitting an estimator does not alter its '__dict__', which is a common invariant for scikit-learn estimators.",
    "One potential solution is to modify the test to allow certain changes in '__dict__' or to implement the encoder storage in such a way that it does not alter '__dict__'. Another approach could be to review and modify the 'check_dict_unchanged' test to account for acceptable changes when storing fitted components.",
    "The encoding strategy determines how the transformed data is represented. For example, 'onehot' and 'onehot-dense' strategies encode the data into binary or dense matrices, which are useful for certain machine learning models that require binary input. Supporting multiple encoding types allows users to choose the most suitable representation for their specific use case, enhancing the flexibility and usability of the KBinsDiscretizer.",
    "The challenges include ensuring that the inverse transformation accurately reconstructs the original data from the encoded format and handling edge cases such as missing data or unused bins. These can be addressed by carefully defining the inverse mapping logic, possibly leveraging the existing implementation in OneHotEncoder, and thoroughly testing with edge cases to ensure robustness."
  ]
}