{
  "repo_name": "scikit-learn_scikit-learn",
  "issue_id": "10252",
  "issue_description": "# validation_curve and learning_curve should support fit_params\n\nSimilar functions like cross_validation and cross_val_score support fit_params to pass sample_weight etc into the model being fitted. These other model selection routines should probably support the same.",
  "issue_comments": [
    {
      "id": 349165096,
      "user": "ido-sh",
      "body": "Hi @jnothman, can I give it a shot?"
    },
    {
      "id": 349172542,
      "user": "jnothman",
      "body": "Sure, have a go. Remember to include or update a test"
    },
    {
      "id": 349290163,
      "user": "ggc87",
      "body": "Hi @jnothman,\r\nI was checking this issue and I just saw that the tests for the validation_curve are in test_learning_curve, which is testing the deprecated implementation in learning_curve.py.\r\nShould this test moved in test_cross_validation where validation_curve is implemented now?"
    },
    {
      "id": 349302348,
      "user": "jnothman",
      "body": "don't worry about the deprecated module. tests should be in\nmodel_selection/tests\n"
    },
    {
      "id": 349308814,
      "user": "ggc87",
      "body": "Right, I didn't see that the tests are in the model_selection/tests folder, I just saw the test_validation.py in the main test folder. "
    },
    {
      "id": 350348298,
      "user": "gxyd",
      "body": "@ggc87 @ido-sh Is anyone here working on the issue? I have a pull request ready locally, but since you already asked to work on the issue, I'll wait if you still want to work on it."
    },
    {
      "id": 350370456,
      "user": "ido-sh",
      "body": "@gxyd if yours is ready then please feel free to go ahead"
    },
    {
      "id": 350664316,
      "user": "ggc87",
      "body": "@gxyd same here I was waiting for @ido-sh since he asked before."
    },
    {
      "id": 350708016,
      "user": "gxyd",
      "body": "I've close my PR."
    },
    {
      "id": 350711338,
      "user": "ggc87",
      "body": "@gxyd I'm sorry this has been a bit confusing.  You had no reason to close your PR, you can go ahead :)\r\n"
    },
    {
      "id": 351286598,
      "user": "akshkr",
      "body": "Can I take this?"
    },
    {
      "id": 351293363,
      "user": "jnothman",
      "body": "This is in progress.\n\nOn 13 December 2017 at 16:25, Akash <notifications@github.com> wrote:\n\n> Can I take this?\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/10252#issuecomment-351286598>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz66b-whp3Sv9T7Kz4nMO4C6ex3FzCks5s_1_SgaJpZM4Q1Y7O>\n> .\n>\n"
    },
    {
      "id": 357996731,
      "user": "uwaisiqbal",
      "body": "Has this been implemented? I wanted to use this for something I'm currently working on."
    },
    {
      "id": 357997700,
      "user": "gxyd",
      "body": "See https://github.com/scikit-learn/scikit-learn/pull/10273 . I need to address a few comments before it becomes available."
    },
    {
      "id": 402963849,
      "user": "littlewine",
      "body": "What is the status of this item? \r\nIn case I am not mistaken, the code is ready and awaiting to be merged in the master branch, right?\r\n\r\nCan I use the code already by updating the code from [here](https://github.com/gxyd/scikit-learn/blob/777aa8e0b16357987ef3b840cb78787804901e6a/sklearn/model_selection/_validation.py)?\r\n\r\nThanks in advance for your answer and sorry if I am not supposed to post such questions here :)"
    }
  ],
  "text_context": "# validation_curve and learning_curve should support fit_params\n\nSimilar functions like cross_validation and cross_val_score support fit_params to pass sample_weight etc into the model being fitted. These other model selection routines should probably support the same.\n\nHi @jnothman, can I give it a shot?\n\nSure, have a go. Remember to include or update a test\n\nHi @jnothman,\r\nI was checking this issue and I just saw that the tests for the validation_curve are in test_learning_curve, which is testing the deprecated implementation in learning_curve.py.\r\nShould this test moved in test_cross_validation where validation_curve is implemented now?\n\ndon't worry about the deprecated module. tests should be in\nmodel_selection/tests\n\n\nRight, I didn't see that the tests are in the model_selection/tests folder, I just saw the test_validation.py in the main test folder. \n\n@ggc87 @ido-sh Is anyone here working on the issue? I have a pull request ready locally, but since you already asked to work on the issue, I'll wait if you still want to work on it.\n\n@gxyd if yours is ready then please feel free to go ahead\n\n@gxyd same here I was waiting for @ido-sh since he asked before.\n\nI've close my PR.\n\n@gxyd I'm sorry this has been a bit confusing.  You had no reason to close your PR, you can go ahead :)\r\n\n\nCan I take this?\n\nThis is in progress.\n\nOn 13 December 2017 at 16:25, Akash <notifications@github.com> wrote:\n\n> Can I take this?\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/10252#issuecomment-351286598>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz66b-whp3Sv9T7Kz4nMO4C6ex3FzCks5s_1_SgaJpZM4Q1Y7O>\n> .\n>\n\n\nHas this been implemented? I wanted to use this for something I'm currently working on.\n\nSee https://github.com/scikit-learn/scikit-learn/pull/10273 . I need to address a few comments before it becomes available.\n\nWhat is the status of this item? \r\nIn case I am not mistaken, the code is ready and awaiting to be merged in the master branch, right?\r\n\r\nCan I use the code already by updating the code from [here](https://github.com/gxyd/scikit-learn/blob/777aa8e0b16357987ef3b840cb78787804901e6a/sklearn/model_selection/_validation.py)?\r\n\r\nThanks in advance for your answer and sorry if I am not supposed to post such questions here :)",
  "pr_link": "https://github.com/scikit-learn/scikit-learn/pull/10273",
  "code_context": [
    {
      "filename": "sklearn/model_selection/_validation.py",
      "content": "\"\"\"\nThe :mod:`sklearn.model_selection._validation` module includes classes and\nfunctions to validate the model.\n\"\"\"\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#         Gael Varoquaux <gael.varoquaux@normalesup.org>\n#         Olivier Grisel <olivier.grisel@ensta.org>\n#         Raghav RV <rvraghav93@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport warnings\nimport numbers\nimport time\nfrom traceback import format_exception_only\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom ..base import is_classifier, clone\nfrom ..utils import indexable, check_random_state, safe_indexing\nfrom ..utils.deprecation import DeprecationDict\nfrom ..utils.validation import _is_arraylike, _num_samples\nfrom ..utils.metaestimators import _safe_split\nfrom ..externals.joblib import Parallel, delayed, logger\nfrom ..externals.six.moves import zip\nfrom ..metrics.scorer import check_scoring, _check_multimetric_scoring\nfrom ..exceptions import FitFailedWarning\nfrom ._split import check_cv\nfrom ..preprocessing import LabelEncoder\n\n\n__all__ = ['cross_validate', 'cross_val_score', 'cross_val_predict',\n           'permutation_test_score', 'learning_curve', 'validation_curve']\n\n\ndef cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,\n                   n_jobs=1, verbose=0, fit_params=None,\n                   pre_dispatch='2*n_jobs', return_train_score=\"warn\",\n                   return_estimator=False):\n    \"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\n\n    Read more in the :ref:`User Guide <multimetric_cross_validation>`.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit'\n        The object to use to fit the data.\n\n    X : array-like\n        The data to fit. Can be for example a list, or an array.\n\n    y : array-like, optional, default: None\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    groups : array-like, with shape (n_samples,), optional\n        Group labels for the samples used while splitting the dataset into\n        train/test set.\n\n    scoring : string, callable, list/tuple, dict or None, default: None\n        A single string (see :ref:`scoring_parameter`) or a callable\n        (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n        For evaluating multiple metrics, either give a list of (unique) strings\n        or a dict with names as keys and callables as values.\n\n        NOTE that when using custom scorers, each scorer should return a single\n        value. Metric functions returning a list/array of values can be wrapped\n        into multiple scorers that return one value each.\n\n        See :ref:`multimetric_grid_search` for an example.\n\n        If None, the estimator's default scorer (if available) is used.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - An object to be used as a cross-validation generator.\n        - An iterable yielding train, test splits.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    n_jobs : integer, optional\n        The number of CPUs to use to do the computation. -1 means\n        'all CPUs'.\n\n    verbose : integer, optional\n        The verbosity level.\n\n    fit_params : dict, optional\n        Parameters to pass to the fit method of the estimator.\n\n    pre_dispatch : int, or string, optional\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A string, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    return_train_score : boolean, optional\n        Whether to include train scores.\n\n        Current default is ``'warn'``, which behaves as ``True`` in addition\n        to raising a warning when a training score is looked up.\n        That default will be changed to ``False`` in 0.21.\n        Computing training scores is used to get insights on how different\n        parameter settings impact the overfitting/underfitting trade-off.\n        However computing the scores on the training set can be computationally\n        expensive and is not strictly required to select the parameters that\n        yield the best generalization performance.\n\n    return_estimator : boolean, default False\n        Whether to return the estimators fitted on each split.\n\n    Returns\n    -------\n    scores : dict of float arrays of shape=(n_splits,)\n        Array of scores of the estimator for each run of the cross validation.\n\n        A dict of arrays containing the score/time arrays for each scorer is\n        returned. The possible keys for this ``dict`` are:\n\n            ``test_score``\n                The score array for test scores on each cv split.\n            ``train_score``\n                The score array for train scores on each cv split.\n                This is available only if ``return_train_score`` parameter\n                is ``True``.\n            ``fit_time``\n                The time for fitting the estimator on the train\n                set for each cv split.\n            ``score_time``\n                The time for scoring the estimator on the test set for each\n                cv split. (Note time for scoring on the train set is not\n                included even if ``return_train_score`` is set to ``True``\n            ``estimator``\n                The estimator objects for each cv split.\n                This is available only if ``return_estimator`` parameter\n                is set to ``True``.\n\n    Examples\n    --------\n    >>> from sklearn import datasets, linear_model\n    >>> from sklearn.model_selection import cross_validate\n    >>> from sklearn.metrics.scorer import make_scorer\n    >>> from sklearn.metrics import confusion_matrix\n    >>> from sklearn.svm import LinearSVC\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n\n    Single metric evaluation using ``cross_validate``\n\n    >>> cv_results = cross_validate(lasso, X, y, return_train_score=False)\n    >>> sorted(cv_results.keys())                         # doctest: +ELLIPSIS\n    ['fit_time', 'score_time', 'test_score']\n    >>> cv_results['test_score']    # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n    array([0.33150734, 0.08022311, 0.03531764])\n\n    Multiple metric evaluation using ``cross_validate``\n    (please refer the ``scoring`` parameter doc for more information)\n\n    >>> scores = cross_validate(lasso, X, y,\n    ...                         scoring=('r2', 'neg_mean_squared_error'))\n    >>> print(scores['test_neg_mean_squared_error'])      # doctest: +ELLIPSIS\n    [-3635.5... -3573.3... -6114.7...]\n    >>> print(scores['train_r2'])                         # doctest: +ELLIPSIS\n    [0.28010158 0.39088426 0.22784852]\n\n    See Also\n    ---------\n    :func:`sklearn.model_selection.cross_val_score`:\n        Run cross-validation for single metric evaluation.\n\n    :func:`sklearn.model_selection.cross_val_predict`:\n        Get predictions from each split of cross-validation for diagnostic\n        purposes.\n\n    :func:`sklearn.metrics.make_scorer`:\n        Make a scorer from a performance metric or loss function.\n\n    \"\"\"\n    X, y, groups = indexable(X, y, groups)\n\n    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n    scorers, _ = _check_multimetric_scoring(estimator, scoring=scoring)\n\n    # We clone the estimator to make sure that all the folds are\n    # independent, and that it is pickle-able.\n    parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n                        pre_dispatch=pre_dispatch)\n    scores = parallel(\n        delayed(_fit_and_score)(\n            clone(estimator), X, y, scorers, train, test, verbose, None,\n            fit_params, return_train_score=return_train_score,\n            return_times=True, return_estimator=return_estimator)\n        for train, test in cv.split(X, y, groups))\n\n    zipped_scores = list(zip(*scores))\n    if return_train_score:\n        train_scores = zipped_scores.pop(0)\n        train_scores = _aggregate_score_dicts(train_scores)\n    if return_estimator:\n        fitted_estimators = zipped_scores.pop()\n    test_scores, fit_times, score_times = zipped_scores\n    test_scores = _aggregate_score_dicts(test_scores)\n\n    # TODO: replace by a dict in 0.21\n    ret = DeprecationDict() if return_train_score == 'warn' else {}\n    ret['fit_time'] = np.array(fit_times)\n    ret['score_time'] = np.array(score_times)\n\n    if return_estimator:\n        ret['estimator'] = fitted_estimators\n\n    for name in scorers:\n        ret['test_%s' % name] = np.array(test_scores[name])\n        if return_train_score:\n            key = 'train_%s' % name\n            ret[key] = np.array(train_scores[name])\n            if return_train_score == 'warn':\n                message = (\n                    'You are accessing a training score ({!r}), '\n                    'which will not be available by default '\n                    'any more in 0.21. If you need training scores, '\n                    'please set return_train_score=True').format(key)\n                # warn on key access\n                ret.add_warning(key, message, FutureWarning)\n\n    return ret\n\n\ndef cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,\n                    n_jobs=1, verbose=0, fit_params=None,\n                    pre_dispatch='2*n_jobs'):\n    \"\"\"Evaluate a score by cross-validation\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit'\n        The object to use to fit the data.\n\n    X : array-like\n        The data to fit. Can be for example a list, or an array.\n\n    y : array-like, optional, default: None\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    groups : array-like, with shape (n_samples,), optional\n        Group labels for the samples used while splitting the dataset into\n        train/test set.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - An object to be used as a cross-validation generator.\n        - An iterable yielding train, test splits.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    n_jobs : integer, optional\n        The number of CPUs to use to do the computation. -1 means\n        'all CPUs'.\n\n    verbose : integer, optional\n        The verbosity level.\n\n    fit_params : dict, optional\n        Parameters to pass to the fit method of the estimator.\n\n    pre_dispatch : int, or string, optional\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A string, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    Returns\n    -------\n    scores : array of float, shape=(len(list(cv)),)\n        Array of scores of the estimator for each run of the cross validation.\n\n    Examples\n    --------\n    >>> from sklearn import datasets, linear_model\n    >>> from sklearn.model_selection import cross_val_score\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n    >>> print(cross_val_score(lasso, X, y))  # doctest: +ELLIPSIS\n    [0.33150734 0.08022311 0.03531764]\n\n    See Also\n    ---------\n    :func:`sklearn.model_selection.cross_validate`:\n        To run cross-validation on multiple metrics and also to return\n        train scores, fit times and score times.\n\n    :func:`sklearn.model_selection.cross_val_predict`:\n        Get predictions from each split of cross-validation for diagnostic\n        purposes.\n\n    :func:`sklearn.metrics.make_scorer`:\n        Make a scorer from a performance metric or loss function.\n\n    \"\"\"\n    # To ensure multimetric format is not supported\n    scorer = check_scoring(estimator, scoring=scoring)\n\n    cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n                                scoring={'score': scorer}, cv=cv,\n                                return_train_score=False,\n                                n_jobs=n_jobs, verbose=verbose,\n                                fit_params=fit_params,\n                                pre_dispatch=pre_dispatch)\n    return cv_results['test_score']\n\n\ndef _fit_and_score(estimator, X, y, scorer, train, test, verbose,\n                   parameters, fit_params, return_train_score=False,\n                   return_parameters=False, return_n_test_samples=False,\n                   return_times=False, return_estimator=False,\n                   error_score='raise-deprecating'):\n    \"\"\"Fit estimator and compute scores for a given dataset split.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit'\n        The object to use to fit the data.\n\n    X : array-like of shape at least 2D\n        The data to fit.\n\n    y : array-like, optional, default: None\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    scorer : A single callable or dict mapping scorer name to the callable\n        If it is a single callable, the return value for ``train_scores`` and\n        ``test_scores`` is a single float.\n\n        For a dict, it should be one mapping the scorer name to the scorer\n        callable object / function.\n\n        The callable object / fn should have signature\n        ``scorer(estimator, X, y)``.\n\n    train : array-like, shape (n_train_samples,)\n        Indices of training samples.\n\n    test : array-like, shape (n_test_samples,)\n        Indices of test samples.\n\n    verbose : integer\n        The verbosity level.\n\n    error_score : 'raise' or numeric\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised. If a numeric value is given,\n        FitFailedWarning is raised. This parameter does not affect the refit\n        step, which will always raise the error. Default is 'raise' but from\n        version 0.22 it will change to np.nan.\n\n    parameters : dict or None\n        Parameters to be set on the estimator.\n\n    fit_params : dict or None\n        Parameters that will be passed to ``estimator.fit``.\n\n    return_train_score : boolean, optional, default: False\n        Compute and return score on training set.\n\n    return_parameters : boolean, optional, default: False\n        Return parameters that has been used for the estimator.\n\n    return_n_test_samples : boolean, optional, default: False\n        Whether to return the ``n_test_samples``\n\n    return_times : boolean, optional, default: False\n        Whether to return the fit/score times.\n\n    return_estimator : boolean, optional, default: False\n        Whether to return the fitted estimator.\n\n    Returns\n    -------\n    train_scores : dict of scorer name -> float, optional\n        Score on training set (for all the scorers),\n        returned only if `return_train_score` is `True`.\n\n    test_scores : dict of scorer name -> float, optional\n        Score on testing set (for all the scorers).\n\n    n_test_samples : int\n        Number of test samples.\n\n    fit_time : float\n        Time spent for fitting in seconds.\n\n    score_time : float\n        Time spent for scoring in seconds.\n\n    parameters : dict or None, optional\n        The parameters that have been evaluated.\n\n    estimator : estimator object\n        The fitted estimator\n    \"\"\"\n    if verbose > 1:\n        if parameters is None:\n            msg = ''\n        else:\n            msg = '%s' % (', '.join('%s=%s' % (k, v)\n                          for k, v in parameters.items()))\n        print(\"[CV] %s %s\" % (msg, (64 - len(msg)) * '.'))\n\n    # Adjust length of sample weights\n    fit_params = fit_params if fit_params is not None else {}\n    fit_params = dict([(k, _index_param_value(X, v, train))\n                      for k, v in fit_params.items()])\n\n    train_scores = {}\n    if parameters is not None:\n        estimator.set_params(**parameters)\n\n    start_time = time.time()\n\n    X_train, y_train = _safe_split(estimator, X, y, train)\n    X_test, y_test = _safe_split(estimator, X, y, test, train)\n\n    is_multimetric = not callable(scorer)\n    n_scorers = len(scorer.keys()) if is_multimetric else 1\n\n    try:\n        if y_train is None:\n            estimator.fit(X_train, **fit_params)\n        else:\n            estimator.fit(X_train, y_train, **fit_params)\n\n    except Exception as e:\n        # Note fit time as time until error\n        fit_time = time.time() - start_time\n        score_time = 0.0\n        if error_score == 'raise':\n            raise\n        elif error_score == 'raise-deprecating':\n            warnings.warn(\"From version 0.22, errors during fit will result \"\n                          \"in a cross validation score of NaN by default. Use \"\n                          \"error_score='raise' if you want an exception \"\n                          \"raised or error_score=np.nan to adopt the \"\n                          \"behavior from version 0.22.\",\n                          FutureWarning)\n            raise\n        elif isinstance(error_score, numbers.Number):\n            if is_multimetric:\n                test_scores = dict(zip(scorer.keys(),\n                                   [error_score, ] * n_scorers))\n                if return_train_score:\n                    train_scores = dict(zip(scorer.keys(),\n                                        [error_score, ] * n_scorers))\n            else:\n                test_scores = error_score\n                if return_train_score:\n                    train_scores = error_score\n            warnings.warn(\"Estimator fit failed. The score on this train-test\"\n                          \" partition for these parameters will be set to %f. \"\n                          \"Details: \\n%s\" %\n                          (error_score, format_exception_only(type(e), e)[0]),\n                          FitFailedWarning)\n        else:\n            raise ValueError(\"error_score must be the string 'raise' or a\"\n                             \" numeric value. (Hint: if using 'raise', please\"\n                             \" make sure that it has been spelled correctly.)\")\n\n    else:\n        fit_time = time.time() - start_time\n        # _score will return dict if is_multimetric is True\n        test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)\n        score_time = time.time() - start_time - fit_time\n        if return_train_score:\n            train_scores = _score(estimator, X_train, y_train, scorer,\n                                  is_multimetric)\n\n    if verbose > 2:\n        if is_multimetric:\n            for scorer_name, score in test_scores.items():\n                msg += \", %s=%s\" % (scorer_name, score)\n        else:\n            msg += \", score=%s\" % test_scores\n    if verbose > 1:\n        total_time = score_time + fit_time\n        end_msg = \"%s, total=%s\" % (msg, logger.short_format_time(total_time))\n        print(\"[CV] %s %s\" % ((64 - len(end_msg)) * '.', end_msg))\n\n    ret = [train_scores, test_scores] if return_train_score else [test_scores]\n\n    if return_n_test_samples:\n        ret.append(_num_samples(X_test))\n    if return_times:\n        ret.extend([fit_time, score_time])\n    if return_parameters:\n        ret.append(parameters)\n    if return_estimator:\n        ret.append(estimator)\n    return ret\n\n\ndef _score(estimator, X_test, y_test, scorer, is_multimetric=False):\n    \"\"\"Compute the score(s) of an estimator on a given test set.\n\n    Will return a single float if is_multimetric is False and a dict of floats,\n    if is_multimetric is True\n    \"\"\"\n    if is_multimetric:\n        return _multimetric_score(estimator, X_test, y_test, scorer)\n    else:\n        if y_test is None:\n            score = scorer(estimator, X_test)\n        else:\n            score = scorer(estimator, X_test, y_test)\n\n        if hasattr(score, 'item'):\n            try:\n                # e.g. unwrap memmapped scalars\n                score = score.item()\n            except ValueError:\n                # non-scalar?\n                pass\n\n        if not isinstance(score, numbers.Number):\n            raise ValueError(\"scoring must return a number, got %s (%s) \"\n                             \"instead. (scorer=%r)\"\n                             % (str(score), type(score), scorer))\n    return score\n\n\ndef _multimetric_score(estimator, X_test, y_test, scorers):\n    \"\"\"Return a dict of score for multimetric scoring\"\"\"\n    scores = {}\n\n    for name, scorer in scorers.items():\n        if y_test is None:\n            score = scorer(estimator, X_test)\n        else:\n            score = scorer(estimator, X_test, y_test)\n\n        if hasattr(score, 'item'):\n            try:\n                # e.g. unwrap memmapped scalars\n                score = score.item()\n            except ValueError:\n                # non-scalar?\n                pass\n        scores[name] = score\n\n        if not isinstance(score, numbers.Number):\n            raise ValueError(\"scoring must return a number, got %s (%s) \"\n                             \"instead. (scorer=%s)\"\n                             % (str(score), type(score), name))\n    return scores\n\n\ndef cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1,\n                      verbose=0, fit_params=None, pre_dispatch='2*n_jobs',\n                      method='predict'):\n    \"\"\"Generate cross-validated estimates for each input data point\n\n    It is not appropriate to pass these predictions into an evaluation\n    metric. Use :func:`cross_validate` to measure generalization error.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit' and 'predict'\n        The object to use to fit the data.\n\n    X : array-like\n        The data to fit. Can be, for example a list, or an array at least 2d.\n\n    y : array-like, optional, default: None\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    groups : array-like, with shape (n_samples,), optional\n        Group labels for the samples used while splitting the dataset into\n        train/test set.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - An object to be used as a cross-validation generator.\n        - An iterable yielding train, test splits.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    n_jobs : integer, optional\n        The number of CPUs to use to do the computation. -1 means\n        'all CPUs'.\n\n    verbose : integer, optional\n        The verbosity level.\n\n    fit_params : dict, optional\n        Parameters to pass to the fit method of the estimator.\n\n    pre_dispatch : int, or string, optional\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A string, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    method : string, optional, default: 'predict'\n        Invokes the passed method name of the passed estimator. For\n        method='predict_proba', the columns correspond to the classes\n        in sorted order.\n\n    Returns\n    -------\n    predictions : ndarray\n        This is the result of calling ``method``\n\n    See also\n    --------\n    cross_val_score : calculate score for each CV split\n\n    cross_validate : calculate one or more scores and timings for each CV split\n\n    Notes\n    -----\n    In the case that one or more classes are absent in a training portion, a\n    default score needs to be assigned to all instances for that class if\n    ``method`` produces columns per class, as in {'decision_function',\n    'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is\n    0.  In order to ensure finite output, we approximate negative infinity by\n    the minimum finite float value for the dtype in other cases.\n\n    Examples\n    --------\n    >>> from sklearn import datasets, linear_model\n    >>> from sklearn.model_selection import cross_val_predict\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n    >>> y_pred = cross_val_predict(lasso, X, y)\n    \"\"\"\n    X, y, groups = indexable(X, y, groups)\n\n    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n\n    if method in ['decision_function', 'predict_proba', 'predict_log_proba']:\n        le = LabelEncoder()\n        y = le.fit_transform(y)\n\n    # We clone the estimator to make sure that all the folds are\n    # independent, and that it is pickle-able.\n    parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n                        pre_dispatch=pre_dispatch)\n    prediction_blocks = parallel(delayed(_fit_and_predict)(\n        clone(estimator), X, y, train, test, verbose, fit_params, method)\n        for train, test in cv.split(X, y, groups))\n\n    # Concatenate the predictions\n    predictions = [pred_block_i for pred_block_i, _ in prediction_blocks]\n    test_indices = np.concatenate([indices_i\n                                   for _, indices_i in prediction_blocks])\n\n    if not _check_is_permutation(test_indices, _num_samples(X)):\n        raise ValueError('cross_val_predict only works for partitions')\n\n    inv_test_indices = np.empty(len(test_indices), dtype=int)\n    inv_test_indices[test_indices] = np.arange(len(test_indices))\n\n    # Check for sparse predictions\n    if sp.issparse(predictions[0]):\n        predictions = sp.vstack(predictions, format=predictions[0].format)\n    else:\n        predictions = np.concatenate(predictions)\n    return predictions[inv_test_indices]\n\n\ndef _fit_and_predict(estimator, X, y, train, test, verbose, fit_params,\n                     method):\n    \"\"\"Fit estimator and predict values for a given dataset split.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit' and 'predict'\n        The object to use to fit the data.\n\n    X : array-like of shape at least 2D\n        The data to fit.\n\n    y : array-like, optional, default: None\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    train : array-like, shape (n_train_samples,)\n        Indices of training samples.\n\n    test : array-like, shape (n_test_samples,)\n        Indices of test samples.\n\n    verbose : integer\n        The verbosity level.\n\n    fit_params : dict or None\n        Parameters that will be passed to ``estimator.fit``.\n\n    method : string\n        Invokes the passed method name of the passed estimator.\n\n    Returns\n    -------\n    predictions : sequence\n        Result of calling 'estimator.method'\n\n    test : array-like\n        This is the value of the test parameter\n    \"\"\"\n    # Adjust length of sample weights\n    fit_params = fit_params if fit_params is not None else {}\n    fit_params = dict([(k, _index_param_value(X, v, train))\n                      for k, v in fit_params.items()])\n\n    X_train, y_train = _safe_split(estimator, X, y, train)\n    X_test, _ = _safe_split(estimator, X, y, test, train)\n\n    if y_train is None:\n        estimator.fit(X_train, **fit_params)\n    else:\n        estimator.fit(X_train, y_train, **fit_params)\n    func = getattr(estimator, method)\n    predictions = func(X_test)\n    if method in ['decision_function', 'predict_proba', 'predict_log_proba']:\n        n_classes = len(set(y))\n        if n_classes != len(estimator.classes_):\n            recommendation = (\n                'To fix this, use a cross-validation '\n                'technique resulting in properly '\n                'stratified folds')\n            warnings.warn('Number of classes in training fold ({}) does '\n                          'not match total number of classes ({}). '\n                          'Results may not be appropriate for your use case. '\n                          '{}'.format(len(estimator.classes_),\n                                      n_classes, recommendation),\n                          RuntimeWarning)\n            if method == 'decision_function':\n                if (predictions.ndim == 2 and\n                        predictions.shape[1] != len(estimator.classes_)):\n                    # This handles the case when the shape of predictions\n                    # does not match the number of classes used to train\n                    # it with. This case is found when sklearn.svm.SVC is\n                    # set to `decision_function_shape='ovo'`.\n                    raise ValueError('Output shape {} of {} does not match '\n                                     'number of classes ({}) in fold. '\n                                     'Irregular decision_function outputs '\n                                     'are not currently supported by '\n                                     'cross_val_predict'.format(\n                                        predictions.shape, method,\n                                        len(estimator.classes_),\n                                        recommendation))\n                if len(estimator.classes_) <= 2:\n                    # In this special case, `predictions` contains a 1D array.\n                    raise ValueError('Only {} class/es in training fold, this '\n                                     'is not supported for decision_function '\n                                     'with imbalanced folds. {}'.format(\n                                        len(estimator.classes_),\n                                        recommendation))\n\n            float_min = np.finfo(predictions.dtype).min\n            default_values = {'decision_function': float_min,\n                              'predict_log_proba': float_min,\n                              'predict_proba': 0}\n            predictions_for_all_classes = np.full((_num_samples(predictions),\n                                                   n_classes),\n                                                  default_values[method])\n            predictions_for_all_classes[:, estimator.classes_] = predictions\n            predictions = predictions_for_all_classes\n    return predictions, test\n\n\ndef _check_is_permutation(indices, n_samples):\n    \"\"\"Check whether indices is a reordering of the array np.arange(n_samples)\n\n    Parameters\n    ----------\n    indices : ndarray\n        integer array to test\n    n_samples : int\n        number of expected elements\n\n    Returns\n    -------\n    is_partition : bool\n        True iff sorted(indices) is np.arange(n)\n    \"\"\"\n    if len(indices) != n_samples:\n        return False\n    hit = np.zeros(n_samples, dtype=bool)\n    hit[indices] = True\n    if not np.all(hit):\n        return False\n    return True\n\n\ndef _index_param_value(X, v, indices):\n    \"\"\"Private helper function for parameter value indexing.\"\"\"\n    if not _is_arraylike(v) or _num_samples(v) != _num_samples(X):\n        # pass through: skip indexing\n        return v\n    if sp.issparse(v):\n        v = v.tocsr()\n    return safe_indexing(v, indices)\n\n\ndef permutation_test_score(estimator, X, y, groups=None, cv=None,\n                           n_permutations=100, n_jobs=1, random_state=0,\n                           verbose=0, scoring=None, fit_params=None):\n    \"\"\"Evaluate the significance of a cross-validated score with permutations\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit'\n        The object to use to fit the data.\n\n    X : array-like of shape at least 2D\n        The data to fit.\n\n    y : array-like\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    groups : array-like, with shape (n_samples,), optional\n        Labels to constrain permutation within groups, i.e. ``y`` values\n        are permuted among samples with the same group identifier.\n        When not specified, ``y`` values are permuted among all samples.\n\n        When a grouped cross-validator is used, the group labels are\n        also passed on to the ``split`` method of the cross-validator. The\n        cross-validator uses them for grouping the samples  while splitting\n        the dataset into train/test set.\n\n    scoring : string, callable or None, optional, default: None\n        A single string (see :ref:`scoring_parameter`) or a callable\n        (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n        If None the estimator's default scorer, if available, is used.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - An object to be used as a cross-validation generator.\n        - An iterable yielding train, test splits.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    n_permutations : integer, optional\n        Number of times to permute ``y``.\n\n    n_jobs : integer, optional\n        The number of CPUs to use to do the computation. -1 means\n        'all CPUs'.\n\n    random_state : int, RandomState instance or None, optional (default=0)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    verbose : integer, optional\n        The verbosity level.\n\n    fit_params : dict, optional\n        Parameters to pass to the fit method of the estimator.\n\n    Returns\n    -------\n    score : float\n        The true score without permuting targets.\n\n    permutation_scores : array, shape (n_permutations,)\n        The scores obtained for each permutations.\n\n    pvalue : float\n        The p-value, which approximates the probability that the score would\n        be obtained by chance. This is calculated as:\n\n        `(C + 1) / (n_permutations + 1)`\n\n        Where C is the number of permutations whose score >= the true score.\n\n        The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.\n\n    Notes\n    -----\n    This function implements Test 1 in:\n\n        Ojala and Garriga. Permutation Tests for Studying Classifier\n        Performance.  The Journal of Machine Learning Research (2010)\n        vol. 11\n\n    \"\"\"\n    X, y, groups = indexable(X, y, groups)\n\n    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n    scorer = check_scoring(estimator, scoring=scoring)\n    random_state = check_random_state(random_state)\n\n    # We clone the estimator to make sure that all the folds are\n    # independent, and that it is pickle-able.\n    score = _permutation_test_score(clone(estimator), X, y, groups, cv, scorer,\n                                    fit_params=fit_params)\n    permutation_scores = Parallel(n_jobs=n_jobs, verbose=verbose)(\n        delayed(_permutation_test_score)(\n            clone(estimator), X, _shuffle(y, groups, random_state),\n            groups, cv, scorer, fit_params=fit_params)\n        for _ in range(n_permutations))\n    permutation_scores = np.array(permutation_scores)\n    pvalue = (np.sum(permutation_scores >= score) + 1.0) / (n_permutations + 1)\n    return score, permutation_scores, pvalue\n\n\ndef _permutation_test_score(estimator, X, y, groups, cv, scorer,\n                            fit_params):\n    \"\"\"Auxiliary function for permutation_test_score\"\"\"\n    # Adjust length of sample weights\n    fit_params = fit_params if fit_params is not None else {}\n    avg_score = []\n    for train, test in cv.split(X, y, groups):\n        X_train, y_train = _safe_split(estimator, X, y, train)\n        X_test, y_test = _safe_split(estimator, X, y, test, train)\n        train_fit_params = dict([(k, _index_param_value(X, v, train))\n                                 for k, v in fit_params.items()])\n        estimator.fit(X_train, y_train, **train_fit_params)\n        avg_score.append(scorer(estimator, X_test, y_test))\n    return np.mean(avg_score)\n\n\ndef _shuffle(y, groups, random_state):\n    \"\"\"Return a shuffled copy of y eventually shuffle among same groups.\"\"\"\n    if groups is None:\n        indices = random_state.permutation(len(y))\n    else:\n        indices = np.arange(len(groups))\n        for group in np.unique(groups):\n            this_mask = (groups == group)\n            indices[this_mask] = random_state.permutation(indices[this_mask])\n    return safe_indexing(y, indices)\n\n\ndef learning_curve(estimator, X, y, groups=None,\n                   train_sizes=np.linspace(0.1, 1.0, 5), cv=None, scoring=None,\n                   exploit_incremental_learning=False, n_jobs=1,\n                   pre_dispatch=\"all\", verbose=0, shuffle=False,\n                   random_state=None, fit_params=None):\n    \"\"\"Learning curve.\n\n    Determines cross-validated training and test scores for different training\n    set sizes.\n\n    A cross-validation generator splits the whole dataset k times in training\n    and test data. Subsets of the training set with varying sizes will be used\n    to train the estimator and a score for each training subset size and the\n    test set will be computed. Afterwards, the scores will be averaged over\n    all k runs for each training subset size.\n\n    Read more in the :ref:`User Guide <learning_curve>`.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    groups : array-like, with shape (n_samples,), optional\n        Group labels for the samples used while splitting the dataset into\n        train/test set.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - An object to be used as a cross-validation generator.\n        - An iterable yielding train, test splits.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    exploit_incremental_learning : boolean, optional, default: False\n        If the estimator supports incremental learning, this will be\n        used to speed up fitting for different training set sizes.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n\n    pre_dispatch : integer or string, optional\n        Number of predispatched jobs for parallel execution (default is\n        all). The option can reduce the allocated memory. The string can\n        be an expression like '2*n_jobs'.\n\n    verbose : integer, optional\n        Controls the verbosity: the higher, the more messages.\n\n    shuffle : boolean, optional\n        Whether to shuffle training data before taking prefixes of it\n        based on``train_sizes``.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``shuffle`` is True.\n\n    fit_params : dict, optional\n        Parameters to pass to the fit method of the estimator.\n\n    Returns\n    -------\n    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\n        Numbers of training examples that has been used to generate the\n        learning curve. Note that the number of ticks might be less\n        than n_ticks because duplicate entries will be removed.\n\n    train_scores : array, shape (n_ticks, n_cv_folds)\n        Scores on training sets.\n\n    test_scores : array, shape (n_ticks, n_cv_folds)\n        Scores on test set.\n\n    Notes\n    -----\n    See :ref:`examples/model_selection/plot_learning_curve.py\n    <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`\n    \"\"\"\n    if exploit_incremental_learning and not hasattr(estimator, \"partial_fit\"):\n        raise ValueError(\"An estimator must support the partial_fit interface \"\n                         \"to exploit incremental learning\")\n    X, y, groups = indexable(X, y, groups)\n\n    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n    # Store it as list as we will be iterating over the list multiple times\n    cv_iter = list(cv.split(X, y, groups))\n\n    scorer = check_scoring(estimator, scoring=scoring)\n\n    n_max_training_samples = len(cv_iter[0][0])\n    # Because the lengths of folds can be significantly different, it is\n    # not guaranteed that we use all of the available training data when we\n    # use the first 'n_max_training_samples' samples.\n    train_sizes_abs = _translate_train_sizes(train_sizes,\n                                             n_max_training_samples)\n    n_unique_ticks = train_sizes_abs.shape[0]\n    if verbose > 0:\n        print(\"[learning_curve] Training set sizes: \" + str(train_sizes_abs))\n\n    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\n                        verbose=verbose)\n\n    if shuffle:\n        rng = check_random_state(random_state)\n        cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)\n\n    if exploit_incremental_learning:\n        classes = np.unique(y) if is_classifier(estimator) else None\n        out = parallel(delayed(_incremental_fit_estimator)(\n            clone(estimator), X, y, classes, train, test, train_sizes_abs,\n            scorer, verbose, fit_params=fit_params) for train, test in cv_iter)\n    else:\n        train_test_proportions = []\n        for train, test in cv_iter:\n            for n_train_samples in train_sizes_abs:\n                train_test_proportions.append((train[:n_train_samples], test))\n\n        out = parallel(delayed(_fit_and_score)(\n            clone(estimator), X, y, scorer, train, test,\n            verbose, parameters=None, fit_params=fit_params,\n            return_train_score=True)\n            for train, test in train_test_proportions)\n        out = np.array(out)\n        n_cv_folds = out.shape[0] // n_unique_ticks\n        out = out.reshape(n_cv_folds, n_unique_ticks, 2)\n\n    out = np.asarray(out).transpose((2, 1, 0))\n\n    return train_sizes_abs, out[0], out[1]\n\n\ndef _translate_train_sizes(train_sizes, n_max_training_samples):\n    \"\"\"Determine absolute sizes of training subsets and validate 'train_sizes'.\n\n    Examples:\n        _translate_train_sizes([0.5, 1.0], 10) -> [5, 10]\n        _translate_train_sizes([5, 10], 10) -> [5, 10]\n\n    Parameters\n    ----------\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Numbers of training examples that will be used to generate the\n        learning curve. If the dtype is float, it is regarded as a\n        fraction of 'n_max_training_samples', i.e. it has to be within (0, 1].\n\n    n_max_training_samples : int\n        Maximum number of training samples (upper bound of 'train_sizes').\n\n    Returns\n    -------\n    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\n        Numbers of training examples that will be used to generate the\n        learning curve. Note that the number of ticks might be less\n        than n_ticks because duplicate entries will be removed.\n    \"\"\"\n    train_sizes_abs = np.asarray(train_sizes)\n    n_ticks = train_sizes_abs.shape[0]\n    n_min_required_samples = np.min(train_sizes_abs)\n    n_max_required_samples = np.max(train_sizes_abs)\n    if np.issubdtype(train_sizes_abs.dtype, np.floating):\n        if n_min_required_samples <= 0.0 or n_max_required_samples > 1.0:\n            raise ValueError(\"train_sizes has been interpreted as fractions \"\n                             \"of the maximum number of training samples and \"\n                             \"must be within (0, 1], but is within [%f, %f].\"\n                             % (n_min_required_samples,\n                                n_max_required_samples))\n        train_sizes_abs = (train_sizes_abs * n_max_training_samples).astype(\n                             dtype=np.int, copy=False)\n        train_sizes_abs = np.clip(train_sizes_abs, 1,\n                                  n_max_training_samples)\n    else:\n        if (n_min_required_samples <= 0 or\n                n_max_required_samples > n_max_training_samples):\n            raise ValueError(\"train_sizes has been interpreted as absolute \"\n                             \"numbers of training samples and must be within \"\n                             \"(0, %d], but is within [%d, %d].\"\n                             % (n_max_training_samples,\n                                n_min_required_samples,\n                                n_max_required_samples))\n\n    train_sizes_abs = np.unique(train_sizes_abs)\n    if n_ticks > train_sizes_abs.shape[0]:\n        warnings.warn(\"Removed duplicate entries from 'train_sizes'. Number \"\n                      \"of ticks will be less than the size of \"\n                      \"'train_sizes' %d instead of %d).\"\n                      % (train_sizes_abs.shape[0], n_ticks), RuntimeWarning)\n\n    return train_sizes_abs\n\n\ndef _incremental_fit_estimator(estimator, X, y, classes, train, test,\n                               train_sizes, scorer, verbose, fit_params):\n    \"\"\"Train estimator on training subsets incrementally and compute scores.\"\"\"\n    train_scores, test_scores = [], []\n    partitions = zip(train_sizes, np.split(train, train_sizes)[:-1])\n    for n_train_samples, partial_train in partitions:\n        train_subset = train[:n_train_samples]\n        X_train, y_train = _safe_split(estimator, X, y, train_subset)\n        X_partial_train, y_partial_train = _safe_split(estimator, X, y,\n                                                       partial_train)\n        X_test, y_test = _safe_split(estimator, X, y, test, train_subset)\n        if y_partial_train is None:\n            estimator.partial_fit(X_partial_train, classes=classes)\n        else:\n            estimator.partial_fit(X_partial_train, y_partial_train,\n                                  classes=classes)\n        train_scores.append(_score(estimator, X_train, y_train, scorer))\n        test_scores.append(_score(estimator, X_test, y_test, scorer))\n    return np.array((train_scores, test_scores)).T\n\n\ndef validation_curve(estimator, X, y, param_name, param_range, groups=None,\n                     cv=None, scoring=None, n_jobs=1, pre_dispatch=\"all\",\n                     verbose=0, fit_params=None):\n    \"\"\"Validation curve.\n\n    Determine training and test scores for varying parameter values.\n\n    Compute scores for an estimator with different values of a specified\n    parameter. This is similar to grid search with one parameter. However, this\n    will also compute training scores and is merely a utility for plotting the\n    results.\n\n    Read more in the :ref:`User Guide <learning_curve>`.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    param_name : string\n        Name of the parameter that will be varied.\n\n    param_range : array-like, shape (n_values,)\n        The values of the parameter that will be evaluated.\n\n    groups : array-like, with shape (n_samples,), optional\n        Group labels for the samples used while splitting the dataset into\n        train/test set.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - An object to be used as a cross-validation generator.\n        - An iterable yielding train, test splits.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n\n    pre_dispatch : integer or string, optional\n        Number of predispatched jobs for parallel execution (default is\n        all). The option can reduce the allocated memory. The string can\n        be an expression like '2*n_jobs'.\n\n    verbose : integer, optional\n        Controls the verbosity: the higher, the more messages.\n\n    fit_params : dict, optional\n        Parameters to pass to the fit method of the estimator.\n\n    Returns\n    -------\n    train_scores : array, shape (n_ticks, n_cv_folds)\n        Scores on training sets.\n\n    test_scores : array, shape (n_ticks, n_cv_folds)\n        Scores on test set.\n\n    Notes\n    -----\n    See :ref:`sphx_glr_auto_examples_model_selection_plot_validation_curve.py`\n\n    \"\"\"\n    X, y, groups = indexable(X, y, groups)\n\n    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n    scorer = check_scoring(estimator, scoring=scoring)\n\n    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\n                        verbose=verbose)\n    out = parallel(delayed(_fit_and_score)(\n        clone(estimator), X, y, scorer, train, test, verbose,\n        parameters={param_name: v}, fit_params=fit_params,\n        return_train_score=True)\n        # NOTE do not change order of iteration to allow one time cv splitters\n        for train, test in cv.split(X, y, groups) for v in param_range)\n    out = np.asarray(out)\n    n_params = len(param_range)\n    n_cv_folds = out.shape[0] // n_params\n    out = out.reshape(n_cv_folds, n_params, 2).transpose((2, 1, 0))\n\n    return out[0], out[1]\n\n\ndef _aggregate_score_dicts(scores):\n    \"\"\"Aggregate the list of dict to dict of np ndarray\n\n    The aggregated output of _fit_and_score will be a list of dict\n    of form [{'prec': 0.1, 'acc':1.0}, {'prec': 0.1, 'acc':1.0}, ...]\n    Convert it to a dict of array {'prec': np.array([0.1 ...]), ...}\n\n    Parameters\n    ----------\n\n    scores : list of dict\n        List of dicts of the scores for all scorers. This is a flat list,\n        assumed originally to be of row major order.\n\n    Example\n    -------\n\n    >>> scores = [{'a': 1, 'b':10}, {'a': 2, 'b':2}, {'a': 3, 'b':3},\n    ...           {'a': 10, 'b': 10}]                         # doctest: +SKIP\n    >>> _aggregate_score_dicts(scores)                        # doctest: +SKIP\n    {'a': array([1, 2, 3, 10]),\n     'b': array([10, 2, 3, 10])}\n    \"\"\"\n    out = {}\n    for key in scores[0]:\n        out[key] = np.asarray([score[key] for score in scores])\n    return out\n"
    },
    {
      "filename": "sklearn/model_selection/tests/test_validation.py",
      "content": "\"\"\"Test the validation module\"\"\"\nfrom __future__ import division\n\nimport sys\nimport warnings\nimport tempfile\nimport os\nfrom time import sleep\n\nimport pytest\nimport numpy as np\nfrom scipy.sparse import coo_matrix, csr_matrix\nfrom sklearn.exceptions import FitFailedWarning\n\nfrom sklearn.model_selection.tests.test_search import FailingClassifier\n\nfrom sklearn.utils.testing import assert_true\nfrom sklearn.utils.testing import assert_false\nfrom sklearn.utils.testing import assert_equal\nfrom sklearn.utils.testing import assert_almost_equal\nfrom sklearn.utils.testing import assert_raises\nfrom sklearn.utils.testing import assert_raise_message\nfrom sklearn.utils.testing import assert_warns\nfrom sklearn.utils.testing import assert_warns_message\nfrom sklearn.utils.testing import assert_no_warnings\nfrom sklearn.utils.testing import assert_raises_regex\nfrom sklearn.utils.testing import assert_greater\nfrom sklearn.utils.testing import assert_less\nfrom sklearn.utils.testing import assert_array_almost_equal\nfrom sklearn.utils.testing import assert_array_equal\nfrom sklearn.utils.mocking import CheckingClassifier, MockDataFrame\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import permutation_test_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom sklearn.model_selection import LeavePGroupsOut\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection._validation import _check_is_permutation\nfrom sklearn.model_selection._validation import _fit_and_score\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.datasets import load_boston\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_digits\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics.scorer import check_scoring\n\nfrom sklearn.linear_model import Ridge, LogisticRegression, SGDClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.externals.six.moves import cStringIO as StringIO\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import clone\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.utils import shuffle\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import make_multilabel_classification\n\nfrom sklearn.model_selection.tests.common import OneTimeSplitter\nfrom sklearn.model_selection import GridSearchCV\n\n\ntry:\n    WindowsError\nexcept NameError:\n    WindowsError = None\n\n\nclass MockImprovingEstimator(BaseEstimator):\n    \"\"\"Dummy classifier to test the learning curve\"\"\"\n    def __init__(self, n_max_train_sizes):\n        self.n_max_train_sizes = n_max_train_sizes\n        self.train_sizes = 0\n        self.X_subset = None\n\n    def fit(self, X_subset, y_subset=None):\n        self.X_subset = X_subset\n        self.train_sizes = X_subset.shape[0]\n        return self\n\n    def predict(self, X):\n        raise NotImplementedError\n\n    def score(self, X=None, Y=None):\n        # training score becomes worse (2 -> 1), test error better (0 -> 1)\n        if self._is_training_data(X):\n            return 2. - float(self.train_sizes) / self.n_max_train_sizes\n        else:\n            return float(self.train_sizes) / self.n_max_train_sizes\n\n    def _is_training_data(self, X):\n        return X is self.X_subset\n\n\nclass MockIncrementalImprovingEstimator(MockImprovingEstimator):\n    \"\"\"Dummy classifier that provides partial_fit\"\"\"\n    def __init__(self, n_max_train_sizes):\n        super(MockIncrementalImprovingEstimator,\n              self).__init__(n_max_train_sizes)\n        self.x = None\n\n    def _is_training_data(self, X):\n        return self.x in X\n\n    def partial_fit(self, X, y=None, **params):\n        self.train_sizes += X.shape[0]\n        self.x = X[0]\n\n\nclass MockEstimatorWithParameter(BaseEstimator):\n    \"\"\"Dummy classifier to test the validation curve\"\"\"\n    def __init__(self, param=0.5):\n        self.X_subset = None\n        self.param = param\n\n    def fit(self, X_subset, y_subset):\n        self.X_subset = X_subset\n        self.train_sizes = X_subset.shape[0]\n        return self\n\n    def predict(self, X):\n        raise NotImplementedError\n\n    def score(self, X=None, y=None):\n        return self.param if self._is_training_data(X) else 1 - self.param\n\n    def _is_training_data(self, X):\n        return X is self.X_subset\n\n\nclass MockEstimatorWithSingleFitCallAllowed(MockEstimatorWithParameter):\n    \"\"\"Dummy classifier that disallows repeated calls of fit method\"\"\"\n\n    def fit(self, X_subset, y_subset):\n        assert_false(\n            hasattr(self, 'fit_called_'),\n            'fit is called the second time'\n        )\n        self.fit_called_ = True\n        return super(type(self), self).fit(X_subset, y_subset)\n\n    def predict(self, X):\n        raise NotImplementedError\n\n\nclass MockClassifier(object):\n    \"\"\"Dummy classifier to test the cross-validation\"\"\"\n\n    def __init__(self, a=0, allow_nd=False):\n        self.a = a\n        self.allow_nd = allow_nd\n\n    def fit(self, X, Y=None, sample_weight=None, class_prior=None,\n            sparse_sample_weight=None, sparse_param=None, dummy_int=None,\n            dummy_str=None, dummy_obj=None, callback=None):\n        \"\"\"The dummy arguments are to test that this fit function can\n        accept non-array arguments through cross-validation, such as:\n            - int\n            - str (this is actually array-like)\n            - object\n            - function\n        \"\"\"\n        self.dummy_int = dummy_int\n        self.dummy_str = dummy_str\n        self.dummy_obj = dummy_obj\n        if callback is not None:\n            callback(self)\n\n        if self.allow_nd:\n            X = X.reshape(len(X), -1)\n        if X.ndim >= 3 and not self.allow_nd:\n            raise ValueError('X cannot be d')\n        if sample_weight is not None:\n            assert_true(sample_weight.shape[0] == X.shape[0],\n                        'MockClassifier extra fit_param sample_weight.shape[0]'\n                        ' is {0}, should be {1}'.format(sample_weight.shape[0],\n                                                        X.shape[0]))\n        if class_prior is not None:\n            assert_true(class_prior.shape[0] == len(np.unique(y)),\n                        'MockClassifier extra fit_param class_prior.shape[0]'\n                        ' is {0}, should be {1}'.format(class_prior.shape[0],\n                                                        len(np.unique(y))))\n        if sparse_sample_weight is not None:\n            fmt = ('MockClassifier extra fit_param sparse_sample_weight'\n                   '.shape[0] is {0}, should be {1}')\n            assert_true(sparse_sample_weight.shape[0] == X.shape[0],\n                        fmt.format(sparse_sample_weight.shape[0], X.shape[0]))\n        if sparse_param is not None:\n            fmt = ('MockClassifier extra fit_param sparse_param.shape '\n                   'is ({0}, {1}), should be ({2}, {3})')\n            assert_true(sparse_param.shape == P_sparse.shape,\n                        fmt.format(sparse_param.shape[0],\n                                   sparse_param.shape[1],\n                                   P_sparse.shape[0], P_sparse.shape[1]))\n        return self\n\n    def predict(self, T):\n        if self.allow_nd:\n            T = T.reshape(len(T), -1)\n        return T[:, 0]\n\n    def score(self, X=None, Y=None):\n        return 1. / (1 + np.abs(self.a))\n\n    def get_params(self, deep=False):\n        return {'a': self.a, 'allow_nd': self.allow_nd}\n\n\n# XXX: use 2D array, since 1D X is being detected as a single sample in\n# check_consistent_length\nX = np.ones((10, 2))\nX_sparse = coo_matrix(X)\ny = np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])\n# The number of samples per class needs to be > n_splits,\n# for StratifiedKFold(n_splits=3)\ny2 = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 3])\nP_sparse = coo_matrix(np.eye(5))\n\n\ndef test_cross_val_score():\n    clf = MockClassifier()\n\n    for a in range(-10, 10):\n        clf.a = a\n        # Smoke test\n        scores = cross_val_score(clf, X, y2)\n        assert_array_equal(scores, clf.score(X, y2))\n\n        # test with multioutput y\n        multioutput_y = np.column_stack([y2, y2[::-1]])\n        scores = cross_val_score(clf, X_sparse, multioutput_y)\n        assert_array_equal(scores, clf.score(X_sparse, multioutput_y))\n\n        scores = cross_val_score(clf, X_sparse, y2)\n        assert_array_equal(scores, clf.score(X_sparse, y2))\n\n        # test with multioutput y\n        scores = cross_val_score(clf, X_sparse, multioutput_y)\n        assert_array_equal(scores, clf.score(X_sparse, multioutput_y))\n\n    # test with X and y as list\n    list_check = lambda x: isinstance(x, list)\n    clf = CheckingClassifier(check_X=list_check)\n    scores = cross_val_score(clf, X.tolist(), y2.tolist())\n\n    clf = CheckingClassifier(check_y=list_check)\n    scores = cross_val_score(clf, X, y2.tolist())\n\n    assert_raises(ValueError, cross_val_score, clf, X, y2, scoring=\"sklearn\")\n\n    # test with 3d X and\n    X_3d = X[:, :, np.newaxis]\n    clf = MockClassifier(allow_nd=True)\n    scores = cross_val_score(clf, X_3d, y2)\n\n    clf = MockClassifier(allow_nd=False)\n    assert_raises(ValueError, cross_val_score, clf, X_3d, y2)\n\n\ndef test_cross_validate_invalid_scoring_param():\n    X, y = make_classification(random_state=0)\n    estimator = MockClassifier()\n\n    # Test the errors\n    error_message_regexp = \".*must be unique strings.*\"\n\n    # List/tuple of callables should raise a message advising users to use\n    # dict of names to callables mapping\n    assert_raises_regex(ValueError, error_message_regexp,\n                        cross_validate, estimator, X, y,\n                        scoring=(make_scorer(precision_score),\n                                 make_scorer(accuracy_score)))\n    assert_raises_regex(ValueError, error_message_regexp,\n                        cross_validate, estimator, X, y,\n                        scoring=(make_scorer(precision_score),))\n\n    # So should empty lists/tuples\n    assert_raises_regex(ValueError, error_message_regexp + \"Empty list.*\",\n                        cross_validate, estimator, X, y, scoring=())\n\n    # So should duplicated entries\n    assert_raises_regex(ValueError, error_message_regexp + \"Duplicate.*\",\n                        cross_validate, estimator, X, y,\n                        scoring=('f1_micro', 'f1_micro'))\n\n    # Nested Lists should raise a generic error message\n    assert_raises_regex(ValueError, error_message_regexp,\n                        cross_validate, estimator, X, y,\n                        scoring=[[make_scorer(precision_score)]])\n\n    error_message_regexp = (\".*should either be.*string or callable.*for \"\n                            \"single.*.*dict.*for multi.*\")\n\n    # Empty dict should raise invalid scoring error\n    assert_raises_regex(ValueError, \"An empty dict\",\n                        cross_validate, estimator, X, y, scoring=(dict()))\n\n    # And so should any other invalid entry\n    assert_raises_regex(ValueError, error_message_regexp,\n                        cross_validate, estimator, X, y, scoring=5)\n\n    multiclass_scorer = make_scorer(precision_recall_fscore_support)\n\n    # Multiclass Scorers that return multiple values are not supported yet\n    assert_raises_regex(ValueError,\n                        \"Classification metrics can't handle a mix of \"\n                        \"binary and continuous targets\",\n                        cross_validate, estimator, X, y,\n                        scoring=multiclass_scorer)\n    assert_raises_regex(ValueError,\n                        \"Classification metrics can't handle a mix of \"\n                        \"binary and continuous targets\",\n                        cross_validate, estimator, X, y,\n                        scoring={\"foo\": multiclass_scorer})\n\n    multivalued_scorer = make_scorer(confusion_matrix)\n\n    # Multiclass Scorers that return multiple values are not supported yet\n    assert_raises_regex(ValueError, \"scoring must return a number, got\",\n                        cross_validate, SVC(gamma='scale'), X, y,\n                        scoring=multivalued_scorer)\n    assert_raises_regex(ValueError, \"scoring must return a number, got\",\n                        cross_validate, SVC(gamma='scale'), X, y,\n                        scoring={\"foo\": multivalued_scorer})\n\n    assert_raises_regex(ValueError, \"'mse' is not a valid scoring value.\",\n                        cross_validate, SVC(), X, y, scoring=\"mse\")\n\n\ndef test_cross_validate():\n    # Compute train and test mse/r2 scores\n    cv = KFold(n_splits=5)\n\n    # Regression\n    X_reg, y_reg = make_regression(n_samples=30, random_state=0)\n    reg = Ridge(random_state=0)\n\n    # Classification\n    X_clf, y_clf = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n\n    for X, y, est in ((X_reg, y_reg, reg), (X_clf, y_clf, clf)):\n        # It's okay to evaluate regression metrics on classification too\n        mse_scorer = check_scoring(est, 'neg_mean_squared_error')\n        r2_scorer = check_scoring(est, 'r2')\n        train_mse_scores = []\n        test_mse_scores = []\n        train_r2_scores = []\n        test_r2_scores = []\n        fitted_estimators = []\n        for train, test in cv.split(X, y):\n            est = clone(reg).fit(X[train], y[train])\n            train_mse_scores.append(mse_scorer(est, X[train], y[train]))\n            train_r2_scores.append(r2_scorer(est, X[train], y[train]))\n            test_mse_scores.append(mse_scorer(est, X[test], y[test]))\n            test_r2_scores.append(r2_scorer(est, X[test], y[test]))\n            fitted_estimators.append(est)\n\n        train_mse_scores = np.array(train_mse_scores)\n        test_mse_scores = np.array(test_mse_scores)\n        train_r2_scores = np.array(train_r2_scores)\n        test_r2_scores = np.array(test_r2_scores)\n        fitted_estimators = np.array(fitted_estimators)\n\n        scores = (train_mse_scores, test_mse_scores, train_r2_scores,\n                  test_r2_scores, fitted_estimators)\n\n        check_cross_validate_single_metric(est, X, y, scores)\n        check_cross_validate_multi_metric(est, X, y, scores)\n\n\ndef test_cross_validate_return_train_score_warn():\n    # Test that warnings are raised. Will be removed in 0.21\n\n    X, y = make_classification(random_state=0)\n    estimator = MockClassifier()\n\n    result = {}\n    for val in [False, True, 'warn']:\n        result[val] = assert_no_warnings(cross_validate, estimator, X, y,\n                                         return_train_score=val)\n\n    msg = (\n        'You are accessing a training score ({!r}), '\n        'which will not be available by default '\n        'any more in 0.21. If you need training scores, '\n        'please set return_train_score=True').format('train_score')\n    train_score = assert_warns_message(FutureWarning, msg,\n                                       result['warn'].get, 'train_score')\n    assert np.allclose(train_score, result[True]['train_score'])\n    assert 'train_score' not in result[False]\n\n\ndef check_cross_validate_single_metric(clf, X, y, scores):\n    (train_mse_scores, test_mse_scores, train_r2_scores,\n     test_r2_scores, fitted_estimators) = scores\n    # Test single metric evaluation when scoring is string or singleton list\n    for (return_train_score, dict_len) in ((True, 4), (False, 3)):\n        # Single metric passed as a string\n        if return_train_score:\n            # It must be True by default\n            mse_scores_dict = cross_validate(clf, X, y, cv=5,\n                                             scoring='neg_mean_squared_error')\n            assert_array_almost_equal(mse_scores_dict['train_score'],\n                                      train_mse_scores)\n        else:\n            mse_scores_dict = cross_validate(clf, X, y, cv=5,\n                                             scoring='neg_mean_squared_error',\n                                             return_train_score=False)\n        assert_true(isinstance(mse_scores_dict, dict))\n        assert_equal(len(mse_scores_dict), dict_len)\n        assert_array_almost_equal(mse_scores_dict['test_score'],\n                                  test_mse_scores)\n\n        # Single metric passed as a list\n        if return_train_score:\n            # It must be True by default\n            r2_scores_dict = cross_validate(clf, X, y, cv=5, scoring=['r2'])\n            assert_array_almost_equal(r2_scores_dict['train_r2'],\n                                      train_r2_scores)\n        else:\n            r2_scores_dict = cross_validate(clf, X, y, cv=5, scoring=['r2'],\n                                            return_train_score=False)\n        assert_true(isinstance(r2_scores_dict, dict))\n        assert_equal(len(r2_scores_dict), dict_len)\n        assert_array_almost_equal(r2_scores_dict['test_r2'], test_r2_scores)\n\n    # Test return_estimator option\n    mse_scores_dict = cross_validate(clf, X, y, cv=5,\n                                     scoring='neg_mean_squared_error',\n                                     return_estimator=True)\n    for k, est in enumerate(mse_scores_dict['estimator']):\n        assert_almost_equal(est.coef_, fitted_estimators[k].coef_)\n        assert_almost_equal(est.intercept_, fitted_estimators[k].intercept_)\n\n\ndef check_cross_validate_multi_metric(clf, X, y, scores):\n    # Test multimetric evaluation when scoring is a list / dict\n    (train_mse_scores, test_mse_scores, train_r2_scores,\n     test_r2_scores, fitted_estimators) = scores\n    all_scoring = (('r2', 'neg_mean_squared_error'),\n                   {'r2': make_scorer(r2_score),\n                    'neg_mean_squared_error': 'neg_mean_squared_error'})\n\n    keys_sans_train = set(('test_r2', 'test_neg_mean_squared_error',\n                           'fit_time', 'score_time'))\n    keys_with_train = keys_sans_train.union(\n        set(('train_r2', 'train_neg_mean_squared_error')))\n\n    for return_train_score in (True, False):\n        for scoring in all_scoring:\n            if return_train_score:\n                # return_train_score must be True by default\n                cv_results = cross_validate(clf, X, y, cv=5, scoring=scoring)\n                assert_array_almost_equal(cv_results['train_r2'],\n                                          train_r2_scores)\n                assert_array_almost_equal(\n                    cv_results['train_neg_mean_squared_error'],\n                    train_mse_scores)\n            else:\n                cv_results = cross_validate(clf, X, y, cv=5, scoring=scoring,\n                                            return_train_score=False)\n            assert_true(isinstance(cv_results, dict))\n            assert_equal(set(cv_results.keys()),\n                         keys_with_train if return_train_score\n                         else keys_sans_train)\n            assert_array_almost_equal(cv_results['test_r2'], test_r2_scores)\n            assert_array_almost_equal(\n                cv_results['test_neg_mean_squared_error'], test_mse_scores)\n\n            # Make sure all the arrays are of np.ndarray type\n            assert type(cv_results['test_r2']) == np.ndarray\n            assert (type(cv_results['test_neg_mean_squared_error']) ==\n                    np.ndarray)\n            assert type(cv_results['fit_time']) == np.ndarray\n            assert type(cv_results['score_time']) == np.ndarray\n\n            # Ensure all the times are within sane limits\n            assert np.all(cv_results['fit_time'] >= 0)\n            assert np.all(cv_results['fit_time'] < 10)\n            assert np.all(cv_results['score_time'] >= 0)\n            assert np.all(cv_results['score_time'] < 10)\n\n\ndef test_cross_val_score_predict_groups():\n    # Check if ValueError (when groups is None) propagates to cross_val_score\n    # and cross_val_predict\n    # And also check if groups is correctly passed to the cv object\n    X, y = make_classification(n_samples=20, n_classes=2, random_state=0)\n\n    clf = SVC(kernel=\"linear\")\n\n    group_cvs = [LeaveOneGroupOut(), LeavePGroupsOut(2), GroupKFold(),\n                 GroupShuffleSplit()]\n    for cv in group_cvs:\n        assert_raise_message(ValueError,\n                             \"The 'groups' parameter should not be None.\",\n                             cross_val_score, estimator=clf, X=X, y=y, cv=cv)\n        assert_raise_message(ValueError,\n                             \"The 'groups' parameter should not be None.\",\n                             cross_val_predict, estimator=clf, X=X, y=y, cv=cv)\n\n\ndef test_cross_val_score_pandas():\n    # check cross_val_score doesn't destroy pandas dataframe\n    types = [(MockDataFrame, MockDataFrame)]\n    try:\n        from pandas import Series, DataFrame\n        types.append((Series, DataFrame))\n    except ImportError:\n        pass\n    for TargetType, InputFeatureType in types:\n        # X dataframe, y series\n        # 3 fold cross val is used so we need atleast 3 samples per class\n        X_df, y_ser = InputFeatureType(X), TargetType(y2)\n        check_df = lambda x: isinstance(x, InputFeatureType)\n        check_series = lambda x: isinstance(x, TargetType)\n        clf = CheckingClassifier(check_X=check_df, check_y=check_series)\n        cross_val_score(clf, X_df, y_ser)\n\n\ndef test_cross_val_score_mask():\n    # test that cross_val_score works with boolean masks\n    svm = SVC(kernel=\"linear\")\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    kfold = KFold(5)\n    scores_indices = cross_val_score(svm, X, y, cv=kfold)\n    kfold = KFold(5)\n    cv_masks = []\n    for train, test in kfold.split(X, y):\n        mask_train = np.zeros(len(y), dtype=np.bool)\n        mask_test = np.zeros(len(y), dtype=np.bool)\n        mask_train[train] = 1\n        mask_test[test] = 1\n        cv_masks.append((train, test))\n    scores_masks = cross_val_score(svm, X, y, cv=cv_masks)\n    assert_array_equal(scores_indices, scores_masks)\n\n\ndef test_cross_val_score_precomputed():\n    # test for svm with precomputed kernel\n    svm = SVC(kernel=\"precomputed\")\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    linear_kernel = np.dot(X, X.T)\n    score_precomputed = cross_val_score(svm, linear_kernel, y)\n    svm = SVC(kernel=\"linear\")\n    score_linear = cross_val_score(svm, X, y)\n    assert_array_almost_equal(score_precomputed, score_linear)\n\n    # test with callable\n    svm = SVC(gamma='scale', kernel=lambda x, y: np.dot(x, y.T))\n    score_callable = cross_val_score(svm, X, y)\n    assert_array_almost_equal(score_precomputed, score_callable)\n\n    # Error raised for non-square X\n    svm = SVC(kernel=\"precomputed\")\n    assert_raises(ValueError, cross_val_score, svm, X, y)\n\n    # test error is raised when the precomputed kernel is not array-like\n    # or sparse\n    assert_raises(ValueError, cross_val_score, svm,\n                  linear_kernel.tolist(), y)\n\n\ndef test_cross_val_score_fit_params():\n    clf = MockClassifier()\n    n_samples = X.shape[0]\n    n_classes = len(np.unique(y))\n\n    W_sparse = coo_matrix((np.array([1]), (np.array([1]), np.array([0]))),\n                          shape=(10, 1))\n    P_sparse = coo_matrix(np.eye(5))\n\n    DUMMY_INT = 42\n    DUMMY_STR = '42'\n    DUMMY_OBJ = object()\n\n    def assert_fit_params(clf):\n        # Function to test that the values are passed correctly to the\n        # classifier arguments for non-array type\n\n        assert_equal(clf.dummy_int, DUMMY_INT)\n        assert_equal(clf.dummy_str, DUMMY_STR)\n        assert_equal(clf.dummy_obj, DUMMY_OBJ)\n\n    fit_params = {'sample_weight': np.ones(n_samples),\n                  'class_prior': np.ones(n_classes) / n_classes,\n                  'sparse_sample_weight': W_sparse,\n                  'sparse_param': P_sparse,\n                  'dummy_int': DUMMY_INT,\n                  'dummy_str': DUMMY_STR,\n                  'dummy_obj': DUMMY_OBJ,\n                  'callback': assert_fit_params}\n    cross_val_score(clf, X, y, fit_params=fit_params)\n\n\ndef test_cross_val_score_score_func():\n    clf = MockClassifier()\n    _score_func_args = []\n\n    def score_func(y_test, y_predict):\n        _score_func_args.append((y_test, y_predict))\n        return 1.0\n\n    with warnings.catch_warnings(record=True):\n        scoring = make_scorer(score_func)\n        score = cross_val_score(clf, X, y, scoring=scoring, cv=3)\n    assert_array_equal(score, [1.0, 1.0, 1.0])\n    # Test that score function is called only 3 times (for cv=3)\n    assert len(_score_func_args) == 3\n\n\ndef test_cross_val_score_errors():\n    class BrokenEstimator:\n        pass\n\n    assert_raises(TypeError, cross_val_score, BrokenEstimator(), X)\n\n\ndef test_cross_val_score_with_score_func_classification():\n    iris = load_iris()\n    clf = SVC(kernel='linear')\n\n    # Default score (should be the accuracy score)\n    scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n    assert_array_almost_equal(scores, [0.97, 1., 0.97, 0.97, 1.], 2)\n\n    # Correct classification score (aka. zero / one score) - should be the\n    # same as the default estimator score\n    zo_scores = cross_val_score(clf, iris.data, iris.target,\n                                scoring=\"accuracy\", cv=5)\n    assert_array_almost_equal(zo_scores, [0.97, 1., 0.97, 0.97, 1.], 2)\n\n    # F1 score (class are balanced so f1_score should be equal to zero/one\n    # score\n    f1_scores = cross_val_score(clf, iris.data, iris.target,\n                                scoring=\"f1_weighted\", cv=5)\n    assert_array_almost_equal(f1_scores, [0.97, 1., 0.97, 0.97, 1.], 2)\n\n\ndef test_cross_val_score_with_score_func_regression():\n    X, y = make_regression(n_samples=30, n_features=20, n_informative=5,\n                           random_state=0)\n    reg = Ridge()\n\n    # Default score of the Ridge regression estimator\n    scores = cross_val_score(reg, X, y, cv=5)\n    assert_array_almost_equal(scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)\n\n    # R2 score (aka. determination coefficient) - should be the\n    # same as the default estimator score\n    r2_scores = cross_val_score(reg, X, y, scoring=\"r2\", cv=5)\n    assert_array_almost_equal(r2_scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)\n\n    # Mean squared error; this is a loss function, so \"scores\" are negative\n    neg_mse_scores = cross_val_score(reg, X, y, cv=5,\n                                     scoring=\"neg_mean_squared_error\")\n    expected_neg_mse = np.array([-763.07, -553.16, -274.38, -273.26, -1681.99])\n    assert_array_almost_equal(neg_mse_scores, expected_neg_mse, 2)\n\n    # Explained variance\n    scoring = make_scorer(explained_variance_score)\n    ev_scores = cross_val_score(reg, X, y, cv=5, scoring=scoring)\n    assert_array_almost_equal(ev_scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)\n\n\ndef test_permutation_score():\n    iris = load_iris()\n    X = iris.data\n    X_sparse = coo_matrix(X)\n    y = iris.target\n    svm = SVC(kernel='linear')\n    cv = StratifiedKFold(2)\n\n    score, scores, pvalue = permutation_test_score(\n        svm, X, y, n_permutations=30, cv=cv, scoring=\"accuracy\")\n    assert_greater(score, 0.9)\n    assert_almost_equal(pvalue, 0.0, 1)\n\n    score_group, _, pvalue_group = permutation_test_score(\n        svm, X, y, n_permutations=30, cv=cv, scoring=\"accuracy\",\n        groups=np.ones(y.size), random_state=0)\n    assert_true(score_group == score)\n    assert_true(pvalue_group == pvalue)\n\n    # check that we obtain the same results with a sparse representation\n    svm_sparse = SVC(kernel='linear')\n    cv_sparse = StratifiedKFold(2)\n    score_group, _, pvalue_group = permutation_test_score(\n        svm_sparse, X_sparse, y, n_permutations=30, cv=cv_sparse,\n        scoring=\"accuracy\", groups=np.ones(y.size), random_state=0)\n\n    assert_true(score_group == score)\n    assert_true(pvalue_group == pvalue)\n\n    # test with custom scoring object\n    def custom_score(y_true, y_pred):\n        return (((y_true == y_pred).sum() - (y_true != y_pred).sum()) /\n                y_true.shape[0])\n\n    scorer = make_scorer(custom_score)\n    score, _, pvalue = permutation_test_score(\n        svm, X, y, n_permutations=100, scoring=scorer, cv=cv, random_state=0)\n    assert_almost_equal(score, .93, 2)\n    assert_almost_equal(pvalue, 0.01, 3)\n\n    # set random y\n    y = np.mod(np.arange(len(y)), 3)\n\n    score, scores, pvalue = permutation_test_score(\n        svm, X, y, n_permutations=30, cv=cv, scoring=\"accuracy\")\n\n    assert_less(score, 0.5)\n    assert_greater(pvalue, 0.2)\n\n\ndef test_permutation_test_score_allow_nans():\n    # Check that permutation_test_score allows input data with NaNs\n    X = np.arange(200, dtype=np.float64).reshape(10, -1)\n    X[2, :] = np.nan\n    y = np.repeat([0, 1], X.shape[0] / 2)\n    p = Pipeline([\n        ('imputer', SimpleImputer(strategy='mean', missing_values=np.nan)),\n        ('classifier', MockClassifier()),\n    ])\n    permutation_test_score(p, X, y, cv=5)\n\n\ndef test_permutation_test_score_fit_params():\n    est = SVC(random_state=0)\n    n_samples = 50\n    X, y = make_blobs(n_samples=n_samples, n_features=2, centers=10,\n                      random_state=0)\n    weights = [i for i in range(n_samples)]\n    score_without_fit_params = permutation_test_score(est, X, y)\n    mean_without_fit_params = score_without_fit_params[1].mean()\n    # checking for sample weight with list and numpy array\n    for method in [list, np.array]:\n        w = method(weights)\n        score_with_fit_params = permutation_test_score(est, X, y, fit_params={\n                                                       'sample_weight': w})\n        mean_with_fit_params = score_with_fit_params[1].mean()\n        assert not np.isclose(mean_with_fit_params, mean_without_fit_params)\n\n\ndef test_cross_val_score_allow_nans():\n    # Check that cross_val_score allows input data with NaNs\n    X = np.arange(200, dtype=np.float64).reshape(10, -1)\n    X[2, :] = np.nan\n    y = np.repeat([0, 1], X.shape[0] / 2)\n    p = Pipeline([\n        ('imputer', SimpleImputer(strategy='mean', missing_values=np.nan)),\n        ('classifier', MockClassifier()),\n    ])\n    cross_val_score(p, X, y, cv=5)\n\n\ndef test_cross_val_score_multilabel():\n    X = np.array([[-3, 4], [2, 4], [3, 3], [0, 2], [-3, 1],\n                  [-2, 1], [0, 0], [-2, -1], [-1, -2], [1, -2]])\n    y = np.array([[1, 1], [0, 1], [0, 1], [0, 1], [1, 1],\n                  [0, 1], [1, 0], [1, 1], [1, 0], [0, 0]])\n    clf = KNeighborsClassifier(n_neighbors=1)\n    scoring_micro = make_scorer(precision_score, average='micro')\n    scoring_macro = make_scorer(precision_score, average='macro')\n    scoring_samples = make_scorer(precision_score, average='samples')\n    score_micro = cross_val_score(clf, X, y, scoring=scoring_micro, cv=5)\n    score_macro = cross_val_score(clf, X, y, scoring=scoring_macro, cv=5)\n    score_samples = cross_val_score(clf, X, y, scoring=scoring_samples, cv=5)\n    assert_almost_equal(score_micro, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 3])\n    assert_almost_equal(score_macro, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 4])\n    assert_almost_equal(score_samples, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 4])\n\n\ndef test_cross_val_predict():\n    boston = load_boston()\n    X, y = boston.data, boston.target\n    cv = KFold()\n\n    est = Ridge()\n\n    # Naive loop (should be same as cross_val_predict):\n    preds2 = np.zeros_like(y)\n    for train, test in cv.split(X, y):\n        est.fit(X[train], y[train])\n        preds2[test] = est.predict(X[test])\n\n    preds = cross_val_predict(est, X, y, cv=cv)\n    assert_array_almost_equal(preds, preds2)\n\n    preds = cross_val_predict(est, X, y)\n    assert_equal(len(preds), len(y))\n\n    cv = LeaveOneOut()\n    preds = cross_val_predict(est, X, y, cv=cv)\n    assert_equal(len(preds), len(y))\n\n    Xsp = X.copy()\n    Xsp *= (Xsp > np.median(Xsp))\n    Xsp = coo_matrix(Xsp)\n    preds = cross_val_predict(est, Xsp, y)\n    assert_array_almost_equal(len(preds), len(y))\n\n    preds = cross_val_predict(KMeans(), X)\n    assert_equal(len(preds), len(y))\n\n    class BadCV():\n        def split(self, X, y=None, groups=None):\n            for i in range(4):\n                yield np.array([0, 1, 2, 3]), np.array([4, 5, 6, 7, 8])\n\n    assert_raises(ValueError, cross_val_predict, est, X, y, cv=BadCV())\n\n    X, y = load_iris(return_X_y=True)\n\n    warning_message = ('Number of classes in training fold (2) does '\n                       'not match total number of classes (3). '\n                       'Results may not be appropriate for your use case.')\n    assert_warns_message(RuntimeWarning, warning_message,\n                         cross_val_predict, LogisticRegression(),\n                         X, y, method='predict_proba', cv=KFold(2))\n\n\ndef test_cross_val_predict_decision_function_shape():\n    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)\n\n    preds = cross_val_predict(LogisticRegression(), X, y,\n                              method='decision_function')\n    assert_equal(preds.shape, (50,))\n\n    X, y = load_iris(return_X_y=True)\n\n    preds = cross_val_predict(LogisticRegression(), X, y,\n                              method='decision_function')\n    assert_equal(preds.shape, (150, 3))\n\n    # This specifically tests imbalanced splits for binary\n    # classification with decision_function. This is only\n    # applicable to classifiers that can be fit on a single\n    # class.\n    X = X[:100]\n    y = y[:100]\n    assert_raise_message(ValueError,\n                         'Only 1 class/es in training fold, this'\n                         ' is not supported for decision_function'\n                         ' with imbalanced folds. To fix '\n                         'this, use a cross-validation technique '\n                         'resulting in properly stratified folds',\n                         cross_val_predict, RidgeClassifier(), X, y,\n                         method='decision_function', cv=KFold(2))\n\n    X, y = load_digits(return_X_y=True)\n    est = SVC(kernel='linear', decision_function_shape='ovo')\n\n    preds = cross_val_predict(est,\n                              X, y,\n                              method='decision_function')\n    assert_equal(preds.shape, (1797, 45))\n\n    ind = np.argsort(y)\n    X, y = X[ind], y[ind]\n    assert_raises_regex(ValueError,\n                        r'Output shape \\(599L?, 21L?\\) of decision_function '\n                        r'does not match number of classes \\(7\\) in fold. '\n                        'Irregular decision_function .*',\n                        cross_val_predict, est, X, y,\n                        cv=KFold(n_splits=3), method='decision_function')\n\n\ndef test_cross_val_predict_predict_proba_shape():\n    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)\n\n    preds = cross_val_predict(LogisticRegression(), X, y,\n                              method='predict_proba')\n    assert_equal(preds.shape, (50, 2))\n\n    X, y = load_iris(return_X_y=True)\n\n    preds = cross_val_predict(LogisticRegression(), X, y,\n                              method='predict_proba')\n    assert_equal(preds.shape, (150, 3))\n\n\ndef test_cross_val_predict_predict_log_proba_shape():\n    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)\n\n    preds = cross_val_predict(LogisticRegression(), X, y,\n                              method='predict_log_proba')\n    assert_equal(preds.shape, (50, 2))\n\n    X, y = load_iris(return_X_y=True)\n\n    preds = cross_val_predict(LogisticRegression(), X, y,\n                              method='predict_log_proba')\n    assert_equal(preds.shape, (150, 3))\n\n\ndef test_cross_val_predict_input_types():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    X_sparse = coo_matrix(X)\n    multioutput_y = np.column_stack([y, y[::-1]])\n\n    clf = Ridge(fit_intercept=False, random_state=0)\n    # 3 fold cv is used --> atleast 3 samples per class\n    # Smoke test\n    predictions = cross_val_predict(clf, X, y)\n    assert_equal(predictions.shape, (150,))\n\n    # test with multioutput y\n    predictions = cross_val_predict(clf, X_sparse, multioutput_y)\n    assert_equal(predictions.shape, (150, 2))\n\n    predictions = cross_val_predict(clf, X_sparse, y)\n    assert_array_equal(predictions.shape, (150,))\n\n    # test with multioutput y\n    predictions = cross_val_predict(clf, X_sparse, multioutput_y)\n    assert_array_equal(predictions.shape, (150, 2))\n\n    # test with X and y as list\n    list_check = lambda x: isinstance(x, list)\n    clf = CheckingClassifier(check_X=list_check)\n    predictions = cross_val_predict(clf, X.tolist(), y.tolist())\n\n    clf = CheckingClassifier(check_y=list_check)\n    predictions = cross_val_predict(clf, X, y.tolist())\n\n    # test with X and y as list and non empty method\n    predictions = cross_val_predict(LogisticRegression(), X.tolist(),\n                                    y.tolist(), method='decision_function')\n    predictions = cross_val_predict(LogisticRegression(), X,\n                                    y.tolist(), method='decision_function')\n\n    # test with 3d X and\n    X_3d = X[:, :, np.newaxis]\n    check_3d = lambda x: x.ndim == 3\n    clf = CheckingClassifier(check_X=check_3d)\n    predictions = cross_val_predict(clf, X_3d, y)\n    assert_array_equal(predictions.shape, (150,))\n\n\ndef test_cross_val_predict_pandas():\n    # check cross_val_score doesn't destroy pandas dataframe\n    types = [(MockDataFrame, MockDataFrame)]\n    try:\n        from pandas import Series, DataFrame\n        types.append((Series, DataFrame))\n    except ImportError:\n        pass\n    for TargetType, InputFeatureType in types:\n        # X dataframe, y series\n        X_df, y_ser = InputFeatureType(X), TargetType(y2)\n        check_df = lambda x: isinstance(x, InputFeatureType)\n        check_series = lambda x: isinstance(x, TargetType)\n        clf = CheckingClassifier(check_X=check_df, check_y=check_series)\n        cross_val_predict(clf, X_df, y_ser)\n\n\ndef test_cross_val_score_sparse_fit_params():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    clf = MockClassifier()\n    fit_params = {'sparse_sample_weight': coo_matrix(np.eye(X.shape[0]))}\n    a = cross_val_score(clf, X, y, fit_params=fit_params)\n    assert_array_equal(a, np.ones(3))\n\n\ndef test_learning_curve():\n    n_samples = 30\n    n_splits = 3\n    X, y = make_classification(n_samples=n_samples, n_features=1,\n                               n_informative=1, n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(n_samples * ((n_splits - 1) / n_splits))\n    for shuffle_train in [False, True]:\n        with warnings.catch_warnings(record=True) as w:\n            train_sizes, train_scores, test_scores = learning_curve(\n                estimator, X, y, cv=KFold(n_splits=n_splits),\n                train_sizes=np.linspace(0.1, 1.0, 10),\n                shuffle=shuffle_train)\n        if len(w) > 0:\n            raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)\n        assert_equal(train_scores.shape, (10, 3))\n        assert_equal(test_scores.shape, (10, 3))\n        assert_array_equal(train_sizes, np.linspace(2, 20, 10))\n        assert_array_almost_equal(train_scores.mean(axis=1),\n                                  np.linspace(1.9, 1.0, 10))\n        assert_array_almost_equal(test_scores.mean(axis=1),\n                                  np.linspace(0.1, 1.0, 10))\n\n        # Test a custom cv splitter that can iterate only once\n        with warnings.catch_warnings(record=True) as w:\n            train_sizes2, train_scores2, test_scores2 = learning_curve(\n                estimator, X, y,\n                cv=OneTimeSplitter(n_splits=n_splits, n_samples=n_samples),\n                train_sizes=np.linspace(0.1, 1.0, 10),\n                shuffle=shuffle_train)\n        if len(w) > 0:\n            raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)\n        assert_array_almost_equal(train_scores2, train_scores)\n        assert_array_almost_equal(test_scores2, test_scores)\n\n\ndef test_learning_curve_unsupervised():\n    X, _ = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(20)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y=None, cv=3, train_sizes=np.linspace(0.1, 1.0, 10))\n    assert_array_equal(train_sizes, np.linspace(2, 20, 10))\n    assert_array_almost_equal(train_scores.mean(axis=1),\n                              np.linspace(1.9, 1.0, 10))\n    assert_array_almost_equal(test_scores.mean(axis=1),\n                              np.linspace(0.1, 1.0, 10))\n\n\ndef test_learning_curve_verbose():\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(20)\n\n    old_stdout = sys.stdout\n    sys.stdout = StringIO()\n    try:\n        train_sizes, train_scores, test_scores = \\\n            learning_curve(estimator, X, y, cv=3, verbose=1)\n    finally:\n        out = sys.stdout.getvalue()\n        sys.stdout.close()\n        sys.stdout = old_stdout\n\n    assert(\"[learning_curve]\" in out)\n\n\ndef test_learning_curve_incremental_learning_not_possible():\n    X, y = make_classification(n_samples=2, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    # The mockup does not have partial_fit()\n    estimator = MockImprovingEstimator(1)\n    assert_raises(ValueError, learning_curve, estimator, X, y,\n                  exploit_incremental_learning=True)\n\n\ndef test_learning_curve_incremental_learning():\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockIncrementalImprovingEstimator(20)\n    for shuffle_train in [False, True]:\n        train_sizes, train_scores, test_scores = learning_curve(\n            estimator, X, y, cv=3, exploit_incremental_learning=True,\n            train_sizes=np.linspace(0.1, 1.0, 10), shuffle=shuffle_train)\n        assert_array_equal(train_sizes, np.linspace(2, 20, 10))\n        assert_array_almost_equal(train_scores.mean(axis=1),\n                                  np.linspace(1.9, 1.0, 10))\n        assert_array_almost_equal(test_scores.mean(axis=1),\n                                  np.linspace(0.1, 1.0, 10))\n\n\ndef test_learning_curve_incremental_learning_unsupervised():\n    X, _ = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockIncrementalImprovingEstimator(20)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y=None, cv=3, exploit_incremental_learning=True,\n        train_sizes=np.linspace(0.1, 1.0, 10))\n    assert_array_equal(train_sizes, np.linspace(2, 20, 10))\n    assert_array_almost_equal(train_scores.mean(axis=1),\n                              np.linspace(1.9, 1.0, 10))\n    assert_array_almost_equal(test_scores.mean(axis=1),\n                              np.linspace(0.1, 1.0, 10))\n\n\ndef test_learning_curve_batch_and_incremental_learning_are_equal():\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    train_sizes = np.linspace(0.2, 1.0, 5)\n    estimator = PassiveAggressiveClassifier(max_iter=1, tol=None,\n                                            shuffle=False)\n\n    train_sizes_inc, train_scores_inc, test_scores_inc = \\\n        learning_curve(\n            estimator, X, y, train_sizes=train_sizes,\n            cv=3, exploit_incremental_learning=True)\n    train_sizes_batch, train_scores_batch, test_scores_batch = \\\n        learning_curve(\n            estimator, X, y, cv=3, train_sizes=train_sizes,\n            exploit_incremental_learning=False)\n\n    assert_array_equal(train_sizes_inc, train_sizes_batch)\n    assert_array_almost_equal(train_scores_inc.mean(axis=1),\n                              train_scores_batch.mean(axis=1))\n    assert_array_almost_equal(test_scores_inc.mean(axis=1),\n                              test_scores_batch.mean(axis=1))\n\n\ndef test_learning_curve_n_sample_range_out_of_bounds():\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(20)\n    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,\n                  train_sizes=[0, 1])\n    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,\n                  train_sizes=[0.0, 1.0])\n    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,\n                  train_sizes=[0.1, 1.1])\n    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,\n                  train_sizes=[0, 20])\n    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,\n                  train_sizes=[1, 21])\n\n\ndef test_learning_curve_remove_duplicate_sample_sizes():\n    X, y = make_classification(n_samples=3, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(2)\n    train_sizes, _, _ = assert_warns(\n        RuntimeWarning, learning_curve, estimator, X, y, cv=3,\n        train_sizes=np.linspace(0.33, 1.0, 3))\n    assert_array_equal(train_sizes, [1, 2])\n\n\ndef test_learning_curve_with_boolean_indices():\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(20)\n    cv = KFold(n_splits=3)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, train_sizes=np.linspace(0.1, 1.0, 10))\n    assert_array_equal(train_sizes, np.linspace(2, 20, 10))\n    assert_array_almost_equal(train_scores.mean(axis=1),\n                              np.linspace(1.9, 1.0, 10))\n    assert_array_almost_equal(test_scores.mean(axis=1),\n                              np.linspace(0.1, 1.0, 10))\n\n\ndef test_learning_curve_with_shuffle():\n    # Following test case was designed this way to verify the code\n    # changes made in pull request: #7506.\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [11, 12], [13, 14], [15, 16],\n                 [17, 18], [19, 20], [7, 8], [9, 10], [11, 12], [13, 14],\n                 [15, 16], [17, 18]])\n    y = np.array([1, 1, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4])\n    groups = np.array([1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 4, 4, 4, 4])\n    # Splits on these groups fail without shuffle as the first iteration\n    # of the learning curve doesn't contain label 4 in the training set.\n    estimator = PassiveAggressiveClassifier(max_iter=5, tol=None,\n                                            shuffle=False)\n\n    cv = GroupKFold(n_splits=2)\n    train_sizes_batch, train_scores_batch, test_scores_batch = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3),\n        groups=groups, shuffle=True, random_state=2)\n    assert_array_almost_equal(train_scores_batch.mean(axis=1),\n                              np.array([0.75, 0.3, 0.36111111]))\n    assert_array_almost_equal(test_scores_batch.mean(axis=1),\n                              np.array([0.36111111, 0.25, 0.25]))\n    assert_raises(ValueError, learning_curve, estimator, X, y, cv=cv, n_jobs=1,\n                  train_sizes=np.linspace(0.3, 1.0, 3), groups=groups)\n\n    train_sizes_inc, train_scores_inc, test_scores_inc = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3),\n        groups=groups, shuffle=True, random_state=2,\n        exploit_incremental_learning=True)\n    assert_array_almost_equal(train_scores_inc.mean(axis=1),\n                              train_scores_batch.mean(axis=1))\n    assert_array_almost_equal(test_scores_inc.mean(axis=1),\n                              test_scores_batch.mean(axis=1))\n\n\ndef test_learning_curve_fit_params():\n    est = SVC(random_state=0)\n    n_samples = 50\n    X, y = make_blobs(n_samples=n_samples, n_features=2, centers=10,\n                      random_state=0)\n    # sample weight is a list\n    w = [i for i in range(n_samples)]\n    l_with_fit_params = learning_curve(est, X, y,\n                                       fit_params={'sample_weight': w})\n    l_without_fit_params = learning_curve(est, X, y)\n    l_without_fit_params_m1 = l_without_fit_params[1].mean()\n    assert not np.isclose(l_with_fit_params[1].mean(),\n                          l_without_fit_params_m1)\n    # sample weight is a numpy array\n    W = np.array(w)\n    l_with_fit_params = learning_curve(est, X, y,\n                                       fit_params={'sample_weight': W})\n    assert not np.isclose(l_with_fit_params[1].mean(),\n                          l_without_fit_params_m1)\n\n\ndef test_validation_curve():\n    X, y = make_classification(n_samples=2, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    param_range = np.linspace(0, 1, 10)\n    with warnings.catch_warnings(record=True) as w:\n        train_scores, test_scores = validation_curve(\n            MockEstimatorWithParameter(), X, y, param_name=\"param\",\n            param_range=param_range, cv=2\n        )\n    if len(w) > 0:\n        raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)\n\n    assert_array_almost_equal(train_scores.mean(axis=1), param_range)\n    assert_array_almost_equal(test_scores.mean(axis=1), 1 - param_range)\n\n\ndef test_validation_curve_clone_estimator():\n    X, y = make_classification(n_samples=2, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n\n    param_range = np.linspace(1, 0, 10)\n    _, _ = validation_curve(\n        MockEstimatorWithSingleFitCallAllowed(), X, y,\n        param_name=\"param\", param_range=param_range, cv=2\n    )\n\n\ndef test_validation_curve_cv_splits_consistency():\n    n_samples = 100\n    n_splits = 5\n    X, y = make_classification(n_samples=100, random_state=0)\n\n    scores1 = validation_curve(SVC(kernel='linear', random_state=0), X, y,\n                               'C', [0.1, 0.1, 0.2, 0.2],\n                               cv=OneTimeSplitter(n_splits=n_splits,\n                                                  n_samples=n_samples))\n    # The OneTimeSplitter is a non-re-entrant cv splitter. Unless, the\n    # `split` is called for each parameter, the following should produce\n    # identical results for param setting 1 and param setting 2 as both have\n    # the same C value.\n    assert_array_almost_equal(*np.vsplit(np.hstack(scores1)[(0, 2, 1, 3), :],\n                                         2))\n\n    scores2 = validation_curve(SVC(kernel='linear', random_state=0), X, y,\n                               'C', [0.1, 0.1, 0.2, 0.2],\n                               cv=KFold(n_splits=n_splits, shuffle=True))\n\n    # For scores2, compare the 1st and 2nd parameter's scores\n    # (Since the C value for 1st two param setting is 0.1, they must be\n    # consistent unless the train test folds differ between the param settings)\n    assert_array_almost_equal(*np.vsplit(np.hstack(scores2)[(0, 2, 1, 3), :],\n                                         2))\n\n    scores3 = validation_curve(SVC(kernel='linear', random_state=0), X, y,\n                               'C', [0.1, 0.1, 0.2, 0.2],\n                               cv=KFold(n_splits=n_splits))\n\n    # OneTimeSplitter is basically unshuffled KFold(n_splits=5). Sanity check.\n    assert_array_almost_equal(np.array(scores3), np.array(scores1))\n\n\ndef test_validation_curve_fit_params():\n    est = SVC(random_state=0)\n    n_samples = 50\n    X, y = make_blobs(n_samples=n_samples, n_features=2, centers=10,\n                      random_state=0)\n    # sample_weight is a list\n    w = [i for i in range(n_samples)]\n    gamma_range = np.logspace(-6, -1, 5)\n    l_with_fit_params = validation_curve(est, X, y, \"gamma\", gamma_range,\n                                         fit_params={'sample_weight': w})\n    l_without_fit_params = validation_curve(est, X, y, \"gamma\", gamma_range)\n    l_without_fit_params_m0 = l_without_fit_params[0].mean()\n    l_without_fit_params_m1 = l_without_fit_params[1].mean()\n    assert not np.isclose(l_with_fit_params[0].mean(),\n                          l_without_fit_params_m0)\n    assert not np.isclose(l_with_fit_params[1].mean(),\n                          l_without_fit_params_m1)\n    # sample_weight is a numpy array\n    W = np.array(w)\n    l_with_fit_params = validation_curve(est, X, y, \"gamma\", gamma_range,\n                                         fit_params={'sample_weight': W})\n    assert not np.isclose(l_with_fit_params[0].mean(),\n                          l_without_fit_params_m0)\n    assert not np.isclose(l_with_fit_params[1].mean(),\n                          l_without_fit_params_m1)\n\n\ndef test_check_is_permutation():\n    rng = np.random.RandomState(0)\n    p = np.arange(100)\n    rng.shuffle(p)\n    assert_true(_check_is_permutation(p, 100))\n    assert_false(_check_is_permutation(np.delete(p, 23), 100))\n\n    p[0] = 23\n    assert_false(_check_is_permutation(p, 100))\n\n    # Check if the additional duplicate indices are caught\n    assert_false(_check_is_permutation(np.hstack((p, 0)), 100))\n\n\ndef test_cross_val_predict_sparse_prediction():\n    # check that cross_val_predict gives same result for sparse and dense input\n    X, y = make_multilabel_classification(n_classes=2, n_labels=1,\n                                          allow_unlabeled=False,\n                                          return_indicator=True,\n                                          random_state=1)\n    X_sparse = csr_matrix(X)\n    y_sparse = csr_matrix(y)\n    classif = OneVsRestClassifier(SVC(kernel='linear'))\n    preds = cross_val_predict(classif, X, y, cv=10)\n    preds_sparse = cross_val_predict(classif, X_sparse, y_sparse, cv=10)\n    preds_sparse = preds_sparse.toarray()\n    assert_array_almost_equal(preds_sparse, preds)\n\n\ndef check_cross_val_predict_with_method(est):\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    X, y = shuffle(X, y, random_state=0)\n    classes = len(set(y))\n\n    kfold = KFold()\n\n    methods = ['decision_function', 'predict_proba', 'predict_log_proba']\n    for method in methods:\n        predictions = cross_val_predict(est, X, y, method=method)\n        assert_equal(len(predictions), len(y))\n\n        expected_predictions = np.zeros([len(y), classes])\n        func = getattr(est, method)\n\n        # Naive loop (should be same as cross_val_predict):\n        for train, test in kfold.split(X, y):\n            est.fit(X[train], y[train])\n            expected_predictions[test] = func(X[test])\n\n        predictions = cross_val_predict(est, X, y, method=method,\n                                        cv=kfold)\n        assert_array_almost_equal(expected_predictions, predictions)\n\n        # Test alternative representations of y\n        predictions_y1 = cross_val_predict(est, X, y + 1, method=method,\n                                           cv=kfold)\n        assert_array_equal(predictions, predictions_y1)\n\n        predictions_y2 = cross_val_predict(est, X, y - 2, method=method,\n                                           cv=kfold)\n        assert_array_equal(predictions, predictions_y2)\n\n        predictions_ystr = cross_val_predict(est, X, y.astype('str'),\n                                             method=method, cv=kfold)\n        assert_array_equal(predictions, predictions_ystr)\n\n\ndef test_cross_val_predict_with_method():\n    check_cross_val_predict_with_method(LogisticRegression())\n\n\ndef test_cross_val_predict_method_checking():\n    # Regression test for issue #9639. Tests that cross_val_predict does not\n    # check estimator methods (e.g. predict_proba) before fitting\n    est = SGDClassifier(loss='log', random_state=2)\n    check_cross_val_predict_with_method(est)\n\n\ndef test_gridsearchcv_cross_val_predict_with_method():\n    est = GridSearchCV(LogisticRegression(random_state=42),\n                       {'C': [0.1, 1]},\n                       cv=2)\n    check_cross_val_predict_with_method(est)\n\n\ndef get_expected_predictions(X, y, cv, classes, est, method):\n\n    expected_predictions = np.zeros([len(y), classes])\n    func = getattr(est, method)\n\n    for train, test in cv.split(X, y):\n        est.fit(X[train], y[train])\n        expected_predictions_ = func(X[test])\n        # To avoid 2 dimensional indexing\n        if method is 'predict_proba':\n            exp_pred_test = np.zeros((len(test), classes))\n        else:\n            exp_pred_test = np.full((len(test), classes),\n                                    np.finfo(expected_predictions.dtype).min)\n        exp_pred_test[:, est.classes_] = expected_predictions_\n        expected_predictions[test] = exp_pred_test\n\n    return expected_predictions\n\n\ndef test_cross_val_predict_class_subset():\n\n    X = np.arange(200).reshape(100, 2)\n    y = np.array([x//10 for x in range(100)])\n    classes = 10\n\n    kfold3 = KFold(n_splits=3)\n    kfold4 = KFold(n_splits=4)\n\n    le = LabelEncoder()\n\n    methods = ['decision_function', 'predict_proba', 'predict_log_proba']\n    for method in methods:\n        est = LogisticRegression()\n\n        # Test with n_splits=3\n        predictions = cross_val_predict(est, X, y, method=method,\n                                        cv=kfold3)\n\n        # Runs a naive loop (should be same as cross_val_predict):\n        expected_predictions = get_expected_predictions(X, y, kfold3, classes,\n                                                        est, method)\n        assert_array_almost_equal(expected_predictions, predictions)\n\n        # Test with n_splits=4\n        predictions = cross_val_predict(est, X, y, method=method,\n                                        cv=kfold4)\n        expected_predictions = get_expected_predictions(X, y, kfold4, classes,\n                                                        est, method)\n        assert_array_almost_equal(expected_predictions, predictions)\n\n        # Testing unordered labels\n        y = shuffle(np.repeat(range(10), 10), random_state=0)\n        predictions = cross_val_predict(est, X, y, method=method,\n                                        cv=kfold3)\n        y = le.fit_transform(y)\n        expected_predictions = get_expected_predictions(X, y, kfold3, classes,\n                                                        est, method)\n        assert_array_almost_equal(expected_predictions, predictions)\n\n\ndef test_score_memmap():\n    # Ensure a scalar score of memmap type is accepted\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    clf = MockClassifier()\n    tf = tempfile.NamedTemporaryFile(mode='wb', delete=False)\n    tf.write(b'Hello world!!!!!')\n    tf.close()\n    scores = np.memmap(tf.name, dtype=np.float64)\n    score = np.memmap(tf.name, shape=(), mode='r', dtype=np.float64)\n    try:\n        cross_val_score(clf, X, y, scoring=lambda est, X, y: score)\n        # non-scalar should still fail\n        assert_raises(ValueError, cross_val_score, clf, X, y,\n                      scoring=lambda est, X, y: scores)\n    finally:\n        # Best effort to release the mmap file handles before deleting the\n        # backing file under Windows\n        scores, score = None, None\n        for _ in range(3):\n            try:\n                os.unlink(tf.name)\n                break\n            except WindowsError:\n                sleep(1.)\n\n\ndef test_permutation_test_score_pandas():\n    # check permutation_test_score doesn't destroy pandas dataframe\n    types = [(MockDataFrame, MockDataFrame)]\n    try:\n        from pandas import Series, DataFrame\n        types.append((Series, DataFrame))\n    except ImportError:\n        pass\n    for TargetType, InputFeatureType in types:\n        # X dataframe, y series\n        iris = load_iris()\n        X, y = iris.data, iris.target\n        X_df, y_ser = InputFeatureType(X), TargetType(y)\n        check_df = lambda x: isinstance(x, InputFeatureType)\n        check_series = lambda x: isinstance(x, TargetType)\n        clf = CheckingClassifier(check_X=check_df, check_y=check_series)\n        permutation_test_score(clf, X_df, y_ser)\n\n\ndef test_fit_and_score():\n    # Create a failing classifier to deliberately fail\n    failing_clf = FailingClassifier(FailingClassifier.FAILING_PARAMETER)\n    # dummy X data\n    X = np.arange(1, 10)\n    fit_and_score_args = [failing_clf, X, None, dict(), None, None, 0,\n                          None, None]\n    # passing error score to trigger the warning message\n    fit_and_score_kwargs = {'error_score': 0}\n    # check if the warning message type is as expected\n    assert_warns(FitFailedWarning, _fit_and_score, *fit_and_score_args,\n                 **fit_and_score_kwargs)\n    # since we're using FailingClassfier, our error will be the following\n    error_message = \"ValueError: Failing classifier failed as required\"\n    # the warning message we're expecting to see\n    warning_message = (\"Estimator fit failed. The score on this train-test \"\n                       \"partition for these parameters will be set to %f. \"\n                       \"Details: \\n%s\" % (fit_and_score_kwargs['error_score'],\n                                          error_message))\n    # check if the same warning is triggered\n    assert_warns_message(FitFailedWarning, warning_message, _fit_and_score,\n                         *fit_and_score_args, **fit_and_score_kwargs)\n\n    # check if exception is raised, with default error_score argument\n    assert_raise_message(ValueError, \"Failing classifier failed as required\",\n                         _fit_and_score, *fit_and_score_args)\n\n    # check if warning was raised, with default error_score argument\n    warning_message = (\"From version 0.22, errors during fit will result \"\n                       \"in a cross validation score of NaN by default. Use \"\n                       \"error_score='raise' if you want an exception \"\n                       \"raised or error_score=np.nan to adopt the \"\n                       \"behavior from version 0.22.\")\n    with pytest.raises(ValueError):\n        assert_warns_message(FutureWarning, warning_message, _fit_and_score,\n                             *fit_and_score_args)\n\n    fit_and_score_kwargs = {'error_score': 'raise'}\n    # check if exception was raised, with default error_score='raise'\n    assert_raise_message(ValueError, \"Failing classifier failed as required\",\n                         _fit_and_score, *fit_and_score_args,\n                         **fit_and_score_kwargs)\n"
    }
  ],
  "questions": [
    "Hi @jnothman, can I give it a shot?",
    "Hi @jnothman,\r\nI was checking this issue and I just saw that the tests for the validation_curve are in test_learning_curve, which is testing the deprecated implementation in learning_curve.py.\r\nShould this test moved in test_cross_validation where validation_curve is implemented now?"
  ],
  "golden_answers": [
    "Hi @jnothman,\r\nI was checking this issue and I just saw that the tests for the validation_curve are in test_learning_curve, which is testing the deprecated implementation in learning_curve.py.\r\nShould this test moved in test_cross_validation where validation_curve is implemented now?",
    "don't worry about the deprecated module. tests should be in\nmodel_selection/tests"
  ],
  "questions_generated": [
    "What is the purpose of adding support for 'fit_params' in 'validation_curve' and 'learning_curve' functions?",
    "Where should the tests for the 'validation_curve' and 'learning_curve' functionalities be located in the repository?",
    "How does the 'cross_validate' function utilize 'fit_params' within its implementation?",
    "What are the potential challenges or considerations when implementing 'fit_params' support in 'validation_curve' and 'learning_curve'?",
    "Why is it important for 'validation_curve' and 'learning_curve' to support 'fit_params', considering other model selection functions already do?",
    "What steps should be taken if there is confusion regarding where to implement tests for new functionality in the scikit-learn repository?"
  ],
  "golden_answers_generated": [
    "The 'fit_params' support in 'validation_curve' and 'learning_curve' functions would allow users to pass additional arguments, such as 'sample_weight', into the model being fitted. This feature is already supported in similar functions like 'cross_validate' and 'cross_val_score', and adding it to 'validation_curve' and 'learning_curve' would enhance consistency and flexibility across model selection routines.",
    "The tests for 'validation_curve' and 'learning_curve' should be located in the 'model_selection/tests' directory. This is the appropriate location for tests related to model selection functionalities, as indicated in the discussion context.",
    "The 'cross_validate' function uses the 'fit_params' parameter to pass additional arguments to the 'fit' method of the estimator. This allows for custom parameters, such as 'sample_weight', to be utilized during the fitting process, providing more control over the model training.",
    "Implementing 'fit_params' support requires ensuring that additional parameters are correctly passed to the estimator's 'fit' method without affecting existing functionality. This includes updating or adding new tests to verify that the parameters are handled correctly and do not introduce any regressions or unexpected behavior in the model selection routines.",
    "Adding 'fit_params' support to 'validation_curve' and 'learning_curve' ensures consistency across the scikit-learn library's model selection functions. It allows users to apply the same parameter customization to these functions as they do with 'cross_validate' and 'cross_val_score', making the library more intuitive and flexible for users who need to pass additional fitting parameters.",
    "If there is confusion about where to implement tests, it is advisable to check the current organization of tests in the repository and follow the existing structure. If uncertainty persists, developers may consult with maintainers or contributors familiar with the codebase, as demonstrated in the discussion where guidance was provided to place tests in the 'model_selection/tests' folder."
  ]
}