{
  "repo_name": "scikit-learn_scikit-learn",
  "issue_id": "14351",
  "issue_description": "# Remove redundant parameter assignment in examples/tests\n\nI don't think we're good at removing redundant parameter assignment in examples/tests.\r\ne.g., in the examples folder\r\n```\r\ngit grep \"cv=5\" | wc -l\r\n27\r\n```\r\n(we no longer need cv=5 because we've finished the deprecation).\r\nI can't come up with an automatic solution for this issue (an awkward solution is to manually go through all the API changes and search the repo).",
  "issue_comments": [
    {
      "id": 511188003,
      "user": "aalva14",
      "body": "I will look into this.\r\n"
    },
    {
      "id": 512094338,
      "user": "jackransomlovell",
      "body": "Hi @aalva14 & @qinhanmin2014 I'd really love to help on this if I can, could I collaborate with either or both of you?"
    },
    {
      "id": 512127106,
      "user": "aalva14",
      "body": "@jack-rl-ovell would have loved to collaborate, but my pull request has already been merged to the master. May be there is some other you can look up :)"
    },
    {
      "id": 512184302,
      "user": "Harsh2098",
      "body": "Close this issue then"
    },
    {
      "id": 512189202,
      "user": "rth",
      "body": "The merged PR https://github.com/scikit-learn/scikit-learn/pull/14360 was about fixing this for `cv=5`, but there are a lot of other parameters where this issue could apply. The strategy for detecting those could be,\r\n - to read though the example, check each time if the parameter used is the default. Once you found one like it, search (using `git grep`) though all the code base to find other instances of it.\r\n - read the release notes for 0.19 (two versions ago) and earlier, see which default parameters where changed, and search for those."
    },
    {
      "id": 512213510,
      "user": "Harsh2098",
      "body": "Alright, I would like to take up this"
    },
    {
      "id": 512355711,
      "user": "jackransomlovell",
      "body": "I would like to take this up as well!\n\nOn Wed, Jul 17, 2019 at 5:26 AM Harsh Mahajan <notifications@github.com>\nwrote:\n\n> Alright, I would like to take up this\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/14351?email_source=notifications&email_token=AL7OWPAYMK74SOT5ETRCUITP736YDA5CNFSM4IDIIKS2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2D4EBQ#issuecomment-512213510>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AL7OWPB2WCP25QCHIWL6GY3P736YDANCNFSM4IDIIKSQ>\n> .\n>\n"
    },
    {
      "id": 513474767,
      "user": "maxKenngott",
      "body": "If there's still space to work on this, I'd love to help out."
    },
    {
      "id": 515009718,
      "user": "vinidixit",
      "body": "@qinhanmin2014 Hey! I would really like to work on this issue. Please let me know if you've room for me?"
    },
    {
      "id": 515021815,
      "user": "qinhanmin2014",
      "body": "you can manually go through all the API changes and search the repo."
    },
    {
      "id": 517756336,
      "user": "Harsh2098",
      "body": "@qinhanmin2014 @rth Can you review this please? #14556"
    },
    {
      "id": 517808879,
      "user": "Harsh2098",
      "body": "@rth I removed unused tolerance parameter from `Perceptron`. Waiting for test results. Please review after test results pass. #14558 "
    },
    {
      "id": 518083457,
      "user": "ManishAradwad",
      "body": "Is this issue till open?? I'd like to help!!"
    },
    {
      "id": 518962663,
      "user": "anuraglal1",
      "body": "Is this issue still open? I would like to contribute!"
    },
    {
      "id": 524573342,
      "user": "NicolasHug",
      "body": "While I like the idea, I think we should still be careful when removing these, espcially in the tests: sometimes a test is easier to understand when we explicitly mention some parameters, even though these are the defaults. "
    }
  ],
  "text_context": "# Remove redundant parameter assignment in examples/tests\n\nI don't think we're good at removing redundant parameter assignment in examples/tests.\r\ne.g., in the examples folder\r\n```\r\ngit grep \"cv=5\" | wc -l\r\n27\r\n```\r\n(we no longer need cv=5 because we've finished the deprecation).\r\nI can't come up with an automatic solution for this issue (an awkward solution is to manually go through all the API changes and search the repo).\n\nI will look into this.\r\n\n\nHi @aalva14 & @qinhanmin2014 I'd really love to help on this if I can, could I collaborate with either or both of you?\n\n@jack-rl-ovell would have loved to collaborate, but my pull request has already been merged to the master. May be there is some other you can look up :)\n\nClose this issue then\n\nThe merged PR https://github.com/scikit-learn/scikit-learn/pull/14360 was about fixing this for `cv=5`, but there are a lot of other parameters where this issue could apply. The strategy for detecting those could be,\r\n - to read though the example, check each time if the parameter used is the default. Once you found one like it, search (using `git grep`) though all the code base to find other instances of it.\r\n - read the release notes for 0.19 (two versions ago) and earlier, see which default parameters where changed, and search for those.\n\nAlright, I would like to take up this\n\nI would like to take this up as well!\n\nOn Wed, Jul 17, 2019 at 5:26 AM Harsh Mahajan <notifications@github.com>\nwrote:\n\n> Alright, I would like to take up this\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/14351?email_source=notifications&email_token=AL7OWPAYMK74SOT5ETRCUITP736YDA5CNFSM4IDIIKS2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2D4EBQ#issuecomment-512213510>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AL7OWPB2WCP25QCHIWL6GY3P736YDANCNFSM4IDIIKSQ>\n> .\n>\n\n\nIf there's still space to work on this, I'd love to help out.\n\n@qinhanmin2014 Hey! I would really like to work on this issue. Please let me know if you've room for me?\n\nyou can manually go through all the API changes and search the repo.\n\n@qinhanmin2014 @rth Can you review this please? #14556\n\n@rth I removed unused tolerance parameter from `Perceptron`. Waiting for test results. Please review after test results pass. #14558 \n\nIs this issue till open?? I'd like to help!!\n\nIs this issue still open? I would like to contribute!\n\nWhile I like the idea, I think we should still be careful when removing these, espcially in the tests: sometimes a test is easier to understand when we explicitly mention some parameters, even though these are the defaults. ",
  "pr_link": "https://github.com/scikit-learn/scikit-learn/pull/14360",
  "code_context": [
    {
      "filename": "examples/applications/plot_face_recognition.py",
      "content": "\"\"\"\n===================================================\nFaces recognition example using eigenfaces and SVMs\n===================================================\n\nThe dataset used in this example is a preprocessed excerpt of the\n\"Labeled Faces in the Wild\", aka LFW_:\n\n  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n\n.. _LFW: http://vis-www.cs.umass.edu/lfw/\n\nExpected results for the top 5 most represented people in the dataset:\n\n================== ============ ======= ========== =======\n                   precision    recall  f1-score   support\n================== ============ ======= ========== =======\n     Ariel Sharon       0.67      0.92      0.77        13\n     Colin Powell       0.75      0.78      0.76        60\n  Donald Rumsfeld       0.78      0.67      0.72        27\n    George W Bush       0.86      0.86      0.86       146\nGerhard Schroeder       0.76      0.76      0.76        25\n      Hugo Chavez       0.67      0.67      0.67        15\n       Tony Blair       0.81      0.69      0.75        36\n\n      avg / total       0.80      0.80      0.80       322\n================== ============ ======= ========== =======\n\n\"\"\"\nfrom time import time\nimport logging\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\n\n\nprint(__doc__)\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n\n\n# #############################################################################\n# Download the data, if not already on disk and load it as numpy arrays\n\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n# introspect the images arrays to find the shapes (for plotting)\nn_samples, h, w = lfw_people.images.shape\n\n# for machine learning we use the 2 data directly (as relative pixel\n# positions info is ignored by this model)\nX = lfw_people.data\nn_features = X.shape[1]\n\n# the label to predict is the id of the person\ny = lfw_people.target\ntarget_names = lfw_people.target_names\nn_classes = target_names.shape[0]\n\nprint(\"Total dataset size:\")\nprint(\"n_samples: %d\" % n_samples)\nprint(\"n_features: %d\" % n_features)\nprint(\"n_classes: %d\" % n_classes)\n\n\n# #############################################################################\n# Split into a training set and a test set using a stratified k fold\n\n# split into a training and testing set\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42)\n\n\n# #############################################################################\n# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n# dataset): unsupervised feature extraction / dimensionality reduction\nn_components = 150\n\nprint(\"Extracting the top %d eigenfaces from %d faces\"\n      % (n_components, X_train.shape[0]))\nt0 = time()\npca = PCA(n_components=n_components, svd_solver='randomized',\n          whiten=True).fit(X_train)\nprint(\"done in %0.3fs\" % (time() - t0))\n\neigenfaces = pca.components_.reshape((n_components, h, w))\n\nprint(\"Projecting the input data on the eigenfaces orthonormal basis\")\nt0 = time()\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\nprint(\"done in %0.3fs\" % (time() - t0))\n\n\n# #############################################################################\n# Train a SVM classification model\n\nprint(\"Fitting the classifier to the training set\")\nt0 = time()\nparam_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\nclf = GridSearchCV(\n    SVC(kernel='rbf', class_weight='balanced'), param_grid\n)\nclf = clf.fit(X_train_pca, y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(\"Best estimator found by grid search:\")\nprint(clf.best_estimator_)\n\n\n# #############################################################################\n# Quantitative evaluation of the model quality on the test set\n\nprint(\"Predicting people's names on the test set\")\nt0 = time()\ny_pred = clf.predict(X_test_pca)\nprint(\"done in %0.3fs\" % (time() - t0))\n\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n\n\n# #############################################################################\n# Qualitative evaluation of the predictions using matplotlib\n\ndef plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(titles[i], size=12)\n        plt.xticks(())\n        plt.yticks(())\n\n\n# plot the result of the prediction on a portion of the test set\n\ndef title(y_pred, y_test, target_names, i):\n    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n\nprediction_titles = [title(y_pred, y_test, target_names, i)\n                     for i in range(y_pred.shape[0])]\n\nplot_gallery(X_test, prediction_titles, h, w)\n\n# plot the gallery of the most significative eigenfaces\n\neigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\nplot_gallery(eigenfaces, eigenface_titles, h, w)\n\nplt.show()\n"
    },
    {
      "filename": "examples/applications/plot_stock_market.py",
      "content": "\"\"\"\n=======================================\nVisualizing the stock market structure\n=======================================\n\nThis example employs several unsupervised learning techniques to extract\nthe stock market structure from variations in historical quotes.\n\nThe quantity that we use is the daily variation in quote price: quotes\nthat are linked tend to cofluctuate during a day.\n\n.. _stock_market:\n\nLearning a graph structure\n--------------------------\n\nWe use sparse inverse covariance estimation to find which quotes are\ncorrelated conditionally on the others. Specifically, sparse inverse\ncovariance gives us a graph, that is a list of connection. For each\nsymbol, the symbols that it is connected too are those useful to explain\nits fluctuations.\n\nClustering\n----------\n\nWe use clustering to group together quotes that behave similarly. Here,\namongst the :ref:`various clustering techniques <clustering>` available\nin the scikit-learn, we use :ref:`affinity_propagation` as it does\nnot enforce equal-size clusters, and it can choose automatically the\nnumber of clusters from the data.\n\nNote that this gives us a different indication than the graph, as the\ngraph reflects conditional relations between variables, while the\nclustering reflects marginal properties: variables clustered together can\nbe considered as having a similar impact at the level of the full stock\nmarket.\n\nEmbedding in 2D space\n---------------------\n\nFor visualization purposes, we need to lay out the different symbols on a\n2D canvas. For this we use :ref:`manifold` techniques to retrieve 2D\nembedding.\n\n\nVisualization\n-------------\n\nThe output of the 3 models are combined in a 2D graph where nodes\nrepresents the stocks and edges the:\n\n- cluster labels are used to define the color of the nodes\n- the sparse covariance model is used to display the strength of the edges\n- the 2D embedding is used to position the nodes in the plan\n\nThis example has a fair amount of visualization-related code, as\nvisualization is crucial here to display the graph. One of the challenge\nis to position the labels minimizing overlap. For this we use an\nheuristic based on the direction of the nearest neighbor along each\naxis.\n\"\"\"\n\n# Author: Gael Varoquaux gael.varoquaux@normalesup.org\n# License: BSD 3 clause\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\n\nimport pandas as pd\n\nfrom sklearn import cluster, covariance, manifold\n\nprint(__doc__)\n\n\n# #############################################################################\n# Retrieve the data from Internet\n\n# The data is from 2003 - 2008. This is reasonably calm: (not too long ago so\n# that we get high-tech firms, and before the 2008 crash). This kind of\n# historical data can be obtained for from APIs like the quandl.com and\n# alphavantage.co ones.\n\nsymbol_dict = {\n    'TOT': 'Total',\n    'XOM': 'Exxon',\n    'CVX': 'Chevron',\n    'COP': 'ConocoPhillips',\n    'VLO': 'Valero Energy',\n    'MSFT': 'Microsoft',\n    'IBM': 'IBM',\n    'TWX': 'Time Warner',\n    'CMCSA': 'Comcast',\n    'CVC': 'Cablevision',\n    'YHOO': 'Yahoo',\n    'DELL': 'Dell',\n    'HPQ': 'HP',\n    'AMZN': 'Amazon',\n    'TM': 'Toyota',\n    'CAJ': 'Canon',\n    'SNE': 'Sony',\n    'F': 'Ford',\n    'HMC': 'Honda',\n    'NAV': 'Navistar',\n    'NOC': 'Northrop Grumman',\n    'BA': 'Boeing',\n    'KO': 'Coca Cola',\n    'MMM': '3M',\n    'MCD': 'McDonald\\'s',\n    'PEP': 'Pepsi',\n    'K': 'Kellogg',\n    'UN': 'Unilever',\n    'MAR': 'Marriott',\n    'PG': 'Procter Gamble',\n    'CL': 'Colgate-Palmolive',\n    'GE': 'General Electrics',\n    'WFC': 'Wells Fargo',\n    'JPM': 'JPMorgan Chase',\n    'AIG': 'AIG',\n    'AXP': 'American express',\n    'BAC': 'Bank of America',\n    'GS': 'Goldman Sachs',\n    'AAPL': 'Apple',\n    'SAP': 'SAP',\n    'CSCO': 'Cisco',\n    'TXN': 'Texas Instruments',\n    'XRX': 'Xerox',\n    'WMT': 'Wal-Mart',\n    'HD': 'Home Depot',\n    'GSK': 'GlaxoSmithKline',\n    'PFE': 'Pfizer',\n    'SNY': 'Sanofi-Aventis',\n    'NVS': 'Novartis',\n    'KMB': 'Kimberly-Clark',\n    'R': 'Ryder',\n    'GD': 'General Dynamics',\n    'RTN': 'Raytheon',\n    'CVS': 'CVS',\n    'CAT': 'Caterpillar',\n    'DD': 'DuPont de Nemours'}\n\n\nsymbols, names = np.array(sorted(symbol_dict.items())).T\n\nquotes = []\n\nfor symbol in symbols:\n    print('Fetching quote history for %r' % symbol, file=sys.stderr)\n    url = ('https://raw.githubusercontent.com/scikit-learn/examples-data/'\n           'master/financial-data/{}.csv')\n    quotes.append(pd.read_csv(url.format(symbol)))\n\nclose_prices = np.vstack([q['close'] for q in quotes])\nopen_prices = np.vstack([q['open'] for q in quotes])\n\n# The daily variations of the quotes are what carry most information\nvariation = close_prices - open_prices\n\n\n# #############################################################################\n# Learn a graphical structure from the correlations\nedge_model = covariance.GraphicalLassoCV()\n\n# standardize the time series: using correlations rather than covariance\n# is more efficient for structure recovery\nX = variation.copy().T\nX /= X.std(axis=0)\nedge_model.fit(X)\n\n# #############################################################################\n# Cluster using affinity propagation\n\n_, labels = cluster.affinity_propagation(edge_model.covariance_)\nn_labels = labels.max()\n\nfor i in range(n_labels + 1):\n    print('Cluster %i: %s' % ((i + 1), ', '.join(names[labels == i])))\n\n# #############################################################################\n# Find a low-dimension embedding for visualization: find the best position of\n# the nodes (the stocks) on a 2D plane\n\n# We use a dense eigen_solver to achieve reproducibility (arpack is\n# initiated with random vectors that we don't control). In addition, we\n# use a large number of neighbors to capture the large-scale structure.\nnode_position_model = manifold.LocallyLinearEmbedding(\n    n_components=2, eigen_solver='dense', n_neighbors=6)\n\nembedding = node_position_model.fit_transform(X.T).T\n\n# #############################################################################\n# Visualization\nplt.figure(1, facecolor='w', figsize=(10, 8))\nplt.clf()\nax = plt.axes([0., 0., 1., 1.])\nplt.axis('off')\n\n# Display a graph of the partial correlations\npartial_correlations = edge_model.precision_.copy()\nd = 1 / np.sqrt(np.diag(partial_correlations))\npartial_correlations *= d\npartial_correlations *= d[:, np.newaxis]\nnon_zero = (np.abs(np.triu(partial_correlations, k=1)) > 0.02)\n\n# Plot the nodes using the coordinates of our embedding\nplt.scatter(embedding[0], embedding[1], s=100 * d ** 2, c=labels,\n            cmap=plt.cm.nipy_spectral)\n\n# Plot the edges\nstart_idx, end_idx = np.where(non_zero)\n# a sequence of (*line0*, *line1*, *line2*), where::\n#            linen = (x0, y0), (x1, y1), ... (xm, ym)\nsegments = [[embedding[:, start], embedding[:, stop]]\n            for start, stop in zip(start_idx, end_idx)]\nvalues = np.abs(partial_correlations[non_zero])\nlc = LineCollection(segments,\n                    zorder=0, cmap=plt.cm.hot_r,\n                    norm=plt.Normalize(0, .7 * values.max()))\nlc.set_array(values)\nlc.set_linewidths(15 * values)\nax.add_collection(lc)\n\n# Add a label to each node. The challenge here is that we want to\n# position the labels to avoid overlap with other labels\nfor index, (name, label, (x, y)) in enumerate(\n        zip(names, labels, embedding.T)):\n\n    dx = x - embedding[0]\n    dx[index] = 1\n    dy = y - embedding[1]\n    dy[index] = 1\n    this_dx = dx[np.argmin(np.abs(dy))]\n    this_dy = dy[np.argmin(np.abs(dx))]\n    if this_dx > 0:\n        horizontalalignment = 'left'\n        x = x + .002\n    else:\n        horizontalalignment = 'right'\n        x = x - .002\n    if this_dy > 0:\n        verticalalignment = 'bottom'\n        y = y + .002\n    else:\n        verticalalignment = 'top'\n        y = y - .002\n    plt.text(x, y, name, size=10,\n             horizontalalignment=horizontalalignment,\n             verticalalignment=verticalalignment,\n             bbox=dict(facecolor='w',\n                       edgecolor=plt.cm.nipy_spectral(label / float(n_labels)),\n                       alpha=.6))\n\nplt.xlim(embedding[0].min() - .15 * embedding[0].ptp(),\n         embedding[0].max() + .10 * embedding[0].ptp(),)\nplt.ylim(embedding[1].min() - .03 * embedding[1].ptp(),\n         embedding[1].max() + .03 * embedding[1].ptp())\n\nplt.show()\n"
    },
    {
      "filename": "examples/compose/plot_compare_reduction.py",
      "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\n=================================================================\nSelecting dimensionality reduction with Pipeline and GridSearchCV\n=================================================================\n\nThis example constructs a pipeline that does dimensionality\nreduction followed by prediction with a support vector\nclassifier. It demonstrates the use of ``GridSearchCV`` and\n``Pipeline`` to optimize over different classes of estimators in a\nsingle CV run -- unsupervised ``PCA`` and ``NMF`` dimensionality\nreductions are compared to univariate feature selection during\nthe grid search.\n\nAdditionally, ``Pipeline`` can be instantiated with the ``memory``\nargument to memoize the transformers within the pipeline, avoiding to fit\nagain the same transformers over and over.\n\nNote that the use of ``memory`` to enable caching becomes interesting when the\nfitting of a transformer is costly.\n\"\"\"\n\n###############################################################################\n# Illustration of ``Pipeline`` and ``GridSearchCV``\n###############################################################################\n# This section illustrates the use of a ``Pipeline`` with\n# ``GridSearchCV``\n\n# Authors: Robert McGibbon, Joel Nothman, Guillaume Lemaitre\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.decomposition import PCA, NMF\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nprint(__doc__)\n\npipe = Pipeline([\n    # the reduce_dim stage is populated by the param_grid\n    ('reduce_dim', 'passthrough'),\n    ('classify', LinearSVC(dual=False, max_iter=10000))\n])\n\nN_FEATURES_OPTIONS = [2, 4, 8]\nC_OPTIONS = [1, 10, 100, 1000]\nparam_grid = [\n    {\n        'reduce_dim': [PCA(iterated_power=7), NMF()],\n        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n        'classify__C': C_OPTIONS\n    },\n    {\n        'reduce_dim': [SelectKBest(chi2)],\n        'reduce_dim__k': N_FEATURES_OPTIONS,\n        'classify__C': C_OPTIONS\n    },\n]\nreducer_labels = ['PCA', 'NMF', 'KBest(chi2)']\n\ngrid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid)\ndigits = load_digits()\ngrid.fit(digits.data, digits.target)\n\nmean_scores = np.array(grid.cv_results_['mean_test_score'])\n# scores are in the order of param_grid iteration, which is alphabetical\nmean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n# select score for best C\nmean_scores = mean_scores.max(axis=0)\nbar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *\n               (len(reducer_labels) + 1) + .5)\n\nplt.figure()\nCOLORS = 'bgrcmyk'\nfor i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n    plt.bar(bar_offsets + i, reducer_scores, label=label, color=COLORS[i])\n\nplt.title(\"Comparing feature reduction techniques\")\nplt.xlabel('Reduced number of features')\nplt.xticks(bar_offsets + len(reducer_labels) / 2, N_FEATURES_OPTIONS)\nplt.ylabel('Digit classification accuracy')\nplt.ylim((0, 1))\nplt.legend(loc='upper left')\n\nplt.show()\n\n###############################################################################\n# Caching transformers within a ``Pipeline``\n###############################################################################\n# It is sometimes worthwhile storing the state of a specific transformer\n# since it could be used again. Using a pipeline in ``GridSearchCV`` triggers\n# such situations. Therefore, we use the argument ``memory`` to enable caching.\n#\n# .. warning::\n#     Note that this example is, however, only an illustration since for this\n#     specific case fitting PCA is not necessarily slower than loading the\n#     cache. Hence, use the ``memory`` constructor parameter when the fitting\n#     of a transformer is costly.\n\nfrom joblib import Memory\nfrom shutil import rmtree\n\n# Create a temporary folder to store the transformers of the pipeline\nlocation = 'cachedir'\nmemory = Memory(location=location, verbose=10)\ncached_pipe = Pipeline([('reduce_dim', PCA()),\n                        ('classify', LinearSVC(dual=False, max_iter=10000))],\n                       memory=memory)\n\n# This time, a cached pipeline will be used within the grid search\ngrid = GridSearchCV(cached_pipe, n_jobs=1, param_grid=param_grid)\ndigits = load_digits()\ngrid.fit(digits.data, digits.target)\n\n# Delete the temporary cache before exiting\nmemory.clear(warn=False)\nrmtree(location)\n\n###############################################################################\n# The ``PCA`` fitting is only computed at the evaluation of the first\n# configuration of the ``C`` parameter of the ``LinearSVC`` classifier. The\n# other configurations of ``C`` will trigger the loading of the cached ``PCA``\n# estimator data, leading to save processing time. Therefore, the use of\n# caching the pipeline using ``memory`` is highly beneficial when fitting\n# a transformer is costly.\n"
    },
    {
      "filename": "examples/compose/plot_digits_pipe.py",
      "content": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n\"\"\"\n=========================================================\nPipelining: chaining a PCA and a logistic regression\n=========================================================\n\nThe PCA does an unsupervised dimensionality reduction, while the logistic\nregression does the prediction.\n\nWe use a GridSearchCV to set the dimensionality of the PCA\n\n\"\"\"\nprint(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Define a pipeline to search for the best combination of PCA truncation\n# and classifier regularization.\nlogistic = SGDClassifier(loss='log', penalty='l2', early_stopping=True,\n                         max_iter=10000, tol=1e-5, random_state=0)\npca = PCA()\npipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n\ndigits = datasets.load_digits()\nX_digits = digits.data\ny_digits = digits.target\n\n# Parameters of pipelines can be set using ‘__’ separated parameter names:\nparam_grid = {\n    'pca__n_components': [5, 20, 30, 40, 50, 64],\n    'logistic__alpha': np.logspace(-4, 4, 5),\n}\nsearch = GridSearchCV(pipe, param_grid)\nsearch.fit(X_digits, y_digits)\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)\n\n# Plot the PCA spectrum\npca.fit(X_digits)\n\nfig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\nax0.plot(pca.explained_variance_ratio_, linewidth=2)\nax0.set_ylabel('PCA explained variance')\n\nax0.axvline(search.best_estimator_.named_steps['pca'].n_components,\n            linestyle=':', label='n_components chosen')\nax0.legend(prop=dict(size=12))\n\n# For each number of components, find the best classifier results\nresults = pd.DataFrame(search.cv_results_)\ncomponents_col = 'param_pca__n_components'\nbest_clfs = results.groupby(components_col).apply(\n    lambda g: g.nlargest(1, 'mean_test_score'))\n\nbest_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',\n               legend=False, ax=ax1)\nax1.set_ylabel('Classification accuracy (val)')\nax1.set_xlabel('n_components')\n\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "filename": "examples/compose/plot_feature_union.py",
      "content": "\"\"\"\n=================================================\nConcatenating multiple feature extraction methods\n=================================================\n\nIn many real-world examples, there are many ways to extract features from a\ndataset. Often it is beneficial to combine several methods to obtain good\nperformance. This example shows how to use ``FeatureUnion`` to combine\nfeatures obtained by PCA and univariate selection.\n\nCombining features using this transformer has the benefit that it allows\ncross validation and grid searches over the whole process.\n\nThe combination used in this example is not particularly helpful on this\ndataset and is only used to illustrate the usage of FeatureUnion.\n\"\"\"\n\n# Author: Andreas Mueller <amueller@ais.uni-bonn.de>\n#\n# License: BSD 3 clause\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\niris = load_iris()\n\nX, y = iris.data, iris.target\n\n# This dataset is way too high-dimensional. Better do PCA:\npca = PCA(n_components=2)\n\n# Maybe some original features where good, too?\nselection = SelectKBest(k=1)\n\n# Build estimator from PCA and Univariate selection:\n\ncombined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n\n# Use combined features to transform dataset:\nX_features = combined_features.fit(X, y).transform(X)\nprint(\"Combined space has\", X_features.shape[1], \"features\")\n\nsvm = SVC(kernel=\"linear\")\n\n# Do grid search over k, n_components and C:\n\npipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n\nparam_grid = dict(features__pca__n_components=[1, 2, 3],\n                  features__univ_select__k=[1, 2],\n                  svm__C=[0.1, 1, 10])\n\ngrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\ngrid_search.fit(X, y)\nprint(grid_search.best_estimator_)\n"
    },
    {
      "filename": "examples/covariance/plot_covariance_estimation.py",
      "content": "\"\"\"\n=======================================================================\nShrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood\n=======================================================================\n\nWhen working with covariance estimation, the usual approach is to use\na maximum likelihood estimator, such as the\n:class:`sklearn.covariance.EmpiricalCovariance`. It is unbiased, i.e. it\nconverges to the true (population) covariance when given many\nobservations. However, it can also be beneficial to regularize it, in\norder to reduce its variance; this, in turn, introduces some bias. This\nexample illustrates the simple regularization used in\n:ref:`shrunk_covariance` estimators. In particular, it focuses on how to\nset the amount of regularization, i.e. how to choose the bias-variance\ntrade-off.\n\nHere we compare 3 approaches:\n\n* Setting the parameter by cross-validating the likelihood on three folds\n  according to a grid of potential shrinkage parameters.\n\n* A close formula proposed by Ledoit and Wolf to compute\n  the asymptotically optimal regularization parameter (minimizing a MSE\n  criterion), yielding the :class:`sklearn.covariance.LedoitWolf`\n  covariance estimate.\n\n* An improvement of the Ledoit-Wolf shrinkage, the\n  :class:`sklearn.covariance.OAS`, proposed by Chen et al. Its\n  convergence is significantly better under the assumption that the data\n  are Gaussian, in particular for small samples.\n\nTo quantify estimation error, we plot the likelihood of unseen data for\ndifferent values of the shrinkage parameter. We also show the choices by\ncross-validation, or with the LedoitWolf and OAS estimates.\n\nNote that the maximum likelihood estimate corresponds to no shrinkage,\nand thus performs poorly. The Ledoit-Wolf estimate performs really well,\nas it is close to the optimal and is computational not costly. In this\nexample, the OAS estimate is a bit further away. Interestingly, both\napproaches outperform cross-validation, which is significantly most\ncomputationally costly.\n\n\"\"\"\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg\n\nfrom sklearn.covariance import LedoitWolf, OAS, ShrunkCovariance, \\\n    log_likelihood, empirical_covariance\nfrom sklearn.model_selection import GridSearchCV\n\n\n# #############################################################################\n# Generate sample data\nn_features, n_samples = 40, 20\nnp.random.seed(42)\nbase_X_train = np.random.normal(size=(n_samples, n_features))\nbase_X_test = np.random.normal(size=(n_samples, n_features))\n\n# Color samples\ncoloring_matrix = np.random.normal(size=(n_features, n_features))\nX_train = np.dot(base_X_train, coloring_matrix)\nX_test = np.dot(base_X_test, coloring_matrix)\n\n# #############################################################################\n# Compute the likelihood on test data\n\n# spanning a range of possible shrinkage coefficient values\nshrinkages = np.logspace(-2, 0, 30)\nnegative_logliks = [-ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test)\n                    for s in shrinkages]\n\n# under the ground-truth model, which we would not have access to in real\n# settings\nreal_cov = np.dot(coloring_matrix.T, coloring_matrix)\nemp_cov = empirical_covariance(X_train)\nloglik_real = -log_likelihood(emp_cov, linalg.inv(real_cov))\n\n# #############################################################################\n# Compare different approaches to setting the parameter\n\n# GridSearch for an optimal shrinkage coefficient\ntuned_parameters = [{'shrinkage': shrinkages}]\ncv = GridSearchCV(ShrunkCovariance(), tuned_parameters)\ncv.fit(X_train)\n\n# Ledoit-Wolf optimal shrinkage coefficient estimate\nlw = LedoitWolf()\nloglik_lw = lw.fit(X_train).score(X_test)\n\n# OAS coefficient estimate\noa = OAS()\nloglik_oa = oa.fit(X_train).score(X_test)\n\n# #############################################################################\n# Plot results\nfig = plt.figure()\nplt.title(\"Regularized covariance: likelihood and shrinkage coefficient\")\nplt.xlabel('Regularization parameter: shrinkage coefficient')\nplt.ylabel('Error: negative log-likelihood on test data')\n# range shrinkage curve\nplt.loglog(shrinkages, negative_logliks, label=\"Negative log-likelihood\")\n\nplt.plot(plt.xlim(), 2 * [loglik_real], '--r',\n         label=\"Real covariance likelihood\")\n\n# adjust view\nlik_max = np.amax(negative_logliks)\nlik_min = np.amin(negative_logliks)\nymin = lik_min - 6. * np.log((plt.ylim()[1] - plt.ylim()[0]))\nymax = lik_max + 10. * np.log(lik_max - lik_min)\nxmin = shrinkages[0]\nxmax = shrinkages[-1]\n# LW likelihood\nplt.vlines(lw.shrinkage_, ymin, -loglik_lw, color='magenta',\n           linewidth=3, label='Ledoit-Wolf estimate')\n# OAS likelihood\nplt.vlines(oa.shrinkage_, ymin, -loglik_oa, color='purple',\n           linewidth=3, label='OAS estimate')\n# best CV estimator likelihood\nplt.vlines(cv.best_estimator_.shrinkage, ymin,\n           -cv.best_estimator_.score(X_test), color='cyan',\n           linewidth=3, label='Cross-validation best estimate')\n\nplt.ylim(ymin, ymax)\nplt.xlim(xmin, xmax)\nplt.legend()\n\nplt.show()\n"
    },
    {
      "filename": "examples/covariance/plot_sparse_cov.py",
      "content": "\"\"\"\n======================================\nSparse inverse covariance estimation\n======================================\n\nUsing the GraphicalLasso estimator to learn a covariance and sparse precision\nfrom a small number of samples.\n\nTo estimate a probabilistic model (e.g. a Gaussian model), estimating the\nprecision matrix, that is the inverse covariance matrix, is as important\nas estimating the covariance matrix. Indeed a Gaussian model is\nparametrized by the precision matrix.\n\nTo be in favorable recovery conditions, we sample the data from a model\nwith a sparse inverse covariance matrix. In addition, we ensure that the\ndata is not too much correlated (limiting the largest coefficient of the\nprecision matrix) and that there a no small coefficients in the\nprecision matrix that cannot be recovered. In addition, with a small\nnumber of observations, it is easier to recover a correlation matrix\nrather than a covariance, thus we scale the time series.\n\nHere, the number of samples is slightly larger than the number of\ndimensions, thus the empirical covariance is still invertible. However,\nas the observations are strongly correlated, the empirical covariance\nmatrix is ill-conditioned and as a result its inverse --the empirical\nprecision matrix-- is very far from the ground truth.\n\nIf we use l2 shrinkage, as with the Ledoit-Wolf estimator, as the number\nof samples is small, we need to shrink a lot. As a result, the\nLedoit-Wolf precision is fairly close to the ground truth precision, that\nis not far from being diagonal, but the off-diagonal structure is lost.\n\nThe l1-penalized estimator can recover part of this off-diagonal\nstructure. It learns a sparse precision. It is not able to\nrecover the exact sparsity pattern: it detects too many non-zero\ncoefficients. However, the highest non-zero coefficients of the l1\nestimated correspond to the non-zero coefficients in the ground truth.\nFinally, the coefficients of the l1 precision estimate are biased toward\nzero: because of the penalty, they are all smaller than the corresponding\nground truth value, as can be seen on the figure.\n\nNote that, the color range of the precision matrices is tweaked to\nimprove readability of the figure. The full range of values of the\nempirical precision is not displayed.\n\nThe alpha parameter of the GraphicalLasso setting the sparsity of the model is\nset by internal cross-validation in the GraphicalLassoCV. As can be\nseen on figure 2, the grid to compute the cross-validation score is\niteratively refined in the neighborhood of the maximum.\n\"\"\"\nprint(__doc__)\n# author: Gael Varoquaux <gael.varoquaux@inria.fr>\n# License: BSD 3 clause\n# Copyright: INRIA\n\nimport numpy as np\nfrom scipy import linalg\nfrom sklearn.datasets import make_sparse_spd_matrix\nfrom sklearn.covariance import GraphicalLassoCV, ledoit_wolf\nimport matplotlib.pyplot as plt\n\n# #############################################################################\n# Generate the data\nn_samples = 60\nn_features = 20\n\nprng = np.random.RandomState(1)\nprec = make_sparse_spd_matrix(n_features, alpha=.98,\n                              smallest_coef=.4,\n                              largest_coef=.7,\n                              random_state=prng)\ncov = linalg.inv(prec)\nd = np.sqrt(np.diag(cov))\ncov /= d\ncov /= d[:, np.newaxis]\nprec *= d\nprec *= d[:, np.newaxis]\nX = prng.multivariate_normal(np.zeros(n_features), cov, size=n_samples)\nX -= X.mean(axis=0)\nX /= X.std(axis=0)\n\n# #############################################################################\n# Estimate the covariance\nemp_cov = np.dot(X.T, X) / n_samples\n\nmodel = GraphicalLassoCV()\nmodel.fit(X)\ncov_ = model.covariance_\nprec_ = model.precision_\n\nlw_cov_, _ = ledoit_wolf(X)\nlw_prec_ = linalg.inv(lw_cov_)\n\n# #############################################################################\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.subplots_adjust(left=0.02, right=0.98)\n\n# plot the covariances\ncovs = [('Empirical', emp_cov), ('Ledoit-Wolf', lw_cov_),\n        ('GraphicalLassoCV', cov_), ('True', cov)]\nvmax = cov_.max()\nfor i, (name, this_cov) in enumerate(covs):\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(this_cov, interpolation='nearest', vmin=-vmax, vmax=vmax,\n               cmap=plt.cm.RdBu_r)\n    plt.xticks(())\n    plt.yticks(())\n    plt.title('%s covariance' % name)\n\n\n# plot the precisions\nprecs = [('Empirical', linalg.inv(emp_cov)), ('Ledoit-Wolf', lw_prec_),\n         ('GraphicalLasso', prec_), ('True', prec)]\nvmax = .9 * prec_.max()\nfor i, (name, this_prec) in enumerate(precs):\n    ax = plt.subplot(2, 4, i + 5)\n    plt.imshow(np.ma.masked_equal(this_prec, 0),\n               interpolation='nearest', vmin=-vmax, vmax=vmax,\n               cmap=plt.cm.RdBu_r)\n    plt.xticks(())\n    plt.yticks(())\n    plt.title('%s precision' % name)\n    if hasattr(ax, 'set_facecolor'):\n        ax.set_facecolor('.7')\n    else:\n        ax.set_axis_bgcolor('.7')\n\n# plot the model selection metric\nplt.figure(figsize=(4, 3))\nplt.axes([.2, .15, .75, .7])\nplt.plot(model.cv_alphas_, np.mean(model.grid_scores_, axis=1), 'o-')\nplt.axvline(model.alpha_, color='.5')\nplt.title('Model selection')\nplt.ylabel('Cross-validation score')\nplt.xlabel('alpha')\n\nplt.show()\n"
    },
    {
      "filename": "examples/decomposition/plot_pca_vs_fa_model_selection.py",
      "content": "\"\"\"\n===============================================================\nModel selection with Probabilistic PCA and Factor Analysis (FA)\n===============================================================\n\nProbabilistic PCA and Factor Analysis are probabilistic models.\nThe consequence is that the likelihood of new data can be used\nfor model selection and covariance estimation.\nHere we compare PCA and FA with cross-validation on low rank data corrupted\nwith homoscedastic noise (noise variance\nis the same for each feature) or heteroscedastic noise (noise variance\nis the different for each feature). In a second step we compare the model\nlikelihood to the likelihoods obtained from shrinkage covariance estimators.\n\nOne can observe that with homoscedastic noise both FA and PCA succeed\nin recovering the size of the low rank subspace. The likelihood with PCA\nis higher than FA in this case. However PCA fails and overestimates\nthe rank when heteroscedastic noise is present. Under appropriate\ncircumstances the low rank models are more likely than shrinkage models.\n\nThe automatic estimation from\nAutomatic Choice of Dimensionality for PCA. NIPS 2000: 598-604\nby Thomas P. Minka is also compared.\n\n\"\"\"\n\n# Authors: Alexandre Gramfort\n#          Denis A. Engemann\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg\n\nfrom sklearn.decomposition import PCA, FactorAnalysis\nfrom sklearn.covariance import ShrunkCovariance, LedoitWolf\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nprint(__doc__)\n\n# #############################################################################\n# Create the data\n\nn_samples, n_features, rank = 1000, 50, 10\nsigma = 1.\nrng = np.random.RandomState(42)\nU, _, _ = linalg.svd(rng.randn(n_features, n_features))\nX = np.dot(rng.randn(n_samples, rank), U[:, :rank].T)\n\n# Adding homoscedastic noise\nX_homo = X + sigma * rng.randn(n_samples, n_features)\n\n# Adding heteroscedastic noise\nsigmas = sigma * rng.rand(n_features) + sigma / 2.\nX_hetero = X + rng.randn(n_samples, n_features) * sigmas\n\n# #############################################################################\n# Fit the models\n\nn_components = np.arange(0, n_features, 5)  # options for n_components\n\n\ndef compute_scores(X):\n    pca = PCA(svd_solver='full')\n    fa = FactorAnalysis()\n\n    pca_scores, fa_scores = [], []\n    for n in n_components:\n        pca.n_components = n\n        fa.n_components = n\n        pca_scores.append(np.mean(cross_val_score(pca, X)))\n        fa_scores.append(np.mean(cross_val_score(fa, X)))\n\n    return pca_scores, fa_scores\n\n\ndef shrunk_cov_score(X):\n    shrinkages = np.logspace(-2, 0, 30)\n    cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages})\n    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))\n\n\ndef lw_score(X):\n    return np.mean(cross_val_score(LedoitWolf(), X))\n\n\nfor X, title in [(X_homo, 'Homoscedastic Noise'),\n                 (X_hetero, 'Heteroscedastic Noise')]:\n    pca_scores, fa_scores = compute_scores(X)\n    n_components_pca = n_components[np.argmax(pca_scores)]\n    n_components_fa = n_components[np.argmax(fa_scores)]\n\n    pca = PCA(svd_solver='full', n_components='mle')\n    pca.fit(X)\n    n_components_pca_mle = pca.n_components_\n\n    print(\"best n_components by PCA CV = %d\" % n_components_pca)\n    print(\"best n_components by FactorAnalysis CV = %d\" % n_components_fa)\n    print(\"best n_components by PCA MLE = %d\" % n_components_pca_mle)\n\n    plt.figure()\n    plt.plot(n_components, pca_scores, 'b', label='PCA scores')\n    plt.plot(n_components, fa_scores, 'r', label='FA scores')\n    plt.axvline(rank, color='g', label='TRUTH: %d' % rank, linestyle='-')\n    plt.axvline(n_components_pca, color='b',\n                label='PCA CV: %d' % n_components_pca, linestyle='--')\n    plt.axvline(n_components_fa, color='r',\n                label='FactorAnalysis CV: %d' % n_components_fa,\n                linestyle='--')\n    plt.axvline(n_components_pca_mle, color='k',\n                label='PCA MLE: %d' % n_components_pca_mle, linestyle='--')\n\n    # compare with other covariance estimators\n    plt.axhline(shrunk_cov_score(X), color='violet',\n                label='Shrunk Covariance MLE', linestyle='-.')\n    plt.axhline(lw_score(X), color='orange',\n                label='LedoitWolf MLE' % n_components_pca_mle, linestyle='-.')\n\n    plt.xlabel('nb of components')\n    plt.ylabel('CV scores')\n    plt.legend(loc='lower right')\n    plt.title(title)\n\nplt.show()\n"
    },
    {
      "filename": "examples/exercises/plot_cv_digits.py",
      "content": "\"\"\"\n=============================================\nCross-validation on Digits Dataset Exercise\n=============================================\n\nA tutorial exercise using Cross-validation with an SVM on the Digits dataset.\n\nThis exercise is used in the :ref:`cv_generators_tut` part of the\n:ref:`model_selection_tut` section of the :ref:`stat_learn_tut_index`.\n\"\"\"\nprint(__doc__)\n\n\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import datasets, svm\n\ndigits = datasets.load_digits()\nX = digits.data\ny = digits.target\n\nsvc = svm.SVC(kernel='linear')\nC_s = np.logspace(-10, 0, 10)\n\nscores = list()\nscores_std = list()\nfor C in C_s:\n    svc.C = C\n    this_scores = cross_val_score(svc, X, y, n_jobs=1)\n    scores.append(np.mean(this_scores))\n    scores_std.append(np.std(this_scores))\n\n# Do the plotting\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.semilogx(C_s, scores)\nplt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--')\nplt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'b--')\nlocs, labels = plt.yticks()\nplt.yticks(locs, list(map(lambda x: \"%g\" % x, locs)))\nplt.ylabel('CV score')\nplt.xlabel('Parameter C')\nplt.ylim(0, 1.1)\nplt.show()\n"
    },
    {
      "filename": "examples/feature_selection/plot_select_from_model_boston.py",
      "content": "\"\"\"\n===================================================\nFeature selection using SelectFromModel and LassoCV\n===================================================\n\nUse SelectFromModel meta-transformer along with Lasso to select the best\ncouple of features from the Boston dataset.\n\"\"\"\n# Author: Manoj Kumar <mks542@nyu.edu>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV\n\n# Load the boston dataset.\nboston = load_boston()\nX, y = boston['data'], boston['target']\n\n# We use the base estimator LassoCV since the L1 norm promotes sparsity of features.\nclf = LassoCV()\n\n# Set a minimum threshold of 0.25\nsfm = SelectFromModel(clf, threshold=0.25)\nsfm.fit(X, y)\nn_features = sfm.transform(X).shape[1]\n\n# Reset the threshold till the number of features equals two.\n# Note that the attribute can be set directly instead of repeatedly\n# fitting the metatransformer.\nwhile n_features > 2:\n    sfm.threshold += 0.1\n    X_transform = sfm.transform(X)\n    n_features = X_transform.shape[1]\n\n# Plot the selected two features from X.\nplt.title(\n    \"Features selected from Boston using SelectFromModel with \"\n    \"threshold %0.3f.\" % sfm.threshold)\nfeature1 = X_transform[:, 0]\nfeature2 = X_transform[:, 1] \nplt.plot(feature1, feature2, 'r.')\nplt.xlabel(\"Feature number 1\")\nplt.ylabel(\"Feature number 2\")\nplt.ylim([np.min(feature2), np.max(feature2)])\nplt.show()\n"
    },
    {
      "filename": "examples/gaussian_process/plot_compare_gpr_krr.py",
      "content": "\"\"\"\n==========================================================\nComparison of kernel ridge and Gaussian process regression\n==========================================================\n\nBoth kernel ridge regression (KRR) and Gaussian process regression (GPR) learn\na target function by employing internally the \"kernel trick\". KRR learns a\nlinear function in the space induced by the respective kernel which corresponds\nto a non-linear function in the original space. The linear function in the\nkernel space is chosen based on the mean-squared error loss with\nridge regularization. GPR uses the kernel to define the covariance of\na prior distribution over the target functions and uses the observed training\ndata to define a likelihood function. Based on Bayes theorem, a (Gaussian)\nposterior distribution over target functions is defined, whose mean is used\nfor prediction.\n\nA major difference is that GPR can choose the kernel's hyperparameters based\non gradient-ascent on the marginal likelihood function while KRR needs to\nperform a grid search on a cross-validated loss function (mean-squared error\nloss). A further difference is that GPR learns a generative, probabilistic\nmodel of the target function and can thus provide meaningful confidence\nintervals and posterior samples along with the predictions while KRR only\nprovides predictions.\n\nThis example illustrates both methods on an artificial dataset, which\nconsists of a sinusoidal target function and strong noise. The figure compares\nthe learned model of KRR and GPR based on a ExpSineSquared kernel, which is\nsuited for learning periodic functions. The kernel's hyperparameters control\nthe smoothness (l) and periodicity of the kernel (p). Moreover, the noise level\nof the data is learned explicitly by GPR by an additional WhiteKernel component\nin the kernel and by the regularization parameter alpha of KRR.\n\nThe figure shows that both methods learn reasonable models of the target\nfunction. GPR correctly identifies the periodicity of the function to be\nroughly 2*pi (6.28), while KRR chooses the doubled periodicity 4*pi. Besides\nthat, GPR provides reasonable confidence bounds on the prediction which are not\navailable for KRR. A major difference between the two methods is the time\nrequired for fitting and predicting: while fitting KRR is fast in principle,\nthe grid-search for hyperparameter optimization scales exponentially with the\nnumber of hyperparameters (\"curse of dimensionality\"). The gradient-based\noptimization of the parameters in GPR does not suffer from this exponential\nscaling and is thus considerable faster on this example with 3-dimensional\nhyperparameter space. The time for predicting is similar; however, generating\nthe variance of the predictive distribution of GPR takes considerable longer\nthan just predicting the mean.\n\"\"\"\nprint(__doc__)\n\n# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD 3 clause\n\n\nimport time\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared\n\nrng = np.random.RandomState(0)\n\n# Generate sample data\nX = 15 * rng.rand(100, 1)\ny = np.sin(X).ravel()\ny += 3 * (0.5 - rng.rand(X.shape[0]))  # add noise\n\n# Fit KernelRidge with parameter selection based on 5-fold cross validation\nparam_grid = {\"alpha\": [1e0, 1e-1, 1e-2, 1e-3],\n              \"kernel\": [ExpSineSquared(l, p)\n                         for l in np.logspace(-2, 2, 10)\n                         for p in np.logspace(0, 2, 10)]}\nkr = GridSearchCV(KernelRidge(), param_grid=param_grid)\nstime = time.time()\nkr.fit(X, y)\nprint(\"Time for KRR fitting: %.3f\" % (time.time() - stime))\n\ngp_kernel = ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) \\\n    + WhiteKernel(1e-1)\ngpr = GaussianProcessRegressor(kernel=gp_kernel)\nstime = time.time()\ngpr.fit(X, y)\nprint(\"Time for GPR fitting: %.3f\" % (time.time() - stime))\n\n# Predict using kernel ridge\nX_plot = np.linspace(0, 20, 10000)[:, None]\nstime = time.time()\ny_kr = kr.predict(X_plot)\nprint(\"Time for KRR prediction: %.3f\" % (time.time() - stime))\n\n# Predict using gaussian process regressor\nstime = time.time()\ny_gpr = gpr.predict(X_plot, return_std=False)\nprint(\"Time for GPR prediction: %.3f\" % (time.time() - stime))\n\nstime = time.time()\ny_gpr, y_std = gpr.predict(X_plot, return_std=True)\nprint(\"Time for GPR prediction with standard-deviation: %.3f\"\n      % (time.time() - stime))\n\n# Plot results\nplt.figure(figsize=(10, 5))\nlw = 2\nplt.scatter(X, y, c='k', label='data')\nplt.plot(X_plot, np.sin(X_plot), color='navy', lw=lw, label='True')\nplt.plot(X_plot, y_kr, color='turquoise', lw=lw,\n         label='KRR (%s)' % kr.best_params_)\nplt.plot(X_plot, y_gpr, color='darkorange', lw=lw,\n         label='GPR (%s)' % gpr.kernel_)\nplt.fill_between(X_plot[:, 0], y_gpr - y_std, y_gpr + y_std, color='darkorange',\n                 alpha=0.2)\nplt.xlabel('data')\nplt.ylabel('target')\nplt.xlim(0, 20)\nplt.ylim(-4, 4)\nplt.title('GPR versus Kernel Ridge')\nplt.legend(loc=\"best\",  scatterpoints=1, prop={'size': 8})\nplt.show()\n"
    },
    {
      "filename": "examples/linear_model/plot_omp.py",
      "content": "\"\"\"\n===========================\nOrthogonal Matching Pursuit\n===========================\n\nUsing orthogonal matching pursuit for recovering a sparse signal from a noisy\nmeasurement encoded with a dictionary\n\"\"\"\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import OrthogonalMatchingPursuit\nfrom sklearn.linear_model import OrthogonalMatchingPursuitCV\nfrom sklearn.datasets import make_sparse_coded_signal\n\nn_components, n_features = 512, 100\nn_nonzero_coefs = 17\n\n# generate the data\n\n# y = Xw\n# |x|_0 = n_nonzero_coefs\n\ny, X, w = make_sparse_coded_signal(n_samples=1,\n                                   n_components=n_components,\n                                   n_features=n_features,\n                                   n_nonzero_coefs=n_nonzero_coefs,\n                                   random_state=0)\n\nidx, = w.nonzero()\n\n# distort the clean signal\ny_noisy = y + 0.05 * np.random.randn(len(y))\n\n# plot the sparse signal\nplt.figure(figsize=(7, 7))\nplt.subplot(4, 1, 1)\nplt.xlim(0, 512)\nplt.title(\"Sparse signal\")\nplt.stem(idx, w[idx], use_line_collection=True)\n\n# plot the noise-free reconstruction\nomp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)\nomp.fit(X, y)\ncoef = omp.coef_\nidx_r, = coef.nonzero()\nplt.subplot(4, 1, 2)\nplt.xlim(0, 512)\nplt.title(\"Recovered signal from noise-free measurements\")\nplt.stem(idx_r, coef[idx_r], use_line_collection=True)\n\n# plot the noisy reconstruction\nomp.fit(X, y_noisy)\ncoef = omp.coef_\nidx_r, = coef.nonzero()\nplt.subplot(4, 1, 3)\nplt.xlim(0, 512)\nplt.title(\"Recovered signal from noisy measurements\")\nplt.stem(idx_r, coef[idx_r], use_line_collection=True)\n\n# plot the noisy reconstruction with number of non-zeros set by CV\nomp_cv = OrthogonalMatchingPursuitCV()\nomp_cv.fit(X, y_noisy)\ncoef = omp_cv.coef_\nidx_r, = coef.nonzero()\nplt.subplot(4, 1, 4)\nplt.xlim(0, 512)\nplt.title(\"Recovered signal from noisy measurements with CV\")\nplt.stem(idx_r, coef[idx_r], use_line_collection=True)\n\nplt.subplots_adjust(0.06, 0.04, 0.94, 0.90, 0.20, 0.38)\nplt.suptitle('Sparse signal recovery with Orthogonal Matching Pursuit',\n             fontsize=16)\nplt.show()\n"
    },
    {
      "filename": "examples/model_selection/plot_grid_search_digits.py",
      "content": "\"\"\"\n============================================================\nParameter estimation using grid search with cross-validation\n============================================================\n\nThis examples shows how a classifier is optimized by cross-validation,\nwhich is done using the :class:`sklearn.model_selection.GridSearchCV` object\non a development set that comprises only half of the available labeled data.\n\nThe performance of the selected hyper-parameters and trained model is\nthen measured on a dedicated evaluation set that was not used during\nthe model selection step.\n\nMore details on tools available for model selection can be found in the\nsections on :ref:`cross_validation` and :ref:`grid_search`.\n\n\"\"\"\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC\n\nprint(__doc__)\n\n# Loading the Digits dataset\ndigits = datasets.load_digits()\n\n# To apply an classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.images)\nX = digits.images.reshape((n_samples, -1))\ny = digits.target\n\n# Split the dataset in two equal parts\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=0)\n\n# Set the parameters by cross-validation\ntuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n\nscores = ['precision', 'recall']\n\nfor score in scores:\n    print(\"# Tuning hyper-parameters for %s\" % score)\n    print()\n\n    clf = GridSearchCV(\n        SVC(), tuned_parameters, scoring='%s_macro' % score\n    )\n    clf.fit(X_train, y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(clf.best_params_)\n    print()\n    print(\"Grid scores on development set:\")\n    print()\n    means = clf.cv_results_['mean_test_score']\n    stds = clf.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n        print(\"%0.3f (+/-%0.03f) for %r\"\n              % (mean, std * 2, params))\n    print()\n\n    print(\"Detailed classification report:\")\n    print()\n    print(\"The model is trained on the full development set.\")\n    print(\"The scores are computed on the full evaluation set.\")\n    print()\n    y_true, y_pred = y_test, clf.predict(X_test)\n    print(classification_report(y_true, y_pred))\n    print()\n\n# Note the problem is too easy: the hyperparameter plateau is too flat and the\n# output model is the same for precision and recall with ties in quality.\n"
    },
    {
      "filename": "examples/model_selection/plot_multi_metric_evaluation.py",
      "content": "\"\"\"\n============================================================================\nDemonstration of multi-metric evaluation on cross_val_score and GridSearchCV\n============================================================================\n\nMultiple metric parameter search can be done by setting the ``scoring``\nparameter to a list of metric scorer names or a dict mapping the scorer names\nto the scorer callables.\n\nThe scores of all the scorers are available in the ``cv_results_`` dict at keys\nending in ``'_<scorer_name>'`` (``'mean_test_precision'``,\n``'rank_test_precision'``, etc...)\n\nThe ``best_estimator_``, ``best_index_``, ``best_score_`` and ``best_params_``\ncorrespond to the scorer (key) that is set to the ``refit`` attribute.\n\"\"\"\n\n# Author: Raghav RV <rvraghav93@gmail.com>\n# License: BSD\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_hastie_10_2\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nprint(__doc__)\n\n###############################################################################\n# Running ``GridSearchCV`` using multiple evaluation metrics\n# ----------------------------------------------------------\n#\n\nX, y = make_hastie_10_2(n_samples=8000, random_state=42)\n\n# The scorers can be either be one of the predefined metric strings or a scorer\n# callable, like the one returned by make_scorer\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n\n# Setting refit='AUC', refits an estimator on the whole dataset with the\n# parameter setting that has the best cross-validated AUC score.\n# That estimator is made available at ``gs.best_estimator_`` along with\n# parameters like ``gs.best_score_``, ``gs.best_params_`` and\n# ``gs.best_index_``\ngs = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                  param_grid={'min_samples_split': range(2, 403, 10)},\n                  scoring=scoring, refit='AUC', return_train_score=True)\ngs.fit(X, y)\nresults = gs.cv_results_\n\n###############################################################################\n# Plotting the result\n# -------------------\n\nplt.figure(figsize=(13, 13))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n          fontsize=16)\n\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Score\")\n\nax = plt.gca()\nax.set_xlim(0, 402)\nax.set_ylim(0.73, 1)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_min_samples_split'].data, dtype=float)\n\nfor scorer, color in zip(sorted(scoring), ['g', 'k']):\n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = results['mean_test_%s' % scorer][best_index]\n\n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid(False)\nplt.show()\n"
    },
    {
      "filename": "examples/model_selection/plot_randomized_search.py",
      "content": "\"\"\"\n=========================================================================\nComparing randomized search and grid search for hyperparameter estimation\n=========================================================================\n\nCompare randomized search and grid search for optimizing hyperparameters of a\nrandom forest.\nAll parameters that influence the learning are searched simultaneously\n(except for the number of estimators, which poses a time / quality tradeoff).\n\nThe randomized search and the grid search explore exactly the same space of\nparameters. The result in parameter settings is quite similar, while the run\ntime for randomized search is drastically lower.\n\nThe performance is slightly worse for the randomized search, though this\nis most likely a noise effect and would not carry over to a held-out test set.\n\nNote that in practice, one would not search over this many different parameters\nsimultaneously using grid search, but pick only the ones deemed most important.\n\"\"\"\nprint(__doc__)\n\nimport numpy as np\n\nfrom time import time\nfrom scipy.stats import randint as sp_randint\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.datasets import load_digits\nfrom sklearn.ensemble import RandomForestClassifier\n\n# get some data\ndigits = load_digits()\nX, y = digits.data, digits.target\n\n# build a classifier\nclf = RandomForestClassifier(n_estimators=20)\n\n\n# Utility function to report best scores\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\n\n\n# specify parameters and distributions to sample from\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": sp_randint(1, 11),\n              \"min_samples_split\": sp_randint(2, 11),\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# run randomized search\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   n_iter=n_iter_search)\n\nstart = time()\nrandom_search.fit(X, y)\nprint(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n      \" parameter settings.\" % ((time() - start), n_iter_search))\nreport(random_search.cv_results_)\n\n# use a full grid over all parameters\nparam_grid = {\"max_depth\": [3, None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# run grid search\ngrid_search = GridSearchCV(clf, param_grid=param_grid)\nstart = time()\ngrid_search.fit(X, y)\n\nprint(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n      % (time() - start, len(grid_search.cv_results_['params'])))\nreport(grid_search.cv_results_)\n"
    },
    {
      "filename": "examples/model_selection/plot_validation_curve.py",
      "content": "\"\"\"\n==========================\nPlotting Validation Curves\n==========================\n\nIn this plot you can see the training scores and validation scores of an SVM\nfor different values of the kernel parameter gamma. For very low values of\ngamma, you can see that both the training score and the validation score are\nlow. This is called underfitting. Medium values of gamma will result in high\nvalues for both scores, i.e. the classifier is performing fairly well. If gamma\nis too high, the classifier will overfit, which means that the training score\nis good but the validation score is poor.\n\"\"\"\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import validation_curve\n\ndigits = load_digits()\nX, y = digits.data, digits.target\n\nparam_range = np.logspace(-6, -1, 5)\ntrain_scores, test_scores = validation_curve(\n    SVC(), X, y, param_name=\"gamma\", param_range=param_range,\n    scoring=\"accuracy\", n_jobs=1)\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nplt.title(\"Validation Curve with SVM\")\nplt.xlabel(r\"$\\gamma$\")\nplt.ylabel(\"Score\")\nplt.ylim(0.0, 1.1)\nlw = 2\nplt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n             color=\"darkorange\", lw=lw)\nplt.fill_between(param_range, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\nplt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n             color=\"navy\", lw=lw)\nplt.fill_between(param_range, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\nplt.legend(loc=\"best\")\nplt.show()\n"
    },
    {
      "filename": "examples/neighbors/plot_digits_kde_sampling.py",
      "content": "\"\"\"\n=========================\nKernel Density Estimation\n=========================\n\nThis example shows how kernel density estimation (KDE), a powerful\nnon-parametric density estimation technique, can be used to learn\na generative model for a dataset.  With this generative model in place,\nnew samples can be drawn.  These new samples reflect the underlying model\nof the data.\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\n\n# load the data\ndigits = load_digits()\n\n# project the 64-dimensional data to a lower dimension\npca = PCA(n_components=15, whiten=False)\ndata = pca.fit_transform(digits.data)\n\n# use grid search cross-validation to optimize the bandwidth\nparams = {'bandwidth': np.logspace(-1, 1, 20)}\ngrid = GridSearchCV(KernelDensity(), params)\ngrid.fit(data)\n\nprint(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n\n# use the best estimator to compute the kernel density estimate\nkde = grid.best_estimator_\n\n# sample 44 new points from the data\nnew_data = kde.sample(44, random_state=0)\nnew_data = pca.inverse_transform(new_data)\n\n# turn data into a 4x11 grid\nnew_data = new_data.reshape((4, 11, -1))\nreal_data = digits.data[:44].reshape((4, 11, -1))\n\n# plot real digits and resampled digits\nfig, ax = plt.subplots(9, 11, subplot_kw=dict(xticks=[], yticks=[]))\nfor j in range(11):\n    ax[4, j].set_visible(False)\n    for i in range(4):\n        im = ax[i, j].imshow(real_data[i, j].reshape((8, 8)),\n                             cmap=plt.cm.binary, interpolation='nearest')\n        im.set_clim(0, 16)\n        im = ax[i + 5, j].imshow(new_data[i, j].reshape((8, 8)),\n                                 cmap=plt.cm.binary, interpolation='nearest')\n        im.set_clim(0, 16)\n\nax[0, 5].set_title('Selection from the input data')\nax[5, 5].set_title('\"New\" digits drawn from the kernel density model')\n\nplt.show()\n"
    },
    {
      "filename": "examples/plot_kernel_ridge_regression.py",
      "content": "\"\"\"\n=============================================\nComparison of kernel ridge regression and SVR\n=============================================\n\nBoth kernel ridge regression (KRR) and SVR learn a non-linear function by\nemploying the kernel trick, i.e., they learn a linear function in the space\ninduced by the respective kernel which corresponds to a non-linear function in\nthe original space. They differ in the loss functions (ridge versus\nepsilon-insensitive loss). In contrast to SVR, fitting a KRR can be done in\nclosed-form and is typically faster for medium-sized datasets. On the other\nhand, the learned model is non-sparse and thus slower than SVR at\nprediction-time.\n\nThis example illustrates both methods on an artificial dataset, which\nconsists of a sinusoidal target function and strong noise added to every fifth\ndatapoint. The first figure compares the learned model of KRR and SVR when both\ncomplexity/regularization and bandwidth of the RBF kernel are optimized using\ngrid-search. The learned functions are very similar; however, fitting KRR is\napprox. seven times faster than fitting SVR (both with grid-search). However,\nprediction of 100000 target values is more than tree times faster with SVR\nsince it has learned a sparse model using only approx. 1/3 of the 100 training\ndatapoints as support vectors.\n\nThe next figure compares the time for fitting and prediction of KRR and SVR for\ndifferent sizes of the training set. Fitting KRR is faster than SVR for medium-\nsized training sets (less than 1000 samples); however, for larger training sets\nSVR scales better. With regard to prediction time, SVR is faster than\nKRR for all sizes of the training set because of the learned sparse\nsolution. Note that the degree of sparsity and thus the prediction time depends\non the parameters epsilon and C of the SVR.\n\"\"\"\n\n# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD 3 clause\n\n\nimport time\n\nimport numpy as np\n\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.kernel_ridge import KernelRidge\nimport matplotlib.pyplot as plt\n\nrng = np.random.RandomState(0)\n\n# #############################################################################\n# Generate sample data\nX = 5 * rng.rand(10000, 1)\ny = np.sin(X).ravel()\n\n# Add noise to targets\ny[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\n\nX_plot = np.linspace(0, 5, 100000)[:, None]\n\n# #############################################################################\n# Fit regression model\ntrain_size = 100\nsvr = GridSearchCV(SVR(kernel='rbf', gamma=0.1),\n                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n                               \"gamma\": np.logspace(-2, 2, 5)})\n\nkr = GridSearchCV(KernelRidge(kernel='rbf', gamma=0.1),\n                  param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n                              \"gamma\": np.logspace(-2, 2, 5)})\n\nt0 = time.time()\nsvr.fit(X[:train_size], y[:train_size])\nsvr_fit = time.time() - t0\nprint(\"SVR complexity and bandwidth selected and model fitted in %.3f s\"\n      % svr_fit)\n\nt0 = time.time()\nkr.fit(X[:train_size], y[:train_size])\nkr_fit = time.time() - t0\nprint(\"KRR complexity and bandwidth selected and model fitted in %.3f s\"\n      % kr_fit)\n\nsv_ratio = svr.best_estimator_.support_.shape[0] / train_size\nprint(\"Support vector ratio: %.3f\" % sv_ratio)\n\nt0 = time.time()\ny_svr = svr.predict(X_plot)\nsvr_predict = time.time() - t0\nprint(\"SVR prediction for %d inputs in %.3f s\"\n      % (X_plot.shape[0], svr_predict))\n\nt0 = time.time()\ny_kr = kr.predict(X_plot)\nkr_predict = time.time() - t0\nprint(\"KRR prediction for %d inputs in %.3f s\"\n      % (X_plot.shape[0], kr_predict))\n\n\n# #############################################################################\n# Look at the results\nsv_ind = svr.best_estimator_.support_\nplt.scatter(X[sv_ind], y[sv_ind], c='r', s=50, label='SVR support vectors',\n            zorder=2, edgecolors=(0, 0, 0))\nplt.scatter(X[:100], y[:100], c='k', label='data', zorder=1,\n            edgecolors=(0, 0, 0))\nplt.plot(X_plot, y_svr, c='r',\n         label='SVR (fit: %.3fs, predict: %.3fs)' % (svr_fit, svr_predict))\nplt.plot(X_plot, y_kr, c='g',\n         label='KRR (fit: %.3fs, predict: %.3fs)' % (kr_fit, kr_predict))\nplt.xlabel('data')\nplt.ylabel('target')\nplt.title('SVR versus Kernel Ridge')\nplt.legend()\n\n# Visualize training and prediction time\nplt.figure()\n\n# Generate sample data\nX = 5 * rng.rand(10000, 1)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\nsizes = np.logspace(1, 4, 7).astype(np.int)\nfor name, estimator in {\"KRR\": KernelRidge(kernel='rbf', alpha=0.1,\n                                           gamma=10),\n                        \"SVR\": SVR(kernel='rbf', C=1e1, gamma=10)}.items():\n    train_time = []\n    test_time = []\n    for train_test_size in sizes:\n        t0 = time.time()\n        estimator.fit(X[:train_test_size], y[:train_test_size])\n        train_time.append(time.time() - t0)\n\n        t0 = time.time()\n        estimator.predict(X_plot[:1000])\n        test_time.append(time.time() - t0)\n\n    plt.plot(sizes, train_time, 'o-', color=\"r\" if name == \"SVR\" else \"g\",\n             label=\"%s (train)\" % name)\n    plt.plot(sizes, test_time, 'o--', color=\"r\" if name == \"SVR\" else \"g\",\n             label=\"%s (test)\" % name)\n\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"Train size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title('Execution Time')\nplt.legend(loc=\"best\")\n\n# Visualize learning curves\nplt.figure()\n\nsvr = SVR(kernel='rbf', C=1e1, gamma=0.1)\nkr = KernelRidge(kernel='rbf', alpha=0.1, gamma=0.1)\ntrain_sizes, train_scores_svr, test_scores_svr = \\\n    learning_curve(svr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),\n                   scoring=\"neg_mean_squared_error\", cv=10)\ntrain_sizes_abs, train_scores_kr, test_scores_kr = \\\n    learning_curve(kr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),\n                   scoring=\"neg_mean_squared_error\", cv=10)\n\nplt.plot(train_sizes, -test_scores_svr.mean(1), 'o-', color=\"r\",\n         label=\"SVR\")\nplt.plot(train_sizes, -test_scores_kr.mean(1), 'o-', color=\"g\",\n         label=\"KRR\")\nplt.xlabel(\"Train size\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title('Learning curves')\nplt.legend(loc=\"best\")\n\nplt.show()\n"
    },
    {
      "filename": "examples/preprocessing/plot_discretization_classification.py",
      "content": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\"\"\"\n======================\nFeature discretization\n======================\n\nA demonstration of feature discretization on synthetic classification datasets.\nFeature discretization decomposes each feature into a set of bins, here equally\ndistributed in width. The discrete values are then one-hot encoded, and given\nto a linear classifier. This preprocessing enables a non-linear behavior even\nthough the classifier is linear.\n\nOn this example, the first two rows represent linearly non-separable datasets\n(moons and concentric circles) while the third is approximately linearly\nseparable. On the two linearly non-separable datasets, feature discretization\nlargely increases the performance of linear classifiers. On the linearly\nseparable dataset, feature discretization decreases the performance of linear\nclassifiers. Two non-linear classifiers are also shown for comparison.\n\nThis example should be taken with a grain of salt, as the intuition conveyed\ndoes not necessarily carry over to real datasets. Particularly in\nhigh-dimensional spaces, data can more easily be separated linearly. Moreover,\nusing feature discretization and one-hot encoding increases the number of\nfeatures, which easily lead to overfitting when the number of samples is small.\n\nThe plots show training points in solid colors and testing points\nsemi-transparent. The lower right shows the classification accuracy on the test\nset.\n\"\"\"\n# Code source: Tom Dupré la Tour\n# Adapted from plot_classifier_comparison by Gaël Varoquaux and Andreas Müller\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\nprint(__doc__)\n\nh = .02  # step size in the mesh\n\n\ndef get_name(estimator):\n    name = estimator.__class__.__name__\n    if name == 'Pipeline':\n        name = [get_name(est[1]) for est in estimator.steps]\n        name = ' + '.join(name)\n    return name\n\n\n# list of (estimator, param_grid), where param_grid is used in GridSearchCV\nclassifiers = [\n    (LogisticRegression(random_state=0), {\n        'C': np.logspace(-2, 7, 10)\n    }),\n    (LinearSVC(random_state=0), {\n        'C': np.logspace(-2, 7, 10)\n    }),\n    (make_pipeline(\n        KBinsDiscretizer(encode='onehot'),\n        LogisticRegression(random_state=0)), {\n            'kbinsdiscretizer__n_bins': np.arange(2, 10),\n            'logisticregression__C': np.logspace(-2, 7, 10),\n        }),\n    (make_pipeline(\n        KBinsDiscretizer(encode='onehot'), LinearSVC(random_state=0)), {\n            'kbinsdiscretizer__n_bins': np.arange(2, 10),\n            'linearsvc__C': np.logspace(-2, 7, 10),\n        }),\n    (GradientBoostingClassifier(n_estimators=50, random_state=0), {\n        'learning_rate': np.logspace(-4, 0, 10)\n    }),\n    (SVC(random_state=0), {\n        'C': np.logspace(-2, 7, 10)\n    }),\n]\n\nnames = [get_name(e) for e, g in classifiers]\n\nn_samples = 100\ndatasets = [\n    make_moons(n_samples=n_samples, noise=0.2, random_state=0),\n    make_circles(n_samples=n_samples, noise=0.2, factor=0.5, random_state=1),\n    make_classification(n_samples=n_samples, n_features=2, n_redundant=0,\n                        n_informative=2, random_state=2,\n                        n_clusters_per_class=1)\n]\n\nfig, axes = plt.subplots(nrows=len(datasets), ncols=len(classifiers) + 1,\n                         figsize=(21, 9))\n\ncm = plt.cm.PiYG\ncm_bright = ListedColormap(['#b30065', '#178000'])\n\n# iterate over datasets\nfor ds_cnt, (X, y) in enumerate(datasets):\n    print('\\ndataset %d\\n---------' % ds_cnt)\n\n    # preprocess dataset, split into training and test part\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=.5, random_state=42)\n\n    # create the grid for background colors\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(\n        np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # plot the dataset first\n    ax = axes[ds_cnt, 0]\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n               edgecolors='k')\n    # and testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n               edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n    # iterate over classifiers\n    for est_idx, (name, (estimator, param_grid)) in \\\n            enumerate(zip(names, classifiers)):\n        ax = axes[ds_cnt, est_idx + 1]\n\n        clf = GridSearchCV(estimator=estimator, param_grid=param_grid)\n        with ignore_warnings(category=ConvergenceWarning):\n            clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        print('%s: %.2f' % (name, score))\n\n        # plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]*[y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors='k')\n        # and testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   edgecolors='k', alpha=0.6)\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n\n        if ds_cnt == 0:\n            ax.set_title(name.replace(' + ', '\\n'))\n        ax.text(0.95, 0.06, ('%.2f' % score).lstrip('0'), size=15,\n                bbox=dict(boxstyle='round', alpha=0.8, facecolor='white'),\n                transform=ax.transAxes, horizontalalignment='right')\n\n\nplt.tight_layout()\n\n# Add suptitles above the figure\nplt.subplots_adjust(top=0.90)\nsuptitles = [\n    'Linear classifiers',\n    'Feature discretization and linear classifiers',\n    'Non-linear classifiers',\n]\nfor i, suptitle in zip([1, 3, 5], suptitles):\n    ax = axes[0, i]\n    ax.text(1.05, 1.25, suptitle, transform=ax.transAxes,\n            horizontalalignment='center', size='x-large')\nplt.show()\n"
    },
    {
      "filename": "examples/svm/plot_svm_anova.py",
      "content": "\"\"\"\n=================================================\nSVM-Anova: SVM with univariate feature selection\n=================================================\n\nThis example shows how to perform univariate feature selection before running a\nSVC (support vector classifier) to improve the classification scores. We use\nthe iris dataset (4 features) and add 36 non-informative features. We can find\nthat our model achieves best performance when we select around 10% of features.\n\"\"\"\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.feature_selection import SelectPercentile, chi2\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\n\n# #############################################################################\n# Import some data to play with\nX, y = load_iris(return_X_y=True)\n# Add non-informative features\nnp.random.seed(0)\nX = np.hstack((X, 2 * np.random.random((X.shape[0], 36))))\n\n# #############################################################################\n# Create a feature-selection transform, a scaler and an instance of SVM that we\n# combine together to have an full-blown estimator\nclf = Pipeline([('anova', SelectPercentile(chi2)),\n                ('scaler', StandardScaler()),\n                ('svc', SVC(gamma=\"auto\"))])\n\n# #############################################################################\n# Plot the cross-validation score as a function of percentile of features\nscore_means = list()\nscore_stds = list()\npercentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)\n\nfor percentile in percentiles:\n    clf.set_params(anova__percentile=percentile)\n    this_scores = cross_val_score(clf, X, y)\n    score_means.append(this_scores.mean())\n    score_stds.append(this_scores.std())\n\nplt.errorbar(percentiles, score_means, np.array(score_stds))\nplt.title(\n    'Performance of the SVM-Anova varying the percentile of features selected')\nplt.xticks(np.linspace(0, 100, 11, endpoint=True))\nplt.xlabel('Percentile')\nplt.ylabel('Accuracy Score')\nplt.axis('tight')\nplt.show()\n"
    }
  ],
  "questions": [
    "@qinhanmin2014 @rth Can you review this please? #14556"
  ],
  "golden_answers": [
    "@rth I removed unused tolerance parameter from `Perceptron`. Waiting for test results. Please review after test results pass. #14558"
  ],
  "questions_generated": [
    "What is the primary objective of the issue discussed in scikit-learn's repository, and how does it relate to parameter assignments?",
    "In the context of the scikit-learn repository, what strategy was proposed for identifying redundant parameters, and what challenges are associated with it?",
    "Why is it important to be cautious when removing explicit parameter assignments in tests within the scikit-learn repository?",
    "Describe the role of the 'fetch_lfw_people' function in the context of the given code snippet from scikit-learn.",
    "What are the potential benefits of removing redundant parameter assignments from examples and tests in scikit-learn's codebase?",
    "How would you approach the task of ensuring that examples in scikit-learn's repository are updated to reflect the latest API changes without redundant parameters?"
  ],
  "golden_answers_generated": [
    "The primary objective of the issue is to remove redundant parameter assignments in examples and tests within the scikit-learn repository. Specifically, it addresses the unnecessary use of parameters that have been deprecated or set to default values, such as 'cv=5'. The issue highlights the need to clean up the code by removing these redundant assignments to enhance code readability and maintainability.",
    "The proposed strategy for identifying redundant parameters involves reading through examples to check if used parameters match their default values. Once identified, these parameters should be searched across the codebase using 'git grep' to find other occurrences. Another suggested approach is to review release notes from past versions to identify changed default parameters and search for their redundant use. The challenge lies in the manual effort required to accurately detect and remove these redundant parameters without affecting the functionality.",
    "It is important to be cautious when removing explicit parameter assignments in tests because explicit parameters can enhance the clarity and understanding of tests. Even if these parameters are set to their default values, they might be included intentionally to make the test's intention clear to developers. Removing them could potentially reduce the readability and comprehensibility of the tests, making it harder for developers to understand what specific functionality is being tested.",
    "In the given code snippet, the 'fetch_lfw_people' function is used to download and load the 'Labeled Faces in the Wild' (LFW) dataset into memory as numpy arrays. The function parameters, 'min_faces_per_person=70' and 'resize=0.4', filter the dataset to include only individuals with a minimum number of face images and resize the images to reduce computational complexity. This dataset is used for training and evaluating face recognition models in the example.",
    "The potential benefits of removing redundant parameter assignments include improving code readability and maintainability, reducing confusion for new contributors, and ensuring that examples and tests reflect the most current API usage. It also helps in preventing potential bugs or issues that could arise from outdated parameter values or deprecated features being used unintentionally.",
    "To ensure examples are updated, I would start by reviewing the release notes of recent versions to identify any changes in default parameters. Then, I would use 'git grep' to search for these parameters within the codebase, focusing on the examples and tests directories. Each occurrence would be examined to verify if the parameter is redundant due to being set to its default value. After removing unnecessary parameters, I would run the examples and tests to ensure that their functionality remains unchanged. This task might also involve collaboration with other contributors to cross-verify changes and ensure comprehensive updates."
  ]
}