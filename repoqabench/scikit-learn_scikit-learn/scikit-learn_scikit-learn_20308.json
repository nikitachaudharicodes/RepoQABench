{
  "repo_name": "scikit-learn_scikit-learn",
  "issue_id": "20308",
  "issue_description": "# Ensure that docstrings pass numpydoc validation\n\n1. Make sure you have the [development dependencies](https://scikit-learn.org/stable/developers/contributing.html#how-to-contribute) and [documentation dependencies](https://scikit-learn.org/stable/developers/contributing.html#building-the-documentation) installed.\r\n2. Pick an estimator from the list below and **leave a comment saying you are going to work on it**. This way we can keep track of what everyone is working on.\r\n3. Remove the estimator from the list at: https://github.com/scikit-learn/scikit-learn/blob/bb6117b228e2940cada2627dce86b49d0662220c/maint_tools/test_docstrings.py#L11\r\n4. Let's say you picked `StandardScaler`, run numpydoc validation as follows (Adding the `-` at the end helps with the regex).\r\n\r\n```\r\npytest maint_tools/test_docstrings.py -k StandardScaler- \r\n```\r\n\r\n\r\n5. If you see failing test, please fix them by following the recommendation provided by the failing test.\r\n6. If you see all the tests past, you do not need to do any additional changes.\r\n7. Commit your changes.\r\n8. Open a Pull Request with an opening message `Addresses #20308`. Note that each item should be submitted in a **separate** Pull Request.\r\n9. Include the estimator name in the title of the pull request. For example: \"DOC Ensures that StandardScaler passes numpydoc validation\".\r\n\r\n- [x] #20381 ARDRegression\r\n- [x] #20374 AdaBoostClassifier\r\n- [x] #20400 AdaBoostRegressor\r\n- [x] #20536 AdditiveChi2Sampler\r\n- [x] #20532 AffinityPropagation\r\n- [x] #20544 AgglomerativeClustering\r\n- [x] #20407 BaggingClassifier\r\n- [x] #20498 BaggingRegressor\r\n- [x] #20384 BayesianGaussianMixture\r\n- [x] #20389 BayesianRidge\r\n- [x] BernoulliNB\r\n- [x] #20533 BernoulliRBM\r\n- [x] #20422 Binarizer\r\n- [x] Birch\r\n- [x] #20504 CCA\r\n- [x] CalibratedClassifierCV\r\n- [x] #20445 CategoricalNB\r\n- [x] ClassifierChain\r\n- [x] ColumnTransformer\r\n- [x] #20440 ComplementNB\r\n- [x] #20403 CountVectorizer\r\n- [x] #20375 DBSCAN\r\n- [x] #20399 DecisionTreeClassifier\r\n- [x] DecisionTreeRegressor\r\n- [x] DictVectorizer\r\n- [x] DictionaryLearning\r\n- [x] DummyClassifier\r\n- [x] #20394 DummyRegressor\r\n- [x] #20454 ElasticNet\r\n- [x] ElasticNetCV\r\n- [x] #20548 EllipticEnvelope\r\n- [x] #20551 EmpiricalCovariance\r\n- [x] ExtraTreeClassifier\r\n- [x] ExtraTreeRegressor\r\n- [x] ExtraTreesClassifier\r\n- [x] ExtraTreesRegressor\r\n- [x] FactorAnalysis\r\n- [x] #20405 FastICA\r\n- [x] FeatureAgglomeration\r\n- [x] FeatureHasher\r\n- [x] FeatureUnion\r\n- [x] FunctionTransformer\r\n- [x] GammaRegressor\r\n- [x] GaussianMixture\r\n- [x] #20440 GaussianNB\r\n- [x] GaussianProcessClassifier\r\n- [x] GaussianProcessRegressor\r\n- [x] GaussianRandomProjection\r\n- [x] #20495 GenericUnivariateSelect\r\n- [x] GradientBoostingClassifier\r\n- [x] GradientBoostingRegressor\r\n- [x] #20527 GraphicalLasso\r\n- [x] #20546 GraphicalLassoCV\r\n- [x] GridSearchCV\r\n- [x] HalvingGridSearchCV\r\n- [x] HalvingRandomSearchCV\r\n- [x] HashingVectorizer\r\n- [x] HistGradientBoostingClassifier\r\n- [x] HistGradientBoostingRegressor\r\n- [x] HuberRegressor\r\n- [x] IncrementalPCA\r\n- [x] https://github.com/scikit-learn/scikit-learn/pull/20437 IsolationForest\r\n- [x] Isomap\r\n- [x] #20514 IsotonicRegression\r\n- [x] IterativeImputer\r\n- [x] KBinsDiscretizer\r\n- [x] #20377 KMeans\r\n- [x] KNNImputer\r\n- [x] #20373 KNeighborsClassifier\r\n- [x] #20378 KNeighborsRegressor\r\n- [x] KNeighborsTransformer\r\n- [x] KernelCenterer\r\n- [x] KernelDensity\r\n- [x] KernelPCA\r\n- [x] KernelRidge\r\n- [x] LabelBinarizer\r\n- [x] #20456 LabelEncoder\r\n- [x] LabelPropagation\r\n- [x] LabelSpreading\r\n- [x] #20472 Lars\r\n- [x] #20501 LarsCV\r\n- [x] #20409 Lasso\r\n- [x] #20453 LassoCV\r\n- [x] #20459 LassoLars\r\n- [x] #20462 LassoLarsCV\r\n- [x] #20465 LassoLarsIC\r\n- [x] #20402 LatentDirichletAllocation\r\n- [x] #20578 LedoitWolf\r\n- [x] LinearDiscriminantAnalysis\r\n- [x] #20369 LinearRegression\r\n- [x] #20458 LinearSVC\r\n- [x] LinearSVR\r\n- [x] LocalOutlierFactor\r\n- [x] LocallyLinearEmbedding\r\n- [x] #20370 LogisticRegression\r\n- [x] #20376 LogisticRegressionCV\r\n- [x] MDS\r\n- [x] #20444 MLPClassifier\r\n- [x] MLPRegressor\r\n- [x] #20455 MaxAbsScaler\r\n- [x] MeanShift\r\n- [x] #20580 MinCovDet\r\n- [x] MinMaxScaler\r\n- [x] MiniBatchDictionaryLearning\r\n- [x] MiniBatchKMeans\r\n- [x] MiniBatchSparsePCA\r\n- [x] MissingIndicator\r\n- [x] MultiLabelBinarizer\r\n- [x] MultiOutputClassifier\r\n- [x] MultiOutputRegressor\r\n- [x] MultiTaskElasticNet\r\n- [x] MultiTaskElasticNetCV\r\n- [x] MultiTaskLasso\r\n- [x] MultiTaskLassoCV\r\n- [x] #20440 MultinomialNB\r\n- [x] NMF\r\n- [x] NearestCentroid\r\n- [x] #20446 NearestNeighbors\r\n- [x] NeighborhoodComponentsAnalysis\r\n- [x] Normalizer\r\n- [x] #20461 NuSVC\r\n- [x] NuSVR\r\n- [x] Nystroem\r\n- [x] #20579 OAS\r\n- [x] OPTICS\r\n- [x] #20463 OneClassSVM\r\n- [x] #20406 OneHotEncoder\r\n- [x] OneVsOneClassifier\r\n- [x] OneVsRestClassifier\r\n- [x] OrdinalEncoder\r\n- [x] OrthogonalMatchingPursuit\r\n- [x] OrthogonalMatchingPursuitCV\r\n- [x] OutputCodeClassifier\r\n- [x] PCA\r\n- [x] PLSCanonical\r\n- [x] PLSRegression\r\n- [x] PLSSVD\r\n- [x] PassiveAggressiveClassifier\r\n- [x] PassiveAggressiveRegressor\r\n- [x] PatchExtractor\r\n- [x] #20404 Perceptron\r\n- [x] Pipeline\r\n- [x]  #20386 PoissonRegressor\r\n- [x] PolynomialCountSketch\r\n- [x] PolynomialFeatures\r\n- [x] PowerTransformer\r\n- [x] QuadraticDiscriminantAnalysis\r\n- [x] QuantileRegressor\r\n- [x] QuantileTransformer\r\n- [x] RANSACRegressor\r\n- [x] RBFSampler\r\n- [x] #20419 RFE\r\n- [x] #20452 RFECV\r\n- [x] RadiusNeighborsClassifier\r\n- [x] RadiusNeighborsRegressor\r\n- [x] RadiusNeighborsTransformer\r\n- [x] #20383 RandomForestClassifer\r\n- [x] RandomForestRegressor\r\n- [x] RandomTreesEmbedding\r\n- [x] RandomizedSearchCV\r\n- [x] RegressorChain\r\n- [x] #20499 Ridge\r\n- [x] #20503 RidgeCV\r\n- [x] RidgeClassifier\r\n- [x] RidgeClassifierCV\r\n- [x] RobustScaler\r\n- [x] SGDOneClassSVM\r\n- [x] SGDRegressor\r\n- [x] #20457 SVC\r\n- [x] SVR\r\n- [x] SelectFdr\r\n- [x] SelectFpr\r\n- [x] SelectFromModel\r\n- [x] SelectFwe\r\n- [x] SelectKBest\r\n- [x] SelectPercentile\r\n- [x] #21277 SelfTrainingClassifier\r\n- [x] SequentialFeatureSelector\r\n- [x] #20571 ShrunkCovariance\r\n- [x] SimpleImputer\r\n- [x] SkewedChi2Sampler\r\n- [x] SparseCoder\r\n- [x] #20395 SparsePCA\r\n- [x] SparseRandomProjection\r\n- [x] SpectralBiclustering\r\n- [x] SpectralClustering\r\n- [x] SpectralCoclustering #21463\r\n- [x] SpectralEmbedding\r\n- [x] SplineTransformer\r\n- [x] StackingClassifier\r\n- [x] StackingRegressor\r\n- [x] #20368 StandardScalar\r\n- [x] TSNE\r\n- [x] #20379 TfidfVectorizer\r\n- [x] TheilSenRegressor\r\n- [x] TransformedTargetRegressor\r\n- [x] TruncatedSVD\r\n- [x] TweedieRegressor\r\n- [x] VarianceThreshold\r\n- [x] VotingClassifier\r\n- [x] #20450 VotingRegressor",
  "issue_comments": [
    {
      "id": 869028698,
      "user": "j3nnn1",
      "body": "Can I take the estimator: ¨AdaBoostClassifier¨?  **partner**  cc: @genvalen"
    },
    {
      "id": 869028818,
      "user": "alinealfa",
      "body": "@marenwestermann and I are working on `RandomForestClassifier`"
    },
    {
      "id": 869028885,
      "user": "NicolasMillerr",
      "body": "@mattnp and I are starting off with the `StandardScaler`"
    },
    {
      "id": 869029075,
      "user": "gloriamacia",
      "body": "@caherrera-meli and I are going for the LinearRegression\r\n"
    },
    {
      "id": 869029140,
      "user": "lacouth",
      "body": "@gitdoluquita and I are going for the LogisticRegression"
    },
    {
      "id": 869029187,
      "user": "ludigoncalves",
      "body": "I'm going with `ExtraTreeClassifier`!\r\n"
    },
    {
      "id": 869030191,
      "user": "pibieta",
      "body": "@g4brielvs and I are working on `KNeighborsClassifier` "
    },
    {
      "id": 869030230,
      "user": "Anavelyz",
      "body": "@marielaraj and me are going for `PCA`"
    },
    {
      "id": 869030364,
      "user": "LucyJimenez",
      "body": "@eugeniaft and me are going for `DecisionTreeClassifier`. "
    },
    {
      "id": 869030405,
      "user": "trhughes",
      "body": "@napoles-uach and I are working on KNeighborsRegressor"
    },
    {
      "id": 869030508,
      "user": "g4brielvs",
      "body": "@g4brielvs and @pibieta are working on `KMeans`\r\n"
    },
    {
      "id": 869030886,
      "user": "sebastiandres",
      "body": "With @leonardorocc0 will be taking `ARDRegression`\r\n"
    },
    {
      "id": 869031285,
      "user": "jmloyola",
      "body": "With @tomasmoreyra will be taking `TfidfTransformer`"
    },
    {
      "id": 869031390,
      "user": "jbsilva",
      "body": "I'm working on `BaggingClassifier`."
    },
    {
      "id": 869031429,
      "user": "asnramos",
      "body": "**BayesianGaussianMixture** Estamos trabajando con @nicolas471"
    },
    {
      "id": 869031793,
      "user": "fbidu",
      "body": "I'm working on `DBSCAN` with @ijpulidos"
    },
    {
      "id": 869032660,
      "user": "GabrielBernardoMC",
      "body": "me and @joaovitormascarenhas are going for **CountVectorizer**\r\n"
    },
    {
      "id": 869032883,
      "user": "MattNP",
      "body": "Me and @NicolasMillerr will be working on `GaussianProcessClassifier`"
    },
    {
      "id": 869036818,
      "user": "gitdoluquita",
      "body": "@lacouth  and I are going for the `LogisticRegressionCV`"
    },
    {
      "id": 869037618,
      "user": "caherrera-meli",
      "body": "@gloriamacia and I will continue with DummyRegressor"
    },
    {
      "id": 869040748,
      "user": "genvalen",
      "body": " Going to take ``BernoulliNB`` with @j3nnn1"
    },
    {
      "id": 869041045,
      "user": "lacouth",
      "body": "@gitdoluquita and I are working on PoissonRegressor"
    },
    {
      "id": 869042538,
      "user": "g4brielvs",
      "body": "@g4brielvs and @pibieta are working on `GaussianMixture`"
    },
    {
      "id": 869044572,
      "user": "g4brielvs",
      "body": "I'm (@g4brielvs ) working on `LinearDiscriminantAnalysis`"
    },
    {
      "id": 869046454,
      "user": "leonardorocc0",
      "body": "I'm working on this `FeatureUnion` w/ @sebastiandres \r\n"
    },
    {
      "id": 869047176,
      "user": "pibieta",
      "body": "@g4brielvs and I are working on `BayesianRidge` now"
    },
    {
      "id": 869048621,
      "user": "alinealfa",
      "body": "@marenwestermann  and I are working on RandomForestRegressor"
    },
    {
      "id": 869049301,
      "user": "asnramos",
      "body": "Trabajando con @nicolas471 en **BayesianRidge**"
    },
    {
      "id": 869049644,
      "user": "pibieta",
      "body": "> Trabajando con @nicolas471 en **BayesianRidge**\r\n\r\nYo también ando trabajando en ese con @g4brielvs. Tal vez puede haber un conflicto..."
    },
    {
      "id": 869049887,
      "user": "ijpulidos",
      "body": "Taking over `SparsePCA` one."
    }
  ],
  "text_context": "# Ensure that docstrings pass numpydoc validation\n\n1. Make sure you have the [development dependencies](https://scikit-learn.org/stable/developers/contributing.html#how-to-contribute) and [documentation dependencies](https://scikit-learn.org/stable/developers/contributing.html#building-the-documentation) installed.\r\n2. Pick an estimator from the list below and **leave a comment saying you are going to work on it**. This way we can keep track of what everyone is working on.\r\n3. Remove the estimator from the list at: https://github.com/scikit-learn/scikit-learn/blob/bb6117b228e2940cada2627dce86b49d0662220c/maint_tools/test_docstrings.py#L11\r\n4. Let's say you picked `StandardScaler`, run numpydoc validation as follows (Adding the `-` at the end helps with the regex).\r\n\r\n```\r\npytest maint_tools/test_docstrings.py -k StandardScaler- \r\n```\r\n\r\n\r\n5. If you see failing test, please fix them by following the recommendation provided by the failing test.\r\n6. If you see all the tests past, you do not need to do any additional changes.\r\n7. Commit your changes.\r\n8. Open a Pull Request with an opening message `Addresses #20308`. Note that each item should be submitted in a **separate** Pull Request.\r\n9. Include the estimator name in the title of the pull request. For example: \"DOC Ensures that StandardScaler passes numpydoc validation\".\r\n\r\n- [x] #20381 ARDRegression\r\n- [x] #20374 AdaBoostClassifier\r\n- [x] #20400 AdaBoostRegressor\r\n- [x] #20536 AdditiveChi2Sampler\r\n- [x] #20532 AffinityPropagation\r\n- [x] #20544 AgglomerativeClustering\r\n- [x] #20407 BaggingClassifier\r\n- [x] #20498 BaggingRegressor\r\n- [x] #20384 BayesianGaussianMixture\r\n- [x] #20389 BayesianRidge\r\n- [x] BernoulliNB\r\n- [x] #20533 BernoulliRBM\r\n- [x] #20422 Binarizer\r\n- [x] Birch\r\n- [x] #20504 CCA\r\n- [x] CalibratedClassifierCV\r\n- [x] #20445 CategoricalNB\r\n- [x] ClassifierChain\r\n- [x] ColumnTransformer\r\n- [x] #20440 ComplementNB\r\n- [x] #20403 CountVectorizer\r\n- [x] #20375 DBSCAN\r\n- [x] #20399 DecisionTreeClassifier\r\n- [x] DecisionTreeRegressor\r\n- [x] DictVectorizer\r\n- [x] DictionaryLearning\r\n- [x] DummyClassifier\r\n- [x] #20394 DummyRegressor\r\n- [x] #20454 ElasticNet\r\n- [x] ElasticNetCV\r\n- [x] #20548 EllipticEnvelope\r\n- [x] #20551 EmpiricalCovariance\r\n- [x] ExtraTreeClassifier\r\n- [x] ExtraTreeRegressor\r\n- [x] ExtraTreesClassifier\r\n- [x] ExtraTreesRegressor\r\n- [x] FactorAnalysis\r\n- [x] #20405 FastICA\r\n- [x] FeatureAgglomeration\r\n- [x] FeatureHasher\r\n- [x] FeatureUnion\r\n- [x] FunctionTransformer\r\n- [x] GammaRegressor\r\n- [x] GaussianMixture\r\n- [x] #20440 GaussianNB\r\n- [x] GaussianProcessClassifier\r\n- [x] GaussianProcessRegressor\r\n- [x] GaussianRandomProjection\r\n- [x] #20495 GenericUnivariateSelect\r\n- [x] GradientBoostingClassifier\r\n- [x] GradientBoostingRegressor\r\n- [x] #20527 GraphicalLasso\r\n- [x] #20546 GraphicalLassoCV\r\n- [x] GridSearchCV\r\n- [x] HalvingGridSearchCV\r\n- [x] HalvingRandomSearchCV\r\n- [x] HashingVectorizer\r\n- [x] HistGradientBoostingClassifier\r\n- [x] HistGradientBoostingRegressor\r\n- [x] HuberRegressor\r\n- [x] IncrementalPCA\r\n- [x] https://github.com/scikit-learn/scikit-learn/pull/20437 IsolationForest\r\n- [x] Isomap\r\n- [x] #20514 IsotonicRegression\r\n- [x] IterativeImputer\r\n- [x] KBinsDiscretizer\r\n- [x] #20377 KMeans\r\n- [x] KNNImputer\r\n- [x] #20373 KNeighborsClassifier\r\n- [x] #20378 KNeighborsRegressor\r\n- [x] KNeighborsTransformer\r\n- [x] KernelCenterer\r\n- [x] KernelDensity\r\n- [x] KernelPCA\r\n- [x] KernelRidge\r\n- [x] LabelBinarizer\r\n- [x] #20456 LabelEncoder\r\n- [x] LabelPropagation\r\n- [x] LabelSpreading\r\n- [x] #20472 Lars\r\n- [x] #20501 LarsCV\r\n- [x] #20409 Lasso\r\n- [x] #20453 LassoCV\r\n- [x] #20459 LassoLars\r\n- [x] #20462 LassoLarsCV\r\n- [x] #20465 LassoLarsIC\r\n- [x] #20402 LatentDirichletAllocation\r\n- [x] #20578 LedoitWolf\r\n- [x] LinearDiscriminantAnalysis\r\n- [x] #20369 LinearRegression\r\n- [x] #20458 LinearSVC\r\n- [x] LinearSVR\r\n- [x] LocalOutlierFactor\r\n- [x] LocallyLinearEmbedding\r\n- [x] #20370 LogisticRegression\r\n- [x] #20376 LogisticRegressionCV\r\n- [x] MDS\r\n- [x] #20444 MLPClassifier\r\n- [x] MLPRegressor\r\n- [x] #20455 MaxAbsScaler\r\n- [x] MeanShift\r\n- [x] #20580 MinCovDet\r\n- [x] MinMaxScaler\r\n- [x] MiniBatchDictionaryLearning\r\n- [x] MiniBatchKMeans\r\n- [x] MiniBatchSparsePCA\r\n- [x] MissingIndicator\r\n- [x] MultiLabelBinarizer\r\n- [x] MultiOutputClassifier\r\n- [x] MultiOutputRegressor\r\n- [x] MultiTaskElasticNet\r\n- [x] MultiTaskElasticNetCV\r\n- [x] MultiTaskLasso\r\n- [x] MultiTaskLassoCV\r\n- [x] #20440 MultinomialNB\r\n- [x] NMF\r\n- [x] NearestCentroid\r\n- [x] #20446 NearestNeighbors\r\n- [x] NeighborhoodComponentsAnalysis\r\n- [x] Normalizer\r\n- [x] #20461 NuSVC\r\n- [x] NuSVR\r\n- [x] Nystroem\r\n- [x] #20579 OAS\r\n- [x] OPTICS\r\n- [x] #20463 OneClassSVM\r\n- [x] #20406 OneHotEncoder\r\n- [x] OneVsOneClassifier\r\n- [x] OneVsRestClassifier\r\n- [x] OrdinalEncoder\r\n- [x] OrthogonalMatchingPursuit\r\n- [x] OrthogonalMatchingPursuitCV\r\n- [x] OutputCodeClassifier\r\n- [x] PCA\r\n- [x] PLSCanonical\r\n- [x] PLSRegression\r\n- [x] PLSSVD\r\n- [x] PassiveAggressiveClassifier\r\n- [x] PassiveAggressiveRegressor\r\n- [x] PatchExtractor\r\n- [x] #20404 Perceptron\r\n- [x] Pipeline\r\n- [x]  #20386 PoissonRegressor\r\n- [x] PolynomialCountSketch\r\n- [x] PolynomialFeatures\r\n- [x] PowerTransformer\r\n- [x] QuadraticDiscriminantAnalysis\r\n- [x] QuantileRegressor\r\n- [x] QuantileTransformer\r\n- [x] RANSACRegressor\r\n- [x] RBFSampler\r\n- [x] #20419 RFE\r\n- [x] #20452 RFECV\r\n- [x] RadiusNeighborsClassifier\r\n- [x] RadiusNeighborsRegressor\r\n- [x] RadiusNeighborsTransformer\r\n- [x] #20383 RandomForestClassifer\r\n- [x] RandomForestRegressor\r\n- [x] RandomTreesEmbedding\r\n- [x] RandomizedSearchCV\r\n- [x] RegressorChain\r\n- [x] #20499 Ridge\r\n- [x] #20503 RidgeCV\r\n- [x] RidgeClassifier\r\n- [x] RidgeClassifierCV\r\n- [x] RobustScaler\r\n- [x] SGDOneClassSVM\r\n- [x] SGDRegressor\r\n- [x] #20457 SVC\r\n- [x] SVR\r\n- [x] SelectFdr\r\n- [x] SelectFpr\r\n- [x] SelectFromModel\r\n- [x] SelectFwe\r\n- [x] SelectKBest\r\n- [x] SelectPercentile\r\n- [x] #21277 SelfTrainingClassifier\r\n- [x] SequentialFeatureSelector\r\n- [x] #20571 ShrunkCovariance\r\n- [x] SimpleImputer\r\n- [x] SkewedChi2Sampler\r\n- [x] SparseCoder\r\n- [x] #20395 SparsePCA\r\n- [x] SparseRandomProjection\r\n- [x] SpectralBiclustering\r\n- [x] SpectralClustering\r\n- [x] SpectralCoclustering #21463\r\n- [x] SpectralEmbedding\r\n- [x] SplineTransformer\r\n- [x] StackingClassifier\r\n- [x] StackingRegressor\r\n- [x] #20368 StandardScalar\r\n- [x] TSNE\r\n- [x] #20379 TfidfVectorizer\r\n- [x] TheilSenRegressor\r\n- [x] TransformedTargetRegressor\r\n- [x] TruncatedSVD\r\n- [x] TweedieRegressor\r\n- [x] VarianceThreshold\r\n- [x] VotingClassifier\r\n- [x] #20450 VotingRegressor\n\nCan I take the estimator: ¨AdaBoostClassifier¨?  **partner**  cc: @genvalen\n\n@marenwestermann and I are working on `RandomForestClassifier`\n\n@mattnp and I are starting off with the `StandardScaler`\n\n@caherrera-meli and I are going for the LinearRegression\r\n\n\n@gitdoluquita and I are going for the LogisticRegression\n\nI'm going with `ExtraTreeClassifier`!\r\n\n\n@g4brielvs and I are working on `KNeighborsClassifier` \n\n@marielaraj and me are going for `PCA`\n\n@eugeniaft and me are going for `DecisionTreeClassifier`. \n\n@napoles-uach and I are working on KNeighborsRegressor\n\n@g4brielvs and @pibieta are working on `KMeans`\r\n\n\nWith @leonardorocc0 will be taking `ARDRegression`\r\n\n\nWith @tomasmoreyra will be taking `TfidfTransformer`\n\nI'm working on `BaggingClassifier`.\n\n**BayesianGaussianMixture** Estamos trabajando con @nicolas471\n\nI'm working on `DBSCAN` with @ijpulidos\n\nme and @joaovitormascarenhas are going for **CountVectorizer**\r\n\n\nMe and @NicolasMillerr will be working on `GaussianProcessClassifier`\n\n@lacouth  and I are going for the `LogisticRegressionCV`\n\n@gloriamacia and I will continue with DummyRegressor\n\n Going to take ``BernoulliNB`` with @j3nnn1\n\n@gitdoluquita and I are working on PoissonRegressor\n\n@g4brielvs and @pibieta are working on `GaussianMixture`\n\nI'm (@g4brielvs ) working on `LinearDiscriminantAnalysis`\n\nI'm working on this `FeatureUnion` w/ @sebastiandres \r\n\n\n@g4brielvs and I are working on `BayesianRidge` now\n\n@marenwestermann  and I are working on RandomForestRegressor\n\nTrabajando con @nicolas471 en **BayesianRidge**\n\n> Trabajando con @nicolas471 en **BayesianRidge**\r\n\r\nYo también ando trabajando en ese con @g4brielvs. Tal vez puede haber un conflicto...\n\nTaking over `SparsePCA` one.",
  "pr_link": "https://github.com/scikit-learn/scikit-learn/pull/20437",
  "code_context": [
    {
      "filename": "maint_tools/test_docstrings.py",
      "content": "import re\nfrom inspect import signature\nfrom typing import Optional\n\nimport pytest\nfrom sklearn.utils import all_estimators\n\nnumpydoc_validation = pytest.importorskip(\"numpydoc.validate\")\n\n# List of modules ignored when checking for numpydoc validation.\nDOCSTRING_IGNORE_LIST = [\n    \"AdditiveChi2Sampler\",\n    \"AffinityPropagation\",\n    \"AgglomerativeClustering\",\n    \"BaggingRegressor\",\n    \"BernoulliRBM\",\n    \"Birch\",\n    \"CCA\",\n    \"CalibratedClassifierCV\",\n    \"CategoricalNB\",\n    \"ClassifierChain\",\n    \"ColumnTransformer\",\n    \"ComplementNB\",\n    \"CountVectorizer\",\n    \"DecisionTreeRegressor\",\n    \"DictVectorizer\",\n    \"DictionaryLearning\",\n    \"DummyClassifier\",\n    \"DummyRegressor\",\n    \"ElasticNet\",\n    \"ElasticNetCV\",\n    \"EllipticEnvelope\",\n    \"EmpiricalCovariance\",\n    \"ExtraTreeClassifier\",\n    \"ExtraTreeRegressor\",\n    \"ExtraTreesClassifier\",\n    \"ExtraTreesRegressor\",\n    \"FactorAnalysis\",\n    \"FastICA\",\n    \"FeatureAgglomeration\",\n    \"FeatureHasher\",\n    \"FeatureUnion\",\n    \"FunctionTransformer\",\n    \"GammaRegressor\",\n    \"GaussianMixture\",\n    \"GaussianNB\",\n    \"GaussianProcessRegressor\",\n    \"GaussianRandomProjection\",\n    \"GenericUnivariateSelect\",\n    \"GradientBoostingClassifier\",\n    \"GradientBoostingRegressor\",\n    \"GraphicalLasso\",\n    \"GraphicalLassoCV\",\n    \"GridSearchCV\",\n    \"HalvingGridSearchCV\",\n    \"HalvingRandomSearchCV\",\n    \"HashingVectorizer\",\n    \"HistGradientBoostingClassifier\",\n    \"HistGradientBoostingRegressor\",\n    \"HuberRegressor\",\n    \"IncrementalPCA\",\n    \"Isomap\",\n    \"IsotonicRegression\",\n    \"IterativeImputer\",\n    \"KBinsDiscretizer\",\n    \"KNNImputer\",\n    \"KNeighborsRegressor\",\n    \"KNeighborsTransformer\",\n    \"KernelCenterer\",\n    \"KernelDensity\",\n    \"KernelPCA\",\n    \"KernelRidge\",\n    \"LabelBinarizer\",\n    \"LabelEncoder\",\n    \"LabelPropagation\",\n    \"LabelSpreading\",\n    \"Lars\",\n    \"LarsCV\",\n    \"LassoCV\",\n    \"LassoLars\",\n    \"LassoLarsCV\",\n    \"LassoLarsIC\",\n    \"LatentDirichletAllocation\",\n    \"LedoitWolf\",\n    \"LinearSVC\",\n    \"LinearSVR\",\n    \"LocalOutlierFactor\",\n    \"LocallyLinearEmbedding\",\n    \"MDS\",\n    \"MLPClassifier\",\n    \"MLPRegressor\",\n    \"MaxAbsScaler\",\n    \"MeanShift\",\n    \"MinCovDet\",\n    \"MiniBatchDictionaryLearning\",\n    \"MiniBatchKMeans\",\n    \"MiniBatchSparsePCA\",\n    \"MissingIndicator\",\n    \"MultiLabelBinarizer\",\n    \"MultiOutputClassifier\",\n    \"MultiOutputRegressor\",\n    \"MultiTaskElasticNet\",\n    \"MultiTaskElasticNetCV\",\n    \"MultiTaskLasso\",\n    \"MultiTaskLassoCV\",\n    \"MultinomialNB\",\n    \"NMF\",\n    \"NearestCentroid\",\n    \"NearestNeighbors\",\n    \"NeighborhoodComponentsAnalysis\",\n    \"Normalizer\",\n    \"NuSVC\",\n    \"NuSVR\",\n    \"Nystroem\",\n    \"OAS\",\n    \"OPTICS\",\n    \"OneClassSVM\",\n    \"OneVsOneClassifier\",\n    \"OneVsRestClassifier\",\n    \"OrdinalEncoder\",\n    \"OrthogonalMatchingPursuit\",\n    \"OrthogonalMatchingPursuitCV\",\n    \"OutputCodeClassifier\",\n    \"PLSCanonical\",\n    \"PLSRegression\",\n    \"PLSSVD\",\n    \"PassiveAggressiveClassifier\",\n    \"PassiveAggressiveRegressor\",\n    \"PatchExtractor\",\n    \"Pipeline\",\n    \"PoissonRegressor\",\n    \"PolynomialCountSketch\",\n    \"PolynomialFeatures\",\n    \"PowerTransformer\",\n    \"QuadraticDiscriminantAnalysis\",\n    \"QuantileRegressor\",\n    \"QuantileTransformer\",\n    \"RANSACRegressor\",\n    \"RBFSampler\",\n    \"RFE\",\n    \"RFECV\",\n    \"RadiusNeighborsClassifier\",\n    \"RadiusNeighborsRegressor\",\n    \"RadiusNeighborsTransformer\",\n    \"RandomForestClassifier\",\n    \"RandomTreesEmbedding\",\n    \"RandomizedSearchCV\",\n    \"RegressorChain\",\n    \"Ridge\",\n    \"RidgeCV\",\n    \"RidgeClassifier\",\n    \"RidgeClassifierCV\",\n    \"RobustScaler\",\n    \"SGDOneClassSVM\",\n    \"SGDRegressor\",\n    \"SVC\",\n    \"SVR\",\n    \"SelectFdr\",\n    \"SelectFpr\",\n    \"SelectFromModel\",\n    \"SelectFwe\",\n    \"SelectKBest\",\n    \"SelectPercentile\",\n    \"SelfTrainingClassifier\",\n    \"SequentialFeatureSelector\",\n    \"ShrunkCovariance\",\n    \"SimpleImputer\",\n    \"SkewedChi2Sampler\",\n    \"SparseCoder\",\n    \"SparsePCA\",\n    \"SparseRandomProjection\",\n    \"SpectralBiclustering\",\n    \"SpectralClustering\",\n    \"SpectralCoclustering\",\n    \"SpectralEmbedding\",\n    \"SplineTransformer\",\n    \"StackingClassifier\",\n    \"StackingRegressor\",\n    \"TSNE\",\n    \"TfidfVectorizer\",\n    \"TheilSenRegressor\",\n    \"TransformedTargetRegressor\",\n    \"TruncatedSVD\",\n    \"TweedieRegressor\",\n    \"VarianceThreshold\",\n    \"VotingClassifier\",\n    \"VotingRegressor\",\n]\n\n\ndef get_all_methods():\n    estimators = all_estimators()\n    for name, Estimator in estimators:\n        if name.startswith(\"_\"):\n            # skip private classes\n            continue\n        methods = []\n        for name in dir(Estimator):\n            if name.startswith(\"_\"):\n                continue\n            method_obj = getattr(Estimator, name)\n            if hasattr(method_obj, \"__call__\") or isinstance(method_obj, property):\n                methods.append(name)\n        methods.append(None)\n\n        for method in sorted(methods, key=lambda x: str(x)):\n            yield Estimator, method\n\n\ndef filter_errors(errors, method, Estimator=None):\n    \"\"\"\n    Ignore some errors based on the method type.\n\n    These rules are specific for scikit-learn.\"\"\"\n    for code, message in errors:\n        # We ignore following error code,\n        #  - RT02: The first line of the Returns section\n        #    should contain only the type, ..\n        #   (as we may need refer to the name of the returned\n        #    object)\n        #  - GL01: Docstring text (summary) should start in the line\n        #  immediately after the opening quotes (not in the same line,\n        #  or leaving a blank line in between)\n\n        if code in [\"RT02\", \"GL01\"]:\n            continue\n\n        # Ignore PR02: Unknown parameters for properties. We sometimes use\n        # properties for ducktyping, i.e. SGDClassifier.predict_proba\n        if code == \"PR02\" and Estimator is not None and method is not None:\n            method_obj = getattr(Estimator, method)\n            if isinstance(method_obj, property):\n                continue\n\n        # Following codes are only taken into account for the\n        # top level class docstrings:\n        #  - ES01: No extended summary found\n        #  - SA01: See Also section not found\n        #  - EX01: No examples section found\n\n        if method is not None and code in [\"EX01\", \"SA01\", \"ES01\"]:\n            continue\n        yield code, message\n\n\ndef repr_errors(res, estimator=None, method: Optional[str] = None) -> str:\n    \"\"\"Pretty print original docstring and the obtained errors\n\n    Parameters\n    ----------\n    res : dict\n        result of numpydoc.validate.validate\n    estimator : {estimator, None}\n        estimator object or None\n    method : str\n        if estimator is not None, either the method name or None.\n\n    Returns\n    -------\n    str\n       String representation of the error.\n    \"\"\"\n    if method is None:\n        if hasattr(estimator, \"__init__\"):\n            method = \"__init__\"\n        elif estimator is None:\n            raise ValueError(\"At least one of estimator, method should be provided\")\n        else:\n            raise NotImplementedError\n\n    if estimator is not None:\n        obj = getattr(estimator, method)\n        try:\n            obj_signature = signature(obj)\n        except TypeError:\n            # In particular we can't parse the signature of properties\n            obj_signature = (\n                \"\\nParsing of the method signature failed, \"\n                \"possibly because this is a property.\"\n            )\n\n        obj_name = estimator.__name__ + \".\" + method\n    else:\n        obj_signature = \"\"\n        obj_name = method\n\n    msg = \"\\n\\n\" + \"\\n\\n\".join(\n        [\n            str(res[\"file\"]),\n            obj_name + str(obj_signature),\n            res[\"docstring\"],\n            \"# Errors\",\n            \"\\n\".join(\n                \" - {}: {}\".format(code, message) for code, message in res[\"errors\"]\n            ),\n        ]\n    )\n    return msg\n\n\n@pytest.mark.parametrize(\"Estimator, method\", get_all_methods())\ndef test_docstring(Estimator, method, request):\n    base_import_path = Estimator.__module__\n    import_path = [base_import_path, Estimator.__name__]\n    if method is not None:\n        import_path.append(method)\n\n    import_path = \".\".join(import_path)\n\n    if Estimator.__name__ in DOCSTRING_IGNORE_LIST:\n        request.applymarker(\n            pytest.mark.xfail(run=False, reason=\"TODO pass numpydoc validation\")\n        )\n\n    res = numpydoc_validation.validate(import_path)\n\n    res[\"errors\"] = list(filter_errors(res[\"errors\"], method, Estimator=Estimator))\n\n    if res[\"errors\"]:\n        msg = repr_errors(res, Estimator, method)\n\n        raise ValueError(msg)\n\n\nif __name__ == \"__main__\":\n    import sys\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"Validate docstring with numpydoc.\")\n    parser.add_argument(\"import_path\", help=\"Import path to validate\")\n\n    args = parser.parse_args()\n\n    res = numpydoc_validation.validate(args.import_path)\n\n    import_path_sections = args.import_path.split(\".\")\n    # When applied to classes, detect class method. For functions\n    # method = None.\n    # TODO: this detection can be improved. Currently we assume that we have\n    # class # methods if the second path element before last is in camel case.\n    if len(import_path_sections) >= 2 and re.match(\n        r\"(?:[A-Z][a-z]*)+\", import_path_sections[-2]\n    ):\n        method = import_path_sections[-1]\n    else:\n        method = None\n\n    res[\"errors\"] = list(filter_errors(res[\"errors\"], method))\n\n    if res[\"errors\"]:\n        msg = repr_errors(res, method=args.import_path)\n\n        print(msg)\n        sys.exit(1)\n    else:\n        print(\"All docstring checks passed for {}!\".format(args.import_path))\n"
    },
    {
      "filename": "sklearn/base.py",
      "content": "\"\"\"Base classes for all estimators.\"\"\"\n\n# Author: Gael Varoquaux <gael.varoquaux@normalesup.org>\n# License: BSD 3 clause\n\nimport copy\nimport warnings\nfrom collections import defaultdict\nimport platform\nimport inspect\nimport re\n\nimport numpy as np\n\nfrom . import __version__\nfrom ._config import get_config\nfrom .utils import _IS_32BIT\nfrom .utils._tags import (\n    _DEFAULT_TAGS,\n    _safe_tags,\n)\nfrom .utils.validation import check_X_y\nfrom .utils.validation import check_array\nfrom .utils.validation import _check_y\nfrom .utils.validation import _num_features\nfrom .utils._estimator_html_repr import estimator_html_repr\n\n\ndef clone(estimator, *, safe=True):\n    \"\"\"Constructs a new unfitted estimator with the same parameters.\n\n    Clone does a deep copy of the model in an estimator\n    without actually copying attached data. It yields a new estimator\n    with the same parameters that has not been fitted on any data.\n\n    If the estimator's `random_state` parameter is an integer (or if the\n    estimator doesn't have a `random_state` parameter), an *exact clone* is\n    returned: the clone and the original estimator will give the exact same\n    results. Otherwise, *statistical clone* is returned: the clone might\n    yield different results from the original estimator. More details can be\n    found in :ref:`randomness`.\n\n    Parameters\n    ----------\n    estimator : {list, tuple, set} of estimator instance or a single \\\n            estimator instance\n        The estimator or group of estimators to be cloned.\n\n    safe : bool, default=True\n        If safe is False, clone will fall back to a deep copy on objects\n        that are not estimators.\n\n    \"\"\"\n    estimator_type = type(estimator)\n    # XXX: not handling dictionaries\n    if estimator_type in (list, tuple, set, frozenset):\n        return estimator_type([clone(e, safe=safe) for e in estimator])\n    elif not hasattr(estimator, \"get_params\") or isinstance(estimator, type):\n        if not safe:\n            return copy.deepcopy(estimator)\n        else:\n            if isinstance(estimator, type):\n                raise TypeError(\n                    \"Cannot clone object. \"\n                    + \"You should provide an instance of \"\n                    + \"scikit-learn estimator instead of a class.\"\n                )\n            else:\n                raise TypeError(\n                    \"Cannot clone object '%s' (type %s): \"\n                    \"it does not seem to be a scikit-learn \"\n                    \"estimator as it does not implement a \"\n                    \"'get_params' method.\" % (repr(estimator), type(estimator))\n                )\n\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n    new_object = klass(**new_object_params)\n    params_set = new_object.get_params(deep=False)\n\n    # quick sanity check of the parameters of the clone\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n    return new_object\n\n\ndef _pprint(params, offset=0, printer=repr):\n    \"\"\"Pretty print the dictionary 'params'\n\n    Parameters\n    ----------\n    params : dict\n        The dictionary to pretty print\n\n    offset : int, default=0\n        The offset in characters to add at the begin of each line.\n\n    printer : callable, default=repr\n        The function to convert entries to strings, typically\n        the builtin str or repr\n\n    \"\"\"\n    # Do a multi-line justified repr:\n    options = np.get_printoptions()\n    np.set_printoptions(precision=5, threshold=64, edgeitems=2)\n    params_list = list()\n    this_line_length = offset\n    line_sep = \",\\n\" + (1 + offset // 2) * \" \"\n    for i, (k, v) in enumerate(sorted(params.items())):\n        if type(v) is float:\n            # use str for representing floating point numbers\n            # this way we get consistent representation across\n            # architectures and versions.\n            this_repr = \"%s=%s\" % (k, str(v))\n        else:\n            # use repr of the rest\n            this_repr = \"%s=%s\" % (k, printer(v))\n        if len(this_repr) > 500:\n            this_repr = this_repr[:300] + \"...\" + this_repr[-100:]\n        if i > 0:\n            if this_line_length + len(this_repr) >= 75 or \"\\n\" in this_repr:\n                params_list.append(line_sep)\n                this_line_length = len(line_sep)\n            else:\n                params_list.append(\", \")\n                this_line_length += 2\n        params_list.append(this_repr)\n        this_line_length += len(this_repr)\n\n    np.set_printoptions(**options)\n    lines = \"\".join(params_list)\n    # Strip trailing space to avoid nightmare in doctests\n    lines = \"\\n\".join(l.rstrip(\" \") for l in lines.split(\"\\n\"))\n    return lines\n\n\nclass BaseEstimator:\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n    \"\"\"\n\n    @classmethod\n    def _get_param_names(cls):\n        \"\"\"Get parameter names for the estimator\"\"\"\n        # fetch the constructor or the original constructor before\n        # deprecation wrapping if any\n        init = getattr(cls.__init__, \"deprecated_original\", cls.__init__)\n        if init is object.__init__:\n            # No explicit constructor to introspect\n            return []\n\n        # introspect the constructor arguments to find the model parameters\n        # to represent\n        init_signature = inspect.signature(init)\n        # Consider the constructor parameters excluding 'self'\n        parameters = [\n            p\n            for p in init_signature.parameters.values()\n            if p.name != \"self\" and p.kind != p.VAR_KEYWORD\n        ]\n        for p in parameters:\n            if p.kind == p.VAR_POSITIONAL:\n                raise RuntimeError(\n                    \"scikit-learn estimators should always \"\n                    \"specify their parameters in the signature\"\n                    \" of their __init__ (no varargs).\"\n                    \" %s with constructor %s doesn't \"\n                    \" follow this convention.\" % (cls, init_signature)\n                )\n        # Extract and sort argument names excluding 'self'\n        return sorted([p.name for p in parameters])\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : dict\n            Parameter names mapped to their values.\n        \"\"\"\n        out = dict()\n        for key in self._get_param_names():\n            value = getattr(self, key)\n            if deep and hasattr(value, \"get_params\"):\n                deep_items = value.get_params().items()\n                out.update((key + \"__\" + k, val) for k, val in deep_items)\n            out[key] = value\n        return out\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.\n\n        Returns\n        -------\n        self : estimator instance\n            Estimator instance.\n        \"\"\"\n        if not params:\n            # Simple optimization to gain speed (inspect is slow)\n            return self\n        valid_params = self.get_params(deep=True)\n\n        nested_params = defaultdict(dict)  # grouped by prefix\n        for key, value in params.items():\n            key, delim, sub_key = key.partition(\"__\")\n            if key not in valid_params:\n                raise ValueError(\n                    \"Invalid parameter %s for estimator %s. \"\n                    \"Check the list of available parameters \"\n                    \"with `estimator.get_params().keys()`.\" % (key, self)\n                )\n\n            if delim:\n                nested_params[key][sub_key] = value\n            else:\n                setattr(self, key, value)\n                valid_params[key] = value\n\n        for key, sub_params in nested_params.items():\n            valid_params[key].set_params(**sub_params)\n\n        return self\n\n    def __repr__(self, N_CHAR_MAX=700):\n        # N_CHAR_MAX is the (approximate) maximum number of non-blank\n        # characters to render. We pass it as an optional parameter to ease\n        # the tests.\n\n        from .utils._pprint import _EstimatorPrettyPrinter\n\n        N_MAX_ELEMENTS_TO_SHOW = 30  # number of elements to show in sequences\n\n        # use ellipsis for sequences with a lot of elements\n        pp = _EstimatorPrettyPrinter(\n            compact=True,\n            indent=1,\n            indent_at_name=True,\n            n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW,\n        )\n\n        repr_ = pp.pformat(self)\n\n        # Use bruteforce ellipsis when there are a lot of non-blank characters\n        n_nonblank = len(\"\".join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2  # apprx number of chars to keep on both ends\n            regex = r\"^(\\s*\\S){%d}\" % lim\n            # The regex '^(\\s*\\S){%d}' % n\n            # matches from the start of the string until the nth non-blank\n            # character:\n            # - ^ matches the start of string\n            # - (pattern){n} matches n repetitions of pattern\n            # - \\s*\\S matches a non-blank char following zero or more blanks\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n\n            if \"\\n\" in repr_[left_lim:-right_lim]:\n                # The left side and right side aren't on the same line.\n                # To avoid weird cuts, e.g.:\n                # categoric...ore',\n                # we need to start the right side with an appropriate newline\n                # character so that it renders properly as:\n                # categoric...\n                # handle_unknown='ignore',\n                # so we add [^\\n]*\\n which matches until the next \\n\n                regex += r\"[^\\n]*\\n\"\n                right_lim = re.match(regex, repr_[::-1]).end()\n\n            ellipsis = \"...\"\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                # Only add ellipsis if it results in a shorter repr\n                repr_ = repr_[:left_lim] + \"...\" + repr_[-right_lim:]\n\n        return repr_\n\n    def __getstate__(self):\n        try:\n            state = super().__getstate__()\n        except AttributeError:\n            state = self.__dict__.copy()\n\n        if type(self).__module__.startswith(\"sklearn.\"):\n            return dict(state.items(), _sklearn_version=__version__)\n        else:\n            return state\n\n    def __setstate__(self, state):\n        if type(self).__module__.startswith(\"sklearn.\"):\n            pickle_version = state.pop(\"_sklearn_version\", \"pre-0.18\")\n            if pickle_version != __version__:\n                warnings.warn(\n                    \"Trying to unpickle estimator {0} from version {1} when \"\n                    \"using version {2}. This might lead to breaking code or \"\n                    \"invalid results. Use at your own risk.\".format(\n                        self.__class__.__name__, pickle_version, __version__\n                    ),\n                    UserWarning,\n                )\n        try:\n            super().__setstate__(state)\n        except AttributeError:\n            self.__dict__.update(state)\n\n    def _more_tags(self):\n        return _DEFAULT_TAGS\n\n    def _get_tags(self):\n        collected_tags = {}\n        for base_class in reversed(inspect.getmro(self.__class__)):\n            if hasattr(base_class, \"_more_tags\"):\n                # need the if because mixins might not have _more_tags\n                # but might do redundant work in estimators\n                # (i.e. calling more tags on BaseEstimator multiple times)\n                more_tags = base_class._more_tags(self)\n                collected_tags.update(more_tags)\n        return collected_tags\n\n    def _check_n_features(self, X, reset):\n        \"\"\"Set the `n_features_in_` attribute, or check against it.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n        reset : bool\n            If True, the `n_features_in_` attribute is set to `X.shape[1]`.\n            If False and the attribute exists, then check that it is equal to\n            `X.shape[1]`. If False and the attribute does *not* exist, then\n            the check is skipped.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n        \"\"\"\n        try:\n            n_features = _num_features(X)\n        except TypeError as e:\n            if not reset and hasattr(self, \"n_features_in_\"):\n                raise ValueError(\n                    \"X does not contain any features, but \"\n                    f\"{self.__class__.__name__} is expecting \"\n                    f\"{self.n_features_in_} features\"\n                ) from e\n            # If the number of features is not defined and reset=True,\n            # then we skip this check\n            return\n\n        if reset:\n            self.n_features_in_ = n_features\n            return\n\n        if not hasattr(self, \"n_features_in_\"):\n            # Skip this check if the expected number of expected input features\n            # was not recorded by calling fit first. This is typically the case\n            # for stateless transformers.\n            return\n\n        if n_features != self.n_features_in_:\n            raise ValueError(\n                f\"X has {n_features} features, but {self.__class__.__name__} \"\n                f\"is expecting {self.n_features_in_} features as input.\"\n            )\n\n    def _validate_data(\n        self,\n        X=\"no_validation\",\n        y=\"no_validation\",\n        reset=True,\n        validate_separately=False,\n        **check_params,\n    ):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape \\\n                (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        if y is None and self._get_tags()[\"requires_y\"]:\n            raise ValueError(\n                f\"This {self.__class__.__name__} estimator \"\n                \"requires y to be passed, but the target y is None.\"\n            )\n\n        no_val_X = isinstance(X, str) and X == \"no_validation\"\n        no_val_y = y is None or isinstance(y, str) and y == \"no_validation\"\n\n        if no_val_X and no_val_y:\n            raise ValueError(\"Validation should be done on X, y or both.\")\n        elif not no_val_X and no_val_y:\n            X = check_array(X, **check_params)\n            out = X\n        elif no_val_X and not no_val_y:\n            y = _check_y(y, **check_params)\n            out = y\n        else:\n            if validate_separately:\n                # We need this because some estimators validate X and y\n                # separately, and in general, separately calling check_array()\n                # on X and y isn't equivalent to just calling check_X_y()\n                # :(\n                check_X_params, check_y_params = validate_separately\n                X = check_array(X, **check_X_params)\n                y = check_array(y, **check_y_params)\n            else:\n                X, y = check_X_y(X, y, **check_params)\n            out = X, y\n\n        if not no_val_X and check_params.get(\"ensure_2d\", True):\n            self._check_n_features(X, reset=reset)\n\n        return out\n\n    @property\n    def _repr_html_(self):\n        \"\"\"HTML representation of estimator.\n\n        This is redundant with the logic of `_repr_mimebundle_`. The latter\n        should be favorted in the long term, `_repr_html_` is only\n        implemented for consumers who do not interpret `_repr_mimbundle_`.\n        \"\"\"\n        if get_config()[\"display\"] != \"diagram\":\n            raise AttributeError(\n                \"_repr_html_ is only defined when the \"\n                \"'display' configuration option is set to \"\n                \"'diagram'\"\n            )\n        return self._repr_html_inner\n\n    def _repr_html_inner(self):\n        \"\"\"This function is returned by the @property `_repr_html_` to make\n        `hasattr(estimator, \"_repr_html_\") return `True` or `False` depending\n        on `get_config()[\"display\"]`.\n        \"\"\"\n        return estimator_html_repr(self)\n\n    def _repr_mimebundle_(self, **kwargs):\n        \"\"\"Mime bundle used by jupyter kernels to display estimator\"\"\"\n        output = {\"text/plain\": repr(self)}\n        if get_config()[\"display\"] == \"diagram\":\n            output[\"text/html\"] = estimator_html_repr(self)\n        return output\n\n\nclass ClassifierMixin:\n    \"\"\"Mixin class for all classifiers in scikit-learn.\"\"\"\n\n    _estimator_type = \"classifier\"\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` wrt. `y`.\n        \"\"\"\n        from .metrics import accuracy_score\n\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n\n    def _more_tags(self):\n        return {\"requires_y\": True}\n\n\nclass RegressorMixin:\n    \"\"\"Mixin class for all regression estimators in scikit-learn.\"\"\"\n\n    _estimator_type = \"regressor\"\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\\\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).\n        \"\"\"\n\n        from .metrics import r2_score\n\n        y_pred = self.predict(X)\n        return r2_score(y, y_pred, sample_weight=sample_weight)\n\n    def _more_tags(self):\n        return {\"requires_y\": True}\n\n\nclass ClusterMixin:\n    \"\"\"Mixin class for all cluster estimators in scikit-learn.\"\"\"\n\n    _estimator_type = \"clusterer\"\n\n    def fit_predict(self, X, y=None):\n        \"\"\"\n        Perform clustering on `X` and returns cluster labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,), dtype=np.int64\n            Cluster labels.\n        \"\"\"\n        # non-optimized default implementation; override when a better\n        # method is possible for a given clustering algorithm\n        self.fit(X)\n        return self.labels_\n\n    def _more_tags(self):\n        return {\"preserves_dtype\": []}\n\n\nclass BiclusterMixin:\n    \"\"\"Mixin class for all bicluster estimators in scikit-learn.\"\"\"\n\n    @property\n    def biclusters_(self):\n        \"\"\"Convenient way to get row and column indicators together.\n\n        Returns the ``rows_`` and ``columns_`` members.\n        \"\"\"\n        return self.rows_, self.columns_\n\n    def get_indices(self, i):\n        \"\"\"Row and column indices of the `i`'th bicluster.\n\n        Only works if ``rows_`` and ``columns_`` attributes exist.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.\n\n        Returns\n        -------\n        row_ind : ndarray, dtype=np.intp\n            Indices of rows in the dataset that belong to the bicluster.\n        col_ind : ndarray, dtype=np.intp\n            Indices of columns in the dataset that belong to the bicluster.\n\n        \"\"\"\n        rows = self.rows_[i]\n        columns = self.columns_[i]\n        return np.nonzero(rows)[0], np.nonzero(columns)[0]\n\n    def get_shape(self, i):\n        \"\"\"Shape of the `i`'th bicluster.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.\n\n        Returns\n        -------\n        n_rows : int\n            Number of rows in the bicluster.\n\n        n_cols : int\n            Number of columns in the bicluster.\n        \"\"\"\n        indices = self.get_indices(i)\n        return tuple(len(i) for i in indices)\n\n    def get_submatrix(self, i, data):\n        \"\"\"Return the submatrix corresponding to bicluster `i`.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.\n        data : array-like of shape (n_samples, n_features)\n            The data.\n\n        Returns\n        -------\n        submatrix : ndarray of shape (n_rows, n_cols)\n            The submatrix corresponding to bicluster `i`.\n\n        Notes\n        -----\n        Works with sparse matrices. Only works if ``rows_`` and\n        ``columns_`` attributes exist.\n        \"\"\"\n        from .utils.validation import check_array\n\n        data = check_array(data, accept_sparse=\"csr\")\n        row_ind, col_ind = self.get_indices(i)\n        return data[row_ind[:, np.newaxis], col_ind]\n\n\nclass TransformerMixin:\n    \"\"\"Mixin class for all transformers in scikit-learn.\"\"\"\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n                default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.\n\n        Returns\n        -------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.\n        \"\"\"\n        # non-optimized default implementation; override when a better\n        # method is possible for a given clustering algorithm\n        if y is None:\n            # fit method of arity 1 (unsupervised transformation)\n            return self.fit(X, **fit_params).transform(X)\n        else:\n            # fit method of arity 2 (supervised transformation)\n            return self.fit(X, y, **fit_params).transform(X)\n\n\nclass DensityMixin:\n    \"\"\"Mixin class for all density estimators in scikit-learn.\"\"\"\n\n    _estimator_type = \"DensityEstimator\"\n\n    def score(self, X, y=None):\n        \"\"\"Return the score of the model on the data `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        score : float\n        \"\"\"\n        pass\n\n\nclass OutlierMixin:\n    \"\"\"Mixin class for all outlier detection estimators in scikit-learn.\"\"\"\n\n    _estimator_type = \"outlier_detector\"\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Perform fit on X and returns labels for X.\n\n        Returns -1 for outliers and 1 for inliers.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            1 for inliers, -1 for outliers.\n        \"\"\"\n        # override for transductive outlier detectors like LocalOulierFactor\n        return self.fit(X).predict(X)\n\n\nclass MetaEstimatorMixin:\n    _required_parameters = [\"estimator\"]\n    \"\"\"Mixin class for all meta estimators in scikit-learn.\"\"\"\n\n\nclass MultiOutputMixin:\n    \"\"\"Mixin to mark estimators that support multioutput.\"\"\"\n\n    def _more_tags(self):\n        return {\"multioutput\": True}\n\n\nclass _UnstableArchMixin:\n    \"\"\"Mark estimators that are non-determinstic on 32bit or PowerPC\"\"\"\n\n    def _more_tags(self):\n        return {\n            \"non_deterministic\": (\n                _IS_32BIT or platform.machine().startswith((\"ppc\", \"powerpc\"))\n            )\n        }\n\n\ndef is_classifier(estimator):\n    \"\"\"Return True if the given estimator is (probably) a classifier.\n\n    Parameters\n    ----------\n    estimator : object\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if estimator is a classifier and False otherwise.\n    \"\"\"\n    return getattr(estimator, \"_estimator_type\", None) == \"classifier\"\n\n\ndef is_regressor(estimator):\n    \"\"\"Return True if the given estimator is (probably) a regressor.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if estimator is a regressor and False otherwise.\n    \"\"\"\n    return getattr(estimator, \"_estimator_type\", None) == \"regressor\"\n\n\ndef is_outlier_detector(estimator):\n    \"\"\"Return True if the given estimator is (probably) an outlier detector.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if estimator is an outlier detector and False otherwise.\n    \"\"\"\n    return getattr(estimator, \"_estimator_type\", None) == \"outlier_detector\"\n\n\ndef _is_pairwise(estimator):\n    \"\"\"Returns True if estimator is pairwise.\n\n    - If the `_pairwise` attribute and the tag are present and consistent,\n      then use the value and not issue a warning.\n    - If the `_pairwise` attribute and the tag are present and not\n      consistent, use the `_pairwise` value and issue a deprecation\n      warning.\n    - If only the `_pairwise` attribute is present and it is not False,\n      issue a deprecation warning and use the `_pairwise` value.\n\n    Parameters\n    ----------\n    estimator : object\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if the estimator is pairwise and False otherwise.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n        has_pairwise_attribute = hasattr(estimator, \"_pairwise\")\n        pairwise_attribute = getattr(estimator, \"_pairwise\", False)\n    pairwise_tag = _safe_tags(estimator, key=\"pairwise\")\n\n    if has_pairwise_attribute:\n        if pairwise_attribute != pairwise_tag:\n            warnings.warn(\n                \"_pairwise was deprecated in 0.24 and will be removed in 1.1 \"\n                \"(renaming of 0.26). Set the estimator tags of your estimator \"\n                \"instead\",\n                FutureWarning,\n            )\n        return pairwise_attribute\n\n    # use pairwise tag when the attribute is not present\n    return pairwise_tag\n"
    }
  ]
}