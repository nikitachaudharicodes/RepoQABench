{
  "repo_name": "scikit-learn_scikit-learn",
  "issue_id": "3450",
  "issue_description": "# Add sample weight support to more metrics\n\nMost metrics now supports `sample_weights` except for the following one:\n- [x] hamming_loss\n- [x] confusion_matrix\n- [x] hinge_loss\n- [x] jaccard_similarity_score\n- [x] log_loss\n- [x] matthews_corrcoef_score\n- [ ] median_absolute_error\n\nNote that there is already a general test in `sklearn/metrics/tests/test_common.py` for `sample_weight`.\n\nIdeally, it would be one pull request per metric to ease review.\n",
  "issue_comments": [
    {
      "id": 50301044,
      "user": "jatinshah",
      "body": "I want to get involved in contributing to scikit-learn. This seems straightforward, would like to take it up. Let me know if this works and if this is still an issue that we want to be implemented.\n"
    },
    {
      "id": 50325182,
      "user": "arjoly",
      "body": "You can take any metric and add sample_weight support. Thanks @jatinshah !\n"
    },
    {
      "id": 50325581,
      "user": "jnothman",
      "body": "But for any multilabel metrics, you may be better off waiting for #3395,\nwhich I should make an effort to wrap up.\n\nOn 28 July 2014 21:01, Arnaud Joly notifications@github.com wrote:\n\n>  You can take any metric and add sample_weight support. Thanks @jatinshah\n> https://github.com/jatinshah !\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-50325182\n> .\n"
    },
    {
      "id": 50466581,
      "user": "jatinshah",
      "body": "Thanks, @jnothman. I will keep it in mind.\n\n@arjoly, Here is a pull request for jaccard_similarity_score #3497. Let me know if there are any issues I will work on a few others as well. \n"
    },
    {
      "id": 50586655,
      "user": "arjoly",
      "body": "Thanks to @jatinshah, jaccard similarity support sample weight! \n"
    },
    {
      "id": 51167196,
      "user": "jatinshah",
      "body": "I am working on adding sample_weight to log_loss. The implementation is straightforward, but tests are failing.\n\nlog_loss input parameters are y_true as labels and y_pred as probabilities. However, the tests for sample_weights assume that both y_true and y_pred are labels. Is there a simple way to resolve this issue?\n"
    },
    {
      "id": 51169900,
      "user": "arjoly",
      "body": "I would say that the test need to be enhance/modify to take this new case into account.\n"
    },
    {
      "id": 51183953,
      "user": "jatinshah",
      "body": "OK, modified the tests. Here is the pull request: #3531\n"
    },
    {
      "id": 51236631,
      "user": "jatinshah",
      "body": "For matthews_corrcoef_score, I need to replace numpy.corrcoef with a weighted correlation function. Since numpy does not have a weighted version, I am planning to place the function in utils/wcorrcoef.py and related unit tests in utils/tests/test_wcorrcoef.py.\n\nDoes that work? Is there a better way to organize the code?\n\n```\ndef wcorrcoef(X, Y, w):\n    mX = np.average(X, weights=w)\n    mY = np.average(Y, weights=w)\n    covXY = np.average((X-mX)*(Y-mY), weights=w)\n    covXX = np.average((X-mX)*(X-mX), weights=w)\n    covYY = np.average((Y-mY)*(Y-mY), weights=w)\n    return covXY/np.sqrt(covXX * covYY)\n```\n"
    },
    {
      "id": 51267319,
      "user": "jnothman",
      "body": "IMO keep it locally in metrics, unless you get a patch supporting sample\nweights accepted to numpy or scipy and instead it goes in\nsklearn.utils.fixes. Then again, I don't know how complicated the\nimplementation is.\n\nOn 6 August 2014 04:10, Jatin Shah notifications@github.com wrote:\n\n>  For matthews_corrcoef_score, I need to replace numpy.corrcoef with a\n> weight correlation function. Since numpy does not have a weighted version,\n> I am planning to place the function in utils/wcorrcoef.py and related unit\n> tests in utils/tests/test_wcorrcoef.py.\n> \n> Does that work? Is there a better way to organize the code?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-51236631\n> .\n"
    },
    {
      "id": 51312506,
      "user": "arjoly",
      "body": "Sample weight added to the log_loss thanks to @jatinshah in https://github.com/scikit-learn/scikit-learn/pull/3531.\n"
    },
    {
      "id": 51372824,
      "user": "jatinshah",
      "body": "I am trying to add `sample_weight` to `matthews_corrcoef_score` and I noticed some strange stuff. `roc_auc_score` and `average_precision_score` are not tested in `tests_common.py` for binary inputs. They are added to `METRIC_UNDEFINED_MULTICLASS` and these functions are skipped in both binary and multi class unit tests (see [here](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/tests/test_common.py#L975)).\n\nSecond, I am unable to change `average` and `sample_weight` parameters for both `roc_auc_score` and `average_precision_score`.\n\n```\n>>> from sklearn.metrics import roc_auc_score\n>>> import bumpy as np\n>>> y_true = np.array([0, 0, 1, 1])\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> roc_auc_score(y_true, y_scores, average='micro')\nTypeError: roc_auc_score() got an unexpected keyword argument 'average'\n```\n"
    },
    {
      "id": 51393957,
      "user": "arjoly",
      "body": "> I am trying to add sample_weight to matthews_corrcoef_score and I noticed some strange stuff. roc_auc_score and average_precision_score are not tested in tests_common.py for binary inputs.\n\nThose metrics doesn't support multi-class either by definition (samples averaged metrics) or it's not implemented.\n"
    },
    {
      "id": 51394210,
      "user": "arjoly",
      "body": "The micro-averaged roc auc doesn't support binary output (only multilabel output).\nBut the binary (traditional) roc auc supports it \n\n```\nIn [3]: import numpy as np\n\nIn [4]:  y_true = np.array([0, 0, 1, 1])\n\nIn [5]: y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n\nIn [6]: roc_auc_score(y_true, y_scores)\nOut[6]: 0.75\n```\n"
    },
    {
      "id": 51394447,
      "user": "arjoly",
      "body": "> Second, I am unable to change average and sample_weight parameters for both roc_auc_score and average_precision_score.\n\nIn order to use the average argument, you need to have [multilabel data](http://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification-format)\n"
    },
    {
      "id": 59643059,
      "user": "SaurabhJha",
      "body": "Here is a pull request for median_absolute_error. #3784 \n"
    },
    {
      "id": 64984798,
      "user": "kubami",
      "body": "I am working on this issue, and currently focusing on the `hamming_loss`.\nAt first I thought it might not make sense for this metric to be _weighted_. Now I think it would be usable to be able weight specific transitions in the hamming distance:\n\nIf we define normal hamming distance as:\n\n\\sum_t \\delta(y_t, z_t)\n\nwe can add a transition weight, `w` as follows:\n\n\\sum weight(y_t, z_t) \\delta(y_t, z_t)\n\nIt would be nicer to add it to the\nhttp://docs.scipy.org/doc/scipy-0.14.0/reference/spatial.distance.html\n\nas `whamming(u,v,w)`.\n\nand then add it to the `hamming_loss` here.\nWhat do you think?\n\nsee example of this being used in\nhttp://aclweb.org/anthology/N/N13/N13-1102.pdf\n"
    },
    {
      "id": 65223885,
      "user": "arjoly",
      "body": "If I understand correctly, what you suggest is to weight differently each dimension (here label) differently. The issue is about sample-wise weight.\n\nFor instance, \n\n```\n>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n0.75\n```\n\nWhile with `sample_weight=[0.5, 1]`, it should give\n\n```\n>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)), sample_weight=[0.5, 1])\n0.625 # (1 / 2 * 0.5 + 2 / 2 * 1) / 2\n```\n"
    },
    {
      "id": 67992797,
      "user": "DanielSidhion",
      "body": "I'd like to begin contributing to scikit-learn, and I'm interested in adding sample weights to the confusion matrix, however I'd like to clarify how sample weights are applied here. Is it done by simply apllying the weight to each sample's corresponding cell (sample `i` with weight `wi` is a true positive. So I add `wi` to the TP cell instead of adding `1`), or is there anything else that should be done?\n"
    },
    {
      "id": 68015494,
      "user": "jnothman",
      "body": "Yes, I think that's the correct understanding.\n\nOn 24 December 2014 at 07:05, Bernardo Vecchia Stein <\nnotifications@github.com> wrote:\n\n>  I'd like to begin contributing to scikit-learn, and I'm interested in\n> adding sample weights to the confusion matrix, however I'd like to clarify\n> how sample weights are applied here. Is it done by simply apllying the\n> weight to each sample's corresponding cell (sample i with weight wi is a\n> true positive. So I add wi to the TP cell instead of adding 1), or is\n> there anything else that should be done?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-67992797\n> .\n"
    },
    {
      "id": 68066901,
      "user": "DanielSidhion",
      "body": "Thanks @jnothman! I have finished changing the code, however the tests in `test_common.py` are failing because they expect the result from the metric to be a float, not an array (as is the case for the confusion matrix). How should I proceed in this case? Should I write tests exclusively for confusion matrix, or is there something else we can do?\n"
    },
    {
      "id": 68074351,
      "user": "jnothman",
      "body": "I think confusion matrix is currently excluded from other invariance tests.\nYes, it might require its own explicit testing, or there may be a cunning\nway to fix up the existing test.\n\nOn 25 December 2014 at 04:56, Bernardo Vecchia Stein <\nnotifications@github.com> wrote:\n\n>  Thanks @jnothman https://github.com/jnothman! I have finished changing\n> the code, however the tests in test_common.py are failing because they\n> expect the result from the metric to be a float, not an array (as is the\n> case for the confusion matrix). How should I proceed in this case? Should I\n> write tests exclusively for confusion matrix, or is there something else we\n> can do?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-68066901\n> .\n"
    },
    {
      "id": 69649067,
      "user": "raghavrv",
      "body": "@jatinshah Do you plan to go ahead with adding sample weight support to the `matthews_corrcoef_score`? If not would you mind if I proceed with [your implementation](https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-51236631) to submit a PR for the same?\n"
    },
    {
      "id": 69695544,
      "user": "jatinshah",
      "body": "@ragv I don't intend to work on it at the moment. Please go ahead and submit a PR\n"
    },
    {
      "id": 70385679,
      "user": "raghavrv",
      "body": "Thanks! now that completes the todo ( counting the PRs raised for respective metrics )\n"
    },
    {
      "id": 149145679,
      "user": "achab",
      "body": "@othersParticipantsOfParisSprint : I'm working on this one\n"
    },
    {
      "id": 417887295,
      "user": "nikhilchh",
      "body": "Can somebody please explain the difference between:\r\n- average='samples' and\r\n- average='weighted'\r\nfor **MULTILABEL** case.\r\n\r\nI understand 'weighted'. It multiplies weight for each label to the accuracy for that label before taking the average.\r\nWhere, weight for label 'x' = (num samples with 'x' label)/( all samples)s"
    },
    {
      "id": 417935161,
      "user": "jnothman",
      "body": "Your question does not belong on a development issue tracker. Please ask in\nanother forum, such as stack overflow.\n"
    },
    {
      "id": 635135394,
      "user": "lorentzenchr",
      "body": "@jnothman @glemaitre I think this issue can be closed as #17225 was the last piece of the puzzle."
    },
    {
      "id": 635155783,
      "user": "glemaitre",
      "body": "Thanks @lorentzenchr for pointing this out. Closing."
    }
  ],
  "text_context": "# Add sample weight support to more metrics\n\nMost metrics now supports `sample_weights` except for the following one:\n- [x] hamming_loss\n- [x] confusion_matrix\n- [x] hinge_loss\n- [x] jaccard_similarity_score\n- [x] log_loss\n- [x] matthews_corrcoef_score\n- [ ] median_absolute_error\n\nNote that there is already a general test in `sklearn/metrics/tests/test_common.py` for `sample_weight`.\n\nIdeally, it would be one pull request per metric to ease review.\n\n\nI want to get involved in contributing to scikit-learn. This seems straightforward, would like to take it up. Let me know if this works and if this is still an issue that we want to be implemented.\n\n\nYou can take any metric and add sample_weight support. Thanks @jatinshah !\n\n\nBut for any multilabel metrics, you may be better off waiting for #3395,\nwhich I should make an effort to wrap up.\n\nOn 28 July 2014 21:01, Arnaud Joly notifications@github.com wrote:\n\n>  You can take any metric and add sample_weight support. Thanks @jatinshah\n> https://github.com/jatinshah !\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-50325182\n> .\n\n\nThanks, @jnothman. I will keep it in mind.\n\n@arjoly, Here is a pull request for jaccard_similarity_score #3497. Let me know if there are any issues I will work on a few others as well. \n\n\nThanks to @jatinshah, jaccard similarity support sample weight! \n\n\nI am working on adding sample_weight to log_loss. The implementation is straightforward, but tests are failing.\n\nlog_loss input parameters are y_true as labels and y_pred as probabilities. However, the tests for sample_weights assume that both y_true and y_pred are labels. Is there a simple way to resolve this issue?\n\n\nI would say that the test need to be enhance/modify to take this new case into account.\n\n\nOK, modified the tests. Here is the pull request: #3531\n\n\nFor matthews_corrcoef_score, I need to replace numpy.corrcoef with a weighted correlation function. Since numpy does not have a weighted version, I am planning to place the function in utils/wcorrcoef.py and related unit tests in utils/tests/test_wcorrcoef.py.\n\nDoes that work? Is there a better way to organize the code?\n\n```\ndef wcorrcoef(X, Y, w):\n    mX = np.average(X, weights=w)\n    mY = np.average(Y, weights=w)\n    covXY = np.average((X-mX)*(Y-mY), weights=w)\n    covXX = np.average((X-mX)*(X-mX), weights=w)\n    covYY = np.average((Y-mY)*(Y-mY), weights=w)\n    return covXY/np.sqrt(covXX * covYY)\n```\n\n\nIMO keep it locally in metrics, unless you get a patch supporting sample\nweights accepted to numpy or scipy and instead it goes in\nsklearn.utils.fixes. Then again, I don't know how complicated the\nimplementation is.\n\nOn 6 August 2014 04:10, Jatin Shah notifications@github.com wrote:\n\n>  For matthews_corrcoef_score, I need to replace numpy.corrcoef with a\n> weight correlation function. Since numpy does not have a weighted version,\n> I am planning to place the function in utils/wcorrcoef.py and related unit\n> tests in utils/tests/test_wcorrcoef.py.\n> \n> Does that work? Is there a better way to organize the code?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-51236631\n> .\n\n\nSample weight added to the log_loss thanks to @jatinshah in https://github.com/scikit-learn/scikit-learn/pull/3531.\n\n\nI am trying to add `sample_weight` to `matthews_corrcoef_score` and I noticed some strange stuff. `roc_auc_score` and `average_precision_score` are not tested in `tests_common.py` for binary inputs. They are added to `METRIC_UNDEFINED_MULTICLASS` and these functions are skipped in both binary and multi class unit tests (see [here](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/tests/test_common.py#L975)).\n\nSecond, I am unable to change `average` and `sample_weight` parameters for both `roc_auc_score` and `average_precision_score`.\n\n```\n>>> from sklearn.metrics import roc_auc_score\n>>> import bumpy as np\n>>> y_true = np.array([0, 0, 1, 1])\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> roc_auc_score(y_true, y_scores, average='micro')\nTypeError: roc_auc_score() got an unexpected keyword argument 'average'\n```\n\n\n> I am trying to add sample_weight to matthews_corrcoef_score and I noticed some strange stuff. roc_auc_score and average_precision_score are not tested in tests_common.py for binary inputs.\n\nThose metrics doesn't support multi-class either by definition (samples averaged metrics) or it's not implemented.\n\n\nThe micro-averaged roc auc doesn't support binary output (only multilabel output).\nBut the binary (traditional) roc auc supports it \n\n```\nIn [3]: import numpy as np\n\nIn [4]:  y_true = np.array([0, 0, 1, 1])\n\nIn [5]: y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n\nIn [6]: roc_auc_score(y_true, y_scores)\nOut[6]: 0.75\n```\n\n\n> Second, I am unable to change average and sample_weight parameters for both roc_auc_score and average_precision_score.\n\nIn order to use the average argument, you need to have [multilabel data](http://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification-format)\n\n\nHere is a pull request for median_absolute_error. #3784 \n\n\nI am working on this issue, and currently focusing on the `hamming_loss`.\nAt first I thought it might not make sense for this metric to be _weighted_. Now I think it would be usable to be able weight specific transitions in the hamming distance:\n\nIf we define normal hamming distance as:\n\n\\sum_t \\delta(y_t, z_t)\n\nwe can add a transition weight, `w` as follows:\n\n\\sum weight(y_t, z_t) \\delta(y_t, z_t)\n\nIt would be nicer to add it to the\nhttp://docs.scipy.org/doc/scipy-0.14.0/reference/spatial.distance.html\n\nas `whamming(u,v,w)`.\n\nand then add it to the `hamming_loss` here.\nWhat do you think?\n\nsee example of this being used in\nhttp://aclweb.org/anthology/N/N13/N13-1102.pdf\n\n\nIf I understand correctly, what you suggest is to weight differently each dimension (here label) differently. The issue is about sample-wise weight.\n\nFor instance, \n\n```\n>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n0.75\n```\n\nWhile with `sample_weight=[0.5, 1]`, it should give\n\n```\n>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)), sample_weight=[0.5, 1])\n0.625 # (1 / 2 * 0.5 + 2 / 2 * 1) / 2\n```\n\n\nI'd like to begin contributing to scikit-learn, and I'm interested in adding sample weights to the confusion matrix, however I'd like to clarify how sample weights are applied here. Is it done by simply apllying the weight to each sample's corresponding cell (sample `i` with weight `wi` is a true positive. So I add `wi` to the TP cell instead of adding `1`), or is there anything else that should be done?\n\n\nYes, I think that's the correct understanding.\n\nOn 24 December 2014 at 07:05, Bernardo Vecchia Stein <\nnotifications@github.com> wrote:\n\n>  I'd like to begin contributing to scikit-learn, and I'm interested in\n> adding sample weights to the confusion matrix, however I'd like to clarify\n> how sample weights are applied here. Is it done by simply apllying the\n> weight to each sample's corresponding cell (sample i with weight wi is a\n> true positive. So I add wi to the TP cell instead of adding 1), or is\n> there anything else that should be done?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-67992797\n> .\n\n\nThanks @jnothman! I have finished changing the code, however the tests in `test_common.py` are failing because they expect the result from the metric to be a float, not an array (as is the case for the confusion matrix). How should I proceed in this case? Should I write tests exclusively for confusion matrix, or is there something else we can do?\n\n\nI think confusion matrix is currently excluded from other invariance tests.\nYes, it might require its own explicit testing, or there may be a cunning\nway to fix up the existing test.\n\nOn 25 December 2014 at 04:56, Bernardo Vecchia Stein <\nnotifications@github.com> wrote:\n\n>  Thanks @jnothman https://github.com/jnothman! I have finished changing\n> the code, however the tests in test_common.py are failing because they\n> expect the result from the metric to be a float, not an array (as is the\n> case for the confusion matrix). How should I proceed in this case? Should I\n> write tests exclusively for confusion matrix, or is there something else we\n> can do?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-68066901\n> .\n\n\n@jatinshah Do you plan to go ahead with adding sample weight support to the `matthews_corrcoef_score`? If not would you mind if I proceed with [your implementation](https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-51236631) to submit a PR for the same?\n\n\n@ragv I don't intend to work on it at the moment. Please go ahead and submit a PR\n\n\nThanks! now that completes the todo ( counting the PRs raised for respective metrics )\n\n\n@othersParticipantsOfParisSprint : I'm working on this one\n\n\nCan somebody please explain the difference between:\r\n- average='samples' and\r\n- average='weighted'\r\nfor **MULTILABEL** case.\r\n\r\nI understand 'weighted'. It multiplies weight for each label to the accuracy for that label before taking the average.\r\nWhere, weight for label 'x' = (num samples with 'x' label)/( all samples)s\n\nYour question does not belong on a development issue tracker. Please ask in\nanother forum, such as stack overflow.\n\n\n@jnothman @glemaitre I think this issue can be closed as #17225 was the last piece of the puzzle.\n\nThanks @lorentzenchr for pointing this out. Closing.",
  "pr_link": "https://github.com/scikit-learn/scikit-learn/pull/3531",
  "code_context": [
    {
      "filename": "sklearn/metrics/classification.py",
      "content": "\"\"\"Metrics to assess performance on classification task given classe prediction\n\nFunctions named as ``*_score`` return a scalar value to maximize: the higher\nthe better\n\nFunction named as ``*_error`` or ``*_loss`` return a scalar value to minimize:\nthe lower the better\n\"\"\"\n\n# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Arnaud Joly <a.joly@ulg.ac.be>\n#          Jochen Wersdorfer <jochen@wersdoerfer.de>\n#          Lars Buitinck <L.J.Buitinck@uva.nl>\n#          Joel Nothman <joel.nothman@gmail.com>\n#          Noel Dawe <noel@dawe.me>\n#          Jatin Shah <jatindshah@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import division\n\nimport warnings\nimport numpy as np\n\nfrom scipy.sparse import coo_matrix\nfrom scipy.spatial.distance import hamming as sp_hamming\n\nfrom ..externals.six.moves import zip\nfrom ..preprocessing import label_binarize\nfrom ..preprocessing import LabelBinarizer\nfrom ..preprocessing import LabelEncoder\nfrom ..utils import check_array, check_consistent_length\nfrom ..utils import column_or_1d\nfrom ..utils.multiclass import unique_labels\nfrom ..utils.multiclass import type_of_target\n\nfrom .base import UndefinedMetricWarning\n\n\ndef _check_clf_targets(y_true, y_pred):\n    \"\"\"Check that y_true and y_pred belong to the same classification task\n\n    This converts multiclass or binary types to a common shape, and raises a\n    ValueError for a mix of multilabel and multiclass targets, a mix of\n    multilabel formats, for the presence of continuous-valued or multioutput\n    targets, or for targets of different lengths.\n\n    Column vectors are squeezed to 1d.\n\n    Parameters\n    ----------\n    y_true : array-like,\n\n    y_pred : array-like\n\n    Returns\n    -------\n    type_true : one of {'multilabel-indicator', 'multilabel-sequences', \\\n                        'multiclass', 'binary'}\n        The type of the true target data, as output by\n        ``utils.multiclass.type_of_target``\n\n    y_true : array or indicator matrix\n\n    y_pred : array or indicator matrix\n    \"\"\"\n    check_consistent_length(y_true, y_pred)\n    type_true = type_of_target(y_true)\n    type_pred = type_of_target(y_pred)\n\n    y_type = set([type_true, type_pred])\n    if y_type == set([\"binary\", \"multiclass\"]):\n        y_type = set([\"multiclass\"])\n\n    if len(y_type) > 1:\n        raise ValueError(\"Can't handle mix of {0} and {1}\"\n                         \"\".format(type_true, type_pred))\n\n    # We can't have more than one value on y_type => The set is no more needed\n    y_type = y_type.pop()\n\n    # No metrics support \"multiclass-multioutput\" format\n    if (y_type not in [\"binary\", \"multiclass\", \"multilabel-indicator\",\n                       \"multilabel-sequences\"]):\n        raise ValueError(\"{0} is not supported\".format(y_type))\n\n    if y_type in [\"binary\", \"multiclass\"]:\n        y_true = column_or_1d(y_true)\n        y_pred = column_or_1d(y_pred)\n\n    return y_type, y_true, y_pred\n\n\ndef _weighted_sum(sample_score, sample_weight, normalize=False):\n    if normalize:\n        return np.average(sample_score, weights=sample_weight)\n    elif sample_weight is not None:\n        return np.dot(sample_score, sample_weight)\n    else:\n        return sample_score.sum()\n\n\ndef accuracy_score(y_true, y_pred, normalize=True, sample_weight=None):\n    \"\"\"Accuracy classification score.\n\n    In multilabel classification, this function computes subset accuracy:\n    the set of labels predicted for a sample must *exactly* match the\n    corresponding set of labels in y_true.\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) labels.\n\n    y_pred : array-like or label indicator matrix\n        Predicted labels, as returned by a classifier.\n\n    normalize : bool, optional (default=True)\n        If ``False``, return the number of correctly classified samples.\n        Otherwise, return the fraction of correctly classified samples.\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    score : float\n        If ``normalize == True``, return the correctly classified samples\n        (float), else it returns the number of correctly classified samples\n        (int).\n\n        The best performance is 1 with ``normalize == True`` and the number\n        of samples with ``normalize == False``.\n\n    See also\n    --------\n    jaccard_similarity_score, hamming_loss, zero_one_loss\n\n    Notes\n    -----\n    In binary and multiclass classification, this function is equal\n    to the ``jaccard_similarity_score`` function.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.metrics import accuracy_score\n    >>> y_pred = [0, 2, 1, 3]\n    >>> y_true = [0, 1, 2, 3]\n    >>> accuracy_score(y_true, y_pred)\n    0.5\n    >>> accuracy_score(y_true, y_pred, normalize=False)\n    2\n\n    In the multilabel case with binary label indicators:\n    >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n    0.5\n    \"\"\"\n\n    # Compute accuracy for each possible representation\n    y_type, y_true, y_pred = _check_clf_targets(y_true, y_pred)\n    if y_type == 'multilabel-indicator':\n        score = (y_pred != y_true).sum(axis=1) == 0\n    elif y_type == 'multilabel-sequences':\n        score = np.array([len(set(true) ^ set(pred)) == 0\n                          for pred, true in zip(y_pred, y_true)])\n    else:\n        score = y_true == y_pred\n\n    return _weighted_sum(score, sample_weight, normalize)\n\n\ndef confusion_matrix(y_true, y_pred, labels=None):\n    \"\"\"Compute confusion matrix to evaluate the accuracy of a classification\n\n    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n    is equal to the number of observations known to be in group :math:`i` but\n    predicted to be in group :math:`j`.\n\n    Parameters\n    ----------\n    y_true : array, shape = [n_samples]\n        Ground truth (correct) target values.\n\n    y_pred : array, shape = [n_samples]\n        Estimated targets as returned by a classifier.\n\n    labels : array, shape = [n_classes], optional\n        List of labels to index the matrix. This may be used to reorder\n        or select a subset of labels.\n        If none is given, those that appear at least once\n        in ``y_true`` or ``y_pred`` are used in sorted order.\n\n    Returns\n    -------\n    C : array, shape = [n_classes, n_classes]\n        Confusion matrix\n\n    References\n    ----------\n    .. [1] `Wikipedia entry for the Confusion matrix\n           <http://en.wikipedia.org/wiki/Confusion_matrix>`_\n\n    Examples\n    --------\n    >>> from sklearn.metrics import confusion_matrix\n    >>> y_true = [2, 0, 2, 2, 0, 1]\n    >>> y_pred = [0, 0, 2, 2, 0, 2]\n    >>> confusion_matrix(y_true, y_pred)\n    array([[2, 0, 0],\n           [0, 0, 1],\n           [1, 0, 2]])\n\n    \"\"\"\n    y_type, y_true, y_pred = _check_clf_targets(y_true, y_pred)\n    if y_type not in (\"binary\", \"multiclass\"):\n        raise ValueError(\"%s is not supported\" % y_type)\n\n    if labels is None:\n        labels = unique_labels(y_true, y_pred)\n    else:\n        labels = np.asarray(labels)\n\n    n_labels = labels.size\n    label_to_ind = dict((y, x) for x, y in enumerate(labels))\n    # convert yt, yp into index\n    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])\n    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])\n\n    # intersect y_pred, y_true with labels, eliminate items not in labels\n    ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\n    y_pred = y_pred[ind]\n    y_true = y_true[ind]\n\n    CM = coo_matrix((np.ones(y_true.shape[0], dtype=np.int), (y_true, y_pred)),\n                    shape=(n_labels, n_labels)\n                    ).toarray()\n\n    return CM\n\n\ndef jaccard_similarity_score(y_true, y_pred, normalize=True,\n                             sample_weight=None):\n    \"\"\"Jaccard similarity coefficient score\n\n    The Jaccard index [1], or Jaccard similarity coefficient, defined as\n    the size of the intersection divided by the size of the union of two label\n    sets, is used to compare set of predicted labels for a sample to the\n    corresponding set of labels in ``y_true``.\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) labels.\n\n    y_pred : array-like or label indicator matrix\n        Predicted labels, as returned by a classifier.\n\n    normalize : bool, optional (default=True)\n        If ``False``, return the sum of the Jaccard similarity coefficient\n        over the sample set. Otherwise, return the average of Jaccard\n        similarity coefficient.\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    score : float\n        If ``normalize == True``, return the average Jaccard similarity\n        coefficient, else it returns the sum of the Jaccard similarity\n        coefficient over the sample set.\n\n        The best performance is 1 with ``normalize == True`` and the number\n        of samples with ``normalize == False``.\n\n    See also\n    --------\n    accuracy_score, hamming_loss, zero_one_loss\n\n    Notes\n    -----\n    In binary and multiclass classification, this function is equivalent\n    to the ``accuracy_score``. It differs in the multilabel classification\n    problem.\n\n    References\n    ----------\n    .. [1] `Wikipedia entry for the Jaccard index\n           <http://en.wikipedia.org/wiki/Jaccard_index>`_\n\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.metrics import jaccard_similarity_score\n    >>> y_pred = [0, 2, 1, 3]\n    >>> y_true = [0, 1, 2, 3]\n    >>> jaccard_similarity_score(y_true, y_pred)\n    0.5\n    >>> jaccard_similarity_score(y_true, y_pred, normalize=False)\n    2\n\n    In the multilabel case with binary label indicators:\n\n    >>> jaccard_similarity_score(np.array([[0, 1], [1, 1]]),\\\n        np.ones((2, 2)))\n    0.75\n    \"\"\"\n\n    # Compute accuracy for each possible representation\n    y_type, y_true, y_pred = _check_clf_targets(y_true, y_pred)\n    if y_type == 'multilabel-indicator':\n        with np.errstate(divide='ignore', invalid='ignore'):\n            # oddly, we may get an \"invalid\" rather than a \"divide\"\n            # error here\n            y_pred_pos_label = y_pred == 1\n            y_true_pos_label = y_true == 1\n            pred_inter_true = np.sum(np.logical_and(y_pred_pos_label,\n                                                    y_true_pos_label),\n                                     axis=1)\n            pred_union_true = np.sum(np.logical_or(y_pred_pos_label,\n                                                   y_true_pos_label),\n                                     axis=1)\n            score = pred_inter_true / pred_union_true\n\n            # If there is no label, it results in a Nan instead, we set\n            # the jaccard to 1: lim_{x->0} x/x = 1\n            # Note with py2.6 and np 1.3: we can't check safely for nan.\n            score[pred_union_true == 0.0] = 1.0\n\n    elif y_type == 'multilabel-sequences':\n        score = np.empty(len(y_true), dtype=np.float)\n        for i, (true, pred) in enumerate(zip(y_pred, y_true)):\n            true_set = set(true)\n            pred_set = set(pred)\n            size_true_union_pred = len(true_set | pred_set)\n            # If there is no label, it results in a Nan instead, we set\n            # the jaccard to 1: lim_{x->0} x/x = 1\n            if size_true_union_pred == 0:\n                score[i] = 1.\n            else:\n                score[i] = (len(true_set & pred_set) /\n                            size_true_union_pred)\n    else:\n        score = y_true == y_pred\n\n    return _weighted_sum(score, sample_weight, normalize)\n\n\ndef matthews_corrcoef(y_true, y_pred):\n    \"\"\"Compute the Matthews correlation coefficient (MCC) for binary classes\n\n    The Matthews correlation coefficient is used in machine learning as a\n    measure of the quality of binary (two-class) classifications. It takes into\n    account true and false positives and negatives and is generally regarded as\n    a balanced measure which can be used even if the classes are of very\n    different sizes. The MCC is in essence a correlation coefficient value\n    between -1 and +1. A coefficient of +1 represents a perfect prediction, 0\n    an average random prediction and -1 an inverse prediction.  The statistic\n    is also known as the phi coefficient. [source: Wikipedia]\n\n    Only in the binary case does this relate to information about true and\n    false positives and negatives. See references below.\n\n    Parameters\n    ----------\n    y_true : array, shape = [n_samples]\n        Ground truth (correct) target values.\n\n    y_pred : array, shape = [n_samples]\n        Estimated targets as returned by a classifier.\n\n    Returns\n    -------\n    mcc : float\n        The Matthews correlation coefficient (+1 represents a perfect\n        prediction, 0 an average random prediction and -1 and inverse\n        prediction).\n\n    References\n    ----------\n    .. [1] `Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the\n       accuracy of prediction algorithms for classification: an overview\n       <http://dx.doi.org/10.1093/bioinformatics/16.5.412>`_\n\n    .. [2] `Wikipedia entry for the Matthews Correlation Coefficient\n       <http://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_\n\n    Examples\n    --------\n    >>> from sklearn.metrics import matthews_corrcoef\n    >>> y_true = [+1, +1, +1, -1]\n    >>> y_pred = [+1, -1, +1, +1]\n    >>> matthews_corrcoef(y_true, y_pred)  # doctest: +ELLIPSIS\n    -0.33...\n\n    \"\"\"\n    y_type, y_true, y_pred = _check_clf_targets(y_true, y_pred)\n\n    if y_type != \"binary\":\n        raise ValueError(\"%s is not supported\" % y_type)\n\n    lb = LabelEncoder()\n    lb.fit(np.hstack([y_true, y_pred]))\n    y_true = lb.transform(y_true)\n    y_pred = lb.transform(y_pred)\n    with np.errstate(invalid='ignore'):\n        mcc = np.corrcoef(y_true, y_pred)[0, 1]\n\n    if np.isnan(mcc):\n        return 0.\n    else:\n        return mcc\n\n\ndef zero_one_loss(y_true, y_pred, normalize=True, sample_weight=None):\n    \"\"\"Zero-one classification loss.\n\n    If normalize is ``True``, return the fraction of misclassifications\n    (float), else it returns the number of misclassifications (int). The best\n    performance is 0.\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) labels.\n\n    y_pred : array-like or label indicator matrix\n        Predicted labels, as returned by a classifier.\n\n    normalize : bool, optional (default=True)\n        If ``False``, return the number of misclassifications.\n        Otherwise, return the fraction of misclassifications.\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    loss : float or int,\n        If ``normalize == True``, return the fraction of misclassifications\n        (float), else it returns the number of misclassifications (int).\n\n    Notes\n    -----\n    In multilabel classification, the zero_one_loss function corresponds to\n    the subset zero-one loss: for each sample, the entire set of labels must be\n    correctly predicted, otherwise the loss for that sample is equal to one.\n\n    See also\n    --------\n    accuracy_score, hamming_loss, jaccard_similarity_score\n\n    Examples\n    --------\n    >>> from sklearn.metrics import zero_one_loss\n    >>> y_pred = [1, 2, 3, 4]\n    >>> y_true = [2, 2, 3, 4]\n    >>> zero_one_loss(y_true, y_pred)\n    0.25\n    >>> zero_one_loss(y_true, y_pred, normalize=False)\n    1\n\n    In the multilabel case with binary label indicators:\n\n    >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n    0.5\n    \"\"\"\n    score = accuracy_score(y_true, y_pred,\n                           normalize=normalize,\n                           sample_weight=sample_weight)\n\n    if normalize:\n        return 1 - score\n    else:\n        if sample_weight is not None:\n            n_samples = np.sum(sample_weight)\n        else:\n            n_samples = len(y_true)\n        return n_samples - score\n\n\ndef f1_score(y_true, y_pred, labels=None, pos_label=1, average='weighted',\n             sample_weight=None):\n    \"\"\"Compute the F1 score, also known as balanced F-score or F-measure\n\n    The F1 score can be interpreted as a weighted average of the precision and\n    recall, where an F1 score reaches its best value at 1 and worst score at 0.\n    The relative contribution of precision and recall to the F1 score are\n    equal. The formula for the F1 score is::\n\n        F1 = 2 * (precision * recall) / (precision + recall)\n\n    In the multi-class and multi-label case, this is the weighted average of\n    the F1 score of each class.\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) target values.\n\n    y_pred : array-like or label indicator matrix\n        Estimated targets as returned by a classifier.\n\n    labels : array\n        Integer array of labels.\n\n    pos_label : str or int, 1 by default\n        If ``average`` is not ``None`` and the classification target is binary,\n        only this class's scores will be returned.\n\n    average : string, [None, 'micro', 'macro', 'samples', 'weighted' (default)]\n        If ``None``, the scores for each class are returned. Otherwise,\n        unless ``pos_label`` is given in binary classification, this\n        determines the type of averaging performed on the data:\n\n        ``'micro'``:\n            Calculate metrics globally by counting the total true positives,\n            false negatives and false positives.\n        ``'macro'``:\n            Calculate metrics for each label, and find their unweighted\n            mean.  This does not take label imbalance into account.\n        ``'weighted'``:\n            Calculate metrics for each label, and find their average, weighted\n            by support (the number of true instances for each label). This\n            alters 'macro' to account for label imbalance; it can result in an\n            F-score that is not between precision and recall.\n        ``'samples'``:\n            Calculate metrics for each instance, and find their average (only\n            meaningful for multilabel classification where this differs from\n            :func:`accuracy_score`).\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    f1_score : float or array of float, shape = [n_unique_labels]\n        F1 score of the positive class in binary classification or weighted\n        average of the F1 scores of each class for the multiclass task.\n\n    References\n    ----------\n    .. [1] `Wikipedia entry for the F1-score\n           <http://en.wikipedia.org/wiki/F1_score>`_\n\n    Examples\n    --------\n    >>> from sklearn.metrics import f1_score\n    >>> y_true = [0, 1, 2, 0, 1, 2]\n    >>> y_pred = [0, 2, 1, 0, 0, 1]\n    >>> f1_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n    0.26...\n    >>> f1_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n    0.33...\n    >>> f1_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS\n    0.26...\n    >>> f1_score(y_true, y_pred, average=None)\n    array([ 0.8,  0. ,  0. ])\n\n\n    \"\"\"\n    return fbeta_score(y_true, y_pred, 1, labels=labels,\n                       pos_label=pos_label, average=average,\n                       sample_weight=sample_weight)\n\n\ndef fbeta_score(y_true, y_pred, beta, labels=None, pos_label=1,\n                average='weighted', sample_weight=None):\n    \"\"\"Compute the F-beta score\n\n    The F-beta score is the weighted harmonic mean of precision and recall,\n    reaching its optimal value at 1 and its worst value at 0.\n\n    The `beta` parameter determines the weight of precision in the combined\n    score. ``beta < 1`` lends more weight to precision, while ``beta > 1``\n    favors recall (``beta -> 0`` considers only precision, ``beta -> inf``\n    only recall).\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) target values.\n\n    y_pred : array-like or label indicator matrix\n        Estimated targets as returned by a classifier.\n\n    beta: float\n        Weight of precision in harmonic mean.\n\n    labels : array\n        Integer array of labels.\n\n    pos_label : str or int, 1 by default\n        If ``average`` is not ``None`` and the classification target is binary,\n        only this class's scores will be returned.\n\n    average : string, [None, 'micro', 'macro', 'samples', 'weighted' (default)]\n        If ``None``, the scores for each class are returned. Otherwise,\n        unless ``pos_label`` is given in binary classification, this\n        determines the type of averaging performed on the data:\n\n        ``'micro'``:\n            Calculate metrics globally by counting the total true positives,\n            false negatives and false positives.\n        ``'macro'``:\n            Calculate metrics for each label, and find their unweighted\n            mean.  This does not take label imbalance into account.\n        ``'weighted'``:\n            Calculate metrics for each label, and find their average, weighted\n            by support (the number of true instances for each label). This\n            alters 'macro' to account for label imbalance; it can result in an\n            F-score that is not between precision and recall.\n        ``'samples'``:\n            Calculate metrics for each instance, and find their average (only\n            meaningful for multilabel classification where this differs from\n            :func:`accuracy_score`).\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    fbeta_score : float (if average is not None) or array of float, shape =\\\n        [n_unique_labels]\n        F-beta score of the positive class in binary classification or weighted\n        average of the F-beta score of each class for the multiclass task.\n\n    References\n    ----------\n    .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).\n           Modern Information Retrieval. Addison Wesley, pp. 327-328.\n\n    .. [2] `Wikipedia entry for the F1-score\n           <http://en.wikipedia.org/wiki/F1_score>`_\n\n    Examples\n    --------\n    >>> from sklearn.metrics import fbeta_score\n    >>> y_true = [0, 1, 2, 0, 1, 2]\n    >>> y_pred = [0, 2, 1, 0, 0, 1]\n    >>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n    ... # doctest: +ELLIPSIS\n    0.23...\n    >>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n    ... # doctest: +ELLIPSIS\n    0.33...\n    >>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n    ... # doctest: +ELLIPSIS\n    0.23...\n    >>> fbeta_score(y_true, y_pred, average=None, beta=0.5)\n    ... # doctest: +ELLIPSIS\n    array([ 0.71...,  0.        ,  0.        ])\n\n    \"\"\"\n    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n                                                 beta=beta,\n                                                 labels=labels,\n                                                 pos_label=pos_label,\n                                                 average=average,\n                                                 warn_for=('f-score',),\n                                                 sample_weight=sample_weight)\n    return f\n\n\ndef _prf_divide(numerator, denominator, metric, modifier, average, warn_for):\n    \"\"\"Performs division and handles divide-by-zero.\n\n    On zero-division, sets the corresponding result elements to zero\n    and raises a warning.\n\n    The metric, modifier and average arguments are used only for determining\n    an appropriate warning.\n    \"\"\"\n    result = numerator / denominator\n    mask = denominator == 0.0\n    if not np.any(mask):\n        return result\n\n    # remove infs\n    result[mask] = 0.0\n\n    # build appropriate warning\n    # E.g. \"Precision and F-score are ill-defined and being set to 0.0 in\n    # labels with no predicted samples\"\n    axis0 = 'sample'\n    axis1 = 'label'\n    if average == 'samples':\n        axis0, axis1 = axis1, axis0\n\n    if metric in warn_for and 'f-score' in warn_for:\n        msg_start = '{0} and F-score are'.format(metric.title())\n    elif metric in warn_for:\n        msg_start = '{0} is'.format(metric.title())\n    elif 'f-score' in warn_for:\n        msg_start = 'F-score is'\n    else:\n        return result\n\n    msg = ('{0} ill-defined and being set to 0.0 {{0}} '\n           'no {1} {2}s.'.format(msg_start, modifier, axis0))\n    if len(mask) == 1:\n        msg = msg.format('due to')\n    else:\n        msg = msg.format('in {0}s with'.format(axis1))\n    warnings.warn(msg, UndefinedMetricWarning, stacklevel=2)\n    return result\n\n\ndef precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,\n                                    pos_label=1, average=None,\n                                    warn_for=('precision', 'recall',\n                                              'f-score'),\n                                    sample_weight=None):\n    \"\"\"Compute precision, recall, F-measure and support for each class\n\n    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n    true positives and ``fp`` the number of false positives. The precision is\n    intuitively the ability of the classifier not to label as positive a sample\n    that is negative.\n\n    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n    true positives and ``fn`` the number of false negatives. The recall is\n    intuitively the ability of the classifier to find all the positive samples.\n\n    The F-beta score can be interpreted as a weighted harmonic mean of\n    the precision and recall, where an F-beta score reaches its best\n    value at 1 and worst score at 0.\n\n    The F-beta score weights recall more than precision by a factor of\n    ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n\n    The support is the number of occurrences of each class in ``y_true``.\n\n    If ``pos_label is None`` and in binary classification, this function\n    returns the average precision, recall and F-measure if ``average``\n    is one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) target values.\n\n    y_pred : array-like or label indicator matrix\n        Estimated targets as returned by a classifier.\n\n    beta : float, 1.0 by default\n        The strength of recall versus precision in the F-score.\n\n    labels : array\n        Integer array of labels.\n\n    pos_label : str or int, 1 by default\n        If ``average`` is not ``None`` and the classification target is binary,\n        only this class's scores will be returned.\n\n    average : string, [None (default), 'micro', 'macro', 'samples', 'weighted']\n        If ``None``, the scores for each class are returned. Otherwise,\n        unless ``pos_label`` is given in binary classification, this\n        determines the type of averaging performed on the data:\n\n        ``'micro'``:\n            Calculate metrics globally by counting the total true positives,\n            false negatives and false positives.\n        ``'macro'``:\n            Calculate metrics for each label, and find their unweighted\n            mean.  This does not take label imbalance into account.\n        ``'weighted'``:\n            Calculate metrics for each label, and find their average, weighted\n            by support (the number of true instances for each label). This\n            alters 'macro' to account for label imbalance; it can result in an\n            F-score that is not between precision and recall.\n        ``'samples'``:\n            Calculate metrics for each instance, and find their average (only\n            meaningful for multilabel classification where this differs from\n            :func:`accuracy_score`).\n\n    warn_for : tuple or set, for internal use\n        This determines which warnings will be made in the case that this\n        function is being used to return only one of its metrics.\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    precision: float (if average is not None) or array of float, shape =\\\n        [n_unique_labels]\n\n    recall: float (if average is not None) or array of float, , shape =\\\n        [n_unique_labels]\n\n    fbeta_score: float (if average is not None) or array of float, shape =\\\n        [n_unique_labels]\n\n    support: int (if average is not None) or array of int, shape =\\\n        [n_unique_labels]\n        The number of occurrences of each label in ``y_true``.\n\n    References\n    ----------\n    .. [1] `Wikipedia entry for the Precision and recall\n           <http://en.wikipedia.org/wiki/Precision_and_recall>`_\n\n    .. [2] `Wikipedia entry for the F1-score\n           <http://en.wikipedia.org/wiki/F1_score>`_\n\n    .. [3] `Discriminative Methods for Multi-labeled Classification Advances\n           in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n           Godbole, Sunita Sarawagi\n           <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`\n\n    Examples\n    --------\n    >>> from sklearn.metrics import precision_recall_fscore_support\n    >>> y_true = np.array([0, 1, 2, 0, 1, 2])\n    >>> y_pred = np.array([0, 2, 1, 0, 0, 1])\n    >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n    ... # doctest: +ELLIPSIS\n    (0.22..., 0.33..., 0.26..., None)\n    >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n    ... # doctest: +ELLIPSIS\n    (0.33..., 0.33..., 0.33..., None)\n    >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n    ... # doctest: +ELLIPSIS\n    (0.22..., 0.33..., 0.26..., None)\n\n    \"\"\"\n    average_options = (None, 'micro', 'macro', 'weighted', 'samples')\n    if average not in average_options:\n        raise ValueError('average has to be one of ' +\n                         str(average_options))\n    if beta <= 0:\n        raise ValueError(\"beta should be >0 in the F-beta score\")\n\n    y_type, y_true, y_pred = _check_clf_targets(y_true, y_pred)\n\n    label_order = labels  # save this for later\n    if labels is None:\n        labels = unique_labels(y_true, y_pred)\n    else:\n        labels = np.asarray(labels)\n\n    ### Calculate tp_sum, pred_sum, true_sum ###\n\n    if y_type.startswith('multilabel'):\n        if y_type == 'multilabel-sequences':\n            y_true = label_binarize(y_true, labels, multilabel=True)\n            y_pred = label_binarize(y_pred, labels, multilabel=True)\n        else:\n            # set negative labels to zero\n            y_true = y_true == 1\n            y_pred = y_pred == 1\n\n        if sample_weight is None:\n            sum_weight = 1\n            dtype = int\n        else:\n            sum_weight = np.expand_dims(sample_weight, 1)\n            dtype = float\n\n        sum_axis = 1 if average == 'samples' else 0\n        tp_sum = np.multiply(np.logical_and(y_true, y_pred),\n                             sum_weight).sum(axis=sum_axis, dtype=dtype)\n        pred_sum = np.sum(np.multiply(y_pred, sum_weight),\n                          axis=sum_axis, dtype=dtype)\n        true_sum = np.sum(np.multiply(y_true, sum_weight),\n                          axis=sum_axis, dtype=dtype)\n\n    elif average == 'samples':\n        raise ValueError(\"Sample-based precision, recall, fscore is \"\n                         \"not meaningful outside multilabel\"\n                         \"classification. See the accuracy_score instead.\")\n    else:\n        lb = LabelEncoder()\n        lb.fit(labels)\n        y_true = lb.transform(y_true)\n        y_pred = lb.transform(y_pred)\n        labels = lb.classes_\n\n        # labels are now from 0 to len(labels) - 1 -> use bincount\n        tp = y_true == y_pred\n        tp_bins = y_true[tp]\n        if sample_weight is not None:\n            tp_bins_weights = np.asarray(sample_weight)[tp]\n        else:\n            tp_bins_weights = None\n\n        if len(tp_bins):\n            tp_sum = np.bincount(tp_bins, weights=tp_bins_weights,\n                                 minlength=len(labels))\n        else:\n            # Pathological case\n            true_sum = pred_sum = tp_sum = np.zeros(len(labels))\n        if len(y_pred):\n            pred_sum = np.bincount(y_pred, weights=sample_weight,\n                                   minlength=len(labels))\n        if len(y_true):\n            true_sum = np.bincount(y_true, weights=sample_weight,\n                                   minlength=len(labels))\n\n    ### Select labels to keep ###\n\n    if y_type == 'binary' and average is not None and pos_label is not None:\n        if label_order is not None and len(label_order) == 2:\n            warnings.warn('In the future, providing two `labels` values, as '\n                          'well as `average` will average over those '\n                          'labels. For now, please use `labels=None` with '\n                          '`pos_label` to evaluate precision, recall and '\n                          'F-score for the positive label only.',\n                          FutureWarning)\n        if pos_label not in labels:\n            if len(labels) == 1:\n                # Only negative labels\n                return (0., 0., 0., 0)\n            else:\n                raise ValueError(\"pos_label=%r is not a valid label: %r\" %\n                                 (pos_label, labels))\n        pos_label_idx = labels == pos_label\n        tp_sum = tp_sum[pos_label_idx]\n        pred_sum = pred_sum[pos_label_idx]\n        true_sum = true_sum[pos_label_idx]\n\n    elif average == 'micro':\n        tp_sum = np.array([tp_sum.sum()])\n        pred_sum = np.array([pred_sum.sum()])\n        true_sum = np.array([true_sum.sum()])\n\n    ### Finally, we have all our sufficient statistics. Divide! ###\n\n    beta2 = beta ** 2\n    with np.errstate(divide='ignore', invalid='ignore'):\n        # Divide, and on zero-division, set scores to 0 and warn:\n\n        # Oddly, we may get an \"invalid\" rather than a \"divide\" error\n        # here.\n        precision = _prf_divide(tp_sum, pred_sum,\n                                'precision', 'predicted', average, warn_for)\n        recall = _prf_divide(tp_sum, true_sum,\n                             'recall', 'true', average, warn_for)\n        # Don't need to warn for F: either P or R warned, or tp == 0 where pos\n        # and true are nonzero, in which case, F is well-defined and zero\n        f_score = ((1 + beta2) * precision * recall /\n                   (beta2 * precision + recall))\n        f_score[tp_sum == 0] = 0.0\n\n    ## Average the results ##\n\n    if average == 'weighted':\n        weights = true_sum\n        if weights.sum() == 0:\n            return 0, 0, 0, None\n    elif average == 'samples':\n        weights = sample_weight\n    else:\n        weights = None\n\n    if average is not None:\n        precision = np.average(precision, weights=weights)\n        recall = np.average(recall, weights=weights)\n        f_score = np.average(f_score, weights=weights)\n        true_sum = None  # return no support\n    elif label_order is not None:\n        indices = np.searchsorted(labels, label_order)\n        precision = precision[indices]\n        recall = recall[indices]\n        f_score = f_score[indices]\n        true_sum = true_sum[indices]\n\n    return precision, recall, f_score, true_sum\n\n\ndef precision_score(y_true, y_pred, labels=None, pos_label=1,\n                    average='weighted', sample_weight=None):\n    \"\"\"Compute the precision\n\n    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n    true positives and ``fp`` the number of false positives. The precision is\n    intuitively the ability of the classifier not to label as positive a sample\n    that is negative.\n\n    The best value is 1 and the worst value is 0.\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) target values.\n\n    y_pred : array-like or label indicator matrix\n        Estimated targets as returned by a classifier.\n\n    labels : array\n        Integer array of labels.\n\n    pos_label : str or int, 1 by default\n        If ``average`` is not ``None`` and the classification target is binary,\n        only this class's scores will be returned.\n\n    average : string, [None, 'micro', 'macro', 'samples', 'weighted' (default)]\n        If ``None``, the scores for each class are returned. Otherwise,\n        unless ``pos_label`` is given in binary classification, this\n        determines the type of averaging performed on the data:\n\n        ``'micro'``:\n            Calculate metrics globally by counting the total true positives,\n            false negatives and false positives.\n        ``'macro'``:\n            Calculate metrics for each label, and find their unweighted\n            mean.  This does not take label imbalance into account.\n        ``'weighted'``:\n            Calculate metrics for each label, and find their average, weighted\n            by support (the number of true instances for each label). This\n            alters 'macro' to account for label imbalance; it can result in an\n            F-score that is not between precision and recall.\n        ``'samples'``:\n            Calculate metrics for each instance, and find their average (only\n            meaningful for multilabel classification where this differs from\n            :func:`accuracy_score`).\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    precision : float (if average is not None) or array of float, shape =\\\n        [n_unique_labels]\n        Precision of the positive class in binary classification or weighted\n        average of the precision of each class for the multiclass task.\n\n    Examples\n    --------\n\n    >>> from sklearn.metrics import precision_score\n    >>> y_true = [0, 1, 2, 0, 1, 2]\n    >>> y_pred = [0, 2, 1, 0, 0, 1]\n    >>> precision_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n    0.22...\n    >>> precision_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n    0.33...\n    >>> precision_score(y_true, y_pred, average='weighted')\n    ... # doctest: +ELLIPSIS\n    0.22...\n    >>> precision_score(y_true, y_pred, average=None)  # doctest: +ELLIPSIS\n    array([ 0.66...,  0.        ,  0.        ])\n\n    \"\"\"\n    p, _, _, _ = precision_recall_fscore_support(y_true, y_pred,\n                                                 labels=labels,\n                                                 pos_label=pos_label,\n                                                 average=average,\n                                                 warn_for=('precision',),\n                                                 sample_weight=sample_weight)\n    return p\n\n\ndef recall_score(y_true, y_pred, labels=None, pos_label=1, average='weighted',\n                 sample_weight=None):\n    \"\"\"Compute the recall\n\n    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n    true positives and ``fn`` the number of false negatives. The recall is\n    intuitively the ability of the classifier to find all the positive samples.\n\n    The best value is 1 and the worst value is 0.\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) target values.\n\n    y_pred : array-like or label indicator matrix\n        Estimated targets as returned by a classifier.\n\n    labels : array\n        Integer array of labels.\n\n    pos_label : str or int, 1 by default\n        If ``average`` is not ``None`` and the classification target is binary,\n        only this class's scores will be returned.\n\n    average : string, [None, 'micro', 'macro', 'samples', 'weighted' (default)]\n        If ``None``, the scores for each class are returned. Otherwise,\n        unless ``pos_label`` is given in binary classification, this\n        determines the type of averaging performed on the data:\n\n        ``'micro'``:\n            Calculate metrics globally by counting the total true positives,\n            false negatives and false positives.\n        ``'macro'``:\n            Calculate metrics for each label, and find their unweighted\n            mean.  This does not take label imbalance into account.\n        ``'weighted'``:\n            Calculate metrics for each label, and find their average, weighted\n            by support (the number of true instances for each label). This\n            alters 'macro' to account for label imbalance; it can result in an\n            F-score that is not between precision and recall.\n        ``'samples'``:\n            Calculate metrics for each instance, and find their average (only\n            meaningful for multilabel classification where this differs from\n            :func:`accuracy_score`).\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    recall : float (if average is not None) or array of float, shape =\\\n        [n_unique_labels]\n        Recall of the positive class in binary classification or weighted\n        average of the recall of each class for the multiclass task.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import recall_score\n    >>> y_true = [0, 1, 2, 0, 1, 2]\n    >>> y_pred = [0, 2, 1, 0, 0, 1]\n    >>> recall_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n    0.33...\n    >>> recall_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n    0.33...\n    >>> recall_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS\n    0.33...\n    >>> recall_score(y_true, y_pred, average=None)\n    array([ 1.,  0.,  0.])\n\n\n    \"\"\"\n    _, r, _, _ = precision_recall_fscore_support(y_true, y_pred,\n                                                 labels=labels,\n                                                 pos_label=pos_label,\n                                                 average=average,\n                                                 warn_for=('recall',),\n                                                 sample_weight=sample_weight)\n    return r\n\n\ndef classification_report(y_true, y_pred, labels=None, target_names=None,\n                          sample_weight=None):\n    \"\"\"Build a text report showing the main classification metrics\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) target values.\n\n    y_pred : array-like or label indicator matrix\n        Estimated targets as returned by a classifier.\n\n    labels : array, shape = [n_labels]\n        Optional list of label indices to include in the report.\n\n    target_names : list of strings\n        Optional display names matching the labels (same order).\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    report : string\n        Text summary of the precision, recall, F1 score for each class.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import classification_report\n    >>> y_true = [0, 1, 2, 2, 2]\n    >>> y_pred = [0, 0, 2, 2, 1]\n    >>> target_names = ['class 0', 'class 1', 'class 2']\n    >>> print(classification_report(y_true, y_pred, target_names=target_names))\n                 precision    recall  f1-score   support\n    <BLANKLINE>\n        class 0       0.50      1.00      0.67         1\n        class 1       0.00      0.00      0.00         1\n        class 2       1.00      0.67      0.80         3\n    <BLANKLINE>\n    avg / total       0.70      0.60      0.61         5\n    <BLANKLINE>\n\n    \"\"\"\n\n    if labels is None:\n        labels = unique_labels(y_true, y_pred)\n    else:\n        labels = np.asarray(labels)\n\n    last_line_heading = 'avg / total'\n\n    if target_names is None:\n        width = len(last_line_heading)\n        target_names = ['%s' % l for l in labels]\n    else:\n        width = max(len(cn) for cn in target_names)\n        width = max(width, len(last_line_heading))\n\n    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n    fmt = '%% %ds' % width  # first column: class name\n    fmt += '  '\n    fmt += ' '.join(['% 9s' for _ in headers])\n    fmt += '\\n'\n\n    headers = [\"\"] + headers\n    report = fmt % tuple(headers)\n    report += '\\n'\n\n    p, r, f1, s = precision_recall_fscore_support(y_true, y_pred,\n                                                  labels=labels,\n                                                  average=None,\n                                                  sample_weight=sample_weight)\n\n    for i, label in enumerate(labels):\n        values = [target_names[i]]\n        for v in (p[i], r[i], f1[i]):\n            values += [\"{0:0.2f}\".format(v)]\n        values += [\"{0}\".format(s[i])]\n        report += fmt % tuple(values)\n\n    report += '\\n'\n\n    # compute averages\n    values = [last_line_heading]\n    for v in (np.average(p, weights=s),\n              np.average(r, weights=s),\n              np.average(f1, weights=s)):\n        values += [\"{0:0.2f}\".format(v)]\n    values += ['{0}'.format(np.sum(s))]\n    report += fmt % tuple(values)\n    return report\n\n\ndef hamming_loss(y_true, y_pred, classes=None):\n    \"\"\"Compute the average Hamming loss.\n\n    The Hamming loss is the fraction of labels that are incorrectly predicted.\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) labels.\n\n    y_pred : array-like or label indicator matrix\n        Predicted labels, as returned by a classifier.\n\n    classes : array, shape = [n_labels], optional\n        Integer array of labels.\n\n    Returns\n    -------\n    loss : float or int,\n        Return the average Hamming loss between element of ``y_true`` and\n        ``y_pred``.\n\n    See Also\n    --------\n    accuracy_score, jaccard_similarity_score, zero_one_loss\n\n    Notes\n    -----\n    In multiclass classification, the Hamming loss correspond to the Hamming\n    distance between ``y_true`` and ``y_pred`` which is equivalent to the\n    subset ``zero_one_loss`` function.\n\n    In multilabel classification, the Hamming loss is different from the\n    subset zero-one loss. The zero-one loss considers the entire set of labels\n    for a given sample incorrect if it does entirely match the true set of\n    labels. Hamming loss is more forgiving in that it penalizes the individual\n    labels.\n\n    The Hamming loss is upperbounded by the subset zero-one loss. When\n    normalized over samples, the Hamming loss is always between 0 and 1.\n\n    References\n    ----------\n    .. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:\n           An Overview. International Journal of Data Warehousing & Mining,\n           3(3), 1-13, July-September 2007.\n\n    .. [2] `Wikipedia entry on the Hamming distance\n           <http://en.wikipedia.org/wiki/Hamming_distance>`_\n\n    Examples\n    --------\n    >>> from sklearn.metrics import hamming_loss\n    >>> y_pred = [1, 2, 3, 4]\n    >>> y_true = [2, 2, 3, 4]\n    >>> hamming_loss(y_true, y_pred)\n    0.25\n\n    In the multilabel case with binary label indicators:\n\n    >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n    0.75\n    \"\"\"\n    y_type, y_true, y_pred = _check_clf_targets(y_true, y_pred)\n\n    if classes is None:\n        classes = unique_labels(y_true, y_pred)\n    else:\n        classes = np.asarray(classes)\n\n    if y_type == 'multilabel-indicator':\n        return np.mean(y_true != y_pred)\n    elif y_type == 'multilabel-sequences':\n        loss = np.array([len(set(pred).symmetric_difference(true))\n                         for pred, true in zip(y_pred, y_true)])\n\n        return np.mean(loss) / np.size(classes)\n\n    elif y_type in [\"binary\", \"multiclass\"]:\n        return sp_hamming(y_true, y_pred)\n    else:\n        raise ValueError(\"{0} is not supported\".format(y_type))\n\n\ndef log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None):\n    \"\"\"Log loss, aka logistic loss or cross-entropy loss.\n\n    This is the loss function used in (multinomial) logistic regression\n    and extensions of it such as neural networks, defined as the negative\n    log-likelihood of the true labels given a probabilistic classifier's\n    predictions. For a single sample with true label yt in {0,1} and\n    estimated probability yp that yt = 1, the log loss is\n\n        -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) labels for n_samples samples.\n\n    y_pred : array-like of float, shape = (n_samples, n_classes)\n        Predicted probabilities, as returned by a classifier's\n        predict_proba method.\n\n    eps : float\n        Log loss is undefined for p=0 or p=1, so probabilities are\n        clipped to max(eps, min(1 - eps, p)).\n\n    normalize : bool, optional (default=True)\n        If true, return the mean loss per sample.\n        Otherwise, return the sum of the per-sample losses.\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    loss : float\n\n    Examples\n    --------\n    >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],  # doctest: +ELLIPSIS\n    ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n    0.21616...\n\n    References\n    ----------\n    C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n    p. 209.\n\n    Notes\n    -----\n    The logarithm used is the natural logarithm (base-e).\n    \"\"\"\n    lb = LabelBinarizer()\n    T = lb.fit_transform(y_true)\n    if T.shape[1] == 1:\n        T = np.append(1 - T, T, axis=1)\n\n    # Clipping\n    Y = np.clip(y_pred, eps, 1 - eps)\n\n    # This happens in cases when elements in y_pred have type \"str\".\n    if not isinstance(Y, np.ndarray):\n        raise ValueError(\"y_pred should be an array of floats.\")\n\n    # If y_pred is of single dimension, assume y_true to be binary\n    # and then check.\n    if Y.ndim == 1:\n        Y = Y[:, np.newaxis]\n    if Y.shape[1] == 1:\n        Y = np.append(1 - Y, Y, axis=1)\n\n    # Check if dimensions are consistent.\n    check_consistent_length(T, Y)\n    T = check_array(T)\n    Y = check_array(Y)\n    if T.shape[1] != Y.shape[1]:\n        raise ValueError(\"y_true and y_pred have different number of classes \"\n                         \"%d, %d\" % (T.shape[1], Y.shape[1]))\n\n    # Renormalize\n    Y /= Y.sum(axis=1)[:, np.newaxis]\n    loss = -(T * np.log(Y)).sum(axis=1)\n\n    return _weighted_sum(loss, sample_weight, normalize)\n\n\ndef hinge_loss(y_true, pred_decision, pos_label=None, neg_label=None):\n    \"\"\"Average hinge loss (non-regularized)\n\n    Assuming labels in y_true are encoded with +1 and -1, when a prediction\n    mistake is made, ``margin = y_true * pred_decision`` is always negative\n    (since the signs disagree), implying ``1 - margin`` is always greater than\n    1.  The cumulated hinge loss is therefore an upper bound of the number of\n    mistakes made by the classifier.\n\n    Parameters\n    ----------\n    y_true : array, shape = [n_samples]\n        True target, consisting of integers of two values. The positive label\n        must be greater than the negative label.\n\n    pred_decision : array, shape = [n_samples] or [n_samples, n_classes]\n        Predicted decisions, as output by decision_function (floats).\n\n    Returns\n    -------\n    loss : float\n\n    References\n    ----------\n    .. [1] `Wikipedia entry on the Hinge loss\n            <http://en.wikipedia.org/wiki/Hinge_loss>`_\n\n    Examples\n    --------\n    >>> from sklearn import svm\n    >>> from sklearn.metrics import hinge_loss\n    >>> X = [[0], [1]]\n    >>> y = [-1, 1]\n    >>> est = svm.LinearSVC(random_state=0)\n    >>> est.fit(X, y)\n    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n         intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',\n         penalty='l2', random_state=0, tol=0.0001, verbose=0)\n    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n    >>> pred_decision  # doctest: +ELLIPSIS\n    array([-2.18...,  2.36...,  0.09...])\n    >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS\n    0.30...\n\n    \"\"\"\n    # TODO: multi-class hinge-loss\n    check_consistent_length(y_true, pred_decision)\n    y_true = column_or_1d(y_true)\n    pred_decision = column_or_1d(pred_decision)\n\n    # the rest of the code assumes that positive and negative labels\n    # are encoded as +1 and -1 respectively\n    lbin = LabelBinarizer(neg_label=-1)\n    y_true = lbin.fit_transform(y_true)[:, 0]\n\n    if len(lbin.classes_) > 2 or (pred_decision.ndim == 2\n                                  and pred_decision.shape[1] != 1):\n        raise ValueError(\"Multi-class hinge loss not supported\")\n    pred_decision = np.ravel(pred_decision)\n\n    try:\n        margin = y_true * pred_decision\n    except TypeError:\n        raise TypeError(\"pred_decision should be an array of floats.\")\n    losses = 1 - margin\n    # The hinge doesn't penalize good enough predictions.\n    losses[losses <= 0] = 0\n    return np.mean(losses)\n"
    },
    {
      "filename": "sklearn/metrics/tests/test_common.py",
      "content": "from __future__ import division, print_function\n\nimport numpy as np\nfrom functools import partial\nfrom itertools import product\n\nfrom sklearn.datasets import make_multilabel_classification\nfrom sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils.validation import check_random_state\nfrom sklearn.utils import shuffle\n\nfrom sklearn.utils.testing import assert_almost_equal\nfrom sklearn.utils.testing import assert_array_almost_equal\nfrom sklearn.utils.testing import assert_array_equal\nfrom sklearn.utils.testing import assert_equal\nfrom sklearn.utils.testing import assert_greater\nfrom sklearn.utils.testing import assert_not_equal\nfrom sklearn.utils.testing import assert_raises\nfrom sklearn.utils.testing import assert_true\nfrom sklearn.utils.testing import assert_warns\nfrom sklearn.utils.testing import ignore_warnings\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import hamming_loss\nfrom sklearn.metrics import hinge_loss\nfrom sklearn.metrics import jaccard_similarity_score\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import zero_one_loss\n\n# TODO Curve are currently not coverd by invariance test\n# from sklearn.metrics import precision_recall_curve\n# from sklearn.metrics import roc_curve\n\n\nfrom sklearn.metrics.base import _average_binary_score\n\n\n# Note toward developers about metric testing\n# -------------------------------------------\n# It is often possible to write one general test for several metrics:\n#\n#   - invariance properties, e.g. invariance to sample order\n#   - common behavior for an argument, e.g. the \"normalize\" with value True\n#     will return the mean of the metrics and with value False will return\n#     the sum of the metrics.\n#\n# In order to improve the overall metric testing, it is a good idea to write\n# first a specific test for the given metric and then add a general test for\n# all metrics that have the same behavior.\n#\n# Two types of datastructures are used in order to implement this system:\n# dictionaries of metrics and lists of metrics wit common properties.\n#\n# Dictionaries of metrics\n# ------------------------\n# The goal of having those dictionaries is to have an easy way to call a\n# particular metric and associate a name to each function:\n#\n#   - REGRESSION_METRICS: all regression metrics.\n#   - CLASSIFICATION_METRICS: all classification metrics\n#     which compare a ground truth and the estimated targets as returned by a\n#     classifier.\n#   - THRESHOLDED_METRICS: all classification metrics which\n#     compare a ground truth and a score, e.g. estimated probabilities or\n#     decision function (format might vary)\n#\n# Those dictionaries will be used to test systematically some invariance\n# properties, e.g. invariance toward several input layout.\n#\n\nREGRESSION_METRICS = {\n    \"mean_absolute_error\": mean_absolute_error,\n    \"mean_squared_error\": mean_squared_error,\n    \"explained_variance_score\": explained_variance_score,\n    \"r2_score\": r2_score,\n}\n\nCLASSIFICATION_METRICS = {\n    \"accuracy_score\": accuracy_score,\n    \"unnormalized_accuracy_score\": partial(accuracy_score, normalize=False),\n    \"confusion_matrix\": confusion_matrix,\n    \"hamming_loss\": hamming_loss,\n\n    \"jaccard_similarity_score\": jaccard_similarity_score,\n    \"unnormalized_jaccard_similarity_score\":\n    partial(jaccard_similarity_score, normalize=False),\n\n    \"zero_one_loss\": zero_one_loss,\n    \"unnormalized_zero_one_loss\": partial(zero_one_loss, normalize=False),\n\n    \"precision_score\": precision_score,\n    \"recall_score\": recall_score,\n    \"f1_score\": f1_score,\n    \"f2_score\": partial(fbeta_score, beta=2),\n    \"f0.5_score\": partial(fbeta_score, beta=0.5),\n    \"matthews_corrcoef_score\": matthews_corrcoef,\n\n    \"weighted_f0.5_score\": partial(fbeta_score, average=\"weighted\", beta=0.5),\n    \"weighted_f1_score\": partial(f1_score, average=\"weighted\"),\n    \"weighted_f2_score\": partial(fbeta_score, average=\"weighted\", beta=2),\n    \"weighted_precision_score\": partial(precision_score, average=\"weighted\"),\n    \"weighted_recall_score\": partial(recall_score, average=\"weighted\"),\n\n    \"micro_f0.5_score\": partial(fbeta_score, average=\"micro\", beta=0.5),\n    \"micro_f1_score\": partial(f1_score, average=\"micro\"),\n    \"micro_f2_score\": partial(fbeta_score, average=\"micro\", beta=2),\n    \"micro_precision_score\": partial(precision_score, average=\"micro\"),\n    \"micro_recall_score\": partial(recall_score, average=\"micro\"),\n\n    \"macro_f0.5_score\": partial(fbeta_score, average=\"macro\", beta=0.5),\n    \"macro_f1_score\": partial(f1_score, average=\"macro\"),\n    \"macro_f2_score\": partial(fbeta_score, average=\"macro\", beta=2),\n    \"macro_precision_score\": partial(precision_score, average=\"macro\"),\n    \"macro_recall_score\": partial(recall_score, average=\"macro\"),\n\n    \"samples_f0.5_score\": partial(fbeta_score, average=\"samples\", beta=0.5),\n    \"samples_f1_score\": partial(f1_score, average=\"samples\"),\n    \"samples_f2_score\": partial(fbeta_score, average=\"samples\", beta=2),\n    \"samples_precision_score\": partial(precision_score, average=\"samples\"),\n    \"samples_recall_score\": partial(recall_score, average=\"samples\"),\n}\n\nTHRESHOLDED_METRICS = {\n    \"log_loss\": log_loss,\n    \"unnormalized_log_loss\": partial(log_loss, normalize=False),\n\n    \"hinge_loss\": hinge_loss,\n\n    \"roc_auc_score\": roc_auc_score,\n    \"weighted_roc_auc\": partial(roc_auc_score, average=\"weighted\"),\n    \"samples_roc_auc\": partial(roc_auc_score, average=\"samples\"),\n    \"micro_roc_auc\": partial(roc_auc_score, average=\"micro\"),\n    \"macro_roc_auc\": partial(roc_auc_score, average=\"macro\"),\n\n    \"average_precision_score\": average_precision_score,\n    \"weighted_average_precision_score\":\n    partial(average_precision_score, average=\"weighted\"),\n    \"samples_average_precision_score\":\n    partial(average_precision_score, average=\"samples\"),\n    \"micro_average_precision_score\":\n    partial(average_precision_score, average=\"micro\"),\n    \"macro_average_precision_score\":\n    partial(average_precision_score, average=\"macro\"),\n    \"label_ranking_average_precision_score\":\n    label_ranking_average_precision_score,\n}\n\nALL_METRICS = dict()\nALL_METRICS.update(THRESHOLDED_METRICS)\nALL_METRICS.update(CLASSIFICATION_METRICS)\nALL_METRICS.update(REGRESSION_METRICS)\n\n# Lists of metrics with common properties\n# ---------------------------------------\n# Lists of metrics with common properties are used to test systematically some\n# functionalities and invariance, e.g. SYMMETRIC_METRICS lists all metrics that\n# are symmetric with respect to their input argument y_true and y_pred.\n#\n# When you add a new metric or functionality, check if a general test\n# is already written.\n\n# Metric undefined with \"binary\" or \"multiclass\" input\nMETRIC_UNDEFINED_MULTICLASS = [\n    \"samples_f0.5_score\", \"samples_f1_score\", \"samples_f2_score\",\n    \"samples_precision_score\", \"samples_recall_score\",\n\n    # Those metrics don't support multiclass outputs\n    \"average_precision_score\", \"weighted_average_precision_score\",\n    \"micro_average_precision_score\", \"macro_average_precision_score\",\n    \"samples_average_precision_score\",\n\n    \"label_ranking_average_precision_score\",\n\n    \"roc_auc_score\", \"micro_roc_auc\", \"weighted_roc_auc\",\n    \"macro_roc_auc\",  \"samples_roc_auc\",\n]\n\n# Metrics with an \"average\" argument\nMETRICS_WITH_AVERAGING = [\n    \"precision_score\", \"recall_score\", \"f1_score\", \"f2_score\", \"f0.5_score\"\n]\n\n# Treshold-based metrics with an \"average\" argument\nTHRESHOLDED_METRICS_WITH_AVERAGING = [\n    \"roc_auc_score\", \"average_precision_score\",\n]\n\n# Metrics with a \"pos_label\" argument\nMETRICS_WITH_POS_LABEL = [\n    \"roc_curve\", \"hinge_loss\",\n\n    \"precision_score\", \"recall_score\", \"f1_score\", \"f2_score\", \"f0.5_score\",\n\n    \"weighted_f0.5_score\", \"weighted_f1_score\", \"weighted_f2_score\",\n    \"weighted_precision_score\", \"weighted_recall_score\",\n\n    \"micro_f0.5_score\", \"micro_f1_score\", \"micro_f2_score\",\n    \"micro_precision_score\", \"micro_recall_score\",\n\n    \"macro_f0.5_score\", \"macro_f1_score\", \"macro_f2_score\",\n    \"macro_precision_score\", \"macro_recall_score\",\n]\n\n# Metrics with a \"labels\" argument\nMETRICS_WITH_LABELS = [\n    \"confusion_matrix\",\n\n    \"precision_score\", \"recall_score\", \"f1_score\", \"f2_score\", \"f0.5_score\",\n\n    \"weighted_f0.5_score\", \"weighted_f1_score\", \"weighted_f2_score\",\n    \"weighted_precision_score\", \"weighted_recall_score\",\n\n    \"micro_f0.5_score\", \"micro_f1_score\", \"micro_f2_score\",\n    \"micro_precision_score\", \"micro_recall_score\",\n\n    \"macro_f0.5_score\", \"macro_f1_score\", \"macro_f2_score\",\n    \"macro_precision_score\", \"macro_recall_score\",\n]\n\n# Metrics with a \"normalize\" option\nMETRICS_WITH_NORMALIZE_OPTION = [\n    \"accuracy_score\",\n    \"jaccard_similarity_score\",\n    \"zero_one_loss\",\n]\n\n# Threshold-based metrics with \"multilabel-indicator\" format support\nTHRESHOLDED_MULTILABEL_METRICS = [\n    \"log_loss\",\n    \"unnormalized_log_loss\",\n\n    \"roc_auc_score\", \"weighted_roc_auc\", \"samples_roc_auc\",\n    \"micro_roc_auc\", \"macro_roc_auc\",\n\n    \"average_precision_score\", \"weighted_average_precision_score\",\n    \"samples_average_precision_score\", \"micro_average_precision_score\",\n    \"macro_average_precision_score\",\n]\n\n# Classification metrics with  \"multilabel-indicator\" and\n# \"multilabel-sequence\" format support\nMULTILABELS_METRICS = [\n    \"accuracy_score\", \"unnormalized_accuracy_score\",\n    \"hamming_loss\",\n    \"jaccard_similarity_score\", \"unnormalized_jaccard_similarity_score\",\n    \"zero_one_loss\", \"unnormalized_zero_one_loss\",\n\n    \"precision_score\", \"recall_score\", \"f1_score\", \"f2_score\", \"f0.5_score\",\n\n    \"weighted_f0.5_score\", \"weighted_f1_score\", \"weighted_f2_score\",\n    \"weighted_precision_score\", \"weighted_recall_score\",\n\n    \"micro_f0.5_score\", \"micro_f1_score\", \"micro_f2_score\",\n    \"micro_precision_score\", \"micro_recall_score\",\n\n    \"macro_f0.5_score\", \"macro_f1_score\", \"macro_f2_score\",\n    \"macro_precision_score\", \"macro_recall_score\",\n\n    \"samples_f0.5_score\", \"samples_f1_score\", \"samples_f2_score\",\n    \"samples_precision_score\", \"samples_recall_score\",\n]\n\n# Regression metrics with \"multioutput-continuous\" format support\nMULTIOUTPUT_METRICS = [\n    \"mean_absolute_error\", \"mean_squared_error\", \"r2_score\",\n]\n\n# Symmetric with respect to their input arguments y_true and y_pred\n# metric(y_true, y_pred) == metric(y_pred, y_true).\nSYMMETRIC_METRICS = [\n    \"accuracy_score\", \"unnormalized_accuracy_score\",\n    \"hamming_loss\",\n    \"jaccard_similarity_score\", \"unnormalized_jaccard_similarity_score\",\n    \"zero_one_loss\", \"unnormalized_zero_one_loss\",\n\n    \"f1_score\", \"weighted_f1_score\", \"micro_f1_score\", \"macro_f1_score\",\n\n    \"matthews_corrcoef_score\", \"mean_absolute_error\", \"mean_squared_error\"\n\n]\n\n# Asymmetric with respect to their input arguments y_true and y_pred\n# metric(y_true, y_pred) != metric(y_pred, y_true).\nNOT_SYMMETRIC_METRICS = [\n    \"explained_variance_score\",\n    \"r2_score\",\n    \"confusion_matrix\",\n\n    \"precision_score\", \"recall_score\", \"f2_score\", \"f0.5_score\",\n\n    \"weighted_f0.5_score\", \"weighted_f2_score\", \"weighted_precision_score\",\n    \"weighted_recall_score\",\n\n    \"micro_f0.5_score\", \"micro_f2_score\", \"micro_precision_score\",\n    \"micro_recall_score\",\n\n    \"macro_f0.5_score\", \"macro_f2_score\", \"macro_precision_score\",\n    \"macro_recall_score\", \"log_loss\", \"hinge_loss\"\n]\n\n\n# No Sample weight support\nMETRICS_WITHOUT_SAMPLE_WEIGHT = [\n    \"confusion_matrix\",\n    \"hamming_loss\",\n    \"hinge_loss\",\n    \"matthews_corrcoef_score\",\n]\n\n\ndef test_symmetry():\n    \"\"\"Test the symmetry of score and loss functions\"\"\"\n    random_state = check_random_state(0)\n    y_true = random_state.randint(0, 2, size=(20, ))\n    y_pred = random_state.randint(0, 2, size=(20, ))\n\n    # We shouldn't forget any metrics\n    assert_equal(set(SYMMETRIC_METRICS).union(NOT_SYMMETRIC_METRICS,\n                                              THRESHOLDED_METRICS,\n                                              METRIC_UNDEFINED_MULTICLASS),\n                 set(ALL_METRICS))\n\n    assert_equal(\n        set(SYMMETRIC_METRICS).intersection(set(NOT_SYMMETRIC_METRICS)),\n        set([]))\n\n    # Symmetric metric\n    for name in SYMMETRIC_METRICS:\n        metric = ALL_METRICS[name]\n        assert_almost_equal(metric(y_true, y_pred),\n                            metric(y_pred, y_true),\n                            err_msg=\"%s is not symmetric\" % name)\n\n    # Not symmetric metrics\n    for name in NOT_SYMMETRIC_METRICS:\n        metric = ALL_METRICS[name]\n        assert_true(np.any(metric(y_true, y_pred) != metric(y_pred, y_true)),\n                    msg=\"%s seems to be symmetric\" % name)\n\n\ndef test_sample_order_invariance():\n    random_state = check_random_state(0)\n    y_true = random_state.randint(0, 2, size=(20, ))\n    y_pred = random_state.randint(0, 2, size=(20, ))\n    y_true_shuffle, y_pred_shuffle = shuffle(y_true, y_pred, random_state=0)\n\n    for name, metric in ALL_METRICS.items():\n        if name in METRIC_UNDEFINED_MULTICLASS:\n            continue\n\n        assert_almost_equal(metric(y_true, y_pred),\n                            metric(y_true_shuffle, y_pred_shuffle),\n                            err_msg=\"%s is not sample order invariant\"\n                                    % name)\n\n\ndef test_sample_order_invariance_multilabel_and_multioutput():\n    random_state = check_random_state(0)\n\n    # Generate some data\n    y_true = random_state.randint(0, 2, size=(20, 25))\n    y_pred = random_state.randint(0, 2, size=(20, 25))\n    y_score = random_state.normal(size=y_true.shape)\n\n    y_true_shuffle, y_pred_shuffle, y_score_shuffle = shuffle(y_true,\n                                                              y_pred,\n                                                              y_score,\n                                                              random_state=0)\n\n    for name in MULTILABELS_METRICS:\n        metric = ALL_METRICS[name]\n        assert_almost_equal(metric(y_true, y_pred),\n                            metric(y_true_shuffle, y_pred_shuffle),\n                            err_msg=\"%s is not sample order invariant\"\n                                    % name)\n\n    for name in THRESHOLDED_MULTILABEL_METRICS:\n        metric = ALL_METRICS[name]\n        assert_almost_equal(metric(y_true, y_score),\n                            metric(y_true_shuffle, y_score_shuffle),\n                            err_msg=\"%s is not sample order invariant\"\n                                    % name)\n\n    for name in MULTIOUTPUT_METRICS:\n        metric = ALL_METRICS[name]\n        assert_almost_equal(metric(y_true, y_score),\n                            metric(y_true_shuffle, y_score_shuffle),\n                            err_msg=\"%s is not sample order invariant\"\n                                    % name)\n        assert_almost_equal(metric(y_true, y_pred),\n                            metric(y_true_shuffle, y_pred_shuffle),\n                            err_msg=\"%s is not sample order invariant\"\n                                    % name)\n\n\ndef test_format_invariance_with_1d_vectors():\n    random_state = check_random_state(0)\n    y1 = random_state.randint(0, 2, size=(20, ))\n    y2 = random_state.randint(0, 2, size=(20, ))\n\n    y1_list = list(y1)\n    y2_list = list(y2)\n\n    y1_1d, y2_1d = np.array(y1), np.array(y2)\n    assert_equal(y1_1d.ndim, 1)\n    assert_equal(y2_1d.ndim, 1)\n    y1_column = np.reshape(y1_1d, (-1, 1))\n    y2_column = np.reshape(y2_1d, (-1, 1))\n    y1_row = np.reshape(y1_1d, (1, -1))\n    y2_row = np.reshape(y2_1d, (1, -1))\n\n    for name, metric in ALL_METRICS.items():\n        if name in METRIC_UNDEFINED_MULTICLASS:\n            continue\n\n        measure = metric(y1, y2)\n\n        assert_almost_equal(metric(y1_list, y2_list), measure,\n                            err_msg=\"%s is not representation invariant \"\n                                    \"with list\" % name)\n\n        assert_almost_equal(metric(y1_1d, y2_1d), measure,\n                            err_msg=\"%s is not representation invariant \"\n                                    \"with np-array-1d\" % name)\n\n        assert_almost_equal(metric(y1_column, y2_column), measure,\n                            err_msg=\"%s is not representation invariant \"\n                                    \"with np-array-column\" % name)\n\n        # Mix format support\n        assert_almost_equal(metric(y1_1d, y2_list), measure,\n                            err_msg=\"%s is not representation invariant \"\n                                    \"with mix np-array-1d and list\" % name)\n\n        assert_almost_equal(metric(y1_list, y2_1d), measure,\n                            err_msg=\"%s is not representation invariant \"\n                                    \"with mix np-array-1d and list\" % name)\n\n        assert_almost_equal(metric(y1_1d, y2_column), measure,\n                            err_msg=\"%s is not representation invariant \"\n                                    \"with mix np-array-1d and np-array-column\"\n                                    % name)\n\n        assert_almost_equal(metric(y1_column, y2_1d), measure,\n                            err_msg=\"%s is not representation invariant \"\n                                    \"with mix np-array-1d and np-array-column\"\n                                    % name)\n\n        assert_almost_equal(metric(y1_list, y2_column), measure,\n                            err_msg=\"%s is not representation invariant \"\n                                    \"with mix list and np-array-column\"\n                                    % name)\n\n        assert_almost_equal(metric(y1_column, y2_list), measure,\n                            err_msg=\"%s is not representation invariant \"\n                                    \"with mix list and np-array-column\"\n                                    % name)\n\n        # These mix representations aren't allowed\n        assert_raises(ValueError, metric, y1_1d, y2_row)\n        assert_raises(ValueError, metric, y1_row, y2_1d)\n        assert_raises(ValueError, metric, y1_list, y2_row)\n        assert_raises(ValueError, metric, y1_row, y2_list)\n        assert_raises(ValueError, metric, y1_column, y2_row)\n        assert_raises(ValueError, metric, y1_row, y2_column)\n\n        # NB: We do not test for y1_row, y2_row as these may be\n        # interpreted as multilabel or multioutput data.\n        if (name not in (MULTIOUTPUT_METRICS + THRESHOLDED_MULTILABEL_METRICS +\n                         MULTILABELS_METRICS)):\n            assert_raises(ValueError, metric, y1_row, y2_row)\n\n\ndef test_invariance_string_vs_numbers_labels():\n    \"\"\"Ensure that classification metrics with string labels\"\"\"\n    random_state = check_random_state(0)\n    y1 = random_state.randint(0, 2, size=(20, ))\n    y2 = random_state.randint(0, 2, size=(20, ))\n\n    y1_str = np.array([\"eggs\", \"spam\"])[y1]\n    y2_str = np.array([\"eggs\", \"spam\"])[y2]\n\n    pos_label_str = \"spam\"\n    labels_str = [\"eggs\", \"spam\"]\n\n    for name, metric in CLASSIFICATION_METRICS.items():\n        if name in METRIC_UNDEFINED_MULTICLASS:\n            continue\n\n        measure_with_number = metric(y1, y2)\n\n        # Ugly, but handle case with a pos_label and label\n        metric_str = metric\n        if name in METRICS_WITH_POS_LABEL:\n            metric_str = partial(metric_str, pos_label=pos_label_str)\n\n        measure_with_str = metric_str(y1_str, y2_str)\n\n        assert_array_equal(measure_with_number, measure_with_str,\n                           err_msg=\"{0} failed string vs number invariance \"\n                                   \"test\".format(name))\n\n        measure_with_strobj = metric_str(y1_str.astype('O'),\n                                         y2_str.astype('O'))\n        assert_array_equal(measure_with_number, measure_with_strobj,\n                           err_msg=\"{0} failed string object vs number \"\n                                   \"invariance test\".format(name))\n\n        if name in METRICS_WITH_LABELS:\n            metric_str = partial(metric_str, labels=labels_str)\n            measure_with_str = metric_str(y1_str, y2_str)\n            assert_array_equal(measure_with_number, measure_with_str,\n                               err_msg=\"{0} failed string vs number  \"\n                                       \"invariance test\".format(name))\n\n            measure_with_strobj = metric_str(y1_str.astype('O'),\n                                             y2_str.astype('O'))\n            assert_array_equal(measure_with_number, measure_with_strobj,\n                               err_msg=\"{0} failed string vs number  \"\n                                       \"invariance test\".format(name))\n\n    for name, metric in THRESHOLDED_METRICS.items():\n        if name in (\"log_loss\", \"hinge_loss\", \"unnormalized_log_loss\"):\n            measure_with_number = metric(y1, y2)\n            measure_with_str = metric(y1_str, y2)\n            assert_array_equal(measure_with_number, measure_with_str,\n                               err_msg=\"{0} failed string vs number \"\n                                       \"invariance test\".format(name))\n\n            measure_with_strobj = metric(y1_str.astype('O'), y2)\n            assert_array_equal(measure_with_number, measure_with_strobj,\n                               err_msg=\"{0} failed string object vs number \"\n                                       \"invariance test\".format(name))\n        else:\n            # TODO those metrics doesn't support string label yet\n            assert_raises(ValueError, metric, y1_str, y2)\n            assert_raises(ValueError, metric, y1_str.astype('O'), y2)\n\n\n@ignore_warnings\ndef check_single_sample(name):\n    \"\"\"Non-regression test: scores should work with a single sample.\n\n    This is important for leave-one-out cross validation.\n    Score functions tested are those that formerly called np.squeeze,\n    which turns an array of size 1 into a 0-d array (!).\n    \"\"\"\n    metric = ALL_METRICS[name]\n\n    # assert that no exception is thrown\n    for i, j in product([0, 1], repeat=2):\n        metric([i], [j])\n\n\n@ignore_warnings\ndef check_single_sample_multioutput(name):\n    metric = ALL_METRICS[name]\n    for i, j, k, l in product([0, 1], repeat=4):\n        metric(np.array([[i, j]]), np.array([[k, l]]))\n\n\ndef test_single_sample():\n    for name in ALL_METRICS:\n        if name in METRIC_UNDEFINED_MULTICLASS or name in THRESHOLDED_METRICS:\n            # Those metrics are not always defined with one sample\n            # or in multiclass classification\n            continue\n\n        yield check_single_sample, name\n\n    for name in MULTIOUTPUT_METRICS + MULTILABELS_METRICS:\n        yield check_single_sample_multioutput, name\n\n\ndef test_multioutput_number_of_output_differ():\n    y_true = np.array([[1, 0, 0, 1], [0, 1, 1, 1], [1, 1, 0, 1]])\n    y_pred = np.array([[0, 0], [1, 0], [0, 0]])\n\n    for name in MULTIOUTPUT_METRICS:\n        metric = ALL_METRICS[name]\n        assert_raises(ValueError, metric, y_true, y_pred)\n\n\ndef test_multioutput_regression_invariance_to_dimension_shuffling():\n    # test invariance to dimension shuffling\n    random_state = check_random_state(0)\n    y_true = random_state.uniform(0, 2, size=(20, 5))\n    y_pred = random_state.uniform(0, 2, size=(20, 5))\n\n    for name in MULTIOUTPUT_METRICS:\n        metric = ALL_METRICS[name]\n        error = metric(y_true, y_pred)\n\n        for _ in range(3):\n            perm = random_state.permutation(y_true.shape[1])\n            assert_almost_equal(metric(y_true[:, perm], y_pred[:, perm]),\n                                error,\n                                err_msg=\"%s is not dimension shuffling \"\n                                        \"invariant\" % name)\n\n\ndef test_multilabel_representation_invariance():\n\n    # Generate some data\n    n_classes = 4\n    n_samples = 50\n    # using sequence of sequences is deprecated, but still tested\n    make_ml = ignore_warnings(make_multilabel_classification)\n    _, y1 = make_ml(n_features=1, n_classes=n_classes, random_state=0,\n                    n_samples=n_samples)\n    _, y2 = make_ml(n_features=1, n_classes=n_classes, random_state=1,\n                    n_samples=n_samples)\n\n    # Be sure to have at least one empty label\n    y1 += ([], )\n    y2 += ([], )\n\n    # NOTE: The \"sorted\" trick is necessary to shuffle labels, because it\n    # allows to return the shuffled tuple.\n    rng = check_random_state(42)\n    shuffled = lambda x: sorted(x, key=lambda *args: rng.rand())\n    y1_shuffle = [shuffled(x) for x in y1]\n    y2_shuffle = [shuffled(x) for x in y2]\n\n    # Let's have redundant labels\n    y1_redundant = [x * rng.randint(1, 4) for x in y1]\n    y2_redundant = [x * rng.randint(1, 4) for x in y2]\n\n    # Binary indicator matrix format\n    lb = MultiLabelBinarizer().fit([range(n_classes)])\n    y1_binary_indicator = lb.transform(y1)\n    y2_binary_indicator = lb.transform(y2)\n\n    y1_shuffle_binary_indicator = lb.transform(y1_shuffle)\n    y2_shuffle_binary_indicator = lb.transform(y2_shuffle)\n\n    for name in MULTILABELS_METRICS:\n        metric = ALL_METRICS[name]\n\n        # XXX cruel hack to work with partial functions\n        if isinstance(metric, partial):\n            metric.__module__ = 'tmp'\n            metric.__name__ = name\n        # Check warning for sequence of sequences\n        measure = assert_warns(DeprecationWarning, metric, y1, y2)\n        metric = ignore_warnings(metric)\n\n        # Check representation invariance\n        assert_almost_equal(metric(y1_binary_indicator,\n                                   y2_binary_indicator),\n                            measure,\n                            err_msg=\"%s failed representation invariance  \"\n                                    \"between list of list of labels \"\n                                    \"format and dense binary indicator \"\n                                    \"format.\" % name)\n\n        with ignore_warnings():  # sequence of sequences is deprecated\n            # Check invariance with redundant labels with list of labels\n            assert_almost_equal(metric(y1, y2_redundant), measure,\n                                err_msg=\"%s failed rendundant label invariance\"\n                                        % name)\n\n            assert_almost_equal(metric(y1_redundant, y2_redundant), measure,\n                                err_msg=\"%s failed rendundant label invariance\"\n                                        % name)\n\n            assert_almost_equal(metric(y1_redundant, y2), measure,\n                                err_msg=\"%s failed rendundant label invariance\"\n                                        % name)\n\n            # Check shuffling invariance with list of labels\n            assert_almost_equal(metric(y1_shuffle, y2_shuffle), measure,\n                                err_msg=\"%s failed shuffling invariance \"\n                                        \"with list of list of labels format.\"\n                                        % name)\n\n        # Check shuffling invariance with dense binary indicator matrix\n        assert_almost_equal(metric(y1_shuffle_binary_indicator,\n                                   y2_shuffle_binary_indicator), measure,\n                            err_msg=\"%s failed shuffling invariance \"\n                                    \" with dense binary indicator format.\"\n                                    % name)\n\n        with ignore_warnings():  # sequence of sequences is deprecated\n            # Check raises error with mix input representation\n            assert_raises(ValueError, metric, y1, y2_binary_indicator)\n            assert_raises(ValueError, metric, y1_binary_indicator, y2)\n\n\ndef test_normalize_option_binary_classification(n_samples=20):\n    # Test in the binary case\n    random_state = check_random_state(0)\n    y_true = random_state.randint(0, 2, size=(n_samples, ))\n    y_pred = random_state.randint(0, 2, size=(n_samples, ))\n\n    for name in METRICS_WITH_NORMALIZE_OPTION:\n        metrics = ALL_METRICS[name]\n        measure = metrics(y_true, y_pred, normalize=True)\n        assert_greater(measure, 0,\n                       msg=\"We failed to test correctly the normalize option\")\n        assert_almost_equal(metrics(y_true, y_pred, normalize=False)\n                            / n_samples, measure)\n\n\ndef test_normalize_option_multiclasss_classification():\n    # Test in the multiclass case\n    random_state = check_random_state(0)\n    y_true = random_state.randint(0, 4, size=(20, ))\n    y_pred = random_state.randint(0, 4, size=(20, ))\n    n_samples = y_true.shape[0]\n\n    for name in METRICS_WITH_NORMALIZE_OPTION:\n        metrics = ALL_METRICS[name]\n        measure = metrics(y_true, y_pred, normalize=True)\n        assert_greater(measure, 0,\n                       msg=\"We failed to test correctly the normalize option\")\n        assert_almost_equal(metrics(y_true, y_pred, normalize=False)\n                            / n_samples, measure)\n\n\ndef test_normalize_option_multilabel_classification():\n    # Test in the multilabel case\n    n_classes = 4\n    n_samples = 100\n    # using sequence of sequences is deprecated, but still tested\n    make_ml = ignore_warnings(make_multilabel_classification)\n    _, y_true = make_ml(n_features=1, n_classes=n_classes,\n                        random_state=0, n_samples=n_samples)\n    _, y_pred = make_ml(n_features=1, n_classes=n_classes,\n                        random_state=1, n_samples=n_samples)\n\n    # Be sure to have at least one empty label\n    y_true += ([], )\n    y_pred += ([], )\n    n_samples += 1\n\n    lb = MultiLabelBinarizer().fit([range(n_classes)])\n    y_true_binary_indicator = lb.transform(y_true)\n    y_pred_binary_indicator = lb.transform(y_pred)\n\n    for name in METRICS_WITH_NORMALIZE_OPTION:\n        metrics = ALL_METRICS[name]\n\n        # List of list of labels\n        measure = assert_warns(DeprecationWarning, metrics, y_true, y_pred,\n                               normalize=True)\n        assert_greater(measure, 0,\n                       msg=\"We failed to test correctly the normalize option\")\n        assert_almost_equal(ignore_warnings(metrics)(y_true, y_pred,\n                                                     normalize=False)\n                            / n_samples, measure,\n                            err_msg=\"Failed with %s\" % name)\n\n        # Indicator matrix format\n        measure = metrics(y_true_binary_indicator,\n                          y_pred_binary_indicator, normalize=True)\n        assert_greater(measure, 0,\n                       msg=\"We failed to test correctly the normalize option\")\n        assert_almost_equal(metrics(y_true_binary_indicator,\n                                    y_pred_binary_indicator, normalize=False)\n                            / n_samples, measure,\n                            err_msg=\"Failed with %s\" % name)\n\n\n@ignore_warnings\ndef _check_averaging(metric, y_true, y_pred, y_true_binarize, y_pred_binarize,\n                     is_multilabel):\n    n_samples, n_classes = y_true_binarize.shape\n\n    # No averaging\n    label_measure = metric(y_true, y_pred, average=None)\n    assert_array_almost_equal(label_measure,\n                              [metric(y_true_binarize[:, i],\n                                      y_pred_binarize[:, i])\n                               for i in range(n_classes)])\n\n    # Micro measure\n    micro_measure = metric(y_true, y_pred, average=\"micro\")\n    assert_almost_equal(micro_measure, metric(y_true_binarize.ravel(),\n                                              y_pred_binarize.ravel()))\n\n    # Macro measure\n    macro_measure = metric(y_true, y_pred, average=\"macro\")\n    assert_almost_equal(macro_measure, np.mean(label_measure))\n\n    # Weighted measure\n    weights = np.sum(y_true_binarize, axis=0, dtype=int)\n\n    if np.sum(weights) != 0:\n        weighted_measure = metric(y_true, y_pred, average=\"weighted\")\n        assert_almost_equal(weighted_measure, np.average(label_measure,\n                                                         weights=weights))\n    else:\n        weighted_measure = metric(y_true, y_pred, average=\"weighted\")\n        assert_almost_equal(weighted_measure, 0)\n\n    # Sample measure\n    if is_multilabel:\n        sample_measure = metric(y_true, y_pred, average=\"samples\")\n        assert_almost_equal(sample_measure,\n                            np.mean([metric(y_true_binarize[i],\n                                            y_pred_binarize[i])\n                                     for i in range(n_samples)]))\n\n    assert_raises(ValueError, metric, y_true, y_pred, average=\"unknown\")\n    assert_raises(ValueError, metric, y_true, y_pred, average=\"garbage\")\n\n\ndef check_averaging(name, y_true, y_true_binarize, y_pred, y_pred_binarize,\n                    y_score):\n    is_multilabel = type_of_target(y_true).startswith(\"multilabel\")\n\n    metric = ALL_METRICS[name]\n\n    if name in METRICS_WITH_AVERAGING:\n        _check_averaging(metric, y_true, y_pred, y_true_binarize,\n                         y_pred_binarize, is_multilabel)\n    elif name in THRESHOLDED_METRICS_WITH_AVERAGING:\n        _check_averaging(metric, y_true, y_score, y_true_binarize,\n                         y_score, is_multilabel)\n    else:\n        raise ValueError(\"Metric is not recorded as having an average option\")\n\n\ndef test_averaging_multiclass(n_samples=50, n_classes=3):\n    random_state = check_random_state(0)\n    y_true = random_state.randint(0, n_classes, size=(n_samples, ))\n    y_pred = random_state.randint(0, n_classes, size=(n_samples, ))\n    y_score = random_state.uniform(size=(n_samples, n_classes))\n\n    lb = LabelBinarizer().fit(y_true)\n    y_true_binarize = lb.transform(y_true)\n    y_pred_binarize = lb.transform(y_pred)\n\n    for name in METRICS_WITH_AVERAGING:\n        yield (check_averaging, name, y_true, y_true_binarize, y_pred,\n               y_pred_binarize, y_score)\n\n\ndef test_averaging_multilabel(n_classes=5, n_samples=40):\n    _, y = make_multilabel_classification(n_features=1, n_classes=n_classes,\n                                          random_state=5, n_samples=n_samples,\n                                          return_indicator=True,\n                                          allow_unlabeled=False)\n    y_true = y[:20]\n    y_pred = y[20:]\n    y_score = check_random_state(0).normal(size=(20, n_classes))\n    y_true_binarize = y_true\n    y_pred_binarize = y_pred\n\n    for name in METRICS_WITH_AVERAGING + THRESHOLDED_METRICS_WITH_AVERAGING:\n        yield (check_averaging, name, y_true, y_true_binarize, y_pred,\n               y_pred_binarize, y_score)\n\n\ndef test_averaging_multilabel_all_zeroes():\n    y_true = np.zeros((20, 3))\n    y_pred = np.zeros((20, 3))\n    y_score = np.zeros((20, 3))\n    y_true_binarize = y_true\n    y_pred_binarize = y_pred\n\n    for name in METRICS_WITH_AVERAGING:\n        yield (check_averaging, name, y_true, y_true_binarize, y_pred,\n               y_pred_binarize, y_score)\n\n    # Test _average_binary_score for weight.sum() == 0\n    binary_metric = (lambda y_true, y_score, average=\"macro\":\n                     _average_binary_score(\n                         precision_score, y_true, y_score, average))\n    _check_averaging(binary_metric, y_true, y_pred, y_true_binarize,\n                     y_pred_binarize, is_multilabel=True)\n\n\ndef test_averaging_multilabel_all_ones():\n    y_true = np.ones((20, 3))\n    y_pred = np.ones((20, 3))\n    y_score = np.ones((20, 3))\n    y_true_binarize = y_true\n    y_pred_binarize = y_pred\n\n    for name in METRICS_WITH_AVERAGING:\n        yield (check_averaging, name, y_true, y_true_binarize, y_pred,\n               y_pred_binarize, y_score)\n\n\n@ignore_warnings\ndef check_sample_weight_invariance(name, metric, y1, y2):\n    rng = np.random.RandomState(0)\n    sample_weight = rng.randint(1, 10, size=len(y1))\n\n    # check that unit weights gives the same score as no weight\n    unweighted_score = metric(y1, y2, sample_weight=None)\n    assert_almost_equal(\n        unweighted_score,\n        metric(y1, y2, sample_weight=np.ones(shape=len(y1))),\n        err_msg=\"For %s sample_weight=None is not equivalent to \"\n            \"sample_weight=ones\" % name)\n\n    # check that the weighted and unweighted scores are unequal\n    weighted_score = metric(y1, y2, sample_weight=sample_weight)\n    assert_not_equal(\n        unweighted_score, weighted_score,\n        msg=\"Unweighted and weighted scores are unexpectedly \"\n            \"equal (%f) for %s\" % (weighted_score, name))\n\n    # check that sample_weight can be a list\n    weighted_score_list = metric(y1, y2,\n                                 sample_weight=sample_weight.tolist())\n    assert_almost_equal(\n        weighted_score, weighted_score_list,\n        err_msg=\"Weighted scores for array and list sample_weight input are \"\n            \"not equal (%f != %f) for %s\" % (\n                weighted_score, weighted_score_list, name))\n\n    # check that integer weights is the same as repeated samples\n    repeat_weighted_score = metric(\n        np.repeat(y1, sample_weight, axis=0),\n        np.repeat(y2, sample_weight, axis=0), sample_weight=None)\n    assert_almost_equal(\n        weighted_score, repeat_weighted_score,\n        err_msg=\"Weighting %s is not equal to repeating samples\" % name)\n\n    # check that ignoring a fraction of the samples is equivalent to setting\n    # the corresponding weights to zero\n    sample_weight_subset = sample_weight[1::2]\n    sample_weight_zeroed = np.copy(sample_weight)\n    sample_weight_zeroed[::2] = 0\n    y1_subset = y1[1::2]\n    y2_subset = y2[1::2]\n    weighted_score_subset = metric(y1_subset, y2_subset,\n                                   sample_weight=sample_weight_subset)\n    weighted_score_zeroed = metric(y1, y2,\n                                   sample_weight=sample_weight_zeroed)\n    assert_almost_equal(\n        weighted_score_subset, weighted_score_zeroed,\n        err_msg=(\"Zeroing weights does not give the same result as \"\n                 \"removing the corresponding samples (%f != %f) for %s\" %\n                 (weighted_score_zeroed, weighted_score_subset, name)))\n\n    if not name.startswith('unnormalized'):\n        # check that the score is invariant under scaling of the weights by a\n        # common factor\n        for scaling in [2, 0.3]:\n            assert_almost_equal(\n                weighted_score,\n                metric(y1, y2, sample_weight=sample_weight * scaling),\n                err_msg=\"%s sample_weight is not invariant \"\n                        \"under scaling\" % name)\n\n\ndef test_sample_weight_invariance(n_samples=50):\n    random_state = check_random_state(0)\n\n    # binary output\n    random_state = check_random_state(0)\n    y_true = random_state.randint(0, 2, size=(n_samples, ))\n    y_pred = random_state.randint(0, 2, size=(n_samples, ))\n    y_score = random_state.random_sample(size=(n_samples,))\n    for name in ALL_METRICS:\n        if (name in METRICS_WITHOUT_SAMPLE_WEIGHT or\n                name in METRIC_UNDEFINED_MULTICLASS):\n            continue\n        metric = ALL_METRICS[name]\n        if name in THRESHOLDED_METRICS:\n            yield check_sample_weight_invariance, name, metric, y_true, y_score\n        else:\n            yield check_sample_weight_invariance, name, metric, y_true, y_pred\n\n    # multiclass\n    random_state = check_random_state(0)\n    y_true = random_state.randint(0, 5, size=(n_samples, ))\n    y_pred = random_state.randint(0, 5, size=(n_samples, ))\n    y_score = random_state.random_sample(size=(n_samples, 5))\n    for name in ALL_METRICS:\n        if (name in METRICS_WITHOUT_SAMPLE_WEIGHT or\n                name in METRIC_UNDEFINED_MULTICLASS):\n            continue\n        metric = ALL_METRICS[name]\n        if name in THRESHOLDED_METRICS:\n            yield check_sample_weight_invariance, name, metric, y_true, y_score\n        else:\n            yield check_sample_weight_invariance, name, metric, y_true, y_pred\n\n    # multilabel sequence\n    y_true = 2 * [(1, 2, ), (1, ), (0, ), (0, 1), (1, 2)]\n    y_pred = 2 * [(0, 2, ), (2, ), (0, ), (2, ), (1,)]\n    y_score = random_state.randn(10, 3)\n\n    for name in MULTILABELS_METRICS:\n        if name in METRICS_WITHOUT_SAMPLE_WEIGHT:\n            continue\n        metric = ALL_METRICS[name]\n\n        if name in THRESHOLDED_METRICS:\n            yield (check_sample_weight_invariance, name, metric, y_true,\n                   y_score)\n        else:\n            yield (check_sample_weight_invariance, name, metric, y_true,\n                   y_pred)\n\n    # multilabel indicator\n    _, ya = make_multilabel_classification(\n        n_features=1, n_classes=20,\n        random_state=0, n_samples=100,\n        return_indicator=True, allow_unlabeled=False)\n    _, yb = make_multilabel_classification(\n        n_features=1, n_classes=20,\n        random_state=1, n_samples=100,\n        return_indicator=True, allow_unlabeled=False)\n    y_true = np.vstack([ya, yb])\n    y_pred = np.vstack([ya, ya])\n    y_score = random_state.randint(1, 4, size=y_true.shape)\n\n    for name in (MULTILABELS_METRICS + THRESHOLDED_MULTILABEL_METRICS +\n                 MULTIOUTPUT_METRICS):\n        if name in METRICS_WITHOUT_SAMPLE_WEIGHT:\n            continue\n\n        metric = ALL_METRICS[name]\n        if name in THRESHOLDED_METRICS:\n            yield (check_sample_weight_invariance, name, metric, y_true,\n                   y_score)\n        else:\n            yield (check_sample_weight_invariance, name, metric, y_true,\n                   y_pred)\n"
    }
  ],
  "questions": [
    "For matthews_corrcoef_score, I need to replace numpy.corrcoef with a weighted correlation function. Since numpy does not have a weighted version, I am planning to place the function in utils/wcorrcoef.py and related unit tests in utils/tests/test_wcorrcoef.py.\n\nDoes that work? Is there a better way to organize the code?\n\n```\ndef wcorrcoef(X, Y, w):\n    mX = np.average(X, weights=w)\n    mY = np.average(Y, weights=w)\n    covXY = np.average((X-mX)*(Y-mY), weights=w)\n    covXX = np.average((X-mX)*(X-mX), weights=w)\n    covYY = np.average((Y-mY)*(Y-mY), weights=w)\n    return covXY/np.sqrt(covXX * covYY)\n```",
    "IMO keep it locally in metrics, unless you get a patch supporting sample\nweights accepted to numpy or scipy and instead it goes in\nsklearn.utils.fixes. Then again, I don't know how complicated the\nimplementation is.\n\nOn 6 August 2014 04:10, Jatin Shah notifications@github.com wrote:\n\n>  For matthews_corrcoef_score, I need to replace numpy.corrcoef with a\n> weight correlation function. Since numpy does not have a weighted version,\n> I am planning to place the function in utils/wcorrcoef.py and related unit\n> tests in utils/tests/test_wcorrcoef.py.\n> \n> Does that work? Is there a better way to organize the code?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-51236631\n> .",
    "I am working on this issue, and currently focusing on the `hamming_loss`.\nAt first I thought it might not make sense for this metric to be _weighted_. Now I think it would be usable to be able weight specific transitions in the hamming distance:\n\nIf we define normal hamming distance as:\n\n\\sum_t \\delta(y_t, z_t)\n\nwe can add a transition weight, `w` as follows:\n\n\\sum weight(y_t, z_t) \\delta(y_t, z_t)\n\nIt would be nicer to add it to the\nhttp://docs.scipy.org/doc/scipy-0.14.0/reference/spatial.distance.html\n\nas `whamming(u,v,w)`.\n\nand then add it to the `hamming_loss` here.\nWhat do you think?\n\nsee example of this being used in\nhttp://aclweb.org/anthology/N/N13/N13-1102.pdf",
    "I'd like to begin contributing to scikit-learn, and I'm interested in adding sample weights to the confusion matrix, however I'd like to clarify how sample weights are applied here. Is it done by simply apllying the weight to each sample's corresponding cell (sample `i` with weight `wi` is a true positive. So I add `wi` to the TP cell instead of adding `1`), or is there anything else that should be done?",
    "Thanks @jnothman! I have finished changing the code, however the tests in `test_common.py` are failing because they expect the result from the metric to be a float, not an array (as is the case for the confusion matrix). How should I proceed in this case? Should I write tests exclusively for confusion matrix, or is there something else we can do?",
    "I think confusion matrix is currently excluded from other invariance tests.\nYes, it might require its own explicit testing, or there may be a cunning\nway to fix up the existing test.\n\nOn 25 December 2014 at 04:56, Bernardo Vecchia Stein <\nnotifications@github.com> wrote:\n\n>  Thanks @jnothman https://github.com/jnothman! I have finished changing\n> the code, however the tests in test_common.py are failing because they\n> expect the result from the metric to be a float, not an array (as is the\n> case for the confusion matrix). How should I proceed in this case? Should I\n> write tests exclusively for confusion matrix, or is there something else we\n> can do?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-68066901\n> ."
  ],
  "golden_answers": [
    "IMO keep it locally in metrics, unless you get a patch supporting sample\nweights accepted to numpy or scipy and instead it goes in\nsklearn.utils.fixes. Then again, I don't know how complicated the\nimplementation is.\n\nOn 6 August 2014 04:10, Jatin Shah notifications@github.com wrote:\n\n>  For matthews_corrcoef_score, I need to replace numpy.corrcoef with a\n> weight correlation function. Since numpy does not have a weighted version,\n> I am planning to place the function in utils/wcorrcoef.py and related unit\n> tests in utils/tests/test_wcorrcoef.py.\n> \n> Does that work? Is there a better way to organize the code?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-51236631\n> .",
    "I am trying to add `sample_weight` to `matthews_corrcoef_score` and I noticed some strange stuff. `roc_auc_score` and `average_precision_score` are not tested in `tests_common.py` for binary inputs. They are added to `METRIC_UNDEFINED_MULTICLASS` and these functions are skipped in both binary and multi class unit tests (see [here](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/tests/test_common.py#L975)).\n\nSecond, I am unable to change `average` and `sample_weight` parameters for both `roc_auc_score` and `average_precision_score`.\n\n```\n>>> from sklearn.metrics import roc_auc_score\n>>> import bumpy as np\n>>> y_true = np.array([0, 0, 1, 1])\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> roc_auc_score(y_true, y_scores, average='micro')\nTypeError: roc_auc_score() got an unexpected keyword argument 'average'\n```",
    "If I understand correctly, what you suggest is to weight differently each dimension (here label) differently. The issue is about sample-wise weight.\n\nFor instance, \n\n```\n>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n0.75\n```\n\nWhile with `sample_weight=[0.5, 1]`, it should give\n\n```\n>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)), sample_weight=[0.5, 1])\n0.625 # (1 / 2 * 0.5 + 2 / 2 * 1) / 2\n```",
    "Thanks @jnothman! I have finished changing the code, however the tests in `test_common.py` are failing because they expect the result from the metric to be a float, not an array (as is the case for the confusion matrix). How should I proceed in this case? Should I write tests exclusively for confusion matrix, or is there something else we can do?",
    "I think confusion matrix is currently excluded from other invariance tests.\nYes, it might require its own explicit testing, or there may be a cunning\nway to fix up the existing test.\n\nOn 25 December 2014 at 04:56, Bernardo Vecchia Stein <\nnotifications@github.com> wrote:\n\n>  Thanks @jnothman https://github.com/jnothman! I have finished changing\n> the code, however the tests in test_common.py are failing because they\n> expect the result from the metric to be a float, not an array (as is the\n> case for the confusion matrix). How should I proceed in this case? Should I\n> write tests exclusively for confusion matrix, or is there something else we\n> can do?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-68066901\n> .",
    "@jatinshah Do you plan to go ahead with adding sample weight support to the `matthews_corrcoef_score`? If not would you mind if I proceed with [your implementation](https://github.com/scikit-learn/scikit-learn/issues/3450#issuecomment-51236631) to submit a PR for the same?"
  ],
  "questions_generated": [
    "What is the main task described in the scikit-learn issue regarding sample weight support?",
    "Why is it suggested to implement sample weight support for one metric per pull request?",
    "In the context of the issue, what challenge was encountered with the log_loss metric and how was it addressed?",
    "What implementation detail is discussed regarding the matthews_corrcoef_score metric and what solution was proposed?",
    "How does the issue propose handling multilabel metrics in relation to sample weight support?",
    "What is the role of the test_common.py file in the context of this issue?",
    "What considerations are made when deciding where to place the custom weighted correlation function for matthews_corrcoef_score?",
    "What is the significance of the functions named *_score and *_error or *_loss in the classification.py file?",
    "What changes were made to the test cases for the log_loss metric to incorporate sample weight support?"
  ],
  "golden_answers_generated": [
    "The main task is to add sample weight support to more metrics in the scikit-learn library. Specifically, the issue mentions that sample weight support needs to be added to metrics like median_absolute_error, as most other metrics already support it.",
    "It is suggested to implement sample weight support for one metric per pull request to ease the review process. This approach allows for more focused and manageable reviews, reducing the complexity and potential for errors in each individual pull request.",
    "The challenge encountered with the log_loss metric was that its tests were failing because the tests assumed both y_true and y_pred were labels, whereas log_loss expects y_true as labels and y_pred as probabilities. This issue was addressed by modifying the tests to account for this new case.",
    "For the matthews_corrcoef_score metric, there was a need to replace numpy.corrcoef with a weighted correlation function since numpy does not have a weighted version. The proposed solution was to create a custom function called wcorrcoef to handle this, placing it either locally in metrics or in a utility file if accepted by numpy or scipy.",
    "The issue suggests waiting for the completion of another task, identified as #3395, before handling multilabel metrics with sample weight support. This is to ensure that any changes align with ongoing work on related features.",
    "The test_common.py file contains a general test for sample_weight. This implies that any metric added with sample weight support should be compatible with the existing tests in this file, ensuring consistent testing across different metrics.",
    "Considerations for placing the custom weighted correlation function include keeping it locally in the metrics module unless it gets accepted as a patch for sample weights in numpy or scipy, in which case it should go in sklearn.utils.fixes. The complexity of the implementation also plays a role in this decision.",
    "In the classification.py file, functions named *_score are designed to return a scalar value that should be maximized, indicating better performance. Conversely, functions named *_error or *_loss return a scalar value that should be minimized, indicating better performance as well.",
    "The test cases for the log_loss metric were enhanced or modified to consider the scenario where y_true is labels and y_pred is probabilities, as opposed to both being labels. This adjustment was necessary to properly test the metric with sample weight support."
  ]
}