{
  "repo_name": "scikit-learn_scikit-learn",
  "issue_id": "15426",
  "issue_description": "# Label docstrings with versionadded / versionchanged\n\nWe should be using Sphinx's versionadded and versionchanged directives to indicate when classes, functions, methods and parameters are added or modified in their semantics. We are not very good at ensuring this labelling is done.\r\n\r\nTowards the upcoming release of v0.22, it would be useful if some contributors (or sprinters) scoured the change log and identified whether any added/changed parameters/classes needed a `versionadded` or `versionchanged` label, and to produce pull requests adding them when otherwise omitted.\r\n\r\nI also note past omissions, for example KBinsDiscretizer and IterativeImputer do not mention their versionadded, while ColumnTransformer does. Recording for each estimator when it was first released would be helpful for users when looking at the documentation.\r\n\r\n### Suggested procedure\r\n\r\nCheck the changelog in doc/whats_new/ one what's a new file at a time.\r\n**Make a single PR for an estimator and a specific version. Do not include multiple estimators and multiple versions at once (it makes it difficult to review).\r\n\r\nFocus on Public API: if a method has been made private, check the tickbox and move to another.\r\n\r\n```\r\n.. versionadded:: 0.xx\r\n```\r\n\r\nshould be included only for new estimators/parameters \r\n\r\n```\r\n.. versionchanged:: 0.xx\r\n   `param_xxx` change from 'xx' to 'yy'\r\n```\r\nshould be included when the default value of a parameter change.\r\n\r\n### Classes lacking label\r\nThe list may be incomplete.\r\n\r\n- [ ] ARDRegression PRs #15482 #15973 #16015\r\n- [x] AdaBoostClassifier\r\n- [x] AdaBoostRegressor\r\n- [ ] AdamOptimizer\r\n- [ ] AdditiveChi2Sampler\r\n- [ ] AffinityPropagation\r\n- [ ] AgglomerationTransform\r\n- [ ] BaggingRegressor\r\n- [ ] BaseBagging\r\n- [ ] BaseCrossValidator\r\n- [ ] BaseDecisionTree\r\n- [ ] BaseDiscreteNB\r\n- [ ] BaseEnsemble\r\n- [ ] BaseForest\r\n- [ ] BaseGradientBoosting\r\n- [ ] BaseHistGradientBoosting\r\n- [ ] BaseLabelPropagation\r\n- [ ] BaseLibSVM\r\n- [ ] BaseLoss\r\n- [ ] BaseMixture\r\n- [ ] BaseNB\r\n- [ ] BaseRandomProjection\r\n- [ ] BaseSGD\r\n- [ ] BaseSVC\r\n- [ ] BaseSearchCV\r\n- [ ] BaseShuffleSplit\r\n- [ ] BaseSpectral\r\n- [ ] BaseWeightBoosting\r\n- [ ] BernoulliNB\r\n- [ ] BernoulliRBM\r\n- [ ] Binarizer\r\n- [ ] BinaryCrossEntropy\r\n- [ ] BinomialDeviance\r\n- [ ] BinomialDeviance\r\n- [ ] Birch\r\n- [ ] Bunch\r\n- [ ] CCA\r\n- [ ] CalibratedClassifierCV\r\n- [ ] CategoricalCrossEntropy\r\n- [ ] CategoricalNB\r\n- [ ] ChangedBehaviorWarning\r\n- [ ] CheckingClassifier\r\n- [ ] ClassificationLossFunction\r\n- [ ] ClassifierChain\r\n- [ ] ComplementNB\r\n- [ ] ConvergenceWarning\r\n- [ ] CountVectorizer\r\n- [ ] DataConversionWarning\r\n- [ ] DataDimensionalityWarning\r\n- [ ] DictVectorizer\r\n- [ ] DummyRegressor\r\n- [ ] ElasticNet\r\n- [ ] ElasticNetCV\r\n- [ ] EllipticEnvelope\r\n- [ ] EmpiricalCovariance\r\n- [ ] ExponentialLoss\r\n- [ ] ExponentialLoss\r\n- [ ] FactorAnalysis\r\n- [ ] FastICA\r\n- [ ] FeatureHasher\r\n- [ ] FeatureUnion\r\n- [ ] FitFailedWarning\r\n- [ ] ForestClassifier\r\n- [ ] ForestRegressor\r\n- [ ] GaussianNB\r\n- [ ] GaussianRandomProjection\r\n- [ ] GenericUnivariateSelect\r\n- [ ] GraphicalLasso\r\n- [ ] GraphicalLassoCV\r\n- [ ] GridSearchCV\r\n- [ ] GroupKFold\r\n- [ ] HistGradientBoostingClassifier\r\n- [ ] HistGradientBoostingRegressor\r\n- [ ] HuberLossFunction\r\n- [ ] IncrementalPCA\r\n- [ ] IsotonicRegression\r\n- [ ] IterativeImputer Open [PR](https://github.com/scikit-learn/scikit-learn/pull/15549)\r\n- [ ] KBinsDiscretizer\r\n- [ ] KFold\r\n- [ ] KMeans\r\n- [ ] KNeighborsClassifier\r\n- [x] KNeighborsRegressor\r\n- [ ] KernelCenterer\r\n- [ ] KernelDensity\r\n- [ ] KernelRidge\r\n- [ ] KeyValTuple\r\n- [ ] KeyValTupleParam\r\n- [ ] LabelBinarizer\r\n- [x] LabelEncoder\r\n- [ ] LabelPropagation\r\n- [ ] LabelSpreading\r\n- [ ] Lars\r\n- [ ] LarsCV\r\n- [ ] Lasso\r\n- [ ] LassoCV\r\n- [ ] LassoLars\r\n- [ ] LassoLarsCV\r\n- [ ] LassoLarsIC\r\n- [ ] LeastAbsoluteDeviation\r\n- [ ] LeastAbsoluteError\r\n- [ ] LeastSquares\r\n- [ ] LeastSquaresError\r\n- [ ] LeaveOneGroupOut\r\n- [ ] LeaveOneOut\r\n- [ ] LeavePGroupsOut\r\n- [ ] LeavePOut\r\n- [ ] LedoitWolf\r\n- [ ] LinearClassifierMixin\r\n- [ ] LinearModel\r\n- [ ] LinearModelCV\r\n- [ ] LinearRegression\r\n- [ ] LinearSVC\r\n- [ ] LinearSVR\r\n- [ ] LocalOutlierFactor\r\n- [ ] LocallyLinearEmbedding\r\n- [ ] LossFunction\r\n- [ ] MDS\r\n- [ ] MinCovDet\r\n- [ ] MiniBatchKMeans\r\n- [ ] MissingIndicator Open [PR](https://github.com/scikit-learn/scikit-learn/pull/15549)\r\n- [ ] Module_six_moves_urllib\r\n- [ ] Module_six_moves_urllib_error\r\n- [ ] Module_six_moves_urllib_parse\r\n- [ ] Module_six_moves_urllib_request\r\n- [ ] Module_six_moves_urllib_response\r\n- [ ] Module_six_moves_urllib_robotparser\r\n- [ ] MultiLabelBinarizer\r\n- [ ] MultiOutputClassifier\r\n- [ ] MultiOutputRegressor\r\n- [ ] MultiTaskElasticNet\r\n- [ ] MultiTaskElasticNetCV\r\n- [ ] MultiTaskLasso\r\n- [ ] MultiTaskLassoCV\r\n- [ ] MultinomialDeviance\r\n- [ ] MultinomialNB\r\n- [ ] NearestCentroid\r\n- [x] NearestNeighbors\r\n- [ ] NeighborhoodComponentsAnalysis\r\n- [ ] NeighborsBase\r\n- [ ] NoSampleWeightWrapper\r\n- [ ] NonBLASDotWarning\r\n- [ ] Normalizer\r\n- [ ] NotFittedError\r\n- [ ] NuSVR\r\n- [ ] Nystroem\r\n- [ ] OAS\r\n- [ ] OPTICS\r\n- [ ] OneClassSVM\r\n- [x] OneHotEncoder\r\n- [ ] OneVsOneClassifier\r\n- [ ] OneVsRestClassifier\r\n- [x] OrdinalEncoder\r\n- [ ] OrthogonalMatchingPursuit\r\n- [ ] OrthogonalMatchingPursuitCV\r\n- [ ] OutputCodeClassifier\r\n- [x] PLSCanonical\r\n- [x] PLSRegression\r\n- [x] PLSSVD\r\n- [x] PatchExtractor\r\n- [x] Pipeline\r\n- [x] PowerTransformer\r\n- [x] PredefinedSplit\r\n- [ ] QuantileLossFunction\r\n- [ ] QuantileTransformer\r\n- [ ] RBFSampler\r\n- [ ] RFE\r\n- [ ] RFECV\r\n- [ ] RadiusNeighborsClassifier\r\n- [x] RadiusNeighborsRegressor\r\n- [ ] RandomizedSearchCV\r\n- [ ] RegressionLossFunction\r\n- [ ] RegressorChain\r\n- [ ] RepeatedKFold\r\n- [ ] RepeatedStratifiedKFold\r\n- [ ] RidgeCV\r\n- [ ] RidgeClassifierCV\r\n- [ ] SGDOptimizer\r\n- [ ] SVR\r\n- [ ] ScaledLogOddsEstimator\r\n- [ ] SelectFdr\r\n- [ ] SelectFpr\r\n- [ ] SelectFwe\r\n- [ ] SelectKBest\r\n- [ ] SelectPercentile\r\n- [ ] SelectorMixin\r\n- [ ] ShrunkCovariance\r\n- [ ] ShuffleSplit\r\n- [ ] SkewedChi2Sampler\r\n- [ ] SkipTestWarning\r\n- [ ] SimpleImputer Open [PR](https://github.com/scikit-learn/scikit-learn/pull/15549)\r\n- [ ] SparseCodingMixin\r\n- [ ] SparseRandomProjection\r\n- [ ] SpectralBiclustering\r\n- [ ] SpectralClustering\r\n- [ ] SpectralCoclustering\r\n- [ ] SpectralEmbedding\r\n- [ ] StratifiedKFold\r\n- [ ] StratifiedShuffleSplit\r\n- [ ] TfidfTransformer PRs #15482 #15973 #16015\r\n- [ ] TfidfVectorizer PRs #15482 #15973 #16015\r\n- [ ] TheilSenRegressor\r\n- [ ] TimeSeriesSplit\r\n- [ ] TransformedTargetRegressor\r\n- [ ] TruncatedSVD\r\n- [ ] UndefinedMetricWarning\r\n- [ ] VarianceThreshold",
  "issue_comments": [
    {
      "id": 548644563,
      "user": "jnothman",
      "body": "@rth, please feel free to add a Sprint tag if you think this is suitable...\r\n\r\nBtw, these classes appear to lack a versionadded... not sure though that we want to add to all of them (and I've not filtered private classes out): ARDRegression, AdaBoostClassifier, AdaBoostRegressor, AdamOptimizer, AdditiveChi2Sampler, AffinityPropagation, AgglomerationTransform, BaggingRegressor, BaseBagging, BaseCrossValidator, BaseDecisionTree, BaseDiscreteNB, BaseEnsemble, BaseForest, BaseGradientBoosting, BaseHistGradientBoosting, BaseLabelPropagation, BaseLibSVM, BaseLoss, BaseMixture, BaseNB, BaseRandomProjection, BaseSGD, BaseSVC, BaseSearchCV, BaseShuffleSplit, BaseSpectral, BaseWeightBoosting, BernoulliNB, BernoulliRBM, Binarizer, BinaryCrossEntropy, BinomialDeviance, BinomialDeviance, Birch, Bunch, CCA, CalibratedClassifierCV, CategoricalCrossEntropy, CategoricalNB, ChangedBehaviorWarning, CheckingClassifier, ClassificationLossFunction, ClassificationLossFunction, ClassifierChain, ComplementNB, ConvergenceWarning, CountVectorizer, DataConversionWarning, DataDimensionalityWarning, DictVectorizer, DummyRegressor, ElasticNet, ElasticNetCV, EllipticEnvelope, EmpiricalCovariance, ExponentialLoss, ExponentialLoss, FactorAnalysis, FastICA, FeatureHasher, FeatureUnion, FitFailedWarning, ForestClassifier, ForestRegressor, GaussianNB, GaussianRandomProjection, GenericUnivariateSelect, GraphicalLasso, GraphicalLassoCV, GridSearchCV, GroupKFold, HistGradientBoostingClassifier, HistGradientBoostingRegressor, HuberLossFunction, HuberLossFunction, IncrementalPCA, IsotonicRegression, IterativeImputer, KBinsDiscretizer, KFold, KMeans, KNeighborsClassifier, KNeighborsRegressor, KernelCenterer, KernelDensity, KernelRidge, KeyValTuple, KeyValTupleParam, LabelBinarizer, LabelEncoder, LabelPropagation, LabelSpreading, Lars, LarsCV, Lasso, LassoCV, LassoLars, LassoLarsCV, LassoLarsIC, LeastAbsoluteDeviation, LeastAbsoluteError, LeastAbsoluteError, LeastSquares, LeastSquaresError, LeastSquaresError, LeaveOneGroupOut, LeaveOneOut, LeavePGroupsOut, LeavePOut, LedoitWolf, LinearClassifierMixin, LinearModel, LinearModelCV, LinearRegression, LinearSVC, LinearSVR, LocalOutlierFactor, LocallyLinearEmbedding, LossFunction, LossFunction, MDS, MinCovDet, MiniBatchKMeans, MissingIndicator, Module_six_moves_urllib, Module_six_moves_urllib_error, Module_six_moves_urllib_parse, Module_six_moves_urllib_request, Module_six_moves_urllib_response, Module_six_moves_urllib_robotparser, MultiLabelBinarizer, MultiOutputClassifier, MultiOutputRegressor, MultiTaskElasticNet, MultiTaskElasticNetCV, MultiTaskLasso, MultiTaskLassoCV, MultinomialDeviance, MultinomialDeviance, MultinomialNB, NearestCentroid, NearestNeighbors, NeighborhoodComponentsAnalysis, NeighborsBase, NoSampleWeightWrapper, NonBLASDotWarning, Normalizer, NotFittedError, NuSVR, Nystroem, OAS, OPTICS, OneClassSVM, OneHotEncoder, OneVsOneClassifier, OneVsRestClassifier, OrdinalEncoder, OrthogonalMatchingPursuit, OrthogonalMatchingPursuitCV, OutputCodeClassifier, PLSCanonical, PLSRegression, PLSSVD, PatchExtractor, Pipeline, PowerTransformer, PredefinedSplit, QuantileLossFunction, QuantileLossFunction, QuantileTransformer, RBFSampler, RFE, RFECV, RadiusNeighborsClassifier, RadiusNeighborsRegressor, RandomizedSearchCV, RegressionLossFunction, RegressionLossFunction, RegressorChain, RepeatedKFold, RepeatedStratifiedKFold, RidgeCV, RidgeClassifierCV, SGDOptimizer, SVR, ScaledLogOddsEstimator, SelectFdr, SelectFpr, SelectFwe, SelectKBest, SelectPercentile, SelectorMixin, ShrunkCovariance, ShuffleSplit, SkewedChi2Sampler, SkipTestWarning, SparseCodingMixin, SparseRandomProjection, SpectralBiclustering, SpectralClustering, SpectralCoclustering, SpectralEmbedding, StratifiedKFold, StratifiedShuffleSplit, TfidfTransformer, TfidfVectorizer, TheilSenRegressor, TimeSeriesSplit, TransformedTargetRegressor, TruncatedSVD, UndefinedMetricWarning, VarianceThreshold"
    },
    {
      "id": 548659338,
      "user": "rth",
      "body": "Thanks @jnothman !"
    },
    {
      "id": 549065489,
      "user": "natashaborders-zz",
      "body": "I'll work on AdaBoostClassifier, AdaBoostRegressor.\r\nPut in a pull request."
    },
    {
      "id": 549065494,
      "user": "alekslovesdata",
      "body": "LIST OF STUFF I SUBMITTED PULL REQUESTS FOR:\r\nARDRegression\r\nAffinityPropogation\r\nKNeighborsClassifier\r\nRadiusNeighborsClassifier\r\nBernoulliRBM\r\nCCA\r\nCalibratedClassifierCV\r\nCategoricalNB\r\nGroupKFold\r\nStratifiedKFold\r\nLeaveOneGroupOut\r\nLeavePGroupsOut\r\nVarianceThreshold\r\nSpectralBiclustering\r\nSpectralCoclustering\r\nSpectralEmbedding\r\nBaseBagging\r\nBaseSGD\r\n_______________________\r\nOngoing Progress:\r\n\r\n\r\n\r\n"
    },
    {
      "id": 549068174,
      "user": "lotusea",
      "body": "@lotusea worked on KernelRidge, AgglomerativeClustering"
    },
    {
      "id": 549068537,
      "user": "EricaXia",
      "body": "@EricaXia @mzeej are working on TfidfTransformer, TfidfVectorizer, TheilSenRegressor, TimeSeriesSplit, TransformedTargetRegressor, TruncatedSVD"
    },
    {
      "id": 549069752,
      "user": "geoninja",
      "body": "@geoninja is working on 'OrdinalEncoder' and 'OneHotEncoder' and LabelEncoder'"
    },
    {
      "id": 549071760,
      "user": "harini-sridhar",
      "body": "@drajan and I are working on PLSCanonical, PLSRegression, PLSSVD, PatchExtractor, Pipeline, PowerTransformer, PredefinedSplit."
    },
    {
      "id": 549073111,
      "user": "natashaborders-zz",
      "body": "Going to work on BaggingRegressor, BaseBagging, BaseCrossValidator, BaseDecisionTree, BaseDiscreteNB, BaseEnsemble, BaseForest, BaseGradientBoosting, BaseHistGradientBoosting, BaseLabelPropagation, BaseLibSVM, BaseLoss, BaseMixture, BaseNB, BaseRandomProjection, BaseSGD, BaseSVC, BaseSearchCV, BaseShuffleSplit, BaseSpectral, BaseWeightBoosting."
    },
    {
      "id": 549073255,
      "user": "ghost",
      "body": "@MZeej @EricaXia Going to work on:\r\n\r\n CheckingClassifier, ClassificationLossFunction, ClassificationLossFunction, ClassifierChain, ComplementNB, ConvergenceWarning, CountVectorizer"
    },
    {
      "id": 549077293,
      "user": "ghost",
      "body": "Please see ongoing list of items which versioning is being added during the WiMLDS sprint: \r\n\r\nhttps://tinyurl.com/y57jsvbg\r\n\r\nI think we should merge all our changes into one branch and submit a single PR to the reviewers from the account of someone who can commit to addressing any comments the reviewers have."
    },
    {
      "id": 549078227,
      "user": "ghost",
      "body": "Claiming MultinomialNB"
    },
    {
      "id": 549080796,
      "user": "EricaXia",
      "body": "Working on DataConversionWarning, DataDimensionalityWarning, DictVectorizer , DummyRegressor\r\n\r\n"
    },
    {
      "id": 549081234,
      "user": "natashaborders-zz",
      "body": "LocalOutlierFactor"
    },
    {
      "id": 549081765,
      "user": "natashaborders-zz",
      "body": "QuantileTransformer"
    },
    {
      "id": 549082088,
      "user": "natashaborders-zz",
      "body": "ClassifierChain"
    },
    {
      "id": 549082539,
      "user": "EricaXia",
      "body": "> ClassifierChain\r\n\r\nHi Natasha, see above ^ we have worked on ClassifierChain. Thank you!"
    },
    {
      "id": 549082722,
      "user": "natashaborders-zz",
      "body": "> > ClassifierChain\r\n> \r\n> Hi Natasha, see above ^ we have worked on ClassifierChain. Thank you!\r\n\r\nI'm so sorry! I was in the version 0.19 changelog and they had a list of new classes so I went through them all and didn't see the comment. Apologies!"
    },
    {
      "id": 549082894,
      "user": "alekslovesdata",
      "body": "> Please see ongoing list of items which versioning is being added during the WiMLDS sprint:\r\n> \r\n> https://tinyurl.com/y57jsvbg\r\n> \r\n> I think we should merge all our changes into one branch and submit a single PR to the reviewers from the account of someone who can commit to addressing any comments the reviewers have.\r\n\r\nHappy to volunteer if no one else has. I have the ongoing pull request open and have been adding to it. \r\n\r\nhttps://github.com/alekslovesdata/scikit-learn/tree/adding_version"
    },
    {
      "id": 549083652,
      "user": "EricaXia",
      "body": "@alekslovesdata I submitted a pull request to your repo to merge our changes. Thanks!"
    },
    {
      "id": 549084072,
      "user": "alekslovesdata",
      "body": "> @alekslovesdata I submitted a pull request to your repo to merge our changes. Thanks!\r\n\r\nJust merged it!"
    },
    {
      "id": 549089347,
      "user": "thomasjpfan",
      "body": "Do we need a policy regarding really old versions? I do not know if `versionadded: 0.5` is helpful."
    },
    {
      "id": 559725746,
      "user": "rth",
      "body": "> Do we need a policy regarding really old versions? I do not know if versionadded: 0.5 is helpful.\r\n\r\nYes, I would say not add versionadded before 0.10 maybe?"
    },
    {
      "id": 559947730,
      "user": "jnothman",
      "body": "It doesn't hurt to have for completeness.\n"
    },
    {
      "id": 559947827,
      "user": "jnothman",
      "body": "But we should label one version at a time rather than one module at a time,\nI now reckon\n"
    },
    {
      "id": 572556272,
      "user": "glemaitre",
      "body": "@cmarmo I updated the list on the top."
    },
    {
      "id": 572578231,
      "user": "cmarmo",
      "body": "> @cmarmo I updated the list on the top.\r\n\r\nThanks!"
    },
    {
      "id": 576843267,
      "user": "alhewpl",
      "body": "Could @Schindst and I work on the  LinearClassifierMixin, LinearModel, LinearRegression? "
    },
    {
      "id": 576868383,
      "user": "jnothman",
      "body": "I think it is better to work version (perhaps split by subpackage) by\nversion than class by class.\n"
    },
    {
      "id": 578390594,
      "user": "alhewpl",
      "body": "ok, we'll take v0.21 - with @Schindst"
    }
  ],
  "text_context": "# Label docstrings with versionadded / versionchanged\n\nWe should be using Sphinx's versionadded and versionchanged directives to indicate when classes, functions, methods and parameters are added or modified in their semantics. We are not very good at ensuring this labelling is done.\r\n\r\nTowards the upcoming release of v0.22, it would be useful if some contributors (or sprinters) scoured the change log and identified whether any added/changed parameters/classes needed a `versionadded` or `versionchanged` label, and to produce pull requests adding them when otherwise omitted.\r\n\r\nI also note past omissions, for example KBinsDiscretizer and IterativeImputer do not mention their versionadded, while ColumnTransformer does. Recording for each estimator when it was first released would be helpful for users when looking at the documentation.\r\n\r\n### Suggested procedure\r\n\r\nCheck the changelog in doc/whats_new/ one what's a new file at a time.\r\n**Make a single PR for an estimator and a specific version. Do not include multiple estimators and multiple versions at once (it makes it difficult to review).\r\n\r\nFocus on Public API: if a method has been made private, check the tickbox and move to another.\r\n\r\n```\r\n.. versionadded:: 0.xx\r\n```\r\n\r\nshould be included only for new estimators/parameters \r\n\r\n```\r\n.. versionchanged:: 0.xx\r\n   `param_xxx` change from 'xx' to 'yy'\r\n```\r\nshould be included when the default value of a parameter change.\r\n\r\n### Classes lacking label\r\nThe list may be incomplete.\r\n\r\n- [ ] ARDRegression PRs #15482 #15973 #16015\r\n- [x] AdaBoostClassifier\r\n- [x] AdaBoostRegressor\r\n- [ ] AdamOptimizer\r\n- [ ] AdditiveChi2Sampler\r\n- [ ] AffinityPropagation\r\n- [ ] AgglomerationTransform\r\n- [ ] BaggingRegressor\r\n- [ ] BaseBagging\r\n- [ ] BaseCrossValidator\r\n- [ ] BaseDecisionTree\r\n- [ ] BaseDiscreteNB\r\n- [ ] BaseEnsemble\r\n- [ ] BaseForest\r\n- [ ] BaseGradientBoosting\r\n- [ ] BaseHistGradientBoosting\r\n- [ ] BaseLabelPropagation\r\n- [ ] BaseLibSVM\r\n- [ ] BaseLoss\r\n- [ ] BaseMixture\r\n- [ ] BaseNB\r\n- [ ] BaseRandomProjection\r\n- [ ] BaseSGD\r\n- [ ] BaseSVC\r\n- [ ] BaseSearchCV\r\n- [ ] BaseShuffleSplit\r\n- [ ] BaseSpectral\r\n- [ ] BaseWeightBoosting\r\n- [ ] BernoulliNB\r\n- [ ] BernoulliRBM\r\n- [ ] Binarizer\r\n- [ ] BinaryCrossEntropy\r\n- [ ] BinomialDeviance\r\n- [ ] BinomialDeviance\r\n- [ ] Birch\r\n- [ ] Bunch\r\n- [ ] CCA\r\n- [ ] CalibratedClassifierCV\r\n- [ ] CategoricalCrossEntropy\r\n- [ ] CategoricalNB\r\n- [ ] ChangedBehaviorWarning\r\n- [ ] CheckingClassifier\r\n- [ ] ClassificationLossFunction\r\n- [ ] ClassifierChain\r\n- [ ] ComplementNB\r\n- [ ] ConvergenceWarning\r\n- [ ] CountVectorizer\r\n- [ ] DataConversionWarning\r\n- [ ] DataDimensionalityWarning\r\n- [ ] DictVectorizer\r\n- [ ] DummyRegressor\r\n- [ ] ElasticNet\r\n- [ ] ElasticNetCV\r\n- [ ] EllipticEnvelope\r\n- [ ] EmpiricalCovariance\r\n- [ ] ExponentialLoss\r\n- [ ] ExponentialLoss\r\n- [ ] FactorAnalysis\r\n- [ ] FastICA\r\n- [ ] FeatureHasher\r\n- [ ] FeatureUnion\r\n- [ ] FitFailedWarning\r\n- [ ] ForestClassifier\r\n- [ ] ForestRegressor\r\n- [ ] GaussianNB\r\n- [ ] GaussianRandomProjection\r\n- [ ] GenericUnivariateSelect\r\n- [ ] GraphicalLasso\r\n- [ ] GraphicalLassoCV\r\n- [ ] GridSearchCV\r\n- [ ] GroupKFold\r\n- [ ] HistGradientBoostingClassifier\r\n- [ ] HistGradientBoostingRegressor\r\n- [ ] HuberLossFunction\r\n- [ ] IncrementalPCA\r\n- [ ] IsotonicRegression\r\n- [ ] IterativeImputer Open [PR](https://github.com/scikit-learn/scikit-learn/pull/15549)\r\n- [ ] KBinsDiscretizer\r\n- [ ] KFold\r\n- [ ] KMeans\r\n- [ ] KNeighborsClassifier\r\n- [x] KNeighborsRegressor\r\n- [ ] KernelCenterer\r\n- [ ] KernelDensity\r\n- [ ] KernelRidge\r\n- [ ] KeyValTuple\r\n- [ ] KeyValTupleParam\r\n- [ ] LabelBinarizer\r\n- [x] LabelEncoder\r\n- [ ] LabelPropagation\r\n- [ ] LabelSpreading\r\n- [ ] Lars\r\n- [ ] LarsCV\r\n- [ ] Lasso\r\n- [ ] LassoCV\r\n- [ ] LassoLars\r\n- [ ] LassoLarsCV\r\n- [ ] LassoLarsIC\r\n- [ ] LeastAbsoluteDeviation\r\n- [ ] LeastAbsoluteError\r\n- [ ] LeastSquares\r\n- [ ] LeastSquaresError\r\n- [ ] LeaveOneGroupOut\r\n- [ ] LeaveOneOut\r\n- [ ] LeavePGroupsOut\r\n- [ ] LeavePOut\r\n- [ ] LedoitWolf\r\n- [ ] LinearClassifierMixin\r\n- [ ] LinearModel\r\n- [ ] LinearModelCV\r\n- [ ] LinearRegression\r\n- [ ] LinearSVC\r\n- [ ] LinearSVR\r\n- [ ] LocalOutlierFactor\r\n- [ ] LocallyLinearEmbedding\r\n- [ ] LossFunction\r\n- [ ] MDS\r\n- [ ] MinCovDet\r\n- [ ] MiniBatchKMeans\r\n- [ ] MissingIndicator Open [PR](https://github.com/scikit-learn/scikit-learn/pull/15549)\r\n- [ ] Module_six_moves_urllib\r\n- [ ] Module_six_moves_urllib_error\r\n- [ ] Module_six_moves_urllib_parse\r\n- [ ] Module_six_moves_urllib_request\r\n- [ ] Module_six_moves_urllib_response\r\n- [ ] Module_six_moves_urllib_robotparser\r\n- [ ] MultiLabelBinarizer\r\n- [ ] MultiOutputClassifier\r\n- [ ] MultiOutputRegressor\r\n- [ ] MultiTaskElasticNet\r\n- [ ] MultiTaskElasticNetCV\r\n- [ ] MultiTaskLasso\r\n- [ ] MultiTaskLassoCV\r\n- [ ] MultinomialDeviance\r\n- [ ] MultinomialNB\r\n- [ ] NearestCentroid\r\n- [x] NearestNeighbors\r\n- [ ] NeighborhoodComponentsAnalysis\r\n- [ ] NeighborsBase\r\n- [ ] NoSampleWeightWrapper\r\n- [ ] NonBLASDotWarning\r\n- [ ] Normalizer\r\n- [ ] NotFittedError\r\n- [ ] NuSVR\r\n- [ ] Nystroem\r\n- [ ] OAS\r\n- [ ] OPTICS\r\n- [ ] OneClassSVM\r\n- [x] OneHotEncoder\r\n- [ ] OneVsOneClassifier\r\n- [ ] OneVsRestClassifier\r\n- [x] OrdinalEncoder\r\n- [ ] OrthogonalMatchingPursuit\r\n- [ ] OrthogonalMatchingPursuitCV\r\n- [ ] OutputCodeClassifier\r\n- [x] PLSCanonical\r\n- [x] PLSRegression\r\n- [x] PLSSVD\r\n- [x] PatchExtractor\r\n- [x] Pipeline\r\n- [x] PowerTransformer\r\n- [x] PredefinedSplit\r\n- [ ] QuantileLossFunction\r\n- [ ] QuantileTransformer\r\n- [ ] RBFSampler\r\n- [ ] RFE\r\n- [ ] RFECV\r\n- [ ] RadiusNeighborsClassifier\r\n- [x] RadiusNeighborsRegressor\r\n- [ ] RandomizedSearchCV\r\n- [ ] RegressionLossFunction\r\n- [ ] RegressorChain\r\n- [ ] RepeatedKFold\r\n- [ ] RepeatedStratifiedKFold\r\n- [ ] RidgeCV\r\n- [ ] RidgeClassifierCV\r\n- [ ] SGDOptimizer\r\n- [ ] SVR\r\n- [ ] ScaledLogOddsEstimator\r\n- [ ] SelectFdr\r\n- [ ] SelectFpr\r\n- [ ] SelectFwe\r\n- [ ] SelectKBest\r\n- [ ] SelectPercentile\r\n- [ ] SelectorMixin\r\n- [ ] ShrunkCovariance\r\n- [ ] ShuffleSplit\r\n- [ ] SkewedChi2Sampler\r\n- [ ] SkipTestWarning\r\n- [ ] SimpleImputer Open [PR](https://github.com/scikit-learn/scikit-learn/pull/15549)\r\n- [ ] SparseCodingMixin\r\n- [ ] SparseRandomProjection\r\n- [ ] SpectralBiclustering\r\n- [ ] SpectralClustering\r\n- [ ] SpectralCoclustering\r\n- [ ] SpectralEmbedding\r\n- [ ] StratifiedKFold\r\n- [ ] StratifiedShuffleSplit\r\n- [ ] TfidfTransformer PRs #15482 #15973 #16015\r\n- [ ] TfidfVectorizer PRs #15482 #15973 #16015\r\n- [ ] TheilSenRegressor\r\n- [ ] TimeSeriesSplit\r\n- [ ] TransformedTargetRegressor\r\n- [ ] TruncatedSVD\r\n- [ ] UndefinedMetricWarning\r\n- [ ] VarianceThreshold\n\n@rth, please feel free to add a Sprint tag if you think this is suitable...\r\n\r\nBtw, these classes appear to lack a versionadded... not sure though that we want to add to all of them (and I've not filtered private classes out): ARDRegression, AdaBoostClassifier, AdaBoostRegressor, AdamOptimizer, AdditiveChi2Sampler, AffinityPropagation, AgglomerationTransform, BaggingRegressor, BaseBagging, BaseCrossValidator, BaseDecisionTree, BaseDiscreteNB, BaseEnsemble, BaseForest, BaseGradientBoosting, BaseHistGradientBoosting, BaseLabelPropagation, BaseLibSVM, BaseLoss, BaseMixture, BaseNB, BaseRandomProjection, BaseSGD, BaseSVC, BaseSearchCV, BaseShuffleSplit, BaseSpectral, BaseWeightBoosting, BernoulliNB, BernoulliRBM, Binarizer, BinaryCrossEntropy, BinomialDeviance, BinomialDeviance, Birch, Bunch, CCA, CalibratedClassifierCV, CategoricalCrossEntropy, CategoricalNB, ChangedBehaviorWarning, CheckingClassifier, ClassificationLossFunction, ClassificationLossFunction, ClassifierChain, ComplementNB, ConvergenceWarning, CountVectorizer, DataConversionWarning, DataDimensionalityWarning, DictVectorizer, DummyRegressor, ElasticNet, ElasticNetCV, EllipticEnvelope, EmpiricalCovariance, ExponentialLoss, ExponentialLoss, FactorAnalysis, FastICA, FeatureHasher, FeatureUnion, FitFailedWarning, ForestClassifier, ForestRegressor, GaussianNB, GaussianRandomProjection, GenericUnivariateSelect, GraphicalLasso, GraphicalLassoCV, GridSearchCV, GroupKFold, HistGradientBoostingClassifier, HistGradientBoostingRegressor, HuberLossFunction, HuberLossFunction, IncrementalPCA, IsotonicRegression, IterativeImputer, KBinsDiscretizer, KFold, KMeans, KNeighborsClassifier, KNeighborsRegressor, KernelCenterer, KernelDensity, KernelRidge, KeyValTuple, KeyValTupleParam, LabelBinarizer, LabelEncoder, LabelPropagation, LabelSpreading, Lars, LarsCV, Lasso, LassoCV, LassoLars, LassoLarsCV, LassoLarsIC, LeastAbsoluteDeviation, LeastAbsoluteError, LeastAbsoluteError, LeastSquares, LeastSquaresError, LeastSquaresError, LeaveOneGroupOut, LeaveOneOut, LeavePGroupsOut, LeavePOut, LedoitWolf, LinearClassifierMixin, LinearModel, LinearModelCV, LinearRegression, LinearSVC, LinearSVR, LocalOutlierFactor, LocallyLinearEmbedding, LossFunction, LossFunction, MDS, MinCovDet, MiniBatchKMeans, MissingIndicator, Module_six_moves_urllib, Module_six_moves_urllib_error, Module_six_moves_urllib_parse, Module_six_moves_urllib_request, Module_six_moves_urllib_response, Module_six_moves_urllib_robotparser, MultiLabelBinarizer, MultiOutputClassifier, MultiOutputRegressor, MultiTaskElasticNet, MultiTaskElasticNetCV, MultiTaskLasso, MultiTaskLassoCV, MultinomialDeviance, MultinomialDeviance, MultinomialNB, NearestCentroid, NearestNeighbors, NeighborhoodComponentsAnalysis, NeighborsBase, NoSampleWeightWrapper, NonBLASDotWarning, Normalizer, NotFittedError, NuSVR, Nystroem, OAS, OPTICS, OneClassSVM, OneHotEncoder, OneVsOneClassifier, OneVsRestClassifier, OrdinalEncoder, OrthogonalMatchingPursuit, OrthogonalMatchingPursuitCV, OutputCodeClassifier, PLSCanonical, PLSRegression, PLSSVD, PatchExtractor, Pipeline, PowerTransformer, PredefinedSplit, QuantileLossFunction, QuantileLossFunction, QuantileTransformer, RBFSampler, RFE, RFECV, RadiusNeighborsClassifier, RadiusNeighborsRegressor, RandomizedSearchCV, RegressionLossFunction, RegressionLossFunction, RegressorChain, RepeatedKFold, RepeatedStratifiedKFold, RidgeCV, RidgeClassifierCV, SGDOptimizer, SVR, ScaledLogOddsEstimator, SelectFdr, SelectFpr, SelectFwe, SelectKBest, SelectPercentile, SelectorMixin, ShrunkCovariance, ShuffleSplit, SkewedChi2Sampler, SkipTestWarning, SparseCodingMixin, SparseRandomProjection, SpectralBiclustering, SpectralClustering, SpectralCoclustering, SpectralEmbedding, StratifiedKFold, StratifiedShuffleSplit, TfidfTransformer, TfidfVectorizer, TheilSenRegressor, TimeSeriesSplit, TransformedTargetRegressor, TruncatedSVD, UndefinedMetricWarning, VarianceThreshold\n\nThanks @jnothman !\n\nI'll work on AdaBoostClassifier, AdaBoostRegressor.\r\nPut in a pull request.\n\nLIST OF STUFF I SUBMITTED PULL REQUESTS FOR:\r\nARDRegression\r\nAffinityPropogation\r\nKNeighborsClassifier\r\nRadiusNeighborsClassifier\r\nBernoulliRBM\r\nCCA\r\nCalibratedClassifierCV\r\nCategoricalNB\r\nGroupKFold\r\nStratifiedKFold\r\nLeaveOneGroupOut\r\nLeavePGroupsOut\r\nVarianceThreshold\r\nSpectralBiclustering\r\nSpectralCoclustering\r\nSpectralEmbedding\r\nBaseBagging\r\nBaseSGD\r\n_______________________\r\nOngoing Progress:\r\n\r\n\r\n\r\n\n\n@lotusea worked on KernelRidge, AgglomerativeClustering\n\n@EricaXia @mzeej are working on TfidfTransformer, TfidfVectorizer, TheilSenRegressor, TimeSeriesSplit, TransformedTargetRegressor, TruncatedSVD\n\n@geoninja is working on 'OrdinalEncoder' and 'OneHotEncoder' and LabelEncoder'\n\n@drajan and I are working on PLSCanonical, PLSRegression, PLSSVD, PatchExtractor, Pipeline, PowerTransformer, PredefinedSplit.\n\nGoing to work on BaggingRegressor, BaseBagging, BaseCrossValidator, BaseDecisionTree, BaseDiscreteNB, BaseEnsemble, BaseForest, BaseGradientBoosting, BaseHistGradientBoosting, BaseLabelPropagation, BaseLibSVM, BaseLoss, BaseMixture, BaseNB, BaseRandomProjection, BaseSGD, BaseSVC, BaseSearchCV, BaseShuffleSplit, BaseSpectral, BaseWeightBoosting.\n\n@MZeej @EricaXia Going to work on:\r\n\r\n CheckingClassifier, ClassificationLossFunction, ClassificationLossFunction, ClassifierChain, ComplementNB, ConvergenceWarning, CountVectorizer\n\nPlease see ongoing list of items which versioning is being added during the WiMLDS sprint: \r\n\r\nhttps://tinyurl.com/y57jsvbg\r\n\r\nI think we should merge all our changes into one branch and submit a single PR to the reviewers from the account of someone who can commit to addressing any comments the reviewers have.\n\nClaiming MultinomialNB\n\nWorking on DataConversionWarning, DataDimensionalityWarning, DictVectorizer , DummyRegressor\r\n\r\n\n\nLocalOutlierFactor\n\nQuantileTransformer\n\nClassifierChain\n\n> ClassifierChain\r\n\r\nHi Natasha, see above ^ we have worked on ClassifierChain. Thank you!\n\n> > ClassifierChain\r\n> \r\n> Hi Natasha, see above ^ we have worked on ClassifierChain. Thank you!\r\n\r\nI'm so sorry! I was in the version 0.19 changelog and they had a list of new classes so I went through them all and didn't see the comment. Apologies!\n\n> Please see ongoing list of items which versioning is being added during the WiMLDS sprint:\r\n> \r\n> https://tinyurl.com/y57jsvbg\r\n> \r\n> I think we should merge all our changes into one branch and submit a single PR to the reviewers from the account of someone who can commit to addressing any comments the reviewers have.\r\n\r\nHappy to volunteer if no one else has. I have the ongoing pull request open and have been adding to it. \r\n\r\nhttps://github.com/alekslovesdata/scikit-learn/tree/adding_version\n\n@alekslovesdata I submitted a pull request to your repo to merge our changes. Thanks!\n\n> @alekslovesdata I submitted a pull request to your repo to merge our changes. Thanks!\r\n\r\nJust merged it!\n\nDo we need a policy regarding really old versions? I do not know if `versionadded: 0.5` is helpful.\n\n> Do we need a policy regarding really old versions? I do not know if versionadded: 0.5 is helpful.\r\n\r\nYes, I would say not add versionadded before 0.10 maybe?\n\nIt doesn't hurt to have for completeness.\n\n\nBut we should label one version at a time rather than one module at a time,\nI now reckon\n\n\n@cmarmo I updated the list on the top.\n\n> @cmarmo I updated the list on the top.\r\n\r\nThanks!\n\nCould @Schindst and I work on the  LinearClassifierMixin, LinearModel, LinearRegression? \n\nI think it is better to work version (perhaps split by subpackage) by\nversion than class by class.\n\n\nok, we'll take v0.21 - with @Schindst",
  "pr_link": "https://github.com/scikit-learn/scikit-learn/pull/15549",
  "code_context": [
    {
      "filename": "sklearn/impute/_base.py",
      "content": "# Authors: Nicolas Tresegnie <nicolas.tresegnie@gmail.com>\n#          Sergey Feldman <sergeyfeldman@gmail.com>\n# License: BSD 3 clause\n\nimport numbers\nimport warnings\n\nimport numpy as np\nimport numpy.ma as ma\nfrom scipy import sparse\nfrom scipy import stats\n\nfrom ..base import BaseEstimator, TransformerMixin\nfrom ..utils.sparsefuncs import _get_median\nfrom ..utils.validation import check_is_fitted\nfrom ..utils.validation import FLOAT_DTYPES\nfrom ..utils._mask import _get_mask\nfrom ..utils import is_scalar_nan\nfrom ..utils import check_array\n\n\ndef _check_inputs_dtype(X, missing_values):\n    if (X.dtype.kind in (\"f\", \"i\", \"u\") and\n            not isinstance(missing_values, numbers.Real)):\n        raise ValueError(\"'X' and 'missing_values' types are expected to be\"\n                         \" both numerical. Got X.dtype={} and \"\n                         \" type(missing_values)={}.\"\n                         .format(X.dtype, type(missing_values)))\n\n\ndef _most_frequent(array, extra_value, n_repeat):\n    \"\"\"Compute the most frequent value in a 1d array extended with\n       [extra_value] * n_repeat, where extra_value is assumed to be not part\n       of the array.\"\"\"\n    # Compute the most frequent value in array only\n    if array.size > 0:\n        with warnings.catch_warnings():\n            # stats.mode raises a warning when input array contains objects due\n            # to incapacity to detect NaNs. Irrelevant here since input array\n            # has already been NaN-masked.\n            warnings.simplefilter(\"ignore\", RuntimeWarning)\n            mode = stats.mode(array)\n\n        most_frequent_value = mode[0][0]\n        most_frequent_count = mode[1][0]\n    else:\n        most_frequent_value = 0\n        most_frequent_count = 0\n\n    # Compare to array + [extra_value] * n_repeat\n    if most_frequent_count == 0 and n_repeat == 0:\n        return np.nan\n    elif most_frequent_count < n_repeat:\n        return extra_value\n    elif most_frequent_count > n_repeat:\n        return most_frequent_value\n    elif most_frequent_count == n_repeat:\n        # Ties the breaks. Copy the behaviour of scipy.stats.mode\n        if most_frequent_value < extra_value:\n            return most_frequent_value\n        else:\n            return extra_value\n\n\nclass _BaseImputer(TransformerMixin, BaseEstimator):\n    \"\"\"Base class for all imputers.\n\n    It adds automatically support for `add_indicator`.\n    \"\"\"\n\n    def __init__(self, missing_values=np.nan, add_indicator=False):\n        self.missing_values = missing_values\n        self.add_indicator = add_indicator\n\n    def _fit_indicator(self, X):\n        \"\"\"Fit a MissingIndicator.\"\"\"\n        if self.add_indicator:\n            self.indicator_ = MissingIndicator(\n                missing_values=self.missing_values, error_on_new=False\n            )\n            self.indicator_.fit(X)\n        else:\n            self.indicator_ = None\n\n    def _transform_indicator(self, X):\n        \"\"\"Compute the indicator mask.'\n\n        Note that X must be the original data as passed to the imputer before\n        any imputation, since imputation may be done inplace in some cases.\n        \"\"\"\n        if self.add_indicator:\n            if not hasattr(self, 'indicator_'):\n                raise ValueError(\n                    \"Make sure to call _fit_indicator before \"\n                    \"_transform_indicator\"\n                )\n            return self.indicator_.transform(X)\n\n    def _concatenate_indicator(self, X_imputed, X_indicator):\n        \"\"\"Concatenate indicator mask with the imputed data.\"\"\"\n        if not self.add_indicator:\n            return X_imputed\n\n        hstack = sparse.hstack if sparse.issparse(X_imputed) else np.hstack\n        if X_indicator is None:\n            raise ValueError(\n                \"Data from the missing indicator are not provided. Call \"\n                \"_fit_indicator and _transform_indicator in the imputer \"\n                \"implementation.\"\n                )\n\n        return hstack((X_imputed, X_indicator))\n\n    def _more_tags(self):\n        return {'allow_nan': is_scalar_nan(self.missing_values)}\n\n\nclass SimpleImputer(_BaseImputer):\n    \"\"\"Imputation transformer for completing missing values.\n\n    Read more in the :ref:`User Guide <impute>`.\n\n    .. versionadded:: 0.20\n       `SimpleImputer` replaces the previous `sklearn.preprocessing.Imputer`\n       estimator which is now removed.\n\n    Parameters\n    ----------\n    missing_values : number, string, np.nan (default) or None\n        The placeholder for the missing values. All occurrences of\n        `missing_values` will be imputed.\n\n    strategy : string, default='mean'\n        The imputation strategy.\n\n        - If \"mean\", then replace missing values using the mean along\n          each column. Can only be used with numeric data.\n        - If \"median\", then replace missing values using the median along\n          each column. Can only be used with numeric data.\n        - If \"most_frequent\", then replace missing using the most frequent\n          value along each column. Can be used with strings or numeric data.\n        - If \"constant\", then replace missing values with fill_value. Can be\n          used with strings or numeric data.\n\n        .. versionadded:: 0.20\n           strategy=\"constant\" for fixed value imputation.\n\n    fill_value : string or numerical value, default=None\n        When strategy == \"constant\", fill_value is used to replace all\n        occurrences of missing_values.\n        If left to the default, fill_value will be 0 when imputing numerical\n        data and \"missing_value\" for strings or object data types.\n\n    verbose : integer, default=0\n        Controls the verbosity of the imputer.\n\n    copy : boolean, default=True\n        If True, a copy of X will be created. If False, imputation will\n        be done in-place whenever possible. Note that, in the following cases,\n        a new copy will always be made, even if `copy=False`:\n\n        - If X is not an array of floating values;\n        - If X is encoded as a CSR matrix;\n        - If add_indicator=True.\n\n    add_indicator : boolean, default=False\n        If True, a :class:`MissingIndicator` transform will stack onto output\n        of the imputer's transform. This allows a predictive estimator\n        to account for missingness despite imputation. If a feature has no\n        missing values at fit/train time, the feature won't appear on\n        the missing indicator even if there are missing values at\n        transform/test time.\n\n    Attributes\n    ----------\n    statistics_ : array of shape (n_features,)\n        The imputation fill value for each feature.\n        Computing statistics can result in `np.nan` values.\n        During :meth:`transform`, features corresponding to `np.nan`\n        statistics will be discarded.\n\n    indicator_ : :class:`sklearn.impute.MissingIndicator`\n        Indicator used to add binary indicators for missing values.\n        ``None`` if add_indicator is False.\n\n    See also\n    --------\n    IterativeImputer : Multivariate imputation of missing values.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.impute import SimpleImputer\n    >>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n    >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n    SimpleImputer()\n    >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n    >>> print(imp_mean.transform(X))\n    [[ 7.   2.   3. ]\n     [ 4.   3.5  6. ]\n     [10.   3.5  9. ]]\n\n    Notes\n    -----\n    Columns which only contained missing values at :meth:`fit` are discarded\n    upon :meth:`transform` if strategy is not \"constant\".\n\n    \"\"\"\n    def __init__(self, missing_values=np.nan, strategy=\"mean\",\n                 fill_value=None, verbose=0, copy=True, add_indicator=False):\n        super().__init__(\n            missing_values=missing_values,\n            add_indicator=add_indicator\n        )\n        self.strategy = strategy\n        self.fill_value = fill_value\n        self.verbose = verbose\n        self.copy = copy\n\n    def _validate_input(self, X):\n        allowed_strategies = [\"mean\", \"median\", \"most_frequent\", \"constant\"]\n        if self.strategy not in allowed_strategies:\n            raise ValueError(\"Can only use these strategies: {0} \"\n                             \" got strategy={1}\".format(allowed_strategies,\n                                                        self.strategy))\n\n        if self.strategy in (\"most_frequent\", \"constant\"):\n            dtype = None\n        else:\n            dtype = FLOAT_DTYPES\n\n        if not is_scalar_nan(self.missing_values):\n            force_all_finite = True\n        else:\n            force_all_finite = \"allow-nan\"\n\n        try:\n            X = check_array(X, accept_sparse='csc', dtype=dtype,\n                            force_all_finite=force_all_finite, copy=self.copy)\n        except ValueError as ve:\n            if \"could not convert\" in str(ve):\n                new_ve = ValueError(\"Cannot use {} strategy with non-numeric \"\n                                    \"data:\\n{}\".format(self.strategy, ve))\n                raise new_ve from None\n            else:\n                raise ve\n\n        _check_inputs_dtype(X, self.missing_values)\n        if X.dtype.kind not in (\"i\", \"u\", \"f\", \"O\"):\n            raise ValueError(\"SimpleImputer does not support data with dtype \"\n                             \"{0}. Please provide either a numeric array (with\"\n                             \" a floating point or integer dtype) or \"\n                             \"categorical data represented either as an array \"\n                             \"with integer dtype or an array of string values \"\n                             \"with an object dtype.\".format(X.dtype))\n\n        return X\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the imputer on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Input data, where ``n_samples`` is the number of samples and\n            ``n_features`` is the number of features.\n\n        Returns\n        -------\n        self : SimpleImputer\n        \"\"\"\n        X = self._validate_input(X)\n        super()._fit_indicator(X)\n\n        # default fill_value is 0 for numerical input and \"missing_value\"\n        # otherwise\n        if self.fill_value is None:\n            if X.dtype.kind in (\"i\", \"u\", \"f\"):\n                fill_value = 0\n            else:\n                fill_value = \"missing_value\"\n        else:\n            fill_value = self.fill_value\n\n        # fill_value should be numerical in case of numerical input\n        if (self.strategy == \"constant\" and\n                X.dtype.kind in (\"i\", \"u\", \"f\") and\n                not isinstance(fill_value, numbers.Real)):\n            raise ValueError(\"'fill_value'={0} is invalid. Expected a \"\n                             \"numerical value when imputing numerical \"\n                             \"data\".format(fill_value))\n\n        if sparse.issparse(X):\n            # missing_values = 0 not allowed with sparse data as it would\n            # force densification\n            if self.missing_values == 0:\n                raise ValueError(\"Imputation not possible when missing_values \"\n                                 \"== 0 and input is sparse. Provide a dense \"\n                                 \"array instead.\")\n            else:\n                self.statistics_ = self._sparse_fit(X,\n                                                    self.strategy,\n                                                    self.missing_values,\n                                                    fill_value)\n        else:\n            self.statistics_ = self._dense_fit(X,\n                                               self.strategy,\n                                               self.missing_values,\n                                               fill_value)\n        return self\n\n    def _sparse_fit(self, X, strategy, missing_values, fill_value):\n        \"\"\"Fit the transformer on sparse data.\"\"\"\n        mask_data = _get_mask(X.data, missing_values)\n        n_implicit_zeros = X.shape[0] - np.diff(X.indptr)\n\n        statistics = np.empty(X.shape[1])\n\n        if strategy == \"constant\":\n            # for constant strategy, self.statistcs_ is used to store\n            # fill_value in each column\n            statistics.fill(fill_value)\n        else:\n            for i in range(X.shape[1]):\n                column = X.data[X.indptr[i]:X.indptr[i + 1]]\n                mask_column = mask_data[X.indptr[i]:X.indptr[i + 1]]\n                column = column[~mask_column]\n\n                # combine explicit and implicit zeros\n                mask_zeros = _get_mask(column, 0)\n                column = column[~mask_zeros]\n                n_explicit_zeros = mask_zeros.sum()\n                n_zeros = n_implicit_zeros[i] + n_explicit_zeros\n\n                if strategy == \"mean\":\n                    s = column.size + n_zeros\n                    statistics[i] = np.nan if s == 0 else column.sum() / s\n\n                elif strategy == \"median\":\n                    statistics[i] = _get_median(column,\n                                                n_zeros)\n\n                elif strategy == \"most_frequent\":\n                    statistics[i] = _most_frequent(column,\n                                                   0,\n                                                   n_zeros)\n        return statistics\n\n    def _dense_fit(self, X, strategy, missing_values, fill_value):\n        \"\"\"Fit the transformer on dense data.\"\"\"\n        mask = _get_mask(X, missing_values)\n        masked_X = ma.masked_array(X, mask=mask)\n\n        # Mean\n        if strategy == \"mean\":\n            mean_masked = np.ma.mean(masked_X, axis=0)\n            # Avoid the warning \"Warning: converting a masked element to nan.\"\n            mean = np.ma.getdata(mean_masked)\n            mean[np.ma.getmask(mean_masked)] = np.nan\n\n            return mean\n\n        # Median\n        elif strategy == \"median\":\n            median_masked = np.ma.median(masked_X, axis=0)\n            # Avoid the warning \"Warning: converting a masked element to nan.\"\n            median = np.ma.getdata(median_masked)\n            median[np.ma.getmaskarray(median_masked)] = np.nan\n\n            return median\n\n        # Most frequent\n        elif strategy == \"most_frequent\":\n            # Avoid use of scipy.stats.mstats.mode due to the required\n            # additional overhead and slow benchmarking performance.\n            # See Issue 14325 and PR 14399 for full discussion.\n\n            # To be able access the elements by columns\n            X = X.transpose()\n            mask = mask.transpose()\n\n            if X.dtype.kind == \"O\":\n                most_frequent = np.empty(X.shape[0], dtype=object)\n            else:\n                most_frequent = np.empty(X.shape[0])\n\n            for i, (row, row_mask) in enumerate(zip(X[:], mask[:])):\n                row_mask = np.logical_not(row_mask).astype(np.bool)\n                row = row[row_mask]\n                most_frequent[i] = _most_frequent(row, np.nan, 0)\n\n            return most_frequent\n\n        # Constant\n        elif strategy == \"constant\":\n            # for constant strategy, self.statistcs_ is used to store\n            # fill_value in each column\n            return np.full(X.shape[1], fill_value, dtype=X.dtype)\n\n    def transform(self, X):\n        \"\"\"Impute all missing values in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            The input data to complete.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._validate_input(X)\n        X_indicator = super()._transform_indicator(X)\n\n        statistics = self.statistics_\n\n        if X.shape[1] != statistics.shape[0]:\n            raise ValueError(\"X has %d features per sample, expected %d\"\n                             % (X.shape[1], self.statistics_.shape[0]))\n\n        # Delete the invalid columns if strategy is not constant\n        if self.strategy == \"constant\":\n            valid_statistics = statistics\n        else:\n            # same as np.isnan but also works for object dtypes\n            invalid_mask = _get_mask(statistics, np.nan)\n            valid_mask = np.logical_not(invalid_mask)\n            valid_statistics = statistics[valid_mask]\n            valid_statistics_indexes = np.flatnonzero(valid_mask)\n\n            if invalid_mask.any():\n                missing = np.arange(X.shape[1])[invalid_mask]\n                if self.verbose:\n                    warnings.warn(\"Deleting features without \"\n                                  \"observed values: %s\" % missing)\n                X = X[:, valid_statistics_indexes]\n\n        # Do actual imputation\n        if sparse.issparse(X):\n            if self.missing_values == 0:\n                raise ValueError(\"Imputation not possible when missing_values \"\n                                 \"== 0 and input is sparse. Provide a dense \"\n                                 \"array instead.\")\n            else:\n                mask = _get_mask(X.data, self.missing_values)\n                indexes = np.repeat(\n                    np.arange(len(X.indptr) - 1, dtype=np.int),\n                    np.diff(X.indptr))[mask]\n\n                X.data[mask] = valid_statistics[indexes].astype(X.dtype,\n                                                                copy=False)\n        else:\n            mask = _get_mask(X, self.missing_values)\n            n_missing = np.sum(mask, axis=0)\n            values = np.repeat(valid_statistics, n_missing)\n            coordinates = np.where(mask.transpose())[::-1]\n\n            X[coordinates] = values\n\n        return super()._concatenate_indicator(X, X_indicator)\n\n\nclass MissingIndicator(TransformerMixin, BaseEstimator):\n    \"\"\"Binary indicators for missing values.\n\n    Note that this component typically should not be used in a vanilla\n    :class:`Pipeline` consisting of transformers and a classifier, but rather\n    could be added using a :class:`FeatureUnion` or :class:`ColumnTransformer`.\n\n    Read more in the :ref:`User Guide <impute>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    missing_values : number, string, np.nan (default) or None\n        The placeholder for the missing values. All occurrences of\n        `missing_values` will be indicated (True in the output array), the\n        other values will be marked as False.\n\n    features : str, default=None\n        Whether the imputer mask should represent all or a subset of\n        features.\n\n        - If \"missing-only\" (default), the imputer mask will only represent\n          features containing missing values during fit time.\n        - If \"all\", the imputer mask will represent all features.\n\n    sparse : boolean or \"auto\", default=None\n        Whether the imputer mask format should be sparse or dense.\n\n        - If \"auto\" (default), the imputer mask will be of same type as\n          input.\n        - If True, the imputer mask will be a sparse matrix.\n        - If False, the imputer mask will be a numpy array.\n\n    error_on_new : boolean, default=None\n        If True (default), transform will raise an error when there are\n        features with missing values in transform that have no missing values\n        in fit. This is applicable only when ``features=\"missing-only\"``.\n\n    Attributes\n    ----------\n    features_ : ndarray, shape (n_missing_features,) or (n_features,)\n        The features indices which will be returned when calling ``transform``.\n        They are computed during ``fit``. For ``features='all'``, it is\n        to ``range(n_features)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.impute import MissingIndicator\n    >>> X1 = np.array([[np.nan, 1, 3],\n    ...                [4, 0, np.nan],\n    ...                [8, 1, 0]])\n    >>> X2 = np.array([[5, 1, np.nan],\n    ...                [np.nan, 2, 3],\n    ...                [2, 4, 0]])\n    >>> indicator = MissingIndicator()\n    >>> indicator.fit(X1)\n    MissingIndicator()\n    >>> X2_tr = indicator.transform(X2)\n    >>> X2_tr\n    array([[False,  True],\n           [ True, False],\n           [False, False]])\n\n    \"\"\"\n\n    def __init__(self, missing_values=np.nan, features=\"missing-only\",\n                 sparse=\"auto\", error_on_new=True):\n        self.missing_values = missing_values\n        self.features = features\n        self.sparse = sparse\n        self.error_on_new = error_on_new\n\n    def _get_missing_features_info(self, X):\n        \"\"\"Compute the imputer mask and the indices of the features\n        containing missing values.\n\n        Parameters\n        ----------\n        X : {ndarray or sparse matrix}, shape (n_samples, n_features)\n            The input data with missing values. Note that ``X`` has been\n            checked in ``fit`` and ``transform`` before to call this function.\n\n        Returns\n        -------\n        imputer_mask : {ndarray or sparse matrix}, shape \\\n        (n_samples, n_features)\n            The imputer mask of the original data.\n\n        features_with_missing : ndarray, shape (n_features_with_missing)\n            The features containing missing values.\n\n        \"\"\"\n        if sparse.issparse(X):\n            mask = _get_mask(X.data, self.missing_values)\n\n            # The imputer mask will be constructed with the same sparse format\n            # as X.\n            sparse_constructor = (sparse.csr_matrix if X.format == 'csr'\n                                  else sparse.csc_matrix)\n            imputer_mask = sparse_constructor(\n                (mask, X.indices.copy(), X.indptr.copy()),\n                shape=X.shape, dtype=bool)\n            imputer_mask.eliminate_zeros()\n\n            if self.features == 'missing-only':\n                n_missing = imputer_mask.getnnz(axis=0)\n\n            if self.sparse is False:\n                imputer_mask = imputer_mask.toarray()\n            elif imputer_mask.format == 'csr':\n                imputer_mask = imputer_mask.tocsc()\n        else:\n            imputer_mask = _get_mask(X, self.missing_values)\n\n            if self.features == 'missing-only':\n                n_missing = imputer_mask.sum(axis=0)\n\n            if self.sparse is True:\n                imputer_mask = sparse.csc_matrix(imputer_mask)\n\n        if self.features == 'all':\n            features_indices = np.arange(X.shape[1])\n        else:\n            features_indices = np.flatnonzero(n_missing)\n\n        return imputer_mask, features_indices\n\n    def _validate_input(self, X):\n        if not is_scalar_nan(self.missing_values):\n            force_all_finite = True\n        else:\n            force_all_finite = \"allow-nan\"\n        X = check_array(X, accept_sparse=('csc', 'csr'), dtype=None,\n                        force_all_finite=force_all_finite)\n        _check_inputs_dtype(X, self.missing_values)\n        if X.dtype.kind not in (\"i\", \"u\", \"f\", \"O\"):\n            raise ValueError(\"MissingIndicator does not support data with \"\n                             \"dtype {0}. Please provide either a numeric array\"\n                             \" (with a floating point or integer dtype) or \"\n                             \"categorical data represented either as an array \"\n                             \"with integer dtype or an array of string values \"\n                             \"with an object dtype.\".format(X.dtype))\n\n        if sparse.issparse(X) and self.missing_values == 0:\n            # missing_values = 0 not allowed with sparse data as it would\n            # force densification\n            raise ValueError(\"Sparse input with missing_values=0 is \"\n                             \"not supported. Provide a dense \"\n                             \"array instead.\")\n\n        return X\n\n    def _fit(self, X, y=None):\n        \"\"\"Fit the transformer on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Input data, where ``n_samples`` is the number of samples and\n            ``n_features`` is the number of features.\n\n        Returns\n        -------\n        imputer_mask : {ndarray or sparse matrix}, shape (n_samples, \\\n        n_features)\n            The imputer mask of the original data.\n\n        \"\"\"\n        X = self._validate_input(X)\n        self._n_features = X.shape[1]\n\n        if self.features not in ('missing-only', 'all'):\n            raise ValueError(\"'features' has to be either 'missing-only' or \"\n                             \"'all'. Got {} instead.\".format(self.features))\n\n        if not ((isinstance(self.sparse, str) and\n                self.sparse == \"auto\") or isinstance(self.sparse, bool)):\n            raise ValueError(\"'sparse' has to be a boolean or 'auto'. \"\n                             \"Got {!r} instead.\".format(self.sparse))\n\n        missing_features_info = self._get_missing_features_info(X)\n        self.features_ = missing_features_info[1]\n\n        return missing_features_info[0]\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the transformer on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Input data, where ``n_samples`` is the number of samples and\n            ``n_features`` is the number of features.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        self._fit(X, y)\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Generate missing values indicator for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            The input data to complete.\n\n        Returns\n        -------\n        Xt : {ndarray or sparse matrix}, shape (n_samples, n_features) \\\n        or (n_samples, n_features_with_missing)\n            The missing indicator for input data. The data type of ``Xt``\n            will be boolean.\n\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_input(X)\n\n        if X.shape[1] != self._n_features:\n            raise ValueError(\"X has a different number of features \"\n                             \"than during fitting.\")\n\n        imputer_mask, features = self._get_missing_features_info(X)\n\n        if self.features == \"missing-only\":\n            features_diff_fit_trans = np.setdiff1d(features, self.features_)\n            if (self.error_on_new and features_diff_fit_trans.size > 0):\n                raise ValueError(\"The features {} have missing values \"\n                                 \"in transform but have no missing values \"\n                                 \"in fit.\".format(features_diff_fit_trans))\n\n            if self.features_.size < self._n_features:\n                imputer_mask = imputer_mask[:, self.features_]\n\n        return imputer_mask\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Generate missing values indicator for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            The input data to complete.\n\n        Returns\n        -------\n        Xt : {ndarray or sparse matrix}, shape (n_samples, n_features) \\\n        or (n_samples, n_features_with_missing)\n            The missing indicator for input data. The data type of ``Xt``\n            will be boolean.\n\n        \"\"\"\n        imputer_mask = self._fit(X, y)\n\n        if self.features_.size < self._n_features:\n            imputer_mask = imputer_mask[:, self.features_]\n\n        return imputer_mask\n\n    def _more_tags(self):\n        return {'allow_nan': True,\n                'X_types': ['2darray', 'string']}\n"
    },
    {
      "filename": "sklearn/impute/_iterative.py",
      "content": "\nfrom time import time\nfrom distutils.version import LooseVersion\nfrom collections import namedtuple\nimport warnings\n\nimport scipy\nfrom scipy import stats\nimport numpy as np\n\nfrom ..base import clone\nfrom ..exceptions import ConvergenceWarning\nfrom ..preprocessing import normalize\nfrom ..utils import check_array, check_random_state, _safe_indexing\nfrom ..utils.validation import FLOAT_DTYPES, check_is_fitted\nfrom ..utils import is_scalar_nan\nfrom ..utils._mask import _get_mask\n\nfrom ._base import _BaseImputer\nfrom ._base import SimpleImputer\nfrom ._base import _check_inputs_dtype\n\n\n_ImputerTriplet = namedtuple('_ImputerTriplet', ['feat_idx',\n                                                 'neighbor_feat_idx',\n                                                 'estimator'])\n\n\nclass IterativeImputer(_BaseImputer):\n    \"\"\"Multivariate imputer that estimates each feature from all the others.\n\n    A strategy for imputing missing values by modeling each feature with\n    missing values as a function of other features in a round-robin fashion.\n\n    Read more in the :ref:`User Guide <iterative_imputer>`.\n\n    .. versionadded:: 0.21\n\n    .. note::\n\n      This estimator is still **experimental** for now: the predictions\n      and the API might change without any deprecation cycle. To use it,\n      you need to explicitly import ``enable_iterative_imputer``::\n\n        >>> # explicitly require this experimental feature\n        >>> from sklearn.experimental import enable_iterative_imputer  # noqa\n        >>> # now you can import normally from sklearn.impute\n        >>> from sklearn.impute import IterativeImputer\n\n    Parameters\n    ----------\n    estimator : estimator object, default=BayesianRidge()\n        The estimator to use at each step of the round-robin imputation.\n        If ``sample_posterior`` is True, the estimator must support\n        ``return_std`` in its ``predict`` method.\n\n    missing_values : int, np.nan, default=np.nan\n        The placeholder for the missing values. All occurrences of\n        ``missing_values`` will be imputed.\n\n    sample_posterior : boolean, default=False\n        Whether to sample from the (Gaussian) predictive posterior of the\n        fitted estimator for each imputation. Estimator must support\n        ``return_std`` in its ``predict`` method if set to ``True``. Set to\n        ``True`` if using ``IterativeImputer`` for multiple imputations.\n\n    max_iter : int, default=10\n        Maximum number of imputation rounds to perform before returning the\n        imputations computed during the final round. A round is a single\n        imputation of each feature with missing values. The stopping criterion\n        is met once `abs(max(X_t - X_{t-1}))/abs(max(X[known_vals]))` < tol,\n        where `X_t` is `X` at iteration `t. Note that early stopping is only\n        applied if ``sample_posterior=False``.\n\n    tol : float, default=1e-3\n        Tolerance of the stopping condition.\n\n    n_nearest_features : int, default=None\n        Number of other features to use to estimate the missing values of\n        each feature column. Nearness between features is measured using\n        the absolute correlation coefficient between each feature pair (after\n        initial imputation). To ensure coverage of features throughout the\n        imputation process, the neighbor features are not necessarily nearest,\n        but are drawn with probability proportional to correlation for each\n        imputed target feature. Can provide significant speed-up when the\n        number of features is huge. If ``None``, all features will be used.\n\n    initial_strategy : str, default='mean'\n        Which strategy to use to initialize the missing values. Same as the\n        ``strategy`` parameter in :class:`sklearn.impute.SimpleImputer`\n        Valid values: {\"mean\", \"median\", \"most_frequent\", or \"constant\"}.\n\n    imputation_order : str, default='ascending'\n        The order in which the features will be imputed. Possible values:\n\n        \"ascending\"\n            From features with fewest missing values to most.\n        \"descending\"\n            From features with most missing values to fewest.\n        \"roman\"\n            Left to right.\n        \"arabic\"\n            Right to left.\n        \"random\"\n            A random order for each round.\n\n    skip_complete : boolean, default=False\n        If ``True`` then features with missing values during ``transform``\n        which did not have any missing values during ``fit`` will be imputed\n        with the initial imputation method only. Set to ``True`` if you have\n        many features with no missing values at both ``fit`` and ``transform``\n        time to save compute.\n\n    min_value : float, default=None\n        Minimum possible imputed value. Default of ``None`` will set minimum\n        to negative infinity.\n\n    max_value : float, default=None\n        Maximum possible imputed value. Default of ``None`` will set maximum\n        to positive infinity.\n\n    verbose : int, default=0\n        Verbosity flag, controls the debug messages that are issued\n        as functions are evaluated. The higher, the more verbose. Can be 0, 1,\n        or 2.\n\n    random_state : int, RandomState instance or None, default=None\n        The seed of the pseudo random number generator to use. Randomizes\n        selection of estimator features if n_nearest_features is not None, the\n        ``imputation_order`` if ``random``, and the sampling from posterior if\n        ``sample_posterior`` is True. Use an integer for determinism.\n        See :term:`the Glossary <random_state>`.\n\n    add_indicator : boolean, default=False\n        If True, a :class:`MissingIndicator` transform will stack onto output\n        of the imputer's transform. This allows a predictive estimator\n        to account for missingness despite imputation. If a feature has no\n        missing values at fit/train time, the feature won't appear on\n        the missing indicator even if there are missing values at\n        transform/test time.\n\n    Attributes\n    ----------\n    initial_imputer_ : object of type :class:`sklearn.impute.SimpleImputer`\n        Imputer used to initialize the missing values.\n\n    imputation_sequence_ : list of tuples\n        Each tuple has ``(feat_idx, neighbor_feat_idx, estimator)``, where\n        ``feat_idx`` is the current feature to be imputed,\n        ``neighbor_feat_idx`` is the array of other features used to impute the\n        current feature, and ``estimator`` is the trained estimator used for\n        the imputation. Length is ``self.n_features_with_missing_ *\n        self.n_iter_``.\n\n    n_iter_ : int\n        Number of iteration rounds that occurred. Will be less than\n        ``self.max_iter`` if early stopping criterion was reached.\n\n    n_features_with_missing_ : int\n        Number of features with missing values.\n\n    indicator_ : :class:`sklearn.impute.MissingIndicator`\n        Indicator used to add binary indicators for missing values.\n        ``None`` if add_indicator is False.\n\n    random_state_ : RandomState instance\n        RandomState instance that is generated either from a seed, the random\n        number generator or by `np.random`.\n\n    See also\n    --------\n    SimpleImputer : Univariate imputation of missing values.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.experimental import enable_iterative_imputer  \n    >>> from sklearn.impute import IterativeImputer\n    >>> imp_mean = IterativeImputer(random_state=0)\n    >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n    IterativeImputer(random_state=0)\n    >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n    >>> imp_mean.transform(X)\n    array([[ 6.9584...,  2.       ,  3.        ],\n           [ 4.       ,  2.6000...,  6.        ],\n           [10.       ,  4.9999...,  9.        ]])\n\n    Notes\n    -----\n    To support imputation in inductive mode we store each feature's estimator\n    during the ``fit`` phase, and predict without refitting (in order) during\n    the ``transform`` phase.\n\n    Features which contain all missing values at ``fit`` are discarded upon\n    ``transform``.\n\n    References\n    ----------\n    .. [1] `Stef van Buuren, Karin Groothuis-Oudshoorn (2011). \"mice:\n        Multivariate Imputation by Chained Equations in R\". Journal of\n        Statistical Software 45: 1-67.\n        <https://www.jstatsoft.org/article/view/v045i03>`_\n\n    .. [2] `S. F. Buck, (1960). \"A Method of Estimation of Missing Values in\n        Multivariate Data Suitable for use with an Electronic Computer\".\n        Journal of the Royal Statistical Society 22(2): 302-306.\n        <https://www.jstor.org/stable/2984099>`_\n    \"\"\"\n\n    def __init__(self,\n                 estimator=None,\n                 missing_values=np.nan,\n                 sample_posterior=False,\n                 max_iter=10,\n                 tol=1e-3,\n                 n_nearest_features=None,\n                 initial_strategy=\"mean\",\n                 imputation_order='ascending',\n                 skip_complete=False,\n                 min_value=None,\n                 max_value=None,\n                 verbose=0,\n                 random_state=None,\n                 add_indicator=False):\n        super().__init__(\n            missing_values=missing_values,\n            add_indicator=add_indicator\n        )\n\n        self.estimator = estimator\n        self.sample_posterior = sample_posterior\n        self.max_iter = max_iter\n        self.tol = tol\n        self.n_nearest_features = n_nearest_features\n        self.initial_strategy = initial_strategy\n        self.imputation_order = imputation_order\n        self.skip_complete = skip_complete\n        self.min_value = min_value\n        self.max_value = max_value\n        self.verbose = verbose\n        self.random_state = random_state\n\n    def _impute_one_feature(self,\n                            X_filled,\n                            mask_missing_values,\n                            feat_idx,\n                            neighbor_feat_idx,\n                            estimator=None,\n                            fit_mode=True):\n        \"\"\"Impute a single feature from the others provided.\n\n        This function predicts the missing values of one of the features using\n        the current estimates of all the other features. The ``estimator`` must\n        support ``return_std=True`` in its ``predict`` method for this function\n        to work.\n\n        Parameters\n        ----------\n        X_filled : ndarray\n            Input data with the most recent imputations.\n\n        mask_missing_values : ndarray\n            Input data's missing indicator matrix.\n\n        feat_idx : int\n            Index of the feature currently being imputed.\n\n        neighbor_feat_idx : ndarray\n            Indices of the features to be used in imputing ``feat_idx``.\n\n        estimator : object\n            The estimator to use at this step of the round-robin imputation.\n            If ``sample_posterior`` is True, the estimator must support\n            ``return_std`` in its ``predict`` method.\n            If None, it will be cloned from self._estimator.\n\n        fit_mode : boolean, default=True\n            Whether to fit and predict with the estimator or just predict.\n\n        Returns\n        -------\n        X_filled : ndarray\n            Input data with ``X_filled[missing_row_mask, feat_idx]`` updated.\n\n        estimator : estimator with sklearn API\n            The fitted estimator used to impute\n            ``X_filled[missing_row_mask, feat_idx]``.\n        \"\"\"\n        if estimator is None and fit_mode is False:\n            raise ValueError(\"If fit_mode is False, then an already-fitted \"\n                             \"estimator should be passed in.\")\n\n        if estimator is None:\n            estimator = clone(self._estimator)\n\n        missing_row_mask = mask_missing_values[:, feat_idx]\n        if fit_mode:\n            X_train = _safe_indexing(X_filled[:, neighbor_feat_idx],\n                                    ~missing_row_mask)\n            y_train = _safe_indexing(X_filled[:, feat_idx],\n                                    ~missing_row_mask)\n            estimator.fit(X_train, y_train)\n\n        # if no missing values, don't predict\n        if np.sum(missing_row_mask) == 0:\n            return X_filled, estimator\n\n        # get posterior samples if there is at least one missing value\n        X_test = _safe_indexing(X_filled[:, neighbor_feat_idx],\n                                missing_row_mask)\n        if self.sample_posterior:\n            mus, sigmas = estimator.predict(X_test, return_std=True)\n            imputed_values = np.zeros(mus.shape, dtype=X_filled.dtype)\n            # two types of problems: (1) non-positive sigmas\n            # (2) mus outside legal range of min_value and max_value\n            # (results in inf sample)\n            positive_sigmas = sigmas > 0\n            imputed_values[~positive_sigmas] = mus[~positive_sigmas]\n            mus_too_low = mus < self._min_value\n            imputed_values[mus_too_low] = self._min_value\n            mus_too_high = mus > self._max_value\n            imputed_values[mus_too_high] = self._max_value\n            # the rest can be sampled without statistical issues\n            inrange_mask = positive_sigmas & ~mus_too_low & ~mus_too_high\n            mus = mus[inrange_mask]\n            sigmas = sigmas[inrange_mask]\n            a = (self._min_value - mus) / sigmas\n            b = (self._max_value - mus) / sigmas\n\n            if scipy.__version__ < LooseVersion('0.18'):\n                # bug with vector-valued `a` in old scipy\n                imputed_values[inrange_mask] = [\n                    stats.truncnorm(a=a_, b=b_,\n                                    loc=loc_, scale=scale_).rvs(\n                                        random_state=self.random_state_)\n                    for a_, b_, loc_, scale_\n                    in zip(a, b, mus, sigmas)]\n            else:\n                truncated_normal = stats.truncnorm(a=a, b=b,\n                                                   loc=mus, scale=sigmas)\n                imputed_values[inrange_mask] = truncated_normal.rvs(\n                    random_state=self.random_state_)\n        else:\n            imputed_values = estimator.predict(X_test)\n            imputed_values = np.clip(imputed_values,\n                                     self._min_value,\n                                     self._max_value)\n\n        # update the feature\n        X_filled[missing_row_mask, feat_idx] = imputed_values\n        return X_filled, estimator\n\n    def _get_neighbor_feat_idx(self,\n                               n_features,\n                               feat_idx,\n                               abs_corr_mat):\n        \"\"\"Get a list of other features to predict ``feat_idx``.\n\n        If self.n_nearest_features is less than or equal to the total\n        number of features, then use a probability proportional to the absolute\n        correlation between ``feat_idx`` and each other feature to randomly\n        choose a subsample of the other features (without replacement).\n\n        Parameters\n        ----------\n        n_features : int\n            Number of features in ``X``.\n\n        feat_idx : int\n            Index of the feature currently being imputed.\n\n        abs_corr_mat : ndarray, shape (n_features, n_features)\n            Absolute correlation matrix of ``X``. The diagonal has been zeroed\n            out and each feature has been normalized to sum to 1. Can be None.\n\n        Returns\n        -------\n        neighbor_feat_idx : array-like\n            The features to use to impute ``feat_idx``.\n        \"\"\"\n        if (self.n_nearest_features is not None and\n                self.n_nearest_features < n_features):\n            p = abs_corr_mat[:, feat_idx]\n            neighbor_feat_idx = self.random_state_.choice(\n                np.arange(n_features), self.n_nearest_features, replace=False,\n                p=p)\n        else:\n            inds_left = np.arange(feat_idx)\n            inds_right = np.arange(feat_idx + 1, n_features)\n            neighbor_feat_idx = np.concatenate((inds_left, inds_right))\n        return neighbor_feat_idx\n\n    def _get_ordered_idx(self, mask_missing_values):\n        \"\"\"Decide in what order we will update the features.\n\n        As a homage to the MICE R package, we will have 4 main options of\n        how to order the updates, and use a random order if anything else\n        is specified.\n\n        Also, this function skips features which have no missing values.\n\n        Parameters\n        ----------\n        mask_missing_values : array-like, shape (n_samples, n_features)\n            Input data's missing indicator matrix, where \"n_samples\" is the\n            number of samples and \"n_features\" is the number of features.\n\n        Returns\n        -------\n        ordered_idx : ndarray, shape (n_features,)\n            The order in which to impute the features.\n        \"\"\"\n        frac_of_missing_values = mask_missing_values.mean(axis=0)\n        if self.skip_complete:\n            missing_values_idx = np.flatnonzero(frac_of_missing_values)\n        else:\n            missing_values_idx = np.arange(np.shape(frac_of_missing_values)[0])\n        if self.imputation_order == 'roman':\n            ordered_idx = missing_values_idx\n        elif self.imputation_order == 'arabic':\n            ordered_idx = missing_values_idx[::-1]\n        elif self.imputation_order == 'ascending':\n            n = len(frac_of_missing_values) - len(missing_values_idx)\n            ordered_idx = np.argsort(frac_of_missing_values,\n                                     kind='mergesort')[n:]\n        elif self.imputation_order == 'descending':\n            n = len(frac_of_missing_values) - len(missing_values_idx)\n            ordered_idx = np.argsort(frac_of_missing_values,\n                                     kind='mergesort')[n:][::-1]\n        elif self.imputation_order == 'random':\n            ordered_idx = missing_values_idx\n            self.random_state_.shuffle(ordered_idx)\n        else:\n            raise ValueError(\"Got an invalid imputation order: '{0}'. It must \"\n                             \"be one of the following: 'roman', 'arabic', \"\n                             \"'ascending', 'descending', or \"\n                             \"'random'.\".format(self.imputation_order))\n        return ordered_idx\n\n    def _get_abs_corr_mat(self, X_filled, tolerance=1e-6):\n        \"\"\"Get absolute correlation matrix between features.\n\n        Parameters\n        ----------\n        X_filled : ndarray, shape (n_samples, n_features)\n            Input data with the most recent imputations.\n\n        tolerance : float, default=1e-6\n            ``abs_corr_mat`` can have nans, which will be replaced\n            with ``tolerance``.\n\n        Returns\n        -------\n        abs_corr_mat : ndarray, shape (n_features, n_features)\n            Absolute correlation matrix of ``X`` at the beginning of the\n            current round. The diagonal has been zeroed out and each feature's\n            absolute correlations with all others have been normalized to sum\n            to 1.\n        \"\"\"\n        n_features = X_filled.shape[1]\n        if (self.n_nearest_features is None or\n                self.n_nearest_features >= n_features):\n            return None\n        with np.errstate(invalid='ignore'):\n            # if a feature in the neighboorhood has only a single value\n            # (e.g., categorical feature), the std. dev. will be null and\n            # np.corrcoef will raise a warning due to a division by zero\n            abs_corr_mat = np.abs(np.corrcoef(X_filled.T))\n        # np.corrcoef is not defined for features with zero std\n        abs_corr_mat[np.isnan(abs_corr_mat)] = tolerance\n        # ensures exploration, i.e. at least some probability of sampling\n        np.clip(abs_corr_mat, tolerance, None, out=abs_corr_mat)\n        # features are not their own neighbors\n        np.fill_diagonal(abs_corr_mat, 0)\n        # needs to sum to 1 for np.random.choice sampling\n        abs_corr_mat = normalize(abs_corr_mat, norm='l1', axis=0, copy=False)\n        return abs_corr_mat\n\n    def _initial_imputation(self, X):\n        \"\"\"Perform initial imputation for input X.\n\n        Parameters\n        ----------\n        X : ndarray, shape (n_samples, n_features)\n            Input data, where \"n_samples\" is the number of samples and\n            \"n_features\" is the number of features.\n\n        Returns\n        -------\n        Xt : ndarray, shape (n_samples, n_features)\n            Input data, where \"n_samples\" is the number of samples and\n            \"n_features\" is the number of features.\n\n        X_filled : ndarray, shape (n_samples, n_features)\n            Input data with the most recent imputations.\n\n        mask_missing_values : ndarray, shape (n_samples, n_features)\n            Input data's missing indicator matrix, where \"n_samples\" is the\n            number of samples and \"n_features\" is the number of features.\n        \"\"\"\n        if is_scalar_nan(self.missing_values):\n            force_all_finite = \"allow-nan\"\n        else:\n            force_all_finite = True\n\n        X = check_array(X, dtype=FLOAT_DTYPES, order=\"F\",\n                        force_all_finite=force_all_finite)\n        _check_inputs_dtype(X, self.missing_values)\n\n        mask_missing_values = _get_mask(X, self.missing_values)\n        if self.initial_imputer_ is None:\n            self.initial_imputer_ = SimpleImputer(\n                missing_values=self.missing_values,\n                strategy=self.initial_strategy\n            )\n            X_filled = self.initial_imputer_.fit_transform(X)\n        else:\n            X_filled = self.initial_imputer_.transform(X)\n\n        valid_mask = np.flatnonzero(np.logical_not(\n            np.isnan(self.initial_imputer_.statistics_)))\n        Xt = X[:, valid_mask]\n        mask_missing_values = mask_missing_values[:, valid_mask]\n\n        return Xt, X_filled, mask_missing_values\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Fits the imputer on X and return the transformed X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Input data, where \"n_samples\" is the number of samples and\n            \"n_features\" is the number of features.\n\n        y : ignored.\n\n        Returns\n        -------\n        Xt : array-like, shape (n_samples, n_features)\n            The imputed input data.\n        \"\"\"\n        self.random_state_ = getattr(self, \"random_state_\",\n                                     check_random_state(self.random_state))\n\n        if self.max_iter < 0:\n            raise ValueError(\n                \"'max_iter' should be a positive integer. Got {} instead.\"\n                .format(self.max_iter))\n\n        if self.tol < 0:\n            raise ValueError(\n                \"'tol' should be a non-negative float. Got {} instead.\"\n                .format(self.tol)\n            )\n\n        if self.estimator is None:\n            from ..linear_model import BayesianRidge\n            self._estimator = BayesianRidge()\n        else:\n            self._estimator = clone(self.estimator)\n\n        if hasattr(self._estimator, 'random_state'):\n            self._estimator.random_state = self.random_state_\n\n        self.imputation_sequence_ = []\n\n        self._min_value = -np.inf if self.min_value is None else self.min_value\n        self._max_value = np.inf if self.max_value is None else self.max_value\n\n        self.initial_imputer_ = None\n        super()._fit_indicator(X)\n        X_indicator = super()._transform_indicator(X)\n        X, Xt, mask_missing_values = self._initial_imputation(X)\n        if self.max_iter == 0 or np.all(mask_missing_values):\n            self.n_iter_ = 0\n            return super()._concatenate_indicator(Xt, X_indicator)\n\n        # Edge case: a single feature. We return the initial ...\n        if Xt.shape[1] == 1:\n            self.n_iter_ = 0\n            return super()._concatenate_indicator(Xt, X_indicator)\n\n        # order in which to impute\n        # note this is probably too slow for large feature data (d > 100000)\n        # and a better way would be good.\n        # see: https://goo.gl/KyCNwj and subsequent comments\n        ordered_idx = self._get_ordered_idx(mask_missing_values)\n        self.n_features_with_missing_ = len(ordered_idx)\n\n        abs_corr_mat = self._get_abs_corr_mat(Xt)\n\n        n_samples, n_features = Xt.shape\n        if self.verbose > 0:\n            print(\"[IterativeImputer] Completing matrix with shape %s\"\n                  % (X.shape,))\n        start_t = time()\n        if not self.sample_posterior:\n            Xt_previous = Xt.copy()\n            normalized_tol = self.tol * np.max(\n                np.abs(X[~mask_missing_values])\n            )\n        for self.n_iter_ in range(1, self.max_iter + 1):\n            if self.imputation_order == 'random':\n                ordered_idx = self._get_ordered_idx(mask_missing_values)\n\n            for feat_idx in ordered_idx:\n                neighbor_feat_idx = self._get_neighbor_feat_idx(n_features,\n                                                                feat_idx,\n                                                                abs_corr_mat)\n                Xt, estimator = self._impute_one_feature(\n                    Xt, mask_missing_values, feat_idx, neighbor_feat_idx,\n                    estimator=None, fit_mode=True)\n                estimator_triplet = _ImputerTriplet(feat_idx,\n                                                    neighbor_feat_idx,\n                                                    estimator)\n                self.imputation_sequence_.append(estimator_triplet)\n\n            if self.verbose > 1:\n                print('[IterativeImputer] Ending imputation round '\n                      '%d/%d, elapsed time %0.2f'\n                      % (self.n_iter_, self.max_iter, time() - start_t))\n\n            if not self.sample_posterior:\n                inf_norm = np.linalg.norm(Xt - Xt_previous, ord=np.inf,\n                                          axis=None)\n                if self.verbose > 0:\n                    print('[IterativeImputer] '\n                          'Change: {}, scaled tolerance: {} '.format(\n                              inf_norm, normalized_tol))\n                if inf_norm < normalized_tol:\n                    if self.verbose > 0:\n                        print('[IterativeImputer] Early stopping criterion '\n                              'reached.')\n                    break\n                Xt_previous = Xt.copy()\n        else:\n            if not self.sample_posterior:\n                warnings.warn(\"[IterativeImputer] Early stopping criterion not\"\n                              \" reached.\", ConvergenceWarning)\n        Xt[~mask_missing_values] = X[~mask_missing_values]\n        return super()._concatenate_indicator(Xt, X_indicator)\n\n    def transform(self, X):\n        \"\"\"Imputes all missing values in X.\n\n        Note that this is stochastic, and that if random_state is not fixed,\n        repeated calls, or permuted input, will yield different results.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input data to complete.\n\n        Returns\n        -------\n        Xt : array-like, shape (n_samples, n_features)\n             The imputed input data.\n        \"\"\"\n        check_is_fitted(self)\n\n        X_indicator = super()._transform_indicator(X)\n        X, Xt, mask_missing_values = self._initial_imputation(X)\n\n        if self.n_iter_ == 0 or np.all(mask_missing_values):\n            return super()._concatenate_indicator(Xt, X_indicator)\n\n        imputations_per_round = len(self.imputation_sequence_) // self.n_iter_\n        i_rnd = 0\n        if self.verbose > 0:\n            print(\"[IterativeImputer] Completing matrix with shape %s\"\n                  % (X.shape,))\n        start_t = time()\n        for it, estimator_triplet in enumerate(self.imputation_sequence_):\n            Xt, _ = self._impute_one_feature(\n                Xt,\n                mask_missing_values,\n                estimator_triplet.feat_idx,\n                estimator_triplet.neighbor_feat_idx,\n                estimator=estimator_triplet.estimator,\n                fit_mode=False\n            )\n            if not (it + 1) % imputations_per_round:\n                if self.verbose > 1:\n                    print('[IterativeImputer] Ending imputation round '\n                          '%d/%d, elapsed time %0.2f'\n                          % (i_rnd + 1, self.n_iter_, time() - start_t))\n                i_rnd += 1\n\n        Xt[~mask_missing_values] = X[~mask_missing_values]\n\n        return super()._concatenate_indicator(Xt, X_indicator)\n\n    def fit(self, X, y=None):\n        \"\"\"Fits the imputer on X and return self.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Input data, where \"n_samples\" is the number of samples and\n            \"n_features\" is the number of features.\n\n        y : ignored\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        self.fit_transform(X)\n        return self\n"
    }
  ]
}