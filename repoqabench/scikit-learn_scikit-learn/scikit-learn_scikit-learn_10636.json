{
  "repo_name": "scikit-learn_scikit-learn",
  "issue_id": "10636",
  "issue_description": "# Remove SimpleImputer's axis parameter\n\nWe recently (#10483) moved sklearn.preprocessing.Imputer to sklearn.impute.SimpleImputer to make way for other imputation strategies (and to acknowledge that imputation is a kind of learning problem in itself).\r\n\r\nWe also want to get rid of the axis parameter. In fact we did this in #10558, before realising that if we're deprecating all of preprocessing.Imputer, it's unnecessary. Since SimpleImputer is not yet released, we do not need to deprecate the parameter.\r\n\r\nSo your mission is to emulate #10558 but don't bother with deprecation:\r\n* to remove all mentions of SimpleImputer's `axis`\r\n* to update the what's new entry about moving Imputer to SimpleImputer with a comment that axis has been removed",
  "issue_comments": [
    {
      "id": 365606905,
      "user": "qinhanmin2014",
      "body": "To dear contributors: It's recommended to go through the comments in #10483  (e.g., https://github.com/scikit-learn/scikit-learn/pull/10483#issuecomment-365485168)"
    },
    {
      "id": 365623646,
      "user": "giba0",
      "body": "Newbie here! Can I work on this?"
    },
    {
      "id": 365627198,
      "user": "qinhanmin2014",
      "body": "@gilbertoolimpio go ahead please"
    },
    {
      "id": 365723920,
      "user": "jnothman",
      "body": "Sorry, I didn't realise I hadn't merged #10483\n"
    },
    {
      "id": 365724127,
      "user": "jnothman",
      "body": "Now you can have a go.\n"
    },
    {
      "id": 365792545,
      "user": "giba0",
      "body": "@qinhanmin2014  and @jnothman I need help, I'm a little confused, should I remove all mentions from the SimpleImputer `axis` only in the documentation? And besides, could you help me how could be the comment that the `axis` was removed?\r\n\r\nIn the code the `axis` often appears as `axis = 0`, should I change to `None` or delete these entries as well?\r\n\r\nFor example in the code below (in impute.py class SimpleImpute) should I remove these entries from the `axis` parameter or set `None`?\r\n```python\r\n    def __init__(self, missing_values=\"NaN\", strategy=\"mean\",\r\n                 axis=0, verbose=0, copy=True):\r\n        self.missing_values = missing_values\r\n        self.strategy = strategy\r\n        self.axis = axis\r\n        self.verbose = verbose\r\n        self.copy = copy\r\n```\r\ntks! =)"
    },
    {
      "id": 365795134,
      "user": "jnothman",
      "body": "Because this is not released code, it can be safely removed without breaking backwards compatibility. Just drop axis there. Don't worry about looking at the previous work and comments too much.\r\n\r\nYes, you need to remove from the documentation too, if necessary adjusting the wording to say imputation is done over each column.\r\n\r\nIn terms of adding a comment on axis being removed, we ultimately want to say something like \"sklearn.preprocessing.Imputer has been renamed to sklearn.impute.SimpleImputer, and its axis parameter is no longer available. The deprecated Imputer will be removed in version 0.22.\" in doc/whats_new/v0.20.rst where the deprecation of Imputer is currently mentioned (but you can use your own words). A mention of axis disappearing is also appropriate in the deprecation warning at https://github.com/scikit-learn/scikit-learn/blob/4d4116097deee37d9ea38c447401c29456000e78/sklearn/preprocessing/imputation.py#L64"
    },
    {
      "id": 365795250,
      "user": "jnothman",
      "body": "We might also want to help users a bit more and say\r\n```\r\nFuture (and default) behavior is equivalent to ``axis=0``\r\n(impute along columns). Row-wise imputation can be performed with\r\nFunctionTransformer (e.g.,\r\n``FunctionTransformer(lambda X: Imputer().fit_transform(X.T).T)``).\r\n```\r\n\r\nin what's new."
    },
    {
      "id": 368179674,
      "user": "bhaveshpoddar94",
      "body": "First Time Contributor. Can I look into this issue?"
    },
    {
      "id": 368184477,
      "user": "qinhanmin2014",
      "body": "@bhaveshpoddar94 Someone has taken the issue. See the above PR which refers to the issue."
    }
  ],
  "text_context": "# Remove SimpleImputer's axis parameter\n\nWe recently (#10483) moved sklearn.preprocessing.Imputer to sklearn.impute.SimpleImputer to make way for other imputation strategies (and to acknowledge that imputation is a kind of learning problem in itself).\r\n\r\nWe also want to get rid of the axis parameter. In fact we did this in #10558, before realising that if we're deprecating all of preprocessing.Imputer, it's unnecessary. Since SimpleImputer is not yet released, we do not need to deprecate the parameter.\r\n\r\nSo your mission is to emulate #10558 but don't bother with deprecation:\r\n* to remove all mentions of SimpleImputer's `axis`\r\n* to update the what's new entry about moving Imputer to SimpleImputer with a comment that axis has been removed\n\nTo dear contributors: It's recommended to go through the comments in #10483  (e.g., https://github.com/scikit-learn/scikit-learn/pull/10483#issuecomment-365485168)\n\nNewbie here! Can I work on this?\n\n@gilbertoolimpio go ahead please\n\nSorry, I didn't realise I hadn't merged #10483\n\n\nNow you can have a go.\n\n\n@qinhanmin2014  and @jnothman I need help, I'm a little confused, should I remove all mentions from the SimpleImputer `axis` only in the documentation? And besides, could you help me how could be the comment that the `axis` was removed?\r\n\r\nIn the code the `axis` often appears as `axis = 0`, should I change to `None` or delete these entries as well?\r\n\r\nFor example in the code below (in impute.py class SimpleImpute) should I remove these entries from the `axis` parameter or set `None`?\r\n```python\r\n    def __init__(self, missing_values=\"NaN\", strategy=\"mean\",\r\n                 axis=0, verbose=0, copy=True):\r\n        self.missing_values = missing_values\r\n        self.strategy = strategy\r\n        self.axis = axis\r\n        self.verbose = verbose\r\n        self.copy = copy\r\n```\r\ntks! =)\n\nBecause this is not released code, it can be safely removed without breaking backwards compatibility. Just drop axis there. Don't worry about looking at the previous work and comments too much.\r\n\r\nYes, you need to remove from the documentation too, if necessary adjusting the wording to say imputation is done over each column.\r\n\r\nIn terms of adding a comment on axis being removed, we ultimately want to say something like \"sklearn.preprocessing.Imputer has been renamed to sklearn.impute.SimpleImputer, and its axis parameter is no longer available. The deprecated Imputer will be removed in version 0.22.\" in doc/whats_new/v0.20.rst where the deprecation of Imputer is currently mentioned (but you can use your own words). A mention of axis disappearing is also appropriate in the deprecation warning at https://github.com/scikit-learn/scikit-learn/blob/4d4116097deee37d9ea38c447401c29456000e78/sklearn/preprocessing/imputation.py#L64\n\nWe might also want to help users a bit more and say\r\n```\r\nFuture (and default) behavior is equivalent to ``axis=0``\r\n(impute along columns). Row-wise imputation can be performed with\r\nFunctionTransformer (e.g.,\r\n``FunctionTransformer(lambda X: Imputer().fit_transform(X.T).T)``).\r\n```\r\n\r\nin what's new.\n\nFirst Time Contributor. Can I look into this issue?\n\n@bhaveshpoddar94 Someone has taken the issue. See the above PR which refers to the issue.",
  "pr_link": "https://github.com/scikit-learn/scikit-learn/pull/10483",
  "code_context": [
    {
      "filename": "examples/plot_missing_values.py",
      "content": "\"\"\"\n======================================================\nImputing missing values before building an estimator\n======================================================\n\nThis example shows that imputing the missing values can give better\nresults than discarding the samples containing any missing value.\nImputing does not always improve the predictions, so please check via\ncross-validation.  Sometimes dropping rows or using marker values is\nmore effective.\n\nMissing values can be replaced by the mean, the median or the most frequent\nvalue using the ``strategy`` hyper-parameter.\nThe median is a more robust estimator for data with high magnitude variables\nwhich could dominate results (otherwise known as a 'long tail').\n\nScript output::\n\n  Score with the entire dataset = 0.56\n  Score without the samples containing missing values = 0.48\n  Score after imputation of the missing values = 0.55\n\nIn this case, imputing helps the classifier get close to the original score.\n\n\"\"\"\nimport numpy as np\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\n\nrng = np.random.RandomState(0)\n\ndataset = load_boston()\nX_full, y_full = dataset.data, dataset.target\nn_samples = X_full.shape[0]\nn_features = X_full.shape[1]\n\n# Estimate the score on the entire dataset, with no missing values\nestimator = RandomForestRegressor(random_state=0, n_estimators=100)\nscore = cross_val_score(estimator, X_full, y_full).mean()\nprint(\"Score with the entire dataset = %.2f\" % score)\n\n# Add missing values in 75% of the lines\nmissing_rate = 0.75\nn_missing_samples = int(np.floor(n_samples * missing_rate))\nmissing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,\n                                      dtype=np.bool),\n                             np.ones(n_missing_samples,\n                                     dtype=np.bool)))\nrng.shuffle(missing_samples)\nmissing_features = rng.randint(0, n_features, n_missing_samples)\n\n# Estimate the score without the lines containing missing values\nX_filtered = X_full[~missing_samples, :]\ny_filtered = y_full[~missing_samples]\nestimator = RandomForestRegressor(random_state=0, n_estimators=100)\nscore = cross_val_score(estimator, X_filtered, y_filtered).mean()\nprint(\"Score without the samples containing missing values = %.2f\" % score)\n\n# Estimate the score after imputation of the missing values\nX_missing = X_full.copy()\nX_missing[np.where(missing_samples)[0], missing_features] = 0\ny_missing = y_full.copy()\nestimator = Pipeline([(\"imputer\", SimpleImputer(missing_values=0,\n                                                strategy=\"mean\",\n                                                axis=0)),\n                      (\"forest\", RandomForestRegressor(random_state=0,\n                                                       n_estimators=100))])\nscore = cross_val_score(estimator, X_missing, y_missing).mean()\nprint(\"Score after imputation of the missing values = %.2f\" % score)\n"
    },
    {
      "filename": "sklearn/__init__.py",
      "content": "\"\"\"\nMachine learning module for Python\n==================================\n\nsklearn is a Python module integrating classical machine\nlearning algorithms in the tightly-knit world of scientific Python\npackages (numpy, scipy, matplotlib).\n\nIt aims to provide simple and efficient solutions to learning problems\nthat are accessible to everybody and reusable in various contexts:\nmachine-learning as a versatile tool for science and engineering.\n\nSee http://scikit-learn.org for complete documentation.\n\"\"\"\nimport sys\nimport re\nimport warnings\nimport logging\n\nfrom ._config import get_config, set_config, config_context\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.StreamHandler())\nlogger.setLevel(logging.INFO)\n\n\n# Make sure that DeprecationWarning within this package always gets printed\nwarnings.filterwarnings('always', category=DeprecationWarning,\n                        module=r'^{0}\\.'.format(re.escape(__name__)))\n\n# PEP0440 compatible formatted version, see:\n# https://www.python.org/dev/peps/pep-0440/\n#\n# Generic release markers:\n#   X.Y\n#   X.Y.Z   # For bugfix releases\n#\n# Admissible pre-release markers:\n#   X.YaN   # Alpha release\n#   X.YbN   # Beta release\n#   X.YrcN  # Release Candidate\n#   X.Y     # Final release\n#\n# Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.\n# 'X.Y.dev0' is the canonical version of 'X.Y.dev'\n#\n__version__ = '0.20.dev0'\n\n\ntry:\n    # This variable is injected in the __builtins__ by the build\n    # process. It used to enable importing subpackages of sklearn when\n    # the binaries are not built\n    __SKLEARN_SETUP__\nexcept NameError:\n    __SKLEARN_SETUP__ = False\n\nif __SKLEARN_SETUP__:\n    sys.stderr.write('Partial import of sklearn during the build process.\\n')\n    # We are not importing the rest of scikit-learn during the build\n    # process, as it may not be compiled yet\nelse:\n    from . import __check_build\n    from .base import clone\n    __check_build  # avoid flakes unused variable error\n\n    __all__ = ['calibration', 'cluster', 'covariance', 'cross_decomposition',\n               'cross_validation', 'datasets', 'decomposition', 'dummy',\n               'ensemble', 'exceptions', 'externals', 'feature_extraction',\n               'feature_selection', 'gaussian_process', 'grid_search',\n               'isotonic', 'kernel_approximation', 'kernel_ridge',\n               'learning_curve', 'linear_model', 'manifold', 'metrics',\n               'mixture', 'model_selection', 'multiclass', 'multioutput',\n               'naive_bayes', 'neighbors', 'neural_network', 'pipeline',\n               'preprocessing', 'random_projection', 'semi_supervised',\n               'svm', 'tree', 'discriminant_analysis', 'impute',\n               # Non-modules:\n               'clone', 'get_config', 'set_config', 'config_context']\n\n\ndef setup_module(module):\n    \"\"\"Fixture for the tests to assure globally controllable seeding of RNGs\"\"\"\n    import os\n    import numpy as np\n    import random\n\n    # It could have been provided in the environment\n    _random_seed = os.environ.get('SKLEARN_SEED', None)\n    if _random_seed is None:\n        _random_seed = np.random.uniform() * (2 ** 31 - 1)\n    _random_seed = int(_random_seed)\n    print(\"I: Seeding RNGs with %r\" % _random_seed)\n    np.random.seed(_random_seed)\n    random.seed(_random_seed)\n"
    },
    {
      "filename": "sklearn/impute.py",
      "content": "\"\"\"Transformers for missing value imputation\"\"\"\n# Authors: Nicolas Tresegnie <nicolas.tresegnie@gmail.com>\n# License: BSD 3 clause\n\nimport warnings\n\nimport numpy as np\nimport numpy.ma as ma\nfrom scipy import sparse\nfrom scipy import stats\n\nfrom .base import BaseEstimator, TransformerMixin\nfrom .utils import check_array\nfrom .utils.sparsefuncs import _get_median\nfrom .utils.validation import check_is_fitted\nfrom .utils.validation import FLOAT_DTYPES\n\nfrom .externals import six\n\nzip = six.moves.zip\nmap = six.moves.map\n\n__all__ = [\n    'SimpleImputer',\n]\n\n\ndef _get_mask(X, value_to_mask):\n    \"\"\"Compute the boolean mask X == missing_values.\"\"\"\n    if value_to_mask == \"NaN\" or np.isnan(value_to_mask):\n        return np.isnan(X)\n    else:\n        return X == value_to_mask\n\n\ndef _most_frequent(array, extra_value, n_repeat):\n    \"\"\"Compute the most frequent value in a 1d array extended with\n       [extra_value] * n_repeat, where extra_value is assumed to be not part\n       of the array.\"\"\"\n    # Compute the most frequent value in array only\n    if array.size > 0:\n        mode = stats.mode(array)\n        most_frequent_value = mode[0][0]\n        most_frequent_count = mode[1][0]\n    else:\n        most_frequent_value = 0\n        most_frequent_count = 0\n\n    # Compare to array + [extra_value] * n_repeat\n    if most_frequent_count == 0 and n_repeat == 0:\n        return np.nan\n    elif most_frequent_count < n_repeat:\n        return extra_value\n    elif most_frequent_count > n_repeat:\n        return most_frequent_value\n    elif most_frequent_count == n_repeat:\n        # Ties the breaks. Copy the behaviour of scipy.stats.mode\n        if most_frequent_value < extra_value:\n            return most_frequent_value\n        else:\n            return extra_value\n\n\nclass SimpleImputer(BaseEstimator, TransformerMixin):\n    \"\"\"Imputation transformer for completing missing values.\n\n    Read more in the :ref:`User Guide <impute>`.\n\n    Parameters\n    ----------\n    missing_values : integer or \"NaN\", optional (default=\"NaN\")\n        The placeholder for the missing values. All occurrences of\n        `missing_values` will be imputed. For missing values encoded as np.nan,\n        use the string value \"NaN\".\n\n    strategy : string, optional (default=\"mean\")\n        The imputation strategy.\n\n        - If \"mean\", then replace missing values using the mean along\n          the axis.\n        - If \"median\", then replace missing values using the median along\n          the axis.\n        - If \"most_frequent\", then replace missing using the most frequent\n          value along the axis.\n\n    axis : integer, optional (default=0)\n        The axis along which to impute.\n\n        - If `axis=0`, then impute along columns.\n        - If `axis=1`, then impute along rows.\n\n    verbose : integer, optional (default=0)\n        Controls the verbosity of the imputer.\n\n    copy : boolean, optional (default=True)\n        If True, a copy of X will be created. If False, imputation will\n        be done in-place whenever possible. Note that, in the following cases,\n        a new copy will always be made, even if `copy=False`:\n\n        - If X is not an array of floating values;\n        - If X is sparse and `missing_values=0`;\n        - If `axis=0` and X is encoded as a CSR matrix;\n        - If `axis=1` and X is encoded as a CSC matrix.\n\n    Attributes\n    ----------\n    statistics_ : array of shape (n_features,)\n        The imputation fill value for each feature if axis == 0.\n\n    Notes\n    -----\n    - When ``axis=0``, columns which only contained missing values at `fit`\n      are discarded upon `transform`.\n    - When ``axis=1``, an exception is raised if there are rows for which it is\n      not possible to fill in the missing values (e.g., because they only\n      contain missing values).\n    \"\"\"\n    def __init__(self, missing_values=\"NaN\", strategy=\"mean\",\n                 axis=0, verbose=0, copy=True):\n        self.missing_values = missing_values\n        self.strategy = strategy\n        self.axis = axis\n        self.verbose = verbose\n        self.copy = copy\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the imputer on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Input data, where ``n_samples`` is the number of samples and\n            ``n_features`` is the number of features.\n\n        Returns\n        -------\n        self : SimpleImputer\n        \"\"\"\n        # Check parameters\n        allowed_strategies = [\"mean\", \"median\", \"most_frequent\"]\n        if self.strategy not in allowed_strategies:\n            raise ValueError(\"Can only use these strategies: {0} \"\n                             \" got strategy={1}\".format(allowed_strategies,\n                                                        self.strategy))\n\n        if self.axis not in [0, 1]:\n            raise ValueError(\"Can only impute missing values on axis 0 and 1, \"\n                             \" got axis={0}\".format(self.axis))\n\n        # Since two different arrays can be provided in fit(X) and\n        # transform(X), the imputation data will be computed in transform()\n        # when the imputation is done per sample (i.e., when axis=1).\n        if self.axis == 0:\n            X = check_array(X, accept_sparse='csc', dtype=np.float64,\n                            force_all_finite=False)\n\n            if sparse.issparse(X):\n                self.statistics_ = self._sparse_fit(X,\n                                                    self.strategy,\n                                                    self.missing_values,\n                                                    self.axis)\n            else:\n                self.statistics_ = self._dense_fit(X,\n                                                   self.strategy,\n                                                   self.missing_values,\n                                                   self.axis)\n\n        return self\n\n    def _sparse_fit(self, X, strategy, missing_values, axis):\n        \"\"\"Fit the transformer on sparse data.\"\"\"\n        # Imputation is done \"by column\", so if we want to do it\n        # by row we only need to convert the matrix to csr format.\n        if axis == 1:\n            X = X.tocsr()\n        else:\n            X = X.tocsc()\n\n        # Count the zeros\n        if missing_values == 0:\n            n_zeros_axis = np.zeros(X.shape[not axis], dtype=int)\n        else:\n            n_zeros_axis = X.shape[axis] - np.diff(X.indptr)\n\n        # Mean\n        if strategy == \"mean\":\n            if missing_values != 0:\n                n_non_missing = n_zeros_axis\n\n                # Mask the missing elements\n                mask_missing_values = _get_mask(X.data, missing_values)\n                mask_valids = np.logical_not(mask_missing_values)\n\n                # Sum only the valid elements\n                new_data = X.data.copy()\n                new_data[mask_missing_values] = 0\n                X = sparse.csc_matrix((new_data, X.indices, X.indptr),\n                                      copy=False)\n                sums = X.sum(axis=0)\n\n                # Count the elements != 0\n                mask_non_zeros = sparse.csc_matrix(\n                    (mask_valids.astype(np.float64),\n                     X.indices,\n                     X.indptr), copy=False)\n                s = mask_non_zeros.sum(axis=0)\n                n_non_missing = np.add(n_non_missing, s)\n\n            else:\n                sums = X.sum(axis=axis)\n                n_non_missing = np.diff(X.indptr)\n\n            # Ignore the error, columns with a np.nan statistics_\n            # are not an error at this point. These columns will\n            # be removed in transform\n            with np.errstate(all=\"ignore\"):\n                return np.ravel(sums) / np.ravel(n_non_missing)\n\n        # Median + Most frequent\n        else:\n            # Remove the missing values, for each column\n            columns_all = np.hsplit(X.data, X.indptr[1:-1])\n            mask_missing_values = _get_mask(X.data, missing_values)\n            mask_valids = np.hsplit(np.logical_not(mask_missing_values),\n                                    X.indptr[1:-1])\n\n            # astype necessary for bug in numpy.hsplit before v1.9\n            columns = [col[mask.astype(bool, copy=False)]\n                       for col, mask in zip(columns_all, mask_valids)]\n\n            # Median\n            if strategy == \"median\":\n                median = np.empty(len(columns))\n                for i, column in enumerate(columns):\n                    median[i] = _get_median(column, n_zeros_axis[i])\n\n                return median\n\n            # Most frequent\n            elif strategy == \"most_frequent\":\n                most_frequent = np.empty(len(columns))\n\n                for i, column in enumerate(columns):\n                    most_frequent[i] = _most_frequent(column,\n                                                      0,\n                                                      n_zeros_axis[i])\n\n                return most_frequent\n\n    def _dense_fit(self, X, strategy, missing_values, axis):\n        \"\"\"Fit the transformer on dense data.\"\"\"\n        X = check_array(X, force_all_finite=False)\n        mask = _get_mask(X, missing_values)\n        masked_X = ma.masked_array(X, mask=mask)\n\n        # Mean\n        if strategy == \"mean\":\n            mean_masked = np.ma.mean(masked_X, axis=axis)\n            # Avoid the warning \"Warning: converting a masked element to nan.\"\n            mean = np.ma.getdata(mean_masked)\n            mean[np.ma.getmask(mean_masked)] = np.nan\n\n            return mean\n\n        # Median\n        elif strategy == \"median\":\n            if tuple(int(v) for v in np.__version__.split('.')[:2]) < (1, 5):\n                # In old versions of numpy, calling a median on an array\n                # containing nans returns nan. This is different is\n                # recent versions of numpy, which we want to mimic\n                masked_X.mask = np.logical_or(masked_X.mask,\n                                              np.isnan(X))\n            median_masked = np.ma.median(masked_X, axis=axis)\n            # Avoid the warning \"Warning: converting a masked element to nan.\"\n            median = np.ma.getdata(median_masked)\n            median[np.ma.getmaskarray(median_masked)] = np.nan\n\n            return median\n\n        # Most frequent\n        elif strategy == \"most_frequent\":\n            # scipy.stats.mstats.mode cannot be used because it will no work\n            # properly if the first element is masked and if its frequency\n            # is equal to the frequency of the most frequent valid element\n            # See https://github.com/scipy/scipy/issues/2636\n\n            # To be able access the elements by columns\n            if axis == 0:\n                X = X.transpose()\n                mask = mask.transpose()\n\n            most_frequent = np.empty(X.shape[0])\n\n            for i, (row, row_mask) in enumerate(zip(X[:], mask[:])):\n                row_mask = np.logical_not(row_mask).astype(np.bool)\n                row = row[row_mask]\n                most_frequent[i] = _most_frequent(row, np.nan, 0)\n\n            return most_frequent\n\n    def transform(self, X):\n        \"\"\"Impute all missing values in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            The input data to complete.\n        \"\"\"\n        if self.axis == 0:\n            check_is_fitted(self, 'statistics_')\n            X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,\n                            force_all_finite=False, copy=self.copy)\n            statistics = self.statistics_\n            if X.shape[1] != statistics.shape[0]:\n                raise ValueError(\"X has %d features per sample, expected %d\"\n                                 % (X.shape[1], self.statistics_.shape[0]))\n\n        # Since two different arrays can be provided in fit(X) and\n        # transform(X), the imputation data need to be recomputed\n        # when the imputation is done per sample\n        else:\n            X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES,\n                            force_all_finite=False, copy=self.copy)\n\n            if sparse.issparse(X):\n                statistics = self._sparse_fit(X,\n                                              self.strategy,\n                                              self.missing_values,\n                                              self.axis)\n\n            else:\n                statistics = self._dense_fit(X,\n                                             self.strategy,\n                                             self.missing_values,\n                                             self.axis)\n\n        # Delete the invalid rows/columns\n        invalid_mask = np.isnan(statistics)\n        valid_mask = np.logical_not(invalid_mask)\n        valid_statistics = statistics[valid_mask]\n        valid_statistics_indexes = np.where(valid_mask)[0]\n        missing = np.arange(X.shape[not self.axis])[invalid_mask]\n\n        if self.axis == 0 and invalid_mask.any():\n            if self.verbose:\n                warnings.warn(\"Deleting features without \"\n                              \"observed values: %s\" % missing)\n            X = X[:, valid_statistics_indexes]\n        elif self.axis == 1 and invalid_mask.any():\n            raise ValueError(\"Some rows only contain \"\n                             \"missing values: %s\" % missing)\n\n        # Do actual imputation\n        if sparse.issparse(X) and self.missing_values != 0:\n            mask = _get_mask(X.data, self.missing_values)\n            indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),\n                                np.diff(X.indptr))[mask]\n\n            X.data[mask] = valid_statistics[indexes].astype(X.dtype,\n                                                            copy=False)\n        else:\n            if sparse.issparse(X):\n                X = X.toarray()\n\n            mask = _get_mask(X, self.missing_values)\n            n_missing = np.sum(mask, axis=self.axis)\n            values = np.repeat(valid_statistics, n_missing)\n\n            if self.axis == 0:\n                coordinates = np.where(mask.transpose())[::-1]\n            else:\n                coordinates = mask\n\n            X[coordinates] = values\n\n        return X\n"
    },
    {
      "filename": "sklearn/model_selection/tests/test_search.py",
      "content": "\"\"\"Test the search module\"\"\"\n\nfrom collections import Iterable, Sized\nfrom sklearn.externals.six.moves import cStringIO as StringIO\nfrom sklearn.externals.six.moves import xrange\nfrom sklearn.externals.joblib._compat import PY3_OR_LATER\nfrom itertools import chain, product\nimport pickle\nimport sys\nfrom types import GeneratorType\nimport re\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom sklearn.utils.fixes import sp_version\nfrom sklearn.utils.testing import assert_equal\nfrom sklearn.utils.testing import assert_not_equal\nfrom sklearn.utils.testing import assert_raises\nfrom sklearn.utils.testing import assert_warns\nfrom sklearn.utils.testing import assert_warns_message\nfrom sklearn.utils.testing import assert_no_warnings\nfrom sklearn.utils.testing import assert_raise_message\nfrom sklearn.utils.testing import assert_false, assert_true\nfrom sklearn.utils.testing import assert_array_equal\nfrom sklearn.utils.testing import assert_array_almost_equal\nfrom sklearn.utils.testing import assert_almost_equal\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.utils.mocking import CheckingClassifier, MockDataFrame\n\nfrom scipy.stats import bernoulli, expon, uniform\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import clone\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_multilabel_classification\n\nfrom sklearn.model_selection import fit_grid_point\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom sklearn.model_selection import LeavePGroupsOut\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.model_selection import ParameterSampler\n\nfrom sklearn.model_selection._validation import FitFailedWarning\n\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge, SGDClassifier\n\nfrom sklearn.model_selection.tests.common import OneTimeSplitter\n\n\n# Neither of the following two estimators inherit from BaseEstimator,\n# to test hyperparameter search on user-defined classifiers.\nclass MockClassifier(object):\n    \"\"\"Dummy classifier to test the parameter search algorithms\"\"\"\n    def __init__(self, foo_param=0):\n        self.foo_param = foo_param\n\n    def fit(self, X, Y):\n        assert_true(len(X) == len(Y))\n        self.classes_ = np.unique(Y)\n        return self\n\n    def predict(self, T):\n        return T.shape[0]\n\n    def transform(self, X):\n        return X + self.foo_param\n\n    def inverse_transform(self, X):\n        return X - self.foo_param\n\n    predict_proba = predict\n    predict_log_proba = predict\n    decision_function = predict\n\n    def score(self, X=None, Y=None):\n        if self.foo_param > 1:\n            score = 1.\n        else:\n            score = 0.\n        return score\n\n    def get_params(self, deep=False):\n        return {'foo_param': self.foo_param}\n\n    def set_params(self, **params):\n        self.foo_param = params['foo_param']\n        return self\n\n\nclass LinearSVCNoScore(LinearSVC):\n    \"\"\"An LinearSVC classifier that has no score method.\"\"\"\n    @property\n    def score(self):\n        raise AttributeError\n\n\nX = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\ny = np.array([1, 1, 2, 2])\n\n\ndef assert_grid_iter_equals_getitem(grid):\n    assert_equal(list(grid), [grid[i] for i in range(len(grid))])\n\n\ndef test_parameter_grid():\n    # Test basic properties of ParameterGrid.\n    params1 = {\"foo\": [1, 2, 3]}\n    grid1 = ParameterGrid(params1)\n    assert_true(isinstance(grid1, Iterable))\n    assert_true(isinstance(grid1, Sized))\n    assert_equal(len(grid1), 3)\n    assert_grid_iter_equals_getitem(grid1)\n\n    params2 = {\"foo\": [4, 2],\n               \"bar\": [\"ham\", \"spam\", \"eggs\"]}\n    grid2 = ParameterGrid(params2)\n    assert_equal(len(grid2), 6)\n\n    # loop to assert we can iterate over the grid multiple times\n    for i in xrange(2):\n        # tuple + chain transforms {\"a\": 1, \"b\": 2} to (\"a\", 1, \"b\", 2)\n        points = set(tuple(chain(*(sorted(p.items())))) for p in grid2)\n        assert_equal(points,\n                     set((\"bar\", x, \"foo\", y)\n                         for x, y in product(params2[\"bar\"], params2[\"foo\"])))\n    assert_grid_iter_equals_getitem(grid2)\n\n    # Special case: empty grid (useful to get default estimator settings)\n    empty = ParameterGrid({})\n    assert_equal(len(empty), 1)\n    assert_equal(list(empty), [{}])\n    assert_grid_iter_equals_getitem(empty)\n    assert_raises(IndexError, lambda: empty[1])\n\n    has_empty = ParameterGrid([{'C': [1, 10]}, {}, {'C': [.5]}])\n    assert_equal(len(has_empty), 4)\n    assert_equal(list(has_empty), [{'C': 1}, {'C': 10}, {}, {'C': .5}])\n    assert_grid_iter_equals_getitem(has_empty)\n\n\ndef test_grid_search():\n    # Test that the best estimator contains the right value for foo_param\n    clf = MockClassifier()\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, verbose=3)\n    # make sure it selects the smallest parameter in case of ties\n    old_stdout = sys.stdout\n    sys.stdout = StringIO()\n    grid_search.fit(X, y)\n    sys.stdout = old_stdout\n    assert_equal(grid_search.best_estimator_.foo_param, 2)\n\n    assert_array_equal(grid_search.cv_results_[\"param_foo_param\"].data,\n                       [1, 2, 3])\n\n    # Smoke test the score etc:\n    grid_search.score(X, y)\n    grid_search.predict_proba(X)\n    grid_search.decision_function(X)\n    grid_search.transform(X)\n\n    # Test exception handling on scoring\n    grid_search.scoring = 'sklearn'\n    assert_raises(ValueError, grid_search.fit, X, y)\n\n\ndef check_hyperparameter_searcher_with_fit_params(klass, **klass_kwargs):\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n    clf = CheckingClassifier(expected_fit_params=['spam', 'eggs'])\n    searcher = klass(clf, {'foo_param': [1, 2, 3]}, cv=2, **klass_kwargs)\n\n    # The CheckingClassifier generates an assertion error if\n    # a parameter is missing or has length != len(X).\n    assert_raise_message(AssertionError,\n                         \"Expected fit parameter(s) ['eggs'] not seen.\",\n                         searcher.fit, X, y, spam=np.ones(10))\n    assert_raise_message(AssertionError,\n                         \"Fit parameter spam has length 1; expected 4.\",\n                         searcher.fit, X, y, spam=np.ones(1),\n                         eggs=np.zeros(10))\n    searcher.fit(X, y, spam=np.ones(10), eggs=np.zeros(10))\n\n\ndef test_grid_search_with_fit_params():\n    check_hyperparameter_searcher_with_fit_params(GridSearchCV)\n\n\ndef test_random_search_with_fit_params():\n    check_hyperparameter_searcher_with_fit_params(RandomizedSearchCV, n_iter=1)\n\n\ndef test_grid_search_fit_params_deprecation():\n    # NOTE: Remove this test in v0.21\n\n    # Use of `fit_params` in the class constructor is deprecated,\n    # but will still work until v0.21.\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n    clf = CheckingClassifier(expected_fit_params=['spam'])\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]},\n                               fit_params={'spam': np.ones(10)})\n    assert_warns(DeprecationWarning, grid_search.fit, X, y)\n\n\ndef test_grid_search_fit_params_two_places():\n    # NOTE: Remove this test in v0.21\n\n    # If users try to input fit parameters in both\n    # the constructor (deprecated use) and the `fit`\n    # method, we'll ignore the values passed to the constructor.\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n    clf = CheckingClassifier(expected_fit_params=['spam'])\n\n    # The \"spam\" array is too short and will raise an\n    # error in the CheckingClassifier if used.\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]},\n                               fit_params={'spam': np.ones(1)})\n\n    expected_warning = ('Ignoring fit_params passed as a constructor '\n                        'argument in favor of keyword arguments to '\n                        'the \"fit\" method.')\n    assert_warns_message(RuntimeWarning, expected_warning,\n                         grid_search.fit, X, y, spam=np.ones(10))\n\n    # Verify that `fit` prefers its own kwargs by giving valid\n    # kwargs in the constructor and invalid in the method call\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]},\n                               fit_params={'spam': np.ones(10)})\n    assert_raise_message(AssertionError, \"Fit parameter spam has length 1\",\n                         grid_search.fit, X, y, spam=np.ones(1))\n\n\n@ignore_warnings\ndef test_grid_search_no_score():\n    # Test grid-search on classifier that has no score function.\n    clf = LinearSVC(random_state=0)\n    X, y = make_blobs(random_state=0, centers=2)\n    Cs = [.1, 1, 10]\n    clf_no_score = LinearSVCNoScore(random_state=0)\n    grid_search = GridSearchCV(clf, {'C': Cs}, scoring='accuracy')\n    grid_search.fit(X, y)\n\n    grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs},\n                                        scoring='accuracy')\n    # smoketest grid search\n    grid_search_no_score.fit(X, y)\n\n    # check that best params are equal\n    assert_equal(grid_search_no_score.best_params_, grid_search.best_params_)\n    # check that we can call score and that it gives the correct result\n    assert_equal(grid_search.score(X, y), grid_search_no_score.score(X, y))\n\n    # giving no scoring function raises an error\n    grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs})\n    assert_raise_message(TypeError, \"no scoring\", grid_search_no_score.fit,\n                         [[1]])\n\n\ndef test_grid_search_score_method():\n    X, y = make_classification(n_samples=100, n_classes=2, flip_y=.2,\n                               random_state=0)\n    clf = LinearSVC(random_state=0)\n    grid = {'C': [.1]}\n\n    search_no_scoring = GridSearchCV(clf, grid, scoring=None).fit(X, y)\n    search_accuracy = GridSearchCV(clf, grid, scoring='accuracy').fit(X, y)\n    search_no_score_method_auc = GridSearchCV(LinearSVCNoScore(), grid,\n                                              scoring='roc_auc').fit(X, y)\n    search_auc = GridSearchCV(clf, grid, scoring='roc_auc').fit(X, y)\n\n    # Check warning only occurs in situation where behavior changed:\n    # estimator requires score method to compete with scoring parameter\n    score_no_scoring = search_no_scoring.score(X, y)\n    score_accuracy = search_accuracy.score(X, y)\n    score_no_score_auc = search_no_score_method_auc.score(X, y)\n    score_auc = search_auc.score(X, y)\n\n    # ensure the test is sane\n    assert_true(score_auc < 1.0)\n    assert_true(score_accuracy < 1.0)\n    assert_not_equal(score_auc, score_accuracy)\n\n    assert_almost_equal(score_accuracy, score_no_scoring)\n    assert_almost_equal(score_auc, score_no_score_auc)\n\n\ndef test_grid_search_groups():\n    # Check if ValueError (when groups is None) propagates to GridSearchCV\n    # And also check if groups is correctly passed to the cv object\n    rng = np.random.RandomState(0)\n\n    X, y = make_classification(n_samples=15, n_classes=2, random_state=0)\n    groups = rng.randint(0, 3, 15)\n\n    clf = LinearSVC(random_state=0)\n    grid = {'C': [1]}\n\n    group_cvs = [LeaveOneGroupOut(), LeavePGroupsOut(2), GroupKFold(),\n                 GroupShuffleSplit()]\n    for cv in group_cvs:\n        gs = GridSearchCV(clf, grid, cv=cv)\n        assert_raise_message(ValueError,\n                             \"The 'groups' parameter should not be None.\",\n                             gs.fit, X, y)\n        gs.fit(X, y, groups=groups)\n\n    non_group_cvs = [StratifiedKFold(), StratifiedShuffleSplit()]\n    for cv in non_group_cvs:\n        gs = GridSearchCV(clf, grid, cv=cv)\n        # Should not raise an error\n        gs.fit(X, y)\n\n\ndef test_return_train_score_warn():\n    # Test that warnings are raised. Will be removed in 0.21\n\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n    grid = {'C': [1, 2]}\n\n    estimators = [GridSearchCV(LinearSVC(random_state=0), grid, iid=False),\n                  RandomizedSearchCV(LinearSVC(random_state=0), grid,\n                                     n_iter=2, iid=False)]\n\n    result = {}\n    for estimator in estimators:\n        for val in [True, False, 'warn']:\n            estimator.set_params(return_train_score=val)\n            result[val] = assert_no_warnings(estimator.fit, X, y).cv_results_\n\n    train_keys = ['split0_train_score', 'split1_train_score',\n                  'split2_train_score', 'mean_train_score', 'std_train_score']\n    for key in train_keys:\n        msg = (\n            'You are accessing a training score ({!r}), '\n            'which will not be available by default '\n            'any more in 0.21. If you need training scores, '\n            'please set return_train_score=True').format(key)\n        train_score = assert_warns_message(FutureWarning, msg,\n                                           result['warn'].get, key)\n        assert np.allclose(train_score, result[True][key])\n        assert key not in result[False]\n\n    for key in result['warn']:\n        if key not in train_keys:\n            assert_no_warnings(result['warn'].get, key)\n\n\ndef test_classes__property():\n    # Test that classes_ property matches best_estimator_.classes_\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n    Cs = [.1, 1, 10]\n\n    grid_search = GridSearchCV(LinearSVC(random_state=0), {'C': Cs})\n    grid_search.fit(X, y)\n    assert_array_equal(grid_search.best_estimator_.classes_,\n                       grid_search.classes_)\n\n    # Test that regressors do not have a classes_ attribute\n    grid_search = GridSearchCV(Ridge(), {'alpha': [1.0, 2.0]})\n    grid_search.fit(X, y)\n    assert_false(hasattr(grid_search, 'classes_'))\n\n    # Test that the grid searcher has no classes_ attribute before it's fit\n    grid_search = GridSearchCV(LinearSVC(random_state=0), {'C': Cs})\n    assert_false(hasattr(grid_search, 'classes_'))\n\n    # Test that the grid searcher has no classes_ attribute without a refit\n    grid_search = GridSearchCV(LinearSVC(random_state=0),\n                               {'C': Cs}, refit=False)\n    grid_search.fit(X, y)\n    assert_false(hasattr(grid_search, 'classes_'))\n\n\ndef test_trivial_cv_results_attr():\n    # Test search over a \"grid\" with only one point.\n    # Non-regression test: grid_scores_ wouldn't be set by GridSearchCV.\n    clf = MockClassifier()\n    grid_search = GridSearchCV(clf, {'foo_param': [1]})\n    grid_search.fit(X, y)\n    assert_true(hasattr(grid_search, \"cv_results_\"))\n\n    random_search = RandomizedSearchCV(clf, {'foo_param': [0]}, n_iter=1)\n    random_search.fit(X, y)\n    assert_true(hasattr(grid_search, \"cv_results_\"))\n\n\ndef test_no_refit():\n    # Test that GSCV can be used for model selection alone without refitting\n    clf = MockClassifier()\n    for scoring in [None, ['accuracy', 'precision']]:\n        grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, refit=False)\n        grid_search.fit(X, y)\n        assert_true(not hasattr(grid_search, \"best_estimator_\") and\n                    hasattr(grid_search, \"best_index_\") and\n                    hasattr(grid_search, \"best_params_\"))\n\n        # Make sure the functions predict/transform etc raise meaningful\n        # error messages\n        for fn_name in ('predict', 'predict_proba', 'predict_log_proba',\n                        'transform', 'inverse_transform'):\n            assert_raise_message(NotFittedError,\n                                 ('refit=False. %s is available only after '\n                                  'refitting on the best parameters'\n                                  % fn_name), getattr(grid_search, fn_name), X)\n\n    # Test that an invalid refit param raises appropriate error messages\n    for refit in [\"\", 5, True, 'recall', 'accuracy']:\n        assert_raise_message(ValueError, \"For multi-metric scoring, the \"\n                             \"parameter refit must be set to a scorer key\",\n                             GridSearchCV(clf, {}, refit=refit,\n                                          scoring={'acc': 'accuracy',\n                                                   'prec': 'precision'}).fit,\n                             X, y)\n\n\ndef test_grid_search_error():\n    # Test that grid search will capture errors on data with different length\n    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)\n\n    clf = LinearSVC()\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})\n    assert_raises(ValueError, cv.fit, X_[:180], y_)\n\n\ndef test_grid_search_one_grid_point():\n    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)\n    param_dict = {\"C\": [1.0], \"kernel\": [\"rbf\"], \"gamma\": [0.1]}\n\n    clf = SVC()\n    cv = GridSearchCV(clf, param_dict)\n    cv.fit(X_, y_)\n\n    clf = SVC(C=1.0, kernel=\"rbf\", gamma=0.1)\n    clf.fit(X_, y_)\n\n    assert_array_equal(clf.dual_coef_, cv.best_estimator_.dual_coef_)\n\n\ndef test_grid_search_when_param_grid_includes_range():\n    # Test that the best estimator contains the right value for foo_param\n    clf = MockClassifier()\n    grid_search = None\n    if PY3_OR_LATER:\n        grid_search = GridSearchCV(clf, {'foo_param': range(1, 4)})\n    else:\n        grid_search = GridSearchCV(clf, {'foo_param': xrange(1, 4)})\n    grid_search.fit(X, y)\n    assert_equal(grid_search.best_estimator_.foo_param, 2)\n\n\ndef test_grid_search_bad_param_grid():\n    param_dict = {\"C\": 1.0}\n    clf = SVC()\n    assert_raise_message(\n        ValueError,\n        \"Parameter values for parameter (C) need to be a sequence\"\n        \"(but not a string) or np.ndarray.\",\n        GridSearchCV, clf, param_dict)\n\n    param_dict = {\"C\": []}\n    clf = SVC()\n    assert_raise_message(\n        ValueError,\n        \"Parameter values for parameter (C) need to be a non-empty sequence.\",\n        GridSearchCV, clf, param_dict)\n\n    param_dict = {\"C\": \"1,2,3\"}\n    clf = SVC()\n    assert_raise_message(\n        ValueError,\n        \"Parameter values for parameter (C) need to be a sequence\"\n        \"(but not a string) or np.ndarray.\",\n        GridSearchCV, clf, param_dict)\n\n    param_dict = {\"C\": np.ones(6).reshape(3, 2)}\n    clf = SVC()\n    assert_raises(ValueError, GridSearchCV, clf, param_dict)\n\n\ndef test_grid_search_sparse():\n    # Test that grid search works with both dense and sparse matrices\n    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)\n\n    clf = LinearSVC()\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})\n    cv.fit(X_[:180], y_[:180])\n    y_pred = cv.predict(X_[180:])\n    C = cv.best_estimator_.C\n\n    X_ = sp.csr_matrix(X_)\n    clf = LinearSVC()\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})\n    cv.fit(X_[:180].tocoo(), y_[:180])\n    y_pred2 = cv.predict(X_[180:])\n    C2 = cv.best_estimator_.C\n\n    assert_true(np.mean(y_pred == y_pred2) >= .9)\n    assert_equal(C, C2)\n\n\ndef test_grid_search_sparse_scoring():\n    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)\n\n    clf = LinearSVC()\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]}, scoring=\"f1\")\n    cv.fit(X_[:180], y_[:180])\n    y_pred = cv.predict(X_[180:])\n    C = cv.best_estimator_.C\n\n    X_ = sp.csr_matrix(X_)\n    clf = LinearSVC()\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]}, scoring=\"f1\")\n    cv.fit(X_[:180], y_[:180])\n    y_pred2 = cv.predict(X_[180:])\n    C2 = cv.best_estimator_.C\n\n    assert_array_equal(y_pred, y_pred2)\n    assert_equal(C, C2)\n    # Smoke test the score\n    # np.testing.assert_allclose(f1_score(cv.predict(X_[:180]), y[:180]),\n    #                            cv.score(X_[:180], y[:180]))\n\n    # test loss where greater is worse\n    def f1_loss(y_true_, y_pred_):\n        return -f1_score(y_true_, y_pred_)\n    F1Loss = make_scorer(f1_loss, greater_is_better=False)\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]}, scoring=F1Loss)\n    cv.fit(X_[:180], y_[:180])\n    y_pred3 = cv.predict(X_[180:])\n    C3 = cv.best_estimator_.C\n\n    assert_equal(C, C3)\n    assert_array_equal(y_pred, y_pred3)\n\n\ndef test_grid_search_precomputed_kernel():\n    # Test that grid search works when the input features are given in the\n    # form of a precomputed kernel matrix\n    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)\n\n    # compute the training kernel matrix corresponding to the linear kernel\n    K_train = np.dot(X_[:180], X_[:180].T)\n    y_train = y_[:180]\n\n    clf = SVC(kernel='precomputed')\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})\n    cv.fit(K_train, y_train)\n\n    assert_true(cv.best_score_ >= 0)\n\n    # compute the test kernel matrix\n    K_test = np.dot(X_[180:], X_[:180].T)\n    y_test = y_[180:]\n\n    y_pred = cv.predict(K_test)\n\n    assert_true(np.mean(y_pred == y_test) >= 0)\n\n    # test error is raised when the precomputed kernel is not array-like\n    # or sparse\n    assert_raises(ValueError, cv.fit, K_train.tolist(), y_train)\n\n\ndef test_grid_search_precomputed_kernel_error_nonsquare():\n    # Test that grid search returns an error with a non-square precomputed\n    # training kernel matrix\n    K_train = np.zeros((10, 20))\n    y_train = np.ones((10, ))\n    clf = SVC(kernel='precomputed')\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})\n    assert_raises(ValueError, cv.fit, K_train, y_train)\n\n\nclass BrokenClassifier(BaseEstimator):\n    \"\"\"Broken classifier that cannot be fit twice\"\"\"\n\n    def __init__(self, parameter=None):\n        self.parameter = parameter\n\n    def fit(self, X, y):\n        assert_true(not hasattr(self, 'has_been_fit_'))\n        self.has_been_fit_ = True\n\n    def predict(self, X):\n        return np.zeros(X.shape[0])\n\n\n@ignore_warnings\ndef test_refit():\n    # Regression test for bug in refitting\n    # Simulates re-fitting a broken estimator; this used to break with\n    # sparse SVMs.\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n\n    clf = GridSearchCV(BrokenClassifier(), [{'parameter': [0, 1]}],\n                       scoring=\"precision\", refit=True)\n    clf.fit(X, y)\n\n\ndef test_gridsearch_nd():\n    # Pass X as list in GridSearchCV\n    X_4d = np.arange(10 * 5 * 3 * 2).reshape(10, 5, 3, 2)\n    y_3d = np.arange(10 * 7 * 11).reshape(10, 7, 11)\n    check_X = lambda x: x.shape[1:] == (5, 3, 2)\n    check_y = lambda x: x.shape[1:] == (7, 11)\n    clf = CheckingClassifier(check_X=check_X, check_y=check_y)\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]})\n    grid_search.fit(X_4d, y_3d).score(X, y)\n    assert_true(hasattr(grid_search, \"cv_results_\"))\n\n\ndef test_X_as_list():\n    # Pass X as list in GridSearchCV\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n\n    clf = CheckingClassifier(check_X=lambda x: isinstance(x, list))\n    cv = KFold(n_splits=3)\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, cv=cv)\n    grid_search.fit(X.tolist(), y).score(X, y)\n    assert_true(hasattr(grid_search, \"cv_results_\"))\n\n\ndef test_y_as_list():\n    # Pass y as list in GridSearchCV\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n\n    clf = CheckingClassifier(check_y=lambda x: isinstance(x, list))\n    cv = KFold(n_splits=3)\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, cv=cv)\n    grid_search.fit(X, y.tolist()).score(X, y)\n    assert_true(hasattr(grid_search, \"cv_results_\"))\n\n\n@ignore_warnings\ndef test_pandas_input():\n    # check cross_val_score doesn't destroy pandas dataframe\n    types = [(MockDataFrame, MockDataFrame)]\n    try:\n        from pandas import Series, DataFrame\n        types.append((DataFrame, Series))\n    except ImportError:\n        pass\n\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n\n    for InputFeatureType, TargetType in types:\n        # X dataframe, y series\n        X_df, y_ser = InputFeatureType(X), TargetType(y)\n\n        def check_df(x):\n            return isinstance(x, InputFeatureType)\n\n        def check_series(x):\n            return isinstance(x, TargetType)\n\n        clf = CheckingClassifier(check_X=check_df, check_y=check_series)\n\n        grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]})\n        grid_search.fit(X_df, y_ser).score(X_df, y_ser)\n        grid_search.predict(X_df)\n        assert_true(hasattr(grid_search, \"cv_results_\"))\n\n\ndef test_unsupervised_grid_search():\n    # test grid-search with unsupervised estimator\n    X, y = make_blobs(random_state=0)\n    km = KMeans(random_state=0)\n\n    # Multi-metric evaluation unsupervised\n    scoring = ['adjusted_rand_score', 'fowlkes_mallows_score']\n    for refit in ['adjusted_rand_score', 'fowlkes_mallows_score']:\n        grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4]),\n                                   scoring=scoring, refit=refit)\n        grid_search.fit(X, y)\n        # Both ARI and FMS can find the right number :)\n        assert_equal(grid_search.best_params_[\"n_clusters\"], 3)\n\n    # Single metric evaluation unsupervised\n    grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4]),\n                               scoring='fowlkes_mallows_score')\n    grid_search.fit(X, y)\n    assert_equal(grid_search.best_params_[\"n_clusters\"], 3)\n\n    # Now without a score, and without y\n    grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4]))\n    grid_search.fit(X)\n    assert_equal(grid_search.best_params_[\"n_clusters\"], 4)\n\n\ndef test_gridsearch_no_predict():\n    # test grid-search with an estimator without predict.\n    # slight duplication of a test from KDE\n    def custom_scoring(estimator, X):\n        return 42 if estimator.bandwidth == .1 else 0\n    X, _ = make_blobs(cluster_std=.1, random_state=1,\n                      centers=[[0, 1], [1, 0], [0, 0]])\n    search = GridSearchCV(KernelDensity(),\n                          param_grid=dict(bandwidth=[.01, .1, 1]),\n                          scoring=custom_scoring)\n    search.fit(X)\n    assert_equal(search.best_params_['bandwidth'], .1)\n    assert_equal(search.best_score_, 42)\n\n\ndef test_param_sampler():\n    # test basic properties of param sampler\n    param_distributions = {\"kernel\": [\"rbf\", \"linear\"],\n                           \"C\": uniform(0, 1)}\n    sampler = ParameterSampler(param_distributions=param_distributions,\n                               n_iter=10, random_state=0)\n    samples = [x for x in sampler]\n    assert_equal(len(samples), 10)\n    for sample in samples:\n        assert_true(sample[\"kernel\"] in [\"rbf\", \"linear\"])\n        assert_true(0 <= sample[\"C\"] <= 1)\n\n    # test that repeated calls yield identical parameters\n    param_distributions = {\"C\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n    sampler = ParameterSampler(param_distributions=param_distributions,\n                               n_iter=3, random_state=0)\n    assert_equal([x for x in sampler], [x for x in sampler])\n\n    if sp_version >= (0, 16):\n        param_distributions = {\"C\": uniform(0, 1)}\n        sampler = ParameterSampler(param_distributions=param_distributions,\n                                   n_iter=10, random_state=0)\n        assert_equal([x for x in sampler], [x for x in sampler])\n\n\ndef check_cv_results_array_types(search, param_keys, score_keys):\n    # Check if the search `cv_results`'s array are of correct types\n    cv_results = search.cv_results_\n    assert_true(all(isinstance(cv_results[param], np.ma.MaskedArray)\n                    for param in param_keys))\n    assert_true(all(cv_results[key].dtype == object for key in param_keys))\n    assert_false(any(isinstance(cv_results[key], np.ma.MaskedArray)\n                     for key in score_keys))\n    assert_true(all(cv_results[key].dtype == np.float64\n                    for key in score_keys if not key.startswith('rank')))\n\n    scorer_keys = search.scorer_.keys() if search.multimetric_ else ['score']\n\n    for key in scorer_keys:\n        assert_true(cv_results['rank_test_%s' % key].dtype == np.int32)\n\n\ndef check_cv_results_keys(cv_results, param_keys, score_keys, n_cand):\n    # Test the search.cv_results_ contains all the required results\n    assert_array_equal(sorted(cv_results.keys()),\n                       sorted(param_keys + score_keys + ('params',)))\n    assert_true(all(cv_results[key].shape == (n_cand,)\n                    for key in param_keys + score_keys))\n\n\ndef check_cv_results_grid_scores_consistency(search):\n    # TODO Remove test in 0.20\n    if search.multimetric_:\n        assert_raise_message(AttributeError, \"not available for multi-metric\",\n                             getattr, search, 'grid_scores_')\n    else:\n        cv_results = search.cv_results_\n        res_scores = np.vstack(list([cv_results[\"split%d_test_score\" % i]\n                                     for i in range(search.n_splits_)])).T\n        res_means = cv_results[\"mean_test_score\"]\n        res_params = cv_results[\"params\"]\n        n_cand = len(res_params)\n        grid_scores = assert_warns(DeprecationWarning, getattr,\n                                   search, 'grid_scores_')\n        assert_equal(len(grid_scores), n_cand)\n        # Check consistency of the structure of grid_scores\n        for i in range(n_cand):\n            assert_equal(grid_scores[i].parameters, res_params[i])\n            assert_array_equal(grid_scores[i].cv_validation_scores,\n                               res_scores[i, :])\n            assert_array_equal(grid_scores[i].mean_validation_score,\n                               res_means[i])\n\n\ndef test_grid_search_cv_results():\n    X, y = make_classification(n_samples=50, n_features=4,\n                               random_state=42)\n\n    n_splits = 3\n    n_grid_points = 6\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n\n    param_keys = ('param_C', 'param_degree', 'param_gamma', 'param_kernel')\n    score_keys = ('mean_test_score', 'mean_train_score',\n                  'rank_test_score',\n                  'split0_test_score', 'split1_test_score',\n                  'split2_test_score',\n                  'split0_train_score', 'split1_train_score',\n                  'split2_train_score',\n                  'std_test_score', 'std_train_score',\n                  'mean_fit_time', 'std_fit_time',\n                  'mean_score_time', 'std_score_time')\n    n_candidates = n_grid_points\n\n    for iid in (False, True):\n        search = GridSearchCV(SVC(), cv=n_splits, iid=iid, param_grid=params)\n        search.fit(X, y)\n        assert_equal(iid, search.iid)\n        cv_results = search.cv_results_\n        # Check if score and timing are reasonable\n        assert_true(all(cv_results['rank_test_score'] >= 1))\n        assert_true(all(cv_results[k] >= 0) for k in score_keys\n                    if k is not 'rank_test_score')\n        assert_true(all(cv_results[k] <= 1) for k in score_keys\n                    if 'time' not in k and\n                    k is not 'rank_test_score')\n        # Check cv_results structure\n        check_cv_results_array_types(search, param_keys, score_keys)\n        check_cv_results_keys(cv_results, param_keys, score_keys, n_candidates)\n        # Check masking\n        cv_results = search.cv_results_\n        n_candidates = len(search.cv_results_['params'])\n        assert_true(all((cv_results['param_C'].mask[i] and\n                         cv_results['param_gamma'].mask[i] and\n                         not cv_results['param_degree'].mask[i])\n                        for i in range(n_candidates)\n                        if cv_results['param_kernel'][i] == 'linear'))\n        assert_true(all((not cv_results['param_C'].mask[i] and\n                         not cv_results['param_gamma'].mask[i] and\n                         cv_results['param_degree'].mask[i])\n                        for i in range(n_candidates)\n                        if cv_results['param_kernel'][i] == 'rbf'))\n        check_cv_results_grid_scores_consistency(search)\n\n\ndef test_random_search_cv_results():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    n_splits = 3\n    n_search_iter = 30\n\n    params = dict(C=expon(scale=10), gamma=expon(scale=0.1))\n    param_keys = ('param_C', 'param_gamma')\n    score_keys = ('mean_test_score', 'mean_train_score',\n                  'rank_test_score',\n                  'split0_test_score', 'split1_test_score',\n                  'split2_test_score',\n                  'split0_train_score', 'split1_train_score',\n                  'split2_train_score',\n                  'std_test_score', 'std_train_score',\n                  'mean_fit_time', 'std_fit_time',\n                  'mean_score_time', 'std_score_time')\n    n_cand = n_search_iter\n\n    for iid in (False, True):\n        search = RandomizedSearchCV(SVC(), n_iter=n_search_iter, cv=n_splits,\n                                    iid=iid, param_distributions=params)\n        search.fit(X, y)\n        assert_equal(iid, search.iid)\n        cv_results = search.cv_results_\n        # Check results structure\n        check_cv_results_array_types(search, param_keys, score_keys)\n        check_cv_results_keys(cv_results, param_keys, score_keys, n_cand)\n        # For random_search, all the param array vals should be unmasked\n        assert_false(any(cv_results['param_C'].mask) or\n                     any(cv_results['param_gamma'].mask))\n        check_cv_results_grid_scores_consistency(search)\n\n\n@ignore_warnings(category=DeprecationWarning)\ndef test_search_iid_param():\n    # Test the IID parameter\n    # noise-free simple 2d-data\n    X, y = make_blobs(centers=[[0, 0], [1, 0], [0, 1], [1, 1]], random_state=0,\n                      cluster_std=0.1, shuffle=False, n_samples=80)\n    # split dataset into two folds that are not iid\n    # first one contains data of all 4 blobs, second only from two.\n    mask = np.ones(X.shape[0], dtype=np.bool)\n    mask[np.where(y == 1)[0][::2]] = 0\n    mask[np.where(y == 2)[0][::2]] = 0\n    # this leads to perfect classification on one fold and a score of 1/3 on\n    # the other\n    # create \"cv\" for splits\n    cv = [[mask, ~mask], [~mask, mask]]\n    # once with iid=True (default)\n    grid_search = GridSearchCV(SVC(), param_grid={'C': [1, 10]}, cv=cv)\n    random_search = RandomizedSearchCV(SVC(), n_iter=2,\n                                       param_distributions={'C': [1, 10]},\n                                       cv=cv)\n    for search in (grid_search, random_search):\n        search.fit(X, y)\n        assert_true(search.iid or search.iid is None)\n\n        test_cv_scores = np.array(list(search.cv_results_['split%d_test_score'\n                                                          % s_i][0]\n                                       for s_i in range(search.n_splits_)))\n        test_mean = search.cv_results_['mean_test_score'][0]\n        test_std = search.cv_results_['std_test_score'][0]\n\n        train_cv_scores = np.array(list(search.cv_results_['split%d_train_'\n                                                           'score' % s_i][0]\n                                        for s_i in range(search.n_splits_)))\n        train_mean = search.cv_results_['mean_train_score'][0]\n        train_std = search.cv_results_['std_train_score'][0]\n\n        # Test the first candidate\n        assert_equal(search.cv_results_['param_C'][0], 1)\n        assert_array_almost_equal(test_cv_scores, [1, 1. / 3.])\n        assert_array_almost_equal(train_cv_scores, [1, 1])\n\n        # for first split, 1/4 of dataset is in test, for second 3/4.\n        # take weighted average and weighted std\n        expected_test_mean = 1 * 1. / 4. + 1. / 3. * 3. / 4.\n        expected_test_std = np.sqrt(1. / 4 * (expected_test_mean - 1) ** 2 +\n                                    3. / 4 * (expected_test_mean - 1. / 3.) **\n                                    2)\n        assert_almost_equal(test_mean, expected_test_mean)\n        assert_almost_equal(test_std, expected_test_std)\n        assert_array_almost_equal(test_cv_scores,\n                                  cross_val_score(SVC(C=1), X, y, cv=cv))\n\n        # For the train scores, we do not take a weighted mean irrespective of\n        # i.i.d. or not\n        assert_almost_equal(train_mean, 1)\n        assert_almost_equal(train_std, 0)\n\n    # once with iid=False\n    grid_search = GridSearchCV(SVC(),\n                               param_grid={'C': [1, 10]},\n                               cv=cv, iid=False)\n    random_search = RandomizedSearchCV(SVC(), n_iter=2,\n                                       param_distributions={'C': [1, 10]},\n                                       cv=cv, iid=False)\n\n    for search in (grid_search, random_search):\n        search.fit(X, y)\n        assert_false(search.iid)\n\n        test_cv_scores = np.array(list(search.cv_results_['split%d_test_score'\n                                                          % s][0]\n                                       for s in range(search.n_splits_)))\n        test_mean = search.cv_results_['mean_test_score'][0]\n        test_std = search.cv_results_['std_test_score'][0]\n\n        train_cv_scores = np.array(list(search.cv_results_['split%d_train_'\n                                                           'score' % s][0]\n                                        for s in range(search.n_splits_)))\n        train_mean = search.cv_results_['mean_train_score'][0]\n        train_std = search.cv_results_['std_train_score'][0]\n\n        assert_equal(search.cv_results_['param_C'][0], 1)\n        # scores are the same as above\n        assert_array_almost_equal(test_cv_scores, [1, 1. / 3.])\n        # Unweighted mean/std is used\n        assert_almost_equal(test_mean, np.mean(test_cv_scores))\n        assert_almost_equal(test_std, np.std(test_cv_scores))\n\n        # For the train scores, we do not take a weighted mean irrespective of\n        # i.i.d. or not\n        assert_almost_equal(train_mean, 1)\n        assert_almost_equal(train_std, 0)\n\n\ndef test_grid_search_cv_results_multimetric():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    n_splits = 3\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n\n    for iid in (False, True):\n        grid_searches = []\n        for scoring in ({'accuracy': make_scorer(accuracy_score),\n                         'recall': make_scorer(recall_score)},\n                        'accuracy', 'recall'):\n            grid_search = GridSearchCV(SVC(), cv=n_splits, iid=iid,\n                                       param_grid=params, scoring=scoring,\n                                       refit=False)\n            grid_search.fit(X, y)\n            assert_equal(grid_search.iid, iid)\n            grid_searches.append(grid_search)\n\n        compare_cv_results_multimetric_with_single(*grid_searches, iid=iid)\n\n\ndef test_random_search_cv_results_multimetric():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    n_splits = 3\n    n_search_iter = 30\n    scoring = ('accuracy', 'recall')\n\n    # Scipy 0.12's stats dists do not accept seed, hence we use param grid\n    params = dict(C=np.logspace(-10, 1), gamma=np.logspace(-5, 0, base=0.1))\n    for iid in (True, False):\n        for refit in (True, False):\n            random_searches = []\n            for scoring in (('accuracy', 'recall'), 'accuracy', 'recall'):\n                # If True, for multi-metric pass refit='accuracy'\n                if refit:\n                    refit = 'accuracy' if isinstance(scoring, tuple) else refit\n                clf = SVC(probability=True, random_state=42)\n                random_search = RandomizedSearchCV(clf, n_iter=n_search_iter,\n                                                   cv=n_splits, iid=iid,\n                                                   param_distributions=params,\n                                                   scoring=scoring,\n                                                   refit=refit, random_state=0)\n                random_search.fit(X, y)\n                random_searches.append(random_search)\n\n            compare_cv_results_multimetric_with_single(*random_searches,\n                                                       iid=iid)\n            if refit:\n                compare_refit_methods_when_refit_with_acc(\n                    random_searches[0], random_searches[1], refit)\n\n\ndef compare_cv_results_multimetric_with_single(\n        search_multi, search_acc, search_rec, iid):\n    \"\"\"Compare multi-metric cv_results with the ensemble of multiple\n    single metric cv_results from single metric grid/random search\"\"\"\n\n    assert_equal(search_multi.iid, iid)\n    assert_true(search_multi.multimetric_)\n    assert_array_equal(sorted(search_multi.scorer_),\n                       ('accuracy', 'recall'))\n\n    cv_results_multi = search_multi.cv_results_\n    cv_results_acc_rec = {re.sub('_score$', '_accuracy', k): v\n                          for k, v in search_acc.cv_results_.items()}\n    cv_results_acc_rec.update({re.sub('_score$', '_recall', k): v\n                               for k, v in search_rec.cv_results_.items()})\n\n    # Check if score and timing are reasonable, also checks if the keys\n    # are present\n    assert_true(all((np.all(cv_results_multi[k] <= 1) for k in (\n                    'mean_score_time', 'std_score_time', 'mean_fit_time',\n                    'std_fit_time'))))\n\n    # Compare the keys, other than time keys, among multi-metric and\n    # single metric grid search results. np.testing.assert_equal performs a\n    # deep nested comparison of the two cv_results dicts\n    np.testing.assert_equal({k: v for k, v in cv_results_multi.items()\n                             if not k.endswith('_time')},\n                            {k: v for k, v in cv_results_acc_rec.items()\n                             if not k.endswith('_time')})\n\n\ndef compare_refit_methods_when_refit_with_acc(search_multi, search_acc, refit):\n    \"\"\"Compare refit multi-metric search methods with single metric methods\"\"\"\n    if refit:\n        assert_equal(search_multi.refit, 'accuracy')\n    else:\n        assert_false(search_multi.refit)\n    assert_equal(search_acc.refit, refit)\n\n    X, y = make_blobs(n_samples=100, n_features=4, random_state=42)\n    for method in ('predict', 'predict_proba', 'predict_log_proba'):\n        assert_almost_equal(getattr(search_multi, method)(X),\n                            getattr(search_acc, method)(X))\n    assert_almost_equal(search_multi.score(X, y), search_acc.score(X, y))\n    for key in ('best_index_', 'best_score_', 'best_params_'):\n        assert_equal(getattr(search_multi, key), getattr(search_acc, key))\n\n\ndef test_search_cv_results_rank_tie_breaking():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(), param_grid=param_grid)\n    random_search = RandomizedSearchCV(SVC(), n_iter=3,\n                                       param_distributions=param_grid)\n\n    for search in (grid_search, random_search):\n        search.fit(X, y)\n        cv_results = search.cv_results_\n        # Check tie breaking strategy -\n        # Check that there is a tie in the mean scores between\n        # candidates 1 and 2 alone\n        assert_almost_equal(cv_results['mean_test_score'][0],\n                            cv_results['mean_test_score'][1])\n        assert_almost_equal(cv_results['mean_train_score'][0],\n                            cv_results['mean_train_score'][1])\n        assert_false(np.allclose(cv_results['mean_test_score'][1],\n                                 cv_results['mean_test_score'][2]))\n        assert_false(np.allclose(cv_results['mean_train_score'][1],\n                                 cv_results['mean_train_score'][2]))\n        # 'min' rank should be assigned to the tied candidates\n        assert_almost_equal(search.cv_results_['rank_test_score'], [1, 1, 3])\n\n\ndef test_search_cv_results_none_param():\n    X, y = [[1], [2], [3], [4], [5]], [0, 0, 0, 0, 1]\n    estimators = (DecisionTreeRegressor(), DecisionTreeClassifier())\n    est_parameters = {\"random_state\": [0, None]}\n    cv = KFold(random_state=0)\n\n    for est in estimators:\n        grid_search = GridSearchCV(est, est_parameters, cv=cv).fit(X, y)\n        assert_array_equal(grid_search.cv_results_['param_random_state'],\n                           [0, None])\n\n\n@ignore_warnings()\ndef test_search_cv_timing():\n    svc = LinearSVC(random_state=0)\n\n    X = [[1, ], [2, ], [3, ], [4, ]]\n    y = [0, 1, 1, 0]\n\n    gs = GridSearchCV(svc, {'C': [0, 1]}, cv=2, error_score=0)\n    rs = RandomizedSearchCV(svc, {'C': [0, 1]}, cv=2, error_score=0, n_iter=2)\n\n    for search in (gs, rs):\n        search.fit(X, y)\n        for key in ['mean_fit_time', 'std_fit_time']:\n            # NOTE The precision of time.time in windows is not high\n            # enough for the fit/score times to be non-zero for trivial X and y\n            assert_true(np.all(search.cv_results_[key] >= 0))\n            assert_true(np.all(search.cv_results_[key] < 1))\n\n        for key in ['mean_score_time', 'std_score_time']:\n            assert_true(search.cv_results_[key][1] >= 0)\n            assert_true(search.cv_results_[key][0] == 0.0)\n            assert_true(np.all(search.cv_results_[key] < 1))\n\n\ndef test_grid_search_correct_score_results():\n    # test that correct scores are used\n    n_splits = 3\n    clf = LinearSVC(random_state=0)\n    X, y = make_blobs(random_state=0, centers=2)\n    Cs = [.1, 1, 10]\n    for score in ['f1', 'roc_auc']:\n        grid_search = GridSearchCV(clf, {'C': Cs}, scoring=score, cv=n_splits)\n        cv_results = grid_search.fit(X, y).cv_results_\n\n        # Test scorer names\n        result_keys = list(cv_results.keys())\n        expected_keys = ((\"mean_test_score\", \"rank_test_score\") +\n                         tuple(\"split%d_test_score\" % cv_i\n                               for cv_i in range(n_splits)))\n        assert_true(all(np.in1d(expected_keys, result_keys)))\n\n        cv = StratifiedKFold(n_splits=n_splits)\n        n_splits = grid_search.n_splits_\n        for candidate_i, C in enumerate(Cs):\n            clf.set_params(C=C)\n            cv_scores = np.array(\n                list(grid_search.cv_results_['split%d_test_score'\n                                             % s][candidate_i]\n                     for s in range(n_splits)))\n            for i, (train, test) in enumerate(cv.split(X, y)):\n                clf.fit(X[train], y[train])\n                if score == \"f1\":\n                    correct_score = f1_score(y[test], clf.predict(X[test]))\n                elif score == \"roc_auc\":\n                    dec = clf.decision_function(X[test])\n                    correct_score = roc_auc_score(y[test], dec)\n                assert_almost_equal(correct_score, cv_scores[i])\n\n\ndef test_fit_grid_point():\n    X, y = make_classification(random_state=0)\n    cv = StratifiedKFold(random_state=0)\n    svc = LinearSVC(random_state=0)\n    scorer = make_scorer(accuracy_score)\n\n    for params in ({'C': 0.1}, {'C': 0.01}, {'C': 0.001}):\n        for train, test in cv.split(X, y):\n            this_scores, this_params, n_test_samples = fit_grid_point(\n                X, y, clone(svc), params, train, test,\n                scorer, verbose=False)\n\n            est = clone(svc).set_params(**params)\n            est.fit(X[train], y[train])\n            expected_score = scorer(est, X[test], y[test])\n\n            # Test the return values of fit_grid_point\n            assert_almost_equal(this_scores, expected_score)\n            assert_equal(params, this_params)\n            assert_equal(n_test_samples, test.size)\n\n    # Should raise an error upon multimetric scorer\n    assert_raise_message(ValueError, \"scoring value should either be a \"\n                         \"callable, string or None.\", fit_grid_point, X, y,\n                         svc, params, train, test, {'score': scorer},\n                         verbose=True)\n\n\ndef test_pickle():\n    # Test that a fit search can be pickled\n    clf = MockClassifier()\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, refit=True)\n    grid_search.fit(X, y)\n    grid_search_pickled = pickle.loads(pickle.dumps(grid_search))\n    assert_array_almost_equal(grid_search.predict(X),\n                              grid_search_pickled.predict(X))\n\n    random_search = RandomizedSearchCV(clf, {'foo_param': [1, 2, 3]},\n                                       refit=True, n_iter=3)\n    random_search.fit(X, y)\n    random_search_pickled = pickle.loads(pickle.dumps(random_search))\n    assert_array_almost_equal(random_search.predict(X),\n                              random_search_pickled.predict(X))\n\n\ndef test_grid_search_with_multioutput_data():\n    # Test search with multi-output estimator\n\n    X, y = make_multilabel_classification(return_indicator=True,\n                                          random_state=0)\n\n    est_parameters = {\"max_depth\": [1, 2, 3, 4]}\n    cv = KFold(random_state=0)\n\n    estimators = [DecisionTreeRegressor(random_state=0),\n                  DecisionTreeClassifier(random_state=0)]\n\n    # Test with grid search cv\n    for est in estimators:\n        grid_search = GridSearchCV(est, est_parameters, cv=cv)\n        grid_search.fit(X, y)\n        res_params = grid_search.cv_results_['params']\n        for cand_i in range(len(res_params)):\n            est.set_params(**res_params[cand_i])\n\n            for i, (train, test) in enumerate(cv.split(X, y)):\n                est.fit(X[train], y[train])\n                correct_score = est.score(X[test], y[test])\n                assert_almost_equal(\n                    correct_score,\n                    grid_search.cv_results_['split%d_test_score' % i][cand_i])\n\n    # Test with a randomized search\n    for est in estimators:\n        random_search = RandomizedSearchCV(est, est_parameters,\n                                           cv=cv, n_iter=3)\n        random_search.fit(X, y)\n        res_params = random_search.cv_results_['params']\n        for cand_i in range(len(res_params)):\n            est.set_params(**res_params[cand_i])\n\n            for i, (train, test) in enumerate(cv.split(X, y)):\n                est.fit(X[train], y[train])\n                correct_score = est.score(X[test], y[test])\n                assert_almost_equal(\n                    correct_score,\n                    random_search.cv_results_['split%d_test_score'\n                                              % i][cand_i])\n\n\ndef test_predict_proba_disabled():\n    # Test predict_proba when disabled on estimator.\n    X = np.arange(20).reshape(5, -1)\n    y = [0, 0, 1, 1, 1]\n    clf = SVC(probability=False)\n    gs = GridSearchCV(clf, {}, cv=2).fit(X, y)\n    assert_false(hasattr(gs, \"predict_proba\"))\n\n\ndef test_grid_search_allows_nans():\n    # Test GridSearchCV with SimpleImputer\n    X = np.arange(20, dtype=np.float64).reshape(5, -1)\n    X[2, :] = np.nan\n    y = [0, 0, 1, 1, 1]\n    p = Pipeline([\n        ('imputer', SimpleImputer(strategy='mean', missing_values='NaN')),\n        ('classifier', MockClassifier()),\n    ])\n    GridSearchCV(p, {'classifier__foo_param': [1, 2, 3]}, cv=2).fit(X, y)\n\n\nclass FailingClassifier(BaseEstimator):\n    \"\"\"Classifier that raises a ValueError on fit()\"\"\"\n\n    FAILING_PARAMETER = 2\n\n    def __init__(self, parameter=None):\n        self.parameter = parameter\n\n    def fit(self, X, y=None):\n        if self.parameter == FailingClassifier.FAILING_PARAMETER:\n            raise ValueError(\"Failing classifier failed as required\")\n\n    def predict(self, X):\n        return np.zeros(X.shape[0])\n\n\ndef test_grid_search_failing_classifier():\n    # GridSearchCV with on_error != 'raise'\n    # Ensures that a warning is raised and score reset where appropriate.\n\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n\n    clf = FailingClassifier()\n\n    # refit=False because we only want to check that errors caused by fits\n    # to individual folds will be caught and warnings raised instead. If\n    # refit was done, then an exception would be raised on refit and not\n    # caught by grid_search (expected behavior), and this would cause an\n    # error in this test.\n    gs = GridSearchCV(clf, [{'parameter': [0, 1, 2]}], scoring='accuracy',\n                      refit=False, error_score=0.0)\n    assert_warns(FitFailedWarning, gs.fit, X, y)\n    n_candidates = len(gs.cv_results_['params'])\n\n    # Ensure that grid scores were set to zero as required for those fits\n    # that are expected to fail.\n    def get_cand_scores(i):\n        return np.array(list(gs.cv_results_['split%d_test_score' % s][i]\n                             for s in range(gs.n_splits_)))\n\n    assert all((np.all(get_cand_scores(cand_i) == 0.0)\n                for cand_i in range(n_candidates)\n                if gs.cv_results_['param_parameter'][cand_i] ==\n                FailingClassifier.FAILING_PARAMETER))\n\n    gs = GridSearchCV(clf, [{'parameter': [0, 1, 2]}], scoring='accuracy',\n                      refit=False, error_score=float('nan'))\n    assert_warns(FitFailedWarning, gs.fit, X, y)\n    n_candidates = len(gs.cv_results_['params'])\n    assert all(np.all(np.isnan(get_cand_scores(cand_i)))\n               for cand_i in range(n_candidates)\n               if gs.cv_results_['param_parameter'][cand_i] ==\n               FailingClassifier.FAILING_PARAMETER)\n\n\ndef test_grid_search_failing_classifier_raise():\n    # GridSearchCV with on_error == 'raise' raises the error\n\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n\n    clf = FailingClassifier()\n\n    # refit=False because we want to test the behaviour of the grid search part\n    gs = GridSearchCV(clf, [{'parameter': [0, 1, 2]}], scoring='accuracy',\n                      refit=False, error_score='raise')\n\n    # FailingClassifier issues a ValueError so this is what we look for.\n    assert_raises(ValueError, gs.fit, X, y)\n\n\ndef test_parameters_sampler_replacement():\n    # raise error if n_iter too large\n    params = {'first': [0, 1], 'second': ['a', 'b', 'c']}\n    sampler = ParameterSampler(params, n_iter=7)\n    assert_raises(ValueError, list, sampler)\n    # degenerates to GridSearchCV if n_iter the same as grid_size\n    sampler = ParameterSampler(params, n_iter=6)\n    samples = list(sampler)\n    assert_equal(len(samples), 6)\n    for values in ParameterGrid(params):\n        assert_true(values in samples)\n\n    # test sampling without replacement in a large grid\n    params = {'a': range(10), 'b': range(10), 'c': range(10)}\n    sampler = ParameterSampler(params, n_iter=99, random_state=42)\n    samples = list(sampler)\n    assert_equal(len(samples), 99)\n    hashable_samples = [\"a%db%dc%d\" % (p['a'], p['b'], p['c'])\n                        for p in samples]\n    assert_equal(len(set(hashable_samples)), 99)\n\n    # doesn't go into infinite loops\n    params_distribution = {'first': bernoulli(.5), 'second': ['a', 'b', 'c']}\n    sampler = ParameterSampler(params_distribution, n_iter=7)\n    samples = list(sampler)\n    assert_equal(len(samples), 7)\n\n\ndef test_stochastic_gradient_loss_param():\n    # Make sure the predict_proba works when loss is specified\n    # as one of the parameters in the param_grid.\n    param_grid = {\n        'loss': ['log'],\n    }\n    X = np.arange(24).reshape(6, -1)\n    y = [0, 0, 0, 1, 1, 1]\n    clf = GridSearchCV(estimator=SGDClassifier(tol=1e-3, loss='hinge'),\n                       param_grid=param_grid)\n\n    # When the estimator is not fitted, `predict_proba` is not available as the\n    # loss is 'hinge'.\n    assert_false(hasattr(clf, \"predict_proba\"))\n    clf.fit(X, y)\n    clf.predict_proba(X)\n    clf.predict_log_proba(X)\n\n    # Make sure `predict_proba` is not available when setting loss=['hinge']\n    # in param_grid\n    param_grid = {\n        'loss': ['hinge'],\n    }\n    clf = GridSearchCV(estimator=SGDClassifier(tol=1e-3, loss='hinge'),\n                       param_grid=param_grid)\n    assert_false(hasattr(clf, \"predict_proba\"))\n    clf.fit(X, y)\n    assert_false(hasattr(clf, \"predict_proba\"))\n\n\ndef test_search_train_scores_set_to_false():\n    X = np.arange(6).reshape(6, -1)\n    y = [0, 0, 0, 1, 1, 1]\n    clf = LinearSVC(random_state=0)\n\n    gs = GridSearchCV(clf, param_grid={'C': [0.1, 0.2]},\n                      return_train_score=False)\n    gs.fit(X, y)\n\n\ndef test_grid_search_cv_splits_consistency():\n    # Check if a one time iterable is accepted as a cv parameter.\n    n_samples = 100\n    n_splits = 5\n    X, y = make_classification(n_samples=n_samples, random_state=0)\n\n    gs = GridSearchCV(LinearSVC(random_state=0),\n                      param_grid={'C': [0.1, 0.2, 0.3]},\n                      cv=OneTimeSplitter(n_splits=n_splits,\n                                         n_samples=n_samples))\n    gs.fit(X, y)\n\n    gs2 = GridSearchCV(LinearSVC(random_state=0),\n                       param_grid={'C': [0.1, 0.2, 0.3]},\n                       cv=KFold(n_splits=n_splits))\n    gs2.fit(X, y)\n\n    # Give generator as a cv parameter\n    assert_true(isinstance(KFold(n_splits=n_splits,\n                                 shuffle=True, random_state=0).split(X, y),\n                           GeneratorType))\n    gs3 = GridSearchCV(LinearSVC(random_state=0),\n                       param_grid={'C': [0.1, 0.2, 0.3]},\n                       cv=KFold(n_splits=n_splits, shuffle=True,\n                                random_state=0).split(X, y))\n    gs3.fit(X, y)\n\n    gs4 = GridSearchCV(LinearSVC(random_state=0),\n                       param_grid={'C': [0.1, 0.2, 0.3]},\n                       cv=KFold(n_splits=n_splits, shuffle=True,\n                                random_state=0))\n    gs4.fit(X, y)\n\n    def _pop_time_keys(cv_results):\n        for key in ('mean_fit_time', 'std_fit_time',\n                    'mean_score_time', 'std_score_time'):\n            cv_results.pop(key)\n        return cv_results\n\n    # Check if generators are supported as cv and\n    # that the splits are consistent\n    np.testing.assert_equal(_pop_time_keys(gs3.cv_results_),\n                            _pop_time_keys(gs4.cv_results_))\n\n    # OneTimeSplitter is a non-re-entrant cv where split can be called only\n    # once if ``cv.split`` is called once per param setting in GridSearchCV.fit\n    # the 2nd and 3rd parameter will not be evaluated as no train/test indices\n    # will be generated for the 2nd and subsequent cv.split calls.\n    # This is a check to make sure cv.split is not called once per param\n    # setting.\n    np.testing.assert_equal({k: v for k, v in gs.cv_results_.items()\n                             if not k.endswith('_time')},\n                            {k: v for k, v in gs2.cv_results_.items()\n                             if not k.endswith('_time')})\n\n    # Check consistency of folds across the parameters\n    gs = GridSearchCV(LinearSVC(random_state=0),\n                      param_grid={'C': [0.1, 0.1, 0.2, 0.2]},\n                      cv=KFold(n_splits=n_splits, shuffle=True))\n    gs.fit(X, y)\n\n    # As the first two param settings (C=0.1) and the next two param\n    # settings (C=0.2) are same, the test and train scores must also be\n    # same as long as the same train/test indices are generated for all\n    # the cv splits, for both param setting\n    for score_type in ('train', 'test'):\n        per_param_scores = {}\n        for param_i in range(4):\n            per_param_scores[param_i] = list(\n                gs.cv_results_['split%d_%s_score' % (s, score_type)][param_i]\n                for s in range(5))\n\n        assert_array_almost_equal(per_param_scores[0],\n                                  per_param_scores[1])\n        assert_array_almost_equal(per_param_scores[2],\n                                  per_param_scores[3])\n\n\ndef test_transform_inverse_transform_round_trip():\n    clf = MockClassifier()\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, verbose=3)\n\n    grid_search.fit(X, y)\n    X_round_trip = grid_search.inverse_transform(grid_search.transform(X))\n    assert_array_equal(X, X_round_trip)\n\n\ndef test_deprecated_grid_search_iid():\n    depr_message = (\"The default of the `iid` parameter will change from True \"\n                    \"to False in version 0.22\")\n    X, y = make_blobs(n_samples=54, random_state=0, centers=2)\n    grid = GridSearchCV(SVC(), param_grid={'C': [1]}, cv=3)\n    # no warning with equally sized test sets\n    assert_no_warnings(grid.fit, X, y)\n\n    grid = GridSearchCV(SVC(), param_grid={'C': [1]}, cv=5)\n    # warning because 54 % 5 != 0\n    assert_warns_message(DeprecationWarning, depr_message, grid.fit, X, y)\n\n    grid = GridSearchCV(SVC(), param_grid={'C': [1]}, cv=2)\n    # warning because stratification into two classes and 27 % 2 != 0\n    assert_warns_message(DeprecationWarning, depr_message, grid.fit, X, y)\n\n    grid = GridSearchCV(SVC(), param_grid={'C': [1]}, cv=KFold(2))\n    # no warning because no stratification and 54 % 2 == 0\n    assert_no_warnings(grid.fit, X, y)\n"
    },
    {
      "filename": "sklearn/model_selection/tests/test_validation.py",
      "content": "\"\"\"Test the validation module\"\"\"\nfrom __future__ import division\n\nimport sys\nimport warnings\nimport tempfile\nimport os\nfrom time import sleep\n\nimport numpy as np\nfrom scipy.sparse import coo_matrix, csr_matrix\nfrom sklearn.exceptions import FitFailedWarning\n\nfrom sklearn.tests.test_grid_search import FailingClassifier\n\nfrom sklearn.utils.testing import assert_true\nfrom sklearn.utils.testing import assert_false\nfrom sklearn.utils.testing import assert_equal\nfrom sklearn.utils.testing import assert_almost_equal\nfrom sklearn.utils.testing import assert_raises\nfrom sklearn.utils.testing import assert_raise_message\nfrom sklearn.utils.testing import assert_warns\nfrom sklearn.utils.testing import assert_warns_message\nfrom sklearn.utils.testing import assert_no_warnings\nfrom sklearn.utils.testing import assert_raises_regex\nfrom sklearn.utils.testing import assert_greater\nfrom sklearn.utils.testing import assert_less\nfrom sklearn.utils.testing import assert_array_almost_equal\nfrom sklearn.utils.testing import assert_array_equal\nfrom sklearn.utils.mocking import CheckingClassifier, MockDataFrame\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import permutation_test_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom sklearn.model_selection import LeavePGroupsOut\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection._validation import _check_is_permutation\nfrom sklearn.model_selection._validation import _fit_and_score\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.datasets import load_boston\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_digits\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics.scorer import check_scoring\n\nfrom sklearn.linear_model import Ridge, LogisticRegression, SGDClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.externals.six.moves import cStringIO as StringIO\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import clone\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.utils import shuffle\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import make_multilabel_classification\n\nfrom sklearn.model_selection.tests.common import OneTimeSplitter\nfrom sklearn.model_selection import GridSearchCV\n\n\ntry:\n    WindowsError\nexcept NameError:\n    WindowsError = None\n\n\nclass MockImprovingEstimator(BaseEstimator):\n    \"\"\"Dummy classifier to test the learning curve\"\"\"\n    def __init__(self, n_max_train_sizes):\n        self.n_max_train_sizes = n_max_train_sizes\n        self.train_sizes = 0\n        self.X_subset = None\n\n    def fit(self, X_subset, y_subset=None):\n        self.X_subset = X_subset\n        self.train_sizes = X_subset.shape[0]\n        return self\n\n    def predict(self, X):\n        raise NotImplementedError\n\n    def score(self, X=None, Y=None):\n        # training score becomes worse (2 -> 1), test error better (0 -> 1)\n        if self._is_training_data(X):\n            return 2. - float(self.train_sizes) / self.n_max_train_sizes\n        else:\n            return float(self.train_sizes) / self.n_max_train_sizes\n\n    def _is_training_data(self, X):\n        return X is self.X_subset\n\n\nclass MockIncrementalImprovingEstimator(MockImprovingEstimator):\n    \"\"\"Dummy classifier that provides partial_fit\"\"\"\n    def __init__(self, n_max_train_sizes):\n        super(MockIncrementalImprovingEstimator,\n              self).__init__(n_max_train_sizes)\n        self.x = None\n\n    def _is_training_data(self, X):\n        return self.x in X\n\n    def partial_fit(self, X, y=None, **params):\n        self.train_sizes += X.shape[0]\n        self.x = X[0]\n\n\nclass MockEstimatorWithParameter(BaseEstimator):\n    \"\"\"Dummy classifier to test the validation curve\"\"\"\n    def __init__(self, param=0.5):\n        self.X_subset = None\n        self.param = param\n\n    def fit(self, X_subset, y_subset):\n        self.X_subset = X_subset\n        self.train_sizes = X_subset.shape[0]\n        return self\n\n    def predict(self, X):\n        raise NotImplementedError\n\n    def score(self, X=None, y=None):\n        return self.param if self._is_training_data(X) else 1 - self.param\n\n    def _is_training_data(self, X):\n        return X is self.X_subset\n\n\nclass MockEstimatorWithSingleFitCallAllowed(MockEstimatorWithParameter):\n    \"\"\"Dummy classifier that disallows repeated calls of fit method\"\"\"\n\n    def fit(self, X_subset, y_subset):\n        assert_false(\n            hasattr(self, 'fit_called_'),\n            'fit is called the second time'\n        )\n        self.fit_called_ = True\n        return super(type(self), self).fit(X_subset, y_subset)\n\n    def predict(self, X):\n        raise NotImplementedError\n\n\nclass MockClassifier(object):\n    \"\"\"Dummy classifier to test the cross-validation\"\"\"\n\n    def __init__(self, a=0, allow_nd=False):\n        self.a = a\n        self.allow_nd = allow_nd\n\n    def fit(self, X, Y=None, sample_weight=None, class_prior=None,\n            sparse_sample_weight=None, sparse_param=None, dummy_int=None,\n            dummy_str=None, dummy_obj=None, callback=None):\n        \"\"\"The dummy arguments are to test that this fit function can\n        accept non-array arguments through cross-validation, such as:\n            - int\n            - str (this is actually array-like)\n            - object\n            - function\n        \"\"\"\n        self.dummy_int = dummy_int\n        self.dummy_str = dummy_str\n        self.dummy_obj = dummy_obj\n        if callback is not None:\n            callback(self)\n\n        if self.allow_nd:\n            X = X.reshape(len(X), -1)\n        if X.ndim >= 3 and not self.allow_nd:\n            raise ValueError('X cannot be d')\n        if sample_weight is not None:\n            assert_true(sample_weight.shape[0] == X.shape[0],\n                        'MockClassifier extra fit_param sample_weight.shape[0]'\n                        ' is {0}, should be {1}'.format(sample_weight.shape[0],\n                                                        X.shape[0]))\n        if class_prior is not None:\n            assert_true(class_prior.shape[0] == len(np.unique(y)),\n                        'MockClassifier extra fit_param class_prior.shape[0]'\n                        ' is {0}, should be {1}'.format(class_prior.shape[0],\n                                                        len(np.unique(y))))\n        if sparse_sample_weight is not None:\n            fmt = ('MockClassifier extra fit_param sparse_sample_weight'\n                   '.shape[0] is {0}, should be {1}')\n            assert_true(sparse_sample_weight.shape[0] == X.shape[0],\n                        fmt.format(sparse_sample_weight.shape[0], X.shape[0]))\n        if sparse_param is not None:\n            fmt = ('MockClassifier extra fit_param sparse_param.shape '\n                   'is ({0}, {1}), should be ({2}, {3})')\n            assert_true(sparse_param.shape == P_sparse.shape,\n                        fmt.format(sparse_param.shape[0],\n                                   sparse_param.shape[1],\n                                   P_sparse.shape[0], P_sparse.shape[1]))\n        return self\n\n    def predict(self, T):\n        if self.allow_nd:\n            T = T.reshape(len(T), -1)\n        return T[:, 0]\n\n    def score(self, X=None, Y=None):\n        return 1. / (1 + np.abs(self.a))\n\n    def get_params(self, deep=False):\n        return {'a': self.a, 'allow_nd': self.allow_nd}\n\n\n# XXX: use 2D array, since 1D X is being detected as a single sample in\n# check_consistent_length\nX = np.ones((10, 2))\nX_sparse = coo_matrix(X)\ny = np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])\n# The number of samples per class needs to be > n_splits,\n# for StratifiedKFold(n_splits=3)\ny2 = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 3])\nP_sparse = coo_matrix(np.eye(5))\n\n\ndef test_cross_val_score():\n    clf = MockClassifier()\n\n    for a in range(-10, 10):\n        clf.a = a\n        # Smoke test\n        scores = cross_val_score(clf, X, y2)\n        assert_array_equal(scores, clf.score(X, y2))\n\n        # test with multioutput y\n        multioutput_y = np.column_stack([y2, y2[::-1]])\n        scores = cross_val_score(clf, X_sparse, multioutput_y)\n        assert_array_equal(scores, clf.score(X_sparse, multioutput_y))\n\n        scores = cross_val_score(clf, X_sparse, y2)\n        assert_array_equal(scores, clf.score(X_sparse, y2))\n\n        # test with multioutput y\n        scores = cross_val_score(clf, X_sparse, multioutput_y)\n        assert_array_equal(scores, clf.score(X_sparse, multioutput_y))\n\n    # test with X and y as list\n    list_check = lambda x: isinstance(x, list)\n    clf = CheckingClassifier(check_X=list_check)\n    scores = cross_val_score(clf, X.tolist(), y2.tolist())\n\n    clf = CheckingClassifier(check_y=list_check)\n    scores = cross_val_score(clf, X, y2.tolist())\n\n    assert_raises(ValueError, cross_val_score, clf, X, y2, scoring=\"sklearn\")\n\n    # test with 3d X and\n    X_3d = X[:, :, np.newaxis]\n    clf = MockClassifier(allow_nd=True)\n    scores = cross_val_score(clf, X_3d, y2)\n\n    clf = MockClassifier(allow_nd=False)\n    assert_raises(ValueError, cross_val_score, clf, X_3d, y2)\n\n\ndef test_cross_validate_invalid_scoring_param():\n    X, y = make_classification(random_state=0)\n    estimator = MockClassifier()\n\n    # Test the errors\n    error_message_regexp = \".*must be unique strings.*\"\n\n    # List/tuple of callables should raise a message advising users to use\n    # dict of names to callables mapping\n    assert_raises_regex(ValueError, error_message_regexp,\n                        cross_validate, estimator, X, y,\n                        scoring=(make_scorer(precision_score),\n                                 make_scorer(accuracy_score)))\n    assert_raises_regex(ValueError, error_message_regexp,\n                        cross_validate, estimator, X, y,\n                        scoring=(make_scorer(precision_score),))\n\n    # So should empty lists/tuples\n    assert_raises_regex(ValueError, error_message_regexp + \"Empty list.*\",\n                        cross_validate, estimator, X, y, scoring=())\n\n    # So should duplicated entries\n    assert_raises_regex(ValueError, error_message_regexp + \"Duplicate.*\",\n                        cross_validate, estimator, X, y,\n                        scoring=('f1_micro', 'f1_micro'))\n\n    # Nested Lists should raise a generic error message\n    assert_raises_regex(ValueError, error_message_regexp,\n                        cross_validate, estimator, X, y,\n                        scoring=[[make_scorer(precision_score)]])\n\n    error_message_regexp = (\".*should either be.*string or callable.*for \"\n                            \"single.*.*dict.*for multi.*\")\n\n    # Empty dict should raise invalid scoring error\n    assert_raises_regex(ValueError, \"An empty dict\",\n                        cross_validate, estimator, X, y, scoring=(dict()))\n\n    # And so should any other invalid entry\n    assert_raises_regex(ValueError, error_message_regexp,\n                        cross_validate, estimator, X, y, scoring=5)\n\n    multiclass_scorer = make_scorer(precision_recall_fscore_support)\n\n    # Multiclass Scorers that return multiple values are not supported yet\n    assert_raises_regex(ValueError,\n                        \"Classification metrics can't handle a mix of \"\n                        \"binary and continuous targets\",\n                        cross_validate, estimator, X, y,\n                        scoring=multiclass_scorer)\n    assert_raises_regex(ValueError,\n                        \"Classification metrics can't handle a mix of \"\n                        \"binary and continuous targets\",\n                        cross_validate, estimator, X, y,\n                        scoring={\"foo\": multiclass_scorer})\n\n    multivalued_scorer = make_scorer(confusion_matrix)\n\n    # Multiclass Scorers that return multiple values are not supported yet\n    assert_raises_regex(ValueError, \"scoring must return a number, got\",\n                        cross_validate, SVC(), X, y,\n                        scoring=multivalued_scorer)\n    assert_raises_regex(ValueError, \"scoring must return a number, got\",\n                        cross_validate, SVC(), X, y,\n                        scoring={\"foo\": multivalued_scorer})\n\n    assert_raises_regex(ValueError, \"'mse' is not a valid scoring value.\",\n                        cross_validate, SVC(), X, y, scoring=\"mse\")\n\n\ndef test_cross_validate():\n    # Compute train and test mse/r2 scores\n    cv = KFold(n_splits=5)\n\n    # Regression\n    X_reg, y_reg = make_regression(n_samples=30, random_state=0)\n    reg = Ridge(random_state=0)\n\n    # Classification\n    X_clf, y_clf = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n\n    for X, y, est in ((X_reg, y_reg, reg), (X_clf, y_clf, clf)):\n        # It's okay to evaluate regression metrics on classification too\n        mse_scorer = check_scoring(est, 'neg_mean_squared_error')\n        r2_scorer = check_scoring(est, 'r2')\n        train_mse_scores = []\n        test_mse_scores = []\n        train_r2_scores = []\n        test_r2_scores = []\n        for train, test in cv.split(X, y):\n            est = clone(reg).fit(X[train], y[train])\n            train_mse_scores.append(mse_scorer(est, X[train], y[train]))\n            train_r2_scores.append(r2_scorer(est, X[train], y[train]))\n            test_mse_scores.append(mse_scorer(est, X[test], y[test]))\n            test_r2_scores.append(r2_scorer(est, X[test], y[test]))\n\n        train_mse_scores = np.array(train_mse_scores)\n        test_mse_scores = np.array(test_mse_scores)\n        train_r2_scores = np.array(train_r2_scores)\n        test_r2_scores = np.array(test_r2_scores)\n\n        scores = (train_mse_scores, test_mse_scores, train_r2_scores,\n                  test_r2_scores)\n\n        yield check_cross_validate_single_metric, est, X, y, scores\n        yield check_cross_validate_multi_metric, est, X, y, scores\n\n\ndef test_cross_validate_return_train_score_warn():\n    # Test that warnings are raised. Will be removed in 0.21\n\n    X, y = make_classification(random_state=0)\n    estimator = MockClassifier()\n\n    result = {}\n    for val in [False, True, 'warn']:\n        result[val] = assert_no_warnings(cross_validate, estimator, X, y,\n                                         return_train_score=val)\n\n    msg = (\n        'You are accessing a training score ({!r}), '\n        'which will not be available by default '\n        'any more in 0.21. If you need training scores, '\n        'please set return_train_score=True').format('train_score')\n    train_score = assert_warns_message(FutureWarning, msg,\n                                       result['warn'].get, 'train_score')\n    assert np.allclose(train_score, result[True]['train_score'])\n    assert 'train_score' not in result[False]\n\n\ndef check_cross_validate_single_metric(clf, X, y, scores):\n    (train_mse_scores, test_mse_scores, train_r2_scores,\n     test_r2_scores) = scores\n    # Test single metric evaluation when scoring is string or singleton list\n    for (return_train_score, dict_len) in ((True, 4), (False, 3)):\n        # Single metric passed as a string\n        if return_train_score:\n            # It must be True by default\n            mse_scores_dict = cross_validate(clf, X, y, cv=5,\n                                             scoring='neg_mean_squared_error')\n            assert_array_almost_equal(mse_scores_dict['train_score'],\n                                      train_mse_scores)\n        else:\n            mse_scores_dict = cross_validate(clf, X, y, cv=5,\n                                             scoring='neg_mean_squared_error',\n                                             return_train_score=False)\n        assert_true(isinstance(mse_scores_dict, dict))\n        assert_equal(len(mse_scores_dict), dict_len)\n        assert_array_almost_equal(mse_scores_dict['test_score'],\n                                  test_mse_scores)\n\n        # Single metric passed as a list\n        if return_train_score:\n            # It must be True by default\n            r2_scores_dict = cross_validate(clf, X, y, cv=5, scoring=['r2'])\n            assert_array_almost_equal(r2_scores_dict['train_r2'],\n                                      train_r2_scores)\n        else:\n            r2_scores_dict = cross_validate(clf, X, y, cv=5, scoring=['r2'],\n                                            return_train_score=False)\n        assert_true(isinstance(r2_scores_dict, dict))\n        assert_equal(len(r2_scores_dict), dict_len)\n        assert_array_almost_equal(r2_scores_dict['test_r2'], test_r2_scores)\n\n\ndef check_cross_validate_multi_metric(clf, X, y, scores):\n    # Test multimetric evaluation when scoring is a list / dict\n    (train_mse_scores, test_mse_scores, train_r2_scores,\n     test_r2_scores) = scores\n    all_scoring = (('r2', 'neg_mean_squared_error'),\n                   {'r2': make_scorer(r2_score),\n                    'neg_mean_squared_error': 'neg_mean_squared_error'})\n\n    keys_sans_train = set(('test_r2', 'test_neg_mean_squared_error',\n                           'fit_time', 'score_time'))\n    keys_with_train = keys_sans_train.union(\n        set(('train_r2', 'train_neg_mean_squared_error')))\n\n    for return_train_score in (True, False):\n        for scoring in all_scoring:\n            if return_train_score:\n                # return_train_score must be True by default\n                cv_results = cross_validate(clf, X, y, cv=5, scoring=scoring)\n                assert_array_almost_equal(cv_results['train_r2'],\n                                          train_r2_scores)\n                assert_array_almost_equal(\n                    cv_results['train_neg_mean_squared_error'],\n                    train_mse_scores)\n            else:\n                cv_results = cross_validate(clf, X, y, cv=5, scoring=scoring,\n                                            return_train_score=False)\n            assert_true(isinstance(cv_results, dict))\n            assert_equal(set(cv_results.keys()),\n                         keys_with_train if return_train_score\n                         else keys_sans_train)\n            assert_array_almost_equal(cv_results['test_r2'], test_r2_scores)\n            assert_array_almost_equal(\n                cv_results['test_neg_mean_squared_error'], test_mse_scores)\n\n            # Make sure all the arrays are of np.ndarray type\n            assert type(cv_results['test_r2']) == np.ndarray\n            assert (type(cv_results['test_neg_mean_squared_error']) ==\n                    np.ndarray)\n            assert type(cv_results['fit_time']) == np.ndarray\n            assert type(cv_results['score_time']) == np.ndarray\n\n            # Ensure all the times are within sane limits\n            assert np.all(cv_results['fit_time'] >= 0)\n            assert np.all(cv_results['fit_time'] < 10)\n            assert np.all(cv_results['score_time'] >= 0)\n            assert np.all(cv_results['score_time'] < 10)\n\n\ndef test_cross_val_score_predict_groups():\n    # Check if ValueError (when groups is None) propagates to cross_val_score\n    # and cross_val_predict\n    # And also check if groups is correctly passed to the cv object\n    X, y = make_classification(n_samples=20, n_classes=2, random_state=0)\n\n    clf = SVC(kernel=\"linear\")\n\n    group_cvs = [LeaveOneGroupOut(), LeavePGroupsOut(2), GroupKFold(),\n                 GroupShuffleSplit()]\n    for cv in group_cvs:\n        assert_raise_message(ValueError,\n                             \"The 'groups' parameter should not be None.\",\n                             cross_val_score, estimator=clf, X=X, y=y, cv=cv)\n        assert_raise_message(ValueError,\n                             \"The 'groups' parameter should not be None.\",\n                             cross_val_predict, estimator=clf, X=X, y=y, cv=cv)\n\n\ndef test_cross_val_score_pandas():\n    # check cross_val_score doesn't destroy pandas dataframe\n    types = [(MockDataFrame, MockDataFrame)]\n    try:\n        from pandas import Series, DataFrame\n        types.append((Series, DataFrame))\n    except ImportError:\n        pass\n    for TargetType, InputFeatureType in types:\n        # X dataframe, y series\n        # 3 fold cross val is used so we need atleast 3 samples per class\n        X_df, y_ser = InputFeatureType(X), TargetType(y2)\n        check_df = lambda x: isinstance(x, InputFeatureType)\n        check_series = lambda x: isinstance(x, TargetType)\n        clf = CheckingClassifier(check_X=check_df, check_y=check_series)\n        cross_val_score(clf, X_df, y_ser)\n\n\ndef test_cross_val_score_mask():\n    # test that cross_val_score works with boolean masks\n    svm = SVC(kernel=\"linear\")\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    kfold = KFold(5)\n    scores_indices = cross_val_score(svm, X, y, cv=kfold)\n    kfold = KFold(5)\n    cv_masks = []\n    for train, test in kfold.split(X, y):\n        mask_train = np.zeros(len(y), dtype=np.bool)\n        mask_test = np.zeros(len(y), dtype=np.bool)\n        mask_train[train] = 1\n        mask_test[test] = 1\n        cv_masks.append((train, test))\n    scores_masks = cross_val_score(svm, X, y, cv=cv_masks)\n    assert_array_equal(scores_indices, scores_masks)\n\n\ndef test_cross_val_score_precomputed():\n    # test for svm with precomputed kernel\n    svm = SVC(kernel=\"precomputed\")\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    linear_kernel = np.dot(X, X.T)\n    score_precomputed = cross_val_score(svm, linear_kernel, y)\n    svm = SVC(kernel=\"linear\")\n    score_linear = cross_val_score(svm, X, y)\n    assert_array_almost_equal(score_precomputed, score_linear)\n\n    # test with callable\n    svm = SVC(kernel=lambda x, y: np.dot(x, y.T))\n    score_callable = cross_val_score(svm, X, y)\n    assert_array_almost_equal(score_precomputed, score_callable)\n\n    # Error raised for non-square X\n    svm = SVC(kernel=\"precomputed\")\n    assert_raises(ValueError, cross_val_score, svm, X, y)\n\n    # test error is raised when the precomputed kernel is not array-like\n    # or sparse\n    assert_raises(ValueError, cross_val_score, svm,\n                  linear_kernel.tolist(), y)\n\n\ndef test_cross_val_score_fit_params():\n    clf = MockClassifier()\n    n_samples = X.shape[0]\n    n_classes = len(np.unique(y))\n\n    W_sparse = coo_matrix((np.array([1]), (np.array([1]), np.array([0]))),\n                          shape=(10, 1))\n    P_sparse = coo_matrix(np.eye(5))\n\n    DUMMY_INT = 42\n    DUMMY_STR = '42'\n    DUMMY_OBJ = object()\n\n    def assert_fit_params(clf):\n        # Function to test that the values are passed correctly to the\n        # classifier arguments for non-array type\n\n        assert_equal(clf.dummy_int, DUMMY_INT)\n        assert_equal(clf.dummy_str, DUMMY_STR)\n        assert_equal(clf.dummy_obj, DUMMY_OBJ)\n\n    fit_params = {'sample_weight': np.ones(n_samples),\n                  'class_prior': np.ones(n_classes) / n_classes,\n                  'sparse_sample_weight': W_sparse,\n                  'sparse_param': P_sparse,\n                  'dummy_int': DUMMY_INT,\n                  'dummy_str': DUMMY_STR,\n                  'dummy_obj': DUMMY_OBJ,\n                  'callback': assert_fit_params}\n    cross_val_score(clf, X, y, fit_params=fit_params)\n\n\ndef test_cross_val_score_score_func():\n    clf = MockClassifier()\n    _score_func_args = []\n\n    def score_func(y_test, y_predict):\n        _score_func_args.append((y_test, y_predict))\n        return 1.0\n\n    with warnings.catch_warnings(record=True):\n        scoring = make_scorer(score_func)\n        score = cross_val_score(clf, X, y, scoring=scoring, cv=3)\n    assert_array_equal(score, [1.0, 1.0, 1.0])\n    # Test that score function is called only 3 times (for cv=3)\n    assert len(_score_func_args) == 3\n\n\ndef test_cross_val_score_errors():\n    class BrokenEstimator:\n        pass\n\n    assert_raises(TypeError, cross_val_score, BrokenEstimator(), X)\n\n\ndef test_cross_val_score_with_score_func_classification():\n    iris = load_iris()\n    clf = SVC(kernel='linear')\n\n    # Default score (should be the accuracy score)\n    scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n    assert_array_almost_equal(scores, [0.97, 1., 0.97, 0.97, 1.], 2)\n\n    # Correct classification score (aka. zero / one score) - should be the\n    # same as the default estimator score\n    zo_scores = cross_val_score(clf, iris.data, iris.target,\n                                scoring=\"accuracy\", cv=5)\n    assert_array_almost_equal(zo_scores, [0.97, 1., 0.97, 0.97, 1.], 2)\n\n    # F1 score (class are balanced so f1_score should be equal to zero/one\n    # score\n    f1_scores = cross_val_score(clf, iris.data, iris.target,\n                                scoring=\"f1_weighted\", cv=5)\n    assert_array_almost_equal(f1_scores, [0.97, 1., 0.97, 0.97, 1.], 2)\n\n\ndef test_cross_val_score_with_score_func_regression():\n    X, y = make_regression(n_samples=30, n_features=20, n_informative=5,\n                           random_state=0)\n    reg = Ridge()\n\n    # Default score of the Ridge regression estimator\n    scores = cross_val_score(reg, X, y, cv=5)\n    assert_array_almost_equal(scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)\n\n    # R2 score (aka. determination coefficient) - should be the\n    # same as the default estimator score\n    r2_scores = cross_val_score(reg, X, y, scoring=\"r2\", cv=5)\n    assert_array_almost_equal(r2_scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)\n\n    # Mean squared error; this is a loss function, so \"scores\" are negative\n    neg_mse_scores = cross_val_score(reg, X, y, cv=5,\n                                     scoring=\"neg_mean_squared_error\")\n    expected_neg_mse = np.array([-763.07, -553.16, -274.38, -273.26, -1681.99])\n    assert_array_almost_equal(neg_mse_scores, expected_neg_mse, 2)\n\n    # Explained variance\n    scoring = make_scorer(explained_variance_score)\n    ev_scores = cross_val_score(reg, X, y, cv=5, scoring=scoring)\n    assert_array_almost_equal(ev_scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)\n\n\ndef test_permutation_score():\n    iris = load_iris()\n    X = iris.data\n    X_sparse = coo_matrix(X)\n    y = iris.target\n    svm = SVC(kernel='linear')\n    cv = StratifiedKFold(2)\n\n    score, scores, pvalue = permutation_test_score(\n        svm, X, y, n_permutations=30, cv=cv, scoring=\"accuracy\")\n    assert_greater(score, 0.9)\n    assert_almost_equal(pvalue, 0.0, 1)\n\n    score_group, _, pvalue_group = permutation_test_score(\n        svm, X, y, n_permutations=30, cv=cv, scoring=\"accuracy\",\n        groups=np.ones(y.size), random_state=0)\n    assert_true(score_group == score)\n    assert_true(pvalue_group == pvalue)\n\n    # check that we obtain the same results with a sparse representation\n    svm_sparse = SVC(kernel='linear')\n    cv_sparse = StratifiedKFold(2)\n    score_group, _, pvalue_group = permutation_test_score(\n        svm_sparse, X_sparse, y, n_permutations=30, cv=cv_sparse,\n        scoring=\"accuracy\", groups=np.ones(y.size), random_state=0)\n\n    assert_true(score_group == score)\n    assert_true(pvalue_group == pvalue)\n\n    # test with custom scoring object\n    def custom_score(y_true, y_pred):\n        return (((y_true == y_pred).sum() - (y_true != y_pred).sum()) /\n                y_true.shape[0])\n\n    scorer = make_scorer(custom_score)\n    score, _, pvalue = permutation_test_score(\n        svm, X, y, n_permutations=100, scoring=scorer, cv=cv, random_state=0)\n    assert_almost_equal(score, .93, 2)\n    assert_almost_equal(pvalue, 0.01, 3)\n\n    # set random y\n    y = np.mod(np.arange(len(y)), 3)\n\n    score, scores, pvalue = permutation_test_score(\n        svm, X, y, n_permutations=30, cv=cv, scoring=\"accuracy\")\n\n    assert_less(score, 0.5)\n    assert_greater(pvalue, 0.2)\n\n\ndef test_permutation_test_score_allow_nans():\n    # Check that permutation_test_score allows input data with NaNs\n    X = np.arange(200, dtype=np.float64).reshape(10, -1)\n    X[2, :] = np.nan\n    y = np.repeat([0, 1], X.shape[0] / 2)\n    p = Pipeline([\n        ('imputer', SimpleImputer(strategy='mean', missing_values='NaN')),\n        ('classifier', MockClassifier()),\n    ])\n    permutation_test_score(p, X, y, cv=5)\n\n\ndef test_cross_val_score_allow_nans():\n    # Check that cross_val_score allows input data with NaNs\n    X = np.arange(200, dtype=np.float64).reshape(10, -1)\n    X[2, :] = np.nan\n    y = np.repeat([0, 1], X.shape[0] / 2)\n    p = Pipeline([\n        ('imputer', SimpleImputer(strategy='mean', missing_values='NaN')),\n        ('classifier', MockClassifier()),\n    ])\n    cross_val_score(p, X, y, cv=5)\n\n\ndef test_cross_val_score_multilabel():\n    X = np.array([[-3, 4], [2, 4], [3, 3], [0, 2], [-3, 1],\n                  [-2, 1], [0, 0], [-2, -1], [-1, -2], [1, -2]])\n    y = np.array([[1, 1], [0, 1], [0, 1], [0, 1], [1, 1],\n                  [0, 1], [1, 0], [1, 1], [1, 0], [0, 0]])\n    clf = KNeighborsClassifier(n_neighbors=1)\n    scoring_micro = make_scorer(precision_score, average='micro')\n    scoring_macro = make_scorer(precision_score, average='macro')\n    scoring_samples = make_scorer(precision_score, average='samples')\n    score_micro = cross_val_score(clf, X, y, scoring=scoring_micro, cv=5)\n    score_macro = cross_val_score(clf, X, y, scoring=scoring_macro, cv=5)\n    score_samples = cross_val_score(clf, X, y, scoring=scoring_samples, cv=5)\n    assert_almost_equal(score_micro, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 3])\n    assert_almost_equal(score_macro, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 4])\n    assert_almost_equal(score_samples, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 4])\n\n\ndef test_cross_val_predict():\n    boston = load_boston()\n    X, y = boston.data, boston.target\n    cv = KFold()\n\n    est = Ridge()\n\n    # Naive loop (should be same as cross_val_predict):\n    preds2 = np.zeros_like(y)\n    for train, test in cv.split(X, y):\n        est.fit(X[train], y[train])\n        preds2[test] = est.predict(X[test])\n\n    preds = cross_val_predict(est, X, y, cv=cv)\n    assert_array_almost_equal(preds, preds2)\n\n    preds = cross_val_predict(est, X, y)\n    assert_equal(len(preds), len(y))\n\n    cv = LeaveOneOut()\n    preds = cross_val_predict(est, X, y, cv=cv)\n    assert_equal(len(preds), len(y))\n\n    Xsp = X.copy()\n    Xsp *= (Xsp > np.median(Xsp))\n    Xsp = coo_matrix(Xsp)\n    preds = cross_val_predict(est, Xsp, y)\n    assert_array_almost_equal(len(preds), len(y))\n\n    preds = cross_val_predict(KMeans(), X)\n    assert_equal(len(preds), len(y))\n\n    class BadCV():\n        def split(self, X, y=None, groups=None):\n            for i in range(4):\n                yield np.array([0, 1, 2, 3]), np.array([4, 5, 6, 7, 8])\n\n    assert_raises(ValueError, cross_val_predict, est, X, y, cv=BadCV())\n\n    X, y = load_iris(return_X_y=True)\n\n    warning_message = ('Number of classes in training fold (2) does '\n                       'not match total number of classes (3). '\n                       'Results may not be appropriate for your use case.')\n    assert_warns_message(RuntimeWarning, warning_message,\n                         cross_val_predict, LogisticRegression(),\n                         X, y, method='predict_proba', cv=KFold(2))\n\n\ndef test_cross_val_predict_decision_function_shape():\n    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)\n\n    preds = cross_val_predict(LogisticRegression(), X, y,\n                              method='decision_function')\n    assert_equal(preds.shape, (50,))\n\n    X, y = load_iris(return_X_y=True)\n\n    preds = cross_val_predict(LogisticRegression(), X, y,\n                              method='decision_function')\n    assert_equal(preds.shape, (150, 3))\n\n    # This specifically tests imbalanced splits for binary\n    # classification with decision_function. This is only\n    # applicable to classifiers that can be fit on a single\n    # class.\n    X = X[:100]\n    y = y[:100]\n    assert_raise_message(ValueError,\n                         'Only 1 class/es in training fold, this'\n                         ' is not supported for decision_function'\n                         ' with imbalanced folds. To fix '\n                         'this, use a cross-validation technique '\n                         'resulting in properly stratified folds',\n                         cross_val_predict, RidgeClassifier(), X, y,\n                         method='decision_function', cv=KFold(2))\n\n    X, y = load_digits(return_X_y=True)\n    est = SVC(kernel='linear', decision_function_shape='ovo')\n\n    preds = cross_val_predict(est,\n                              X, y,\n                              method='decision_function')\n    assert_equal(preds.shape, (1797, 45))\n\n    ind = np.argsort(y)\n    X, y = X[ind], y[ind]\n    assert_raises_regex(ValueError,\n                        r'Output shape \\(599L?, 21L?\\) of decision_function '\n                        r'does not match number of classes \\(7\\) in fold. '\n                        'Irregular decision_function .*',\n                        cross_val_predict, est, X, y,\n                        cv=KFold(n_splits=3), method='decision_function')\n\n\ndef test_cross_val_predict_predict_proba_shape():\n    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)\n\n    preds = cross_val_predict(LogisticRegression(), X, y,\n                              method='predict_proba')\n    assert_equal(preds.shape, (50, 2))\n\n    X, y = load_iris(return_X_y=True)\n\n    preds = cross_val_predict(LogisticRegression(), X, y,\n                              method='predict_proba')\n    assert_equal(preds.shape, (150, 3))\n\n\ndef test_cross_val_predict_predict_log_proba_shape():\n    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)\n\n    preds = cross_val_predict(LogisticRegression(), X, y,\n                              method='predict_log_proba')\n    assert_equal(preds.shape, (50, 2))\n\n    X, y = load_iris(return_X_y=True)\n\n    preds = cross_val_predict(LogisticRegression(), X, y,\n                              method='predict_log_proba')\n    assert_equal(preds.shape, (150, 3))\n\n\ndef test_cross_val_predict_input_types():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    X_sparse = coo_matrix(X)\n    multioutput_y = np.column_stack([y, y[::-1]])\n\n    clf = Ridge(fit_intercept=False, random_state=0)\n    # 3 fold cv is used --> atleast 3 samples per class\n    # Smoke test\n    predictions = cross_val_predict(clf, X, y)\n    assert_equal(predictions.shape, (150,))\n\n    # test with multioutput y\n    predictions = cross_val_predict(clf, X_sparse, multioutput_y)\n    assert_equal(predictions.shape, (150, 2))\n\n    predictions = cross_val_predict(clf, X_sparse, y)\n    assert_array_equal(predictions.shape, (150,))\n\n    # test with multioutput y\n    predictions = cross_val_predict(clf, X_sparse, multioutput_y)\n    assert_array_equal(predictions.shape, (150, 2))\n\n    # test with X and y as list\n    list_check = lambda x: isinstance(x, list)\n    clf = CheckingClassifier(check_X=list_check)\n    predictions = cross_val_predict(clf, X.tolist(), y.tolist())\n\n    clf = CheckingClassifier(check_y=list_check)\n    predictions = cross_val_predict(clf, X, y.tolist())\n\n    # test with X and y as list and non empty method\n    predictions = cross_val_predict(LogisticRegression(), X.tolist(),\n                                    y.tolist(), method='decision_function')\n    predictions = cross_val_predict(LogisticRegression(), X,\n                                    y.tolist(), method='decision_function')\n\n    # test with 3d X and\n    X_3d = X[:, :, np.newaxis]\n    check_3d = lambda x: x.ndim == 3\n    clf = CheckingClassifier(check_X=check_3d)\n    predictions = cross_val_predict(clf, X_3d, y)\n    assert_array_equal(predictions.shape, (150,))\n\n\ndef test_cross_val_predict_pandas():\n    # check cross_val_score doesn't destroy pandas dataframe\n    types = [(MockDataFrame, MockDataFrame)]\n    try:\n        from pandas import Series, DataFrame\n        types.append((Series, DataFrame))\n    except ImportError:\n        pass\n    for TargetType, InputFeatureType in types:\n        # X dataframe, y series\n        X_df, y_ser = InputFeatureType(X), TargetType(y2)\n        check_df = lambda x: isinstance(x, InputFeatureType)\n        check_series = lambda x: isinstance(x, TargetType)\n        clf = CheckingClassifier(check_X=check_df, check_y=check_series)\n        cross_val_predict(clf, X_df, y_ser)\n\n\ndef test_cross_val_score_sparse_fit_params():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    clf = MockClassifier()\n    fit_params = {'sparse_sample_weight': coo_matrix(np.eye(X.shape[0]))}\n    a = cross_val_score(clf, X, y, fit_params=fit_params)\n    assert_array_equal(a, np.ones(3))\n\n\ndef test_learning_curve():\n    n_samples = 30\n    n_splits = 3\n    X, y = make_classification(n_samples=n_samples, n_features=1,\n                               n_informative=1, n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(n_samples * ((n_splits - 1) / n_splits))\n    for shuffle_train in [False, True]:\n        with warnings.catch_warnings(record=True) as w:\n            train_sizes, train_scores, test_scores = learning_curve(\n                estimator, X, y, cv=KFold(n_splits=n_splits),\n                train_sizes=np.linspace(0.1, 1.0, 10),\n                shuffle=shuffle_train)\n        if len(w) > 0:\n            raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)\n        assert_equal(train_scores.shape, (10, 3))\n        assert_equal(test_scores.shape, (10, 3))\n        assert_array_equal(train_sizes, np.linspace(2, 20, 10))\n        assert_array_almost_equal(train_scores.mean(axis=1),\n                                  np.linspace(1.9, 1.0, 10))\n        assert_array_almost_equal(test_scores.mean(axis=1),\n                                  np.linspace(0.1, 1.0, 10))\n\n        # Test a custom cv splitter that can iterate only once\n        with warnings.catch_warnings(record=True) as w:\n            train_sizes2, train_scores2, test_scores2 = learning_curve(\n                estimator, X, y,\n                cv=OneTimeSplitter(n_splits=n_splits, n_samples=n_samples),\n                train_sizes=np.linspace(0.1, 1.0, 10),\n                shuffle=shuffle_train)\n        if len(w) > 0:\n            raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)\n        assert_array_almost_equal(train_scores2, train_scores)\n        assert_array_almost_equal(test_scores2, test_scores)\n\n\ndef test_learning_curve_unsupervised():\n    X, _ = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(20)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y=None, cv=3, train_sizes=np.linspace(0.1, 1.0, 10))\n    assert_array_equal(train_sizes, np.linspace(2, 20, 10))\n    assert_array_almost_equal(train_scores.mean(axis=1),\n                              np.linspace(1.9, 1.0, 10))\n    assert_array_almost_equal(test_scores.mean(axis=1),\n                              np.linspace(0.1, 1.0, 10))\n\n\ndef test_learning_curve_verbose():\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(20)\n\n    old_stdout = sys.stdout\n    sys.stdout = StringIO()\n    try:\n        train_sizes, train_scores, test_scores = \\\n            learning_curve(estimator, X, y, cv=3, verbose=1)\n    finally:\n        out = sys.stdout.getvalue()\n        sys.stdout.close()\n        sys.stdout = old_stdout\n\n    assert(\"[learning_curve]\" in out)\n\n\ndef test_learning_curve_incremental_learning_not_possible():\n    X, y = make_classification(n_samples=2, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    # The mockup does not have partial_fit()\n    estimator = MockImprovingEstimator(1)\n    assert_raises(ValueError, learning_curve, estimator, X, y,\n                  exploit_incremental_learning=True)\n\n\ndef test_learning_curve_incremental_learning():\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockIncrementalImprovingEstimator(20)\n    for shuffle_train in [False, True]:\n        train_sizes, train_scores, test_scores = learning_curve(\n            estimator, X, y, cv=3, exploit_incremental_learning=True,\n            train_sizes=np.linspace(0.1, 1.0, 10), shuffle=shuffle_train)\n        assert_array_equal(train_sizes, np.linspace(2, 20, 10))\n        assert_array_almost_equal(train_scores.mean(axis=1),\n                                  np.linspace(1.9, 1.0, 10))\n        assert_array_almost_equal(test_scores.mean(axis=1),\n                                  np.linspace(0.1, 1.0, 10))\n\n\ndef test_learning_curve_incremental_learning_unsupervised():\n    X, _ = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockIncrementalImprovingEstimator(20)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y=None, cv=3, exploit_incremental_learning=True,\n        train_sizes=np.linspace(0.1, 1.0, 10))\n    assert_array_equal(train_sizes, np.linspace(2, 20, 10))\n    assert_array_almost_equal(train_scores.mean(axis=1),\n                              np.linspace(1.9, 1.0, 10))\n    assert_array_almost_equal(test_scores.mean(axis=1),\n                              np.linspace(0.1, 1.0, 10))\n\n\ndef test_learning_curve_batch_and_incremental_learning_are_equal():\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    train_sizes = np.linspace(0.2, 1.0, 5)\n    estimator = PassiveAggressiveClassifier(max_iter=1, tol=None,\n                                            shuffle=False)\n\n    train_sizes_inc, train_scores_inc, test_scores_inc = \\\n        learning_curve(\n            estimator, X, y, train_sizes=train_sizes,\n            cv=3, exploit_incremental_learning=True)\n    train_sizes_batch, train_scores_batch, test_scores_batch = \\\n        learning_curve(\n            estimator, X, y, cv=3, train_sizes=train_sizes,\n            exploit_incremental_learning=False)\n\n    assert_array_equal(train_sizes_inc, train_sizes_batch)\n    assert_array_almost_equal(train_scores_inc.mean(axis=1),\n                              train_scores_batch.mean(axis=1))\n    assert_array_almost_equal(test_scores_inc.mean(axis=1),\n                              test_scores_batch.mean(axis=1))\n\n\ndef test_learning_curve_n_sample_range_out_of_bounds():\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(20)\n    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,\n                  train_sizes=[0, 1])\n    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,\n                  train_sizes=[0.0, 1.0])\n    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,\n                  train_sizes=[0.1, 1.1])\n    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,\n                  train_sizes=[0, 20])\n    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,\n                  train_sizes=[1, 21])\n\n\ndef test_learning_curve_remove_duplicate_sample_sizes():\n    X, y = make_classification(n_samples=3, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(2)\n    train_sizes, _, _ = assert_warns(\n        RuntimeWarning, learning_curve, estimator, X, y, cv=3,\n        train_sizes=np.linspace(0.33, 1.0, 3))\n    assert_array_equal(train_sizes, [1, 2])\n\n\ndef test_learning_curve_with_boolean_indices():\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(20)\n    cv = KFold(n_splits=3)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, train_sizes=np.linspace(0.1, 1.0, 10))\n    assert_array_equal(train_sizes, np.linspace(2, 20, 10))\n    assert_array_almost_equal(train_scores.mean(axis=1),\n                              np.linspace(1.9, 1.0, 10))\n    assert_array_almost_equal(test_scores.mean(axis=1),\n                              np.linspace(0.1, 1.0, 10))\n\n\ndef test_learning_curve_with_shuffle():\n    # Following test case was designed this way to verify the code\n    # changes made in pull request: #7506.\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [11, 12], [13, 14], [15, 16],\n                 [17, 18], [19, 20], [7, 8], [9, 10], [11, 12], [13, 14],\n                 [15, 16], [17, 18]])\n    y = np.array([1, 1, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4])\n    groups = np.array([1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 4, 4, 4, 4])\n    # Splits on these groups fail without shuffle as the first iteration\n    # of the learning curve doesn't contain label 4 in the training set.\n    estimator = PassiveAggressiveClassifier(max_iter=5, tol=None,\n                                            shuffle=False)\n\n    cv = GroupKFold(n_splits=2)\n    train_sizes_batch, train_scores_batch, test_scores_batch = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3),\n        groups=groups, shuffle=True, random_state=2)\n    assert_array_almost_equal(train_scores_batch.mean(axis=1),\n                              np.array([0.75, 0.3, 0.36111111]))\n    assert_array_almost_equal(test_scores_batch.mean(axis=1),\n                              np.array([0.36111111, 0.25, 0.25]))\n    assert_raises(ValueError, learning_curve, estimator, X, y, cv=cv, n_jobs=1,\n                  train_sizes=np.linspace(0.3, 1.0, 3), groups=groups)\n\n    train_sizes_inc, train_scores_inc, test_scores_inc = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3),\n        groups=groups, shuffle=True, random_state=2,\n        exploit_incremental_learning=True)\n    assert_array_almost_equal(train_scores_inc.mean(axis=1),\n                              train_scores_batch.mean(axis=1))\n    assert_array_almost_equal(test_scores_inc.mean(axis=1),\n                              test_scores_batch.mean(axis=1))\n\n\ndef test_validation_curve():\n    X, y = make_classification(n_samples=2, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    param_range = np.linspace(0, 1, 10)\n    with warnings.catch_warnings(record=True) as w:\n        train_scores, test_scores = validation_curve(\n            MockEstimatorWithParameter(), X, y, param_name=\"param\",\n            param_range=param_range, cv=2\n        )\n    if len(w) > 0:\n        raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)\n\n    assert_array_almost_equal(train_scores.mean(axis=1), param_range)\n    assert_array_almost_equal(test_scores.mean(axis=1), 1 - param_range)\n\n\ndef test_validation_curve_clone_estimator():\n    X, y = make_classification(n_samples=2, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n\n    param_range = np.linspace(1, 0, 10)\n    _, _ = validation_curve(\n        MockEstimatorWithSingleFitCallAllowed(), X, y,\n        param_name=\"param\", param_range=param_range, cv=2\n    )\n\n\ndef test_validation_curve_cv_splits_consistency():\n    n_samples = 100\n    n_splits = 5\n    X, y = make_classification(n_samples=100, random_state=0)\n\n    scores1 = validation_curve(SVC(kernel='linear', random_state=0), X, y,\n                               'C', [0.1, 0.1, 0.2, 0.2],\n                               cv=OneTimeSplitter(n_splits=n_splits,\n                                                  n_samples=n_samples))\n    # The OneTimeSplitter is a non-re-entrant cv splitter. Unless, the\n    # `split` is called for each parameter, the following should produce\n    # identical results for param setting 1 and param setting 2 as both have\n    # the same C value.\n    assert_array_almost_equal(*np.vsplit(np.hstack(scores1)[(0, 2, 1, 3), :],\n                                         2))\n\n    scores2 = validation_curve(SVC(kernel='linear', random_state=0), X, y,\n                               'C', [0.1, 0.1, 0.2, 0.2],\n                               cv=KFold(n_splits=n_splits, shuffle=True))\n\n    # For scores2, compare the 1st and 2nd parameter's scores\n    # (Since the C value for 1st two param setting is 0.1, they must be\n    # consistent unless the train test folds differ between the param settings)\n    assert_array_almost_equal(*np.vsplit(np.hstack(scores2)[(0, 2, 1, 3), :],\n                                         2))\n\n    scores3 = validation_curve(SVC(kernel='linear', random_state=0), X, y,\n                               'C', [0.1, 0.1, 0.2, 0.2],\n                               cv=KFold(n_splits=n_splits))\n\n    # OneTimeSplitter is basically unshuffled KFold(n_splits=5). Sanity check.\n    assert_array_almost_equal(np.array(scores3), np.array(scores1))\n\n\ndef test_check_is_permutation():\n    rng = np.random.RandomState(0)\n    p = np.arange(100)\n    rng.shuffle(p)\n    assert_true(_check_is_permutation(p, 100))\n    assert_false(_check_is_permutation(np.delete(p, 23), 100))\n\n    p[0] = 23\n    assert_false(_check_is_permutation(p, 100))\n\n    # Check if the additional duplicate indices are caught\n    assert_false(_check_is_permutation(np.hstack((p, 0)), 100))\n\n\ndef test_cross_val_predict_sparse_prediction():\n    # check that cross_val_predict gives same result for sparse and dense input\n    X, y = make_multilabel_classification(n_classes=2, n_labels=1,\n                                          allow_unlabeled=False,\n                                          return_indicator=True,\n                                          random_state=1)\n    X_sparse = csr_matrix(X)\n    y_sparse = csr_matrix(y)\n    classif = OneVsRestClassifier(SVC(kernel='linear'))\n    preds = cross_val_predict(classif, X, y, cv=10)\n    preds_sparse = cross_val_predict(classif, X_sparse, y_sparse, cv=10)\n    preds_sparse = preds_sparse.toarray()\n    assert_array_almost_equal(preds_sparse, preds)\n\n\ndef check_cross_val_predict_with_method(est):\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    X, y = shuffle(X, y, random_state=0)\n    classes = len(set(y))\n\n    kfold = KFold()\n\n    methods = ['decision_function', 'predict_proba', 'predict_log_proba']\n    for method in methods:\n        predictions = cross_val_predict(est, X, y, method=method)\n        assert_equal(len(predictions), len(y))\n\n        expected_predictions = np.zeros([len(y), classes])\n        func = getattr(est, method)\n\n        # Naive loop (should be same as cross_val_predict):\n        for train, test in kfold.split(X, y):\n            est.fit(X[train], y[train])\n            expected_predictions[test] = func(X[test])\n\n        predictions = cross_val_predict(est, X, y, method=method,\n                                        cv=kfold)\n        assert_array_almost_equal(expected_predictions, predictions)\n\n        # Test alternative representations of y\n        predictions_y1 = cross_val_predict(est, X, y + 1, method=method,\n                                           cv=kfold)\n        assert_array_equal(predictions, predictions_y1)\n\n        predictions_y2 = cross_val_predict(est, X, y - 2, method=method,\n                                           cv=kfold)\n        assert_array_equal(predictions, predictions_y2)\n\n        predictions_ystr = cross_val_predict(est, X, y.astype('str'),\n                                             method=method, cv=kfold)\n        assert_array_equal(predictions, predictions_ystr)\n\n\ndef test_cross_val_predict_with_method():\n    check_cross_val_predict_with_method(LogisticRegression())\n\n\ndef test_cross_val_predict_method_checking():\n    # Regression test for issue #9639. Tests that cross_val_predict does not\n    # check estimator methods (e.g. predict_proba) before fitting\n    est = SGDClassifier(loss='log', random_state=2)\n    check_cross_val_predict_with_method(est)\n\n\ndef test_gridsearchcv_cross_val_predict_with_method():\n    est = GridSearchCV(LogisticRegression(random_state=42),\n                       {'C': [0.1, 1]},\n                       cv=2)\n    check_cross_val_predict_with_method(est)\n\n\ndef get_expected_predictions(X, y, cv, classes, est, method):\n\n    expected_predictions = np.zeros([len(y), classes])\n    func = getattr(est, method)\n\n    for train, test in cv.split(X, y):\n        est.fit(X[train], y[train])\n        expected_predictions_ = func(X[test])\n        # To avoid 2 dimensional indexing\n        if method is 'predict_proba':\n            exp_pred_test = np.zeros((len(test), classes))\n        else:\n            exp_pred_test = np.full((len(test), classes),\n                                    np.finfo(expected_predictions.dtype).min)\n        exp_pred_test[:, est.classes_] = expected_predictions_\n        expected_predictions[test] = exp_pred_test\n\n    return expected_predictions\n\n\ndef test_cross_val_predict_class_subset():\n\n    X = np.arange(200).reshape(100, 2)\n    y = np.array([x//10 for x in range(100)])\n    classes = 10\n\n    kfold3 = KFold(n_splits=3)\n    kfold4 = KFold(n_splits=4)\n\n    le = LabelEncoder()\n\n    methods = ['decision_function', 'predict_proba', 'predict_log_proba']\n    for method in methods:\n        est = LogisticRegression()\n\n        # Test with n_splits=3\n        predictions = cross_val_predict(est, X, y, method=method,\n                                        cv=kfold3)\n\n        # Runs a naive loop (should be same as cross_val_predict):\n        expected_predictions = get_expected_predictions(X, y, kfold3, classes,\n                                                        est, method)\n        assert_array_almost_equal(expected_predictions, predictions)\n\n        # Test with n_splits=4\n        predictions = cross_val_predict(est, X, y, method=method,\n                                        cv=kfold4)\n        expected_predictions = get_expected_predictions(X, y, kfold4, classes,\n                                                        est, method)\n        assert_array_almost_equal(expected_predictions, predictions)\n\n        # Testing unordered labels\n        y = shuffle(np.repeat(range(10), 10), random_state=0)\n        predictions = cross_val_predict(est, X, y, method=method,\n                                        cv=kfold3)\n        y = le.fit_transform(y)\n        expected_predictions = get_expected_predictions(X, y, kfold3, classes,\n                                                        est, method)\n        assert_array_almost_equal(expected_predictions, predictions)\n\n\ndef test_score_memmap():\n    # Ensure a scalar score of memmap type is accepted\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    clf = MockClassifier()\n    tf = tempfile.NamedTemporaryFile(mode='wb', delete=False)\n    tf.write(b'Hello world!!!!!')\n    tf.close()\n    scores = np.memmap(tf.name, dtype=np.float64)\n    score = np.memmap(tf.name, shape=(), mode='r', dtype=np.float64)\n    try:\n        cross_val_score(clf, X, y, scoring=lambda est, X, y: score)\n        # non-scalar should still fail\n        assert_raises(ValueError, cross_val_score, clf, X, y,\n                      scoring=lambda est, X, y: scores)\n    finally:\n        # Best effort to release the mmap file handles before deleting the\n        # backing file under Windows\n        scores, score = None, None\n        for _ in range(3):\n            try:\n                os.unlink(tf.name)\n                break\n            except WindowsError:\n                sleep(1.)\n\n\ndef test_permutation_test_score_pandas():\n    # check permutation_test_score doesn't destroy pandas dataframe\n    types = [(MockDataFrame, MockDataFrame)]\n    try:\n        from pandas import Series, DataFrame\n        types.append((Series, DataFrame))\n    except ImportError:\n        pass\n    for TargetType, InputFeatureType in types:\n        # X dataframe, y series\n        iris = load_iris()\n        X, y = iris.data, iris.target\n        X_df, y_ser = InputFeatureType(X), TargetType(y)\n        check_df = lambda x: isinstance(x, InputFeatureType)\n        check_series = lambda x: isinstance(x, TargetType)\n        clf = CheckingClassifier(check_X=check_df, check_y=check_series)\n        permutation_test_score(clf, X_df, y_ser)\n\n\ndef test_fit_and_score():\n    # Create a failing classifier to deliberately fail\n    failing_clf = FailingClassifier(FailingClassifier.FAILING_PARAMETER)\n    # dummy X data\n    X = np.arange(1, 10)\n    fit_and_score_args = [failing_clf, X, None, dict(), None, None, 0,\n                          None, None]\n    # passing error score to trigger the warning message\n    fit_and_score_kwargs = {'error_score': 0}\n    # check if the warning message type is as expected\n    assert_warns(FitFailedWarning, _fit_and_score, *fit_and_score_args,\n                 **fit_and_score_kwargs)\n    # since we're using FailingClassfier, our error will be the following\n    error_message = \"ValueError: Failing classifier failed as required\"\n    # the warning message we're expecting to see\n    warning_message = (\"Estimator fit failed. The score on this train-test \"\n                       \"partition for these parameters will be set to %f. \"\n                       \"Details: \\n%s\" % (fit_and_score_kwargs['error_score'],\n                                          error_message))\n    # check if the same warning is triggered\n    assert_warns_message(FitFailedWarning, warning_message, _fit_and_score,\n                         *fit_and_score_args, **fit_and_score_kwargs)\n"
    },
    {
      "filename": "sklearn/preprocessing/imputation.py",
      "content": "# Authors: Nicolas Tresegnie <nicolas.tresegnie@gmail.com>\n# License: BSD 3 clause\n\nimport warnings\n\nimport numpy as np\nimport numpy.ma as ma\nfrom scipy import sparse\nfrom scipy import stats\n\nfrom ..base import BaseEstimator, TransformerMixin\nfrom ..utils import check_array\nfrom ..utils import deprecated\nfrom ..utils.sparsefuncs import _get_median\nfrom ..utils.validation import check_is_fitted\nfrom ..utils.validation import FLOAT_DTYPES\n\nfrom ..externals import six\n\nzip = six.moves.zip\nmap = six.moves.map\n\n__all__ = [\n    'Imputer',\n]\n\n\ndef _get_mask(X, value_to_mask):\n    \"\"\"Compute the boolean mask X == missing_values.\"\"\"\n    if value_to_mask == \"NaN\" or np.isnan(value_to_mask):\n        return np.isnan(X)\n    else:\n        return X == value_to_mask\n\n\ndef _most_frequent(array, extra_value, n_repeat):\n    \"\"\"Compute the most frequent value in a 1d array extended with\n       [extra_value] * n_repeat, where extra_value is assumed to be not part\n       of the array.\"\"\"\n    # Compute the most frequent value in array only\n    if array.size > 0:\n        mode = stats.mode(array)\n        most_frequent_value = mode[0][0]\n        most_frequent_count = mode[1][0]\n    else:\n        most_frequent_value = 0\n        most_frequent_count = 0\n\n    # Compare to array + [extra_value] * n_repeat\n    if most_frequent_count == 0 and n_repeat == 0:\n        return np.nan\n    elif most_frequent_count < n_repeat:\n        return extra_value\n    elif most_frequent_count > n_repeat:\n        return most_frequent_value\n    elif most_frequent_count == n_repeat:\n        # Ties the breaks. Copy the behaviour of scipy.stats.mode\n        if most_frequent_value < extra_value:\n            return most_frequent_value\n        else:\n            return extra_value\n\n\n@deprecated(\"Imputer was deprecated in version 0.20 and will be \"\n            \"removed in 0.22. Import impute.SimpleImputer from \"\n            \"sklearn instead.\")\nclass Imputer(BaseEstimator, TransformerMixin):\n    \"\"\"Imputation transformer for completing missing values.\n\n    Read more in the :ref:`User Guide <imputation>`.\n\n    Parameters\n    ----------\n    missing_values : integer or \"NaN\", optional (default=\"NaN\")\n        The placeholder for the missing values. All occurrences of\n        `missing_values` will be imputed. For missing values encoded as np.nan,\n        use the string value \"NaN\".\n\n    strategy : string, optional (default=\"mean\")\n        The imputation strategy.\n\n        - If \"mean\", then replace missing values using the mean along\n          the axis.\n        - If \"median\", then replace missing values using the median along\n          the axis.\n        - If \"most_frequent\", then replace missing using the most frequent\n          value along the axis.\n\n    axis : integer, optional (default=0)\n        The axis along which to impute.\n\n        - If `axis=0`, then impute along columns.\n        - If `axis=1`, then impute along rows.\n\n    verbose : integer, optional (default=0)\n        Controls the verbosity of the imputer.\n\n    copy : boolean, optional (default=True)\n        If True, a copy of X will be created. If False, imputation will\n        be done in-place whenever possible. Note that, in the following cases,\n        a new copy will always be made, even if `copy=False`:\n\n        - If X is not an array of floating values;\n        - If X is sparse and `missing_values=0`;\n        - If `axis=0` and X is encoded as a CSR matrix;\n        - If `axis=1` and X is encoded as a CSC matrix.\n\n    Attributes\n    ----------\n    statistics_ : array of shape (n_features,)\n        The imputation fill value for each feature if axis == 0.\n\n    Notes\n    -----\n    - When ``axis=0``, columns which only contained missing values at `fit`\n      are discarded upon `transform`.\n    - When ``axis=1``, an exception is raised if there are rows for which it is\n      not possible to fill in the missing values (e.g., because they only\n      contain missing values).\n    \"\"\"\n    def __init__(self, missing_values=\"NaN\", strategy=\"mean\",\n                 axis=0, verbose=0, copy=True):\n        self.missing_values = missing_values\n        self.strategy = strategy\n        self.axis = axis\n        self.verbose = verbose\n        self.copy = copy\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the imputer on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Input data, where ``n_samples`` is the number of samples and\n            ``n_features`` is the number of features.\n\n        Returns\n        -------\n        self : Imputer\n        \"\"\"\n        # Check parameters\n        allowed_strategies = [\"mean\", \"median\", \"most_frequent\"]\n        if self.strategy not in allowed_strategies:\n            raise ValueError(\"Can only use these strategies: {0} \"\n                             \" got strategy={1}\".format(allowed_strategies,\n                                                        self.strategy))\n\n        if self.axis not in [0, 1]:\n            raise ValueError(\"Can only impute missing values on axis 0 and 1, \"\n                             \" got axis={0}\".format(self.axis))\n\n        # Since two different arrays can be provided in fit(X) and\n        # transform(X), the imputation data will be computed in transform()\n        # when the imputation is done per sample (i.e., when axis=1).\n        if self.axis == 0:\n            X = check_array(X, accept_sparse='csc', dtype=np.float64,\n                            force_all_finite=False)\n\n            if sparse.issparse(X):\n                self.statistics_ = self._sparse_fit(X,\n                                                    self.strategy,\n                                                    self.missing_values,\n                                                    self.axis)\n            else:\n                self.statistics_ = self._dense_fit(X,\n                                                   self.strategy,\n                                                   self.missing_values,\n                                                   self.axis)\n\n        return self\n\n    def _sparse_fit(self, X, strategy, missing_values, axis):\n        \"\"\"Fit the transformer on sparse data.\"\"\"\n        # Imputation is done \"by column\", so if we want to do it\n        # by row we only need to convert the matrix to csr format.\n        if axis == 1:\n            X = X.tocsr()\n        else:\n            X = X.tocsc()\n\n        # Count the zeros\n        if missing_values == 0:\n            n_zeros_axis = np.zeros(X.shape[not axis], dtype=int)\n        else:\n            n_zeros_axis = X.shape[axis] - np.diff(X.indptr)\n\n        # Mean\n        if strategy == \"mean\":\n            if missing_values != 0:\n                n_non_missing = n_zeros_axis\n\n                # Mask the missing elements\n                mask_missing_values = _get_mask(X.data, missing_values)\n                mask_valids = np.logical_not(mask_missing_values)\n\n                # Sum only the valid elements\n                new_data = X.data.copy()\n                new_data[mask_missing_values] = 0\n                X = sparse.csc_matrix((new_data, X.indices, X.indptr),\n                                      copy=False)\n                sums = X.sum(axis=0)\n\n                # Count the elements != 0\n                mask_non_zeros = sparse.csc_matrix(\n                    (mask_valids.astype(np.float64),\n                     X.indices,\n                     X.indptr), copy=False)\n                s = mask_non_zeros.sum(axis=0)\n                n_non_missing = np.add(n_non_missing, s)\n\n            else:\n                sums = X.sum(axis=axis)\n                n_non_missing = np.diff(X.indptr)\n\n            # Ignore the error, columns with a np.nan statistics_\n            # are not an error at this point. These columns will\n            # be removed in transform\n            with np.errstate(all=\"ignore\"):\n                return np.ravel(sums) / np.ravel(n_non_missing)\n\n        # Median + Most frequent\n        else:\n            # Remove the missing values, for each column\n            columns_all = np.hsplit(X.data, X.indptr[1:-1])\n            mask_missing_values = _get_mask(X.data, missing_values)\n            mask_valids = np.hsplit(np.logical_not(mask_missing_values),\n                                    X.indptr[1:-1])\n\n            # astype necessary for bug in numpy.hsplit before v1.9\n            columns = [col[mask.astype(bool, copy=False)]\n                       for col, mask in zip(columns_all, mask_valids)]\n\n            # Median\n            if strategy == \"median\":\n                median = np.empty(len(columns))\n                for i, column in enumerate(columns):\n                    median[i] = _get_median(column, n_zeros_axis[i])\n\n                return median\n\n            # Most frequent\n            elif strategy == \"most_frequent\":\n                most_frequent = np.empty(len(columns))\n\n                for i, column in enumerate(columns):\n                    most_frequent[i] = _most_frequent(column,\n                                                      0,\n                                                      n_zeros_axis[i])\n\n                return most_frequent\n\n    def _dense_fit(self, X, strategy, missing_values, axis):\n        \"\"\"Fit the transformer on dense data.\"\"\"\n        X = check_array(X, force_all_finite=False)\n        mask = _get_mask(X, missing_values)\n        masked_X = ma.masked_array(X, mask=mask)\n\n        # Mean\n        if strategy == \"mean\":\n            mean_masked = np.ma.mean(masked_X, axis=axis)\n            # Avoid the warning \"Warning: converting a masked element to nan.\"\n            mean = np.ma.getdata(mean_masked)\n            mean[np.ma.getmask(mean_masked)] = np.nan\n\n            return mean\n\n        # Median\n        elif strategy == \"median\":\n            if tuple(int(v) for v in np.__version__.split('.')[:2]) < (1, 5):\n                # In old versions of numpy, calling a median on an array\n                # containing nans returns nan. This is different is\n                # recent versions of numpy, which we want to mimic\n                masked_X.mask = np.logical_or(masked_X.mask,\n                                              np.isnan(X))\n            median_masked = np.ma.median(masked_X, axis=axis)\n            # Avoid the warning \"Warning: converting a masked element to nan.\"\n            median = np.ma.getdata(median_masked)\n            median[np.ma.getmaskarray(median_masked)] = np.nan\n\n            return median\n\n        # Most frequent\n        elif strategy == \"most_frequent\":\n            # scipy.stats.mstats.mode cannot be used because it will no work\n            # properly if the first element is masked and if its frequency\n            # is equal to the frequency of the most frequent valid element\n            # See https://github.com/scipy/scipy/issues/2636\n\n            # To be able access the elements by columns\n            if axis == 0:\n                X = X.transpose()\n                mask = mask.transpose()\n\n            most_frequent = np.empty(X.shape[0])\n\n            for i, (row, row_mask) in enumerate(zip(X[:], mask[:])):\n                row_mask = np.logical_not(row_mask).astype(np.bool)\n                row = row[row_mask]\n                most_frequent[i] = _most_frequent(row, np.nan, 0)\n\n            return most_frequent\n\n    def transform(self, X):\n        \"\"\"Impute all missing values in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            The input data to complete.\n        \"\"\"\n        if self.axis == 0:\n            check_is_fitted(self, 'statistics_')\n            X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,\n                            force_all_finite=False, copy=self.copy)\n            statistics = self.statistics_\n            if X.shape[1] != statistics.shape[0]:\n                raise ValueError(\"X has %d features per sample, expected %d\"\n                                 % (X.shape[1], self.statistics_.shape[0]))\n\n        # Since two different arrays can be provided in fit(X) and\n        # transform(X), the imputation data need to be recomputed\n        # when the imputation is done per sample\n        else:\n            X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES,\n                            force_all_finite=False, copy=self.copy)\n\n            if sparse.issparse(X):\n                statistics = self._sparse_fit(X,\n                                              self.strategy,\n                                              self.missing_values,\n                                              self.axis)\n\n            else:\n                statistics = self._dense_fit(X,\n                                             self.strategy,\n                                             self.missing_values,\n                                             self.axis)\n\n        # Delete the invalid rows/columns\n        invalid_mask = np.isnan(statistics)\n        valid_mask = np.logical_not(invalid_mask)\n        valid_statistics = statistics[valid_mask]\n        valid_statistics_indexes = np.where(valid_mask)[0]\n        missing = np.arange(X.shape[not self.axis])[invalid_mask]\n\n        if self.axis == 0 and invalid_mask.any():\n            if self.verbose:\n                warnings.warn(\"Deleting features without \"\n                              \"observed values: %s\" % missing)\n            X = X[:, valid_statistics_indexes]\n        elif self.axis == 1 and invalid_mask.any():\n            raise ValueError(\"Some rows only contain \"\n                             \"missing values: %s\" % missing)\n\n        # Do actual imputation\n        if sparse.issparse(X) and self.missing_values != 0:\n            mask = _get_mask(X.data, self.missing_values)\n            indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),\n                                np.diff(X.indptr))[mask]\n\n            X.data[mask] = valid_statistics[indexes].astype(X.dtype,\n                                                            copy=False)\n        else:\n            if sparse.issparse(X):\n                X = X.toarray()\n\n            mask = _get_mask(X, self.missing_values)\n            n_missing = np.sum(mask, axis=self.axis)\n            values = np.repeat(valid_statistics, n_missing)\n\n            if self.axis == 0:\n                coordinates = np.where(mask.transpose())[::-1]\n            else:\n                coordinates = mask\n\n            X[coordinates] = values\n\n        return X\n"
    },
    {
      "filename": "sklearn/preprocessing/tests/test_imputation.py",
      "content": "\nimport numpy as np\nfrom scipy import sparse\n\nfrom sklearn.utils.testing import assert_equal\nfrom sklearn.utils.testing import assert_array_equal\nfrom sklearn.utils.testing import assert_array_almost_equal\nfrom sklearn.utils.testing import assert_raises\nfrom sklearn.utils.testing import assert_false\nfrom sklearn.utils.testing import ignore_warnings\n\nfrom sklearn.preprocessing.imputation import Imputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import tree\nfrom sklearn.random_projection import sparse_random_matrix\n\n\n@ignore_warnings\ndef _check_statistics(X, X_true,\n                      strategy, statistics, missing_values):\n    \"\"\"Utility function for testing imputation for a given strategy.\n\n    Test:\n        - along the two axes\n        - with dense and sparse arrays\n\n    Check that:\n        - the statistics (mean, median, mode) are correct\n        - the missing values are imputed correctly\"\"\"\n\n    err_msg = \"Parameters: strategy = %s, missing_values = %s, \" \\\n              \"axis = {0}, sparse = {1}\" % (strategy, missing_values)\n\n    assert_ae = assert_array_equal\n    if X.dtype.kind == 'f' or X_true.dtype.kind == 'f':\n        assert_ae = assert_array_almost_equal\n\n    # Normal matrix, axis = 0\n    imputer = Imputer(missing_values, strategy=strategy, axis=0)\n    X_trans = imputer.fit(X).transform(X.copy())\n    assert_ae(imputer.statistics_, statistics,\n              err_msg=err_msg.format(0, False))\n    assert_ae(X_trans, X_true, err_msg=err_msg.format(0, False))\n\n    # Normal matrix, axis = 1\n    imputer = Imputer(missing_values, strategy=strategy, axis=1)\n    imputer.fit(X.transpose())\n    if np.isnan(statistics).any():\n        assert_raises(ValueError, imputer.transform, X.copy().transpose())\n    else:\n        X_trans = imputer.transform(X.copy().transpose())\n        assert_ae(X_trans, X_true.transpose(),\n                  err_msg=err_msg.format(1, False))\n\n    # Sparse matrix, axis = 0\n    imputer = Imputer(missing_values, strategy=strategy, axis=0)\n    imputer.fit(sparse.csc_matrix(X))\n    X_trans = imputer.transform(sparse.csc_matrix(X.copy()))\n\n    if sparse.issparse(X_trans):\n        X_trans = X_trans.toarray()\n\n    assert_ae(imputer.statistics_, statistics,\n              err_msg=err_msg.format(0, True))\n    assert_ae(X_trans, X_true, err_msg=err_msg.format(0, True))\n\n    # Sparse matrix, axis = 1\n    imputer = Imputer(missing_values, strategy=strategy, axis=1)\n    imputer.fit(sparse.csc_matrix(X.transpose()))\n    if np.isnan(statistics).any():\n        assert_raises(ValueError, imputer.transform,\n                      sparse.csc_matrix(X.copy().transpose()))\n    else:\n        X_trans = imputer.transform(sparse.csc_matrix(X.copy().transpose()))\n\n        if sparse.issparse(X_trans):\n            X_trans = X_trans.toarray()\n\n        assert_ae(X_trans, X_true.transpose(),\n                  err_msg=err_msg.format(1, True))\n\n\n@ignore_warnings\ndef test_imputation_shape():\n    # Verify the shapes of the imputed matrix for different strategies.\n    X = np.random.randn(10, 2)\n    X[::2] = np.nan\n\n    for strategy in ['mean', 'median', 'most_frequent']:\n        imputer = Imputer(strategy=strategy)\n        X_imputed = imputer.fit_transform(X)\n        assert_equal(X_imputed.shape, (10, 2))\n        X_imputed = imputer.fit_transform(sparse.csr_matrix(X))\n        assert_equal(X_imputed.shape, (10, 2))\n\n\n@ignore_warnings\ndef test_imputation_mean_median_only_zero():\n    # Test imputation using the mean and median strategies, when\n    # missing_values == 0.\n    X = np.array([\n        [np.nan, 0, 0, 0, 5],\n        [np.nan, 1, 0, np.nan, 3],\n        [np.nan, 2, 0, 0, 0],\n        [np.nan, 6, 0, 5, 13],\n    ])\n\n    X_imputed_mean = np.array([\n        [3, 5],\n        [1, 3],\n        [2, 7],\n        [6, 13],\n    ])\n    statistics_mean = [np.nan, 3, np.nan, np.nan, 7]\n\n    # Behaviour of median with NaN is undefined, e.g. different results in\n    # np.median and np.ma.median\n    X_for_median = X[:, [0, 1, 2, 4]]\n    X_imputed_median = np.array([\n        [2, 5],\n        [1, 3],\n        [2, 5],\n        [6, 13],\n    ])\n    statistics_median = [np.nan, 2, np.nan, 5]\n\n    _check_statistics(X, X_imputed_mean, \"mean\", statistics_mean, 0)\n    _check_statistics(X_for_median, X_imputed_median, \"median\",\n                      statistics_median, 0)\n\n\ndef safe_median(arr, *args, **kwargs):\n    # np.median([]) raises a TypeError for numpy >= 1.10.1\n    length = arr.size if hasattr(arr, 'size') else len(arr)\n    return np.nan if length == 0 else np.median(arr, *args, **kwargs)\n\n\ndef safe_mean(arr, *args, **kwargs):\n    # np.mean([]) raises a RuntimeWarning for numpy >= 1.10.1\n    length = arr.size if hasattr(arr, 'size') else len(arr)\n    return np.nan if length == 0 else np.mean(arr, *args, **kwargs)\n\n\n@ignore_warnings\ndef test_imputation_mean_median():\n    # Test imputation using the mean and median strategies, when\n    # missing_values != 0.\n    rng = np.random.RandomState(0)\n\n    dim = 10\n    dec = 10\n    shape = (dim * dim, dim + dec)\n\n    zeros = np.zeros(shape[0])\n    values = np.arange(1, shape[0] + 1)\n    values[4::2] = - values[4::2]\n\n    tests = [(\"mean\", \"NaN\", lambda z, v, p: safe_mean(np.hstack((z, v)))),\n             (\"mean\", 0, lambda z, v, p: np.mean(v)),\n             (\"median\", \"NaN\", lambda z, v, p: safe_median(np.hstack((z, v)))),\n             (\"median\", 0, lambda z, v, p: np.median(v))]\n\n    for strategy, test_missing_values, true_value_fun in tests:\n        X = np.empty(shape)\n        X_true = np.empty(shape)\n        true_statistics = np.empty(shape[1])\n\n        # Create a matrix X with columns\n        #    - with only zeros,\n        #    - with only missing values\n        #    - with zeros, missing values and values\n        # And a matrix X_true containing all true values\n        for j in range(shape[1]):\n            nb_zeros = (j - dec + 1 > 0) * (j - dec + 1) * (j - dec + 1)\n            nb_missing_values = max(shape[0] + dec * dec\n                                    - (j + dec) * (j + dec), 0)\n            nb_values = shape[0] - nb_zeros - nb_missing_values\n\n            z = zeros[:nb_zeros]\n            p = np.repeat(test_missing_values, nb_missing_values)\n            v = values[rng.permutation(len(values))[:nb_values]]\n\n            true_statistics[j] = true_value_fun(z, v, p)\n\n            # Create the columns\n            X[:, j] = np.hstack((v, z, p))\n\n            if 0 == test_missing_values:\n                X_true[:, j] = np.hstack((v,\n                                          np.repeat(\n                                              true_statistics[j],\n                                              nb_missing_values + nb_zeros)))\n            else:\n                X_true[:, j] = np.hstack((v,\n                                          z,\n                                          np.repeat(true_statistics[j],\n                                                    nb_missing_values)))\n\n            # Shuffle them the same way\n            np.random.RandomState(j).shuffle(X[:, j])\n            np.random.RandomState(j).shuffle(X_true[:, j])\n\n        # Mean doesn't support columns containing NaNs, median does\n        if strategy == \"median\":\n            cols_to_keep = ~np.isnan(X_true).any(axis=0)\n        else:\n            cols_to_keep = ~np.isnan(X_true).all(axis=0)\n\n        X_true = X_true[:, cols_to_keep]\n\n        _check_statistics(X, X_true, strategy,\n                          true_statistics, test_missing_values)\n\n\n@ignore_warnings\ndef test_imputation_median_special_cases():\n    # Test median imputation with sparse boundary cases\n    X = np.array([\n        [0, np.nan, np.nan],  # odd: implicit zero\n        [5, np.nan, np.nan],  # odd: explicit nonzero\n        [0, 0, np.nan],    # even: average two zeros\n        [-5, 0, np.nan],   # even: avg zero and neg\n        [0, 5, np.nan],    # even: avg zero and pos\n        [4, 5, np.nan],    # even: avg nonzeros\n        [-4, -5, np.nan],  # even: avg negatives\n        [-1, 2, np.nan],   # even: crossing neg and pos\n    ]).transpose()\n\n    X_imputed_median = np.array([\n        [0, 0, 0],\n        [5, 5, 5],\n        [0, 0, 0],\n        [-5, 0, -2.5],\n        [0, 5, 2.5],\n        [4, 5, 4.5],\n        [-4, -5, -4.5],\n        [-1, 2, .5],\n    ]).transpose()\n    statistics_median = [0, 5, 0, -2.5, 2.5, 4.5, -4.5, .5]\n\n    _check_statistics(X, X_imputed_median, \"median\",\n                      statistics_median, 'NaN')\n\n\n@ignore_warnings\ndef test_imputation_most_frequent():\n    # Test imputation using the most-frequent strategy.\n    X = np.array([\n        [-1, -1, 0, 5],\n        [-1, 2, -1, 3],\n        [-1, 1, 3, -1],\n        [-1, 2, 3, 7],\n    ])\n\n    X_true = np.array([\n        [2, 0, 5],\n        [2, 3, 3],\n        [1, 3, 3],\n        [2, 3, 7],\n    ])\n\n    # scipy.stats.mode, used in Imputer, doesn't return the first most\n    # frequent as promised in the doc but the lowest most frequent. When this\n    # test will fail after an update of scipy, Imputer will need to be updated\n    # to be consistent with the new (correct) behaviour\n    _check_statistics(X, X_true, \"most_frequent\", [np.nan, 2, 3, 3], -1)\n\n\n@ignore_warnings\ndef test_imputation_pipeline_grid_search():\n    # Test imputation within a pipeline + gridsearch.\n    pipeline = Pipeline([('imputer', Imputer(missing_values=0)),\n                         ('tree', tree.DecisionTreeRegressor(random_state=0))])\n\n    parameters = {\n        'imputer__strategy': [\"mean\", \"median\", \"most_frequent\"],\n        'imputer__axis': [0, 1]\n    }\n\n    l = 100\n    X = sparse_random_matrix(l, l, density=0.10)\n    Y = sparse_random_matrix(l, 1, density=0.10).toarray()\n    gs = GridSearchCV(pipeline, parameters)\n    gs.fit(X, Y)\n\n\n@ignore_warnings\ndef test_imputation_pickle():\n    # Test for pickling imputers.\n    import pickle\n\n    l = 100\n    X = sparse_random_matrix(l, l, density=0.10)\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = Imputer(missing_values=0, strategy=strategy)\n        imputer.fit(X)\n\n        imputer_pickled = pickle.loads(pickle.dumps(imputer))\n\n        assert_array_almost_equal(\n            imputer.transform(X.copy()),\n            imputer_pickled.transform(X.copy()),\n            err_msg=\"Fail to transform the data after pickling \"\n            \"(strategy = %s)\" % (strategy)\n        )\n\n\n@ignore_warnings\ndef test_imputation_copy():\n    # Test imputation with copy\n    X_orig = sparse_random_matrix(5, 5, density=0.75, random_state=0)\n\n    # copy=True, dense => copy\n    X = X_orig.copy().toarray()\n    imputer = Imputer(missing_values=0, strategy=\"mean\", copy=True)\n    Xt = imputer.fit(X).transform(X)\n    Xt[0, 0] = -1\n    assert_false(np.all(X == Xt))\n\n    # copy=True, sparse csr => copy\n    X = X_orig.copy()\n    imputer = Imputer(missing_values=X.data[0], strategy=\"mean\", copy=True)\n    Xt = imputer.fit(X).transform(X)\n    Xt.data[0] = -1\n    assert_false(np.all(X.data == Xt.data))\n\n    # copy=False, dense => no copy\n    X = X_orig.copy().toarray()\n    imputer = Imputer(missing_values=0, strategy=\"mean\", copy=False)\n    Xt = imputer.fit(X).transform(X)\n    Xt[0, 0] = -1\n    assert_array_almost_equal(X, Xt)\n\n    # copy=False, sparse csr, axis=1 => no copy\n    X = X_orig.copy()\n    imputer = Imputer(missing_values=X.data[0], strategy=\"mean\",\n                      copy=False, axis=1)\n    Xt = imputer.fit(X).transform(X)\n    Xt.data[0] = -1\n    assert_array_almost_equal(X.data, Xt.data)\n\n    # copy=False, sparse csc, axis=0 => no copy\n    X = X_orig.copy().tocsc()\n    imputer = Imputer(missing_values=X.data[0], strategy=\"mean\",\n                      copy=False, axis=0)\n    Xt = imputer.fit(X).transform(X)\n    Xt.data[0] = -1\n    assert_array_almost_equal(X.data, Xt.data)\n\n    # copy=False, sparse csr, axis=0 => copy\n    X = X_orig.copy()\n    imputer = Imputer(missing_values=X.data[0], strategy=\"mean\",\n                      copy=False, axis=0)\n    Xt = imputer.fit(X).transform(X)\n    Xt.data[0] = -1\n    assert_false(np.all(X.data == Xt.data))\n\n    # copy=False, sparse csc, axis=1 => copy\n    X = X_orig.copy().tocsc()\n    imputer = Imputer(missing_values=X.data[0], strategy=\"mean\",\n                      copy=False, axis=1)\n    Xt = imputer.fit(X).transform(X)\n    Xt.data[0] = -1\n    assert_false(np.all(X.data == Xt.data))\n\n    # copy=False, sparse csr, axis=1, missing_values=0 => copy\n    X = X_orig.copy()\n    imputer = Imputer(missing_values=0, strategy=\"mean\",\n                      copy=False, axis=1)\n    Xt = imputer.fit(X).transform(X)\n    assert_false(sparse.issparse(Xt))\n\n    # Note: If X is sparse and if missing_values=0, then a (dense) copy of X is\n    # made, even if copy=False.\n"
    },
    {
      "filename": "sklearn/tests/test_calibration.py",
      "content": "# Authors: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n# License: BSD 3 clause\n\nfrom __future__ import division\nimport numpy as np\nfrom scipy import sparse\nfrom sklearn.model_selection import LeaveOneOut\n\nfrom sklearn.utils.testing import (assert_array_almost_equal, assert_equal,\n                                   assert_greater, assert_almost_equal,\n                                   assert_greater_equal,\n                                   assert_array_equal,\n                                   assert_raises,\n                                   ignore_warnings)\nfrom sklearn.datasets import make_classification, make_blobs\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import brier_score_loss, log_loss\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.calibration import _sigmoid_calibration, _SigmoidCalibration\nfrom sklearn.calibration import calibration_curve\n\n\n@ignore_warnings\ndef test_calibration():\n    \"\"\"Test calibration objects with isotonic and sigmoid\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=2 * n_samples, n_features=6,\n                               random_state=42)\n    sample_weight = np.random.RandomState(seed=42).uniform(size=y.size)\n\n    X -= X.min()  # MultinomialNB only allows positive X\n\n    # split train and test\n    X_train, y_train, sw_train = \\\n        X[:n_samples], y[:n_samples], sample_weight[:n_samples]\n    X_test, y_test = X[n_samples:], y[n_samples:]\n\n    # Naive-Bayes\n    clf = MultinomialNB().fit(X_train, y_train, sample_weight=sw_train)\n    prob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n    pc_clf = CalibratedClassifierCV(clf, cv=y.size + 1)\n    assert_raises(ValueError, pc_clf.fit, X, y)\n\n    # Naive Bayes with calibration\n    for this_X_train, this_X_test in [(X_train, X_test),\n                                      (sparse.csr_matrix(X_train),\n                                       sparse.csr_matrix(X_test))]:\n        for method in ['isotonic', 'sigmoid']:\n            pc_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n            # Note that this fit overwrites the fit on the entire training\n            # set\n            pc_clf.fit(this_X_train, y_train, sample_weight=sw_train)\n            prob_pos_pc_clf = pc_clf.predict_proba(this_X_test)[:, 1]\n\n            # Check that brier score has improved after calibration\n            assert_greater(brier_score_loss(y_test, prob_pos_clf),\n                           brier_score_loss(y_test, prob_pos_pc_clf))\n\n            # Check invariance against relabeling [0, 1] -> [1, 2]\n            pc_clf.fit(this_X_train, y_train + 1, sample_weight=sw_train)\n            prob_pos_pc_clf_relabeled = pc_clf.predict_proba(this_X_test)[:, 1]\n            assert_array_almost_equal(prob_pos_pc_clf,\n                                      prob_pos_pc_clf_relabeled)\n\n            # Check invariance against relabeling [0, 1] -> [-1, 1]\n            pc_clf.fit(this_X_train, 2 * y_train - 1, sample_weight=sw_train)\n            prob_pos_pc_clf_relabeled = pc_clf.predict_proba(this_X_test)[:, 1]\n            assert_array_almost_equal(prob_pos_pc_clf,\n                                      prob_pos_pc_clf_relabeled)\n\n            # Check invariance against relabeling [0, 1] -> [1, 0]\n            pc_clf.fit(this_X_train, (y_train + 1) % 2,\n                       sample_weight=sw_train)\n            prob_pos_pc_clf_relabeled = \\\n                pc_clf.predict_proba(this_X_test)[:, 1]\n            if method == \"sigmoid\":\n                assert_array_almost_equal(prob_pos_pc_clf,\n                                          1 - prob_pos_pc_clf_relabeled)\n            else:\n                # Isotonic calibration is not invariant against relabeling\n                # but should improve in both cases\n                assert_greater(brier_score_loss(y_test, prob_pos_clf),\n                               brier_score_loss((y_test + 1) % 2,\n                                                prob_pos_pc_clf_relabeled))\n\n        # Check failure cases:\n        # only \"isotonic\" and \"sigmoid\" should be accepted as methods\n        clf_invalid_method = CalibratedClassifierCV(clf, method=\"foo\")\n        assert_raises(ValueError, clf_invalid_method.fit, X_train, y_train)\n\n        # base-estimators should provide either decision_function or\n        # predict_proba (most regressors, for instance, should fail)\n        clf_base_regressor = \\\n            CalibratedClassifierCV(RandomForestRegressor(), method=\"sigmoid\")\n        assert_raises(RuntimeError, clf_base_regressor.fit, X_train, y_train)\n\n\ndef test_sample_weight():\n    n_samples = 100\n    X, y = make_classification(n_samples=2 * n_samples, n_features=6,\n                               random_state=42)\n\n    sample_weight = np.random.RandomState(seed=42).uniform(size=len(y))\n    X_train, y_train, sw_train = \\\n        X[:n_samples], y[:n_samples], sample_weight[:n_samples]\n    X_test = X[n_samples:]\n\n    for method in ['sigmoid', 'isotonic']:\n        base_estimator = LinearSVC(random_state=42)\n        calibrated_clf = CalibratedClassifierCV(base_estimator, method=method)\n        calibrated_clf.fit(X_train, y_train, sample_weight=sw_train)\n        probs_with_sw = calibrated_clf.predict_proba(X_test)\n\n        # As the weights are used for the calibration, they should still yield\n        # a different predictions\n        calibrated_clf.fit(X_train, y_train)\n        probs_without_sw = calibrated_clf.predict_proba(X_test)\n\n        diff = np.linalg.norm(probs_with_sw - probs_without_sw)\n        assert_greater(diff, 0.1)\n\n\ndef test_calibration_multiclass():\n    \"\"\"Test calibration for multiclass \"\"\"\n    # test multi-class setting with classifier that implements\n    # only decision function\n    clf = LinearSVC()\n    X, y_idx = make_blobs(n_samples=100, n_features=2, random_state=42,\n                          centers=3, cluster_std=3.0)\n\n    # Use categorical labels to check that CalibratedClassifierCV supports\n    # them correctly\n    target_names = np.array(['a', 'b', 'c'])\n    y = target_names[y_idx]\n\n    X_train, y_train = X[::2], y[::2]\n    X_test, y_test = X[1::2], y[1::2]\n\n    clf.fit(X_train, y_train)\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        cal_clf.fit(X_train, y_train)\n        probas = cal_clf.predict_proba(X_test)\n        assert_array_almost_equal(np.sum(probas, axis=1), np.ones(len(X_test)))\n\n        # Check that log-loss of calibrated classifier is smaller than\n        # log-loss of naively turned OvR decision function to probabilities\n        # via softmax\n        def softmax(y_pred):\n            e = np.exp(-y_pred)\n            return e / e.sum(axis=1).reshape(-1, 1)\n\n        uncalibrated_log_loss = \\\n            log_loss(y_test, softmax(clf.decision_function(X_test)))\n        calibrated_log_loss = log_loss(y_test, probas)\n        assert_greater_equal(uncalibrated_log_loss, calibrated_log_loss)\n\n    # Test that calibration of a multiclass classifier decreases log-loss\n    # for RandomForestClassifier\n    X, y = make_blobs(n_samples=100, n_features=2, random_state=42,\n                      cluster_std=3.0)\n    X_train, y_train = X[::2], y[::2]\n    X_test, y_test = X[1::2], y[1::2]\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n    clf.fit(X_train, y_train)\n    clf_probs = clf.predict_proba(X_test)\n    loss = log_loss(y_test, clf_probs)\n\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=3)\n        cal_clf.fit(X_train, y_train)\n        cal_clf_probs = cal_clf.predict_proba(X_test)\n        cal_loss = log_loss(y_test, cal_clf_probs)\n        assert_greater(loss, cal_loss)\n\n\ndef test_calibration_prefit():\n    \"\"\"Test calibration for prefitted classifiers\"\"\"\n    n_samples = 50\n    X, y = make_classification(n_samples=3 * n_samples, n_features=6,\n                               random_state=42)\n    sample_weight = np.random.RandomState(seed=42).uniform(size=y.size)\n\n    X -= X.min()  # MultinomialNB only allows positive X\n\n    # split train and test\n    X_train, y_train, sw_train = \\\n        X[:n_samples], y[:n_samples], sample_weight[:n_samples]\n    X_calib, y_calib, sw_calib = \\\n        X[n_samples:2 * n_samples], y[n_samples:2 * n_samples], \\\n        sample_weight[n_samples:2 * n_samples]\n    X_test, y_test = X[2 * n_samples:], y[2 * n_samples:]\n\n    # Naive-Bayes\n    clf = MultinomialNB()\n    clf.fit(X_train, y_train, sw_train)\n    prob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n    # Naive Bayes with calibration\n    for this_X_calib, this_X_test in [(X_calib, X_test),\n                                      (sparse.csr_matrix(X_calib),\n                                       sparse.csr_matrix(X_test))]:\n        for method in ['isotonic', 'sigmoid']:\n            pc_clf = CalibratedClassifierCV(clf, method=method, cv=\"prefit\")\n\n            for sw in [sw_calib, None]:\n                pc_clf.fit(this_X_calib, y_calib, sample_weight=sw)\n                y_prob = pc_clf.predict_proba(this_X_test)\n                y_pred = pc_clf.predict(this_X_test)\n                prob_pos_pc_clf = y_prob[:, 1]\n                assert_array_equal(y_pred,\n                                   np.array([0, 1])[np.argmax(y_prob, axis=1)])\n\n                assert_greater(brier_score_loss(y_test, prob_pos_clf),\n                               brier_score_loss(y_test, prob_pos_pc_clf))\n\n\ndef test_sigmoid_calibration():\n    \"\"\"Test calibration values with Platt sigmoid model\"\"\"\n    exF = np.array([5, -4, 1.0])\n    exY = np.array([1, -1, -1])\n    # computed from my python port of the C++ code in LibSVM\n    AB_lin_libsvm = np.array([-0.20261354391187855, 0.65236314980010512])\n    assert_array_almost_equal(AB_lin_libsvm,\n                              _sigmoid_calibration(exF, exY), 3)\n    lin_prob = 1. / (1. + np.exp(AB_lin_libsvm[0] * exF + AB_lin_libsvm[1]))\n    sk_prob = _SigmoidCalibration().fit(exF, exY).predict(exF)\n    assert_array_almost_equal(lin_prob, sk_prob, 6)\n\n    # check that _SigmoidCalibration().fit only accepts 1d array or 2d column\n    # arrays\n    assert_raises(ValueError, _SigmoidCalibration().fit,\n                  np.vstack((exF, exF)), exY)\n\n\ndef test_calibration_curve():\n    \"\"\"Check calibration_curve function\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2)\n    prob_true_unnormalized, prob_pred_unnormalized = \\\n        calibration_curve(y_true, y_pred * 2, n_bins=2, normalize=True)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 2)\n    assert_almost_equal(prob_true, [0, 1])\n    assert_almost_equal(prob_pred, [0.1, 0.9])\n    assert_almost_equal(prob_true, prob_true_unnormalized)\n    assert_almost_equal(prob_pred, prob_pred_unnormalized)\n\n    # probabilities outside [0, 1] should not be accepted when normalize\n    # is set to False\n    assert_raises(ValueError, calibration_curve, [1.1], [-0.1],\n                  normalize=False)\n\n\ndef test_calibration_nan_imputer():\n    \"\"\"Test that calibration can accept nan\"\"\"\n    X, y = make_classification(n_samples=10, n_features=2,\n                               n_informative=2, n_redundant=0,\n                               random_state=42)\n    X[0, 0] = np.nan\n    clf = Pipeline(\n        [('imputer', SimpleImputer()),\n         ('rf', RandomForestClassifier(n_estimators=1))])\n    clf_c = CalibratedClassifierCV(clf, cv=2, method='isotonic')\n    clf_c.fit(X, y)\n    clf_c.predict(X)\n\n\ndef test_calibration_prob_sum():\n    # Test that sum of probabilities is 1. A non-regression test for\n    # issue #7796\n    num_classes = 2\n    X, y = make_classification(n_samples=10, n_features=5,\n                               n_classes=num_classes)\n    clf = LinearSVC(C=1.0)\n    clf_prob = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=LeaveOneOut())\n    clf_prob.fit(X, y)\n\n    probs = clf_prob.predict_proba(X)\n    assert_array_almost_equal(probs.sum(axis=1), np.ones(probs.shape[0]))\n\n\ndef test_calibration_less_classes():\n    # Test to check calibration works fine when train set in a test-train\n    # split does not contain all classes\n    # Since this test uses LOO, at each iteration train set will not contain a\n    # class label\n    X = np.random.randn(10, 5)\n    y = np.arange(10)\n    clf = LinearSVC(C=1.0)\n    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=LeaveOneOut())\n    cal_clf.fit(X, y)\n\n    for i, calibrated_classifier in \\\n            enumerate(cal_clf.calibrated_classifiers_):\n        proba = calibrated_classifier.predict_proba(X)\n        assert_array_equal(proba[:, i], np.zeros(len(y)))\n        assert_equal(np.all(np.hstack([proba[:, :i],\n                                       proba[:, i + 1:]])), True)\n"
    },
    {
      "filename": "sklearn/tests/test_impute.py",
      "content": "\nimport numpy as np\nfrom scipy import sparse\n\nfrom sklearn.utils.testing import assert_equal\nfrom sklearn.utils.testing import assert_array_equal\nfrom sklearn.utils.testing import assert_array_almost_equal\nfrom sklearn.utils.testing import assert_raises\nfrom sklearn.utils.testing import assert_false\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import tree\nfrom sklearn.random_projection import sparse_random_matrix\n\n\ndef _check_statistics(X, X_true,\n                      strategy, statistics, missing_values):\n    \"\"\"Utility function for testing imputation for a given strategy.\n\n    Test:\n        - along the two axes\n        - with dense and sparse arrays\n\n    Check that:\n        - the statistics (mean, median, mode) are correct\n        - the missing values are imputed correctly\"\"\"\n\n    err_msg = \"Parameters: strategy = %s, missing_values = %s, \" \\\n              \"axis = {0}, sparse = {1}\" % (strategy, missing_values)\n\n    assert_ae = assert_array_equal\n    if X.dtype.kind == 'f' or X_true.dtype.kind == 'f':\n        assert_ae = assert_array_almost_equal\n\n    # Normal matrix, axis = 0\n    imputer = SimpleImputer(missing_values, strategy=strategy, axis=0)\n    X_trans = imputer.fit(X).transform(X.copy())\n    assert_ae(imputer.statistics_, statistics,\n              err_msg=err_msg.format(0, False))\n    assert_ae(X_trans, X_true, err_msg=err_msg.format(0, False))\n\n    # Normal matrix, axis = 1\n    imputer = SimpleImputer(missing_values, strategy=strategy, axis=1)\n    imputer.fit(X.transpose())\n    if np.isnan(statistics).any():\n        assert_raises(ValueError, imputer.transform, X.copy().transpose())\n    else:\n        X_trans = imputer.transform(X.copy().transpose())\n        assert_ae(X_trans, X_true.transpose(),\n                  err_msg=err_msg.format(1, False))\n\n    # Sparse matrix, axis = 0\n    imputer = SimpleImputer(missing_values, strategy=strategy, axis=0)\n    imputer.fit(sparse.csc_matrix(X))\n    X_trans = imputer.transform(sparse.csc_matrix(X.copy()))\n\n    if sparse.issparse(X_trans):\n        X_trans = X_trans.toarray()\n\n    assert_ae(imputer.statistics_, statistics,\n              err_msg=err_msg.format(0, True))\n    assert_ae(X_trans, X_true, err_msg=err_msg.format(0, True))\n\n    # Sparse matrix, axis = 1\n    imputer = SimpleImputer(missing_values, strategy=strategy, axis=1)\n    imputer.fit(sparse.csc_matrix(X.transpose()))\n    if np.isnan(statistics).any():\n        assert_raises(ValueError, imputer.transform,\n                      sparse.csc_matrix(X.copy().transpose()))\n    else:\n        X_trans = imputer.transform(sparse.csc_matrix(X.copy().transpose()))\n\n        if sparse.issparse(X_trans):\n            X_trans = X_trans.toarray()\n\n        assert_ae(X_trans, X_true.transpose(),\n                  err_msg=err_msg.format(1, True))\n\n\ndef test_imputation_shape():\n    # Verify the shapes of the imputed matrix for different strategies.\n    X = np.random.randn(10, 2)\n    X[::2] = np.nan\n\n    for strategy in ['mean', 'median', 'most_frequent']:\n        imputer = SimpleImputer(strategy=strategy)\n        X_imputed = imputer.fit_transform(X)\n        assert_equal(X_imputed.shape, (10, 2))\n        X_imputed = imputer.fit_transform(sparse.csr_matrix(X))\n        assert_equal(X_imputed.shape, (10, 2))\n\n\ndef test_imputation_mean_median_only_zero():\n    # Test imputation using the mean and median strategies, when\n    # missing_values == 0.\n    X = np.array([\n        [np.nan, 0, 0, 0, 5],\n        [np.nan, 1, 0, np.nan, 3],\n        [np.nan, 2, 0, 0, 0],\n        [np.nan, 6, 0, 5, 13],\n    ])\n\n    X_imputed_mean = np.array([\n        [3, 5],\n        [1, 3],\n        [2, 7],\n        [6, 13],\n    ])\n    statistics_mean = [np.nan, 3, np.nan, np.nan, 7]\n\n    # Behaviour of median with NaN is undefined, e.g. different results in\n    # np.median and np.ma.median\n    X_for_median = X[:, [0, 1, 2, 4]]\n    X_imputed_median = np.array([\n        [2, 5],\n        [1, 3],\n        [2, 5],\n        [6, 13],\n    ])\n    statistics_median = [np.nan, 2, np.nan, 5]\n\n    _check_statistics(X, X_imputed_mean, \"mean\", statistics_mean, 0)\n    _check_statistics(X_for_median, X_imputed_median, \"median\",\n                      statistics_median, 0)\n\n\ndef safe_median(arr, *args, **kwargs):\n    # np.median([]) raises a TypeError for numpy >= 1.10.1\n    length = arr.size if hasattr(arr, 'size') else len(arr)\n    return np.nan if length == 0 else np.median(arr, *args, **kwargs)\n\n\ndef safe_mean(arr, *args, **kwargs):\n    # np.mean([]) raises a RuntimeWarning for numpy >= 1.10.1\n    length = arr.size if hasattr(arr, 'size') else len(arr)\n    return np.nan if length == 0 else np.mean(arr, *args, **kwargs)\n\n\ndef test_imputation_mean_median():\n    # Test imputation using the mean and median strategies, when\n    # missing_values != 0.\n    rng = np.random.RandomState(0)\n\n    dim = 10\n    dec = 10\n    shape = (dim * dim, dim + dec)\n\n    zeros = np.zeros(shape[0])\n    values = np.arange(1, shape[0] + 1)\n    values[4::2] = - values[4::2]\n\n    tests = [(\"mean\", \"NaN\", lambda z, v, p: safe_mean(np.hstack((z, v)))),\n             (\"mean\", 0, lambda z, v, p: np.mean(v)),\n             (\"median\", \"NaN\", lambda z, v, p: safe_median(np.hstack((z, v)))),\n             (\"median\", 0, lambda z, v, p: np.median(v))]\n\n    for strategy, test_missing_values, true_value_fun in tests:\n        X = np.empty(shape)\n        X_true = np.empty(shape)\n        true_statistics = np.empty(shape[1])\n\n        # Create a matrix X with columns\n        #    - with only zeros,\n        #    - with only missing values\n        #    - with zeros, missing values and values\n        # And a matrix X_true containing all true values\n        for j in range(shape[1]):\n            nb_zeros = (j - dec + 1 > 0) * (j - dec + 1) * (j - dec + 1)\n            nb_missing_values = max(shape[0] + dec * dec\n                                    - (j + dec) * (j + dec), 0)\n            nb_values = shape[0] - nb_zeros - nb_missing_values\n\n            z = zeros[:nb_zeros]\n            p = np.repeat(test_missing_values, nb_missing_values)\n            v = values[rng.permutation(len(values))[:nb_values]]\n\n            true_statistics[j] = true_value_fun(z, v, p)\n\n            # Create the columns\n            X[:, j] = np.hstack((v, z, p))\n\n            if 0 == test_missing_values:\n                X_true[:, j] = np.hstack((v,\n                                          np.repeat(\n                                              true_statistics[j],\n                                              nb_missing_values + nb_zeros)))\n            else:\n                X_true[:, j] = np.hstack((v,\n                                          z,\n                                          np.repeat(true_statistics[j],\n                                                    nb_missing_values)))\n\n            # Shuffle them the same way\n            np.random.RandomState(j).shuffle(X[:, j])\n            np.random.RandomState(j).shuffle(X_true[:, j])\n\n        # Mean doesn't support columns containing NaNs, median does\n        if strategy == \"median\":\n            cols_to_keep = ~np.isnan(X_true).any(axis=0)\n        else:\n            cols_to_keep = ~np.isnan(X_true).all(axis=0)\n\n        X_true = X_true[:, cols_to_keep]\n\n        _check_statistics(X, X_true, strategy,\n                          true_statistics, test_missing_values)\n\n\ndef test_imputation_median_special_cases():\n    # Test median imputation with sparse boundary cases\n    X = np.array([\n        [0, np.nan, np.nan],  # odd: implicit zero\n        [5, np.nan, np.nan],  # odd: explicit nonzero\n        [0, 0, np.nan],    # even: average two zeros\n        [-5, 0, np.nan],   # even: avg zero and neg\n        [0, 5, np.nan],    # even: avg zero and pos\n        [4, 5, np.nan],    # even: avg nonzeros\n        [-4, -5, np.nan],  # even: avg negatives\n        [-1, 2, np.nan],   # even: crossing neg and pos\n    ]).transpose()\n\n    X_imputed_median = np.array([\n        [0, 0, 0],\n        [5, 5, 5],\n        [0, 0, 0],\n        [-5, 0, -2.5],\n        [0, 5, 2.5],\n        [4, 5, 4.5],\n        [-4, -5, -4.5],\n        [-1, 2, .5],\n    ]).transpose()\n    statistics_median = [0, 5, 0, -2.5, 2.5, 4.5, -4.5, .5]\n\n    _check_statistics(X, X_imputed_median, \"median\",\n                      statistics_median, 'NaN')\n\n\ndef test_imputation_most_frequent():\n    # Test imputation using the most-frequent strategy.\n    X = np.array([\n        [-1, -1, 0, 5],\n        [-1, 2, -1, 3],\n        [-1, 1, 3, -1],\n        [-1, 2, 3, 7],\n    ])\n\n    X_true = np.array([\n        [2, 0, 5],\n        [2, 3, 3],\n        [1, 3, 3],\n        [2, 3, 7],\n    ])\n\n    # scipy.stats.mode, used in SimpleImputer, doesn't return the first most\n    # frequent as promised in the doc but the lowest most frequent. When this\n    # test will fail after an update of scipy, SimpleImputer will need to be\n    # updated to be consistent with the new (correct) behaviour\n    _check_statistics(X, X_true, \"most_frequent\", [np.nan, 2, 3, 3], -1)\n\n\ndef test_imputation_pipeline_grid_search():\n    # Test imputation within a pipeline + gridsearch.\n    pipeline = Pipeline([('imputer', SimpleImputer(missing_values=0)),\n                         ('tree', tree.DecisionTreeRegressor(random_state=0))])\n\n    parameters = {\n        'imputer__strategy': [\"mean\", \"median\", \"most_frequent\"],\n        'imputer__axis': [0, 1]\n    }\n\n    X = sparse_random_matrix(100, 100, density=0.10)\n    Y = sparse_random_matrix(100, 1, density=0.10).toarray()\n    gs = GridSearchCV(pipeline, parameters)\n    gs.fit(X, Y)\n\n\ndef test_imputation_pickle():\n    # Test for pickling imputers.\n    import pickle\n\n    X = sparse_random_matrix(100, 100, density=0.10)\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = SimpleImputer(missing_values=0, strategy=strategy)\n        imputer.fit(X)\n\n        imputer_pickled = pickle.loads(pickle.dumps(imputer))\n\n        assert_array_almost_equal(\n            imputer.transform(X.copy()),\n            imputer_pickled.transform(X.copy()),\n            err_msg=\"Fail to transform the data after pickling \"\n            \"(strategy = %s)\" % (strategy)\n        )\n\n\ndef test_imputation_copy():\n    # Test imputation with copy\n    X_orig = sparse_random_matrix(5, 5, density=0.75, random_state=0)\n\n    # copy=True, dense => copy\n    X = X_orig.copy().toarray()\n    imputer = SimpleImputer(missing_values=0, strategy=\"mean\", copy=True)\n    Xt = imputer.fit(X).transform(X)\n    Xt[0, 0] = -1\n    assert_false(np.all(X == Xt))\n\n    # copy=True, sparse csr => copy\n    X = X_orig.copy()\n    imputer = SimpleImputer(missing_values=X.data[0], strategy=\"mean\",\n                            copy=True)\n    Xt = imputer.fit(X).transform(X)\n    Xt.data[0] = -1\n    assert_false(np.all(X.data == Xt.data))\n\n    # copy=False, dense => no copy\n    X = X_orig.copy().toarray()\n    imputer = SimpleImputer(missing_values=0, strategy=\"mean\", copy=False)\n    Xt = imputer.fit(X).transform(X)\n    Xt[0, 0] = -1\n    assert_array_almost_equal(X, Xt)\n\n    # copy=False, sparse csr, axis=1 => no copy\n    X = X_orig.copy()\n    imputer = SimpleImputer(missing_values=X.data[0], strategy=\"mean\",\n                            copy=False, axis=1)\n    Xt = imputer.fit(X).transform(X)\n    Xt.data[0] = -1\n    assert_array_almost_equal(X.data, Xt.data)\n\n    # copy=False, sparse csc, axis=0 => no copy\n    X = X_orig.copy().tocsc()\n    imputer = SimpleImputer(missing_values=X.data[0], strategy=\"mean\",\n                            copy=False, axis=0)\n    Xt = imputer.fit(X).transform(X)\n    Xt.data[0] = -1\n    assert_array_almost_equal(X.data, Xt.data)\n\n    # copy=False, sparse csr, axis=0 => copy\n    X = X_orig.copy()\n    imputer = SimpleImputer(missing_values=X.data[0], strategy=\"mean\",\n                            copy=False, axis=0)\n    Xt = imputer.fit(X).transform(X)\n    Xt.data[0] = -1\n    assert_false(np.all(X.data == Xt.data))\n\n    # copy=False, sparse csc, axis=1 => copy\n    X = X_orig.copy().tocsc()\n    imputer = SimpleImputer(missing_values=X.data[0], strategy=\"mean\",\n                            copy=False, axis=1)\n    Xt = imputer.fit(X).transform(X)\n    Xt.data[0] = -1\n    assert_false(np.all(X.data == Xt.data))\n\n    # copy=False, sparse csr, axis=1, missing_values=0 => copy\n    X = X_orig.copy()\n    imputer = SimpleImputer(missing_values=0, strategy=\"mean\",\n                            copy=False, axis=1)\n    Xt = imputer.fit(X).transform(X)\n    assert_false(sparse.issparse(Xt))\n\n    # Note: If X is sparse and if missing_values=0, then a (dense) copy of X is\n    # made, even if copy=False.\n"
    },
    {
      "filename": "sklearn/utils/estimator_checks.py",
      "content": "from __future__ import print_function\n\nimport types\nimport warnings\nimport sys\nimport traceback\nimport pickle\nfrom copy import deepcopy\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.stats import rankdata\nimport struct\n\nfrom sklearn.externals.six.moves import zip\nfrom sklearn.externals.joblib import hash, Memory\nfrom sklearn.utils.testing import assert_raises, _get_args\nfrom sklearn.utils.testing import assert_raises_regex\nfrom sklearn.utils.testing import assert_raise_message\nfrom sklearn.utils.testing import assert_equal\nfrom sklearn.utils.testing import assert_not_equal\nfrom sklearn.utils.testing import assert_true\nfrom sklearn.utils.testing import assert_false\nfrom sklearn.utils.testing import assert_in\nfrom sklearn.utils.testing import assert_array_equal\nfrom sklearn.utils.testing import assert_allclose\nfrom sklearn.utils.testing import assert_allclose_dense_sparse\nfrom sklearn.utils.testing import assert_warns_message\nfrom sklearn.utils.testing import META_ESTIMATORS\nfrom sklearn.utils.testing import set_random_state\nfrom sklearn.utils.testing import assert_greater\nfrom sklearn.utils.testing import assert_greater_equal\nfrom sklearn.utils.testing import SkipTest\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.utils.testing import assert_dict_equal\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nfrom sklearn.base import (clone, TransformerMixin, ClusterMixin,\n                          BaseEstimator, is_classifier, is_regressor)\n\nfrom sklearn.metrics import accuracy_score, adjusted_rand_score, f1_score\n\nfrom sklearn.random_projection import BaseRandomProjection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.svm.base import BaseLibSVM\nfrom sklearn.linear_model.stochastic_gradient import BaseSGD\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.exceptions import DataConversionWarning\nfrom sklearn.exceptions import SkipTestWarning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import (rbf_kernel, linear_kernel,\n                                      pairwise_distances)\n\nfrom sklearn.utils import shuffle\nfrom sklearn.utils.fixes import signature\nfrom sklearn.utils.validation import has_fit_parameter, _num_samples\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris, load_boston, make_blobs\n\n\nBOSTON = None\nCROSS_DECOMPOSITION = ['PLSCanonical', 'PLSRegression', 'CCA', 'PLSSVD']\nMULTI_OUTPUT = ['CCA', 'DecisionTreeRegressor', 'ElasticNet',\n                'ExtraTreeRegressor', 'ExtraTreesRegressor', 'GaussianProcess',\n                'GaussianProcessRegressor', 'TransformedTargetRegressor',\n                'KNeighborsRegressor', 'KernelRidge', 'Lars', 'Lasso',\n                'LassoLars', 'LinearRegression', 'MultiTaskElasticNet',\n                'MultiTaskElasticNetCV', 'MultiTaskLasso', 'MultiTaskLassoCV',\n                'OrthogonalMatchingPursuit', 'PLSCanonical', 'PLSRegression',\n                'RANSACRegressor', 'RadiusNeighborsRegressor',\n                'RandomForestRegressor', 'Ridge', 'RidgeCV']\n\n\ndef _yield_non_meta_checks(name, estimator):\n    yield check_estimators_dtypes\n    yield check_fit_score_takes_y\n    yield check_dtype_object\n    yield check_sample_weights_pandas_series\n    yield check_sample_weights_list\n    yield check_estimators_fit_returns_self\n    yield check_complex_data\n\n    # Check that all estimator yield informative messages when\n    # trained on empty datasets\n    yield check_estimators_empty_data_messages\n\n    if name not in CROSS_DECOMPOSITION + ['SpectralEmbedding']:\n        # SpectralEmbedding is non-deterministic,\n        # see issue #4236\n        # cross-decomposition's \"transform\" returns X and Y\n        yield check_pipeline_consistency\n\n    if name not in ['SimpleImputer', 'Imputer']:\n        # Test that all estimators check their input for NaN's and infs\n        yield check_estimators_nan_inf\n\n    if name not in ['GaussianProcess']:\n        # FIXME!\n        # in particular GaussianProcess!\n        yield check_estimators_overwrite_params\n    if hasattr(estimator, 'sparsify'):\n        yield check_sparsify_coefficients\n\n    yield check_estimator_sparse_data\n\n    # Test that estimators can be pickled, and once pickled\n    # give the same answer as before.\n    yield check_estimators_pickle\n\n\ndef _yield_classifier_checks(name, classifier):\n    # test classifiers can handle non-array data\n    yield check_classifier_data_not_an_array\n    # test classifiers trained on a single label always return this label\n    yield check_classifiers_one_label\n    yield check_classifiers_classes\n    yield check_estimators_partial_fit_n_features\n    # basic consistency testing\n    yield check_classifiers_train\n    yield check_classifiers_regression_target\n    if (name not in [\"MultinomialNB\", \"ComplementNB\", \"LabelPropagation\",\n                     \"LabelSpreading\"] and\n        # TODO some complication with -1 label\n            name not in [\"DecisionTreeClassifier\", \"ExtraTreeClassifier\"]):\n        # We don't raise a warning in these classifiers, as\n        # the column y interface is used by the forests.\n\n        yield check_supervised_y_2d\n    yield check_supervised_y_no_nan\n    # test if NotFittedError is raised\n    yield check_estimators_unfitted\n    if 'class_weight' in classifier.get_params().keys():\n        yield check_class_weight_classifiers\n\n    yield check_non_transformer_estimators_n_iter\n    # test if predict_proba is a monotonic transformation of decision_function\n    yield check_decision_proba_consistency\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_supervised_y_no_nan(name, estimator_orig):\n    # Checks that the Estimator targets are not NaN.\n    estimator = clone(estimator_orig)\n    rng = np.random.RandomState(888)\n    X = rng.randn(10, 5)\n    y = np.ones(10) * np.inf\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n\n    errmsg = \"Input contains NaN, infinity or a value too large for \" \\\n             \"dtype('float64').\"\n    try:\n        estimator.fit(X, y)\n    except ValueError as e:\n        if str(e) != errmsg:\n            raise ValueError(\"Estimator {0} raised error as expected, but \"\n                             \"does not match expected error message\"\n                             .format(name))\n    else:\n        raise ValueError(\"Estimator {0} should have raised error on fitting \"\n                         \"array y with NaN value.\".format(name))\n\n\ndef _yield_regressor_checks(name, regressor):\n    # TODO: test with intercept\n    # TODO: test with multiple responses\n    # basic testing\n    yield check_regressors_train\n    yield check_regressor_data_not_an_array\n    yield check_estimators_partial_fit_n_features\n    yield check_regressors_no_decision_function\n    yield check_supervised_y_2d\n    yield check_supervised_y_no_nan\n    if name != 'CCA':\n        # check that the regressor handles int input\n        yield check_regressors_int\n    if name != \"GaussianProcessRegressor\":\n        # Test if NotFittedError is raised\n        yield check_estimators_unfitted\n    yield check_non_transformer_estimators_n_iter\n\n\ndef _yield_transformer_checks(name, transformer):\n    # All transformers should either deal with sparse data or raise an\n    # exception with type TypeError and an intelligible error message\n    if name not in ['AdditiveChi2Sampler', 'Binarizer', 'Normalizer',\n                    'PLSCanonical', 'PLSRegression', 'CCA', 'PLSSVD']:\n        yield check_transformer_data_not_an_array\n    # these don't actually fit the data, so don't raise errors\n    if name not in ['AdditiveChi2Sampler', 'Binarizer',\n                    'FunctionTransformer', 'Normalizer']:\n        # basic tests\n        yield check_transformer_general\n        yield check_transformers_unfitted\n    # Dependent on external solvers and hence accessing the iter\n    # param is non-trivial.\n    external_solver = ['Isomap', 'KernelPCA', 'LocallyLinearEmbedding',\n                       'RandomizedLasso', 'LogisticRegressionCV']\n    if name not in external_solver:\n        yield check_transformer_n_iter\n\n\ndef _yield_clustering_checks(name, clusterer):\n    yield check_clusterer_compute_labels_predict\n    if name not in ('WardAgglomeration', \"FeatureAgglomeration\"):\n        # this is clustering on the features\n        # let's not test that here.\n        yield check_clustering\n        yield check_estimators_partial_fit_n_features\n    yield check_non_transformer_estimators_n_iter\n\n\ndef _yield_all_checks(name, estimator):\n    for check in _yield_non_meta_checks(name, estimator):\n        yield check\n    if is_classifier(estimator):\n        for check in _yield_classifier_checks(name, estimator):\n            yield check\n    if is_regressor(estimator):\n        for check in _yield_regressor_checks(name, estimator):\n            yield check\n    if hasattr(estimator, 'transform'):\n        for check in _yield_transformer_checks(name, estimator):\n            yield check\n    if isinstance(estimator, ClusterMixin):\n        for check in _yield_clustering_checks(name, estimator):\n            yield check\n    yield check_fit2d_predict1d\n    yield check_methods_subset_invariance\n    if name != 'GaussianProcess':  # FIXME\n        # XXX GaussianProcess deprecated in 0.20\n        yield check_fit2d_1sample\n    yield check_fit2d_1feature\n    yield check_fit1d\n    yield check_get_params_invariance\n    yield check_dict_unchanged\n    yield check_dont_overwrite_parameters\n\n\ndef check_estimator(Estimator):\n    \"\"\"Check if estimator adheres to scikit-learn conventions.\n\n    This estimator will run an extensive test-suite for input validation,\n    shapes, etc.\n    Additional tests for classifiers, regressors, clustering or transformers\n    will be run if the Estimator class inherits from the corresponding mixin\n    from sklearn.base.\n\n    This test can be applied to classes or instances.\n    Classes currently have some additional tests that related to construction,\n    while passing instances allows the testing of multiple options.\n\n    Parameters\n    ----------\n    estimator : estimator object or class\n        Estimator to check. Estimator is a class object or instance.\n\n    \"\"\"\n    if isinstance(Estimator, type):\n        # got a class\n        name = Estimator.__name__\n        estimator = Estimator()\n        check_parameters_default_constructible(name, Estimator)\n        check_no_attributes_set_in_init(name, estimator)\n    else:\n        # got an instance\n        estimator = Estimator\n        name = type(estimator).__name__\n\n    for check in _yield_all_checks(name, estimator):\n        try:\n            check(name, estimator)\n        except SkipTest as message:\n            # the only SkipTest thrown currently results from not\n            # being able to import pandas.\n            warnings.warn(message, SkipTestWarning)\n\n\ndef _boston_subset(n_samples=200):\n    global BOSTON\n    if BOSTON is None:\n        boston = load_boston()\n        X, y = boston.data, boston.target\n        X, y = shuffle(X, y, random_state=0)\n        X, y = X[:n_samples], y[:n_samples]\n        X = StandardScaler().fit_transform(X)\n        BOSTON = X, y\n    return BOSTON\n\n\ndef set_checking_parameters(estimator):\n    # set parameters to speed up some estimators and\n    # avoid deprecated behaviour\n    params = estimator.get_params()\n    if (\"n_iter\" in params and estimator.__class__.__name__ != \"TSNE\"\n            and not isinstance(estimator, BaseSGD)):\n        estimator.set_params(n_iter=5)\n    if \"max_iter\" in params:\n        warnings.simplefilter(\"ignore\", ConvergenceWarning)\n        if estimator.max_iter is not None:\n            estimator.set_params(max_iter=min(5, estimator.max_iter))\n        # LinearSVR, LinearSVC\n        if estimator.__class__.__name__ in ['LinearSVR', 'LinearSVC']:\n            estimator.set_params(max_iter=20)\n        # NMF\n        if estimator.__class__.__name__ == 'NMF':\n            estimator.set_params(max_iter=100)\n        # MLP\n        if estimator.__class__.__name__ in ['MLPClassifier', 'MLPRegressor']:\n            estimator.set_params(max_iter=100)\n    if \"n_resampling\" in params:\n        # randomized lasso\n        estimator.set_params(n_resampling=5)\n    if \"n_estimators\" in params:\n        # especially gradient boosting with default 100\n        estimator.set_params(n_estimators=min(5, estimator.n_estimators))\n    if \"max_trials\" in params:\n        # RANSAC\n        estimator.set_params(max_trials=10)\n    if \"n_init\" in params:\n        # K-Means\n        estimator.set_params(n_init=2)\n    if \"decision_function_shape\" in params:\n        # SVC\n        estimator.set_params(decision_function_shape='ovo')\n\n    if estimator.__class__.__name__ == \"SelectFdr\":\n        # be tolerant of noisy datasets (not actually speed)\n        estimator.set_params(alpha=.5)\n\n    if estimator.__class__.__name__ == \"TheilSenRegressor\":\n        estimator.max_subpopulation = 100\n\n    if isinstance(estimator, BaseRandomProjection):\n        # Due to the jl lemma and often very few samples, the number\n        # of components of the random matrix projection will be probably\n        # greater than the number of features.\n        # So we impose a smaller number (avoid \"auto\" mode)\n        estimator.set_params(n_components=2)\n\n    if isinstance(estimator, SelectKBest):\n        # SelectKBest has a default of k=10\n        # which is more feature than we have in most case.\n        estimator.set_params(k=1)\n\n\nclass NotAnArray(object):\n    \" An object that is convertable to an array\"\n\n    def __init__(self, data):\n        self.data = data\n\n    def __array__(self, dtype=None):\n        return self.data\n\n\ndef _is_32bit():\n    \"\"\"Detect if process is 32bit Python.\"\"\"\n    return struct.calcsize('P') * 8 == 32\n\n\ndef _is_pairwise(estimator):\n    \"\"\"Returns True if estimator has a _pairwise attribute set to True.\n\n    Parameters\n    ----------\n    estimator : object\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if _pairwise is set to True and False otherwise.\n    \"\"\"\n    return bool(getattr(estimator, \"_pairwise\", False))\n\n\ndef _is_pairwise_metric(estimator):\n    \"\"\"Returns True if estimator accepts pairwise metric.\n\n    Parameters\n    ----------\n    estimator : object\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if _pairwise is set to True and False otherwise.\n    \"\"\"\n    metric = getattr(estimator,  \"metric\", None)\n\n    return bool(metric == 'precomputed')\n\n\ndef pairwise_estimator_convert_X(X, estimator, kernel=linear_kernel):\n\n    if _is_pairwise_metric(estimator):\n        return pairwise_distances(X, metric='euclidean')\n    if _is_pairwise(estimator):\n        return kernel(X, X)\n\n    return X\n\n\ndef check_estimator_sparse_data(name, estimator_orig):\n\n    rng = np.random.RandomState(0)\n    X = rng.rand(40, 10)\n    X[X < .8] = 0\n    X = pairwise_estimator_convert_X(X, estimator_orig)\n    X_csr = sparse.csr_matrix(X)\n    y = (4 * rng.rand(40)).astype(np.int)\n    # catch deprecation warnings\n    with ignore_warnings(category=DeprecationWarning):\n        estimator = clone(estimator_orig)\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n    for sparse_format in ['csr', 'csc', 'dok', 'lil', 'coo', 'dia', 'bsr']:\n        X = X_csr.asformat(sparse_format)\n        # catch deprecation warnings\n        with ignore_warnings(category=(DeprecationWarning, FutureWarning)):\n            if name in ['Scaler', 'StandardScaler']:\n                estimator = clone(estimator).set_params(with_mean=False)\n            else:\n                estimator = clone(estimator)\n        # fit and predict\n        try:\n            with ignore_warnings(category=(DeprecationWarning, FutureWarning)):\n                estimator.fit(X, y)\n            if hasattr(estimator, \"predict\"):\n                pred = estimator.predict(X)\n                assert_equal(pred.shape, (X.shape[0],))\n            if hasattr(estimator, 'predict_proba'):\n                probs = estimator.predict_proba(X)\n                assert_equal(probs.shape, (X.shape[0], 4))\n        except (TypeError, ValueError) as e:\n            if 'sparse' not in repr(e).lower():\n                print(\"Estimator %s doesn't seem to fail gracefully on \"\n                      \"sparse data: error message state explicitly that \"\n                      \"sparse input is not supported if this is not the case.\"\n                      % name)\n                raise\n        except Exception:\n            print(\"Estimator %s doesn't seem to fail gracefully on \"\n                  \"sparse data: it should raise a TypeError if sparse input \"\n                  \"is explicitly not supported.\" % name)\n            raise\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_sample_weights_pandas_series(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type pandas.Series in the 'fit' function.\n    estimator = clone(estimator_orig)\n    if has_fit_parameter(estimator, \"sample_weight\"):\n        try:\n            import pandas as pd\n            X = np.array([[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3]])\n            X = pd.DataFrame(pairwise_estimator_convert_X(X, estimator_orig))\n            y = pd.Series([1, 1, 1, 2, 2, 2])\n            weights = pd.Series([1] * 6)\n            try:\n                estimator.fit(X, y, sample_weight=weights)\n            except ValueError:\n                raise ValueError(\"Estimator {0} raises error if \"\n                                 \"'sample_weight' parameter is of \"\n                                 \"type pandas.Series\".format(name))\n        except ImportError:\n            raise SkipTest(\"pandas is not installed: not testing for \"\n                           \"input of type pandas.Series to class weight.\")\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_sample_weights_list(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type list in the 'fit' function.\n    if has_fit_parameter(estimator_orig, \"sample_weight\"):\n        estimator = clone(estimator_orig)\n        rnd = np.random.RandomState(0)\n        X = pairwise_estimator_convert_X(rnd.uniform(size=(10, 3)),\n                                         estimator_orig)\n        y = np.arange(10) % 3\n        y = multioutput_estimator_convert_y_2d(estimator, y)\n        sample_weight = [3] * 10\n        # Test that estimators don't raise any exception\n        estimator.fit(X, y, sample_weight=sample_weight)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning, UserWarning))\ndef check_dtype_object(name, estimator_orig):\n    # check that estimators treat dtype object as numeric if possible\n    rng = np.random.RandomState(0)\n    X = pairwise_estimator_convert_X(rng.rand(40, 10), estimator_orig)\n    X = X.astype(object)\n    y = (X[:, 0] * 4).astype(np.int)\n    estimator = clone(estimator_orig)\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n\n    estimator.fit(X, y)\n    if hasattr(estimator, \"predict\"):\n        estimator.predict(X)\n\n    if hasattr(estimator, \"transform\"):\n        estimator.transform(X)\n\n    try:\n        estimator.fit(X, y.astype(object))\n    except Exception as e:\n        if \"Unknown label type\" not in str(e):\n            raise\n\n    X[0, 0] = {'foo': 'bar'}\n    msg = \"argument must be a string or a number\"\n    assert_raises_regex(TypeError, msg, estimator.fit, X, y)\n\n\ndef check_complex_data(name, estimator_orig):\n    # check that estimators raise an exception on providing complex data\n    X = np.random.sample(10) + 1j * np.random.sample(10)\n    X = X.reshape(-1, 1)\n    y = np.random.sample(10) + 1j * np.random.sample(10)\n    estimator = clone(estimator_orig)\n    assert_raises_regex(ValueError, \"Complex data not supported\",\n                        estimator.fit, X, y)\n\n\n@ignore_warnings\ndef check_dict_unchanged(name, estimator_orig):\n    # this estimator raises\n    # ValueError: Found array with 0 feature(s) (shape=(23, 0))\n    # while a minimum of 1 is required.\n    # error\n    if name in ['SpectralCoclustering']:\n        return\n    rnd = np.random.RandomState(0)\n    if name in ['RANSACRegressor']:\n        X = 3 * rnd.uniform(size=(20, 3))\n    else:\n        X = 2 * rnd.uniform(size=(20, 3))\n\n    X = pairwise_estimator_convert_X(X, estimator_orig)\n\n    y = X[:, 0].astype(np.int)\n    estimator = clone(estimator_orig)\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    if hasattr(estimator, \"n_best\"):\n        estimator.n_best = 1\n\n    set_random_state(estimator, 1)\n\n    estimator.fit(X, y)\n    for method in [\"predict\", \"transform\", \"decision_function\",\n                   \"predict_proba\"]:\n        if hasattr(estimator, method):\n            dict_before = estimator.__dict__.copy()\n            getattr(estimator, method)(X)\n            assert_dict_equal(estimator.__dict__, dict_before,\n                              'Estimator changes __dict__ during %s' % method)\n\n\ndef is_public_parameter(attr):\n    return not (attr.startswith('_') or attr.endswith('_'))\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_dont_overwrite_parameters(name, estimator_orig):\n    # check that fit method only changes or sets private attributes\n    if hasattr(estimator_orig.__init__, \"deprecated_original\"):\n        # to not check deprecated classes\n        return\n    estimator = clone(estimator_orig)\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20, 3))\n    X = pairwise_estimator_convert_X(X, estimator_orig)\n    y = X[:, 0].astype(np.int)\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    dict_before_fit = estimator.__dict__.copy()\n    estimator.fit(X, y)\n\n    dict_after_fit = estimator.__dict__\n\n    public_keys_after_fit = [key for key in dict_after_fit.keys()\n                             if is_public_parameter(key)]\n\n    attrs_added_by_fit = [key for key in public_keys_after_fit\n                          if key not in dict_before_fit.keys()]\n\n    # check that fit doesn't add any public attribute\n    assert_true(not attrs_added_by_fit,\n                ('Estimator adds public attribute(s) during'\n                 ' the fit method.'\n                 ' Estimators are only allowed to add private attributes'\n                 ' either started with _ or ended'\n                 ' with _ but %s added' % ', '.join(attrs_added_by_fit)))\n\n    # check that fit doesn't change any public attribute\n    attrs_changed_by_fit = [key for key in public_keys_after_fit\n                            if (dict_before_fit[key]\n                                is not dict_after_fit[key])]\n\n    assert_true(not attrs_changed_by_fit,\n                ('Estimator changes public attribute(s) during'\n                 ' the fit method. Estimators are only allowed'\n                 ' to change attributes started'\n                 ' or ended with _, but'\n                 ' %s changed' % ', '.join(attrs_changed_by_fit)))\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_fit2d_predict1d(name, estimator_orig):\n    # check by fitting a 2d array and predicting with a 1d array\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20, 3))\n    X = pairwise_estimator_convert_X(X, estimator_orig)\n    y = X[:, 0].astype(np.int)\n    estimator = clone(estimator_orig)\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    estimator.fit(X, y)\n\n    for method in [\"predict\", \"transform\", \"decision_function\",\n                   \"predict_proba\"]:\n        if hasattr(estimator, method):\n            assert_raise_message(ValueError, \"Reshape your data\",\n                                 getattr(estimator, method), X[0])\n\n\ndef _apply_func(func, X):\n    # apply function on the whole set and on mini batches\n    result_full = func(X)\n    n_features = X.shape[1]\n    result_by_batch = [func(batch.reshape(1, n_features))\n                       for batch in X]\n    # func can output tuple (e.g. score_samples)\n    if type(result_full) == tuple:\n        result_full = result_full[0]\n        result_by_batch = list(map(lambda x: x[0], result_by_batch))\n\n    return np.ravel(result_full), np.ravel(result_by_batch)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_methods_subset_invariance(name, estimator_orig):\n    # check that method gives invariant results if applied\n    # on mini bathes or the whole set\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20, 3))\n    X = pairwise_estimator_convert_X(X, estimator_orig)\n    y = X[:, 0].astype(np.int)\n    estimator = clone(estimator_orig)\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    estimator.fit(X, y)\n\n    for method in [\"predict\", \"transform\", \"decision_function\",\n                   \"score_samples\", \"predict_proba\"]:\n\n        msg = (\"{method} of {name} is not invariant when applied \"\n               \"to a subset.\").format(method=method, name=name)\n        # TODO remove cases when corrected\n        if (name, method) in [('SVC', 'decision_function'),\n                              ('SparsePCA', 'transform'),\n                              ('MiniBatchSparsePCA', 'transform'),\n                              ('BernoulliRBM', 'score_samples')]:\n            raise SkipTest(msg)\n\n        if hasattr(estimator, method):\n            result_full, result_by_batch = _apply_func(\n                getattr(estimator, method), X)\n            assert_allclose(result_full, result_by_batch,\n                            atol=1e-7, err_msg=msg)\n\n\n@ignore_warnings\ndef check_fit2d_1sample(name, estimator_orig):\n    # Check that fitting a 2d array with only one sample either works or\n    # returns an informative message. The error message should either mention\n    # the number of samples or the number of classes.\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(1, 10))\n    y = X[:, 0].astype(np.int)\n    estimator = clone(estimator_orig)\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n\n    msgs = [\"1 sample\", \"n_samples = 1\", \"n_samples=1\", \"one sample\",\n            \"1 class\", \"one class\"]\n\n    try:\n        estimator.fit(X, y)\n    except ValueError as e:\n        if all(msg not in repr(e) for msg in msgs):\n            raise e\n\n\n@ignore_warnings\ndef check_fit2d_1feature(name, estimator_orig):\n    # check fitting a 2d array with only 1 feature either works or returns\n    # informative message\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(10, 1))\n    X = pairwise_estimator_convert_X(X, estimator_orig)\n    y = X[:, 0].astype(np.int)\n    estimator = clone(estimator_orig)\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n    # ensure two labels in subsample for RandomizedLogisticRegression\n    if name == 'RandomizedLogisticRegression':\n        estimator.sample_fraction = 1\n    # ensure non skipped trials for RANSACRegressor\n    if name == 'RANSACRegressor':\n        estimator.residual_threshold = 0.5\n\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n    set_random_state(estimator, 1)\n\n    msgs = [\"1 feature(s)\", \"n_features = 1\", \"n_features=1\"]\n\n    try:\n        estimator.fit(X, y)\n    except ValueError as e:\n        if all(msg not in repr(e) for msg in msgs):\n            raise e\n\n\n@ignore_warnings\ndef check_fit1d(name, estimator_orig):\n    # check fitting 1d X array raises a ValueError\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20))\n    y = X.astype(np.int)\n    estimator = clone(estimator_orig)\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    assert_raises(ValueError, estimator.fit, X, y)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_transformer_general(name, transformer):\n    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],\n                      random_state=0, n_features=2, cluster_std=0.1)\n    X = StandardScaler().fit_transform(X)\n    X -= X.min()\n    if name == 'PowerTransformer':\n        # Box-Cox requires positive, non-zero data\n        X += 1\n    _check_transformer(name, transformer, X, y)\n    _check_transformer(name, transformer, X.tolist(), y.tolist())\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_transformer_data_not_an_array(name, transformer):\n    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],\n                      random_state=0, n_features=2, cluster_std=0.1)\n    X = StandardScaler().fit_transform(X)\n    # We need to make sure that we have non negative data, for things\n    # like NMF\n    X -= X.min() - .1\n    this_X = NotAnArray(X)\n    this_y = NotAnArray(np.asarray(y))\n    _check_transformer(name, transformer, this_X, this_y)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_transformers_unfitted(name, transformer):\n    X, y = _boston_subset()\n\n    transformer = clone(transformer)\n    with assert_raises((AttributeError, ValueError), msg=\"The unfitted \"\n                       \"transformer {} does not raise an error when \"\n                       \"transform is called. Perhaps use \"\n                       \"check_is_fitted in transform.\".format(name)):\n        transformer.transform(X)\n\n\ndef _check_transformer(name, transformer_orig, X, y):\n    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _is_32bit():\n        # Those transformers yield non-deterministic output when executed on\n        # a 32bit Python. The same transformers are stable on 64bit Python.\n        # FIXME: try to isolate a minimalistic reproduction case only depending\n        # on numpy & scipy and/or maybe generate a test dataset that does not\n        # cause such unstable behaviors.\n        msg = name + ' is non deterministic on 32bit Python'\n        raise SkipTest(msg)\n    n_samples, n_features = np.asarray(X).shape\n    transformer = clone(transformer_orig)\n    set_random_state(transformer)\n\n    # fit\n\n    if name in CROSS_DECOMPOSITION:\n        y_ = np.c_[y, y]\n        y_[::2, 1] *= 2\n    else:\n        y_ = y\n\n    transformer.fit(X, y_)\n    # fit_transform method should work on non fitted estimator\n    transformer_clone = clone(transformer)\n    X_pred = transformer_clone.fit_transform(X, y=y_)\n\n    if isinstance(X_pred, tuple):\n        for x_pred in X_pred:\n            assert_equal(x_pred.shape[0], n_samples)\n    else:\n        # check for consistent n_samples\n        assert_equal(X_pred.shape[0], n_samples)\n\n    if hasattr(transformer, 'transform'):\n        if name in CROSS_DECOMPOSITION:\n            X_pred2 = transformer.transform(X, y_)\n            X_pred3 = transformer.fit_transform(X, y=y_)\n        else:\n            X_pred2 = transformer.transform(X)\n            X_pred3 = transformer.fit_transform(X, y=y_)\n        if isinstance(X_pred, tuple) and isinstance(X_pred2, tuple):\n            for x_pred, x_pred2, x_pred3 in zip(X_pred, X_pred2, X_pred3):\n                assert_allclose_dense_sparse(\n                    x_pred, x_pred2, atol=1e-2,\n                    err_msg=\"fit_transform and transform outcomes \"\n                            \"not consistent in %s\"\n                    % transformer)\n                assert_allclose_dense_sparse(\n                    x_pred, x_pred3, atol=1e-2,\n                    err_msg=\"consecutive fit_transform outcomes \"\n                            \"not consistent in %s\"\n                    % transformer)\n        else:\n            assert_allclose_dense_sparse(\n                X_pred, X_pred2,\n                err_msg=\"fit_transform and transform outcomes \"\n                        \"not consistent in %s\"\n                % transformer, atol=1e-2)\n            assert_allclose_dense_sparse(\n                X_pred, X_pred3, atol=1e-2,\n                err_msg=\"consecutive fit_transform outcomes \"\n                        \"not consistent in %s\"\n                % transformer)\n            assert_equal(_num_samples(X_pred2), n_samples)\n            assert_equal(_num_samples(X_pred3), n_samples)\n\n        # raises error on malformed input for transform\n        if hasattr(X, 'T'):\n            # If it's not an array, it does not have a 'T' property\n            with assert_raises(ValueError, msg=\"The transformer {} does \"\n                               \"not raise an error when the number of \"\n                               \"features in transform is different from\"\n                               \" the number of features in \"\n                               \"fit.\".format(name)):\n                transformer.transform(X.T)\n\n\n@ignore_warnings\ndef check_pipeline_consistency(name, estimator_orig):\n    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _is_32bit():\n        # Those transformers yield non-deterministic output when executed on\n        # a 32bit Python. The same transformers are stable on 64bit Python.\n        # FIXME: try to isolate a minimalistic reproduction case only depending\n        # scipy and/or maybe generate a test dataset that does not\n        # cause such unstable behaviors.\n        msg = name + ' is non deterministic on 32bit Python'\n        raise SkipTest(msg)\n\n    # check that make_pipeline(est) gives same score as est\n    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],\n                      random_state=0, n_features=2, cluster_std=0.1)\n    X -= X.min()\n    if name == 'PowerTransformer':\n        # Box-Cox requires positive, non-zero data\n        X += 1\n    X = pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)\n    estimator = clone(estimator_orig)\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n    set_random_state(estimator)\n    pipeline = make_pipeline(estimator)\n    estimator.fit(X, y)\n    pipeline.fit(X, y)\n\n    funcs = [\"score\", \"fit_transform\"]\n\n    for func_name in funcs:\n        func = getattr(estimator, func_name, None)\n        if func is not None:\n            func_pipeline = getattr(pipeline, func_name)\n            result = func(X, y)\n            result_pipe = func_pipeline(X, y)\n            assert_allclose_dense_sparse(result, result_pipe)\n\n\n@ignore_warnings\ndef check_fit_score_takes_y(name, estimator_orig):\n    # check that all estimators accept an optional y\n    # in fit and score so they can be used in pipelines\n    rnd = np.random.RandomState(0)\n    X = rnd.uniform(size=(10, 3))\n    X = pairwise_estimator_convert_X(X, estimator_orig)\n    y = np.arange(10) % 3\n    estimator = clone(estimator_orig)\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n    set_random_state(estimator)\n\n    funcs = [\"fit\", \"score\", \"partial_fit\", \"fit_predict\", \"fit_transform\"]\n    for func_name in funcs:\n        func = getattr(estimator, func_name, None)\n        if func is not None:\n            func(X, y)\n            args = [p.name for p in signature(func).parameters.values()]\n            if args[0] == \"self\":\n                # if_delegate_has_method makes methods into functions\n                # with an explicit \"self\", so need to shift arguments\n                args = args[1:]\n            assert_true(args[1] in [\"y\", \"Y\"],\n                        \"Expected y or Y as second argument for method \"\n                        \"%s of %s. Got arguments: %r.\"\n                        % (func_name, type(estimator).__name__, args))\n\n\n@ignore_warnings\ndef check_estimators_dtypes(name, estimator_orig):\n    rnd = np.random.RandomState(0)\n    X_train_32 = 3 * rnd.uniform(size=(20, 5)).astype(np.float32)\n    X_train_32 = pairwise_estimator_convert_X(X_train_32, estimator_orig)\n    X_train_64 = X_train_32.astype(np.float64)\n    X_train_int_64 = X_train_32.astype(np.int64)\n    X_train_int_32 = X_train_32.astype(np.int32)\n    y = X_train_int_64[:, 0]\n    y = multioutput_estimator_convert_y_2d(estimator_orig, y)\n\n    methods = [\"predict\", \"transform\", \"decision_function\", \"predict_proba\"]\n\n    for X_train in [X_train_32, X_train_64, X_train_int_64, X_train_int_32]:\n        if name == 'PowerTransformer':\n            # Box-Cox requires positive, non-zero data\n            X_train = np.abs(X_train) + 1\n        estimator = clone(estimator_orig)\n        set_random_state(estimator, 1)\n        estimator.fit(X_train, y)\n\n        for method in methods:\n            if hasattr(estimator, method):\n                getattr(estimator, method)(X_train)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_estimators_empty_data_messages(name, estimator_orig):\n    e = clone(estimator_orig)\n    set_random_state(e, 1)\n\n    X_zero_samples = np.empty(0).reshape(0, 3)\n    # The precise message can change depending on whether X or y is\n    # validated first. Let us test the type of exception only:\n    with assert_raises(ValueError, msg=\"The estimator {} does not\"\n                       \" raise an error when an empty data is used \"\n                       \"to train. Perhaps use \"\n                       \"check_array in train.\".format(name)):\n        e.fit(X_zero_samples, [])\n\n    X_zero_features = np.empty(0).reshape(3, 0)\n    # the following y should be accepted by both classifiers and regressors\n    # and ignored by unsupervised models\n    y = multioutput_estimator_convert_y_2d(e, np.array([1, 0, 1]))\n    msg = (r\"0 feature\\(s\\) \\(shape=\\(3, 0\\)\\) while a minimum of \\d* \"\n           \"is required.\")\n    assert_raises_regex(ValueError, msg, e.fit, X_zero_features, y)\n\n\n@ignore_warnings(category=DeprecationWarning)\ndef check_estimators_nan_inf(name, estimator_orig):\n    # Checks that Estimator X's do not contain NaN or inf.\n    rnd = np.random.RandomState(0)\n    X_train_finite = pairwise_estimator_convert_X(rnd.uniform(size=(10, 3)),\n                                                  estimator_orig)\n    X_train_nan = rnd.uniform(size=(10, 3))\n    X_train_nan[0, 0] = np.nan\n    X_train_inf = rnd.uniform(size=(10, 3))\n    X_train_inf[0, 0] = np.inf\n    y = np.ones(10)\n    y[:5] = 0\n    y = multioutput_estimator_convert_y_2d(estimator_orig, y)\n    error_string_fit = \"Estimator doesn't check for NaN and inf in fit.\"\n    error_string_predict = (\"Estimator doesn't check for NaN and inf in\"\n                            \" predict.\")\n    error_string_transform = (\"Estimator doesn't check for NaN and inf in\"\n                              \" transform.\")\n    for X_train in [X_train_nan, X_train_inf]:\n        # catch deprecation warnings\n        with ignore_warnings(category=(DeprecationWarning, FutureWarning)):\n            estimator = clone(estimator_orig)\n            set_random_state(estimator, 1)\n            # try to fit\n            try:\n                estimator.fit(X_train, y)\n            except ValueError as e:\n                if 'inf' not in repr(e) and 'NaN' not in repr(e):\n                    print(error_string_fit, estimator, e)\n                    traceback.print_exc(file=sys.stdout)\n                    raise e\n            except Exception as exc:\n                print(error_string_fit, estimator, exc)\n                traceback.print_exc(file=sys.stdout)\n                raise exc\n            else:\n                raise AssertionError(error_string_fit, estimator)\n            # actually fit\n            estimator.fit(X_train_finite, y)\n\n            # predict\n            if hasattr(estimator, \"predict\"):\n                try:\n                    estimator.predict(X_train)\n                except ValueError as e:\n                    if 'inf' not in repr(e) and 'NaN' not in repr(e):\n                        print(error_string_predict, estimator, e)\n                        traceback.print_exc(file=sys.stdout)\n                        raise e\n                except Exception as exc:\n                    print(error_string_predict, estimator, exc)\n                    traceback.print_exc(file=sys.stdout)\n                else:\n                    raise AssertionError(error_string_predict, estimator)\n\n            # transform\n            if hasattr(estimator, \"transform\"):\n                try:\n                    estimator.transform(X_train)\n                except ValueError as e:\n                    if 'inf' not in repr(e) and 'NaN' not in repr(e):\n                        print(error_string_transform, estimator, e)\n                        traceback.print_exc(file=sys.stdout)\n                        raise e\n                except Exception as exc:\n                    print(error_string_transform, estimator, exc)\n                    traceback.print_exc(file=sys.stdout)\n                else:\n                    raise AssertionError(error_string_transform, estimator)\n\n\n@ignore_warnings\ndef check_estimators_pickle(name, estimator_orig):\n    \"\"\"Test that we can pickle all estimators\"\"\"\n    check_methods = [\"predict\", \"transform\", \"decision_function\",\n                     \"predict_proba\"]\n\n    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],\n                      random_state=0, n_features=2, cluster_std=0.1)\n\n    # some estimators can't do features less than 0\n    X -= X.min()\n    if name == 'PowerTransformer':\n        # Box-Cox requires positive, non-zero data\n        X += 1\n    X = pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)\n\n    estimator = clone(estimator_orig)\n\n    # some estimators only take multioutputs\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n\n    set_random_state(estimator)\n    estimator.fit(X, y)\n\n    result = dict()\n    for method in check_methods:\n        if hasattr(estimator, method):\n            result[method] = getattr(estimator, method)(X)\n\n    # pickle and unpickle!\n    pickled_estimator = pickle.dumps(estimator)\n    if estimator.__module__.startswith('sklearn.'):\n        assert_true(b\"version\" in pickled_estimator)\n    unpickled_estimator = pickle.loads(pickled_estimator)\n\n    for method in result:\n        unpickled_result = getattr(unpickled_estimator, method)(X)\n        assert_allclose_dense_sparse(result[method], unpickled_result)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_estimators_partial_fit_n_features(name, estimator_orig):\n    # check if number of features changes between calls to partial_fit.\n    if not hasattr(estimator_orig, 'partial_fit'):\n        return\n    estimator = clone(estimator_orig)\n    X, y = make_blobs(n_samples=50, random_state=1)\n    X -= X.min()\n\n    try:\n        if is_classifier(estimator):\n            classes = np.unique(y)\n            estimator.partial_fit(X, y, classes=classes)\n        else:\n            estimator.partial_fit(X, y)\n    except NotImplementedError:\n        return\n\n    with assert_raises(ValueError,\n                       msg=\"The estimator {} does not raise an\"\n                           \" error when the number of features\"\n                           \" changes between calls to \"\n                           \"partial_fit.\".format(name)):\n        estimator.partial_fit(X[:, :-1], y)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_clustering(name, clusterer_orig):\n    clusterer = clone(clusterer_orig)\n    X, y = make_blobs(n_samples=50, random_state=1)\n    X, y = shuffle(X, y, random_state=7)\n    X = StandardScaler().fit_transform(X)\n    n_samples, n_features = X.shape\n    # catch deprecation and neighbors warnings\n    if hasattr(clusterer, \"n_clusters\"):\n        clusterer.set_params(n_clusters=3)\n    set_random_state(clusterer)\n    if name == 'AffinityPropagation':\n        clusterer.set_params(preference=-100)\n        clusterer.set_params(max_iter=100)\n\n    # fit\n    clusterer.fit(X)\n    # with lists\n    clusterer.fit(X.tolist())\n\n    pred = clusterer.labels_\n    assert_equal(pred.shape, (n_samples,))\n    assert_greater(adjusted_rand_score(pred, y), 0.4)\n    # fit another time with ``fit_predict`` and compare results\n    if name == 'SpectralClustering':\n        # there is no way to make Spectral clustering deterministic :(\n        return\n    set_random_state(clusterer)\n    with warnings.catch_warnings(record=True):\n        pred2 = clusterer.fit_predict(X)\n    assert_array_equal(pred, pred2)\n\n    # fit_predict(X) and labels_ should be of type int\n    assert_in(pred.dtype, [np.dtype('int32'), np.dtype('int64')])\n    assert_in(pred2.dtype, [np.dtype('int32'), np.dtype('int64')])\n\n    # Add noise to X to test the possible values of the labels\n    rng = np.random.RandomState(7)\n    X_noise = np.concatenate([X, rng.uniform(low=-3, high=3, size=(5, 2))])\n    labels = clusterer.fit_predict(X_noise)\n\n    # There should be at least one sample in every cluster. Equivalently\n    # labels_ should contain all the consecutive values between its\n    # min and its max.\n    labels_sorted = np.unique(labels)\n    assert_array_equal(labels_sorted, np.arange(labels_sorted[0],\n                                                labels_sorted[-1] + 1))\n\n    # Labels are expected to start at 0 (no noise) or -1 (if noise)\n    assert_true(labels_sorted[0] in [0, -1])\n    # Labels should be less than n_clusters - 1\n    if hasattr(clusterer, 'n_clusters'):\n        n_clusters = getattr(clusterer, 'n_clusters')\n        assert_greater_equal(n_clusters - 1, labels_sorted[-1])\n    # else labels should be less than max(labels_) which is necessarily true\n\n\n@ignore_warnings(category=DeprecationWarning)\ndef check_clusterer_compute_labels_predict(name, clusterer_orig):\n    \"\"\"Check that predict is invariant of compute_labels\"\"\"\n    X, y = make_blobs(n_samples=20, random_state=0)\n    clusterer = clone(clusterer_orig)\n\n    if hasattr(clusterer, \"compute_labels\"):\n        # MiniBatchKMeans\n        if hasattr(clusterer, \"random_state\"):\n            clusterer.set_params(random_state=0)\n\n        X_pred1 = clusterer.fit(X).predict(X)\n        clusterer.set_params(compute_labels=False)\n        X_pred2 = clusterer.fit(X).predict(X)\n        assert_array_equal(X_pred1, X_pred2)\n\n\n@ignore_warnings(category=DeprecationWarning)\ndef check_classifiers_one_label(name, classifier_orig):\n    error_string_fit = \"Classifier can't train when only one class is present.\"\n    error_string_predict = (\"Classifier can't predict when only one class is \"\n                            \"present.\")\n    rnd = np.random.RandomState(0)\n    X_train = rnd.uniform(size=(10, 3))\n    X_test = rnd.uniform(size=(10, 3))\n    y = np.ones(10)\n    # catch deprecation warnings\n    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):\n        classifier = clone(classifier_orig)\n        # try to fit\n        try:\n            classifier.fit(X_train, y)\n        except ValueError as e:\n            if 'class' not in repr(e):\n                print(error_string_fit, classifier, e)\n                traceback.print_exc(file=sys.stdout)\n                raise e\n            else:\n                return\n        except Exception as exc:\n            print(error_string_fit, classifier, exc)\n            traceback.print_exc(file=sys.stdout)\n            raise exc\n        # predict\n        try:\n            assert_array_equal(classifier.predict(X_test), y)\n        except Exception as exc:\n            print(error_string_predict, classifier, exc)\n            raise exc\n\n\n@ignore_warnings  # Warnings are raised by decision function\ndef check_classifiers_train(name, classifier_orig):\n    X_m, y_m = make_blobs(n_samples=300, random_state=0)\n    X_m, y_m = shuffle(X_m, y_m, random_state=7)\n    X_m = StandardScaler().fit_transform(X_m)\n    # generate binary problem from multi-class one\n    y_b = y_m[y_m != 2]\n    X_b = X_m[y_m != 2]\n    for (X, y) in [(X_m, y_m), (X_b, y_b)]:\n        classes = np.unique(y)\n        n_classes = len(classes)\n        n_samples, n_features = X.shape\n        classifier = clone(classifier_orig)\n        if name in ['BernoulliNB', 'MultinomialNB', 'ComplementNB']:\n            X -= X.min()\n        X = pairwise_estimator_convert_X(X, classifier_orig)\n        set_random_state(classifier)\n        # raises error on malformed input for fit\n        with assert_raises(ValueError, msg=\"The classifer {} does not\"\n                           \" raise an error when incorrect/malformed input \"\n                           \"data for fit is passed. The number of training \"\n                           \"examples is not the same as the number of labels.\"\n                           \" Perhaps use check_X_y in fit.\".format(name)):\n            classifier.fit(X, y[:-1])\n\n        # fit\n        classifier.fit(X, y)\n        # with lists\n        classifier.fit(X.tolist(), y.tolist())\n        assert_true(hasattr(classifier, \"classes_\"))\n        y_pred = classifier.predict(X)\n        assert_equal(y_pred.shape, (n_samples,))\n        # training set performance\n        if name not in ['BernoulliNB', 'MultinomialNB', 'ComplementNB']:\n            assert_greater(accuracy_score(y, y_pred), 0.83)\n\n        # raises error on malformed input for predict\n        if _is_pairwise(classifier):\n            with assert_raises(ValueError, msg=\"The classifier {} does not\"\n                               \" raise an error when shape of X\"\n                               \"in predict is not equal to (n_test_samples,\"\n                               \"n_training_samples)\".format(name)):\n                classifier.predict(X.reshape(-1, 1))\n        else:\n            with assert_raises(ValueError, msg=\"The classifier {} does not\"\n                               \" raise an error when the number of features \"\n                               \"in predict is different from the number of\"\n                               \" features in fit.\".format(name)):\n                classifier.predict(X.T)\n        if hasattr(classifier, \"decision_function\"):\n            try:\n                # decision_function agrees with predict\n                decision = classifier.decision_function(X)\n                if n_classes == 2:\n                    assert_equal(decision.shape, (n_samples,))\n                    dec_pred = (decision.ravel() > 0).astype(np.int)\n                    assert_array_equal(dec_pred, y_pred)\n                if (n_classes == 3 and\n                        # 1on1 of LibSVM works differently\n                        not isinstance(classifier, BaseLibSVM)):\n                    assert_equal(decision.shape, (n_samples, n_classes))\n                    assert_array_equal(np.argmax(decision, axis=1), y_pred)\n\n                # raises error on malformed input for decision_function\n                if _is_pairwise(classifier):\n                    with assert_raises(ValueError, msg=\"The classifier {} does\"\n                                       \" not raise an error when the  \"\n                                       \"shape of X in decision_function is \"\n                                       \"not equal to (n_test_samples, \"\n                                       \"n_training_samples) in fit.\"\n                                       .format(name)):\n                        classifier.decision_function(X.reshape(-1, 1))\n                else:\n                    with assert_raises(ValueError, msg=\"The classifier {} does\"\n                                       \" not raise an error when the number \"\n                                       \"of features in decision_function is \"\n                                       \"different from the number of features\"\n                                       \" in fit.\".format(name)):\n                        classifier.decision_function(X.T)\n            except NotImplementedError:\n                pass\n        if hasattr(classifier, \"predict_proba\"):\n            # predict_proba agrees with predict\n            y_prob = classifier.predict_proba(X)\n            assert_equal(y_prob.shape, (n_samples, n_classes))\n            assert_array_equal(np.argmax(y_prob, axis=1), y_pred)\n            # check that probas for all classes sum to one\n            assert_allclose(np.sum(y_prob, axis=1), np.ones(n_samples))\n            # raises error on malformed input for predict_proba\n            if _is_pairwise(classifier_orig):\n                with assert_raises(ValueError, msg=\"The classifier {} does not\"\n                                   \" raise an error when the shape of X\"\n                                   \"in predict_proba is not equal to \"\n                                   \"(n_test_samples, n_training_samples).\"\n                                   .format(name)):\n                    classifier.predict_proba(X.reshape(-1, 1))\n            else:\n                with assert_raises(ValueError, msg=\"The classifier {} does not\"\n                                   \" raise an error when the number of \"\n                                   \"features in predict_proba is different \"\n                                   \"from the number of features in fit.\"\n                                   .format(name)):\n                    classifier.predict_proba(X.T)\n            if hasattr(classifier, \"predict_log_proba\"):\n                # predict_log_proba is a transformation of predict_proba\n                y_log_prob = classifier.predict_log_proba(X)\n                assert_allclose(y_log_prob, np.log(y_prob), 8, atol=1e-9)\n                assert_array_equal(np.argsort(y_log_prob), np.argsort(y_prob))\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_estimators_fit_returns_self(name, estimator_orig):\n    \"\"\"Check if self is returned when calling fit\"\"\"\n    X, y = make_blobs(random_state=0, n_samples=9, n_features=4)\n    # some want non-negative input\n    X -= X.min()\n    if name == 'PowerTransformer':\n        # Box-Cox requires positive, non-zero data\n        X += 1\n    X = pairwise_estimator_convert_X(X, estimator_orig)\n\n    estimator = clone(estimator_orig)\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n\n    set_random_state(estimator)\n\n    assert_true(estimator.fit(X, y) is estimator)\n\n\n@ignore_warnings\ndef check_estimators_unfitted(name, estimator_orig):\n    \"\"\"Check that predict raises an exception in an unfitted estimator.\n\n    Unfitted estimators should raise either AttributeError or ValueError.\n    The specific exception type NotFittedError inherits from both and can\n    therefore be adequately raised for that purpose.\n    \"\"\"\n\n    # Common test for Regressors as well as Classifiers\n    X, y = _boston_subset()\n\n    est = clone(estimator_orig)\n\n    msg = \"fit\"\n    if hasattr(est, 'predict'):\n        assert_raise_message((AttributeError, ValueError), msg,\n                             est.predict, X)\n\n    if hasattr(est, 'decision_function'):\n        assert_raise_message((AttributeError, ValueError), msg,\n                             est.decision_function, X)\n\n    if hasattr(est, 'predict_proba'):\n        assert_raise_message((AttributeError, ValueError), msg,\n                             est.predict_proba, X)\n\n    if hasattr(est, 'predict_log_proba'):\n        assert_raise_message((AttributeError, ValueError), msg,\n                             est.predict_log_proba, X)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_supervised_y_2d(name, estimator_orig):\n    if \"MultiTask\" in name:\n        # These only work on 2d, so this test makes no sense\n        return\n    rnd = np.random.RandomState(0)\n    X = pairwise_estimator_convert_X(rnd.uniform(size=(10, 3)), estimator_orig)\n    y = np.arange(10) % 3\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    # fit\n    estimator.fit(X, y)\n    y_pred = estimator.predict(X)\n\n    set_random_state(estimator)\n    # Check that when a 2D y is given, a DataConversionWarning is\n    # raised\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\", DataConversionWarning)\n        warnings.simplefilter(\"ignore\", RuntimeWarning)\n        estimator.fit(X, y[:, np.newaxis])\n    y_pred_2d = estimator.predict(X)\n    msg = \"expected 1 DataConversionWarning, got: %s\" % (\n        \", \".join([str(w_x) for w_x in w]))\n    if name not in MULTI_OUTPUT:\n        # check that we warned if we don't support multi-output\n        assert_greater(len(w), 0, msg)\n        assert_true(\"DataConversionWarning('A column-vector y\"\n                    \" was passed when a 1d array was expected\" in msg)\n    assert_allclose(y_pred.ravel(), y_pred_2d.ravel())\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_classifiers_classes(name, classifier_orig):\n    X, y = make_blobs(n_samples=30, random_state=0, cluster_std=0.1)\n    X, y = shuffle(X, y, random_state=7)\n    X = StandardScaler().fit_transform(X)\n    # We need to make sure that we have non negative data, for things\n    # like NMF\n    X -= X.min() - .1\n    X = pairwise_estimator_convert_X(X, classifier_orig)\n    y_names = np.array([\"one\", \"two\", \"three\"])[y]\n\n    for y_names in [y_names, y_names.astype('O')]:\n        if name in [\"LabelPropagation\", \"LabelSpreading\"]:\n            # TODO some complication with -1 label\n            y_ = y\n        else:\n            y_ = y_names\n\n        classes = np.unique(y_)\n        classifier = clone(classifier_orig)\n        if name == 'BernoulliNB':\n            X = X > X.mean()\n        set_random_state(classifier)\n        # fit\n        classifier.fit(X, y_)\n\n        y_pred = classifier.predict(X)\n        # training set performance\n        if name != \"ComplementNB\":\n            # This is a pathological data set for ComplementNB.\n            assert_array_equal(np.unique(y_), np.unique(y_pred))\n        if np.any(classifier.classes_ != classes):\n            print(\"Unexpected classes_ attribute for %r: \"\n                  \"expected %s, got %s\" %\n                  (classifier, classes, classifier.classes_))\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_regressors_int(name, regressor_orig):\n    X, _ = _boston_subset()\n    X = pairwise_estimator_convert_X(X[:50], regressor_orig)\n    rnd = np.random.RandomState(0)\n    y = rnd.randint(3, size=X.shape[0])\n    y = multioutput_estimator_convert_y_2d(regressor_orig, y)\n    rnd = np.random.RandomState(0)\n    # separate estimators to control random seeds\n    regressor_1 = clone(regressor_orig)\n    regressor_2 = clone(regressor_orig)\n    set_random_state(regressor_1)\n    set_random_state(regressor_2)\n\n    if name in CROSS_DECOMPOSITION:\n        y_ = np.vstack([y, 2 * y + rnd.randint(2, size=len(y))])\n        y_ = y_.T\n    else:\n        y_ = y\n\n    # fit\n    regressor_1.fit(X, y_)\n    pred1 = regressor_1.predict(X)\n    regressor_2.fit(X, y_.astype(np.float))\n    pred2 = regressor_2.predict(X)\n    assert_allclose(pred1, pred2, atol=1e-2, err_msg=name)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_regressors_train(name, regressor_orig):\n    X, y = _boston_subset()\n    X = pairwise_estimator_convert_X(X, regressor_orig)\n    y = StandardScaler().fit_transform(y.reshape(-1, 1))  # X is already scaled\n    y = y.ravel()\n    regressor = clone(regressor_orig)\n    y = multioutput_estimator_convert_y_2d(regressor, y)\n    rnd = np.random.RandomState(0)\n    if not hasattr(regressor, 'alphas') and hasattr(regressor, 'alpha'):\n        # linear regressors need to set alpha, but not generalized CV ones\n        regressor.alpha = 0.01\n    if name == 'PassiveAggressiveRegressor':\n        regressor.C = 0.01\n\n    # raises error on malformed input for fit\n    with assert_raises(ValueError, msg=\"The classifer {} does not\"\n                       \" raise an error when incorrect/malformed input \"\n                       \"data for fit is passed. The number of training \"\n                       \"examples is not the same as the number of \"\n                       \"labels. Perhaps use check_X_y in fit.\".format(name)):\n        regressor.fit(X, y[:-1])\n    # fit\n    if name in CROSS_DECOMPOSITION:\n        y_ = np.vstack([y, 2 * y + rnd.randint(2, size=len(y))])\n        y_ = y_.T\n    else:\n        y_ = y\n    set_random_state(regressor)\n    regressor.fit(X, y_)\n    regressor.fit(X.tolist(), y_.tolist())\n    y_pred = regressor.predict(X)\n    assert_equal(y_pred.shape, y_.shape)\n\n    # TODO: find out why PLS and CCA fail. RANSAC is random\n    # and furthermore assumes the presence of outliers, hence\n    # skipped\n    if name not in ('PLSCanonical', 'CCA', 'RANSACRegressor'):\n        assert_greater(regressor.score(X, y_), 0.5)\n\n\n@ignore_warnings\ndef check_regressors_no_decision_function(name, regressor_orig):\n    # checks whether regressors have decision_function or predict_proba\n    rng = np.random.RandomState(0)\n    X = rng.normal(size=(10, 4))\n    regressor = clone(regressor_orig)\n    y = multioutput_estimator_convert_y_2d(regressor, X[:, 0])\n\n    if hasattr(regressor, \"n_components\"):\n        # FIXME CCA, PLS is not robust to rank 1 effects\n        regressor.n_components = 1\n\n    regressor.fit(X, y)\n    funcs = [\"decision_function\", \"predict_proba\", \"predict_log_proba\"]\n    for func_name in funcs:\n        func = getattr(regressor, func_name, None)\n        if func is None:\n            # doesn't have function\n            continue\n        # has function. Should raise deprecation warning\n        msg = func_name\n        assert_warns_message(DeprecationWarning, msg, func, X)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_class_weight_classifiers(name, classifier_orig):\n    if name == \"NuSVC\":\n        # the sparse version has a parameter that doesn't do anything\n        raise SkipTest(\"Not testing NuSVC class weight as it is ignored.\")\n    if name.endswith(\"NB\"):\n        # NaiveBayes classifiers have a somewhat different interface.\n        # FIXME SOON!\n        raise SkipTest\n\n    for n_centers in [2, 3]:\n        # create a very noisy dataset\n        X, y = make_blobs(centers=n_centers, random_state=0, cluster_std=20)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n                                                            random_state=0)\n\n        # can't use gram_if_pairwise() here, setting up gram matrix manually\n        if _is_pairwise(classifier_orig):\n            X_test = rbf_kernel(X_test, X_train)\n            X_train = rbf_kernel(X_train, X_train)\n\n        n_centers = len(np.unique(y_train))\n\n        if n_centers == 2:\n            class_weight = {0: 1000, 1: 0.0001}\n        else:\n            class_weight = {0: 1000, 1: 0.0001, 2: 0.0001}\n\n        classifier = clone(classifier_orig).set_params(\n            class_weight=class_weight)\n        if hasattr(classifier, \"n_iter\"):\n            classifier.set_params(n_iter=100)\n        if hasattr(classifier, \"max_iter\"):\n            classifier.set_params(max_iter=1000)\n        if hasattr(classifier, \"min_weight_fraction_leaf\"):\n            classifier.set_params(min_weight_fraction_leaf=0.01)\n\n        set_random_state(classifier)\n        classifier.fit(X_train, y_train)\n        y_pred = classifier.predict(X_test)\n        # XXX: Generally can use 0.89 here. On Windows, LinearSVC gets\n        #      0.88 (Issue #9111)\n        assert_greater(np.mean(y_pred == 0), 0.87)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_class_weight_balanced_classifiers(name, classifier_orig, X_train,\n                                            y_train, X_test, y_test, weights):\n    classifier = clone(classifier_orig)\n    if hasattr(classifier, \"n_iter\"):\n        classifier.set_params(n_iter=100)\n    if hasattr(classifier, \"max_iter\"):\n        classifier.set_params(max_iter=1000)\n\n    set_random_state(classifier)\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n\n    classifier.set_params(class_weight='balanced')\n    classifier.fit(X_train, y_train)\n    y_pred_balanced = classifier.predict(X_test)\n    assert_greater(f1_score(y_test, y_pred_balanced, average='weighted'),\n                   f1_score(y_test, y_pred, average='weighted'))\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_class_weight_balanced_linear_classifier(name, Classifier):\n    \"\"\"Test class weights with non-contiguous class labels.\"\"\"\n    # this is run on classes, not instances, though this should be changed\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                  [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([1, 1, 1, -1, -1])\n\n    classifier = Classifier()\n\n    if hasattr(classifier, \"n_iter\"):\n        # This is a very small dataset, default n_iter are likely to prevent\n        # convergence\n        classifier.set_params(n_iter=1000)\n    if hasattr(classifier, \"max_iter\"):\n        classifier.set_params(max_iter=1000)\n    set_random_state(classifier)\n\n    # Let the model compute the class frequencies\n    classifier.set_params(class_weight='balanced')\n    coef_balanced = classifier.fit(X, y).coef_.copy()\n\n    # Count each label occurrence to reweight manually\n    n_samples = len(y)\n    n_classes = float(len(np.unique(y)))\n\n    class_weight = {1: n_samples / (np.sum(y == 1) * n_classes),\n                    -1: n_samples / (np.sum(y == -1) * n_classes)}\n    classifier.set_params(class_weight=class_weight)\n    coef_manual = classifier.fit(X, y).coef_.copy()\n\n    assert_allclose(coef_balanced, coef_manual)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_estimators_overwrite_params(name, estimator_orig):\n    X, y = make_blobs(random_state=0, n_samples=9)\n    # some want non-negative input\n    X -= X.min()\n    if name == 'PowerTransformer':\n        # Box-Cox requires positive, non-zero data\n        X += 1\n    X = pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)\n    estimator = clone(estimator_orig)\n    y = multioutput_estimator_convert_y_2d(estimator, y)\n\n    set_random_state(estimator)\n\n    # Make a physical copy of the original estimator parameters before fitting.\n    params = estimator.get_params()\n    original_params = deepcopy(params)\n\n    # Fit the model\n    estimator.fit(X, y)\n\n    # Compare the state of the model parameters with the original parameters\n    new_params = estimator.get_params()\n    for param_name, original_value in original_params.items():\n        new_value = new_params[param_name]\n\n        # We should never change or mutate the internal state of input\n        # parameters by default. To check this we use the joblib.hash function\n        # that introspects recursively any subobjects to compute a checksum.\n        # The only exception to this rule of immutable constructor parameters\n        # is possible RandomState instance but in this check we explicitly\n        # fixed the random_state params recursively to be integer seeds.\n        assert_equal(hash(new_value), hash(original_value),\n                     \"Estimator %s should not change or mutate \"\n                     \" the parameter %s from %s to %s during fit.\"\n                     % (name, param_name, original_value, new_value))\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_no_attributes_set_in_init(name, estimator):\n    \"\"\"Check setting during init. \"\"\"\n\n    if hasattr(type(estimator).__init__, \"deprecated_original\"):\n        return\n\n    init_params = _get_args(type(estimator).__init__)\n    parents_init_params = [param for params_parent in\n                           (_get_args(parent) for parent in\n                            type(estimator).__mro__)\n                           for param in params_parent]\n\n    # Test for no setting apart from parameters during init\n    invalid_attr = (set(vars(estimator)) - set(init_params)\n                    - set(parents_init_params))\n    assert_false(invalid_attr,\n                 \"Estimator %s should not set any attribute apart\"\n                 \" from parameters during init. Found attributes %s.\"\n                 % (name, sorted(invalid_attr)))\n    # Ensure that each parameter is set in init\n    invalid_attr = (set(init_params) - set(vars(estimator))\n                    - set([\"self\"]))\n    assert_false(invalid_attr,\n                 \"Estimator %s should store all parameters\"\n                 \" as an attribute during init. Did not find \"\n                 \"attributes %s.\" % (name, sorted(invalid_attr)))\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_sparsify_coefficients(name, estimator_orig):\n    X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1],\n                  [-1, -2], [2, 2], [-2, -2]])\n    y = [1, 1, 1, 2, 2, 2, 3, 3, 3]\n    est = clone(estimator_orig)\n\n    est.fit(X, y)\n    pred_orig = est.predict(X)\n\n    # test sparsify with dense inputs\n    est.sparsify()\n    assert_true(sparse.issparse(est.coef_))\n    pred = est.predict(X)\n    assert_array_equal(pred, pred_orig)\n\n    # pickle and unpickle with sparse coef_\n    est = pickle.loads(pickle.dumps(est))\n    assert_true(sparse.issparse(est.coef_))\n    pred = est.predict(X)\n    assert_array_equal(pred, pred_orig)\n\n\n@ignore_warnings(category=DeprecationWarning)\ndef check_classifier_data_not_an_array(name, estimator_orig):\n    X = np.array([[3, 0], [0, 1], [0, 2], [1, 1], [1, 2], [2, 1]])\n    X = pairwise_estimator_convert_X(X, estimator_orig)\n    y = [1, 1, 1, 2, 2, 2]\n    y = multioutput_estimator_convert_y_2d(estimator_orig, y)\n    check_estimators_data_not_an_array(name, estimator_orig, X, y)\n\n\n@ignore_warnings(category=DeprecationWarning)\ndef check_regressor_data_not_an_array(name, estimator_orig):\n    X, y = _boston_subset(n_samples=50)\n    X = pairwise_estimator_convert_X(X, estimator_orig)\n    y = multioutput_estimator_convert_y_2d(estimator_orig, y)\n    check_estimators_data_not_an_array(name, estimator_orig, X, y)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_estimators_data_not_an_array(name, estimator_orig, X, y):\n    if name in CROSS_DECOMPOSITION:\n        raise SkipTest(\"Skipping check_estimators_data_not_an_array \"\n                       \"for cross decomposition module as estimators \"\n                       \"are not deterministic.\")\n    # separate estimators to control random seeds\n    estimator_1 = clone(estimator_orig)\n    estimator_2 = clone(estimator_orig)\n    set_random_state(estimator_1)\n    set_random_state(estimator_2)\n\n    y_ = NotAnArray(np.asarray(y))\n    X_ = NotAnArray(np.asarray(X))\n\n    # fit\n    estimator_1.fit(X_, y_)\n    pred1 = estimator_1.predict(X_)\n    estimator_2.fit(X, y)\n    pred2 = estimator_2.predict(X)\n    assert_allclose(pred1, pred2, atol=1e-2, err_msg=name)\n\n\ndef check_parameters_default_constructible(name, Estimator):\n    # this check works on classes, not instances\n    classifier = LinearDiscriminantAnalysis()\n    # test default-constructibility\n    # get rid of deprecation warnings\n    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):\n        if name in META_ESTIMATORS:\n            estimator = Estimator(classifier)\n        else:\n            estimator = Estimator()\n        # test cloning\n        clone(estimator)\n        # test __repr__\n        repr(estimator)\n        # test that set_params returns self\n        assert_true(estimator.set_params() is estimator)\n\n        # test if init does nothing but set parameters\n        # this is important for grid_search etc.\n        # We get the default parameters from init and then\n        # compare these against the actual values of the attributes.\n\n        # this comes from getattr. Gets rid of deprecation decorator.\n        init = getattr(estimator.__init__, 'deprecated_original',\n                       estimator.__init__)\n\n        try:\n            def param_filter(p):\n                \"\"\"Identify hyper parameters of an estimator\"\"\"\n                return (p.name != 'self' and\n                        p.kind != p.VAR_KEYWORD and\n                        p.kind != p.VAR_POSITIONAL)\n\n            init_params = [p for p in signature(init).parameters.values()\n                           if param_filter(p)]\n        except (TypeError, ValueError):\n            # init is not a python function.\n            # true for mixins\n            return\n        params = estimator.get_params()\n        if name in META_ESTIMATORS:\n            # they can need a non-default argument\n            init_params = init_params[1:]\n\n        for init_param in init_params:\n            assert_not_equal(init_param.default, init_param.empty,\n                             \"parameter %s for %s has no default value\"\n                             % (init_param.name, type(estimator).__name__))\n            assert_in(type(init_param.default),\n                      [str, int, float, bool, tuple, type(None),\n                       np.float64, types.FunctionType, Memory])\n            if init_param.name not in params.keys():\n                # deprecated parameter, not in get_params\n                assert_true(init_param.default is None)\n                continue\n\n            if (issubclass(Estimator, BaseSGD) and\n                    init_param.name in ['tol', 'max_iter']):\n                # To remove in 0.21, when they get their future default values\n                continue\n\n            param_value = params[init_param.name]\n            if isinstance(param_value, np.ndarray):\n                assert_array_equal(param_value, init_param.default)\n            else:\n                assert_equal(param_value, init_param.default, init_param.name)\n\n\ndef multioutput_estimator_convert_y_2d(estimator, y):\n    # Estimators in mono_output_task_error raise ValueError if y is of 1-D\n    # Convert into a 2-D y for those estimators.\n    if \"MultiTask\" in estimator.__class__.__name__:\n        return np.reshape(y, (-1, 1))\n    return y\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_non_transformer_estimators_n_iter(name, estimator_orig):\n    # Test that estimators that are not transformers with a parameter\n    # max_iter, return the attribute of n_iter_ at least 1.\n\n    # These models are dependent on external solvers like\n    # libsvm and accessing the iter parameter is non-trivial.\n    not_run_check_n_iter = ['Ridge', 'SVR', 'NuSVR', 'NuSVC',\n                            'RidgeClassifier', 'SVC', 'RandomizedLasso',\n                            'LogisticRegressionCV', 'LinearSVC',\n                            'LogisticRegression']\n\n    # Tested in test_transformer_n_iter\n    not_run_check_n_iter += CROSS_DECOMPOSITION\n    if name in not_run_check_n_iter:\n        return\n\n    # LassoLars stops early for the default alpha=1.0 the iris dataset.\n    if name == 'LassoLars':\n        estimator = clone(estimator_orig).set_params(alpha=0.)\n    else:\n        estimator = clone(estimator_orig)\n    if hasattr(estimator, 'max_iter'):\n        iris = load_iris()\n        X, y_ = iris.data, iris.target\n        y_ = multioutput_estimator_convert_y_2d(estimator, y_)\n\n        set_random_state(estimator, 0)\n        if name == 'AffinityPropagation':\n            estimator.fit(X)\n        else:\n            estimator.fit(X, y_)\n\n        # HuberRegressor depends on scipy.optimize.fmin_l_bfgs_b\n        # which doesn't return a n_iter for old versions of SciPy.\n        if not (name == 'HuberRegressor' and estimator.n_iter_ is None):\n            assert_greater_equal(estimator.n_iter_, 1)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_transformer_n_iter(name, estimator_orig):\n    # Test that transformers with a parameter max_iter, return the\n    # attribute of n_iter_ at least 1.\n    estimator = clone(estimator_orig)\n    if hasattr(estimator, \"max_iter\"):\n        if name in CROSS_DECOMPOSITION:\n            # Check using default data\n            X = [[0., 0., 1.], [1., 0., 0.], [2., 2., 2.], [2., 5., 4.]]\n            y_ = [[0.1, -0.2], [0.9, 1.1], [0.1, -0.5], [0.3, -0.2]]\n\n        else:\n            X, y_ = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],\n                               random_state=0, n_features=2, cluster_std=0.1)\n            X -= X.min() - 0.1\n        set_random_state(estimator, 0)\n        estimator.fit(X, y_)\n\n        # These return a n_iter per component.\n        if name in CROSS_DECOMPOSITION:\n            for iter_ in estimator.n_iter_:\n                assert_greater_equal(iter_, 1)\n        else:\n            assert_greater_equal(estimator.n_iter_, 1)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_get_params_invariance(name, estimator_orig):\n    # Checks if get_params(deep=False) is a subset of get_params(deep=True)\n    class T(BaseEstimator):\n        \"\"\"Mock classifier\n        \"\"\"\n\n        def __init__(self):\n            pass\n\n        def fit(self, X, y):\n            return self\n\n        def transform(self, X):\n            return X\n\n    e = clone(estimator_orig)\n\n    shallow_params = e.get_params(deep=False)\n    deep_params = e.get_params(deep=True)\n\n    assert_true(all(item in deep_params.items() for item in\n                    shallow_params.items()))\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_classifiers_regression_target(name, estimator_orig):\n    # Check if classifier throws an exception when fed regression targets\n\n    boston = load_boston()\n    X, y = boston.data, boston.target\n    e = clone(estimator_orig)\n    msg = 'Unknown label type: '\n    assert_raises_regex(ValueError, msg, e.fit, X, y)\n\n\n@ignore_warnings(category=(DeprecationWarning, FutureWarning))\ndef check_decision_proba_consistency(name, estimator_orig):\n    # Check whether an estimator having both decision_function and\n    # predict_proba methods has outputs with perfect rank correlation.\n\n    centers = [(2, 2), (4, 4)]\n    X, y = make_blobs(n_samples=100, random_state=0, n_features=4,\n                      centers=centers, cluster_std=1.0, shuffle=True)\n    X_test = np.random.randn(20, 2) + 4\n    estimator = clone(estimator_orig)\n\n    if (hasattr(estimator, \"decision_function\") and\n            hasattr(estimator, \"predict_proba\")):\n\n        estimator.fit(X, y)\n        a = estimator.predict_proba(X_test)[:, 1]\n        b = estimator.decision_function(X_test)\n        assert_array_equal(rankdata(a), rankdata(b))\n"
    }
  ],
  "questions": [
    "@qinhanmin2014  and @jnothman I need help, I'm a little confused, should I remove all mentions from the SimpleImputer `axis` only in the documentation? And besides, could you help me how could be the comment that the `axis` was removed?\r\n\r\nIn the code the `axis` often appears as `axis = 0`, should I change to `None` or delete these entries as well?\r\n\r\nFor example in the code below (in impute.py class SimpleImpute) should I remove these entries from the `axis` parameter or set `None`?\r\n```python\r\n    def __init__(self, missing_values=\"NaN\", strategy=\"mean\",\r\n                 axis=0, verbose=0, copy=True):\r\n        self.missing_values = missing_values\r\n        self.strategy = strategy\r\n        self.axis = axis\r\n        self.verbose = verbose\r\n        self.copy = copy\r\n```\r\ntks! =)"
  ],
  "golden_answers": [
    "Because this is not released code, it can be safely removed without breaking backwards compatibility. Just drop axis there. Don't worry about looking at the previous work and comments too much.\r\n\r\nYes, you need to remove from the documentation too, if necessary adjusting the wording to say imputation is done over each column.\r\n\r\nIn terms of adding a comment on axis being removed, we ultimately want to say something like \"sklearn.preprocessing.Imputer has been renamed to sklearn.impute.SimpleImputer, and its axis parameter is no longer available. The deprecated Imputer will be removed in version 0.22.\" in doc/whats_new/v0.20.rst where the deprecation of Imputer is currently mentioned (but you can use your own words). A mention of axis disappearing is also appropriate in the deprecation warning at https://github.com/scikit-learn/scikit-learn/blob/4d4116097deee37d9ea38c447401c29456000e78/sklearn/preprocessing/imputation.py#L64"
  ],
  "questions_generated": [
    "What is the main goal of the issue regarding the SimpleImputer's axis parameter in the scikit-learn repository?",
    "How should the constructor of the SimpleImputer class be modified to remove the axis parameter?",
    "What update needs to be made to the documentation to reflect the removal of the axis parameter from SimpleImputer?",
    "What is the suggested message to include in the 'what's new' entry after the removal of the axis parameter?",
    "In the context of this issue, what is the recommended approach for adjusting the code that previously utilized the axis parameter?"
  ],
  "golden_answers_generated": [
    "The main goal of the issue is to remove the axis parameter from the SimpleImputer class in the scikit-learn repository. Since SimpleImputer has not yet been released, the removal can be done without deprecating the parameter. The issue also involves updating the 'what's new' entry to reflect that the axis parameter has been removed.",
    "The constructor of the SimpleImputer class should be modified by removing the axis parameter altogether. This involves deleting the axis parameter from the method signature and any associated variables or logic related to it within the class implementation.",
    "The documentation should be updated to remove any mention of the axis parameter in association with SimpleImputer. Additionally, the wording should be adjusted to specify that imputation is performed over each column. A note should also be added to the 'what's new' section indicating that the axis parameter has been removed, along with guidance for users that future behavior is equivalent to axis=0 (imputation along columns).",
    "The suggested message for the 'what's new' entry is: 'sklearn.preprocessing.Imputer has been renamed to sklearn.impute.SimpleImputer, and its axis parameter is no longer available. The deprecated Imputer will be removed in version 0.22.' This should also advise users that the future behavior is equivalent to axis=0, meaning imputation will occur along columns.",
    "The recommended approach is to remove any code that utilizes the axis parameter, as it is no longer necessary. Any logic or conditions based on the axis parameter should be refactored or removed, ensuring that imputation is done column-wise by default. This aligns with the future behavior of the SimpleImputer class, which will assume imputation along columns."
  ]
}