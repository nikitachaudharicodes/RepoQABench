{
  "repo_name": "scikit-learn_scikit-learn",
  "issue_id": "12895",
  "issue_description": "# Implement RMSE (root-mean-square error) metric and scorer\n\nRMSE seems to be a popular metric but now one has to calculate it through ``np.sqrt(mean_squared_error(XXX, XXX))``. Maybe we can add ``squared`` option to ``mean_squared_error`` and add a scorer ``neg_root_mean_squared_error``.\r\nWiki page: https://en.wikipedia.org/wiki/Root-mean-square_deviation",
  "issue_comments": [
    {
      "id": 450756687,
      "user": "gykovacs",
      "body": "As the square root is a monotonic function on the positive domain, taking the square root would have no effect on any model selection. Could you please mention a use-case when it taking the root has some real advantage?"
    },
    {
      "id": 450770389,
      "user": "qinhanmin2014",
      "body": "> As the square root is a monotonic function on the positive domain, taking the square root would have no effect on any model selection\r\n\r\nThis is why we reject it previously I think (though I'm unable to find relevant discussions)\r\nI'd argue that given the popularity of RMSE, it might be worthwhile to add several lines of (redundant) code for it (we only need <5 lines of code for the metric I think)\r\nSometimes users might want to report the RMSE of their model instead of MSE, because RMSE is more meaningful (i.e., it reflects the deviation between actual value and predicted value).\r\n"
    },
    {
      "id": 450927997,
      "user": "urvang96",
      "body": "Hi,\r\nIf there is a consensus on this I would like to give this a try."
    },
    {
      "id": 451176794,
      "user": "qinhanmin2014",
      "body": "> If there is a consensus on this I would like to give this a try.\r\n\r\nnot yet, please wait or try another issue."
    },
    {
      "id": 472031069,
      "user": "qinhanmin2014",
      "body": "Hmm, I found https://github.com/scikit-learn/scikit-learn/pull/6457#issuecomment-253975180"
    },
    {
      "id": 473680984,
      "user": "urvang96",
      "body": "I would like to work on this."
    },
    {
      "id": 518168749,
      "user": "alokranjan1234",
      "body": "Is this still open? I would like to work on it.\r\n"
    },
    {
      "id": 518260044,
      "user": "qinhanmin2014",
      "body": "you can start your work based on the stalled PR."
    },
    {
      "id": 518260630,
      "user": "amueller",
      "body": "I think we should indeed add this."
    },
    {
      "id": 518261920,
      "user": "urvang96",
      "body": "@amueller @qinhanmin2014  Could you please review the PR, which I created. PR #13467"
    },
    {
      "id": 856435627,
      "user": "linehammer",
      "body": "Mean Squared Error ( MSE ) is defined as Mean or Average of the square of the difference between actual and estimated values. This means that MSE is calculated by the square of the difference between the predicted and actual target variables, divided by the number of data points. It is always non–negative values and close to zero are better.\r\n\r\n[Root Mean Squared Error](http://net-informations.com/ds/psa/rmse.htm) is the square root of Mean Squared Error (MSE). This is the same as Mean Squared Error (MSE) but the root of the value is considered while determining the accuracy of the model."
    }
  ],
  "text_context": "# Implement RMSE (root-mean-square error) metric and scorer\n\nRMSE seems to be a popular metric but now one has to calculate it through ``np.sqrt(mean_squared_error(XXX, XXX))``. Maybe we can add ``squared`` option to ``mean_squared_error`` and add a scorer ``neg_root_mean_squared_error``.\r\nWiki page: https://en.wikipedia.org/wiki/Root-mean-square_deviation\n\nAs the square root is a monotonic function on the positive domain, taking the square root would have no effect on any model selection. Could you please mention a use-case when it taking the root has some real advantage?\n\n> As the square root is a monotonic function on the positive domain, taking the square root would have no effect on any model selection\r\n\r\nThis is why we reject it previously I think (though I'm unable to find relevant discussions)\r\nI'd argue that given the popularity of RMSE, it might be worthwhile to add several lines of (redundant) code for it (we only need <5 lines of code for the metric I think)\r\nSometimes users might want to report the RMSE of their model instead of MSE, because RMSE is more meaningful (i.e., it reflects the deviation between actual value and predicted value).\r\n\n\nHi,\r\nIf there is a consensus on this I would like to give this a try.\n\n> If there is a consensus on this I would like to give this a try.\r\n\r\nnot yet, please wait or try another issue.\n\nHmm, I found https://github.com/scikit-learn/scikit-learn/pull/6457#issuecomment-253975180\n\nI would like to work on this.\n\nIs this still open? I would like to work on it.\r\n\n\nyou can start your work based on the stalled PR.\n\nI think we should indeed add this.\n\n@amueller @qinhanmin2014  Could you please review the PR, which I created. PR #13467\n\nMean Squared Error ( MSE ) is defined as Mean or Average of the square of the difference between actual and estimated values. This means that MSE is calculated by the square of the difference between the predicted and actual target variables, divided by the number of data points. It is always non–negative values and close to zero are better.\r\n\r\n[Root Mean Squared Error](http://net-informations.com/ds/psa/rmse.htm) is the square root of Mean Squared Error (MSE). This is the same as Mean Squared Error (MSE) but the root of the value is considered while determining the accuracy of the model.",
  "pr_link": "https://github.com/scikit-learn/scikit-learn/pull/6457",
  "code_context": [
    {
      "filename": "sklearn/metrics/__init__.py",
      "content": "\"\"\"\nThe :mod:`sklearn.metrics` module includes score functions, performance metrics\nand pairwise metrics and distance computations.\n\"\"\"\n\n\nfrom .ranking import auc\nfrom .ranking import average_precision_score\nfrom .ranking import coverage_error\nfrom .ranking import label_ranking_average_precision_score\nfrom .ranking import label_ranking_loss\nfrom .ranking import precision_recall_curve\nfrom .ranking import roc_auc_score\nfrom .ranking import roc_curve\n\nfrom .classification import accuracy_score\nfrom .classification import classification_report\nfrom .classification import cohen_kappa_score\nfrom .classification import confusion_matrix\nfrom .classification import f1_score\nfrom .classification import fbeta_score\nfrom .classification import hamming_loss\nfrom .classification import hinge_loss\nfrom .classification import jaccard_similarity_score\nfrom .classification import log_loss\nfrom .classification import matthews_corrcoef\nfrom .classification import precision_recall_fscore_support\nfrom .classification import precision_score\nfrom .classification import recall_score\nfrom .classification import zero_one_loss\nfrom .classification import brier_score_loss\n\nfrom . import cluster\nfrom .cluster import adjusted_mutual_info_score\nfrom .cluster import adjusted_rand_score\nfrom .cluster import completeness_score\nfrom .cluster import consensus_score\nfrom .cluster import homogeneity_completeness_v_measure\nfrom .cluster import homogeneity_score\nfrom .cluster import mutual_info_score\nfrom .cluster import normalized_mutual_info_score\nfrom .cluster import fowlkes_mallows_score\nfrom .cluster import silhouette_samples\nfrom .cluster import silhouette_score\nfrom .cluster import calinski_harabaz_score\nfrom .cluster import v_measure_score\n\nfrom .pairwise import euclidean_distances\nfrom .pairwise import pairwise_distances\nfrom .pairwise import pairwise_distances_argmin\nfrom .pairwise import pairwise_distances_argmin_min\nfrom .pairwise import pairwise_kernels\n\nfrom .regression import explained_variance_score\nfrom .regression import mean_absolute_error\nfrom .regression import mean_squared_error\nfrom .regression import root_mean_squared_error\nfrom .regression import median_absolute_error\nfrom .regression import r2_score\n\nfrom .scorer import make_scorer\nfrom .scorer import SCORERS\nfrom .scorer import get_scorer\n\n__all__ = [\n    'accuracy_score',\n    'adjusted_mutual_info_score',\n    'adjusted_rand_score',\n    'auc',\n    'average_precision_score',\n    'classification_report',\n    'cluster',\n    'completeness_score',\n    'confusion_matrix',\n    'consensus_score',\n    'coverage_error',\n    'euclidean_distances',\n    'explained_variance_score',\n    'f1_score',\n    'fbeta_score',\n    'get_scorer',\n    'hamming_loss',\n    'hinge_loss',\n    'homogeneity_completeness_v_measure',\n    'homogeneity_score',\n    'jaccard_similarity_score',\n    'label_ranking_average_precision_score',\n    'label_ranking_loss',\n    'log_loss',\n    'make_scorer',\n    'matthews_corrcoef',\n    'mean_absolute_error',\n    'mean_squared_error',\n    'root_mean_squared_error',\n    'median_absolute_error',\n    'mutual_info_score',\n    'normalized_mutual_info_score',\n    'pairwise_distances',\n    'pairwise_distances_argmin',\n    'pairwise_distances_argmin_min',\n    'pairwise_distances_argmin_min',\n    'pairwise_kernels',\n    'precision_recall_curve',\n    'precision_recall_fscore_support',\n    'precision_score',\n    'r2_score',\n    'recall_score',\n    'roc_auc_score',\n    'roc_curve',\n    'SCORERS',\n    'silhouette_samples',\n    'silhouette_score',\n    'v_measure_score',\n    'zero_one_loss',\n    'brier_score_loss',\n]\n"
    },
    {
      "filename": "sklearn/metrics/regression.py",
      "content": "\"\"\"Metrics to assess performance on regression task\n\nFunctions named as ``*_score`` return a scalar value to maximize: the higher\nthe better\n\nFunction named as ``*_error`` or ``*_loss`` return a scalar value to minimize:\nthe lower the better\n\"\"\"\n\n# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Arnaud Joly <a.joly@ulg.ac.be>\n#          Jochen Wersdorfer <jochen@wersdoerfer.de>\n#          Lars Buitinck\n#          Joel Nothman <joel.nothman@gmail.com>\n#          Noel Dawe <noel@dawe.me>\n#          Manoj Kumar <manojkumarsivaraj334@gmail.com>\n#          Michael Eickenberg <michael.eickenberg@gmail.com>\n#          Konstantin Shmelkov <konstantin.shmelkov@polytechnique.edu>\n# License: BSD 3 clause\n\nfrom __future__ import division\n\nimport numpy as np\n\nfrom ..utils.validation import check_array, check_consistent_length\nfrom ..utils.validation import column_or_1d\nfrom ..externals.six import string_types\n\nimport warnings\n\n__ALL__ = [\n    \"mean_absolute_error\",\n    \"mean_squared_error\",\n    \"root_mean_squared_error\",\n    \"median_absolute_error\",\n    \"r2_score\",\n    \"explained_variance_score\"\n]\n\n\ndef _check_reg_targets(y_true, y_pred, multioutput):\n    \"\"\"Check that y_true and y_pred belong to the same regression task\n\n    Parameters\n    ----------\n    y_true : array-like,\n\n    y_pred : array-like,\n\n    multioutput : array-like or string in ['raw_values', uniform_average',\n        'variance_weighted'] or None\n        None is accepted due to backward compatibility of r2_score().\n\n    Returns\n    -------\n    type_true : one of {'continuous', continuous-multioutput'}\n        The type of the true target data, as output by\n        'utils.multiclass.type_of_target'\n\n    y_true : array-like of shape = (n_samples, n_outputs)\n        Ground truth (correct) target values.\n\n    y_pred : array-like of shape = (n_samples, n_outputs)\n        Estimated target values.\n\n    multioutput : array-like of shape = (n_outputs) or string in ['raw_values',\n        uniform_average', 'variance_weighted'] or None\n        Custom output weights if ``multioutput`` is array-like or\n        just the corresponding argument if ``multioutput`` is a\n        correct keyword.\n\n    \"\"\"\n    check_consistent_length(y_true, y_pred)\n    y_true = check_array(y_true, ensure_2d=False)\n    y_pred = check_array(y_pred, ensure_2d=False)\n\n    if y_true.ndim == 1:\n        y_true = y_true.reshape((-1, 1))\n\n    if y_pred.ndim == 1:\n        y_pred = y_pred.reshape((-1, 1))\n\n    if y_true.shape[1] != y_pred.shape[1]:\n        raise ValueError(\"y_true and y_pred have different number of output \"\n                         \"({0}!={1})\".format(y_true.shape[1], y_pred.shape[1]))\n\n    n_outputs = y_true.shape[1]\n    multioutput_options = (None, 'raw_values', 'uniform_average',\n                           'variance_weighted')\n    if multioutput not in multioutput_options:\n        multioutput = check_array(multioutput, ensure_2d=False)\n        if n_outputs == 1:\n            raise ValueError(\"Custom weights are useful only in \"\n                             \"multi-output cases.\")\n        elif n_outputs != len(multioutput):\n            raise ValueError((\"There must be equally many custom weights \"\n                              \"(%d) as outputs (%d).\") %\n                             (len(multioutput), n_outputs))\n    y_type = 'continuous' if n_outputs == 1 else 'continuous-multioutput'\n\n    return y_type, y_true, y_pred, multioutput\n\n\ndef mean_absolute_error(y_true, y_pred,\n                        sample_weight=None,\n                        multioutput='uniform_average'):\n    \"\"\"Mean absolute error regression loss\n\n    Read more in the :ref:`User Guide <mean_absolute_error>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n\n    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n\n    sample_weight : array-like of shape = (n_samples), optional\n        Sample weights.\n\n    multioutput : string in ['raw_values', 'uniform_average']\n        or array-like of shape (n_outputs)\n        Defines aggregating of multiple output values.\n        Array-like value defines weights used to average errors.\n\n        'raw_values' :\n            Returns a full set of errors in case of multioutput input.\n\n        'uniform_average' :\n            Errors of all outputs are averaged with uniform weight.\n\n\n    Returns\n    -------\n    loss : float or ndarray of floats\n        If multioutput is 'raw_values', then mean absolute error is returned\n        for each output separately.\n        If multioutput is 'uniform_average' or an ndarray of weights, then the\n        weighted average of all output errors is returned.\n\n        MAE output is non-negative floating point. The best value is 0.0.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import mean_absolute_error\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> mean_absolute_error(y_true, y_pred)\n    0.5\n    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n    >>> mean_absolute_error(y_true, y_pred)\n    0.75\n    >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n    array([ 0.5,  1. ])\n    >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n    ... # doctest: +ELLIPSIS\n    0.849...\n    \"\"\"\n    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n        y_true, y_pred, multioutput)\n    output_errors = np.average(np.abs(y_pred - y_true),\n                               weights=sample_weight, axis=0)\n    if isinstance(multioutput, string_types):\n        if multioutput == 'raw_values':\n            return output_errors\n        elif multioutput == 'uniform_average':\n            # pass None as weights to np.average: uniform mean\n            multioutput = None\n\n    return np.average(output_errors, weights=multioutput)\n\n\ndef mean_squared_error(y_true, y_pred,\n                       sample_weight=None,\n                       multioutput='uniform_average'):\n    \"\"\"Mean squared error regression loss\n\n    Read more in the :ref:`User Guide <mean_squared_error>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n\n    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n\n    sample_weight : array-like of shape = (n_samples), optional\n        Sample weights.\n\n    multioutput : string in ['raw_values', 'uniform_average']\n        or array-like of shape (n_outputs)\n        Defines aggregating of multiple output values.\n        Array-like value defines weights used to average errors.\n\n        'raw_values' :\n            Returns a full set of errors in case of multioutput input.\n\n        'uniform_average' :\n            Errors of all outputs are averaged with uniform weight.\n\n    Returns\n    -------\n    loss : float or ndarray of floats\n        A non-negative floating point value (the best value is 0.0), or an\n        array of floating point values, one for each individual target.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import mean_squared_error\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> mean_squared_error(y_true, y_pred)\n    0.375\n    >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n    >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n    >>> mean_squared_error(y_true, y_pred)  # doctest: +ELLIPSIS\n    0.708...\n    >>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n    ... # doctest: +ELLIPSIS\n    array([ 0.416...,  1.        ])\n    >>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n    ... # doctest: +ELLIPSIS\n    0.824...\n\n    \"\"\"\n    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n        y_true, y_pred, multioutput)\n    output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n                               weights=sample_weight)\n    if isinstance(multioutput, string_types):\n        if multioutput == 'raw_values':\n            return output_errors\n        elif multioutput == 'uniform_average':\n            # pass None as weights to np.average: uniform mean\n            multioutput = None\n\n    return np.average(output_errors, weights=multioutput)\n\n\ndef root_mean_squared_error(y_true, y_pred,\n                            sample_weight=None,\n                            multioutput='uniform_average'):\n    \"\"\"Root mean squared error regression loss\n\n    Read more in the :ref:`User Guide <root_mean_squared_error>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n\n    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n\n    sample_weight : array-like of shape = (n_samples), optional\n        Sample weights.\n\n    multioutput : string in ['raw_values', 'uniform_average']\n        or array-like of shape (n_outputs)\n        Defines aggregating of multiple output values.\n        Array-like value defines weights used to average errors.\n\n        'raw_values' :\n            Returns a full set of errors in case of multioutput input.\n\n        'uniform_average' :\n            Errors of all outputs are averaged with uniform weight.\n\n    Returns\n    -------\n    loss : float or ndarray of floats\n        A non-negative floating point value (the best value is 0.0), or an\n        array of floating point values, one for each individual target.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import root_mean_squared_error\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> root_mean_squared_error(y_true, y_pred)  # doctest: +ELLIPSIS\n    0.612...\n    >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n    >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n    >>> root_mean_squared_error(y_true, y_pred)  # doctest: +ELLIPSIS\n    0.841...\n    >>> root_mean_squared_error(y_true, y_pred, multioutput='raw_values')\n    ... # doctest: +ELLIPSIS\n    array([ 0.645...,  1.        ])\n    >>> root_mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n    ... # doctest: +ELLIPSIS\n    0.908...\n\n    \"\"\"\n    error = mean_squared_error(y_true, y_pred, sample_weight, multioutput)\n    return error ** 0.5\n\n\ndef median_absolute_error(y_true, y_pred):\n    \"\"\"Median absolute error regression loss\n\n    Read more in the :ref:`User Guide <median_absolute_error>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape = (n_samples)\n        Ground truth (correct) target values.\n\n    y_pred : array-like of shape = (n_samples)\n        Estimated target values.\n\n    Returns\n    -------\n    loss : float\n        A positive floating point value (the best value is 0.0).\n\n    Examples\n    --------\n    >>> from sklearn.metrics import median_absolute_error\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> median_absolute_error(y_true, y_pred)\n    0.5\n\n    \"\"\"\n    y_type, y_true, y_pred, _ = _check_reg_targets(y_true, y_pred,\n                                                   'uniform_average')\n    if y_type == 'continuous-multioutput':\n        raise ValueError(\"Multioutput not supported in median_absolute_error\")\n    return np.median(np.abs(y_pred - y_true))\n\n\ndef explained_variance_score(y_true, y_pred,\n                             sample_weight=None,\n                             multioutput='uniform_average'):\n    \"\"\"Explained variance regression score function\n\n    Best possible score is 1.0, lower values are worse.\n\n    Read more in the :ref:`User Guide <explained_variance_score>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n\n    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n\n    sample_weight : array-like of shape = (n_samples), optional\n        Sample weights.\n\n    multioutput : string in ['raw_values', 'uniform_average', \\\n                'variance_weighted'] or array-like of shape (n_outputs)\n        Defines aggregating of multiple output scores.\n        Array-like value defines weights used to average scores.\n\n        'raw_values' :\n            Returns a full set of scores in case of multioutput input.\n\n        'uniform_average' :\n            Scores of all outputs are averaged with uniform weight.\n\n        'variance_weighted' :\n            Scores of all outputs are averaged, weighted by the variances\n            of each individual output.\n\n    Returns\n    -------\n    score : float or ndarray of floats\n        The explained variance or ndarray if 'multioutput' is 'raw_values'.\n\n    Notes\n    -----\n    This is not a symmetric function.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import explained_variance_score\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> explained_variance_score(y_true, y_pred)  # doctest: +ELLIPSIS\n    0.957...\n    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n    >>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n    ... # doctest: +ELLIPSIS\n    0.983...\n\n    \"\"\"\n    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n        y_true, y_pred, multioutput)\n\n    y_diff_avg = np.average(y_true - y_pred, weights=sample_weight, axis=0)\n    numerator = np.average((y_true - y_pred - y_diff_avg) ** 2,\n                           weights=sample_weight, axis=0)\n\n    y_true_avg = np.average(y_true, weights=sample_weight, axis=0)\n    denominator = np.average((y_true - y_true_avg) ** 2,\n                             weights=sample_weight, axis=0)\n\n    nonzero_numerator = numerator != 0\n    nonzero_denominator = denominator != 0\n    valid_score = nonzero_numerator & nonzero_denominator\n    output_scores = np.ones(y_true.shape[1])\n\n    output_scores[valid_score] = 1 - (numerator[valid_score] /\n                                      denominator[valid_score])\n    output_scores[nonzero_numerator & ~nonzero_denominator] = 0.\n    if isinstance(multioutput, string_types):\n        if multioutput == 'raw_values':\n            # return scores individually\n            return output_scores\n        elif multioutput == 'uniform_average':\n            # passing to np.average() None as weights results is uniform mean\n            avg_weights = None\n        elif multioutput == 'variance_weighted':\n            avg_weights = denominator\n    else:\n        avg_weights = multioutput\n\n    return np.average(output_scores, weights=avg_weights)\n\n\ndef r2_score(y_true, y_pred,\n             sample_weight=None,\n             multioutput=None):\n    \"\"\"R^2 (coefficient of determination) regression score function.\n\n    Best possible score is 1.0 and it can be negative (because the\n    model can be arbitrarily worse). A constant model that always\n    predicts the expected value of y, disregarding the input features,\n    would get a R^2 score of 0.0.\n\n    Read more in the :ref:`User Guide <r2_score>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n\n    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n\n    sample_weight : array-like of shape = (n_samples), optional\n        Sample weights.\n\n    multioutput : string in ['raw_values', 'uniform_average', \\\n'variance_weighted'] or None or array-like of shape (n_outputs)\n\n        Defines aggregating of multiple output scores.\n        Array-like value defines weights used to average scores.\n        Default value corresponds to 'variance_weighted', this behaviour is\n        deprecated since version 0.17 and will be changed to 'uniform_average'\n        starting from 0.19.\n\n        'raw_values' :\n            Returns a full set of scores in case of multioutput input.\n\n        'uniform_average' :\n            Scores of all outputs are averaged with uniform weight.\n\n        'variance_weighted' :\n            Scores of all outputs are averaged, weighted by the variances\n            of each individual output.\n\n    Returns\n    -------\n    z : float or ndarray of floats\n        The R^2 score or ndarray of scores if 'multioutput' is\n        'raw_values'.\n\n    Notes\n    -----\n    This is not a symmetric function.\n\n    Unlike most other scores, R^2 score may be negative (it need not actually\n    be the square of a quantity R).\n\n    References\n    ----------\n    .. [1] `Wikipedia entry on the Coefficient of determination\n            <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n\n    Examples\n    --------\n    >>> from sklearn.metrics import r2_score\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> r2_score(y_true, y_pred)  # doctest: +ELLIPSIS\n    0.948...\n    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n    >>> r2_score(y_true, y_pred, multioutput='variance_weighted')  # doctest: +ELLIPSIS\n    0.938...\n\n    \"\"\"\n    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n        y_true, y_pred, multioutput)\n\n    if sample_weight is not None:\n        sample_weight = column_or_1d(sample_weight)\n        weight = sample_weight[:, np.newaxis]\n    else:\n        weight = 1.\n\n    numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0,\n                                                      dtype=np.float64)\n    denominator = (weight * (y_true - np.average(\n        y_true, axis=0, weights=sample_weight)) ** 2).sum(axis=0,\n                                                          dtype=np.float64)\n    nonzero_denominator = denominator != 0\n    nonzero_numerator = numerator != 0\n    valid_score = nonzero_denominator & nonzero_numerator\n    output_scores = np.ones([y_true.shape[1]])\n    output_scores[valid_score] = 1 - (numerator[valid_score] /\n                                      denominator[valid_score])\n    # arbitrary set to zero to avoid -inf scores, having a constant\n    # y_true is not interesting for scoring a regression anyway\n    output_scores[nonzero_numerator & ~nonzero_denominator] = 0.\n    if multioutput is None and y_true.shape[1] != 1:\n        warnings.warn(\"Default 'multioutput' behavior now corresponds to \"\n                      \"'variance_weighted' value which is deprecated since \"\n                      \"0.17, it will be changed to 'uniform_average' \"\n                      \"starting from 0.19.\",\n                      DeprecationWarning)\n        multioutput = 'variance_weighted'\n    if isinstance(multioutput, string_types):\n        if multioutput == 'raw_values':\n            # return scores individually\n            return output_scores\n        elif multioutput == 'uniform_average':\n            # passing None as weights results is uniform mean\n            avg_weights = None\n        elif multioutput == 'variance_weighted':\n            avg_weights = denominator\n            # avoid fail on constant y or one-element arrays\n            if not np.any(nonzero_denominator):\n                if not np.any(nonzero_numerator):\n                    return 1.0\n                else:\n                    return 0.0\n    else:\n        avg_weights = multioutput\n\n    return np.average(output_scores, weights=avg_weights)\n"
    },
    {
      "filename": "sklearn/metrics/scorer.py",
      "content": "\"\"\"\nThe :mod:`sklearn.metrics.scorer` submodule implements a flexible\ninterface for model selection and evaluation using\narbitrary score functions.\n\nA scorer object is a callable that can be passed to\n:class:`sklearn.model_selection.GridSearchCV` or\n:func:`sklearn.model_selection.cross_val_score` as the ``scoring``\nparameter, to specify how a model should be evaluated.\n\nThe signature of the call is ``(estimator, X, y)`` where ``estimator``\nis the model to be evaluated, ``X`` is the test data and ``y`` is the\nground truth labeling (or ``None`` in the case of unsupervised models).\n\"\"\"\n\n# Authors: Andreas Mueller <amueller@ais.uni-bonn.de>\n#          Lars Buitinck\n#          Arnaud Joly <arnaud.v.joly@gmail.com>\n# License: Simplified BSD\n\nfrom abc import ABCMeta, abstractmethod\nimport warnings\n\nimport numpy as np\n\nfrom . import (r2_score, median_absolute_error, mean_absolute_error,\n               mean_squared_error, root_mean_squared_error,\n               accuracy_score, f1_score,\n               roc_auc_score, average_precision_score,\n               precision_score, recall_score, log_loss)\nfrom .cluster import adjusted_rand_score\nfrom ..utils.multiclass import type_of_target\nfrom ..externals import six\nfrom ..base import is_regressor\n\n\nclass _BaseScorer(six.with_metaclass(ABCMeta, object)):\n    def __init__(self, score_func, sign, kwargs):\n        self._kwargs = kwargs\n        self._score_func = score_func\n        self._sign = sign\n        # XXX After removing the deprecated scorers (v0.20) remove the\n        # XXX deprecation_msg property again and remove __call__'s body again\n        self._deprecation_msg = None\n\n    @abstractmethod\n    def __call__(self, estimator, X, y, sample_weight=None):\n        if self._deprecation_msg is not None:\n            warnings.warn(self._deprecation_msg,\n                          category=DeprecationWarning,\n                          stacklevel=2)\n\n    def __repr__(self):\n        kwargs_string = \"\".join([\", %s=%s\" % (str(k), str(v))\n                                 for k, v in self._kwargs.items()])\n        return (\"make_scorer(%s%s%s%s)\"\n                % (self._score_func.__name__,\n                   \"\" if self._sign > 0 else \", greater_is_better=False\",\n                   self._factory_args(), kwargs_string))\n\n    def _factory_args(self):\n        \"\"\"Return non-default make_scorer arguments for repr.\"\"\"\n        return \"\"\n\n\nclass _PredictScorer(_BaseScorer):\n    def __call__(self, estimator, X, y_true, sample_weight=None):\n        \"\"\"Evaluate predicted target values for X relative to y_true.\n\n        Parameters\n        ----------\n        estimator : object\n            Trained estimator to use for scoring. Must have a predict_proba\n            method; the output of that is used to compute the score.\n\n        X : array-like or sparse matrix\n            Test data that will be fed to estimator.predict.\n\n        y_true : array-like\n            Gold standard target values for X.\n\n        sample_weight : array-like, optional (default=None)\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Score function applied to prediction of estimator on X.\n        \"\"\"\n        super(_PredictScorer, self).__call__(estimator, X, y_true,\n                                             sample_weight=sample_weight)\n        y_pred = estimator.predict(X)\n        if sample_weight is not None:\n            return self._sign * self._score_func(y_true, y_pred,\n                                                 sample_weight=sample_weight,\n                                                 **self._kwargs)\n        else:\n            return self._sign * self._score_func(y_true, y_pred,\n                                                 **self._kwargs)\n\n\nclass _ProbaScorer(_BaseScorer):\n    def __call__(self, clf, X, y, sample_weight=None):\n        \"\"\"Evaluate predicted probabilities for X relative to y_true.\n\n        Parameters\n        ----------\n        clf : object\n            Trained classifier to use for scoring. Must have a predict_proba\n            method; the output of that is used to compute the score.\n\n        X : array-like or sparse matrix\n            Test data that will be fed to clf.predict_proba.\n\n        y : array-like\n            Gold standard target values for X. These must be class labels,\n            not probabilities.\n\n        sample_weight : array-like, optional (default=None)\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Score function applied to prediction of estimator on X.\n        \"\"\"\n        super(_ProbaScorer, self).__call__(clf, X, y,\n                                           sample_weight=sample_weight)\n        y_pred = clf.predict_proba(X)\n        if sample_weight is not None:\n            return self._sign * self._score_func(y, y_pred,\n                                                 sample_weight=sample_weight,\n                                                 **self._kwargs)\n        else:\n            return self._sign * self._score_func(y, y_pred, **self._kwargs)\n\n    def _factory_args(self):\n        return \", needs_proba=True\"\n\n\nclass _ThresholdScorer(_BaseScorer):\n    def __call__(self, clf, X, y, sample_weight=None):\n        \"\"\"Evaluate decision function output for X relative to y_true.\n\n        Parameters\n        ----------\n        clf : object\n            Trained classifier to use for scoring. Must have either a\n            decision_function method or a predict_proba method; the output of\n            that is used to compute the score.\n\n        X : array-like or sparse matrix\n            Test data that will be fed to clf.decision_function or\n            clf.predict_proba.\n\n        y : array-like\n            Gold standard target values for X. These must be class labels,\n            not decision function values.\n\n        sample_weight : array-like, optional (default=None)\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Score function applied to prediction of estimator on X.\n        \"\"\"\n        super(_ThresholdScorer, self).__call__(clf, X, y,\n                                               sample_weight=sample_weight)\n        y_type = type_of_target(y)\n        if y_type not in (\"binary\", \"multilabel-indicator\"):\n            raise ValueError(\"{0} format is not supported\".format(y_type))\n\n        if is_regressor(clf):\n            y_pred = clf.predict(X)\n        else:\n            try:\n                y_pred = clf.decision_function(X)\n\n                # For multi-output multi-class estimator\n                if isinstance(y_pred, list):\n                    y_pred = np.vstack(p for p in y_pred).T\n\n            except (NotImplementedError, AttributeError):\n                y_pred = clf.predict_proba(X)\n\n                if y_type == \"binary\":\n                    y_pred = y_pred[:, 1]\n                elif isinstance(y_pred, list):\n                    y_pred = np.vstack([p[:, -1] for p in y_pred]).T\n\n        if sample_weight is not None:\n            return self._sign * self._score_func(y, y_pred,\n                                                 sample_weight=sample_weight,\n                                                 **self._kwargs)\n        else:\n            return self._sign * self._score_func(y, y_pred, **self._kwargs)\n\n    def _factory_args(self):\n        return \", needs_threshold=True\"\n\n\ndef get_scorer(scoring):\n    if isinstance(scoring, six.string_types):\n        try:\n            scorer = SCORERS[scoring]\n        except KeyError:\n            scorers = [scorer for scorer in SCORERS\n                       if SCORERS[scorer]._deprecation_msg is None]\n            raise ValueError('%r is not a valid scoring value. '\n                             'Valid options are %s'\n                             % (scoring, sorted(scorers)))\n    else:\n        scorer = scoring\n    return scorer\n\n\ndef _passthrough_scorer(estimator, *args, **kwargs):\n    \"\"\"Function that wraps estimator.score\"\"\"\n    return estimator.score(*args, **kwargs)\n\n\ndef check_scoring(estimator, scoring=None, allow_none=False):\n    \"\"\"Determine scorer from user options.\n\n    A TypeError will be thrown if the estimator cannot be scored.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit'\n        The object to use to fit the data.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    allow_none : boolean, optional, default: False\n        If no scoring is specified and the estimator has no score function, we\n        can either return None or raise an exception.\n\n    Returns\n    -------\n    scoring : callable\n        A scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n    \"\"\"\n    has_scoring = scoring is not None\n    if not hasattr(estimator, 'fit'):\n        raise TypeError(\"estimator should be an estimator implementing \"\n                        \"'fit' method, %r was passed\" % estimator)\n    if isinstance(scoring, six.string_types):\n        return get_scorer(scoring)\n    elif has_scoring:\n        # Heuristic to ensure user has not passed a metric\n        module = getattr(scoring, '__module__', None)\n        if hasattr(module, 'startswith') and \\\n           module.startswith('sklearn.metrics.') and \\\n           not module.startswith('sklearn.metrics.scorer') and \\\n           not module.startswith('sklearn.metrics.tests.'):\n            raise ValueError('scoring value %r looks like it is a metric '\n                             'function rather than a scorer. A scorer should '\n                             'require an estimator as its first parameter. '\n                             'Please use `make_scorer` to convert a metric '\n                             'to a scorer.' % scoring)\n        return get_scorer(scoring)\n    elif hasattr(estimator, 'score'):\n        return _passthrough_scorer\n    elif allow_none:\n        return None\n    else:\n        raise TypeError(\n            \"If no scoring is specified, the estimator passed should \"\n            \"have a 'score' method. The estimator %r does not.\" % estimator)\n\n\ndef make_scorer(score_func, greater_is_better=True, needs_proba=False,\n                needs_threshold=False, **kwargs):\n    \"\"\"Make a scorer from a performance metric or loss function.\n\n    This factory function wraps scoring functions for use in GridSearchCV\n    and cross_val_score. It takes a score function, such as ``accuracy_score``,\n    ``mean_squared_error``, ``adjusted_rand_index`` or ``average_precision``\n    and returns a callable that scores an estimator's output.\n\n    Read more in the :ref:`User Guide <scoring>`.\n\n    Parameters\n    ----------\n    score_func : callable,\n        Score function (or loss function) with signature\n        ``score_func(y, y_pred, **kwargs)``.\n\n    greater_is_better : boolean, default=True\n        Whether score_func is a score function (default), meaning high is good,\n        or a loss function, meaning low is good. In the latter case, the\n        scorer object will sign-flip the outcome of the score_func.\n\n    needs_proba : boolean, default=False\n        Whether score_func requires predict_proba to get probability estimates\n        out of a classifier.\n\n    needs_threshold : boolean, default=False\n        Whether score_func takes a continuous decision certainty.\n        This only works for binary classification using estimators that\n        have either a decision_function or predict_proba method.\n\n        For example ``average_precision`` or the area under the roc curve\n        can not be computed using discrete predictions alone.\n\n    **kwargs : additional arguments\n        Additional parameters to be passed to score_func.\n\n    Returns\n    -------\n    scorer : callable\n        Callable object that returns a scalar score; greater is better.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import fbeta_score, make_scorer\n    >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n    >>> ftwo_scorer\n    make_scorer(fbeta_score, beta=2)\n    >>> from sklearn.model_selection import GridSearchCV\n    >>> from sklearn.svm import LinearSVC\n    >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n    ...                     scoring=ftwo_scorer)\n    \"\"\"\n    sign = 1 if greater_is_better else -1\n    if needs_proba and needs_threshold:\n        raise ValueError(\"Set either needs_proba or needs_threshold to True,\"\n                         \" but not both.\")\n    if needs_proba:\n        cls = _ProbaScorer\n    elif needs_threshold:\n        cls = _ThresholdScorer\n    else:\n        cls = _PredictScorer\n    return cls(score_func, sign, kwargs)\n\n\n# Standard regression scores\nr2_scorer = make_scorer(r2_score)\nneg_mean_squared_error_scorer = make_scorer(mean_squared_error,\n                                            greater_is_better=False)\ndeprecation_msg = ('Scoring method mean_squared_error was renamed to '\n                   'neg_mean_squared_error in version 0.18 and will '\n                   'be removed in 0.20.')\nmean_squared_error_scorer = make_scorer(mean_squared_error,\n                                        greater_is_better=False)\nneg_root_mean_squared_error_scorer = make_scorer(root_mean_squared_error,\n                                                 greater_is_better=False)\nmean_squared_error_scorer._deprecation_msg = deprecation_msg\nneg_mean_absolute_error_scorer = make_scorer(mean_absolute_error,\n                                             greater_is_better=False)\ndeprecation_msg = ('Scoring method mean_absolute_error was renamed to '\n                   'neg_mean_absolute_error in version 0.18 and will '\n                   'be removed in 0.20.')\nmean_absolute_error_scorer = make_scorer(mean_absolute_error,\n                                         greater_is_better=False)\nmean_absolute_error_scorer._deprecation_msg = deprecation_msg\nneg_median_absolute_error_scorer = make_scorer(median_absolute_error,\n                                               greater_is_better=False)\ndeprecation_msg = ('Scoring method median_absolute_error was renamed to '\n                   'neg_median_absolute_error in version 0.18 and will '\n                   'be removed in 0.20.')\nmedian_absolute_error_scorer = make_scorer(median_absolute_error,\n                                           greater_is_better=False)\nmedian_absolute_error_scorer._deprecation_msg = deprecation_msg\n\n\n# Standard Classification Scores\naccuracy_scorer = make_scorer(accuracy_score)\nf1_scorer = make_scorer(f1_score)\n\n# Score functions that need decision values\nroc_auc_scorer = make_scorer(roc_auc_score, greater_is_better=True,\n                             needs_threshold=True)\naverage_precision_scorer = make_scorer(average_precision_score,\n                                       needs_threshold=True)\nprecision_scorer = make_scorer(precision_score)\nrecall_scorer = make_scorer(recall_score)\n\n# Score function for probabilistic classification\nneg_log_loss_scorer = make_scorer(log_loss, greater_is_better=False,\n                                  needs_proba=True)\ndeprecation_msg = ('Scoring method log_loss was renamed to '\n                   'neg_log_loss in version 0.18 and will be removed in 0.20.')\nlog_loss_scorer = make_scorer(log_loss, greater_is_better=False,\n                              needs_proba=True)\nlog_loss_scorer._deprecation_msg = deprecation_msg\n\n\n# Clustering scores\nadjusted_rand_scorer = make_scorer(adjusted_rand_score)\n\nSCORERS = dict(r2=r2_scorer,\n               neg_median_absolute_error=neg_median_absolute_error_scorer,\n               neg_mean_absolute_error=neg_mean_absolute_error_scorer,\n               neg_mean_squared_error=neg_mean_squared_error_scorer,\n               neg_rmse=neg_root_mean_squared_error_scorer,\n               median_absolute_error=median_absolute_error_scorer,\n               mean_absolute_error=mean_absolute_error_scorer,\n               mean_squared_error=mean_squared_error_scorer,\n               accuracy=accuracy_scorer, roc_auc=roc_auc_scorer,\n               average_precision=average_precision_scorer,\n               log_loss=log_loss_scorer,\n               neg_log_loss=neg_log_loss_scorer,\n               adjusted_rand_score=adjusted_rand_scorer)\n\nfor name, metric in [('precision', precision_score),\n                     ('recall', recall_score), ('f1', f1_score)]:\n    SCORERS[name] = make_scorer(metric)\n    for average in ['macro', 'micro', 'samples', 'weighted']:\n        qualified_name = '{0}_{1}'.format(name, average)\n        SCORERS[qualified_name] = make_scorer(metric, pos_label=None,\n                                              average=average)\n"
    },
    {
      "filename": "sklearn/metrics/tests/test_score_objects.py",
      "content": "import pickle\nimport tempfile\nimport shutil\nimport os\nimport numbers\n\nimport numpy as np\n\nfrom sklearn.utils.testing import assert_almost_equal\nfrom sklearn.utils.testing import assert_array_equal\nfrom sklearn.utils.testing import assert_raises\nfrom sklearn.utils.testing import assert_raises_regexp\nfrom sklearn.utils.testing import assert_true\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.utils.testing import assert_not_equal\nfrom sklearn.utils.testing import assert_warns_message\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.metrics import (f1_score, r2_score, roc_auc_score, fbeta_score,\n                             log_loss, precision_score, recall_score)\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom sklearn.metrics.scorer import (check_scoring, _PredictScorer,\n                                    _passthrough_scorer)\nfrom sklearn.metrics import make_scorer, get_scorer, SCORERS\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.cluster import KMeans\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import Ridge, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import make_multilabel_classification\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.externals import joblib\n\n\nREGRESSION_SCORERS = ['r2', 'neg_mean_absolute_error',\n                      'neg_mean_squared_error', 'neg_median_absolute_error',\n                      'neg_rmse', 'mean_absolute_error',\n                      'mean_squared_error', 'median_absolute_error']\n\nCLF_SCORERS = ['accuracy', 'f1', 'f1_weighted', 'f1_macro', 'f1_micro',\n               'roc_auc', 'average_precision', 'precision',\n               'precision_weighted', 'precision_macro', 'precision_micro',\n               'recall', 'recall_weighted', 'recall_macro', 'recall_micro',\n               'neg_log_loss', 'log_loss',\n               'adjusted_rand_score'  # not really, but works\n               ]\n\nMULTILABEL_ONLY_SCORERS = ['precision_samples', 'recall_samples', 'f1_samples']\n\n\ndef _make_estimators(X_train, y_train, y_ml_train):\n    # Make estimators that make sense to test various scoring methods\n    sensible_regr = DummyRegressor(strategy='median')\n    sensible_regr.fit(X_train, y_train)\n    sensible_clf = DecisionTreeClassifier(random_state=0)\n    sensible_clf.fit(X_train, y_train)\n    sensible_ml_clf = DecisionTreeClassifier(random_state=0)\n    sensible_ml_clf.fit(X_train, y_ml_train)\n    return dict(\n        [(name, sensible_regr) for name in REGRESSION_SCORERS] +\n        [(name, sensible_clf) for name in CLF_SCORERS] +\n        [(name, sensible_ml_clf) for name in MULTILABEL_ONLY_SCORERS]\n    )\n\n\nX_mm, y_mm, y_ml_mm = None, None, None\nESTIMATORS = None\nTEMP_FOLDER = None\n\n\ndef setup_module():\n    # Create some memory mapped data\n    global X_mm, y_mm, y_ml_mm, TEMP_FOLDER, ESTIMATORS\n    TEMP_FOLDER = tempfile.mkdtemp(prefix='sklearn_test_score_objects_')\n    X, y = make_classification(n_samples=30, n_features=5, random_state=0)\n    _, y_ml = make_multilabel_classification(n_samples=X.shape[0],\n                                             random_state=0)\n    filename = os.path.join(TEMP_FOLDER, 'test_data.pkl')\n    joblib.dump((X, y, y_ml), filename)\n    X_mm, y_mm, y_ml_mm = joblib.load(filename, mmap_mode='r')\n    ESTIMATORS = _make_estimators(X_mm, y_mm, y_ml_mm)\n\n\ndef teardown_module():\n    global X_mm, y_mm, y_ml_mm, TEMP_FOLDER, ESTIMATORS\n    # GC closes the mmap file descriptors\n    X_mm, y_mm, y_ml_mm, ESTIMATORS = None, None, None, None\n    shutil.rmtree(TEMP_FOLDER)\n\n\nclass EstimatorWithoutFit(object):\n    \"\"\"Dummy estimator to test check_scoring\"\"\"\n    pass\n\n\nclass EstimatorWithFit(BaseEstimator):\n    \"\"\"Dummy estimator to test check_scoring\"\"\"\n    def fit(self, X, y):\n        return self\n\n\nclass EstimatorWithFitAndScore(object):\n    \"\"\"Dummy estimator to test check_scoring\"\"\"\n    def fit(self, X, y):\n        return self\n\n    def score(self, X, y):\n        return 1.0\n\n\nclass EstimatorWithFitAndPredict(object):\n    \"\"\"Dummy estimator to test check_scoring\"\"\"\n    def fit(self, X, y):\n        self.y = y\n        return self\n\n    def predict(self, X):\n        return self.y\n\n\nclass DummyScorer(object):\n    \"\"\"Dummy scorer that always returns 1.\"\"\"\n    def __call__(self, est, X, y):\n        return 1\n\n\ndef test_all_scorers_repr():\n    # Test that all scorers have a working repr\n    for name, scorer in SCORERS.items():\n        repr(scorer)\n\n\ndef test_check_scoring():\n    # Test all branches of check_scoring\n    estimator = EstimatorWithoutFit()\n    pattern = (r\"estimator should be an estimator implementing 'fit' method,\"\n               r\" .* was passed\")\n    assert_raises_regexp(TypeError, pattern, check_scoring, estimator)\n\n    estimator = EstimatorWithFitAndScore()\n    estimator.fit([[1]], [1])\n    scorer = check_scoring(estimator)\n    assert_true(scorer is _passthrough_scorer)\n    assert_almost_equal(scorer(estimator, [[1]], [1]), 1.0)\n\n    estimator = EstimatorWithFitAndPredict()\n    estimator.fit([[1]], [1])\n    pattern = (r\"If no scoring is specified, the estimator passed should have\"\n               r\" a 'score' method\\. The estimator .* does not\\.\")\n    assert_raises_regexp(TypeError, pattern, check_scoring, estimator)\n\n    scorer = check_scoring(estimator, \"accuracy\")\n    assert_almost_equal(scorer(estimator, [[1]], [1]), 1.0)\n\n    estimator = EstimatorWithFit()\n    scorer = check_scoring(estimator, \"accuracy\")\n    assert_true(isinstance(scorer, _PredictScorer))\n\n    estimator = EstimatorWithFit()\n    scorer = check_scoring(estimator, allow_none=True)\n    assert_true(scorer is None)\n\n\ndef test_check_scoring_gridsearchcv():\n    # test that check_scoring works on GridSearchCV and pipeline.\n    # slightly redundant non-regression test.\n\n    grid = GridSearchCV(LinearSVC(), param_grid={'C': [.1, 1]})\n    scorer = check_scoring(grid, \"f1\")\n    assert_true(isinstance(scorer, _PredictScorer))\n\n    pipe = make_pipeline(LinearSVC())\n    scorer = check_scoring(pipe, \"f1\")\n    assert_true(isinstance(scorer, _PredictScorer))\n\n    # check that cross_val_score definitely calls the scorer\n    # and doesn't make any assumptions about the estimator apart from having a\n    # fit.\n    scores = cross_val_score(EstimatorWithFit(), [[1], [2], [3]], [1, 0, 1],\n                             scoring=DummyScorer())\n    assert_array_equal(scores, 1)\n\n\ndef test_make_scorer():\n    # Sanity check on the make_scorer factory function.\n    f = lambda *args: 0\n    assert_raises(ValueError, make_scorer, f, needs_threshold=True,\n                  needs_proba=True)\n\n\ndef test_classification_scores():\n    # Test classification scorers.\n    X, y = make_blobs(random_state=0, centers=2)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    clf = LinearSVC(random_state=0)\n    clf.fit(X_train, y_train)\n\n    for prefix, metric in [('f1', f1_score), ('precision', precision_score),\n                           ('recall', recall_score)]:\n\n        score1 = get_scorer('%s_weighted' % prefix)(clf, X_test, y_test)\n        score2 = metric(y_test, clf.predict(X_test), pos_label=None,\n                        average='weighted')\n        assert_almost_equal(score1, score2)\n\n        score1 = get_scorer('%s_macro' % prefix)(clf, X_test, y_test)\n        score2 = metric(y_test, clf.predict(X_test), pos_label=None,\n                        average='macro')\n        assert_almost_equal(score1, score2)\n\n        score1 = get_scorer('%s_micro' % prefix)(clf, X_test, y_test)\n        score2 = metric(y_test, clf.predict(X_test), pos_label=None,\n                        average='micro')\n        assert_almost_equal(score1, score2)\n\n        score1 = get_scorer('%s' % prefix)(clf, X_test, y_test)\n        score2 = metric(y_test, clf.predict(X_test), pos_label=1)\n        assert_almost_equal(score1, score2)\n\n    # test fbeta score that takes an argument\n    scorer = make_scorer(fbeta_score, beta=2)\n    score1 = scorer(clf, X_test, y_test)\n    score2 = fbeta_score(y_test, clf.predict(X_test), beta=2)\n    assert_almost_equal(score1, score2)\n\n    # test that custom scorer can be pickled\n    unpickled_scorer = pickle.loads(pickle.dumps(scorer))\n    score3 = unpickled_scorer(clf, X_test, y_test)\n    assert_almost_equal(score1, score3)\n\n    # smoke test the repr:\n    repr(fbeta_score)\n\n\ndef test_regression_scorers():\n    # Test regression scorers.\n    diabetes = load_diabetes()\n    X, y = diabetes.data, diabetes.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    clf = Ridge()\n    clf.fit(X_train, y_train)\n    score1 = get_scorer('r2')(clf, X_test, y_test)\n    score2 = r2_score(y_test, clf.predict(X_test))\n    assert_almost_equal(score1, score2)\n\n\ndef test_thresholded_scorers():\n    # Test scorers that take thresholds.\n    X, y = make_blobs(random_state=0, centers=2)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_train, y_train)\n    score1 = get_scorer('roc_auc')(clf, X_test, y_test)\n    score2 = roc_auc_score(y_test, clf.decision_function(X_test))\n    score3 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n    assert_almost_equal(score1, score2)\n    assert_almost_equal(score1, score3)\n\n    logscore = get_scorer('neg_log_loss')(clf, X_test, y_test)\n    logloss = log_loss(y_test, clf.predict_proba(X_test))\n    assert_almost_equal(-logscore, logloss)\n\n    # same for an estimator without decision_function\n    clf = DecisionTreeClassifier()\n    clf.fit(X_train, y_train)\n    score1 = get_scorer('roc_auc')(clf, X_test, y_test)\n    score2 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n    assert_almost_equal(score1, score2)\n\n    # test with a regressor (no decision_function)\n    reg = DecisionTreeRegressor()\n    reg.fit(X_train, y_train)\n    score1 = get_scorer('roc_auc')(reg, X_test, y_test)\n    score2 = roc_auc_score(y_test, reg.predict(X_test))\n    assert_almost_equal(score1, score2)\n\n    # Test that an exception is raised on more than two classes\n    X, y = make_blobs(random_state=0, centers=3)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    clf.fit(X_train, y_train)\n    assert_raises(ValueError, get_scorer('roc_auc'), clf, X_test, y_test)\n\n\ndef test_thresholded_scorers_multilabel_indicator_data():\n    # Test that the scorer work with multilabel-indicator format\n    # for multilabel and multi-output multi-class classifier\n    X, y = make_multilabel_classification(allow_unlabeled=False,\n                                          random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    # Multi-output multi-class predict_proba\n    clf = DecisionTreeClassifier()\n    clf.fit(X_train, y_train)\n    y_proba = clf.predict_proba(X_test)\n    score1 = get_scorer('roc_auc')(clf, X_test, y_test)\n    score2 = roc_auc_score(y_test, np.vstack(p[:, -1] for p in y_proba).T)\n    assert_almost_equal(score1, score2)\n\n    # Multi-output multi-class decision_function\n    # TODO Is there any yet?\n    clf = DecisionTreeClassifier()\n    clf.fit(X_train, y_train)\n    clf._predict_proba = clf.predict_proba\n    clf.predict_proba = None\n    clf.decision_function = lambda X: [p[:, 1] for p in clf._predict_proba(X)]\n\n    y_proba = clf.decision_function(X_test)\n    score1 = get_scorer('roc_auc')(clf, X_test, y_test)\n    score2 = roc_auc_score(y_test, np.vstack(p for p in y_proba).T)\n    assert_almost_equal(score1, score2)\n\n    # Multilabel predict_proba\n    clf = OneVsRestClassifier(DecisionTreeClassifier())\n    clf.fit(X_train, y_train)\n    score1 = get_scorer('roc_auc')(clf, X_test, y_test)\n    score2 = roc_auc_score(y_test, clf.predict_proba(X_test))\n    assert_almost_equal(score1, score2)\n\n    # Multilabel decision function\n    clf = OneVsRestClassifier(LinearSVC(random_state=0))\n    clf.fit(X_train, y_train)\n    score1 = get_scorer('roc_auc')(clf, X_test, y_test)\n    score2 = roc_auc_score(y_test, clf.decision_function(X_test))\n    assert_almost_equal(score1, score2)\n\n\ndef test_unsupervised_scorers():\n    # Test clustering scorers against gold standard labeling.\n    # We don't have any real unsupervised Scorers yet.\n    X, y = make_blobs(random_state=0, centers=2)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    km = KMeans(n_clusters=3)\n    km.fit(X_train)\n    score1 = get_scorer('adjusted_rand_score')(km, X_test, y_test)\n    score2 = adjusted_rand_score(y_test, km.predict(X_test))\n    assert_almost_equal(score1, score2)\n\n\n@ignore_warnings\ndef test_raises_on_score_list():\n    # Test that when a list of scores is returned, we raise proper errors.\n    X, y = make_blobs(random_state=0)\n    f1_scorer_no_average = make_scorer(f1_score, average=None)\n    clf = DecisionTreeClassifier()\n    assert_raises(ValueError, cross_val_score, clf, X, y,\n                  scoring=f1_scorer_no_average)\n    grid_search = GridSearchCV(clf, scoring=f1_scorer_no_average,\n                               param_grid={'max_depth': [1, 2]})\n    assert_raises(ValueError, grid_search.fit, X, y)\n\n\n@ignore_warnings\ndef test_scorer_sample_weight():\n    # Test that scorers support sample_weight or raise sensible errors\n\n    # Unlike the metrics invariance test, in the scorer case it's harder\n    # to ensure that, on the classifier output, weighted and unweighted\n    # scores really should be unequal.\n    X, y = make_classification(random_state=0)\n    _, y_ml = make_multilabel_classification(n_samples=X.shape[0],\n                                             random_state=0)\n    split = train_test_split(X, y, y_ml, random_state=0)\n    X_train, X_test, y_train, y_test, y_ml_train, y_ml_test = split\n\n    sample_weight = np.ones_like(y_test)\n    sample_weight[:10] = 0\n\n    # get sensible estimators for each metric\n    estimator = _make_estimators(X_train, y_train, y_ml_train)\n\n    for name, scorer in SCORERS.items():\n        if name in MULTILABEL_ONLY_SCORERS:\n            target = y_ml_test\n        else:\n            target = y_test\n        try:\n            weighted = scorer(estimator[name], X_test, target,\n                              sample_weight=sample_weight)\n            ignored = scorer(estimator[name], X_test[10:], target[10:])\n            unweighted = scorer(estimator[name], X_test, target)\n            assert_not_equal(weighted, unweighted,\n                             msg=\"scorer {0} behaves identically when \"\n                             \"called with sample weights: {1} vs \"\n                             \"{2}\".format(name, weighted, unweighted))\n            assert_almost_equal(weighted, ignored,\n                                err_msg=\"scorer {0} behaves differently when \"\n                                \"ignoring samples and setting sample_weight to\"\n                                \" 0: {1} vs {2}\".format(name, weighted,\n                                                        ignored))\n\n        except TypeError as e:\n            assert_true(\"sample_weight\" in str(e),\n                        \"scorer {0} raises unhelpful exception when called \"\n                        \"with sample weights: {1}\".format(name, str(e)))\n\n\n@ignore_warnings  # UndefinedMetricWarning for P / R scores\ndef check_scorer_memmap(scorer_name):\n    scorer, estimator = SCORERS[scorer_name], ESTIMATORS[scorer_name]\n    if scorer_name in MULTILABEL_ONLY_SCORERS:\n        score = scorer(estimator, X_mm, y_ml_mm)\n    else:\n        score = scorer(estimator, X_mm, y_mm)\n    assert isinstance(score, numbers.Number), scorer_name\n\n\ndef test_scorer_memmap_input():\n    # Non-regression test for #6147: some score functions would\n    # return singleton memmap when computed on memmap data instead of scalar\n    # float values.\n    for name in SCORERS.keys():\n        yield check_scorer_memmap, name\n\n\ndef test_deprecated_names():\n    X, y = make_blobs(random_state=0, centers=2)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_train, y_train)\n\n    for name in ('mean_absolute_error', 'mean_squared_error',\n                 'median_absolute_error', 'log_loss'):\n        warning_msg = \"Scoring method %s was renamed to\" % name\n        for scorer in (get_scorer(name), SCORERS[name]):\n            assert_warns_message(DeprecationWarning,\n                                 warning_msg,\n                                 scorer, clf, X, y)\n\n        assert_warns_message(DeprecationWarning,\n                             warning_msg,\n                             cross_val_score, clf, X, y, scoring=name)\n\n\ndef test_scoring_is_not_metric():\n    assert_raises_regexp(ValueError, 'make_scorer', check_scoring,\n                         LogisticRegression(), f1_score)\n    assert_raises_regexp(ValueError, 'make_scorer', check_scoring,\n                         LogisticRegression(), roc_auc_score)\n    assert_raises_regexp(ValueError, 'make_scorer', check_scoring,\n                         Ridge(), r2_score)\n    assert_raises_regexp(ValueError, 'make_scorer', check_scoring,\n                         KMeans(), adjusted_rand_score)\n"
    },
    {
      "filename": "sklearn/tests/test_cross_validation.py",
      "content": "\"\"\"Test the cross_validation module\"\"\"\nfrom __future__ import division\nimport warnings\n\nimport numpy as np\nfrom scipy.sparse import coo_matrix\nfrom scipy.sparse import csr_matrix\nfrom scipy import stats\n\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.utils.testing import assert_true\nfrom sklearn.utils.testing import assert_false\nfrom sklearn.utils.testing import assert_equal\nfrom sklearn.utils.testing import assert_almost_equal\nfrom sklearn.utils.testing import assert_raises\nfrom sklearn.utils.testing import assert_greater\nfrom sklearn.utils.testing import assert_greater_equal\nfrom sklearn.utils.testing import assert_less\nfrom sklearn.utils.testing import assert_not_equal\nfrom sklearn.utils.testing import assert_array_almost_equal\nfrom sklearn.utils.testing import assert_array_equal\nfrom sklearn.utils.testing import assert_warns_message\nfrom sklearn.utils.testing import assert_raise_message\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.utils.mocking import CheckingClassifier, MockDataFrame\n\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    from sklearn import cross_validation as cval\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.datasets import load_boston\nfrom sklearn.datasets import load_digits\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import make_multilabel_classification\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import precision_score\nfrom sklearn.externals import six\nfrom sklearn.externals.six.moves import zip\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\n\n\nclass MockClassifier(object):\n    \"\"\"Dummy classifier to test the cross-validation\"\"\"\n\n    def __init__(self, a=0, allow_nd=False):\n        self.a = a\n        self.allow_nd = allow_nd\n\n    def fit(self, X, Y=None, sample_weight=None, class_prior=None,\n            sparse_sample_weight=None, sparse_param=None, dummy_int=None,\n            dummy_str=None, dummy_obj=None, callback=None):\n        \"\"\"The dummy arguments are to test that this fit function can\n        accept non-array arguments through cross-validation, such as:\n            - int\n            - str (this is actually array-like)\n            - object\n            - function\n        \"\"\"\n        self.dummy_int = dummy_int\n        self.dummy_str = dummy_str\n        self.dummy_obj = dummy_obj\n        if callback is not None:\n            callback(self)\n\n        if self.allow_nd:\n            X = X.reshape(len(X), -1)\n        if X.ndim >= 3 and not self.allow_nd:\n            raise ValueError('X cannot be d')\n        if sample_weight is not None:\n            assert_true(sample_weight.shape[0] == X.shape[0],\n                        'MockClassifier extra fit_param sample_weight.shape[0]'\n                        ' is {0}, should be {1}'.format(sample_weight.shape[0],\n                                                        X.shape[0]))\n        if class_prior is not None:\n            assert_true(class_prior.shape[0] == len(np.unique(y)),\n                        'MockClassifier extra fit_param class_prior.shape[0]'\n                        ' is {0}, should be {1}'.format(class_prior.shape[0],\n                                                        len(np.unique(y))))\n        if sparse_sample_weight is not None:\n            fmt = ('MockClassifier extra fit_param sparse_sample_weight'\n                   '.shape[0] is {0}, should be {1}')\n            assert_true(sparse_sample_weight.shape[0] == X.shape[0],\n                        fmt.format(sparse_sample_weight.shape[0], X.shape[0]))\n        if sparse_param is not None:\n            fmt = ('MockClassifier extra fit_param sparse_param.shape '\n                   'is ({0}, {1}), should be ({2}, {3})')\n            assert_true(sparse_param.shape == P_sparse.shape,\n                        fmt.format(sparse_param.shape[0],\n                                   sparse_param.shape[1],\n                                   P_sparse.shape[0], P_sparse.shape[1]))\n        return self\n\n    def predict(self, T):\n        if self.allow_nd:\n            T = T.reshape(len(T), -1)\n        return T[:, 0]\n\n    def score(self, X=None, Y=None):\n        return 1. / (1 + np.abs(self.a))\n\n    def get_params(self, deep=False):\n        return {'a': self.a, 'allow_nd': self.allow_nd}\n\nX = np.ones((10, 2))\nX_sparse = coo_matrix(X)\nW_sparse = coo_matrix((np.array([1]), (np.array([1]), np.array([0]))),\n                      shape=(10, 1))\nP_sparse = coo_matrix(np.eye(5))\n\n# avoid StratifiedKFold's Warning about least populated class in y\ny = np.arange(10) % 3\n\n##############################################################################\n# Tests\n\n\ndef check_valid_split(train, test, n_samples=None):\n    # Use python sets to get more informative assertion failure messages\n    train, test = set(train), set(test)\n\n    # Train and test split should not overlap\n    assert_equal(train.intersection(test), set())\n\n    if n_samples is not None:\n        # Check that the union of train an test split cover all the indices\n        assert_equal(train.union(test), set(range(n_samples)))\n\n\ndef check_cv_coverage(cv, expected_n_iter=None, n_samples=None):\n    # Check that a all the samples appear at least once in a test fold\n    if expected_n_iter is not None:\n        assert_equal(len(cv), expected_n_iter)\n    else:\n        expected_n_iter = len(cv)\n\n    collected_test_samples = set()\n    iterations = 0\n    for train, test in cv:\n        check_valid_split(train, test, n_samples=n_samples)\n        iterations += 1\n        collected_test_samples.update(test)\n\n    # Check that the accumulated test samples cover the whole dataset\n    assert_equal(iterations, expected_n_iter)\n    if n_samples is not None:\n        assert_equal(collected_test_samples, set(range(n_samples)))\n\n\ndef test_kfold_valueerrors():\n    # Check that errors are raised if there is not enough samples\n    assert_raises(ValueError, cval.KFold, 3, 4)\n\n    # Check that a warning is raised if the least populated class has too few\n    # members.\n    y = [3, 3, -1, -1, 3]\n\n    cv = assert_warns_message(Warning, \"The least populated class\",\n                              cval.StratifiedKFold, y, 3)\n\n    # Check that despite the warning the folds are still computed even\n    # though all the classes are not necessarily represented at on each\n    # side of the split at each split\n    check_cv_coverage(cv, expected_n_iter=3, n_samples=len(y))\n\n    # Check that errors are raised if all n_labels for individual\n    # classes are less than n_folds.\n    y = [3, 3, -1, -1, 2]\n\n    assert_raises(ValueError, cval.StratifiedKFold, y, 3)\n\n    # Error when number of folds is <= 1\n    assert_raises(ValueError, cval.KFold, 2, 0)\n    assert_raises(ValueError, cval.KFold, 2, 1)\n    error_string = (\"k-fold cross validation requires at least one\"\n                    \" train / test split\")\n    assert_raise_message(ValueError, error_string,\n                         cval.StratifiedKFold, y, 0)\n    assert_raise_message(ValueError, error_string,\n                         cval.StratifiedKFold, y, 1)\n\n    # When n is not integer:\n    assert_raises(ValueError, cval.KFold, 2.5, 2)\n\n    # When n_folds is not integer:\n    assert_raises(ValueError, cval.KFold, 5, 1.5)\n    assert_raises(ValueError, cval.StratifiedKFold, y, 1.5)\n\n\ndef test_kfold_indices():\n    # Check all indices are returned in the test folds\n    kf = cval.KFold(300, 3)\n    check_cv_coverage(kf, expected_n_iter=3, n_samples=300)\n\n    # Check all indices are returned in the test folds even when equal-sized\n    # folds are not possible\n    kf = cval.KFold(17, 3)\n    check_cv_coverage(kf, expected_n_iter=3, n_samples=17)\n\n\ndef test_kfold_no_shuffle():\n    # Manually check that KFold preserves the data ordering on toy datasets\n    splits = iter(cval.KFold(4, 2))\n    train, test = next(splits)\n    assert_array_equal(test, [0, 1])\n    assert_array_equal(train, [2, 3])\n\n    train, test = next(splits)\n    assert_array_equal(test, [2, 3])\n    assert_array_equal(train, [0, 1])\n\n    splits = iter(cval.KFold(5, 2))\n    train, test = next(splits)\n    assert_array_equal(test, [0, 1, 2])\n    assert_array_equal(train, [3, 4])\n\n    train, test = next(splits)\n    assert_array_equal(test, [3, 4])\n    assert_array_equal(train, [0, 1, 2])\n\n\ndef test_stratified_kfold_no_shuffle():\n    # Manually check that StratifiedKFold preserves the data ordering as much\n    # as possible on toy datasets in order to avoid hiding sample dependencies\n    # when possible\n    splits = iter(cval.StratifiedKFold([1, 1, 0, 0], 2))\n    train, test = next(splits)\n    assert_array_equal(test, [0, 2])\n    assert_array_equal(train, [1, 3])\n\n    train, test = next(splits)\n    assert_array_equal(test, [1, 3])\n    assert_array_equal(train, [0, 2])\n\n    splits = iter(cval.StratifiedKFold([1, 1, 1, 0, 0, 0, 0], 2))\n    train, test = next(splits)\n    assert_array_equal(test, [0, 1, 3, 4])\n    assert_array_equal(train, [2, 5, 6])\n\n    train, test = next(splits)\n    assert_array_equal(test, [2, 5, 6])\n    assert_array_equal(train, [0, 1, 3, 4])\n\n\ndef test_stratified_kfold_ratios():\n    # Check that stratified kfold preserves label ratios in individual splits\n    # Repeat with shuffling turned off and on\n    n_samples = 1000\n    labels = np.array([4] * int(0.10 * n_samples) +\n                      [0] * int(0.89 * n_samples) +\n                      [1] * int(0.01 * n_samples))\n    for shuffle in [False, True]:\n        for train, test in cval.StratifiedKFold(labels, 5, shuffle=shuffle):\n            assert_almost_equal(np.sum(labels[train] == 4) / len(train), 0.10,\n                                2)\n            assert_almost_equal(np.sum(labels[train] == 0) / len(train), 0.89,\n                                2)\n            assert_almost_equal(np.sum(labels[train] == 1) / len(train), 0.01,\n                                2)\n            assert_almost_equal(np.sum(labels[test] == 4) / len(test), 0.10, 2)\n            assert_almost_equal(np.sum(labels[test] == 0) / len(test), 0.89, 2)\n            assert_almost_equal(np.sum(labels[test] == 1) / len(test), 0.01, 2)\n\n\ndef test_kfold_balance():\n    # Check that KFold returns folds with balanced sizes\n    for kf in [cval.KFold(i, 5) for i in range(11, 17)]:\n        sizes = []\n        for _, test in kf:\n            sizes.append(len(test))\n\n        assert_true((np.max(sizes) - np.min(sizes)) <= 1)\n        assert_equal(np.sum(sizes), kf.n)\n\n\ndef test_stratifiedkfold_balance():\n    # Check that KFold returns folds with balanced sizes (only when\n    # stratification is possible)\n    # Repeat with shuffling turned off and on\n    labels = [0] * 3 + [1] * 14\n    for shuffle in [False, True]:\n        for skf in [cval.StratifiedKFold(labels[:i], 3, shuffle=shuffle)\n                    for i in range(11, 17)]:\n            sizes = []\n            for _, test in skf:\n                sizes.append(len(test))\n\n            assert_true((np.max(sizes) - np.min(sizes)) <= 1)\n            assert_equal(np.sum(sizes), skf.n)\n\n\ndef test_shuffle_kfold():\n    # Check the indices are shuffled properly, and that all indices are\n    # returned in the different test folds\n    kf = cval.KFold(300, 3, shuffle=True, random_state=0)\n    ind = np.arange(300)\n\n    all_folds = None\n    for train, test in kf:\n        assert_true(np.any(np.arange(100) != ind[test]))\n        assert_true(np.any(np.arange(100, 200) != ind[test]))\n        assert_true(np.any(np.arange(200, 300) != ind[test]))\n\n        if all_folds is None:\n            all_folds = ind[test].copy()\n        else:\n            all_folds = np.concatenate((all_folds, ind[test]))\n\n    all_folds.sort()\n    assert_array_equal(all_folds, ind)\n\n\ndef test_shuffle_stratifiedkfold():\n    # Check that shuffling is happening when requested, and for proper\n    # sample coverage\n    labels = [0] * 20 + [1] * 20\n    kf0 = list(cval.StratifiedKFold(labels, 5, shuffle=True, random_state=0))\n    kf1 = list(cval.StratifiedKFold(labels, 5, shuffle=True, random_state=1))\n    for (_, test0), (_, test1) in zip(kf0, kf1):\n        assert_true(set(test0) != set(test1))\n    check_cv_coverage(kf0, expected_n_iter=5, n_samples=40)\n\n\ndef test_kfold_can_detect_dependent_samples_on_digits():  # see #2372\n    # The digits samples are dependent: they are apparently grouped by authors\n    # although we don't have any information on the groups segment locations\n    # for this data. We can highlight this fact be computing k-fold cross-\n    # validation with and without shuffling: we observe that the shuffling case\n    # wrongly makes the IID assumption and is therefore too optimistic: it\n    # estimates a much higher accuracy (around 0.96) than the non\n    # shuffling variant (around 0.86).\n\n    digits = load_digits()\n    X, y = digits.data[:800], digits.target[:800]\n    model = SVC(C=10, gamma=0.005)\n    n = len(y)\n\n    cv = cval.KFold(n, 5, shuffle=False)\n    mean_score = cval.cross_val_score(model, X, y, cv=cv).mean()\n    assert_greater(0.88, mean_score)\n    assert_greater(mean_score, 0.85)\n\n    # Shuffling the data artificially breaks the dependency and hides the\n    # overfitting of the model with regards to the writing style of the authors\n    # by yielding a seriously overestimated score:\n\n    cv = cval.KFold(n, 5, shuffle=True, random_state=0)\n    mean_score = cval.cross_val_score(model, X, y, cv=cv).mean()\n    assert_greater(mean_score, 0.95)\n\n    cv = cval.KFold(n, 5, shuffle=True, random_state=1)\n    mean_score = cval.cross_val_score(model, X, y, cv=cv).mean()\n    assert_greater(mean_score, 0.95)\n\n    # Similarly, StratifiedKFold should try to shuffle the data as little\n    # as possible (while respecting the balanced class constraints)\n    # and thus be able to detect the dependency by not overestimating\n    # the CV score either. As the digits dataset is approximately balanced\n    # the estimated mean score is close to the score measured with\n    # non-shuffled KFold\n\n    cv = cval.StratifiedKFold(y, 5)\n    mean_score = cval.cross_val_score(model, X, y, cv=cv).mean()\n    assert_greater(0.88, mean_score)\n    assert_greater(mean_score, 0.85)\n\n\ndef test_label_kfold():\n    rng = np.random.RandomState(0)\n\n    # Parameters of the test\n    n_labels = 15\n    n_samples = 1000\n    n_folds = 5\n\n    # Construct the test data\n    tolerance = 0.05 * n_samples  # 5 percent error allowed\n    labels = rng.randint(0, n_labels, n_samples)\n    folds = cval.LabelKFold(labels, n_folds=n_folds).idxs\n    ideal_n_labels_per_fold = n_samples // n_folds\n\n    # Check that folds have approximately the same size\n    assert_equal(len(folds), len(labels))\n    for i in np.unique(folds):\n        assert_greater_equal(tolerance,\n                             abs(sum(folds == i) - ideal_n_labels_per_fold))\n\n    # Check that each label appears only in 1 fold\n    for label in np.unique(labels):\n        assert_equal(len(np.unique(folds[labels == label])), 1)\n\n    # Check that no label is on both sides of the split\n    labels = np.asarray(labels, dtype=object)\n    for train, test in cval.LabelKFold(labels, n_folds=n_folds):\n        assert_equal(len(np.intersect1d(labels[train], labels[test])), 0)\n\n    # Construct the test data\n    labels = ['Albert', 'Jean', 'Bertrand', 'Michel', 'Jean',\n              'Francis', 'Robert', 'Michel', 'Rachel', 'Lois',\n              'Michelle', 'Bernard', 'Marion', 'Laura', 'Jean',\n              'Rachel', 'Franck', 'John', 'Gael', 'Anna', 'Alix',\n              'Robert', 'Marion', 'David', 'Tony', 'Abel', 'Becky',\n              'Madmood', 'Cary', 'Mary', 'Alexandre', 'David', 'Francis',\n              'Barack', 'Abdoul', 'Rasha', 'Xi', 'Silvia']\n    labels = np.asarray(labels, dtype=object)\n\n    n_labels = len(np.unique(labels))\n    n_samples = len(labels)\n    n_folds = 5\n    tolerance = 0.05 * n_samples  # 5 percent error allowed\n    folds = cval.LabelKFold(labels, n_folds=n_folds).idxs\n    ideal_n_labels_per_fold = n_samples // n_folds\n\n    # Check that folds have approximately the same size\n    assert_equal(len(folds), len(labels))\n    for i in np.unique(folds):\n        assert_greater_equal(tolerance,\n                             abs(sum(folds == i) - ideal_n_labels_per_fold))\n\n    # Check that each label appears only in 1 fold\n    for label in np.unique(labels):\n        assert_equal(len(np.unique(folds[labels == label])), 1)\n\n    # Check that no label is on both sides of the split\n    for train, test in cval.LabelKFold(labels, n_folds=n_folds):\n        assert_equal(len(np.intersect1d(labels[train], labels[test])), 0)\n\n    # Should fail if there are more folds than labels\n    labels = np.array([1, 1, 1, 2, 2])\n    assert_raises(ValueError, cval.LabelKFold, labels, n_folds=3)\n\n\ndef test_shuffle_split():\n    ss1 = cval.ShuffleSplit(10, test_size=0.2, random_state=0)\n    ss2 = cval.ShuffleSplit(10, test_size=2, random_state=0)\n    ss3 = cval.ShuffleSplit(10, test_size=np.int32(2), random_state=0)\n    for typ in six.integer_types:\n        ss4 = cval.ShuffleSplit(10, test_size=typ(2), random_state=0)\n    for t1, t2, t3, t4 in zip(ss1, ss2, ss3, ss4):\n        assert_array_equal(t1[0], t2[0])\n        assert_array_equal(t2[0], t3[0])\n        assert_array_equal(t3[0], t4[0])\n        assert_array_equal(t1[1], t2[1])\n        assert_array_equal(t2[1], t3[1])\n        assert_array_equal(t3[1], t4[1])\n\n\ndef test_stratified_shuffle_split_init():\n    y = np.asarray([0, 1, 1, 1, 2, 2, 2])\n    # Check that error is raised if there is a class with only one sample\n    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 0.2)\n\n    # Check that error is raised if the test set size is smaller than n_classes\n    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 2)\n    # Check that error is raised if the train set size is smaller than\n    # n_classes\n    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 3, 2)\n\n    y = np.asarray([0, 0, 0, 1, 1, 1, 2, 2, 2])\n    # Check that errors are raised if there is not enough samples\n    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 0.5, 0.6)\n    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 8, 0.6)\n    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 0.6, 8)\n\n    # Train size or test size too small\n    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, train_size=2)\n    assert_raises(ValueError, cval.StratifiedShuffleSplit, y, test_size=2)\n\n\ndef test_stratified_shuffle_split_iter():\n    ys = [np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]),\n          np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]),\n          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2),\n          np.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]),\n          np.array([-1] * 800 + [1] * 50)\n          ]\n\n    for y in ys:\n        sss = cval.StratifiedShuffleSplit(y, 6, test_size=0.33,\n                                          random_state=0)\n        test_size = np.ceil(0.33 * len(y))\n        train_size = len(y) - test_size\n        for train, test in sss:\n            assert_array_equal(np.unique(y[train]), np.unique(y[test]))\n            # Checks if folds keep classes proportions\n            p_train = (np.bincount(np.unique(y[train],\n                                   return_inverse=True)[1]) /\n                       float(len(y[train])))\n            p_test = (np.bincount(np.unique(y[test],\n                                  return_inverse=True)[1]) /\n                      float(len(y[test])))\n            assert_array_almost_equal(p_train, p_test, 1)\n            assert_equal(len(train) + len(test), y.size)\n            assert_equal(len(train), train_size)\n            assert_equal(len(test), test_size)\n            assert_array_equal(np.lib.arraysetops.intersect1d(train, test), [])\n\n\ndef test_stratified_shuffle_split_even():\n    # Test the StratifiedShuffleSplit, indices are drawn with a\n    # equal chance\n    n_folds = 5\n    n_iter = 1000\n\n    def assert_counts_are_ok(idx_counts, p):\n        # Here we test that the distribution of the counts\n        # per index is close enough to a binomial\n        threshold = 0.05 / n_splits\n        bf = stats.binom(n_splits, p)\n        for count in idx_counts:\n            p = bf.pmf(count)\n            assert_true(p > threshold,\n                        \"An index is not drawn with chance corresponding \"\n                        \"to even draws\")\n\n    for n_samples in (6, 22):\n        labels = np.array((n_samples // 2) * [0, 1])\n        splits = cval.StratifiedShuffleSplit(labels, n_iter=n_iter,\n                                             test_size=1. / n_folds,\n                                             random_state=0)\n\n        train_counts = [0] * n_samples\n        test_counts = [0] * n_samples\n        n_splits = 0\n        for train, test in splits:\n            n_splits += 1\n            for counter, ids in [(train_counts, train), (test_counts, test)]:\n                for id in ids:\n                    counter[id] += 1\n        assert_equal(n_splits, n_iter)\n\n        assert_equal(len(train), splits.n_train)\n        assert_equal(len(test), splits.n_test)\n        assert_equal(len(set(train).intersection(test)), 0)\n\n        label_counts = np.unique(labels)\n        assert_equal(splits.test_size, 1.0 / n_folds)\n        assert_equal(splits.n_train + splits.n_test, len(labels))\n        assert_equal(len(label_counts), 2)\n        ex_test_p = float(splits.n_test) / n_samples\n        ex_train_p = float(splits.n_train) / n_samples\n\n        assert_counts_are_ok(train_counts, ex_train_p)\n        assert_counts_are_ok(test_counts, ex_test_p)\n\n\ndef test_stratified_shuffle_split_overlap_train_test_bug():\n    # See https://github.com/scikit-learn/scikit-learn/issues/6121 for\n    # the original bug report\n    labels = [0, 1, 2, 3] * 3 + [4, 5] * 5\n\n    splits = cval.StratifiedShuffleSplit(labels, n_iter=1,\n                                         test_size=0.5, random_state=0)\n    train, test = next(iter(splits))\n\n    assert_array_equal(np.intersect1d(train, test), [])\n\n\ndef test_predefinedsplit_with_kfold_split():\n    # Check that PredefinedSplit can reproduce a split generated by Kfold.\n    folds = -1 * np.ones(10)\n    kf_train = []\n    kf_test = []\n    for i, (train_ind, test_ind) in enumerate(cval.KFold(10, 5, shuffle=True)):\n        kf_train.append(train_ind)\n        kf_test.append(test_ind)\n        folds[test_ind] = i\n    ps_train = []\n    ps_test = []\n    ps = cval.PredefinedSplit(folds)\n    for train_ind, test_ind in ps:\n        ps_train.append(train_ind)\n        ps_test.append(test_ind)\n    assert_array_equal(ps_train, kf_train)\n    assert_array_equal(ps_test, kf_test)\n\n\ndef test_label_shuffle_split():\n    ys = [np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]),\n          np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]),\n          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),\n          np.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]),\n          ]\n\n    for y in ys:\n        n_iter = 6\n        test_size = 1. / 3\n        slo = cval.LabelShuffleSplit(y, n_iter, test_size=test_size,\n                                     random_state=0)\n\n        # Make sure the repr works\n        repr(slo)\n\n        # Test that the length is correct\n        assert_equal(len(slo), n_iter)\n\n        y_unique = np.unique(y)\n\n        for train, test in slo:\n            # First test: no train label is in the test set and vice versa\n            y_train_unique = np.unique(y[train])\n            y_test_unique = np.unique(y[test])\n            assert_false(np.any(np.in1d(y[train], y_test_unique)))\n            assert_false(np.any(np.in1d(y[test], y_train_unique)))\n\n            # Second test: train and test add up to all the data\n            assert_equal(y[train].size + y[test].size, y.size)\n\n            # Third test: train and test are disjoint\n            assert_array_equal(np.intersect1d(train, test), [])\n\n            # Fourth test: # unique train and test labels are correct,\n            #              +- 1 for rounding error\n            assert_true(abs(len(y_test_unique) -\n                            round(test_size * len(y_unique))) <= 1)\n            assert_true(abs(len(y_train_unique) -\n                            round((1.0 - test_size) * len(y_unique))) <= 1)\n\n\ndef test_leave_label_out_changing_labels():\n    # Check that LeaveOneLabelOut and LeavePLabelOut work normally if\n    # the labels variable is changed before calling __iter__\n    labels = np.array([0, 1, 2, 1, 1, 2, 0, 0])\n    labels_changing = np.array(labels, copy=True)\n    lolo = cval.LeaveOneLabelOut(labels)\n    lolo_changing = cval.LeaveOneLabelOut(labels_changing)\n    lplo = cval.LeavePLabelOut(labels, p=2)\n    lplo_changing = cval.LeavePLabelOut(labels_changing, p=2)\n    labels_changing[:] = 0\n    for llo, llo_changing in [(lolo, lolo_changing), (lplo, lplo_changing)]:\n        for (train, test), (train_chan, test_chan) in zip(llo, llo_changing):\n            assert_array_equal(train, train_chan)\n            assert_array_equal(test, test_chan)\n\n\ndef test_cross_val_score():\n    clf = MockClassifier()\n    for a in range(-10, 10):\n        clf.a = a\n        # Smoke test\n        scores = cval.cross_val_score(clf, X, y)\n        assert_array_equal(scores, clf.score(X, y))\n\n        # test with multioutput y\n        scores = cval.cross_val_score(clf, X_sparse, X)\n        assert_array_equal(scores, clf.score(X_sparse, X))\n\n        scores = cval.cross_val_score(clf, X_sparse, y)\n        assert_array_equal(scores, clf.score(X_sparse, y))\n\n        # test with multioutput y\n        scores = cval.cross_val_score(clf, X_sparse, X)\n        assert_array_equal(scores, clf.score(X_sparse, X))\n\n    # test with X and y as list\n    list_check = lambda x: isinstance(x, list)\n    clf = CheckingClassifier(check_X=list_check)\n    scores = cval.cross_val_score(clf, X.tolist(), y.tolist())\n\n    clf = CheckingClassifier(check_y=list_check)\n    scores = cval.cross_val_score(clf, X, y.tolist())\n\n    assert_raises(ValueError, cval.cross_val_score, clf, X, y,\n                  scoring=\"sklearn\")\n\n    # test with 3d X and\n    X_3d = X[:, :, np.newaxis]\n    clf = MockClassifier(allow_nd=True)\n    scores = cval.cross_val_score(clf, X_3d, y)\n\n    clf = MockClassifier(allow_nd=False)\n    assert_raises(ValueError, cval.cross_val_score, clf, X_3d, y)\n\n\ndef test_cross_val_score_pandas():\n    # check cross_val_score doesn't destroy pandas dataframe\n    types = [(MockDataFrame, MockDataFrame)]\n    try:\n        from pandas import Series, DataFrame\n        types.append((Series, DataFrame))\n    except ImportError:\n        pass\n    for TargetType, InputFeatureType in types:\n        # X dataframe, y series\n        X_df, y_ser = InputFeatureType(X), TargetType(y)\n        check_df = lambda x: isinstance(x, InputFeatureType)\n        check_series = lambda x: isinstance(x, TargetType)\n        clf = CheckingClassifier(check_X=check_df, check_y=check_series)\n        cval.cross_val_score(clf, X_df, y_ser)\n\n\ndef test_cross_val_score_mask():\n    # test that cross_val_score works with boolean masks\n    svm = SVC(kernel=\"linear\")\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    cv_indices = cval.KFold(len(y), 5)\n    scores_indices = cval.cross_val_score(svm, X, y, cv=cv_indices)\n    cv_indices = cval.KFold(len(y), 5)\n    cv_masks = []\n    for train, test in cv_indices:\n        mask_train = np.zeros(len(y), dtype=np.bool)\n        mask_test = np.zeros(len(y), dtype=np.bool)\n        mask_train[train] = 1\n        mask_test[test] = 1\n        cv_masks.append((train, test))\n    scores_masks = cval.cross_val_score(svm, X, y, cv=cv_masks)\n    assert_array_equal(scores_indices, scores_masks)\n\n\ndef test_cross_val_score_precomputed():\n    # test for svm with precomputed kernel\n    svm = SVC(kernel=\"precomputed\")\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    linear_kernel = np.dot(X, X.T)\n    score_precomputed = cval.cross_val_score(svm, linear_kernel, y)\n    svm = SVC(kernel=\"linear\")\n    score_linear = cval.cross_val_score(svm, X, y)\n    assert_array_equal(score_precomputed, score_linear)\n\n    # Error raised for non-square X\n    svm = SVC(kernel=\"precomputed\")\n    assert_raises(ValueError, cval.cross_val_score, svm, X, y)\n\n    # test error is raised when the precomputed kernel is not array-like\n    # or sparse\n    assert_raises(ValueError, cval.cross_val_score, svm,\n                  linear_kernel.tolist(), y)\n\n\ndef test_cross_val_score_fit_params():\n    clf = MockClassifier()\n    n_samples = X.shape[0]\n    n_classes = len(np.unique(y))\n\n    DUMMY_INT = 42\n    DUMMY_STR = '42'\n    DUMMY_OBJ = object()\n\n    def assert_fit_params(clf):\n        # Function to test that the values are passed correctly to the\n        # classifier arguments for non-array type\n\n        assert_equal(clf.dummy_int, DUMMY_INT)\n        assert_equal(clf.dummy_str, DUMMY_STR)\n        assert_equal(clf.dummy_obj, DUMMY_OBJ)\n\n    fit_params = {'sample_weight': np.ones(n_samples),\n                  'class_prior': np.ones(n_classes) / n_classes,\n                  'sparse_sample_weight': W_sparse,\n                  'sparse_param': P_sparse,\n                  'dummy_int': DUMMY_INT,\n                  'dummy_str': DUMMY_STR,\n                  'dummy_obj': DUMMY_OBJ,\n                  'callback': assert_fit_params}\n    cval.cross_val_score(clf, X, y, fit_params=fit_params)\n\n\ndef test_cross_val_score_score_func():\n    clf = MockClassifier()\n    _score_func_args = []\n\n    def score_func(y_test, y_predict):\n        _score_func_args.append((y_test, y_predict))\n        return 1.0\n\n    with warnings.catch_warnings(record=True):\n        scoring = make_scorer(score_func)\n        score = cval.cross_val_score(clf, X, y, scoring=scoring)\n    assert_array_equal(score, [1.0, 1.0, 1.0])\n    assert len(_score_func_args) == 3\n\n\ndef test_cross_val_score_errors():\n    class BrokenEstimator:\n        pass\n\n    assert_raises(TypeError, cval.cross_val_score, BrokenEstimator(), X)\n\n\ndef test_train_test_split_errors():\n    assert_raises(ValueError, cval.train_test_split)\n    assert_raises(ValueError, cval.train_test_split, range(3), train_size=1.1)\n    assert_raises(ValueError, cval.train_test_split, range(3), test_size=0.6,\n                  train_size=0.6)\n    assert_raises(ValueError, cval.train_test_split, range(3),\n                  test_size=np.float32(0.6), train_size=np.float32(0.6))\n    assert_raises(ValueError, cval.train_test_split, range(3),\n                  test_size=\"wrong_type\")\n    assert_raises(ValueError, cval.train_test_split, range(3), test_size=2,\n                  train_size=4)\n    assert_raises(TypeError, cval.train_test_split, range(3),\n                  some_argument=1.1)\n    assert_raises(ValueError, cval.train_test_split, range(3), range(42))\n\n\ndef test_train_test_split():\n    X = np.arange(100).reshape((10, 10))\n    X_s = coo_matrix(X)\n    y = np.arange(10)\n\n    # simple test\n    split = cval.train_test_split(X, y, test_size=None, train_size=.5)\n    X_train, X_test, y_train, y_test = split\n    assert_equal(len(y_test), len(y_train))\n    # test correspondence of X and y\n    assert_array_equal(X_train[:, 0], y_train * 10)\n    assert_array_equal(X_test[:, 0], y_test * 10)\n\n    # conversion of lists to arrays (deprecated?)\n    with warnings.catch_warnings(record=True):\n        split = cval.train_test_split(X, X_s, y.tolist())\n    X_train, X_test, X_s_train, X_s_test, y_train, y_test = split\n    assert_array_equal(X_train, X_s_train.toarray())\n    assert_array_equal(X_test, X_s_test.toarray())\n\n    # don't convert lists to anything else by default\n    split = cval.train_test_split(X, X_s, y.tolist())\n    X_train, X_test, X_s_train, X_s_test, y_train, y_test = split\n    assert_true(isinstance(y_train, list))\n    assert_true(isinstance(y_test, list))\n\n    # allow nd-arrays\n    X_4d = np.arange(10 * 5 * 3 * 2).reshape(10, 5, 3, 2)\n    y_3d = np.arange(10 * 7 * 11).reshape(10, 7, 11)\n    split = cval.train_test_split(X_4d, y_3d)\n    assert_equal(split[0].shape, (7, 5, 3, 2))\n    assert_equal(split[1].shape, (3, 5, 3, 2))\n    assert_equal(split[2].shape, (7, 7, 11))\n    assert_equal(split[3].shape, (3, 7, 11))\n\n    # test stratification option\n    y = np.array([1, 1, 1, 1, 2, 2, 2, 2])\n    for test_size, exp_test_size in zip([2, 4, 0.25, 0.5, 0.75],\n                                        [2, 4, 2, 4, 6]):\n        train, test = cval.train_test_split(y,\n                                            test_size=test_size,\n                                            stratify=y,\n                                            random_state=0)\n        assert_equal(len(test), exp_test_size)\n        assert_equal(len(test) + len(train), len(y))\n        # check the 1:1 ratio of ones and twos in the data is preserved\n        assert_equal(np.sum(train == 1), np.sum(train == 2))\n\n\ndef train_test_split_pandas():\n    # check cross_val_score doesn't destroy pandas dataframe\n    types = [MockDataFrame]\n    try:\n        from pandas import DataFrame\n        types.append(DataFrame)\n    except ImportError:\n        pass\n    for InputFeatureType in types:\n        # X dataframe\n        X_df = InputFeatureType(X)\n        X_train, X_test = cval.train_test_split(X_df)\n        assert_true(isinstance(X_train, InputFeatureType))\n        assert_true(isinstance(X_test, InputFeatureType))\n\ndef train_test_split_mock_pandas():\n    # X mock dataframe\n    X_df = MockDataFrame(X)\n    X_train, X_test = cval.train_test_split(X_df)\n    assert_true(isinstance(X_train, MockDataFrame))\n    assert_true(isinstance(X_test, MockDataFrame))\n\n\ndef test_cross_val_score_with_score_func_classification():\n    iris = load_iris()\n    clf = SVC(kernel='linear')\n\n    # Default score (should be the accuracy score)\n    scores = cval.cross_val_score(clf, iris.data, iris.target, cv=5)\n    assert_array_almost_equal(scores, [0.97, 1., 0.97, 0.97, 1.], 2)\n\n    # Correct classification score (aka. zero / one score) - should be the\n    # same as the default estimator score\n    zo_scores = cval.cross_val_score(clf, iris.data, iris.target,\n                                     scoring=\"accuracy\", cv=5)\n    assert_array_almost_equal(zo_scores, [0.97, 1., 0.97, 0.97, 1.], 2)\n\n    # F1 score (class are balanced so f1_score should be equal to zero/one\n    # score\n    f1_scores = cval.cross_val_score(clf, iris.data, iris.target,\n                                     scoring=\"f1_weighted\", cv=5)\n    assert_array_almost_equal(f1_scores, [0.97, 1., 0.97, 0.97, 1.], 2)\n\n\ndef test_cross_val_score_with_score_func_regression():\n    X, y = make_regression(n_samples=30, n_features=20, n_informative=5,\n                           random_state=0)\n    reg = Ridge()\n\n    # Default score of the Ridge regression estimator\n    scores = cval.cross_val_score(reg, X, y, cv=5)\n    assert_array_almost_equal(scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)\n\n    # R2 score (aka. determination coefficient) - should be the\n    # same as the default estimator score\n    r2_scores = cval.cross_val_score(reg, X, y, scoring=\"r2\", cv=5)\n    assert_array_almost_equal(r2_scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)\n\n    # Mean squared error; this is a loss function, so \"scores\" are negative\n    neg_mse_scores = cval.cross_val_score(reg, X, y, cv=5,\n                                          scoring=\"neg_mean_squared_error\")\n    expected_neg_mse = np.array([-763.07, -553.16, -274.38, -273.26, -1681.99])\n    assert_array_almost_equal(neg_mse_scores, expected_neg_mse, 2)\n\n    # (negated) Root mean squared error; same as mean squared.\n    rmse_scores = cval.cross_val_score(reg, X, y, cv=5,\n                                       scoring=\"neg_rmse\")\n    expected_rmse = np.array([-27.62, -23.52, -16.56, -16.53, -41.01])\n    assert_array_almost_equal(rmse_scores, expected_rmse, 2)\n\n    # Explained variance\n    scoring = make_scorer(explained_variance_score)\n    ev_scores = cval.cross_val_score(reg, X, y, cv=5, scoring=scoring)\n    assert_array_almost_equal(ev_scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)\n\n\ndef test_permutation_score():\n    iris = load_iris()\n    X = iris.data\n    X_sparse = coo_matrix(X)\n    y = iris.target\n    svm = SVC(kernel='linear')\n    cv = cval.StratifiedKFold(y, 2)\n\n    score, scores, pvalue = cval.permutation_test_score(\n        svm, X, y, n_permutations=30, cv=cv, scoring=\"accuracy\")\n    assert_greater(score, 0.9)\n    assert_almost_equal(pvalue, 0.0, 1)\n\n    score_label, _, pvalue_label = cval.permutation_test_score(\n        svm, X, y, n_permutations=30, cv=cv, scoring=\"accuracy\",\n        labels=np.ones(y.size), random_state=0)\n    assert_true(score_label == score)\n    assert_true(pvalue_label == pvalue)\n\n    # check that we obtain the same results with a sparse representation\n    svm_sparse = SVC(kernel='linear')\n    cv_sparse = cval.StratifiedKFold(y, 2)\n    score_label, _, pvalue_label = cval.permutation_test_score(\n        svm_sparse, X_sparse, y, n_permutations=30, cv=cv_sparse,\n        scoring=\"accuracy\", labels=np.ones(y.size), random_state=0)\n\n    assert_true(score_label == score)\n    assert_true(pvalue_label == pvalue)\n\n    # test with custom scoring object\n    def custom_score(y_true, y_pred):\n        return (((y_true == y_pred).sum() - (y_true != y_pred).sum())\n                / y_true.shape[0])\n\n    scorer = make_scorer(custom_score)\n    score, _, pvalue = cval.permutation_test_score(\n        svm, X, y, n_permutations=100, scoring=scorer, cv=cv, random_state=0)\n    assert_almost_equal(score, .93, 2)\n    assert_almost_equal(pvalue, 0.01, 3)\n\n    # set random y\n    y = np.mod(np.arange(len(y)), 3)\n\n    score, scores, pvalue = cval.permutation_test_score(\n        svm, X, y, n_permutations=30, cv=cv, scoring=\"accuracy\")\n\n    assert_less(score, 0.5)\n    assert_greater(pvalue, 0.2)\n\n\ndef test_cross_val_generator_with_indices():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 1, 2, 2])\n    labels = np.array([1, 2, 3, 4])\n    # explicitly passing indices value is deprecated\n    loo = cval.LeaveOneOut(4)\n    lpo = cval.LeavePOut(4, 2)\n    kf = cval.KFold(4, 2)\n    skf = cval.StratifiedKFold(y, 2)\n    lolo = cval.LeaveOneLabelOut(labels)\n    lopo = cval.LeavePLabelOut(labels, 2)\n    ps = cval.PredefinedSplit([1, 1, 2, 2])\n    ss = cval.ShuffleSplit(2)\n    for cv in [loo, lpo, kf, skf, lolo, lopo, ss, ps]:\n        for train, test in cv:\n            assert_not_equal(np.asarray(train).dtype.kind, 'b')\n            assert_not_equal(np.asarray(train).dtype.kind, 'b')\n            X[train], X[test]\n            y[train], y[test]\n\n\n@ignore_warnings\ndef test_cross_val_generator_with_default_indices():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 1, 2, 2])\n    labels = np.array([1, 2, 3, 4])\n    loo = cval.LeaveOneOut(4)\n    lpo = cval.LeavePOut(4, 2)\n    kf = cval.KFold(4, 2)\n    skf = cval.StratifiedKFold(y, 2)\n    lolo = cval.LeaveOneLabelOut(labels)\n    lopo = cval.LeavePLabelOut(labels, 2)\n    ss = cval.ShuffleSplit(2)\n    ps = cval.PredefinedSplit([1, 1, 2, 2])\n    for cv in [loo, lpo, kf, skf, lolo, lopo, ss, ps]:\n        for train, test in cv:\n            assert_not_equal(np.asarray(train).dtype.kind, 'b')\n            assert_not_equal(np.asarray(train).dtype.kind, 'b')\n            X[train], X[test]\n            y[train], y[test]\n\n\ndef test_shufflesplit_errors():\n    assert_raises(ValueError, cval.ShuffleSplit, 10, test_size=2.0)\n    assert_raises(ValueError, cval.ShuffleSplit, 10, test_size=1.0)\n    assert_raises(ValueError, cval.ShuffleSplit, 10, test_size=0.1,\n                  train_size=0.95)\n    assert_raises(ValueError, cval.ShuffleSplit, 10, test_size=11)\n    assert_raises(ValueError, cval.ShuffleSplit, 10, test_size=10)\n    assert_raises(ValueError, cval.ShuffleSplit, 10, test_size=8, train_size=3)\n    assert_raises(ValueError, cval.ShuffleSplit, 10, train_size=1j)\n    assert_raises(ValueError, cval.ShuffleSplit, 10, test_size=None,\n                  train_size=None)\n\n\ndef test_shufflesplit_reproducible():\n    # Check that iterating twice on the ShuffleSplit gives the same\n    # sequence of train-test when the random_state is given\n    ss = cval.ShuffleSplit(10, random_state=21)\n    assert_array_equal(list(a for a, b in ss), list(a for a, b in ss))\n\n\ndef test_safe_split_with_precomputed_kernel():\n    clf = SVC()\n    clfp = SVC(kernel=\"precomputed\")\n\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    K = np.dot(X, X.T)\n\n    cv = cval.ShuffleSplit(X.shape[0], test_size=0.25, random_state=0)\n    tr, te = list(cv)[0]\n\n    X_tr, y_tr = cval._safe_split(clf, X, y, tr)\n    K_tr, y_tr2 = cval._safe_split(clfp, K, y, tr)\n    assert_array_almost_equal(K_tr, np.dot(X_tr, X_tr.T))\n\n    X_te, y_te = cval._safe_split(clf, X, y, te, tr)\n    K_te, y_te2 = cval._safe_split(clfp, K, y, te, tr)\n    assert_array_almost_equal(K_te, np.dot(X_te, X_tr.T))\n\n\ndef test_cross_val_score_allow_nans():\n    # Check that cross_val_score allows input data with NaNs\n    X = np.arange(200, dtype=np.float64).reshape(10, -1)\n    X[2, :] = np.nan\n    y = np.repeat([0, 1], X.shape[0] / 2)\n    p = Pipeline([\n        ('imputer', Imputer(strategy='mean', missing_values='NaN')),\n        ('classifier', MockClassifier()),\n    ])\n    cval.cross_val_score(p, X, y, cv=5)\n\n\ndef test_train_test_split_allow_nans():\n    # Check that train_test_split allows input data with NaNs\n    X = np.arange(200, dtype=np.float64).reshape(10, -1)\n    X[2, :] = np.nan\n    y = np.repeat([0, 1], X.shape[0] / 2)\n    cval.train_test_split(X, y, test_size=0.2, random_state=42)\n\n\ndef test_permutation_test_score_allow_nans():\n    # Check that permutation_test_score allows input data with NaNs\n    X = np.arange(200, dtype=np.float64).reshape(10, -1)\n    X[2, :] = np.nan\n    y = np.repeat([0, 1], X.shape[0] / 2)\n    p = Pipeline([\n        ('imputer', Imputer(strategy='mean', missing_values='NaN')),\n        ('classifier', MockClassifier()),\n    ])\n    cval.permutation_test_score(p, X, y, cv=5)\n\n\ndef test_check_cv_return_types():\n    X = np.ones((9, 2))\n    cv = cval.check_cv(3, X, classifier=False)\n    assert_true(isinstance(cv, cval.KFold))\n\n    y_binary = np.array([0, 1, 0, 1, 0, 0, 1, 1, 1])\n    cv = cval.check_cv(3, X, y_binary, classifier=True)\n    assert_true(isinstance(cv, cval.StratifiedKFold))\n\n    y_multiclass = np.array([0, 1, 0, 1, 2, 1, 2, 0, 2])\n    cv = cval.check_cv(3, X, y_multiclass, classifier=True)\n    assert_true(isinstance(cv, cval.StratifiedKFold))\n\n    X = np.ones((5, 2))\n    y_multilabel = [[1, 0, 1], [1, 1, 0], [0, 0, 0], [0, 1, 1], [1, 0, 0]]\n    cv = cval.check_cv(3, X, y_multilabel, classifier=True)\n    assert_true(isinstance(cv, cval.KFold))\n\n    y_multioutput = np.array([[1, 2], [0, 3], [0, 0], [3, 1], [2, 0]])\n    cv = cval.check_cv(3, X, y_multioutput, classifier=True)\n    assert_true(isinstance(cv, cval.KFold))\n\n\ndef test_cross_val_score_multilabel():\n    X = np.array([[-3, 4], [2, 4], [3, 3], [0, 2], [-3, 1],\n                  [-2, 1], [0, 0], [-2, -1], [-1, -2], [1, -2]])\n    y = np.array([[1, 1], [0, 1], [0, 1], [0, 1], [1, 1],\n                  [0, 1], [1, 0], [1, 1], [1, 0], [0, 0]])\n    clf = KNeighborsClassifier(n_neighbors=1)\n    scoring_micro = make_scorer(precision_score, average='micro')\n    scoring_macro = make_scorer(precision_score, average='macro')\n    scoring_samples = make_scorer(precision_score, average='samples')\n    score_micro = cval.cross_val_score(clf, X, y, scoring=scoring_micro, cv=5)\n    score_macro = cval.cross_val_score(clf, X, y, scoring=scoring_macro, cv=5)\n    score_samples = cval.cross_val_score(clf, X, y,\n                                         scoring=scoring_samples, cv=5)\n    assert_almost_equal(score_micro, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 3])\n    assert_almost_equal(score_macro, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 4])\n    assert_almost_equal(score_samples, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 4])\n\n\ndef test_cross_val_predict():\n    boston = load_boston()\n    X, y = boston.data, boston.target\n    cv = cval.KFold(len(boston.target))\n\n    est = Ridge()\n\n    # Naive loop (should be same as cross_val_predict):\n    preds2 = np.zeros_like(y)\n    for train, test in cv:\n        est.fit(X[train], y[train])\n        preds2[test] = est.predict(X[test])\n\n    preds = cval.cross_val_predict(est, X, y, cv=cv)\n    assert_array_almost_equal(preds, preds2)\n\n    preds = cval.cross_val_predict(est, X, y)\n    assert_equal(len(preds), len(y))\n\n    cv = cval.LeaveOneOut(len(y))\n    preds = cval.cross_val_predict(est, X, y, cv=cv)\n    assert_equal(len(preds), len(y))\n\n    Xsp = X.copy()\n    Xsp *= (Xsp > np.median(Xsp))\n    Xsp = coo_matrix(Xsp)\n    preds = cval.cross_val_predict(est, Xsp, y)\n    assert_array_almost_equal(len(preds), len(y))\n\n    preds = cval.cross_val_predict(KMeans(), X)\n    assert_equal(len(preds), len(y))\n\n    def bad_cv():\n        for i in range(4):\n            yield np.array([0, 1, 2, 3]), np.array([4, 5, 6, 7, 8])\n\n    assert_raises(ValueError, cval.cross_val_predict, est, X, y, cv=bad_cv())\n\n\ndef test_cross_val_predict_input_types():\n    clf = Ridge()\n    # Smoke test\n    predictions = cval.cross_val_predict(clf, X, y)\n    assert_equal(predictions.shape, (10,))\n\n    # test with multioutput y\n    with ignore_warnings(category=ConvergenceWarning):\n        predictions = cval.cross_val_predict(clf, X_sparse, X)\n    assert_equal(predictions.shape, (10, 2))\n\n    predictions = cval.cross_val_predict(clf, X_sparse, y)\n    assert_array_equal(predictions.shape, (10,))\n\n    # test with multioutput y\n    with ignore_warnings(category=ConvergenceWarning):\n        predictions = cval.cross_val_predict(clf, X_sparse, X)\n    assert_array_equal(predictions.shape, (10, 2))\n\n    # test with X and y as list\n    list_check = lambda x: isinstance(x, list)\n    clf = CheckingClassifier(check_X=list_check)\n    predictions = cval.cross_val_predict(clf, X.tolist(), y.tolist())\n\n    clf = CheckingClassifier(check_y=list_check)\n    predictions = cval.cross_val_predict(clf, X, y.tolist())\n\n    # test with 3d X and\n    X_3d = X[:, :, np.newaxis]\n    check_3d = lambda x: x.ndim == 3\n    clf = CheckingClassifier(check_X=check_3d)\n    predictions = cval.cross_val_predict(clf, X_3d, y)\n    assert_array_equal(predictions.shape, (10,))\n\n\ndef test_cross_val_predict_pandas():\n    # check cross_val_score doesn't destroy pandas dataframe\n    types = [(MockDataFrame, MockDataFrame)]\n    try:\n        from pandas import Series, DataFrame\n        types.append((Series, DataFrame))\n    except ImportError:\n        pass\n    for TargetType, InputFeatureType in types:\n        # X dataframe, y series\n        X_df, y_ser = InputFeatureType(X), TargetType(y)\n        check_df = lambda x: isinstance(x, InputFeatureType)\n        check_series = lambda x: isinstance(x, TargetType)\n        clf = CheckingClassifier(check_X=check_df, check_y=check_series)\n        cval.cross_val_predict(clf, X_df, y_ser)\n\n\ndef test_sparse_fit_params():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    clf = MockClassifier()\n    fit_params = {'sparse_sample_weight': coo_matrix(np.eye(X.shape[0]))}\n    a = cval.cross_val_score(clf, X, y, fit_params=fit_params)\n    assert_array_equal(a, np.ones(3))\n\n\ndef test_check_is_partition():\n    p = np.arange(100)\n    assert_true(cval._check_is_partition(p, 100))\n    assert_false(cval._check_is_partition(np.delete(p, 23), 100))\n\n    p[0] = 23\n    assert_false(cval._check_is_partition(p, 100))\n\n\ndef test_cross_val_predict_sparse_prediction():\n    # check that cross_val_predict gives same result for sparse and dense input\n    X, y = make_multilabel_classification(n_classes=2, n_labels=1,\n                                          allow_unlabeled=False,\n                                          return_indicator=True,\n                                          random_state=1)\n    X_sparse = csr_matrix(X)\n    y_sparse = csr_matrix(y)\n    classif = OneVsRestClassifier(SVC(kernel='linear'))\n    preds = cval.cross_val_predict(classif, X, y, cv=10)\n    preds_sparse = cval.cross_val_predict(classif, X_sparse, y_sparse, cv=10)\n    preds_sparse = preds_sparse.toarray()\n    assert_array_almost_equal(preds_sparse, preds)\n"
    }
  ],
  "questions": [
    "As the square root is a monotonic function on the positive domain, taking the square root would have no effect on any model selection. Could you please mention a use-case when it taking the root has some real advantage?"
  ],
  "golden_answers": [
    "> As the square root is a monotonic function on the positive domain, taking the square root would have no effect on any model selection\r\n\r\nThis is why we reject it previously I think (though I'm unable to find relevant discussions)\r\nI'd argue that given the popularity of RMSE, it might be worthwhile to add several lines of (redundant) code for it (we only need <5 lines of code for the metric I think)\r\nSometimes users might want to report the RMSE of their model instead of MSE, because RMSE is more meaningful (i.e., it reflects the deviation between actual value and predicted value)."
  ],
  "questions_generated": [
    "How is the RMSE metric proposed to be implemented in the scikit-learn library?",
    "Why might RMSE be considered more meaningful compared to MSE for certain users?",
    "What is the significance of the square root being a monotonic function in the context of RMSE implementation?",
    "What are some potential advantages of adding RMSE directly as a metric in scikit-learn?",
    "In terms of code structure, where would the RMSE metric likely be added in the scikit-learn repository?"
  ],
  "golden_answers_generated": [
    "The RMSE metric is proposed to be implemented by adding a 'squared' option to the existing 'mean_squared_error' function to return the root of the value directly, and by creating a new scorer 'neg_root_mean_squared_error' for model evaluation.",
    "RMSE might be considered more meaningful because it reflects the deviation between actual and predicted values in the same units as the target variable, making it easier to interpret the model's accuracy in a real-world context.",
    "The significance is that taking the square root of MSE to compute RMSE does not impact model selection, as the square root is a monotonic function over positive values. This means the ranking of models based on RMSE will be consistent with that based on MSE.",
    "Adding RMSE directly as a metric in scikit-learn simplifies its computation for users, provides a more intuitive metric for reporting model performance, and aligns with its popularity and usage in various fields where interpreting errors in the same units as the original data is beneficial.",
    "The RMSE metric would likely be added to the 'metrics' module within the scikit-learn library, as indicated by the existing imports and the structure of the module that includes score functions and performance metrics."
  ]
}