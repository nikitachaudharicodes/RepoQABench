{
  "repo_name": "scikit-learn_scikit-learn",
  "issue_id": "10430",
  "issue_description": "# AdaBoost has misleading defaults\n\n#### Description\r\nThe default AdaBoostClassifier algorithm has a DecisionTreeClassifier as the default base classifier. This results in the AdaBoost only training one decision tree, since the default decision tree has no limit on the number of leaves, and always fits the training set perfectly.\r\n\r\nIn the examples with AdaBoost, a custom decision tree is used, except for [this one](scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) which just uses `AdaBoost()`, and the normal AdaBoost behavior is significantly different from what's shown in the example (closer to the random forest one).\r\n\r\nI think either the default decision tree instance for AdaBoost should have a `max_depth` not `None`, or there should be a warning in the AdaBoost page.\r\n\r\n#### Versions\r\n```\r\n>>> import platform; print(platform.platform())\r\nLinux-4.13.16-041316-generic-x86_64-with-Ubuntu-17.10-artful\r\n>>> import sys; print(\"Python\", sys.version)\r\nPython 3.6.3 (default, Oct  3 2017, 21:45:48) \r\n[GCC 7.2.0]\r\n>>> import numpy; print(\"NumPy\", numpy.__version__)\r\nNumPy 1.13.3\r\n>>> import scipy; print(\"SciPy\", scipy.__version__)\r\nSciPy 1.0.0\r\n>>> import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\nScikit-Learn 0.19.1\r\n```\r\n",
  "issue_comments": [
    {
      "id": 356152189,
      "user": "jnothman",
      "body": "yes, I think we should raise a FutureWarning (\"From version 0.22 the\ndefault base_estimator will have max_depth=5 to facilitate boosting\"). Is 5\na good pick?\n\nOn 9 Jan 2018 10:37 am, \"EmreAtes\" <notifications@github.com> wrote:\n\n> Description\n>\n> The default AdaBoostClassifier algorithm has a DecisionTreeClassifier as\n> the default base classifier. This results in the AdaBoost only training one\n> decision tree, since the default decision tree has no limit on the number\n> of leaves, and always fits the training set perfectly.\n>\n> In the examples with AdaBoost, a custom decision tree is used, except for this\n> one\n> <http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html>\n> which just uses AdaBoost(), and the normal AdaBoost behavior is\n> significantly different from what's shown in the example (closer to the\n> random forest one).\n>\n> I think either the default decision tree instance for AdaBoost should have\n> a max_depth not None, or there should be a warning in the AdaBoost page.\n> Versions\n>\n> >>> import platform; print(platform.platform())\n> Linux-4.13.16-041316-generic-x86_64-with-Ubuntu-17.10-artful\n> >>> import sys; print(\"Python\", sys.version)\n> Python 3.6.3 (default, Oct  3 2017, 21:45:48)\n> [GCC 7.2.0]\n> >>> import numpy; print(\"NumPy\", numpy.__version__)\n> NumPy 1.13.3\n> >>> import scipy; print(\"SciPy\", scipy.__version__)\n> SciPy 1.0.0\n> >>> import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\n> Scikit-Learn 0.19.1\n>\n> â€”\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/10430>, or mute the\n> thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz60r0kghbkCVt16NwB4pMk5aFCUoQks5tIqajgaJpZM4RXHGW>\n> .\n>\n"
    },
    {
      "id": 356284234,
      "user": "glemaitre",
      "body": "Isn't the AdaBoostClassifier using a stump classifier as a default classifier:\r\nhttps://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/ensemble/weight_boosting.py#L417\r\n\r\nIn the below example, the fitted estimators are decision tree with a `max_depth=1`\r\n\r\n@EmreAtes \r\nDo I miss or misunderstand something in your issue?\r\n\r\n<details>\r\n\r\n```python\r\nIn [1]: from sklearn.datasets import load_iris\r\n\r\nIn [2]: from sklearn.ensemble import AdaBoostClassifier\r\n\r\nIn [3]: clf = AdaBoostClassifier()\r\n\r\nIn [4]: data = load_iris()\r\n\r\nIn [5]: clf.fit(data.data, data.target)\r\nOut[5]: \r\nAdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\r\n          learning_rate=1.0, n_estimators=50, random_state=None)\r\n\r\nIn [6]: clf.estimators_\r\nOut[6]: \r\n[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1977170892, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=795379450, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1456218076, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=141098983, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1421020861, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1911911899, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=872599047, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=92810223, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1914305116, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1739451437, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=880881156, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2078050865, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2028548245, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1496792866, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=119634708, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=870421654, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1128325060, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=153464182, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=749895396, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1584307081, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1050288811, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=499395746, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1420258338, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1479026734, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1143216818, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=39206585, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=742527837, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2130971080, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1807654776, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=734202664, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=279272917, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1726675867, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1903622624, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=360410346, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1553419754, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=563013807, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1631395988, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1430216705, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1531174573, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1467792894, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1210745182, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=582262190, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1204234760, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=240103665, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=434182951, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1724223259, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=735868759, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=157727173, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2059406381, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=885738331, splitter='best')]\r\n```"
    },
    {
      "id": 356302482,
      "user": "qinhanmin2014",
      "body": "Maybe we should try to document the default base estimator we use? E.g., `AdaBoostRegressor` is using `DecisionTreeRegressor(max_depth=3)` as default, which at least confuse me some time in the past. "
    },
    {
      "id": 356314997,
      "user": "glemaitre",
      "body": "@qinhanmin2014 I am +1 to add to the documentation that AdaBoostClassifier used a decision stump (tree with max_depth=1) and the regressor as a max_depth at 3.\r\n\r\nI am not sure that it is worth changing the behavior of the regressor (using a decision stump as well). We will have to emit a FutureWarning which will introduce some noise. I am not super familiar with regression problem using AdaBoost but I don't think that there is rule in the machine learning community which enforce to have a stump decision as a weak learner. @jnothman @qinhanmin2014 WDYT?"
    },
    {
      "id": 356318307,
      "user": "qinhanmin2014",
      "body": "> I am not sure that it is worth changing the behavior of the regressor\r\n\r\nI don't mean to change it. Just a bit curious about how the value is chosen. \r\n\r\n> I am +1 to add to the documentation that AdaBoostClassifier used a decision stump (tree with max_depth=1) and the regressor as a max_depth at 3.\r\n\r\nLGTM. This is also what I want to express in my comment."
    },
    {
      "id": 356353898,
      "user": "EmreAtes",
      "body": "@glemaitre you're right, my problem occurred because I wanted to use `class_weight='balanced'`, and I used a default decision tree with only `class_weight` set, which caused AdaBoost to exit after fitting the single decision tree. I agree that documentation would help."
    },
    {
      "id": 356355147,
      "user": "glemaitre",
      "body": "Oh yes in this case a default DecisionTreeClassifier has max_depth to None."
    },
    {
      "id": 356363683,
      "user": "pgad",
      "body": "Hi, can I take this issue?"
    },
    {
      "id": 356365593,
      "user": "glemaitre",
      "body": "                                                                                      â€ŽGo for it. Â "
    },
    {
      "id": 356436020,
      "user": "jnothman",
      "body": "sorry that I took the issue on face value! good to see we only need doc\nimprovements!\n"
    },
    {
      "id": 356496838,
      "user": "qinhanmin2014",
      "body": "Btw @pgad \r\nIt will be better if you can try to figure out why  `AdaBoostRegressor` is using `DecisionTreeRegressor(max_depth=3)` as default (e.g., go through the reference and relevant PR). Thanks."
    },
    {
      "id": 356567349,
      "user": "glemaitre",
      "body": "@qinhanmin2014 \r\n\r\nCheck https://github.com/ndawe/scikit-learn/pull/21 for the reason to have regression with a max_depth at 3.FYI, Originally both classifier and regressor has a max_depth=3"
    },
    {
      "id": 356587399,
      "user": "qinhanmin2014",
      "body": "Thanks @glemaitre for the investigation, here is the complete history:\r\n2011.12 https://github.com/scikit-learn/scikit-learn/commit/b454d56bd366034872e7c76f45798f8ead2e77e5 \r\nAdaBoostClassifier:DecisionTreeClassifier()\r\nAdaBoostRegressor:DecisionTreeRegressor()\r\n\r\n2012.12 https://github.com/scikit-learn/scikit-learn/commit/0f620c1062761dcef648a35a2b8509b76125dd4d\r\nAdaBoostClassifier:DecisionTreeClassifier(max_depth=3)\r\nAdaBoostRegressor:DecisionTreeRegressor(max_depth=3)\r\n\r\n2013.1 https://github.com/scikit-learn/scikit-learn/commit/03f67a5a5219f7a6519e5fa83b1f1f806f435a1b\r\nAdaBoostClassifier:DecisionTreeClassifier(max_depth=1)\r\nAdaBoostRegressor:DecisionTreeRegressor(max_depth=3)\r\n\r\nI'm a bit uncomfortable about something like \"For regression, I kept things as they were since the same setting broke the test.\" (I suppose we should focus on providing right implementation?) But I might consider it unnecessary to spend too much time focusing on the default value (According to my limited knowledge, there's not a consensus on it, or maybe I'm wrong?). Document current default is enough from my side. A reason for current default serves as bouns point. WDYT? Thanks."
    },
    {
      "id": 357530247,
      "user": "7ayushgupta",
      "body": "Can I add the statement to the documentation? "
    },
    {
      "id": 357530521,
      "user": "glemaitre",
      "body": "@pgad Are you working on it?"
    },
    {
      "id": 357945992,
      "user": "7ayushgupta",
      "body": "Maybe I can proceed on it now?"
    },
    {
      "id": 357946562,
      "user": "jnothman",
      "body": "go ahead"
    },
    {
      "id": 357954338,
      "user": "7ayushgupta",
      "body": "Where should I add the line? I think this would be okay?\r\n![screenshot-2018-1-16 sklearn ensemble adaboostclassifier scikit-learn 0 19 1 documentation 1](https://user-images.githubusercontent.com/33892472/34990573-9617fe00-faec-11e7-8671-f3edeccfa105.png)"
    },
    {
      "id": 357955058,
      "user": "jnothman",
      "body": "It's easier to see the diff in a pull request..."
    },
    {
      "id": 357972789,
      "user": "qinhanmin2014",
      "body": "@7ayushgupta I think you should also change `default=DecisionTreeClassifier` to `default=None` like Bagging. Then considering something like `If None, then the base estimator is DecisionTreeClassifier(max_depth=1).`\r\n(I might also prefer to make things clear instead of using something like decision stump)"
    },
    {
      "id": 357977384,
      "user": "7ayushgupta",
      "body": "I'll need some time. It's my first time with such a huge code base and it's confusing me :sweat_smile:"
    },
    {
      "id": 357980890,
      "user": "qinhanmin2014",
      "body": "Don't worry and feel free to ask. The [contributing guide](http://scikit-learn.org/stable/developers/contributing.html) might be a good reference :)"
    },
    {
      "id": 358720738,
      "user": "7ayushgupta",
      "body": "Okay, sorry but I am quite confused. I made some changes in the weightboosting.py, but when I `make` the docs, they don't reflect in the generated html files."
    },
    {
      "id": 360636158,
      "user": "pgad",
      "body": "Hi, I'm sorry, I got super busy, and couldn't get to it. I'm happy someone else did."
    }
  ],
  "text_context": "# AdaBoost has misleading defaults\n\n#### Description\r\nThe default AdaBoostClassifier algorithm has a DecisionTreeClassifier as the default base classifier. This results in the AdaBoost only training one decision tree, since the default decision tree has no limit on the number of leaves, and always fits the training set perfectly.\r\n\r\nIn the examples with AdaBoost, a custom decision tree is used, except for [this one](scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) which just uses `AdaBoost()`, and the normal AdaBoost behavior is significantly different from what's shown in the example (closer to the random forest one).\r\n\r\nI think either the default decision tree instance for AdaBoost should have a `max_depth` not `None`, or there should be a warning in the AdaBoost page.\r\n\r\n#### Versions\r\n```\r\n>>> import platform; print(platform.platform())\r\nLinux-4.13.16-041316-generic-x86_64-with-Ubuntu-17.10-artful\r\n>>> import sys; print(\"Python\", sys.version)\r\nPython 3.6.3 (default, Oct  3 2017, 21:45:48) \r\n[GCC 7.2.0]\r\n>>> import numpy; print(\"NumPy\", numpy.__version__)\r\nNumPy 1.13.3\r\n>>> import scipy; print(\"SciPy\", scipy.__version__)\r\nSciPy 1.0.0\r\n>>> import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\nScikit-Learn 0.19.1\r\n```\r\n\n\nyes, I think we should raise a FutureWarning (\"From version 0.22 the\ndefault base_estimator will have max_depth=5 to facilitate boosting\"). Is 5\na good pick?\n\nOn 9 Jan 2018 10:37 am, \"EmreAtes\" <notifications@github.com> wrote:\n\n> Description\n>\n> The default AdaBoostClassifier algorithm has a DecisionTreeClassifier as\n> the default base classifier. This results in the AdaBoost only training one\n> decision tree, since the default decision tree has no limit on the number\n> of leaves, and always fits the training set perfectly.\n>\n> In the examples with AdaBoost, a custom decision tree is used, except for this\n> one\n> <http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html>\n> which just uses AdaBoost(), and the normal AdaBoost behavior is\n> significantly different from what's shown in the example (closer to the\n> random forest one).\n>\n> I think either the default decision tree instance for AdaBoost should have\n> a max_depth not None, or there should be a warning in the AdaBoost page.\n> Versions\n>\n> >>> import platform; print(platform.platform())\n> Linux-4.13.16-041316-generic-x86_64-with-Ubuntu-17.10-artful\n> >>> import sys; print(\"Python\", sys.version)\n> Python 3.6.3 (default, Oct  3 2017, 21:45:48)\n> [GCC 7.2.0]\n> >>> import numpy; print(\"NumPy\", numpy.__version__)\n> NumPy 1.13.3\n> >>> import scipy; print(\"SciPy\", scipy.__version__)\n> SciPy 1.0.0\n> >>> import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\n> Scikit-Learn 0.19.1\n>\n> â€”\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/10430>, or mute the\n> thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz60r0kghbkCVt16NwB4pMk5aFCUoQks5tIqajgaJpZM4RXHGW>\n> .\n>\n\n\nIsn't the AdaBoostClassifier using a stump classifier as a default classifier:\r\nhttps://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/ensemble/weight_boosting.py#L417\r\n\r\nIn the below example, the fitted estimators are decision tree with a `max_depth=1`\r\n\r\n@EmreAtes \r\nDo I miss or misunderstand something in your issue?\r\n\r\n<details>\r\n\r\n```python\r\nIn [1]: from sklearn.datasets import load_iris\r\n\r\nIn [2]: from sklearn.ensemble import AdaBoostClassifier\r\n\r\nIn [3]: clf = AdaBoostClassifier()\r\n\r\nIn [4]: data = load_iris()\r\n\r\nIn [5]: clf.fit(data.data, data.target)\r\nOut[5]: \r\nAdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\r\n          learning_rate=1.0, n_estimators=50, random_state=None)\r\n\r\nIn [6]: clf.estimators_\r\nOut[6]: \r\n[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1977170892, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=795379450, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1456218076, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=141098983, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1421020861, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1911911899, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=872599047, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=92810223, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1914305116, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1739451437, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=880881156, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2078050865, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2028548245, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1496792866, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=119634708, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=870421654, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1128325060, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=153464182, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=749895396, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1584307081, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1050288811, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=499395746, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1420258338, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1479026734, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1143216818, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=39206585, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=742527837, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2130971080, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1807654776, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=734202664, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=279272917, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1726675867, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1903622624, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=360410346, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1553419754, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=563013807, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1631395988, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1430216705, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1531174573, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1467792894, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1210745182, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=582262190, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1204234760, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=240103665, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=434182951, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1724223259, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=735868759, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=157727173, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2059406381, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=885738331, splitter='best')]\r\n```\n\nMaybe we should try to document the default base estimator we use? E.g., `AdaBoostRegressor` is using `DecisionTreeRegressor(max_depth=3)` as default, which at least confuse me some time in the past. \n\n@qinhanmin2014 I am +1 to add to the documentation that AdaBoostClassifier used a decision stump (tree with max_depth=1) and the regressor as a max_depth at 3.\r\n\r\nI am not sure that it is worth changing the behavior of the regressor (using a decision stump as well). We will have to emit a FutureWarning which will introduce some noise. I am not super familiar with regression problem using AdaBoost but I don't think that there is rule in the machine learning community which enforce to have a stump decision as a weak learner. @jnothman @qinhanmin2014 WDYT?\n\n> I am not sure that it is worth changing the behavior of the regressor\r\n\r\nI don't mean to change it. Just a bit curious about how the value is chosen. \r\n\r\n> I am +1 to add to the documentation that AdaBoostClassifier used a decision stump (tree with max_depth=1) and the regressor as a max_depth at 3.\r\n\r\nLGTM. This is also what I want to express in my comment.\n\n@glemaitre you're right, my problem occurred because I wanted to use `class_weight='balanced'`, and I used a default decision tree with only `class_weight` set, which caused AdaBoost to exit after fitting the single decision tree. I agree that documentation would help.\n\nOh yes in this case a default DecisionTreeClassifier has max_depth to None.\n\nHi, can I take this issue?\n\n                                                                                      â€ŽGo for it. Â \n\nsorry that I took the issue on face value! good to see we only need doc\nimprovements!\n\n\nBtw @pgad \r\nIt will be better if you can try to figure out why  `AdaBoostRegressor` is using `DecisionTreeRegressor(max_depth=3)` as default (e.g., go through the reference and relevant PR). Thanks.\n\n@qinhanmin2014 \r\n\r\nCheck https://github.com/ndawe/scikit-learn/pull/21 for the reason to have regression with a max_depth at 3.FYI, Originally both classifier and regressor has a max_depth=3\n\nThanks @glemaitre for the investigation, here is the complete history:\r\n2011.12 https://github.com/scikit-learn/scikit-learn/commit/b454d56bd366034872e7c76f45798f8ead2e77e5 \r\nAdaBoostClassifier:DecisionTreeClassifier()\r\nAdaBoostRegressor:DecisionTreeRegressor()\r\n\r\n2012.12 https://github.com/scikit-learn/scikit-learn/commit/0f620c1062761dcef648a35a2b8509b76125dd4d\r\nAdaBoostClassifier:DecisionTreeClassifier(max_depth=3)\r\nAdaBoostRegressor:DecisionTreeRegressor(max_depth=3)\r\n\r\n2013.1 https://github.com/scikit-learn/scikit-learn/commit/03f67a5a5219f7a6519e5fa83b1f1f806f435a1b\r\nAdaBoostClassifier:DecisionTreeClassifier(max_depth=1)\r\nAdaBoostRegressor:DecisionTreeRegressor(max_depth=3)\r\n\r\nI'm a bit uncomfortable about something like \"For regression, I kept things as they were since the same setting broke the test.\" (I suppose we should focus on providing right implementation?) But I might consider it unnecessary to spend too much time focusing on the default value (According to my limited knowledge, there's not a consensus on it, or maybe I'm wrong?). Document current default is enough from my side. A reason for current default serves as bouns point. WDYT? Thanks.\n\nCan I add the statement to the documentation? \n\n@pgad Are you working on it?\n\nMaybe I can proceed on it now?\n\ngo ahead\n\nWhere should I add the line? I think this would be okay?\r\n![screenshot-2018-1-16 sklearn ensemble adaboostclassifier scikit-learn 0 19 1 documentation 1](https://user-images.githubusercontent.com/33892472/34990573-9617fe00-faec-11e7-8671-f3edeccfa105.png)\n\nIt's easier to see the diff in a pull request...\n\n@7ayushgupta I think you should also change `default=DecisionTreeClassifier` to `default=None` like Bagging. Then considering something like `If None, then the base estimator is DecisionTreeClassifier(max_depth=1).`\r\n(I might also prefer to make things clear instead of using something like decision stump)\n\nI'll need some time. It's my first time with such a huge code base and it's confusing me :sweat_smile:\n\nDon't worry and feel free to ask. The [contributing guide](http://scikit-learn.org/stable/developers/contributing.html) might be a good reference :)\n\nOkay, sorry but I am quite confused. I made some changes in the weightboosting.py, but when I `make` the docs, they don't reflect in the generated html files.\n\nHi, I'm sorry, I got super busy, and couldn't get to it. I'm happy someone else did.",
  "pr_link": "https://github.com/ndawe/scikit-learn/pull/21",
  "code_context": [
    {
      "filename": "sklearn/ensemble/tests/test_weight_boosting.py",
      "content": "\"\"\"\nTesting for the boost module (sklearn.ensemble.boost).\n\"\"\"\n\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom numpy.testing import assert_array_almost_equal\nfrom numpy.testing import assert_equal\nfrom nose.tools import assert_true\nfrom nose.tools import assert_raises\n\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.utils import shuffle\nfrom sklearn import datasets\n\n\n# Common random state\nrng = np.random.RandomState(0)\n\n# Toy sample\nX = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\ny = [-1, -1, -1, 1, 1, 1]\nT = [[-1, -1], [2, 2], [3, 2]]\ntrue_result = [-1, 1, 1]\n\n# Load the iris dataset and randomly permute it\niris = datasets.load_iris()\nperm = rng.permutation(iris.target.size)\niris.data, iris.target = shuffle(iris.data, iris.target, random_state=rng)\n\n# Load the boston dataset and randomly permute it\nboston = datasets.load_boston()\nboston.data, boston.target = shuffle(boston.data, boston.target, random_state=rng)\n\n\ndef test_classification_toy():\n    \"\"\"Check classification on a toy dataset.\"\"\"\n    clf = AdaBoostClassifier()\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n\n\ndef test_regression_toy():\n    \"\"\"Check classification on a toy dataset.\"\"\"\n    clf = AdaBoostRegressor()\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n\n\ndef test_iris():\n    \"\"\"Check consistency on dataset iris.\"\"\"\n    clf = AdaBoostClassifier()\n    clf.fit(iris.data, iris.target)\n    score = clf.score(iris.data, iris.target)\n    assert score > 0.9, \"Failed with criterion %s and score = %f\" % (c,\n                                                                     score)\n\n\ndef test_boston():\n    \"\"\"Check consistency on dataset boston house prices.\"\"\"\n    clf = AdaBoostRegressor()\n    clf.fit(boston.data, boston.target)\n    score = clf.score(boston.data, boston.target)\n    assert score > 0.85\n\n\ndef test_staged_predict():\n    \"\"\"Check staged predictions.\"\"\"\n    # AdaBoost classification\n    clf = AdaBoostClassifier(n_estimators=10)\n    clf.fit(iris.data, iris.target)\n\n    predictions = clf.predict(iris.data)\n    staged_predictions = [p for p in clf.staged_predict(iris.data)]\n    proba = clf.predict_proba(iris.data)\n    staged_probas = [p for p in clf.staged_predict_proba(iris.data)]\n    score = clf.score(iris.data, iris.target)\n    staged_scores = [s for s in clf.staged_score(iris.data, iris.target)]\n\n    assert_equal(len(staged_predictions), 10)\n    assert_array_almost_equal(predictions, staged_predictions[-1])\n    assert_equal(len(staged_probas), 10)\n    assert_array_almost_equal(proba, staged_probas[-1])\n    assert_equal(len(staged_scores), 10)\n    assert_array_almost_equal(score, staged_scores[-1])\n\n    # AdaBoost regression\n    clf = AdaBoostRegressor(n_estimators=10)\n    clf.fit(boston.data, boston.target)\n\n    predictions = clf.predict(boston.data)\n    staged_predictions = [p for p in clf.staged_predict(boston.data)]\n    score = clf.score(boston.data, boston.target)\n    staged_scores = [s for s in clf.staged_score(boston.data, boston.target)]\n\n    assert_equal(len(staged_predictions), 10)\n    assert_array_almost_equal(predictions, staged_predictions[-1])\n    assert_equal(len(staged_scores), 10)\n    assert_array_almost_equal(score, staged_scores[-1])\n\n\ndef test_gridsearch():\n    \"\"\"Check that base trees can be grid-searched.\"\"\"\n    # AdaBoost classification\n    boost = AdaBoostClassifier()\n    parameters = {'n_estimators': (1, 2),\n                  'base_estimator__max_depth': (1, 2)}\n    clf = GridSearchCV(boost, parameters)\n    clf.fit(iris.data, iris.target)\n\n    # AdaBoost regression\n    boost = AdaBoostRegressor()\n    parameters = {'n_estimators': (1, 2),\n                  'base_estimator__max_depth': (1, 2)}\n    clf = GridSearchCV(boost, parameters)\n    clf.fit(boston.data, boston.target)\n\n\ndef test_pickle():\n    \"\"\"Check pickability.\"\"\"\n    import pickle\n\n    # Adaboost classifier\n    obj = AdaBoostClassifier()\n    obj.fit(iris.data, iris.target)\n    score = obj.score(iris.data, iris.target)\n    s = pickle.dumps(obj)\n\n    obj2 = pickle.loads(s)\n    assert_equal(type(obj2), obj.__class__)\n    score2 = obj2.score(iris.data, iris.target)\n    assert score == score2\n\n    # Adaboost regressor\n    obj = AdaBoostRegressor()\n    obj.fit(boston.data, boston.target)\n    score = obj.score(boston.data, boston.target)\n    s = pickle.dumps(obj)\n\n    obj2 = pickle.loads(s)\n    assert_equal(type(obj2), obj.__class__)\n    score2 = obj2.score(boston.data, boston.target)\n    assert score == score2\n\n\ndef test_importances():\n    \"\"\"Check variable importances.\"\"\"\n    X, y = datasets.make_classification(n_samples=2000,\n                                        n_features=10,\n                                        n_informative=3,\n                                        n_redundant=0,\n                                        n_repeated=0,\n                                        shuffle=False,\n                                        random_state=1)\n\n    clf = AdaBoostClassifier(compute_importances=True)\n\n    clf.fit(X, y)\n    importances = clf.feature_importances_\n    n_important = sum(importances > 0.1)\n\n    assert_equal(importances.shape[0], 10)\n    assert_equal(n_important, 3)\n\n    clf = AdaBoostClassifier()\n    clf.fit(X, y)\n    assert_true(clf.feature_importances_ is None)\n\n\ndef test_error():\n    \"\"\"Test that it gives proper exception on deficient input.\"\"\"\n    from sklearn.dummy import DummyClassifier\n    from sklearn.dummy import DummyRegressor\n\n    # Invalid values for parameters\n    assert_raises(ValueError,\n                  AdaBoostClassifier(learning_rate=-1).fit,\n                  X, y)\n\n    assert_raises(TypeError,\n                  AdaBoostClassifier(base_estimator=DummyRegressor()).fit,\n                  X, y)\n\n    assert_raises(TypeError,\n                  AdaBoostRegressor(base_estimator=DummyClassifier()).fit,\n                  X, y)\n\n\ndef test_base_estimator():\n    \"\"\"Test different base estimators.\"\"\"\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.svm import SVC\n\n    clf = AdaBoostClassifier(RandomForestClassifier())\n    clf.fit(X, y)\n\n    clf = AdaBoostClassifier(SVC(), real=False)\n    clf.fit(X, y)\n\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.svm import SVR\n\n    clf = AdaBoostRegressor(RandomForestRegressor())\n    clf.fit(X, y)\n\n    clf = AdaBoostRegressor(SVR())\n    clf.fit(X, y)\n\n\nif __name__ == \"__main__\":\n    import nose\n    nose.runmodule()\n"
    },
    {
      "filename": "sklearn/ensemble/weight_boosting.py",
      "content": "\"\"\"Weight Boosting\n\nThis module contains weight boosting estimators for both classification and\nregression.\n\nThe module structure is the following:\n\n- The ``BaseAdaBoost`` base class implements a common ``fit`` method\n  for all the estimators in the module. Regression and classification\n  only differ from each other in the loss function that is optimized.\n\n- ``AdaBoostClassifier`` implements adaptive boosting (AdaBoost-SAMME) for\n  classification problems.\n\n- ``AdaBoostRegressor`` implements adaptive boosting (AdaBoost.R2) for\n  regression problems.\n\"\"\"\n\n# Authors: Noel Dawe, Gilles Louppe\n# License: BSD Style\n\nfrom abc import ABCMeta, abstractmethod\n\nimport numpy as np\nfrom numpy.core.umath_tests import inner1d\n\nfrom .base import BaseEnsemble\nfrom ..base import ClassifierMixin, RegressorMixin\nfrom ..tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom ..utils import check_arrays\nfrom ..metrics import r2_score\n\n\n__all__ = [\n    'AdaBoostClassifier',\n    'AdaBoostRegressor',\n]\n\n\nclass BaseWeightBoosting(BaseEnsemble):\n    \"\"\"Abstract base class for weight boosting. \"\"\"\n\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __init__(self,\n                 base_estimator,\n                 n_estimators=50,\n                 estimator_params=tuple(),\n                 learning_rate=0.5,\n                 compute_importances=False):\n        super(BaseWeightBoosting, self).__init__(\n            base_estimator=base_estimator,\n            n_estimators=n_estimators,\n            estimator_params=estimator_params)\n\n        self.weights_ = None\n        self.errors_ = None\n        self.learning_rate = learning_rate\n        self.compute_importances = compute_importances\n        self.feature_importances_ = None\n\n    def fit(self, X, y, sample_weight=None, boost_method=None):\n        \"\"\"Build a boosted classifier/regressor from the training set (X, y).\n\n        Parameters\n        ----------\n        X : array-like of shape = [n_samples, n_features]\n            The training input samples.\n\n        y : array-like of shape = [n_samples]\n            The target values (integers that correspond to classes in\n            classification, real numbers in regression).\n\n        sample_weight : array-like of shape = [n_samples], optional\n            Sample weights.\n\n        boost_method : function, optional\n            The boosting step.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        # Check parameters\n        if self.learning_rate <= 0:\n            raise ValueError(\"``learning_rate`` must be greater than zero\")\n\n        if self.compute_importances:\n            self.base_estimator.set_params(compute_importances=True)\n\n        # Check data\n        X, y = check_arrays(X, y, sparse_format=\"dense\")\n\n        if sample_weight is None:\n            # initialize weights to 1 / n_samples\n            sample_weight = np.ones(X.shape[0], dtype=np.float) / X.shape[0]\n        else:\n            # or normalize them\n            sample_weight = np.copy(sample_weight) / sample_weight.sum()\n\n        # Clear any previous fit results\n        self.estimators_ = []\n        self.weights_ = np.zeros(self.n_estimators, dtype=np.float)\n        self.errors_ = np.ones(self.n_estimators, dtype=np.float)\n\n        if boost_method is None:\n            boost_method = self._boost\n\n        for iboost in xrange(self.n_estimators):\n            # Boosting step\n            sample_weight, weight, error = boost_method(\n                iboost,\n                X, y,\n                sample_weight)\n\n            # Early termination\n            if sample_weight is None:\n                break\n\n            self.weights_[iboost] = weight\n            self.errors_[iboost] = error\n\n            # Stop if error is zero\n            if error == 0:\n                break\n\n            if iboost < self.n_estimators - 1:\n                # normalize\n                sample_weight /= sample_weight.sum()\n\n        # Sum the importances\n        try:\n            if self.compute_importances:\n                norm = self.weights_.sum()\n                self.feature_importances_ = (\n                    sum(weight * clf.feature_importances_ for weight, clf\n                        in zip(self.weights_, self.estimators_))\n                    / norm)\n\n        except AttributeError:\n            raise AttributeError(\n                \"Unable to compute feature importances \"\n                \"since base_estimator does not have a \"\n                \"``feature_importances_`` attribute\")\n\n        return self\n\n    def staged_score(self, X, y, n_estimators=-1):\n        \"\"\"Return staged scores for X, y.\n\n        This generator method yields the ensemble score after each iteration of\n        boosting and therefore allows monitoring, such as to determine the\n        score on a test set after each boost.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Training set.\n\n        y : array-like, shape = [n_samples]\n            Labels for X.\n\n        Returns\n        -------\n        z : float\n\n        \"\"\"\n        for y_pred in self.staged_predict(X, n_estimators=n_estimators):\n            if isinstance(self, ClassifierMixin):\n                yield np.mean(y_pred == y)\n            else:\n                yield r2_score(y, y_pred)\n\n\nclass AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):\n    \"\"\"An AdaBoost classifier.\n\n    An AdaBoost classifier is a meta-estimator that begins by fitting a\n    classifier on the original dataset and then fits additional copies of the\n    classifer on the same dataset but where the weights of incorrectly\n    classified instances are adjusted such that subsequent classifiers focus\n    more on difficult cases.\n\n    This class implements the algorithm known as AdaBoost-SAMME [2].\n\n    Parameters\n    ----------\n    base_estimator : object, optional (default=DecisionTreeClassifier)\n        The base estimator from which the boosted ensemble is built.\n        Support for sample weighting is required, as well as proper `classes_`\n        and `n_classes_` attributes.\n\n    n_estimators : integer, optional (default=50)\n        The maximum number of estimators at which boosting is terminated.\n        In case of perfect fit, the learning procedure is stopped early.\n\n    learning_rate : float, optional (default=0.1)\n        Learning rate shrinks the contribution of each classifier by\n        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n        ``n_estimators``.\n\n    real : boolean, optional (default=True)\n        If True then use the real SAMME.R boosting algorithm.\n        ``base_estimator`` must support calculation of class probabilities.\n        If False then use the discrete SAMME boosting algorithm.\n\n    compute_importances : boolean, optional (default=False)\n        Whether feature importances are computed and stored in the\n        ``feature_importances_`` attribute when calling fit.\n\n    Attributes\n    ----------\n    `estimators_` : list of classifiers\n        The collection of fitted sub-estimators.\n\n    `classes_` : array of shape = [n_classes]\n        The classes labels.\n\n    `n_classes_` : int\n        The number of classes.\n\n    `weights_` : list of floats\n        Weights for each estimator in the boosted ensemble.\n\n    `errors_` : list of floats\n        Classification error for each estimator in the boosted\n        ensemble.\n\n    `feature_importances_` : array of shape = [n_features]\n        The feature importances if supported by the ``base_estimator``.\n        Only computed if ``compute_importances=True``.\n\n    See also\n    --------\n    AdaBoostRegressor, GradientBoostingClassifier, DecisionTreeClassifier\n\n    References\n    ----------\n\n    .. [1] Yoav Freund, Robert E. Schapire. \"A Decision-Theoretic\n           Generalization of on-Line Learning and an Application\n           to Boosting\", 1995.\n\n    .. [2] Ji Zhu, Hui Zou, Saharon Rosset, Trevor Hastie.\n           \"Multi-class AdaBoost\", 2009.\n    \"\"\"\n    def __init__(self,\n                 base_estimator=DecisionTreeClassifier(max_depth=1),\n                 n_estimators=50,\n                 learning_rate=0.5,\n                 real=True,\n                 compute_importances=False):\n        super(AdaBoostClassifier, self).__init__(\n            base_estimator=base_estimator,\n            n_estimators=n_estimators,\n            learning_rate=learning_rate,\n            compute_importances=compute_importances)\n\n        self.real = real\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Build a boosted classifier from the training set (X, y).\n\n        Parameters\n        ----------\n        X : array-like of shape = [n_samples, n_features]\n            The training input samples.\n\n        y : array-like of shape = [n_samples]\n            The target values (integers that correspond to classes).\n\n        sample_weight : array-like of shape = [n_samples], optional\n            Sample weights.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        # Check that the base estimator is a classifier\n        if not isinstance(self.base_estimator, ClassifierMixin):\n            raise TypeError(\"``base_estimator`` must be a \"\n                            \"subclass of ``ClassifierMixin``\")\n\n        # 'Real' boosting step\n        if self.real:\n            if not hasattr(self.base_estimator, \"predict_proba\"):\n                raise TypeError(\n                    \"The real AdaBoost algorithm requires that the weak\"\n                    \"learner supports the calculation of class probabilities\")\n\n            return super(AdaBoostClassifier, self).fit(\n                X, y, sample_weight, self._boost_real)\n\n        # 'Discrete' boosting step\n        else:\n            return super(AdaBoostClassifier, self).fit(\n                    X, y, sample_weight, self._boost_discrete)\n\n    def _boost_real(self, iboost, X, y, sample_weight):\n        \"\"\"Implement a single boost using the real algorithm.\n\n        Perform a single boost according to the real multi-class SAMME.R\n        algorithm and return the updated sample weights.\n\n        Parameters\n        ----------\n        iboost : int\n            The index of the current boost iteration.\n\n        X : array-like of shape = [n_samples, n_features]\n            The training input samples.\n\n        y : array-like of shape = [n_samples]\n            The target values (integers that correspond to classes).\n\n        sample_weight : array-like of shape = [n_samples]\n            The current sample weights.\n\n        Returns\n        -------\n        sample_weight : array-like of shape = [n_samples] or None\n            The reweighted sample weights.\n            If None then boosting has terminated early.\n\n        weight : float\n            The weight for the current boost.\n            If None then boosting has terminated early.\n\n        error : float\n            The classification error for the current boost.\n            If None then boosting has terminated early.\n        \"\"\"\n        estimator = self._make_estimator()\n\n        if hasattr(estimator, 'fit_predict_proba'):\n            # optim for estimators that are able to save redundant\n            # computations when calling fit + predict_proba\n            # on the same input X\n            y_predict_proba = estimator.fit_predict_proba(\n                X, y, sample_weight=sample_weight)\n        else:\n            y_predict_proba = estimator.fit(\n                X, y, sample_weight=sample_weight).predict_proba(X)\n\n        if iboost == 0:\n            self.classes_ = getattr(estimator, 'classes_', None)\n            self.n_classes_ = getattr(estimator, 'n_classes_',\n                                      getattr(estimator, 'n_classes', 1))\n\n        y_predict = np.array(self.classes_.take(\n            np.argmax(y_predict_proba, axis=1), axis=0))\n\n        # instances incorrectly classified\n        incorrect = y_predict != y\n\n        # error fraction\n        error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n\n        # stop if classification is perfect\n        if error == 0:\n            return sample_weight, 1., 0.\n\n        # negative sample weights can yield an overall negative error...\n        if error < 0:\n            # use the absolute value\n            # if you have a better idea of how to handle negative\n            # sample weights let me know\n            error = abs(error)\n\n        # construct y coding\n        n_classes = self.n_classes_\n        classes = np.array(self.classes_)\n        y_codes = np.array([-1. / (n_classes - 1), 1.])\n        y_coding = y_codes.take(classes == y.reshape(y.shape[0], 1))\n\n        # boost weight using multi-class AdaBoost SAMME.R alg\n        weight = -1. * self.learning_rate * (\n            ((n_classes - 1.) / n_classes) *\n            inner1d(y_coding, np.log(y_predict_proba + 1e-200)))\n\n        # only boost the weights if I will fit again\n        if not iboost == self.n_estimators - 1:\n            sample_weight *= np.exp(weight)\n\n        return sample_weight, 1., error\n\n    def _boost_discrete(self, iboost, X, y, sample_weight):\n        \"\"\"Implement a single boost using the discrete algorithm.\n\n        Perform a single boost according to the discrete multi-class SAMME\n        algorithm and return the updated sample weights.\n\n        Parameters\n        ----------\n        iboost : int\n            The index of the current boost iteration.\n\n        X : array-like of shape = [n_samples, n_features]\n            The training input samples.\n\n        y : array-like of shape = [n_samples]\n            The target values (integers that correspond to classes).\n\n        sample_weight : array-like of shape = [n_samples]\n            The current sample weights.\n\n        Returns\n        -------\n        sample_weight : array-like of shape = [n_samples] or None\n            The reweighted sample weights.\n            If None then boosting has terminated early.\n\n        weight : float\n            The weight for the current boost.\n            If None then boosting has terminated early.\n\n        error : float\n            The classification error for the current boost.\n            If None then boosting has terminated early.\n        \"\"\"\n        estimator = self._make_estimator()\n\n        if hasattr(estimator, 'fit_predict'):\n            # optim for estimators that are able to save redundant\n            # computations when calling fit + predict\n            # on the same input X\n            y_predict = estimator.fit_predict(\n                X, y, sample_weight=sample_weight)\n        else:\n            y_predict = estimator.fit(\n                X, y, sample_weight=sample_weight).predict(X)\n\n        if iboost == 0:\n            self.classes_ = getattr(estimator, 'classes_', None)\n            self.n_classes_ = getattr(estimator, 'n_classes_',\n                                      getattr(estimator, 'n_classes', 1))\n\n        # instances incorrectly classified\n        incorrect = y_predict != y\n\n        # error fraction\n        error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n\n        # stop if classification is perfect\n        if error == 0:\n            return sample_weight, 1., 0.\n\n        # negative sample weights can yield an overall negative error...\n        if error < 0:\n            # use the absolute value\n            # if you have a better idea of how to handle negative\n            # sample weights let me know\n            error = abs(error)\n\n        n_classes = self.n_classes_\n\n        # stop if the error is at least as bad as random guessing\n        if error >= 1. - (1. / n_classes):\n            self.estimators_.pop(-1)\n            return None, None, None\n\n        # boost weight using multi-class AdaBoost SAMME alg\n        weight = self.learning_rate * (\n            np.log((1. - error) / error) +\n            np.log(n_classes - 1.))\n\n        # only boost the weights if I will fit again\n        if not iboost == self.n_estimators - 1:\n            sample_weight *= np.exp(weight * incorrect)\n\n        return sample_weight, weight, error\n\n    def predict(self, X, n_estimators=-1):\n        \"\"\"Predict classes for X.\n\n        The predicted class of an input sample is computed\n        as the weighted mean prediction of the classifiers in the ensemble.\n\n        Parameters\n        ----------\n        X : array-like of shape = [n_samples, n_features]\n            The input samples.\n\n        n_estimators : int, optional (default=-1)\n            Use only the first ``n_estimators`` classifiers for the prediction.\n            This is useful for grid searching the ``n_estimators`` parameter\n            since it is not necessary to fit separately for all choices of\n            ``n_estimators``, but only the highest ``n_estimators``. Any\n            negative value will result in all estimators being used.\n\n        Returns\n        -------\n        y : array of shape = [n_samples]\n            The predicted classes.\n        \"\"\"\n        if n_estimators == 0:\n            raise ValueError(\"``n_estimators`` must not equal zero\")\n\n        if not self.estimators_:\n            raise RuntimeError(\n                (\"{0} is not initialized. \"\n                 \"Perform a fit first\").format(self.__class__.__name__))\n\n        n_classes = self.n_classes_\n        classes = self.classes_\n        pred = None\n\n        for i, (weight, estimator) in enumerate(\n                zip(self.weights_, self.estimators_)):\n\n            if i == n_estimators:\n                break\n\n            if self.real:\n                current_pred = estimator.predict_proba(X) + 1e-200\n                current_pred = (n_classes - 1) * (\n                    np.log(current_pred) -\n                    (1. / n_classes) *\n                    np.log(current_pred).sum(axis=1)[:, np.newaxis])\n            else:\n                current_pred = estimator.predict(X)\n                current_pred = (\n                    current_pred == classes[:, np.newaxis]).T * weight\n\n            if pred is None:\n                pred = current_pred\n            else:\n                pred += current_pred\n\n        return np.array(classes.take(\n            np.argmax(pred, axis=1), axis=0))\n\n    def staged_predict(self, X, n_estimators=-1):\n        \"\"\"Return staged predictions for X.\n\n        The predicted class of an input sample is computed\n        as the weighted mean prediction of the classifiers in the ensemble.\n\n        This generator method yields the ensemble prediction after each\n        iteration of boosting and therefore allows monitoring, such as to\n        determine the prediction on a test set after each boost.\n\n        Parameters\n        ----------\n        X : array-like of shape = [n_samples, n_features]\n            The input samples.\n\n        n_estimators : int, optional (default=-1)\n            Use only the first ``n_estimators`` classifiers for the prediction.\n            This is useful for grid searching the ``n_estimators`` parameter\n            since it is not necessary to fit separately for all choices of\n            ``n_estimators``, but only the highest ``n_estimators``. Any\n            negative value will result in all estimators being used.\n\n        Returns\n        -------\n        y : array of shape = [n_samples]\n            The predicted classes.\n        \"\"\"\n        if n_estimators == 0:\n            raise ValueError(\"``n_estimators`` must not equal zero\")\n\n        if not self.estimators_:\n            raise RuntimeError(\n                (\"{0} is not initialized. \"\n                 \"Perform a fit first\").format(self.__class__.__name__))\n\n        n_classes = self.n_classes_\n        classes = self.classes_\n        pred = None\n\n        for i, (weight, estimator) in enumerate(\n                zip(self.weights_, self.estimators_)):\n\n            if i == n_estimators:\n                break\n\n            if self.real:\n                current_pred = estimator.predict_proba(X) + 1e-200\n                current_pred = (n_classes - 1) * (\n                    np.log(current_pred) -\n                    (1. / n_classes) *\n                    np.log(current_pred).sum(axis=1)[:, np.newaxis])\n            else:\n                current_pred = estimator.predict(X)\n                current_pred = (\n                    current_pred == classes[:, np.newaxis]).T * weight\n\n            if pred is None:\n                pred = current_pred\n            else:\n                pred += current_pred\n\n            yield np.array(classes.take(\n                np.argmax(pred, axis=1), axis=0))\n\n    def predict_proba(self, X, n_estimators=-1):\n        \"\"\"Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample is computed as\n        the weighted mean predicted class probabilities\n        of the classifiers in the ensemble.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each boost. See examples/ensemble/plot_adaboost_error.py\n\n        Parameters\n        ----------\n        X : array-like of shape = [n_samples, n_features]\n            The input samples.\n\n        n_estimators : int, optional (default=-1)\n            Use only the first ``n_estimators`` classifiers for the prediction.\n            This is useful for grid searching the ``n_estimators`` parameter\n            since it is not necessary to fit separately for all choices of\n            ``n_estimators``, but only the highest ``n_estimators``. Any\n            negative value will result in all estimators being used.\n\n        Returns\n        -------\n        p : array of shape = [n_samples]\n            The class probabilities of the input samples. Classes are\n            ordered by arithmetical order.\n        \"\"\"\n        if not self.real:\n            raise TypeError(\n                \"Prediction of class probabilities is only supported with the \"\n                \"real AdaBoost algorithm (``real=True``)\")\n\n        if n_estimators == 0:\n            raise ValueError(\"``n_estimators`` must not equal zero\")\n\n        proba = None\n\n        for i, (weight, estimator) in enumerate(\n                zip(self.weights_, self.estimators_)):\n\n            if i == n_estimators:\n                break\n\n            current_proba = estimator.predict_proba(X)\n\n            if proba is None:\n                proba = current_proba\n            else:\n                proba += current_proba\n\n        normalizer = proba.sum(axis=1)[:, np.newaxis]\n        normalizer[normalizer == 0.0] = 1.0\n        proba /= normalizer\n\n        return proba\n\n    def staged_predict_proba(self, X, n_estimators=-1):\n        \"\"\"Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample is computed as\n        the weighted mean predicted class probabilities\n        of the classifiers in the ensemble.\n\n        This generator method yields the ensemble predicted class probabilities\n        after each iteration of boosting and therefore allows monitoring, such\n        as to determine the predicted class probabilities on a test set after\n        each boost.\n\n        Parameters\n        ----------\n        X : array-like of shape = [n_samples, n_features]\n            The input samples.\n\n        n_estimators : int, optional (default=-1)\n            Use only the first ``n_estimators`` classifiers for the prediction.\n            This is useful for grid searching the ``n_estimators`` parameter\n            since it is not necessary to fit separately for all choices of\n            ``n_estimators``, but only the highest ``n_estimators``. Any\n            negative value will result in all estimators being used.\n\n        Returns\n        -------\n        p : array of shape = [n_samples]\n            The class probabilities of the input samples. Classes are\n            ordered by arithmetical order.\n        \"\"\"\n        if not self.real:\n            raise TypeError(\n                \"Prediction of class probabilities is only supported with the \"\n                \"real AdaBoost algorithm (``real=True``)\")\n\n        if n_estimators == 0:\n            raise ValueError(\"``n_estimators`` must not equal zero\")\n\n        proba = None\n\n        for i, (weight, estimator) in enumerate(\n                zip(self.weights_, self.estimators_)):\n\n            if i == n_estimators:\n                break\n\n            current_proba = estimator.predict_proba(X)\n\n            if proba is None:\n                proba = current_proba\n            else:\n                proba += current_proba\n\n            normalizer = proba.sum(axis=1)[:, np.newaxis]\n            normalizer[normalizer == 0.0] = 1.0\n\n            yield proba / normalizer\n\n    def predict_log_proba(self, X, n_estimators=-1):\n        \"\"\"Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the weighted mean predicted class log-probabilities\n        of the classifiers in the ensemble.\n\n        Parameters\n        ----------\n        X : array-like of shape = [n_samples, n_features]\n            The input samples.\n\n        n_estimators : int, optional (default=-1)\n            Use only the first ``n_estimators`` classifiers for the prediction.\n            This is useful for grid searching the ``n_estimators`` parameter\n            since it is not necessary to fit separately for all choices of\n            ``n_estimators``, but only the highest ``n_estimators``. Any\n            negative value will result in all estimators being used.\n\n        Returns\n        -------\n        p : array of shape = [n_samples]\n            The class log-probabilities of the input samples. Classes are\n            ordered by arithmetical order.\n        \"\"\"\n        return np.log(self.predict_proba(X, n_estimators=n_estimators))\n\n\nclass AdaBoostRegressor(BaseWeightBoosting, RegressorMixin):\n    \"\"\"An AdaBoost regressor.\n\n    An AdaBoost regressor is a meta-estimator that begins by fitting a\n    regressor on the original dataset and then fits additional copies of the\n    regressor on the same dataset but where the weights of instances are\n    adjusted according to the error of the current prediction. As such,\n    subsequent regressors focus more on difficult cases.\n\n    This class implements the algorithm known as AdaBoost.R2 [2].\n\n    Parameters\n    ----------\n    base_estimator : object, optional (default=DecisionTreeRegressor)\n        The base estimator from which the boosted ensemble is built.\n        Support for sample weighting is required.\n\n    n_estimators : integer, optional (default=50)\n        The maximum number of estimators at which boosting is terminated.\n        In case of perfect fit, the learning procedure is stopped early.\n\n    learning_rate : float, optional (default=0.1)\n        Learning rate shrinks the contribution of each regressor by\n        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n        ``n_estimators``.\n\n    compute_importances : boolean, optional (default=False)\n        Whether feature importances are computed and stored in the\n        ``feature_importances_`` attribute when calling fit.\n\n    Attributes\n    ----------\n    `estimators_` : list of classifiers\n        The collection of fitted sub-estimators.\n\n    `weights_` : list of floats\n        Weights for each estimator in the boosted ensemble.\n\n    `errors_` : list of floats\n        Regression error for each estimator in the boosted ensemble.\n\n    `feature_importances_` : array of shape = [n_features]\n        The feature importances if supported by the ``base_estimator``.\n        Only computed if ``compute_importances=True``.\n\n    See also\n    --------\n    AdaBoostClassifier, GradientBoostingRegressor, DecisionTreeRegressor\n\n    References\n    ----------\n\n    .. [1] Yoav Freund, Robert E. Schapire. \"A Decision-Theoretic\n           Generalization of on-Line Learning and an Application\n           to Boosting\", 1995.\n\n    .. [2] Harris Drucker. \"Improving Regressor using Boosting Techniques\",\n           1997.\n    \"\"\"\n    def __init__(self,\n                 base_estimator=DecisionTreeRegressor(max_depth=3),\n                 n_estimators=50,\n                 learning_rate=0.1,\n                 compute_importances=False):\n        super(AdaBoostRegressor, self).__init__(\n            base_estimator=base_estimator,\n            n_estimators=n_estimators,\n            learning_rate=learning_rate,\n            compute_importances=compute_importances)\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Build a boosted regressor from the training set (X, y).\n\n        Parameters\n        ----------\n        X : array-like of shape = [n_samples, n_features]\n            The training input samples.\n\n        y : array-like of shape = [n_samples]\n            The target values (real numbers).\n\n        sample_weight : array-like of shape = [n_samples], optional\n            Sample weights.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        # Check that the base estimator is a regressor\n        if not isinstance(self.base_estimator, RegressorMixin):\n            raise TypeError(\"``base_estimator`` must be a \"\n                            \"subclass of ``RegressorMixin``\")\n\n        # Fit\n        return super(AdaBoostRegressor, self).fit(X, y, sample_weight)\n\n    def _boost(self, iboost, X, y, sample_weight):\n        \"\"\"Implement a single boost for regression\n\n        Perform a single boost according to the AdaBoost.R2 algorithm and\n        return the updated sample weights.\n\n        Parameters\n        ----------\n        iboost : int\n            The index of the current boost iteration.\n\n        X : array-like of shape = [n_samples, n_features]\n            The training input samples.\n\n        y : array-like of shape = [n_samples]\n            The target values (integers that correspond to classes in\n            classification, real numbers in regression).\n\n        sample_weight : array-like of shape = [n_samples]\n            The current sample weights.\n\n        Returns\n        -------\n        sample_weight : array-like of shape = [n_samples] or None\n            The reweighted sample weights.\n            If None then boosting has terminated early.\n\n        weight : float\n            The weight for the current boost.\n            If None then boosting has terminated early.\n\n        error : float\n            The regression error for the current boost.\n            If None then boosting has terminated early.\n        \"\"\"\n        estimator = self._make_estimator()\n\n        if hasattr(estimator, 'fit_predict'):\n            # optim for estimators that are able to save redundant\n            # computations when calling fit + predict\n            # on the same input X\n            y_predict = estimator.fit_predict(\n                X, y, sample_weight=sample_weight)\n        else:\n            y_predict = estimator.fit(\n                X, y, sample_weight=sample_weight).predict(X)\n\n        error_vect = np.abs(y_predict - y)\n        error_max = error_vect.max()\n\n        if error_max != 0.:\n            error_vect /= error_vect.max()\n\n        error = (sample_weight * error_vect).sum()\n\n        # stop if fit is perfect\n        if error == 0:\n            return sample_weight, 1., 0.\n\n        # negative sample weights can yield an overall negative error...\n        if error < 0:\n            # use the absolute value\n            # if you have a better idea of how to handle negative\n            # sample weights let me know\n            error = abs(error)\n\n        beta = error / (1. - error)\n\n        # boost weight using AdaBoost.R2 alg\n        weight = self.learning_rate * np.log(1. / beta)\n\n        if not iboost == self.n_estimators - 1:\n            sample_weight *= np.power(\n                beta,\n                (1. - error_vect) * self.learning_rate)\n\n        return sample_weight, weight, error\n\n    def predict(self, X, n_estimators=-1):\n        \"\"\"Predict regression value for X.\n\n        The predicted regression value of an input sample is computed\n        as the weighted mean prediction of the classifiers in the ensemble.\n\n        Parameters\n        ----------\n        X : array-like of shape = [n_samples, n_features]\n            The input samples.\n\n        n_estimators : int, optional (default=-1)\n            Use only the first ``n_estimators`` classifiers for the prediction.\n            This is useful for grid searching the ``n_estimators`` parameter\n            since it is not necessary to fit separately for all choices of\n            ``n_estimators``, but only the highest ``n_estimators``. Any\n            negative value will result in all estimators being used.\n\n        Returns\n        -------\n        y : array of shape = [n_samples]\n            The predicted regression values.\n        \"\"\"\n        if n_estimators == 0:\n            raise ValueError(\"``n_estimators`` must not equal zero\")\n\n        if not self.estimators_:\n            raise RuntimeError(\n                (\"{0} is not initialized. \"\n                 \"Perform a fit first\").format(self.__class__.__name__))\n\n        pred = None\n\n        for i, (weight, estimator) in enumerate(\n                zip(self.weights_, self.estimators_)):\n            if i == n_estimators:\n                break\n\n            current_pred = estimator.predict(X)\n\n            if pred is None:\n                pred = current_pred * weight\n            else:\n                pred += current_pred * weight\n\n        pred /= self.weights_.sum()\n\n        return pred\n\n    def staged_predict(self, X, n_estimators=-1):\n        \"\"\"Return staged predictions for X.\n\n        The predicted regression value of an input sample is computed\n        as the weighted mean prediction of the classifiers in the ensemble.\n\n        This generator method yields the ensemble prediction after each\n        iteration of boosting and therefore allows monitoring, such as to\n        determine the prediction on a test set after each boost.\n\n        Parameters\n        ----------\n        X : array-like of shape = [n_samples, n_features]\n            The input samples.\n\n        n_estimators : int, optional (default=-1)\n            Use only the first ``n_estimators`` classifiers for the prediction.\n            This is useful for grid searching the ``n_estimators`` parameter\n            since it is not necessary to fit separately for all choices of\n            ``n_estimators``, but only the highest ``n_estimators``. Any\n            negative value will result in all estimators being used.\n\n        Returns\n        -------\n        y : array of shape = [n_samples]\n            The predicted regression values.\n        \"\"\"\n        if n_estimators == 0:\n            raise ValueError(\"``n_estimators`` must not equal zero\")\n\n        if not self.estimators_:\n            raise RuntimeError(\n                (\"{0} is not initialized. \"\n                 \"Perform a fit first\").format(self.__class__.__name__))\n\n        pred = None\n        norm = 0.\n\n        for i, (weight, estimator) in enumerate(\n                zip(self.weights_, self.estimators_)):\n            if i == n_estimators:\n                break\n\n            current_pred = estimator.predict(X)\n\n            if pred is None:\n                pred = current_pred * weight\n            else:\n                pred += current_pred * weight\n\n            norm += weight\n            normed_pred = pred / norm\n\n            yield normed_pred\n"
    }
  ],
  "questions": [
    "yes, I think we should raise a FutureWarning (\"From version 0.22 the\ndefault base_estimator will have max_depth=5 to facilitate boosting\"). Is 5\na good pick?\n\nOn 9 Jan 2018 10:37 am, \"EmreAtes\" <notifications@github.com> wrote:\n\n> Description\n>\n> The default AdaBoostClassifier algorithm has a DecisionTreeClassifier as\n> the default base classifier. This results in the AdaBoost only training one\n> decision tree, since the default decision tree has no limit on the number\n> of leaves, and always fits the training set perfectly.\n>\n> In the examples with AdaBoost, a custom decision tree is used, except for this\n> one\n> <http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html>\n> which just uses AdaBoost(), and the normal AdaBoost behavior is\n> significantly different from what's shown in the example (closer to the\n> random forest one).\n>\n> I think either the default decision tree instance for AdaBoost should have\n> a max_depth not None, or there should be a warning in the AdaBoost page.\n> Versions\n>\n> >>> import platform; print(platform.platform())\n> Linux-4.13.16-041316-generic-x86_64-with-Ubuntu-17.10-artful\n> >>> import sys; print(\"Python\", sys.version)\n> Python 3.6.3 (default, Oct  3 2017, 21:45:48)\n> [GCC 7.2.0]\n> >>> import numpy; print(\"NumPy\", numpy.__version__)\n> NumPy 1.13.3\n> >>> import scipy; print(\"SciPy\", scipy.__version__)\n> SciPy 1.0.0\n> >>> import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\n> Scikit-Learn 0.19.1\n>\n> â€”\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/10430>, or mute the\n> thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz60r0kghbkCVt16NwB4pMk5aFCUoQks5tIqajgaJpZM4RXHGW>\n> .\n>",
    "Isn't the AdaBoostClassifier using a stump classifier as a default classifier:\r\nhttps://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/ensemble/weight_boosting.py#L417\r\n\r\nIn the below example, the fitted estimators are decision tree with a `max_depth=1`\r\n\r\n@EmreAtes \r\nDo I miss or misunderstand something in your issue?\r\n\r\n<details>\r\n\r\n```python\r\nIn [1]: from sklearn.datasets import load_iris\r\n\r\nIn [2]: from sklearn.ensemble import AdaBoostClassifier\r\n\r\nIn [3]: clf = AdaBoostClassifier()\r\n\r\nIn [4]: data = load_iris()\r\n\r\nIn [5]: clf.fit(data.data, data.target)\r\nOut[5]: \r\nAdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\r\n          learning_rate=1.0, n_estimators=50, random_state=None)\r\n\r\nIn [6]: clf.estimators_\r\nOut[6]: \r\n[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1977170892, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=795379450, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1456218076, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=141098983, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1421020861, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1911911899, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=872599047, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=92810223, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1914305116, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1739451437, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=880881156, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2078050865, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2028548245, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1496792866, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=119634708, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=870421654, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1128325060, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=153464182, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=749895396, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1584307081, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1050288811, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=499395746, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1420258338, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1479026734, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1143216818, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=39206585, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=742527837, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2130971080, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1807654776, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=734202664, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=279272917, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1726675867, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1903622624, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=360410346, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1553419754, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=563013807, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1631395988, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1430216705, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1531174573, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1467792894, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1210745182, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=582262190, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1204234760, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=240103665, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=434182951, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1724223259, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=735868759, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=157727173, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2059406381, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=885738331, splitter='best')]\r\n```",
    "@qinhanmin2014 I am +1 to add to the documentation that AdaBoostClassifier used a decision stump (tree with max_depth=1) and the regressor as a max_depth at 3.\r\n\r\nI am not sure that it is worth changing the behavior of the regressor (using a decision stump as well). We will have to emit a FutureWarning which will introduce some noise. I am not super familiar with regression problem using AdaBoost but I don't think that there is rule in the machine learning community which enforce to have a stump decision as a weak learner. @jnothman @qinhanmin2014 WDYT?",
    "Where should I add the line? I think this would be okay?\r\n![screenshot-2018-1-16 sklearn ensemble adaboostclassifier scikit-learn 0 19 1 documentation 1](https://user-images.githubusercontent.com/33892472/34990573-9617fe00-faec-11e7-8671-f3edeccfa105.png)"
  ],
  "golden_answers": [
    "Isn't the AdaBoostClassifier using a stump classifier as a default classifier:\r\nhttps://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/ensemble/weight_boosting.py#L417\r\n\r\nIn the below example, the fitted estimators are decision tree with a `max_depth=1`\r\n\r\n@EmreAtes \r\nDo I miss or misunderstand something in your issue?\r\n\r\n<details>\r\n\r\n```python\r\nIn [1]: from sklearn.datasets import load_iris\r\n\r\nIn [2]: from sklearn.ensemble import AdaBoostClassifier\r\n\r\nIn [3]: clf = AdaBoostClassifier()\r\n\r\nIn [4]: data = load_iris()\r\n\r\nIn [5]: clf.fit(data.data, data.target)\r\nOut[5]: \r\nAdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\r\n          learning_rate=1.0, n_estimators=50, random_state=None)\r\n\r\nIn [6]: clf.estimators_\r\nOut[6]: \r\n[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1977170892, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=795379450, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1456218076, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=141098983, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1421020861, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1911911899, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=872599047, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=92810223, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1914305116, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1739451437, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=880881156, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2078050865, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2028548245, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1496792866, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=119634708, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=870421654, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1128325060, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=153464182, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=749895396, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1584307081, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1050288811, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=499395746, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1420258338, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1479026734, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1143216818, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=39206585, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=742527837, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2130971080, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1807654776, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=734202664, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=279272917, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1726675867, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1903622624, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=360410346, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1553419754, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=563013807, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1631395988, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1430216705, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1531174573, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1467792894, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1210745182, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=582262190, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1204234760, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=240103665, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=434182951, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=1724223259, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=735868759, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=157727173, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=2059406381, splitter='best'),\r\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\r\n             max_features=None, max_leaf_nodes=None,\r\n             min_impurity_decrease=0.0, min_impurity_split=None,\r\n             min_samples_leaf=1, min_samples_split=2,\r\n             min_weight_fraction_leaf=0.0, presort=False,\r\n             random_state=885738331, splitter='best')]\r\n```",
    "Maybe we should try to document the default base estimator we use? E.g., `AdaBoostRegressor` is using `DecisionTreeRegressor(max_depth=3)` as default, which at least confuse me some time in the past.",
    "@glemaitre you're right, my problem occurred because I wanted to use `class_weight='balanced'`, and I used a default decision tree with only `class_weight` set, which caused AdaBoost to exit after fitting the single decision tree. I agree that documentation would help.",
    "@7ayushgupta I think you should also change `default=DecisionTreeClassifier` to `default=None` like Bagging. Then considering something like `If None, then the base estimator is DecisionTreeClassifier(max_depth=1).`\r\n(I might also prefer to make things clear instead of using something like decision stump)"
  ],
  "questions_generated": [
    "What is the default base estimator for AdaBoostClassifier in scikit-learn, and why is it considered misleading?",
    "How does the default behavior of AdaBoostClassifier differ from the example provided in scikit-learn's documentation?",
    "What solution has been proposed for the issue with AdaBoostClassifier's default settings in scikit-learn?",
    "In the test cases provided, how is AdaBoostClassifier initialized, and what datasets are used to test its functionality?",
    "Why is setting a 'max_depth' for the default DecisionTreeClassifier important when using it as a base estimator in AdaBoostClassifier?",
    "What impact does the current default base estimator have on AdaBoostClassifier's performance and its comparison to random forests?",
    "How can a user currently modify the AdaBoostClassifier to obtain the desired boosting behavior in scikit-learn?"
  ],
  "golden_answers_generated": [
    "The default base estimator for AdaBoostClassifier in scikit-learn is a DecisionTreeClassifier with no limit on the number of leaves. This is considered misleading because it results in AdaBoost training only one decision tree, which perfectly fits the training data, rather than creating a true ensemble of weak classifiers as intended in boosting.",
    "In scikit-learn's documentation, a custom decision tree is often used as the base estimator for AdaBoost, with parameters that limit its complexity, such as 'max_depth'. This contrasts with the default behavior, where the DecisionTreeClassifier has no limits, leading to a model that trains only one strong classifier instead of a series of weak ones.",
    "One proposed solution is to set a 'max_depth' for the default DecisionTreeClassifier used as the base estimator for AdaBoostClassifier. A suggestion was made to set 'max_depth' to 5. Additionally, a FutureWarning could be implemented to inform users about the change in default behavior from version 0.22.",
    "In the test cases, AdaBoostClassifier is initialized without any parameters, which means it uses the default settings. The toy dataset and the iris dataset are used to test its classification functionality, ensuring that the classifier can fit and predict correctly on these datasets.",
    "Setting a 'max_depth' for the DecisionTreeClassifier is important because it limits the complexity of each tree, allowing AdaBoost to create an ensemble of weak learners. This ensures the boosting algorithm functions as intended by combining multiple weak classifiers to form a strong classifier, rather than relying on a single perfect-fitting tree.",
    "The current default base estimator, a DecisionTreeClassifier with no restrictions, causes AdaBoost to behave more like a single strong learner rather than an ensemble of weak learners. This behavior makes its performance more similar to that of a random forest, which also trains fully grown trees, rather than capturing the intended boosting effect.",
    "A user can modify the AdaBoostClassifier to obtain the desired boosting behavior by explicitly setting a base estimator with limited complexity, such as a DecisionTreeClassifier with a specified 'max_depth'. This can be done by passing the custom estimator to the 'base_estimator' parameter when initializing AdaBoostClassifier."
  ]
}