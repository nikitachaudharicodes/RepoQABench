{
  "repo_name": "scikit-learn_scikit-learn",
  "issue_id": "10229",
  "issue_description": "# check_array(X, dtype='numeric') should fail if X has strings\n\nCurrently, dtype='numeric' is defined as \"dtype is preserved unless array.dtype is object\". This seems overly lenient and strange behaviour, as in #9342 where @qinhanmin2014 shows that `check_array(['a', 'b', 'c'], dtype='numeric')` works without error and produces an array of strings! This behaviour is not tested and it's hard to believe that it is useful and intended. Perhaps we need a deprecation cycle, but I think dtype='numeric' should raise an error, or attempt to coerce, if the data does not actually have a numeric, real-valued dtype. ",
  "issue_comments": [
    {
      "id": 348111808,
      "user": "qinhanmin2014",
      "body": "ping @jnothman \r\n\r\n> This seems overly lenient and strange behaviour, as in #9342 where @qinhanmin2014 shows that check_array(['a', 'b', 'c'], dtype='numeric') works without error and produces an array of strings!\r\n\r\nI think you mean #9835 (https://github.com/scikit-learn/scikit-learn/pull/9835#issuecomment-348069380) ?\r\n\r\nYes, from my perspective, if it is not intended, it seems a bug."
    },
    {
      "id": 348119230,
      "user": "jnothman",
      "body": "it seems memorising four digit numbers is not my speciality\n"
    },
    {
      "id": 351103825,
      "user": "amueller",
      "body": "Five now!\r\nAnd I'm not entirely sure what my intended behavior was, but I agree with your assessment. This should error on strings."
    },
    {
      "id": 351186123,
      "user": "jnothman",
      "body": "I think, @amueller, for the next little while, the mere knowledge that a\nnumber has five digits leaves the first with distinctly low entropy\n"
    },
    {
      "id": 351186740,
      "user": "amueller",
      "body": "Well, I guess it's one more bit, though ;)"
    },
    {
      "id": 358173231,
      "user": "rtlee9",
      "body": "@jnothman I'd be happy to give this a go with some limited guidance if no one else is working on it already. Looks like the behavior you noted comes from [this line](https://github.com/scikit-learn/scikit-learn/blob/202b5321f1798c4980abf69ac8c0a0969f01a2ec/sklearn/utils/validation.py#L480), where we're checking the array against the numpy object type when we'd like to check it against string and unicode types as well -- the `[['a', 'b', 'c']]` list in your example appears to be cast to the numpy unicode array type in your example by the time it reaches that line. Sound right?"
    },
    {
      "id": 358173627,
      "user": "rtlee9",
      "body": "I'd also be curious to hear what you had in mind in terms of longer term solution, i.e., what would replace `check_array` if deprecated?\r\n> Perhaps we need a deprecation cycle"
    },
    {
      "id": 358173688,
      "user": "jnothman",
      "body": "Something like that. Basically if dtype_numeric and array.dtype is not an\nobject dtype or a numeric dtype, we should raise.\n\nOn 17 January 2018 at 13:17, Ryan <notifications@github.com> wrote:\n\n> @jnothman <https://github.com/jnothman> I'd be happy to give this a go\n> with some limited guidance if no one else is working on it already. Looks\n> like the behavior you noted comes from this line\n> <https://github.com/scikit-learn/scikit-learn/blob/202b5321f1798c4980abf69ac8c0a0969f01a2ec/sklearn/utils/validation.py#L480>,\n> where we're checking the array against the numpy object type when we'd like\n> to check it against string and unicode types as well -- the [['a', 'b',\n> 'c']] list in your example appears to be cast to the numpy unicode array\n> type in your example by the time it reaches that line. Sound right?\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/10229#issuecomment-358173231>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz69hcymywNoXaDwoNalOeRc93uF3Uks5tLVg8gaJpZM4QwJIl>\n> .\n>\n"
    },
    {
      "id": 358173909,
      "user": "jnothman",
      "body": "We wouldn't deprecate `check_array` entirely, but we would warn for two releases that \"In the future, this data with dtype('Uxx') would be rejected because it is not of a numeric dtype.\""
    },
    {
      "id": 366687867,
      "user": "SpirinEgor",
      "body": "Hi, I'm the newcomer here, can I take this? "
    },
    {
      "id": 366701416,
      "user": "lesteve",
      "body": "There is already a PR open on this one #10229, so I would encourage you to find another issue to work on.\r\n\r\nIf you are a newcomer I would encourage you to read our [contribution guidelines](https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md), in particular look at issues with the \"good first issue\" tag."
    },
    {
      "id": 366818230,
      "user": "SpirinEgor",
      "body": "Iesteve, I find this issue by this tag.\r\nAs far as I see, it was reopened, so I think, that I can help.\r\nNevertheless thank you, I will find another issue, where I can start my contributing way :)"
    },
    {
      "id": 700288777,
      "user": "zeromh",
      "body": "I'm confused about the fix here. I'm using sklearn 0.23.2, and the behavior that @jnothman called out as a problem is still the same as he described. To reproduce:\r\n\r\n```\r\narr = np.array([[1, 's'],\r\n                [1, 1]])\r\ncheck_array(arr, dtype='numeric')\r\n\r\nFutureWarning: Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers\r\n if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in \r\nscikit-learn, for example by using your_array = your_array.astype(np.float64).\r\n  return f(**kwargs)\r\n\r\narray([['1', 's'],\r\n       ['1', '1']], dtype='<U21')\r\n```\r\n\r\nNow everything's a string. It looks like the warning message was added in 2018 but the behavior was never changed. Am I missing something?"
    },
    {
      "id": 716652220,
      "user": "ogrisel",
      "body": "@zeromh I agree. We have a new opportunity to fix it in #18496."
    }
  ],
  "text_context": "# check_array(X, dtype='numeric') should fail if X has strings\n\nCurrently, dtype='numeric' is defined as \"dtype is preserved unless array.dtype is object\". This seems overly lenient and strange behaviour, as in #9342 where @qinhanmin2014 shows that `check_array(['a', 'b', 'c'], dtype='numeric')` works without error and produces an array of strings! This behaviour is not tested and it's hard to believe that it is useful and intended. Perhaps we need a deprecation cycle, but I think dtype='numeric' should raise an error, or attempt to coerce, if the data does not actually have a numeric, real-valued dtype. \n\nping @jnothman \r\n\r\n> This seems overly lenient and strange behaviour, as in #9342 where @qinhanmin2014 shows that check_array(['a', 'b', 'c'], dtype='numeric') works without error and produces an array of strings!\r\n\r\nI think you mean #9835 (https://github.com/scikit-learn/scikit-learn/pull/9835#issuecomment-348069380) ?\r\n\r\nYes, from my perspective, if it is not intended, it seems a bug.\n\nit seems memorising four digit numbers is not my speciality\n\n\nFive now!\r\nAnd I'm not entirely sure what my intended behavior was, but I agree with your assessment. This should error on strings.\n\nI think, @amueller, for the next little while, the mere knowledge that a\nnumber has five digits leaves the first with distinctly low entropy\n\n\nWell, I guess it's one more bit, though ;)\n\n@jnothman I'd be happy to give this a go with some limited guidance if no one else is working on it already. Looks like the behavior you noted comes from [this line](https://github.com/scikit-learn/scikit-learn/blob/202b5321f1798c4980abf69ac8c0a0969f01a2ec/sklearn/utils/validation.py#L480), where we're checking the array against the numpy object type when we'd like to check it against string and unicode types as well -- the `[['a', 'b', 'c']]` list in your example appears to be cast to the numpy unicode array type in your example by the time it reaches that line. Sound right?\n\nI'd also be curious to hear what you had in mind in terms of longer term solution, i.e., what would replace `check_array` if deprecated?\r\n> Perhaps we need a deprecation cycle\n\nSomething like that. Basically if dtype_numeric and array.dtype is not an\nobject dtype or a numeric dtype, we should raise.\n\nOn 17 January 2018 at 13:17, Ryan <notifications@github.com> wrote:\n\n> @jnothman <https://github.com/jnothman> I'd be happy to give this a go\n> with some limited guidance if no one else is working on it already. Looks\n> like the behavior you noted comes from this line\n> <https://github.com/scikit-learn/scikit-learn/blob/202b5321f1798c4980abf69ac8c0a0969f01a2ec/sklearn/utils/validation.py#L480>,\n> where we're checking the array against the numpy object type when we'd like\n> to check it against string and unicode types as well -- the [['a', 'b',\n> 'c']] list in your example appears to be cast to the numpy unicode array\n> type in your example by the time it reaches that line. Sound right?\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/10229#issuecomment-358173231>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz69hcymywNoXaDwoNalOeRc93uF3Uks5tLVg8gaJpZM4QwJIl>\n> .\n>\n\n\nWe wouldn't deprecate `check_array` entirely, but we would warn for two releases that \"In the future, this data with dtype('Uxx') would be rejected because it is not of a numeric dtype.\"\n\nHi, I'm the newcomer here, can I take this? \n\nThere is already a PR open on this one #10229, so I would encourage you to find another issue to work on.\r\n\r\nIf you are a newcomer I would encourage you to read our [contribution guidelines](https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md), in particular look at issues with the \"good first issue\" tag.\n\nIesteve, I find this issue by this tag.\r\nAs far as I see, it was reopened, so I think, that I can help.\r\nNevertheless thank you, I will find another issue, where I can start my contributing way :)\n\nI'm confused about the fix here. I'm using sklearn 0.23.2, and the behavior that @jnothman called out as a problem is still the same as he described. To reproduce:\r\n\r\n```\r\narr = np.array([[1, 's'],\r\n                [1, 1]])\r\ncheck_array(arr, dtype='numeric')\r\n\r\nFutureWarning: Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers\r\n if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in \r\nscikit-learn, for example by using your_array = your_array.astype(np.float64).\r\n  return f(**kwargs)\r\n\r\narray([['1', 's'],\r\n       ['1', '1']], dtype='<U21')\r\n```\r\n\r\nNow everything's a string. It looks like the warning message was added in 2018 but the behavior was never changed. Am I missing something?\n\n@zeromh I agree. We have a new opportunity to fix it in #18496.",
  "pr_link": "https://github.com/scikit-learn/scikit-learn/pull/9835",
  "code_context": [
    {
      "filename": "sklearn/dummy.py",
      "content": "# Author: Mathieu Blondel <mathieu@mblondel.org>\n#         Arnaud Joly <a.joly@ulg.ac.be>\n#         Maheshakya Wijewardena <maheshakya.10@cse.mrt.ac.lk>\n# License: BSD 3 clause\nfrom __future__ import division\n\nimport warnings\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom .base import BaseEstimator, ClassifierMixin, RegressorMixin\nfrom .utils import check_random_state\nfrom .utils.validation import _num_samples\nfrom .utils.validation import check_array\nfrom .utils.validation import check_consistent_length\nfrom .utils.validation import check_is_fitted\nfrom .utils.random import random_choice_csc\nfrom .utils.stats import _weighted_percentile\nfrom .utils.multiclass import class_distribution\n\n\nclass DummyClassifier(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    DummyClassifier is a classifier that makes predictions using simple rules.\n\n    This classifier is useful as a simple baseline to compare with other\n    (real) classifiers. Do not use it for real problems.\n\n    Read more in the :ref:`User Guide <dummy_estimators>`.\n\n    Parameters\n    ----------\n    strategy : str, default=\"stratified\"\n        Strategy to use to generate predictions.\n\n        * \"stratified\": generates predictions by respecting the training\n          set's class distribution.\n        * \"most_frequent\": always predicts the most frequent label in the\n          training set.\n        * \"prior\": always predicts the class that maximizes the class prior\n          (like \"most_frequent\") and ``predict_proba`` returns the class prior.\n        * \"uniform\": generates predictions uniformly at random.\n        * \"constant\": always predicts a constant label that is provided by\n          the user. This is useful for metrics that evaluate a non-majority\n          class\n\n          .. versionadded:: 0.17\n             Dummy Classifier now supports prior fitting strategy using\n             parameter *prior*.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    constant : int or str or array of shape = [n_outputs]\n        The explicit constant as predicted by the \"constant\" strategy. This\n        parameter is useful only for the \"constant\" strategy.\n\n    Attributes\n    ----------\n    classes_ : array or list of array of shape = [n_classes]\n        Class labels for each output.\n\n    n_classes_ : array or list of array of shape = [n_classes]\n        Number of label for each output.\n\n    class_prior_ : array or list of array of shape = [n_classes]\n        Probability of each class for each output.\n\n    n_outputs_ : int,\n        Number of outputs.\n\n    outputs_2d_ : bool,\n        True if the output at fit is 2d, else false.\n\n    sparse_output_ : bool,\n        True if the array returned from predict is to be in sparse CSC format.\n        Is automatically set to True if the input y is passed in sparse format.\n\n    \"\"\"\n\n    def __init__(self, strategy=\"stratified\", random_state=None,\n                 constant=None):\n        self.strategy = strategy\n        self.random_state = random_state\n        self.constant = constant\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the random classifier.\n\n        Parameters\n        ----------\n        X : {array-like, object with finite length or shape}\n            Training data, requires length = n_samples\n\n        y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n            Target values.\n\n        sample_weight : array-like of shape = [n_samples], optional\n            Sample weights.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        if self.strategy not in (\"most_frequent\", \"stratified\", \"uniform\",\n                                 \"constant\", \"prior\"):\n            raise ValueError(\"Unknown strategy type.\")\n\n        if self.strategy == \"uniform\" and sp.issparse(y):\n            y = y.toarray()\n            warnings.warn('A local copy of the target data has been converted '\n                          'to a numpy array. Predicting on sparse target data '\n                          'with the uniform strategy would not save memory '\n                          'and would be slower.',\n                          UserWarning)\n\n        self.sparse_output_ = sp.issparse(y)\n\n        if not self.sparse_output_:\n            y = np.atleast_1d(y)\n\n        self.output_2d_ = y.ndim == 2\n        if y.ndim == 1:\n            y = np.reshape(y, (-1, 1))\n\n        self.n_outputs_ = y.shape[1]\n\n        check_consistent_length(X, y, sample_weight)\n\n        if self.strategy == \"constant\":\n            if self.constant is None:\n                raise ValueError(\"Constant target value has to be specified \"\n                                 \"when the constant strategy is used.\")\n            else:\n                constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))\n                if constant.shape[0] != self.n_outputs_:\n                    raise ValueError(\"Constant target value should have \"\n                                     \"shape (%d, 1).\" % self.n_outputs_)\n\n        (self.classes_,\n         self.n_classes_,\n         self.class_prior_) = class_distribution(y, sample_weight)\n\n        if (self.strategy == \"constant\" and\n                any(constant[k] not in self.classes_[k]\n                    for k in range(self.n_outputs_))):\n            # Checking in case of constant strategy if the constant\n            # provided by the user is in y.\n            raise ValueError(\"The constant target value must be \"\n                             \"present in training data\")\n\n        if self.n_outputs_ == 1 and not self.output_2d_:\n            self.n_classes_ = self.n_classes_[0]\n            self.classes_ = self.classes_[0]\n            self.class_prior_ = self.class_prior_[0]\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Perform classification on test vectors X.\n\n        Parameters\n        ----------\n        X : {array-like, object with finite length or shape}\n            Training data, requires length = n_samples\n\n        Returns\n        -------\n        y : array, shape = [n_samples] or [n_samples, n_outputs]\n            Predicted target values for X.\n        \"\"\"\n        check_is_fitted(self, 'classes_')\n\n        # numpy random_state expects Python int and not long as size argument\n        # under Windows\n        n_samples = _num_samples(X)\n        rs = check_random_state(self.random_state)\n\n        n_classes_ = self.n_classes_\n        classes_ = self.classes_\n        class_prior_ = self.class_prior_\n        constant = self.constant\n        if self.n_outputs_ == 1:\n            # Get same type even for self.n_outputs_ == 1\n            n_classes_ = [n_classes_]\n            classes_ = [classes_]\n            class_prior_ = [class_prior_]\n            constant = [constant]\n        # Compute probability only once\n        if self.strategy == \"stratified\":\n            proba = self.predict_proba(X)\n            if self.n_outputs_ == 1:\n                proba = [proba]\n\n        if self.sparse_output_:\n            class_prob = None\n            if self.strategy in (\"most_frequent\", \"prior\"):\n                classes_ = [np.array([cp.argmax()]) for cp in class_prior_]\n\n            elif self.strategy == \"stratified\":\n                class_prob = class_prior_\n\n            elif self.strategy == \"uniform\":\n                raise ValueError(\"Sparse target prediction is not \"\n                                 \"supported with the uniform strategy\")\n\n            elif self.strategy == \"constant\":\n                classes_ = [np.array([c]) for c in constant]\n\n            y = random_choice_csc(n_samples, classes_, class_prob,\n                                  self.random_state)\n        else:\n            if self.strategy in (\"most_frequent\", \"prior\"):\n                y = np.tile([classes_[k][class_prior_[k].argmax()] for\n                             k in range(self.n_outputs_)], [n_samples, 1])\n\n            elif self.strategy == \"stratified\":\n                y = np.vstack(classes_[k][proba[k].argmax(axis=1)] for\n                              k in range(self.n_outputs_)).T\n\n            elif self.strategy == \"uniform\":\n                ret = [classes_[k][rs.randint(n_classes_[k], size=n_samples)]\n                       for k in range(self.n_outputs_)]\n                y = np.vstack(ret).T\n\n            elif self.strategy == \"constant\":\n                y = np.tile(self.constant, (n_samples, 1))\n\n            if self.n_outputs_ == 1 and not self.output_2d_:\n                y = np.ravel(y)\n\n        return y\n\n    def predict_proba(self, X):\n        \"\"\"\n        Return probability estimates for the test vectors X.\n\n        Parameters\n        ----------\n        X : {array-like, object with finite length or shape}\n            Training data, requires length = n_samples\n\n        Returns\n        -------\n        P : array-like or list of array-lke of shape = [n_samples, n_classes]\n            Returns the probability of the sample for each class in\n            the model, where classes are ordered arithmetically, for each\n            output.\n        \"\"\"\n        check_is_fitted(self, 'classes_')\n\n        # numpy random_state expects Python int and not long as size argument\n        # under Windows\n        n_samples = _num_samples(X)\n        rs = check_random_state(self.random_state)\n\n        n_classes_ = self.n_classes_\n        classes_ = self.classes_\n        class_prior_ = self.class_prior_\n        constant = self.constant\n        if self.n_outputs_ == 1 and not self.output_2d_:\n            # Get same type even for self.n_outputs_ == 1\n            n_classes_ = [n_classes_]\n            classes_ = [classes_]\n            class_prior_ = [class_prior_]\n            constant = [constant]\n\n        P = []\n        for k in range(self.n_outputs_):\n            if self.strategy == \"most_frequent\":\n                ind = class_prior_[k].argmax()\n                out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)\n                out[:, ind] = 1.0\n            elif self.strategy == \"prior\":\n                out = np.ones((n_samples, 1)) * class_prior_[k]\n\n            elif self.strategy == \"stratified\":\n                out = rs.multinomial(1, class_prior_[k], size=n_samples)\n\n            elif self.strategy == \"uniform\":\n                out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)\n                out /= n_classes_[k]\n\n            elif self.strategy == \"constant\":\n                ind = np.where(classes_[k] == constant[k])\n                out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)\n                out[:, ind] = 1.0\n\n            P.append(out)\n\n        if self.n_outputs_ == 1 and not self.output_2d_:\n            P = P[0]\n\n        return P\n\n    def predict_log_proba(self, X):\n        \"\"\"\n        Return log probability estimates for the test vectors X.\n\n        Parameters\n        ----------\n        X : {array-like, object with finite length or shape}\n            Training data, requires length = n_samples\n\n        Returns\n        -------\n        P : array-like or list of array-like of shape = [n_samples, n_classes]\n            Returns the log probability of the sample for each class in\n            the model, where classes are ordered arithmetically for each\n            output.\n        \"\"\"\n        proba = self.predict_proba(X)\n        if self.n_outputs_ == 1:\n            return np.log(proba)\n        else:\n            return [np.log(p) for p in proba]\n\n\nclass DummyRegressor(BaseEstimator, RegressorMixin):\n    \"\"\"\n    DummyRegressor is a regressor that makes predictions using\n    simple rules.\n\n    This regressor is useful as a simple baseline to compare with other\n    (real) regressors. Do not use it for real problems.\n\n    Read more in the :ref:`User Guide <dummy_estimators>`.\n\n    Parameters\n    ----------\n    strategy : str\n        Strategy to use to generate predictions.\n\n        * \"mean\": always predicts the mean of the training set\n        * \"median\": always predicts the median of the training set\n        * \"quantile\": always predicts a specified quantile of the training set,\n          provided with the quantile parameter.\n        * \"constant\": always predicts a constant value that is provided by\n          the user.\n\n    constant : int or float or array of shape = [n_outputs]\n        The explicit constant as predicted by the \"constant\" strategy. This\n        parameter is useful only for the \"constant\" strategy.\n\n    quantile : float in [0.0, 1.0]\n        The quantile to predict using the \"quantile\" strategy. A quantile of\n        0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the\n        maximum.\n\n    Attributes\n    ----------\n    constant_ : float or array of shape [n_outputs]\n        Mean or median or quantile of the training targets or constant value\n        given by the user.\n\n    n_outputs_ : int,\n        Number of outputs.\n\n    outputs_2d_ : bool,\n        True if the output at fit is 2d, else false.\n    \"\"\"\n\n    def __init__(self, strategy=\"mean\", constant=None, quantile=None):\n        self.strategy = strategy\n        self.constant = constant\n        self.quantile = quantile\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the random regressor.\n\n        Parameters\n        ----------\n        X : {array-like, object with finite length or shape}\n            Training data, requires length = n_samples\n\n        y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n            Target values.\n\n        sample_weight : array-like of shape = [n_samples], optional\n            Sample weights.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        if self.strategy not in (\"mean\", \"median\", \"quantile\", \"constant\"):\n            raise ValueError(\"Unknown strategy type: %s, expected \"\n                             \"'mean', 'median', 'quantile' or 'constant'\"\n                             % self.strategy)\n\n        y = check_array(y, ensure_2d=False)\n        if len(y) == 0:\n            raise ValueError(\"y must not be empty.\")\n\n        self.output_2d_ = y.ndim == 2\n        if y.ndim == 1:\n            y = np.reshape(y, (-1, 1))\n        self.n_outputs_ = y.shape[1]\n\n        check_consistent_length(X, y, sample_weight)\n\n        if self.strategy == \"mean\":\n            self.constant_ = np.average(y, axis=0, weights=sample_weight)\n\n        elif self.strategy == \"median\":\n            if sample_weight is None:\n                self.constant_ = np.median(y, axis=0)\n            else:\n                self.constant_ = [_weighted_percentile(y[:, k], sample_weight,\n                                                       percentile=50.)\n                                  for k in range(self.n_outputs_)]\n\n        elif self.strategy == \"quantile\":\n            if self.quantile is None or not np.isscalar(self.quantile):\n                raise ValueError(\"Quantile must be a scalar in the range \"\n                                 \"[0.0, 1.0], but got %s.\" % self.quantile)\n\n            percentile = self.quantile * 100.0\n            if sample_weight is None:\n                self.constant_ = np.percentile(y, axis=0, q=percentile)\n            else:\n                self.constant_ = [_weighted_percentile(y[:, k], sample_weight,\n                                                       percentile=percentile)\n                                  for k in range(self.n_outputs_)]\n\n        elif self.strategy == \"constant\":\n            if self.constant is None:\n                raise TypeError(\"Constant target value has to be specified \"\n                                \"when the constant strategy is used.\")\n\n            self.constant = check_array(self.constant,\n                                        accept_sparse=['csr', 'csc', 'coo'],\n                                        ensure_2d=False, ensure_min_samples=0)\n\n            if self.output_2d_ and self.constant.shape[0] != y.shape[1]:\n                raise ValueError(\n                    \"Constant target value should have \"\n                    \"shape (%d, 1).\" % y.shape[1])\n\n            self.constant_ = self.constant\n\n        self.constant_ = np.reshape(self.constant_, (1, -1))\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Perform classification on test vectors X.\n\n        Parameters\n        ----------\n        X : {array-like, object with finite length or shape}\n            Training data, requires length = n_samples\n\n        Returns\n        -------\n        y : array, shape = [n_samples]  or [n_samples, n_outputs]\n            Predicted target values for X.\n        \"\"\"\n        check_is_fitted(self, \"constant_\")\n        n_samples = _num_samples(X)\n\n        y = np.ones((n_samples, 1)) * self.constant_\n\n        if self.n_outputs_ == 1 and not self.output_2d_:\n            y = np.ravel(y)\n\n        return y\n"
    },
    {
      "filename": "sklearn/tests/test_dummy.py",
      "content": "from __future__ import division\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom sklearn.base import clone\nfrom sklearn.utils.testing import assert_array_equal\nfrom sklearn.utils.testing import assert_array_almost_equal\nfrom sklearn.utils.testing import assert_equal\nfrom sklearn.utils.testing import assert_almost_equal\nfrom sklearn.utils.testing import assert_raises\nfrom sklearn.utils.testing import assert_true\nfrom sklearn.utils.testing import assert_warns_message\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.utils.stats import _weighted_percentile\n\nfrom sklearn.dummy import DummyClassifier, DummyRegressor\n\n\n@ignore_warnings\ndef _check_predict_proba(clf, X, y):\n    proba = clf.predict_proba(X)\n    # We know that we can have division by zero\n    log_proba = clf.predict_log_proba(X)\n\n    y = np.atleast_1d(y)\n    if y.ndim == 1:\n        y = np.reshape(y, (-1, 1))\n\n    n_outputs = y.shape[1]\n    n_samples = len(X)\n\n    if n_outputs == 1:\n        proba = [proba]\n        log_proba = [log_proba]\n\n    for k in range(n_outputs):\n        assert_equal(proba[k].shape[0], n_samples)\n        assert_equal(proba[k].shape[1], len(np.unique(y[:, k])))\n        assert_array_almost_equal(proba[k].sum(axis=1), np.ones(len(X)))\n        # We know that we can have division by zero\n        assert_array_almost_equal(np.log(proba[k]), log_proba[k])\n\n\ndef _check_behavior_2d(clf):\n    # 1d case\n    X = np.array([[0], [0], [0], [0]])  # ignored\n    y = np.array([1, 2, 1, 1])\n    est = clone(clf)\n    est.fit(X, y)\n    y_pred = est.predict(X)\n    assert_equal(y.shape, y_pred.shape)\n\n    # 2d case\n    y = np.array([[1, 0],\n                  [2, 0],\n                  [1, 0],\n                  [1, 3]])\n    est = clone(clf)\n    est.fit(X, y)\n    y_pred = est.predict(X)\n    assert_equal(y.shape, y_pred.shape)\n\n\ndef _check_behavior_2d_for_constant(clf):\n    # 2d case only\n    X = np.array([[0], [0], [0], [0]])  # ignored\n    y = np.array([[1, 0, 5, 4, 3],\n                  [2, 0, 1, 2, 5],\n                  [1, 0, 4, 5, 2],\n                  [1, 3, 3, 2, 0]])\n    est = clone(clf)\n    est.fit(X, y)\n    y_pred = est.predict(X)\n    assert_equal(y.shape, y_pred.shape)\n\n\ndef _check_equality_regressor(statistic, y_learn, y_pred_learn,\n                              y_test, y_pred_test):\n    assert_array_almost_equal(np.tile(statistic, (y_learn.shape[0], 1)),\n                              y_pred_learn)\n    assert_array_almost_equal(np.tile(statistic, (y_test.shape[0], 1)),\n                              y_pred_test)\n\n\ndef test_most_frequent_and_prior_strategy():\n    X = [[0], [0], [0], [0]]  # ignored\n    y = [1, 2, 1, 1]\n\n    for strategy in (\"most_frequent\", \"prior\"):\n        clf = DummyClassifier(strategy=strategy, random_state=0)\n        clf.fit(X, y)\n        assert_array_equal(clf.predict(X), np.ones(len(X)))\n        _check_predict_proba(clf, X, y)\n\n        if strategy == \"prior\":\n            assert_array_almost_equal(clf.predict_proba([X[0]]),\n                                      clf.class_prior_.reshape((1, -1)))\n        else:\n            assert_array_almost_equal(clf.predict_proba([X[0]]),\n                                      clf.class_prior_.reshape((1, -1)) > 0.5)\n\n\ndef test_most_frequent_and_prior_strategy_multioutput():\n    X = [[0], [0], [0], [0]]  # ignored\n    y = np.array([[1, 0],\n                  [2, 0],\n                  [1, 0],\n                  [1, 3]])\n\n    n_samples = len(X)\n\n    for strategy in (\"prior\", \"most_frequent\"):\n        clf = DummyClassifier(strategy=strategy, random_state=0)\n        clf.fit(X, y)\n        assert_array_equal(clf.predict(X),\n                           np.hstack([np.ones((n_samples, 1)),\n                                      np.zeros((n_samples, 1))]))\n        _check_predict_proba(clf, X, y)\n        _check_behavior_2d(clf)\n\n\ndef test_stratified_strategy():\n    X = [[0]] * 5  # ignored\n    y = [1, 2, 1, 1, 2]\n    clf = DummyClassifier(strategy=\"stratified\", random_state=0)\n    clf.fit(X, y)\n\n    X = [[0]] * 500\n    y_pred = clf.predict(X)\n    p = np.bincount(y_pred) / float(len(X))\n    assert_almost_equal(p[1], 3. / 5, decimal=1)\n    assert_almost_equal(p[2], 2. / 5, decimal=1)\n    _check_predict_proba(clf, X, y)\n\n\ndef test_stratified_strategy_multioutput():\n    X = [[0]] * 5  # ignored\n    y = np.array([[2, 1],\n                  [2, 2],\n                  [1, 1],\n                  [1, 2],\n                  [1, 1]])\n\n    clf = DummyClassifier(strategy=\"stratified\", random_state=0)\n    clf.fit(X, y)\n\n    X = [[0]] * 500\n    y_pred = clf.predict(X)\n\n    for k in range(y.shape[1]):\n        p = np.bincount(y_pred[:, k]) / float(len(X))\n        assert_almost_equal(p[1], 3. / 5, decimal=1)\n        assert_almost_equal(p[2], 2. / 5, decimal=1)\n        _check_predict_proba(clf, X, y)\n\n    _check_behavior_2d(clf)\n\n\ndef test_uniform_strategy():\n    X = [[0]] * 4  # ignored\n    y = [1, 2, 1, 1]\n    clf = DummyClassifier(strategy=\"uniform\", random_state=0)\n    clf.fit(X, y)\n\n    X = [[0]] * 500\n    y_pred = clf.predict(X)\n    p = np.bincount(y_pred) / float(len(X))\n    assert_almost_equal(p[1], 0.5, decimal=1)\n    assert_almost_equal(p[2], 0.5, decimal=1)\n    _check_predict_proba(clf, X, y)\n\n\ndef test_uniform_strategy_multioutput():\n    X = [[0]] * 4  # ignored\n    y = np.array([[2, 1],\n                  [2, 2],\n                  [1, 2],\n                  [1, 1]])\n    clf = DummyClassifier(strategy=\"uniform\", random_state=0)\n    clf.fit(X, y)\n\n    X = [[0]] * 500\n    y_pred = clf.predict(X)\n\n    for k in range(y.shape[1]):\n        p = np.bincount(y_pred[:, k]) / float(len(X))\n        assert_almost_equal(p[1], 0.5, decimal=1)\n        assert_almost_equal(p[2], 0.5, decimal=1)\n        _check_predict_proba(clf, X, y)\n\n    _check_behavior_2d(clf)\n\n\ndef test_string_labels():\n    X = [[0]] * 5\n    y = [\"paris\", \"paris\", \"tokyo\", \"amsterdam\", \"berlin\"]\n    clf = DummyClassifier(strategy=\"most_frequent\")\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X), [\"paris\"] * 5)\n\n\ndef test_classifier_exceptions():\n    clf = DummyClassifier(strategy=\"unknown\")\n    assert_raises(ValueError, clf.fit, [], [])\n\n    assert_raises(ValueError, clf.predict, [])\n    assert_raises(ValueError, clf.predict_proba, [])\n\n\ndef test_mean_strategy_regressor():\n\n    random_state = np.random.RandomState(seed=1)\n\n    X = [[0]] * 4  # ignored\n    y = random_state.randn(4)\n\n    reg = DummyRegressor()\n    reg.fit(X, y)\n    assert_array_equal(reg.predict(X), [np.mean(y)] * len(X))\n\n\ndef test_mean_strategy_multioutput_regressor():\n\n    random_state = np.random.RandomState(seed=1)\n\n    X_learn = random_state.randn(10, 10)\n    y_learn = random_state.randn(10, 5)\n\n    mean = np.mean(y_learn, axis=0).reshape((1, -1))\n\n    X_test = random_state.randn(20, 10)\n    y_test = random_state.randn(20, 5)\n\n    # Correctness oracle\n    est = DummyRegressor()\n    est.fit(X_learn, y_learn)\n    y_pred_learn = est.predict(X_learn)\n    y_pred_test = est.predict(X_test)\n\n    _check_equality_regressor(mean, y_learn, y_pred_learn, y_test, y_pred_test)\n    _check_behavior_2d(est)\n\n\ndef test_regressor_exceptions():\n    reg = DummyRegressor()\n    assert_raises(ValueError, reg.predict, [])\n\n\ndef test_median_strategy_regressor():\n\n    random_state = np.random.RandomState(seed=1)\n\n    X = [[0]] * 5  # ignored\n    y = random_state.randn(5)\n\n    reg = DummyRegressor(strategy=\"median\")\n    reg.fit(X, y)\n    assert_array_equal(reg.predict(X), [np.median(y)] * len(X))\n\n\ndef test_median_strategy_multioutput_regressor():\n\n    random_state = np.random.RandomState(seed=1)\n\n    X_learn = random_state.randn(10, 10)\n    y_learn = random_state.randn(10, 5)\n\n    median = np.median(y_learn, axis=0).reshape((1, -1))\n\n    X_test = random_state.randn(20, 10)\n    y_test = random_state.randn(20, 5)\n\n    # Correctness oracle\n    est = DummyRegressor(strategy=\"median\")\n    est.fit(X_learn, y_learn)\n    y_pred_learn = est.predict(X_learn)\n    y_pred_test = est.predict(X_test)\n\n    _check_equality_regressor(\n        median, y_learn, y_pred_learn, y_test, y_pred_test)\n    _check_behavior_2d(est)\n\n\ndef test_quantile_strategy_regressor():\n\n    random_state = np.random.RandomState(seed=1)\n\n    X = [[0]] * 5  # ignored\n    y = random_state.randn(5)\n\n    reg = DummyRegressor(strategy=\"quantile\", quantile=0.5)\n    reg.fit(X, y)\n    assert_array_equal(reg.predict(X), [np.median(y)] * len(X))\n\n    reg = DummyRegressor(strategy=\"quantile\", quantile=0)\n    reg.fit(X, y)\n    assert_array_equal(reg.predict(X), [np.min(y)] * len(X))\n\n    reg = DummyRegressor(strategy=\"quantile\", quantile=1)\n    reg.fit(X, y)\n    assert_array_equal(reg.predict(X), [np.max(y)] * len(X))\n\n    reg = DummyRegressor(strategy=\"quantile\", quantile=0.3)\n    reg.fit(X, y)\n    assert_array_equal(reg.predict(X), [np.percentile(y, q=30)] * len(X))\n\n\ndef test_quantile_strategy_multioutput_regressor():\n\n    random_state = np.random.RandomState(seed=1)\n\n    X_learn = random_state.randn(10, 10)\n    y_learn = random_state.randn(10, 5)\n\n    median = np.median(y_learn, axis=0).reshape((1, -1))\n    quantile_values = np.percentile(y_learn, axis=0, q=80).reshape((1, -1))\n\n    X_test = random_state.randn(20, 10)\n    y_test = random_state.randn(20, 5)\n\n    # Correctness oracle\n    est = DummyRegressor(strategy=\"quantile\", quantile=0.5)\n    est.fit(X_learn, y_learn)\n    y_pred_learn = est.predict(X_learn)\n    y_pred_test = est.predict(X_test)\n\n    _check_equality_regressor(\n        median, y_learn, y_pred_learn, y_test, y_pred_test)\n    _check_behavior_2d(est)\n\n    # Correctness oracle\n    est = DummyRegressor(strategy=\"quantile\", quantile=0.8)\n    est.fit(X_learn, y_learn)\n    y_pred_learn = est.predict(X_learn)\n    y_pred_test = est.predict(X_test)\n\n    _check_equality_regressor(\n        quantile_values, y_learn, y_pred_learn, y_test, y_pred_test)\n    _check_behavior_2d(est)\n\n\ndef test_quantile_invalid():\n\n    X = [[0]] * 5  # ignored\n    y = [0] * 5  # ignored\n\n    est = DummyRegressor(strategy=\"quantile\")\n    assert_raises(ValueError, est.fit, X, y)\n\n    est = DummyRegressor(strategy=\"quantile\", quantile=None)\n    assert_raises(ValueError, est.fit, X, y)\n\n    est = DummyRegressor(strategy=\"quantile\", quantile=[0])\n    assert_raises(ValueError, est.fit, X, y)\n\n    est = DummyRegressor(strategy=\"quantile\", quantile=-0.1)\n    assert_raises(ValueError, est.fit, X, y)\n\n    est = DummyRegressor(strategy=\"quantile\", quantile=1.1)\n    assert_raises(ValueError, est.fit, X, y)\n\n    est = DummyRegressor(strategy=\"quantile\", quantile='abc')\n    assert_raises(TypeError, est.fit, X, y)\n\n\ndef test_quantile_strategy_empty_train():\n    est = DummyRegressor(strategy=\"quantile\", quantile=0.4)\n    assert_raises(ValueError, est.fit, [], [])\n\n\ndef test_constant_strategy_regressor():\n\n    random_state = np.random.RandomState(seed=1)\n\n    X = [[0]] * 5  # ignored\n    y = random_state.randn(5)\n\n    reg = DummyRegressor(strategy=\"constant\", constant=[43])\n    reg.fit(X, y)\n    assert_array_equal(reg.predict(X), [43] * len(X))\n\n    reg = DummyRegressor(strategy=\"constant\", constant=43)\n    reg.fit(X, y)\n    assert_array_equal(reg.predict(X), [43] * len(X))\n\n\ndef test_constant_strategy_multioutput_regressor():\n\n    random_state = np.random.RandomState(seed=1)\n\n    X_learn = random_state.randn(10, 10)\n    y_learn = random_state.randn(10, 5)\n\n    # test with 2d array\n    constants = random_state.randn(5)\n\n    X_test = random_state.randn(20, 10)\n    y_test = random_state.randn(20, 5)\n\n    # Correctness oracle\n    est = DummyRegressor(strategy=\"constant\", constant=constants)\n    est.fit(X_learn, y_learn)\n    y_pred_learn = est.predict(X_learn)\n    y_pred_test = est.predict(X_test)\n\n    _check_equality_regressor(\n        constants, y_learn, y_pred_learn, y_test, y_pred_test)\n    _check_behavior_2d_for_constant(est)\n\n\ndef test_y_mean_attribute_regressor():\n    X = [[0]] * 5\n    y = [1, 2, 4, 6, 8]\n    # when strategy = 'mean'\n    est = DummyRegressor(strategy='mean')\n    est.fit(X, y)\n\n    assert_equal(est.constant_, np.mean(y))\n\n\ndef test_unknown_strategey_regressor():\n    X = [[0]] * 5\n    y = [1, 2, 4, 6, 8]\n\n    est = DummyRegressor(strategy='gona')\n    assert_raises(ValueError, est.fit, X, y)\n\n\ndef test_constants_not_specified_regressor():\n    X = [[0]] * 5\n    y = [1, 2, 4, 6, 8]\n\n    est = DummyRegressor(strategy='constant')\n    assert_raises(TypeError, est.fit, X, y)\n\n\ndef test_constant_size_multioutput_regressor():\n    random_state = np.random.RandomState(seed=1)\n    X = random_state.randn(10, 10)\n    y = random_state.randn(10, 5)\n\n    est = DummyRegressor(strategy='constant', constant=[1, 2, 3, 4])\n    assert_raises(ValueError, est.fit, X, y)\n\n\ndef test_constant_strategy():\n    X = [[0], [0], [0], [0]]  # ignored\n    y = [2, 1, 2, 2]\n\n    clf = DummyClassifier(strategy=\"constant\", random_state=0, constant=1)\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X), np.ones(len(X)))\n    _check_predict_proba(clf, X, y)\n\n    X = [[0], [0], [0], [0]]  # ignored\n    y = ['two', 'one', 'two', 'two']\n    clf = DummyClassifier(strategy=\"constant\", random_state=0, constant='one')\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X), np.array(['one'] * 4))\n    _check_predict_proba(clf, X, y)\n\n\ndef test_constant_strategy_multioutput():\n    X = [[0], [0], [0], [0]]  # ignored\n    y = np.array([[2, 3],\n                  [1, 3],\n                  [2, 3],\n                  [2, 0]])\n\n    n_samples = len(X)\n\n    clf = DummyClassifier(strategy=\"constant\", random_state=0,\n                          constant=[1, 0])\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X),\n                       np.hstack([np.ones((n_samples, 1)),\n                                  np.zeros((n_samples, 1))]))\n    _check_predict_proba(clf, X, y)\n\n\ndef test_constant_strategy_exceptions():\n    X = [[0], [0], [0], [0]]  # ignored\n    y = [2, 1, 2, 2]\n    clf = DummyClassifier(strategy=\"constant\", random_state=0)\n    assert_raises(ValueError, clf.fit, X, y)\n    clf = DummyClassifier(strategy=\"constant\", random_state=0,\n                          constant=[2, 0])\n    assert_raises(ValueError, clf.fit, X, y)\n\n\ndef test_classification_sample_weight():\n    X = [[0], [0], [1]]\n    y = [0, 1, 0]\n    sample_weight = [0.1, 1., 0.1]\n\n    clf = DummyClassifier().fit(X, y, sample_weight)\n    assert_array_almost_equal(clf.class_prior_, [0.2 / 1.2, 1. / 1.2])\n\n\ndef test_constant_strategy_sparse_target():\n    X = [[0]] * 5  # ignored\n    y = sp.csc_matrix(np.array([[0, 1],\n                                [4, 0],\n                                [1, 1],\n                                [1, 4],\n                                [1, 1]]))\n\n    n_samples = len(X)\n\n    clf = DummyClassifier(strategy=\"constant\", random_state=0, constant=[1, 0])\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert_true(sp.issparse(y_pred))\n    assert_array_equal(y_pred.toarray(), np.hstack([np.ones((n_samples, 1)),\n                                                    np.zeros((n_samples, 1))]))\n\n\ndef test_uniform_strategy_sparse_target_warning():\n    X = [[0]] * 5  # ignored\n    y = sp.csc_matrix(np.array([[2, 1],\n                                [2, 2],\n                                [1, 4],\n                                [4, 2],\n                                [1, 1]]))\n\n    clf = DummyClassifier(strategy=\"uniform\", random_state=0)\n    assert_warns_message(UserWarning,\n                         \"the uniform strategy would not save memory\",\n                         clf.fit, X, y)\n\n    X = [[0]] * 500\n    y_pred = clf.predict(X)\n\n    for k in range(y.shape[1]):\n        p = np.bincount(y_pred[:, k]) / float(len(X))\n        assert_almost_equal(p[1], 1 / 3, decimal=1)\n        assert_almost_equal(p[2], 1 / 3, decimal=1)\n        assert_almost_equal(p[4], 1 / 3, decimal=1)\n\n\ndef test_stratified_strategy_sparse_target():\n    X = [[0]] * 5  # ignored\n    y = sp.csc_matrix(np.array([[4, 1],\n                                [0, 0],\n                                [1, 1],\n                                [1, 4],\n                                [1, 1]]))\n\n    clf = DummyClassifier(strategy=\"stratified\", random_state=0)\n    clf.fit(X, y)\n\n    X = [[0]] * 500\n    y_pred = clf.predict(X)\n    assert_true(sp.issparse(y_pred))\n    y_pred = y_pred.toarray()\n\n    for k in range(y.shape[1]):\n        p = np.bincount(y_pred[:, k]) / float(len(X))\n        assert_almost_equal(p[1], 3. / 5, decimal=1)\n        assert_almost_equal(p[0], 1. / 5, decimal=1)\n        assert_almost_equal(p[4], 1. / 5, decimal=1)\n\n\ndef test_most_frequent_and_prior_strategy_sparse_target():\n    X = [[0]] * 5  # ignored\n    y = sp.csc_matrix(np.array([[1, 0],\n                                [1, 3],\n                                [4, 0],\n                                [0, 1],\n                                [1, 0]]))\n\n    n_samples = len(X)\n    y_expected = np.hstack([np.ones((n_samples, 1)), np.zeros((n_samples, 1))])\n    for strategy in (\"most_frequent\", \"prior\"):\n        clf = DummyClassifier(strategy=strategy, random_state=0)\n        clf.fit(X, y)\n\n        y_pred = clf.predict(X)\n        assert_true(sp.issparse(y_pred))\n        assert_array_equal(y_pred.toarray(), y_expected)\n\n\ndef test_dummy_regressor_sample_weight(n_samples=10):\n    random_state = np.random.RandomState(seed=1)\n\n    X = [[0]] * n_samples\n    y = random_state.rand(n_samples)\n    sample_weight = random_state.rand(n_samples)\n\n    est = DummyRegressor(strategy=\"mean\").fit(X, y, sample_weight)\n    assert_equal(est.constant_, np.average(y, weights=sample_weight))\n\n    est = DummyRegressor(strategy=\"median\").fit(X, y, sample_weight)\n    assert_equal(est.constant_, _weighted_percentile(y, sample_weight, 50.))\n\n    est = DummyRegressor(strategy=\"quantile\", quantile=.95).fit(X, y,\n                                                                sample_weight)\n    assert_equal(est.constant_, _weighted_percentile(y, sample_weight, 95.))\n\n\ndef test_dummy_regressor_on_3D_array():\n    X = np.array([[['foo']], [['bar']], [['baz']]])\n    y = np.array([2, 2, 2])\n    y_expected = np.array([2, 2, 2])\n    cls = DummyRegressor()\n    cls.fit(X, y)\n    y_pred = cls.predict(X)\n    assert_array_equal(y_pred, y_expected)\n\n\ndef test_dummy_classifier_on_3D_array():\n    X = np.array([[['foo']], [['bar']], [['baz']]])\n    y = [2, 2, 2]\n    y_expected = [2, 2, 2]\n    y_proba_expected = [[1], [1], [1]]\n    cls = DummyClassifier()\n    cls.fit(X, y)\n    y_pred = cls.predict(X)\n    y_pred_proba = cls.predict_proba(X)\n    assert_array_equal(y_pred, y_expected)\n    assert_array_equal(y_pred_proba, y_proba_expected)\n"
    }
  ],
  "questions": [
    "@jnothman I'd be happy to give this a go with some limited guidance if no one else is working on it already. Looks like the behavior you noted comes from [this line](https://github.com/scikit-learn/scikit-learn/blob/202b5321f1798c4980abf69ac8c0a0969f01a2ec/sklearn/utils/validation.py#L480), where we're checking the array against the numpy object type when we'd like to check it against string and unicode types as well -- the `[['a', 'b', 'c']]` list in your example appears to be cast to the numpy unicode array type in your example by the time it reaches that line. Sound right?",
    "I'd also be curious to hear what you had in mind in terms of longer term solution, i.e., what would replace `check_array` if deprecated?\r\n> Perhaps we need a deprecation cycle",
    "Something like that. Basically if dtype_numeric and array.dtype is not an\nobject dtype or a numeric dtype, we should raise.\n\nOn 17 January 2018 at 13:17, Ryan <notifications@github.com> wrote:\n\n> @jnothman <https://github.com/jnothman> I'd be happy to give this a go\n> with some limited guidance if no one else is working on it already. Looks\n> like the behavior you noted comes from this line\n> <https://github.com/scikit-learn/scikit-learn/blob/202b5321f1798c4980abf69ac8c0a0969f01a2ec/sklearn/utils/validation.py#L480>,\n> where we're checking the array against the numpy object type when we'd like\n> to check it against string and unicode types as well -- the [['a', 'b',\n> 'c']] list in your example appears to be cast to the numpy unicode array\n> type in your example by the time it reaches that line. Sound right?\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/10229#issuecomment-358173231>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz69hcymywNoXaDwoNalOeRc93uF3Uks5tLVg8gaJpZM4QwJIl>\n> .\n>"
  ],
  "golden_answers": [
    "I'd also be curious to hear what you had in mind in terms of longer term solution, i.e., what would replace `check_array` if deprecated?\r\n> Perhaps we need a deprecation cycle",
    "Something like that. Basically if dtype_numeric and array.dtype is not an\nobject dtype or a numeric dtype, we should raise.\n\nOn 17 January 2018 at 13:17, Ryan <notifications@github.com> wrote:\n\n> @jnothman <https://github.com/jnothman> I'd be happy to give this a go\n> with some limited guidance if no one else is working on it already. Looks\n> like the behavior you noted comes from this line\n> <https://github.com/scikit-learn/scikit-learn/blob/202b5321f1798c4980abf69ac8c0a0969f01a2ec/sklearn/utils/validation.py#L480>,\n> where we're checking the array against the numpy object type when we'd like\n> to check it against string and unicode types as well -- the [['a', 'b',\n> 'c']] list in your example appears to be cast to the numpy unicode array\n> type in your example by the time it reaches that line. Sound right?\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/10229#issuecomment-358173231>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz69hcymywNoXaDwoNalOeRc93uF3Uks5tLVg8gaJpZM4QwJIl>\n> .\n>",
    "We wouldn't deprecate `check_array` entirely, but we would warn for two releases that \"In the future, this data with dtype('Uxx') would be rejected because it is not of a numeric dtype.\""
  ],
  "questions_generated": [
    "What is the intended behavior of the `check_array` function when `dtype='numeric'` is specified, and how is it currently behaving incorrectly?",
    "How does the current implementation of `check_array` determine whether to preserve the dtype of an input array?",
    "Why might the current behavior of `check_array` with `dtype='numeric'` be considered a bug, and what solution has been proposed in the discussion?",
    "In the context of the `check_array` issue, what change is suggested to handle arrays with string or unicode types?",
    "What potential impact might the proposed changes to `check_array` have on existing codebases using scikit-learn, and what transitional measure is mentioned?"
  ],
  "golden_answers_generated": [
    "The intended behavior of `check_array` with `dtype='numeric'` is to ensure that the input array has a numeric, real-valued dtype. However, it is currently behaving incorrectly by allowing arrays with string elements to pass without error, which results in an array of strings instead of raising an error or attempting to coerce the data to a numeric type.",
    "The current implementation of `check_array` preserves the dtype of an input array unless the array's dtype is 'object'. This means that it does not perform any checks or conversions for arrays that are not explicitly of object dtype, leading to unexpected behavior when the array contains non-numeric types like strings.",
    "The current behavior of `check_array` is considered a bug because it allows arrays with string elements to be processed without error, which is not useful or intended for numeric operations. The proposed solution is to modify the function so that it raises an error if the array is not of a numeric dtype or is of object dtype when `dtype='numeric'` is specified.",
    "The suggested change is to update the `check_array` function to check arrays for string and unicode types as well, rather than only checking against the numpy object type. This would ensure that arrays containing strings are flagged as incompatible when `dtype='numeric'` is specified.",
    "The proposed changes to `check_array` might impact existing codebases that rely on the current behavior of allowing non-numeric data to pass through without error. To mitigate this, a deprecation cycle is suggested, where the updated behavior would initially raise warnings before becoming the default, giving developers time to adapt their code."
  ]
}