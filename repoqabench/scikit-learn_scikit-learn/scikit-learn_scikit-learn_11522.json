{
  "repo_name": "scikit-learn_scikit-learn",
  "issue_id": "11522",
  "issue_description": "# Add sklearn.show_versions() similar to pandas.show_versions (with numpy blas binding info)\n\nSome numeric issues are related to the specific blas that numpy is using. I'm wondering if it makes sense to add the relevant ``system_info`` invocations to the template to make it easier for people to report.",
  "issue_comments": [
    {
      "id": 405187530,
      "user": "jnothman",
      "body": "I don't see why not!\n"
    },
    {
      "id": 405296109,
      "user": "rth",
      "body": "@lesteve suggested we could add something like `pandas.show_versions()` that would print all the relevant information for debugging. For instance, on my laptop, I get,\r\n\r\n<details>\r\n\r\n```\r\n>>> pd.show_versions()\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.5.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.11.6-gentoo\r\nmachine: x86_64\r\nprocessor: Intel(R) Core(TM) i5-6200U CPU @ 2.30GHz\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\nLOCALE: en_GB.UTF-8\r\n\r\npandas: 0.23.0\r\npytest: 3.5.1\r\npip: 10.0.1\r\nsetuptools: 39.1.0\r\nCython: 0.27.3\r\nnumpy: 1.14.3\r\nscipy: 1.1.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.4.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.7.3\r\npytz: 2018.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.2.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n```\r\n</details>\r\n\r\nwe certainly don't care about all the dependencies that pandas might, but I agree that for scikit-learn, having  e.g.\r\n - BLAS information\r\n - whether \"conda\" is in path\r\n - with https://github.com/scikit-learn/scikit-learn/pull/11166 whether the bundled or unbundled joblib is used\r\n \r\nwould be definitely useful for debuging. It's more practical to have small function for those, than a copy passable snippet (particularly if it becomes more complex).\r\n\r\nTagging this for v0.20 as this would be fairly easy to do, and help with the maintenance after the release..\r\n"
    },
    {
      "id": 405298415,
      "user": "lesteve",
      "body": "+10 about `sklearn.show_versions()` I edited the issue title."
    },
    {
      "id": 405299633,
      "user": "lesteve",
      "body": "I think what we want to do:\r\n* add sklearn.show_versions with similar info as pandas.show_versions and whatever we ask for in the ISSUE_TEMPLATE.md\r\n* modify ISSUE_TEMPLATE.md\r\n* amend the docs, this git grep can be handy:\r\n```\r\n❯ git grep platform.platform\r\nCONTRIBUTING.md:  import platform; print(platform.platform())\r\nISSUE_TEMPLATE.md:import platform; print(platform.platform())\r\ndoc/developers/contributing.rst:     import platform; print(platform.platform())\r\ndoc/developers/tips.rst:        import platform; print(platform.platform())\r\nsklearn/feature_extraction/dict_vectorizer.py:            \" include the output from platform.platform() in your bug report\")\r\n```"
    },
    {
      "id": 405388988,
      "user": "massich",
      "body": "This might be of help\r\n```py\r\nfrom sklearn._build_utils import get_blas_info\r\n```"
    },
    {
      "id": 405416039,
      "user": "jnothman",
      "body": "The problem with doing this is that it won't be runnable before 0.20!​\n"
    },
    {
      "id": 405611504,
      "user": "aboucaud",
      "body": "Hi guys, I'm looking at this right now.\r\n\r\nFew questions to help me get this done:\r\n\r\n- what are the relevant information from the `get_blas_info` that need to be printed ? \r\n  unfortunately the compilation information is printed (probably through `cythonize`) but not returned.\r\n- shall I restrict the printed python libraries to the main ones ? \r\n  I'd suggest `numpy`, `scipy`, `pandas`, `matplotlib`, `Cython`, `pip`, `setuptools`, `pytest`.\r\n- is `sklearn/utils` the right place to put it ?"
    },
    {
      "id": 405623405,
      "user": "lesteve",
      "body": "> The problem with doing this is that it won't be runnable before 0.20!​\r\n\r\nGood point we can modify only the rst doc for now and delay the changes in .md until the release.\r\n\r\nNot an expert, but I think all the `get_blas_info` is useful. About the dependencies, I am not sure look at what pandas is doing and do something similar (they have things about sys.executable, 32bit vs 64bit, bitness which may be useful). It would be good to keep it as short as possible. For example I am not convinced `pytest` makes sense.\r\n\r\n> is sklearn/utils the right place to put it ?\r\n\r\nYou can probably put the code in `sklearn/utils`. I would be in favour of making it accessible at from the root namespace so that you can do `from sklearn import show_versions`\r\n\r\n\r\n\r\n"
    },
    {
      "id": 405624708,
      "user": "amueller",
      "body": "+1 for adding show_versions.\r\nMaybe optionally include the blas stuff?"
    },
    {
      "id": 405630461,
      "user": "aboucaud",
      "body": "> Good point we can modify only the rst doc for now and delay the changes in .md until the release.\r\n\r\nWill do.\r\n\r\n> I would be in favour of making it accessible at from the root namespace so that you can do from `sklearn import show_versions`\r\n\r\nAgreed. I would have suggested to add `from sklearn.utils import show_versions` in `sklearn.__init__.py` instead of putting the file there. Does that sound ok ?\r\n\r\n> Maybe optionally include the blas stuff?\r\n\r\nI wrote it as an option for now."
    }
  ],
  "text_context": "# Add sklearn.show_versions() similar to pandas.show_versions (with numpy blas binding info)\n\nSome numeric issues are related to the specific blas that numpy is using. I'm wondering if it makes sense to add the relevant ``system_info`` invocations to the template to make it easier for people to report.\n\nI don't see why not!\n\n\n@lesteve suggested we could add something like `pandas.show_versions()` that would print all the relevant information for debugging. For instance, on my laptop, I get,\r\n\r\n<details>\r\n\r\n```\r\n>>> pd.show_versions()\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.5.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.11.6-gentoo\r\nmachine: x86_64\r\nprocessor: Intel(R) Core(TM) i5-6200U CPU @ 2.30GHz\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\nLOCALE: en_GB.UTF-8\r\n\r\npandas: 0.23.0\r\npytest: 3.5.1\r\npip: 10.0.1\r\nsetuptools: 39.1.0\r\nCython: 0.27.3\r\nnumpy: 1.14.3\r\nscipy: 1.1.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.4.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.7.3\r\npytz: 2018.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.2.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n```\r\n</details>\r\n\r\nwe certainly don't care about all the dependencies that pandas might, but I agree that for scikit-learn, having  e.g.\r\n - BLAS information\r\n - whether \"conda\" is in path\r\n - with https://github.com/scikit-learn/scikit-learn/pull/11166 whether the bundled or unbundled joblib is used\r\n \r\nwould be definitely useful for debuging. It's more practical to have small function for those, than a copy passable snippet (particularly if it becomes more complex).\r\n\r\nTagging this for v0.20 as this would be fairly easy to do, and help with the maintenance after the release..\r\n\n\n+10 about `sklearn.show_versions()` I edited the issue title.\n\nI think what we want to do:\r\n* add sklearn.show_versions with similar info as pandas.show_versions and whatever we ask for in the ISSUE_TEMPLATE.md\r\n* modify ISSUE_TEMPLATE.md\r\n* amend the docs, this git grep can be handy:\r\n```\r\n❯ git grep platform.platform\r\nCONTRIBUTING.md:  import platform; print(platform.platform())\r\nISSUE_TEMPLATE.md:import platform; print(platform.platform())\r\ndoc/developers/contributing.rst:     import platform; print(platform.platform())\r\ndoc/developers/tips.rst:        import platform; print(platform.platform())\r\nsklearn/feature_extraction/dict_vectorizer.py:            \" include the output from platform.platform() in your bug report\")\r\n```\n\nThis might be of help\r\n```py\r\nfrom sklearn._build_utils import get_blas_info\r\n```\n\nThe problem with doing this is that it won't be runnable before 0.20!​\n\n\nHi guys, I'm looking at this right now.\r\n\r\nFew questions to help me get this done:\r\n\r\n- what are the relevant information from the `get_blas_info` that need to be printed ? \r\n  unfortunately the compilation information is printed (probably through `cythonize`) but not returned.\r\n- shall I restrict the printed python libraries to the main ones ? \r\n  I'd suggest `numpy`, `scipy`, `pandas`, `matplotlib`, `Cython`, `pip`, `setuptools`, `pytest`.\r\n- is `sklearn/utils` the right place to put it ?\n\n> The problem with doing this is that it won't be runnable before 0.20!​\r\n\r\nGood point we can modify only the rst doc for now and delay the changes in .md until the release.\r\n\r\nNot an expert, but I think all the `get_blas_info` is useful. About the dependencies, I am not sure look at what pandas is doing and do something similar (they have things about sys.executable, 32bit vs 64bit, bitness which may be useful). It would be good to keep it as short as possible. For example I am not convinced `pytest` makes sense.\r\n\r\n> is sklearn/utils the right place to put it ?\r\n\r\nYou can probably put the code in `sklearn/utils`. I would be in favour of making it accessible at from the root namespace so that you can do `from sklearn import show_versions`\r\n\r\n\r\n\r\n\n\n+1 for adding show_versions.\r\nMaybe optionally include the blas stuff?\n\n> Good point we can modify only the rst doc for now and delay the changes in .md until the release.\r\n\r\nWill do.\r\n\r\n> I would be in favour of making it accessible at from the root namespace so that you can do from `sklearn import show_versions`\r\n\r\nAgreed. I would have suggested to add `from sklearn.utils import show_versions` in `sklearn.__init__.py` instead of putting the file there. Does that sound ok ?\r\n\r\n> Maybe optionally include the blas stuff?\r\n\r\nI wrote it as an option for now.",
  "pr_link": "https://github.com/scikit-learn/scikit-learn/pull/11166",
  "code_context": [
    {
      "filename": "sklearn/externals/_joblib/__init__.py",
      "content": "\"\"\"Joblib is a set of tools to provide **lightweight pipelining in\nPython**. In particular, joblib offers:\n\n1. transparent disk-caching of the output values and lazy re-evaluation\n   (memoize pattern)\n\n2. easy simple parallel computing\n\n3. logging and tracing of the execution\n\nJoblib is optimized to be **fast** and **robust** in particular on large\ndata and has specific optimizations for `numpy` arrays. It is\n**BSD-licensed**.\n\n\n    ========================= ================================================\n    **User documentation:**        http://pythonhosted.org/joblib\n\n    **Download packages:**         http://pypi.python.org/pypi/joblib#downloads\n\n    **Source code:**               http://github.com/joblib/joblib\n\n    **Report issues:**             http://github.com/joblib/joblib/issues\n    ========================= ================================================\n\n\nVision\n--------\n\nThe vision is to provide tools to easily achieve better performance and\nreproducibility when working with long running jobs.\n\n *  **Avoid computing twice the same thing**: code is rerun over an\n    over, for instance when prototyping computational-heavy jobs (as in\n    scientific development), but hand-crafted solution to alleviate this\n    issue is error-prone and often leads to unreproducible results\n\n *  **Persist to disk transparently**: persisting in an efficient way\n    arbitrary objects containing large data is hard. Using\n    joblib's caching mechanism avoids hand-written persistence and\n    implicitly links the file on disk to the execution context of\n    the original Python object. As a result, joblib's persistence is\n    good for resuming an application status or computational job, eg\n    after a crash.\n\nJoblib strives to address these problems while **leaving your code and\nyour flow control as unmodified as possible** (no framework, no new\nparadigms).\n\nMain features\n------------------\n\n1) **Transparent and fast disk-caching of output value:** a memoize or\n   make-like functionality for Python functions that works well for\n   arbitrary Python objects, including very large numpy arrays. Separate\n   persistence and flow-execution logic from domain logic or algorithmic\n   code by writing the operations as a set of steps with well-defined\n   inputs and  outputs: Python functions. Joblib can save their\n   computation to disk and rerun it only if necessary::\n\n      >>> from sklearn.externals.joblib import Memory\n      >>> mem = Memory(cachedir='/tmp/joblib')\n      >>> import numpy as np\n      >>> a = np.vander(np.arange(3)).astype(np.float)\n      >>> square = mem.cache(np.square)\n      >>> b = square(a)                                   # doctest: +ELLIPSIS\n      ________________________________________________________________________________\n      [Memory] Calling square...\n      square(array([[ 0.,  0.,  1.],\n             [ 1.,  1.,  1.],\n             [ 4.,  2.,  1.]]))\n      ___________________________________________________________square - 0...s, 0.0min\n\n      >>> c = square(a)\n      >>> # The above call did not trigger an evaluation\n\n2) **Embarrassingly parallel helper:** to make it easy to write readable\n   parallel code and debug it quickly::\n\n      >>> from sklearn.externals.joblib import Parallel, delayed\n      >>> from math import sqrt\n      >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))\n      [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n\n\n3) **Logging/tracing:** The different functionalities will\n   progressively acquire better logging mechanism to help track what\n   has been ran, and capture I/O easily. In addition, Joblib will\n   provide a few I/O primitives, to easily define logging and\n   display streams, and provide a way of compiling a report.\n   We want to be able to quickly inspect what has been run.\n\n4) **Fast compressed Persistence**: a replacement for pickle to work\n   efficiently on Python objects containing large data (\n   *joblib.dump* & *joblib.load* ).\n\n..\n    >>> import shutil ; shutil.rmtree('/tmp/joblib/')\n\n\"\"\"\n\n# PEP0440 compatible formatted version, see:\n# https://www.python.org/dev/peps/pep-0440/\n#\n# Generic release markers:\n# X.Y\n# X.Y.Z # For bugfix releases\n#\n# Admissible pre-release markers:\n# X.YaN # Alpha release\n# X.YbN # Beta release\n# X.YrcN # Release Candidate\n# X.Y # Final release\n#\n# Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.\n# 'X.Y.dev0' is the canonical version of 'X.Y.dev'\n#\n__version__ = '0.11'\n\n\nfrom .memory import Memory, MemorizedResult\nfrom .logger import PrintTime\nfrom .logger import Logger\nfrom .hashing import hash\nfrom .numpy_pickle import dump\nfrom .numpy_pickle import load\nfrom .parallel import Parallel\nfrom .parallel import delayed\nfrom .parallel import cpu_count\nfrom .parallel import register_parallel_backend\nfrom .parallel import parallel_backend\nfrom .parallel import effective_n_jobs\n\n\n__all__ = ['Memory', 'MemorizedResult', 'PrintTime', 'Logger', 'hash', 'dump',\n           'load', 'Parallel', 'delayed', 'cpu_count', 'effective_n_jobs',\n           'register_parallel_backend', 'parallel_backend']\n"
    },
    {
      "filename": "sklearn/externals/_joblib/_compat.py",
      "content": "\"\"\"\nCompatibility layer for Python 3/Python 2 single codebase\n\"\"\"\nimport sys\n\nPY3_OR_LATER = sys.version_info[0] >= 3\nPY27 = sys.version_info[:2] == (2, 7)\n\ntry:\n    _basestring = basestring\n    _bytes_or_unicode = (str, unicode)\nexcept NameError:\n    _basestring = str\n    _bytes_or_unicode = (bytes, str)\n\n\ndef with_metaclass(meta, *bases):\n    \"\"\"Create a base class with a metaclass.\"\"\"\n    return meta(\"NewBase\", bases, {})\n"
    },
    {
      "filename": "sklearn/externals/_joblib/_memory_helpers.py",
      "content": "try:\n    # Available in Python 3\n    from tokenize import open as open_py_source\n\nexcept ImportError:\n    # Copied from python3 tokenize\n    from codecs import lookup, BOM_UTF8\n    import re\n    from io import TextIOWrapper, open\n    cookie_re = re.compile(r\"coding[:=]\\s*([-\\w.]+)\")\n\n    def _get_normal_name(orig_enc):\n        \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n        # Only care about the first 12 characters.\n        enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n        if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n            return \"utf-8\"\n        if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or \\\n           enc.startswith((\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")):\n            return \"iso-8859-1\"\n        return orig_enc\n\n    def _detect_encoding(readline):\n        \"\"\"\n        The detect_encoding() function is used to detect the encoding that\n        should be used to decode a Python source file.  It requires one\n        argment, readline, in the same way as the tokenize() generator.\n\n        It will call readline a maximum of twice, and return the encoding used\n        (as a string) and a list of any lines (left as bytes) it has read in.\n\n        It detects the encoding from the presence of a utf-8 bom or an encoding\n        cookie as specified in pep-0263.  If both a bom and a cookie are\n        present, but disagree, a SyntaxError will be raised.  If the encoding\n        cookie is an invalid charset, raise a SyntaxError.  Note that if a\n        utf-8 bom is found, 'utf-8-sig' is returned.\n\n        If no encoding is specified, then the default of 'utf-8' will be\n        returned.\n        \"\"\"\n        bom_found = False\n        encoding = None\n        default = 'utf-8'\n\n        def read_or_stop():\n            try:\n                return readline()\n            except StopIteration:\n                return b''\n\n        def find_cookie(line):\n            try:\n                line_string = line.decode('ascii')\n            except UnicodeDecodeError:\n                return None\n\n            matches = cookie_re.findall(line_string)\n            if not matches:\n                return None\n            encoding = _get_normal_name(matches[0])\n            try:\n                codec = lookup(encoding)\n            except LookupError:\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"unknown encoding: \" + encoding)\n\n            if bom_found:\n                if codec.name != 'utf-8':\n                    # This behaviour mimics the Python interpreter\n                    raise SyntaxError('encoding problem: utf-8')\n                encoding += '-sig'\n            return encoding\n\n        first = read_or_stop()\n        if first.startswith(BOM_UTF8):\n            bom_found = True\n            first = first[3:]\n            default = 'utf-8-sig'\n        if not first:\n            return default, []\n\n        encoding = find_cookie(first)\n        if encoding:\n            return encoding, [first]\n\n        second = read_or_stop()\n        if not second:\n            return default, [first]\n\n        encoding = find_cookie(second)\n        if encoding:\n            return encoding, [first, second]\n\n        return default, [first, second]\n\n    def open_py_source(filename):\n        \"\"\"Open a file in read only mode using the encoding detected by\n        detect_encoding().\n        \"\"\"\n        buffer = open(filename, 'rb')\n        encoding, lines = _detect_encoding(buffer.readline)\n        buffer.seek(0)\n        text = TextIOWrapper(buffer, encoding, line_buffering=True)\n        text.mode = 'r'\n        return text\n"
    },
    {
      "filename": "sklearn/externals/_joblib/_multiprocessing_helpers.py",
      "content": "\"\"\"Helper module to factorize the conditional multiprocessing import logic\n\nWe use a distinct module to simplify import statements and avoid introducing\ncircular dependencies (for instance for the assert_spawning name).\n\"\"\"\nimport os\nimport warnings\n\n\n# Obtain possible configuration from the environment, assuming 1 (on)\n# by default, upon 0 set to None. Should instructively fail if some non\n# 0/1 value is set.\nmp = int(os.environ.get('JOBLIB_MULTIPROCESSING', 1)) or None\nif mp:\n    try:\n        import multiprocessing as mp\n    except ImportError:\n        mp = None\n\n# 2nd stage: validate that locking is available on the system and\n#            issue a warning if not\nif mp is not None:\n    try:\n        _sem = mp.Semaphore()\n        del _sem  # cleanup\n    except (ImportError, OSError) as e:\n        mp = None\n        warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n\n\n# 3rd stage: backward compat for the assert_spawning helper\nif mp is not None:\n    try:\n        # Python 3.4+\n        from multiprocessing.context import assert_spawning\n    except ImportError:\n        from multiprocessing.forking import assert_spawning\nelse:\n    assert_spawning = None\n"
    },
    {
      "filename": "sklearn/externals/_joblib/_parallel_backends.py",
      "content": "\"\"\"\nBackends for embarrassingly parallel code.\n\"\"\"\n\nimport gc\nimport os\nimport sys\nimport warnings\nimport threading\nfrom abc import ABCMeta, abstractmethod\n\nfrom .format_stack import format_exc\nfrom .my_exceptions import WorkerInterrupt, TransportableException\nfrom ._multiprocessing_helpers import mp\nfrom ._compat import with_metaclass\nif mp is not None:\n    from .pool import MemmapingPool\n    from multiprocessing.pool import ThreadPool\n\n\nclass ParallelBackendBase(with_metaclass(ABCMeta)):\n    \"\"\"Helper abc which defines all methods a ParallelBackend must implement\"\"\"\n\n    supports_timeout = False\n\n    @abstractmethod\n    def effective_n_jobs(self, n_jobs):\n        \"\"\"Determine the number of jobs that can actually run in parallel\n\n        n_jobs is the number of workers requested by the callers. Passing\n        n_jobs=-1 means requesting all available workers for instance matching\n        the number of CPU cores on the worker host(s).\n\n        This method should return a guesstimate of the number of workers that\n        can actually perform work concurrently. The primary use case is to make\n        it possible for the caller to know in how many chunks to slice the\n        work.\n\n        In general working on larger data chunks is more efficient (less\n        scheduling overhead and better use of CPU cache prefetching heuristics)\n        as long as all the workers have enough work to do.\n        \"\"\"\n\n    @abstractmethod\n    def apply_async(self, func, callback=None):\n        \"\"\"Schedule a func to be run\"\"\"\n\n    def configure(self, n_jobs=1, parallel=None, **backend_args):\n        \"\"\"Reconfigure the backend and return the number of workers.\n\n        This makes it possible to reuse an existing backend instance for\n        successive independent calls to Parallel with different parameters.\n        \"\"\"\n        self.parallel = parallel\n        return self.effective_n_jobs(n_jobs)\n\n    def terminate(self):\n        \"\"\"Shutdown the process or thread pool\"\"\"\n\n    def compute_batch_size(self):\n        \"\"\"Determine the optimal batch size\"\"\"\n        return 1\n\n    def batch_completed(self, batch_size, duration):\n        \"\"\"Callback indicate how long it took to run a batch\"\"\"\n\n    def get_exceptions(self):\n        \"\"\"List of exception types to be captured.\"\"\"\n        return []\n\n    def abort_everything(self, ensure_ready=True):\n        \"\"\"Abort any running tasks\n\n        This is called when an exception has been raised when executing a tasks\n        and all the remaining tasks will be ignored and can therefore be\n        aborted to spare computation resources.\n\n        If ensure_ready is True, the backend should be left in an operating\n        state as future tasks might be re-submitted via that same backend\n        instance.\n\n        If ensure_ready is False, the implementer of this method can decide\n        to leave the backend in a closed / terminated state as no new task\n        are expected to be submitted to this backend.\n\n        Setting ensure_ready to False is an optimization that can be leveraged\n        when aborting tasks via killing processes from a local process pool\n        managed by the backend it-self: if we expect no new tasks, there is no\n        point in re-creating a new working pool.\n        \"\"\"\n        # Does nothing by default: to be overridden in subclasses when canceling\n        # tasks is possible.\n        pass\n\n\nclass SequentialBackend(ParallelBackendBase):\n    \"\"\"A ParallelBackend which will execute all batches sequentially.\n\n    Does not use/create any threading objects, and hence has minimal\n    overhead. Used when n_jobs == 1.\n    \"\"\"\n\n    def effective_n_jobs(self, n_jobs):\n        \"\"\"Determine the number of jobs which are going to run in parallel\"\"\"\n        if n_jobs == 0:\n            raise ValueError('n_jobs == 0 in Parallel has no meaning')\n        return 1\n\n    def apply_async(self, func, callback=None):\n        \"\"\"Schedule a func to be run\"\"\"\n        result = ImmediateResult(func)\n        if callback:\n            callback(result)\n        return result\n\n\nclass PoolManagerMixin(object):\n    \"\"\"A helper class for managing pool of workers.\"\"\"\n\n    def effective_n_jobs(self, n_jobs):\n        \"\"\"Determine the number of jobs which are going to run in parallel\"\"\"\n        if n_jobs == 0:\n            raise ValueError('n_jobs == 0 in Parallel has no meaning')\n        elif mp is None or n_jobs is None:\n            # multiprocessing is not available or disabled, fallback\n            # to sequential mode\n            return 1\n        elif n_jobs < 0:\n            n_jobs = max(mp.cpu_count() + 1 + n_jobs, 1)\n        return n_jobs\n\n    def terminate(self):\n        \"\"\"Shutdown the process or thread pool\"\"\"\n        if self._pool is not None:\n            self._pool.close()\n            self._pool.terminate()  # terminate does a join()\n            self._pool = None\n\n    def apply_async(self, func, callback=None):\n        \"\"\"Schedule a func to be run\"\"\"\n        return self._pool.apply_async(SafeFunction(func), callback=callback)\n\n    def abort_everything(self, ensure_ready=True):\n        \"\"\"Shutdown the pool and restart a new one with the same parameters\"\"\"\n        self.terminate()\n        if ensure_ready:\n            self.configure(n_jobs=self.parallel.n_jobs, parallel=self.parallel,\n                           **self.parallel._backend_args)\n\n\nclass AutoBatchingMixin(object):\n    \"\"\"A helper class for automagically batching jobs.\"\"\"\n\n    # In seconds, should be big enough to hide multiprocessing dispatching\n    # overhead.\n    # This settings was found by running benchmarks/bench_auto_batching.py\n    # with various parameters on various platforms.\n    MIN_IDEAL_BATCH_DURATION = .2\n\n    # Should not be too high to avoid stragglers: long jobs running alone\n    # on a single worker while other workers have no work to process any more.\n    MAX_IDEAL_BATCH_DURATION = 2\n\n    # Batching counters\n    _effective_batch_size = 1\n    _smoothed_batch_duration = 0.0\n\n    def compute_batch_size(self):\n        \"\"\"Determine the optimal batch size\"\"\"\n        old_batch_size = self._effective_batch_size\n        batch_duration = self._smoothed_batch_duration\n        if (batch_duration > 0 and\n                batch_duration < self.MIN_IDEAL_BATCH_DURATION):\n            # The current batch size is too small: the duration of the\n            # processing of a batch of task is not large enough to hide\n            # the scheduling overhead.\n            ideal_batch_size = int(old_batch_size *\n                                   self.MIN_IDEAL_BATCH_DURATION /\n                                   batch_duration)\n            # Multiply by two to limit oscilations between min and max.\n            batch_size = max(2 * ideal_batch_size, 1)\n            self._effective_batch_size = batch_size\n            if self.parallel.verbose >= 10:\n                self.parallel._print(\n                    \"Batch computation too fast (%.4fs.) \"\n                    \"Setting batch_size=%d.\", (batch_duration, batch_size))\n        elif (batch_duration > self.MAX_IDEAL_BATCH_DURATION and\n              old_batch_size >= 2):\n            # The current batch size is too big. If we schedule overly long\n            # running batches some CPUs might wait with nothing left to do\n            # while a couple of CPUs a left processing a few long running\n            # batches. Better reduce the batch size a bit to limit the\n            # likelihood of scheduling such stragglers.\n            batch_size = old_batch_size // 2\n            self._effective_batch_size = batch_size\n            if self.parallel.verbose >= 10:\n                self.parallel._print(\n                    \"Batch computation too slow (%.4fs.) \"\n                    \"Setting batch_size=%d.\", (batch_duration, batch_size))\n        else:\n            # No batch size adjustment\n            batch_size = old_batch_size\n\n        if batch_size != old_batch_size:\n            # Reset estimation of the smoothed mean batch duration: this\n            # estimate is updated in the multiprocessing apply_async\n            # CallBack as long as the batch_size is constant. Therefore\n            # we need to reset the estimate whenever we re-tune the batch\n            # size.\n            self._smoothed_batch_duration = 0\n\n        return batch_size\n\n    def batch_completed(self, batch_size, duration):\n        \"\"\"Callback indicate how long it took to run a batch\"\"\"\n        if batch_size == self._effective_batch_size:\n            # Update the smoothed streaming estimate of the duration of a batch\n            # from dispatch to completion\n            old_duration = self._smoothed_batch_duration\n            if old_duration == 0:\n                # First record of duration for this batch size after the last\n                # reset.\n                new_duration = duration\n            else:\n                # Update the exponentially weighted average of the duration of\n                # batch for the current effective size.\n                new_duration = 0.8 * old_duration + 0.2 * duration\n            self._smoothed_batch_duration = new_duration\n\n\nclass ThreadingBackend(PoolManagerMixin, ParallelBackendBase):\n    \"\"\"A ParallelBackend which will use a thread pool to execute batches in.\n\n    This is a low-overhead backend but it suffers from the Python Global\n    Interpreter Lock if the called function relies a lot on Python objects.\n    Mostly useful when the execution bottleneck is a compiled extension that\n    explicitly releases the GIL (for instance a Cython loop wrapped in a\n    \"with nogil\" block or an expensive call to a library such as NumPy).\n    \"\"\"\n\n    supports_timeout = True\n\n    def configure(self, n_jobs=1, parallel=None, **backend_args):\n        \"\"\"Build a process or thread pool and return the number of workers\"\"\"\n        n_jobs = self.effective_n_jobs(n_jobs)\n        if n_jobs == 1:\n            # Avoid unnecessary overhead and use sequential backend instead.\n            raise FallbackToBackend(SequentialBackend())\n        self.parallel = parallel\n        self._pool = ThreadPool(n_jobs)\n        return n_jobs\n\n\nclass MultiprocessingBackend(PoolManagerMixin, AutoBatchingMixin,\n                             ParallelBackendBase):\n    \"\"\"A ParallelBackend which will use a multiprocessing.Pool.\n\n    Will introduce some communication and memory overhead when exchanging\n    input and output data with the with the worker Python processes.\n    However, does not suffer from the Python Global Interpreter Lock.\n    \"\"\"\n\n    # Environment variables to protect against bad situations when nesting\n    JOBLIB_SPAWNED_PROCESS = \"__JOBLIB_SPAWNED_PARALLEL__\"\n\n    supports_timeout = True\n\n    def effective_n_jobs(self, n_jobs):\n        \"\"\"Determine the number of jobs which are going to run in parallel.\n\n        This also checks if we are attempting to create a nested parallel\n        loop.\n        \"\"\"\n        if mp is None:\n            return 1\n\n        if mp.current_process().daemon:\n            # Daemonic processes cannot have children\n            if n_jobs != 1:\n                warnings.warn(\n                    'Multiprocessing-backed parallel loops cannot be nested,'\n                    ' setting n_jobs=1',\n                    stacklevel=3)\n            return 1\n\n        if not isinstance(threading.current_thread(), threading._MainThread):\n            # Prevent posix fork inside in non-main posix threads\n            warnings.warn(\n                'Multiprocessing-backed parallel loops cannot be nested'\n                ' below threads, setting n_jobs=1',\n                stacklevel=3)\n            return 1\n\n        return super(MultiprocessingBackend, self).effective_n_jobs(n_jobs)\n\n    def configure(self, n_jobs=1, parallel=None, **backend_args):\n        \"\"\"Build a process or thread pool and return the number of workers\"\"\"\n        n_jobs = self.effective_n_jobs(n_jobs)\n        if n_jobs == 1:\n            raise FallbackToBackend(SequentialBackend())\n\n        already_forked = int(os.environ.get(self.JOBLIB_SPAWNED_PROCESS, 0))\n        if already_forked:\n            raise ImportError(\n                '[joblib] Attempting to do parallel computing '\n                'without protecting your import on a system that does '\n                'not support forking. To use parallel-computing in a '\n                'script, you must protect your main loop using \"if '\n                \"__name__ == '__main__'\"\n                '\". Please see the joblib documentation on Parallel '\n                'for more information')\n        # Set an environment variable to avoid infinite loops\n        os.environ[self.JOBLIB_SPAWNED_PROCESS] = '1'\n\n        # Make sure to free as much memory as possible before forking\n        gc.collect()\n        self._pool = MemmapingPool(n_jobs, **backend_args)\n        self.parallel = parallel\n        return n_jobs\n\n    def terminate(self):\n        \"\"\"Shutdown the process or thread pool\"\"\"\n        super(MultiprocessingBackend, self).terminate()\n        if self.JOBLIB_SPAWNED_PROCESS in os.environ:\n            del os.environ[self.JOBLIB_SPAWNED_PROCESS]\n\n\nclass ImmediateResult(object):\n    def __init__(self, batch):\n        # Don't delay the application, to avoid keeping the input\n        # arguments in memory\n        self.results = batch()\n\n    def get(self):\n        return self.results\n\n\nclass SafeFunction(object):\n    \"\"\"Wrapper that handles the serialization of exception tracebacks.\n\n    If an exception is triggered when calling the inner function, a copy of\n    the full traceback is captured to make it possible to serialize\n    it so that it can be rendered in a different Python process.\n    \"\"\"\n    def __init__(self, func):\n        self.func = func\n\n    def __call__(self, *args, **kwargs):\n        try:\n            return self.func(*args, **kwargs)\n        except KeyboardInterrupt:\n            # We capture the KeyboardInterrupt and reraise it as\n            # something different, as multiprocessing does not\n            # interrupt processing for a KeyboardInterrupt\n            raise WorkerInterrupt()\n        except:\n            e_type, e_value, e_tb = sys.exc_info()\n            text = format_exc(e_type, e_value, e_tb, context=10, tb_offset=1)\n            raise TransportableException(text, e_type)\n\n\nclass FallbackToBackend(Exception):\n    \"\"\"Raised when configuration should fallback to another backend\"\"\"\n\n    def __init__(self, backend):\n        self.backend = backend\n"
    },
    {
      "filename": "sklearn/externals/_joblib/backports.py",
      "content": "\"\"\"\nBackports of fixes for joblib dependencies\n\"\"\"\nimport os\nimport time\nimport ctypes\nimport sys\n\nfrom distutils.version import LooseVersion\n\ntry:\n    import numpy as np\n\n    def make_memmap(filename, dtype='uint8', mode='r+', offset=0,\n                    shape=None, order='C'):\n        \"\"\"Backport of numpy memmap offset fix.\n\n        See https://github.com/numpy/numpy/pull/8443 for more details.\n\n        The numpy fix will be available in numpy 1.13.\n        \"\"\"\n        mm = np.memmap(filename, dtype=dtype, mode=mode, offset=offset,\n                       shape=shape, order=order)\n        if LooseVersion(np.__version__) < '1.13':\n            mm.offset = offset\n        return mm\nexcept ImportError:\n    def make_memmap(filename, dtype='uint8', mode='r+', offset=0,\n                    shape=None, order='C'):\n        raise NotImplementedError(\n            \"'joblib.backports.make_memmap' should not be used \"\n            'if numpy is not installed.')\n\n\nif os.name == 'nt':\n    error_access_denied = 5\n    try:\n        from os import replace\n    except ImportError:\n        # Python 2.7\n        def replace(src, dst):\n            if not isinstance(src, unicode):  # noqa\n                src = unicode(src, sys.getfilesystemencoding())  # noqa\n            if not isinstance(dst, unicode):  # noqa\n                dst = unicode(dst, sys.getfilesystemencoding())  # noqa\n\n            movefile_replace_existing = 0x1\n            return_value = ctypes.windll.kernel32.MoveFileExW(\n                src, dst, movefile_replace_existing)\n            if return_value == 0:\n                raise ctypes.WinError()\n\n    def concurrency_safe_rename(src, dst):\n        \"\"\"Renames ``src`` into ``dst`` overwriting ``dst`` if it exists.\n\n        On Windows os.replace (or for Python 2.7 its implementation\n        through MoveFileExW) can yield permission errors if executed by\n        two different processes.\n        \"\"\"\n        max_sleep_time = 1\n        total_sleep_time = 0\n        sleep_time = 0.001\n        while total_sleep_time < max_sleep_time:\n            try:\n                replace(src, dst)\n                break\n            except Exception as exc:\n                if getattr(exc, 'winerror', None) == error_access_denied:\n                    time.sleep(sleep_time)\n                    total_sleep_time += sleep_time\n                    sleep_time *= 2\n                else:\n                    raise\n        else:\n            raise\nelse:\n    try:\n        from os import replace as concurrency_safe_rename\n    except ImportError:\n        from os import rename as concurrency_safe_rename  # noqa\n"
    },
    {
      "filename": "sklearn/externals/_joblib/disk.py",
      "content": "\"\"\"\nDisk management utilities.\n\"\"\"\n\n# Authors: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n#          Lars Buitinck\n# Copyright (c) 2010 Gael Varoquaux\n# License: BSD Style, 3 clauses.\n\n\nimport errno\nimport os\nimport shutil\nimport sys\nimport time\n\n\ndef disk_used(path):\n    \"\"\" Return the disk usage in a directory.\"\"\"\n    size = 0\n    for file in os.listdir(path) + ['.']:\n        stat = os.stat(os.path.join(path, file))\n        if hasattr(stat, 'st_blocks'):\n            size += stat.st_blocks * 512\n        else:\n            # on some platform st_blocks is not available (e.g., Windows)\n            # approximate by rounding to next multiple of 512\n            size += (stat.st_size // 512 + 1) * 512\n    # We need to convert to int to avoid having longs on some systems (we\n    # don't want longs to avoid problems we SQLite)\n    return int(size / 1024.)\n\n\ndef memstr_to_bytes(text):\n    \"\"\" Convert a memory text to its value in bytes.\n    \"\"\"\n    kilo = 1024\n    units = dict(K=kilo, M=kilo ** 2, G=kilo ** 3)\n    try:\n        size = int(units[text[-1]] * float(text[:-1]))\n    except (KeyError, ValueError):\n        raise ValueError(\n            \"Invalid literal for size give: %s (type %s) should be \"\n            \"alike '10G', '500M', '50K'.\" % (text, type(text)))\n    return size\n\n\ndef mkdirp(d):\n    \"\"\"Ensure directory d exists (like mkdir -p on Unix)\n    No guarantee that the directory is writable.\n    \"\"\"\n    try:\n        os.makedirs(d)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n\n\n# if a rmtree operation fails in rm_subdirs, wait for this much time (in secs),\n# then retry once. if it still fails, raise the exception\nRM_SUBDIRS_RETRY_TIME = 0.1\n\n\ndef rm_subdirs(path, onerror=None):\n    \"\"\"Remove all subdirectories in this path.\n\n    The directory indicated by `path` is left in place, and its subdirectories\n    are erased.\n\n    If onerror is set, it is called to handle the error with arguments (func,\n    path, exc_info) where func is os.listdir, os.remove, or os.rmdir;\n    path is the argument to that function that caused it to fail; and\n    exc_info is a tuple returned by sys.exc_info().  If onerror is None,\n    an exception is raised.\n    \"\"\"\n\n    # NOTE this code is adapted from the one in shutil.rmtree, and is\n    # just as fast\n\n    names = []\n    try:\n        names = os.listdir(path)\n    except os.error as err:\n        if onerror is not None:\n            onerror(os.listdir, path, sys.exc_info())\n        else:\n            raise\n\n    for name in names:\n        fullname = os.path.join(path, name)\n        if os.path.isdir(fullname):\n            if onerror is not None:\n                shutil.rmtree(fullname, False, onerror)\n            else:\n                # allow the rmtree to fail once, wait and re-try.\n                # if the error is raised again, fail\n                err_count = 0\n                while True:\n                    try:\n                        shutil.rmtree(fullname, False, None)\n                        break\n                    except os.error:\n                        if err_count > 0:\n                            raise\n                        err_count += 1\n                        time.sleep(RM_SUBDIRS_RETRY_TIME)\n"
    },
    {
      "filename": "sklearn/externals/_joblib/format_stack.py",
      "content": "\"\"\"\nRepresent an exception with a lot of information.\n\nProvides 2 useful functions:\n\nformat_exc: format an exception into a complete traceback, with full\n            debugging instruction.\n\nformat_outer_frames: format the current position in the stack call.\n\nAdapted from IPython's VerboseTB.\n\"\"\"\n# Authors: Gael Varoquaux < gael dot varoquaux at normalesup dot org >\n#          Nathaniel Gray <n8gray@caltech.edu>\n#          Fernando Perez <fperez@colorado.edu>\n# Copyright: 2010, Gael Varoquaux\n#            2001-2004, Fernando Perez\n#            2001 Nathaniel Gray\n# License: BSD 3 clause\n\n\nimport inspect\nimport keyword\nimport linecache\nimport os\nimport pydoc\nimport sys\nimport time\nimport tokenize\nimport traceback\n\ntry:                           # Python 2\n    generate_tokens = tokenize.generate_tokens\nexcept AttributeError:         # Python 3\n    generate_tokens = tokenize.tokenize\n\nINDENT = ' ' * 8\n\n\n###############################################################################\n# some internal-use functions\ndef safe_repr(value):\n    \"\"\"Hopefully pretty robust repr equivalent.\"\"\"\n    # this is pretty horrible but should always return *something*\n    try:\n        return pydoc.text.repr(value)\n    except KeyboardInterrupt:\n        raise\n    except:\n        try:\n            return repr(value)\n        except KeyboardInterrupt:\n            raise\n        except:\n            try:\n                # all still in an except block so we catch\n                # getattr raising\n                name = getattr(value, '__name__', None)\n                if name:\n                    # ick, recursion\n                    return safe_repr(name)\n                klass = getattr(value, '__class__', None)\n                if klass:\n                    return '%s instance' % safe_repr(klass)\n            except KeyboardInterrupt:\n                raise\n            except:\n                return 'UNRECOVERABLE REPR FAILURE'\n\n\ndef eq_repr(value, repr=safe_repr):\n    return '=%s' % repr(value)\n\n\n###############################################################################\ndef uniq_stable(elems):\n    \"\"\"uniq_stable(elems) -> list\n\n    Return from an iterable, a list of all the unique elements in the input,\n    but maintaining the order in which they first appear.\n\n    A naive solution to this problem which just makes a dictionary with the\n    elements as keys fails to respect the stability condition, since\n    dictionaries are unsorted by nature.\n\n    Note: All elements in the input must be hashable.\n    \"\"\"\n    unique = []\n    unique_set = set()\n    for nn in elems:\n        if nn not in unique_set:\n            unique.append(nn)\n            unique_set.add(nn)\n    return unique\n\n\n###############################################################################\ndef fix_frame_records_filenames(records):\n    \"\"\"Try to fix the filenames in each record from inspect.getinnerframes().\n\n    Particularly, modules loaded from within zip files have useless filenames\n    attached to their code object, and inspect.getinnerframes() just uses it.\n    \"\"\"\n    fixed_records = []\n    for frame, filename, line_no, func_name, lines, index in records:\n        # Look inside the frame's globals dictionary for __file__, which should\n        # be better.\n        better_fn = frame.f_globals.get('__file__', None)\n        if isinstance(better_fn, str):\n            # Check the type just in case someone did something weird with\n            # __file__. It might also be None if the error occurred during\n            # import.\n            filename = better_fn\n        fixed_records.append((frame, filename, line_no, func_name, lines,\n                              index))\n    return fixed_records\n\n\ndef _fixed_getframes(etb, context=1, tb_offset=0):\n    LNUM_POS, LINES_POS, INDEX_POS = 2, 4, 5\n\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n\n    # If the error is at the console, don't build any context, since it would\n    # otherwise produce 5 blank lines printed out (there is no file at the\n    # console)\n    rec_check = records[tb_offset:]\n    try:\n        rname = rec_check[0][1]\n        if rname == '<ipython console>' or rname.endswith('<string>'):\n            return rec_check\n    except IndexError:\n        pass\n\n    aux = traceback.extract_tb(etb)\n    assert len(records) == len(aux)\n    for i, (file, lnum, _, _) in enumerate(aux):\n        maybe_start = lnum - 1 - context // 2\n        start = max(maybe_start, 0)\n        end = start + context\n        lines = linecache.getlines(file)[start:end]\n        buf = list(records[i])\n        buf[LNUM_POS] = lnum\n        buf[INDEX_POS] = lnum - 1 - start\n        buf[LINES_POS] = lines\n        records[i] = tuple(buf)\n    return records[tb_offset:]\n\n\ndef _format_traceback_lines(lnum, index, lines, lvals=None):\n    numbers_width = 7\n    res = []\n    i = lnum - index\n\n    for line in lines:\n        if i == lnum:\n            # This is the line with the error\n            pad = numbers_width - len(str(i))\n            if pad >= 3:\n                marker = '-' * (pad - 3) + '-> '\n            elif pad == 2:\n                marker = '> '\n            elif pad == 1:\n                marker = '>'\n            else:\n                marker = ''\n            num = marker + str(i)\n        else:\n            num = '%*s' % (numbers_width, i)\n        line = '%s %s' % (num, line)\n\n        res.append(line)\n        if lvals and i == lnum:\n            res.append(lvals + '\\n')\n        i = i + 1\n    return res\n\n\ndef format_records(records):   # , print_globals=False):\n    # Loop over all records printing context and info\n    frames = []\n    abspath = os.path.abspath\n    for frame, file, lnum, func, lines, index in records:\n        try:\n            file = file and abspath(file) or '?'\n        except OSError:\n            # if file is '<console>' or something not in the filesystem,\n            # the abspath call will throw an OSError.  Just ignore it and\n            # keep the original file string.\n            pass\n\n        if file.endswith('.pyc'):\n            file = file[:-4] + '.py'\n\n        link = file\n\n        args, varargs, varkw, locals = inspect.getargvalues(frame)\n\n        if func == '?':\n            call = ''\n        else:\n            # Decide whether to include variable details or not\n            try:\n                call = 'in %s%s' % (func, inspect.formatargvalues(args,\n                                            varargs, varkw, locals,\n                                            formatvalue=eq_repr))\n            except KeyError:\n                # Very odd crash from inspect.formatargvalues().  The\n                # scenario under which it appeared was a call to\n                # view(array,scale) in NumTut.view.view(), where scale had\n                # been defined as a scalar (it should be a tuple). Somehow\n                # inspect messes up resolving the argument list of view()\n                # and barfs out. At some point I should dig into this one\n                # and file a bug report about it.\n                print(\"\\nJoblib's exception reporting continues...\\n\")\n                call = 'in %s(***failed resolving arguments***)' % func\n\n        # Initialize a list of names on the current line, which the\n        # tokenizer below will populate.\n        names = []\n\n        def tokeneater(token_type, token, start, end, line):\n            \"\"\"Stateful tokeneater which builds dotted names.\n\n            The list of names it appends to (from the enclosing scope) can\n            contain repeated composite names.  This is unavoidable, since\n            there is no way to disambiguate partial dotted structures until\n            the full list is known.  The caller is responsible for pruning\n            the final list of duplicates before using it.\"\"\"\n\n            # build composite names\n            if token == '.':\n                try:\n                    names[-1] += '.'\n                    # store state so the next token is added for x.y.z names\n                    tokeneater.name_cont = True\n                    return\n                except IndexError:\n                    pass\n            if token_type == tokenize.NAME and token not in keyword.kwlist:\n                if tokeneater.name_cont:\n                    # Dotted names\n                    names[-1] += token\n                    tokeneater.name_cont = False\n                else:\n                    # Regular new names.  We append everything, the caller\n                    # will be responsible for pruning the list later.  It's\n                    # very tricky to try to prune as we go, b/c composite\n                    # names can fool us.  The pruning at the end is easy\n                    # to do (or the caller can print a list with repeated\n                    # names if so desired.\n                    names.append(token)\n            elif token_type == tokenize.NEWLINE:\n                raise IndexError\n        # we need to store a bit of state in the tokenizer to build\n        # dotted names\n        tokeneater.name_cont = False\n\n        def linereader(file=file, lnum=[lnum], getline=linecache.getline):\n            line = getline(file, lnum[0])\n            lnum[0] += 1\n            return line\n\n        # Build the list of names on this line of code where the exception\n        # occurred.\n        try:\n            # This builds the names list in-place by capturing it from the\n            # enclosing scope.\n            for token in generate_tokens(linereader):\n                tokeneater(*token)\n        except (IndexError, UnicodeDecodeError, SyntaxError):\n            # signals exit of tokenizer\n            # SyntaxError can happen when trying to tokenize\n            # a compiled (e.g. .so or .pyd) extension\n            pass\n        except tokenize.TokenError as msg:\n            _m = (\"An unexpected error occurred while tokenizing input file %s\\n\"\n                  \"The following traceback may be corrupted or invalid\\n\"\n                  \"The error message is: %s\\n\" % (file, msg))\n            print(_m)\n\n        # prune names list of duplicates, but keep the right order\n        unique_names = uniq_stable(names)\n\n        # Start loop over vars\n        lvals = []\n        for name_full in unique_names:\n            name_base = name_full.split('.', 1)[0]\n            if name_base in frame.f_code.co_varnames:\n                if name_base in locals.keys():\n                    try:\n                        value = safe_repr(eval(name_full, locals))\n                    except:\n                        value = \"undefined\"\n                else:\n                    value = \"undefined\"\n                name = name_full\n                lvals.append('%s = %s' % (name, value))\n            #elif print_globals:\n            #    if frame.f_globals.has_key(name_base):\n            #        try:\n            #            value = safe_repr(eval(name_full,frame.f_globals))\n            #        except:\n            #            value = \"undefined\"\n            #    else:\n            #        value = \"undefined\"\n            #    name = 'global %s' % name_full\n            #    lvals.append('%s = %s' % (name,value))\n        if lvals:\n            lvals = '%s%s' % (INDENT, ('\\n%s' % INDENT).join(lvals))\n        else:\n            lvals = ''\n\n        level = '%s\\n%s %s\\n' % (75 * '.', link, call)\n\n        if index is None:\n            frames.append(level)\n        else:\n            frames.append('%s%s' % (level, ''.join(\n                _format_traceback_lines(lnum, index, lines, lvals))))\n\n    return frames\n\n\n###############################################################################\ndef format_exc(etype, evalue, etb, context=5, tb_offset=0):\n    \"\"\" Return a nice text document describing the traceback.\n\n        Parameters\n        -----------\n        etype, evalue, etb: as returned by sys.exc_info\n        context: number of lines of the source file to plot\n        tb_offset: the number of stack frame not to use (0 = use all)\n\n    \"\"\"\n    # some locals\n    try:\n        etype = etype.__name__\n    except AttributeError:\n        pass\n\n    # Header with the exception type, python version, and date\n    pyver = 'Python ' + sys.version.split()[0] + ': ' + sys.executable\n    date = time.ctime(time.time())\n    pid = 'PID: %i' % os.getpid()\n\n    head = '%s%s%s\\n%s%s%s' % (\n        etype, ' ' * (75 - len(str(etype)) - len(date)),\n        date, pid, ' ' * (75 - len(str(pid)) - len(pyver)),\n        pyver)\n\n    # Drop topmost frames if requested\n    records = _fixed_getframes(etb, context, tb_offset)\n\n    # Get (safely) a string form of the exception info\n    try:\n        etype_str, evalue_str = map(str, (etype, evalue))\n    except:\n        # User exception is improperly defined.\n        etype, evalue = str, sys.exc_info()[:2]\n        etype_str, evalue_str = map(str, (etype, evalue))\n    # ... and format it\n    exception = ['%s: %s' % (etype_str, evalue_str)]\n    frames = format_records(records)\n    return '%s\\n%s\\n%s' % (head, '\\n'.join(frames), ''.join(exception[0]))\n\n\n###############################################################################\ndef format_outer_frames(context=5, stack_start=None, stack_end=None,\n                        ignore_ipython=True):\n    LNUM_POS, LINES_POS, INDEX_POS = 2, 4, 5\n    records = inspect.getouterframes(inspect.currentframe())\n    output = list()\n\n    for i, (frame, filename, line_no, func_name, lines, index) \\\n                                                in enumerate(records):\n        # Look inside the frame's globals dictionary for __file__, which should\n        # be better.\n        better_fn = frame.f_globals.get('__file__', None)\n        if isinstance(better_fn, str):\n            # Check the type just in case someone did something weird with\n            # __file__. It might also be None if the error occurred during\n            # import.\n            filename = better_fn\n            if filename.endswith('.pyc'):\n                filename = filename[:-4] + '.py'\n        if ignore_ipython:\n            # Hack to avoid printing the internals of IPython\n            if (os.path.basename(filename) in ('iplib.py', 'py3compat.py')\n                        and func_name in ('execfile', 'safe_execfile', 'runcode')):\n                break\n        maybe_start = line_no - 1 - context // 2\n        start = max(maybe_start, 0)\n        end = start + context\n        lines = linecache.getlines(filename)[start:end]\n        buf = list(records[i])\n        buf[LNUM_POS] = line_no\n        buf[INDEX_POS] = line_no - 1 - start\n        buf[LINES_POS] = lines\n        output.append(tuple(buf))\n    return '\\n'.join(format_records(output[stack_end:stack_start:-1]))\n"
    },
    {
      "filename": "sklearn/externals/_joblib/func_inspect.py",
      "content": "\"\"\"\nMy own variation on function-specific inspect-like features.\n\"\"\"\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n# Copyright (c) 2009 Gael Varoquaux\n# License: BSD Style, 3 clauses.\n\nfrom itertools import islice\nimport inspect\nimport warnings\nimport re\nimport os\n\nfrom ._compat import _basestring\nfrom .logger import pformat\nfrom ._memory_helpers import open_py_source\nfrom ._compat import PY3_OR_LATER\n\n\ndef get_func_code(func):\n    \"\"\" Attempts to retrieve a reliable function code hash.\n\n        The reason we don't use inspect.getsource is that it caches the\n        source, whereas we want this to be modified on the fly when the\n        function is modified.\n\n        Returns\n        -------\n        func_code: string\n            The function code\n        source_file: string\n            The path to the file in which the function is defined.\n        first_line: int\n            The first line of the code in the source file.\n\n        Notes\n        ------\n        This function does a bit more magic than inspect, and is thus\n        more robust.\n    \"\"\"\n    source_file = None\n    try:\n        code = func.__code__\n        source_file = code.co_filename\n        if not os.path.exists(source_file):\n            # Use inspect for lambda functions and functions defined in an\n            # interactive shell, or in doctests\n            source_code = ''.join(inspect.getsourcelines(func)[0])\n            line_no = 1\n            if source_file.startswith('<doctest '):\n                source_file, line_no = re.match(\n                    r'\\<doctest (.*\\.rst)\\[(.*)\\]\\>', source_file).groups()\n                line_no = int(line_no)\n                source_file = '<doctest %s>' % source_file\n            return source_code, source_file, line_no\n        # Try to retrieve the source code.\n        with open_py_source(source_file) as source_file_obj:\n            first_line = code.co_firstlineno\n            # All the lines after the function definition:\n            source_lines = list(islice(source_file_obj, first_line - 1, None))\n        return ''.join(inspect.getblock(source_lines)), source_file, first_line\n    except:\n        # If the source code fails, we use the hash. This is fragile and\n        # might change from one session to another.\n        if hasattr(func, '__code__'):\n            # Python 3.X\n            return str(func.__code__.__hash__()), source_file, -1\n        else:\n            # Weird objects like numpy ufunc don't have __code__\n            # This is fragile, as quite often the id of the object is\n            # in the repr, so it might not persist across sessions,\n            # however it will work for ufuncs.\n            return repr(func), source_file, -1\n\n\ndef _clean_win_chars(string):\n    \"\"\"Windows cannot encode some characters in filename.\"\"\"\n    import urllib\n    if hasattr(urllib, 'quote'):\n        quote = urllib.quote\n    else:\n        # In Python 3, quote is elsewhere\n        import urllib.parse\n        quote = urllib.parse.quote\n    for char in ('<', '>', '!', ':', '\\\\'):\n        string = string.replace(char, quote(char))\n    return string\n\n\ndef get_func_name(func, resolv_alias=True, win_characters=True):\n    \"\"\" Return the function import path (as a list of module names), and\n        a name for the function.\n\n        Parameters\n        ----------\n        func: callable\n            The func to inspect\n        resolv_alias: boolean, optional\n            If true, possible local aliases are indicated.\n        win_characters: boolean, optional\n            If true, substitute special characters using urllib.quote\n            This is useful in Windows, as it cannot encode some filenames\n    \"\"\"\n    if hasattr(func, '__module__'):\n        module = func.__module__\n    else:\n        try:\n            module = inspect.getmodule(func)\n        except TypeError:\n            if hasattr(func, '__class__'):\n                module = func.__class__.__module__\n            else:\n                module = 'unknown'\n    if module is None:\n        # Happens in doctests, eg\n        module = ''\n    if module == '__main__':\n        try:\n            filename = os.path.abspath(inspect.getsourcefile(func))\n        except:\n            filename = None\n        if filename is not None:\n            # mangling of full path to filename\n            parts = filename.split(os.sep)\n            if parts[-1].startswith('<ipython-input'):\n                # function is defined in an IPython session. The filename\n                # will change with every new kernel instance. This hack\n                # always returns the same filename\n                parts[-1] = '__ipython-input__'\n            filename = '-'.join(parts)\n            if filename.endswith('.py'):\n                filename = filename[:-3]\n            module = module + '-' + filename\n    module = module.split('.')\n    if hasattr(func, 'func_name'):\n        name = func.func_name\n    elif hasattr(func, '__name__'):\n        name = func.__name__\n    else:\n        name = 'unknown'\n    # Hack to detect functions not defined at the module-level\n    if resolv_alias:\n        # TODO: Maybe add a warning here?\n        if hasattr(func, 'func_globals') and name in func.func_globals:\n            if not func.func_globals[name] is func:\n                name = '%s-alias' % name\n    if inspect.ismethod(func):\n        # We need to add the name of the class\n        if hasattr(func, 'im_class'):\n            klass = func.im_class\n            module.append(klass.__name__)\n    if os.name == 'nt' and win_characters:\n        # Stupid windows can't encode certain characters in filenames\n        name = _clean_win_chars(name)\n        module = [_clean_win_chars(s) for s in module]\n    return module, name\n\n\ndef getfullargspec(func):\n    \"\"\"Compatibility function to provide inspect.getfullargspec in Python 2\n\n    This should be rewritten using a backport of Python 3 signature\n    once we drop support for Python 2.6. We went for a simpler\n    approach at the time of writing because signature uses OrderedDict\n    which is not available in Python 2.6.\n    \"\"\"\n    try:\n        return inspect.getfullargspec(func)\n    except AttributeError:\n        arg_spec = inspect.getargspec(func)\n        import collections\n        tuple_fields = ('args varargs varkw defaults kwonlyargs '\n                        'kwonlydefaults annotations')\n        tuple_type = collections.namedtuple('FullArgSpec', tuple_fields)\n\n        return tuple_type(args=arg_spec.args,\n                          varargs=arg_spec.varargs,\n                          varkw=arg_spec.keywords,\n                          defaults=arg_spec.defaults,\n                          kwonlyargs=[],\n                          kwonlydefaults=None,\n                          annotations={})\n\n\ndef _signature_str(function_name, arg_spec):\n    \"\"\"Helper function to output a function signature\"\"\"\n    # inspect.formatargspec can not deal with the same\n    # number of arguments in python 2 and 3\n    arg_spec_for_format = arg_spec[:7 if PY3_OR_LATER else 4]\n\n    arg_spec_str = inspect.formatargspec(*arg_spec_for_format)\n    return '{}{}'.format(function_name, arg_spec_str)\n\n\ndef _function_called_str(function_name, args, kwargs):\n    \"\"\"Helper function to output a function call\"\"\"\n    template_str = '{0}({1}, {2})'\n\n    args_str = repr(args)[1:-1]\n    kwargs_str = ', '.join('%s=%s' % (k, v)\n                           for k, v in kwargs.items())\n    return template_str.format(function_name, args_str,\n                               kwargs_str)\n\n\ndef filter_args(func, ignore_lst, args=(), kwargs=dict()):\n    \"\"\" Filters the given args and kwargs using a list of arguments to\n        ignore, and a function specification.\n\n        Parameters\n        ----------\n        func: callable\n            Function giving the argument specification\n        ignore_lst: list of strings\n            List of arguments to ignore (either a name of an argument\n            in the function spec, or '*', or '**')\n        *args: list\n            Positional arguments passed to the function.\n        **kwargs: dict\n            Keyword arguments passed to the function\n\n        Returns\n        -------\n        filtered_args: list\n            List of filtered positional and keyword arguments.\n    \"\"\"\n    args = list(args)\n    if isinstance(ignore_lst, _basestring):\n        # Catch a common mistake\n        raise ValueError(\n            'ignore_lst must be a list of parameters to ignore '\n            '%s (type %s) was given' % (ignore_lst, type(ignore_lst)))\n    # Special case for functools.partial objects\n    if (not inspect.ismethod(func) and not inspect.isfunction(func)):\n        if ignore_lst:\n            warnings.warn('Cannot inspect object %s, ignore list will '\n                          'not work.' % func, stacklevel=2)\n        return {'*': args, '**': kwargs}\n    arg_spec = getfullargspec(func)\n    arg_names = arg_spec.args + arg_spec.kwonlyargs\n    arg_defaults = arg_spec.defaults or ()\n    arg_defaults = arg_defaults + tuple(arg_spec.kwonlydefaults[k]\n                                        for k in arg_spec.kwonlyargs)\n    arg_varargs = arg_spec.varargs\n    arg_varkw = arg_spec.varkw\n\n    if inspect.ismethod(func):\n        # First argument is 'self', it has been removed by Python\n        # we need to add it back:\n        args = [func.__self__, ] + args\n    # XXX: Maybe I need an inspect.isbuiltin to detect C-level methods, such\n    # as on ndarrays.\n\n    _, name = get_func_name(func, resolv_alias=False)\n    arg_dict = dict()\n    arg_position = -1\n    for arg_position, arg_name in enumerate(arg_names):\n        if arg_position < len(args):\n            # Positional argument or keyword argument given as positional\n            if arg_name not in arg_spec.kwonlyargs:\n                arg_dict[arg_name] = args[arg_position]\n            else:\n                raise ValueError(\n                    \"Keyword-only parameter '%s' was passed as \"\n                    'positional parameter for %s:\\n'\n                    '     %s was called.'\n                    % (arg_name,\n                       _signature_str(name, arg_spec),\n                       _function_called_str(name, args, kwargs))\n                )\n\n        else:\n            position = arg_position - len(arg_names)\n            if arg_name in kwargs:\n                arg_dict[arg_name] = kwargs.pop(arg_name)\n            else:\n                try:\n                    arg_dict[arg_name] = arg_defaults[position]\n                except (IndexError, KeyError):\n                    # Missing argument\n                    raise ValueError(\n                        'Wrong number of arguments for %s:\\n'\n                        '     %s was called.'\n                        % (_signature_str(name, arg_spec),\n                           _function_called_str(name, args, kwargs))\n                    )\n\n    varkwargs = dict()\n    for arg_name, arg_value in sorted(kwargs.items()):\n        if arg_name in arg_dict:\n            arg_dict[arg_name] = arg_value\n        elif arg_varkw is not None:\n            varkwargs[arg_name] = arg_value\n        else:\n            raise TypeError(\"Ignore list for %s() contains an unexpected \"\n                            \"keyword argument '%s'\" % (name, arg_name))\n\n    if arg_varkw is not None:\n        arg_dict['**'] = varkwargs\n    if arg_varargs is not None:\n        varargs = args[arg_position + 1:]\n        arg_dict['*'] = varargs\n\n    # Now remove the arguments to be ignored\n    for item in ignore_lst:\n        if item in arg_dict:\n            arg_dict.pop(item)\n        else:\n            raise ValueError(\"Ignore list: argument '%s' is not defined for \"\n                             \"function %s\"\n                             % (item,\n                                _signature_str(name, arg_spec))\n                             )\n    # XXX: Return a sorted list of pairs?\n    return arg_dict\n\n\ndef _format_arg(arg):\n    formatted_arg = pformat(arg, indent=2)\n    if len(formatted_arg) > 1500:\n        formatted_arg = '%s...' % formatted_arg[:700]\n    return formatted_arg\n\n\ndef format_signature(func, *args, **kwargs):\n    # XXX: Should this use inspect.formatargvalues/formatargspec?\n    module, name = get_func_name(func)\n    module = [m for m in module if m]\n    if module:\n        module.append(name)\n        module_path = '.'.join(module)\n    else:\n        module_path = name\n    arg_str = list()\n    previous_length = 0\n    for arg in args:\n        formatted_arg = _format_arg(arg)\n        if previous_length > 80:\n            formatted_arg = '\\n%s' % formatted_arg\n        previous_length = len(formatted_arg)\n        arg_str.append(formatted_arg)\n    arg_str.extend(['%s=%s' % (v, _format_arg(i)) for v, i in kwargs.items()])\n    arg_str = ', '.join(arg_str)\n\n    signature = '%s(%s)' % (name, arg_str)\n    return module_path, signature\n\n\ndef format_call(func, args, kwargs, object_name=\"Memory\"):\n    \"\"\" Returns a nicely formatted statement displaying the function\n        call with the given arguments.\n    \"\"\"\n    path, signature = format_signature(func, *args, **kwargs)\n    msg = '%s\\n[%s] Calling %s...\\n%s' % (80 * '_', object_name,\n                                          path, signature)\n    return msg\n    # XXX: Not using logging framework\n    # self.debug(msg)\n"
    },
    {
      "filename": "sklearn/externals/_joblib/hashing.py",
      "content": "\"\"\"\nFast cryptographic hash of Python objects, with a special case for fast\nhashing of numpy arrays.\n\"\"\"\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n# Copyright (c) 2009 Gael Varoquaux\n# License: BSD Style, 3 clauses.\n\nimport pickle\nimport hashlib\nimport sys\nimport types\nimport struct\nimport io\nimport decimal\n\nfrom ._compat import _bytes_or_unicode, PY3_OR_LATER\n\n\nif PY3_OR_LATER:\n    Pickler = pickle._Pickler\nelse:\n    Pickler = pickle.Pickler\n\n\nclass _ConsistentSet(object):\n    \"\"\" Class used to ensure the hash of Sets is preserved\n        whatever the order of its items.\n    \"\"\"\n    def __init__(self, set_sequence):\n        # Forces order of elements in set to ensure consistent hash.\n        try:\n            # Trying first to order the set assuming the type of elements is\n            # consistent and orderable.\n            # This fails on python 3 when elements are unorderable\n            # but we keep it in a try as it's faster.\n            self._sequence = sorted(set_sequence)\n        except (TypeError, decimal.InvalidOperation):\n            # If elements are unorderable, sorting them using their hash.\n            # This is slower but works in any case.\n            self._sequence = sorted((hash(e) for e in set_sequence))\n\n\nclass _MyHash(object):\n    \"\"\" Class used to hash objects that won't normally pickle \"\"\"\n\n    def __init__(self, *args):\n        self.args = args\n\n\nclass Hasher(Pickler):\n    \"\"\" A subclass of pickler, to do cryptographic hashing, rather than\n        pickling.\n    \"\"\"\n\n    def __init__(self, hash_name='md5'):\n        self.stream = io.BytesIO()\n        # By default we want a pickle protocol that only changes with\n        # the major python version and not the minor one\n        protocol = (pickle.DEFAULT_PROTOCOL if PY3_OR_LATER\n                    else pickle.HIGHEST_PROTOCOL)\n        Pickler.__init__(self, self.stream, protocol=protocol)\n        # Initialise the hash obj\n        self._hash = hashlib.new(hash_name)\n\n    def hash(self, obj, return_digest=True):\n        try:\n            self.dump(obj)\n        except pickle.PicklingError as e:\n            e.args += ('PicklingError while hashing %r: %r' % (obj, e),)\n            raise\n        dumps = self.stream.getvalue()\n        self._hash.update(dumps)\n        if return_digest:\n            return self._hash.hexdigest()\n\n    def save(self, obj):\n        if isinstance(obj, (types.MethodType, type({}.pop))):\n            # the Pickler cannot pickle instance methods; here we decompose\n            # them into components that make them uniquely identifiable\n            if hasattr(obj, '__func__'):\n                func_name = obj.__func__.__name__\n            else:\n                func_name = obj.__name__\n            inst = obj.__self__\n            if type(inst) == type(pickle):\n                obj = _MyHash(func_name, inst.__name__)\n            elif inst is None:\n                # type(None) or type(module) do not pickle\n                obj = _MyHash(func_name, inst)\n            else:\n                cls = obj.__self__.__class__\n                obj = _MyHash(func_name, inst, cls)\n        Pickler.save(self, obj)\n\n    def memoize(self, obj):\n        # We want hashing to be sensitive to value instead of reference.\n        # For example we want ['aa', 'aa'] and ['aa', 'aaZ'[:2]]\n        # to hash to the same value and that's why we disable memoization\n        # for strings\n        if isinstance(obj, _bytes_or_unicode):\n            return\n        Pickler.memoize(self, obj)\n\n    # The dispatch table of the pickler is not accessible in Python\n    # 3, as these lines are only bugware for IPython, we skip them.\n    def save_global(self, obj, name=None, pack=struct.pack):\n        # We have to override this method in order to deal with objects\n        # defined interactively in IPython that are not injected in\n        # __main__\n        kwargs = dict(name=name, pack=pack)\n        if sys.version_info >= (3, 4):\n            del kwargs['pack']\n        try:\n            Pickler.save_global(self, obj, **kwargs)\n        except pickle.PicklingError:\n            Pickler.save_global(self, obj, **kwargs)\n            module = getattr(obj, \"__module__\", None)\n            if module == '__main__':\n                my_name = name\n                if my_name is None:\n                    my_name = obj.__name__\n                mod = sys.modules[module]\n                if not hasattr(mod, my_name):\n                    # IPython doesn't inject the variables define\n                    # interactively in __main__\n                    setattr(mod, my_name, obj)\n\n    dispatch = Pickler.dispatch.copy()\n    # builtin\n    dispatch[type(len)] = save_global\n    # type\n    dispatch[type(object)] = save_global\n    # classobj\n    dispatch[type(Pickler)] = save_global\n    # function\n    dispatch[type(pickle.dump)] = save_global\n\n    def _batch_setitems(self, items):\n        # forces order of keys in dict to ensure consistent hash.\n        try:\n            # Trying first to compare dict assuming the type of keys is\n            # consistent and orderable.\n            # This fails on python 3 when keys are unorderable\n            # but we keep it in a try as it's faster.\n            Pickler._batch_setitems(self, iter(sorted(items)))\n        except TypeError:\n            # If keys are unorderable, sorting them using their hash. This is\n            # slower but works in any case.\n            Pickler._batch_setitems(self, iter(sorted((hash(k), v)\n                                                      for k, v in items)))\n\n    def save_set(self, set_items):\n        # forces order of items in Set to ensure consistent hash\n        Pickler.save(self, _ConsistentSet(set_items))\n\n    dispatch[type(set())] = save_set\n\n\nclass NumpyHasher(Hasher):\n    \"\"\" Special case the hasher for when numpy is loaded.\n    \"\"\"\n\n    def __init__(self, hash_name='md5', coerce_mmap=False):\n        \"\"\"\n            Parameters\n            ----------\n            hash_name: string\n                The hash algorithm to be used\n            coerce_mmap: boolean\n                Make no difference between np.memmap and np.ndarray\n                objects.\n        \"\"\"\n        self.coerce_mmap = coerce_mmap\n        Hasher.__init__(self, hash_name=hash_name)\n        # delayed import of numpy, to avoid tight coupling\n        import numpy as np\n        self.np = np\n        if hasattr(np, 'getbuffer'):\n            self._getbuffer = np.getbuffer\n        else:\n            self._getbuffer = memoryview\n\n    def save(self, obj):\n        \"\"\" Subclass the save method, to hash ndarray subclass, rather\n            than pickling them. Off course, this is a total abuse of\n            the Pickler class.\n        \"\"\"\n        if isinstance(obj, self.np.ndarray) and not obj.dtype.hasobject:\n            # Compute a hash of the object\n            # The update function of the hash requires a c_contiguous buffer.\n            if obj.shape == ():\n                # 0d arrays need to be flattened because viewing them as bytes\n                # raises a ValueError exception.\n                obj_c_contiguous = obj.flatten()\n            elif obj.flags.c_contiguous:\n                obj_c_contiguous = obj\n            elif obj.flags.f_contiguous:\n                obj_c_contiguous = obj.T\n            else:\n                # Cater for non-single-segment arrays: this creates a\n                # copy, and thus aleviates this issue.\n                # XXX: There might be a more efficient way of doing this\n                obj_c_contiguous = obj.flatten()\n\n            # memoryview is not supported for some dtypes, e.g. datetime64, see\n            # https://github.com/numpy/numpy/issues/4983. The\n            # workaround is to view the array as bytes before\n            # taking the memoryview.\n            self._hash.update(\n                self._getbuffer(obj_c_contiguous.view(self.np.uint8)))\n\n            # We store the class, to be able to distinguish between\n            # Objects with the same binary content, but different\n            # classes.\n            if self.coerce_mmap and isinstance(obj, self.np.memmap):\n                # We don't make the difference between memmap and\n                # normal ndarrays, to be able to reload previously\n                # computed results with memmap.\n                klass = self.np.ndarray\n            else:\n                klass = obj.__class__\n            # We also return the dtype and the shape, to distinguish\n            # different views on the same data with different dtypes.\n\n            # The object will be pickled by the pickler hashed at the end.\n            obj = (klass, ('HASHED', obj.dtype, obj.shape, obj.strides))\n        elif isinstance(obj, self.np.dtype):\n            # Atomic dtype objects are interned by their default constructor:\n            # np.dtype('f8') is np.dtype('f8')\n            # This interning is not maintained by a\n            # pickle.loads + pickle.dumps cycle, because __reduce__\n            # uses copy=True in the dtype constructor. This\n            # non-deterministic behavior causes the internal memoizer\n            # of the hasher to generate different hash values\n            # depending on the history of the dtype object.\n            # To prevent the hash from being sensitive to this, we use\n            # .descr which is a full (and never interned) description of\n            # the array dtype according to the numpy doc.\n            klass = obj.__class__\n            obj = (klass, ('HASHED', obj.descr))\n        Hasher.save(self, obj)\n\n\ndef hash(obj, hash_name='md5', coerce_mmap=False):\n    \"\"\" Quick calculation of a hash to identify uniquely Python objects\n        containing numpy arrays.\n\n\n        Parameters\n        -----------\n        hash_name: 'md5' or 'sha1'\n            Hashing algorithm used. sha1 is supposedly safer, but md5 is\n            faster.\n        coerce_mmap: boolean\n            Make no difference between np.memmap and np.ndarray\n    \"\"\"\n    if 'numpy' in sys.modules:\n        hasher = NumpyHasher(hash_name=hash_name, coerce_mmap=coerce_mmap)\n    else:\n        hasher = Hasher(hash_name=hash_name)\n    return hasher.hash(obj)\n"
    },
    {
      "filename": "sklearn/externals/_joblib/logger.py",
      "content": "\"\"\"\nHelpers for logging.\n\nThis module needs much love to become useful.\n\"\"\"\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n# Copyright (c) 2008 Gael Varoquaux\n# License: BSD Style, 3 clauses.\n\nfrom __future__ import print_function\n\nimport time\nimport sys\nimport os\nimport shutil\nimport logging\nimport pprint\n\nfrom .disk import mkdirp\n\n\ndef _squeeze_time(t):\n    \"\"\"Remove .1s to the time under Windows: this is the time it take to\n    stat files. This is needed to make results similar to timings under\n    Unix, for tests\n    \"\"\"\n    if sys.platform.startswith('win'):\n        return max(0, t - .1)\n    else:\n        return t\n\n\ndef format_time(t):\n    t = _squeeze_time(t)\n    return \"%.1fs, %.1fmin\" % (t, t / 60.)\n\n\ndef short_format_time(t):\n    t = _squeeze_time(t)\n    if t > 60:\n        return \"%4.1fmin\" % (t / 60.)\n    else:\n        return \" %5.1fs\" % (t)\n\n\ndef pformat(obj, indent=0, depth=3):\n    if 'numpy' in sys.modules:\n        import numpy as np\n        print_options = np.get_printoptions()\n        np.set_printoptions(precision=6, threshold=64, edgeitems=1)\n    else:\n        print_options = None\n    out = pprint.pformat(obj, depth=depth, indent=indent)\n    if print_options:\n        np.set_printoptions(**print_options)\n    return out\n\n\n###############################################################################\n# class `Logger`\n###############################################################################\nclass Logger(object):\n    \"\"\" Base class for logging messages.\n    \"\"\"\n\n    def __init__(self, depth=3):\n        \"\"\"\n            Parameters\n            ----------\n            depth: int, optional\n                The depth of objects printed.\n        \"\"\"\n        self.depth = depth\n\n    def warn(self, msg):\n        logging.warning(\"[%s]: %s\" % (self, msg))\n\n    def debug(self, msg):\n        # XXX: This conflicts with the debug flag used in children class\n        logging.debug(\"[%s]: %s\" % (self, msg))\n\n    def format(self, obj, indent=0):\n        \"\"\" Return the formatted representation of the object.\n        \"\"\"\n        return pformat(obj, indent=indent, depth=self.depth)\n\n\n###############################################################################\n# class `PrintTime`\n###############################################################################\nclass PrintTime(object):\n    \"\"\" Print and log messages while keeping track of time.\n    \"\"\"\n\n    def __init__(self, logfile=None, logdir=None):\n        if logfile is not None and logdir is not None:\n            raise ValueError('Cannot specify both logfile and logdir')\n        # XXX: Need argument docstring\n        self.last_time = time.time()\n        self.start_time = self.last_time\n        if logdir is not None:\n            logfile = os.path.join(logdir, 'joblib.log')\n        self.logfile = logfile\n        if logfile is not None:\n            mkdirp(os.path.dirname(logfile))\n            if os.path.exists(logfile):\n                # Rotate the logs\n                for i in range(1, 9):\n                    try:\n                        shutil.move(logfile + '.%i' % i,\n                                    logfile + '.%i' % (i + 1))\n                    except:\n                        \"No reason failing here\"\n                # Use a copy rather than a move, so that a process\n                # monitoring this file does not get lost.\n                try:\n                    shutil.copy(logfile, logfile + '.1')\n                except:\n                    \"No reason failing here\"\n            try:\n                with open(logfile, 'w') as logfile:\n                    logfile.write('\\nLogging joblib python script\\n')\n                    logfile.write('\\n---%s---\\n' % time.ctime(self.last_time))\n            except:\n                \"\"\" Multiprocessing writing to files can create race\n                    conditions. Rather fail silently than crash the\n                    computation.\n                \"\"\"\n                # XXX: We actually need a debug flag to disable this\n                # silent failure.\n\n    def __call__(self, msg='', total=False):\n        \"\"\" Print the time elapsed between the last call and the current\n            call, with an optional message.\n        \"\"\"\n        if not total:\n            time_lapse = time.time() - self.last_time\n            full_msg = \"%s: %s\" % (msg, format_time(time_lapse))\n        else:\n            # FIXME: Too much logic duplicated\n            time_lapse = time.time() - self.start_time\n            full_msg = \"%s: %.2fs, %.1f min\" % (msg, time_lapse,\n                                                time_lapse / 60)\n        print(full_msg, file=sys.stderr)\n        if self.logfile is not None:\n            try:\n                with open(self.logfile, 'a') as f:\n                    print(full_msg, file=f)\n            except:\n                \"\"\" Multiprocessing writing to files can create race\n                    conditions. Rather fail silently than crash the\n                    calculation.\n                \"\"\"\n                # XXX: We actually need a debug flag to disable this\n                # silent failure.\n        self.last_time = time.time()\n"
    },
    {
      "filename": "sklearn/externals/_joblib/memory.py",
      "content": "\"\"\"\nA context object for caching a function's return value each time it\nis called with the same input arguments.\n\n\"\"\"\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n# Copyright (c) 2009 Gael Varoquaux\n# License: BSD Style, 3 clauses.\n\n\nfrom __future__ import with_statement\nimport os\nimport shutil\nimport time\nimport pydoc\nimport re\nimport functools\nimport traceback\nimport warnings\nimport inspect\nimport json\nimport weakref\nimport io\nimport operator\nimport collections\nimport datetime\nimport threading\n\n# Local imports\nfrom . import hashing\nfrom .func_inspect import get_func_code, get_func_name, filter_args\nfrom .func_inspect import format_call\nfrom .func_inspect import format_signature\nfrom ._memory_helpers import open_py_source\nfrom .logger import Logger, format_time, pformat\nfrom . import numpy_pickle\nfrom .disk import mkdirp, rm_subdirs, memstr_to_bytes\nfrom ._compat import _basestring, PY3_OR_LATER\nfrom .backports import concurrency_safe_rename\n\nFIRST_LINE_TEXT = \"# first line:\"\n\nCacheItemInfo = collections.namedtuple('CacheItemInfo',\n                                       'path size last_access')\n\n# TODO: The following object should have a data store object as a sub\n# object, and the interface to persist and query should be separated in\n# the data store.\n#\n# This would enable creating 'Memory' objects with a different logic for\n# pickling that would simply span a MemorizedFunc with the same\n# store (or do we want to copy it to avoid cross-talks?), for instance to\n# implement HDF5 pickling.\n\n# TODO: Same remark for the logger, and probably use the Python logging\n# mechanism.\n\n\ndef extract_first_line(func_code):\n    \"\"\" Extract the first line information from the function code\n        text if available.\n    \"\"\"\n    if func_code.startswith(FIRST_LINE_TEXT):\n        func_code = func_code.split('\\n')\n        first_line = int(func_code[0][len(FIRST_LINE_TEXT):])\n        func_code = '\\n'.join(func_code[1:])\n    else:\n        first_line = -1\n    return func_code, first_line\n\n\nclass JobLibCollisionWarning(UserWarning):\n    \"\"\" Warn that there might be a collision between names of functions.\n    \"\"\"\n\n\ndef _get_func_fullname(func):\n    \"\"\"Compute the part of part associated with a function.\n\n    See code of_cache_key_to_dir() for details\n    \"\"\"\n    modules, funcname = get_func_name(func)\n    modules.append(funcname)\n    return os.path.join(*modules)\n\n\ndef _cache_key_to_dir(cachedir, func, argument_hash):\n    \"\"\"Compute directory associated with a given cache key.\n\n    func can be a function or a string as returned by _get_func_fullname().\n    \"\"\"\n    parts = [cachedir]\n    if isinstance(func, _basestring):\n        parts.append(func)\n    else:\n        parts.append(_get_func_fullname(func))\n\n    if argument_hash is not None:\n        parts.append(argument_hash)\n    return os.path.join(*parts)\n\n\ndef _load_output(output_dir, func_name, timestamp=None, metadata=None,\n                 mmap_mode=None, verbose=0):\n    \"\"\"Load output of a computation.\"\"\"\n    if verbose > 1:\n        signature = \"\"\n        try:\n            if metadata is not None:\n                args = \", \".join(['%s=%s' % (name, value)\n                                  for name, value\n                                  in metadata['input_args'].items()])\n                signature = \"%s(%s)\" % (os.path.basename(func_name),\n                                             args)\n            else:\n                signature = os.path.basename(func_name)\n        except KeyError:\n            pass\n\n        if timestamp is not None:\n            t = \"% 16s\" % format_time(time.time() - timestamp)\n        else:\n            t = \"\"\n\n        if verbose < 10:\n            print('[Memory]%s: Loading %s...' % (t, str(signature)))\n        else:\n            print('[Memory]%s: Loading %s from %s' % (\n                    t, str(signature), output_dir))\n\n    filename = os.path.join(output_dir, 'output.pkl')\n    if not os.path.isfile(filename):\n        raise KeyError(\n            \"Non-existing cache value (may have been cleared).\\n\"\n            \"File %s does not exist\" % filename)\n    result = numpy_pickle.load(filename, mmap_mode=mmap_mode)\n\n    return result\n\n\ndef _get_cache_items(root_path):\n    \"\"\"Get cache information for reducing the size of the cache.\"\"\"\n    cache_items = []\n\n    for dirpath, dirnames, filenames in os.walk(root_path):\n        is_cache_hash_dir = re.match('[a-f0-9]{32}', os.path.basename(dirpath))\n\n        if is_cache_hash_dir:\n            output_filename = os.path.join(dirpath, 'output.pkl')\n            try:\n                last_access = os.path.getatime(output_filename)\n            except OSError:\n                try:\n                    last_access = os.path.getatime(dirpath)\n                except OSError:\n                    # The directory has already been deleted\n                    continue\n\n            last_access = datetime.datetime.fromtimestamp(last_access)\n            try:\n                full_filenames = [os.path.join(dirpath, fn)\n                                  for fn in filenames]\n                dirsize = sum(os.path.getsize(fn)\n                              for fn in full_filenames)\n            except OSError:\n                # Either output_filename or one of the files in\n                # dirpath does not exist any more. We assume this\n                # directory is being cleaned by another process already\n                continue\n\n            cache_items.append(CacheItemInfo(dirpath, dirsize, last_access))\n\n    return cache_items\n\n\ndef _get_cache_items_to_delete(root_path, bytes_limit):\n    \"\"\"Get cache items to delete to keep the cache under a size limit.\"\"\"\n    if isinstance(bytes_limit, _basestring):\n        bytes_limit = memstr_to_bytes(bytes_limit)\n\n    cache_items = _get_cache_items(root_path)\n    cache_size = sum(item.size for item in cache_items)\n\n    to_delete_size = cache_size - bytes_limit\n    if to_delete_size < 0:\n        return []\n\n    # We want to delete first the cache items that were accessed a\n    # long time ago\n    cache_items.sort(key=operator.attrgetter('last_access'))\n\n    cache_items_to_delete = []\n    size_so_far = 0\n\n    for item in cache_items:\n        if size_so_far > to_delete_size:\n            break\n\n        cache_items_to_delete.append(item)\n        size_so_far += item.size\n\n    return cache_items_to_delete\n\n\ndef concurrency_safe_write(to_write, filename, write_func):\n    \"\"\"Writes an object into a file in a concurrency-safe way.\"\"\"\n    thread_id = id(threading.current_thread())\n    temporary_filename = '{}.thread-{}-pid-{}'.format(\n        filename, thread_id, os.getpid())\n    write_func(to_write, temporary_filename)\n    concurrency_safe_rename(temporary_filename, filename)\n\n\n# An in-memory store to avoid looking at the disk-based function\n# source code to check if a function definition has changed\n_FUNCTION_HASHES = weakref.WeakKeyDictionary()\n\n\n###############################################################################\n# class `MemorizedResult`\n###############################################################################\nclass MemorizedResult(Logger):\n    \"\"\"Object representing a cached value.\n\n    Attributes\n    ----------\n    cachedir: string\n        path to root of joblib cache\n\n    func: function or string\n        function whose output is cached. The string case is intended only for\n        instanciation based on the output of repr() on another instance.\n        (namely eval(repr(memorized_instance)) works).\n\n    argument_hash: string\n        hash of the function arguments\n\n    mmap_mode: {None, 'r+', 'r', 'w+', 'c'}\n        The memmapping mode used when loading from cache numpy arrays. See\n        numpy.load for the meaning of the different values.\n\n    verbose: int\n        verbosity level (0 means no message)\n\n    timestamp, metadata: string\n        for internal use only\n    \"\"\"\n    def __init__(self, cachedir, func, argument_hash,\n                 mmap_mode=None, verbose=0, timestamp=None, metadata=None):\n        Logger.__init__(self)\n        if isinstance(func, _basestring):\n            self.func = func\n        else:\n            self.func = _get_func_fullname(func)\n        self.argument_hash = argument_hash\n        self.cachedir = cachedir\n        self.mmap_mode = mmap_mode\n\n        self._output_dir = _cache_key_to_dir(cachedir, self.func,\n                                             argument_hash)\n\n        if metadata is not None:\n            self.metadata = metadata\n        else:\n            self.metadata = {}\n            # No error is relevant here.\n            try:\n                with open(os.path.join(self._output_dir, 'metadata.json'),\n                          'rb') as f:\n                    self.metadata = json.load(f)\n            except:\n                pass\n\n        self.duration = self.metadata.get('duration', None)\n        self.verbose = verbose\n        self.timestamp = timestamp\n\n    def get(self):\n        \"\"\"Read value from cache and return it.\"\"\"\n        return _load_output(self._output_dir, _get_func_fullname(self.func),\n                            timestamp=self.timestamp,\n                            metadata=self.metadata, mmap_mode=self.mmap_mode,\n                            verbose=self.verbose)\n\n    def clear(self):\n        \"\"\"Clear value from cache\"\"\"\n        shutil.rmtree(self._output_dir, ignore_errors=True)\n\n    def __repr__(self):\n        return ('{class_name}(cachedir=\"{cachedir}\", func=\"{func}\", '\n                'argument_hash=\"{argument_hash}\")'.format(\n                    class_name=self.__class__.__name__,\n                    cachedir=self.cachedir,\n                    func=self.func,\n                    argument_hash=self.argument_hash\n                    ))\n\n    def __reduce__(self):\n        return (self.__class__, (self.cachedir, self.func, self.argument_hash),\n                {'mmap_mode': self.mmap_mode})\n\n\nclass NotMemorizedResult(object):\n    \"\"\"Class representing an arbitrary value.\n\n    This class is a replacement for MemorizedResult when there is no cache.\n    \"\"\"\n    __slots__ = ('value', 'valid')\n\n    def __init__(self, value):\n        self.value = value\n        self.valid = True\n\n    def get(self):\n        if self.valid:\n            return self.value\n        else:\n            raise KeyError(\"No value stored.\")\n\n    def clear(self):\n        self.valid = False\n        self.value = None\n\n    def __repr__(self):\n        if self.valid:\n            return '{class_name}({value})'.format(\n                class_name=self.__class__.__name__,\n                value=pformat(self.value)\n                )\n        else:\n            return self.__class__.__name__ + ' with no value'\n\n    # __getstate__ and __setstate__ are required because of __slots__\n    def __getstate__(self):\n        return {\"valid\": self.valid, \"value\": self.value}\n\n    def __setstate__(self, state):\n        self.valid = state[\"valid\"]\n        self.value = state[\"value\"]\n\n\n###############################################################################\n# class `NotMemorizedFunc`\n###############################################################################\nclass NotMemorizedFunc(object):\n    \"\"\"No-op object decorating a function.\n\n    This class replaces MemorizedFunc when there is no cache. It provides an\n    identical API but does not write anything on disk.\n\n    Attributes\n    ----------\n    func: callable\n        Original undecorated function.\n    \"\"\"\n    # Should be a light as possible (for speed)\n    def __init__(self, func):\n        self.func = func\n\n    def __call__(self, *args, **kwargs):\n        return self.func(*args, **kwargs)\n\n    def call_and_shelve(self, *args, **kwargs):\n        return NotMemorizedResult(self.func(*args, **kwargs))\n\n    def __reduce__(self):\n        return (self.__class__, (self.func,))\n\n    def __repr__(self):\n        return '%s(func=%s)' % (\n                    self.__class__.__name__,\n                    self.func\n            )\n\n    def clear(self, warn=True):\n        # Argument \"warn\" is for compatibility with MemorizedFunc.clear\n        pass\n\n\n###############################################################################\n# class `MemorizedFunc`\n###############################################################################\nclass MemorizedFunc(Logger):\n    \"\"\" Callable object decorating a function for caching its return value\n        each time it is called.\n\n        All values are cached on the filesystem, in a deep directory\n        structure. Methods are provided to inspect the cache or clean it.\n\n        Attributes\n        ----------\n        func: callable\n            The original, undecorated, function.\n\n        cachedir: string\n            Path to the base cache directory of the memory context.\n\n        ignore: list or None\n            List of variable names to ignore when choosing whether to\n            recompute.\n\n        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}\n            The memmapping mode used when loading from cache\n            numpy arrays. See numpy.load for the meaning of the different\n            values.\n\n        compress: boolean, or integer\n            Whether to zip the stored data on disk. If an integer is\n            given, it should be between 1 and 9, and sets the amount\n            of compression. Note that compressed arrays cannot be\n            read by memmapping.\n\n        verbose: int, optional\n            The verbosity flag, controls messages that are issued as\n            the function is evaluated.\n    \"\"\"\n    #-------------------------------------------------------------------------\n    # Public interface\n    #-------------------------------------------------------------------------\n\n    def __init__(self, func, cachedir, ignore=None, mmap_mode=None,\n                 compress=False, verbose=1, timestamp=None):\n        \"\"\"\n            Parameters\n            ----------\n            func: callable\n                The function to decorate\n            cachedir: string\n                The path of the base directory to use as a data store\n            ignore: list or None\n                List of variable names to ignore.\n            mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional\n                The memmapping mode used when loading from cache\n                numpy arrays. See numpy.load for the meaning of the\n                arguments.\n            compress : boolean, or integer\n                Whether to zip the stored data on disk. If an integer is\n                given, it should be between 1 and 9, and sets the amount\n                of compression. Note that compressed arrays cannot be\n                read by memmapping.\n            verbose: int, optional\n                Verbosity flag, controls the debug messages that are issued\n                as functions are evaluated. The higher, the more verbose\n            timestamp: float, optional\n                The reference time from which times in tracing messages\n                are reported.\n        \"\"\"\n        Logger.__init__(self)\n        self.mmap_mode = mmap_mode\n        self.func = func\n        if ignore is None:\n            ignore = []\n        self.ignore = ignore\n\n        self._verbose = verbose\n        self.cachedir = cachedir\n        self.compress = compress\n        if compress and self.mmap_mode is not None:\n            warnings.warn('Compressed results cannot be memmapped',\n                          stacklevel=2)\n        if timestamp is None:\n            timestamp = time.time()\n        self.timestamp = timestamp\n        mkdirp(self.cachedir)\n        try:\n            functools.update_wrapper(self, func)\n        except:\n            \" Objects like ufunc don't like that \"\n        if inspect.isfunction(func):\n            doc = pydoc.TextDoc().document(func)\n            # Remove blank line\n            doc = doc.replace('\\n', '\\n\\n', 1)\n            # Strip backspace-overprints for compatibility with autodoc\n            doc = re.sub('\\x08.', '', doc)\n        else:\n            # Pydoc does a poor job on other objects\n            doc = func.__doc__\n        self.__doc__ = 'Memoized version of %s' % doc\n\n    def _cached_call(self, args, kwargs):\n        \"\"\"Call wrapped function and cache result, or read cache if available.\n\n        This function returns the wrapped function output and some metadata.\n\n        Returns\n        -------\n        output: value or tuple\n            what is returned by wrapped function\n\n        argument_hash: string\n            hash of function arguments\n\n        metadata: dict\n            some metadata about wrapped function call (see _persist_input())\n        \"\"\"\n        # Compare the function code with the previous to see if the\n        # function code has changed\n        output_dir, argument_hash = self._get_output_dir(*args, **kwargs)\n        metadata = None\n        output_pickle_path = os.path.join(output_dir, 'output.pkl')\n        # FIXME: The statements below should be try/excepted\n        if not (self._check_previous_func_code(stacklevel=4) and\n                os.path.isfile(output_pickle_path)):\n            if self._verbose > 10:\n                _, name = get_func_name(self.func)\n                self.warn('Computing func %s, argument hash %s in '\n                          'directory %s'\n                        % (name, argument_hash, output_dir))\n            out, metadata = self.call(*args, **kwargs)\n            if self.mmap_mode is not None:\n                # Memmap the output at the first call to be consistent with\n                # later calls\n                out = _load_output(output_dir, _get_func_fullname(self.func),\n                                   timestamp=self.timestamp,\n                                   mmap_mode=self.mmap_mode,\n                                   verbose=self._verbose)\n        else:\n            try:\n                t0 = time.time()\n                out = _load_output(output_dir, _get_func_fullname(self.func),\n                                   timestamp=self.timestamp,\n                                   metadata=metadata, mmap_mode=self.mmap_mode,\n                                   verbose=self._verbose)\n                if self._verbose > 4:\n                    t = time.time() - t0\n                    _, name = get_func_name(self.func)\n                    msg = '%s cache loaded - %s' % (name, format_time(t))\n                    print(max(0, (80 - len(msg))) * '_' + msg)\n            except Exception:\n                # XXX: Should use an exception logger\n                _, signature = format_signature(self.func, *args, **kwargs)\n                self.warn('Exception while loading results for '\n                          '{}\\n {}'.format(\n                              signature, traceback.format_exc()))\n                out, metadata = self.call(*args, **kwargs)\n                argument_hash = None\n        return (out, argument_hash, metadata)\n\n    def call_and_shelve(self, *args, **kwargs):\n        \"\"\"Call wrapped function, cache result and return a reference.\n\n        This method returns a reference to the cached result instead of the\n        result itself. The reference object is small and pickeable, allowing\n        to send or store it easily. Call .get() on reference object to get\n        result.\n\n        Returns\n        -------\n        cached_result: MemorizedResult or NotMemorizedResult\n            reference to the value returned by the wrapped function. The\n            class \"NotMemorizedResult\" is used when there is no cache\n            activated (e.g. cachedir=None in Memory).\n        \"\"\"\n        _, argument_hash, metadata = self._cached_call(args, kwargs)\n\n        return MemorizedResult(self.cachedir, self.func, argument_hash,\n            metadata=metadata, verbose=self._verbose - 1,\n            timestamp=self.timestamp)\n\n    def __call__(self, *args, **kwargs):\n        return self._cached_call(args, kwargs)[0]\n\n    def __reduce__(self):\n        \"\"\" We don't store the timestamp when pickling, to avoid the hash\n            depending from it.\n            In addition, when unpickling, we run the __init__\n        \"\"\"\n        return (self.__class__, (self.func, self.cachedir, self.ignore,\n                self.mmap_mode, self.compress, self._verbose))\n\n    #-------------------------------------------------------------------------\n    # Private interface\n    #-------------------------------------------------------------------------\n\n    def _get_argument_hash(self, *args, **kwargs):\n        return hashing.hash(filter_args(self.func, self.ignore,\n                                         args, kwargs),\n                             coerce_mmap=(self.mmap_mode is not None))\n\n    def _get_output_dir(self, *args, **kwargs):\n        \"\"\" Return the directory in which are persisted the result\n            of the function called with the given arguments.\n        \"\"\"\n        argument_hash = self._get_argument_hash(*args, **kwargs)\n        output_dir = os.path.join(self._get_func_dir(self.func),\n                                  argument_hash)\n        return output_dir, argument_hash\n\n    get_output_dir = _get_output_dir  # backward compatibility\n\n    def _get_func_dir(self, mkdir=True):\n        \"\"\" Get the directory corresponding to the cache for the\n            function.\n        \"\"\"\n        func_dir = _cache_key_to_dir(self.cachedir, self.func, None)\n        if mkdir:\n            mkdirp(func_dir)\n        return func_dir\n\n    def _hash_func(self):\n        \"\"\"Hash a function to key the online cache\"\"\"\n        func_code_h = hash(getattr(self.func, '__code__', None))\n        return id(self.func), hash(self.func), func_code_h\n\n    def _write_func_code(self, filename, func_code, first_line):\n        \"\"\" Write the function code and the filename to a file.\n        \"\"\"\n        # We store the first line because the filename and the function\n        # name is not always enough to identify a function: people\n        # sometimes have several functions named the same way in a\n        # file. This is bad practice, but joblib should be robust to bad\n        # practice.\n        func_code = u'%s %i\\n%s' % (FIRST_LINE_TEXT, first_line, func_code)\n        with io.open(filename, 'w', encoding=\"UTF-8\") as out:\n            out.write(func_code)\n        # Also store in the in-memory store of function hashes\n        is_named_callable = False\n        if PY3_OR_LATER:\n            is_named_callable = (hasattr(self.func, '__name__')\n                                 and self.func.__name__ != '<lambda>')\n        else:\n            is_named_callable = (hasattr(self.func, 'func_name')\n                                 and self.func.func_name != '<lambda>')\n        if is_named_callable:\n            # Don't do this for lambda functions or strange callable\n            # objects, as it ends up being too fragile\n            func_hash = self._hash_func()\n            try:\n                _FUNCTION_HASHES[self.func] = func_hash\n            except TypeError:\n                # Some callable are not hashable\n                pass\n\n    def _check_previous_func_code(self, stacklevel=2):\n        \"\"\"\n            stacklevel is the depth a which this function is called, to\n            issue useful warnings to the user.\n        \"\"\"\n        # First check if our function is in the in-memory store.\n        # Using the in-memory store not only makes things faster, but it\n        # also renders us robust to variations of the files when the\n        # in-memory version of the code does not vary\n        try:\n            if self.func in _FUNCTION_HASHES:\n                # We use as an identifier the id of the function and its\n                # hash. This is more likely to falsely change than have hash\n                # collisions, thus we are on the safe side.\n                func_hash = self._hash_func()\n                if func_hash == _FUNCTION_HASHES[self.func]:\n                    return True\n        except TypeError:\n            # Some callables are not hashable\n            pass\n\n        # Here, we go through some effort to be robust to dynamically\n        # changing code and collision. We cannot inspect.getsource\n        # because it is not reliable when using IPython's magic \"%run\".\n        func_code, source_file, first_line = get_func_code(self.func)\n        func_dir = self._get_func_dir()\n        func_code_file = os.path.join(func_dir, 'func_code.py')\n\n        try:\n            with io.open(func_code_file, encoding=\"UTF-8\") as infile:\n                old_func_code, old_first_line = \\\n                            extract_first_line(infile.read())\n        except IOError:\n                self._write_func_code(func_code_file, func_code, first_line)\n                return False\n        if old_func_code == func_code:\n            return True\n\n        # We have differing code, is this because we are referring to\n        # different functions, or because the function we are referring to has\n        # changed?\n\n        _, func_name = get_func_name(self.func, resolv_alias=False,\n                                     win_characters=False)\n        if old_first_line == first_line == -1 or func_name == '<lambda>':\n            if not first_line == -1:\n                func_description = '%s (%s:%i)' % (func_name,\n                                                source_file, first_line)\n            else:\n                func_description = func_name\n            warnings.warn(JobLibCollisionWarning(\n                \"Cannot detect name collisions for function '%s'\"\n                        % func_description), stacklevel=stacklevel)\n\n        # Fetch the code at the old location and compare it. If it is the\n        # same than the code store, we have a collision: the code in the\n        # file has not changed, but the name we have is pointing to a new\n        # code block.\n        if not old_first_line == first_line and source_file is not None:\n            possible_collision = False\n            if os.path.exists(source_file):\n                _, func_name = get_func_name(self.func, resolv_alias=False)\n                num_lines = len(func_code.split('\\n'))\n                with open_py_source(source_file) as f:\n                    on_disk_func_code = f.readlines()[\n                        old_first_line - 1:old_first_line - 1 + num_lines - 1]\n                on_disk_func_code = ''.join(on_disk_func_code)\n                possible_collision = (on_disk_func_code.rstrip()\n                                      == old_func_code.rstrip())\n            else:\n                possible_collision = source_file.startswith('<doctest ')\n            if possible_collision:\n                warnings.warn(JobLibCollisionWarning(\n                        'Possible name collisions between functions '\n                        \"'%s' (%s:%i) and '%s' (%s:%i)\" %\n                        (func_name, source_file, old_first_line,\n                        func_name, source_file, first_line)),\n                                stacklevel=stacklevel)\n\n        # The function has changed, wipe the cache directory.\n        # XXX: Should be using warnings, and giving stacklevel\n        if self._verbose > 10:\n            _, func_name = get_func_name(self.func, resolv_alias=False)\n            self.warn(\"Function %s (stored in %s) has changed.\" %\n                        (func_name, func_dir))\n        self.clear(warn=True)\n        return False\n\n    def clear(self, warn=True):\n        \"\"\" Empty the function's cache.\n        \"\"\"\n        func_dir = self._get_func_dir(mkdir=False)\n        if self._verbose > 0 and warn:\n            self.warn(\"Clearing cache %s\" % func_dir)\n        if os.path.exists(func_dir):\n            shutil.rmtree(func_dir, ignore_errors=True)\n        mkdirp(func_dir)\n        func_code, _, first_line = get_func_code(self.func)\n        func_code_file = os.path.join(func_dir, 'func_code.py')\n        self._write_func_code(func_code_file, func_code, first_line)\n\n    def call(self, *args, **kwargs):\n        \"\"\" Force the execution of the function with the given arguments and\n            persist the output values.\n        \"\"\"\n        start_time = time.time()\n        output_dir, _ = self._get_output_dir(*args, **kwargs)\n        if self._verbose > 0:\n            print(format_call(self.func, args, kwargs))\n        output = self.func(*args, **kwargs)\n        self._persist_output(output, output_dir)\n        duration = time.time() - start_time\n        metadata = self._persist_input(output_dir, duration, args, kwargs)\n\n        if self._verbose > 0:\n            _, name = get_func_name(self.func)\n            msg = '%s - %s' % (name, format_time(duration))\n            print(max(0, (80 - len(msg))) * '_' + msg)\n        return output, metadata\n\n    # Make public\n    def _persist_output(self, output, dir):\n        \"\"\" Persist the given output tuple in the directory.\n        \"\"\"\n        try:\n            filename = os.path.join(dir, 'output.pkl')\n            mkdirp(dir)\n            write_func = functools.partial(numpy_pickle.dump,\n                                           compress=self.compress)\n            concurrency_safe_write(output, filename, write_func)\n            if self._verbose > 10:\n                print('Persisting in %s' % dir)\n        except OSError:\n            \" Race condition in the creation of the directory \"\n\n    def _persist_input(self, output_dir, duration, args, kwargs,\n                       this_duration_limit=0.5):\n        \"\"\" Save a small summary of the call using json format in the\n            output directory.\n\n            output_dir: string\n                directory where to write metadata.\n\n            duration: float\n                time taken by hashing input arguments, calling the wrapped\n                function and persisting its output.\n\n            args, kwargs: list and dict\n                input arguments for wrapped function\n\n            this_duration_limit: float\n                Max execution time for this function before issuing a warning.\n        \"\"\"\n        start_time = time.time()\n        argument_dict = filter_args(self.func, self.ignore,\n                                    args, kwargs)\n\n        input_repr = dict((k, repr(v)) for k, v in argument_dict.items())\n        # This can fail due to race-conditions with multiple\n        # concurrent joblibs removing the file or the directory\n        metadata = {\"duration\": duration, \"input_args\": input_repr}\n        try:\n            mkdirp(output_dir)\n            filename = os.path.join(output_dir, 'metadata.json')\n\n            def write_func(output, dest_filename):\n                with open(dest_filename, 'w') as f:\n                    json.dump(output, f)\n\n            concurrency_safe_write(metadata, filename, write_func)\n        except Exception:\n            pass\n\n        this_duration = time.time() - start_time\n        if this_duration > this_duration_limit:\n            # This persistence should be fast. It will not be if repr() takes\n            # time and its output is large, because json.dump will have to\n            # write a large file. This should not be an issue with numpy arrays\n            # for which repr() always output a short representation, but can\n            # be with complex dictionaries. Fixing the problem should be a\n            # matter of replacing repr() above by something smarter.\n            warnings.warn(\"Persisting input arguments took %.2fs to run.\\n\"\n                          \"If this happens often in your code, it can cause \"\n                          \"performance problems \\n\"\n                          \"(results will be correct in all cases). \\n\"\n                          \"The reason for this is probably some large input \"\n                          \"arguments for a wrapped\\n\"\n                          \" function (e.g. large strings).\\n\"\n                          \"THIS IS A JOBLIB ISSUE. If you can, kindly provide \"\n                          \"the joblib's team with an\\n\"\n                          \" example so that they can fix the problem.\"\n                          % this_duration, stacklevel=5)\n        return metadata\n\n    # XXX: Need a method to check if results are available.\n\n\n    #-------------------------------------------------------------------------\n    # Private `object` interface\n    #-------------------------------------------------------------------------\n\n    def __repr__(self):\n        return '%s(func=%s, cachedir=%s)' % (\n                    self.__class__.__name__,\n                    self.func,\n                    repr(self.cachedir),\n                    )\n\n\n###############################################################################\n# class `Memory`\n###############################################################################\nclass Memory(Logger):\n    \"\"\" A context object for caching a function's return value each time it\n        is called with the same input arguments.\n\n        All values are cached on the filesystem, in a deep directory\n        structure.\n\n        see :ref:`memory_reference`\n    \"\"\"\n    #-------------------------------------------------------------------------\n    # Public interface\n    #-------------------------------------------------------------------------\n\n    def __init__(self, cachedir, mmap_mode=None, compress=False, verbose=1,\n                 bytes_limit=None):\n        \"\"\"\n            Parameters\n            ----------\n            cachedir: string or None\n                The path of the base directory to use as a data store\n                or None. If None is given, no caching is done and\n                the Memory object is completely transparent.\n            mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional\n                The memmapping mode used when loading from cache\n                numpy arrays. See numpy.load for the meaning of the\n                arguments.\n            compress: boolean, or integer\n                Whether to zip the stored data on disk. If an integer is\n                given, it should be between 1 and 9, and sets the amount\n                of compression. Note that compressed arrays cannot be\n                read by memmapping.\n            verbose: int, optional\n                Verbosity flag, controls the debug messages that are issued\n                as functions are evaluated.\n            bytes_limit: int, optional\n                Limit in bytes of the size of the cache\n        \"\"\"\n        # XXX: Bad explanation of the None value of cachedir\n        Logger.__init__(self)\n        self._verbose = verbose\n        self.mmap_mode = mmap_mode\n        self.timestamp = time.time()\n        self.compress = compress\n        self.bytes_limit = bytes_limit\n        if compress and mmap_mode is not None:\n            warnings.warn('Compressed results cannot be memmapped',\n                          stacklevel=2)\n        if cachedir is None:\n            self.cachedir = None\n        else:\n            self.cachedir = os.path.join(cachedir, 'joblib')\n            mkdirp(self.cachedir)\n\n    def cache(self, func=None, ignore=None, verbose=None,\n                        mmap_mode=False):\n        \"\"\" Decorates the given function func to only compute its return\n            value for input arguments not cached on disk.\n\n            Parameters\n            ----------\n            func: callable, optional\n                The function to be decorated\n            ignore: list of strings\n                A list of arguments name to ignore in the hashing\n            verbose: integer, optional\n                The verbosity mode of the function. By default that\n                of the memory object is used.\n            mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional\n                The memmapping mode used when loading from cache\n                numpy arrays. See numpy.load for the meaning of the\n                arguments. By default that of the memory object is used.\n\n            Returns\n            -------\n            decorated_func: MemorizedFunc object\n                The returned object is a MemorizedFunc object, that is\n                callable (behaves like a function), but offers extra\n                methods for cache lookup and management. See the\n                documentation for :class:`joblib.memory.MemorizedFunc`.\n        \"\"\"\n        if func is None:\n            # Partial application, to be able to specify extra keyword\n            # arguments in decorators\n            return functools.partial(self.cache, ignore=ignore,\n                                     verbose=verbose, mmap_mode=mmap_mode)\n        if self.cachedir is None:\n            return NotMemorizedFunc(func)\n        if verbose is None:\n            verbose = self._verbose\n        if mmap_mode is False:\n            mmap_mode = self.mmap_mode\n        if isinstance(func, MemorizedFunc):\n            func = func.func\n        return MemorizedFunc(func, cachedir=self.cachedir,\n                                   mmap_mode=mmap_mode,\n                                   ignore=ignore,\n                                   compress=self.compress,\n                                   verbose=verbose,\n                                   timestamp=self.timestamp)\n\n    def clear(self, warn=True):\n        \"\"\" Erase the complete cache directory.\n        \"\"\"\n        if warn:\n            self.warn('Flushing completely the cache')\n        if self.cachedir is not None:\n            rm_subdirs(self.cachedir)\n\n    def reduce_size(self):\n        \"\"\"Remove cache folders to make cache size fit in ``bytes_limit``.\"\"\"\n        if self.cachedir is not None and self.bytes_limit is not None:\n            cache_items_to_delete = _get_cache_items_to_delete(\n                self.cachedir, self.bytes_limit)\n\n            for cache_item in cache_items_to_delete:\n                if self._verbose > 10:\n                    print('Deleting cache item {}'.format(cache_item))\n                try:\n                    shutil.rmtree(cache_item.path, ignore_errors=True)\n                except OSError:\n                    # Even with ignore_errors=True can shutil.rmtree\n                    # can raise OSErrror with [Errno 116] Stale file\n                    # handle if another process has deleted the folder\n                    # already.\n                    pass\n\n    def eval(self, func, *args, **kwargs):\n        \"\"\" Eval function func with arguments `*args` and `**kwargs`,\n            in the context of the memory.\n\n            This method works similarly to the builtin `apply`, except\n            that the function is called only if the cache is not\n            up to date.\n\n        \"\"\"\n        if self.cachedir is None:\n            return func(*args, **kwargs)\n        return self.cache(func)(*args, **kwargs)\n\n    #-------------------------------------------------------------------------\n    # Private `object` interface\n    #-------------------------------------------------------------------------\n\n    def __repr__(self):\n        return '%s(cachedir=%s)' % (\n                    self.__class__.__name__,\n                    repr(self.cachedir),\n                    )\n\n    def __reduce__(self):\n        \"\"\" We don't store the timestamp when pickling, to avoid the hash\n            depending from it.\n            In addition, when unpickling, we run the __init__\n        \"\"\"\n        # We need to remove 'joblib' from the end of cachedir\n        cachedir = self.cachedir[:-7] if self.cachedir is not None else None\n        return (self.__class__, (cachedir,\n                self.mmap_mode, self.compress, self._verbose))\n"
    },
    {
      "filename": "sklearn/externals/_joblib/my_exceptions.py",
      "content": "\"\"\"\nExceptions\n\"\"\"\n# Author: Gael Varoquaux < gael dot varoquaux at normalesup dot org >\n# Copyright: 2010, Gael Varoquaux\n# License: BSD 3 clause\n\nfrom ._compat import PY3_OR_LATER\n\nclass JoblibException(Exception):\n    \"\"\"A simple exception with an error message that you can get to.\"\"\"\n    def __init__(self, *args):\n        # We need to implement __init__ so that it is picked in the\n        # multiple heritance hierarchy in the class created in\n        # _mk_exception. Note: in Python 2, if you implement __init__\n        # in your exception class you need to set .args correctly,\n        # otherwise you can dump an exception instance with pickle but\n        # not load it (at load time an empty .args will be passed to\n        # the constructor). Also we want to be explicit and not use\n        # 'super' here. Using 'super' can cause a sibling class method\n        # to be called and we have no control the sibling class method\n        # constructor signature in the exception returned by\n        # _mk_exception.\n        Exception.__init__(self, *args)\n\n    def __repr__(self):\n        if hasattr(self, 'args') and len(self.args) > 0:\n            message = self.args[0]\n        else:\n            message = ''\n\n        name = self.__class__.__name__\n        return '%s\\n%s\\n%s\\n%s' % (name, 75 * '_', message, 75 * '_')\n\n    __str__ = __repr__\n\n\nclass TransportableException(JoblibException):\n    \"\"\"An exception containing all the info to wrap an original\n        exception and recreate it.\n    \"\"\"\n\n    def __init__(self, message, etype):\n        # The next line set the .args correctly. This is needed to\n        # make the exception loadable with pickle\n        JoblibException.__init__(self, message, etype)\n        self.message = message\n        self.etype = etype\n\n\nclass WorkerInterrupt(Exception):\n    \"\"\" An exception that is not KeyboardInterrupt to allow subprocesses\n        to be interrupted.\n    \"\"\"\n    pass\n\n\n_exception_mapping = dict()\n\n\ndef _mk_exception(exception, name=None):\n    # Create an exception inheriting from both JoblibException\n    # and that exception\n    if name is None:\n        name = exception.__name__\n    this_name = 'Joblib%s' % name\n    if this_name in _exception_mapping:\n        # Avoid creating twice the same exception\n        this_exception = _exception_mapping[this_name]\n    else:\n        if exception is Exception:\n            # JoblibException is already a subclass of Exception. No\n            # need to use multiple inheritance\n            return JoblibException, this_name\n        try:\n            this_exception = type(\n                this_name, (JoblibException, exception), {})\n            _exception_mapping[this_name] = this_exception\n        except TypeError:\n            # This happens if \"Cannot create a consistent method\n            # resolution order\", e.g. because 'exception' is a\n            # subclass of JoblibException or 'exception' is not an\n            # acceptable base class\n            this_exception = JoblibException\n\n    return this_exception, this_name\n\n\ndef _mk_common_exceptions():\n    namespace = dict()\n    if PY3_OR_LATER:\n        import builtins as _builtin_exceptions\n        common_exceptions = filter(\n            lambda x: x.endswith('Error'),\n            dir(_builtin_exceptions))\n    else:\n        import exceptions as _builtin_exceptions\n        common_exceptions = dir(_builtin_exceptions)\n\n    for name in common_exceptions:\n        obj = getattr(_builtin_exceptions, name)\n        if isinstance(obj, type) and issubclass(obj, BaseException):\n            this_obj, this_name = _mk_exception(obj, name=name)\n            namespace[this_name] = this_obj\n    return namespace\n\n\n# Updating module locals so that the exceptions pickle right. AFAIK this\n# works only at module-creation time\nlocals().update(_mk_common_exceptions())\n"
    },
    {
      "filename": "sklearn/externals/_joblib/numpy_pickle.py",
      "content": "\"\"\"Utilities for fast persistence of big data, with optional compression.\"\"\"\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n# Copyright (c) 2009 Gael Varoquaux\n# License: BSD Style, 3 clauses.\n\nimport pickle\nimport os\nimport sys\nimport warnings\ntry:\n    from pathlib import Path\nexcept ImportError:\n    Path = None\n\nfrom .numpy_pickle_utils import _COMPRESSORS\nfrom .numpy_pickle_utils import BinaryZlibFile\nfrom .numpy_pickle_utils import Unpickler, Pickler\nfrom .numpy_pickle_utils import _read_fileobject, _write_fileobject\nfrom .numpy_pickle_utils import _read_bytes, BUFFER_SIZE\nfrom .numpy_pickle_compat import load_compatibility\nfrom .numpy_pickle_compat import NDArrayWrapper\n# For compatibility with old versions of joblib, we need ZNDArrayWrapper\n# to be visible in the current namespace.\n# Explicitly skipping next line from flake8 as it triggers an F401 warning\n# which we don't care.\nfrom .numpy_pickle_compat import ZNDArrayWrapper  # noqa\nfrom ._compat import _basestring, PY3_OR_LATER\nfrom .backports import make_memmap\n\n###############################################################################\n# Utility objects for persistence.\n\n\nclass NumpyArrayWrapper(object):\n    \"\"\"An object to be persisted instead of numpy arrays.\n\n    This object is used to hack into the pickle machinery and read numpy\n    array data from our custom persistence format.\n    More precisely, this object is used for:\n    * carrying the information of the persisted array: subclass, shape, order,\n    dtype. Those ndarray metadata are used to correctly reconstruct the array\n    with low level numpy functions.\n    * determining if memmap is allowed on the array.\n    * reading the array bytes from a file.\n    * reading the array using memorymap from a file.\n    * writing the array bytes to a file.\n\n    Attributes\n    ----------\n    subclass: numpy.ndarray subclass\n        Determine the subclass of the wrapped array.\n    shape: numpy.ndarray shape\n        Determine the shape of the wrapped array.\n    order: {'C', 'F'}\n        Determine the order of wrapped array data. 'C' is for C order, 'F' is\n        for fortran order.\n    dtype: numpy.ndarray dtype\n        Determine the data type of the wrapped array.\n    allow_mmap: bool\n        Determine if memory mapping is allowed on the wrapped array.\n        Default: False.\n    \"\"\"\n\n    def __init__(self, subclass, shape, order, dtype, allow_mmap=False):\n        \"\"\"Constructor. Store the useful information for later.\"\"\"\n        self.subclass = subclass\n        self.shape = shape\n        self.order = order\n        self.dtype = dtype\n        self.allow_mmap = allow_mmap\n\n    def write_array(self, array, pickler):\n        \"\"\"Write array bytes to pickler file handle.\n\n        This function is an adaptation of the numpy write_array function\n        available in version 1.10.1 in numpy/lib/format.py.\n        \"\"\"\n        # Set buffer size to 16 MiB to hide the Python loop overhead.\n        buffersize = max(16 * 1024 ** 2 // array.itemsize, 1)\n        if array.dtype.hasobject:\n            # We contain Python objects so we cannot write out the data\n            # directly. Instead, we will pickle it out with version 2 of the\n            # pickle protocol.\n            pickle.dump(array, pickler.file_handle, protocol=2)\n        else:\n            for chunk in pickler.np.nditer(array,\n                                           flags=['external_loop',\n                                                  'buffered',\n                                                  'zerosize_ok'],\n                                           buffersize=buffersize,\n                                           order=self.order):\n                pickler.file_handle.write(chunk.tostring('C'))\n\n    def read_array(self, unpickler):\n        \"\"\"Read array from unpickler file handle.\n\n        This function is an adaptation of the numpy read_array function\n        available in version 1.10.1 in numpy/lib/format.py.\n        \"\"\"\n        if len(self.shape) == 0:\n            count = 1\n        else:\n            count = unpickler.np.multiply.reduce(self.shape)\n        # Now read the actual data.\n        if self.dtype.hasobject:\n            # The array contained Python objects. We need to unpickle the data.\n            array = pickle.load(unpickler.file_handle)\n        else:\n            if (not PY3_OR_LATER and\n                    unpickler.np.compat.isfileobj(unpickler.file_handle)):\n                # In python 2, gzip.GzipFile is considered as a file so one\n                # can use numpy.fromfile().\n                # For file objects, use np.fromfile function.\n                # This function is faster than the memory-intensive\n                # method below.\n                array = unpickler.np.fromfile(unpickler.file_handle,\n                                              dtype=self.dtype, count=count)\n            else:\n                # This is not a real file. We have to read it the\n                # memory-intensive way.\n                # crc32 module fails on reads greater than 2 ** 32 bytes,\n                # breaking large reads from gzip streams. Chunk reads to\n                # BUFFER_SIZE bytes to avoid issue and reduce memory overhead\n                # of the read. In non-chunked case count < max_read_count, so\n                # only one read is performed.\n                max_read_count = BUFFER_SIZE // min(BUFFER_SIZE,\n                                                    self.dtype.itemsize)\n\n                array = unpickler.np.empty(count, dtype=self.dtype)\n                for i in range(0, count, max_read_count):\n                    read_count = min(max_read_count, count - i)\n                    read_size = int(read_count * self.dtype.itemsize)\n                    data = _read_bytes(unpickler.file_handle,\n                                       read_size, \"array data\")\n                    array[i:i + read_count] = \\\n                        unpickler.np.frombuffer(data, dtype=self.dtype,\n                                                count=read_count)\n                    del data\n\n            if self.order == 'F':\n                array.shape = self.shape[::-1]\n                array = array.transpose()\n            else:\n                array.shape = self.shape\n\n        return array\n\n    def read_mmap(self, unpickler):\n        \"\"\"Read an array using numpy memmap.\"\"\"\n        offset = unpickler.file_handle.tell()\n        if unpickler.mmap_mode == 'w+':\n            unpickler.mmap_mode = 'r+'\n\n        marray = make_memmap(unpickler.filename,\n                             dtype=self.dtype,\n                             shape=self.shape,\n                             order=self.order,\n                             mode=unpickler.mmap_mode,\n                             offset=offset)\n        # update the offset so that it corresponds to the end of the read array\n        unpickler.file_handle.seek(offset + marray.nbytes)\n\n        return marray\n\n    def read(self, unpickler):\n        \"\"\"Read the array corresponding to this wrapper.\n\n        Use the unpickler to get all information to correctly read the array.\n\n        Parameters\n        ----------\n        unpickler: NumpyUnpickler\n\n        Returns\n        -------\n        array: numpy.ndarray\n\n        \"\"\"\n        # When requested, only use memmap mode if allowed.\n        if unpickler.mmap_mode is not None and self.allow_mmap:\n            array = self.read_mmap(unpickler)\n        else:\n            array = self.read_array(unpickler)\n\n        # Manage array subclass case\n        if (hasattr(array, '__array_prepare__') and\n            self.subclass not in (unpickler.np.ndarray,\n                                  unpickler.np.memmap)):\n            # We need to reconstruct another subclass\n            new_array = unpickler.np.core.multiarray._reconstruct(\n                self.subclass, (0,), 'b')\n            return new_array.__array_prepare__(array)\n        else:\n            return array\n\n###############################################################################\n# Pickler classes\n\n\nclass NumpyPickler(Pickler):\n    \"\"\"A pickler to persist big data efficiently.\n\n    The main features of this object are:\n    * persistence of numpy arrays in a single file.\n    * optional compression with a special care on avoiding memory copies.\n\n    Attributes\n    ----------\n    fp: file\n        File object handle used for serializing the input object.\n    protocol: int\n        Pickle protocol used. Default is pickle.DEFAULT_PROTOCOL under\n        python 3, pickle.HIGHEST_PROTOCOL otherwise.\n    \"\"\"\n\n    dispatch = Pickler.dispatch.copy()\n\n    def __init__(self, fp, protocol=None):\n        self.file_handle = fp\n        self.buffered = isinstance(self.file_handle, BinaryZlibFile)\n\n        # By default we want a pickle protocol that only changes with\n        # the major python version and not the minor one\n        if protocol is None:\n            protocol = (pickle.DEFAULT_PROTOCOL if PY3_OR_LATER\n                        else pickle.HIGHEST_PROTOCOL)\n\n        Pickler.__init__(self, self.file_handle, protocol=protocol)\n        # delayed import of numpy, to avoid tight coupling\n        try:\n            import numpy as np\n        except ImportError:\n            np = None\n        self.np = np\n\n    def _create_array_wrapper(self, array):\n        \"\"\"Create and returns a numpy array wrapper from a numpy array.\"\"\"\n        order = 'F' if (array.flags.f_contiguous and\n                        not array.flags.c_contiguous) else 'C'\n        allow_mmap = not self.buffered and not array.dtype.hasobject\n        wrapper = NumpyArrayWrapper(type(array),\n                                    array.shape, order, array.dtype,\n                                    allow_mmap=allow_mmap)\n\n        return wrapper\n\n    def save(self, obj):\n        \"\"\"Subclass the Pickler `save` method.\n\n        This is a total abuse of the Pickler class in order to use the numpy\n        persistence function `save` instead of the default pickle\n        implementation. The numpy array is replaced by a custom wrapper in the\n        pickle persistence stack and the serialized array is written right\n        after in the file. Warning: the file produced does not follow the\n        pickle format. As such it can not be read with `pickle.load`.\n        \"\"\"\n        if self.np is not None and type(obj) in (self.np.ndarray,\n                                                 self.np.matrix,\n                                                 self.np.memmap):\n            if type(obj) is self.np.memmap:\n                # Pickling doesn't work with memmapped arrays\n                obj = self.np.asanyarray(obj)\n\n            # The array wrapper is pickled instead of the real array.\n            wrapper = self._create_array_wrapper(obj)\n            Pickler.save(self, wrapper)\n\n            # A framer was introduced with pickle protocol 4 and we want to\n            # ensure the wrapper object is written before the numpy array\n            # buffer in the pickle file.\n            # See https://www.python.org/dev/peps/pep-3154/#framing to get\n            # more information on the framer behavior.\n            if self.proto >= 4:\n                self.framer.commit_frame(force=True)\n\n            # And then array bytes are written right after the wrapper.\n            wrapper.write_array(obj, self)\n            return\n\n        return Pickler.save(self, obj)\n\n\nclass NumpyUnpickler(Unpickler):\n    \"\"\"A subclass of the Unpickler to unpickle our numpy pickles.\n\n    Attributes\n    ----------\n    mmap_mode: str\n        The memorymap mode to use for reading numpy arrays.\n    file_handle: file_like\n        File object to unpickle from.\n    filename: str\n        Name of the file to unpickle from. It should correspond to file_handle.\n        This parameter is required when using mmap_mode.\n    np: module\n        Reference to numpy module if numpy is installed else None.\n\n    \"\"\"\n\n    dispatch = Unpickler.dispatch.copy()\n\n    def __init__(self, filename, file_handle, mmap_mode=None):\n        # The next line is for backward compatibility with pickle generated\n        # with joblib versions less than 0.10.\n        self._dirname = os.path.dirname(filename)\n\n        self.mmap_mode = mmap_mode\n        self.file_handle = file_handle\n        # filename is required for numpy mmap mode.\n        self.filename = filename\n        self.compat_mode = False\n        Unpickler.__init__(self, self.file_handle)\n        try:\n            import numpy as np\n        except ImportError:\n            np = None\n        self.np = np\n\n    def load_build(self):\n        \"\"\"Called to set the state of a newly created object.\n\n        We capture it to replace our place-holder objects, NDArrayWrapper or\n        NumpyArrayWrapper, by the array we are interested in. We\n        replace them directly in the stack of pickler.\n        NDArrayWrapper is used for backward compatibility with joblib <= 0.9.\n        \"\"\"\n        Unpickler.load_build(self)\n\n        # For backward compatibility, we support NDArrayWrapper objects.\n        if isinstance(self.stack[-1], (NDArrayWrapper, NumpyArrayWrapper)):\n            if self.np is None:\n                raise ImportError(\"Trying to unpickle an ndarray, \"\n                                  \"but numpy didn't import correctly\")\n            array_wrapper = self.stack.pop()\n            # If any NDArrayWrapper is found, we switch to compatibility mode,\n            # this will be used to raise a DeprecationWarning to the user at\n            # the end of the unpickling.\n            if isinstance(array_wrapper, NDArrayWrapper):\n                self.compat_mode = True\n            self.stack.append(array_wrapper.read(self))\n\n    # Be careful to register our new method.\n    if PY3_OR_LATER:\n        dispatch[pickle.BUILD[0]] = load_build\n    else:\n        dispatch[pickle.BUILD] = load_build\n\n\n###############################################################################\n# Utility functions\n\ndef dump(value, filename, compress=0, protocol=None, cache_size=None):\n    \"\"\"Persist an arbitrary Python object into one file.\n\n    Parameters\n    -----------\n    value: any Python object\n        The object to store to disk.\n    filename: str or pathlib.Path\n        The path of the file in which it is to be stored. The compression\n        method corresponding to one of the supported filename extensions ('.z',\n        '.gz', '.bz2', '.xz' or '.lzma') will be used automatically.\n    compress: int from 0 to 9 or bool or 2-tuple, optional\n        Optional compression level for the data. 0 or False is no compression.\n        Higher value means more compression, but also slower read and\n        write times. Using a value of 3 is often a good compromise.\n        See the notes for more details.\n        If compress is True, the compression level used is 3.\n        If compress is a 2-tuple, the first element must correspond to a string\n        between supported compressors (e.g 'zlib', 'gzip', 'bz2', 'lzma'\n        'xz'), the second element must be an integer from 0 to 9, corresponding\n        to the compression level.\n    protocol: positive int\n        Pickle protocol, see pickle.dump documentation for more details.\n    cache_size: positive int, optional\n        This option is deprecated in 0.10 and has no effect.\n\n    Returns\n    -------\n    filenames: list of strings\n        The list of file names in which the data is stored. If\n        compress is false, each array is stored in a different file.\n\n    See Also\n    --------\n    joblib.load : corresponding loader\n\n    Notes\n    -----\n    Memmapping on load cannot be used for compressed files. Thus\n    using compression can significantly slow down loading. In\n    addition, compressed files take extra extra memory during\n    dump and load.\n\n    \"\"\"\n\n    if Path is not None and isinstance(filename, Path):\n        filename = str(filename)\n\n    is_filename = isinstance(filename, _basestring)\n    is_fileobj = hasattr(filename, \"write\")\n\n    compress_method = 'zlib'  # zlib is the default compression method.\n    if compress is True:\n        # By default, if compress is enabled, we want to be using 3 by default\n        compress_level = 3\n    elif isinstance(compress, tuple):\n        # a 2-tuple was set in compress\n        if len(compress) != 2:\n            raise ValueError(\n                'Compress argument tuple should contain exactly 2 elements: '\n                '(compress method, compress level), you passed {}'\n                .format(compress))\n        compress_method, compress_level = compress\n    else:\n        compress_level = compress\n\n    if compress_level is not False and compress_level not in range(10):\n        # Raising an error if a non valid compress level is given.\n        raise ValueError(\n            'Non valid compress level given: \"{}\". Possible values are '\n            '{}.'.format(compress_level, list(range(10))))\n\n    if compress_method not in _COMPRESSORS:\n        # Raising an error if an unsupported compression method is given.\n        raise ValueError(\n            'Non valid compression method given: \"{}\". Possible values are '\n            '{}.'.format(compress_method, _COMPRESSORS))\n\n    if not is_filename and not is_fileobj:\n        # People keep inverting arguments, and the resulting error is\n        # incomprehensible\n        raise ValueError(\n            'Second argument should be a filename or a file-like object, '\n            '%s (type %s) was given.'\n            % (filename, type(filename))\n        )\n\n    if is_filename and not isinstance(compress, tuple):\n        # In case no explicit compression was requested using both compression\n        # method and level in a tuple and the filename has an explicit\n        # extension, we select the corresponding compressor.\n        if filename.endswith('.z'):\n            compress_method = 'zlib'\n        elif filename.endswith('.gz'):\n            compress_method = 'gzip'\n        elif filename.endswith('.bz2'):\n            compress_method = 'bz2'\n        elif filename.endswith('.lzma'):\n            compress_method = 'lzma'\n        elif filename.endswith('.xz'):\n            compress_method = 'xz'\n        else:\n            # no matching compression method found, we unset the variable to\n            # be sure no compression level is set afterwards.\n            compress_method = None\n\n        if compress_method in _COMPRESSORS and compress_level == 0:\n            # we choose a default compress_level of 3 in case it was not given\n            # as an argument (using compress).\n            compress_level = 3\n\n    if not PY3_OR_LATER and compress_method in ('lzma', 'xz'):\n        raise NotImplementedError(\"{} compression is only available for \"\n                                  \"python version >= 3.3. You are using \"\n                                  \"{}.{}\".format(compress_method,\n                                                 sys.version_info[0],\n                                                 sys.version_info[1]))\n\n    if cache_size is not None:\n        # Cache size is deprecated starting from version 0.10\n        warnings.warn(\"Please do not set 'cache_size' in joblib.dump, \"\n                      \"this parameter has no effect and will be removed. \"\n                      \"You used 'cache_size={}'\".format(cache_size),\n                      DeprecationWarning, stacklevel=2)\n\n    if compress_level != 0:\n        with _write_fileobject(filename, compress=(compress_method,\n                                                   compress_level)) as f:\n            NumpyPickler(f, protocol=protocol).dump(value)\n    elif is_filename:\n        with open(filename, 'wb') as f:\n            NumpyPickler(f, protocol=protocol).dump(value)\n    else:\n        NumpyPickler(filename, protocol=protocol).dump(value)\n\n    # If the target container is a file object, nothing is returned.\n    if is_fileobj:\n        return\n\n    # For compatibility, the list of created filenames (e.g with one element\n    # after 0.10.0) is returned by default.\n    return [filename]\n\n\ndef _unpickle(fobj, filename=\"\", mmap_mode=None):\n    \"\"\"Internal unpickling function.\"\"\"\n    # We are careful to open the file handle early and keep it open to\n    # avoid race-conditions on renames.\n    # That said, if data is stored in companion files, which can be\n    # the case with the old persistence format, moving the directory\n    # will create a race when joblib tries to access the companion\n    # files.\n    unpickler = NumpyUnpickler(filename, fobj, mmap_mode=mmap_mode)\n    obj = None\n    try:\n        obj = unpickler.load()\n        if unpickler.compat_mode:\n            warnings.warn(\"The file '%s' has been generated with a \"\n                          \"joblib version less than 0.10. \"\n                          \"Please regenerate this pickle file.\"\n                          % filename,\n                          DeprecationWarning, stacklevel=3)\n    except UnicodeDecodeError as exc:\n        # More user-friendly error message\n        if PY3_OR_LATER:\n            new_exc = ValueError(\n                'You may be trying to read with '\n                'python 3 a joblib pickle generated with python 2. '\n                'This feature is not supported by joblib.')\n            new_exc.__cause__ = exc\n            raise new_exc\n        # Reraise exception with Python 2\n        raise\n\n    return obj\n\n\ndef load(filename, mmap_mode=None):\n    \"\"\"Reconstruct a Python object from a file persisted with joblib.dump.\n\n    Parameters\n    -----------\n    filename: str or pathlib.Path\n        The path of the file from which to load the object\n    mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional\n        If not None, the arrays are memory-mapped from the disk. This\n        mode has no effect for compressed files. Note that in this\n        case the reconstructed object might not longer match exactly\n        the originally pickled object.\n\n    Returns\n    -------\n    result: any Python object\n        The object stored in the file.\n\n    See Also\n    --------\n    joblib.dump : function to save an object\n\n    Notes\n    -----\n\n    This function can load numpy array files saved separately during the\n    dump. If the mmap_mode argument is given, it is passed to np.load and\n    arrays are loaded as memmaps. As a consequence, the reconstructed\n    object might not match the original pickled object. Note that if the\n    file was saved with compression, the arrays cannot be memmaped.\n    \"\"\"\n    if Path is not None and isinstance(filename, Path):\n        filename = str(filename)\n\n    if hasattr(filename, \"read\"):\n        fobj = filename\n        filename = getattr(fobj, 'name', '')\n        with _read_fileobject(fobj, filename, mmap_mode) as fobj:\n            obj = _unpickle(fobj)\n    else:\n        with open(filename, 'rb') as f:\n            with _read_fileobject(f, filename, mmap_mode) as fobj:\n                if isinstance(fobj, _basestring):\n                    # if the returned file object is a string, this means we\n                    # try to load a pickle file generated with an version of\n                    # Joblib so we load it with joblib compatibility function.\n                    return load_compatibility(fobj)\n\n                obj = _unpickle(fobj, filename, mmap_mode)\n\n    return obj\n"
    },
    {
      "filename": "sklearn/externals/_joblib/numpy_pickle_compat.py",
      "content": "\"\"\"Numpy pickle compatibility functions.\"\"\"\n\nimport pickle\nimport os\nimport zlib\nfrom io import BytesIO\n\nfrom ._compat import PY3_OR_LATER\nfrom .numpy_pickle_utils import _ZFILE_PREFIX\nfrom .numpy_pickle_utils import Unpickler\n\n\ndef hex_str(an_int):\n    \"\"\"Convert an int to an hexadecimal string.\"\"\"\n    return '{:#x}'.format(an_int)\n\nif PY3_OR_LATER:\n    def asbytes(s):\n        if isinstance(s, bytes):\n            return s\n        return s.encode('latin1')\nelse:\n    asbytes = str\n\n_MAX_LEN = len(hex_str(2 ** 64))\n_CHUNK_SIZE = 64 * 1024\n\n\ndef read_zfile(file_handle):\n    \"\"\"Read the z-file and return the content as a string.\n\n    Z-files are raw data compressed with zlib used internally by joblib\n    for persistence. Backward compatibility is not guaranteed. Do not\n    use for external purposes.\n    \"\"\"\n    file_handle.seek(0)\n    header_length = len(_ZFILE_PREFIX) + _MAX_LEN\n    length = file_handle.read(header_length)\n    length = length[len(_ZFILE_PREFIX):]\n    length = int(length, 16)\n\n    # With python2 and joblib version <= 0.8.4 compressed pickle header is one\n    # character wider so we need to ignore an additional space if present.\n    # Note: the first byte of the zlib data is guaranteed not to be a\n    # space according to\n    # https://tools.ietf.org/html/rfc6713#section-2.1\n    next_byte = file_handle.read(1)\n    if next_byte != b' ':\n        # The zlib compressed data has started and we need to go back\n        # one byte\n        file_handle.seek(header_length)\n\n    # We use the known length of the data to tell Zlib the size of the\n    # buffer to allocate.\n    data = zlib.decompress(file_handle.read(), 15, length)\n    assert len(data) == length, (\n        \"Incorrect data length while decompressing %s.\"\n        \"The file could be corrupted.\" % file_handle)\n    return data\n\n\ndef write_zfile(file_handle, data, compress=1):\n    \"\"\"Write the data in the given file as a Z-file.\n\n    Z-files are raw data compressed with zlib used internally by joblib\n    for persistence. Backward compatibility is not guarantied. Do not\n    use for external purposes.\n    \"\"\"\n    file_handle.write(_ZFILE_PREFIX)\n    length = hex_str(len(data))\n    # Store the length of the data\n    file_handle.write(asbytes(length.ljust(_MAX_LEN)))\n    file_handle.write(zlib.compress(asbytes(data), compress))\n\n###############################################################################\n# Utility objects for persistence.\n\n\nclass NDArrayWrapper(object):\n    \"\"\"An object to be persisted instead of numpy arrays.\n\n    The only thing this object does, is to carry the filename in which\n    the array has been persisted, and the array subclass.\n    \"\"\"\n\n    def __init__(self, filename, subclass, allow_mmap=True):\n        \"\"\"Constructor. Store the useful information for later.\"\"\"\n        self.filename = filename\n        self.subclass = subclass\n        self.allow_mmap = allow_mmap\n\n    def read(self, unpickler):\n        \"\"\"Reconstruct the array.\"\"\"\n        filename = os.path.join(unpickler._dirname, self.filename)\n        # Load the array from the disk\n        # use getattr instead of self.allow_mmap to ensure backward compat\n        # with NDArrayWrapper instances pickled with joblib < 0.9.0\n        allow_mmap = getattr(self, 'allow_mmap', True)\n        memmap_kwargs = ({} if not allow_mmap\n                         else {'mmap_mode': unpickler.mmap_mode})\n        array = unpickler.np.load(filename, **memmap_kwargs)\n        # Reconstruct subclasses. This does not work with old\n        # versions of numpy\n        if (hasattr(array, '__array_prepare__') and\n            self.subclass not in (unpickler.np.ndarray,\n                                  unpickler.np.memmap)):\n            # We need to reconstruct another subclass\n            new_array = unpickler.np.core.multiarray._reconstruct(\n                self.subclass, (0,), 'b')\n            return new_array.__array_prepare__(array)\n        else:\n            return array\n\n\nclass ZNDArrayWrapper(NDArrayWrapper):\n    \"\"\"An object to be persisted instead of numpy arrays.\n\n    This object store the Zfile filename in which\n    the data array has been persisted, and the meta information to\n    retrieve it.\n    The reason that we store the raw buffer data of the array and\n    the meta information, rather than array representation routine\n    (tostring) is that it enables us to use completely the strided\n    model to avoid memory copies (a and a.T store as fast). In\n    addition saving the heavy information separately can avoid\n    creating large temporary buffers when unpickling data with\n    large arrays.\n    \"\"\"\n\n    def __init__(self, filename, init_args, state):\n        \"\"\"Constructor. Store the useful information for later.\"\"\"\n        self.filename = filename\n        self.state = state\n        self.init_args = init_args\n\n    def read(self, unpickler):\n        \"\"\"Reconstruct the array from the meta-information and the z-file.\"\"\"\n        # Here we a simply reproducing the unpickling mechanism for numpy\n        # arrays\n        filename = os.path.join(unpickler._dirname, self.filename)\n        array = unpickler.np.core.multiarray._reconstruct(*self.init_args)\n        with open(filename, 'rb') as f:\n            data = read_zfile(f)\n        state = self.state + (data,)\n        array.__setstate__(state)\n        return array\n\n\nclass ZipNumpyUnpickler(Unpickler):\n    \"\"\"A subclass of the Unpickler to unpickle our numpy pickles.\"\"\"\n\n    dispatch = Unpickler.dispatch.copy()\n\n    def __init__(self, filename, file_handle, mmap_mode=None):\n        \"\"\"Constructor.\"\"\"\n        self._filename = os.path.basename(filename)\n        self._dirname = os.path.dirname(filename)\n        self.mmap_mode = mmap_mode\n        self.file_handle = self._open_pickle(file_handle)\n        Unpickler.__init__(self, self.file_handle)\n        try:\n            import numpy as np\n        except ImportError:\n            np = None\n        self.np = np\n\n    def _open_pickle(self, file_handle):\n        return BytesIO(read_zfile(file_handle))\n\n    def load_build(self):\n        \"\"\"Set the state of a newly created object.\n\n        We capture it to replace our place-holder objects,\n        NDArrayWrapper, by the array we are interested in. We\n        replace them directly in the stack of pickler.\n        \"\"\"\n        Unpickler.load_build(self)\n        if isinstance(self.stack[-1], NDArrayWrapper):\n            if self.np is None:\n                raise ImportError(\"Trying to unpickle an ndarray, \"\n                                  \"but numpy didn't import correctly\")\n            nd_array_wrapper = self.stack.pop()\n            array = nd_array_wrapper.read(self)\n            self.stack.append(array)\n\n    # Be careful to register our new method.\n    if PY3_OR_LATER:\n        dispatch[pickle.BUILD[0]] = load_build\n    else:\n        dispatch[pickle.BUILD] = load_build\n\n\ndef load_compatibility(filename):\n    \"\"\"Reconstruct a Python object from a file persisted with joblib.dump.\n\n    This function ensures the compatibility with joblib old persistence format\n    (<= 0.9.3).\n\n    Parameters\n    -----------\n    filename: string\n        The name of the file from which to load the object\n\n    Returns\n    -------\n    result: any Python object\n        The object stored in the file.\n\n    See Also\n    --------\n    joblib.dump : function to save an object\n\n    Notes\n    -----\n\n    This function can load numpy array files saved separately during the\n    dump.\n    \"\"\"\n    with open(filename, 'rb') as file_handle:\n        # We are careful to open the file handle early and keep it open to\n        # avoid race-conditions on renames. That said, if data is stored in\n        # companion files, moving the directory will create a race when\n        # joblib tries to access the companion files.\n        unpickler = ZipNumpyUnpickler(filename, file_handle=file_handle)\n        try:\n            obj = unpickler.load()\n        except UnicodeDecodeError as exc:\n            # More user-friendly error message\n            if PY3_OR_LATER:\n                new_exc = ValueError(\n                    'You may be trying to read with '\n                    'python 3 a joblib pickle generated with python 2. '\n                    'This feature is not supported by joblib.')\n                new_exc.__cause__ = exc\n                raise new_exc\n        finally:\n            if hasattr(unpickler, 'file_handle'):\n                unpickler.file_handle.close()\n        return obj\n"
    },
    {
      "filename": "sklearn/externals/_joblib/numpy_pickle_utils.py",
      "content": "\"\"\"Utilities for fast persistence of big data, with optional compression.\"\"\"\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n# Copyright (c) 2009 Gael Varoquaux\n# License: BSD Style, 3 clauses.\n\nimport pickle\nimport sys\nimport io\nimport zlib\nimport gzip\nimport warnings\nimport contextlib\nfrom contextlib import closing\n\nfrom ._compat import PY3_OR_LATER, PY27, _basestring\n\ntry:\n    from threading import RLock\nexcept ImportError:\n    from dummy_threading import RLock\n\nif PY3_OR_LATER:\n    Unpickler = pickle._Unpickler\n    Pickler = pickle._Pickler\n    xrange = range\nelse:\n    Unpickler = pickle.Unpickler\n    Pickler = pickle.Pickler\n\ntry:\n    import numpy as np\nexcept ImportError:\n    np = None\n\ntry:\n    import lzma\nexcept ImportError:\n    lzma = None\n\n\ntry:\n    # The python standard library can be built without bz2 so we make bz2\n    # usage optional.\n    # see https://github.com/scikit-learn/scikit-learn/issues/7526 for more\n    # details.\n    import bz2\nexcept ImportError:\n    bz2 = None\n\n\n# Magic numbers of supported compression file formats.        '\n_ZFILE_PREFIX = b'ZF'  # used with pickle files created before 0.9.3.\n_ZLIB_PREFIX = b'\\x78'\n_GZIP_PREFIX = b'\\x1f\\x8b'\n_BZ2_PREFIX = b'BZ'\n_XZ_PREFIX = b'\\xfd\\x37\\x7a\\x58\\x5a'\n_LZMA_PREFIX = b'\\x5d\\x00'\n\n# Supported compressors\n_COMPRESSORS = ('zlib', 'bz2', 'lzma', 'xz', 'gzip')\n_COMPRESSOR_CLASSES = [gzip.GzipFile]\n\nif bz2 is not None:\n    _COMPRESSOR_CLASSES.append(bz2.BZ2File)\n\nif lzma is not None:\n    _COMPRESSOR_CLASSES.append(lzma.LZMAFile)\n\n# The max magic number length of supported compression file types.\n_MAX_PREFIX_LEN = max(len(prefix)\n                      for prefix in (_ZFILE_PREFIX, _GZIP_PREFIX, _BZ2_PREFIX,\n                                     _XZ_PREFIX, _LZMA_PREFIX))\n\n# Buffer size used in io.BufferedReader and io.BufferedWriter\n_IO_BUFFER_SIZE = 1024 ** 2\n\n\ndef _is_raw_file(fileobj):\n    \"\"\"Check if fileobj is a raw file object, e.g created with open.\"\"\"\n    if PY3_OR_LATER:\n        fileobj = getattr(fileobj, 'raw', fileobj)\n        return isinstance(fileobj, io.FileIO)\n    else:\n        return isinstance(fileobj, file)  # noqa\n\n\n###############################################################################\n# Cache file utilities\ndef _detect_compressor(fileobj):\n    \"\"\"Return the compressor matching fileobj.\n\n    Parameters\n    ----------\n    fileobj: file object\n\n    Returns\n    -------\n    str in {'zlib', 'gzip', 'bz2', 'lzma', 'xz', 'compat', 'not-compressed'}\n    \"\"\"\n    # Read the magic number in the first bytes of the file.\n    if hasattr(fileobj, 'peek'):\n        # Peek allows to read those bytes without moving the cursor in the\n        # file which.\n        first_bytes = fileobj.peek(_MAX_PREFIX_LEN)\n    else:\n        # Fallback to seek if the fileobject is not peekable.\n        first_bytes = fileobj.read(_MAX_PREFIX_LEN)\n        fileobj.seek(0)\n\n    if first_bytes.startswith(_ZLIB_PREFIX):\n        return \"zlib\"\n    elif first_bytes.startswith(_GZIP_PREFIX):\n        return \"gzip\"\n    elif first_bytes.startswith(_BZ2_PREFIX):\n        return \"bz2\"\n    elif first_bytes.startswith(_LZMA_PREFIX):\n        return \"lzma\"\n    elif first_bytes.startswith(_XZ_PREFIX):\n        return \"xz\"\n    elif first_bytes.startswith(_ZFILE_PREFIX):\n        return \"compat\"\n\n    return \"not-compressed\"\n\n\ndef _buffered_read_file(fobj):\n    \"\"\"Return a buffered version of a read file object.\"\"\"\n    if PY27 and bz2 is not None and isinstance(fobj, bz2.BZ2File):\n        # Python 2.7 doesn't work with BZ2File through a buffer: \"no\n        # attribute 'readable'\" error.\n        return fobj\n    else:\n        return io.BufferedReader(fobj, buffer_size=_IO_BUFFER_SIZE)\n\n\ndef _buffered_write_file(fobj):\n    \"\"\"Return a buffered version of a write file object.\"\"\"\n    if PY27 and bz2 is not None and isinstance(fobj, bz2.BZ2File):\n        # Python 2.7 doesn't work with BZ2File through a buffer: no attribute\n        # 'writable'.\n        # BZ2File doesn't implement the file object context manager in python 2\n        # so we wrap the fileobj using `closing`.\n        return closing(fobj)\n    else:\n        return io.BufferedWriter(fobj, buffer_size=_IO_BUFFER_SIZE)\n\n\n@contextlib.contextmanager\ndef _read_fileobject(fileobj, filename, mmap_mode=None):\n    \"\"\"Utility function opening the right fileobject from a filename.\n\n    The magic number is used to choose between the type of file object to open:\n    * regular file object (default)\n    * zlib file object\n    * gzip file object\n    * bz2 file object\n    * lzma file object (for xz and lzma compressor)\n\n    Parameters\n    ----------\n    fileobj: file object\n    compressor: str in {'zlib', 'gzip', 'bz2', 'lzma', 'xz', 'compat',\n                        'not-compressed'}\n    filename: str\n        filename path corresponding to the fileobj parameter.\n    mmap_mode: str\n        memory map mode that should be used to open the pickle file. This\n        parameter is useful to verify that the user is not trying to one with\n        compression. Default: None.\n\n    Returns\n    -------\n        a file like object\n\n    \"\"\"\n    # Detect if the fileobj contains compressed data.\n    compressor = _detect_compressor(fileobj)\n\n    if compressor == 'compat':\n        # Compatibility with old pickle mode: simply return the input\n        # filename \"as-is\" and let the compatibility function be called by the\n        # caller.\n        warnings.warn(\"The file '%s' has been generated with a joblib \"\n                      \"version less than 0.10. \"\n                      \"Please regenerate this pickle file.\" % filename,\n                      DeprecationWarning, stacklevel=2)\n        yield filename\n    else:\n        # based on the compressor detected in the file, we open the\n        # correct decompressor file object, wrapped in a buffer.\n        if compressor == 'zlib':\n            fileobj = _buffered_read_file(BinaryZlibFile(fileobj, 'rb'))\n        elif compressor == 'gzip':\n            fileobj = _buffered_read_file(BinaryGzipFile(fileobj, 'rb'))\n        elif compressor == 'bz2' and bz2 is not None:\n            if PY3_OR_LATER:\n                fileobj = _buffered_read_file(bz2.BZ2File(fileobj, 'rb'))\n            else:\n                # In python 2, BZ2File doesn't support a fileobj opened in\n                # binary mode. In this case, we pass the filename.\n                fileobj = _buffered_read_file(bz2.BZ2File(fileobj.name, 'rb'))\n        elif (compressor == 'lzma' or compressor == 'xz'):\n            if PY3_OR_LATER and lzma is not None:\n                # We support lzma only in python 3 because in python 2 users\n                # may have installed the pyliblzma package, which also provides\n                # the lzma module, but that unfortunately doesn't fully support\n                # the buffer interface required by joblib.\n                # See https://github.com/joblib/joblib/issues/403 for details.\n                fileobj = _buffered_read_file(lzma.LZMAFile(fileobj, 'rb'))\n            else:\n                raise NotImplementedError(\"Lzma decompression is not \"\n                                          \"supported for this version of \"\n                                          \"python ({}.{})\"\n                                          .format(sys.version_info[0],\n                                                  sys.version_info[1]))\n        # Checking if incompatible load parameters with the type of file:\n        # mmap_mode cannot be used with compressed file or in memory buffers\n        # such as io.BytesIO.\n        if mmap_mode is not None:\n            if isinstance(fileobj, io.BytesIO):\n                warnings.warn('In memory persistence is not compatible with '\n                              'mmap_mode \"%(mmap_mode)s\" flag passed. '\n                              'mmap_mode option will be ignored.'\n                              % locals(), stacklevel=2)\n            elif compressor != 'not-compressed':\n                warnings.warn('mmap_mode \"%(mmap_mode)s\" is not compatible '\n                              'with compressed file %(filename)s. '\n                              '\"%(mmap_mode)s\" flag will be ignored.'\n                              % locals(), stacklevel=2)\n            elif not _is_raw_file(fileobj):\n                warnings.warn('\"%(fileobj)r\" is not a raw file, mmap_mode '\n                              '\"%(mmap_mode)s\" flag will be ignored.'\n                              % locals(), stacklevel=2)\n\n        yield fileobj\n\n\ndef _write_fileobject(filename, compress=(\"zlib\", 3)):\n    \"\"\"Return the right compressor file object in write mode.\"\"\"\n    compressmethod = compress[0]\n    compresslevel = compress[1]\n    if compressmethod == \"gzip\":\n        return _buffered_write_file(BinaryGzipFile(filename, 'wb',\n                                    compresslevel=compresslevel))\n    elif compressmethod == \"bz2\" and bz2 is not None:\n        return _buffered_write_file(bz2.BZ2File(filename, 'wb',\n                                                compresslevel=compresslevel))\n    elif lzma is not None and compressmethod == \"xz\":\n        return _buffered_write_file(lzma.LZMAFile(filename, 'wb',\n                                                  check=lzma.CHECK_NONE,\n                                                  preset=compresslevel))\n    elif lzma is not None and compressmethod == \"lzma\":\n        return _buffered_write_file(lzma.LZMAFile(filename, 'wb',\n                                                  preset=compresslevel,\n                                                  format=lzma.FORMAT_ALONE))\n    else:\n        return _buffered_write_file(BinaryZlibFile(filename, 'wb',\n                                    compresslevel=compresslevel))\n\n\n###############################################################################\n#  Joblib zlib compression file object definition\n\n_MODE_CLOSED = 0\n_MODE_READ = 1\n_MODE_READ_EOF = 2\n_MODE_WRITE = 3\n_BUFFER_SIZE = 8192\n\n\nclass BinaryZlibFile(io.BufferedIOBase):\n    \"\"\"A file object providing transparent zlib (de)compression.\n\n    A BinaryZlibFile can act as a wrapper for an existing file object, or refer\n    directly to a named file on disk.\n\n    Note that BinaryZlibFile provides only a *binary* file interface: data read\n    is returned as bytes, and data to be written should be given as bytes.\n\n    This object is an adaptation of the BZ2File object and is compatible with\n    versions of python >= 2.7.\n\n    If filename is a str or bytes object, it gives the name\n    of the file to be opened. Otherwise, it should be a file object,\n    which will be used to read or write the compressed data.\n\n    mode can be 'rb' for reading (default) or 'wb' for (over)writing\n\n    If mode is 'wb', compresslevel can be a number between 1\n    and 9 specifying the level of compression: 1 produces the least\n    compression, and 9 (default) produces the most compression.\n    \"\"\"\n\n    wbits = zlib.MAX_WBITS\n\n    def __init__(self, filename, mode=\"rb\", compresslevel=9):\n        # This lock must be recursive, so that BufferedIOBase's\n        # readline(), readlines() and writelines() don't deadlock.\n        self._lock = RLock()\n        self._fp = None\n        self._closefp = False\n        self._mode = _MODE_CLOSED\n        self._pos = 0\n        self._size = -1\n\n        if not isinstance(compresslevel, int) or not (1 <= compresslevel <= 9):\n            raise ValueError(\"'compresslevel' must be an integer \"\n                             \"between 1 and 9. You provided 'compresslevel={}'\"\n                             .format(compresslevel))\n\n        if mode == \"rb\":\n            mode_code = _MODE_READ\n            self._decompressor = zlib.decompressobj(self.wbits)\n            self._buffer = b\"\"\n            self._buffer_offset = 0\n        elif mode == \"wb\":\n            mode_code = _MODE_WRITE\n            self._compressor = zlib.compressobj(compresslevel,\n                                                zlib.DEFLATED,\n                                                self.wbits,\n                                                zlib.DEF_MEM_LEVEL,\n                                                0)\n        else:\n            raise ValueError(\"Invalid mode: %r\" % (mode,))\n\n        if isinstance(filename, _basestring):\n            self._fp = io.open(filename, mode)\n            self._closefp = True\n            self._mode = mode_code\n        elif hasattr(filename, \"read\") or hasattr(filename, \"write\"):\n            self._fp = filename\n            self._mode = mode_code\n        else:\n            raise TypeError(\"filename must be a str or bytes object, \"\n                            \"or a file\")\n\n    def close(self):\n        \"\"\"Flush and close the file.\n\n        May be called more than once without error. Once the file is\n        closed, any other operation on it will raise a ValueError.\n        \"\"\"\n        with self._lock:\n            if self._mode == _MODE_CLOSED:\n                return\n            try:\n                if self._mode in (_MODE_READ, _MODE_READ_EOF):\n                    self._decompressor = None\n                elif self._mode == _MODE_WRITE:\n                    self._fp.write(self._compressor.flush())\n                    self._compressor = None\n            finally:\n                try:\n                    if self._closefp:\n                        self._fp.close()\n                finally:\n                    self._fp = None\n                    self._closefp = False\n                    self._mode = _MODE_CLOSED\n                    self._buffer = b\"\"\n                    self._buffer_offset = 0\n\n    @property\n    def closed(self):\n        \"\"\"True if this file is closed.\"\"\"\n        return self._mode == _MODE_CLOSED\n\n    def fileno(self):\n        \"\"\"Return the file descriptor for the underlying file.\"\"\"\n        self._check_not_closed()\n        return self._fp.fileno()\n\n    def seekable(self):\n        \"\"\"Return whether the file supports seeking.\"\"\"\n        return self.readable() and self._fp.seekable()\n\n    def readable(self):\n        \"\"\"Return whether the file was opened for reading.\"\"\"\n        self._check_not_closed()\n        return self._mode in (_MODE_READ, _MODE_READ_EOF)\n\n    def writable(self):\n        \"\"\"Return whether the file was opened for writing.\"\"\"\n        self._check_not_closed()\n        return self._mode == _MODE_WRITE\n\n    # Mode-checking helper functions.\n\n    def _check_not_closed(self):\n        if self.closed:\n            fname = getattr(self._fp, 'name', None)\n            msg = \"I/O operation on closed file\"\n            if fname is not None:\n                msg += \" {}\".format(fname)\n            msg += \".\"\n            raise ValueError(msg)\n\n    def _check_can_read(self):\n        if self._mode not in (_MODE_READ, _MODE_READ_EOF):\n            self._check_not_closed()\n            raise io.UnsupportedOperation(\"File not open for reading\")\n\n    def _check_can_write(self):\n        if self._mode != _MODE_WRITE:\n            self._check_not_closed()\n            raise io.UnsupportedOperation(\"File not open for writing\")\n\n    def _check_can_seek(self):\n        if self._mode not in (_MODE_READ, _MODE_READ_EOF):\n            self._check_not_closed()\n            raise io.UnsupportedOperation(\"Seeking is only supported \"\n                                          \"on files open for reading\")\n        if not self._fp.seekable():\n            raise io.UnsupportedOperation(\"The underlying file object \"\n                                          \"does not support seeking\")\n\n    # Fill the readahead buffer if it is empty. Returns False on EOF.\n    def _fill_buffer(self):\n        if self._mode == _MODE_READ_EOF:\n            return False\n        # Depending on the input data, our call to the decompressor may not\n        # return any data. In this case, try again after reading another block.\n        while self._buffer_offset == len(self._buffer):\n            try:\n                rawblock = (self._decompressor.unused_data or\n                            self._fp.read(_BUFFER_SIZE))\n\n                if not rawblock:\n                    raise EOFError\n            except EOFError:\n                # End-of-stream marker and end of file. We're good.\n                self._mode = _MODE_READ_EOF\n                self._size = self._pos\n                return False\n            else:\n                self._buffer = self._decompressor.decompress(rawblock)\n            self._buffer_offset = 0\n        return True\n\n    # Read data until EOF.\n    # If return_data is false, consume the data without returning it.\n    def _read_all(self, return_data=True):\n        # The loop assumes that _buffer_offset is 0. Ensure that this is true.\n        self._buffer = self._buffer[self._buffer_offset:]\n        self._buffer_offset = 0\n\n        blocks = []\n        while self._fill_buffer():\n            if return_data:\n                blocks.append(self._buffer)\n            self._pos += len(self._buffer)\n            self._buffer = b\"\"\n        if return_data:\n            return b\"\".join(blocks)\n\n    # Read a block of up to n bytes.\n    # If return_data is false, consume the data without returning it.\n    def _read_block(self, n_bytes, return_data=True):\n        # If we have enough data buffered, return immediately.\n        end = self._buffer_offset + n_bytes\n        if end <= len(self._buffer):\n            data = self._buffer[self._buffer_offset: end]\n            self._buffer_offset = end\n            self._pos += len(data)\n            return data if return_data else None\n\n        # The loop assumes that _buffer_offset is 0. Ensure that this is true.\n        self._buffer = self._buffer[self._buffer_offset:]\n        self._buffer_offset = 0\n\n        blocks = []\n        while n_bytes > 0 and self._fill_buffer():\n            if n_bytes < len(self._buffer):\n                data = self._buffer[:n_bytes]\n                self._buffer_offset = n_bytes\n            else:\n                data = self._buffer\n                self._buffer = b\"\"\n            if return_data:\n                blocks.append(data)\n            self._pos += len(data)\n            n_bytes -= len(data)\n        if return_data:\n            return b\"\".join(blocks)\n\n    def read(self, size=-1):\n        \"\"\"Read up to size uncompressed bytes from the file.\n\n        If size is negative or omitted, read until EOF is reached.\n        Returns b'' if the file is already at EOF.\n        \"\"\"\n        with self._lock:\n            self._check_can_read()\n            if size == 0:\n                return b\"\"\n            elif size < 0:\n                return self._read_all()\n            else:\n                return self._read_block(size)\n\n    def readinto(self, b):\n        \"\"\"Read up to len(b) bytes into b.\n\n        Returns the number of bytes read (0 for EOF).\n        \"\"\"\n        with self._lock:\n            return io.BufferedIOBase.readinto(self, b)\n\n    def write(self, data):\n        \"\"\"Write a byte string to the file.\n\n        Returns the number of uncompressed bytes written, which is\n        always len(data). Note that due to buffering, the file on disk\n        may not reflect the data written until close() is called.\n        \"\"\"\n        with self._lock:\n            self._check_can_write()\n            # Convert data type if called by io.BufferedWriter.\n            if isinstance(data, memoryview):\n                data = data.tobytes()\n\n            compressed = self._compressor.compress(data)\n            self._fp.write(compressed)\n            self._pos += len(data)\n            return len(data)\n\n    # Rewind the file to the beginning of the data stream.\n    def _rewind(self):\n        self._fp.seek(0, 0)\n        self._mode = _MODE_READ\n        self._pos = 0\n        self._decompressor = zlib.decompressobj(self.wbits)\n        self._buffer = b\"\"\n        self._buffer_offset = 0\n\n    def seek(self, offset, whence=0):\n        \"\"\"Change the file position.\n\n        The new position is specified by offset, relative to the\n        position indicated by whence. Values for whence are:\n\n            0: start of stream (default); offset must not be negative\n            1: current stream position\n            2: end of stream; offset must not be positive\n\n        Returns the new file position.\n\n        Note that seeking is emulated, so depending on the parameters,\n        this operation may be extremely slow.\n        \"\"\"\n        with self._lock:\n            self._check_can_seek()\n\n            # Recalculate offset as an absolute file position.\n            if whence == 0:\n                pass\n            elif whence == 1:\n                offset = self._pos + offset\n            elif whence == 2:\n                # Seeking relative to EOF - we need to know the file's size.\n                if self._size < 0:\n                    self._read_all(return_data=False)\n                offset = self._size + offset\n            else:\n                raise ValueError(\"Invalid value for whence: %s\" % (whence,))\n\n            # Make it so that offset is the number of bytes to skip forward.\n            if offset < self._pos:\n                self._rewind()\n            else:\n                offset -= self._pos\n\n            # Read and discard data until we reach the desired position.\n            self._read_block(offset, return_data=False)\n\n            return self._pos\n\n    def tell(self):\n        \"\"\"Return the current file position.\"\"\"\n        with self._lock:\n            self._check_not_closed()\n            return self._pos\n\n\nclass BinaryGzipFile(BinaryZlibFile):\n    \"\"\"A file object providing transparent gzip (de)compression.\n\n    If filename is a str or bytes object, it gives the name\n    of the file to be opened. Otherwise, it should be a file object,\n    which will be used to read or write the compressed data.\n\n    mode can be 'rb' for reading (default) or 'wb' for (over)writing\n\n    If mode is 'wb', compresslevel can be a number between 1\n    and 9 specifying the level of compression: 1 produces the least\n    compression, and 9 (default) produces the most compression.\n    \"\"\"\n\n    wbits = 31  # zlib compressor/decompressor wbits value for gzip format.\n\n\n# Utility functions/variables from numpy required for writing arrays.\n# We need at least the functions introduced in version 1.9 of numpy. Here,\n# we use the ones from numpy 1.10.2.\nBUFFER_SIZE = 2 ** 18  # size of buffer for reading npz files in bytes\n\n\ndef _read_bytes(fp, size, error_template=\"ran out of data\"):\n    \"\"\"Read from file-like object until size bytes are read.\n\n    Raises ValueError if not EOF is encountered before size bytes are read.\n    Non-blocking objects only supported if they derive from io objects.\n\n    Required as e.g. ZipExtFile in python 2.6 can return less data than\n    requested.\n\n    This function was taken from numpy/lib/format.py in version 1.10.2.\n\n    Parameters\n    ----------\n    fp: file-like object\n    size: int\n    error_template: str\n\n    Returns\n    -------\n    a bytes object\n        The data read in bytes.\n\n    \"\"\"\n    data = bytes()\n    while True:\n        # io files (default in python3) return None or raise on\n        # would-block, python2 file will truncate, probably nothing can be\n        # done about that.  note that regular files can't be non-blocking\n        try:\n            r = fp.read(size - len(data))\n            data += r\n            if len(r) == 0 or len(data) == size:\n                break\n        except io.BlockingIOError:\n            pass\n    if len(data) != size:\n        msg = \"EOF: reading %s, expected %d bytes got %d\"\n        raise ValueError(msg % (error_template, size, len(data)))\n    else:\n        return data\n"
    },
    {
      "filename": "sklearn/externals/_joblib/parallel.py",
      "content": "\"\"\"\nHelpers for embarrassingly parallel code.\n\"\"\"\n# Author: Gael Varoquaux < gael dot varoquaux at normalesup dot org >\n# Copyright: 2010, Gael Varoquaux\n# License: BSD 3 clause\n\nfrom __future__ import division\n\nimport os\nimport sys\nfrom math import sqrt\nimport functools\nimport time\nimport threading\nimport itertools\nfrom numbers import Integral\nfrom contextlib import contextmanager\nimport warnings\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\n\nfrom ._multiprocessing_helpers import mp\n\nfrom .format_stack import format_outer_frames\nfrom .logger import Logger, short_format_time\nfrom .my_exceptions import TransportableException, _mk_exception\nfrom .disk import memstr_to_bytes\nfrom ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n                                 ThreadingBackend, SequentialBackend)\nfrom ._compat import _basestring\n\n# Make sure that those two classes are part of the public joblib.parallel API\n# so that 3rd party backend implementers can import them from here.\nfrom ._parallel_backends import AutoBatchingMixin  # noqa\nfrom ._parallel_backends import ParallelBackendBase  # noqa\n\nBACKENDS = {\n    'multiprocessing': MultiprocessingBackend,\n    'threading': ThreadingBackend,\n    'sequential': SequentialBackend,\n}\n\n# name of the backend used by default by Parallel outside of any context\n# managed by ``parallel_backend``.\nDEFAULT_BACKEND = 'multiprocessing'\nDEFAULT_N_JOBS = 1\n\n# Thread local value that can be overridden by the ``parallel_backend`` context\n# manager\n_backend = threading.local()\n\n\ndef get_active_backend():\n    \"\"\"Return the active default backend\"\"\"\n    active_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)\n    if active_backend_and_jobs is not None:\n        return active_backend_and_jobs\n    # We are outside of the scope of any parallel_backend context manager,\n    # create the default backend instance now\n    active_backend = BACKENDS[DEFAULT_BACKEND]()\n    return active_backend, DEFAULT_N_JOBS\n\n\n@contextmanager\ndef parallel_backend(backend, n_jobs=-1, **backend_params):\n    \"\"\"Change the default backend used by Parallel inside a with block.\n\n    If ``backend`` is a string it must match a previously registered\n    implementation using the ``register_parallel_backend`` function.\n\n    Alternatively backend can be passed directly as an instance.\n\n    By default all available workers will be used (``n_jobs=-1``) unless the\n    caller passes an explicit value for the ``n_jobs`` parameter.\n\n    This is an alternative to passing a ``backend='backend_name'`` argument to\n    the ``Parallel`` class constructor. It is particularly useful when calling\n    into library code that uses joblib internally but does not expose the\n    backend argument in its own API.\n\n    >>> from operator import neg\n    >>> with parallel_backend('threading'):\n    ...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n    ...\n    [-1, -2, -3, -4, -5]\n\n    Warning: this function is experimental and subject to change in a future\n    version of joblib.\n\n    .. versionadded:: 0.10\n\n    \"\"\"\n    if isinstance(backend, _basestring):\n        backend = BACKENDS[backend](**backend_params)\n    old_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)\n    try:\n        _backend.backend_and_jobs = (backend, n_jobs)\n        # return the backend instance to make it easier to write tests\n        yield backend, n_jobs\n    finally:\n        if old_backend_and_jobs is None:\n            if getattr(_backend, 'backend_and_jobs', None) is not None:\n                del _backend.backend_and_jobs\n        else:\n            _backend.backend_and_jobs = old_backend_and_jobs\n\n\n# Under Linux or OS X the default start method of multiprocessing\n# can cause third party libraries to crash. Under Python 3.4+ it is possible\n# to set an environment variable to switch the default start method from\n# 'fork' to 'forkserver' or 'spawn' to avoid this issue albeit at the cost\n# of causing semantic changes and some additional pool instantiation overhead.\nif hasattr(mp, 'get_context'):\n    method = os.environ.get('JOBLIB_START_METHOD', '').strip() or None\n    DEFAULT_MP_CONTEXT = mp.get_context(method=method)\nelse:\n    DEFAULT_MP_CONTEXT = None\n\n\nclass BatchedCalls(object):\n    \"\"\"Wrap a sequence of (func, args, kwargs) tuples as a single callable\"\"\"\n\n    def __init__(self, iterator_slice):\n        self.items = list(iterator_slice)\n        self._size = len(self.items)\n\n    def __call__(self):\n        return [func(*args, **kwargs) for func, args, kwargs in self.items]\n\n    def __len__(self):\n        return self._size\n\n\n###############################################################################\n# CPU count that works also when multiprocessing has been disabled via\n# the JOBLIB_MULTIPROCESSING environment variable\ndef cpu_count():\n    \"\"\"Return the number of CPUs.\"\"\"\n    if mp is None:\n        return 1\n    return mp.cpu_count()\n\n\n###############################################################################\n# For verbosity\n\ndef _verbosity_filter(index, verbose):\n    \"\"\" Returns False for indices increasingly apart, the distance\n        depending on the value of verbose.\n\n        We use a lag increasing as the square of index\n    \"\"\"\n    if not verbose:\n        return True\n    elif verbose > 10:\n        return False\n    if index == 0:\n        return False\n    verbose = .5 * (11 - verbose) ** 2\n    scale = sqrt(index / verbose)\n    next_scale = sqrt((index + 1) / verbose)\n    return (int(next_scale) == int(scale))\n\n\n###############################################################################\ndef delayed(function, check_pickle=True):\n    \"\"\"Decorator used to capture the arguments of a function.\n\n    Pass `check_pickle=False` when:\n\n    - performing a possibly repeated check is too costly and has been done\n      already once outside of the call to delayed.\n\n    - when used in conjunction `Parallel(backend='threading')`.\n\n    \"\"\"\n    # Try to pickle the input function, to catch the problems early when\n    # using with multiprocessing:\n    if check_pickle:\n        pickle.dumps(function)\n\n    def delayed_function(*args, **kwargs):\n        return function, args, kwargs\n    try:\n        delayed_function = functools.wraps(function)(delayed_function)\n    except AttributeError:\n        \" functools.wraps fails on some callable objects \"\n    return delayed_function\n\n\n###############################################################################\nclass BatchCompletionCallBack(object):\n    \"\"\"Callback used by joblib.Parallel's multiprocessing backend.\n\n    This callable is executed by the parent process whenever a worker process\n    has returned the results of a batch of tasks.\n\n    It is used for progress reporting, to update estimate of the batch\n    processing duration and to schedule the next batch of tasks to be\n    processed.\n\n    \"\"\"\n    def __init__(self, dispatch_timestamp, batch_size, parallel):\n        self.dispatch_timestamp = dispatch_timestamp\n        self.batch_size = batch_size\n        self.parallel = parallel\n\n    def __call__(self, out):\n        self.parallel.n_completed_tasks += self.batch_size\n        this_batch_duration = time.time() - self.dispatch_timestamp\n\n        self.parallel._backend.batch_completed(self.batch_size,\n                                               this_batch_duration)\n        self.parallel.print_progress()\n        if self.parallel._original_iterator is not None:\n            self.parallel.dispatch_next()\n\n\n###############################################################################\ndef register_parallel_backend(name, factory, make_default=False):\n    \"\"\"Register a new Parallel backend factory.\n\n    The new backend can then be selected by passing its name as the backend\n    argument to the Parallel class. Moreover, the default backend can be\n    overwritten globally by setting make_default=True.\n\n    The factory can be any callable that takes no argument and return an\n    instance of ``ParallelBackendBase``.\n\n    Warning: this function is experimental and subject to change in a future\n    version of joblib.\n\n    .. versionadded:: 0.10\n\n    \"\"\"\n    BACKENDS[name] = factory\n    if make_default:\n        global DEFAULT_BACKEND\n        DEFAULT_BACKEND = name\n\n\ndef effective_n_jobs(n_jobs=-1):\n    \"\"\"Determine the number of jobs that can actually run in parallel\n\n    n_jobs is the number of workers requested by the callers.\n    Passing n_jobs=-1 means requesting all available workers for instance\n    matching the number of CPU cores on the worker host(s).\n\n    This method should return a guesstimate of the number of workers that can\n    actually perform work concurrently with the currently enabled default\n    backend. The primary use case is to make it possible for the caller to know\n    in how many chunks to slice the work.\n\n    In general working on larger data chunks is more efficient (less\n    scheduling overhead and better use of CPU cache prefetching heuristics)\n    as long as all the workers have enough work to do.\n\n    Warning: this function is experimental and subject to change in a future\n    version of joblib.\n\n    .. versionadded:: 0.10\n\n    \"\"\"\n    backend, _ = get_active_backend()\n    return backend.effective_n_jobs(n_jobs=n_jobs)\n\n\n###############################################################################\nclass Parallel(Logger):\n    ''' Helper class for readable parallel mapping.\n\n        Parameters\n        -----------\n        n_jobs: int, default: 1\n            The maximum number of concurrently running jobs, such as the number\n            of Python worker processes when backend=\"multiprocessing\"\n            or the size of the thread-pool when backend=\"threading\".\n            If -1 all CPUs are used. If 1 is given, no parallel computing code\n            is used at all, which is useful for debugging. For n_jobs below -1,\n            (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all\n            CPUs but one are used.\n        backend: str, ParallelBackendBase instance or None, \\\n                default: 'multiprocessing'\n            Specify the parallelization backend implementation.\n            Supported backends are:\n\n            - \"multiprocessing\" used by default, can induce some\n              communication and memory overhead when exchanging input and\n              output data with the worker Python processes.\n            - \"threading\" is a very low-overhead backend but it suffers\n              from the Python Global Interpreter Lock if the called function\n              relies a lot on Python objects. \"threading\" is mostly useful\n              when the execution bottleneck is a compiled extension that\n              explicitly releases the GIL (for instance a Cython loop wrapped\n              in a \"with nogil\" block or an expensive call to a library such\n              as NumPy).\n            - finally, you can register backends by calling\n              register_parallel_backend. This will allow you to implement\n              a backend of your liking.\n        verbose: int, optional\n            The verbosity level: if non zero, progress messages are\n            printed. Above 50, the output is sent to stdout.\n            The frequency of the messages increases with the verbosity level.\n            If it more than 10, all iterations are reported.\n        timeout: float, optional\n            Timeout limit for each task to complete.  If any task takes longer\n            a TimeOutError will be raised. Only applied when n_jobs != 1\n        pre_dispatch: {'all', integer, or expression, as in '3*n_jobs'}\n            The number of batches (of tasks) to be pre-dispatched.\n            Default is '2*n_jobs'. When batch_size=\"auto\" this is reasonable\n            default and the multiprocessing workers should never starve.\n        batch_size: int or 'auto', default: 'auto'\n            The number of atomic tasks to dispatch at once to each\n            worker. When individual evaluations are very fast, multiprocessing\n            can be slower than sequential computation because of the overhead.\n            Batching fast computations together can mitigate this.\n            The ``'auto'`` strategy keeps track of the time it takes for a batch\n            to complete, and dynamically adjusts the batch size to keep the time\n            on the order of half a second, using a heuristic. The initial batch\n            size is 1.\n            ``batch_size=\"auto\"`` with ``backend=\"threading\"`` will dispatch\n            batches of a single task at a time as the threading backend has\n            very little overhead and using larger batch size has not proved to\n            bring any gain in that case.\n        temp_folder: str, optional\n            Folder to be used by the pool for memmaping large arrays\n            for sharing memory with worker processes. If None, this will try in\n            order:\n\n            - a folder pointed by the JOBLIB_TEMP_FOLDER environment\n              variable,\n            - /dev/shm if the folder exists and is writable: this is a\n              RAMdisk filesystem available by default on modern Linux\n              distributions,\n            - the default system temporary folder that can be\n              overridden with TMP, TMPDIR or TEMP environment\n              variables, typically /tmp under Unix operating systems.\n\n            Only active when backend=\"multiprocessing\".\n        max_nbytes int, str, or None, optional, 1M by default\n            Threshold on the size of arrays passed to the workers that\n            triggers automated memory mapping in temp_folder. Can be an int\n            in Bytes, or a human-readable string, e.g., '1M' for 1 megabyte.\n            Use None to disable memmaping of large arrays.\n            Only active when backend=\"multiprocessing\".\n        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}\n            Memmapping mode for numpy arrays passed to workers.\n            See 'max_nbytes' parameter documentation for more details.\n\n        Notes\n        -----\n\n        This object uses the multiprocessing module to compute in\n        parallel the application of a function to many different\n        arguments. The main functionality it brings in addition to\n        using the raw multiprocessing API are (see examples for details):\n\n        * More readable code, in particular since it avoids\n          constructing list of arguments.\n\n        * Easier debugging:\n            - informative tracebacks even when the error happens on\n              the client side\n            - using 'n_jobs=1' enables to turn off parallel computing\n              for debugging without changing the codepath\n            - early capture of pickling errors\n\n        * An optional progress meter.\n\n        * Interruption of multiprocesses jobs with 'Ctrl-C'\n\n        * Flexible pickling control for the communication to and from\n          the worker processes.\n\n        * Ability to use shared memory efficiently with worker\n          processes for large numpy-based datastructures.\n\n        Examples\n        --------\n\n        A simple example:\n\n        >>> from math import sqrt\n        >>> from sklearn.externals.joblib import Parallel, delayed\n        >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))\n        [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n\n        Reshaping the output when the function has several return\n        values:\n\n        >>> from math import modf\n        >>> from sklearn.externals.joblib import Parallel, delayed\n        >>> r = Parallel(n_jobs=1)(delayed(modf)(i/2.) for i in range(10))\n        >>> res, i = zip(*r)\n        >>> res\n        (0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5)\n        >>> i\n        (0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0)\n\n        The progress meter: the higher the value of `verbose`, the more\n        messages:\n\n        >>> from time import sleep\n        >>> from sklearn.externals.joblib import Parallel, delayed\n        >>> r = Parallel(n_jobs=2, verbose=5)(delayed(sleep)(.1) for _ in range(10)) #doctest: +SKIP\n        [Parallel(n_jobs=2)]: Done   1 out of  10 | elapsed:    0.1s remaining:    0.9s\n        [Parallel(n_jobs=2)]: Done   3 out of  10 | elapsed:    0.2s remaining:    0.5s\n        [Parallel(n_jobs=2)]: Done   6 out of  10 | elapsed:    0.3s remaining:    0.2s\n        [Parallel(n_jobs=2)]: Done   9 out of  10 | elapsed:    0.5s remaining:    0.1s\n        [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    0.5s finished\n\n        Traceback example, note how the line of the error is indicated\n        as well as the values of the parameter passed to the function that\n        triggered the exception, even though the traceback happens in the\n        child process:\n\n        >>> from heapq import nlargest\n        >>> from sklearn.externals.joblib import Parallel, delayed\n        >>> Parallel(n_jobs=2)(delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3)) #doctest: +SKIP\n        #...\n        ---------------------------------------------------------------------------\n        Sub-process traceback:\n        ---------------------------------------------------------------------------\n        TypeError                                          Mon Nov 12 11:37:46 2012\n        PID: 12934                                    Python 2.7.3: /usr/bin/python\n        ...........................................................................\n        /usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)\n            419         if n >= size:\n            420             return sorted(iterable, key=key, reverse=True)[:n]\n            421\n            422     # When key is none, use simpler decoration\n            423     if key is None:\n        --> 424         it = izip(iterable, count(0,-1))                    # decorate\n            425         result = _nlargest(n, it)\n            426         return map(itemgetter(0), result)                   # undecorate\n            427\n            428     # General case, slowest method\n         TypeError: izip argument #1 must support iteration\n        ___________________________________________________________________________\n\n\n        Using pre_dispatch in a producer/consumer situation, where the\n        data is generated on the fly. Note how the producer is first\n        called 3 times before the parallel loop is initiated, and then\n        called to generate new data on the fly. In this case the total\n        number of iterations cannot be reported in the progress messages:\n\n        >>> from math import sqrt\n        >>> from sklearn.externals.joblib import Parallel, delayed\n        >>> def producer():\n        ...     for i in range(6):\n        ...         print('Produced %s' % i)\n        ...         yield i\n        >>> out = Parallel(n_jobs=2, verbose=100, pre_dispatch='1.5*n_jobs')(\n        ...                delayed(sqrt)(i) for i in producer()) #doctest: +SKIP\n        Produced 0\n        Produced 1\n        Produced 2\n        [Parallel(n_jobs=2)]: Done 1 jobs     | elapsed:  0.0s\n        Produced 3\n        [Parallel(n_jobs=2)]: Done 2 jobs     | elapsed:  0.0s\n        Produced 4\n        [Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s\n        Produced 5\n        [Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s\n        [Parallel(n_jobs=2)]: Done 5 out of 6 | elapsed:  0.0s remaining: 0.0s\n        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished\n\n    '''\n    def __init__(self, n_jobs=1, backend=None, verbose=0, timeout=None,\n                 pre_dispatch='2 * n_jobs', batch_size='auto',\n                 temp_folder=None, max_nbytes='1M', mmap_mode='r'):\n        active_backend, default_n_jobs = get_active_backend()\n        if backend is None and n_jobs == 1:\n            # If we are under a parallel_backend context manager, look up\n            # the default number of jobs and use that instead:\n            n_jobs = default_n_jobs\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n        self.timeout = timeout\n        self.pre_dispatch = pre_dispatch\n\n        if isinstance(max_nbytes, _basestring):\n            max_nbytes = memstr_to_bytes(max_nbytes)\n\n        self._backend_args = dict(\n            max_nbytes=max_nbytes,\n            mmap_mode=mmap_mode,\n            temp_folder=temp_folder,\n            verbose=max(0, self.verbose - 50),\n        )\n        if DEFAULT_MP_CONTEXT is not None:\n            self._backend_args['context'] = DEFAULT_MP_CONTEXT\n\n        if backend is None:\n            backend = active_backend\n        elif isinstance(backend, ParallelBackendBase):\n            # Use provided backend as is\n            pass\n        elif hasattr(backend, 'Pool') and hasattr(backend, 'Lock'):\n            # Make it possible to pass a custom multiprocessing context as\n            # backend to change the start method to forkserver or spawn or\n            # preload modules on the forkserver helper process.\n            self._backend_args['context'] = backend\n            backend = MultiprocessingBackend()\n        else:\n            try:\n                backend_factory = BACKENDS[backend]\n            except KeyError:\n                raise ValueError(\"Invalid backend: %s, expected one of %r\"\n                                 % (backend, sorted(BACKENDS.keys())))\n            backend = backend_factory()\n\n        if (batch_size == 'auto' or isinstance(batch_size, Integral) and\n                batch_size > 0):\n            self.batch_size = batch_size\n        else:\n            raise ValueError(\n                \"batch_size must be 'auto' or a positive integer, got: %r\"\n                % batch_size)\n\n        self._backend = backend\n        self._output = None\n        self._jobs = list()\n        self._managed_backend = False\n\n        # This lock is used coordinate the main thread of this process with\n        # the async callback thread of our the pool.\n        self._lock = threading.Lock()\n\n    def __enter__(self):\n        self._managed_backend = True\n        self._initialize_backend()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._terminate_backend()\n        self._managed_backend = False\n\n    def _initialize_backend(self):\n        \"\"\"Build a process or thread pool and return the number of workers\"\"\"\n        try:\n            n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n                                             **self._backend_args)\n            if self.timeout is not None and not self._backend.supports_timeout:\n                warnings.warn(\n                    'The backend class {!r} does not support timeout. '\n                    \"You have set 'timeout={}' in Parallel but \"\n                    \"the 'timeout' parameter will not be used.\".format(\n                        self._backend.__class__.__name__,\n                        self.timeout))\n\n        except FallbackToBackend as e:\n            # Recursively initialize the backend in case of requested fallback.\n            self._backend = e.backend\n            n_jobs = self._initialize_backend()\n\n        return n_jobs\n\n    def _effective_n_jobs(self):\n        if self._backend:\n            return self._backend.effective_n_jobs(self.n_jobs)\n        return 1\n\n    def _terminate_backend(self):\n        if self._backend is not None:\n            self._backend.terminate()\n\n    def _dispatch(self, batch):\n        \"\"\"Queue the batch for computing, with or without multiprocessing\n\n        WARNING: this method is not thread-safe: it should be only called\n        indirectly via dispatch_one_batch.\n\n        \"\"\"\n        # If job.get() catches an exception, it closes the queue:\n        if self._aborting:\n            return\n\n        self.n_dispatched_tasks += len(batch)\n        self.n_dispatched_batches += 1\n\n        dispatch_timestamp = time.time()\n        cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)\n        job = self._backend.apply_async(batch, callback=cb)\n        self._jobs.append(job)\n\n    def dispatch_next(self):\n        \"\"\"Dispatch more data for parallel processing\n\n        This method is meant to be called concurrently by the multiprocessing\n        callback. We rely on the thread-safety of dispatch_one_batch to protect\n        against concurrent consumption of the unprotected iterator.\n\n        \"\"\"\n        if not self.dispatch_one_batch(self._original_iterator):\n            self._iterating = False\n            self._original_iterator = None\n\n    def dispatch_one_batch(self, iterator):\n        \"\"\"Prefetch the tasks for the next batch and dispatch them.\n\n        The effective size of the batch is computed here.\n        If there are no more jobs to dispatch, return False, else return True.\n\n        The iterator consumption and dispatching is protected by the same\n        lock so calling this function should be thread safe.\n\n        \"\"\"\n        if self.batch_size == 'auto':\n            batch_size = self._backend.compute_batch_size()\n        else:\n            # Fixed batch size strategy\n            batch_size = self.batch_size\n\n        with self._lock:\n            tasks = BatchedCalls(itertools.islice(iterator, batch_size))\n            if len(tasks) == 0:\n                # No more tasks available in the iterator: tell caller to stop.\n                return False\n            else:\n                self._dispatch(tasks)\n                return True\n\n    def _print(self, msg, msg_args):\n        \"\"\"Display the message on stout or stderr depending on verbosity\"\"\"\n        # XXX: Not using the logger framework: need to\n        # learn to use logger better.\n        if not self.verbose:\n            return\n        if self.verbose < 50:\n            writer = sys.stderr.write\n        else:\n            writer = sys.stdout.write\n        msg = msg % msg_args\n        writer('[%s]: %s\\n' % (self, msg))\n\n    def print_progress(self):\n        \"\"\"Display the process of the parallel execution only a fraction\n           of time, controlled by self.verbose.\n        \"\"\"\n        if not self.verbose:\n            return\n        elapsed_time = time.time() - self._start_time\n\n        # Original job iterator becomes None once it has been fully\n        # consumed : at this point we know the total number of jobs and we are\n        # able to display an estimation of the remaining time based on already\n        # completed jobs. Otherwise, we simply display the number of completed\n        # tasks.\n        if self._original_iterator is not None:\n            if _verbosity_filter(self.n_dispatched_batches, self.verbose):\n                return\n            self._print('Done %3i tasks      | elapsed: %s',\n                        (self.n_completed_tasks,\n                         short_format_time(elapsed_time), ))\n        else:\n            index = self.n_completed_tasks\n            # We are finished dispatching\n            total_tasks = self.n_dispatched_tasks\n            # We always display the first loop\n            if not index == 0:\n                # Display depending on the number of remaining items\n                # A message as soon as we finish dispatching, cursor is 0\n                cursor = (total_tasks - index + 1 -\n                          self._pre_dispatch_amount)\n                frequency = (total_tasks // self.verbose) + 1\n                is_last_item = (index + 1 == total_tasks)\n                if (is_last_item or cursor % frequency):\n                    return\n            remaining_time = (elapsed_time / index) * \\\n                             (self.n_dispatched_tasks - index * 1.0)\n            # only display status if remaining time is greater or equal to 0\n            self._print('Done %3i out of %3i | elapsed: %s remaining: %s',\n                        (index,\n                         total_tasks,\n                         short_format_time(elapsed_time),\n                         short_format_time(remaining_time),\n                         ))\n\n    def retrieve(self):\n        self._output = list()\n        while self._iterating or len(self._jobs) > 0:\n            if len(self._jobs) == 0:\n                # Wait for an async callback to dispatch new jobs\n                time.sleep(0.01)\n                continue\n            # We need to be careful: the job list can be filling up as\n            # we empty it and Python list are not thread-safe by default hence\n            # the use of the lock\n            with self._lock:\n                job = self._jobs.pop(0)\n\n            try:\n                if getattr(self._backend, 'supports_timeout', False):\n                    self._output.extend(job.get(timeout=self.timeout))\n                else:\n                    self._output.extend(job.get())\n\n            except BaseException as exception:\n                # Note: we catch any BaseException instead of just Exception\n                # instances to also include KeyboardInterrupt.\n\n                # Stop dispatching any new job in the async callback thread\n                self._aborting = True\n\n                # If the backend allows it, cancel or kill remaining running\n                # tasks without waiting for the results as we will raise\n                # the exception we got back to the caller instead of returning\n                # any result.\n                backend = self._backend\n                if (backend is not None and\n                        hasattr(backend, 'abort_everything')):\n                    # If the backend is managed externally we need to make sure\n                    # to leave it in a working state to allow for future jobs\n                    # scheduling.\n                    ensure_ready = self._managed_backend\n                    backend.abort_everything(ensure_ready=ensure_ready)\n\n                if not isinstance(exception, TransportableException):\n                    raise\n                else:\n                    # Capture exception to add information on the local\n                    # stack in addition to the distant stack\n                    this_report = format_outer_frames(context=10,\n                                                      stack_start=1)\n                    report = \"\"\"Multiprocessing exception:\n%s\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\n%s\"\"\" % (this_report, exception.message)\n                    # Convert this to a JoblibException\n                    exception_type = _mk_exception(exception.etype)[0]\n                    exception = exception_type(report)\n\n                    raise exception\n\n    def __call__(self, iterable):\n        if self._jobs:\n            raise ValueError('This Parallel instance is already running')\n        # A flag used to abort the dispatching of jobs in case an\n        # exception is found\n        self._aborting = False\n        if not self._managed_backend:\n            n_jobs = self._initialize_backend()\n        else:\n            n_jobs = self._effective_n_jobs()\n\n        iterator = iter(iterable)\n        pre_dispatch = self.pre_dispatch\n\n        if pre_dispatch == 'all' or n_jobs == 1:\n            # prevent further dispatch via multiprocessing callback thread\n            self._original_iterator = None\n            self._pre_dispatch_amount = 0\n        else:\n            self._original_iterator = iterator\n            if hasattr(pre_dispatch, 'endswith'):\n                pre_dispatch = eval(pre_dispatch)\n            self._pre_dispatch_amount = pre_dispatch = int(pre_dispatch)\n\n            # The main thread will consume the first pre_dispatch items and\n            # the remaining items will later be lazily dispatched by async\n            # callbacks upon task completions.\n            iterator = itertools.islice(iterator, pre_dispatch)\n\n        self._start_time = time.time()\n        self.n_dispatched_batches = 0\n        self.n_dispatched_tasks = 0\n        self.n_completed_tasks = 0\n        try:\n            # Only set self._iterating to True if at least a batch\n            # was dispatched. In particular this covers the edge\n            # case of Parallel used with an exhausted iterator.\n            while self.dispatch_one_batch(iterator):\n                self._iterating = True\n            else:\n                self._iterating = False\n\n            if pre_dispatch == \"all\" or n_jobs == 1:\n                # The iterable was consumed all at once by the above for loop.\n                # No need to wait for async callbacks to trigger to\n                # consumption.\n                self._iterating = False\n            self.retrieve()\n            # Make sure that we get a last message telling us we are done\n            elapsed_time = time.time() - self._start_time\n            self._print('Done %3i out of %3i | elapsed: %s finished',\n                        (len(self._output), len(self._output),\n                         short_format_time(elapsed_time)))\n        finally:\n            if not self._managed_backend:\n                self._terminate_backend()\n            self._jobs = list()\n        output = self._output\n        self._output = None\n        return output\n\n    def __repr__(self):\n        return '%s(n_jobs=%s)' % (self.__class__.__name__, self.n_jobs)\n"
    },
    {
      "filename": "sklearn/externals/_joblib/pool.py",
      "content": "\"\"\"Custom implementation of multiprocessing.Pool with custom pickler.\n\nThis module provides efficient ways of working with data stored in\nshared memory with numpy.memmap arrays without inducing any memory\ncopy between the parent and child processes.\n\nThis module should not be imported if multiprocessing is not\navailable as it implements subclasses of multiprocessing Pool\nthat uses a custom alternative to SimpleQueue.\n\n\"\"\"\n# Author: Olivier Grisel <olivier.grisel@ensta.org>\n# Copyright: 2012, Olivier Grisel\n# License: BSD 3 clause\n\nfrom mmap import mmap\nimport errno\nimport os\nimport stat\nimport sys\nimport threading\nimport atexit\nimport tempfile\nimport shutil\nimport warnings\nfrom time import sleep\n\ntry:\n    WindowsError\nexcept NameError:\n    WindowsError = type(None)\n\nfrom pickle import whichmodule\ntry:\n    # Python 2 compat\n    from cPickle import loads\n    from cPickle import dumps\nexcept ImportError:\n    from pickle import loads\n    from pickle import dumps\n    import copyreg\n\n# Customizable pure Python pickler in Python 2\n# customizable C-optimized pickler under Python 3.3+\nfrom pickle import Pickler\n\nfrom pickle import HIGHEST_PROTOCOL\nfrom io import BytesIO\n\nfrom ._multiprocessing_helpers import mp, assert_spawning\n# We need the class definition to derive from it not the multiprocessing.Pool\n# factory function\nfrom multiprocessing.pool import Pool\n\ntry:\n    import numpy as np\n    from numpy.lib.stride_tricks import as_strided\nexcept ImportError:\n    np = None\n\nfrom .numpy_pickle import load\nfrom .numpy_pickle import dump\nfrom .hashing import hash\nfrom .backports import make_memmap\n# Some system have a ramdisk mounted by default, we can use it instead of /tmp\n# as the default folder to dump big arrays to share with subprocesses\nSYSTEM_SHARED_MEM_FS = '/dev/shm'\n\n# Folder and file permissions to chmod temporary files generated by the\n# memmaping pool. Only the owner of the Python process can access the\n# temporary files and folder.\nFOLDER_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR\nFILE_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR\n\n###############################################################################\n# Support for efficient transient pickling of numpy data structures\n\n\ndef _get_backing_memmap(a):\n    \"\"\"Recursively look up the original np.memmap instance base if any.\"\"\"\n    b = getattr(a, 'base', None)\n    if b is None:\n        # TODO: check scipy sparse datastructure if scipy is installed\n        # a nor its descendants do not have a memmap base\n        return None\n\n    elif isinstance(b, mmap):\n        # a is already a real memmap instance.\n        return a\n\n    else:\n        # Recursive exploration of the base ancestry\n        return _get_backing_memmap(b)\n\n\ndef has_shareable_memory(a):\n    \"\"\"Return True if a is backed by some mmap buffer directly or not.\"\"\"\n    return _get_backing_memmap(a) is not None\n\n\ndef _strided_from_memmap(filename, dtype, mode, offset, order, shape, strides,\n                         total_buffer_len):\n    \"\"\"Reconstruct an array view on a memory mapped file.\"\"\"\n    if mode == 'w+':\n        # Do not zero the original data when unpickling\n        mode = 'r+'\n\n    if strides is None:\n        # Simple, contiguous memmap\n        return make_memmap(filename, dtype=dtype, shape=shape, mode=mode,\n                           offset=offset, order=order)\n    else:\n        # For non-contiguous data, memmap the total enclosing buffer and then\n        # extract the non-contiguous view with the stride-tricks API\n        base = make_memmap(filename, dtype=dtype, shape=total_buffer_len,\n                           mode=mode, offset=offset, order=order)\n        return as_strided(base, shape=shape, strides=strides)\n\n\ndef _reduce_memmap_backed(a, m):\n    \"\"\"Pickling reduction for memmap backed arrays.\n\n    a is expected to be an instance of np.ndarray (or np.memmap)\n    m is expected to be an instance of np.memmap on the top of the ``base``\n    attribute ancestry of a. ``m.base`` should be the real python mmap object.\n    \"\"\"\n    # offset that comes from the striding differences between a and m\n    a_start, a_end = np.byte_bounds(a)\n    m_start = np.byte_bounds(m)[0]\n    offset = a_start - m_start\n\n    # offset from the backing memmap\n    offset += m.offset\n\n    if m.flags['F_CONTIGUOUS']:\n        order = 'F'\n    else:\n        # The backing memmap buffer is necessarily contiguous hence C if not\n        # Fortran\n        order = 'C'\n\n    if a.flags['F_CONTIGUOUS'] or a.flags['C_CONTIGUOUS']:\n        # If the array is a contiguous view, no need to pass the strides\n        strides = None\n        total_buffer_len = None\n    else:\n        # Compute the total number of items to map from which the strided\n        # view will be extracted.\n        strides = a.strides\n        total_buffer_len = (a_end - a_start) // a.itemsize\n    return (_strided_from_memmap,\n            (m.filename, a.dtype, m.mode, offset, order, a.shape, strides,\n             total_buffer_len))\n\n\ndef reduce_memmap(a):\n    \"\"\"Pickle the descriptors of a memmap instance to reopen on same file.\"\"\"\n    m = _get_backing_memmap(a)\n    if m is not None:\n        # m is a real mmap backed memmap instance, reduce a preserving striding\n        # information\n        return _reduce_memmap_backed(a, m)\n    else:\n        # This memmap instance is actually backed by a regular in-memory\n        # buffer: this can happen when using binary operators on numpy.memmap\n        # instances\n        return (loads, (dumps(np.asarray(a), protocol=HIGHEST_PROTOCOL),))\n\n\nclass ArrayMemmapReducer(object):\n    \"\"\"Reducer callable to dump large arrays to memmap files.\n\n    Parameters\n    ----------\n    max_nbytes: int\n        Threshold to trigger memmaping of large arrays to files created\n        a folder.\n    temp_folder: str\n        Path of a folder where files for backing memmaped arrays are created.\n    mmap_mode: 'r', 'r+' or 'c'\n        Mode for the created memmap datastructure. See the documentation of\n        numpy.memmap for more details. Note: 'w+' is coerced to 'r+'\n        automatically to avoid zeroing the data on unpickling.\n    verbose: int, optional, 0 by default\n        If verbose > 0, memmap creations are logged.\n        If verbose > 1, both memmap creations, reuse and array pickling are\n        logged.\n    prewarm: bool, optional, False by default.\n        Force a read on newly memmaped array to make sure that OS pre-cache it\n        memory. This can be useful to avoid concurrent disk access when the\n        same data array is passed to different worker processes.\n    \"\"\"\n\n    def __init__(self, max_nbytes, temp_folder, mmap_mode, verbose=0,\n                 context_id=None, prewarm=True):\n        self._max_nbytes = max_nbytes\n        self._temp_folder = temp_folder\n        self._mmap_mode = mmap_mode\n        self.verbose = int(verbose)\n        self._prewarm = prewarm\n        if context_id is not None:\n            warnings.warn('context_id is deprecated and ignored in joblib'\n                          ' 0.9.4 and will be removed in 0.11',\n                          DeprecationWarning)\n\n    def __call__(self, a):\n        m = _get_backing_memmap(a)\n        if m is not None:\n            # a is already backed by a memmap file, let's reuse it directly\n            return _reduce_memmap_backed(a, m)\n\n        if (not a.dtype.hasobject\n                and self._max_nbytes is not None\n                and a.nbytes > self._max_nbytes):\n            # check that the folder exists (lazily create the pool temp folder\n            # if required)\n            try:\n                os.makedirs(self._temp_folder)\n                os.chmod(self._temp_folder, FOLDER_PERMISSIONS)\n            except OSError as e:\n                if e.errno != errno.EEXIST:\n                    raise e\n\n            # Find a unique, concurrent safe filename for writing the\n            # content of this array only once.\n            basename = \"%d-%d-%s.pkl\" % (\n                os.getpid(), id(threading.current_thread()), hash(a))\n            filename = os.path.join(self._temp_folder, basename)\n\n            # In case the same array with the same content is passed several\n            # times to the pool subprocess children, serialize it only once\n\n            # XXX: implement an explicit reference counting scheme to make it\n            # possible to delete temporary files as soon as the workers are\n            # done processing this data.\n            if not os.path.exists(filename):\n                if self.verbose > 0:\n                    print(\"Memmaping (shape=%r, dtype=%s) to new file %s\" % (\n                        a.shape, a.dtype, filename))\n                for dumped_filename in dump(a, filename):\n                    os.chmod(dumped_filename, FILE_PERMISSIONS)\n\n                if self._prewarm:\n                    # Warm up the data to avoid concurrent disk access in\n                    # multiple children processes\n                    load(filename, mmap_mode=self._mmap_mode).max()\n            elif self.verbose > 1:\n                print(\"Memmaping (shape=%s, dtype=%s) to old file %s\" % (\n                    a.shape, a.dtype, filename))\n\n            # The worker process will use joblib.load to memmap the data\n            return (load, (filename, self._mmap_mode))\n        else:\n            # do not convert a into memmap, let pickler do its usual copy with\n            # the default system pickler\n            if self.verbose > 1:\n                print(\"Pickling array (shape=%r, dtype=%s).\" % (\n                    a.shape, a.dtype))\n            return (loads, (dumps(a, protocol=HIGHEST_PROTOCOL),))\n\n\n###############################################################################\n# Enable custom pickling in Pool queues\n\nclass CustomizablePickler(Pickler):\n    \"\"\"Pickler that accepts custom reducers.\n\n    HIGHEST_PROTOCOL is selected by default as this pickler is used\n    to pickle ephemeral datastructures for interprocess communication\n    hence no backward compatibility is required.\n\n    `reducers` is expected to be a dictionary with key/values\n    being `(type, callable)` pairs where `callable` is a function that\n    give an instance of `type` will return a tuple `(constructor,\n    tuple_of_objects)` to rebuild an instance out of the pickled\n    `tuple_of_objects` as would return a `__reduce__` method. See the\n    standard library documentation on pickling for more details.\n\n    \"\"\"\n\n    # We override the pure Python pickler as its the only way to be able to\n    # customize the dispatch table without side effects in Python 2.7\n    # to 3.2. For Python 3.3+ leverage the new dispatch_table\n    # feature from http://bugs.python.org/issue14166 that makes it possible\n    # to use the C implementation of the Pickler which is faster.\n\n    def __init__(self, writer, reducers=None, protocol=HIGHEST_PROTOCOL):\n        Pickler.__init__(self, writer, protocol=protocol)\n        if reducers is None:\n            reducers = {}\n        if hasattr(Pickler, 'dispatch'):\n            # Make the dispatch registry an instance level attribute instead of\n            # a reference to the class dictionary under Python 2\n            self.dispatch = Pickler.dispatch.copy()\n        else:\n            # Under Python 3 initialize the dispatch table with a copy of the\n            # default registry\n            self.dispatch_table = copyreg.dispatch_table.copy()\n        for type, reduce_func in reducers.items():\n            self.register(type, reduce_func)\n\n    def register(self, type, reduce_func):\n        \"\"\"Attach a reducer function to a given type in the dispatch table.\"\"\"\n        if hasattr(Pickler, 'dispatch'):\n            # Python 2 pickler dispatching is not explicitly customizable.\n            # Let us use a closure to workaround this limitation.\n            def dispatcher(self, obj):\n                reduced = reduce_func(obj)\n                self.save_reduce(obj=obj, *reduced)\n            self.dispatch[type] = dispatcher\n        else:\n            self.dispatch_table[type] = reduce_func\n\n\nclass CustomizablePicklingQueue(object):\n    \"\"\"Locked Pipe implementation that uses a customizable pickler.\n\n    This class is an alternative to the multiprocessing implementation\n    of SimpleQueue in order to make it possible to pass custom\n    pickling reducers, for instance to avoid memory copy when passing\n    memory mapped datastructures.\n\n    `reducers` is expected to be a dict with key / values being\n    `(type, callable)` pairs where `callable` is a function that, given an\n    instance of `type`, will return a tuple `(constructor, tuple_of_objects)`\n    to rebuild an instance out of the pickled `tuple_of_objects` as would\n    return a `__reduce__` method.\n\n    See the standard library documentation on pickling for more details.\n    \"\"\"\n\n    def __init__(self, context, reducers=None):\n        self._reducers = reducers\n        self._reader, self._writer = context.Pipe(duplex=False)\n        self._rlock = context.Lock()\n        if sys.platform == 'win32':\n            self._wlock = None\n        else:\n            self._wlock = context.Lock()\n        self._make_methods()\n\n    def __getstate__(self):\n        assert_spawning(self)\n        return (self._reader, self._writer, self._rlock, self._wlock,\n                self._reducers)\n\n    def __setstate__(self, state):\n        (self._reader, self._writer, self._rlock, self._wlock,\n         self._reducers) = state\n        self._make_methods()\n\n    def empty(self):\n        return not self._reader.poll()\n\n    def _make_methods(self):\n        self._recv = recv = self._reader.recv\n        racquire, rrelease = self._rlock.acquire, self._rlock.release\n\n        def get():\n            racquire()\n            try:\n                return recv()\n            finally:\n                rrelease()\n\n        self.get = get\n\n        if self._reducers:\n            def send(obj):\n                buffer = BytesIO()\n                CustomizablePickler(buffer, self._reducers).dump(obj)\n                self._writer.send_bytes(buffer.getvalue())\n            self._send = send\n        else:\n            self._send = send = self._writer.send\n        if self._wlock is None:\n            # writes to a message oriented win32 pipe are atomic\n            self.put = send\n        else:\n            wlock_acquire, wlock_release = (\n                self._wlock.acquire, self._wlock.release)\n\n            def put(obj):\n                wlock_acquire()\n                try:\n                    return send(obj)\n                finally:\n                    wlock_release()\n\n            self.put = put\n\n\nclass PicklingPool(Pool):\n    \"\"\"Pool implementation with customizable pickling reducers.\n\n    This is useful to control how data is shipped between processes\n    and makes it possible to use shared memory without useless\n    copies induces by the default pickling methods of the original\n    objects passed as arguments to dispatch.\n\n    `forward_reducers` and `backward_reducers` are expected to be\n    dictionaries with key/values being `(type, callable)` pairs where\n    `callable` is a function that, given an instance of `type`, will return a\n    tuple `(constructor, tuple_of_objects)` to rebuild an instance out of the\n    pickled `tuple_of_objects` as would return a `__reduce__` method.\n    See the standard library documentation about pickling for more details.\n\n    \"\"\"\n\n    def __init__(self, processes=None, forward_reducers=None,\n                 backward_reducers=None, **kwargs):\n        if forward_reducers is None:\n            forward_reducers = dict()\n        if backward_reducers is None:\n            backward_reducers = dict()\n        self._forward_reducers = forward_reducers\n        self._backward_reducers = backward_reducers\n        poolargs = dict(processes=processes)\n        poolargs.update(kwargs)\n        super(PicklingPool, self).__init__(**poolargs)\n\n    def _setup_queues(self):\n        context = getattr(self, '_ctx', mp)\n        self._inqueue = CustomizablePicklingQueue(context,\n                                                  self._forward_reducers)\n        self._outqueue = CustomizablePicklingQueue(context,\n                                                   self._backward_reducers)\n        self._quick_put = self._inqueue._send\n        self._quick_get = self._outqueue._recv\n\n\ndef delete_folder(folder_path):\n    \"\"\"Utility function to cleanup a temporary folder if still existing.\"\"\"\n    try:\n        if os.path.exists(folder_path):\n            shutil.rmtree(folder_path)\n    except WindowsError:\n        warnings.warn(\"Failed to clean temporary folder: %s\" % folder_path)\n\n\nclass MemmapingPool(PicklingPool):\n    \"\"\"Process pool that shares large arrays to avoid memory copy.\n\n    This drop-in replacement for `multiprocessing.pool.Pool` makes\n    it possible to work efficiently with shared memory in a numpy\n    context.\n\n    Existing instances of numpy.memmap are preserved: the child\n    suprocesses will have access to the same shared memory in the\n    original mode except for the 'w+' mode that is automatically\n    transformed as 'r+' to avoid zeroing the original data upon\n    instantiation.\n\n    Furthermore large arrays from the parent process are automatically\n    dumped to a temporary folder on the filesystem such as child\n    processes to access their content via memmaping (file system\n    backed shared memory).\n\n    Note: it is important to call the terminate method to collect\n    the temporary folder used by the pool.\n\n    Parameters\n    ----------\n    processes: int, optional\n        Number of worker processes running concurrently in the pool.\n    initializer: callable, optional\n        Callable executed on worker process creation.\n    initargs: tuple, optional\n        Arguments passed to the initializer callable.\n    temp_folder: str, optional\n        Folder to be used by the pool for memmaping large arrays\n        for sharing memory with worker processes. If None, this will try in\n        order:\n        - a folder pointed by the JOBLIB_TEMP_FOLDER environment variable,\n        - /dev/shm if the folder exists and is writable: this is a RAMdisk\n          filesystem available by default on modern Linux distributions,\n        - the default system temporary folder that can be overridden\n          with TMP, TMPDIR or TEMP environment variables, typically /tmp\n          under Unix operating systems.\n    max_nbytes int or None, optional, 1e6 by default\n        Threshold on the size of arrays passed to the workers that\n        triggers automated memory mapping in temp_folder.\n        Use None to disable memmaping of large arrays.\n    mmap_mode: {'r+', 'r', 'w+', 'c'}\n        Memmapping mode for numpy arrays passed to workers.\n        See 'max_nbytes' parameter documentation for more details.\n    forward_reducers: dictionary, optional\n        Reducers used to pickle objects passed from master to worker\n        processes: see below.\n    backward_reducers: dictionary, optional\n        Reducers used to pickle return values from workers back to the\n        master process.\n    verbose: int, optional\n        Make it possible to monitor how the communication of numpy arrays\n        with the subprocess is handled (pickling or memmaping)\n    prewarm: bool or str, optional, \"auto\" by default.\n        If True, force a read on newly memmaped array to make sure that OS pre-\n        cache it in memory. This can be useful to avoid concurrent disk access\n        when the same data array is passed to different worker processes.\n        If \"auto\" (by default), prewarm is set to True, unless the Linux shared\n        memory partition /dev/shm is available and used as temp_folder.\n\n    `forward_reducers` and `backward_reducers` are expected to be\n    dictionaries with key/values being `(type, callable)` pairs where\n    `callable` is a function that give an instance of `type` will return\n    a tuple `(constructor, tuple_of_objects)` to rebuild an instance out\n    of the pickled `tuple_of_objects` as would return a `__reduce__`\n    method. See the standard library documentation on pickling for more\n    details.\n\n    \"\"\"\n\n    def __init__(self, processes=None, temp_folder=None, max_nbytes=1e6,\n                 mmap_mode='r', forward_reducers=None, backward_reducers=None,\n                 verbose=0, context_id=None, prewarm=False, **kwargs):\n        if forward_reducers is None:\n            forward_reducers = dict()\n        if backward_reducers is None:\n            backward_reducers = dict()\n        if context_id is not None:\n            warnings.warn('context_id is deprecated and ignored in joblib'\n                          ' 0.9.4 and will be removed in 0.11',\n                          DeprecationWarning)\n\n        # Prepare a sub-folder name for the serialization of this particular\n        # pool instance (do not create in advance to spare FS write access if\n        # no array is to be dumped):\n        use_shared_mem = False\n        pool_folder_name = \"joblib_memmaping_pool_%d_%d\" % (\n            os.getpid(), id(self))\n        if temp_folder is None:\n            temp_folder = os.environ.get('JOBLIB_TEMP_FOLDER', None)\n        if temp_folder is None:\n            if os.path.exists(SYSTEM_SHARED_MEM_FS):\n                try:\n                    temp_folder = SYSTEM_SHARED_MEM_FS\n                    pool_folder = os.path.join(temp_folder, pool_folder_name)\n                    if not os.path.exists(pool_folder):\n                        os.makedirs(pool_folder)\n                    use_shared_mem = True\n                except IOError:\n                    # Missing rights in the /dev/shm partition,\n                    # fallback to regular temp folder.\n                    temp_folder = None\n        if temp_folder is None:\n            # Fallback to the default tmp folder, typically /tmp\n            temp_folder = tempfile.gettempdir()\n        temp_folder = os.path.abspath(os.path.expanduser(temp_folder))\n        pool_folder = os.path.join(temp_folder, pool_folder_name)\n        self._temp_folder = pool_folder\n\n        # Register the garbage collector at program exit in case caller forgets\n        # to call terminate explicitly: note we do not pass any reference to\n        # self to ensure that this callback won't prevent garbage collection of\n        # the pool instance and related file handler resources such as POSIX\n        # semaphores and pipes\n        pool_module_name = whichmodule(delete_folder, 'delete_folder')\n\n        def _cleanup():\n            # In some cases the Python runtime seems to set delete_folder to\n            # None just before exiting when accessing the delete_folder\n            # function from the closure namespace. So instead we reimport\n            # the delete_folder function explicitly.\n            # https://github.com/joblib/joblib/issues/328\n            # We cannot just use from 'joblib.pool import delete_folder'\n            # because joblib should only use relative imports to allow\n            # easy vendoring.\n            delete_folder = __import__(\n                pool_module_name, fromlist=['delete_folder']).delete_folder\n            delete_folder(pool_folder)\n\n        atexit.register(_cleanup)\n\n        if np is not None:\n            # Register smart numpy.ndarray reducers that detects memmap backed\n            # arrays and that is else able to dump to memmap large in-memory\n            # arrays over the max_nbytes threshold\n            if prewarm == \"auto\":\n                prewarm = not use_shared_mem\n            forward_reduce_ndarray = ArrayMemmapReducer(\n                max_nbytes, pool_folder, mmap_mode, verbose,\n                prewarm=prewarm)\n            forward_reducers[np.ndarray] = forward_reduce_ndarray\n            forward_reducers[np.memmap] = reduce_memmap\n\n            # Communication from child process to the parent process always\n            # pickles in-memory numpy.ndarray without dumping them as memmap\n            # to avoid confusing the caller and make it tricky to collect the\n            # temporary folder\n            backward_reduce_ndarray = ArrayMemmapReducer(\n                None, pool_folder, mmap_mode, verbose)\n            backward_reducers[np.ndarray] = backward_reduce_ndarray\n            backward_reducers[np.memmap] = reduce_memmap\n\n        poolargs = dict(\n            processes=processes,\n            forward_reducers=forward_reducers,\n            backward_reducers=backward_reducers)\n        poolargs.update(kwargs)\n        super(MemmapingPool, self).__init__(**poolargs)\n\n    def terminate(self):\n        n_retries = 10\n        for i in range(n_retries):\n            try:\n                super(MemmapingPool, self).terminate()\n                break\n            except OSError as e:\n                if isinstance(e, WindowsError):\n                    # Workaround  occasional \"[Error 5] Access is denied\" issue\n                    # when trying to terminate a process under windows.\n                    sleep(0.1)\n                    if i + 1 == n_retries:\n                        warnings.warn(\"Failed to terminate worker processes in\"\n                                      \" multiprocessing pool: %r\" % e)\n        delete_folder(self._temp_folder)\n"
    },
    {
      "filename": "sklearn/externals/copy_joblib.sh",
      "content": "#!/bin/sh\n# Script to do a local install of joblib\nset +x\nexport LC_ALL=C\nINSTALL_FOLDER=tmp/joblib_install\nrm -rf joblib $INSTALL_FOLDER\nif [ -z \"$1\" ]\nthen\n        JOBLIB=joblib\nelse\n        JOBLIB=$1\nfi\n\npip install $JOBLIB --target $INSTALL_FOLDER\ncp -r $INSTALL_FOLDER/joblib _joblib\nrm -rf $INSTALL_FOLDER\n\n# Needed to rewrite the doctests\n# Note: BSD sed -i needs an argument unders OSX\n# so first renaming to .bak and then deleting backup files\nfind _joblib -name \"*.py\" | xargs sed -i.bak \"s/from joblib/from sklearn.externals.joblib/\"\nfind _joblib -name \"*.bak\" | xargs rm\n\n# Remove the tests folders to speed-up test time for scikit-learn.\n# joblib is already tested on its own CI infrastructure upstream.\nrm -r joblib/test\n"
    },
    {
      "filename": "sklearn/externals/joblib.py",
      "content": "# We need the absolute_import to avoid the local joblib to override the\n# site one\nfrom __future__ import absolute_import\nimport os as _os\n\n# An environment variable to use the site joblib\nif _os.environ.get('SKLEARN_SITE_JOBLIB', False):\n    from joblib import *\n    from joblib import __version__\n    from joblib import logger\nelse:\n    from ._joblib import *\n    from ._joblib import __version__\n    from ._joblib import logger\n\n"
    },
    {
      "filename": "sklearn/externals/setup.py",
      "content": "# -*- coding: utf-8 -*-\n\n\ndef configuration(parent_package='', top_path=None):\n    from numpy.distutils.misc_util import Configuration\n    config = Configuration('externals', parent_package, top_path)\n    config.add_subpackage('_joblib')\n\n    return config\n"
    },
    {
      "filename": "sklearn/model_selection/tests/test_search.py",
      "content": "\"\"\"Test the search module\"\"\"\n\nfrom collections import Iterable, Sized\nfrom sklearn.externals.six.moves import cStringIO as StringIO\nfrom sklearn.externals.six.moves import xrange\nfrom itertools import chain, product\nimport pickle\nimport sys\nfrom types import GeneratorType\nimport re\n\nimport numpy as np\nimport scipy.sparse as sp\nimport pytest\n\nfrom sklearn.utils.fixes import sp_version\nfrom sklearn.utils.fixes import PY3_OR_LATER\nfrom sklearn.utils.testing import assert_equal\nfrom sklearn.utils.testing import assert_not_equal\nfrom sklearn.utils.testing import assert_raises\nfrom sklearn.utils.testing import assert_warns\nfrom sklearn.utils.testing import assert_warns_message\nfrom sklearn.utils.testing import assert_no_warnings\nfrom sklearn.utils.testing import assert_raise_message\nfrom sklearn.utils.testing import assert_false, assert_true\nfrom sklearn.utils.testing import assert_array_equal\nfrom sklearn.utils.testing import assert_array_almost_equal\nfrom sklearn.utils.testing import assert_almost_equal\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.utils.mocking import CheckingClassifier, MockDataFrame\n\nfrom scipy.stats import bernoulli, expon, uniform\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import clone\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_multilabel_classification\n\nfrom sklearn.model_selection import fit_grid_point\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom sklearn.model_selection import LeavePGroupsOut\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.model_selection import ParameterSampler\n\nfrom sklearn.model_selection._validation import FitFailedWarning\n\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge, SGDClassifier\n\nfrom sklearn.model_selection.tests.common import OneTimeSplitter\n\n\n# Neither of the following two estimators inherit from BaseEstimator,\n# to test hyperparameter search on user-defined classifiers.\nclass MockClassifier(object):\n    \"\"\"Dummy classifier to test the parameter search algorithms\"\"\"\n    def __init__(self, foo_param=0):\n        self.foo_param = foo_param\n\n    def fit(self, X, Y):\n        assert_true(len(X) == len(Y))\n        self.classes_ = np.unique(Y)\n        return self\n\n    def predict(self, T):\n        return T.shape[0]\n\n    def transform(self, X):\n        return X + self.foo_param\n\n    def inverse_transform(self, X):\n        return X - self.foo_param\n\n    predict_proba = predict\n    predict_log_proba = predict\n    decision_function = predict\n\n    def score(self, X=None, Y=None):\n        if self.foo_param > 1:\n            score = 1.\n        else:\n            score = 0.\n        return score\n\n    def get_params(self, deep=False):\n        return {'foo_param': self.foo_param}\n\n    def set_params(self, **params):\n        self.foo_param = params['foo_param']\n        return self\n\n\nclass LinearSVCNoScore(LinearSVC):\n    \"\"\"An LinearSVC classifier that has no score method.\"\"\"\n    @property\n    def score(self):\n        raise AttributeError\n\n\nX = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\ny = np.array([1, 1, 2, 2])\n\n\ndef assert_grid_iter_equals_getitem(grid):\n    assert_equal(list(grid), [grid[i] for i in range(len(grid))])\n\n\n@pytest.mark.parametrize(\n    \"input, error_type, error_message\",\n    [(0, TypeError, 'Parameter grid is not a dict or a list (0)'),\n     ([{'foo': [0]}, 0], TypeError, 'Parameter grid is not a dict (0)'),\n     ({'foo': 0}, TypeError, \"Parameter grid value is not iterable \"\n      \"(key='foo', value=0)\")]\n)\ndef test_validate_parameter_grid_input(input, error_type, error_message):\n    with pytest.raises(error_type, message=error_message):\n        ParameterGrid(input)\n\ndef test_parameter_grid():\n\n    # Test basic properties of ParameterGrid.\n    params1 = {\"foo\": [1, 2, 3]}\n    grid1 = ParameterGrid(params1)\n    assert_true(isinstance(grid1, Iterable))\n    assert_true(isinstance(grid1, Sized))\n    assert_equal(len(grid1), 3)\n    assert_grid_iter_equals_getitem(grid1)\n\n    params2 = {\"foo\": [4, 2],\n               \"bar\": [\"ham\", \"spam\", \"eggs\"]}\n    grid2 = ParameterGrid(params2)\n    assert_equal(len(grid2), 6)\n\n    # loop to assert we can iterate over the grid multiple times\n    for i in xrange(2):\n        # tuple + chain transforms {\"a\": 1, \"b\": 2} to (\"a\", 1, \"b\", 2)\n        points = set(tuple(chain(*(sorted(p.items())))) for p in grid2)\n        assert_equal(points,\n                     set((\"bar\", x, \"foo\", y)\n                         for x, y in product(params2[\"bar\"], params2[\"foo\"])))\n    assert_grid_iter_equals_getitem(grid2)\n\n    # Special case: empty grid (useful to get default estimator settings)\n    empty = ParameterGrid({})\n    assert_equal(len(empty), 1)\n    assert_equal(list(empty), [{}])\n    assert_grid_iter_equals_getitem(empty)\n    assert_raises(IndexError, lambda: empty[1])\n\n    has_empty = ParameterGrid([{'C': [1, 10]}, {}, {'C': [.5]}])\n    assert_equal(len(has_empty), 4)\n    assert_equal(list(has_empty), [{'C': 1}, {'C': 10}, {}, {'C': .5}])\n    assert_grid_iter_equals_getitem(has_empty)\n\n\ndef test_grid_search():\n    # Test that the best estimator contains the right value for foo_param\n    clf = MockClassifier()\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, verbose=3)\n    # make sure it selects the smallest parameter in case of ties\n    old_stdout = sys.stdout\n    sys.stdout = StringIO()\n    grid_search.fit(X, y)\n    sys.stdout = old_stdout\n    assert_equal(grid_search.best_estimator_.foo_param, 2)\n\n    assert_array_equal(grid_search.cv_results_[\"param_foo_param\"].data,\n                       [1, 2, 3])\n\n    # Smoke test the score etc:\n    grid_search.score(X, y)\n    grid_search.predict_proba(X)\n    grid_search.decision_function(X)\n    grid_search.transform(X)\n\n    # Test exception handling on scoring\n    grid_search.scoring = 'sklearn'\n    assert_raises(ValueError, grid_search.fit, X, y)\n\n\ndef check_hyperparameter_searcher_with_fit_params(klass, **klass_kwargs):\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n    clf = CheckingClassifier(expected_fit_params=['spam', 'eggs'])\n    searcher = klass(clf, {'foo_param': [1, 2, 3]}, cv=2, **klass_kwargs)\n\n    # The CheckingClassifier generates an assertion error if\n    # a parameter is missing or has length != len(X).\n    assert_raise_message(AssertionError,\n                         \"Expected fit parameter(s) ['eggs'] not seen.\",\n                         searcher.fit, X, y, spam=np.ones(10))\n    assert_raise_message(AssertionError,\n                         \"Fit parameter spam has length 1; expected 4.\",\n                         searcher.fit, X, y, spam=np.ones(1),\n                         eggs=np.zeros(10))\n    searcher.fit(X, y, spam=np.ones(10), eggs=np.zeros(10))\n\n\ndef test_grid_search_with_fit_params():\n    check_hyperparameter_searcher_with_fit_params(GridSearchCV)\n\n\ndef test_random_search_with_fit_params():\n    check_hyperparameter_searcher_with_fit_params(RandomizedSearchCV, n_iter=1)\n\n\ndef test_grid_search_fit_params_deprecation():\n    # NOTE: Remove this test in v0.21\n\n    # Use of `fit_params` in the class constructor is deprecated,\n    # but will still work until v0.21.\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n    clf = CheckingClassifier(expected_fit_params=['spam'])\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]},\n                               fit_params={'spam': np.ones(10)})\n    assert_warns(DeprecationWarning, grid_search.fit, X, y)\n\n\ndef test_grid_search_fit_params_two_places():\n    # NOTE: Remove this test in v0.21\n\n    # If users try to input fit parameters in both\n    # the constructor (deprecated use) and the `fit`\n    # method, we'll ignore the values passed to the constructor.\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n    clf = CheckingClassifier(expected_fit_params=['spam'])\n\n    # The \"spam\" array is too short and will raise an\n    # error in the CheckingClassifier if used.\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]},\n                               fit_params={'spam': np.ones(1)})\n\n    expected_warning = ('Ignoring fit_params passed as a constructor '\n                        'argument in favor of keyword arguments to '\n                        'the \"fit\" method.')\n    assert_warns_message(RuntimeWarning, expected_warning,\n                         grid_search.fit, X, y, spam=np.ones(10))\n\n    # Verify that `fit` prefers its own kwargs by giving valid\n    # kwargs in the constructor and invalid in the method call\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]},\n                               fit_params={'spam': np.ones(10)})\n    assert_raise_message(AssertionError, \"Fit parameter spam has length 1\",\n                         grid_search.fit, X, y, spam=np.ones(1))\n\n\n@ignore_warnings\ndef test_grid_search_no_score():\n    # Test grid-search on classifier that has no score function.\n    clf = LinearSVC(random_state=0)\n    X, y = make_blobs(random_state=0, centers=2)\n    Cs = [.1, 1, 10]\n    clf_no_score = LinearSVCNoScore(random_state=0)\n    grid_search = GridSearchCV(clf, {'C': Cs}, scoring='accuracy')\n    grid_search.fit(X, y)\n\n    grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs},\n                                        scoring='accuracy')\n    # smoketest grid search\n    grid_search_no_score.fit(X, y)\n\n    # check that best params are equal\n    assert_equal(grid_search_no_score.best_params_, grid_search.best_params_)\n    # check that we can call score and that it gives the correct result\n    assert_equal(grid_search.score(X, y), grid_search_no_score.score(X, y))\n\n    # giving no scoring function raises an error\n    grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs})\n    assert_raise_message(TypeError, \"no scoring\", grid_search_no_score.fit,\n                         [[1]])\n\n\ndef test_grid_search_score_method():\n    X, y = make_classification(n_samples=100, n_classes=2, flip_y=.2,\n                               random_state=0)\n    clf = LinearSVC(random_state=0)\n    grid = {'C': [.1]}\n\n    search_no_scoring = GridSearchCV(clf, grid, scoring=None).fit(X, y)\n    search_accuracy = GridSearchCV(clf, grid, scoring='accuracy').fit(X, y)\n    search_no_score_method_auc = GridSearchCV(LinearSVCNoScore(), grid,\n                                              scoring='roc_auc').fit(X, y)\n    search_auc = GridSearchCV(clf, grid, scoring='roc_auc').fit(X, y)\n\n    # Check warning only occurs in situation where behavior changed:\n    # estimator requires score method to compete with scoring parameter\n    score_no_scoring = search_no_scoring.score(X, y)\n    score_accuracy = search_accuracy.score(X, y)\n    score_no_score_auc = search_no_score_method_auc.score(X, y)\n    score_auc = search_auc.score(X, y)\n\n    # ensure the test is sane\n    assert_true(score_auc < 1.0)\n    assert_true(score_accuracy < 1.0)\n    assert_not_equal(score_auc, score_accuracy)\n\n    assert_almost_equal(score_accuracy, score_no_scoring)\n    assert_almost_equal(score_auc, score_no_score_auc)\n\n\ndef test_grid_search_groups():\n    # Check if ValueError (when groups is None) propagates to GridSearchCV\n    # And also check if groups is correctly passed to the cv object\n    rng = np.random.RandomState(0)\n\n    X, y = make_classification(n_samples=15, n_classes=2, random_state=0)\n    groups = rng.randint(0, 3, 15)\n\n    clf = LinearSVC(random_state=0)\n    grid = {'C': [1]}\n\n    group_cvs = [LeaveOneGroupOut(), LeavePGroupsOut(2), GroupKFold(),\n                 GroupShuffleSplit()]\n    for cv in group_cvs:\n        gs = GridSearchCV(clf, grid, cv=cv)\n        assert_raise_message(ValueError,\n                             \"The 'groups' parameter should not be None.\",\n                             gs.fit, X, y)\n        gs.fit(X, y, groups=groups)\n\n    non_group_cvs = [StratifiedKFold(), StratifiedShuffleSplit()]\n    for cv in non_group_cvs:\n        gs = GridSearchCV(clf, grid, cv=cv)\n        # Should not raise an error\n        gs.fit(X, y)\n\n\ndef test_return_train_score_warn():\n    # Test that warnings are raised. Will be removed in 0.21\n\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n    grid = {'C': [1, 2]}\n\n    estimators = [GridSearchCV(LinearSVC(random_state=0), grid, iid=False),\n                  RandomizedSearchCV(LinearSVC(random_state=0), grid,\n                                     n_iter=2, iid=False)]\n\n    result = {}\n    for estimator in estimators:\n        for val in [True, False, 'warn']:\n            estimator.set_params(return_train_score=val)\n            fit_func = ignore_warnings(estimator.fit,\n                                       category=ConvergenceWarning)\n            result[val] = assert_no_warnings(fit_func, X, y).cv_results_\n\n    train_keys = ['split0_train_score', 'split1_train_score',\n                  'split2_train_score', 'mean_train_score', 'std_train_score']\n    for key in train_keys:\n        msg = (\n            'You are accessing a training score ({!r}), '\n            'which will not be available by default '\n            'any more in 0.21. If you need training scores, '\n            'please set return_train_score=True').format(key)\n        train_score = assert_warns_message(FutureWarning, msg,\n                                           result['warn'].get, key)\n        assert np.allclose(train_score, result[True][key])\n        assert key not in result[False]\n\n    for key in result['warn']:\n        if key not in train_keys:\n            assert_no_warnings(result['warn'].get, key)\n\n\ndef test_classes__property():\n    # Test that classes_ property matches best_estimator_.classes_\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n    Cs = [.1, 1, 10]\n\n    grid_search = GridSearchCV(LinearSVC(random_state=0), {'C': Cs})\n    grid_search.fit(X, y)\n    assert_array_equal(grid_search.best_estimator_.classes_,\n                       grid_search.classes_)\n\n    # Test that regressors do not have a classes_ attribute\n    grid_search = GridSearchCV(Ridge(), {'alpha': [1.0, 2.0]})\n    grid_search.fit(X, y)\n    assert_false(hasattr(grid_search, 'classes_'))\n\n    # Test that the grid searcher has no classes_ attribute before it's fit\n    grid_search = GridSearchCV(LinearSVC(random_state=0), {'C': Cs})\n    assert_false(hasattr(grid_search, 'classes_'))\n\n    # Test that the grid searcher has no classes_ attribute without a refit\n    grid_search = GridSearchCV(LinearSVC(random_state=0),\n                               {'C': Cs}, refit=False)\n    grid_search.fit(X, y)\n    assert_false(hasattr(grid_search, 'classes_'))\n\n\ndef test_trivial_cv_results_attr():\n    # Test search over a \"grid\" with only one point.\n    # Non-regression test: grid_scores_ wouldn't be set by GridSearchCV.\n    clf = MockClassifier()\n    grid_search = GridSearchCV(clf, {'foo_param': [1]})\n    grid_search.fit(X, y)\n    assert_true(hasattr(grid_search, \"cv_results_\"))\n\n    random_search = RandomizedSearchCV(clf, {'foo_param': [0]}, n_iter=1)\n    random_search.fit(X, y)\n    assert_true(hasattr(grid_search, \"cv_results_\"))\n\n\ndef test_no_refit():\n    # Test that GSCV can be used for model selection alone without refitting\n    clf = MockClassifier()\n    for scoring in [None, ['accuracy', 'precision']]:\n        grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, refit=False)\n        grid_search.fit(X, y)\n        assert_true(not hasattr(grid_search, \"best_estimator_\") and\n                    hasattr(grid_search, \"best_index_\") and\n                    hasattr(grid_search, \"best_params_\"))\n\n        # Make sure the functions predict/transform etc raise meaningful\n        # error messages\n        for fn_name in ('predict', 'predict_proba', 'predict_log_proba',\n                        'transform', 'inverse_transform'):\n            assert_raise_message(NotFittedError,\n                                 ('refit=False. %s is available only after '\n                                  'refitting on the best parameters'\n                                  % fn_name), getattr(grid_search, fn_name), X)\n\n    # Test that an invalid refit param raises appropriate error messages\n    for refit in [\"\", 5, True, 'recall', 'accuracy']:\n        assert_raise_message(ValueError, \"For multi-metric scoring, the \"\n                             \"parameter refit must be set to a scorer key\",\n                             GridSearchCV(clf, {}, refit=refit,\n                                          scoring={'acc': 'accuracy',\n                                                   'prec': 'precision'}).fit,\n                             X, y)\n\n\ndef test_grid_search_error():\n    # Test that grid search will capture errors on data with different length\n    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)\n\n    clf = LinearSVC()\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})\n    assert_raises(ValueError, cv.fit, X_[:180], y_)\n\n\ndef test_grid_search_one_grid_point():\n    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)\n    param_dict = {\"C\": [1.0], \"kernel\": [\"rbf\"], \"gamma\": [0.1]}\n\n    clf = SVC()\n    cv = GridSearchCV(clf, param_dict)\n    cv.fit(X_, y_)\n\n    clf = SVC(C=1.0, kernel=\"rbf\", gamma=0.1)\n    clf.fit(X_, y_)\n\n    assert_array_equal(clf.dual_coef_, cv.best_estimator_.dual_coef_)\n\n\ndef test_grid_search_when_param_grid_includes_range():\n    # Test that the best estimator contains the right value for foo_param\n    clf = MockClassifier()\n    grid_search = None\n    if PY3_OR_LATER:\n        grid_search = GridSearchCV(clf, {'foo_param': range(1, 4)})\n    else:\n        grid_search = GridSearchCV(clf, {'foo_param': xrange(1, 4)})\n    grid_search.fit(X, y)\n    assert_equal(grid_search.best_estimator_.foo_param, 2)\n\n\ndef test_grid_search_bad_param_grid():\n    param_dict = {\"C\": 1.0}\n    clf = SVC()\n    assert_raise_message(\n        ValueError,\n        \"Parameter values for parameter (C) need to be a sequence\"\n        \"(but not a string) or np.ndarray.\",\n        GridSearchCV, clf, param_dict)\n\n    param_dict = {\"C\": []}\n    clf = SVC(gamma=\"scale\")\n    assert_raise_message(\n        ValueError,\n        \"Parameter values for parameter (C) need to be a non-empty sequence.\",\n        GridSearchCV, clf, param_dict)\n\n    param_dict = {\"C\": \"1,2,3\"}\n    clf = SVC()\n    assert_raise_message(\n        ValueError,\n        \"Parameter values for parameter (C) need to be a sequence\"\n        \"(but not a string) or np.ndarray.\",\n        GridSearchCV, clf, param_dict)\n\n    param_dict = {\"C\": np.ones(6).reshape(3, 2)}\n    clf = SVC(gamma=\"scale\")\n    assert_raises(ValueError, GridSearchCV, clf, param_dict)\n\n\ndef test_grid_search_sparse():\n    # Test that grid search works with both dense and sparse matrices\n    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)\n\n    clf = LinearSVC()\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})\n    cv.fit(X_[:180], y_[:180])\n    y_pred = cv.predict(X_[180:])\n    C = cv.best_estimator_.C\n\n    X_ = sp.csr_matrix(X_)\n    clf = LinearSVC()\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})\n    cv.fit(X_[:180].tocoo(), y_[:180])\n    y_pred2 = cv.predict(X_[180:])\n    C2 = cv.best_estimator_.C\n\n    assert_true(np.mean(y_pred == y_pred2) >= .9)\n    assert_equal(C, C2)\n\n\ndef test_grid_search_sparse_scoring():\n    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)\n\n    clf = LinearSVC()\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]}, scoring=\"f1\")\n    cv.fit(X_[:180], y_[:180])\n    y_pred = cv.predict(X_[180:])\n    C = cv.best_estimator_.C\n\n    X_ = sp.csr_matrix(X_)\n    clf = LinearSVC()\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]}, scoring=\"f1\")\n    cv.fit(X_[:180], y_[:180])\n    y_pred2 = cv.predict(X_[180:])\n    C2 = cv.best_estimator_.C\n\n    assert_array_equal(y_pred, y_pred2)\n    assert_equal(C, C2)\n    # Smoke test the score\n    # np.testing.assert_allclose(f1_score(cv.predict(X_[:180]), y[:180]),\n    #                            cv.score(X_[:180], y[:180]))\n\n    # test loss where greater is worse\n    def f1_loss(y_true_, y_pred_):\n        return -f1_score(y_true_, y_pred_)\n    F1Loss = make_scorer(f1_loss, greater_is_better=False)\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]}, scoring=F1Loss)\n    cv.fit(X_[:180], y_[:180])\n    y_pred3 = cv.predict(X_[180:])\n    C3 = cv.best_estimator_.C\n\n    assert_equal(C, C3)\n    assert_array_equal(y_pred, y_pred3)\n\n\ndef test_grid_search_precomputed_kernel():\n    # Test that grid search works when the input features are given in the\n    # form of a precomputed kernel matrix\n    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)\n\n    # compute the training kernel matrix corresponding to the linear kernel\n    K_train = np.dot(X_[:180], X_[:180].T)\n    y_train = y_[:180]\n\n    clf = SVC(kernel='precomputed')\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})\n    cv.fit(K_train, y_train)\n\n    assert_true(cv.best_score_ >= 0)\n\n    # compute the test kernel matrix\n    K_test = np.dot(X_[180:], X_[:180].T)\n    y_test = y_[180:]\n\n    y_pred = cv.predict(K_test)\n\n    assert_true(np.mean(y_pred == y_test) >= 0)\n\n    # test error is raised when the precomputed kernel is not array-like\n    # or sparse\n    assert_raises(ValueError, cv.fit, K_train.tolist(), y_train)\n\n\ndef test_grid_search_precomputed_kernel_error_nonsquare():\n    # Test that grid search returns an error with a non-square precomputed\n    # training kernel matrix\n    K_train = np.zeros((10, 20))\n    y_train = np.ones((10, ))\n    clf = SVC(kernel='precomputed')\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})\n    assert_raises(ValueError, cv.fit, K_train, y_train)\n\n\nclass BrokenClassifier(BaseEstimator):\n    \"\"\"Broken classifier that cannot be fit twice\"\"\"\n\n    def __init__(self, parameter=None):\n        self.parameter = parameter\n\n    def fit(self, X, y):\n        assert_true(not hasattr(self, 'has_been_fit_'))\n        self.has_been_fit_ = True\n\n    def predict(self, X):\n        return np.zeros(X.shape[0])\n\n\n@ignore_warnings\ndef test_refit():\n    # Regression test for bug in refitting\n    # Simulates re-fitting a broken estimator; this used to break with\n    # sparse SVMs.\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n\n    clf = GridSearchCV(BrokenClassifier(), [{'parameter': [0, 1]}],\n                       scoring=\"precision\", refit=True)\n    clf.fit(X, y)\n\n\ndef test_gridsearch_nd():\n    # Pass X as list in GridSearchCV\n    X_4d = np.arange(10 * 5 * 3 * 2).reshape(10, 5, 3, 2)\n    y_3d = np.arange(10 * 7 * 11).reshape(10, 7, 11)\n    check_X = lambda x: x.shape[1:] == (5, 3, 2)\n    check_y = lambda x: x.shape[1:] == (7, 11)\n    clf = CheckingClassifier(check_X=check_X, check_y=check_y)\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]})\n    grid_search.fit(X_4d, y_3d).score(X, y)\n    assert_true(hasattr(grid_search, \"cv_results_\"))\n\n\ndef test_X_as_list():\n    # Pass X as list in GridSearchCV\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n\n    clf = CheckingClassifier(check_X=lambda x: isinstance(x, list))\n    cv = KFold(n_splits=3)\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, cv=cv)\n    grid_search.fit(X.tolist(), y).score(X, y)\n    assert_true(hasattr(grid_search, \"cv_results_\"))\n\n\ndef test_y_as_list():\n    # Pass y as list in GridSearchCV\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n\n    clf = CheckingClassifier(check_y=lambda x: isinstance(x, list))\n    cv = KFold(n_splits=3)\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, cv=cv)\n    grid_search.fit(X, y.tolist()).score(X, y)\n    assert_true(hasattr(grid_search, \"cv_results_\"))\n\n\n@ignore_warnings\ndef test_pandas_input():\n    # check cross_val_score doesn't destroy pandas dataframe\n    types = [(MockDataFrame, MockDataFrame)]\n    try:\n        from pandas import Series, DataFrame\n        types.append((DataFrame, Series))\n    except ImportError:\n        pass\n\n    X = np.arange(100).reshape(10, 10)\n    y = np.array([0] * 5 + [1] * 5)\n\n    for InputFeatureType, TargetType in types:\n        # X dataframe, y series\n        X_df, y_ser = InputFeatureType(X), TargetType(y)\n\n        def check_df(x):\n            return isinstance(x, InputFeatureType)\n\n        def check_series(x):\n            return isinstance(x, TargetType)\n\n        clf = CheckingClassifier(check_X=check_df, check_y=check_series)\n\n        grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]})\n        grid_search.fit(X_df, y_ser).score(X_df, y_ser)\n        grid_search.predict(X_df)\n        assert_true(hasattr(grid_search, \"cv_results_\"))\n\n\ndef test_unsupervised_grid_search():\n    # test grid-search with unsupervised estimator\n    X, y = make_blobs(random_state=0)\n    km = KMeans(random_state=0)\n\n    # Multi-metric evaluation unsupervised\n    scoring = ['adjusted_rand_score', 'fowlkes_mallows_score']\n    for refit in ['adjusted_rand_score', 'fowlkes_mallows_score']:\n        grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4]),\n                                   scoring=scoring, refit=refit)\n        grid_search.fit(X, y)\n        # Both ARI and FMS can find the right number :)\n        assert_equal(grid_search.best_params_[\"n_clusters\"], 3)\n\n    # Single metric evaluation unsupervised\n    grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4]),\n                               scoring='fowlkes_mallows_score')\n    grid_search.fit(X, y)\n    assert_equal(grid_search.best_params_[\"n_clusters\"], 3)\n\n    # Now without a score, and without y\n    grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4]))\n    grid_search.fit(X)\n    assert_equal(grid_search.best_params_[\"n_clusters\"], 4)\n\n\ndef test_gridsearch_no_predict():\n    # test grid-search with an estimator without predict.\n    # slight duplication of a test from KDE\n    def custom_scoring(estimator, X):\n        return 42 if estimator.bandwidth == .1 else 0\n    X, _ = make_blobs(cluster_std=.1, random_state=1,\n                      centers=[[0, 1], [1, 0], [0, 0]])\n    search = GridSearchCV(KernelDensity(),\n                          param_grid=dict(bandwidth=[.01, .1, 1]),\n                          scoring=custom_scoring)\n    search.fit(X)\n    assert_equal(search.best_params_['bandwidth'], .1)\n    assert_equal(search.best_score_, 42)\n\n\ndef test_param_sampler():\n    # test basic properties of param sampler\n    param_distributions = {\"kernel\": [\"rbf\", \"linear\"],\n                           \"C\": uniform(0, 1)}\n    sampler = ParameterSampler(param_distributions=param_distributions,\n                               n_iter=10, random_state=0)\n    samples = [x for x in sampler]\n    assert_equal(len(samples), 10)\n    for sample in samples:\n        assert_true(sample[\"kernel\"] in [\"rbf\", \"linear\"])\n        assert_true(0 <= sample[\"C\"] <= 1)\n\n    # test that repeated calls yield identical parameters\n    param_distributions = {\"C\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n    sampler = ParameterSampler(param_distributions=param_distributions,\n                               n_iter=3, random_state=0)\n    assert_equal([x for x in sampler], [x for x in sampler])\n\n    if sp_version >= (0, 16):\n        param_distributions = {\"C\": uniform(0, 1)}\n        sampler = ParameterSampler(param_distributions=param_distributions,\n                                   n_iter=10, random_state=0)\n        assert_equal([x for x in sampler], [x for x in sampler])\n\n\ndef check_cv_results_array_types(search, param_keys, score_keys):\n    # Check if the search `cv_results`'s array are of correct types\n    cv_results = search.cv_results_\n    assert_true(all(isinstance(cv_results[param], np.ma.MaskedArray)\n                    for param in param_keys))\n    assert_true(all(cv_results[key].dtype == object for key in param_keys))\n    assert_false(any(isinstance(cv_results[key], np.ma.MaskedArray)\n                     for key in score_keys))\n    assert_true(all(cv_results[key].dtype == np.float64\n                    for key in score_keys if not key.startswith('rank')))\n\n    scorer_keys = search.scorer_.keys() if search.multimetric_ else ['score']\n\n    for key in scorer_keys:\n        assert_true(cv_results['rank_test_%s' % key].dtype == np.int32)\n\n\ndef check_cv_results_keys(cv_results, param_keys, score_keys, n_cand):\n    # Test the search.cv_results_ contains all the required results\n    assert_array_equal(sorted(cv_results.keys()),\n                       sorted(param_keys + score_keys + ('params',)))\n    assert_true(all(cv_results[key].shape == (n_cand,)\n                    for key in param_keys + score_keys))\n\n\ndef check_cv_results_grid_scores_consistency(search):\n    # TODO Remove test in 0.20\n    if search.multimetric_:\n        assert_raise_message(AttributeError, \"not available for multi-metric\",\n                             getattr, search, 'grid_scores_')\n    else:\n        cv_results = search.cv_results_\n        res_scores = np.vstack(list([cv_results[\"split%d_test_score\" % i]\n                                     for i in range(search.n_splits_)])).T\n        res_means = cv_results[\"mean_test_score\"]\n        res_params = cv_results[\"params\"]\n        n_cand = len(res_params)\n        grid_scores = assert_warns(DeprecationWarning, getattr,\n                                   search, 'grid_scores_')\n        assert_equal(len(grid_scores), n_cand)\n        # Check consistency of the structure of grid_scores\n        for i in range(n_cand):\n            assert_equal(grid_scores[i].parameters, res_params[i])\n            assert_array_equal(grid_scores[i].cv_validation_scores,\n                               res_scores[i, :])\n            assert_array_equal(grid_scores[i].mean_validation_score,\n                               res_means[i])\n\n\ndef test_grid_search_cv_results():\n    X, y = make_classification(n_samples=50, n_features=4,\n                               random_state=42)\n\n    n_splits = 3\n    n_grid_points = 6\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n\n    param_keys = ('param_C', 'param_degree', 'param_gamma', 'param_kernel')\n    score_keys = ('mean_test_score', 'mean_train_score',\n                  'rank_test_score',\n                  'split0_test_score', 'split1_test_score',\n                  'split2_test_score',\n                  'split0_train_score', 'split1_train_score',\n                  'split2_train_score',\n                  'std_test_score', 'std_train_score',\n                  'mean_fit_time', 'std_fit_time',\n                  'mean_score_time', 'std_score_time')\n    n_candidates = n_grid_points\n\n    for iid in (False, True):\n        search = GridSearchCV(SVC(gamma='scale'), cv=n_splits, iid=iid,\n                              param_grid=params)\n        search.fit(X, y)\n        assert_equal(iid, search.iid)\n        cv_results = search.cv_results_\n        # Check if score and timing are reasonable\n        assert_true(all(cv_results['rank_test_score'] >= 1))\n        assert_true(all(cv_results[k] >= 0) for k in score_keys\n                    if k is not 'rank_test_score')\n        assert_true(all(cv_results[k] <= 1) for k in score_keys\n                    if 'time' not in k and\n                    k is not 'rank_test_score')\n        # Check cv_results structure\n        check_cv_results_array_types(search, param_keys, score_keys)\n        check_cv_results_keys(cv_results, param_keys, score_keys, n_candidates)\n        # Check masking\n        cv_results = search.cv_results_\n        n_candidates = len(search.cv_results_['params'])\n        assert_true(all((cv_results['param_C'].mask[i] and\n                         cv_results['param_gamma'].mask[i] and\n                         not cv_results['param_degree'].mask[i])\n                        for i in range(n_candidates)\n                        if cv_results['param_kernel'][i] == 'linear'))\n        assert_true(all((not cv_results['param_C'].mask[i] and\n                         not cv_results['param_gamma'].mask[i] and\n                         cv_results['param_degree'].mask[i])\n                        for i in range(n_candidates)\n                        if cv_results['param_kernel'][i] == 'rbf'))\n        check_cv_results_grid_scores_consistency(search)\n\n\ndef test_random_search_cv_results():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    n_splits = 3\n    n_search_iter = 30\n\n    params = dict(C=expon(scale=10), gamma=expon(scale=0.1))\n    param_keys = ('param_C', 'param_gamma')\n    score_keys = ('mean_test_score', 'mean_train_score',\n                  'rank_test_score',\n                  'split0_test_score', 'split1_test_score',\n                  'split2_test_score',\n                  'split0_train_score', 'split1_train_score',\n                  'split2_train_score',\n                  'std_test_score', 'std_train_score',\n                  'mean_fit_time', 'std_fit_time',\n                  'mean_score_time', 'std_score_time')\n    n_cand = n_search_iter\n\n    for iid in (False, True):\n        search = RandomizedSearchCV(SVC(gamma='scale'), n_iter=n_search_iter,\n                                    cv=n_splits, iid=iid,\n                                    param_distributions=params)\n        search.fit(X, y)\n        assert_equal(iid, search.iid)\n        cv_results = search.cv_results_\n        # Check results structure\n        check_cv_results_array_types(search, param_keys, score_keys)\n        check_cv_results_keys(cv_results, param_keys, score_keys, n_cand)\n        # For random_search, all the param array vals should be unmasked\n        assert_false(any(cv_results['param_C'].mask) or\n                     any(cv_results['param_gamma'].mask))\n        check_cv_results_grid_scores_consistency(search)\n\n\n@ignore_warnings(category=DeprecationWarning)\ndef test_search_iid_param():\n    # Test the IID parameter\n    # noise-free simple 2d-data\n    X, y = make_blobs(centers=[[0, 0], [1, 0], [0, 1], [1, 1]], random_state=0,\n                      cluster_std=0.1, shuffle=False, n_samples=80)\n    # split dataset into two folds that are not iid\n    # first one contains data of all 4 blobs, second only from two.\n    mask = np.ones(X.shape[0], dtype=np.bool)\n    mask[np.where(y == 1)[0][::2]] = 0\n    mask[np.where(y == 2)[0][::2]] = 0\n    # this leads to perfect classification on one fold and a score of 1/3 on\n    # the other\n    # create \"cv\" for splits\n    cv = [[mask, ~mask], [~mask, mask]]\n    # once with iid=True (default)\n    grid_search = GridSearchCV(SVC(), param_grid={'C': [1, 10]},\n                               cv=cv)\n    random_search = RandomizedSearchCV(SVC(), n_iter=2,\n                                       param_distributions={'C': [1, 10]},\n                                       cv=cv)\n    for search in (grid_search, random_search):\n        search.fit(X, y)\n        assert_true(search.iid or search.iid is None)\n\n        test_cv_scores = np.array(list(search.cv_results_['split%d_test_score'\n                                                          % s_i][0]\n                                       for s_i in range(search.n_splits_)))\n        test_mean = search.cv_results_['mean_test_score'][0]\n        test_std = search.cv_results_['std_test_score'][0]\n\n        train_cv_scores = np.array(list(search.cv_results_['split%d_train_'\n                                                           'score' % s_i][0]\n                                        for s_i in range(search.n_splits_)))\n        train_mean = search.cv_results_['mean_train_score'][0]\n        train_std = search.cv_results_['std_train_score'][0]\n\n        # Test the first candidate\n        assert_equal(search.cv_results_['param_C'][0], 1)\n        assert_array_almost_equal(test_cv_scores, [1, 1. / 3.])\n        assert_array_almost_equal(train_cv_scores, [1, 1])\n\n        # for first split, 1/4 of dataset is in test, for second 3/4.\n        # take weighted average and weighted std\n        expected_test_mean = 1 * 1. / 4. + 1. / 3. * 3. / 4.\n        expected_test_std = np.sqrt(1. / 4 * (expected_test_mean - 1) ** 2 +\n                                    3. / 4 * (expected_test_mean - 1. / 3.) **\n                                    2)\n        assert_almost_equal(test_mean, expected_test_mean)\n        assert_almost_equal(test_std, expected_test_std)\n        assert_array_almost_equal(test_cv_scores,\n                                  cross_val_score(SVC(C=1), X,\n                                                  y, cv=cv))\n\n        # For the train scores, we do not take a weighted mean irrespective of\n        # i.i.d. or not\n        assert_almost_equal(train_mean, 1)\n        assert_almost_equal(train_std, 0)\n\n    # once with iid=False\n    grid_search = GridSearchCV(SVC(),\n                               param_grid={'C': [1, 10]},\n                               cv=cv, iid=False)\n    random_search = RandomizedSearchCV(SVC(), n_iter=2,\n                                       param_distributions={'C': [1, 10]},\n                                       cv=cv, iid=False)\n\n    for search in (grid_search, random_search):\n        search.fit(X, y)\n        assert_false(search.iid)\n\n        test_cv_scores = np.array(list(search.cv_results_['split%d_test_score'\n                                                          % s][0]\n                                       for s in range(search.n_splits_)))\n        test_mean = search.cv_results_['mean_test_score'][0]\n        test_std = search.cv_results_['std_test_score'][0]\n\n        train_cv_scores = np.array(list(search.cv_results_['split%d_train_'\n                                                           'score' % s][0]\n                                        for s in range(search.n_splits_)))\n        train_mean = search.cv_results_['mean_train_score'][0]\n        train_std = search.cv_results_['std_train_score'][0]\n\n        assert_equal(search.cv_results_['param_C'][0], 1)\n        # scores are the same as above\n        assert_array_almost_equal(test_cv_scores, [1, 1. / 3.])\n        # Unweighted mean/std is used\n        assert_almost_equal(test_mean, np.mean(test_cv_scores))\n        assert_almost_equal(test_std, np.std(test_cv_scores))\n\n        # For the train scores, we do not take a weighted mean irrespective of\n        # i.i.d. or not\n        assert_almost_equal(train_mean, 1)\n        assert_almost_equal(train_std, 0)\n\n\ndef test_grid_search_cv_results_multimetric():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    n_splits = 3\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n\n    for iid in (False, True):\n        grid_searches = []\n        for scoring in ({'accuracy': make_scorer(accuracy_score),\n                         'recall': make_scorer(recall_score)},\n                        'accuracy', 'recall'):\n            grid_search = GridSearchCV(SVC(gamma='scale'), cv=n_splits,\n                                       iid=iid, param_grid=params,\n                                       scoring=scoring, refit=False)\n            grid_search.fit(X, y)\n            assert_equal(grid_search.iid, iid)\n            grid_searches.append(grid_search)\n\n        compare_cv_results_multimetric_with_single(*grid_searches, iid=iid)\n\n\ndef test_random_search_cv_results_multimetric():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    n_splits = 3\n    n_search_iter = 30\n    scoring = ('accuracy', 'recall')\n\n    # Scipy 0.12's stats dists do not accept seed, hence we use param grid\n    params = dict(C=np.logspace(-10, 1), gamma=np.logspace(-5, 0, base=0.1))\n    for iid in (True, False):\n        for refit in (True, False):\n            random_searches = []\n            for scoring in (('accuracy', 'recall'), 'accuracy', 'recall'):\n                # If True, for multi-metric pass refit='accuracy'\n                if refit:\n                    refit = 'accuracy' if isinstance(scoring, tuple) else refit\n                clf = SVC(probability=True, random_state=42)\n                random_search = RandomizedSearchCV(clf, n_iter=n_search_iter,\n                                                   cv=n_splits, iid=iid,\n                                                   param_distributions=params,\n                                                   scoring=scoring,\n                                                   refit=refit, random_state=0)\n                random_search.fit(X, y)\n                random_searches.append(random_search)\n\n            compare_cv_results_multimetric_with_single(*random_searches,\n                                                       iid=iid)\n            if refit:\n                compare_refit_methods_when_refit_with_acc(\n                    random_searches[0], random_searches[1], refit)\n\n\ndef compare_cv_results_multimetric_with_single(\n        search_multi, search_acc, search_rec, iid):\n    \"\"\"Compare multi-metric cv_results with the ensemble of multiple\n    single metric cv_results from single metric grid/random search\"\"\"\n\n    assert_equal(search_multi.iid, iid)\n    assert_true(search_multi.multimetric_)\n    assert_array_equal(sorted(search_multi.scorer_),\n                       ('accuracy', 'recall'))\n\n    cv_results_multi = search_multi.cv_results_\n    cv_results_acc_rec = {re.sub('_score$', '_accuracy', k): v\n                          for k, v in search_acc.cv_results_.items()}\n    cv_results_acc_rec.update({re.sub('_score$', '_recall', k): v\n                               for k, v in search_rec.cv_results_.items()})\n\n    # Check if score and timing are reasonable, also checks if the keys\n    # are present\n    assert_true(all((np.all(cv_results_multi[k] <= 1) for k in (\n                    'mean_score_time', 'std_score_time', 'mean_fit_time',\n                    'std_fit_time'))))\n\n    # Compare the keys, other than time keys, among multi-metric and\n    # single metric grid search results. np.testing.assert_equal performs a\n    # deep nested comparison of the two cv_results dicts\n    np.testing.assert_equal({k: v for k, v in cv_results_multi.items()\n                             if not k.endswith('_time')},\n                            {k: v for k, v in cv_results_acc_rec.items()\n                             if not k.endswith('_time')})\n\n\ndef compare_refit_methods_when_refit_with_acc(search_multi, search_acc, refit):\n    \"\"\"Compare refit multi-metric search methods with single metric methods\"\"\"\n    if refit:\n        assert_equal(search_multi.refit, 'accuracy')\n    else:\n        assert_false(search_multi.refit)\n    assert_equal(search_acc.refit, refit)\n\n    X, y = make_blobs(n_samples=100, n_features=4, random_state=42)\n    for method in ('predict', 'predict_proba', 'predict_log_proba'):\n        assert_almost_equal(getattr(search_multi, method)(X),\n                            getattr(search_acc, method)(X))\n    assert_almost_equal(search_multi.score(X, y), search_acc.score(X, y))\n    for key in ('best_index_', 'best_score_', 'best_params_'):\n        assert_equal(getattr(search_multi, key), getattr(search_acc, key))\n\n\ndef test_search_cv_results_rank_tie_breaking():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(gamma=\"scale\"), param_grid=param_grid)\n    random_search = RandomizedSearchCV(SVC(gamma=\"scale\"), n_iter=3,\n                                       param_distributions=param_grid)\n\n    for search in (grid_search, random_search):\n        search.fit(X, y)\n        cv_results = search.cv_results_\n        # Check tie breaking strategy -\n        # Check that there is a tie in the mean scores between\n        # candidates 1 and 2 alone\n        assert_almost_equal(cv_results['mean_test_score'][0],\n                            cv_results['mean_test_score'][1])\n        assert_almost_equal(cv_results['mean_train_score'][0],\n                            cv_results['mean_train_score'][1])\n        assert_false(np.allclose(cv_results['mean_test_score'][1],\n                                 cv_results['mean_test_score'][2]))\n        assert_false(np.allclose(cv_results['mean_train_score'][1],\n                                 cv_results['mean_train_score'][2]))\n        # 'min' rank should be assigned to the tied candidates\n        assert_almost_equal(search.cv_results_['rank_test_score'], [1, 1, 3])\n\n\ndef test_search_cv_results_none_param():\n    X, y = [[1], [2], [3], [4], [5]], [0, 0, 0, 0, 1]\n    estimators = (DecisionTreeRegressor(), DecisionTreeClassifier())\n    est_parameters = {\"random_state\": [0, None]}\n    cv = KFold(random_state=0)\n\n    for est in estimators:\n        grid_search = GridSearchCV(est, est_parameters, cv=cv).fit(X, y)\n        assert_array_equal(grid_search.cv_results_['param_random_state'],\n                           [0, None])\n\n\n@ignore_warnings()\ndef test_search_cv_timing():\n    svc = LinearSVC(random_state=0)\n\n    X = [[1, ], [2, ], [3, ], [4, ]]\n    y = [0, 1, 1, 0]\n\n    gs = GridSearchCV(svc, {'C': [0, 1]}, cv=2, error_score=0)\n    rs = RandomizedSearchCV(svc, {'C': [0, 1]}, cv=2, error_score=0, n_iter=2)\n\n    for search in (gs, rs):\n        search.fit(X, y)\n        for key in ['mean_fit_time', 'std_fit_time']:\n            # NOTE The precision of time.time in windows is not high\n            # enough for the fit/score times to be non-zero for trivial X and y\n            assert_true(np.all(search.cv_results_[key] >= 0))\n            assert_true(np.all(search.cv_results_[key] < 1))\n\n        for key in ['mean_score_time', 'std_score_time']:\n            assert_true(search.cv_results_[key][1] >= 0)\n            assert_true(search.cv_results_[key][0] == 0.0)\n            assert_true(np.all(search.cv_results_[key] < 1))\n\n\ndef test_grid_search_correct_score_results():\n    # test that correct scores are used\n    n_splits = 3\n    clf = LinearSVC(random_state=0)\n    X, y = make_blobs(random_state=0, centers=2)\n    Cs = [.1, 1, 10]\n    for score in ['f1', 'roc_auc']:\n        grid_search = GridSearchCV(clf, {'C': Cs}, scoring=score, cv=n_splits)\n        cv_results = grid_search.fit(X, y).cv_results_\n\n        # Test scorer names\n        result_keys = list(cv_results.keys())\n        expected_keys = ((\"mean_test_score\", \"rank_test_score\") +\n                         tuple(\"split%d_test_score\" % cv_i\n                               for cv_i in range(n_splits)))\n        assert_true(all(np.in1d(expected_keys, result_keys)))\n\n        cv = StratifiedKFold(n_splits=n_splits)\n        n_splits = grid_search.n_splits_\n        for candidate_i, C in enumerate(Cs):\n            clf.set_params(C=C)\n            cv_scores = np.array(\n                list(grid_search.cv_results_['split%d_test_score'\n                                             % s][candidate_i]\n                     for s in range(n_splits)))\n            for i, (train, test) in enumerate(cv.split(X, y)):\n                clf.fit(X[train], y[train])\n                if score == \"f1\":\n                    correct_score = f1_score(y[test], clf.predict(X[test]))\n                elif score == \"roc_auc\":\n                    dec = clf.decision_function(X[test])\n                    correct_score = roc_auc_score(y[test], dec)\n                assert_almost_equal(correct_score, cv_scores[i])\n\n\ndef test_fit_grid_point():\n    X, y = make_classification(random_state=0)\n    cv = StratifiedKFold(random_state=0)\n    svc = LinearSVC(random_state=0)\n    scorer = make_scorer(accuracy_score)\n\n    for params in ({'C': 0.1}, {'C': 0.01}, {'C': 0.001}):\n        for train, test in cv.split(X, y):\n            this_scores, this_params, n_test_samples = fit_grid_point(\n                X, y, clone(svc), params, train, test,\n                scorer, verbose=False)\n\n            est = clone(svc).set_params(**params)\n            est.fit(X[train], y[train])\n            expected_score = scorer(est, X[test], y[test])\n\n            # Test the return values of fit_grid_point\n            assert_almost_equal(this_scores, expected_score)\n            assert_equal(params, this_params)\n            assert_equal(n_test_samples, test.size)\n\n    # Should raise an error upon multimetric scorer\n    assert_raise_message(ValueError, \"For evaluating multiple scores, use \"\n                         \"sklearn.model_selection.cross_validate instead.\",\n                         fit_grid_point, X, y, svc, params, train, test,\n                         {'score': scorer}, verbose=True)\n\n\ndef test_pickle():\n    # Test that a fit search can be pickled\n    clf = MockClassifier()\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, refit=True)\n    grid_search.fit(X, y)\n    grid_search_pickled = pickle.loads(pickle.dumps(grid_search))\n    assert_array_almost_equal(grid_search.predict(X),\n                              grid_search_pickled.predict(X))\n\n    random_search = RandomizedSearchCV(clf, {'foo_param': [1, 2, 3]},\n                                       refit=True, n_iter=3)\n    random_search.fit(X, y)\n    random_search_pickled = pickle.loads(pickle.dumps(random_search))\n    assert_array_almost_equal(random_search.predict(X),\n                              random_search_pickled.predict(X))\n\n\ndef test_grid_search_with_multioutput_data():\n    # Test search with multi-output estimator\n\n    X, y = make_multilabel_classification(return_indicator=True,\n                                          random_state=0)\n\n    est_parameters = {\"max_depth\": [1, 2, 3, 4]}\n    cv = KFold(random_state=0)\n\n    estimators = [DecisionTreeRegressor(random_state=0),\n                  DecisionTreeClassifier(random_state=0)]\n\n    # Test with grid search cv\n    for est in estimators:\n        grid_search = GridSearchCV(est, est_parameters, cv=cv)\n        grid_search.fit(X, y)\n        res_params = grid_search.cv_results_['params']\n        for cand_i in range(len(res_params)):\n            est.set_params(**res_params[cand_i])\n\n            for i, (train, test) in enumerate(cv.split(X, y)):\n                est.fit(X[train], y[train])\n                correct_score = est.score(X[test], y[test])\n                assert_almost_equal(\n                    correct_score,\n                    grid_search.cv_results_['split%d_test_score' % i][cand_i])\n\n    # Test with a randomized search\n    for est in estimators:\n        random_search = RandomizedSearchCV(est, est_parameters,\n                                           cv=cv, n_iter=3)\n        random_search.fit(X, y)\n        res_params = random_search.cv_results_['params']\n        for cand_i in range(len(res_params)):\n            est.set_params(**res_params[cand_i])\n\n            for i, (train, test) in enumerate(cv.split(X, y)):\n                est.fit(X[train], y[train])\n                correct_score = est.score(X[test], y[test])\n                assert_almost_equal(\n                    correct_score,\n                    random_search.cv_results_['split%d_test_score'\n                                              % i][cand_i])\n\n\ndef test_predict_proba_disabled():\n    # Test predict_proba when disabled on estimator.\n    X = np.arange(20).reshape(5, -1)\n    y = [0, 0, 1, 1, 1]\n    clf = SVC(gamma='scale', probability=False)\n    gs = GridSearchCV(clf, {}, cv=2).fit(X, y)\n    assert_false(hasattr(gs, \"predict_proba\"))\n\n\ndef test_grid_search_allows_nans():\n    # Test GridSearchCV with SimpleImputer\n    X = np.arange(20, dtype=np.float64).reshape(5, -1)\n    X[2, :] = np.nan\n    y = [0, 0, 1, 1, 1]\n    p = Pipeline([\n        ('imputer', SimpleImputer(strategy='mean', missing_values='NaN')),\n        ('classifier', MockClassifier()),\n    ])\n    GridSearchCV(p, {'classifier__foo_param': [1, 2, 3]}, cv=2).fit(X, y)\n\n\nclass FailingClassifier(BaseEstimator):\n    \"\"\"Classifier that raises a ValueError on fit()\"\"\"\n\n    FAILING_PARAMETER = 2\n\n    def __init__(self, parameter=None):\n        self.parameter = parameter\n\n    def fit(self, X, y=None):\n        if self.parameter == FailingClassifier.FAILING_PARAMETER:\n            raise ValueError(\"Failing classifier failed as required\")\n\n    def predict(self, X):\n        return np.zeros(X.shape[0])\n\n\ndef test_grid_search_failing_classifier():\n    # GridSearchCV with on_error != 'raise'\n    # Ensures that a warning is raised and score reset where appropriate.\n\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n\n    clf = FailingClassifier()\n\n    # refit=False because we only want to check that errors caused by fits\n    # to individual folds will be caught and warnings raised instead. If\n    # refit was done, then an exception would be raised on refit and not\n    # caught by grid_search (expected behavior), and this would cause an\n    # error in this test.\n    gs = GridSearchCV(clf, [{'parameter': [0, 1, 2]}], scoring='accuracy',\n                      refit=False, error_score=0.0)\n    assert_warns(FitFailedWarning, gs.fit, X, y)\n    n_candidates = len(gs.cv_results_['params'])\n\n    # Ensure that grid scores were set to zero as required for those fits\n    # that are expected to fail.\n    def get_cand_scores(i):\n        return np.array(list(gs.cv_results_['split%d_test_score' % s][i]\n                             for s in range(gs.n_splits_)))\n\n    assert all((np.all(get_cand_scores(cand_i) == 0.0)\n                for cand_i in range(n_candidates)\n                if gs.cv_results_['param_parameter'][cand_i] ==\n                FailingClassifier.FAILING_PARAMETER))\n\n    gs = GridSearchCV(clf, [{'parameter': [0, 1, 2]}], scoring='accuracy',\n                      refit=False, error_score=float('nan'))\n    assert_warns(FitFailedWarning, gs.fit, X, y)\n    n_candidates = len(gs.cv_results_['params'])\n    assert all(np.all(np.isnan(get_cand_scores(cand_i)))\n               for cand_i in range(n_candidates)\n               if gs.cv_results_['param_parameter'][cand_i] ==\n               FailingClassifier.FAILING_PARAMETER)\n\n    ranks = gs.cv_results_['rank_test_score']\n\n    # Check that succeeded estimators have lower ranks\n    assert ranks[0] <= 2 and ranks[1] <= 2\n    # Check that failed estimator has the highest rank\n    assert ranks[clf.FAILING_PARAMETER] == 3\n    assert gs.best_index_ != clf.FAILING_PARAMETER\n\n\ndef test_grid_search_failing_classifier_raise():\n    # GridSearchCV with on_error == 'raise' raises the error\n\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n\n    clf = FailingClassifier()\n\n    # refit=False because we want to test the behaviour of the grid search part\n    gs = GridSearchCV(clf, [{'parameter': [0, 1, 2]}], scoring='accuracy',\n                      refit=False, error_score='raise')\n\n    # FailingClassifier issues a ValueError so this is what we look for.\n    assert_raises(ValueError, gs.fit, X, y)\n\n\ndef test_parameters_sampler_replacement():\n    # raise warning if n_iter is bigger than total parameter space\n    params = {'first': [0, 1], 'second': ['a', 'b', 'c']}\n    sampler = ParameterSampler(params, n_iter=7)\n    n_iter = 7\n    grid_size = 6\n    expected_warning = ('The total space of parameters %d is smaller '\n                        'than n_iter=%d. Running %d iterations. For '\n                        'exhaustive searches, use GridSearchCV.'\n                        % (grid_size, n_iter, grid_size))\n    assert_warns_message(UserWarning, expected_warning,\n                         list, sampler)\n\n    # degenerates to GridSearchCV if n_iter the same as grid_size\n    sampler = ParameterSampler(params, n_iter=6)\n    samples = list(sampler)\n    assert_equal(len(samples), 6)\n    for values in ParameterGrid(params):\n        assert_true(values in samples)\n\n    # test sampling without replacement in a large grid\n    params = {'a': range(10), 'b': range(10), 'c': range(10)}\n    sampler = ParameterSampler(params, n_iter=99, random_state=42)\n    samples = list(sampler)\n    assert_equal(len(samples), 99)\n    hashable_samples = [\"a%db%dc%d\" % (p['a'], p['b'], p['c'])\n                        for p in samples]\n    assert_equal(len(set(hashable_samples)), 99)\n\n    # doesn't go into infinite loops\n    params_distribution = {'first': bernoulli(.5), 'second': ['a', 'b', 'c']}\n    sampler = ParameterSampler(params_distribution, n_iter=7)\n    samples = list(sampler)\n    assert_equal(len(samples), 7)\n\n\ndef test_stochastic_gradient_loss_param():\n    # Make sure the predict_proba works when loss is specified\n    # as one of the parameters in the param_grid.\n    param_grid = {\n        'loss': ['log'],\n    }\n    X = np.arange(24).reshape(6, -1)\n    y = [0, 0, 0, 1, 1, 1]\n    clf = GridSearchCV(estimator=SGDClassifier(tol=1e-3, loss='hinge'),\n                       param_grid=param_grid)\n\n    # When the estimator is not fitted, `predict_proba` is not available as the\n    # loss is 'hinge'.\n    assert_false(hasattr(clf, \"predict_proba\"))\n    clf.fit(X, y)\n    clf.predict_proba(X)\n    clf.predict_log_proba(X)\n\n    # Make sure `predict_proba` is not available when setting loss=['hinge']\n    # in param_grid\n    param_grid = {\n        'loss': ['hinge'],\n    }\n    clf = GridSearchCV(estimator=SGDClassifier(tol=1e-3, loss='hinge'),\n                       param_grid=param_grid)\n    assert_false(hasattr(clf, \"predict_proba\"))\n    clf.fit(X, y)\n    assert_false(hasattr(clf, \"predict_proba\"))\n\n\ndef test_search_train_scores_set_to_false():\n    X = np.arange(6).reshape(6, -1)\n    y = [0, 0, 0, 1, 1, 1]\n    clf = LinearSVC(random_state=0)\n\n    gs = GridSearchCV(clf, param_grid={'C': [0.1, 0.2]},\n                      return_train_score=False)\n    gs.fit(X, y)\n\n\ndef test_grid_search_cv_splits_consistency():\n    # Check if a one time iterable is accepted as a cv parameter.\n    n_samples = 100\n    n_splits = 5\n    X, y = make_classification(n_samples=n_samples, random_state=0)\n\n    gs = GridSearchCV(LinearSVC(random_state=0),\n                      param_grid={'C': [0.1, 0.2, 0.3]},\n                      cv=OneTimeSplitter(n_splits=n_splits,\n                                         n_samples=n_samples))\n    gs.fit(X, y)\n\n    gs2 = GridSearchCV(LinearSVC(random_state=0),\n                       param_grid={'C': [0.1, 0.2, 0.3]},\n                       cv=KFold(n_splits=n_splits))\n    gs2.fit(X, y)\n\n    # Give generator as a cv parameter\n    assert_true(isinstance(KFold(n_splits=n_splits,\n                                 shuffle=True, random_state=0).split(X, y),\n                           GeneratorType))\n    gs3 = GridSearchCV(LinearSVC(random_state=0),\n                       param_grid={'C': [0.1, 0.2, 0.3]},\n                       cv=KFold(n_splits=n_splits, shuffle=True,\n                                random_state=0).split(X, y))\n    gs3.fit(X, y)\n\n    gs4 = GridSearchCV(LinearSVC(random_state=0),\n                       param_grid={'C': [0.1, 0.2, 0.3]},\n                       cv=KFold(n_splits=n_splits, shuffle=True,\n                                random_state=0))\n    gs4.fit(X, y)\n\n    def _pop_time_keys(cv_results):\n        for key in ('mean_fit_time', 'std_fit_time',\n                    'mean_score_time', 'std_score_time'):\n            cv_results.pop(key)\n        return cv_results\n\n    # Check if generators are supported as cv and\n    # that the splits are consistent\n    np.testing.assert_equal(_pop_time_keys(gs3.cv_results_),\n                            _pop_time_keys(gs4.cv_results_))\n\n    # OneTimeSplitter is a non-re-entrant cv where split can be called only\n    # once if ``cv.split`` is called once per param setting in GridSearchCV.fit\n    # the 2nd and 3rd parameter will not be evaluated as no train/test indices\n    # will be generated for the 2nd and subsequent cv.split calls.\n    # This is a check to make sure cv.split is not called once per param\n    # setting.\n    np.testing.assert_equal({k: v for k, v in gs.cv_results_.items()\n                             if not k.endswith('_time')},\n                            {k: v for k, v in gs2.cv_results_.items()\n                             if not k.endswith('_time')})\n\n    # Check consistency of folds across the parameters\n    gs = GridSearchCV(LinearSVC(random_state=0),\n                      param_grid={'C': [0.1, 0.1, 0.2, 0.2]},\n                      cv=KFold(n_splits=n_splits, shuffle=True))\n    gs.fit(X, y)\n\n    # As the first two param settings (C=0.1) and the next two param\n    # settings (C=0.2) are same, the test and train scores must also be\n    # same as long as the same train/test indices are generated for all\n    # the cv splits, for both param setting\n    for score_type in ('train', 'test'):\n        per_param_scores = {}\n        for param_i in range(4):\n            per_param_scores[param_i] = list(\n                gs.cv_results_['split%d_%s_score' % (s, score_type)][param_i]\n                for s in range(5))\n\n        assert_array_almost_equal(per_param_scores[0],\n                                  per_param_scores[1])\n        assert_array_almost_equal(per_param_scores[2],\n                                  per_param_scores[3])\n\n\ndef test_transform_inverse_transform_round_trip():\n    clf = MockClassifier()\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, verbose=3)\n\n    grid_search.fit(X, y)\n    X_round_trip = grid_search.inverse_transform(grid_search.transform(X))\n    assert_array_equal(X, X_round_trip)\n\n\ndef test_deprecated_grid_search_iid():\n    depr_message = (\"The default of the `iid` parameter will change from True \"\n                    \"to False in version 0.22\")\n    X, y = make_blobs(n_samples=54, random_state=0, centers=2)\n    grid = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1]}, cv=3)\n    # no warning with equally sized test sets\n    assert_no_warnings(grid.fit, X, y)\n\n    grid = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1]}, cv=5)\n    # warning because 54 % 5 != 0\n    assert_warns_message(DeprecationWarning, depr_message, grid.fit, X, y)\n\n    grid = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1]}, cv=2)\n    # warning because stratification into two classes and 27 % 2 != 0\n    assert_warns_message(DeprecationWarning, depr_message, grid.fit, X, y)\n\n    grid = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1]}, cv=KFold(2))\n    # no warning because no stratification and 54 % 2 == 0\n    assert_no_warnings(grid.fit, X, y)\n"
    },
    {
      "filename": "sklearn/utils/fixes.py",
      "content": "\"\"\"Compatibility fixes for older version of python, numpy and scipy\n\nIf you add content to this file, please give the version of the package\nat which the fixe is no longer needed.\n\"\"\"\n# Authors: Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>\n#          Gael Varoquaux <gael.varoquaux@normalesup.org>\n#          Fabian Pedregosa <fpedregosa@acm.org>\n#          Lars Buitinck\n#\n# License: BSD 3 clause\n\nimport warnings\nimport os\nimport errno\nimport sys\n\nimport numpy as np\nimport scipy.sparse as sp\nimport scipy\n\ntry:\n    from inspect import signature\nexcept ImportError:\n    from ..externals.funcsigs import signature\n\n\ndef _parse_version(version_string):\n    version = []\n    for x in version_string.split('.'):\n        try:\n            version.append(int(x))\n        except ValueError:\n            # x may be of the form dev-1ea1592\n            version.append(x)\n    return tuple(version)\n\n\neuler_gamma = getattr(np, 'euler_gamma',\n                      0.577215664901532860606512090082402431)\n\nnp_version = _parse_version(np.__version__)\nsp_version = _parse_version(scipy.__version__)\nPY3_OR_LATER = sys.version_info[0] >= 3\n\n\n# Remove when minimum required NumPy >= 1.10\ntry:\n    if (not np.allclose(np.divide(.4, 1, casting=\"unsafe\"),\n                        np.divide(.4, 1, casting=\"unsafe\", dtype=np.float64))\n            or not np.allclose(np.divide(.4, 1), .4)):\n        raise TypeError('Divide not working with dtype: '\n                        'https://github.com/numpy/numpy/issues/3484')\n    divide = np.divide\n\nexcept TypeError:\n    # Compat for old versions of np.divide that do not provide support for\n    # the dtype args\n    def divide(x1, x2, out=None, dtype=None):\n        out_orig = out\n        if out is None:\n            out = np.asarray(x1, dtype=dtype)\n            if out is x1:\n                out = x1.copy()\n        else:\n            if out is not x1:\n                out[:] = x1\n        if dtype is not None and out.dtype != dtype:\n            out = out.astype(dtype)\n        out /= x2\n        if out_orig is None and np.isscalar(x1):\n            out = np.asscalar(out)\n        return out\n\n\ntry:\n    with warnings.catch_warnings(record=True):\n        # Don't raise the numpy deprecation warnings that appear in\n        # 1.9, but avoid Python bug due to simplefilter('ignore')\n        warnings.simplefilter('always')\n        sp.csr_matrix([1.0, 2.0, 3.0]).max(axis=0)\nexcept (TypeError, AttributeError):\n    # in scipy < 0.14.0, sparse matrix min/max doesn't accept `axis` argument\n    # the following code is taken from the scipy 0.14 codebase\n\n    def _minor_reduce(X, ufunc):\n        major_index = np.flatnonzero(np.diff(X.indptr))\n        value = ufunc.reduceat(X.data, X.indptr[major_index])\n        return major_index, value\n\n    def _min_or_max_axis(X, axis, min_or_max):\n        N = X.shape[axis]\n        if N == 0:\n            raise ValueError(\"zero-size array to reduction operation\")\n        M = X.shape[1 - axis]\n        mat = X.tocsc() if axis == 0 else X.tocsr()\n        mat.sum_duplicates()\n        major_index, value = _minor_reduce(mat, min_or_max)\n        not_full = np.diff(mat.indptr)[major_index] < N\n        value[not_full] = min_or_max(value[not_full], 0)\n        mask = value != 0\n        major_index = np.compress(mask, major_index)\n        value = np.compress(mask, value)\n\n        from scipy.sparse import coo_matrix\n        if axis == 0:\n            res = coo_matrix((value, (np.zeros(len(value)), major_index)),\n                             dtype=X.dtype, shape=(1, M))\n        else:\n            res = coo_matrix((value, (major_index, np.zeros(len(value)))),\n                             dtype=X.dtype, shape=(M, 1))\n        return res.A.ravel()\n\n    def _sparse_min_or_max(X, axis, min_or_max):\n        if axis is None:\n            if 0 in X.shape:\n                raise ValueError(\"zero-size array to reduction operation\")\n            zero = X.dtype.type(0)\n            if X.nnz == 0:\n                return zero\n            m = min_or_max.reduce(X.data.ravel())\n            if X.nnz != np.product(X.shape):\n                m = min_or_max(zero, m)\n            return m\n        if axis < 0:\n            axis += 2\n        if (axis == 0) or (axis == 1):\n            return _min_or_max_axis(X, axis, min_or_max)\n        else:\n            raise ValueError(\"invalid axis, use 0 for rows, or 1 for columns\")\n\n    def sparse_min_max(X, axis):\n        return (_sparse_min_or_max(X, axis, np.minimum),\n                _sparse_min_or_max(X, axis, np.maximum))\n\nelse:\n    def sparse_min_max(X, axis):\n        return (X.min(axis=axis).toarray().ravel(),\n                X.max(axis=axis).toarray().ravel())\n\n\nif sp_version < (0, 15):\n    # Backport fix for scikit-learn/scikit-learn#2986 / scipy/scipy#4142\n    from ._scipy_sparse_lsqr_backport import lsqr as sparse_lsqr\nelse:\n    from scipy.sparse.linalg import lsqr as sparse_lsqr  # noqa\n\n\ntry:  # SciPy >= 0.19\n    from scipy.special import comb, logsumexp\nexcept ImportError:\n    from scipy.misc import comb, logsumexp  # noqa\n\n\nif sp_version >= (0, 19):\n    def _argmax(arr_or_spmatrix, axis=None):\n        return arr_or_spmatrix.argmax(axis=axis)\nelse:\n    # Backport of argmax functionality from scipy 0.19.1, can be removed\n    # once support for scipy 0.18 and below is dropped\n\n    def _find_missing_index(ind, n):\n        for k, a in enumerate(ind):\n            if k != a:\n                return k\n\n        k += 1\n        if k < n:\n            return k\n        else:\n            return -1\n\n    def _arg_min_or_max_axis(self, axis, op, compare):\n        if self.shape[axis] == 0:\n            raise ValueError(\"Can't apply the operation along a zero-sized \"\n                             \"dimension.\")\n\n        if axis < 0:\n            axis += 2\n\n        zero = self.dtype.type(0)\n\n        mat = self.tocsc() if axis == 0 else self.tocsr()\n        mat.sum_duplicates()\n\n        ret_size, line_size = mat._swap(mat.shape)\n        ret = np.zeros(ret_size, dtype=int)\n\n        nz_lines, = np.nonzero(np.diff(mat.indptr))\n        for i in nz_lines:\n            p, q = mat.indptr[i:i + 2]\n            data = mat.data[p:q]\n            indices = mat.indices[p:q]\n            am = op(data)\n            m = data[am]\n            if compare(m, zero) or q - p == line_size:\n                ret[i] = indices[am]\n            else:\n                zero_ind = _find_missing_index(indices, line_size)\n                if m == zero:\n                    ret[i] = min(am, zero_ind)\n                else:\n                    ret[i] = zero_ind\n\n        if axis == 1:\n            ret = ret.reshape(-1, 1)\n\n        return np.asmatrix(ret)\n\n    def _arg_min_or_max(self, axis, out, op, compare):\n        if out is not None:\n            raise ValueError(\"Sparse matrices do not support \"\n                             \"an 'out' parameter.\")\n\n        # validateaxis(axis)\n\n        if axis is None:\n            if 0 in self.shape:\n                raise ValueError(\"Can't apply the operation to \"\n                                 \"an empty matrix.\")\n\n            if self.nnz == 0:\n                return 0\n            else:\n                zero = self.dtype.type(0)\n                mat = self.tocoo()\n                mat.sum_duplicates()\n                am = op(mat.data)\n                m = mat.data[am]\n\n                if compare(m, zero):\n                    return mat.row[am] * mat.shape[1] + mat.col[am]\n                else:\n                    size = np.product(mat.shape)\n                    if size == mat.nnz:\n                        return am\n                    else:\n                        ind = mat.row * mat.shape[1] + mat.col\n                        zero_ind = _find_missing_index(ind, size)\n                        if m == zero:\n                            return min(zero_ind, am)\n                        else:\n                            return zero_ind\n\n        return _arg_min_or_max_axis(self, axis, op, compare)\n\n    def _sparse_argmax(self, axis=None, out=None):\n        return _arg_min_or_max(self, axis, out, np.argmax, np.greater)\n\n    def _argmax(arr_or_matrix, axis=None):\n        if sp.issparse(arr_or_matrix):\n            return _sparse_argmax(arr_or_matrix, axis=axis)\n        else:\n            return arr_or_matrix.argmax(axis=axis)\n\n\ndef parallel_helper(obj, methodname, *args, **kwargs):\n    \"\"\"Workaround for Python 2 limitations of pickling instance methods\"\"\"\n    return getattr(obj, methodname)(*args, **kwargs)\n\n\nif 'exist_ok' in signature(os.makedirs).parameters:\n    makedirs = os.makedirs\nelse:\n    def makedirs(name, mode=0o777, exist_ok=False):\n        \"\"\"makedirs(name [, mode=0o777][, exist_ok=False])\n\n        Super-mkdir; create a leaf directory and all intermediate ones.  Works\n        like mkdir, except that any intermediate path segment (not just the\n        rightmost) will be created if it does not exist. If the target\n        directory already exists, raise an OSError if exist_ok is False.\n        Otherwise no exception is raised.  This is recursive.\n\n        \"\"\"\n\n        try:\n            os.makedirs(name, mode=mode)\n        except OSError as e:\n            if (not exist_ok or e.errno != errno.EEXIST\n                    or not os.path.isdir(name)):\n                raise\n\n\nif np_version < (1, 12):\n    class MaskedArray(np.ma.MaskedArray):\n        # Before numpy 1.12, np.ma.MaskedArray object is not picklable\n        # This fix is needed to make our model_selection.GridSearchCV\n        # picklable as the ``cv_results_`` param uses MaskedArray\n        def __getstate__(self):\n            \"\"\"Return the internal state of the masked array, for pickling\n            purposes.\n\n            \"\"\"\n            cf = 'CF'[self.flags.fnc]\n            data_state = super(np.ma.MaskedArray, self).__reduce__()[2]\n            return data_state + (np.ma.getmaskarray(self).tostring(cf),\n                                 self._fill_value)\nelse:\n    from numpy.ma import MaskedArray    # noqa\n\n\nif np_version < (1, 11):\n    def nanpercentile(a, q):\n        \"\"\"\n        Compute the qth percentile of the data along the specified axis,\n        while ignoring nan values.\n\n        Returns the qth percentile(s) of the array elements.\n\n        Parameters\n        ----------\n        a : array_like\n            Input array or object that can be converted to an array.\n        q : float in range of [0,100] (or sequence of floats)\n            Percentile to compute, which must be between 0 and 100\n            inclusive.\n\n        Returns\n        -------\n        percentile : scalar or ndarray\n            If `q` is a single percentile and `axis=None`, then the result\n            is a scalar. If multiple percentiles are given, first axis of\n            the result corresponds to the percentiles. The other axes are\n            the axes that remain after the reduction of `a`. If the input\n            contains integers or floats smaller than ``float64``, the output\n            data-type is ``float64``. Otherwise, the output data-type is the\n            same as that of the input. If `out` is specified, that array is\n            returned instead.\n\n        \"\"\"\n        data = np.compress(~np.isnan(a), a)\n        if data.size:\n            return np.percentile(data, q)\n        else:\n            size_q = 1 if np.isscalar(q) else len(q)\n            return np.array([np.nan] * size_q)\nelse:\n    from numpy import nanpercentile  # noqa\n"
    }
  ],
  "questions": [
    "Hi guys, I'm looking at this right now.\r\n\r\nFew questions to help me get this done:\r\n\r\n- what are the relevant information from the `get_blas_info` that need to be printed ? \r\n  unfortunately the compilation information is printed (probably through `cythonize`) but not returned.\r\n- shall I restrict the printed python libraries to the main ones ? \r\n  I'd suggest `numpy`, `scipy`, `pandas`, `matplotlib`, `Cython`, `pip`, `setuptools`, `pytest`.\r\n- is `sklearn/utils` the right place to put it ?",
    "> The problem with doing this is that it won't be runnable before 0.20!​\r\n\r\nGood point we can modify only the rst doc for now and delay the changes in .md until the release.\r\n\r\nNot an expert, but I think all the `get_blas_info` is useful. About the dependencies, I am not sure look at what pandas is doing and do something similar (they have things about sys.executable, 32bit vs 64bit, bitness which may be useful). It would be good to keep it as short as possible. For example I am not convinced `pytest` makes sense.\r\n\r\n> is sklearn/utils the right place to put it ?\r\n\r\nYou can probably put the code in `sklearn/utils`. I would be in favour of making it accessible at from the root namespace so that you can do `from sklearn import show_versions`",
    "+1 for adding show_versions.\r\nMaybe optionally include the blas stuff?"
  ],
  "golden_answers": [
    "> The problem with doing this is that it won't be runnable before 0.20!​\r\n\r\nGood point we can modify only the rst doc for now and delay the changes in .md until the release.\r\n\r\nNot an expert, but I think all the `get_blas_info` is useful. About the dependencies, I am not sure look at what pandas is doing and do something similar (they have things about sys.executable, 32bit vs 64bit, bitness which may be useful). It would be good to keep it as short as possible. For example I am not convinced `pytest` makes sense.\r\n\r\n> is sklearn/utils the right place to put it ?\r\n\r\nYou can probably put the code in `sklearn/utils`. I would be in favour of making it accessible at from the root namespace so that you can do `from sklearn import show_versions`",
    "> Good point we can modify only the rst doc for now and delay the changes in .md until the release.\r\n\r\nWill do.\r\n\r\n> I would be in favour of making it accessible at from the root namespace so that you can do from `sklearn import show_versions`\r\n\r\nAgreed. I would have suggested to add `from sklearn.utils import show_versions` in `sklearn.__init__.py` instead of putting the file there. Does that sound ok ?\r\n\r\n> Maybe optionally include the blas stuff?\r\n\r\nI wrote it as an option for now.",
    "> Good point we can modify only the rst doc for now and delay the changes in .md until the release.\r\n\r\nWill do.\r\n\r\n> I would be in favour of making it accessible at from the root namespace so that you can do from `sklearn import show_versions`\r\n\r\nAgreed. I would have suggested to add `from sklearn.utils import show_versions` in `sklearn.__init__.py` instead of putting the file there. Does that sound ok ?\r\n\r\n> Maybe optionally include the blas stuff?\r\n\r\nI wrote it as an option for now."
  ],
  "questions_generated": [
    "What functionality is being proposed in the issue for the scikit-learn repository?",
    "Why is it important to include BLAS information in the `sklearn.show_versions()` output?",
    "How does the proposed `sklearn.show_versions()` function relate to the existing scikit-learn issue template?",
    "What are some of the specific technical details that the `sklearn.show_versions()` function should include according to the discussion?",
    "Why is there a concern about the `sklearn.show_versions()` function not being runnable before version 0.20?"
  ],
  "golden_answers_generated": [
    "The issue proposes adding a function `sklearn.show_versions()` that would provide detailed version and configuration information similar to `pandas.show_versions()`. This would include information relevant for debugging, such as BLAS details, to help diagnose numeric issues related to specific BLAS bindings used by numpy.",
    "Including BLAS information is important because some numeric issues can be related to the specific BLAS implementation that numpy is using. By providing this information, it makes it easier for users and developers to debug and report issues related to numerical computations in scikit-learn.",
    "The proposed `sklearn.show_versions()` function is intended to be integrated with the scikit-learn issue template, allowing users to easily provide detailed system and library version information when reporting issues. This aligns with the suggestion to modify the `ISSUE_TEMPLATE.md` to include the output of `sklearn.show_versions()` for more effective debugging and maintenance.",
    "According to the discussion, the `sklearn.show_versions()` function should include BLAS information, whether 'conda' is in the system path, and details about the joblib version being used (bundled or unbundled). These details are deemed useful for debugging and ensuring the reproducibility of results.",
    "The concern about the `sklearn.show_versions()` function not being runnable before version 0.20 likely stems from the fact that the necessary code changes and functionalities needed to support this feature are not available in versions prior to 0.20. This means that users on older versions would not be able to utilize this functionality until they upgrade to version 0.20 or newer."
  ]
}