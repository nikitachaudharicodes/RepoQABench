{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "60563",
  "issue_description": "# BUG: value_counts() returns error/wrong result with PyArrow categorical columns with nulls\n\n### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\n# First case: just one column. It gives the error below\r\npd.DataFrame( { 'A': [ 'a1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) ).value_counts( dropna = False )\r\n\r\n# Second case: more than one column. It gives the wrong result below\r\npd.concat( [\r\n    pd.DataFrame( { 'A': [ 'a1', 'a2' ], 'B': [ 'b1', pd.NA ] }, dtype = pd.ArrowDtype( pa.string() ) ),\r\n    pd.DataFrame( { 'C': [ 'c1', 'c2' ], 'D': [ 'd1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) )\r\n], axis = 1 ).value_counts( dropna = False )\n```\n\n\n### Issue Description\n\n### First Case\r\nIt gives the following error:\r\n\r\n```python-traceback\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[6], line 1\r\n----> 1 pd.DataFrame( { 'A': [ 'a1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) ).value_counts( dropna = False )\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\frame.py:7519, in DataFrame.value_counts(self, subset, normalize, sort, ascending, dropna)\r\n   7517 # Force MultiIndex for a list_like subset with a single column\r\n   7518 if is_list_like(subset) and len(subset) == 1:  # type: ignore[arg-type]\r\n-> 7519     counts.index = MultiIndex.from_arrays(\r\n   7520         [counts.index], names=[counts.index.name]\r\n   7521     )\r\n   7523 return counts\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:533, in MultiIndex.from_arrays(cls, arrays, sortorder, names)\r\n    530     if len(arrays[i]) != len(arrays[i - 1]):\r\n    531         raise ValueError(\"all arrays must be same length\")\r\n--> 533 codes, levels = factorize_from_iterables(arrays)\r\n    534 if names is lib.no_default:\r\n    535     names = [getattr(arr, \"name\", None) for arr in arrays]\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:3069, in factorize_from_iterables(iterables)\r\n   3065 if len(iterables) == 0:\r\n   3066     # For consistency, it should return two empty lists.\r\n   3067     return [], []\r\n-> 3069 codes, categories = zip(*(factorize_from_iterable(it) for it in iterables))\r\n   3070 return list(codes), list(categories)\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:3069, in <genexpr>(.0)\r\n   3065 if len(iterables) == 0:\r\n   3066     # For consistency, it should return two empty lists.\r\n   3067     return [], []\r\n-> 3069 codes, categories = zip(*(factorize_from_iterable(it) for it in iterables))\r\n   3070 return list(codes), list(categories)\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:3042, in factorize_from_iterable(values)\r\n   3037     codes = values.codes\r\n   3038 else:\r\n   3039     # The value of ordered is irrelevant since we don't use cat as such,\r\n   3040     # but only the resulting categories, the order of which is independent\r\n   3041     # from ordered. Set ordered to False as default. See GH #15457\r\n-> 3042     cat = Categorical(values, ordered=False)\r\n   3043     categories = cat.categories\r\n   3044     codes = cat.codes\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:451, in Categorical.__init__(self, values, categories, ordered, dtype, fastpath, copy)\r\n    447 if dtype.categories is None:\r\n    448     if isinstance(values.dtype, ArrowDtype) and issubclass(\r\n    449         values.dtype.type, CategoricalDtypeType\r\n    450     ):\r\n--> 451         arr = values._pa_array.combine_chunks()\r\n    452         categories = arr.dictionary.to_pandas(types_mapper=ArrowDtype)\r\n    453         codes = arr.indices.to_numpy()\r\n\r\nAttributeError: 'Index' object has no attribute '_pa_array'\r\n```\r\nIndeed, the same error is returned also if no `pd.NA` is present.\r\n\r\n### Second case\r\nIt gives the following result:\r\n\r\n```python\r\nA   B     C   D \r\na1  b1    c1  d1    1\r\na2  <NA>  c2  d1    1\r\nName: count, dtype: int64\r\n```\r\n\r\n**Note that in second line D is d1 and not `<NA>`.**\r\n\r\nA more complete example in this JupyterLab notebook: [value_counts() Bug.pdf](https://github.com/user-attachments/files/18133225/value_counts.Bug.pdf)\r\n\n\n### Expected Behavior\n\nThe expected behavior is analogous to the result obtained with the NumPy backend.\r\n\r\n### First case\r\n\r\n```python\r\nA  \r\na1     1\r\n<NA>    1\r\nName: count, dtype: int64\r\n```\r\n\r\n### Second case\r\n\r\n```python\r\nA   B     C   D \r\na1  b1    c1  d1    1\r\na2  <NA>  c2  <NA>    1\r\nName: count, dtype: int64\r\n```\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 0691c5cf90477d3503834d983f69350f250a6ff7\r\npython                : 3.12.8\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 2019Server\r\nVersion               : 10.0.17763\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 165 Stepping 2, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : English_United States.1252\r\n\r\npandas                : 2.2.3\r\nnumpy                 : 2.1.2\r\npytz                  : 2024.2\r\ndateutil              : 2.9.0.post0\r\npip                   : 24.3.1\r\nCython                : None\r\nsphinx                : None\r\nIPython               : 8.29.0\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nblosc                 : None\r\nbottleneck            : 1.4.2\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\nhtml5lib              : None\r\nhypothesis            : None\r\ngcsfs                 : None\r\njinja2                : 3.1.4\r\nlxml.etree            : 5.3.0\r\nmatplotlib            : 3.9.2\r\nnumba                 : None\r\nnumexpr               : 2.10.1\r\nodfpy                 : None\r\nopenpyxl              : 3.1.5\r\npandas_gbq            : None\r\npsycopg2              : None\r\npymysql               : None\r\npyarrow               : 18.1.0\r\npyreadstat            : None\r\npytest                : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nxlsxwriter            : None\r\nzstandard             : 0.23.0\r\ntzdata                : 2024.2\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n</details>\r\n",
  "issue_comments": [
    {
      "id": 2543176361,
      "user": "rhshadrach",
      "body": "Thanks for the report, confirmed on main. It appears that changing\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/9501650e22767f8502a1e3edecfaf17c5769f150/pandas/core/arrays/categorical.py#L450\r\n\r\nto be \r\n\r\n```python\r\n                if isinstance(values, Index):\r\n                    arr = values._data._pa_array.combine_chunks()\r\n                else:\r\n                    arr = values._pa_array.combine_chunks()\r\n```\r\n\r\nresolves. Further investigations and PRs to fix are welcome!\r\n\r\nEven with the above fix, we still do not see NA values because of a bug in groupby. I've opened https://github.com/pandas-dev/pandas/issues/60567 for this."
    },
    {
      "id": 2543177937,
      "user": "hardeybisey",
      "body": "take"
    },
    {
      "id": 2543338107,
      "user": "NOBODIDI",
      "body": "take"
    },
    {
      "id": 2543370551,
      "user": "NOBODIDI",
      "body": "> Thanks for the report, confirmed on main. It appears that changing\r\n> \r\n> https://github.com/pandas-dev/pandas/blob/9501650e22767f8502a1e3edecfaf17c5769f150/pandas/core/arrays/categorical.py#L450\r\n> \r\n> to be\r\n> \r\n> ```python\r\n>                 if isinstance(values, Index):\r\n>                     arr = values._data._pa_array.combine_chunks()\r\n>                 else:\r\n>                     arr = values._pa_array.combine_chunks()\r\n> ```\r\n> \r\n> resolves. Further investigations and PRs to fix are welcome!\r\n> \r\n> Even with the above fix, we still do not see NA values because of a bug in groupby. I've opened #60567 for this.\r\n\r\nI used this method but instead I did: \r\n```python \r\nif values.__class__.__name__ == 'Index':\r\n```\r\nso that Index does not need to be imported, this version fixed the issue. I am open to feedback."
    },
    {
      "id": 2543545847,
      "user": "hardeybisey",
      "body": "Hi @NOBODIDI , I noticed you’ve already submitted a PR for this issue. I started working on it and was planning to submit mine by morning my time. In the future, it would be great if we could sync up to avoid overlaps by confirming whether the issue is still being actively worked on, especially when it’s been recently assigned."
    },
    {
      "id": 2561744590,
      "user": "yanweiSu",
      "body": "take"
    },
    {
      "id": 2561761769,
      "user": "ghost",
      "body": "Gio phai làm sao. Mai ra vcbank đang kí lại tk va sinh trac\r\n"
    },
    {
      "id": 2561902966,
      "user": "ghost",
      "body": "Vào Th 4, 25 thg 12, 2024 lúc 16:53 Thịnh Lương ***@***.***>\r\nđã viết:\r\n\r\n> _xác nhập/thông tin. @TandT&All. -you like. #phone:1:9:4+:2:8:2/0ne/. And.\r\n> Family<one:(-)\r\n"
    },
    {
      "id": 2663406379,
      "user": "chilin0525",
      "body": "The bug can still be reproduced on the current main branch. I will continue the work from https://github.com/pandas-dev/pandas/pull/60569/files to attempt a fix."
    },
    {
      "id": 2663406767,
      "user": "chilin0525",
      "body": "take"
    }
  ],
  "text_context": "# BUG: value_counts() returns error/wrong result with PyArrow categorical columns with nulls\n\n### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\n# First case: just one column. It gives the error below\r\npd.DataFrame( { 'A': [ 'a1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) ).value_counts( dropna = False )\r\n\r\n# Second case: more than one column. It gives the wrong result below\r\npd.concat( [\r\n    pd.DataFrame( { 'A': [ 'a1', 'a2' ], 'B': [ 'b1', pd.NA ] }, dtype = pd.ArrowDtype( pa.string() ) ),\r\n    pd.DataFrame( { 'C': [ 'c1', 'c2' ], 'D': [ 'd1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) )\r\n], axis = 1 ).value_counts( dropna = False )\n```\n\n\n### Issue Description\n\n### First Case\r\nIt gives the following error:\r\n\r\n```python-traceback\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[6], line 1\r\n----> 1 pd.DataFrame( { 'A': [ 'a1', pd.NA ] }, dtype = pd.ArrowDtype( pa.dictionary( pa.int32(), pa.utf8() ) ) ).value_counts( dropna = False )\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\frame.py:7519, in DataFrame.value_counts(self, subset, normalize, sort, ascending, dropna)\r\n   7517 # Force MultiIndex for a list_like subset with a single column\r\n   7518 if is_list_like(subset) and len(subset) == 1:  # type: ignore[arg-type]\r\n-> 7519     counts.index = MultiIndex.from_arrays(\r\n   7520         [counts.index], names=[counts.index.name]\r\n   7521     )\r\n   7523 return counts\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:533, in MultiIndex.from_arrays(cls, arrays, sortorder, names)\r\n    530     if len(arrays[i]) != len(arrays[i - 1]):\r\n    531         raise ValueError(\"all arrays must be same length\")\r\n--> 533 codes, levels = factorize_from_iterables(arrays)\r\n    534 if names is lib.no_default:\r\n    535     names = [getattr(arr, \"name\", None) for arr in arrays]\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:3069, in factorize_from_iterables(iterables)\r\n   3065 if len(iterables) == 0:\r\n   3066     # For consistency, it should return two empty lists.\r\n   3067     return [], []\r\n-> 3069 codes, categories = zip(*(factorize_from_iterable(it) for it in iterables))\r\n   3070 return list(codes), list(categories)\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:3069, in <genexpr>(.0)\r\n   3065 if len(iterables) == 0:\r\n   3066     # For consistency, it should return two empty lists.\r\n   3067     return [], []\r\n-> 3069 codes, categories = zip(*(factorize_from_iterable(it) for it in iterables))\r\n   3070 return list(codes), list(categories)\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:3042, in factorize_from_iterable(values)\r\n   3037     codes = values.codes\r\n   3038 else:\r\n   3039     # The value of ordered is irrelevant since we don't use cat as such,\r\n   3040     # but only the resulting categories, the order of which is independent\r\n   3041     # from ordered. Set ordered to False as default. See GH #15457\r\n-> 3042     cat = Categorical(values, ordered=False)\r\n   3043     categories = cat.categories\r\n   3044     codes = cat.codes\r\n\r\nFile C:\\Python\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:451, in Categorical.__init__(self, values, categories, ordered, dtype, fastpath, copy)\r\n    447 if dtype.categories is None:\r\n    448     if isinstance(values.dtype, ArrowDtype) and issubclass(\r\n    449         values.dtype.type, CategoricalDtypeType\r\n    450     ):\r\n--> 451         arr = values._pa_array.combine_chunks()\r\n    452         categories = arr.dictionary.to_pandas(types_mapper=ArrowDtype)\r\n    453         codes = arr.indices.to_numpy()\r\n\r\nAttributeError: 'Index' object has no attribute '_pa_array'\r\n```\r\nIndeed, the same error is returned also if no `pd.NA` is present.\r\n\r\n### Second case\r\nIt gives the following result:\r\n\r\n```python\r\nA   B     C   D \r\na1  b1    c1  d1    1\r\na2  <NA>  c2  d1    1\r\nName: count, dtype: int64\r\n```\r\n\r\n**Note that in second line D is d1 and not `<NA>`.**\r\n\r\nA more complete example in this JupyterLab notebook: [value_counts() Bug.pdf](https://github.com/user-attachments/files/18133225/value_counts.Bug.pdf)\r\n\n\n### Expected Behavior\n\nThe expected behavior is analogous to the result obtained with the NumPy backend.\r\n\r\n### First case\r\n\r\n```python\r\nA  \r\na1     1\r\n<NA>    1\r\nName: count, dtype: int64\r\n```\r\n\r\n### Second case\r\n\r\n```python\r\nA   B     C   D \r\na1  b1    c1  d1    1\r\na2  <NA>  c2  <NA>    1\r\nName: count, dtype: int64\r\n```\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 0691c5cf90477d3503834d983f69350f250a6ff7\r\npython                : 3.12.8\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 2019Server\r\nVersion               : 10.0.17763\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 165 Stepping 2, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : English_United States.1252\r\n\r\npandas                : 2.2.3\r\nnumpy                 : 2.1.2\r\npytz                  : 2024.2\r\ndateutil              : 2.9.0.post0\r\npip                   : 24.3.1\r\nCython                : None\r\nsphinx                : None\r\nIPython               : 8.29.0\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nblosc                 : None\r\nbottleneck            : 1.4.2\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\nhtml5lib              : None\r\nhypothesis            : None\r\ngcsfs                 : None\r\njinja2                : 3.1.4\r\nlxml.etree            : 5.3.0\r\nmatplotlib            : 3.9.2\r\nnumba                 : None\r\nnumexpr               : 2.10.1\r\nodfpy                 : None\r\nopenpyxl              : 3.1.5\r\npandas_gbq            : None\r\npsycopg2              : None\r\npymysql               : None\r\npyarrow               : 18.1.0\r\npyreadstat            : None\r\npytest                : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nxlsxwriter            : None\r\nzstandard             : 0.23.0\r\ntzdata                : 2024.2\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n</details>\r\n\n\nThanks for the report, confirmed on main. It appears that changing\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/9501650e22767f8502a1e3edecfaf17c5769f150/pandas/core/arrays/categorical.py#L450\r\n\r\nto be \r\n\r\n```python\r\n                if isinstance(values, Index):\r\n                    arr = values._data._pa_array.combine_chunks()\r\n                else:\r\n                    arr = values._pa_array.combine_chunks()\r\n```\r\n\r\nresolves. Further investigations and PRs to fix are welcome!\r\n\r\nEven with the above fix, we still do not see NA values because of a bug in groupby. I've opened https://github.com/pandas-dev/pandas/issues/60567 for this.\n\ntake\n\ntake\n\n> Thanks for the report, confirmed on main. It appears that changing\r\n> \r\n> https://github.com/pandas-dev/pandas/blob/9501650e22767f8502a1e3edecfaf17c5769f150/pandas/core/arrays/categorical.py#L450\r\n> \r\n> to be\r\n> \r\n> ```python\r\n>                 if isinstance(values, Index):\r\n>                     arr = values._data._pa_array.combine_chunks()\r\n>                 else:\r\n>                     arr = values._pa_array.combine_chunks()\r\n> ```\r\n> \r\n> resolves. Further investigations and PRs to fix are welcome!\r\n> \r\n> Even with the above fix, we still do not see NA values because of a bug in groupby. I've opened #60567 for this.\r\n\r\nI used this method but instead I did: \r\n```python \r\nif values.__class__.__name__ == 'Index':\r\n```\r\nso that Index does not need to be imported, this version fixed the issue. I am open to feedback.\n\nHi @NOBODIDI , I noticed you’ve already submitted a PR for this issue. I started working on it and was planning to submit mine by morning my time. In the future, it would be great if we could sync up to avoid overlaps by confirming whether the issue is still being actively worked on, especially when it’s been recently assigned.\n\ntake\n\nGio phai làm sao. Mai ra vcbank đang kí lại tk va sinh trac\r\n\n\nVào Th 4, 25 thg 12, 2024 lúc 16:53 Thịnh Lương ***@***.***>\r\nđã viết:\r\n\r\n> _xác nhập/thông tin. @TandT&All. -you like. #phone:1:9:4+:2:8:2/0ne/. And.\r\n> Family<one:(-)\r\n\n\nThe bug can still be reproduced on the current main branch. I will continue the work from https://github.com/pandas-dev/pandas/pull/60569/files to attempt a fix.\n\ntake",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/60949",
  "code_context": [
    {
      "filename": "pandas/core/arrays/categorical.py",
      "content": "from __future__ import annotations\n\nfrom csv import QUOTE_NONNUMERIC\nfrom functools import partial\nimport operator\nfrom shutil import get_terminal_size\nfrom typing import (\n    TYPE_CHECKING,\n    Literal,\n    cast,\n    overload,\n)\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import (\n    NaT,\n    algos as libalgos,\n    lib,\n)\nfrom pandas._libs.arrays import NDArrayBacked\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._validators import validate_bool_kwarg\n\nfrom pandas.core.dtypes.cast import (\n    coerce_indexer_dtype,\n    find_common_type,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_int64,\n    ensure_platform_int,\n    is_any_real_numeric_dtype,\n    is_bool_dtype,\n    is_dict_like,\n    is_hashable,\n    is_integer_dtype,\n    is_list_like,\n    is_scalar,\n    needs_i8_conversion,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import (\n    ArrowDtype,\n    CategoricalDtype,\n    CategoricalDtypeType,\n    ExtensionDtype,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCIndex,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.missing import (\n    is_valid_na_for_dtype,\n    isna,\n)\n\nfrom pandas.core import (\n    algorithms,\n    arraylike,\n    ops,\n)\nfrom pandas.core.accessor import (\n    PandasDelegate,\n    delegate_names,\n)\nfrom pandas.core.algorithms import (\n    factorize,\n    take_nd,\n)\nfrom pandas.core.arrays._mixins import (\n    NDArrayBackedExtensionArray,\n    ravel_compat,\n)\nfrom pandas.core.base import (\n    ExtensionArray,\n    NoNewAttributesMixin,\n    PandasObject,\n)\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    extract_array,\n    sanitize_array,\n)\nfrom pandas.core.ops.common import unpack_zerodim_and_defer\nfrom pandas.core.sorting import nargsort\nfrom pandas.core.strings.object_array import ObjectStringArrayMixin\n\nfrom pandas.io.formats import console\n\nif TYPE_CHECKING:\n    from collections.abc import (\n        Callable,\n        Hashable,\n        Iterator,\n        Sequence,\n    )\n\n    from pandas._typing import (\n        ArrayLike,\n        AstypeArg,\n        AxisInt,\n        Dtype,\n        DtypeObj,\n        NpDtype,\n        Ordered,\n        Self,\n        Shape,\n        SortKind,\n        npt,\n    )\n\n    from pandas import (\n        DataFrame,\n        Index,\n        Series,\n    )\n\n\ndef _cat_compare_op(op):\n    opname = f\"__{op.__name__}__\"\n    fill_value = op is operator.ne\n\n    @unpack_zerodim_and_defer(opname)\n    def func(self, other):\n        hashable = is_hashable(other)\n        if is_list_like(other) and len(other) != len(self) and not hashable:\n            # in hashable case we may have a tuple that is itself a category\n            raise ValueError(\"Lengths must match.\")\n\n        if not self.ordered:\n            if opname in [\"__lt__\", \"__gt__\", \"__le__\", \"__ge__\"]:\n                raise TypeError(\n                    \"Unordered Categoricals can only compare equality or not\"\n                )\n        if isinstance(other, Categorical):\n            # Two Categoricals can only be compared if the categories are\n            # the same (maybe up to ordering, depending on ordered)\n\n            msg = \"Categoricals can only be compared if 'categories' are the same.\"\n            if not self._categories_match_up_to_permutation(other):\n                raise TypeError(msg)\n\n            if not self.ordered and not self.categories.equals(other.categories):\n                # both unordered and different order\n                other_codes = recode_for_categories(\n                    other.codes, other.categories, self.categories, copy=False\n                )\n            else:\n                other_codes = other._codes\n\n            ret = op(self._codes, other_codes)\n            mask = (self._codes == -1) | (other_codes == -1)\n            if mask.any():\n                ret[mask] = fill_value\n            return ret\n\n        if hashable:\n            if other in self.categories:\n                i = self._unbox_scalar(other)\n                ret = op(self._codes, i)\n\n                if opname not in {\"__eq__\", \"__ge__\", \"__gt__\"}:\n                    # GH#29820 performance trick; get_loc will always give i>=0,\n                    #  so in the cases (__ne__, __le__, __lt__) the setting\n                    #  here is a no-op, so can be skipped.\n                    mask = self._codes == -1\n                    ret[mask] = fill_value\n                return ret\n            else:\n                return ops.invalid_comparison(self, other, op)\n        else:\n            # allow categorical vs object dtype array comparisons for equality\n            # these are only positional comparisons\n            if opname not in [\"__eq__\", \"__ne__\"]:\n                raise TypeError(\n                    f\"Cannot compare a Categorical for op {opname} with \"\n                    f\"type {type(other)}.\\nIf you want to compare values, \"\n                    \"use 'np.asarray(cat) <op> other'.\"\n                )\n\n            if isinstance(other, ExtensionArray) and needs_i8_conversion(other.dtype):\n                # We would return NotImplemented here, but that messes up\n                #  ExtensionIndex's wrapped methods\n                return op(other, self)\n            return getattr(np.array(self), opname)(np.array(other))\n\n    func.__name__ = opname\n\n    return func\n\n\ndef contains(cat, key, container) -> bool:\n    \"\"\"\n    Helper for membership check for ``key`` in ``cat``.\n\n    This is a helper method for :method:`__contains__`\n    and :class:`CategoricalIndex.__contains__`.\n\n    Returns True if ``key`` is in ``cat.categories`` and the\n    location of ``key`` in ``categories`` is in ``container``.\n\n    Parameters\n    ----------\n    cat : :class:`Categorical`or :class:`categoricalIndex`\n    key : a hashable object\n        The key to check membership for.\n    container : Container (e.g. list-like or mapping)\n        The container to check for membership in.\n\n    Returns\n    -------\n    is_in : bool\n        True if ``key`` is in ``self.categories`` and location of\n        ``key`` in ``categories`` is in ``container``, else False.\n\n    Notes\n    -----\n    This method does not check for NaN values. Do that separately\n    before calling this method.\n    \"\"\"\n    hash(key)\n\n    # get location of key in categories.\n    # If a KeyError, the key isn't in categories, so logically\n    #  can't be in container either.\n    try:\n        loc = cat.categories.get_loc(key)\n    except (KeyError, TypeError):\n        return False\n\n    # loc is the location of key in categories, but also the *value*\n    # for key in container. So, `key` may be in categories,\n    # but still not in `container`. Example ('b' in categories,\n    # but not in values):\n    # 'b' in Categorical(['a'], categories=['a', 'b'])  # False\n    if is_scalar(loc):\n        return loc in container\n    else:\n        # if categories is an IntervalIndex, loc is an array.\n        return any(loc_ in container for loc_ in loc)\n\n\n# error: Definition of \"delete/ravel/T/repeat/copy\" in base class \"NDArrayBacked\"\n# is incompatible with definition in base class \"ExtensionArray\"\nclass Categorical(NDArrayBackedExtensionArray, PandasObject, ObjectStringArrayMixin):  # type: ignore[misc]\n    \"\"\"\n    Represent a categorical variable in classic R / S-plus fashion.\n\n    `Categoricals` can only take on a limited, and usually fixed, number\n    of possible values (`categories`). In contrast to statistical categorical\n    variables, a `Categorical` might have an order, but numerical operations\n    (additions, divisions, ...) are not possible.\n\n    All values of the `Categorical` are either in `categories` or `np.nan`.\n    Assigning values outside of `categories` will raise a `ValueError`. Order\n    is defined by the order of the `categories`, not lexical order of the\n    values.\n\n    Parameters\n    ----------\n    values : list-like\n        The values of the categorical. If categories are given, values not in\n        categories will be replaced with NaN.\n    categories : Index-like (unique), optional\n        The unique categories for this categorical. If not given, the\n        categories are assumed to be the unique values of `values` (sorted, if\n        possible, otherwise in the order in which they appear).\n    ordered : bool, default False\n        Whether or not this categorical is treated as a ordered categorical.\n        If True, the resulting categorical will be ordered.\n        An ordered categorical respects, when sorted, the order of its\n        `categories` attribute (which in turn is the `categories` argument, if\n        provided).\n    dtype : CategoricalDtype\n        An instance of ``CategoricalDtype`` to use for this categorical.\n    copy : bool, default True\n        Whether to copy if the codes are unchanged.\n\n    Attributes\n    ----------\n    categories : Index\n        The categories of this categorical.\n    codes : ndarray\n        The codes (integer positions, which point to the categories) of this\n        categorical, read only.\n    ordered : bool\n        Whether or not this Categorical is ordered.\n    dtype : CategoricalDtype\n        The instance of ``CategoricalDtype`` storing the ``categories``\n        and ``ordered``.\n\n    Methods\n    -------\n    from_codes\n    as_ordered\n    as_unordered\n    set_categories\n    rename_categories\n    reorder_categories\n    add_categories\n    remove_categories\n    remove_unused_categories\n    map\n    __array__\n\n    Raises\n    ------\n    ValueError\n        If the categories do not validate.\n    TypeError\n        If an explicit ``ordered=True`` is given but no `categories` and the\n        `values` are not sortable.\n\n    See Also\n    --------\n    CategoricalDtype : Type for categorical data.\n    CategoricalIndex : An Index with an underlying ``Categorical``.\n\n    Notes\n    -----\n    See the `user guide\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html>`__\n    for more.\n\n    Examples\n    --------\n    >>> pd.Categorical([1, 2, 3, 1, 2, 3])\n    [1, 2, 3, 1, 2, 3]\n    Categories (3, int64): [1, 2, 3]\n\n    >>> pd.Categorical([\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"])\n    ['a', 'b', 'c', 'a', 'b', 'c']\n    Categories (3, object): ['a', 'b', 'c']\n\n    Missing values are not included as a category.\n\n    >>> c = pd.Categorical([1, 2, 3, 1, 2, 3, np.nan])\n    >>> c\n    [1, 2, 3, 1, 2, 3, NaN]\n    Categories (3, int64): [1, 2, 3]\n\n    However, their presence is indicated in the `codes` attribute\n    by code `-1`.\n\n    >>> c.codes\n    array([ 0,  1,  2,  0,  1,  2, -1], dtype=int8)\n\n    Ordered `Categoricals` can be sorted according to the custom order\n    of the categories and can have a min and max value.\n\n    >>> c = pd.Categorical(\n    ...     [\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"], ordered=True, categories=[\"c\", \"b\", \"a\"]\n    ... )\n    >>> c\n    ['a', 'b', 'c', 'a', 'b', 'c']\n    Categories (3, object): ['c' < 'b' < 'a']\n    >>> c.min()\n    'c'\n    \"\"\"\n\n    # For comparisons, so that numpy uses our implementation if the compare\n    # ops, which raise\n    __array_priority__ = 1000\n    # tolist is not actually deprecated, just suppressed in the __dir__\n    _hidden_attrs = PandasObject._hidden_attrs | frozenset([\"tolist\"])\n    _typ = \"categorical\"\n\n    _dtype: CategoricalDtype\n\n    @classmethod\n    # error: Argument 2 of \"_simple_new\" is incompatible with supertype\n    # \"NDArrayBacked\"; supertype defines the argument type as\n    # \"Union[dtype[Any], ExtensionDtype]\"\n    def _simple_new(  # type: ignore[override]\n        cls, codes: np.ndarray, dtype: CategoricalDtype\n    ) -> Self:\n        # NB: This is not _quite_ as simple as the \"usual\" _simple_new\n        codes = coerce_indexer_dtype(codes, dtype.categories)\n        dtype = CategoricalDtype(ordered=False).update_dtype(dtype)\n        return super()._simple_new(codes, dtype)\n\n    def __init__(\n        self,\n        values,\n        categories=None,\n        ordered=None,\n        dtype: Dtype | None = None,\n        copy: bool = True,\n    ) -> None:\n        dtype = CategoricalDtype._from_values_or_dtype(\n            values, categories, ordered, dtype\n        )\n        # At this point, dtype is always a CategoricalDtype, but\n        # we may have dtype.categories be None, and we need to\n        # infer categories in a factorization step further below\n\n        if not is_list_like(values):\n            # GH#38433\n            raise TypeError(\"Categorical input must be list-like\")\n\n        # null_mask indicates missing values we want to exclude from inference.\n        # This means: only missing values in list-likes (not arrays/ndframes).\n        null_mask = np.array(False)\n\n        # sanitize input\n        vdtype = getattr(values, \"dtype\", None)\n        if isinstance(vdtype, CategoricalDtype):\n            if dtype.categories is None:\n                dtype = CategoricalDtype(values.categories, dtype.ordered)\n        elif isinstance(values, range):\n            from pandas.core.indexes.range import RangeIndex\n\n            values = RangeIndex(values)\n        elif not isinstance(values, (ABCIndex, ABCSeries, ExtensionArray)):\n            values = com.convert_to_list_like(values)\n            if isinstance(values, list) and len(values) == 0:\n                # By convention, empty lists result in object dtype:\n                values = np.array([], dtype=object)\n            elif isinstance(values, np.ndarray):\n                if values.ndim > 1:\n                    # preempt sanitize_array from raising ValueError\n                    raise NotImplementedError(\n                        \"> 1 ndim Categorical are not supported at this time\"\n                    )\n                values = sanitize_array(values, None)\n            else:\n                # i.e. must be a list\n                arr = sanitize_array(values, None)\n                null_mask = isna(arr)\n                if null_mask.any():\n                    # We remove null values here, then below will re-insert\n                    #  them, grep \"full_codes\"\n                    arr_list = [values[idx] for idx in np.where(~null_mask)[0]]\n\n                    # GH#44900 Do not cast to float if we have only missing values\n                    if arr_list or arr.dtype == \"object\":\n                        sanitize_dtype = None\n                    else:\n                        sanitize_dtype = arr.dtype\n\n                    arr = sanitize_array(arr_list, None, dtype=sanitize_dtype)\n                values = arr\n\n        if dtype.categories is None:\n            if isinstance(values.dtype, ArrowDtype) and issubclass(\n                values.dtype.type, CategoricalDtypeType\n            ):\n                from pandas import Index\n\n                if isinstance(values, Index):\n                    arr = values._data._pa_array.combine_chunks()\n                else:\n                    arr = values._pa_array.combine_chunks()\n                categories = arr.dictionary.to_pandas(types_mapper=ArrowDtype)\n                codes = arr.indices.to_numpy()\n                dtype = CategoricalDtype(categories, values.dtype.pyarrow_dtype.ordered)\n            else:\n                if not isinstance(values, ABCIndex):\n                    # in particular RangeIndex xref test_index_equal_range_categories\n                    values = sanitize_array(values, None)\n                try:\n                    codes, categories = factorize(values, sort=True)\n                except TypeError as err:\n                    codes, categories = factorize(values, sort=False)\n                    if dtype.ordered:\n                        # raise, as we don't have a sortable data structure and so\n                        # the user should give us one by specifying categories\n                        raise TypeError(\n                            \"'values' is not ordered, please \"\n                            \"explicitly specify the categories order \"\n                            \"by passing in a categories argument.\"\n                        ) from err\n\n                # we're inferring from values\n                dtype = CategoricalDtype(categories, dtype.ordered)\n\n        elif isinstance(values.dtype, CategoricalDtype):\n            old_codes = extract_array(values)._codes\n            codes = recode_for_categories(\n                old_codes, values.dtype.categories, dtype.categories, copy=copy\n            )\n\n        else:\n            codes = _get_codes_for_values(values, dtype.categories)\n\n        if null_mask.any():\n            # Reinsert -1 placeholders for previously removed missing values\n            full_codes = -np.ones(null_mask.shape, dtype=codes.dtype)\n            full_codes[~null_mask] = codes\n            codes = full_codes\n\n        dtype = CategoricalDtype(ordered=False).update_dtype(dtype)\n        arr = coerce_indexer_dtype(codes, dtype.categories)\n        super().__init__(arr, dtype)\n\n    @property\n    def dtype(self) -> CategoricalDtype:\n        \"\"\"\n        The :class:`~pandas.api.types.CategoricalDtype` for this instance.\n\n        See Also\n        --------\n        astype : Cast argument to a specified dtype.\n        CategoricalDtype : Type for categorical data.\n\n        Examples\n        --------\n        >>> cat = pd.Categorical([\"a\", \"b\"], ordered=True)\n        >>> cat\n        ['a', 'b']\n        Categories (2, object): ['a' < 'b']\n        >>> cat.dtype\n        CategoricalDtype(categories=['a', 'b'], ordered=True, categories_dtype=object)\n        \"\"\"\n        return self._dtype\n\n    @property\n    def _internal_fill_value(self) -> int:\n        # using the specific numpy integer instead of python int to get\n        #  the correct dtype back from _quantile in the all-NA case\n        dtype = self._ndarray.dtype\n        return dtype.type(-1)\n\n    @classmethod\n    def _from_sequence(\n        cls, scalars, *, dtype: Dtype | None = None, copy: bool = False\n    ) -> Self:\n        return cls(scalars, dtype=dtype, copy=copy)\n\n    @classmethod\n    def _from_scalars(cls, scalars, *, dtype: DtypeObj) -> Self:\n        if dtype is None:\n            # The _from_scalars strictness doesn't make much sense in this case.\n            raise NotImplementedError\n\n        res = cls._from_sequence(scalars, dtype=dtype)\n\n        # if there are any non-category elements in scalars, these will be\n        #  converted to NAs in res.\n        mask = isna(scalars)\n        if not (mask == res.isna()).all():\n            # Some non-category element in scalars got converted to NA in res.\n            raise ValueError\n        return res\n\n    @overload\n    def astype(self, dtype: npt.DTypeLike, copy: bool = ...) -> np.ndarray: ...\n\n    @overload\n    def astype(self, dtype: ExtensionDtype, copy: bool = ...) -> ExtensionArray: ...\n\n    @overload\n    def astype(self, dtype: AstypeArg, copy: bool = ...) -> ArrayLike: ...\n\n    def astype(self, dtype: AstypeArg, copy: bool = True) -> ArrayLike:\n        \"\"\"\n        Coerce this type to another dtype\n\n        Parameters\n        ----------\n        dtype : numpy dtype or pandas type\n        copy : bool, default True\n            By default, astype always returns a newly allocated object.\n            If copy is set to False and dtype is categorical, the original\n            object is returned.\n        \"\"\"\n        dtype = pandas_dtype(dtype)\n        result: Categorical | np.ndarray\n        if self.dtype is dtype:\n            result = self.copy() if copy else self\n\n        elif isinstance(dtype, CategoricalDtype):\n            # GH 10696/18593/18630\n            dtype = self.dtype.update_dtype(dtype)\n            self = self.copy() if copy else self\n            result = self._set_dtype(dtype)\n\n        elif isinstance(dtype, ExtensionDtype):\n            return super().astype(dtype, copy=copy)\n\n        elif dtype.kind in \"iu\" and self.isna().any():\n            raise ValueError(\"Cannot convert float NaN to integer\")\n\n        elif len(self.codes) == 0 or len(self.categories) == 0:\n            # For NumPy 1.x compatibility we cannot use copy=None.  And\n            # `copy=False` has the meaning of `copy=None` here:\n            if not copy:\n                result = np.asarray(self, dtype=dtype)\n            else:\n                result = np.array(self, dtype=dtype)\n\n        else:\n            # GH8628 (PERF): astype category codes instead of astyping array\n            new_cats = self.categories._values\n\n            try:\n                new_cats = new_cats.astype(dtype=dtype, copy=copy)\n                fill_value = self.categories._na_value\n                if not is_valid_na_for_dtype(fill_value, dtype):\n                    fill_value = lib.item_from_zerodim(\n                        np.array(self.categories._na_value).astype(dtype)\n                    )\n            except (\n                TypeError,  # downstream error msg for CategoricalIndex is misleading\n                ValueError,\n            ) as err:\n                msg = f\"Cannot cast {self.categories.dtype} dtype to {dtype}\"\n                raise ValueError(msg) from err\n\n            result = take_nd(\n                new_cats, ensure_platform_int(self._codes), fill_value=fill_value\n            )\n\n        return result\n\n    @classmethod\n    def _from_inferred_categories(\n        cls, inferred_categories, inferred_codes, dtype, true_values=None\n    ) -> Self:\n        \"\"\"\n        Construct a Categorical from inferred values.\n\n        For inferred categories (`dtype` is None) the categories are sorted.\n        For explicit `dtype`, the `inferred_categories` are cast to the\n        appropriate type.\n\n        Parameters\n        ----------\n        inferred_categories : Index\n        inferred_codes : Index\n        dtype : CategoricalDtype or 'category'\n        true_values : list, optional\n            If none are provided, the default ones are\n            \"True\", \"TRUE\", and \"true.\"\n\n        Returns\n        -------\n        Categorical\n        \"\"\"\n        from pandas import (\n            Index,\n            to_datetime,\n            to_numeric,\n            to_timedelta,\n        )\n\n        cats = Index(inferred_categories)\n        known_categories = (\n            isinstance(dtype, CategoricalDtype) and dtype.categories is not None\n        )\n\n        if known_categories:\n            # Convert to a specialized type with `dtype` if specified.\n            if is_any_real_numeric_dtype(dtype.categories.dtype):\n                cats = to_numeric(inferred_categories, errors=\"coerce\")\n            elif lib.is_np_dtype(dtype.categories.dtype, \"M\"):\n                cats = to_datetime(inferred_categories, errors=\"coerce\")\n            elif lib.is_np_dtype(dtype.categories.dtype, \"m\"):\n                cats = to_timedelta(inferred_categories, errors=\"coerce\")\n            elif is_bool_dtype(dtype.categories.dtype):\n                if true_values is None:\n                    true_values = [\"True\", \"TRUE\", \"true\"]\n\n                # error: Incompatible types in assignment (expression has type\n                # \"ndarray\", variable has type \"Index\")\n                cats = cats.isin(true_values)  # type: ignore[assignment]\n\n        if known_categories:\n            # Recode from observation order to dtype.categories order.\n            categories = dtype.categories\n            codes = recode_for_categories(inferred_codes, cats, categories)\n        elif not cats.is_monotonic_increasing:\n            # Sort categories and recode for unknown categories.\n            unsorted = cats.copy()\n            categories = cats.sort_values()\n\n            codes = recode_for_categories(inferred_codes, unsorted, categories)\n            dtype = CategoricalDtype(categories, ordered=False)\n        else:\n            dtype = CategoricalDtype(cats, ordered=False)\n            codes = inferred_codes\n\n        return cls._simple_new(codes, dtype=dtype)\n\n    @classmethod\n    def from_codes(\n        cls,\n        codes,\n        categories=None,\n        ordered=None,\n        dtype: Dtype | None = None,\n        validate: bool = True,\n    ) -> Self:\n        \"\"\"\n        Make a Categorical type from codes and categories or dtype.\n\n        This constructor is useful if you already have codes and\n        categories/dtype and so do not need the (computation intensive)\n        factorization step, which is usually done on the constructor.\n\n        If your data does not follow this convention, please use the normal\n        constructor.\n\n        Parameters\n        ----------\n        codes : array-like of int\n            An integer array, where each integer points to a category in\n            categories or dtype.categories, or else is -1 for NaN.\n        categories : index-like, optional\n            The categories for the categorical. Items need to be unique.\n            If the categories are not given here, then they must be provided\n            in `dtype`.\n        ordered : bool, optional\n            Whether or not this categorical is treated as an ordered\n            categorical. If not given here or in `dtype`, the resulting\n            categorical will be unordered.\n        dtype : CategoricalDtype or \"category\", optional\n            If :class:`CategoricalDtype`, cannot be used together with\n            `categories` or `ordered`.\n        validate : bool, default True\n            If True, validate that the codes are valid for the dtype.\n            If False, don't validate that the codes are valid. Be careful about skipping\n            validation, as invalid codes can lead to severe problems, such as segfaults.\n\n            .. versionadded:: 2.1.0\n\n        Returns\n        -------\n        Categorical\n\n        See Also\n        --------\n        codes : The category codes of the categorical.\n        CategoricalIndex : An Index with an underlying ``Categorical``.\n\n        Examples\n        --------\n        >>> dtype = pd.CategoricalDtype([\"a\", \"b\"], ordered=True)\n        >>> pd.Categorical.from_codes(codes=[0, 1, 0, 1], dtype=dtype)\n        ['a', 'b', 'a', 'b']\n        Categories (2, object): ['a' < 'b']\n        \"\"\"\n        dtype = CategoricalDtype._from_values_or_dtype(\n            categories=categories, ordered=ordered, dtype=dtype\n        )\n        if dtype.categories is None:\n            msg = (\n                \"The categories must be provided in 'categories' or \"\n                \"'dtype'. Both were None.\"\n            )\n            raise ValueError(msg)\n\n        if validate:\n            # beware: non-valid codes may segfault\n            codes = cls._validate_codes_for_dtype(codes, dtype=dtype)\n\n        return cls._simple_new(codes, dtype=dtype)\n\n    # ------------------------------------------------------------------\n    # Categories/Codes/Ordered\n\n    @property\n    def categories(self) -> Index:\n        \"\"\"\n        The categories of this categorical.\n\n        Setting assigns new values to each category (effectively a rename of\n        each individual category).\n\n        The assigned value has to be a list-like object. All items must be\n        unique and the number of items in the new categories must be the same\n        as the number of items in the old categories.\n\n        Raises\n        ------\n        ValueError\n            If the new categories do not validate as categories or if the\n            number of new categories is unequal the number of old categories\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n\n        Examples\n        --------\n        For :class:`pandas.Series`:\n\n        >>> ser = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\n        >>> ser.cat.categories\n        Index(['a', 'b', 'c'], dtype='object')\n\n        >>> raw_cat = pd.Categorical([\"a\", \"b\", \"c\", \"a\"], categories=[\"b\", \"c\", \"d\"])\n        >>> ser = pd.Series(raw_cat)\n        >>> ser.cat.categories\n        Index(['b', 'c', 'd'], dtype='object')\n\n        For :class:`pandas.Categorical`:\n\n        >>> cat = pd.Categorical([\"a\", \"b\"], ordered=True)\n        >>> cat.categories\n        Index(['a', 'b'], dtype='object')\n\n        For :class:`pandas.CategoricalIndex`:\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"c\", \"b\", \"a\", \"c\", \"b\"])\n        >>> ci.categories\n        Index(['a', 'b', 'c'], dtype='object')\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"c\"], categories=[\"c\", \"b\", \"a\"])\n        >>> ci.categories\n        Index(['c', 'b', 'a'], dtype='object')\n        \"\"\"\n        return self.dtype.categories\n\n    @property\n    def ordered(self) -> Ordered:\n        \"\"\"\n        Whether the categories have an ordered relationship.\n\n        See Also\n        --------\n        set_ordered : Set the ordered attribute.\n        as_ordered : Set the Categorical to be ordered.\n        as_unordered : Set the Categorical to be unordered.\n\n        Examples\n        --------\n        For :class:`pandas.Series`:\n\n        >>> ser = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\n        >>> ser.cat.ordered\n        False\n\n        >>> raw_cat = pd.Categorical([\"a\", \"b\", \"c\", \"a\"], ordered=True)\n        >>> ser = pd.Series(raw_cat)\n        >>> ser.cat.ordered\n        True\n\n        For :class:`pandas.Categorical`:\n\n        >>> cat = pd.Categorical([\"a\", \"b\"], ordered=True)\n        >>> cat.ordered\n        True\n\n        >>> cat = pd.Categorical([\"a\", \"b\"], ordered=False)\n        >>> cat.ordered\n        False\n\n        For :class:`pandas.CategoricalIndex`:\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"b\"], ordered=True)\n        >>> ci.ordered\n        True\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"b\"], ordered=False)\n        >>> ci.ordered\n        False\n        \"\"\"\n        return self.dtype.ordered\n\n    @property\n    def codes(self) -> np.ndarray:\n        \"\"\"\n        The category codes of this categorical index.\n\n        Codes are an array of integers which are the positions of the actual\n        values in the categories array.\n\n        There is no setter, use the other categorical methods and the normal item\n        setter to change values in the categorical.\n\n        Returns\n        -------\n        ndarray[int]\n            A non-writable view of the ``codes`` array.\n\n        See Also\n        --------\n        Categorical.from_codes : Make a Categorical from codes.\n        CategoricalIndex : An Index with an underlying ``Categorical``.\n\n        Examples\n        --------\n        For :class:`pandas.Categorical`:\n\n        >>> cat = pd.Categorical([\"a\", \"b\"], ordered=True)\n        >>> cat.codes\n        array([0, 1], dtype=int8)\n\n        For :class:`pandas.CategoricalIndex`:\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"])\n        >>> ci.codes\n        array([0, 1, 2, 0, 1, 2], dtype=int8)\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"c\"], categories=[\"c\", \"b\", \"a\"])\n        >>> ci.codes\n        array([2, 0], dtype=int8)\n        \"\"\"\n        v = self._codes.view()\n        v.flags.writeable = False\n        return v\n\n    def _set_categories(self, categories, fastpath: bool = False) -> None:\n        \"\"\"\n        Sets new categories inplace\n\n        Parameters\n        ----------\n        fastpath : bool, default False\n           Don't perform validation of the categories for uniqueness or nulls\n\n        Examples\n        --------\n        >>> c = pd.Categorical([\"a\", \"b\"])\n        >>> c\n        ['a', 'b']\n        Categories (2, object): ['a', 'b']\n\n        >>> c._set_categories(pd.Index([\"a\", \"c\"]))\n        >>> c\n        ['a', 'c']\n        Categories (2, object): ['a', 'c']\n        \"\"\"\n        if fastpath:\n            new_dtype = CategoricalDtype._from_fastpath(categories, self.ordered)\n        else:\n            new_dtype = CategoricalDtype(categories, ordered=self.ordered)\n        if (\n            not fastpath\n            and self.dtype.categories is not None\n            and len(new_dtype.categories) != len(self.dtype.categories)\n        ):\n            raise ValueError(\n                \"new categories need to have the same number of \"\n                \"items as the old categories!\"\n            )\n\n        super().__init__(self._ndarray, new_dtype)\n\n    def _set_dtype(self, dtype: CategoricalDtype) -> Self:\n        \"\"\"\n        Internal method for directly updating the CategoricalDtype\n\n        Parameters\n        ----------\n        dtype : CategoricalDtype\n\n        Notes\n        -----\n        We don't do any validation here. It's assumed that the dtype is\n        a (valid) instance of `CategoricalDtype`.\n        \"\"\"\n        codes = recode_for_categories(self.codes, self.categories, dtype.categories)\n        return type(self)._simple_new(codes, dtype=dtype)\n\n    def set_ordered(self, value: bool) -> Self:\n        \"\"\"\n        Set the ordered attribute to the boolean value.\n\n        Parameters\n        ----------\n        value : bool\n           Set whether this categorical is ordered (True) or not (False).\n        \"\"\"\n        new_dtype = CategoricalDtype(self.categories, ordered=value)\n        cat = self.copy()\n        NDArrayBacked.__init__(cat, cat._ndarray, new_dtype)\n        return cat\n\n    def as_ordered(self) -> Self:\n        \"\"\"\n        Set the Categorical to be ordered.\n\n        Returns\n        -------\n        Categorical\n            Ordered Categorical.\n\n        See Also\n        --------\n        as_unordered : Set the Categorical to be unordered.\n\n        Examples\n        --------\n        For :class:`pandas.Series`:\n\n        >>> ser = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\n        >>> ser.cat.ordered\n        False\n        >>> ser = ser.cat.as_ordered()\n        >>> ser.cat.ordered\n        True\n\n        For :class:`pandas.CategoricalIndex`:\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"b\", \"c\", \"a\"])\n        >>> ci.ordered\n        False\n        >>> ci = ci.as_ordered()\n        >>> ci.ordered\n        True\n        \"\"\"\n        return self.set_ordered(True)\n\n    def as_unordered(self) -> Self:\n        \"\"\"\n        Set the Categorical to be unordered.\n\n        Returns\n        -------\n        Categorical\n            Unordered Categorical.\n\n        See Also\n        --------\n        as_ordered : Set the Categorical to be ordered.\n\n        Examples\n        --------\n        For :class:`pandas.Series`:\n\n        >>> raw_cat = pd.Categorical([\"a\", \"b\", \"c\", \"a\"], ordered=True)\n        >>> ser = pd.Series(raw_cat)\n        >>> ser.cat.ordered\n        True\n        >>> ser = ser.cat.as_unordered()\n        >>> ser.cat.ordered\n        False\n\n        For :class:`pandas.CategoricalIndex`:\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"b\", \"c\", \"a\"], ordered=True)\n        >>> ci.ordered\n        True\n        >>> ci = ci.as_unordered()\n        >>> ci.ordered\n        False\n        \"\"\"\n        return self.set_ordered(False)\n\n    def set_categories(\n        self, new_categories, ordered=None, rename: bool = False\n    ) -> Self:\n        \"\"\"\n        Set the categories to the specified new categories.\n\n        ``new_categories`` can include new categories (which will result in\n        unused categories) or remove old categories (which results in values\n        set to ``NaN``). If ``rename=True``, the categories will simply be renamed\n        (less or more items than in old categories will result in values set to\n        ``NaN`` or in unused categories respectively).\n\n        This method can be used to perform more than one action of adding,\n        removing, and reordering simultaneously and is therefore faster than\n        performing the individual steps via the more specialised methods.\n\n        On the other hand this methods does not do checks (e.g., whether the\n        old categories are included in the new categories on a reorder), which\n        can result in surprising changes, for example when using special string\n        dtypes, which do not consider a S1 string equal to a single char\n        python string.\n\n        Parameters\n        ----------\n        new_categories : Index-like\n           The categories in new order.\n        ordered : bool, default None\n           Whether or not the categorical is treated as a ordered categorical.\n           If not given, do not change the ordered information.\n        rename : bool, default False\n           Whether or not the new_categories should be considered as a rename\n           of the old categories or as reordered categories.\n\n        Returns\n        -------\n        Categorical\n            New categories to be used, with optional ordering changes.\n\n        Raises\n        ------\n        ValueError\n            If new_categories does not validate as categories\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n\n        Examples\n        --------\n        For :class:`pandas.Series`:\n\n        >>> raw_cat = pd.Categorical(\n        ...     [\"a\", \"b\", \"c\", \"A\"], categories=[\"a\", \"b\", \"c\"], ordered=True\n        ... )\n        >>> ser = pd.Series(raw_cat)\n        >>> ser\n        0   a\n        1   b\n        2   c\n        3   NaN\n        dtype: category\n        Categories (3, object): ['a' < 'b' < 'c']\n\n        >>> ser.cat.set_categories([\"A\", \"B\", \"C\"], rename=True)\n        0   A\n        1   B\n        2   C\n        3   NaN\n        dtype: category\n        Categories (3, object): ['A' < 'B' < 'C']\n\n        For :class:`pandas.CategoricalIndex`:\n\n        >>> ci = pd.CategoricalIndex(\n        ...     [\"a\", \"b\", \"c\", \"A\"], categories=[\"a\", \"b\", \"c\"], ordered=True\n        ... )\n        >>> ci\n        CategoricalIndex(['a', 'b', 'c', nan], categories=['a', 'b', 'c'],\n                         ordered=True, dtype='category')\n\n        >>> ci.set_categories([\"A\", \"b\", \"c\"])\n        CategoricalIndex([nan, 'b', 'c', nan], categories=['A', 'b', 'c'],\n                         ordered=True, dtype='category')\n        >>> ci.set_categories([\"A\", \"b\", \"c\"], rename=True)\n        CategoricalIndex(['A', 'b', 'c', nan], categories=['A', 'b', 'c'],\n                         ordered=True, dtype='category')\n        \"\"\"\n\n        if ordered is None:\n            ordered = self.dtype.ordered\n        new_dtype = CategoricalDtype(new_categories, ordered=ordered)\n\n        cat = self.copy()\n        if rename:\n            if cat.dtype.categories is not None and len(new_dtype.categories) < len(\n                cat.dtype.categories\n            ):\n                # remove all _codes which are larger and set to -1/NaN\n                cat._codes[cat._codes >= len(new_dtype.categories)] = -1\n            codes = cat._codes\n        else:\n            codes = recode_for_categories(\n                cat.codes, cat.categories, new_dtype.categories\n            )\n        NDArrayBacked.__init__(cat, codes, new_dtype)\n        return cat\n\n    def rename_categories(self, new_categories) -> Self:\n        \"\"\"\n        Rename categories.\n\n        This method is commonly used to re-label or adjust the\n        category names in categorical data without changing the\n        underlying data. It is useful in situations where you want\n        to modify the labels used for clarity, consistency,\n        or readability.\n\n        Parameters\n        ----------\n        new_categories : list-like, dict-like or callable\n\n            New categories which will replace old categories.\n\n            * list-like: all items must be unique and the number of items in\n              the new categories must match the existing number of categories.\n\n            * dict-like: specifies a mapping from\n              old categories to new. Categories not contained in the mapping\n              are passed through and extra categories in the mapping are\n              ignored.\n\n            * callable : a callable that is called on all items in the old\n              categories and whose return values comprise the new categories.\n\n        Returns\n        -------\n        Categorical\n            Categorical with renamed categories.\n\n        Raises\n        ------\n        ValueError\n            If new categories are list-like and do not have the same number of\n            items than the current categories or do not validate as categories\n\n        See Also\n        --------\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n\n        Examples\n        --------\n        >>> c = pd.Categorical([\"a\", \"a\", \"b\"])\n        >>> c.rename_categories([0, 1])\n        [0, 0, 1]\n        Categories (2, int64): [0, 1]\n\n        For dict-like ``new_categories``, extra keys are ignored and\n        categories not in the dictionary are passed through\n\n        >>> c.rename_categories({\"a\": \"A\", \"c\": \"C\"})\n        ['A', 'A', 'b']\n        Categories (2, object): ['A', 'b']\n\n        You may also provide a callable to create the new categories\n\n        >>> c.rename_categories(lambda x: x.upper())\n        ['A', 'A', 'B']\n        Categories (2, object): ['A', 'B']\n        \"\"\"\n\n        if is_dict_like(new_categories):\n            new_categories = [\n                new_categories.get(item, item) for item in self.categories\n            ]\n        elif callable(new_categories):\n            new_categories = [new_categories(item) for item in self.categories]\n\n        cat = self.copy()\n        cat._set_categories(new_categories)\n        return cat\n\n    def reorder_categories(self, new_categories, ordered=None) -> Self:\n        \"\"\"\n        Reorder categories as specified in new_categories.\n\n        ``new_categories`` need to include all old categories and no new category\n        items.\n\n        Parameters\n        ----------\n        new_categories : Index-like\n           The categories in new order.\n        ordered : bool, optional\n           Whether or not the categorical is treated as a ordered categorical.\n           If not given, do not change the ordered information.\n\n        Returns\n        -------\n        Categorical\n            Categorical with reordered categories.\n\n        Raises\n        ------\n        ValueError\n            If the new categories do not contain all old category items or any\n            new ones\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n\n        Examples\n        --------\n        For :class:`pandas.Series`:\n\n        >>> ser = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\n        >>> ser = ser.cat.reorder_categories([\"c\", \"b\", \"a\"], ordered=True)\n        >>> ser\n        0   a\n        1   b\n        2   c\n        3   a\n        dtype: category\n        Categories (3, object): ['c' < 'b' < 'a']\n\n        >>> ser.sort_values()\n        2   c\n        1   b\n        0   a\n        3   a\n        dtype: category\n        Categories (3, object): ['c' < 'b' < 'a']\n\n        For :class:`pandas.CategoricalIndex`:\n\n        >>> ci = pd.CategoricalIndex([\"a\", \"b\", \"c\", \"a\"])\n        >>> ci\n        CategoricalIndex(['a', 'b', 'c', 'a'], categories=['a', 'b', 'c'],\n                         ordered=False, dtype='category')\n        >>> ci.reorder_categories([\"c\", \"b\", \"a\"], ordered=True)\n        CategoricalIndex(['a', 'b', 'c', 'a'], categories=['c', 'b', 'a'],\n                         ordered=True, dtype='category')\n        \"\"\"\n        if (\n            len(self.categories) != len(new_categories)\n            or not self.categories.difference(new_categories).empty\n        ):\n            raise ValueError(\n                \"items in new_categories are not the same as in old categories\"\n            )\n        return self.set_categories(new_categories, ordered=ordered)\n\n    def add_categories(self, new_categories) -> Self:\n        \"\"\"\n        Add new categories.\n\n        `new_categories` will be included at the last/highest place in the\n        categories and will be unused directly after this call.\n\n        Parameters\n        ----------\n        new_categories : category or list-like of category\n            The new categories to be included.\n\n        Returns\n        -------\n        Categorical\n            Categorical with new categories added.\n\n        Raises\n        ------\n        ValueError\n            If the new categories include old categories or do not validate as\n            categories\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        remove_categories : Remove the specified categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n\n        Examples\n        --------\n        >>> c = pd.Categorical([\"c\", \"b\", \"c\"])\n        >>> c\n        ['c', 'b', 'c']\n        Categories (2, object): ['b', 'c']\n\n        >>> c.add_categories([\"d\", \"a\"])\n        ['c', 'b', 'c']\n        Categories (4, object): ['b', 'c', 'd', 'a']\n        \"\"\"\n\n        if not is_list_like(new_categories):\n            new_categories = [new_categories]\n        already_included = set(new_categories) & set(self.dtype.categories)\n        if len(already_included) != 0:\n            raise ValueError(\n                f\"new categories must not include old categories: {already_included}\"\n            )\n\n        if hasattr(new_categories, \"dtype\"):\n            from pandas import Series\n\n            dtype = find_common_type(\n                [self.dtype.categories.dtype, new_categories.dtype]\n            )\n            new_categories = Series(\n                list(self.dtype.categories) + list(new_categories), dtype=dtype\n            )\n        else:\n            new_categories = list(self.dtype.categories) + list(new_categories)\n\n        new_dtype = CategoricalDtype(new_categories, self.ordered)\n        cat = self.copy()\n        codes = coerce_indexer_dtype(cat._ndarray, new_dtype.categories)\n        NDArrayBacked.__init__(cat, codes, new_dtype)\n        return cat\n\n    def remove_categories(self, removals) -> Self:\n        \"\"\"\n        Remove the specified categories.\n\n        The ``removals`` argument must be a subset of the current categories.\n        Any values that were part of the removed categories will be set to NaN.\n\n        Parameters\n        ----------\n        removals : category or list of categories\n           The categories which should be removed.\n\n        Returns\n        -------\n        Categorical\n            Categorical with removed categories.\n\n        Raises\n        ------\n        ValueError\n            If the removals are not contained in the categories\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_unused_categories : Remove categories which are not used.\n        set_categories : Set the categories to the specified ones.\n\n        Examples\n        --------\n        >>> c = pd.Categorical([\"a\", \"c\", \"b\", \"c\", \"d\"])\n        >>> c\n        ['a', 'c', 'b', 'c', 'd']\n        Categories (4, object): ['a', 'b', 'c', 'd']\n\n        >>> c.remove_categories([\"d\", \"a\"])\n        [NaN, 'c', 'b', 'c', NaN]\n        Categories (2, object): ['b', 'c']\n        \"\"\"\n        from pandas import Index\n\n        if not is_list_like(removals):\n            removals = [removals]\n\n        removals = Index(removals).unique().dropna()\n        new_categories = (\n            self.dtype.categories.difference(removals, sort=False)\n            if self.dtype.ordered is True\n            else self.dtype.categories.difference(removals)\n        )\n        not_included = removals.difference(self.dtype.categories)\n\n        if len(not_included) != 0:\n            not_included = set(not_included)\n            raise ValueError(f\"removals must all be in old categories: {not_included}\")\n\n        return self.set_categories(new_categories, ordered=self.ordered, rename=False)\n\n    def remove_unused_categories(self) -> Self:\n        \"\"\"\n        Remove categories which are not used.\n\n        This method is useful when working with datasets\n        that undergo dynamic changes where categories may no longer be\n        relevant, allowing to maintain a clean, efficient data structure.\n\n        Returns\n        -------\n        Categorical\n            Categorical with unused categories dropped.\n\n        See Also\n        --------\n        rename_categories : Rename categories.\n        reorder_categories : Reorder categories.\n        add_categories : Add new categories.\n        remove_categories : Remove the specified categories.\n        set_categories : Set the categories to the specified ones.\n\n        Examples\n        --------\n        >>> c = pd.Categorical([\"a\", \"c\", \"b\", \"c\", \"d\"])\n        >>> c\n        ['a', 'c', 'b', 'c', 'd']\n        Categories (4, object): ['a', 'b', 'c', 'd']\n\n        >>> c[2] = \"a\"\n        >>> c[4] = \"c\"\n        >>> c\n        ['a', 'c', 'a', 'c', 'c']\n        Categories (4, object): ['a', 'b', 'c', 'd']\n\n        >>> c.remove_unused_categories()\n        ['a', 'c', 'a', 'c', 'c']\n        Categories (2, object): ['a', 'c']\n        \"\"\"\n        idx, inv = np.unique(self._codes, return_inverse=True)\n\n        if idx.size != 0 and idx[0] == -1:  # na sentinel\n            idx, inv = idx[1:], inv - 1\n\n        new_categories = self.dtype.categories.take(idx)\n        new_dtype = CategoricalDtype._from_fastpath(\n            new_categories, ordered=self.ordered\n        )\n        new_codes = coerce_indexer_dtype(inv, new_dtype.categories)\n\n        cat = self.copy()\n        NDArrayBacked.__init__(cat, new_codes, new_dtype)\n        return cat\n\n    # ------------------------------------------------------------------\n\n    def map(\n        self,\n        mapper,\n        na_action: Literal[\"ignore\"] | None = None,\n    ):\n        \"\"\"\n        Map categories using an input mapping or function.\n\n        Maps the categories to new categories. If the mapping correspondence is\n        one-to-one the result is a :class:`~pandas.Categorical` which has the\n        same order property as the original, otherwise a :class:`~pandas.Index`\n        is returned. NaN values are unaffected.\n\n        If a `dict` or :class:`~pandas.Series` is used any unmapped category is\n        mapped to `NaN`. Note that if this happens an :class:`~pandas.Index`\n        will be returned.\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NaN values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        pandas.Categorical or pandas.Index\n            Mapped categorical.\n\n        See Also\n        --------\n        CategoricalIndex.map : Apply a mapping correspondence on a\n            :class:`~pandas.CategoricalIndex`.\n        Index.map : Apply a mapping correspondence on an\n            :class:`~pandas.Index`.\n        Series.map : Apply a mapping correspondence on a\n            :class:`~pandas.Series`.\n        Series.apply : Apply more complex functions on a\n            :class:`~pandas.Series`.\n\n        Examples\n        --------\n        >>> cat = pd.Categorical([\"a\", \"b\", \"c\"])\n        >>> cat\n        ['a', 'b', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> cat.map(lambda x: x.upper(), na_action=None)\n        ['A', 'B', 'C']\n        Categories (3, object): ['A', 'B', 'C']\n        >>> cat.map({\"a\": \"first\", \"b\": \"second\", \"c\": \"third\"}, na_action=None)\n        ['first', 'second', 'third']\n        Categories (3, object): ['first', 'second', 'third']\n\n        If the mapping is one-to-one the ordering of the categories is\n        preserved:\n\n        >>> cat = pd.Categorical([\"a\", \"b\", \"c\"], ordered=True)\n        >>> cat\n        ['a', 'b', 'c']\n        Categories (3, object): ['a' < 'b' < 'c']\n        >>> cat.map({\"a\": 3, \"b\": 2, \"c\": 1}, na_action=None)\n        [3, 2, 1]\n        Categories (3, int64): [3 < 2 < 1]\n\n        If the mapping is not one-to-one an :class:`~pandas.Index` is returned:\n\n        >>> cat.map({\"a\": \"first\", \"b\": \"second\", \"c\": \"first\"}, na_action=None)\n        Index(['first', 'second', 'first'], dtype='object')\n\n        If a `dict` is used, all unmapped categories are mapped to `NaN` and\n        the result is an :class:`~pandas.Index`:\n\n        >>> cat.map({\"a\": \"first\", \"b\": \"second\"}, na_action=None)\n        Index(['first', 'second', nan], dtype='object')\n        \"\"\"\n        assert callable(mapper) or is_dict_like(mapper)\n\n        new_categories = self.categories.map(mapper)\n\n        has_nans = np.any(self._codes == -1)\n\n        na_val = np.nan\n        if na_action is None and has_nans:\n            na_val = mapper(np.nan) if callable(mapper) else mapper.get(np.nan, np.nan)\n\n        if new_categories.is_unique and not new_categories.hasnans and na_val is np.nan:\n            new_dtype = CategoricalDtype(new_categories, ordered=self.ordered)\n            return self.from_codes(self._codes.copy(), dtype=new_dtype, validate=False)\n\n        if has_nans:\n            new_categories = new_categories.insert(len(new_categories), na_val)\n\n        return np.take(new_categories, self._codes)\n\n    __eq__ = _cat_compare_op(operator.eq)\n    __ne__ = _cat_compare_op(operator.ne)\n    __lt__ = _cat_compare_op(operator.lt)\n    __gt__ = _cat_compare_op(operator.gt)\n    __le__ = _cat_compare_op(operator.le)\n    __ge__ = _cat_compare_op(operator.ge)\n\n    # -------------------------------------------------------------\n    # Validators; ideally these can be de-duplicated\n\n    def _validate_setitem_value(self, value):\n        if not is_hashable(value):\n            # wrap scalars and hashable-listlikes in list\n            return self._validate_listlike(value)\n        else:\n            return self._validate_scalar(value)\n\n    def _validate_scalar(self, fill_value):\n        \"\"\"\n        Convert a user-facing fill_value to a representation to use with our\n        underlying ndarray, raising TypeError if this is not possible.\n\n        Parameters\n        ----------\n        fill_value : object\n\n        Returns\n        -------\n        fill_value : int\n\n        Raises\n        ------\n        TypeError\n        \"\"\"\n\n        if is_valid_na_for_dtype(fill_value, self.categories.dtype):\n            fill_value = -1\n        elif fill_value in self.categories:\n            fill_value = self._unbox_scalar(fill_value)\n        else:\n            raise TypeError(\n                \"Cannot setitem on a Categorical with a new \"\n                f\"category ({fill_value}), set the categories first\"\n            ) from None\n        return fill_value\n\n    @classmethod\n    def _validate_codes_for_dtype(cls, codes, *, dtype: CategoricalDtype) -> np.ndarray:\n        if isinstance(codes, ExtensionArray) and is_integer_dtype(codes.dtype):\n            # Avoid the implicit conversion of Int to object\n            if isna(codes).any():\n                raise ValueError(\"codes cannot contain NA values\")\n            codes = codes.to_numpy(dtype=np.int64)\n        else:\n            codes = np.asarray(codes)\n        if len(codes) and codes.dtype.kind not in \"iu\":\n            raise ValueError(\"codes need to be array-like integers\")\n\n        if len(codes) and (codes.max() >= len(dtype.categories) or codes.min() < -1):\n            raise ValueError(\"codes need to be between -1 and len(categories)-1\")\n        return codes\n\n    # -------------------------------------------------------------\n\n    @ravel_compat\n    def __array__(\n        self, dtype: NpDtype | None = None, copy: bool | None = None\n    ) -> np.ndarray:\n        \"\"\"\n        The numpy array interface.\n\n        Users should not call this directly. Rather, it is invoked by\n        :func:`numpy.array` and :func:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : np.dtype or None\n            Specifies the the dtype for the array.\n\n        copy : bool or None, optional\n            See :func:`numpy.asarray`.\n\n        Returns\n        -------\n        numpy.array\n            A numpy array of either the specified dtype or,\n            if dtype==None (default), the same dtype as\n            categorical.categories.dtype.\n\n        See Also\n        --------\n        numpy.asarray : Convert input to numpy.ndarray.\n\n        Examples\n        --------\n\n        >>> cat = pd.Categorical([\"a\", \"b\"], ordered=True)\n\n        The following calls ``cat.__array__``\n\n        >>> np.asarray(cat)\n        array(['a', 'b'], dtype=object)\n        \"\"\"\n        if copy is False:\n            raise ValueError(\n                \"Unable to avoid copy while creating an array as requested.\"\n            )\n\n        ret = take_nd(self.categories._values, self._codes)\n        # When we're a Categorical[ExtensionArray], like Interval,\n        # we need to ensure __array__ gets all the way to an\n        # ndarray.\n\n        # `take_nd` should already make a copy, so don't force again.\n        return np.asarray(ret, dtype=dtype)\n\n    def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n        # for binary ops, use our custom dunder methods\n        result = arraylike.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        if \"out\" in kwargs:\n            # e.g. test_numpy_ufuncs_out\n            return arraylike.dispatch_ufunc_with_out(\n                self, ufunc, method, *inputs, **kwargs\n            )\n\n        if method == \"reduce\":\n            # e.g. TestCategoricalAnalytics::test_min_max_ordered\n            result = arraylike.dispatch_reduction_ufunc(\n                self, ufunc, method, *inputs, **kwargs\n            )\n            if result is not NotImplemented:\n                return result\n\n        # for all other cases, raise for now (similarly as what happens in\n        # Series.__array_prepare__)\n        raise TypeError(\n            f\"Object with dtype {self.dtype} cannot perform \"\n            f\"the numpy op {ufunc.__name__}\"\n        )\n\n    def __setstate__(self, state) -> None:\n        \"\"\"Necessary for making this object picklable\"\"\"\n        if not isinstance(state, dict):\n            return super().__setstate__(state)\n\n        if \"_dtype\" not in state:\n            state[\"_dtype\"] = CategoricalDtype(state[\"_categories\"], state[\"_ordered\"])\n\n        if \"_codes\" in state and \"_ndarray\" not in state:\n            # backward compat, changed what is property vs attribute\n            state[\"_ndarray\"] = state.pop(\"_codes\")\n\n        super().__setstate__(state)\n\n    @property\n    def nbytes(self) -> int:\n        return self._codes.nbytes + self.dtype.categories.values.nbytes\n\n    def memory_usage(self, deep: bool = False) -> int:\n        \"\"\"\n        Memory usage of my values\n\n        Parameters\n        ----------\n        deep : bool\n            Introspect the data deeply, interrogate\n            `object` dtypes for system-level memory consumption\n\n        Returns\n        -------\n        bytes used\n\n        Notes\n        -----\n        Memory usage does not include memory consumed by elements that\n        are not components of the array if deep=False\n\n        See Also\n        --------\n        numpy.ndarray.nbytes\n        \"\"\"\n        return self._codes.nbytes + self.dtype.categories.memory_usage(deep=deep)\n\n    def isna(self) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Detect missing values\n\n        Missing values (-1 in .codes) are detected.\n\n        Returns\n        -------\n        np.ndarray[bool] of whether my values are null\n\n        See Also\n        --------\n        isna : Top-level isna.\n        isnull : Alias of isna.\n        Categorical.notna : Boolean inverse of Categorical.isna.\n\n        \"\"\"\n        return self._codes == -1\n\n    isnull = isna\n\n    def notna(self) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Inverse of isna\n\n        Both missing values (-1 in .codes) and NA as a category are detected as\n        null.\n\n        Returns\n        -------\n        np.ndarray[bool] of whether my values are not null\n\n        See Also\n        --------\n        notna : Top-level notna.\n        notnull : Alias of notna.\n        Categorical.isna : Boolean inverse of Categorical.notna.\n\n        \"\"\"\n        return ~self.isna()\n\n    notnull = notna\n\n    def value_counts(self, dropna: bool = True) -> Series:\n        \"\"\"\n        Return a Series containing counts of each category.\n\n        Every category will have an entry, even those with a count of 0.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't include counts of NaN.\n\n        Returns\n        -------\n        counts : Series\n\n        See Also\n        --------\n        Series.value_counts\n        \"\"\"\n        from pandas import (\n            CategoricalIndex,\n            Series,\n        )\n\n        code, cat = self._codes, self.categories\n        ncat, mask = (len(cat), code >= 0)\n        ix, clean = np.arange(ncat), mask.all()\n\n        if dropna or clean:\n            obs = code if clean else code[mask]\n            count = np.bincount(obs, minlength=ncat or 0)\n        else:\n            count = np.bincount(np.where(mask, code, ncat))\n            ix = np.append(ix, -1)\n\n        ix = coerce_indexer_dtype(ix, self.dtype.categories)\n        ix_categorical = self._from_backing_data(ix)\n\n        return Series(\n            count,\n            index=CategoricalIndex(ix_categorical),\n            dtype=\"int64\",\n            name=\"count\",\n            copy=False,\n        )\n\n    # error: Argument 2 of \"_empty\" is incompatible with supertype\n    # \"NDArrayBackedExtensionArray\"; supertype defines the argument type as\n    # \"ExtensionDtype\"\n    @classmethod\n    def _empty(  # type: ignore[override]\n        cls, shape: Shape, dtype: CategoricalDtype\n    ) -> Self:\n        \"\"\"\n        Analogous to np.empty(shape, dtype=dtype)\n\n        Parameters\n        ----------\n        shape : tuple[int]\n        dtype : CategoricalDtype\n        \"\"\"\n        arr = cls._from_sequence([], dtype=dtype)\n\n        # We have to use np.zeros instead of np.empty otherwise the resulting\n        #  ndarray may contain codes not supported by this dtype, in which\n        #  case repr(result) could segfault.\n        backing = np.zeros(shape, dtype=arr._ndarray.dtype)\n\n        return arr._from_backing_data(backing)\n\n    def _internal_get_values(self) -> ArrayLike:\n        \"\"\"\n        Return the values.\n\n        For internal compatibility with pandas formatting.\n\n        Returns\n        -------\n        np.ndarray or ExtensionArray\n            A numpy array or ExtensionArray of the same dtype as\n            categorical.categories.dtype.\n        \"\"\"\n        # if we are a datetime and period index, return Index to keep metadata\n        if needs_i8_conversion(self.categories.dtype):\n            return self.categories.take(self._codes, fill_value=NaT)._values\n        elif is_integer_dtype(self.categories.dtype) and -1 in self._codes:\n            return (\n                self.categories.astype(\"object\")\n                .take(self._codes, fill_value=np.nan)\n                ._values\n            )\n        return np.array(self)\n\n    def check_for_ordered(self, op) -> None:\n        \"\"\"assert that we are ordered\"\"\"\n        if not self.ordered:\n            raise TypeError(\n                f\"Categorical is not ordered for operation {op}\\n\"\n                \"you can use .as_ordered() to change the \"\n                \"Categorical to an ordered one\\n\"\n            )\n\n    def argsort(\n        self, *, ascending: bool = True, kind: SortKind = \"quicksort\", **kwargs\n    ) -> npt.NDArray[np.intp]:\n        \"\"\"\n        Return the indices that would sort the Categorical.\n\n        Missing values are sorted at the end.\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            Whether the indices should result in an ascending\n            or descending sort.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n            Sorting algorithm.\n        **kwargs:\n            passed through to :func:`numpy.argsort`.\n\n        Returns\n        -------\n        np.ndarray[np.intp]\n\n        See Also\n        --------\n        numpy.ndarray.argsort\n\n        Notes\n        -----\n        While an ordering is applied to the category values, arg-sorting\n        in this context refers more to organizing and grouping together\n        based on matching category values. Thus, this function can be\n        called on an unordered Categorical instance unlike the functions\n        'Categorical.min' and 'Categorical.max'.\n\n        Examples\n        --------\n        >>> pd.Categorical([\"b\", \"b\", \"a\", \"c\"]).argsort()\n        array([2, 0, 1, 3])\n\n        >>> cat = pd.Categorical(\n        ...     [\"b\", \"b\", \"a\", \"c\"], categories=[\"c\", \"b\", \"a\"], ordered=True\n        ... )\n        >>> cat.argsort()\n        array([3, 0, 1, 2])\n\n        Missing values are placed at the end\n\n        >>> cat = pd.Categorical([2, None, 1])\n        >>> cat.argsort()\n        array([2, 0, 1])\n        \"\"\"\n        return super().argsort(ascending=ascending, kind=kind, **kwargs)\n\n    @overload\n    def sort_values(\n        self,\n        *,\n        inplace: Literal[False] = ...,\n        ascending: bool = ...,\n        na_position: str = ...,\n    ) -> Self: ...\n\n    @overload\n    def sort_values(\n        self, *, inplace: Literal[True], ascending: bool = ..., na_position: str = ...\n    ) -> None: ...\n\n    def sort_values(\n        self,\n        *,\n        inplace: bool = False,\n        ascending: bool = True,\n        na_position: str = \"last\",\n    ) -> Self | None:\n        \"\"\"\n        Sort the Categorical by category value returning a new\n        Categorical by default.\n\n        While an ordering is applied to the category values, sorting in this\n        context refers more to organizing and grouping together based on\n        matching category values. Thus, this function can be called on an\n        unordered Categorical instance unlike the functions 'Categorical.min'\n        and 'Categorical.max'.\n\n        Parameters\n        ----------\n        inplace : bool, default False\n            Do operation in place.\n        ascending : bool, default True\n            Order ascending. Passing False orders descending. The\n            ordering parameter provides the method by which the\n            category values are organized.\n        na_position : {'first', 'last'} (optional, default='last')\n            'first' puts NaNs at the beginning\n            'last' puts NaNs at the end\n\n        Returns\n        -------\n        Categorical or None\n\n        See Also\n        --------\n        Categorical.sort\n        Series.sort_values\n\n        Examples\n        --------\n        >>> c = pd.Categorical([1, 2, 2, 1, 5])\n        >>> c\n        [1, 2, 2, 1, 5]\n        Categories (3, int64): [1, 2, 5]\n        >>> c.sort_values()\n        [1, 1, 2, 2, 5]\n        Categories (3, int64): [1, 2, 5]\n        >>> c.sort_values(ascending=False)\n        [5, 2, 2, 1, 1]\n        Categories (3, int64): [1, 2, 5]\n\n        >>> c = pd.Categorical([1, 2, 2, 1, 5])\n\n        'sort_values' behaviour with NaNs. Note that 'na_position'\n        is independent of the 'ascending' parameter:\n\n        >>> c = pd.Categorical([np.nan, 2, 2, np.nan, 5])\n        >>> c\n        [NaN, 2, 2, NaN, 5]\n        Categories (2, int64): [2, 5]\n        >>> c.sort_values()\n        [2, 2, 5, NaN, NaN]\n        Categories (2, int64): [2, 5]\n        >>> c.sort_values(ascending=False)\n        [5, 2, 2, NaN, NaN]\n        Categories (2, int64): [2, 5]\n        >>> c.sort_values(na_position=\"first\")\n        [NaN, NaN, 2, 2, 5]\n        Categories (2, int64): [2, 5]\n        >>> c.sort_values(ascending=False, na_position=\"first\")\n        [NaN, NaN, 5, 2, 2]\n        Categories (2, int64): [2, 5]\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if na_position not in [\"last\", \"first\"]:\n            raise ValueError(f\"invalid na_position: {na_position!r}\")\n\n        sorted_idx = nargsort(self, ascending=ascending, na_position=na_position)\n\n        if not inplace:\n            codes = self._codes[sorted_idx]\n            return self._from_backing_data(codes)\n        self._codes[:] = self._codes[sorted_idx]\n        return None\n\n    def _rank(\n        self,\n        *,\n        axis: AxisInt = 0,\n        method: str = \"average\",\n        na_option: str = \"keep\",\n        ascending: bool = True,\n        pct: bool = False,\n    ):\n        \"\"\"\n        See Series.rank.__doc__.\n        \"\"\"\n        if axis != 0:\n            raise NotImplementedError\n        vff = self._values_for_rank()\n        return algorithms.rank(\n            vff,\n            axis=axis,\n            method=method,\n            na_option=na_option,\n            ascending=ascending,\n            pct=pct,\n        )\n\n    def _values_for_rank(self) -> np.ndarray:\n        \"\"\"\n        For correctly ranking ordered categorical data. See GH#15420\n\n        Ordered categorical data should be ranked on the basis of\n        codes with -1 translated to NaN.\n\n        Returns\n        -------\n        numpy.array\n\n        \"\"\"\n        from pandas import Series\n\n        if self.ordered:\n            values = self.codes\n            mask = values == -1\n            if mask.any():\n                values = values.astype(\"float64\")\n                values[mask] = np.nan\n        elif is_any_real_numeric_dtype(self.categories.dtype):\n            values = np.array(self)\n        else:\n            #  reorder the categories (so rank can use the float codes)\n            #  instead of passing an object array to rank\n            values = np.array(\n                self.rename_categories(\n                    Series(self.categories, copy=False).rank().values\n                )\n            )\n        return values\n\n    def _hash_pandas_object(\n        self, *, encoding: str, hash_key: str, categorize: bool\n    ) -> npt.NDArray[np.uint64]:\n        \"\"\"\n        Hash a Categorical by hashing its categories, and then mapping the codes\n        to the hashes.\n\n        Parameters\n        ----------\n        encoding : str\n        hash_key : str\n        categorize : bool\n            Ignored for Categorical.\n\n        Returns\n        -------\n        np.ndarray[uint64]\n        \"\"\"\n        # Note we ignore categorize, as we are already Categorical.\n        from pandas.core.util.hashing import hash_array\n\n        # Convert ExtensionArrays to ndarrays\n        values = np.asarray(self.categories._values)\n        hashed = hash_array(values, encoding, hash_key, categorize=False)\n\n        # we have uint64, as we don't directly support missing values\n        # we don't want to use take_nd which will coerce to float\n        # instead, directly construct the result with a\n        # max(np.uint64) as the missing value indicator\n        #\n        # TODO: GH#15362\n\n        mask = self.isna()\n        if len(hashed):\n            result = hashed.take(self._codes)\n        else:\n            result = np.zeros(len(mask), dtype=\"uint64\")\n\n        if mask.any():\n            result[mask] = lib.u8max\n\n        return result\n\n    # ------------------------------------------------------------------\n    # NDArrayBackedExtensionArray compat\n\n    @property\n    def _codes(self) -> np.ndarray:\n        return self._ndarray\n\n    def _box_func(self, i: int):\n        if i == -1:\n            return np.nan\n        return self.categories[i]\n\n    def _unbox_scalar(self, key) -> int:\n        # searchsorted is very performance sensitive. By converting codes\n        # to same dtype as self.codes, we get much faster performance.\n        code = self.categories.get_loc(key)\n        code = self._ndarray.dtype.type(code)\n        return code\n\n    # ------------------------------------------------------------------\n\n    def __iter__(self) -> Iterator:\n        \"\"\"\n        Returns an Iterator over the values of this Categorical.\n        \"\"\"\n        if self.ndim == 1:\n            return iter(self._internal_get_values().tolist())\n        else:\n            return (self[n] for n in range(len(self)))\n\n    def __contains__(self, key) -> bool:\n        \"\"\"\n        Returns True if `key` is in this Categorical.\n        \"\"\"\n        # if key is a NaN, check if any NaN is in self.\n        if is_valid_na_for_dtype(key, self.categories.dtype):\n            return bool(self.isna().any())\n\n        return contains(self, key, container=self._codes)\n\n    # ------------------------------------------------------------------\n    # Rendering Methods\n\n    # error: Return type \"None\" of \"_formatter\" incompatible with return\n    # type \"Callable[[Any], str | None]\" in supertype \"ExtensionArray\"\n    def _formatter(self, boxed: bool = False) -> None:  # type: ignore[override]\n        # Returning None here will cause format_array to do inference.\n        return None\n\n    def _repr_categories(self) -> list[str]:\n        \"\"\"\n        return the base repr for the categories\n        \"\"\"\n        max_categories = (\n            10\n            if get_option(\"display.max_categories\") == 0\n            else get_option(\"display.max_categories\")\n        )\n        from pandas.io.formats import format as fmt\n\n        format_array = partial(\n            fmt.format_array, formatter=None, quoting=QUOTE_NONNUMERIC\n        )\n        if len(self.categories) > max_categories:\n            num = max_categories // 2\n            head = format_array(self.categories[:num]._values)\n            tail = format_array(self.categories[-num:]._values)\n            category_strs = head + [\"...\"] + tail\n        else:\n            category_strs = format_array(self.categories._values)\n\n        # Strip all leading spaces, which format_array adds for columns...\n        category_strs = [x.strip() for x in category_strs]\n        return category_strs\n\n    def _get_repr_footer(self) -> str:\n        \"\"\"\n        Returns a string representation of the footer.\n        \"\"\"\n        category_strs = self._repr_categories()\n        dtype = str(self.categories.dtype)\n        levheader = f\"Categories ({len(self.categories)}, {dtype}): \"\n        width, _ = get_terminal_size()\n        max_width = get_option(\"display.width\") or width\n        if console.in_ipython_frontend():\n            # 0 = no breaks\n            max_width = 0\n        levstring = \"\"\n        start = True\n        cur_col_len = len(levheader)  # header\n        sep_len, sep = (3, \" < \") if self.ordered else (2, \", \")\n        linesep = f\"{sep.rstrip()}\\n\"  # remove whitespace\n        for val in category_strs:\n            if max_width != 0 and cur_col_len + sep_len + len(val) > max_width:\n                levstring += linesep + (\" \" * (len(levheader) + 1))\n                cur_col_len = len(levheader) + 1  # header + a whitespace\n            elif not start:\n                levstring += sep\n                cur_col_len += len(val)\n            levstring += val\n            start = False\n        # replace to simple save space by\n        return f\"{levheader}[{levstring.replace(' < ... < ', ' ... ')}]\"\n\n    def _get_values_repr(self) -> str:\n        from pandas.io.formats import format as fmt\n\n        assert len(self) > 0\n\n        vals = self._internal_get_values()\n        fmt_values = fmt.format_array(\n            vals,\n            None,\n            float_format=None,\n            na_rep=\"NaN\",\n            quoting=QUOTE_NONNUMERIC,\n        )\n\n        fmt_values = [i.strip() for i in fmt_values]\n        joined = \", \".join(fmt_values)\n        result = \"[\" + joined + \"]\"\n        return result\n\n    def __repr__(self) -> str:\n        \"\"\"\n        String representation.\n        \"\"\"\n        footer = self._get_repr_footer()\n        length = len(self)\n        max_len = 10\n        if length > max_len:\n            # In long cases we do not display all entries, so we add Length\n            #  information to the __repr__.\n            num = max_len // 2\n            head = self[:num]._get_values_repr()\n            tail = self[-(max_len - num) :]._get_values_repr()\n            body = f\"{head[:-1]}, ..., {tail[1:]}\"\n            length_info = f\"Length: {len(self)}\"\n            result = f\"{body}\\n{length_info}\\n{footer}\"\n        elif length > 0:\n            body = self._get_values_repr()\n            result = f\"{body}\\n{footer}\"\n        else:\n            # In the empty case we use a comma instead of newline to get\n            #  a more compact __repr__\n            body = \"[]\"\n            result = f\"{body}, {footer}\"\n\n        return result\n\n    # ------------------------------------------------------------------\n\n    def _validate_listlike(self, value):\n        # NB: here we assume scalar-like tuples have already been excluded\n        value = extract_array(value, extract_numpy=True)\n\n        # require identical categories set\n        if isinstance(value, Categorical):\n            if self.dtype != value.dtype:\n                raise TypeError(\n                    \"Cannot set a Categorical with another, \"\n                    \"without identical categories\"\n                )\n            # dtype equality implies categories_match_up_to_permutation\n            value = self._encode_with_my_categories(value)\n            return value._codes\n\n        from pandas import Index\n\n        # tupleize_cols=False for e.g. test_fillna_iterable_category GH#41914\n        to_add = Index._with_infer(value, tupleize_cols=False).difference(\n            self.categories\n        )\n\n        # no assignments of values not in categories, but it's always ok to set\n        # something to np.nan\n        if len(to_add) and not isna(to_add).all():\n            raise TypeError(\n                \"Cannot setitem on a Categorical with a new \"\n                \"category, set the categories first\"\n            )\n\n        codes = self.categories.get_indexer(value)\n        return codes.astype(self._ndarray.dtype, copy=False)\n\n    def _reverse_indexer(self) -> dict[Hashable, npt.NDArray[np.intp]]:\n        \"\"\"\n        Compute the inverse of a categorical, returning\n        a dict of categories -> indexers.\n\n        *This is an internal function*\n\n        Returns\n        -------\n        Dict[Hashable, np.ndarray[np.intp]]\n            dict of categories -> indexers\n\n        Examples\n        --------\n        >>> c = pd.Categorical(list(\"aabca\"))\n        >>> c\n        ['a', 'a', 'b', 'c', 'a']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> c.categories\n        Index(['a', 'b', 'c'], dtype='object')\n        >>> c.codes\n        array([0, 0, 1, 2, 0], dtype=int8)\n        >>> c._reverse_indexer()\n        {'a': array([0, 1, 4]), 'b': array([2]), 'c': array([3])}\n\n        \"\"\"\n        categories = self.categories\n        r, counts = libalgos.groupsort_indexer(\n            ensure_platform_int(self.codes), categories.size\n        )\n        counts = ensure_int64(counts).cumsum()\n        _result = (r[start:end] for start, end in zip(counts, counts[1:]))\n        return dict(zip(categories, _result))\n\n    # ------------------------------------------------------------------\n    # Reductions\n\n    def _reduce(\n        self, name: str, *, skipna: bool = True, keepdims: bool = False, **kwargs\n    ):\n        result = super()._reduce(name, skipna=skipna, keepdims=keepdims, **kwargs)\n        if name in [\"argmax\", \"argmin\"]:\n            # don't wrap in Categorical!\n            return result\n        if keepdims:\n            return type(self)(result, dtype=self.dtype)\n        else:\n            return result\n\n    def min(self, *, skipna: bool = True, **kwargs):\n        \"\"\"\n        The minimum value of the object.\n\n        Only ordered `Categoricals` have a minimum!\n\n        Raises\n        ------\n        TypeError\n            If the `Categorical` is not `ordered`.\n\n        Returns\n        -------\n        min : the minimum of this `Categorical`, NA value if empty\n        \"\"\"\n        nv.validate_minmax_axis(kwargs.get(\"axis\", 0))\n        nv.validate_min((), kwargs)\n        self.check_for_ordered(\"min\")\n\n        if not len(self._codes):\n            return self.dtype.na_value\n\n        good = self._codes != -1\n        if not good.all():\n            if skipna and good.any():\n                pointer = self._codes[good].min()\n            else:\n                return np.nan\n        else:\n            pointer = self._codes.min()\n        return self._wrap_reduction_result(None, pointer)\n\n    def max(self, *, skipna: bool = True, **kwargs):\n        \"\"\"\n        The maximum value of the object.\n\n        Only ordered `Categoricals` have a maximum!\n\n        Raises\n        ------\n        TypeError\n            If the `Categorical` is not `ordered`.\n\n        Returns\n        -------\n        max : the maximum of this `Categorical`, NA if array is empty\n        \"\"\"\n        nv.validate_minmax_axis(kwargs.get(\"axis\", 0))\n        nv.validate_max((), kwargs)\n        self.check_for_ordered(\"max\")\n\n        if not len(self._codes):\n            return self.dtype.na_value\n\n        good = self._codes != -1\n        if not good.all():\n            if skipna and good.any():\n                pointer = self._codes[good].max()\n            else:\n                return np.nan\n        else:\n            pointer = self._codes.max()\n        return self._wrap_reduction_result(None, pointer)\n\n    def _mode(self, dropna: bool = True) -> Categorical:\n        codes = self._codes\n        mask = None\n        if dropna:\n            mask = self.isna()\n\n        res_codes = algorithms.mode(codes, mask=mask)\n        res_codes = cast(np.ndarray, res_codes)\n        assert res_codes.dtype == codes.dtype\n        res = self._from_backing_data(res_codes)\n        return res\n\n    # ------------------------------------------------------------------\n    # ExtensionArray Interface\n\n    def unique(self) -> Self:\n        \"\"\"\n        Return the ``Categorical`` which ``categories`` and ``codes`` are\n        unique.\n\n        .. versionchanged:: 1.3.0\n\n            Previously, unused categories were dropped from the new categories.\n\n        Returns\n        -------\n        Categorical\n\n        See Also\n        --------\n        pandas.unique\n        CategoricalIndex.unique\n        Series.unique : Return unique values of Series object.\n\n        Examples\n        --------\n        >>> pd.Categorical(list(\"baabc\")).unique()\n        ['b', 'a', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> pd.Categorical(list(\"baab\"), categories=list(\"abc\"), ordered=True).unique()\n        ['b', 'a']\n        Categories (3, object): ['a' < 'b' < 'c']\n        \"\"\"\n        return super().unique()\n\n    def equals(self, other: object) -> bool:\n        \"\"\"\n        Returns True if categorical arrays are equal.\n\n        Parameters\n        ----------\n        other : `Categorical`\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        if not isinstance(other, Categorical):\n            return False\n        elif self._categories_match_up_to_permutation(other):\n            other = self._encode_with_my_categories(other)\n            return np.array_equal(self._codes, other._codes)\n        return False\n\n    def _accumulate(self, name: str, skipna: bool = True, **kwargs) -> Self:\n        func: Callable\n        if name == \"cummin\":\n            func = np.minimum.accumulate\n        elif name == \"cummax\":\n            func = np.maximum.accumulate\n        else:\n            raise TypeError(f\"Accumulation {name} not supported for {type(self)}\")\n        self.check_for_ordered(name)\n\n        codes = self.codes.copy()\n        mask = self.isna()\n        if func == np.minimum.accumulate:\n            codes[mask] = np.iinfo(codes.dtype.type).max\n        # no need to change codes for maximum because codes[mask] is already -1\n        if not skipna:\n            mask = np.maximum.accumulate(mask)\n\n        codes = func(codes)\n        codes[mask] = -1\n        return self._simple_new(codes, dtype=self._dtype)\n\n    @classmethod\n    def _concat_same_type(cls, to_concat: Sequence[Self], axis: AxisInt = 0) -> Self:\n        from pandas.core.dtypes.concat import union_categoricals\n\n        first = to_concat[0]\n        if axis >= first.ndim:\n            raise ValueError(\n                f\"axis {axis} is out of bounds for array of dimension {first.ndim}\"\n            )\n\n        if axis == 1:\n            # Flatten, concatenate then reshape\n            if not all(x.ndim == 2 for x in to_concat):\n                raise ValueError\n\n            # pass correctly-shaped to union_categoricals\n            tc_flat = []\n            for obj in to_concat:\n                tc_flat.extend([obj[:, i] for i in range(obj.shape[1])])\n\n            res_flat = cls._concat_same_type(tc_flat, axis=0)\n\n            result = res_flat.reshape(len(first), -1, order=\"F\")\n            return result\n\n        # error: Incompatible types in assignment (expression has type \"Categorical\",\n        # variable has type \"Self\")\n        result = union_categoricals(to_concat)  # type: ignore[assignment]\n        return result\n\n    # ------------------------------------------------------------------\n\n    def _encode_with_my_categories(self, other: Categorical) -> Categorical:\n        \"\"\"\n        Re-encode another categorical using this Categorical's categories.\n\n        Notes\n        -----\n        This assumes we have already checked\n        self._categories_match_up_to_permutation(other).\n        \"\"\"\n        # Indexing on codes is more efficient if categories are the same,\n        #  so we can apply some optimizations based on the degree of\n        #  dtype-matching.\n        codes = recode_for_categories(\n            other.codes, other.categories, self.categories, copy=False\n        )\n        return self._from_backing_data(codes)\n\n    def _categories_match_up_to_permutation(self, other: Categorical) -> bool:\n        \"\"\"\n        Returns True if categoricals are the same dtype\n          same categories, and same ordered\n\n        Parameters\n        ----------\n        other : Categorical\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return hash(self.dtype) == hash(other.dtype)\n\n    def describe(self) -> DataFrame:\n        \"\"\"\n        Describes this Categorical\n\n        Returns\n        -------\n        description: `DataFrame`\n            A dataframe with frequency and counts by category.\n        \"\"\"\n        counts = self.value_counts(dropna=False)\n        freqs = counts / counts.sum()\n\n        from pandas import Index\n        from pandas.core.reshape.concat import concat\n\n        result = concat([counts, freqs], ignore_index=True, axis=1)\n        result.columns = Index([\"counts\", \"freqs\"])\n        result.index.name = \"categories\"\n\n        return result\n\n    def isin(self, values: ArrayLike) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Check whether `values` are contained in Categorical.\n\n        Return a boolean NumPy Array showing whether each element in\n        the Categorical matches an element in the passed sequence of\n        `values` exactly.\n\n        Parameters\n        ----------\n        values : np.ndarray or ExtensionArray\n            The sequence of values to test. Passing in a single string will\n            raise a ``TypeError``. Instead, turn a single string into a\n            list of one element.\n\n        Returns\n        -------\n        np.ndarray[bool]\n\n        Raises\n        ------\n        TypeError\n          * If `values` is not a set or list-like\n\n        See Also\n        --------\n        pandas.Series.isin : Equivalent method on Series.\n\n        Examples\n        --------\n        >>> s = pd.Categorical([\"llama\", \"cow\", \"llama\", \"beetle\", \"llama\", \"hippo\"])\n        >>> s.isin([\"cow\", \"llama\"])\n        array([ True,  True,  True, False,  True, False])\n\n        Passing a single string as ``s.isin('llama')`` will raise an error. Use\n        a list of one element instead:\n\n        >>> s.isin([\"llama\"])\n        array([ True, False,  True, False,  True, False])\n        \"\"\"\n        null_mask = np.asarray(isna(values))\n        code_values = self.categories.get_indexer_for(values)\n        code_values = code_values[null_mask | (code_values >= 0)]\n        return algorithms.isin(self.codes, code_values)\n\n    # ------------------------------------------------------------------------\n    # String methods interface\n    def _str_map(\n        self, f, na_value=lib.no_default, dtype=np.dtype(\"object\"), convert: bool = True\n    ):\n        # Optimization to apply the callable `f` to the categories once\n        # and rebuild the result by `take`ing from the result with the codes.\n        # Returns the same type as the object-dtype implementation though.\n        categories = self.categories\n        codes = self.codes\n        if categories.dtype == \"string\":\n            result = categories.array._str_map(f, na_value, dtype)  # type: ignore[attr-defined]\n            if (\n                categories.dtype.na_value is np.nan  # type: ignore[union-attr]\n                and is_bool_dtype(dtype)\n                and (na_value is lib.no_default or isna(na_value))\n            ):\n                # NaN propagates as False for functions with boolean return type\n                na_value = False\n        else:\n            from pandas.core.arrays import NumpyExtensionArray\n\n            result = NumpyExtensionArray(categories.to_numpy())._str_map(\n                f, na_value, dtype\n            )\n        return take_nd(result, codes, fill_value=na_value)\n\n    def _str_get_dummies(self, sep: str = \"|\", dtype: NpDtype | None = None):\n        # sep may not be in categories. Just bail on this.\n        from pandas.core.arrays import NumpyExtensionArray\n\n        return NumpyExtensionArray(self.to_numpy(str, na_value=\"NaN\"))._str_get_dummies(\n            sep, dtype\n        )\n\n    # ------------------------------------------------------------------------\n    # GroupBy Methods\n\n    def _groupby_op(\n        self,\n        *,\n        how: str,\n        has_dropped_na: bool,\n        min_count: int,\n        ngroups: int,\n        ids: npt.NDArray[np.intp],\n        **kwargs,\n    ):\n        from pandas.core.groupby.ops import WrappedCythonOp\n\n        kind = WrappedCythonOp.get_kind_from_how(how)\n        op = WrappedCythonOp(how=how, kind=kind, has_dropped_na=has_dropped_na)\n\n        dtype = self.dtype\n        if how in [\"sum\", \"prod\", \"cumsum\", \"cumprod\", \"skew\", \"kurt\"]:\n            raise TypeError(f\"{dtype} type does not support {how} operations\")\n        if how in [\"min\", \"max\", \"rank\", \"idxmin\", \"idxmax\"] and not dtype.ordered:\n            # raise TypeError instead of NotImplementedError to ensure we\n            #  don't go down a group-by-group path, since in the empty-groups\n            #  case that would fail to raise\n            raise TypeError(f\"Cannot perform {how} with non-ordered Categorical\")\n        if how not in [\n            \"rank\",\n            \"any\",\n            \"all\",\n            \"first\",\n            \"last\",\n            \"min\",\n            \"max\",\n            \"idxmin\",\n            \"idxmax\",\n        ]:\n            if kind == \"transform\":\n                raise TypeError(f\"{dtype} type does not support {how} operations\")\n            raise TypeError(f\"{dtype} dtype does not support aggregation '{how}'\")\n\n        result_mask = None\n        mask = self.isna()\n        if how == \"rank\":\n            assert self.ordered  # checked earlier\n            npvalues = self._ndarray\n        elif how in [\"first\", \"last\", \"min\", \"max\", \"idxmin\", \"idxmax\"]:\n            npvalues = self._ndarray\n            result_mask = np.zeros(ngroups, dtype=bool)\n        else:\n            # any/all\n            npvalues = self.astype(bool)\n\n        res_values = op._cython_op_ndim_compat(\n            npvalues,\n            min_count=min_count,\n            ngroups=ngroups,\n            comp_ids=ids,\n            mask=mask,\n            result_mask=result_mask,\n            **kwargs,\n        )\n\n        if how in op.cast_blocklist:\n            return res_values\n        elif how in [\"first\", \"last\", \"min\", \"max\"]:\n            res_values[result_mask == 1] = -1\n        return self._from_backing_data(res_values)\n\n\n# The Series.cat accessor\n\n\n@delegate_names(\n    delegate=Categorical, accessors=[\"categories\", \"ordered\"], typ=\"property\"\n)\n@delegate_names(\n    delegate=Categorical,\n    accessors=[\n        \"rename_categories\",\n        \"reorder_categories\",\n        \"add_categories\",\n        \"remove_categories\",\n        \"remove_unused_categories\",\n        \"set_categories\",\n        \"as_ordered\",\n        \"as_unordered\",\n    ],\n    typ=\"method\",\n)\nclass CategoricalAccessor(PandasDelegate, PandasObject, NoNewAttributesMixin):\n    \"\"\"\n    Accessor object for categorical properties of the Series values.\n\n    Parameters\n    ----------\n    data : Series or CategoricalIndex\n        The object to which the categorical accessor is attached.\n\n    See Also\n    --------\n    Series.dt : Accessor object for datetimelike properties of the Series values.\n    Series.sparse : Accessor for sparse matrix data types.\n\n    Examples\n    --------\n    >>> s = pd.Series(list(\"abbccc\")).astype(\"category\")\n    >>> s\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n\n    >>> s.cat.categories\n    Index(['a', 'b', 'c'], dtype='object')\n\n    >>> s.cat.rename_categories(list(\"cba\"))\n    0    c\n    1    b\n    2    b\n    3    a\n    4    a\n    5    a\n    dtype: category\n    Categories (3, object): ['c', 'b', 'a']\n\n    >>> s.cat.reorder_categories(list(\"cba\"))\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['c', 'b', 'a']\n\n    >>> s.cat.add_categories([\"d\", \"e\"])\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (5, object): ['a', 'b', 'c', 'd', 'e']\n\n    >>> s.cat.remove_categories([\"a\", \"c\"])\n    0    NaN\n    1      b\n    2      b\n    3    NaN\n    4    NaN\n    5    NaN\n    dtype: category\n    Categories (1, object): ['b']\n\n    >>> s1 = s.cat.add_categories([\"d\", \"e\"])\n    >>> s1.cat.remove_unused_categories()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n\n    >>> s.cat.set_categories(list(\"abcde\"))\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (5, object): ['a', 'b', 'c', 'd', 'e']\n\n    >>> s.cat.as_ordered()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a' < 'b' < 'c']\n\n    >>> s.cat.as_unordered()\n    0    a\n    1    b\n    2    b\n    3    c\n    4    c\n    5    c\n    dtype: category\n    Categories (3, object): ['a', 'b', 'c']\n    \"\"\"\n\n    def __init__(self, data) -> None:\n        self._validate(data)\n        self._parent = data.values\n        self._index = data.index\n        self._name = data.name\n        self._freeze()\n\n    @staticmethod\n    def _validate(data) -> None:\n        if not isinstance(data.dtype, CategoricalDtype):\n            raise AttributeError(\"Can only use .cat accessor with a 'category' dtype\")\n\n    def _delegate_property_get(self, name: str):\n        return getattr(self._parent, name)\n\n    # error: Signature of \"_delegate_property_set\" incompatible with supertype\n    # \"PandasDelegate\"\n    def _delegate_property_set(self, name: str, new_values) -> None:  # type: ignore[override]\n        setattr(self._parent, name, new_values)\n\n    @property\n    def codes(self) -> Series:\n        \"\"\"\n        Return Series of codes as well as the index.\n\n        See Also\n        --------\n        Series.cat.categories : Return the categories of this categorical.\n        Series.cat.as_ordered : Set the Categorical to be ordered.\n        Series.cat.as_unordered : Set the Categorical to be unordered.\n\n        Examples\n        --------\n        >>> raw_cate = pd.Categorical([\"a\", \"b\", \"c\", \"a\"], categories=[\"a\", \"b\"])\n        >>> ser = pd.Series(raw_cate)\n        >>> ser.cat.codes\n        0   0\n        1   1\n        2  -1\n        3   0\n        dtype: int8\n        \"\"\"\n        from pandas import Series\n\n        return Series(self._parent.codes, index=self._index)\n\n    def _delegate_method(self, name: str, *args, **kwargs):\n        from pandas import Series\n\n        method = getattr(self._parent, name)\n        res = method(*args, **kwargs)\n        if res is not None:\n            return Series(res, index=self._index, name=self._name)\n\n\n# utility routines\n\n\ndef _get_codes_for_values(\n    values: Index | Series | ExtensionArray | np.ndarray,\n    categories: Index,\n) -> np.ndarray:\n    \"\"\"\n    utility routine to turn values into codes given the specified categories\n\n    If `values` is known to be a Categorical, use recode_for_categories instead.\n    \"\"\"\n    codes = categories.get_indexer_for(values)\n    return coerce_indexer_dtype(codes, categories)\n\n\ndef recode_for_categories(\n    codes: np.ndarray, old_categories, new_categories, copy: bool = True\n) -> np.ndarray:\n    \"\"\"\n    Convert a set of codes for to a new set of categories\n\n    Parameters\n    ----------\n    codes : np.ndarray\n    old_categories, new_categories : Index\n    copy: bool, default True\n        Whether to copy if the codes are unchanged.\n\n    Returns\n    -------\n    new_codes : np.ndarray[np.int64]\n\n    Examples\n    --------\n    >>> old_cat = pd.Index([\"b\", \"a\", \"c\"])\n    >>> new_cat = pd.Index([\"a\", \"b\"])\n    >>> codes = np.array([0, 1, 1, 2])\n    >>> recode_for_categories(codes, old_cat, new_cat)\n    array([ 1,  0,  0, -1], dtype=int8)\n    \"\"\"\n    if len(old_categories) == 0:\n        # All null anyway, so just retain the nulls\n        if copy:\n            return codes.copy()\n        return codes\n    elif new_categories.equals(old_categories):\n        # Same categories, so no need to actually recode\n        if copy:\n            return codes.copy()\n        return codes\n\n    indexer = coerce_indexer_dtype(\n        new_categories.get_indexer_for(old_categories), new_categories\n    )\n    new_codes = take_nd(indexer, codes, fill_value=-1)\n    return new_codes\n\n\ndef factorize_from_iterable(values) -> tuple[np.ndarray, Index]:\n    \"\"\"\n    Factorize an input `values` into `categories` and `codes`. Preserves\n    categorical dtype in `categories`.\n\n    Parameters\n    ----------\n    values : list-like\n\n    Returns\n    -------\n    codes : ndarray\n    categories : Index\n        If `values` has a categorical dtype, then `categories` is\n        a CategoricalIndex keeping the categories and order of `values`.\n    \"\"\"\n    from pandas import CategoricalIndex\n\n    if not is_list_like(values):\n        raise TypeError(\"Input must be list-like\")\n\n    categories: Index\n\n    vdtype = getattr(values, \"dtype\", None)\n    if isinstance(vdtype, CategoricalDtype):\n        values = extract_array(values)\n        # The Categorical we want to build has the same categories\n        # as values but its codes are by def [0, ..., len(n_categories) - 1]\n        cat_codes = np.arange(len(values.categories), dtype=values.codes.dtype)\n        cat = Categorical.from_codes(cat_codes, dtype=values.dtype, validate=False)\n\n        categories = CategoricalIndex(cat)\n        codes = values.codes\n    else:\n        # The value of ordered is irrelevant since we don't use cat as such,\n        # but only the resulting categories, the order of which is independent\n        # from ordered. Set ordered to False as default. See GH #15457\n        cat = Categorical(values, ordered=False)\n        categories = cat.categories\n        codes = cat.codes\n    return codes, categories\n\n\ndef factorize_from_iterables(iterables) -> tuple[list[np.ndarray], list[Index]]:\n    \"\"\"\n    A higher-level wrapper over `factorize_from_iterable`.\n\n    Parameters\n    ----------\n    iterables : list-like of list-likes\n\n    Returns\n    -------\n    codes : list of ndarrays\n    categories : list of Indexes\n\n    Notes\n    -----\n    See `factorize_from_iterable` for more info.\n    \"\"\"\n    if len(iterables) == 0:\n        # For consistency, it should return two empty lists.\n        return [], []\n\n    codes, categories = zip(*(factorize_from_iterable(it) for it in iterables))\n    return list(codes), list(categories)\n"
    },
    {
      "filename": "pandas/tests/extension/test_arrow.py",
      "content": "\"\"\"\nThis file contains a minimal set of tests for compliance with the extension\narray interface test suite, and should contain no other tests.\nThe test suite for the full functionality of the array is located in\n`pandas/tests/arrays/`.\nThe tests in this file are inherited from the BaseExtensionTests, and only\nminimal tweaks should be applied to get the tests passing (by overwriting a\nparent method).\nAdditional tests should either be added to one of the BaseExtensionTests\nclasses (if they are relevant for the extension interface for all dtypes), or\nbe added to the array-specific tests in `pandas/tests/arrays/`.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import (\n    date,\n    datetime,\n    time,\n    timedelta,\n)\nfrom decimal import Decimal\nfrom io import (\n    BytesIO,\n    StringIO,\n)\nimport operator\nimport pickle\nimport re\nimport sys\n\nimport numpy as np\nimport pytest\n\nfrom pandas._libs import lib\nfrom pandas._libs.tslibs import timezones\nfrom pandas.compat import (\n    PY311,\n    PY312,\n    is_ci_environment,\n    is_platform_windows,\n    pa_version_under11p0,\n    pa_version_under13p0,\n    pa_version_under14p0,\n)\n\nfrom pandas.core.dtypes.dtypes import (\n    ArrowDtype,\n    CategoricalDtypeType,\n)\n\nimport pandas as pd\nimport pandas._testing as tm\nfrom pandas.api.extensions import no_default\nfrom pandas.api.types import (\n    is_bool_dtype,\n    is_datetime64_any_dtype,\n    is_float_dtype,\n    is_integer_dtype,\n    is_numeric_dtype,\n    is_signed_integer_dtype,\n    is_string_dtype,\n    is_unsigned_integer_dtype,\n)\nfrom pandas.tests.extension import base\n\npa = pytest.importorskip(\"pyarrow\")\n\nfrom pandas.core.arrays.arrow.array import (\n    ArrowExtensionArray,\n    get_unit_from_pa_dtype,\n)\nfrom pandas.core.arrays.arrow.extension_types import ArrowPeriodType\n\n\ndef _require_timezone_database(request):\n    if is_platform_windows() and is_ci_environment():\n        mark = pytest.mark.xfail(\n            raises=pa.ArrowInvalid,\n            reason=(\n                \"TODO: Set ARROW_TIMEZONE_DATABASE environment variable \"\n                \"on CI to path to the tzdata for pyarrow.\"\n            ),\n        )\n        request.applymarker(mark)\n\n\n@pytest.fixture(params=tm.ALL_PYARROW_DTYPES, ids=str)\ndef dtype(request):\n    return ArrowDtype(pyarrow_dtype=request.param)\n\n\n@pytest.fixture\ndef data(dtype):\n    pa_dtype = dtype.pyarrow_dtype\n    if pa.types.is_boolean(pa_dtype):\n        data = [True, False] * 4 + [None] + [True, False] * 44 + [None] + [True, False]\n    elif pa.types.is_floating(pa_dtype):\n        data = [1.0, 0.0] * 4 + [None] + [-2.0, -1.0] * 44 + [None] + [0.5, 99.5]\n    elif pa.types.is_signed_integer(pa_dtype):\n        data = [1, 0] * 4 + [None] + [-2, -1] * 44 + [None] + [1, 99]\n    elif pa.types.is_unsigned_integer(pa_dtype):\n        data = [1, 0] * 4 + [None] + [2, 1] * 44 + [None] + [1, 99]\n    elif pa.types.is_decimal(pa_dtype):\n        data = (\n            [Decimal(\"1\"), Decimal(\"0.0\")] * 4\n            + [None]\n            + [Decimal(\"-2.0\"), Decimal(\"-1.0\")] * 44\n            + [None]\n            + [Decimal(\"0.5\"), Decimal(\"33.123\")]\n        )\n    elif pa.types.is_date(pa_dtype):\n        data = (\n            [date(2022, 1, 1), date(1999, 12, 31)] * 4\n            + [None]\n            + [date(2022, 1, 1), date(2022, 1, 1)] * 44\n            + [None]\n            + [date(1999, 12, 31), date(1999, 12, 31)]\n        )\n    elif pa.types.is_timestamp(pa_dtype):\n        data = (\n            [datetime(2020, 1, 1, 1, 1, 1, 1), datetime(1999, 1, 1, 1, 1, 1, 1)] * 4\n            + [None]\n            + [datetime(2020, 1, 1, 1), datetime(1999, 1, 1, 1)] * 44\n            + [None]\n            + [datetime(2020, 1, 1), datetime(1999, 1, 1)]\n        )\n    elif pa.types.is_duration(pa_dtype):\n        data = (\n            [timedelta(1), timedelta(1, 1)] * 4\n            + [None]\n            + [timedelta(-1), timedelta(0)] * 44\n            + [None]\n            + [timedelta(-10), timedelta(10)]\n        )\n    elif pa.types.is_time(pa_dtype):\n        data = (\n            [time(12, 0), time(0, 12)] * 4\n            + [None]\n            + [time(0, 0), time(1, 1)] * 44\n            + [None]\n            + [time(0, 5), time(5, 0)]\n        )\n    elif pa.types.is_string(pa_dtype):\n        data = [\"a\", \"b\"] * 4 + [None] + [\"1\", \"2\"] * 44 + [None] + [\"!\", \">\"]\n    elif pa.types.is_binary(pa_dtype):\n        data = [b\"a\", b\"b\"] * 4 + [None] + [b\"1\", b\"2\"] * 44 + [None] + [b\"!\", b\">\"]\n    else:\n        raise NotImplementedError\n    return pd.array(data, dtype=dtype)\n\n\n@pytest.fixture\ndef data_missing(data):\n    \"\"\"Length-2 array with [NA, Valid]\"\"\"\n    return type(data)._from_sequence([None, data[0]], dtype=data.dtype)\n\n\n@pytest.fixture(params=[\"data\", \"data_missing\"])\ndef all_data(request, data, data_missing):\n    \"\"\"Parametrized fixture returning 'data' or 'data_missing' integer arrays.\n\n    Used to test dtype conversion with and without missing values.\n    \"\"\"\n    if request.param == \"data\":\n        return data\n    elif request.param == \"data_missing\":\n        return data_missing\n\n\n@pytest.fixture\ndef data_for_grouping(dtype):\n    \"\"\"\n    Data for factorization, grouping, and unique tests.\n\n    Expected to be like [B, B, NA, NA, A, A, B, C]\n\n    Where A < B < C and NA is missing\n    \"\"\"\n    pa_dtype = dtype.pyarrow_dtype\n    if pa.types.is_boolean(pa_dtype):\n        A = False\n        B = True\n        C = True\n    elif pa.types.is_floating(pa_dtype):\n        A = -1.1\n        B = 0.0\n        C = 1.1\n    elif pa.types.is_signed_integer(pa_dtype):\n        A = -1\n        B = 0\n        C = 1\n    elif pa.types.is_unsigned_integer(pa_dtype):\n        A = 0\n        B = 1\n        C = 10\n    elif pa.types.is_date(pa_dtype):\n        A = date(1999, 12, 31)\n        B = date(2010, 1, 1)\n        C = date(2022, 1, 1)\n    elif pa.types.is_timestamp(pa_dtype):\n        A = datetime(1999, 1, 1, 1, 1, 1, 1)\n        B = datetime(2020, 1, 1)\n        C = datetime(2020, 1, 1, 1)\n    elif pa.types.is_duration(pa_dtype):\n        A = timedelta(-1)\n        B = timedelta(0)\n        C = timedelta(1, 4)\n    elif pa.types.is_time(pa_dtype):\n        A = time(0, 0)\n        B = time(0, 12)\n        C = time(12, 12)\n    elif pa.types.is_string(pa_dtype):\n        A = \"a\"\n        B = \"b\"\n        C = \"c\"\n    elif pa.types.is_binary(pa_dtype):\n        A = b\"a\"\n        B = b\"b\"\n        C = b\"c\"\n    elif pa.types.is_decimal(pa_dtype):\n        A = Decimal(\"-1.1\")\n        B = Decimal(\"0.0\")\n        C = Decimal(\"1.1\")\n    else:\n        raise NotImplementedError\n    return pd.array([B, B, None, None, A, A, B, C], dtype=dtype)\n\n\n@pytest.fixture\ndef data_for_sorting(data_for_grouping):\n    \"\"\"\n    Length-3 array with a known sort order.\n\n    This should be three items [B, C, A] with\n    A < B < C\n    \"\"\"\n    return type(data_for_grouping)._from_sequence(\n        [data_for_grouping[0], data_for_grouping[7], data_for_grouping[4]],\n        dtype=data_for_grouping.dtype,\n    )\n\n\n@pytest.fixture\ndef data_missing_for_sorting(data_for_grouping):\n    \"\"\"\n    Length-3 array with a known sort order.\n\n    This should be three items [B, NA, A] with\n    A < B and NA missing.\n    \"\"\"\n    return type(data_for_grouping)._from_sequence(\n        [data_for_grouping[0], data_for_grouping[2], data_for_grouping[4]],\n        dtype=data_for_grouping.dtype,\n    )\n\n\n@pytest.fixture\ndef data_for_twos(data):\n    \"\"\"Length-100 array in which all the elements are two.\"\"\"\n    pa_dtype = data.dtype.pyarrow_dtype\n    if (\n        pa.types.is_integer(pa_dtype)\n        or pa.types.is_floating(pa_dtype)\n        or pa.types.is_decimal(pa_dtype)\n        or pa.types.is_duration(pa_dtype)\n    ):\n        return pd.array([2] * 100, dtype=data.dtype)\n    # tests will be xfailed where 2 is not a valid scalar for pa_dtype\n    return data\n    # TODO: skip otherwise?\n\n\nclass TestArrowArray(base.ExtensionTests):\n    def test_compare_scalar(self, data, comparison_op):\n        ser = pd.Series(data)\n        self._compare_other(ser, data, comparison_op, data[0])\n\n    @pytest.mark.parametrize(\"na_action\", [None, \"ignore\"])\n    def test_map(self, data_missing, na_action):\n        if data_missing.dtype.kind in \"mM\":\n            result = data_missing.map(lambda x: x, na_action=na_action)\n            expected = data_missing.to_numpy(dtype=object)\n            tm.assert_numpy_array_equal(result, expected)\n        else:\n            result = data_missing.map(lambda x: x, na_action=na_action)\n            if data_missing.dtype == \"float32[pyarrow]\":\n                # map roundtrips through objects, which converts to float64\n                expected = data_missing.to_numpy(dtype=\"float64\", na_value=np.nan)\n            else:\n                expected = data_missing.to_numpy()\n            tm.assert_numpy_array_equal(result, expected)\n\n    def test_astype_str(self, data, request, using_infer_string):\n        pa_dtype = data.dtype.pyarrow_dtype\n        if pa.types.is_binary(pa_dtype):\n            request.applymarker(\n                pytest.mark.xfail(\n                    reason=f\"For {pa_dtype} .astype(str) decodes.\",\n                )\n            )\n        elif not using_infer_string and (\n            (pa.types.is_timestamp(pa_dtype) and pa_dtype.tz is None)\n            or pa.types.is_duration(pa_dtype)\n        ):\n            request.applymarker(\n                pytest.mark.xfail(\n                    reason=\"pd.Timestamp/pd.Timedelta repr different from numpy repr\",\n                )\n            )\n        super().test_astype_str(data)\n\n    def test_from_dtype(self, data, request):\n        pa_dtype = data.dtype.pyarrow_dtype\n        if pa.types.is_string(pa_dtype) or pa.types.is_decimal(pa_dtype):\n            if pa.types.is_string(pa_dtype):\n                reason = \"ArrowDtype(pa.string()) != StringDtype('pyarrow')\"\n            else:\n                reason = f\"pyarrow.type_for_alias cannot infer {pa_dtype}\"\n\n            request.applymarker(\n                pytest.mark.xfail(\n                    reason=reason,\n                )\n            )\n        super().test_from_dtype(data)\n\n    def test_from_sequence_pa_array(self, data):\n        # https://github.com/pandas-dev/pandas/pull/47034#discussion_r955500784\n        # data._pa_array = pa.ChunkedArray\n        result = type(data)._from_sequence(data._pa_array, dtype=data.dtype)\n        tm.assert_extension_array_equal(result, data)\n        assert isinstance(result._pa_array, pa.ChunkedArray)\n\n        result = type(data)._from_sequence(\n            data._pa_array.combine_chunks(), dtype=data.dtype\n        )\n        tm.assert_extension_array_equal(result, data)\n        assert isinstance(result._pa_array, pa.ChunkedArray)\n\n    def test_from_sequence_pa_array_notimplemented(self, request):\n        dtype = ArrowDtype(pa.month_day_nano_interval())\n        with pytest.raises(NotImplementedError, match=\"Converting strings to\"):\n            ArrowExtensionArray._from_sequence_of_strings([\"12-1\"], dtype=dtype)\n\n    def test_from_sequence_of_strings_pa_array(self, data, request):\n        pa_dtype = data.dtype.pyarrow_dtype\n        if pa.types.is_time64(pa_dtype) and pa_dtype.equals(\"time64[ns]\") and not PY311:\n            request.applymarker(\n                pytest.mark.xfail(\n                    reason=\"Nanosecond time parsing not supported.\",\n                )\n            )\n        elif pa_version_under11p0 and (\n            pa.types.is_duration(pa_dtype) or pa.types.is_decimal(pa_dtype)\n        ):\n            request.applymarker(\n                pytest.mark.xfail(\n                    raises=pa.ArrowNotImplementedError,\n                    reason=f\"pyarrow doesn't support parsing {pa_dtype}\",\n                )\n            )\n        elif pa.types.is_timestamp(pa_dtype) and pa_dtype.tz is not None:\n            _require_timezone_database(request)\n\n        pa_array = data._pa_array.cast(pa.string())\n        result = type(data)._from_sequence_of_strings(pa_array, dtype=data.dtype)\n        tm.assert_extension_array_equal(result, data)\n\n        pa_array = pa_array.combine_chunks()\n        result = type(data)._from_sequence_of_strings(pa_array, dtype=data.dtype)\n        tm.assert_extension_array_equal(result, data)\n\n    def check_accumulate(self, ser, op_name, skipna):\n        result = getattr(ser, op_name)(skipna=skipna)\n\n        pa_type = ser.dtype.pyarrow_dtype\n        if pa.types.is_temporal(pa_type):\n            # Just check that we match the integer behavior.\n            if pa_type.bit_width == 32:\n                int_type = \"int32[pyarrow]\"\n            else:\n                int_type = \"int64[pyarrow]\"\n            ser = ser.astype(int_type)\n            result = result.astype(int_type)\n\n        result = result.astype(\"Float64\")\n        expected = getattr(ser.astype(\"Float64\"), op_name)(skipna=skipna)\n        tm.assert_series_equal(result, expected, check_dtype=False)\n\n    def _supports_accumulation(self, ser: pd.Series, op_name: str) -> bool:\n        # error: Item \"dtype[Any]\" of \"dtype[Any] | ExtensionDtype\" has no\n        # attribute \"pyarrow_dtype\"\n        pa_type = ser.dtype.pyarrow_dtype  # type: ignore[union-attr]\n\n        if pa.types.is_binary(pa_type) or pa.types.is_decimal(pa_type):\n            if op_name in [\"cumsum\", \"cumprod\", \"cummax\", \"cummin\"]:\n                return False\n        elif pa.types.is_string(pa_type):\n            if op_name == \"cumprod\":\n                return False\n        elif pa.types.is_boolean(pa_type):\n            if op_name in [\"cumprod\", \"cummax\", \"cummin\"]:\n                return False\n        elif pa.types.is_temporal(pa_type):\n            if op_name == \"cumsum\" and not pa.types.is_duration(pa_type):\n                return False\n            elif op_name == \"cumprod\":\n                return False\n        return True\n\n    @pytest.mark.parametrize(\"skipna\", [True, False])\n    def test_accumulate_series(self, data, all_numeric_accumulations, skipna, request):\n        pa_type = data.dtype.pyarrow_dtype\n        op_name = all_numeric_accumulations\n\n        if pa.types.is_string(pa_type) and op_name in [\"cumsum\", \"cummin\", \"cummax\"]:\n            # https://github.com/pandas-dev/pandas/pull/60633\n            # Doesn't fit test structure, tested in series/test_cumulative.py instead.\n            return\n\n        ser = pd.Series(data)\n\n        if not self._supports_accumulation(ser, op_name):\n            # The base class test will check that we raise\n            return super().test_accumulate_series(\n                data, all_numeric_accumulations, skipna\n            )\n\n        if pa_version_under13p0 and all_numeric_accumulations != \"cumsum\":\n            # xfailing takes a long time to run because pytest\n            # renders the exception messages even when not showing them\n            opt = request.config.option\n            if opt.markexpr and \"not slow\" in opt.markexpr:\n                pytest.skip(\n                    f\"{all_numeric_accumulations} not implemented for pyarrow < 9\"\n                )\n            mark = pytest.mark.xfail(\n                reason=f\"{all_numeric_accumulations} not implemented for pyarrow < 9\"\n            )\n            request.applymarker(mark)\n\n        elif all_numeric_accumulations == \"cumsum\" and (\n            pa.types.is_boolean(pa_type) or pa.types.is_decimal(pa_type)\n        ):\n            request.applymarker(\n                pytest.mark.xfail(\n                    reason=f\"{all_numeric_accumulations} not implemented for {pa_type}\",\n                    raises=TypeError,\n                )\n            )\n\n        self.check_accumulate(ser, op_name, skipna)\n\n    def _supports_reduction(self, ser: pd.Series, op_name: str) -> bool:\n        if op_name in [\"kurt\", \"skew\"]:\n            return False\n\n        dtype = ser.dtype\n        # error: Item \"dtype[Any]\" of \"dtype[Any] | ExtensionDtype\" has\n        # no attribute \"pyarrow_dtype\"\n        pa_dtype = dtype.pyarrow_dtype  # type: ignore[union-attr]\n        if pa.types.is_temporal(pa_dtype) and op_name in [\"sum\", \"var\", \"prod\"]:\n            if pa.types.is_duration(pa_dtype) and op_name in [\"sum\"]:\n                # summing timedeltas is one case that *is* well-defined\n                pass\n            else:\n                return False\n        elif pa.types.is_binary(pa_dtype) and op_name == \"sum\":\n            return False\n        elif (\n            pa.types.is_string(pa_dtype) or pa.types.is_binary(pa_dtype)\n        ) and op_name in [\n            \"mean\",\n            \"median\",\n            \"prod\",\n            \"std\",\n            \"sem\",\n            \"var\",\n        ]:\n            return False\n\n        if (\n            pa.types.is_temporal(pa_dtype)\n            and not pa.types.is_duration(pa_dtype)\n            and op_name in [\"any\", \"all\"]\n        ):\n            # xref GH#34479 we support this in our non-pyarrow datetime64 dtypes,\n            #  but it isn't obvious we _should_.  For now, we keep the pyarrow\n            #  behavior which does not support this.\n            return False\n\n        if pa.types.is_boolean(pa_dtype) and op_name in [\n            \"median\",\n            \"std\",\n            \"var\",\n            \"skew\",\n            \"kurt\",\n            \"sem\",\n        ]:\n            return False\n\n        return True\n\n    def check_reduce(self, ser: pd.Series, op_name: str, skipna: bool):\n        # error: Item \"dtype[Any]\" of \"dtype[Any] | ExtensionDtype\" has no\n        # attribute \"pyarrow_dtype\"\n        pa_dtype = ser.dtype.pyarrow_dtype  # type: ignore[union-attr]\n        if pa.types.is_integer(pa_dtype) or pa.types.is_floating(pa_dtype):\n            alt = ser.astype(\"Float64\")\n        else:\n            # TODO: in the opposite case, aren't we testing... nothing? For\n            # e.g. date/time dtypes trying to calculate 'expected' by converting\n            # to object will raise for mean, std etc\n            alt = ser\n\n        # TODO: in the opposite case, aren't we testing... nothing?\n        if op_name == \"count\":\n            result = getattr(ser, op_name)()\n            expected = getattr(alt, op_name)()\n        else:\n            result = getattr(ser, op_name)(skipna=skipna)\n            expected = getattr(alt, op_name)(skipna=skipna)\n        tm.assert_almost_equal(result, expected)\n\n    @pytest.mark.parametrize(\"skipna\", [True, False])\n    def test_reduce_series_boolean(\n        self, data, all_boolean_reductions, skipna, na_value, request\n    ):\n        pa_dtype = data.dtype.pyarrow_dtype\n        xfail_mark = pytest.mark.xfail(\n            raises=TypeError,\n            reason=(\n                f\"{all_boolean_reductions} is not implemented in \"\n                f\"pyarrow={pa.__version__} for {pa_dtype}\"\n            ),\n        )\n        if pa.types.is_string(pa_dtype) or pa.types.is_binary(pa_dtype):\n            # We *might* want to make this behave like the non-pyarrow cases,\n            #  but have not yet decided.\n            request.applymarker(xfail_mark)\n\n        return super().test_reduce_series_boolean(data, all_boolean_reductions, skipna)\n\n    def _get_expected_reduction_dtype(self, arr, op_name: str, skipna: bool):\n        pa_type = arr._pa_array.type\n\n        if op_name in [\"max\", \"min\"]:\n            cmp_dtype = arr.dtype\n        elif pa.types.is_temporal(pa_type):\n            if op_name in [\"std\", \"sem\"]:\n                if pa.types.is_duration(pa_type):\n                    cmp_dtype = arr.dtype\n                elif pa.types.is_date(pa_type):\n                    cmp_dtype = ArrowDtype(pa.duration(\"s\"))\n                elif pa.types.is_time(pa_type):\n                    unit = get_unit_from_pa_dtype(pa_type)\n                    cmp_dtype = ArrowDtype(pa.duration(unit))\n                else:\n                    cmp_dtype = ArrowDtype(pa.duration(pa_type.unit))\n            else:\n                cmp_dtype = arr.dtype\n        elif arr.dtype.name == \"decimal128(7, 3)[pyarrow]\":\n            if op_name not in [\"median\", \"var\", \"std\", \"sem\"]:\n                cmp_dtype = arr.dtype\n            else:\n                cmp_dtype = \"float64[pyarrow]\"\n        elif op_name in [\"median\", \"var\", \"std\", \"mean\", \"skew\", \"sem\"]:\n            cmp_dtype = \"float64[pyarrow]\"\n        elif op_name in [\"sum\", \"prod\"] and pa.types.is_boolean(pa_type):\n            cmp_dtype = \"uint64[pyarrow]\"\n        elif op_name == \"sum\" and pa.types.is_string(pa_type):\n            cmp_dtype = arr.dtype\n        else:\n            cmp_dtype = {\n                \"i\": \"int64[pyarrow]\",\n                \"u\": \"uint64[pyarrow]\",\n                \"f\": \"float64[pyarrow]\",\n            }[arr.dtype.kind]\n        return cmp_dtype\n\n    @pytest.mark.parametrize(\"skipna\", [True, False])\n    def test_reduce_frame(self, data, all_numeric_reductions, skipna, request):\n        op_name = all_numeric_reductions\n        if op_name == \"skew\":\n            if data.dtype._is_numeric:\n                mark = pytest.mark.xfail(reason=\"skew not implemented\")\n                request.applymarker(mark)\n        elif (\n            op_name in [\"std\", \"sem\"]\n            and pa.types.is_date64(data._pa_array.type)\n            and skipna\n        ):\n            # overflow\n            mark = pytest.mark.xfail(reason=\"Cannot cast\")\n            request.applymarker(mark)\n        return super().test_reduce_frame(data, all_numeric_reductions, skipna)\n\n    @pytest.mark.parametrize(\"typ\", [\"int64\", \"uint64\", \"float64\"])\n    def test_median_not_approximate(self, typ):\n        # GH 52679\n        result = pd.Series([1, 2], dtype=f\"{typ}[pyarrow]\").median()\n        assert result == 1.5\n\n    def test_construct_from_string_own_name(self, dtype, request):\n        pa_dtype = dtype.pyarrow_dtype\n        if pa.types.is_decimal(pa_dtype):\n            request.applymarker(\n                pytest.mark.xfail(\n                    raises=NotImplementedError,\n                    reason=f\"pyarrow.type_for_alias cannot infer {pa_dtype}\",\n                )\n            )\n\n        if pa.types.is_string(pa_dtype):\n            # We still support StringDtype('pyarrow') over ArrowDtype(pa.string())\n            msg = r\"string\\[pyarrow\\] should be constructed by StringDtype\"\n            with pytest.raises(TypeError, match=msg):\n                dtype.construct_from_string(dtype.name)\n\n            return\n\n        super().test_construct_from_string_own_name(dtype)\n\n    def test_is_dtype_from_name(self, dtype, request):\n        pa_dtype = dtype.pyarrow_dtype\n        if pa.types.is_string(pa_dtype):\n            # We still support StringDtype('pyarrow') over ArrowDtype(pa.string())\n            assert not type(dtype).is_dtype(dtype.name)\n        else:\n            if pa.types.is_decimal(pa_dtype):\n                request.applymarker(\n                    pytest.mark.xfail(\n                        raises=NotImplementedError,\n                        reason=f\"pyarrow.type_for_alias cannot infer {pa_dtype}\",\n                    )\n                )\n            super().test_is_dtype_from_name(dtype)\n\n    def test_construct_from_string_another_type_raises(self, dtype):\n        msg = r\"'another_type' must end with '\\[pyarrow\\]'\"\n        with pytest.raises(TypeError, match=msg):\n            type(dtype).construct_from_string(\"another_type\")\n\n    def test_get_common_dtype(self, dtype, request):\n        pa_dtype = dtype.pyarrow_dtype\n        if (\n            pa.types.is_date(pa_dtype)\n            or pa.types.is_time(pa_dtype)\n            or (pa.types.is_timestamp(pa_dtype) and pa_dtype.tz is not None)\n            or pa.types.is_binary(pa_dtype)\n            or pa.types.is_decimal(pa_dtype)\n        ):\n            request.applymarker(\n                pytest.mark.xfail(\n                    reason=(\n                        f\"{pa_dtype} does not have associated numpy \"\n                        f\"dtype findable by find_common_type\"\n                    )\n                )\n            )\n        super().test_get_common_dtype(dtype)\n\n    def test_is_not_string_type(self, dtype):\n        pa_dtype = dtype.pyarrow_dtype\n        if pa.types.is_string(pa_dtype):\n            assert is_string_dtype(dtype)\n        else:\n            super().test_is_not_string_type(dtype)\n\n    @pytest.mark.xfail(\n        reason=\"GH 45419: pyarrow.ChunkedArray does not support views.\", run=False\n    )\n    def test_view(self, data):\n        super().test_view(data)\n\n    def test_fillna_no_op_returns_copy(self, data):\n        data = data[~data.isna()]\n\n        valid = data[0]\n        result = data.fillna(valid)\n        assert result is not data\n        tm.assert_extension_array_equal(result, data)\n\n    @pytest.mark.xfail(\n        reason=\"GH 45419: pyarrow.ChunkedArray does not support views\", run=False\n    )\n    def test_transpose(self, data):\n        super().test_transpose(data)\n\n    @pytest.mark.xfail(\n        reason=\"GH 45419: pyarrow.ChunkedArray does not support views\", run=False\n    )\n    def test_setitem_preserves_views(self, data):\n        super().test_setitem_preserves_views(data)\n\n    @pytest.mark.parametrize(\"dtype_backend\", [\"pyarrow\", no_default])\n    @pytest.mark.parametrize(\"engine\", [\"c\", \"python\"])\n    def test_EA_types(self, engine, data, dtype_backend, request):\n        pa_dtype = data.dtype.pyarrow_dtype\n        if pa.types.is_decimal(pa_dtype):\n            request.applymarker(\n                pytest.mark.xfail(\n                    raises=NotImplementedError,\n                    reason=f\"Parameterized types {pa_dtype} not supported.\",\n                )\n            )\n        elif pa.types.is_timestamp(pa_dtype) and pa_dtype.unit in (\"us\", \"ns\"):\n            request.applymarker(\n                pytest.mark.xfail(\n                    raises=ValueError,\n                    reason=\"https://github.com/pandas-dev/pandas/issues/49767\",\n                )\n            )\n        elif pa.types.is_binary(pa_dtype):\n            request.applymarker(\n                pytest.mark.xfail(reason=\"CSV parsers don't correctly handle binary\")\n            )\n        df = pd.DataFrame({\"with_dtype\": pd.Series(data, dtype=str(data.dtype))})\n        csv_output = df.to_csv(index=False, na_rep=np.nan)\n        if pa.types.is_binary(pa_dtype):\n            csv_output = BytesIO(csv_output)\n        else:\n            csv_output = StringIO(csv_output)\n        result = pd.read_csv(\n            csv_output,\n            dtype={\"with_dtype\": str(data.dtype)},\n            engine=engine,\n            dtype_backend=dtype_backend,\n        )\n        expected = df\n        tm.assert_frame_equal(result, expected)\n\n    def test_invert(self, data, request):\n        pa_dtype = data.dtype.pyarrow_dtype\n        if not (\n            pa.types.is_boolean(pa_dtype)\n            or pa.types.is_integer(pa_dtype)\n            or pa.types.is_string(pa_dtype)\n        ):\n            request.applymarker(\n                pytest.mark.xfail(\n                    raises=pa.ArrowNotImplementedError,\n                    reason=f\"pyarrow.compute.invert does support {pa_dtype}\",\n                )\n            )\n        if PY312 and pa.types.is_boolean(pa_dtype):\n            with tm.assert_produces_warning(\n                DeprecationWarning, match=\"Bitwise inversion\", check_stacklevel=False\n            ):\n                super().test_invert(data)\n        else:\n            super().test_invert(data)\n\n    @pytest.mark.parametrize(\"periods\", [1, -2])\n    def test_diff(self, data, periods, request):\n        pa_dtype = data.dtype.pyarrow_dtype\n        if pa.types.is_unsigned_integer(pa_dtype) and periods == 1:\n            request.applymarker(\n                pytest.mark.xfail(\n                    raises=pa.ArrowInvalid,\n                    reason=(\n                        f\"diff with {pa_dtype} and periods={periods} will overflow\"\n                    ),\n                )\n            )\n        super().test_diff(data, periods)\n\n    def test_value_counts_returns_pyarrow_int64(self, data):\n        # GH 51462\n        data = data[:10]\n        result = data.value_counts()\n        assert result.dtype == ArrowDtype(pa.int64())\n\n    _combine_le_expected_dtype = \"bool[pyarrow]\"\n\n    def get_op_from_name(self, op_name):\n        short_opname = op_name.strip(\"_\")\n        if short_opname == \"rtruediv\":\n            # use the numpy version that won't raise on division by zero\n\n            def rtruediv(x, y):\n                return np.divide(y, x)\n\n            return rtruediv\n        elif short_opname == \"rfloordiv\":\n            return lambda x, y: np.floor_divide(y, x)\n\n        return tm.get_op_from_name(op_name)\n\n    def _cast_pointwise_result(self, op_name: str, obj, other, pointwise_result):\n        # BaseOpsUtil._combine can upcast expected dtype\n        # (because it generates expected on python scalars)\n        # while ArrowExtensionArray maintains original type\n        expected = pointwise_result\n\n        if op_name in [\"eq\", \"ne\", \"lt\", \"le\", \"gt\", \"ge\"]:\n            return pointwise_result.astype(\"boolean[pyarrow]\")\n\n        was_frame = False\n        if isinstance(expected, pd.DataFrame):\n            was_frame = True\n            expected_data = expected.iloc[:, 0]\n            original_dtype = obj.iloc[:, 0].dtype\n        else:\n            expected_data = expected\n            original_dtype = obj.dtype\n\n        orig_pa_type = original_dtype.pyarrow_dtype\n        if not was_frame and isinstance(other, pd.Series):\n            # i.e. test_arith_series_with_array\n            if not (\n                pa.types.is_floating(orig_pa_type)\n                or (\n                    pa.types.is_integer(orig_pa_type)\n                    and op_name not in [\"__truediv__\", \"__rtruediv__\"]\n                )\n                or pa.types.is_duration(orig_pa_type)\n                or pa.types.is_timestamp(orig_pa_type)\n                or pa.types.is_date(orig_pa_type)\n                or pa.types.is_decimal(orig_pa_type)\n            ):\n                # base class _combine always returns int64, while\n                #  ArrowExtensionArray does not upcast\n                return expected\n        elif not (\n            (op_name == \"__floordiv__\" and pa.types.is_integer(orig_pa_type))\n            or pa.types.is_duration(orig_pa_type)\n            or pa.types.is_timestamp(orig_pa_type)\n            or pa.types.is_date(orig_pa_type)\n            or pa.types.is_decimal(orig_pa_type)\n        ):\n            # base class _combine always returns int64, while\n            #  ArrowExtensionArray does not upcast\n            return expected\n\n        pa_expected = pa.array(expected_data._values)\n\n        if pa.types.is_duration(pa_expected.type):\n            if pa.types.is_date(orig_pa_type):\n                if pa.types.is_date64(orig_pa_type):\n                    # TODO: why is this different vs date32?\n                    unit = \"ms\"\n                else:\n                    unit = \"s\"\n            else:\n                # pyarrow sees sequence of datetime/timedelta objects and defaults\n                #  to \"us\" but the non-pointwise op retains unit\n                # timestamp or duration\n                unit = orig_pa_type.unit\n                if type(other) in [datetime, timedelta] and unit in [\"s\", \"ms\"]:\n                    # pydatetime/pytimedelta objects have microsecond reso, so we\n                    #  take the higher reso of the original and microsecond. Note\n                    #  this matches what we would do with DatetimeArray/TimedeltaArray\n                    unit = \"us\"\n\n            pa_expected = pa_expected.cast(f\"duration[{unit}]\")\n\n        elif pa.types.is_decimal(pa_expected.type) and pa.types.is_decimal(\n            orig_pa_type\n        ):\n            # decimal precision can resize in the result type depending on data\n            # just compare the float values\n            alt = getattr(obj, op_name)(other)\n            alt_dtype = tm.get_dtype(alt)\n            assert isinstance(alt_dtype, ArrowDtype)\n            if op_name == \"__pow__\" and isinstance(other, Decimal):\n                # TODO: would it make more sense to retain Decimal here?\n                alt_dtype = ArrowDtype(pa.float64())\n            elif (\n                op_name == \"__pow__\"\n                and isinstance(other, pd.Series)\n                and other.dtype == original_dtype\n            ):\n                # TODO: would it make more sense to retain Decimal here?\n                alt_dtype = ArrowDtype(pa.float64())\n            else:\n                assert pa.types.is_decimal(alt_dtype.pyarrow_dtype)\n            return expected.astype(alt_dtype)\n\n        else:\n            pa_expected = pa_expected.cast(orig_pa_type)\n\n        pd_expected = type(expected_data._values)(pa_expected)\n        if was_frame:\n            expected = pd.DataFrame(\n                pd_expected, index=expected.index, columns=expected.columns\n            )\n        else:\n            expected = pd.Series(pd_expected)\n        return expected\n\n    def _is_temporal_supported(self, opname, pa_dtype):\n        return (\n            (\n                opname in (\"__add__\", \"__radd__\")\n                or (\n                    opname\n                    in (\"__truediv__\", \"__rtruediv__\", \"__floordiv__\", \"__rfloordiv__\")\n                    and not pa_version_under14p0\n                )\n            )\n            and pa.types.is_duration(pa_dtype)\n        ) or (opname in (\"__sub__\", \"__rsub__\") and pa.types.is_temporal(pa_dtype))\n\n    def _get_expected_exception(\n        self, op_name: str, obj, other\n    ) -> type[Exception] | tuple[type[Exception], ...] | None:\n        if op_name in (\"__divmod__\", \"__rdivmod__\"):\n            return (NotImplementedError, TypeError)\n\n        exc: type[Exception] | tuple[type[Exception], ...] | None\n        dtype = tm.get_dtype(obj)\n        # error: Item \"dtype[Any]\" of \"dtype[Any] | ExtensionDtype\" has no\n        # attribute \"pyarrow_dtype\"\n        pa_dtype = dtype.pyarrow_dtype  # type: ignore[union-attr]\n\n        arrow_temporal_supported = self._is_temporal_supported(op_name, pa_dtype)\n        if op_name in {\n            \"__mod__\",\n            \"__rmod__\",\n        }:\n            exc = (NotImplementedError, TypeError)\n        elif arrow_temporal_supported:\n            exc = None\n        elif op_name in [\"__add__\", \"__radd__\"] and (\n            pa.types.is_string(pa_dtype) or pa.types.is_binary(pa_dtype)\n        ):\n            exc = None\n        elif not (\n            pa.types.is_floating(pa_dtype)\n            or pa.types.is_integer(pa_dtype)\n            or pa.types.is_decimal(pa_dtype)\n        ):\n            exc = TypeError\n        else:\n            exc = None\n        return exc\n\n    def _get_arith_xfail_marker(self, opname, pa_dtype):\n        mark = None\n\n        arrow_temporal_supported = self._is_temporal_supported(opname, pa_dtype)\n\n        if opname == \"__rpow__\" and (\n            pa.types.is_floating(pa_dtype)\n            or pa.types.is_integer(pa_dtype)\n            or pa.types.is_decimal(pa_dtype)\n        ):\n            mark = pytest.mark.xfail(\n                reason=(\n                    f\"GH#29997: 1**pandas.NA == 1 while 1**pyarrow.NA == NULL \"\n                    f\"for {pa_dtype}\"\n                )\n            )\n        elif arrow_temporal_supported and (\n            pa.types.is_time(pa_dtype)\n            or (\n                opname\n                in (\"__truediv__\", \"__rtruediv__\", \"__floordiv__\", \"__rfloordiv__\")\n                and pa.types.is_duration(pa_dtype)\n            )\n        ):\n            mark = pytest.mark.xfail(\n                raises=TypeError,\n                reason=(\n                    f\"{opname} not supported betweenpd.NA and {pa_dtype} Python scalar\"\n                ),\n            )\n        elif opname == \"__rfloordiv__\" and (\n            pa.types.is_integer(pa_dtype) or pa.types.is_decimal(pa_dtype)\n        ):\n            mark = pytest.mark.xfail(\n                raises=pa.ArrowInvalid,\n                reason=\"divide by 0\",\n            )\n        elif opname == \"__rtruediv__\" and pa.types.is_decimal(pa_dtype):\n            mark = pytest.mark.xfail(\n                raises=pa.ArrowInvalid,\n                reason=\"divide by 0\",\n            )\n\n        return mark\n\n    def test_arith_series_with_scalar(self, data, all_arithmetic_operators, request):\n        pa_dtype = data.dtype.pyarrow_dtype\n\n        if all_arithmetic_operators == \"__rmod__\" and pa.types.is_binary(pa_dtype):\n            pytest.skip(\"Skip testing Python string formatting\")\n\n        mark = self._get_arith_xfail_marker(all_arithmetic_operators, pa_dtype)\n        if mark is not None:\n            request.applymarker(mark)\n\n        super().test_arith_series_with_scalar(data, all_arithmetic_operators)\n\n    def test_arith_frame_with_scalar(self, data, all_arithmetic_operators, request):\n        pa_dtype = data.dtype.pyarrow_dtype\n\n        if all_arithmetic_operators == \"__rmod__\" and (\n            pa.types.is_string(pa_dtype) or pa.types.is_binary(pa_dtype)\n        ):\n            pytest.skip(\"Skip testing Python string formatting\")\n\n        mark = self._get_arith_xfail_marker(all_arithmetic_operators, pa_dtype)\n        if mark is not None:\n            request.applymarker(mark)\n\n        super().test_arith_frame_with_scalar(data, all_arithmetic_operators)\n\n    def test_arith_series_with_array(self, data, all_arithmetic_operators, request):\n        pa_dtype = data.dtype.pyarrow_dtype\n\n        if all_arithmetic_operators in (\n            \"__sub__\",\n            \"__rsub__\",\n        ) and pa.types.is_unsigned_integer(pa_dtype):\n            request.applymarker(\n                pytest.mark.xfail(\n                    raises=pa.ArrowInvalid,\n                    reason=(\n                        f\"Implemented pyarrow.compute.subtract_checked \"\n                        f\"which raises on overflow for {pa_dtype}\"\n                    ),\n                )\n            )\n\n        mark = self._get_arith_xfail_marker(all_arithmetic_operators, pa_dtype)\n        if mark is not None:\n            request.applymarker(mark)\n\n        op_name = all_arithmetic_operators\n        ser = pd.Series(data)\n        # pd.Series([ser.iloc[0]] * len(ser)) may not return ArrowExtensionArray\n        # since ser.iloc[0] is a python scalar\n        other = pd.Series(pd.array([ser.iloc[0]] * len(ser), dtype=data.dtype))\n\n        self.check_opname(ser, op_name, other)\n\n    def test_add_series_with_extension_array(self, data, request):\n        pa_dtype = data.dtype.pyarrow_dtype\n\n        if pa_dtype.equals(\"int8\"):\n            request.applymarker(\n                pytest.mark.xfail(\n                    raises=pa.ArrowInvalid,\n                    reason=f\"raises on overflow for {pa_dtype}\",\n                )\n            )\n        super().test_add_series_with_extension_array(data)\n\n    def test_invalid_other_comp(self, data, comparison_op):\n        # GH 48833\n        with pytest.raises(\n            NotImplementedError, match=\".* not implemented for <class 'object'>\"\n        ):\n            comparison_op(data, object())\n\n    @pytest.mark.parametrize(\"masked_dtype\", [\"boolean\", \"Int64\", \"Float64\"])\n    def test_comp_masked_numpy(self, masked_dtype, comparison_op):\n        # GH 52625\n        data = [1, 0, None]\n        ser_masked = pd.Series(data, dtype=masked_dtype)\n        ser_pa = pd.Series(data, dtype=f\"{masked_dtype.lower()}[pyarrow]\")\n        result = comparison_op(ser_pa, ser_masked)\n        if comparison_op in [operator.lt, operator.gt, operator.ne]:\n            exp = [False, False, None]\n        else:\n            exp = [True, True, None]\n        expected = pd.Series(exp, dtype=ArrowDtype(pa.bool_()))\n        tm.assert_series_equal(result, expected)\n\n\nclass TestLogicalOps:\n    \"\"\"Various Series and DataFrame logical ops methods.\"\"\"\n\n    def test_kleene_or(self):\n        a = pd.Series([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean[pyarrow]\")\n        b = pd.Series([True, False, None] * 3, dtype=\"boolean[pyarrow]\")\n        result = a | b\n        expected = pd.Series(\n            [True, True, True, True, False, None, True, None, None],\n            dtype=\"boolean[pyarrow]\",\n        )\n        tm.assert_series_equal(result, expected)\n\n        result = b | a\n        tm.assert_series_equal(result, expected)\n\n        # ensure we haven't mutated anything inplace\n        tm.assert_series_equal(\n            a,\n            pd.Series([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean[pyarrow]\"),\n        )\n        tm.assert_series_equal(\n            b, pd.Series([True, False, None] * 3, dtype=\"boolean[pyarrow]\")\n        )\n\n    @pytest.mark.parametrize(\n        \"other, expected\",\n        [\n            (None, [True, None, None]),\n            (pd.NA, [True, None, None]),\n            (True, [True, True, True]),\n            (np.bool_(True), [True, True, True]),\n            (False, [True, False, None]),\n            (np.bool_(False), [True, False, None]),\n        ],\n    )\n    def test_kleene_or_scalar(self, other, expected):\n        a = pd.Series([True, False, None], dtype=\"boolean[pyarrow]\")\n        result = a | other\n        expected = pd.Series(expected, dtype=\"boolean[pyarrow]\")\n        tm.assert_series_equal(result, expected)\n\n        result = other | a\n        tm.assert_series_equal(result, expected)\n\n        # ensure we haven't mutated anything inplace\n        tm.assert_series_equal(\n            a, pd.Series([True, False, None], dtype=\"boolean[pyarrow]\")\n        )\n\n    def test_kleene_and(self):\n        a = pd.Series([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean[pyarrow]\")\n        b = pd.Series([True, False, None] * 3, dtype=\"boolean[pyarrow]\")\n        result = a & b\n        expected = pd.Series(\n            [True, False, None, False, False, False, None, False, None],\n            dtype=\"boolean[pyarrow]\",\n        )\n        tm.assert_series_equal(result, expected)\n\n        result = b & a\n        tm.assert_series_equal(result, expected)\n\n        # ensure we haven't mutated anything inplace\n        tm.assert_series_equal(\n            a,\n            pd.Series([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean[pyarrow]\"),\n        )\n        tm.assert_series_equal(\n            b, pd.Series([True, False, None] * 3, dtype=\"boolean[pyarrow]\")\n        )\n\n    @pytest.mark.parametrize(\n        \"other, expected\",\n        [\n            (None, [None, False, None]),\n            (pd.NA, [None, False, None]),\n            (True, [True, False, None]),\n            (False, [False, False, False]),\n            (np.bool_(True), [True, False, None]),\n            (np.bool_(False), [False, False, False]),\n        ],\n    )\n    def test_kleene_and_scalar(self, other, expected):\n        a = pd.Series([True, False, None], dtype=\"boolean[pyarrow]\")\n        result = a & other\n        expected = pd.Series(expected, dtype=\"boolean[pyarrow]\")\n        tm.assert_series_equal(result, expected)\n\n        result = other & a\n        tm.assert_series_equal(result, expected)\n\n        # ensure we haven't mutated anything inplace\n        tm.assert_series_equal(\n            a, pd.Series([True, False, None], dtype=\"boolean[pyarrow]\")\n        )\n\n    def test_kleene_xor(self):\n        a = pd.Series([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean[pyarrow]\")\n        b = pd.Series([True, False, None] * 3, dtype=\"boolean[pyarrow]\")\n        result = a ^ b\n        expected = pd.Series(\n            [False, True, None, True, False, None, None, None, None],\n            dtype=\"boolean[pyarrow]\",\n        )\n        tm.assert_series_equal(result, expected)\n\n        result = b ^ a\n        tm.assert_series_equal(result, expected)\n\n        # ensure we haven't mutated anything inplace\n        tm.assert_series_equal(\n            a,\n            pd.Series([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean[pyarrow]\"),\n        )\n        tm.assert_series_equal(\n            b, pd.Series([True, False, None] * 3, dtype=\"boolean[pyarrow]\")\n        )\n\n    @pytest.mark.parametrize(\n        \"other, expected\",\n        [\n            (None, [None, None, None]),\n            (pd.NA, [None, None, None]),\n            (True, [False, True, None]),\n            (np.bool_(True), [False, True, None]),\n            (np.bool_(False), [True, False, None]),\n        ],\n    )\n    def test_kleene_xor_scalar(self, other, expected):\n        a = pd.Series([True, False, None], dtype=\"boolean[pyarrow]\")\n        result = a ^ other\n        expected = pd.Series(expected, dtype=\"boolean[pyarrow]\")\n        tm.assert_series_equal(result, expected)\n\n        result = other ^ a\n        tm.assert_series_equal(result, expected)\n\n        # ensure we haven't mutated anything inplace\n        tm.assert_series_equal(\n            a, pd.Series([True, False, None], dtype=\"boolean[pyarrow]\")\n        )\n\n    @pytest.mark.parametrize(\n        \"op, exp\",\n        [\n            [\"__and__\", True],\n            [\"__or__\", True],\n            [\"__xor__\", False],\n        ],\n    )\n    def test_logical_masked_numpy(self, op, exp):\n        # GH 52625\n        data = [True, False, None]\n        ser_masked = pd.Series(data, dtype=\"boolean\")\n        ser_pa = pd.Series(data, dtype=\"boolean[pyarrow]\")\n        result = getattr(ser_pa, op)(ser_masked)\n        expected = pd.Series([exp, False, None], dtype=ArrowDtype(pa.bool_()))\n        tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"pa_type\", tm.ALL_INT_PYARROW_DTYPES)\ndef test_bitwise(pa_type):\n    # GH 54495\n    dtype = ArrowDtype(pa_type)\n    left = pd.Series([1, None, 3, 4], dtype=dtype)\n    right = pd.Series([None, 3, 5, 4], dtype=dtype)\n\n    result = left | right\n    expected = pd.Series([None, None, 3 | 5, 4 | 4], dtype=dtype)\n    tm.assert_series_equal(result, expected)\n\n    result = left & right\n    expected = pd.Series([None, None, 3 & 5, 4 & 4], dtype=dtype)\n    tm.assert_series_equal(result, expected)\n\n    result = left ^ right\n    expected = pd.Series([None, None, 3 ^ 5, 4 ^ 4], dtype=dtype)\n    tm.assert_series_equal(result, expected)\n\n    result = ~left\n    expected = ~(left.fillna(0).to_numpy())\n    expected = pd.Series(expected, dtype=dtype).mask(left.isnull())\n    tm.assert_series_equal(result, expected)\n\n\ndef test_arrowdtype_construct_from_string_type_with_unsupported_parameters():\n    with pytest.raises(NotImplementedError, match=\"Passing pyarrow type\"):\n        ArrowDtype.construct_from_string(\"not_a_real_dype[s, tz=UTC][pyarrow]\")\n\n    with pytest.raises(NotImplementedError, match=\"Passing pyarrow type\"):\n        ArrowDtype.construct_from_string(\"decimal(7, 2)[pyarrow]\")\n\n\ndef test_arrowdtype_construct_from_string_supports_dt64tz():\n    # as of GH#50689, timestamptz is supported\n    dtype = ArrowDtype.construct_from_string(\"timestamp[s, tz=UTC][pyarrow]\")\n    expected = ArrowDtype(pa.timestamp(\"s\", \"UTC\"))\n    assert dtype == expected\n\n\ndef test_arrowdtype_construct_from_string_type_only_one_pyarrow():\n    # GH#51225\n    invalid = \"int64[pyarrow]foobar[pyarrow]\"\n    msg = (\n        r\"Passing pyarrow type specific parameters \\(\\[pyarrow\\]\\) in the \"\n        r\"string is not supported\\.\"\n    )\n    with pytest.raises(NotImplementedError, match=msg):\n        pd.Series(range(3), dtype=invalid)\n\n\ndef test_arrow_string_multiplication():\n    # GH 56537\n    binary = pd.Series([\"abc\", \"defg\"], dtype=ArrowDtype(pa.string()))\n    repeat = pd.Series([2, -2], dtype=\"int64[pyarrow]\")\n    result = binary * repeat\n    expected = pd.Series([\"abcabc\", \"\"], dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n    reflected_result = repeat * binary\n    tm.assert_series_equal(result, reflected_result)\n\n\ndef test_arrow_string_multiplication_scalar_repeat():\n    binary = pd.Series([\"abc\", \"defg\"], dtype=ArrowDtype(pa.string()))\n    result = binary * 2\n    expected = pd.Series([\"abcabc\", \"defgdefg\"], dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n    reflected_result = 2 * binary\n    tm.assert_series_equal(reflected_result, expected)\n\n\n@pytest.mark.parametrize(\n    \"interpolation\", [\"linear\", \"lower\", \"higher\", \"nearest\", \"midpoint\"]\n)\n@pytest.mark.parametrize(\"quantile\", [0.5, [0.5, 0.5]])\ndef test_quantile(data, interpolation, quantile, request):\n    pa_dtype = data.dtype.pyarrow_dtype\n\n    data = data.take([0, 0, 0])\n    ser = pd.Series(data)\n\n    if (\n        pa.types.is_string(pa_dtype)\n        or pa.types.is_binary(pa_dtype)\n        or pa.types.is_boolean(pa_dtype)\n    ):\n        # For string, bytes, and bool, we don't *expect* to have quantile work\n        # Note this matches the non-pyarrow behavior\n        msg = r\"Function 'quantile' has no kernel matching input types \\(.*\\)\"\n        with pytest.raises(pa.ArrowNotImplementedError, match=msg):\n            ser.quantile(q=quantile, interpolation=interpolation)\n        return\n\n    if (\n        pa.types.is_integer(pa_dtype)\n        or pa.types.is_floating(pa_dtype)\n        or pa.types.is_decimal(pa_dtype)\n    ):\n        pass\n    elif pa.types.is_temporal(data._pa_array.type):\n        pass\n    else:\n        request.applymarker(\n            pytest.mark.xfail(\n                raises=pa.ArrowNotImplementedError,\n                reason=f\"quantile not supported by pyarrow for {pa_dtype}\",\n            )\n        )\n    data = data.take([0, 0, 0])\n    ser = pd.Series(data)\n    result = ser.quantile(q=quantile, interpolation=interpolation)\n\n    if pa.types.is_timestamp(pa_dtype) and interpolation not in [\"lower\", \"higher\"]:\n        # rounding error will make the check below fail\n        #  (e.g. '2020-01-01 01:01:01.000001' vs '2020-01-01 01:01:01.000001024'),\n        #  so we'll check for now that we match the numpy analogue\n        if pa_dtype.tz:\n            pd_dtype = f\"M8[{pa_dtype.unit}, {pa_dtype.tz}]\"\n        else:\n            pd_dtype = f\"M8[{pa_dtype.unit}]\"\n        ser_np = ser.astype(pd_dtype)\n\n        expected = ser_np.quantile(q=quantile, interpolation=interpolation)\n        if quantile == 0.5:\n            if pa_dtype.unit == \"us\":\n                expected = expected.to_pydatetime(warn=False)\n            assert result == expected\n        else:\n            if pa_dtype.unit == \"us\":\n                expected = expected.dt.floor(\"us\")\n            tm.assert_series_equal(result, expected.astype(data.dtype))\n        return\n\n    if quantile == 0.5:\n        assert result == data[0]\n    else:\n        # Just check the values\n        expected = pd.Series(data.take([0, 0]), index=[0.5, 0.5])\n        if (\n            pa.types.is_integer(pa_dtype)\n            or pa.types.is_floating(pa_dtype)\n            or pa.types.is_decimal(pa_dtype)\n        ):\n            expected = expected.astype(\"float64[pyarrow]\")\n            result = result.astype(\"float64[pyarrow]\")\n        tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"take_idx, exp_idx\",\n    [[[0, 0, 2, 2, 4, 4], [4, 0]], [[0, 0, 0, 2, 4, 4], [0]]],\n    ids=[\"multi_mode\", \"single_mode\"],\n)\ndef test_mode_dropna_true(data_for_grouping, take_idx, exp_idx):\n    data = data_for_grouping.take(take_idx)\n    ser = pd.Series(data)\n    result = ser.mode(dropna=True)\n    expected = pd.Series(data_for_grouping.take(exp_idx))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_mode_dropna_false_mode_na(data):\n    # GH 50982\n    more_nans = pd.Series([None, None, data[0]], dtype=data.dtype)\n    result = more_nans.mode(dropna=False)\n    expected = pd.Series([None], dtype=data.dtype)\n    tm.assert_series_equal(result, expected)\n\n    expected = pd.Series([data[0], None], dtype=data.dtype)\n    result = expected.mode(dropna=False)\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"arrow_dtype, expected_type\",\n    [\n        [pa.binary(), bytes],\n        [pa.binary(16), bytes],\n        [pa.large_binary(), bytes],\n        [pa.large_string(), str],\n        [pa.list_(pa.int64()), list],\n        [pa.large_list(pa.int64()), list],\n        [pa.map_(pa.string(), pa.int64()), list],\n        [pa.struct([(\"f1\", pa.int8()), (\"f2\", pa.string())]), dict],\n        [pa.dictionary(pa.int64(), pa.int64()), CategoricalDtypeType],\n    ],\n)\ndef test_arrow_dtype_type(arrow_dtype, expected_type):\n    # GH 51845\n    # TODO: Redundant with test_getitem_scalar once arrow_dtype exists in data fixture\n    assert ArrowDtype(arrow_dtype).type == expected_type\n\n\ndef test_is_bool_dtype():\n    # GH 22667\n    data = ArrowExtensionArray(pa.array([True, False, True]))\n    assert is_bool_dtype(data)\n    assert pd.core.common.is_bool_indexer(data)\n    s = pd.Series(range(len(data)))\n    result = s[data]\n    expected = s[np.asarray(data)]\n    tm.assert_series_equal(result, expected)\n\n\ndef test_is_numeric_dtype(data):\n    # GH 50563\n    pa_type = data.dtype.pyarrow_dtype\n    if (\n        pa.types.is_floating(pa_type)\n        or pa.types.is_integer(pa_type)\n        or pa.types.is_decimal(pa_type)\n    ):\n        assert is_numeric_dtype(data)\n    else:\n        assert not is_numeric_dtype(data)\n\n\ndef test_is_integer_dtype(data):\n    # GH 50667\n    pa_type = data.dtype.pyarrow_dtype\n    if pa.types.is_integer(pa_type):\n        assert is_integer_dtype(data)\n    else:\n        assert not is_integer_dtype(data)\n\n\ndef test_is_signed_integer_dtype(data):\n    pa_type = data.dtype.pyarrow_dtype\n    if pa.types.is_signed_integer(pa_type):\n        assert is_signed_integer_dtype(data)\n    else:\n        assert not is_signed_integer_dtype(data)\n\n\ndef test_is_unsigned_integer_dtype(data):\n    pa_type = data.dtype.pyarrow_dtype\n    if pa.types.is_unsigned_integer(pa_type):\n        assert is_unsigned_integer_dtype(data)\n    else:\n        assert not is_unsigned_integer_dtype(data)\n\n\ndef test_is_datetime64_any_dtype(data):\n    pa_type = data.dtype.pyarrow_dtype\n    if pa.types.is_timestamp(pa_type) or pa.types.is_date(pa_type):\n        assert is_datetime64_any_dtype(data)\n    else:\n        assert not is_datetime64_any_dtype(data)\n\n\ndef test_is_float_dtype(data):\n    pa_type = data.dtype.pyarrow_dtype\n    if pa.types.is_floating(pa_type):\n        assert is_float_dtype(data)\n    else:\n        assert not is_float_dtype(data)\n\n\ndef test_pickle_roundtrip(data):\n    # GH 42600\n    expected = pd.Series(data)\n    expected_sliced = expected.head(2)\n    full_pickled = pickle.dumps(expected)\n    sliced_pickled = pickle.dumps(expected_sliced)\n\n    assert len(full_pickled) > len(sliced_pickled)\n\n    result = pickle.loads(full_pickled)\n    tm.assert_series_equal(result, expected)\n\n    result_sliced = pickle.loads(sliced_pickled)\n    tm.assert_series_equal(result_sliced, expected_sliced)\n\n\ndef test_astype_from_non_pyarrow(data):\n    # GH49795\n    pd_array = data._pa_array.to_pandas().array\n    result = pd_array.astype(data.dtype)\n    assert not isinstance(pd_array.dtype, ArrowDtype)\n    assert isinstance(result.dtype, ArrowDtype)\n    tm.assert_extension_array_equal(result, data)\n\n\ndef test_astype_float_from_non_pyarrow_str():\n    # GH50430\n    ser = pd.Series([\"1.0\"])\n    result = ser.astype(\"float64[pyarrow]\")\n    expected = pd.Series([1.0], dtype=\"float64[pyarrow]\")\n    tm.assert_series_equal(result, expected)\n\n\ndef test_astype_errors_ignore():\n    # GH 55399\n    expected = pd.DataFrame({\"col\": [17000000]}, dtype=\"int32[pyarrow]\")\n    result = expected.astype(\"float[pyarrow]\", errors=\"ignore\")\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_to_numpy_with_defaults(data):\n    # GH49973\n    result = data.to_numpy()\n\n    pa_type = data._pa_array.type\n    if pa.types.is_duration(pa_type) or pa.types.is_timestamp(pa_type):\n        pytest.skip(\"Tested in test_to_numpy_temporal\")\n    elif pa.types.is_date(pa_type):\n        expected = np.array(list(data))\n    else:\n        expected = np.array(data._pa_array)\n\n    if data._hasna and not is_numeric_dtype(data.dtype):\n        expected = expected.astype(object)\n        expected[pd.isna(data)] = pd.NA\n\n    tm.assert_numpy_array_equal(result, expected)\n\n\ndef test_to_numpy_int_with_na():\n    # GH51227: ensure to_numpy does not convert int to float\n    data = [1, None]\n    arr = pd.array(data, dtype=\"int64[pyarrow]\")\n    result = arr.to_numpy()\n    expected = np.array([1, np.nan])\n    assert isinstance(result[0], float)\n    tm.assert_numpy_array_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"na_val, exp\", [(lib.no_default, np.nan), (1, 1)])\ndef test_to_numpy_null_array(na_val, exp):\n    # GH#52443\n    arr = pd.array([pd.NA, pd.NA], dtype=\"null[pyarrow]\")\n    result = arr.to_numpy(dtype=\"float64\", na_value=na_val)\n    expected = np.array([exp] * 2, dtype=\"float64\")\n    tm.assert_numpy_array_equal(result, expected)\n\n\ndef test_to_numpy_null_array_no_dtype():\n    # GH#52443\n    arr = pd.array([pd.NA, pd.NA], dtype=\"null[pyarrow]\")\n    result = arr.to_numpy(dtype=None)\n    expected = np.array([pd.NA] * 2, dtype=\"object\")\n    tm.assert_numpy_array_equal(result, expected)\n\n\ndef test_to_numpy_without_dtype():\n    # GH 54808\n    arr = pd.array([True, pd.NA], dtype=\"boolean[pyarrow]\")\n    result = arr.to_numpy(na_value=False)\n    expected = np.array([True, False], dtype=np.bool_)\n    tm.assert_numpy_array_equal(result, expected)\n\n    arr = pd.array([1.0, pd.NA], dtype=\"float32[pyarrow]\")\n    result = arr.to_numpy(na_value=0.0)\n    expected = np.array([1.0, 0.0], dtype=np.float32)\n    tm.assert_numpy_array_equal(result, expected)\n\n\ndef test_setitem_null_slice(data):\n    # GH50248\n    orig = data.copy()\n\n    result = orig.copy()\n    result[:] = data[0]\n    expected = ArrowExtensionArray._from_sequence(\n        [data[0]] * len(data),\n        dtype=data.dtype,\n    )\n    tm.assert_extension_array_equal(result, expected)\n\n    result = orig.copy()\n    result[:] = data[::-1]\n    expected = data[::-1]\n    tm.assert_extension_array_equal(result, expected)\n\n    result = orig.copy()\n    result[:] = data.tolist()\n    expected = data\n    tm.assert_extension_array_equal(result, expected)\n\n\ndef test_setitem_invalid_dtype(data):\n    # GH50248\n    pa_type = data._pa_array.type\n    if pa.types.is_string(pa_type) or pa.types.is_binary(pa_type):\n        fill_value = 123\n        err = TypeError\n        msg = \"Invalid value '123' for dtype\"\n    elif (\n        pa.types.is_integer(pa_type)\n        or pa.types.is_floating(pa_type)\n        or pa.types.is_boolean(pa_type)\n    ):\n        fill_value = \"foo\"\n        err = pa.ArrowInvalid\n        msg = \"Could not convert\"\n    else:\n        fill_value = \"foo\"\n        err = TypeError\n        msg = \"Invalid value 'foo' for dtype\"\n    with pytest.raises(err, match=msg):\n        data[:] = fill_value\n\n\ndef test_from_arrow_respecting_given_dtype():\n    date_array = pa.array(\n        [pd.Timestamp(\"2019-12-31\"), pd.Timestamp(\"2019-12-31\")], type=pa.date32()\n    )\n    result = date_array.to_pandas(\n        types_mapper={pa.date32(): ArrowDtype(pa.date64())}.get\n    )\n    expected = pd.Series(\n        [pd.Timestamp(\"2019-12-31\"), pd.Timestamp(\"2019-12-31\")],\n        dtype=ArrowDtype(pa.date64()),\n    )\n    tm.assert_series_equal(result, expected)\n\n\ndef test_from_arrow_respecting_given_dtype_unsafe():\n    array = pa.array([1.5, 2.5], type=pa.float64())\n    with tm.external_error_raised(pa.ArrowInvalid):\n        array.to_pandas(types_mapper={pa.float64(): ArrowDtype(pa.int64())}.get)\n\n\ndef test_round():\n    dtype = \"float64[pyarrow]\"\n\n    ser = pd.Series([0.0, 1.23, 2.56, pd.NA], dtype=dtype)\n    result = ser.round(1)\n    expected = pd.Series([0.0, 1.2, 2.6, pd.NA], dtype=dtype)\n    tm.assert_series_equal(result, expected)\n\n    ser = pd.Series([123.4, pd.NA, 56.78], dtype=dtype)\n    result = ser.round(-1)\n    expected = pd.Series([120.0, pd.NA, 60.0], dtype=dtype)\n    tm.assert_series_equal(result, expected)\n\n\ndef test_searchsorted_with_na_raises(data_for_sorting, as_series):\n    # GH50447\n    b, c, a = data_for_sorting\n    arr = data_for_sorting.take([2, 0, 1])  # to get [a, b, c]\n    arr[-1] = pd.NA\n\n    if as_series:\n        arr = pd.Series(arr)\n\n    msg = (\n        \"searchsorted requires array to be sorted, \"\n        \"which is impossible with NAs present.\"\n    )\n    with pytest.raises(ValueError, match=msg):\n        arr.searchsorted(b)\n\n\ndef test_sort_values_dictionary():\n    df = pd.DataFrame(\n        {\n            \"a\": pd.Series(\n                [\"x\", \"y\"], dtype=ArrowDtype(pa.dictionary(pa.int32(), pa.string()))\n            ),\n            \"b\": [1, 2],\n        },\n    )\n    expected = df.copy()\n    result = df.sort_values(by=[\"a\", \"b\"])\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"pat\", [\"abc\", \"a[a-z]{2}\"])\ndef test_str_count(pat):\n    ser = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.count(pat)\n    expected = pd.Series([1, None], dtype=ArrowDtype(pa.int32()))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_str_count_flags_unsupported():\n    ser = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    with pytest.raises(NotImplementedError, match=\"count not\"):\n        ser.str.count(\"abc\", flags=1)\n\n\n@pytest.mark.parametrize(\n    \"side, str_func\", [[\"left\", \"rjust\"], [\"right\", \"ljust\"], [\"both\", \"center\"]]\n)\ndef test_str_pad(side, str_func):\n    ser = pd.Series([\"a\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.pad(width=3, side=side, fillchar=\"x\")\n    expected = pd.Series(\n        [getattr(\"a\", str_func)(3, \"x\"), None], dtype=ArrowDtype(pa.string())\n    )\n    tm.assert_series_equal(result, expected)\n\n\ndef test_str_pad_invalid_side():\n    ser = pd.Series([\"a\", None], dtype=ArrowDtype(pa.string()))\n    with pytest.raises(ValueError, match=\"Invalid side: foo\"):\n        ser.str.pad(3, \"foo\", \"x\")\n\n\n@pytest.mark.parametrize(\n    \"pat, case, na, regex, exp\",\n    [\n        [\"ab\", False, None, False, [True, None]],\n        [\"Ab\", True, None, False, [False, None]],\n        [\"ab\", False, True, False, [True, True]],\n        [\"a[a-z]{1}\", False, None, True, [True, None]],\n        [\"A[a-z]{1}\", True, None, True, [False, None]],\n    ],\n)\ndef test_str_contains(pat, case, na, regex, exp):\n    ser = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.contains(pat, case=case, na=na, regex=regex)\n    expected = pd.Series(exp, dtype=ArrowDtype(pa.bool_()))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_str_contains_flags_unsupported():\n    ser = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    with pytest.raises(NotImplementedError, match=\"contains not\"):\n        ser.str.contains(\"a\", flags=1)\n\n\n@pytest.mark.parametrize(\n    \"side, pat, na, exp\",\n    [\n        [\"startswith\", \"ab\", None, [True, None, False]],\n        [\"startswith\", \"b\", False, [False, False, False]],\n        [\"endswith\", \"b\", True, [False, True, False]],\n        [\"endswith\", \"bc\", None, [True, None, False]],\n        [\"startswith\", (\"a\", \"e\", \"g\"), None, [True, None, True]],\n        [\"endswith\", (\"a\", \"c\", \"g\"), None, [True, None, True]],\n        [\"startswith\", (), None, [False, None, False]],\n        [\"endswith\", (), None, [False, None, False]],\n    ],\n)\ndef test_str_start_ends_with(side, pat, na, exp):\n    ser = pd.Series([\"abc\", None, \"efg\"], dtype=ArrowDtype(pa.string()))\n    result = getattr(ser.str, side)(pat, na=na)\n    expected = pd.Series(exp, dtype=ArrowDtype(pa.bool_()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"side\", (\"startswith\", \"endswith\"))\ndef test_str_starts_ends_with_all_nulls_empty_tuple(side):\n    ser = pd.Series([None, None], dtype=ArrowDtype(pa.string()))\n    result = getattr(ser.str, side)(())\n\n    # bool datatype preserved for all nulls.\n    expected = pd.Series([None, None], dtype=ArrowDtype(pa.bool_()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"arg_name, arg\",\n    [[\"pat\", re.compile(\"b\")], [\"repl\", str], [\"case\", False], [\"flags\", 1]],\n)\ndef test_str_replace_unsupported(arg_name, arg):\n    ser = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    kwargs = {\"pat\": \"b\", \"repl\": \"x\", \"regex\": True}\n    kwargs[arg_name] = arg\n    with pytest.raises(NotImplementedError, match=\"replace is not supported\"):\n        ser.str.replace(**kwargs)\n\n\n@pytest.mark.parametrize(\n    \"pat, repl, n, regex, exp\",\n    [\n        [\"a\", \"x\", -1, False, [\"xbxc\", None]],\n        [\"a\", \"x\", 1, False, [\"xbac\", None]],\n        [\"[a-b]\", \"x\", -1, True, [\"xxxc\", None]],\n    ],\n)\ndef test_str_replace(pat, repl, n, regex, exp):\n    ser = pd.Series([\"abac\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.replace(pat, repl, n=n, regex=regex)\n    expected = pd.Series(exp, dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_str_replace_negative_n():\n    # GH 56404\n    ser = pd.Series([\"abc\", \"aaaaaa\"], dtype=ArrowDtype(pa.string()))\n    actual = ser.str.replace(\"a\", \"\", -3, True)\n    expected = pd.Series([\"bc\", \"\"], dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(expected, actual)\n\n    # Same bug for pyarrow-backed StringArray GH#59628\n    ser2 = ser.astype(pd.StringDtype(storage=\"pyarrow\"))\n    actual2 = ser2.str.replace(\"a\", \"\", -3, True)\n    expected2 = expected.astype(ser2.dtype)\n    tm.assert_series_equal(expected2, actual2)\n\n    ser3 = ser.astype(pd.StringDtype(storage=\"pyarrow\", na_value=np.nan))\n    actual3 = ser3.str.replace(\"a\", \"\", -3, True)\n    expected3 = expected.astype(ser3.dtype)\n    tm.assert_series_equal(expected3, actual3)\n\n\ndef test_str_repeat_unsupported():\n    ser = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    with pytest.raises(NotImplementedError, match=\"repeat is not\"):\n        ser.str.repeat([1, 2])\n\n\ndef test_str_repeat():\n    ser = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.repeat(2)\n    expected = pd.Series([\"abcabc\", None], dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"pat, case, na, exp\",\n    [\n        [\"ab\", False, None, [True, None]],\n        [\"Ab\", True, None, [False, None]],\n        [\"bc\", True, None, [False, None]],\n        [\"ab\", False, True, [True, True]],\n        [\"a[a-z]{1}\", False, None, [True, None]],\n        [\"A[a-z]{1}\", True, None, [False, None]],\n    ],\n)\ndef test_str_match(pat, case, na, exp):\n    ser = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.match(pat, case=case, na=na)\n    expected = pd.Series(exp, dtype=ArrowDtype(pa.bool_()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"pat, case, na, exp\",\n    [\n        [\"abc\", False, None, [True, True, False, None]],\n        [\"Abc\", True, None, [False, False, False, None]],\n        [\"bc\", True, None, [False, False, False, None]],\n        [\"ab\", False, None, [True, True, False, None]],\n        [\"a[a-z]{2}\", False, None, [True, True, False, None]],\n        [\"A[a-z]{1}\", True, None, [False, False, False, None]],\n        # GH Issue: #56652\n        [\"abc$\", False, None, [True, False, False, None]],\n        [\"abc\\\\$\", False, None, [False, True, False, None]],\n        [\"Abc$\", True, None, [False, False, False, None]],\n        [\"Abc\\\\$\", True, None, [False, False, False, None]],\n    ],\n)\ndef test_str_fullmatch(pat, case, na, exp):\n    ser = pd.Series([\"abc\", \"abc$\", \"$abc\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.match(pat, case=case, na=na)\n    expected = pd.Series(exp, dtype=ArrowDtype(pa.bool_()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"sub, start, end, exp, exp_type\",\n    [\n        [\"ab\", 0, None, [0, None], pa.int32()],\n        [\"bc\", 1, 3, [1, None], pa.int64()],\n        [\"ab\", 1, 3, [-1, None], pa.int64()],\n        [\"ab\", -3, -3, [-1, None], pa.int64()],\n    ],\n)\ndef test_str_find(sub, start, end, exp, exp_type):\n    ser = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.find(sub, start=start, end=end)\n    expected = pd.Series(exp, dtype=ArrowDtype(exp_type))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_str_find_negative_start():\n    # GH 56411\n    ser = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.find(sub=\"b\", start=-1000, end=3)\n    expected = pd.Series([1, None], dtype=ArrowDtype(pa.int64()))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_str_find_no_end():\n    ser = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.find(\"ab\", start=1)\n    expected = pd.Series([-1, None], dtype=\"int64[pyarrow]\")\n    tm.assert_series_equal(result, expected)\n\n\ndef test_str_find_negative_start_negative_end():\n    # GH 56791\n    ser = pd.Series([\"abcdefg\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.find(sub=\"d\", start=-6, end=-3)\n    expected = pd.Series([3, None], dtype=ArrowDtype(pa.int64()))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_str_find_large_start():\n    # GH 56791\n    ser = pd.Series([\"abcdefg\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.find(sub=\"d\", start=16)\n    expected = pd.Series([-1, None], dtype=ArrowDtype(pa.int64()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.skipif(\n    pa_version_under13p0, reason=\"https://github.com/apache/arrow/issues/36311\"\n)\n@pytest.mark.parametrize(\"start\", [-15, -3, 0, 1, 15, None])\n@pytest.mark.parametrize(\"end\", [-15, -1, 0, 3, 15, None])\n@pytest.mark.parametrize(\"sub\", [\"\", \"az\", \"abce\", \"a\", \"caa\"])\ndef test_str_find_e2e(start, end, sub):\n    s = pd.Series(\n        [\"abcaadef\", \"abc\", \"abcdeddefgj8292\", \"ab\", \"a\", \"\"],\n        dtype=ArrowDtype(pa.string()),\n    )\n    object_series = s.astype(pd.StringDtype(storage=\"python\"))\n    result = s.str.find(sub, start, end)\n    expected = object_series.str.find(sub, start, end).astype(result.dtype)\n    tm.assert_series_equal(result, expected)\n\n    arrow_str_series = s.astype(pd.StringDtype(storage=\"pyarrow\"))\n    result2 = arrow_str_series.str.find(sub, start, end).astype(result.dtype)\n    tm.assert_series_equal(result2, expected)\n\n\ndef test_str_find_negative_start_negative_end_no_match():\n    # GH 56791\n    ser = pd.Series([\"abcdefg\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.find(sub=\"d\", start=-3, end=-6)\n    expected = pd.Series([-1, None], dtype=ArrowDtype(pa.int64()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"i, exp\",\n    [\n        [1, [\"b\", \"e\", None]],\n        [-1, [\"c\", \"e\", None]],\n        [2, [\"c\", None, None]],\n        [-3, [\"a\", None, None]],\n        [4, [None, None, None]],\n    ],\n)\ndef test_str_get(i, exp):\n    ser = pd.Series([\"abc\", \"de\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.get(i)\n    expected = pd.Series(exp, dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.xfail(\n    reason=\"TODO: StringMethods._validate should support Arrow list types\",\n    raises=AttributeError,\n)\ndef test_str_join():\n    ser = pd.Series(ArrowExtensionArray(pa.array([list(\"abc\"), list(\"123\"), None])))\n    result = ser.str.join(\"=\")\n    expected = pd.Series([\"a=b=c\", \"1=2=3\", None], dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_str_join_string_type():\n    ser = pd.Series(ArrowExtensionArray(pa.array([\"abc\", \"123\", None])))\n    result = ser.str.join(\"=\")\n    expected = pd.Series([\"a=b=c\", \"1=2=3\", None], dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"start, stop, step, exp\",\n    [\n        [None, 2, None, [\"ab\", None]],\n        [None, 2, 1, [\"ab\", None]],\n        [1, 3, 1, [\"bc\", None]],\n        (None, None, -1, [\"dcba\", None]),\n    ],\n)\ndef test_str_slice(start, stop, step, exp):\n    ser = pd.Series([\"abcd\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.slice(start, stop, step)\n    expected = pd.Series(exp, dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"start, stop, repl, exp\",\n    [\n        [1, 2, \"x\", [\"axcd\", None]],\n        [None, 2, \"x\", [\"xcd\", None]],\n        [None, 2, None, [\"cd\", None]],\n    ],\n)\ndef test_str_slice_replace(start, stop, repl, exp):\n    ser = pd.Series([\"abcd\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.slice_replace(start, stop, repl)\n    expected = pd.Series(exp, dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"value, method, exp\",\n    [\n        [\"a1c\", \"isalnum\", True],\n        [\"!|,\", \"isalnum\", False],\n        [\"aaa\", \"isalpha\", True],\n        [\"!!!\", \"isalpha\", False],\n        [\"٠\", \"isdecimal\", True],  # noqa: RUF001\n        [\"~!\", \"isdecimal\", False],\n        [\"2\", \"isdigit\", True],\n        [\"~\", \"isdigit\", False],\n        [\"aaa\", \"islower\", True],\n        [\"aaA\", \"islower\", False],\n        [\"123\", \"isnumeric\", True],\n        [\"11I\", \"isnumeric\", False],\n        [\" \", \"isspace\", True],\n        [\"\", \"isspace\", False],\n        [\"The That\", \"istitle\", True],\n        [\"the That\", \"istitle\", False],\n        [\"AAA\", \"isupper\", True],\n        [\"AAc\", \"isupper\", False],\n    ],\n)\ndef test_str_is_functions(value, method, exp):\n    ser = pd.Series([value, None], dtype=ArrowDtype(pa.string()))\n    result = getattr(ser.str, method)()\n    expected = pd.Series([exp, None], dtype=ArrowDtype(pa.bool_()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"method, exp\",\n    [\n        [\"capitalize\", \"Abc def\"],\n        [\"title\", \"Abc Def\"],\n        [\"swapcase\", \"AbC Def\"],\n        [\"lower\", \"abc def\"],\n        [\"upper\", \"ABC DEF\"],\n        [\"casefold\", \"abc def\"],\n    ],\n)\ndef test_str_transform_functions(method, exp):\n    ser = pd.Series([\"aBc dEF\", None], dtype=ArrowDtype(pa.string()))\n    result = getattr(ser.str, method)()\n    expected = pd.Series([exp, None], dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_str_len():\n    ser = pd.Series([\"abcd\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.len()\n    expected = pd.Series([4, None], dtype=ArrowDtype(pa.int32()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"method, to_strip, val\",\n    [\n        [\"strip\", None, \" abc \"],\n        [\"strip\", \"x\", \"xabcx\"],\n        [\"lstrip\", None, \" abc\"],\n        [\"lstrip\", \"x\", \"xabc\"],\n        [\"rstrip\", None, \"abc \"],\n        [\"rstrip\", \"x\", \"abcx\"],\n    ],\n)\ndef test_str_strip(method, to_strip, val):\n    ser = pd.Series([val, None], dtype=ArrowDtype(pa.string()))\n    result = getattr(ser.str, method)(to_strip=to_strip)\n    expected = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"val\", [\"abc123\", \"abc\"])\ndef test_str_removesuffix(val):\n    ser = pd.Series([val, None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.removesuffix(\"123\")\n    expected = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"val\", [\"123abc\", \"abc\"])\ndef test_str_removeprefix(val):\n    ser = pd.Series([val, None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.removeprefix(\"123\")\n    expected = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"errors\", [\"ignore\", \"strict\"])\n@pytest.mark.parametrize(\n    \"encoding, exp\",\n    [\n        (\"utf8\", {\"little\": b\"abc\", \"big\": \"abc\"}),\n        (\n            \"utf32\",\n            {\n                \"little\": b\"\\xff\\xfe\\x00\\x00a\\x00\\x00\\x00b\\x00\\x00\\x00c\\x00\\x00\\x00\",\n                \"big\": b\"\\x00\\x00\\xfe\\xff\\x00\\x00\\x00a\\x00\\x00\\x00b\\x00\\x00\\x00c\",\n            },\n        ),\n    ],\n    ids=[\"utf8\", \"utf32\"],\n)\ndef test_str_encode(errors, encoding, exp):\n    ser = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.encode(encoding, errors)\n    expected = pd.Series([exp[sys.byteorder], None], dtype=ArrowDtype(pa.binary()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"flags\", [0, 2])\ndef test_str_findall(flags):\n    ser = pd.Series([\"abc\", \"efg\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.findall(\"b\", flags=flags)\n    expected = pd.Series([[\"b\"], [], None], dtype=ArrowDtype(pa.list_(pa.string())))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"method\", [\"index\", \"rindex\"])\n@pytest.mark.parametrize(\n    \"start, end\",\n    [\n        [0, None],\n        [1, 4],\n    ],\n)\ndef test_str_r_index(method, start, end):\n    ser = pd.Series([\"abcba\", None], dtype=ArrowDtype(pa.string()))\n    result = getattr(ser.str, method)(\"c\", start, end)\n    expected = pd.Series([2, None], dtype=ArrowDtype(pa.int64()))\n    tm.assert_series_equal(result, expected)\n\n    with pytest.raises(ValueError, match=\"substring not found\"):\n        getattr(ser.str, method)(\"foo\", start, end)\n\n\n@pytest.mark.parametrize(\"form\", [\"NFC\", \"NFKC\"])\ndef test_str_normalize(form):\n    ser = pd.Series([\"abc\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.normalize(form)\n    expected = ser.copy()\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"start, end\",\n    [\n        [0, None],\n        [1, 4],\n    ],\n)\ndef test_str_rfind(start, end):\n    ser = pd.Series([\"abcba\", \"foo\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.rfind(\"c\", start, end)\n    expected = pd.Series([2, -1, None], dtype=ArrowDtype(pa.int64()))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_str_translate():\n    ser = pd.Series([\"abcba\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.translate({97: \"b\"})\n    expected = pd.Series([\"bbcbb\", None], dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_str_wrap():\n    ser = pd.Series([\"abcba\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.wrap(3)\n    expected = pd.Series([\"abc\\nba\", None], dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_get_dummies():\n    ser = pd.Series([\"a|b\", None, \"a|c\"], dtype=ArrowDtype(pa.string()))\n    result = ser.str.get_dummies()\n    expected = pd.DataFrame(\n        [[True, True, False], [False, False, False], [True, False, True]],\n        dtype=ArrowDtype(pa.bool_()),\n        columns=[\"a\", \"b\", \"c\"],\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_str_partition():\n    ser = pd.Series([\"abcba\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.partition(\"b\")\n    expected = pd.DataFrame(\n        [[\"a\", \"b\", \"cba\"], [None, None, None]],\n        dtype=ArrowDtype(pa.string()),\n        columns=pd.RangeIndex(3),\n    )\n    tm.assert_frame_equal(result, expected, check_column_type=True)\n\n    result = ser.str.partition(\"b\", expand=False)\n    expected = pd.Series(ArrowExtensionArray(pa.array([[\"a\", \"b\", \"cba\"], None])))\n    tm.assert_series_equal(result, expected)\n\n    result = ser.str.rpartition(\"b\")\n    expected = pd.DataFrame(\n        [[\"abc\", \"b\", \"a\"], [None, None, None]],\n        dtype=ArrowDtype(pa.string()),\n        columns=pd.RangeIndex(3),\n    )\n    tm.assert_frame_equal(result, expected, check_column_type=True)\n\n    result = ser.str.rpartition(\"b\", expand=False)\n    expected = pd.Series(ArrowExtensionArray(pa.array([[\"abc\", \"b\", \"a\"], None])))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"method\", [\"rsplit\", \"split\"])\ndef test_str_split_pat_none(method):\n    # GH 56271\n    ser = pd.Series([\"a1 cbc\\nb\", None], dtype=ArrowDtype(pa.string()))\n    result = getattr(ser.str, method)()\n    expected = pd.Series(ArrowExtensionArray(pa.array([[\"a1\", \"cbc\", \"b\"], None])))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_str_split():\n    # GH 52401\n    ser = pd.Series([\"a1cbcb\", \"a2cbcb\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.split(\"c\")\n    expected = pd.Series(\n        ArrowExtensionArray(pa.array([[\"a1\", \"b\", \"b\"], [\"a2\", \"b\", \"b\"], None]))\n    )\n    tm.assert_series_equal(result, expected)\n\n    result = ser.str.split(\"c\", n=1)\n    expected = pd.Series(\n        ArrowExtensionArray(pa.array([[\"a1\", \"bcb\"], [\"a2\", \"bcb\"], None]))\n    )\n    tm.assert_series_equal(result, expected)\n\n    result = ser.str.split(\"[1-2]\", regex=True)\n    expected = pd.Series(\n        ArrowExtensionArray(pa.array([[\"a\", \"cbcb\"], [\"a\", \"cbcb\"], None]))\n    )\n    tm.assert_series_equal(result, expected)\n\n    result = ser.str.split(\"[1-2]\", regex=True, expand=True)\n    expected = pd.DataFrame(\n        {\n            0: ArrowExtensionArray(pa.array([\"a\", \"a\", None])),\n            1: ArrowExtensionArray(pa.array([\"cbcb\", \"cbcb\", None])),\n        }\n    )\n    tm.assert_frame_equal(result, expected)\n\n    result = ser.str.split(\"1\", expand=True)\n    expected = pd.DataFrame(\n        {\n            0: ArrowExtensionArray(pa.array([\"a\", \"a2cbcb\", None])),\n            1: ArrowExtensionArray(pa.array([\"cbcb\", None, None])),\n        }\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_str_rsplit():\n    # GH 52401\n    ser = pd.Series([\"a1cbcb\", \"a2cbcb\", None], dtype=ArrowDtype(pa.string()))\n    result = ser.str.rsplit(\"c\")\n    expected = pd.Series(\n        ArrowExtensionArray(pa.array([[\"a1\", \"b\", \"b\"], [\"a2\", \"b\", \"b\"], None]))\n    )\n    tm.assert_series_equal(result, expected)\n\n    result = ser.str.rsplit(\"c\", n=1)\n    expected = pd.Series(\n        ArrowExtensionArray(pa.array([[\"a1cb\", \"b\"], [\"a2cb\", \"b\"], None]))\n    )\n    tm.assert_series_equal(result, expected)\n\n    result = ser.str.rsplit(\"c\", n=1, expand=True)\n    expected = pd.DataFrame(\n        {\n            0: ArrowExtensionArray(pa.array([\"a1cb\", \"a2cb\", None])),\n            1: ArrowExtensionArray(pa.array([\"b\", \"b\", None])),\n        }\n    )\n    tm.assert_frame_equal(result, expected)\n\n    result = ser.str.rsplit(\"1\", expand=True)\n    expected = pd.DataFrame(\n        {\n            0: ArrowExtensionArray(pa.array([\"a\", \"a2cbcb\", None])),\n            1: ArrowExtensionArray(pa.array([\"cbcb\", None, None])),\n        }\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_str_extract_non_symbolic():\n    ser = pd.Series([\"a1\", \"b2\", \"c3\"], dtype=ArrowDtype(pa.string()))\n    with pytest.raises(ValueError, match=\"pat=.* must contain a symbolic group name.\"):\n        ser.str.extract(r\"[ab](\\d)\")\n\n\n@pytest.mark.parametrize(\"expand\", [True, False])\ndef test_str_extract(expand):\n    ser = pd.Series([\"a1\", \"b2\", \"c3\"], dtype=ArrowDtype(pa.string()))\n    result = ser.str.extract(r\"(?P<letter>[ab])(?P<digit>\\d)\", expand=expand)\n    expected = pd.DataFrame(\n        {\n            \"letter\": ArrowExtensionArray(pa.array([\"a\", \"b\", None])),\n            \"digit\": ArrowExtensionArray(pa.array([\"1\", \"2\", None])),\n        }\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_str_extract_expand():\n    ser = pd.Series([\"a1\", \"b2\", \"c3\"], dtype=ArrowDtype(pa.string()))\n    result = ser.str.extract(r\"[ab](?P<digit>\\d)\", expand=True)\n    expected = pd.DataFrame(\n        {\n            \"digit\": ArrowExtensionArray(pa.array([\"1\", \"2\", None])),\n        }\n    )\n    tm.assert_frame_equal(result, expected)\n\n    result = ser.str.extract(r\"[ab](?P<digit>\\d)\", expand=False)\n    expected = pd.Series(ArrowExtensionArray(pa.array([\"1\", \"2\", None])), name=\"digit\")\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"unit\", [\"ns\", \"us\", \"ms\", \"s\"])\ndef test_duration_from_strings_with_nat(unit):\n    # GH51175\n    strings = [\"1000\", \"NaT\"]\n    pa_type = pa.duration(unit)\n    dtype = ArrowDtype(pa_type)\n    result = ArrowExtensionArray._from_sequence_of_strings(strings, dtype=dtype)\n    expected = ArrowExtensionArray(pa.array([1000, None], type=pa_type))\n    tm.assert_extension_array_equal(result, expected)\n\n\ndef test_unsupported_dt(data):\n    pa_dtype = data.dtype.pyarrow_dtype\n    if not pa.types.is_temporal(pa_dtype):\n        with pytest.raises(\n            AttributeError, match=\"Can only use .dt accessor with datetimelike values\"\n        ):\n            pd.Series(data).dt\n\n\n@pytest.mark.parametrize(\n    \"prop, expected\",\n    [\n        [\"year\", 2023],\n        [\"day\", 2],\n        [\"day_of_week\", 0],\n        [\"dayofweek\", 0],\n        [\"weekday\", 0],\n        [\"day_of_year\", 2],\n        [\"dayofyear\", 2],\n        [\"hour\", 3],\n        [\"minute\", 4],\n        [\"is_leap_year\", False],\n        [\"microsecond\", 2000],\n        [\"month\", 1],\n        [\"nanosecond\", 6],\n        [\"quarter\", 1],\n        [\"second\", 7],\n        [\"date\", date(2023, 1, 2)],\n        [\"time\", time(3, 4, 7, 2000)],\n    ],\n)\ndef test_dt_properties(prop, expected):\n    ser = pd.Series(\n        [\n            pd.Timestamp(\n                year=2023,\n                month=1,\n                day=2,\n                hour=3,\n                minute=4,\n                second=7,\n                microsecond=2000,\n                nanosecond=6,\n            ),\n            None,\n        ],\n        dtype=ArrowDtype(pa.timestamp(\"ns\")),\n    )\n    result = getattr(ser.dt, prop)\n    exp_type = None\n    if isinstance(expected, date):\n        exp_type = pa.date32()\n    elif isinstance(expected, time):\n        exp_type = pa.time64(\"ns\")\n    expected = pd.Series(ArrowExtensionArray(pa.array([expected, None], type=exp_type)))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"microsecond\", [2000, 5, 0])\ndef test_dt_microsecond(microsecond):\n    # GH 59183\n    ser = pd.Series(\n        [\n            pd.Timestamp(\n                year=2024,\n                month=7,\n                day=7,\n                second=5,\n                microsecond=microsecond,\n                nanosecond=6,\n            ),\n            None,\n        ],\n        dtype=ArrowDtype(pa.timestamp(\"ns\")),\n    )\n    result = ser.dt.microsecond\n    expected = pd.Series([microsecond, None], dtype=\"int64[pyarrow]\")\n    tm.assert_series_equal(result, expected)\n\n\ndef test_dt_is_month_start_end():\n    ser = pd.Series(\n        [\n            datetime(year=2023, month=12, day=2, hour=3),\n            datetime(year=2023, month=1, day=1, hour=3),\n            datetime(year=2023, month=3, day=31, hour=3),\n            None,\n        ],\n        dtype=ArrowDtype(pa.timestamp(\"us\")),\n    )\n    result = ser.dt.is_month_start\n    expected = pd.Series([False, True, False, None], dtype=ArrowDtype(pa.bool_()))\n    tm.assert_series_equal(result, expected)\n\n    result = ser.dt.is_month_end\n    expected = pd.Series([False, False, True, None], dtype=ArrowDtype(pa.bool_()))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_dt_is_year_start_end():\n    ser = pd.Series(\n        [\n            datetime(year=2023, month=12, day=31, hour=3),\n            datetime(year=2023, month=1, day=1, hour=3),\n            datetime(year=2023, month=3, day=31, hour=3),\n            None,\n        ],\n        dtype=ArrowDtype(pa.timestamp(\"us\")),\n    )\n    result = ser.dt.is_year_start\n    expected = pd.Series([False, True, False, None], dtype=ArrowDtype(pa.bool_()))\n    tm.assert_series_equal(result, expected)\n\n    result = ser.dt.is_year_end\n    expected = pd.Series([True, False, False, None], dtype=ArrowDtype(pa.bool_()))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_dt_is_quarter_start_end():\n    ser = pd.Series(\n        [\n            datetime(year=2023, month=11, day=30, hour=3),\n            datetime(year=2023, month=1, day=1, hour=3),\n            datetime(year=2023, month=3, day=31, hour=3),\n            None,\n        ],\n        dtype=ArrowDtype(pa.timestamp(\"us\")),\n    )\n    result = ser.dt.is_quarter_start\n    expected = pd.Series([False, True, False, None], dtype=ArrowDtype(pa.bool_()))\n    tm.assert_series_equal(result, expected)\n\n    result = ser.dt.is_quarter_end\n    expected = pd.Series([False, False, True, None], dtype=ArrowDtype(pa.bool_()))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"method\", [\"days_in_month\", \"daysinmonth\"])\ndef test_dt_days_in_month(method):\n    ser = pd.Series(\n        [\n            datetime(year=2023, month=3, day=30, hour=3),\n            datetime(year=2023, month=4, day=1, hour=3),\n            datetime(year=2023, month=2, day=3, hour=3),\n            None,\n        ],\n        dtype=ArrowDtype(pa.timestamp(\"us\")),\n    )\n    result = getattr(ser.dt, method)\n    expected = pd.Series([31, 30, 28, None], dtype=ArrowDtype(pa.int64()))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_dt_normalize():\n    ser = pd.Series(\n        [\n            datetime(year=2023, month=3, day=30),\n            datetime(year=2023, month=4, day=1, hour=3),\n            datetime(year=2023, month=2, day=3, hour=23, minute=59, second=59),\n            None,\n        ],\n        dtype=ArrowDtype(pa.timestamp(\"us\")),\n    )\n    result = ser.dt.normalize()\n    expected = pd.Series(\n        [\n            datetime(year=2023, month=3, day=30),\n            datetime(year=2023, month=4, day=1),\n            datetime(year=2023, month=2, day=3),\n            None,\n        ],\n        dtype=ArrowDtype(pa.timestamp(\"us\")),\n    )\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"unit\", [\"us\", \"ns\"])\ndef test_dt_time_preserve_unit(unit):\n    ser = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(unit)),\n    )\n    assert ser.dt.unit == unit\n\n    result = ser.dt.time\n    expected = pd.Series(\n        ArrowExtensionArray(pa.array([time(3, 0), None], type=pa.time64(unit)))\n    )\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"tz\", [None, \"UTC\", \"US/Pacific\"])\ndef test_dt_tz(tz):\n    ser = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(\"ns\", tz=tz)),\n    )\n    result = ser.dt.tz\n    assert result == timezones.maybe_get_tz(tz)\n\n\ndef test_dt_isocalendar():\n    ser = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(\"ns\")),\n    )\n    result = ser.dt.isocalendar()\n    expected = pd.DataFrame(\n        [[2023, 1, 1], [0, 0, 0]],\n        columns=[\"year\", \"week\", \"day\"],\n        dtype=\"int64[pyarrow]\",\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"method, exp\", [[\"day_name\", \"Sunday\"], [\"month_name\", \"January\"]]\n)\ndef test_dt_day_month_name(method, exp, request):\n    # GH 52388\n    _require_timezone_database(request)\n\n    ser = pd.Series([datetime(2023, 1, 1), None], dtype=ArrowDtype(pa.timestamp(\"ms\")))\n    result = getattr(ser.dt, method)()\n    expected = pd.Series([exp, None], dtype=ArrowDtype(pa.string()))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_dt_strftime(request):\n    _require_timezone_database(request)\n\n    ser = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(\"ns\")),\n    )\n    result = ser.dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n    expected = pd.Series(\n        [\"2023-01-02T03:00:00.000000000\", None], dtype=ArrowDtype(pa.string())\n    )\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"method\", [\"ceil\", \"floor\", \"round\"])\ndef test_dt_roundlike_tz_options_not_supported(method):\n    ser = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(\"ns\")),\n    )\n    with pytest.raises(NotImplementedError, match=\"ambiguous is not supported.\"):\n        getattr(ser.dt, method)(\"1h\", ambiguous=\"NaT\")\n\n    with pytest.raises(NotImplementedError, match=\"nonexistent is not supported.\"):\n        getattr(ser.dt, method)(\"1h\", nonexistent=\"NaT\")\n\n\n@pytest.mark.parametrize(\"method\", [\"ceil\", \"floor\", \"round\"])\ndef test_dt_roundlike_unsupported_freq(method):\n    ser = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(\"ns\")),\n    )\n    with pytest.raises(ValueError, match=\"freq='1B' is not supported\"):\n        getattr(ser.dt, method)(\"1B\")\n\n    with pytest.raises(ValueError, match=\"Must specify a valid frequency: None\"):\n        getattr(ser.dt, method)(None)\n\n\n@pytest.mark.parametrize(\"freq\", [\"D\", \"h\", \"min\", \"s\", \"ms\", \"us\", \"ns\"])\n@pytest.mark.parametrize(\"method\", [\"ceil\", \"floor\", \"round\"])\ndef test_dt_ceil_year_floor(freq, method):\n    ser = pd.Series(\n        [datetime(year=2023, month=1, day=1), None],\n    )\n    pa_dtype = ArrowDtype(pa.timestamp(\"ns\"))\n    expected = getattr(ser.dt, method)(f\"1{freq}\").astype(pa_dtype)\n    result = getattr(ser.astype(pa_dtype).dt, method)(f\"1{freq}\")\n    tm.assert_series_equal(result, expected)\n\n\ndef test_dt_to_pydatetime():\n    # GH 51859\n    data = [datetime(2022, 1, 1), datetime(2023, 1, 1)]\n    ser = pd.Series(data, dtype=ArrowDtype(pa.timestamp(\"ns\")))\n    result = ser.dt.to_pydatetime()\n    expected = pd.Series(data, dtype=object)\n    tm.assert_series_equal(result, expected)\n    assert all(type(expected.iloc[i]) is datetime for i in range(len(expected)))\n\n    expected = ser.astype(\"datetime64[ns]\").dt.to_pydatetime()\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"date_type\", [32, 64])\ndef test_dt_to_pydatetime_date_error(date_type):\n    # GH 52812\n    ser = pd.Series(\n        [date(2022, 12, 31)],\n        dtype=ArrowDtype(getattr(pa, f\"date{date_type}\")()),\n    )\n    with pytest.raises(ValueError, match=\"to_pydatetime cannot be called with\"):\n        ser.dt.to_pydatetime()\n\n\ndef test_dt_tz_localize_unsupported_tz_options():\n    ser = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(\"ns\")),\n    )\n    with pytest.raises(NotImplementedError, match=\"ambiguous='NaT' is not supported\"):\n        ser.dt.tz_localize(\"UTC\", ambiguous=\"NaT\")\n\n    with pytest.raises(NotImplementedError, match=\"nonexistent='NaT' is not supported\"):\n        ser.dt.tz_localize(\"UTC\", nonexistent=\"NaT\")\n\n\ndef test_dt_tz_localize_none():\n    ser = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(\"ns\", tz=\"US/Pacific\")),\n    )\n    result = ser.dt.tz_localize(None)\n    expected = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(\"ns\")),\n    )\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"unit\", [\"us\", \"ns\"])\ndef test_dt_tz_localize(unit, request):\n    _require_timezone_database(request)\n\n    ser = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(unit)),\n    )\n    result = ser.dt.tz_localize(\"US/Pacific\")\n    exp_data = pa.array(\n        [datetime(year=2023, month=1, day=2, hour=3), None], type=pa.timestamp(unit)\n    )\n    exp_data = pa.compute.assume_timezone(exp_data, \"US/Pacific\")\n    expected = pd.Series(ArrowExtensionArray(exp_data))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"nonexistent, exp_date\",\n    [\n        [\"shift_forward\", datetime(year=2023, month=3, day=12, hour=3)],\n        [\"shift_backward\", pd.Timestamp(\"2023-03-12 01:59:59.999999999\")],\n    ],\n)\ndef test_dt_tz_localize_nonexistent(nonexistent, exp_date, request):\n    _require_timezone_database(request)\n\n    ser = pd.Series(\n        [datetime(year=2023, month=3, day=12, hour=2, minute=30), None],\n        dtype=ArrowDtype(pa.timestamp(\"ns\")),\n    )\n    result = ser.dt.tz_localize(\"US/Pacific\", nonexistent=nonexistent)\n    exp_data = pa.array([exp_date, None], type=pa.timestamp(\"ns\"))\n    exp_data = pa.compute.assume_timezone(exp_data, \"US/Pacific\")\n    expected = pd.Series(ArrowExtensionArray(exp_data))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_dt_tz_convert_not_tz_raises():\n    ser = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(\"ns\")),\n    )\n    with pytest.raises(TypeError, match=\"Cannot convert tz-naive timestamps\"):\n        ser.dt.tz_convert(\"UTC\")\n\n\ndef test_dt_tz_convert_none():\n    ser = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(\"ns\", \"US/Pacific\")),\n    )\n    result = ser.dt.tz_convert(None)\n    expected = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(\"ns\")),\n    )\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"unit\", [\"us\", \"ns\"])\ndef test_dt_tz_convert(unit):\n    ser = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(unit, \"US/Pacific\")),\n    )\n    result = ser.dt.tz_convert(\"US/Eastern\")\n    expected = pd.Series(\n        [datetime(year=2023, month=1, day=2, hour=3), None],\n        dtype=ArrowDtype(pa.timestamp(unit, \"US/Eastern\")),\n    )\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"dtype\", [\"timestamp[ms][pyarrow]\", \"duration[ms][pyarrow]\"])\ndef test_as_unit(dtype):\n    # GH 52284\n    ser = pd.Series([1000, None], dtype=dtype)\n    result = ser.dt.as_unit(\"ns\")\n    expected = ser.astype(dtype.replace(\"ms\", \"ns\"))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"prop, expected\",\n    [\n        [\"days\", 1],\n        [\"seconds\", 2],\n        [\"microseconds\", 3],\n        [\"nanoseconds\", 4],\n    ],\n)\ndef test_dt_timedelta_properties(prop, expected):\n    # GH 52284\n    ser = pd.Series(\n        [\n            pd.Timedelta(\n                days=1,\n                seconds=2,\n                microseconds=3,\n                nanoseconds=4,\n            ),\n            None,\n        ],\n        dtype=ArrowDtype(pa.duration(\"ns\")),\n    )\n    result = getattr(ser.dt, prop)\n    expected = pd.Series(\n        ArrowExtensionArray(pa.array([expected, None], type=pa.int32()))\n    )\n    tm.assert_series_equal(result, expected)\n\n\ndef test_dt_timedelta_total_seconds():\n    # GH 52284\n    ser = pd.Series(\n        [\n            pd.Timedelta(\n                days=1,\n                seconds=2,\n                microseconds=3,\n                nanoseconds=4,\n            ),\n            None,\n        ],\n        dtype=ArrowDtype(pa.duration(\"ns\")),\n    )\n    result = ser.dt.total_seconds()\n    expected = pd.Series(\n        ArrowExtensionArray(pa.array([86402.000003, None], type=pa.float64()))\n    )\n    tm.assert_series_equal(result, expected)\n\n\ndef test_dt_to_pytimedelta():\n    # GH 52284\n    data = [timedelta(1, 2, 3), timedelta(1, 2, 4)]\n    ser = pd.Series(data, dtype=ArrowDtype(pa.duration(\"ns\")))\n\n    msg = \"The behavior of ArrowTemporalProperties.to_pytimedelta is deprecated\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        result = ser.dt.to_pytimedelta()\n    expected = np.array(data, dtype=object)\n    tm.assert_numpy_array_equal(result, expected)\n    assert all(type(res) is timedelta for res in result)\n\n    msg = \"The behavior of TimedeltaProperties.to_pytimedelta is deprecated\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        expected = ser.astype(\"timedelta64[ns]\").dt.to_pytimedelta()\n    tm.assert_numpy_array_equal(result, expected)\n\n\ndef test_dt_components():\n    # GH 52284\n    ser = pd.Series(\n        [\n            pd.Timedelta(\n                days=1,\n                seconds=2,\n                microseconds=3,\n                nanoseconds=4,\n            ),\n            None,\n        ],\n        dtype=ArrowDtype(pa.duration(\"ns\")),\n    )\n    result = ser.dt.components\n    expected = pd.DataFrame(\n        [[1, 0, 0, 2, 0, 3, 4], [None, None, None, None, None, None, None]],\n        columns=[\n            \"days\",\n            \"hours\",\n            \"minutes\",\n            \"seconds\",\n            \"milliseconds\",\n            \"microseconds\",\n            \"nanoseconds\",\n        ],\n        dtype=\"int32[pyarrow]\",\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_dt_components_large_values():\n    ser = pd.Series(\n        [\n            pd.Timedelta(\"365 days 23:59:59.999000\"),\n            None,\n        ],\n        dtype=ArrowDtype(pa.duration(\"ns\")),\n    )\n    result = ser.dt.components\n    expected = pd.DataFrame(\n        [[365, 23, 59, 59, 999, 0, 0], [None, None, None, None, None, None, None]],\n        columns=[\n            \"days\",\n            \"hours\",\n            \"minutes\",\n            \"seconds\",\n            \"milliseconds\",\n            \"microseconds\",\n            \"nanoseconds\",\n        ],\n        dtype=\"int32[pyarrow]\",\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"skipna\", [True, False])\ndef test_boolean_reduce_series_all_null(all_boolean_reductions, skipna):\n    # GH51624\n    ser = pd.Series([None], dtype=\"float64[pyarrow]\")\n    result = getattr(ser, all_boolean_reductions)(skipna=skipna)\n    if skipna:\n        expected = all_boolean_reductions == \"all\"\n    else:\n        expected = pd.NA\n    assert result is expected\n\n\ndef test_from_sequence_of_strings_boolean():\n    true_strings = [\"true\", \"TRUE\", \"True\", \"1\", \"1.0\"]\n    false_strings = [\"false\", \"FALSE\", \"False\", \"0\", \"0.0\"]\n    nulls = [None]\n    strings = true_strings + false_strings + nulls\n    bools = (\n        [True] * len(true_strings) + [False] * len(false_strings) + [None] * len(nulls)\n    )\n\n    dtype = ArrowDtype(pa.bool_())\n    result = ArrowExtensionArray._from_sequence_of_strings(strings, dtype=dtype)\n    expected = pd.array(bools, dtype=\"boolean[pyarrow]\")\n    tm.assert_extension_array_equal(result, expected)\n\n    strings = [\"True\", \"foo\"]\n    with pytest.raises(pa.ArrowInvalid, match=\"Failed to parse\"):\n        ArrowExtensionArray._from_sequence_of_strings(strings, dtype=dtype)\n\n\ndef test_concat_empty_arrow_backed_series(dtype):\n    # GH#51734\n    ser = pd.Series([], dtype=dtype)\n    expected = ser.copy()\n    result = pd.concat([ser[np.array([], dtype=np.bool_)]])\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"dtype\", [\"string\", \"string[pyarrow]\"])\ndef test_series_from_string_array(dtype):\n    arr = pa.array(\"the quick brown fox\".split())\n    ser = pd.Series(arr, dtype=dtype)\n    expected = pd.Series(ArrowExtensionArray(arr), dtype=dtype)\n    tm.assert_series_equal(ser, expected)\n\n\n# _data was renamed to _pa_data\nclass OldArrowExtensionArray(ArrowExtensionArray):\n    def __getstate__(self):\n        state = super().__getstate__()\n        state[\"_data\"] = state.pop(\"_pa_array\")\n        return state\n\n\ndef test_pickle_old_arrowextensionarray():\n    data = pa.array([1])\n    expected = OldArrowExtensionArray(data)\n    result = pickle.loads(pickle.dumps(expected))\n    tm.assert_extension_array_equal(result, expected)\n    assert result._pa_array == pa.chunked_array(data)\n    assert not hasattr(result, \"_data\")\n\n\ndef test_setitem_boolean_replace_with_mask_segfault():\n    # GH#52059\n    N = 145_000\n    arr = ArrowExtensionArray(pa.chunked_array([np.ones((N,), dtype=np.bool_)]))\n    expected = arr.copy()\n    arr[np.zeros((N,), dtype=np.bool_)] = False\n    assert arr._pa_array == expected._pa_array\n\n\n@pytest.mark.parametrize(\n    \"data, arrow_dtype\",\n    [\n        ([b\"a\", b\"b\"], pa.large_binary()),\n        ([\"a\", \"b\"], pa.large_string()),\n    ],\n)\ndef test_conversion_large_dtypes_from_numpy_array(data, arrow_dtype):\n    dtype = ArrowDtype(arrow_dtype)\n    result = pd.array(np.array(data), dtype=dtype)\n    expected = pd.array(data, dtype=dtype)\n    tm.assert_extension_array_equal(result, expected)\n\n\ndef test_concat_null_array():\n    df = pd.DataFrame({\"a\": [None, None]}, dtype=ArrowDtype(pa.null()))\n    df2 = pd.DataFrame({\"a\": [0, 1]}, dtype=\"int64[pyarrow]\")\n\n    result = pd.concat([df, df2], ignore_index=True)\n    expected = pd.DataFrame({\"a\": [None, None, 0, 1]}, dtype=\"int64[pyarrow]\")\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"pa_type\", tm.ALL_INT_PYARROW_DTYPES + tm.FLOAT_PYARROW_DTYPES)\ndef test_describe_numeric_data(pa_type):\n    # GH 52470\n    data = pd.Series([1, 2, 3], dtype=ArrowDtype(pa_type))\n    result = data.describe()\n    expected = pd.Series(\n        [3, 2, 1, 1, 1.5, 2.0, 2.5, 3],\n        dtype=ArrowDtype(pa.float64()),\n        index=[\"count\", \"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"],\n    )\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"pa_type\", tm.TIMEDELTA_PYARROW_DTYPES)\ndef test_describe_timedelta_data(pa_type):\n    # GH53001\n    data = pd.Series(range(1, 10), dtype=ArrowDtype(pa_type))\n    result = data.describe()\n    expected = pd.Series(\n        [9] + pd.to_timedelta([5, 2, 1, 3, 5, 7, 9], unit=pa_type.unit).tolist(),\n        dtype=object,\n        index=[\"count\", \"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"],\n    )\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"pa_type\", tm.DATETIME_PYARROW_DTYPES)\ndef test_describe_datetime_data(pa_type):\n    # GH53001\n    data = pd.Series(range(1, 10), dtype=ArrowDtype(pa_type))\n    result = data.describe()\n    expected = pd.Series(\n        [9]\n        + [\n            pd.Timestamp(v, tz=pa_type.tz, unit=pa_type.unit)\n            for v in [5, 1, 3, 5, 7, 9]\n        ],\n        dtype=object,\n        index=[\"count\", \"mean\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"],\n    )\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"pa_type\", tm.DATETIME_PYARROW_DTYPES + tm.TIMEDELTA_PYARROW_DTYPES\n)\ndef test_quantile_temporal(pa_type):\n    # GH52678\n    data = [1, 2, 3]\n    ser = pd.Series(data, dtype=ArrowDtype(pa_type))\n    result = ser.quantile(0.1)\n    expected = ser[0]\n    assert result == expected\n\n\ndef test_date32_repr():\n    # GH48238\n    arrow_dt = pa.array([date.fromisoformat(\"2020-01-01\")], type=pa.date32())\n    ser = pd.Series(arrow_dt, dtype=ArrowDtype(arrow_dt.type))\n    assert repr(ser) == \"0    2020-01-01\\ndtype: date32[day][pyarrow]\"\n\n\ndef test_duration_overflow_from_ndarray_containing_nat():\n    # GH52843\n    data_ts = pd.to_datetime([1, None])\n    data_td = pd.to_timedelta([1, None])\n    ser_ts = pd.Series(data_ts, dtype=ArrowDtype(pa.timestamp(\"ns\")))\n    ser_td = pd.Series(data_td, dtype=ArrowDtype(pa.duration(\"ns\")))\n    result = ser_ts + ser_td\n    expected = pd.Series([2, None], dtype=ArrowDtype(pa.timestamp(\"ns\")))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_infer_dtype_pyarrow_dtype(data, request):\n    res = lib.infer_dtype(data)\n    assert res != \"unknown-array\"\n\n    if data._hasna and res in [\"floating\", \"datetime64\", \"timedelta64\"]:\n        mark = pytest.mark.xfail(\n            reason=\"in infer_dtype pd.NA is not ignored in these cases \"\n            \"even with skipna=True in the list(data) check below\"\n        )\n        request.applymarker(mark)\n\n    assert res == lib.infer_dtype(list(data), skipna=True)\n\n\n@pytest.mark.parametrize(\n    \"pa_type\", tm.DATETIME_PYARROW_DTYPES + tm.TIMEDELTA_PYARROW_DTYPES\n)\ndef test_from_sequence_temporal(pa_type):\n    # GH 53171\n    val = 3\n    unit = pa_type.unit\n    if pa.types.is_duration(pa_type):\n        seq = [pd.Timedelta(val, unit=unit).as_unit(unit)]\n    else:\n        seq = [pd.Timestamp(val, unit=unit, tz=pa_type.tz).as_unit(unit)]\n\n    result = ArrowExtensionArray._from_sequence(seq, dtype=pa_type)\n    expected = ArrowExtensionArray(pa.array([val], type=pa_type))\n    tm.assert_extension_array_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"pa_type\", tm.DATETIME_PYARROW_DTYPES + tm.TIMEDELTA_PYARROW_DTYPES\n)\ndef test_setitem_temporal(pa_type):\n    # GH 53171\n    unit = pa_type.unit\n    if pa.types.is_duration(pa_type):\n        val = pd.Timedelta(1, unit=unit).as_unit(unit)\n    else:\n        val = pd.Timestamp(1, unit=unit, tz=pa_type.tz).as_unit(unit)\n\n    arr = ArrowExtensionArray(pa.array([1, 2, 3], type=pa_type))\n\n    result = arr.copy()\n    result[:] = val\n    expected = ArrowExtensionArray(pa.array([1, 1, 1], type=pa_type))\n    tm.assert_extension_array_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"pa_type\", tm.DATETIME_PYARROW_DTYPES + tm.TIMEDELTA_PYARROW_DTYPES\n)\ndef test_arithmetic_temporal(pa_type, request):\n    # GH 53171\n    arr = ArrowExtensionArray(pa.array([1, 2, 3], type=pa_type))\n    unit = pa_type.unit\n    result = arr - pd.Timedelta(1, unit=unit).as_unit(unit)\n    expected = ArrowExtensionArray(pa.array([0, 1, 2], type=pa_type))\n    tm.assert_extension_array_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"pa_type\", tm.DATETIME_PYARROW_DTYPES + tm.TIMEDELTA_PYARROW_DTYPES\n)\ndef test_comparison_temporal(pa_type):\n    # GH 53171\n    unit = pa_type.unit\n    if pa.types.is_duration(pa_type):\n        val = pd.Timedelta(1, unit=unit).as_unit(unit)\n    else:\n        val = pd.Timestamp(1, unit=unit, tz=pa_type.tz).as_unit(unit)\n\n    arr = ArrowExtensionArray(pa.array([1, 2, 3], type=pa_type))\n\n    result = arr > val\n    expected = ArrowExtensionArray(pa.array([False, True, True], type=pa.bool_()))\n    tm.assert_extension_array_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"pa_type\", tm.DATETIME_PYARROW_DTYPES + tm.TIMEDELTA_PYARROW_DTYPES\n)\ndef test_getitem_temporal(pa_type):\n    # GH 53326\n    arr = ArrowExtensionArray(pa.array([1, 2, 3], type=pa_type))\n    result = arr[1]\n    if pa.types.is_duration(pa_type):\n        expected = pd.Timedelta(2, unit=pa_type.unit).as_unit(pa_type.unit)\n        assert isinstance(result, pd.Timedelta)\n    else:\n        expected = pd.Timestamp(2, unit=pa_type.unit, tz=pa_type.tz).as_unit(\n            pa_type.unit\n        )\n        assert isinstance(result, pd.Timestamp)\n    assert result.unit == expected.unit\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    \"pa_type\", tm.DATETIME_PYARROW_DTYPES + tm.TIMEDELTA_PYARROW_DTYPES\n)\ndef test_iter_temporal(pa_type):\n    # GH 53326\n    arr = ArrowExtensionArray(pa.array([1, None], type=pa_type))\n    result = list(arr)\n    if pa.types.is_duration(pa_type):\n        expected = [\n            pd.Timedelta(1, unit=pa_type.unit).as_unit(pa_type.unit),\n            pd.NA,\n        ]\n        assert isinstance(result[0], pd.Timedelta)\n    else:\n        expected = [\n            pd.Timestamp(1, unit=pa_type.unit, tz=pa_type.tz).as_unit(pa_type.unit),\n            pd.NA,\n        ]\n        assert isinstance(result[0], pd.Timestamp)\n    assert result[0].unit == expected[0].unit\n    assert result == expected\n\n\ndef test_groupby_series_size_returns_pa_int(data):\n    # GH 54132\n    ser = pd.Series(data[:3], index=[\"a\", \"a\", \"b\"])\n    result = ser.groupby(level=0).size()\n    expected = pd.Series([2, 1], dtype=\"int64[pyarrow]\", index=[\"a\", \"b\"])\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"pa_type\", tm.DATETIME_PYARROW_DTYPES + tm.TIMEDELTA_PYARROW_DTYPES, ids=repr\n)\n@pytest.mark.parametrize(\"dtype\", [None, object])\ndef test_to_numpy_temporal(pa_type, dtype):\n    # GH 53326\n    # GH 55997: Return datetime64/timedelta64 types with NaT if possible\n    arr = ArrowExtensionArray(pa.array([1, None], type=pa_type))\n    result = arr.to_numpy(dtype=dtype)\n    if pa.types.is_duration(pa_type):\n        value = pd.Timedelta(1, unit=pa_type.unit).as_unit(pa_type.unit)\n    else:\n        value = pd.Timestamp(1, unit=pa_type.unit, tz=pa_type.tz).as_unit(pa_type.unit)\n\n    if dtype == object or (pa.types.is_timestamp(pa_type) and pa_type.tz is not None):\n        if dtype == object:\n            na = pd.NA\n        else:\n            na = pd.NaT\n        expected = np.array([value, na], dtype=object)\n        assert result[0].unit == value.unit\n    else:\n        na = pa_type.to_pandas_dtype().type(\"nat\", pa_type.unit)\n        value = value.to_numpy()\n        expected = np.array([value, na])\n        assert np.datetime_data(result[0])[0] == pa_type.unit\n    tm.assert_numpy_array_equal(result, expected)\n\n\ndef test_groupby_count_return_arrow_dtype(data_missing):\n    df = pd.DataFrame({\"A\": [1, 1], \"B\": data_missing, \"C\": data_missing})\n    result = df.groupby(\"A\").count()\n    expected = pd.DataFrame(\n        [[1, 1]],\n        index=pd.Index([1], name=\"A\"),\n        columns=[\"B\", \"C\"],\n        dtype=\"int64[pyarrow]\",\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_fixed_size_list():\n    # GH#55000\n    ser = pd.Series(\n        [[1, 2], [3, 4]], dtype=ArrowDtype(pa.list_(pa.int64(), list_size=2))\n    )\n    result = ser.dtype.type\n    assert result == list\n\n\ndef test_arrowextensiondtype_dataframe_repr():\n    # GH 54062\n    df = pd.DataFrame(\n        pd.period_range(\"2012\", periods=3),\n        columns=[\"col\"],\n        dtype=ArrowDtype(ArrowPeriodType(\"D\")),\n    )\n    result = repr(df)\n    # TODO: repr value may not be expected; address how\n    # pyarrow.ExtensionType values are displayed\n    expected = \"     col\\n0  15340\\n1  15341\\n2  15342\"\n    assert result == expected\n\n\ndef test_pow_missing_operand():\n    # GH 55512\n    k = pd.Series([2, None], dtype=\"int64[pyarrow]\")\n    result = k.pow(None, fill_value=3)\n    expected = pd.Series([8, None], dtype=\"int64[pyarrow]\")\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.skipif(\n    pa_version_under11p0, reason=\"Decimal128 to string cast implemented in pyarrow 11\"\n)\ndef test_decimal_parse_raises():\n    # GH 56984\n    ser = pd.Series([\"1.2345\"], dtype=ArrowDtype(pa.string()))\n    with pytest.raises(\n        pa.lib.ArrowInvalid, match=\"Rescaling Decimal128 value would cause data loss\"\n    ):\n        ser.astype(ArrowDtype(pa.decimal128(1, 0)))\n\n\n@pytest.mark.skipif(\n    pa_version_under11p0, reason=\"Decimal128 to string cast implemented in pyarrow 11\"\n)\ndef test_decimal_parse_succeeds():\n    # GH 56984\n    ser = pd.Series([\"1.2345\"], dtype=ArrowDtype(pa.string()))\n    dtype = ArrowDtype(pa.decimal128(5, 4))\n    result = ser.astype(dtype)\n    expected = pd.Series([Decimal(\"1.2345\")], dtype=dtype)\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"pa_type\", tm.TIMEDELTA_PYARROW_DTYPES)\ndef test_duration_fillna_numpy(pa_type):\n    # GH 54707\n    ser1 = pd.Series([None, 2], dtype=ArrowDtype(pa_type))\n    ser2 = pd.Series(np.array([1, 3], dtype=f\"m8[{pa_type.unit}]\"))\n    result = ser1.fillna(ser2)\n    expected = pd.Series([1, 2], dtype=ArrowDtype(pa_type))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_comparison_not_propagating_arrow_error():\n    # GH#54944\n    a = pd.Series([1 << 63], dtype=\"uint64[pyarrow]\")\n    b = pd.Series([None], dtype=\"int64[pyarrow]\")\n    with pytest.raises(pa.lib.ArrowInvalid, match=\"Integer value\"):\n        a < b\n\n\ndef test_factorize_chunked_dictionary():\n    # GH 54844\n    pa_array = pa.chunked_array(\n        [pa.array([\"a\"]).dictionary_encode(), pa.array([\"b\"]).dictionary_encode()]\n    )\n    ser = pd.Series(ArrowExtensionArray(pa_array))\n    res_indices, res_uniques = ser.factorize()\n    exp_indicies = np.array([0, 1], dtype=np.intp)\n    exp_uniques = pd.Index(ArrowExtensionArray(pa_array.combine_chunks()))\n    tm.assert_numpy_array_equal(res_indices, exp_indicies)\n    tm.assert_index_equal(res_uniques, exp_uniques)\n\n\ndef test_dictionary_astype_categorical():\n    # GH#56672\n    arrs = [\n        pa.array(np.array([\"a\", \"x\", \"c\", \"a\"])).dictionary_encode(),\n        pa.array(np.array([\"a\", \"d\", \"c\"])).dictionary_encode(),\n    ]\n    ser = pd.Series(ArrowExtensionArray(pa.chunked_array(arrs)))\n    result = ser.astype(\"category\")\n    categories = pd.Index([\"a\", \"x\", \"c\", \"d\"], dtype=ArrowDtype(pa.string()))\n    expected = pd.Series(\n        [\"a\", \"x\", \"c\", \"a\", \"a\", \"d\", \"c\"],\n        dtype=pd.CategoricalDtype(categories=categories),\n    )\n    tm.assert_series_equal(result, expected)\n\n\ndef test_arrow_floordiv():\n    # GH 55561\n    a = pd.Series([-7], dtype=\"int64[pyarrow]\")\n    b = pd.Series([4], dtype=\"int64[pyarrow]\")\n    expected = pd.Series([-2], dtype=\"int64[pyarrow]\")\n    result = a // b\n    tm.assert_series_equal(result, expected)\n\n\ndef test_arrow_floordiv_large_values():\n    # GH 56645\n    a = pd.Series([1425801600000000000], dtype=\"int64[pyarrow]\")\n    expected = pd.Series([1425801600000], dtype=\"int64[pyarrow]\")\n    result = a // 1_000_000\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"dtype\", [\"int64[pyarrow]\", \"uint64[pyarrow]\"])\ndef test_arrow_floordiv_large_integral_result(dtype):\n    # GH 56676\n    a = pd.Series([18014398509481983], dtype=dtype)\n    result = a // 1\n    tm.assert_series_equal(result, a)\n\n\n@pytest.mark.parametrize(\"pa_type\", tm.SIGNED_INT_PYARROW_DTYPES)\ndef test_arrow_floordiv_larger_divisor(pa_type):\n    # GH 56676\n    dtype = ArrowDtype(pa_type)\n    a = pd.Series([-23], dtype=dtype)\n    result = a // 24\n    expected = pd.Series([-1], dtype=dtype)\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"pa_type\", tm.SIGNED_INT_PYARROW_DTYPES)\ndef test_arrow_floordiv_integral_invalid(pa_type):\n    # GH 56676\n    min_value = np.iinfo(pa_type.to_pandas_dtype()).min\n    a = pd.Series([min_value], dtype=ArrowDtype(pa_type))\n    with pytest.raises(pa.lib.ArrowInvalid, match=\"overflow|not in range\"):\n        a // -1\n    with pytest.raises(pa.lib.ArrowInvalid, match=\"divide by zero\"):\n        a // 0\n\n\n@pytest.mark.parametrize(\"dtype\", tm.FLOAT_PYARROW_DTYPES_STR_REPR)\ndef test_arrow_floordiv_floating_0_divisor(dtype):\n    # GH 56676\n    a = pd.Series([2], dtype=dtype)\n    result = a // 0\n    expected = pd.Series([float(\"inf\")], dtype=dtype)\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"dtype\", [\"float64\", \"datetime64[ns]\", \"timedelta64[ns]\"])\ndef test_astype_int_with_null_to_numpy_dtype(dtype):\n    # GH 57093\n    ser = pd.Series([1, None], dtype=\"int64[pyarrow]\")\n    result = ser.astype(dtype)\n    expected = pd.Series([1, None], dtype=dtype)\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"pa_type\", tm.ALL_INT_PYARROW_DTYPES)\ndef test_arrow_integral_floordiv_large_values(pa_type):\n    # GH 56676\n    max_value = np.iinfo(pa_type.to_pandas_dtype()).max\n    dtype = ArrowDtype(pa_type)\n    a = pd.Series([max_value], dtype=dtype)\n    b = pd.Series([1], dtype=dtype)\n    result = a // b\n    tm.assert_series_equal(result, a)\n\n\n@pytest.mark.parametrize(\"dtype\", [\"int64[pyarrow]\", \"uint64[pyarrow]\"])\ndef test_arrow_true_division_large_divisor(dtype):\n    # GH 56706\n    a = pd.Series([0], dtype=dtype)\n    b = pd.Series([18014398509481983], dtype=dtype)\n    expected = pd.Series([0], dtype=\"float64[pyarrow]\")\n    result = a / b\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"dtype\", [\"int64[pyarrow]\", \"uint64[pyarrow]\"])\ndef test_arrow_floor_division_large_divisor(dtype):\n    # GH 56706\n    a = pd.Series([0], dtype=dtype)\n    b = pd.Series([18014398509481983], dtype=dtype)\n    expected = pd.Series([0], dtype=dtype)\n    result = a // b\n    tm.assert_series_equal(result, expected)\n\n\ndef test_string_to_datetime_parsing_cast():\n    # GH 56266\n    string_dates = [\"2020-01-01 04:30:00\", \"2020-01-02 00:00:00\", \"2020-01-03 00:00:00\"]\n    result = pd.Series(string_dates, dtype=\"timestamp[s][pyarrow]\")\n    expected = pd.Series(\n        ArrowExtensionArray(pa.array(pd.to_datetime(string_dates), from_pandas=True))\n    )\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.skipif(\n    pa_version_under13p0, reason=\"pairwise_diff_checked not implemented in pyarrow\"\n)\ndef test_interpolate_not_numeric(data):\n    if not data.dtype._is_numeric:\n        ser = pd.Series(data)\n        msg = re.escape(f\"Cannot interpolate with {ser.dtype} dtype\")\n        with pytest.raises(TypeError, match=msg):\n            pd.Series(data).interpolate()\n\n\n@pytest.mark.skipif(\n    pa_version_under13p0, reason=\"pairwise_diff_checked not implemented in pyarrow\"\n)\n@pytest.mark.parametrize(\"dtype\", [\"int64[pyarrow]\", \"float64[pyarrow]\"])\ndef test_interpolate_linear(dtype):\n    ser = pd.Series([None, 1, 2, None, 4, None], dtype=dtype)\n    result = ser.interpolate()\n    expected = pd.Series([None, 1, 2, 3, 4, None], dtype=dtype)\n    tm.assert_series_equal(result, expected)\n\n\ndef test_string_to_time_parsing_cast():\n    # GH 56463\n    string_times = [\"11:41:43.076160\"]\n    result = pd.Series(string_times, dtype=\"time64[us][pyarrow]\")\n    expected = pd.Series(\n        ArrowExtensionArray(pa.array([time(11, 41, 43, 76160)], from_pandas=True))\n    )\n    tm.assert_series_equal(result, expected)\n\n\ndef test_to_numpy_float():\n    # GH#56267\n    ser = pd.Series([32, 40, None], dtype=\"float[pyarrow]\")\n    result = ser.astype(\"float64\")\n    expected = pd.Series([32, 40, np.nan], dtype=\"float64\")\n    tm.assert_series_equal(result, expected)\n\n\ndef test_to_numpy_timestamp_to_int():\n    # GH 55997\n    ser = pd.Series([\"2020-01-01 04:30:00\"], dtype=\"timestamp[ns][pyarrow]\")\n    result = ser.to_numpy(dtype=np.int64)\n    expected = np.array([1577853000000000000])\n    tm.assert_numpy_array_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"arrow_type\", [pa.large_string(), pa.string()])\ndef test_cast_dictionary_different_value_dtype(arrow_type):\n    df = pd.DataFrame({\"a\": [\"x\", \"y\"]}, dtype=\"string[pyarrow]\")\n    data_type = ArrowDtype(pa.dictionary(pa.int32(), arrow_type))\n    result = df.astype({\"a\": data_type})\n    assert result.dtypes.iloc[0] == data_type\n\n\ndef test_map_numeric_na_action():\n    ser = pd.Series([32, 40, None], dtype=\"int64[pyarrow]\")\n    result = ser.map(lambda x: 42, na_action=\"ignore\")\n    expected = pd.Series([42.0, 42.0, np.nan], dtype=\"float64\")\n    tm.assert_series_equal(result, expected)\n\n\ndef test_categorical_from_arrow_dictionary():\n    # GH 60563\n    df = pd.DataFrame(\n        {\"A\": [\"a1\", \"a2\"]}, dtype=ArrowDtype(pa.dictionary(pa.int32(), pa.utf8()))\n    )\n    result = df.value_counts(dropna=False)\n    expected = pd.Series(\n        [1, 1],\n        index=pd.MultiIndex.from_arrays(\n            [pd.Index([\"a1\", \"a2\"], dtype=ArrowDtype(pa.string()), name=\"A\")]\n        ),\n        name=\"count\",\n        dtype=\"int64\",\n    )\n    tm.assert_series_equal(result, expected)\n"
    }
  ],
  "questions": [],
  "golden_answers": [],
  "questions_generated": [
    "What error occurs when calling `value_counts()` on a DataFrame with a PyArrow categorical column containing nulls, and what causes it?",
    "In the context of this issue, how does the behavior of `value_counts()` with PyArrow differ from the expected behavior using NumPy?",
    "Why might the use of `pd.ArrowDtype` with `pa.dictionary` lead to issues when calling `value_counts()`?",
    "What is the expected output for the second case in the issue when using NumPy, and what discrepancy is observed with PyArrow?",
    "How does the handling of nulls in PyArrow categorical columns affect the result of `value_counts()` in the described bug?"
  ],
  "golden_answers_generated": [
    "The error that occurs is an AttributeError: 'Index' object has no attribute '_pa_array'. This error is caused because the `value_counts()` method attempts to use an internal attribute `_pa_array` which doesn't exist on the Index object returned by the PyArrow categorical column, leading to the failure.",
    "With PyArrow, `value_counts()` either throws an error or provides incorrect results when handling categorical columns with nulls. In contrast, the expected behavior with NumPy backend is to correctly count the values, including nulls, without errors as shown by the provided expected results for both single and multiple column cases.",
    "The use of `pd.ArrowDtype` with `pa.dictionary` likely leads to issues because the dictionary encoding expects a consistent index mapping for categorical data, but the presence of nulls or the handling of internal PyArrow array attributes like `_pa_array` is not correctly managed within the pandas implementation of `value_counts()`, resulting in errors or incorrect outputs.",
    "The expected output for the second case using NumPy would be a count of the distinct combinations of values across columns, correctly handling nulls. The discrepancy with PyArrow is that the output incorrectly shows a value ('d1') where a null (`<NA>`) should be present in the result for column 'D'.",
    "The handling of nulls in PyArrow categorical columns affects `value_counts()` by either causing an error or leading to incorrect results. Since PyArrow may not properly interpret or manage null values within dictionary-encoded data, the `value_counts()` method fails to correctly account for these nulls, either by failing with an attribute error or by misrepresenting the nulls in the output."
  ]
}