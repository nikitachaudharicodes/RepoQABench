{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "6051",
  "issue_description": "# BUG: read multi-index column csv with index_col=False borks\n\nhttp://stackoverflow.com/questions/21318865/read-multi-index-on-the-columns-from-csv-file\n",
  "issue_comments": [
    {
      "id": 33168092,
      "user": "hayd",
      "body": "For convenience, here's test case:\n\n```\nfrom StringIO import StringIO\ns1 = '''Male, Male, Male, Female, Female\nR, R, L, R, R\n.86, .67, .88, .78, .81'''\n\ns2 = '''Male, Male, Male, Female, Female\nR, R, L, R, R\n.86, .67, .88, .78, .81\n.86, .67, .88, .78, .82'''\n\nIn [11]: pd.read_csv(StringIO(s1), header=[0, 1])\nOut[11]: \nEmpty DataFrame\nColumns: [(Male, R), ( Male,  R), ( Male,  L), ( Female,  R), ( Female,  R)]\nIndex: []\n\nIn [12]: pd.read_csv(StringIO(s2), header=[0, 1])\nOut[12]: \n   (Male, R)  ( Male,  R)  ( Male,  L)  ( Female,  R)  ( Female,  R)\n0       0.86         0.67         0.88           0.82           0.82\n```\n\nseems to skip first row after header.\n\nNote: columns tuplized as wanted to see if this was also a bug in 0.12.\n"
    },
    {
      "id": 33168230,
      "user": "jreback",
      "body": "@hayd could try `tupleize_cols=True` and and see if it works\n"
    },
    {
      "id": 33168431,
      "user": "TomAugspurger",
      "body": "Mine is skipping all the subsequent rows (`s1` and `s2` are from Andy's example):\n\n```\nIn [5]: pd.read_csv(StringIO(s1), header=[0, 1])\nOut[5]: \nEmpty DataFrame\nColumns: [(Male, R), ( Male,  R), ( Male,  L), ( Female,  R), ( Female,  R)]\nIndex: []\n\n[0 rows x 5 columns]\n\nIn [6]: pd.read_csv(StringIO(s2), header=[0, 1])\nOut[6]: \nEmpty DataFrame\nColumns: [(Male, R), ( Male,  R), ( Male,  L), ( Female,  R), ( Female,  R)]\nIndex: []\n\n[0 rows x 5 columns]\n```\n"
    },
    {
      "id": 33168795,
      "user": "hayd",
      "body": "@jreback maybe was just being thick about tuplize columns (forgot repr of mi) is working fine, OT though.\n\nThere is a change in 0.12 and 0.13. I see what @TomAugspurger sees in 0.13.\n"
    },
    {
      "id": 33169245,
      "user": "jreback",
      "body": "The problem is that it is confused by the lack of an `index_col` I think; specify `index_col=0` actually works (but kills the first value....)\n"
    },
    {
      "id": 33169841,
      "user": "hayd",
      "body": "Seems like the column after the header is being used for the naming of the index?\n\n```\nIn [11]: pd.read_csv(StringIO(s1), header=[0, 1], index_col=0)\nOut[11]: \nEmpty DataFrame\nColumns: [( Male,  R), ( Male,  L), ( Female,  R), ( Female,  R)]\nIndex: []\n\nIn [12]: pd.read_csv(StringIO(s2), header=[0, 1], index_col=0)\nOut[12]: \n      ( Male,  R)  ( Male,  L)  ( Female,  R)  ( Female,  R)\n.86                                                         \n0.86         0.67         0.88           0.82           0.82\n```\n"
    },
    {
      "id": 33170690,
      "user": "jreback",
      "body": "@hayd yes if it can, but this is where the index_col matters, it is a heuristic (and maybe wrong in this case)\n"
    },
    {
      "id": 33196002,
      "user": "waitingkuo",
      "body": "Seems the problem is caused by the duplicated columns ( Female, R). If you modify the second row to `a, b, c, d, e`,  the function works normally. Is it a bug? Or should we throw some exception while there're duplicated multi-columns?\n"
    },
    {
      "id": 33217069,
      "user": "jreback",
      "body": "@waitingkuo hmm a duplicated multi index is technically valid (prob not tested very well though)\nI think this my be related to index_col - it basically has to try to guess if their are names present or not \n\nwant to dig in?\n"
    },
    {
      "id": 33451749,
      "user": "waitingkuo",
      "body": "For duplicated single column, some sequence numbers would be append:\n\n```\nIn [13]: pd.read_csv(StringIO('R,R,L,R,R\\n1,2,3,4,5'))\nOut[13]: \n   R  R.1  L  R.2  R.3\n0  1    2  3    4    5\n\n[1 rows x 5 columns]\n```\n\nAccording to this logic, the multi-column one \n\n```\nMale,Male,Male,Female,Female\nR,R,L,R,R\n```\n\nshould be converted to\n\n```\nMale,Male,Male,Female,Female\nR,R.1,L,R,R.1\n```\n\nDoes it make sense?\n"
    },
    {
      "id": 33452659,
      "user": "hayd",
      "body": "That looks correct. However there is also a flag for this, `mangle_dupe_cols`:\n\n```\nIn [7]: pd.read_csv(StringIO('R,R,L,R,R\\n1,2,3,4,5'), mangle_dupe_cols=False)\nOut[7]: \n   R  R  L  R  R\n0  5  5  3  5  5\n\n[1 rows x 5 columns]\n```\n"
    },
    {
      "id": 33452689,
      "user": "hayd",
      "body": "Well... er that's a bug!\n"
    },
    {
      "id": 33454275,
      "user": "waitingkuo",
      "body": "Things also go wrong when we set header as a list\n\n```\nIn [4]: pd.read_csv(StringIO('R,R,L,R,R\\n1,2,3,4,5'), header=[0])\nOut[4]:  \nEmpty DataFrame\nColumns: [R, R, L, R, R]\nIndex: []\n\n[0 rows x 5 columns]\n```\n"
    },
    {
      "id": 33577651,
      "user": "waitingkuo",
      "body": "I've figured out the problem and fixed it in python2. However, I got stuck in python3. Can anyone who have experience in python3 give me a hand?\n\nMy commit\nhttps://github.com/waitingkuo/pandas/commit/b969e9640d72f9f91d8b47513c706294ac139c43\n\nMy Travis Failed build \nhttps://travis-ci.org/waitingkuo/pandas/jobs/17788195\n"
    },
    {
      "id": 33578243,
      "user": "jreback",
      "body": "use lzip instead of zip\nit's imported from pandas.compat\n"
    },
    {
      "id": 33596937,
      "user": "waitingkuo",
      "body": "Thank you for helping :)\nI've made the pull request\n"
    },
    {
      "id": 343168944,
      "user": "Licht-T",
      "body": "@jreback @gfyoung This seems fixed. We need to close this issue."
    },
    {
      "id": 343171620,
      "user": "jreback",
      "body": "are there tests covering this case? if not can u put one up"
    },
    {
      "id": 343172375,
      "user": "Licht-T",
      "body": "@jreback Okay. I'll check."
    },
    {
      "id": 343472336,
      "user": "Licht-T",
      "body": "@jreback Seems that https://github.com/pandas-dev/pandas/pull/17060 fixed this bug. But there is no test for multi-index columns."
    },
    {
      "id": 343487904,
      "user": "jreback",
      "body": "cc @gfyoung "
    }
  ],
  "text_context": "# BUG: read multi-index column csv with index_col=False borks\n\nhttp://stackoverflow.com/questions/21318865/read-multi-index-on-the-columns-from-csv-file\n\n\nFor convenience, here's test case:\n\n```\nfrom StringIO import StringIO\ns1 = '''Male, Male, Male, Female, Female\nR, R, L, R, R\n.86, .67, .88, .78, .81'''\n\ns2 = '''Male, Male, Male, Female, Female\nR, R, L, R, R\n.86, .67, .88, .78, .81\n.86, .67, .88, .78, .82'''\n\nIn [11]: pd.read_csv(StringIO(s1), header=[0, 1])\nOut[11]: \nEmpty DataFrame\nColumns: [(Male, R), ( Male,  R), ( Male,  L), ( Female,  R), ( Female,  R)]\nIndex: []\n\nIn [12]: pd.read_csv(StringIO(s2), header=[0, 1])\nOut[12]: \n   (Male, R)  ( Male,  R)  ( Male,  L)  ( Female,  R)  ( Female,  R)\n0       0.86         0.67         0.88           0.82           0.82\n```\n\nseems to skip first row after header.\n\nNote: columns tuplized as wanted to see if this was also a bug in 0.12.\n\n\n@hayd could try `tupleize_cols=True` and and see if it works\n\n\nMine is skipping all the subsequent rows (`s1` and `s2` are from Andy's example):\n\n```\nIn [5]: pd.read_csv(StringIO(s1), header=[0, 1])\nOut[5]: \nEmpty DataFrame\nColumns: [(Male, R), ( Male,  R), ( Male,  L), ( Female,  R), ( Female,  R)]\nIndex: []\n\n[0 rows x 5 columns]\n\nIn [6]: pd.read_csv(StringIO(s2), header=[0, 1])\nOut[6]: \nEmpty DataFrame\nColumns: [(Male, R), ( Male,  R), ( Male,  L), ( Female,  R), ( Female,  R)]\nIndex: []\n\n[0 rows x 5 columns]\n```\n\n\n@jreback maybe was just being thick about tuplize columns (forgot repr of mi) is working fine, OT though.\n\nThere is a change in 0.12 and 0.13. I see what @TomAugspurger sees in 0.13.\n\n\nThe problem is that it is confused by the lack of an `index_col` I think; specify `index_col=0` actually works (but kills the first value....)\n\n\nSeems like the column after the header is being used for the naming of the index?\n\n```\nIn [11]: pd.read_csv(StringIO(s1), header=[0, 1], index_col=0)\nOut[11]: \nEmpty DataFrame\nColumns: [( Male,  R), ( Male,  L), ( Female,  R), ( Female,  R)]\nIndex: []\n\nIn [12]: pd.read_csv(StringIO(s2), header=[0, 1], index_col=0)\nOut[12]: \n      ( Male,  R)  ( Male,  L)  ( Female,  R)  ( Female,  R)\n.86                                                         \n0.86         0.67         0.88           0.82           0.82\n```\n\n\n@hayd yes if it can, but this is where the index_col matters, it is a heuristic (and maybe wrong in this case)\n\n\nSeems the problem is caused by the duplicated columns ( Female, R). If you modify the second row to `a, b, c, d, e`,  the function works normally. Is it a bug? Or should we throw some exception while there're duplicated multi-columns?\n\n\n@waitingkuo hmm a duplicated multi index is technically valid (prob not tested very well though)\nI think this my be related to index_col - it basically has to try to guess if their are names present or not \n\nwant to dig in?\n\n\nFor duplicated single column, some sequence numbers would be append:\n\n```\nIn [13]: pd.read_csv(StringIO('R,R,L,R,R\\n1,2,3,4,5'))\nOut[13]: \n   R  R.1  L  R.2  R.3\n0  1    2  3    4    5\n\n[1 rows x 5 columns]\n```\n\nAccording to this logic, the multi-column one \n\n```\nMale,Male,Male,Female,Female\nR,R,L,R,R\n```\n\nshould be converted to\n\n```\nMale,Male,Male,Female,Female\nR,R.1,L,R,R.1\n```\n\nDoes it make sense?\n\n\nThat looks correct. However there is also a flag for this, `mangle_dupe_cols`:\n\n```\nIn [7]: pd.read_csv(StringIO('R,R,L,R,R\\n1,2,3,4,5'), mangle_dupe_cols=False)\nOut[7]: \n   R  R  L  R  R\n0  5  5  3  5  5\n\n[1 rows x 5 columns]\n```\n\n\nWell... er that's a bug!\n\n\nThings also go wrong when we set header as a list\n\n```\nIn [4]: pd.read_csv(StringIO('R,R,L,R,R\\n1,2,3,4,5'), header=[0])\nOut[4]:  \nEmpty DataFrame\nColumns: [R, R, L, R, R]\nIndex: []\n\n[0 rows x 5 columns]\n```\n\n\nI've figured out the problem and fixed it in python2. However, I got stuck in python3. Can anyone who have experience in python3 give me a hand?\n\nMy commit\nhttps://github.com/waitingkuo/pandas/commit/b969e9640d72f9f91d8b47513c706294ac139c43\n\nMy Travis Failed build \nhttps://travis-ci.org/waitingkuo/pandas/jobs/17788195\n\n\nuse lzip instead of zip\nit's imported from pandas.compat\n\n\nThank you for helping :)\nI've made the pull request\n\n\n@jreback @gfyoung This seems fixed. We need to close this issue.\n\nare there tests covering this case? if not can u put one up\n\n@jreback Okay. I'll check.\n\n@jreback Seems that https://github.com/pandas-dev/pandas/pull/17060 fixed this bug. But there is no test for multi-index columns.\n\ncc @gfyoung ",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/17060",
  "code_context": [
    {
      "filename": "pandas/io/parsers.py",
      "content": "\"\"\"\nModule contains tools for processing files into DataFrames or other objects\n\"\"\"\nfrom __future__ import print_function\nfrom collections import defaultdict\nimport re\nimport csv\nimport sys\nimport warnings\nimport datetime\nfrom textwrap import fill\n\nimport numpy as np\n\nfrom pandas import compat\nfrom pandas.compat import (range, lrange, PY3, StringIO, lzip,\n                           zip, string_types, map, u)\nfrom pandas.core.dtypes.common import (\n    is_integer, _ensure_object,\n    is_list_like, is_integer_dtype,\n    is_float, is_dtype_equal,\n    is_object_dtype, is_string_dtype,\n    is_scalar, is_categorical_dtype)\nfrom pandas.core.dtypes.missing import isnull\nfrom pandas.core.dtypes.cast import astype_nansafe\nfrom pandas.core.index import Index, MultiIndex, RangeIndex\nfrom pandas.core.series import Series\nfrom pandas.core.frame import DataFrame\nfrom pandas.core.categorical import Categorical\nfrom pandas.core import algorithms\nfrom pandas.core.common import AbstractMethodError\nfrom pandas.io.date_converters import generic_parser\nfrom pandas.errors import ParserWarning, ParserError, EmptyDataError\nfrom pandas.io.common import (get_filepath_or_buffer, is_file_like,\n                              _validate_header_arg, _get_handle,\n                              UnicodeReader, UTF8Recoder, _NA_VALUES,\n                              BaseIterator, _infer_compression)\nfrom pandas.core.tools import datetimes as tools\n\nfrom pandas.util._decorators import Appender\n\nimport pandas._libs.lib as lib\nimport pandas._libs.parsers as parsers\n\n\n# BOM character (byte order mark)\n# This exists at the beginning of a file to indicate endianness\n# of a file (stream). Unfortunately, this marker screws up parsing,\n# so we need to remove it if we see it.\n_BOM = u('\\ufeff')\n\n_parser_params = \"\"\"Also supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the `online docs for IO Tools\n<http://pandas.pydata.org/pandas-docs/stable/io.html>`_.\n\nParameters\n----------\nfilepath_or_buffer : str, pathlib.Path, py._path.local.LocalPath or any \\\nobject with a read() method (such as a file handle or StringIO)\n    The string could be a URL. Valid URL schemes include http, ftp, s3, and\n    file. For file URLs, a host is expected. For instance, a local file could\n    be file ://localhost/path/to/table.csv\n%s\ndelim_whitespace : boolean, default False\n    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n    used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n    is set to True, nothing should be passed in for the ``delimiter``\n    parameter.\n\n    .. versionadded:: 0.18.1 support for the Python parser.\n\nheader : int or list of ints, default 'infer'\n    Row number(s) to use as the column names, and the start of the data.\n    Default behavior is as if set to 0 if no ``names`` passed, otherwise\n    ``None``. Explicitly pass ``header=0`` to be able to replace existing\n    names. The header can be a list of integers that specify row locations for\n    a multi-index on the columns e.g. [0,1,3]. Intervening rows that are not\n    specified will be skipped (e.g. 2 in this example is skipped). Note that\n    this parameter ignores commented lines and empty lines if\n    ``skip_blank_lines=True``, so header=0 denotes the first line of data\n    rather than the first line of the file.\nnames : array-like, default None\n    List of column names to use. If file contains no header row, then you\n    should explicitly pass header=None. Duplicates in this list are not\n    allowed unless mangle_dupe_cols=True, which is the default.\nindex_col : int or sequence or False, default None\n    Column to use as the row labels of the DataFrame. If a sequence is given, a\n    MultiIndex is used. If you have a malformed file with delimiters at the end\n    of each line, you might consider index_col=False to force pandas to _not_\n    use the first column as the index (row names)\nusecols : array-like or callable, default None\n    Return a subset of the columns. If array-like, all elements must either\n    be positional (i.e. integer indices into the document columns) or strings\n    that correspond to column names provided either by the user in `names` or\n    inferred from the document header row(s). For example, a valid array-like\n    `usecols` parameter would be [0, 1, 2] or ['foo', 'bar', 'baz'].\n\n    If callable, the callable function will be evaluated against the column\n    names, returning names where the callable function evaluates to True. An\n    example of a valid callable argument would be ``lambda x: x.upper() in\n    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n    parsing time and lower memory usage.\nas_recarray : boolean, default False\n    .. deprecated:: 0.19.0\n       Please call `pd.read_csv(...).to_records()` instead.\n\n    Return a NumPy recarray instead of a DataFrame after parsing the data.\n    If set to True, this option takes precedence over the `squeeze` parameter.\n    In addition, as row indices are not available in such a format, the\n    `index_col` parameter will be ignored.\nsqueeze : boolean, default False\n    If the parsed data only contains one column then return a Series\nprefix : str, default None\n    Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\nmangle_dupe_cols : boolean, default True\n    Duplicate columns will be specified as 'X.0'...'X.N', rather than\n    'X'...'X'. Passing in False will cause data to be overwritten if there\n    are duplicate names in the columns.\ndtype : Type name or dict of column -> type, default None\n    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}\n    Use `str` or `object` to preserve and not interpret dtype.\n    If converters are specified, they will be applied INSTEAD\n    of dtype conversion.\n%s\nconverters : dict, default None\n    Dict of functions for converting values in certain columns. Keys can either\n    be integers or column labels\ntrue_values : list, default None\n    Values to consider as True\nfalse_values : list, default None\n    Values to consider as False\nskipinitialspace : boolean, default False\n    Skip spaces after delimiter.\nskiprows : list-like or integer or callable, default None\n    Line numbers to skip (0-indexed) or number of lines to skip (int)\n    at the start of the file.\n\n    If callable, the callable function will be evaluated against the row\n    indices, returning True if the row should be skipped and False otherwise.\n    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\nskipfooter : int, default 0\n    Number of lines at bottom of file to skip (Unsupported with engine='c')\nskip_footer : int, default 0\n    .. deprecated:: 0.19.0\n       Use the `skipfooter` parameter instead, as they are identical\nnrows : int, default None\n    Number of rows of file to read. Useful for reading pieces of large files\nna_values : scalar, str, list-like, or dict, default None\n    Additional strings to recognize as NA/NaN. If dict passed, specific\n    per-column NA values.  By default the following values are interpreted as\n    NaN: '\"\"\" + fill(\"', '\".join(sorted(_NA_VALUES)),\n                     70, subsequent_indent=\"    \") + \"\"\"'.\nkeep_default_na : bool, default True\n    If na_values are specified and keep_default_na is False the default NaN\n    values are overridden, otherwise they're appended to.\nna_filter : boolean, default True\n    Detect missing value markers (empty strings and the value of na_values). In\n    data without any NAs, passing na_filter=False can improve the performance\n    of reading a large file\nverbose : boolean, default False\n    Indicate number of NA values placed in non-numeric columns\nskip_blank_lines : boolean, default True\n    If True, skip over blank lines rather than interpreting as NaN values\nparse_dates : boolean or list of ints or names or list of lists or dict, \\\ndefault False\n\n    * boolean. If True -> try parsing the index.\n    * list of ints or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n      a single date column.\n    * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call result\n      'foo'\n\n    If a column or index contains an unparseable date, the entire column or\n    index will be returned unaltered as an object data type. For non-standard\n    datetime parsing, use ``pd.to_datetime`` after ``pd.read_csv``\n\n    Note: A fast-path exists for iso8601-formatted dates.\ninfer_datetime_format : boolean, default False\n    If True and `parse_dates` is enabled, pandas will attempt to infer the\n    format of the datetime strings in the columns, and if it can be inferred,\n    switch to a faster method of parsing them. In some cases this can increase\n    the parsing speed by 5-10x.\nkeep_date_col : boolean, default False\n    If True and `parse_dates` specifies combining multiple columns then\n    keep the original columns.\ndate_parser : function, default None\n    Function to use for converting a sequence of string columns to an array of\n    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. Pandas will try to call `date_parser` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by `parse_dates` into a single array\n    and pass that; and 3) call `date_parser` once for each row using one or\n    more strings (corresponding to the columns defined by `parse_dates`) as\n    arguments.\ndayfirst : boolean, default False\n    DD/MM format dates, international and European format\niterator : boolean, default False\n    Return TextFileReader object for iteration or getting chunks with\n    ``get_chunk()``.\nchunksize : int, default None\n    Return TextFileReader object for iteration.\n    See the `IO Tools docs\n    <http://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n    for more information on ``iterator`` and ``chunksize``.\ncompression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer', then use gzip,\n    bz2, zip or xz if filepath_or_buffer is a string ending in '.gz', '.bz2',\n    '.zip', or 'xz', respectively, and no decompression otherwise. If using\n    'zip', the ZIP file must contain only one data file to be read in.\n    Set to None for no decompression.\n\n    .. versionadded:: 0.18.1 support for 'zip' and 'xz' compression.\n\nthousands : str, default None\n    Thousands separator\ndecimal : str, default '.'\n    Character to recognize as decimal point (e.g. use ',' for European data).\nfloat_precision : string, default None\n    Specifies which converter the C engine should use for floating-point\n    values. The options are `None` for the ordinary converter,\n    `high` for the high-precision converter, and `round_trip` for the\n    round-trip converter.\nlineterminator : str (length 1), default None\n    Character to break file into lines. Only valid with C parser.\nquotechar : str (length 1), optional\n    The character used to denote the start and end of a quoted item. Quoted\n    items can include the delimiter and it will be ignored.\nquoting : int or csv.QUOTE_* instance, default 0\n    Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n    QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\ndoublequote : boolean, default ``True``\n   When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n   whether or not to interpret two consecutive quotechar elements INSIDE a\n   field as a single ``quotechar`` element.\nescapechar : str (length 1), default None\n    One-character string used to escape delimiter when quoting is QUOTE_NONE.\ncomment : str, default None\n    Indicates remainder of line should not be parsed. If found at the beginning\n    of a line, the line will be ignored altogether. This parameter must be a\n    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n    fully commented lines are ignored by the parameter `header` but not by\n    `skiprows`. For example, if comment='#', parsing '#empty\\\\na,b,c\\\\n1,2,3'\n    with `header=0` will result in 'a,b,c' being\n    treated as the header.\nencoding : str, default None\n    Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n    standard encodings\n    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_\ndialect : str or csv.Dialect instance, default None\n    If provided, this parameter will override values (default or not) for the\n    following parameters: `delimiter`, `doublequote`, `escapechar`,\n    `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n    override values, a ParserWarning will be issued. See csv.Dialect\n    documentation for more details.\ntupleize_cols : boolean, default False\n    Leave a list of tuples on columns as is (default is to convert to\n    a Multi Index on the columns)\nerror_bad_lines : boolean, default True\n    Lines with too many fields (e.g. a csv line with too many commas) will by\n    default cause an exception to be raised, and no DataFrame will be returned.\n    If False, then these \"bad lines\" will dropped from the DataFrame that is\n    returned.\nwarn_bad_lines : boolean, default True\n    If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n    \"bad line\" will be output.\nlow_memory : boolean, default True\n    Internally process the file in chunks, resulting in lower memory use\n    while parsing, but possibly mixed type inference.  To ensure no mixed\n    types either set False, or specify the type with the `dtype` parameter.\n    Note that the entire file is read into a single DataFrame regardless,\n    use the `chunksize` or `iterator` parameter to return the data in chunks.\n    (Only valid with C parser)\nbuffer_lines : int, default None\n    .. deprecated:: 0.19.0\n       This argument is not respected by the parser\ncompact_ints : boolean, default False\n    .. deprecated:: 0.19.0\n       Argument moved to ``pd.to_numeric``\n\n    If compact_ints is True, then for any column that is of integer dtype,\n    the parser will attempt to cast it as the smallest integer dtype possible,\n    either signed or unsigned depending on the specification from the\n    `use_unsigned` parameter.\nuse_unsigned : boolean, default False\n    .. deprecated:: 0.19.0\n       Argument moved to ``pd.to_numeric``\n\n    If integer columns are being compacted (i.e. `compact_ints=True`), specify\n    whether the column should be compacted to the smallest signed or unsigned\n    integer dtype.\nmemory_map : boolean, default False\n    If a filepath is provided for `filepath_or_buffer`, map the file object\n    directly onto memory and access the data directly from there. Using this\n    option can improve performance because there is no longer any I/O overhead.\n\nReturns\n-------\nresult : DataFrame or TextParser\n\"\"\"\n\n# engine is not used in read_fwf() so is factored out of the shared docstring\n_engine_doc = \"\"\"engine : {'c', 'python'}, optional\n    Parser engine to use. The C engine is faster while the python engine is\n    currently more feature-complete.\"\"\"\n\n_sep_doc = r\"\"\"sep : str, default {default}\n    Delimiter to use. If sep is None, the C engine cannot automatically detect\n    the separator, but the Python parsing engine can, meaning the latter will\n    be used automatically. In addition, separators longer than 1 character and\n    different from ``'\\s+'`` will be interpreted as regular expressions and\n    will also force the use of the Python parsing engine. Note that regex\n    delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``\ndelimiter : str, default ``None``\n    Alternative argument name for sep.\"\"\"\n\n_read_csv_doc = \"\"\"\nRead CSV (comma-separated) file into DataFrame\n\n%s\n\"\"\" % (_parser_params % (_sep_doc.format(default=\"','\"), _engine_doc))\n\n_read_table_doc = \"\"\"\nRead general delimited file into DataFrame\n\n%s\n\"\"\" % (_parser_params % (_sep_doc.format(default=\"\\\\t (tab-stop)\"),\n                         _engine_doc))\n\n_fwf_widths = \"\"\"\\\ncolspecs : list of pairs (int, int) or 'infer'. optional\n    A list of pairs (tuples) giving the extents of the fixed-width\n    fields of each line as half-open intervals (i.e.,  [from, to[ ).\n    String value 'infer' can be used to instruct the parser to try\n    detecting the column specifications from the first 100 rows of\n    the data which are not being skipped via skiprows (default='infer').\nwidths : list of ints. optional\n    A list of field widths which can be used instead of 'colspecs' if\n    the intervals are contiguous.\ndelimiter : str, default ``'\\t' + ' '``\n    Characters to consider as filler characters in the fixed-width file.\n    Can be used to specify the filler character of the fields\n    if it is not spaces (e.g., '~').\n\"\"\"\n\n_read_fwf_doc = \"\"\"\nRead a table of fixed-width formatted lines into DataFrame\n\n%s\n\"\"\" % (_parser_params % (_fwf_widths, ''))\n\n\ndef _validate_integer(name, val, min_val=0):\n    \"\"\"\n    Checks whether the 'name' parameter for parsing is either\n    an integer OR float that can SAFELY be cast to an integer\n    without losing accuracy. Raises a ValueError if that is\n    not the case.\n\n    Parameters\n    ----------\n    name : string\n        Parameter name (used for error reporting)\n    val : int or float\n        The value to check\n    min_val : int\n        Minimum allowed value (val < min_val will result in a ValueError)\n    \"\"\"\n    msg = \"'{name:s}' must be an integer >={min_val:d}\".format(name=name,\n                                                               min_val=min_val)\n\n    if val is not None:\n        if is_float(val):\n            if int(val) != val:\n                raise ValueError(msg)\n            val = int(val)\n        elif not (is_integer(val) and val >= min_val):\n            raise ValueError(msg)\n\n    return val\n\n\ndef _read(filepath_or_buffer, kwds):\n    \"\"\"Generic reader of line files.\"\"\"\n    encoding = kwds.get('encoding', None)\n    if encoding is not None:\n        encoding = re.sub('_', '-', encoding).lower()\n        kwds['encoding'] = encoding\n\n    compression = kwds.get('compression')\n    compression = _infer_compression(filepath_or_buffer, compression)\n    filepath_or_buffer, _, compression = get_filepath_or_buffer(\n        filepath_or_buffer, encoding, compression)\n    kwds['compression'] = compression\n\n    if kwds.get('date_parser', None) is not None:\n        if isinstance(kwds['parse_dates'], bool):\n            kwds['parse_dates'] = True\n\n    # Extract some of the arguments (pass chunksize on).\n    iterator = kwds.get('iterator', False)\n    chunksize = _validate_integer('chunksize', kwds.get('chunksize', None), 1)\n    nrows = _validate_integer('nrows', kwds.get('nrows', None))\n\n    # Create the parser.\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n\n    if chunksize or iterator:\n        return parser\n\n    try:\n        data = parser.read(nrows)\n    finally:\n        parser.close()\n    return data\n\n\n_parser_defaults = {\n    'delimiter': None,\n\n    'doublequote': True,\n    'escapechar': None,\n    'quotechar': '\"',\n    'quoting': csv.QUOTE_MINIMAL,\n    'skipinitialspace': False,\n    'lineterminator': None,\n\n    'header': 'infer',\n    'index_col': None,\n    'names': None,\n    'prefix': None,\n    'skiprows': None,\n    'na_values': None,\n    'true_values': None,\n    'false_values': None,\n    'converters': None,\n    'dtype': None,\n    'skipfooter': 0,\n\n    'keep_default_na': True,\n    'thousands': None,\n    'comment': None,\n    'decimal': b'.',\n\n    # 'engine': 'c',\n    'parse_dates': False,\n    'keep_date_col': False,\n    'dayfirst': False,\n    'date_parser': None,\n\n    'usecols': None,\n\n    'nrows': None,\n    # 'iterator': False,\n    'chunksize': None,\n    'verbose': False,\n    'encoding': None,\n    'squeeze': False,\n    'compression': None,\n    'mangle_dupe_cols': True,\n    'tupleize_cols': False,\n    'infer_datetime_format': False,\n    'skip_blank_lines': True\n}\n\n\n_c_parser_defaults = {\n    'delim_whitespace': False,\n    'as_recarray': False,\n    'na_filter': True,\n    'compact_ints': False,\n    'use_unsigned': False,\n    'low_memory': True,\n    'memory_map': False,\n    'buffer_lines': None,\n    'error_bad_lines': True,\n    'warn_bad_lines': True,\n    'float_precision': None\n}\n\n_fwf_defaults = {\n    'colspecs': 'infer',\n    'widths': None,\n}\n\n_c_unsupported = set(['skipfooter'])\n_python_unsupported = set([\n    'low_memory',\n    'buffer_lines',\n    'float_precision',\n])\n_deprecated_args = set([\n    'as_recarray',\n    'buffer_lines',\n    'compact_ints',\n    'use_unsigned',\n])\n\n\ndef _make_parser_function(name, sep=','):\n\n    default_sep = sep\n\n    def parser_f(filepath_or_buffer,\n                 sep=sep,\n                 delimiter=None,\n\n                 # Column and Index Locations and Names\n                 header='infer',\n                 names=None,\n                 index_col=None,\n                 usecols=None,\n                 squeeze=False,\n                 prefix=None,\n                 mangle_dupe_cols=True,\n\n                 # General Parsing Configuration\n                 dtype=None,\n                 engine=None,\n                 converters=None,\n                 true_values=None,\n                 false_values=None,\n                 skipinitialspace=False,\n                 skiprows=None,\n                 nrows=None,\n\n                 # NA and Missing Data Handling\n                 na_values=None,\n                 keep_default_na=True,\n                 na_filter=True,\n                 verbose=False,\n                 skip_blank_lines=True,\n\n                 # Datetime Handling\n                 parse_dates=False,\n                 infer_datetime_format=False,\n                 keep_date_col=False,\n                 date_parser=None,\n                 dayfirst=False,\n\n                 # Iteration\n                 iterator=False,\n                 chunksize=None,\n\n                 # Quoting, Compression, and File Format\n                 compression='infer',\n                 thousands=None,\n                 decimal=b'.',\n                 lineterminator=None,\n                 quotechar='\"',\n                 quoting=csv.QUOTE_MINIMAL,\n                 escapechar=None,\n                 comment=None,\n                 encoding=None,\n                 dialect=None,\n                 tupleize_cols=False,\n\n                 # Error Handling\n                 error_bad_lines=True,\n                 warn_bad_lines=True,\n\n                 skipfooter=0,\n                 skip_footer=0,  # deprecated\n\n                 # Internal\n                 doublequote=True,\n                 delim_whitespace=False,\n                 as_recarray=False,\n                 compact_ints=False,\n                 use_unsigned=False,\n                 low_memory=_c_parser_defaults['low_memory'],\n                 buffer_lines=None,\n                 memory_map=False,\n                 float_precision=None):\n\n        # Alias sep -> delimiter.\n        if delimiter is None:\n            delimiter = sep\n\n        if delim_whitespace and delimiter is not default_sep:\n            raise ValueError(\"Specified a delimiter with both sep and\"\n                             \" delim_whitespace=True; you can only\"\n                             \" specify one.\")\n\n        if engine is not None:\n            engine_specified = True\n        else:\n            engine = 'c'\n            engine_specified = False\n\n        if skip_footer != 0:\n            warnings.warn(\"The 'skip_footer' argument has \"\n                          \"been deprecated and will be removed \"\n                          \"in a future version. Please use the \"\n                          \"'skipfooter' argument instead.\",\n                          FutureWarning, stacklevel=2)\n\n        kwds = dict(delimiter=delimiter,\n                    engine=engine,\n                    dialect=dialect,\n                    compression=compression,\n                    engine_specified=engine_specified,\n\n                    doublequote=doublequote,\n                    escapechar=escapechar,\n                    quotechar=quotechar,\n                    quoting=quoting,\n                    skipinitialspace=skipinitialspace,\n                    lineterminator=lineterminator,\n\n                    header=header,\n                    index_col=index_col,\n                    names=names,\n                    prefix=prefix,\n                    skiprows=skiprows,\n                    na_values=na_values,\n                    true_values=true_values,\n                    false_values=false_values,\n                    keep_default_na=keep_default_na,\n                    thousands=thousands,\n                    comment=comment,\n                    decimal=decimal,\n\n                    parse_dates=parse_dates,\n                    keep_date_col=keep_date_col,\n                    dayfirst=dayfirst,\n                    date_parser=date_parser,\n\n                    nrows=nrows,\n                    iterator=iterator,\n                    chunksize=chunksize,\n                    skipfooter=skipfooter or skip_footer,\n                    converters=converters,\n                    dtype=dtype,\n                    usecols=usecols,\n                    verbose=verbose,\n                    encoding=encoding,\n                    squeeze=squeeze,\n                    memory_map=memory_map,\n                    float_precision=float_precision,\n\n                    na_filter=na_filter,\n                    compact_ints=compact_ints,\n                    use_unsigned=use_unsigned,\n                    delim_whitespace=delim_whitespace,\n                    as_recarray=as_recarray,\n                    warn_bad_lines=warn_bad_lines,\n                    error_bad_lines=error_bad_lines,\n                    low_memory=low_memory,\n                    buffer_lines=buffer_lines,\n                    mangle_dupe_cols=mangle_dupe_cols,\n                    tupleize_cols=tupleize_cols,\n                    infer_datetime_format=infer_datetime_format,\n                    skip_blank_lines=skip_blank_lines)\n\n        return _read(filepath_or_buffer, kwds)\n\n    parser_f.__name__ = name\n\n    return parser_f\n\n\nread_csv = _make_parser_function('read_csv', sep=',')\nread_csv = Appender(_read_csv_doc)(read_csv)\n\nread_table = _make_parser_function('read_table', sep='\\t')\nread_table = Appender(_read_table_doc)(read_table)\n\n\n@Appender(_read_fwf_doc)\ndef read_fwf(filepath_or_buffer, colspecs='infer', widths=None, **kwds):\n    # Check input arguments.\n    if colspecs is None and widths is None:\n        raise ValueError(\"Must specify either colspecs or widths\")\n    elif colspecs not in (None, 'infer') and widths is not None:\n        raise ValueError(\"You must specify only one of 'widths' and \"\n                         \"'colspecs'\")\n\n    # Compute 'colspecs' from 'widths', if specified.\n    if widths is not None:\n        colspecs, col = [], 0\n        for w in widths:\n            colspecs.append((col, col + w))\n            col += w\n\n    kwds['colspecs'] = colspecs\n    kwds['engine'] = 'python-fwf'\n    return _read(filepath_or_buffer, kwds)\n\n\nclass TextFileReader(BaseIterator):\n    \"\"\"\n\n    Passed dialect overrides any of the related parser options\n\n    \"\"\"\n\n    def __init__(self, f, engine=None, **kwds):\n\n        self.f = f\n\n        if engine is not None:\n            engine_specified = True\n        else:\n            engine = 'python'\n            engine_specified = False\n\n        self._engine_specified = kwds.get('engine_specified', engine_specified)\n\n        if kwds.get('dialect') is not None:\n            dialect = kwds['dialect']\n            if dialect in csv.list_dialects():\n                dialect = csv.get_dialect(dialect)\n\n            # Any valid dialect should have these attributes.\n            # If any are missing, we will raise automatically.\n            for param in ('delimiter', 'doublequote', 'escapechar',\n                          'skipinitialspace', 'quotechar', 'quoting'):\n                try:\n                    dialect_val = getattr(dialect, param)\n                except AttributeError:\n                    raise ValueError(\"Invalid dialect '{dialect}' provided\"\n                                     .format(dialect=kwds['dialect']))\n                provided = kwds.get(param, _parser_defaults[param])\n\n                # Messages for conflicting values between the dialect instance\n                # and the actual parameters provided.\n                conflict_msgs = []\n\n                if dialect_val != provided:\n                    conflict_msgs.append((\n                        \"Conflicting values for '{param}': '{val}' was \"\n                        \"provided, but the dialect specifies '{diaval}'. \"\n                        \"Using the dialect-specified value.\".format(\n                            param=param, val=provided, diaval=dialect_val)))\n\n                if conflict_msgs:\n                    warnings.warn('\\n\\n'.join(conflict_msgs), ParserWarning,\n                                  stacklevel=2)\n                kwds[param] = dialect_val\n\n        if kwds.get('header', 'infer') == 'infer':\n            kwds['header'] = 0 if kwds.get('names') is None else None\n\n        self.orig_options = kwds\n\n        # miscellanea\n        self.engine = engine\n        self._engine = None\n        self._currow = 0\n\n        options = self._get_options_with_defaults(engine)\n\n        self.chunksize = options.pop('chunksize', None)\n        self.nrows = options.pop('nrows', None)\n        self.squeeze = options.pop('squeeze', False)\n\n        # might mutate self.engine\n        self.engine = self._check_file_or_buffer(f, engine)\n        self.options, self.engine = self._clean_options(options, engine)\n\n        if 'has_index_names' in kwds:\n            self.options['has_index_names'] = kwds['has_index_names']\n\n        self._make_engine(self.engine)\n\n    def close(self):\n        self._engine.close()\n\n    def _get_options_with_defaults(self, engine):\n        kwds = self.orig_options\n\n        options = {}\n\n        for argname, default in compat.iteritems(_parser_defaults):\n            value = kwds.get(argname, default)\n\n            # see gh-12935\n            if argname == 'mangle_dupe_cols' and not value:\n                raise ValueError('Setting mangle_dupe_cols=False is '\n                                 'not supported yet')\n            else:\n                options[argname] = value\n\n        for argname, default in compat.iteritems(_c_parser_defaults):\n            if argname in kwds:\n                value = kwds[argname]\n\n                if engine != 'c' and value != default:\n                    if ('python' in engine and\n                            argname not in _python_unsupported):\n                        pass\n                    else:\n                        raise ValueError(\n                            'The %r option is not supported with the'\n                            ' %r engine' % (argname, engine))\n            else:\n                value = default\n            options[argname] = value\n\n        if engine == 'python-fwf':\n            for argname, default in compat.iteritems(_fwf_defaults):\n                options[argname] = kwds.get(argname, default)\n\n        return options\n\n    def _check_file_or_buffer(self, f, engine):\n        # see gh-16530\n        if is_file_like(f):\n            next_attr = \"__next__\" if PY3 else \"next\"\n\n            # The C engine doesn't need the file-like to have the \"next\" or\n            # \"__next__\" attribute. However, the Python engine explicitly calls\n            # \"next(...)\" when iterating through such an object, meaning it\n            # needs to have that attribute (\"next\" for Python 2.x, \"__next__\"\n            # for Python 3.x)\n            if engine != \"c\" and not hasattr(f, next_attr):\n                msg = (\"The 'python' engine cannot iterate \"\n                       \"through this file buffer.\")\n                raise ValueError(msg)\n\n        return engine\n\n    def _clean_options(self, options, engine):\n        result = options.copy()\n\n        engine_specified = self._engine_specified\n        fallback_reason = None\n\n        sep = options['delimiter']\n        delim_whitespace = options['delim_whitespace']\n\n        # C engine not supported yet\n        if engine == 'c':\n            if options['skipfooter'] > 0:\n                fallback_reason = \"the 'c' engine does not support\"\\\n                                  \" skipfooter\"\n                engine = 'python'\n\n        encoding = sys.getfilesystemencoding() or 'utf-8'\n        if sep is None and not delim_whitespace:\n            if engine == 'c':\n                fallback_reason = \"the 'c' engine does not support\"\\\n                                  \" sep=None with delim_whitespace=False\"\n                engine = 'python'\n        elif sep is not None and len(sep) > 1:\n            if engine == 'c' and sep == '\\s+':\n                result['delim_whitespace'] = True\n                del result['delimiter']\n            elif engine not in ('python', 'python-fwf'):\n                # wait until regex engine integrated\n                fallback_reason = \"the 'c' engine does not support\"\\\n                                  \" regex separators (separators > 1 char and\"\\\n                                  \" different from '\\s+' are\"\\\n                                  \" interpreted as regex)\"\n                engine = 'python'\n        elif delim_whitespace:\n            if 'python' in engine:\n                result['delimiter'] = '\\s+'\n        elif sep is not None:\n            encodeable = True\n            try:\n                if len(sep.encode(encoding)) > 1:\n                    encodeable = False\n            except UnicodeDecodeError:\n                encodeable = False\n            if not encodeable and engine not in ('python', 'python-fwf'):\n                fallback_reason = \"the separator encoded in {encoding}\" \\\n                                  \" is > 1 char long, and the 'c' engine\" \\\n                                  \" does not support such separators\".format(\n                                      encoding=encoding)\n                engine = 'python'\n\n        quotechar = options['quotechar']\n        if (quotechar is not None and\n                isinstance(quotechar, (str, compat.text_type, bytes))):\n            if (len(quotechar) == 1 and ord(quotechar) > 127 and\n                    engine not in ('python', 'python-fwf')):\n                fallback_reason = (\"ord(quotechar) > 127, meaning the \"\n                                   \"quotechar is larger than one byte, \"\n                                   \"and the 'c' engine does not support \"\n                                   \"such quotechars\")\n                engine = 'python'\n\n        if fallback_reason and engine_specified:\n            raise ValueError(fallback_reason)\n\n        if engine == 'c':\n            for arg in _c_unsupported:\n                del result[arg]\n\n        if 'python' in engine:\n            for arg in _python_unsupported:\n                if fallback_reason and result[arg] != _c_parser_defaults[arg]:\n                    msg = (\"Falling back to the 'python' engine because\"\n                           \" {reason}, but this causes {option!r} to be\"\n                           \" ignored as it is not supported by the 'python'\"\n                           \" engine.\").format(reason=fallback_reason,\n                                              option=arg)\n                    raise ValueError(msg)\n                del result[arg]\n\n        if fallback_reason:\n            warnings.warn((\"Falling back to the 'python' engine because\"\n                           \" {0}; you can avoid this warning by specifying\"\n                           \" engine='python'.\").format(fallback_reason),\n                          ParserWarning, stacklevel=5)\n\n        index_col = options['index_col']\n        names = options['names']\n        converters = options['converters']\n        na_values = options['na_values']\n        skiprows = options['skiprows']\n\n        # really delete this one\n        keep_default_na = result.pop('keep_default_na')\n\n        _validate_header_arg(options['header'])\n\n        depr_warning = ''\n\n        for arg in _deprecated_args:\n            parser_default = _c_parser_defaults[arg]\n            msg = (\"The '{arg}' argument has been deprecated \"\n                   \"and will be removed in a future version.\"\n                   .format(arg=arg))\n\n            if arg == 'as_recarray':\n                msg += ' Please call pd.to_csv(...).to_records() instead.'\n\n            if result.get(arg, parser_default) != parser_default:\n                depr_warning += msg + '\\n\\n'\n\n        if depr_warning != '':\n            warnings.warn(depr_warning, FutureWarning, stacklevel=2)\n\n        if index_col is True:\n            raise ValueError(\"The value of index_col couldn't be 'True'\")\n        if _is_index_col(index_col):\n            if not isinstance(index_col, (list, tuple, np.ndarray)):\n                index_col = [index_col]\n        result['index_col'] = index_col\n\n        names = list(names) if names is not None else names\n\n        # type conversion-related\n        if converters is not None:\n            if not isinstance(converters, dict):\n                raise TypeError('Type converters must be a dict or'\n                                ' subclass, input was '\n                                'a {0!r}'.format(type(converters).__name__))\n        else:\n            converters = {}\n\n        # Converting values to NA\n        na_values, na_fvalues = _clean_na_values(na_values, keep_default_na)\n\n        # handle skiprows; this is internally handled by the\n        # c-engine, so only need for python parsers\n        if engine != 'c':\n            if is_integer(skiprows):\n                skiprows = lrange(skiprows)\n            if skiprows is None:\n                skiprows = set()\n            elif not callable(skiprows):\n                skiprows = set(skiprows)\n\n        # put stuff back\n        result['names'] = names\n        result['converters'] = converters\n        result['na_values'] = na_values\n        result['na_fvalues'] = na_fvalues\n        result['skiprows'] = skiprows\n\n        return result, engine\n\n    def __next__(self):\n        try:\n            return self.get_chunk()\n        except StopIteration:\n            self.close()\n            raise\n\n    def _make_engine(self, engine='c'):\n        if engine == 'c':\n            self._engine = CParserWrapper(self.f, **self.options)\n        else:\n            if engine == 'python':\n                klass = PythonParser\n            elif engine == 'python-fwf':\n                klass = FixedWidthFieldParser\n            else:\n                raise ValueError('Unknown engine: {engine} (valid options are'\n                                 ' \"c\", \"python\", or' ' \"python-fwf\")'.format(\n                                     engine=engine))\n            self._engine = klass(self.f, **self.options)\n\n    def _failover_to_python(self):\n        raise AbstractMethodError(self)\n\n    def read(self, nrows=None):\n        if nrows is not None:\n            if self.options.get('skipfooter'):\n                raise ValueError('skipfooter not supported for iteration')\n\n        ret = self._engine.read(nrows)\n\n        if self.options.get('as_recarray'):\n            return ret\n\n        # May alter columns / col_dict\n        index, columns, col_dict = self._create_index(ret)\n\n        if index is None:\n            if col_dict:\n                # Any column is actually fine:\n                new_rows = len(compat.next(compat.itervalues(col_dict)))\n                index = RangeIndex(self._currow, self._currow + new_rows)\n            else:\n                new_rows = 0\n        else:\n            new_rows = len(index)\n\n        df = DataFrame(col_dict, columns=columns, index=index)\n\n        self._currow += new_rows\n\n        if self.squeeze and len(df.columns) == 1:\n            return df[df.columns[0]].copy()\n        return df\n\n    def _create_index(self, ret):\n        index, columns, col_dict = ret\n        return index, columns, col_dict\n\n    def get_chunk(self, size=None):\n        if size is None:\n            size = self.chunksize\n        if self.nrows is not None:\n            if self._currow >= self.nrows:\n                raise StopIteration\n            size = min(size, self.nrows - self._currow)\n        return self.read(nrows=size)\n\n\ndef _is_index_col(col):\n    return col is not None and col is not False\n\n\ndef _evaluate_usecols(usecols, names):\n    \"\"\"\n    Check whether or not the 'usecols' parameter\n    is a callable.  If so, enumerates the 'names'\n    parameter and returns a set of indices for\n    each entry in 'names' that evaluates to True.\n    If not a callable, returns 'usecols'.\n    \"\"\"\n    if callable(usecols):\n        return set([i for i, name in enumerate(names)\n                    if usecols(name)])\n    return usecols\n\n\ndef _validate_skipfooter_arg(skipfooter):\n    \"\"\"\n    Validate the 'skipfooter' parameter.\n\n    Checks whether 'skipfooter' is a non-negative integer.\n    Raises a ValueError if that is not the case.\n\n    Parameters\n    ----------\n    skipfooter : non-negative integer\n        The number of rows to skip at the end of the file.\n\n    Returns\n    -------\n    validated_skipfooter : non-negative integer\n        The original input if the validation succeeds.\n\n    Raises\n    ------\n    ValueError : 'skipfooter' was not a non-negative integer.\n    \"\"\"\n\n    if not is_integer(skipfooter):\n        raise ValueError(\"skipfooter must be an integer\")\n\n    if skipfooter < 0:\n        raise ValueError(\"skipfooter cannot be negative\")\n\n    return skipfooter\n\n\ndef _validate_usecols_arg(usecols):\n    \"\"\"\n    Validate the 'usecols' parameter.\n\n    Checks whether or not the 'usecols' parameter contains all integers\n    (column selection by index), strings (column by name) or is a callable.\n    Raises a ValueError if that is not the case.\n\n    Parameters\n    ----------\n    usecols : array-like, callable, or None\n        List of columns to use when parsing or a callable that can be used\n        to filter a list of table columns.\n\n    Returns\n    -------\n    usecols_tuple : tuple\n        A tuple of (verified_usecols, usecols_dtype).\n\n        'verified_usecols' is either a set if an array-like is passed in or\n        'usecols' if a callable or None is passed in.\n\n        'usecols_dtype` is the inferred dtype of 'usecols' if an array-like\n        is passed in or None if a callable or None is passed in.\n    \"\"\"\n    msg = (\"'usecols' must either be all strings, all unicode, \"\n           \"all integers or a callable\")\n\n    if usecols is not None:\n        if callable(usecols):\n            return usecols, None\n        usecols_dtype = lib.infer_dtype(usecols)\n        if usecols_dtype not in ('empty', 'integer',\n                                 'string', 'unicode'):\n            raise ValueError(msg)\n\n        return set(usecols), usecols_dtype\n    return usecols, None\n\n\ndef _validate_parse_dates_arg(parse_dates):\n    \"\"\"\n    Check whether or not the 'parse_dates' parameter\n    is a non-boolean scalar. Raises a ValueError if\n    that is the case.\n    \"\"\"\n    msg = (\"Only booleans, lists, and \"\n           \"dictionaries are accepted \"\n           \"for the 'parse_dates' parameter\")\n\n    if parse_dates is not None:\n        if is_scalar(parse_dates):\n            if not lib.is_bool(parse_dates):\n                raise TypeError(msg)\n\n        elif not isinstance(parse_dates, (list, dict)):\n            raise TypeError(msg)\n\n    return parse_dates\n\n\nclass ParserBase(object):\n\n    def __init__(self, kwds):\n        self.names = kwds.get('names')\n        self.orig_names = None\n        self.prefix = kwds.pop('prefix', None)\n\n        self.index_col = kwds.get('index_col', None)\n        self.index_names = None\n        self.col_names = None\n\n        self.parse_dates = _validate_parse_dates_arg(\n            kwds.pop('parse_dates', False))\n        self.date_parser = kwds.pop('date_parser', None)\n        self.dayfirst = kwds.pop('dayfirst', False)\n        self.keep_date_col = kwds.pop('keep_date_col', False)\n\n        self.na_values = kwds.get('na_values')\n        self.na_fvalues = kwds.get('na_fvalues')\n        self.true_values = kwds.get('true_values')\n        self.false_values = kwds.get('false_values')\n        self.as_recarray = kwds.get('as_recarray', False)\n        self.tupleize_cols = kwds.get('tupleize_cols', False)\n        self.mangle_dupe_cols = kwds.get('mangle_dupe_cols', True)\n        self.infer_datetime_format = kwds.pop('infer_datetime_format', False)\n\n        self._date_conv = _make_date_converter(\n            date_parser=self.date_parser,\n            dayfirst=self.dayfirst,\n            infer_datetime_format=self.infer_datetime_format\n        )\n\n        # validate header options for mi\n        self.header = kwds.get('header')\n        if isinstance(self.header, (list, tuple, np.ndarray)):\n            if not all(map(is_integer, self.header)):\n                raise ValueError(\"header must be integer or list of integers\")\n            if kwds.get('as_recarray'):\n                raise ValueError(\"cannot specify as_recarray when \"\n                                 \"specifying a multi-index header\")\n            if kwds.get('usecols'):\n                raise ValueError(\"cannot specify usecols when \"\n                                 \"specifying a multi-index header\")\n            if kwds.get('names'):\n                raise ValueError(\"cannot specify names when \"\n                                 \"specifying a multi-index header\")\n\n            # validate index_col that only contains integers\n            if self.index_col is not None:\n                is_sequence = isinstance(self.index_col, (list, tuple,\n                                                          np.ndarray))\n                if not (is_sequence and\n                        all(map(is_integer, self.index_col)) or\n                        is_integer(self.index_col)):\n                    raise ValueError(\"index_col must only contain row numbers \"\n                                     \"when specifying a multi-index header\")\n\n        # GH 16338\n        elif self.header is not None and not is_integer(self.header):\n            raise ValueError(\"header must be integer or list of integers\")\n\n        self._name_processed = False\n\n        self._first_chunk = True\n\n        # GH 13932\n        # keep references to file handles opened by the parser itself\n        self.handles = []\n\n    def close(self):\n        for f in self.handles:\n            f.close()\n\n    @property\n    def _has_complex_date_col(self):\n        return (isinstance(self.parse_dates, dict) or\n                (isinstance(self.parse_dates, list) and\n                 len(self.parse_dates) > 0 and\n                 isinstance(self.parse_dates[0], list)))\n\n    def _should_parse_dates(self, i):\n        if isinstance(self.parse_dates, bool):\n            return self.parse_dates\n        else:\n            if self.index_names is not None:\n                name = self.index_names[i]\n            else:\n                name = None\n            j = self.index_col[i]\n\n            if is_scalar(self.parse_dates):\n                return ((j == self.parse_dates) or\n                        (name is not None and name == self.parse_dates))\n            else:\n                return ((j in self.parse_dates) or\n                        (name is not None and name in self.parse_dates))\n\n    def _extract_multi_indexer_columns(self, header, index_names, col_names,\n                                       passed_names=False):\n        \"\"\" extract and return the names, index_names, col_names\n            header is a list-of-lists returned from the parsers \"\"\"\n        if len(header) < 2:\n            return header[0], index_names, col_names, passed_names\n\n        # the names are the tuples of the header that are not the index cols\n        # 0 is the name of the index, assuming index_col is a list of column\n        # numbers\n        ic = self.index_col\n        if ic is None:\n            ic = []\n\n        if not isinstance(ic, (list, tuple, np.ndarray)):\n            ic = [ic]\n        sic = set(ic)\n\n        # clean the index_names\n        index_names = header.pop(-1)\n        index_names, names, index_col = _clean_index_names(index_names,\n                                                           self.index_col)\n\n        # extract the columns\n        field_count = len(header[0])\n\n        def extract(r):\n            return tuple([r[i] for i in range(field_count) if i not in sic])\n\n        columns = lzip(*[extract(r) for r in header])\n        names = ic + columns\n\n        def tostr(x):\n            return str(x) if not isinstance(x, compat.string_types) else x\n\n        # if we find 'Unnamed' all of a single level, then our header was too\n        # long\n        for n in range(len(columns[0])):\n            if all(['Unnamed' in tostr(c[n]) for c in columns]):\n                raise ParserError(\n                    \"Passed header=[%s] are too many rows for this \"\n                    \"multi_index of columns\"\n                    % ','.join([str(x) for x in self.header])\n                )\n\n        # clean the column names (if we have an index_col)\n        if len(ic):\n            col_names = [r[0] if len(r[0]) and 'Unnamed' not in r[0] else None\n                         for r in header]\n        else:\n            col_names = [None] * len(header)\n\n        passed_names = True\n\n        return names, index_names, col_names, passed_names\n\n    def _maybe_dedup_names(self, names):\n        # see gh-7160 and gh-9424: this helps to provide\n        # immediate alleviation of the duplicate names\n        # issue and appears to be satisfactory to users,\n        # but ultimately, not needing to butcher the names\n        # would be nice!\n        if self.mangle_dupe_cols:\n            names = list(names)  # so we can index\n            counts = {}\n\n            for i, col in enumerate(names):\n                cur_count = counts.get(col, 0)\n\n                if cur_count > 0:\n                    names[i] = '%s.%d' % (col, cur_count)\n\n                counts[col] = cur_count + 1\n\n        return names\n\n    def _maybe_make_multi_index_columns(self, columns, col_names=None):\n        # possibly create a column mi here\n        if (not self.tupleize_cols and len(columns) and\n                not isinstance(columns, MultiIndex) and\n                all([isinstance(c, tuple) for c in columns])):\n            columns = MultiIndex.from_tuples(columns, names=col_names)\n        return columns\n\n    def _make_index(self, data, alldata, columns, indexnamerow=False):\n        if not _is_index_col(self.index_col) or not self.index_col:\n            index = None\n\n        elif not self._has_complex_date_col:\n            index = self._get_simple_index(alldata, columns)\n            index = self._agg_index(index)\n\n        elif self._has_complex_date_col:\n            if not self._name_processed:\n                (self.index_names, _,\n                 self.index_col) = _clean_index_names(list(columns),\n                                                      self.index_col)\n                self._name_processed = True\n            index = self._get_complex_date_index(data, columns)\n            index = self._agg_index(index, try_parse_dates=False)\n\n        # add names for the index\n        if indexnamerow:\n            coffset = len(indexnamerow) - len(columns)\n            index = index.set_names(indexnamerow[:coffset])\n\n        # maybe create a mi on the columns\n        columns = self._maybe_make_multi_index_columns(columns, self.col_names)\n\n        return index, columns\n\n    _implicit_index = False\n\n    def _get_simple_index(self, data, columns):\n        def ix(col):\n            if not isinstance(col, compat.string_types):\n                return col\n            raise ValueError('Index %s invalid' % col)\n        index = None\n\n        to_remove = []\n        index = []\n        for idx in self.index_col:\n            i = ix(idx)\n            to_remove.append(i)\n            index.append(data[i])\n\n        # remove index items from content and columns, don't pop in\n        # loop\n        for i in reversed(sorted(to_remove)):\n            data.pop(i)\n            if not self._implicit_index:\n                columns.pop(i)\n\n        return index\n\n    def _get_complex_date_index(self, data, col_names):\n        def _get_name(icol):\n            if isinstance(icol, compat.string_types):\n                return icol\n\n            if col_names is None:\n                raise ValueError(('Must supply column order to use %s as '\n                                  'index') % str(icol))\n\n            for i, c in enumerate(col_names):\n                if i == icol:\n                    return c\n\n        index = None\n\n        to_remove = []\n        index = []\n        for idx in self.index_col:\n            name = _get_name(idx)\n            to_remove.append(name)\n            index.append(data[name])\n\n        # remove index items from content and columns, don't pop in\n        # loop\n        for c in reversed(sorted(to_remove)):\n            data.pop(c)\n            col_names.remove(c)\n\n        return index\n\n    def _agg_index(self, index, try_parse_dates=True):\n        arrays = []\n\n        for i, arr in enumerate(index):\n\n            if (try_parse_dates and self._should_parse_dates(i)):\n                arr = self._date_conv(arr)\n\n            col_na_values = self.na_values\n            col_na_fvalues = self.na_fvalues\n\n            if isinstance(self.na_values, dict):\n                col_name = self.index_names[i]\n                if col_name is not None:\n                    col_na_values, col_na_fvalues = _get_na_values(\n                        col_name, self.na_values, self.na_fvalues)\n\n            arr, _ = self._infer_types(arr, col_na_values | col_na_fvalues)\n            arrays.append(arr)\n\n        index = MultiIndex.from_arrays(arrays, names=self.index_names)\n\n        return index\n\n    def _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose=False,\n                             converters=None, dtypes=None):\n        result = {}\n        for c, values in compat.iteritems(dct):\n            conv_f = None if converters is None else converters.get(c, None)\n            if isinstance(dtypes, dict):\n                cast_type = dtypes.get(c, None)\n            else:\n                # single dtype or None\n                cast_type = dtypes\n\n            if self.na_filter:\n                col_na_values, col_na_fvalues = _get_na_values(\n                    c, na_values, na_fvalues)\n            else:\n                col_na_values, col_na_fvalues = set(), set()\n\n            if conv_f is not None:\n                # conv_f applied to data before inference\n                if cast_type is not None:\n                    warnings.warn((\"Both a converter and dtype were specified \"\n                                   \"for column {0} - only the converter will \"\n                                   \"be used\").format(c), ParserWarning,\n                                  stacklevel=7)\n\n                try:\n                    values = lib.map_infer(values, conv_f)\n                except ValueError:\n                    mask = algorithms.isin(\n                        values, list(na_values)).view(np.uint8)\n                    values = lib.map_infer_mask(values, conv_f, mask)\n\n                cvals, na_count = self._infer_types(\n                    values, set(col_na_values) | col_na_fvalues,\n                    try_num_bool=False)\n            else:\n                # skip inference if specified dtype is object\n                try_num_bool = not (cast_type and is_string_dtype(cast_type))\n\n                # general type inference and conversion\n                cvals, na_count = self._infer_types(\n                    values, set(col_na_values) | col_na_fvalues,\n                    try_num_bool)\n\n                # type specificed in dtype param\n                if cast_type and not is_dtype_equal(cvals, cast_type):\n                    cvals = self._cast_types(cvals, cast_type, c)\n\n            if issubclass(cvals.dtype.type, np.integer) and self.compact_ints:\n                cvals = lib.downcast_int64(\n                    cvals, parsers.na_values,\n                    self.use_unsigned)\n\n            result[c] = cvals\n            if verbose and na_count:\n                print('Filled %d NA values in column %s' % (na_count, str(c)))\n        return result\n\n    def _infer_types(self, values, na_values, try_num_bool=True):\n        \"\"\"\n        Infer types of values, possibly casting\n\n        Parameters\n        ----------\n        values : ndarray\n        na_values : set\n        try_num_bool : bool, default try\n           try to cast values to numeric (first preference) or boolean\n\n        Returns:\n        --------\n        converted : ndarray\n        na_count : int\n        \"\"\"\n\n        na_count = 0\n        if issubclass(values.dtype.type, (np.number, np.bool_)):\n            mask = algorithms.isin(values, list(na_values))\n            na_count = mask.sum()\n            if na_count > 0:\n                if is_integer_dtype(values):\n                    values = values.astype(np.float64)\n                np.putmask(values, mask, np.nan)\n            return values, na_count\n\n        if try_num_bool:\n            try:\n                result = lib.maybe_convert_numeric(values, na_values, False)\n                na_count = isnull(result).sum()\n            except Exception:\n                result = values\n                if values.dtype == np.object_:\n                    na_count = lib.sanitize_objects(result, na_values, False)\n        else:\n            result = values\n            if values.dtype == np.object_:\n                na_count = lib.sanitize_objects(values, na_values, False)\n\n        if result.dtype == np.object_ and try_num_bool:\n            result = lib.maybe_convert_bool(values,\n                                            true_values=self.true_values,\n                                            false_values=self.false_values)\n\n        return result, na_count\n\n    def _cast_types(self, values, cast_type, column):\n        \"\"\"\n        Cast values to specified type\n\n        Parameters\n        ----------\n        values : ndarray\n        cast_type : string or np.dtype\n           dtype to cast values to\n        column : string\n            column name - used only for error reporting\n\n        Returns\n        -------\n        converted : ndarray\n        \"\"\"\n\n        if is_categorical_dtype(cast_type):\n            # XXX this is for consistency with\n            # c-parser which parses all categories\n            # as strings\n            if not is_object_dtype(values):\n                values = astype_nansafe(values, str)\n            values = Categorical(values)\n        else:\n            try:\n                values = astype_nansafe(values, cast_type, copy=True)\n            except ValueError:\n                raise ValueError(\"Unable to convert column %s to \"\n                                 \"type %s\" % (column, cast_type))\n        return values\n\n    def _do_date_conversions(self, names, data):\n        # returns data, columns\n\n        if self.parse_dates is not None:\n            data, names = _process_date_conversion(\n                data, self._date_conv, self.parse_dates, self.index_col,\n                self.index_names, names, keep_date_col=self.keep_date_col)\n\n        return names, data\n\n\nclass CParserWrapper(ParserBase):\n    \"\"\"\n\n    \"\"\"\n\n    def __init__(self, src, **kwds):\n        self.kwds = kwds\n        kwds = kwds.copy()\n\n        ParserBase.__init__(self, kwds)\n\n        if 'utf-16' in (kwds.get('encoding') or ''):\n            if isinstance(src, compat.string_types):\n                src = open(src, 'rb')\n                self.handles.append(src)\n            src = UTF8Recoder(src, kwds['encoding'])\n            kwds['encoding'] = 'utf-8'\n\n        # #2442\n        kwds['allow_leading_cols'] = self.index_col is not False\n\n        self._reader = parsers.TextReader(src, **kwds)\n\n        # XXX\n        self.usecols, self.usecols_dtype = _validate_usecols_arg(\n            self._reader.usecols)\n\n        passed_names = self.names is None\n\n        if self._reader.header is None:\n            self.names = None\n        else:\n            if len(self._reader.header) > 1:\n                # we have a multi index in the columns\n                self.names, self.index_names, self.col_names, passed_names = (\n                    self._extract_multi_indexer_columns(\n                        self._reader.header, self.index_names, self.col_names,\n                        passed_names\n                    )\n                )\n            else:\n                self.names = list(self._reader.header[0])\n\n        if self.names is None:\n            if self.prefix:\n                self.names = ['%s%d' % (self.prefix, i)\n                              for i in range(self._reader.table_width)]\n            else:\n                self.names = lrange(self._reader.table_width)\n\n        # gh-9755\n        #\n        # need to set orig_names here first\n        # so that proper indexing can be done\n        # with _set_noconvert_columns\n        #\n        # once names has been filtered, we will\n        # then set orig_names again to names\n        self.orig_names = self.names[:]\n\n        if self.usecols:\n            usecols = _evaluate_usecols(self.usecols, self.orig_names)\n\n            # GH 14671\n            if (self.usecols_dtype == 'string' and\n                    not set(usecols).issubset(self.orig_names)):\n                raise ValueError(\"Usecols do not match names.\")\n\n            if len(self.names) > len(usecols):\n                self.names = [n for i, n in enumerate(self.names)\n                              if (i in usecols or n in usecols)]\n\n            if len(self.names) < len(usecols):\n                raise ValueError(\"Usecols do not match names.\")\n\n        self._set_noconvert_columns()\n\n        self.orig_names = self.names\n\n        if not self._has_complex_date_col:\n            if (self._reader.leading_cols == 0 and\n                    _is_index_col(self.index_col)):\n\n                self._name_processed = True\n                (index_names, self.names,\n                 self.index_col) = _clean_index_names(self.names,\n                                                      self.index_col)\n\n                if self.index_names is None:\n                    self.index_names = index_names\n\n            if self._reader.header is None and not passed_names:\n                self.index_names = [None] * len(self.index_names)\n\n        self._implicit_index = self._reader.leading_cols > 0\n\n    def close(self):\n        for f in self.handles:\n            f.close()\n\n        # close additional handles opened by C parser (for compression)\n        try:\n            self._reader.close()\n        except:\n            pass\n\n    def _set_noconvert_columns(self):\n        \"\"\"\n        Set the columns that should not undergo dtype conversions.\n\n        Currently, any column that is involved with date parsing will not\n        undergo such conversions.\n        \"\"\"\n        names = self.orig_names\n        if self.usecols_dtype == 'integer':\n            # A set of integers will be converted to a list in\n            # the correct order every single time.\n            usecols = list(self.usecols)\n        elif (callable(self.usecols) or\n                self.usecols_dtype not in ('empty', None)):\n            # The names attribute should have the correct columns\n            # in the proper order for indexing with parse_dates.\n            usecols = self.names[:]\n        else:\n            # Usecols is empty.\n            usecols = None\n\n        def _set(x):\n            if usecols is not None and is_integer(x):\n                x = usecols[x]\n\n            if not is_integer(x):\n                x = names.index(x)\n\n            self._reader.set_noconvert(x)\n\n        if isinstance(self.parse_dates, list):\n            for val in self.parse_dates:\n                if isinstance(val, list):\n                    for k in val:\n                        _set(k)\n                else:\n                    _set(val)\n\n        elif isinstance(self.parse_dates, dict):\n            for val in self.parse_dates.values():\n                if isinstance(val, list):\n                    for k in val:\n                        _set(k)\n                else:\n                    _set(val)\n\n        elif self.parse_dates:\n            if isinstance(self.index_col, list):\n                for k in self.index_col:\n                    _set(k)\n            elif self.index_col is not None:\n                _set(self.index_col)\n\n    def set_error_bad_lines(self, status):\n        self._reader.set_error_bad_lines(int(status))\n\n    def read(self, nrows=None):\n        try:\n            data = self._reader.read(nrows)\n        except StopIteration:\n            if self._first_chunk:\n                self._first_chunk = False\n                names = self._maybe_dedup_names(self.orig_names)\n                index, columns, col_dict = _get_empty_meta(\n                    names, self.index_col, self.index_names,\n                    dtype=self.kwds.get('dtype'))\n                columns = self._maybe_make_multi_index_columns(\n                    columns, self.col_names)\n\n                if self.usecols is not None:\n                    columns = self._filter_usecols(columns)\n\n                col_dict = dict(filter(lambda item: item[0] in columns,\n                                       col_dict.items()))\n\n                return index, columns, col_dict\n\n            else:\n                raise\n\n        # Done with first read, next time raise StopIteration\n        self._first_chunk = False\n\n        if self.as_recarray:\n            # what to do if there are leading columns?\n            return data\n\n        names = self.names\n\n        if self._reader.leading_cols:\n            if self._has_complex_date_col:\n                raise NotImplementedError('file structure not yet supported')\n\n            # implicit index, no index names\n            arrays = []\n\n            for i in range(self._reader.leading_cols):\n                if self.index_col is None:\n                    values = data.pop(i)\n                else:\n                    values = data.pop(self.index_col[i])\n\n                values = self._maybe_parse_dates(values, i,\n                                                 try_parse_dates=True)\n                arrays.append(values)\n\n            index = MultiIndex.from_arrays(arrays)\n\n            if self.usecols is not None:\n                names = self._filter_usecols(names)\n\n            names = self._maybe_dedup_names(names)\n\n            # rename dict keys\n            data = sorted(data.items())\n            data = dict((k, v) for k, (i, v) in zip(names, data))\n\n            names, data = self._do_date_conversions(names, data)\n\n        else:\n            # rename dict keys\n            data = sorted(data.items())\n\n            # ugh, mutation\n            names = list(self.orig_names)\n            names = self._maybe_dedup_names(names)\n\n            if self.usecols is not None:\n                names = self._filter_usecols(names)\n\n            # columns as list\n            alldata = [x[1] for x in data]\n\n            data = dict((k, v) for k, (i, v) in zip(names, data))\n\n            names, data = self._do_date_conversions(names, data)\n            index, names = self._make_index(data, alldata, names)\n\n        # maybe create a mi on the columns\n        names = self._maybe_make_multi_index_columns(names, self.col_names)\n\n        return index, names, data\n\n    def _filter_usecols(self, names):\n        # hackish\n        usecols = _evaluate_usecols(self.usecols, names)\n        if usecols is not None and len(names) != len(usecols):\n            names = [name for i, name in enumerate(names)\n                     if i in usecols or name in usecols]\n        return names\n\n    def _get_index_names(self):\n        names = list(self._reader.header[0])\n        idx_names = None\n\n        if self._reader.leading_cols == 0 and self.index_col is not None:\n            (idx_names, names,\n             self.index_col) = _clean_index_names(names, self.index_col)\n\n        return names, idx_names\n\n    def _maybe_parse_dates(self, values, index, try_parse_dates=True):\n        if try_parse_dates and self._should_parse_dates(index):\n            values = self._date_conv(values)\n        return values\n\n\ndef TextParser(*args, **kwds):\n    \"\"\"\n    Converts lists of lists/tuples into DataFrames with proper type inference\n    and optional (e.g. string to datetime) conversion. Also enables iterating\n    lazily over chunks of large files\n\n    Parameters\n    ----------\n    data : file-like object or list\n    delimiter : separator character to use\n    dialect : str or csv.Dialect instance, default None\n        Ignored if delimiter is longer than 1 character\n    names : sequence, default\n    header : int, default 0\n        Row to use to parse column labels. Defaults to the first row. Prior\n        rows will be discarded\n    index_col : int or list, default None\n        Column or columns to use as the (possibly hierarchical) index\n    has_index_names: boolean, default False\n        True if the cols defined in index_col have an index name and are\n        not in the header\n    na_values : scalar, str, list-like, or dict, default None\n        Additional strings to recognize as NA/NaN.\n    keep_default_na : bool, default True\n    thousands : str, default None\n        Thousands separator\n    comment : str, default None\n        Comment out remainder of line\n    parse_dates : boolean, default False\n    keep_date_col : boolean, default False\n    date_parser : function, default None\n    skiprows : list of integers\n        Row numbers to skip\n    skipfooter : int\n        Number of line at bottom of file to skip\n    converters : dict, default None\n        Dict of functions for converting values in certain columns. Keys can\n        either be integers or column labels, values are functions that take one\n        input argument, the cell (not column) content, and return the\n        transformed content.\n    encoding : string, default None\n        Encoding to use for UTF when reading/writing (ex. 'utf-8')\n    squeeze : boolean, default False\n        returns Series if only one column\n    infer_datetime_format: boolean, default False\n        If True and `parse_dates` is True for a column, try to infer the\n        datetime format based on the first datetime string. If the format\n        can be inferred, there often will be a large parsing speed-up.\n    float_precision : string, default None\n        Specifies which converter the C engine should use for floating-point\n        values. The options are None for the ordinary converter,\n        'high' for the high-precision converter, and 'round_trip' for the\n        round-trip converter.\n    \"\"\"\n    kwds['engine'] = 'python'\n    return TextFileReader(*args, **kwds)\n\n\ndef count_empty_vals(vals):\n    return sum([1 for v in vals if v == '' or v is None])\n\n\nclass PythonParser(ParserBase):\n\n    def __init__(self, f, **kwds):\n        \"\"\"\n        Workhorse function for processing nested list into DataFrame\n\n        Should be replaced by np.genfromtxt eventually?\n        \"\"\"\n        ParserBase.__init__(self, kwds)\n\n        self.data = None\n        self.buf = []\n        self.pos = 0\n        self.line_pos = 0\n\n        self.encoding = kwds['encoding']\n        self.compression = kwds['compression']\n        self.memory_map = kwds['memory_map']\n        self.skiprows = kwds['skiprows']\n\n        if callable(self.skiprows):\n            self.skipfunc = self.skiprows\n        else:\n            self.skipfunc = lambda x: x in self.skiprows\n\n        self.skipfooter = _validate_skipfooter_arg(kwds['skipfooter'])\n        self.delimiter = kwds['delimiter']\n\n        self.quotechar = kwds['quotechar']\n        if isinstance(self.quotechar, compat.text_type):\n            self.quotechar = str(self.quotechar)\n\n        self.escapechar = kwds['escapechar']\n        self.doublequote = kwds['doublequote']\n        self.skipinitialspace = kwds['skipinitialspace']\n        self.lineterminator = kwds['lineterminator']\n        self.quoting = kwds['quoting']\n        self.usecols, _ = _validate_usecols_arg(kwds['usecols'])\n        self.skip_blank_lines = kwds['skip_blank_lines']\n\n        self.warn_bad_lines = kwds['warn_bad_lines']\n        self.error_bad_lines = kwds['error_bad_lines']\n\n        self.names_passed = kwds['names'] or None\n\n        self.na_filter = kwds['na_filter']\n\n        self.has_index_names = False\n        if 'has_index_names' in kwds:\n            self.has_index_names = kwds['has_index_names']\n\n        self.verbose = kwds['verbose']\n        self.converters = kwds['converters']\n        self.dtype = kwds['dtype']\n\n        self.compact_ints = kwds['compact_ints']\n        self.use_unsigned = kwds['use_unsigned']\n        self.thousands = kwds['thousands']\n        self.decimal = kwds['decimal']\n\n        self.comment = kwds['comment']\n        self._comment_lines = []\n\n        mode = 'r' if PY3 else 'rb'\n        f, handles = _get_handle(f, mode, encoding=self.encoding,\n                                 compression=self.compression,\n                                 memory_map=self.memory_map)\n        self.handles.extend(handles)\n\n        # Set self.data to something that can read lines.\n        if hasattr(f, 'readline'):\n            self._make_reader(f)\n        else:\n            self.data = f\n\n        # Get columns in two steps: infer from data, then\n        # infer column indices from self.usecols if is is specified.\n        self._col_indices = None\n        self.columns, self.num_original_columns = self._infer_columns()\n\n        # Now self.columns has the set of columns that we will process.\n        # The original set is stored in self.original_columns.\n        if len(self.columns) > 1:\n            # we are processing a multi index column\n            self.columns, self.index_names, self.col_names, _ = (\n                self._extract_multi_indexer_columns(\n                    self.columns, self.index_names, self.col_names\n                )\n            )\n            # Update list of original names to include all indices.\n            self.num_original_columns = len(self.columns)\n        else:\n            self.columns = self.columns[0]\n\n        # get popped off for index\n        self.orig_names = list(self.columns)\n\n        # needs to be cleaned/refactored\n        # multiple date column thing turning into a real spaghetti factory\n\n        if not self._has_complex_date_col:\n            (index_names, self.orig_names, self.columns) = (\n                self._get_index_name(self.columns))\n            self._name_processed = True\n            if self.index_names is None:\n                self.index_names = index_names\n\n        if self.parse_dates:\n            self._no_thousands_columns = self._set_no_thousands_columns()\n        else:\n            self._no_thousands_columns = None\n\n        if len(self.decimal) != 1:\n            raise ValueError('Only length-1 decimal markers supported')\n\n        if self.thousands is None:\n            self.nonnum = re.compile('[^-^0-9^%s]+' % self.decimal)\n        else:\n            self.nonnum = re.compile('[^-^0-9^%s^%s]+' % (self.thousands,\n                                                          self.decimal))\n\n    def _set_no_thousands_columns(self):\n        # Create a set of column ids that are not to be stripped of thousands\n        # operators.\n        noconvert_columns = set()\n\n        def _set(x):\n            if is_integer(x):\n                noconvert_columns.add(x)\n            else:\n                noconvert_columns.add(self.columns.index(x))\n\n        if isinstance(self.parse_dates, list):\n            for val in self.parse_dates:\n                if isinstance(val, list):\n                    for k in val:\n                        _set(k)\n                else:\n                    _set(val)\n\n        elif isinstance(self.parse_dates, dict):\n            for val in self.parse_dates.values():\n                if isinstance(val, list):\n                    for k in val:\n                        _set(k)\n                else:\n                    _set(val)\n\n        elif self.parse_dates:\n            if isinstance(self.index_col, list):\n                for k in self.index_col:\n                    _set(k)\n            elif self.index_col is not None:\n                _set(self.index_col)\n\n        return noconvert_columns\n\n    def _make_reader(self, f):\n        sep = self.delimiter\n\n        if sep is None or len(sep) == 1:\n            if self.lineterminator:\n                raise ValueError('Custom line terminators not supported in '\n                                 'python parser (yet)')\n\n            class MyDialect(csv.Dialect):\n                delimiter = self.delimiter\n                quotechar = self.quotechar\n                escapechar = self.escapechar\n                doublequote = self.doublequote\n                skipinitialspace = self.skipinitialspace\n                quoting = self.quoting\n                lineterminator = '\\n'\n\n            dia = MyDialect\n\n            sniff_sep = True\n\n            if sep is not None:\n                sniff_sep = False\n                dia.delimiter = sep\n            # attempt to sniff the delimiter\n            if sniff_sep:\n                line = f.readline()\n                while self.skipfunc(self.pos):\n                    self.pos += 1\n                    line = f.readline()\n\n                line = self._check_comments([line])[0]\n\n                self.pos += 1\n                self.line_pos += 1\n                sniffed = csv.Sniffer().sniff(line)\n                dia.delimiter = sniffed.delimiter\n                if self.encoding is not None:\n                    self.buf.extend(list(\n                        UnicodeReader(StringIO(line),\n                                      dialect=dia,\n                                      encoding=self.encoding)))\n                else:\n                    self.buf.extend(list(csv.reader(StringIO(line),\n                                                    dialect=dia)))\n\n            if self.encoding is not None:\n                reader = UnicodeReader(f, dialect=dia,\n                                       encoding=self.encoding,\n                                       strict=True)\n            else:\n                reader = csv.reader(f, dialect=dia,\n                                    strict=True)\n\n        else:\n            def _read():\n                line = f.readline()\n\n                if compat.PY2 and self.encoding:\n                    line = line.decode(self.encoding)\n\n                pat = re.compile(sep)\n                yield pat.split(line.strip())\n                for line in f:\n                    yield pat.split(line.strip())\n            reader = _read()\n\n        self.data = reader\n\n    def read(self, rows=None):\n        try:\n            content = self._get_lines(rows)\n        except StopIteration:\n            if self._first_chunk:\n                content = []\n            else:\n                raise\n\n        # done with first read, next time raise StopIteration\n        self._first_chunk = False\n\n        columns = list(self.orig_names)\n        if not len(content):  # pragma: no cover\n            # DataFrame with the right metadata, even though it's length 0\n            names = self._maybe_dedup_names(self.orig_names)\n            index, columns, col_dict = _get_empty_meta(\n                names, self.index_col, self.index_names, self.dtype)\n            columns = self._maybe_make_multi_index_columns(\n                columns, self.col_names)\n            return index, columns, col_dict\n\n        # handle new style for names in index\n        count_empty_content_vals = count_empty_vals(content[0])\n        indexnamerow = None\n        if self.has_index_names and count_empty_content_vals == len(columns):\n            indexnamerow = content[0]\n            content = content[1:]\n\n        alldata = self._rows_to_cols(content)\n        data = self._exclude_implicit_index(alldata)\n\n        columns = self._maybe_dedup_names(self.columns)\n        columns, data = self._do_date_conversions(columns, data)\n\n        data = self._convert_data(data)\n        if self.as_recarray:\n            return self._to_recarray(data, columns)\n\n        index, columns = self._make_index(data, alldata, columns, indexnamerow)\n\n        return index, columns, data\n\n    def _exclude_implicit_index(self, alldata):\n        names = self._maybe_dedup_names(self.orig_names)\n\n        if self._implicit_index:\n            excl_indices = self.index_col\n\n            data = {}\n            offset = 0\n            for i, col in enumerate(names):\n                while i + offset in excl_indices:\n                    offset += 1\n                data[col] = alldata[i + offset]\n        else:\n            data = dict((k, v) for k, v in zip(names, alldata))\n\n        return data\n\n    # legacy\n    def get_chunk(self, size=None):\n        if size is None:\n            size = self.chunksize\n        return self.read(rows=size)\n\n    def _convert_data(self, data):\n        # apply converters\n        def _clean_mapping(mapping):\n            \"converts col numbers to names\"\n            clean = {}\n            for col, v in compat.iteritems(mapping):\n                if isinstance(col, int) and col not in self.orig_names:\n                    col = self.orig_names[col]\n                clean[col] = v\n            return clean\n\n        clean_conv = _clean_mapping(self.converters)\n        if not isinstance(self.dtype, dict):\n            # handles single dtype applied to all columns\n            clean_dtypes = self.dtype\n        else:\n            clean_dtypes = _clean_mapping(self.dtype)\n\n        # Apply NA values.\n        clean_na_values = {}\n        clean_na_fvalues = {}\n\n        if isinstance(self.na_values, dict):\n            for col in self.na_values:\n                na_value = self.na_values[col]\n                na_fvalue = self.na_fvalues[col]\n\n                if isinstance(col, int) and col not in self.orig_names:\n                    col = self.orig_names[col]\n\n                clean_na_values[col] = na_value\n                clean_na_fvalues[col] = na_fvalue\n        else:\n            clean_na_values = self.na_values\n            clean_na_fvalues = self.na_fvalues\n\n        return self._convert_to_ndarrays(data, clean_na_values,\n                                         clean_na_fvalues, self.verbose,\n                                         clean_conv, clean_dtypes)\n\n    def _to_recarray(self, data, columns):\n        dtypes = []\n        o = compat.OrderedDict()\n\n        # use the columns to \"order\" the keys\n        # in the unordered 'data' dictionary\n        for col in columns:\n            dtypes.append((str(col), data[col].dtype))\n            o[col] = data[col]\n\n        tuples = lzip(*o.values())\n        return np.array(tuples, dtypes)\n\n    def _infer_columns(self):\n        names = self.names\n        num_original_columns = 0\n        clear_buffer = True\n        if self.header is not None:\n            header = self.header\n\n            # we have a mi columns, so read an extra line\n            if isinstance(header, (list, tuple, np.ndarray)):\n                have_mi_columns = True\n                header = list(header) + [header[-1] + 1]\n            else:\n                have_mi_columns = False\n                header = [header]\n\n            columns = []\n            for level, hr in enumerate(header):\n                try:\n                    line = self._buffered_line()\n\n                    while self.line_pos <= hr:\n                        line = self._next_line()\n\n                except StopIteration:\n                    if self.line_pos < hr:\n                        raise ValueError(\n                            'Passed header=%s but only %d lines in file'\n                            % (hr, self.line_pos + 1))\n\n                    # We have an empty file, so check\n                    # if columns are provided. That will\n                    # serve as the 'line' for parsing\n                    if have_mi_columns and hr > 0:\n                        if clear_buffer:\n                            self._clear_buffer()\n                        columns.append([None] * len(columns[-1]))\n                        return columns, num_original_columns\n\n                    if not self.names:\n                        raise EmptyDataError(\n                            \"No columns to parse from file\")\n\n                    line = self.names[:]\n\n                unnamed_count = 0\n                this_columns = []\n                for i, c in enumerate(line):\n                    if c == '':\n                        if have_mi_columns:\n                            this_columns.append('Unnamed: %d_level_%d'\n                                                % (i, level))\n                        else:\n                            this_columns.append('Unnamed: %d' % i)\n                        unnamed_count += 1\n                    else:\n                        this_columns.append(c)\n\n                if not have_mi_columns and self.mangle_dupe_cols:\n                    counts = {}\n\n                    for i, col in enumerate(this_columns):\n                        cur_count = counts.get(col, 0)\n\n                        while cur_count > 0:\n                            counts[col] = cur_count + 1\n                            col = \"%s.%d\" % (col, cur_count)\n                            cur_count = counts.get(col, 0)\n\n                        this_columns[i] = col\n                        counts[col] = cur_count + 1\n                elif have_mi_columns:\n\n                    # if we have grabbed an extra line, but its not in our\n                    # format so save in the buffer, and create an blank extra\n                    # line for the rest of the parsing code\n                    if hr == header[-1]:\n                        lc = len(this_columns)\n                        ic = (len(self.index_col)\n                              if self.index_col is not None else 0)\n                        if lc != unnamed_count and lc - ic > unnamed_count:\n                            clear_buffer = False\n                            this_columns = [None] * lc\n                            self.buf = [self.buf[-1]]\n\n                columns.append(this_columns)\n                if len(columns) == 1:\n                    num_original_columns = len(this_columns)\n\n            if clear_buffer:\n                self._clear_buffer()\n\n            if names is not None:\n                if ((self.usecols is not None and\n                     len(names) != len(self.usecols)) or\n                    (self.usecols is None and\n                     len(names) != len(columns[0]))):\n                    raise ValueError('Number of passed names did not match '\n                                     'number of header fields in the file')\n                if len(columns) > 1:\n                    raise TypeError('Cannot pass names with multi-index '\n                                    'columns')\n\n                if self.usecols is not None:\n                    # Set _use_cols. We don't store columns because they are\n                    # overwritten.\n                    self._handle_usecols(columns, names)\n                else:\n                    self._col_indices = None\n                    num_original_columns = len(names)\n                columns = [names]\n            else:\n                columns = self._handle_usecols(columns, columns[0])\n        else:\n            try:\n                line = self._buffered_line()\n\n            except StopIteration:\n                if not names:\n                    raise EmptyDataError(\n                        \"No columns to parse from file\")\n\n                line = names[:]\n\n            ncols = len(line)\n            num_original_columns = ncols\n\n            if not names:\n                if self.prefix:\n                    columns = [['%s%d' % (self.prefix, i)\n                                for i in range(ncols)]]\n                else:\n                    columns = [lrange(ncols)]\n                columns = self._handle_usecols(columns, columns[0])\n            else:\n                if self.usecols is None or len(names) >= num_original_columns:\n                    columns = self._handle_usecols([names], names)\n                    num_original_columns = len(names)\n                else:\n                    if (not callable(self.usecols) and\n                            len(names) != len(self.usecols)):\n                        raise ValueError(\n                            'Number of passed names did not match number of '\n                            'header fields in the file'\n                        )\n                    # Ignore output but set used columns.\n                    self._handle_usecols([names], names)\n                    columns = [names]\n                    num_original_columns = ncols\n\n        return columns, num_original_columns\n\n    def _handle_usecols(self, columns, usecols_key):\n        \"\"\"\n        Sets self._col_indices\n\n        usecols_key is used if there are string usecols.\n        \"\"\"\n        if self.usecols is not None:\n            if callable(self.usecols):\n                col_indices = _evaluate_usecols(self.usecols, usecols_key)\n            elif any([isinstance(u, string_types) for u in self.usecols]):\n                if len(columns) > 1:\n                    raise ValueError(\"If using multiple headers, usecols must \"\n                                     \"be integers.\")\n                col_indices = []\n                for col in self.usecols:\n                    if isinstance(col, string_types):\n                        col_indices.append(usecols_key.index(col))\n                    else:\n                        col_indices.append(col)\n            else:\n                col_indices = self.usecols\n\n            columns = [[n for i, n in enumerate(column) if i in col_indices]\n                       for column in columns]\n            self._col_indices = col_indices\n        return columns\n\n    def _buffered_line(self):\n        \"\"\"\n        Return a line from buffer, filling buffer if required.\n        \"\"\"\n        if len(self.buf) > 0:\n            return self.buf[0]\n        else:\n            return self._next_line()\n\n    def _check_for_bom(self, first_row):\n        \"\"\"\n        Checks whether the file begins with the BOM character.\n        If it does, remove it. In addition, if there is quoting\n        in the field subsequent to the BOM, remove it as well\n        because it technically takes place at the beginning of\n        the name, not the middle of it.\n        \"\"\"\n        # first_row will be a list, so we need to check\n        # that that list is not empty before proceeding.\n        if not first_row:\n            return first_row\n\n        # The first element of this row is the one that could have the\n        # BOM that we want to remove. Check that the first element is a\n        # string before proceeding.\n        if not isinstance(first_row[0], compat.string_types):\n            return first_row\n\n        # Check that the string is not empty, as that would\n        # obviously not have a BOM at the start of it.\n        if not first_row[0]:\n            return first_row\n\n        # Since the string is non-empty, check that it does\n        # in fact begin with a BOM.\n        first_elt = first_row[0][0]\n\n        # This is to avoid warnings we get in Python 2.x if\n        # we find ourselves comparing with non-Unicode\n        if compat.PY2 and not isinstance(first_elt, unicode):  # noqa\n            try:\n                first_elt = u(first_elt)\n            except UnicodeDecodeError:\n                return first_row\n\n        if first_elt != _BOM:\n            return first_row\n\n        first_row = first_row[0]\n\n        if len(first_row) > 1 and first_row[1] == self.quotechar:\n            start = 2\n            quote = first_row[1]\n            end = first_row[2:].index(quote) + 2\n\n            # Extract the data between the quotation marks\n            new_row = first_row[start:end]\n\n            # Extract any remaining data after the second\n            # quotation mark.\n            if len(first_row) > end + 1:\n                new_row += first_row[end + 1:]\n            return [new_row]\n        elif len(first_row) > 1:\n            return [first_row[1:]]\n        else:\n            # First row is just the BOM, so we\n            # return an empty string.\n            return [\"\"]\n\n    def _is_line_empty(self, line):\n        \"\"\"\n        Check if a line is empty or not.\n\n        Parameters\n        ----------\n        line : str, array-like\n            The line of data to check.\n\n        Returns\n        -------\n        boolean : Whether or not the line is empty.\n        \"\"\"\n        return not line or all(not x for x in line)\n\n    def _next_line(self):\n        if isinstance(self.data, list):\n            while self.skipfunc(self.pos):\n                self.pos += 1\n\n            while True:\n                try:\n                    line = self._check_comments([self.data[self.pos]])[0]\n                    self.pos += 1\n                    # either uncommented or blank to begin with\n                    if (not self.skip_blank_lines and\n                            (self._is_line_empty(\n                                self.data[self.pos - 1]) or line)):\n                        break\n                    elif self.skip_blank_lines:\n                        ret = self._remove_empty_lines([line])\n                        if ret:\n                            line = ret[0]\n                            break\n                except IndexError:\n                    raise StopIteration\n        else:\n            while self.skipfunc(self.pos):\n                self.pos += 1\n                next(self.data)\n\n            while True:\n                orig_line = self._next_iter_line(row_num=self.pos + 1)\n                self.pos += 1\n\n                if orig_line is not None:\n                    line = self._check_comments([orig_line])[0]\n\n                    if self.skip_blank_lines:\n                        ret = self._remove_empty_lines([line])\n\n                        if ret:\n                            line = ret[0]\n                            break\n                    elif self._is_line_empty(orig_line) or line:\n                        break\n\n        # This was the first line of the file,\n        # which could contain the BOM at the\n        # beginning of it.\n        if self.pos == 1:\n            line = self._check_for_bom(line)\n\n        self.line_pos += 1\n        self.buf.append(line)\n        return line\n\n    def _alert_malformed(self, msg, row_num):\n        \"\"\"\n        Alert a user about a malformed row.\n\n        If `self.error_bad_lines` is True, the alert will be `ParserError`.\n        If `self.warn_bad_lines` is True, the alert will be printed out.\n\n        Parameters\n        ----------\n        msg : The error message to display.\n        row_num : The row number where the parsing error occurred.\n                  Because this row number is displayed, we 1-index,\n                  even though we 0-index internally.\n        \"\"\"\n\n        if self.error_bad_lines:\n            raise ParserError(msg)\n        elif self.warn_bad_lines:\n            base = 'Skipping line {row_num}: '.format(row_num=row_num)\n            sys.stderr.write(base + msg + '\\n')\n\n    def _next_iter_line(self, row_num):\n        \"\"\"\n        Wrapper around iterating through `self.data` (CSV source).\n\n        When a CSV error is raised, we check for specific\n        error messages that allow us to customize the\n        error message displayed to the user.\n\n        Parameters\n        ----------\n        row_num : The row number of the line being parsed.\n        \"\"\"\n\n        try:\n            return next(self.data)\n        except csv.Error as e:\n            if self.warn_bad_lines or self.error_bad_lines:\n                msg = str(e)\n\n                if 'NULL byte' in msg:\n                    msg = ('NULL byte detected. This byte '\n                           'cannot be processed in Python\\'s '\n                           'native csv library at the moment, '\n                           'so please pass in engine=\\'c\\' instead')\n                elif 'newline inside string' in msg:\n                    msg = ('EOF inside string starting with '\n                           'line ' + str(row_num))\n\n                if self.skipfooter > 0:\n                    reason = ('Error could possibly be due to '\n                              'parsing errors in the skipped footer rows '\n                              '(the skipfooter keyword is only applied '\n                              'after Python\\'s csv library has parsed '\n                              'all rows).')\n                    msg += '. ' + reason\n\n                self._alert_malformed(msg, row_num)\n            return None\n\n    def _check_comments(self, lines):\n        if self.comment is None:\n            return lines\n        ret = []\n        for l in lines:\n            rl = []\n            for x in l:\n                if (not isinstance(x, compat.string_types) or\n                        self.comment not in x):\n                    rl.append(x)\n                else:\n                    x = x[:x.find(self.comment)]\n                    if len(x) > 0:\n                        rl.append(x)\n                    break\n            ret.append(rl)\n        return ret\n\n    def _remove_empty_lines(self, lines):\n        \"\"\"\n        Iterate through the lines and remove any that are\n        either empty or contain only one whitespace value\n\n        Parameters\n        ----------\n        lines : array-like\n            The array of lines that we are to filter.\n\n        Returns\n        -------\n        filtered_lines : array-like\n            The same array of lines with the \"empty\" ones removed.\n        \"\"\"\n\n        ret = []\n        for l in lines:\n            # Remove empty lines and lines with only one whitespace value\n            if (len(l) > 1 or len(l) == 1 and\n                    (not isinstance(l[0], compat.string_types) or\n                     l[0].strip())):\n                ret.append(l)\n        return ret\n\n    def _check_thousands(self, lines):\n        if self.thousands is None:\n            return lines\n\n        return self._search_replace_num_columns(lines=lines,\n                                                search=self.thousands,\n                                                replace='')\n\n    def _search_replace_num_columns(self, lines, search, replace):\n        ret = []\n        for l in lines:\n            rl = []\n            for i, x in enumerate(l):\n                if (not isinstance(x, compat.string_types) or\n                    search not in x or\n                    (self._no_thousands_columns and\n                     i in self._no_thousands_columns) or\n                        self.nonnum.search(x.strip())):\n                    rl.append(x)\n                else:\n                    rl.append(x.replace(search, replace))\n            ret.append(rl)\n        return ret\n\n    def _check_decimal(self, lines):\n        if self.decimal == _parser_defaults['decimal']:\n            return lines\n\n        return self._search_replace_num_columns(lines=lines,\n                                                search=self.decimal,\n                                                replace='.')\n\n    def _clear_buffer(self):\n        self.buf = []\n\n    _implicit_index = False\n\n    def _get_index_name(self, columns):\n        \"\"\"\n        Try several cases to get lines:\n\n        0) There are headers on row 0 and row 1 and their\n        total summed lengths equals the length of the next line.\n        Treat row 0 as columns and row 1 as indices\n        1) Look for implicit index: there are more columns\n        on row 1 than row 0. If this is true, assume that row\n        1 lists index columns and row 0 lists normal columns.\n        2) Get index from the columns if it was listed.\n        \"\"\"\n        orig_names = list(columns)\n        columns = list(columns)\n\n        try:\n            line = self._next_line()\n        except StopIteration:\n            line = None\n\n        try:\n            next_line = self._next_line()\n        except StopIteration:\n            next_line = None\n\n        # implicitly index_col=0 b/c 1 fewer column names\n        implicit_first_cols = 0\n        if line is not None:\n            # leave it 0, #2442\n            # Case 1\n            if self.index_col is not False:\n                implicit_first_cols = len(line) - self.num_original_columns\n\n            # Case 0\n            if next_line is not None:\n                if len(next_line) == len(line) + self.num_original_columns:\n                    # column and index names on diff rows\n                    self.index_col = lrange(len(line))\n                    self.buf = self.buf[1:]\n\n                    for c in reversed(line):\n                        columns.insert(0, c)\n\n                    # Update list of original names to include all indices.\n                    orig_names = list(columns)\n                    self.num_original_columns = len(columns)\n                    return line, orig_names, columns\n\n        if implicit_first_cols > 0:\n            # Case 1\n            self._implicit_index = True\n            if self.index_col is None:\n                self.index_col = lrange(implicit_first_cols)\n\n            index_name = None\n\n        else:\n            # Case 2\n            (index_name, columns_,\n             self.index_col) = _clean_index_names(columns, self.index_col)\n\n        return index_name, orig_names, columns\n\n    def _rows_to_cols(self, content):\n        col_len = self.num_original_columns\n\n        if self._implicit_index:\n            col_len += len(self.index_col)\n\n        max_len = max([len(row) for row in content])\n\n        # Check that there are no rows with too many\n        # elements in their row (rows with too few\n        # elements are padded with NaN).\n        if (max_len > col_len and\n                self.index_col is not False and\n                self.usecols is None):\n\n            footers = self.skipfooter if self.skipfooter else 0\n            bad_lines = []\n\n            iter_content = enumerate(content)\n            content_len = len(content)\n            content = []\n\n            for (i, l) in iter_content:\n                actual_len = len(l)\n\n                if actual_len > col_len:\n                    if self.error_bad_lines or self.warn_bad_lines:\n                        row_num = self.pos - (content_len - i + footers)\n                        bad_lines.append((row_num, actual_len))\n\n                        if self.error_bad_lines:\n                            break\n                else:\n                    content.append(l)\n\n            for row_num, actual_len in bad_lines:\n                msg = ('Expected %d fields in line %d, saw %d' %\n                       (col_len, row_num + 1, actual_len))\n                if len(self.delimiter) > 1 and self.quoting != csv.QUOTE_NONE:\n                    # see gh-13374\n                    reason = ('Error could possibly be due to quotes being '\n                              'ignored when a multi-char delimiter is used.')\n                    msg += '. ' + reason\n\n                self._alert_malformed(msg, row_num + 1)\n\n        # see gh-13320\n        zipped_content = list(lib.to_object_array(\n            content, min_width=col_len).T)\n\n        if self.usecols:\n            if self._implicit_index:\n                zipped_content = [\n                    a for i, a in enumerate(zipped_content)\n                    if (i < len(self.index_col) or\n                        i - len(self.index_col) in self._col_indices)]\n            else:\n                zipped_content = [a for i, a in enumerate(zipped_content)\n                                  if i in self._col_indices]\n        return zipped_content\n\n    def _get_lines(self, rows=None):\n        lines = self.buf\n        new_rows = None\n\n        # already fetched some number\n        if rows is not None:\n            # we already have the lines in the buffer\n            if len(self.buf) >= rows:\n                new_rows, self.buf = self.buf[:rows], self.buf[rows:]\n\n            # need some lines\n            else:\n                rows -= len(self.buf)\n\n        if new_rows is None:\n            if isinstance(self.data, list):\n                if self.pos > len(self.data):\n                    raise StopIteration\n                if rows is None:\n                    new_rows = self.data[self.pos:]\n                    new_pos = len(self.data)\n                else:\n                    new_rows = self.data[self.pos:self.pos + rows]\n                    new_pos = self.pos + rows\n\n                # Check for stop rows. n.b.: self.skiprows is a set.\n                if self.skiprows:\n                    new_rows = [row for i, row in enumerate(new_rows)\n                                if not self.skipfunc(i + self.pos)]\n\n                lines.extend(new_rows)\n                self.pos = new_pos\n\n            else:\n                new_rows = []\n                try:\n                    if rows is not None:\n                        for _ in range(rows):\n                            new_rows.append(next(self.data))\n                        lines.extend(new_rows)\n                    else:\n                        rows = 0\n\n                        while True:\n                            new_row = self._next_iter_line(\n                                row_num=self.pos + rows + 1)\n                            rows += 1\n\n                            if new_row is not None:\n                                new_rows.append(new_row)\n\n                except StopIteration:\n                    if self.skiprows:\n                        new_rows = [row for i, row in enumerate(new_rows)\n                                    if not self.skipfunc(i + self.pos)]\n                    lines.extend(new_rows)\n                    if len(lines) == 0:\n                        raise\n                self.pos += len(new_rows)\n\n            self.buf = []\n        else:\n            lines = new_rows\n\n        if self.skipfooter:\n            lines = lines[:-self.skipfooter]\n\n        lines = self._check_comments(lines)\n        if self.skip_blank_lines:\n            lines = self._remove_empty_lines(lines)\n        lines = self._check_thousands(lines)\n        return self._check_decimal(lines)\n\n\ndef _make_date_converter(date_parser=None, dayfirst=False,\n                         infer_datetime_format=False):\n    def converter(*date_cols):\n        if date_parser is None:\n            strs = _concat_date_cols(date_cols)\n\n            try:\n                return tools.to_datetime(\n                    _ensure_object(strs),\n                    utc=None,\n                    box=False,\n                    dayfirst=dayfirst,\n                    errors='ignore',\n                    infer_datetime_format=infer_datetime_format\n                )\n            except:\n                return tools.to_datetime(\n                    lib.try_parse_dates(strs, dayfirst=dayfirst))\n        else:\n            try:\n                result = tools.to_datetime(\n                    date_parser(*date_cols), errors='ignore')\n                if isinstance(result, datetime.datetime):\n                    raise Exception('scalar parser')\n                return result\n            except Exception:\n                try:\n                    return tools.to_datetime(\n                        lib.try_parse_dates(_concat_date_cols(date_cols),\n                                            parser=date_parser,\n                                            dayfirst=dayfirst),\n                        errors='ignore')\n                except Exception:\n                    return generic_parser(date_parser, *date_cols)\n\n    return converter\n\n\ndef _process_date_conversion(data_dict, converter, parse_spec,\n                             index_col, index_names, columns,\n                             keep_date_col=False):\n    def _isindex(colspec):\n        return ((isinstance(index_col, list) and\n                 colspec in index_col) or\n                (isinstance(index_names, list) and\n                 colspec in index_names))\n\n    new_cols = []\n    new_data = {}\n\n    orig_names = columns\n    columns = list(columns)\n\n    date_cols = set()\n\n    if parse_spec is None or isinstance(parse_spec, bool):\n        return data_dict, columns\n\n    if isinstance(parse_spec, list):\n        # list of column lists\n        for colspec in parse_spec:\n            if is_scalar(colspec):\n                if isinstance(colspec, int) and colspec not in data_dict:\n                    colspec = orig_names[colspec]\n                if _isindex(colspec):\n                    continue\n                data_dict[colspec] = converter(data_dict[colspec])\n            else:\n                new_name, col, old_names = _try_convert_dates(\n                    converter, colspec, data_dict, orig_names)\n                if new_name in data_dict:\n                    raise ValueError('New date column already in dict %s' %\n                                     new_name)\n                new_data[new_name] = col\n                new_cols.append(new_name)\n                date_cols.update(old_names)\n\n    elif isinstance(parse_spec, dict):\n        # dict of new name to column list\n        for new_name, colspec in compat.iteritems(parse_spec):\n            if new_name in data_dict:\n                raise ValueError('Date column %s already in dict' %\n                                 new_name)\n\n            _, col, old_names = _try_convert_dates(converter, colspec,\n                                                   data_dict, orig_names)\n\n            new_data[new_name] = col\n            new_cols.append(new_name)\n            date_cols.update(old_names)\n\n    data_dict.update(new_data)\n    new_cols.extend(columns)\n\n    if not keep_date_col:\n        for c in list(date_cols):\n            data_dict.pop(c)\n            new_cols.remove(c)\n\n    return data_dict, new_cols\n\n\ndef _try_convert_dates(parser, colspec, data_dict, columns):\n    colset = set(columns)\n    colnames = []\n\n    for c in colspec:\n        if c in colset:\n            colnames.append(c)\n        elif isinstance(c, int) and c not in columns:\n            colnames.append(columns[c])\n        else:\n            colnames.append(c)\n\n    new_name = '_'.join([str(x) for x in colnames])\n    to_parse = [data_dict[c] for c in colnames if c in data_dict]\n\n    new_col = parser(*to_parse)\n    return new_name, new_col, colnames\n\n\ndef _clean_na_values(na_values, keep_default_na=True):\n\n    if na_values is None:\n        if keep_default_na:\n            na_values = _NA_VALUES\n        else:\n            na_values = set()\n        na_fvalues = set()\n    elif isinstance(na_values, dict):\n        na_values = na_values.copy()  # Prevent aliasing.\n        if keep_default_na:\n            for k, v in compat.iteritems(na_values):\n                if not is_list_like(v):\n                    v = [v]\n                v = set(v) | _NA_VALUES\n                na_values[k] = v\n        na_fvalues = dict([\n            (k, _floatify_na_values(v)) for k, v in na_values.items()  # noqa\n        ])\n    else:\n        if not is_list_like(na_values):\n            na_values = [na_values]\n        na_values = _stringify_na_values(na_values)\n        if keep_default_na:\n            na_values = na_values | _NA_VALUES\n\n        na_fvalues = _floatify_na_values(na_values)\n\n    return na_values, na_fvalues\n\n\ndef _clean_index_names(columns, index_col):\n    if not _is_index_col(index_col):\n        return None, columns, index_col\n\n    columns = list(columns)\n\n    cp_cols = list(columns)\n    index_names = []\n\n    # don't mutate\n    index_col = list(index_col)\n\n    for i, c in enumerate(index_col):\n        if isinstance(c, compat.string_types):\n            index_names.append(c)\n            for j, name in enumerate(cp_cols):\n                if name == c:\n                    index_col[i] = j\n                    columns.remove(name)\n                    break\n        else:\n            name = cp_cols[c]\n            columns.remove(name)\n            index_names.append(name)\n\n    # hack\n    if isinstance(index_names[0], compat.string_types)\\\n            and 'Unnamed' in index_names[0]:\n        index_names[0] = None\n\n    return index_names, columns, index_col\n\n\ndef _get_empty_meta(columns, index_col, index_names, dtype=None):\n    columns = list(columns)\n\n    # Convert `dtype` to a defaultdict of some kind.\n    # This will enable us to write `dtype[col_name]`\n    # without worrying about KeyError issues later on.\n    if not isinstance(dtype, dict):\n        # if dtype == None, default will be np.object.\n        default_dtype = dtype or np.object\n        dtype = defaultdict(lambda: default_dtype)\n    else:\n        # Save a copy of the dictionary.\n        _dtype = dtype.copy()\n        dtype = defaultdict(lambda: np.object)\n\n        # Convert column indexes to column names.\n        for k, v in compat.iteritems(_dtype):\n            col = columns[k] if is_integer(k) else k\n            dtype[col] = v\n\n    if index_col is None or index_col is False:\n        index = Index([])\n    else:\n        index = [Series([], dtype=dtype[index_name])\n                 for index_name in index_names]\n        index = MultiIndex.from_arrays(index, names=index_names)\n        index_col.sort()\n        for i, n in enumerate(index_col):\n            columns.pop(n - i)\n\n    col_dict = dict((col_name,\n                     Series([], dtype=dtype[col_name]))\n                    for col_name in columns)\n\n    return index, columns, col_dict\n\n\ndef _floatify_na_values(na_values):\n    # create float versions of the na_values\n    result = set()\n    for v in na_values:\n        try:\n            v = float(v)\n            if not np.isnan(v):\n                result.add(v)\n        except:\n            pass\n    return result\n\n\ndef _stringify_na_values(na_values):\n    \"\"\" return a stringified and numeric for these values \"\"\"\n    result = []\n    for x in na_values:\n        result.append(str(x))\n        result.append(x)\n        try:\n            v = float(x)\n\n            # we are like 999 here\n            if v == int(v):\n                v = int(v)\n                result.append(\"%s.0\" % v)\n                result.append(str(v))\n\n            result.append(v)\n        except:\n            pass\n        try:\n            result.append(int(x))\n        except:\n            pass\n    return set(result)\n\n\ndef _get_na_values(col, na_values, na_fvalues):\n    if isinstance(na_values, dict):\n        if col in na_values:\n            return na_values[col], na_fvalues[col]\n        else:\n            return _NA_VALUES, set()\n    else:\n        return na_values, na_fvalues\n\n\ndef _get_col_names(colspec, columns):\n    colset = set(columns)\n    colnames = []\n    for c in colspec:\n        if c in colset:\n            colnames.append(c)\n        elif isinstance(c, int):\n            colnames.append(columns[c])\n    return colnames\n\n\ndef _concat_date_cols(date_cols):\n    if len(date_cols) == 1:\n        if compat.PY3:\n            return np.array([compat.text_type(x) for x in date_cols[0]],\n                            dtype=object)\n        else:\n            return np.array([\n                str(x) if not isinstance(x, compat.string_types) else x\n                for x in date_cols[0]\n            ], dtype=object)\n\n    rs = np.array([' '.join([compat.text_type(y) for y in x])\n                   for x in zip(*date_cols)], dtype=object)\n    return rs\n\n\nclass FixedWidthReader(BaseIterator):\n    \"\"\"\n    A reader of fixed-width lines.\n    \"\"\"\n\n    def __init__(self, f, colspecs, delimiter, comment, skiprows=None):\n        self.f = f\n        self.buffer = None\n        self.delimiter = '\\r\\n' + delimiter if delimiter else '\\n\\r\\t '\n        self.comment = comment\n        if colspecs == 'infer':\n            self.colspecs = self.detect_colspecs(skiprows=skiprows)\n        else:\n            self.colspecs = colspecs\n\n        if not isinstance(self.colspecs, (tuple, list)):\n            raise TypeError(\"column specifications must be a list or tuple, \"\n                            \"input was a %r\" % type(colspecs).__name__)\n\n        for colspec in self.colspecs:\n            if not (isinstance(colspec, (tuple, list)) and\n                    len(colspec) == 2 and\n                    isinstance(colspec[0], (int, np.integer, type(None))) and\n                    isinstance(colspec[1], (int, np.integer, type(None)))):\n                raise TypeError('Each column specification must be '\n                                '2 element tuple or list of integers')\n\n    def get_rows(self, n, skiprows=None):\n        \"\"\"\n        Read rows from self.f, skipping as specified.\n\n        We distinguish buffer_rows (the first <= n lines)\n        from the rows returned to detect_colspecs because\n        it's simpler to leave the other locations with\n        skiprows logic alone than to modify them to deal\n        with the fact we skipped some rows here as well.\n\n        Parameters\n        ----------\n        n : int\n            Number of rows to read from self.f, not counting\n            rows that are skipped.\n        skiprows: set, optional\n            Indices of rows to skip.\n\n        Returns\n        -------\n        detect_rows : list of str\n            A list containing the rows to read.\n\n        \"\"\"\n        if skiprows is None:\n            skiprows = set()\n        buffer_rows = []\n        detect_rows = []\n        for i, row in enumerate(self.f):\n            if i not in skiprows:\n                detect_rows.append(row)\n            buffer_rows.append(row)\n            if len(detect_rows) >= n:\n                break\n        self.buffer = iter(buffer_rows)\n        return detect_rows\n\n    def detect_colspecs(self, n=100, skiprows=None):\n        # Regex escape the delimiters\n        delimiters = ''.join([r'\\%s' % x for x in self.delimiter])\n        pattern = re.compile('([^%s]+)' % delimiters)\n        rows = self.get_rows(n, skiprows)\n        if not rows:\n            raise EmptyDataError(\"No rows from which to infer column width\")\n        max_len = max(map(len, rows))\n        mask = np.zeros(max_len + 1, dtype=int)\n        if self.comment is not None:\n            rows = [row.partition(self.comment)[0] for row in rows]\n        for row in rows:\n            for m in pattern.finditer(row):\n                mask[m.start():m.end()] = 1\n        shifted = np.roll(mask, 1)\n        shifted[0] = 0\n        edges = np.where((mask ^ shifted) == 1)[0]\n        edge_pairs = list(zip(edges[::2], edges[1::2]))\n        return edge_pairs\n\n    def __next__(self):\n        if self.buffer is not None:\n            try:\n                line = next(self.buffer)\n            except StopIteration:\n                self.buffer = None\n                line = next(self.f)\n        else:\n            line = next(self.f)\n        # Note: 'colspecs' is a sequence of half-open intervals.\n        return [line[fromm:to].strip(self.delimiter)\n                for (fromm, to) in self.colspecs]\n\n\nclass FixedWidthFieldParser(PythonParser):\n    \"\"\"\n    Specialization that Converts fixed-width fields into DataFrames.\n    See PythonParser for details.\n    \"\"\"\n\n    def __init__(self, f, **kwds):\n        # Support iterators, convert to a list.\n        self.colspecs = kwds.pop('colspecs')\n        PythonParser.__init__(self, f, **kwds)\n\n    def _make_reader(self, f):\n        self.data = FixedWidthReader(f, self.colspecs, self.delimiter,\n                                     self.comment, self.skiprows)\n"
    },
    {
      "filename": "pandas/tests/io/parser/common.py",
      "content": "# -*- coding: utf-8 -*-\n\nimport csv\nimport os\nimport platform\nimport codecs\n\nimport re\nimport sys\nfrom datetime import datetime\n\nimport pytest\nimport numpy as np\nfrom pandas._libs.lib import Timestamp\n\nimport pandas as pd\nimport pandas.util.testing as tm\nfrom pandas import DataFrame, Series, Index, MultiIndex\nfrom pandas import compat\nfrom pandas.compat import (StringIO, BytesIO, PY3,\n                           range, lrange, u)\nfrom pandas.errors import DtypeWarning, EmptyDataError, ParserError\nfrom pandas.io.common import URLError\nfrom pandas.io.parsers import TextFileReader, TextParser\n\n\nclass ParserTests(object):\n    \"\"\"\n    Want to be able to test either C+Cython or Python+Cython parsers\n    \"\"\"\n    data1 = \"\"\"index,A,B,C,D\nfoo,2,3,4,5\nbar,7,8,9,10\nbaz,12,13,14,15\nqux,12,13,14,15\nfoo2,12,13,14,15\nbar2,12,13,14,15\n\"\"\"\n\n    def test_empty_decimal_marker(self):\n        data = \"\"\"A|B|C\n1|2,334|5\n10|13|10.\n\"\"\"\n        # Parsers support only length-1 decimals\n        msg = 'Only length-1 decimal markers supported'\n        with tm.assert_raises_regex(ValueError, msg):\n            self.read_csv(StringIO(data), decimal='')\n\n    def test_bad_stream_exception(self):\n        # Issue 13652:\n        # This test validates that both python engine\n        # and C engine will raise UnicodeDecodeError instead of\n        # c engine raising ParserError and swallowing exception\n        # that caused read to fail.\n        handle = open(self.csv_shiftjs, \"rb\")\n        codec = codecs.lookup(\"utf-8\")\n        utf8 = codecs.lookup('utf-8')\n        # stream must be binary UTF8\n        stream = codecs.StreamRecoder(\n            handle, utf8.encode, utf8.decode, codec.streamreader,\n            codec.streamwriter)\n        if compat.PY3:\n            msg = \"'utf-8' codec can't decode byte\"\n        else:\n            msg = \"'utf8' codec can't decode byte\"\n        with tm.assert_raises_regex(UnicodeDecodeError, msg):\n            self.read_csv(stream)\n        stream.close()\n\n    def test_read_csv(self):\n        if not compat.PY3:\n            if compat.is_platform_windows():\n                prefix = u(\"file:///\")\n            else:\n                prefix = u(\"file://\")\n\n            fname = prefix + compat.text_type(self.csv1)\n            self.read_csv(fname, index_col=0, parse_dates=True)\n\n    def test_1000_sep(self):\n        data = \"\"\"A|B|C\n1|2,334|5\n10|13|10.\n\"\"\"\n        expected = DataFrame({\n            'A': [1, 10],\n            'B': [2334, 13],\n            'C': [5, 10.]\n        })\n\n        df = self.read_csv(StringIO(data), sep='|', thousands=',')\n        tm.assert_frame_equal(df, expected)\n\n        df = self.read_table(StringIO(data), sep='|', thousands=',')\n        tm.assert_frame_equal(df, expected)\n\n    def test_squeeze(self):\n        data = \"\"\"\\\na,1\nb,2\nc,3\n\"\"\"\n        idx = Index(['a', 'b', 'c'], name=0)\n        expected = Series([1, 2, 3], name=1, index=idx)\n        result = self.read_table(StringIO(data), sep=',', index_col=0,\n                                 header=None, squeeze=True)\n        assert isinstance(result, Series)\n        tm.assert_series_equal(result, expected)\n\n    def test_squeeze_no_view(self):\n        # see gh-8217\n        # Series should not be a view\n        data = \"\"\"time,data\\n0,10\\n1,11\\n2,12\\n4,14\\n5,15\\n3,13\"\"\"\n        result = self.read_csv(StringIO(data), index_col='time', squeeze=True)\n        assert not result._is_view\n\n    def test_malformed(self):\n        # see gh-6607\n\n        # all\n        data = \"\"\"ignore\nA,B,C\n1,2,3 # comment\n1,2,3,4,5\n2,3,4\n\"\"\"\n        msg = 'Expected 3 fields in line 4, saw 5'\n        with tm.assert_raises_regex(Exception, msg):\n            self.read_table(StringIO(data), sep=',',\n                            header=1, comment='#')\n\n        # first chunk\n        data = \"\"\"ignore\nA,B,C\nskip\n1,2,3\n3,5,10 # comment\n1,2,3,4,5\n2,3,4\n\"\"\"\n        msg = 'Expected 3 fields in line 6, saw 5'\n        with tm.assert_raises_regex(Exception, msg):\n            it = self.read_table(StringIO(data), sep=',',\n                                 header=1, comment='#',\n                                 iterator=True, chunksize=1,\n                                 skiprows=[2])\n            it.read(5)\n\n        # middle chunk\n        data = \"\"\"ignore\nA,B,C\nskip\n1,2,3\n3,5,10 # comment\n1,2,3,4,5\n2,3,4\n\"\"\"\n        msg = 'Expected 3 fields in line 6, saw 5'\n        with tm.assert_raises_regex(Exception, msg):\n            it = self.read_table(StringIO(data), sep=',', header=1,\n                                 comment='#', iterator=True, chunksize=1,\n                                 skiprows=[2])\n            it.read(3)\n\n        # last chunk\n        data = \"\"\"ignore\nA,B,C\nskip\n1,2,3\n3,5,10 # comment\n1,2,3,4,5\n2,3,4\n\"\"\"\n        msg = 'Expected 3 fields in line 6, saw 5'\n        with tm.assert_raises_regex(Exception, msg):\n            it = self.read_table(StringIO(data), sep=',', header=1,\n                                 comment='#', iterator=True, chunksize=1,\n                                 skiprows=[2])\n            it.read()\n\n        # skipfooter is not supported with the C parser yet\n        if self.engine == 'python':\n            # skipfooter\n            data = \"\"\"ignore\nA,B,C\n1,2,3 # comment\n1,2,3,4,5\n2,3,4\nfooter\n\"\"\"\n            msg = 'Expected 3 fields in line 4, saw 5'\n            with tm.assert_raises_regex(Exception, msg):\n                self.read_table(StringIO(data), sep=',',\n                                header=1, comment='#',\n                                skipfooter=1)\n\n    def test_quoting(self):\n        bad_line_small = \"\"\"printer\\tresult\\tvariant_name\nKlosterdruckerei\\tKlosterdruckerei <Salem> (1611-1804)\\tMuller, Jacob\nKlosterdruckerei\\tKlosterdruckerei <Salem> (1611-1804)\\tMuller, Jakob\nKlosterdruckerei\\tKlosterdruckerei <Kempten> (1609-1805)\\t\"Furststiftische Hofdruckerei,  <Kempten\"\"\nKlosterdruckerei\\tKlosterdruckerei <Kempten> (1609-1805)\\tGaller, Alois\nKlosterdruckerei\\tKlosterdruckerei <Kempten> (1609-1805)\\tHochfurstliche Buchhandlung <Kempten>\"\"\"  # noqa\n        pytest.raises(Exception, self.read_table, StringIO(bad_line_small),\n                      sep='\\t')\n\n        good_line_small = bad_line_small + '\"'\n        df = self.read_table(StringIO(good_line_small), sep='\\t')\n        assert len(df) == 3\n\n    def test_unnamed_columns(self):\n        data = \"\"\"A,B,C,,\n1,2,3,4,5\n6,7,8,9,10\n11,12,13,14,15\n\"\"\"\n        expected = np.array([[1, 2, 3, 4, 5],\n                             [6, 7, 8, 9, 10],\n                             [11, 12, 13, 14, 15]], dtype=np.int64)\n        df = self.read_table(StringIO(data), sep=',')\n        tm.assert_almost_equal(df.values, expected)\n        tm.assert_index_equal(df.columns,\n                              Index(['A', 'B', 'C', 'Unnamed: 3',\n                                     'Unnamed: 4']))\n\n    def test_csv_mixed_type(self):\n        data = \"\"\"A,B,C\na,1,2\nb,3,4\nc,4,5\n\"\"\"\n        expected = DataFrame({'A': ['a', 'b', 'c'],\n                              'B': [1, 3, 4],\n                              'C': [2, 4, 5]})\n        out = self.read_csv(StringIO(data))\n        tm.assert_frame_equal(out, expected)\n\n    def test_read_csv_dataframe(self):\n        df = self.read_csv(self.csv1, index_col=0, parse_dates=True)\n        df2 = self.read_table(self.csv1, sep=',', index_col=0,\n                              parse_dates=True)\n        tm.assert_index_equal(df.columns, pd.Index(['A', 'B', 'C', 'D']))\n        assert df.index.name == 'index'\n        assert isinstance(\n            df.index[0], (datetime, np.datetime64, Timestamp))\n        assert df.values.dtype == np.float64\n        tm.assert_frame_equal(df, df2)\n\n    def test_read_csv_no_index_name(self):\n        df = self.read_csv(self.csv2, index_col=0, parse_dates=True)\n        df2 = self.read_table(self.csv2, sep=',', index_col=0,\n                              parse_dates=True)\n        tm.assert_index_equal(df.columns,\n                              pd.Index(['A', 'B', 'C', 'D', 'E']))\n        assert isinstance(df.index[0], (datetime, np.datetime64, Timestamp))\n        assert df.loc[:, ['A', 'B', 'C', 'D']].values.dtype == np.float64\n        tm.assert_frame_equal(df, df2)\n\n    def test_read_table_unicode(self):\n        fin = BytesIO(u('\\u0141aski, Jan;1').encode('utf-8'))\n        df1 = self.read_table(fin, sep=\";\", encoding=\"utf-8\", header=None)\n        assert isinstance(df1[0].values[0], compat.text_type)\n\n    def test_read_table_wrong_num_columns(self):\n        # too few!\n        data = \"\"\"A,B,C,D,E,F\n1,2,3,4,5,6\n6,7,8,9,10,11,12\n11,12,13,14,15,16\n\"\"\"\n        pytest.raises(ValueError, self.read_csv, StringIO(data))\n\n    def test_read_duplicate_index_explicit(self):\n        data = \"\"\"index,A,B,C,D\nfoo,2,3,4,5\nbar,7,8,9,10\nbaz,12,13,14,15\nqux,12,13,14,15\nfoo,12,13,14,15\nbar,12,13,14,15\n\"\"\"\n\n        result = self.read_csv(StringIO(data), index_col=0)\n        expected = self.read_csv(StringIO(data)).set_index(\n            'index', verify_integrity=False)\n        tm.assert_frame_equal(result, expected)\n\n        result = self.read_table(StringIO(data), sep=',', index_col=0)\n        expected = self.read_table(StringIO(data), sep=',', ).set_index(\n            'index', verify_integrity=False)\n        tm.assert_frame_equal(result, expected)\n\n    def test_read_duplicate_index_implicit(self):\n        data = \"\"\"A,B,C,D\nfoo,2,3,4,5\nbar,7,8,9,10\nbaz,12,13,14,15\nqux,12,13,14,15\nfoo,12,13,14,15\nbar,12,13,14,15\n\"\"\"\n\n        # make sure an error isn't thrown\n        self.read_csv(StringIO(data))\n        self.read_table(StringIO(data), sep=',')\n\n    def test_parse_bools(self):\n        data = \"\"\"A,B\nTrue,1\nFalse,2\nTrue,3\n\"\"\"\n        data = self.read_csv(StringIO(data))\n        assert data['A'].dtype == np.bool_\n\n        data = \"\"\"A,B\nYES,1\nno,2\nyes,3\nNo,3\nYes,3\n\"\"\"\n        data = self.read_csv(StringIO(data),\n                             true_values=['yes', 'Yes', 'YES'],\n                             false_values=['no', 'NO', 'No'])\n        assert data['A'].dtype == np.bool_\n\n        data = \"\"\"A,B\nTRUE,1\nFALSE,2\nTRUE,3\n\"\"\"\n        data = self.read_csv(StringIO(data))\n        assert data['A'].dtype == np.bool_\n\n        data = \"\"\"A,B\nfoo,bar\nbar,foo\"\"\"\n        result = self.read_csv(StringIO(data), true_values=['foo'],\n                               false_values=['bar'])\n        expected = DataFrame({'A': [True, False], 'B': [False, True]})\n        tm.assert_frame_equal(result, expected)\n\n    def test_int_conversion(self):\n        data = \"\"\"A,B\n1.0,1\n2.0,2\n3.0,3\n\"\"\"\n        data = self.read_csv(StringIO(data))\n        assert data['A'].dtype == np.float64\n        assert data['B'].dtype == np.int64\n\n    def test_read_nrows(self):\n        expected = self.read_csv(StringIO(self.data1))[:3]\n\n        df = self.read_csv(StringIO(self.data1), nrows=3)\n        tm.assert_frame_equal(df, expected)\n\n        # see gh-10476\n        df = self.read_csv(StringIO(self.data1), nrows=3.0)\n        tm.assert_frame_equal(df, expected)\n\n        msg = r\"'nrows' must be an integer >=0\"\n\n        with tm.assert_raises_regex(ValueError, msg):\n            self.read_csv(StringIO(self.data1), nrows=1.2)\n\n        with tm.assert_raises_regex(ValueError, msg):\n            self.read_csv(StringIO(self.data1), nrows='foo')\n\n        with tm.assert_raises_regex(ValueError, msg):\n            self.read_csv(StringIO(self.data1), nrows=-1)\n\n    def test_read_chunksize(self):\n        reader = self.read_csv(StringIO(self.data1), index_col=0, chunksize=2)\n        df = self.read_csv(StringIO(self.data1), index_col=0)\n\n        chunks = list(reader)\n\n        tm.assert_frame_equal(chunks[0], df[:2])\n        tm.assert_frame_equal(chunks[1], df[2:4])\n        tm.assert_frame_equal(chunks[2], df[4:])\n\n        # with invalid chunksize value:\n        msg = r\"'chunksize' must be an integer >=1\"\n\n        with tm.assert_raises_regex(ValueError, msg):\n            self.read_csv(StringIO(self.data1), chunksize=1.3)\n\n        with tm.assert_raises_regex(ValueError, msg):\n            self.read_csv(StringIO(self.data1), chunksize='foo')\n\n        with tm.assert_raises_regex(ValueError, msg):\n            self.read_csv(StringIO(self.data1), chunksize=0)\n\n    def test_read_chunksize_and_nrows(self):\n\n        # gh-15755\n        # With nrows\n        reader = self.read_csv(StringIO(self.data1), index_col=0,\n                               chunksize=2, nrows=5)\n        df = self.read_csv(StringIO(self.data1), index_col=0, nrows=5)\n\n        tm.assert_frame_equal(pd.concat(reader), df)\n\n        # chunksize > nrows\n        reader = self.read_csv(StringIO(self.data1), index_col=0,\n                               chunksize=8, nrows=5)\n        df = self.read_csv(StringIO(self.data1), index_col=0, nrows=5)\n\n        tm.assert_frame_equal(pd.concat(reader), df)\n\n        # with changing \"size\":\n        reader = self.read_csv(StringIO(self.data1), index_col=0,\n                               chunksize=8, nrows=5)\n        df = self.read_csv(StringIO(self.data1), index_col=0, nrows=5)\n\n        tm.assert_frame_equal(reader.get_chunk(size=2), df.iloc[:2])\n        tm.assert_frame_equal(reader.get_chunk(size=4), df.iloc[2:5])\n        with pytest.raises(StopIteration):\n            reader.get_chunk(size=3)\n\n    def test_read_chunksize_named(self):\n        reader = self.read_csv(\n            StringIO(self.data1), index_col='index', chunksize=2)\n        df = self.read_csv(StringIO(self.data1), index_col='index')\n\n        chunks = list(reader)\n\n        tm.assert_frame_equal(chunks[0], df[:2])\n        tm.assert_frame_equal(chunks[1], df[2:4])\n        tm.assert_frame_equal(chunks[2], df[4:])\n\n    def test_get_chunk_passed_chunksize(self):\n        data = \"\"\"A,B,C\n1,2,3\n4,5,6\n7,8,9\n1,2,3\"\"\"\n        result = self.read_csv(StringIO(data), chunksize=2)\n\n        piece = result.get_chunk()\n        assert len(piece) == 2\n\n    def test_read_chunksize_generated_index(self):\n        # GH 12185\n        reader = self.read_csv(StringIO(self.data1), chunksize=2)\n        df = self.read_csv(StringIO(self.data1))\n\n        tm.assert_frame_equal(pd.concat(reader), df)\n\n        reader = self.read_csv(StringIO(self.data1), chunksize=2, index_col=0)\n        df = self.read_csv(StringIO(self.data1), index_col=0)\n\n        tm.assert_frame_equal(pd.concat(reader), df)\n\n    def test_read_text_list(self):\n        data = \"\"\"A,B,C\\nfoo,1,2,3\\nbar,4,5,6\"\"\"\n        as_list = [['A', 'B', 'C'], ['foo', '1', '2', '3'], ['bar',\n                                                             '4', '5', '6']]\n        df = self.read_csv(StringIO(data), index_col=0)\n\n        parser = TextParser(as_list, index_col=0, chunksize=2)\n        chunk = parser.read(None)\n\n        tm.assert_frame_equal(chunk, df)\n\n    def test_iterator(self):\n        # See gh-6607\n        reader = self.read_csv(StringIO(self.data1), index_col=0,\n                               iterator=True)\n        df = self.read_csv(StringIO(self.data1), index_col=0)\n\n        chunk = reader.read(3)\n        tm.assert_frame_equal(chunk, df[:3])\n\n        last_chunk = reader.read(5)\n        tm.assert_frame_equal(last_chunk, df[3:])\n\n        # pass list\n        lines = list(csv.reader(StringIO(self.data1)))\n        parser = TextParser(lines, index_col=0, chunksize=2)\n\n        df = self.read_csv(StringIO(self.data1), index_col=0)\n\n        chunks = list(parser)\n        tm.assert_frame_equal(chunks[0], df[:2])\n        tm.assert_frame_equal(chunks[1], df[2:4])\n        tm.assert_frame_equal(chunks[2], df[4:])\n\n        # pass skiprows\n        parser = TextParser(lines, index_col=0, chunksize=2, skiprows=[1])\n        chunks = list(parser)\n        tm.assert_frame_equal(chunks[0], df[1:3])\n\n        treader = self.read_table(StringIO(self.data1), sep=',', index_col=0,\n                                  iterator=True)\n        assert isinstance(treader, TextFileReader)\n\n        # gh-3967: stopping iteration when chunksize is specified\n        data = \"\"\"A,B,C\nfoo,1,2,3\nbar,4,5,6\nbaz,7,8,9\n\"\"\"\n        reader = self.read_csv(StringIO(data), iterator=True)\n        result = list(reader)\n        expected = DataFrame(dict(A=[1, 4, 7], B=[2, 5, 8], C=[\n            3, 6, 9]), index=['foo', 'bar', 'baz'])\n        tm.assert_frame_equal(result[0], expected)\n\n        # chunksize = 1\n        reader = self.read_csv(StringIO(data), chunksize=1)\n        result = list(reader)\n        expected = DataFrame(dict(A=[1, 4, 7], B=[2, 5, 8], C=[\n            3, 6, 9]), index=['foo', 'bar', 'baz'])\n        assert len(result) == 3\n        tm.assert_frame_equal(pd.concat(result), expected)\n\n        # skipfooter is not supported with the C parser yet\n        if self.engine == 'python':\n            # test bad parameter (skipfooter)\n            reader = self.read_csv(StringIO(self.data1), index_col=0,\n                                   iterator=True, skipfooter=1)\n            pytest.raises(ValueError, reader.read, 3)\n\n    def test_pass_names_with_index(self):\n        lines = self.data1.split('\\n')\n        no_header = '\\n'.join(lines[1:])\n\n        # regular index\n        names = ['index', 'A', 'B', 'C', 'D']\n        df = self.read_csv(StringIO(no_header), index_col=0, names=names)\n        expected = self.read_csv(StringIO(self.data1), index_col=0)\n        tm.assert_frame_equal(df, expected)\n\n        # multi index\n        data = \"\"\"index1,index2,A,B,C,D\nfoo,one,2,3,4,5\nfoo,two,7,8,9,10\nfoo,three,12,13,14,15\nbar,one,12,13,14,15\nbar,two,12,13,14,15\n\"\"\"\n        lines = data.split('\\n')\n        no_header = '\\n'.join(lines[1:])\n        names = ['index1', 'index2', 'A', 'B', 'C', 'D']\n        df = self.read_csv(StringIO(no_header), index_col=[0, 1],\n                           names=names)\n        expected = self.read_csv(StringIO(data), index_col=[0, 1])\n        tm.assert_frame_equal(df, expected)\n\n        df = self.read_csv(StringIO(data), index_col=['index1', 'index2'])\n        tm.assert_frame_equal(df, expected)\n\n    def test_multi_index_no_level_names(self):\n        data = \"\"\"index1,index2,A,B,C,D\nfoo,one,2,3,4,5\nfoo,two,7,8,9,10\nfoo,three,12,13,14,15\nbar,one,12,13,14,15\nbar,two,12,13,14,15\n\"\"\"\n\n        data2 = \"\"\"A,B,C,D\nfoo,one,2,3,4,5\nfoo,two,7,8,9,10\nfoo,three,12,13,14,15\nbar,one,12,13,14,15\nbar,two,12,13,14,15\n\"\"\"\n\n        lines = data.split('\\n')\n        no_header = '\\n'.join(lines[1:])\n        names = ['A', 'B', 'C', 'D']\n\n        df = self.read_csv(StringIO(no_header), index_col=[0, 1],\n                           header=None, names=names)\n        expected = self.read_csv(StringIO(data), index_col=[0, 1])\n        tm.assert_frame_equal(df, expected, check_names=False)\n\n        # 2 implicit first cols\n        df2 = self.read_csv(StringIO(data2))\n        tm.assert_frame_equal(df2, df)\n\n        # reverse order of index\n        df = self.read_csv(StringIO(no_header), index_col=[1, 0], names=names,\n                           header=None)\n        expected = self.read_csv(StringIO(data), index_col=[1, 0])\n        tm.assert_frame_equal(df, expected, check_names=False)\n\n    def test_multi_index_blank_df(self):\n        # GH 14545\n        data = \"\"\"a,b\n\"\"\"\n        df = self.read_csv(StringIO(data), header=[0])\n        expected = DataFrame(columns=['a', 'b'])\n        tm.assert_frame_equal(df, expected)\n        round_trip = self.read_csv(StringIO(\n            expected.to_csv(index=False)), header=[0])\n        tm.assert_frame_equal(round_trip, expected)\n\n        data_multiline = \"\"\"a,b\nc,d\n\"\"\"\n        df2 = self.read_csv(StringIO(data_multiline), header=[0, 1])\n        cols = MultiIndex.from_tuples([('a', 'c'), ('b', 'd')])\n        expected2 = DataFrame(columns=cols)\n        tm.assert_frame_equal(df2, expected2)\n        round_trip = self.read_csv(StringIO(\n            expected2.to_csv(index=False)), header=[0, 1])\n        tm.assert_frame_equal(round_trip, expected2)\n\n    def test_no_unnamed_index(self):\n        data = \"\"\" id c0 c1 c2\n0 1 0 a b\n1 2 0 c d\n2 2 2 e f\n\"\"\"\n        df = self.read_table(StringIO(data), sep=' ')\n        assert df.index.name is None\n\n    def test_read_csv_parse_simple_list(self):\n        text = \"\"\"foo\nbar baz\nqux foo\nfoo\nbar\"\"\"\n        df = self.read_csv(StringIO(text), header=None)\n        expected = DataFrame({0: ['foo', 'bar baz', 'qux foo',\n                                  'foo', 'bar']})\n        tm.assert_frame_equal(df, expected)\n\n    @tm.network\n    def test_url(self):\n        # HTTP(S)\n        url = ('https://raw.github.com/pandas-dev/pandas/master/'\n               'pandas/tests/io/parser/data/salaries.csv')\n        url_table = self.read_table(url)\n        dirpath = tm.get_data_path()\n        localtable = os.path.join(dirpath, 'salaries.csv')\n        local_table = self.read_table(localtable)\n        tm.assert_frame_equal(url_table, local_table)\n        # TODO: ftp testing\n\n    @pytest.mark.slow\n    def test_file(self):\n        dirpath = tm.get_data_path()\n        localtable = os.path.join(dirpath, 'salaries.csv')\n        local_table = self.read_table(localtable)\n\n        try:\n            url_table = self.read_table('file://localhost/' + localtable)\n        except URLError:\n            # fails on some systems\n            pytest.skip(\"failing on %s\" %\n                        ' '.join(platform.uname()).strip())\n\n        tm.assert_frame_equal(url_table, local_table)\n\n    def test_path_pathlib(self):\n        df = tm.makeDataFrame()\n        result = tm.round_trip_pathlib(df.to_csv,\n                                       lambda p: self.read_csv(p, index_col=0))\n        tm.assert_frame_equal(df, result)\n\n    def test_path_localpath(self):\n        df = tm.makeDataFrame()\n        result = tm.round_trip_localpath(\n            df.to_csv,\n            lambda p: self.read_csv(p, index_col=0))\n        tm.assert_frame_equal(df, result)\n\n    def test_nonexistent_path(self):\n        # gh-2428: pls no segfault\n        # gh-14086: raise more helpful FileNotFoundError\n        path = '%s.csv' % tm.rands(10)\n        pytest.raises(compat.FileNotFoundError, self.read_csv, path)\n\n    def test_missing_trailing_delimiters(self):\n        data = \"\"\"A,B,C,D\n1,2,3,4\n1,3,3,\n1,4,5\"\"\"\n        result = self.read_csv(StringIO(data))\n        assert result['D'].isnull()[1:].all()\n\n    def test_skipinitialspace(self):\n        s = ('\"09-Apr-2012\", \"01:10:18.300\", 2456026.548822908, 12849, '\n             '1.00361,  1.12551, 330.65659, 0355626618.16711,  73.48821, '\n             '314.11625,  1917.09447,   179.71425,  80.000, 240.000, -350,  '\n             '70.06056, 344.98370, 1,   1, -0.689265, -0.692787,  '\n             '0.212036,    14.7674,   41.605,   -9999.0,   -9999.0,   '\n             '-9999.0,   -9999.0,   -9999.0,  -9999.0, 000, 012, 128')\n\n        sfile = StringIO(s)\n        # it's 33 columns\n        result = self.read_csv(sfile, names=lrange(33), na_values=['-9999.0'],\n                               header=None, skipinitialspace=True)\n        assert pd.isnull(result.iloc[0, 29])\n\n    def test_utf16_bom_skiprows(self):\n        # #2298\n        data = u(\"\"\"skip this\nskip this too\nA\\tB\\tC\n1\\t2\\t3\n4\\t5\\t6\"\"\")\n\n        data2 = u(\"\"\"skip this\nskip this too\nA,B,C\n1,2,3\n4,5,6\"\"\")\n\n        path = '__%s__.csv' % tm.rands(10)\n\n        with tm.ensure_clean(path) as path:\n            for sep, dat in [('\\t', data), (',', data2)]:\n                for enc in ['utf-16', 'utf-16le', 'utf-16be']:\n                    bytes = dat.encode(enc)\n                    with open(path, 'wb') as f:\n                        f.write(bytes)\n\n                    s = BytesIO(dat.encode('utf-8'))\n                    if compat.PY3:\n                        # somewhat False since the code never sees bytes\n                        from io import TextIOWrapper\n                        s = TextIOWrapper(s, encoding='utf-8')\n\n                    result = self.read_csv(path, encoding=enc, skiprows=2,\n                                           sep=sep)\n                    expected = self.read_csv(s, encoding='utf-8', skiprows=2,\n                                             sep=sep)\n                    s.close()\n\n                    tm.assert_frame_equal(result, expected)\n\n    def test_utf16_example(self):\n        path = tm.get_data_path('utf16_ex.txt')\n\n        # it works! and is the right length\n        result = self.read_table(path, encoding='utf-16')\n        assert len(result) == 50\n\n        if not compat.PY3:\n            buf = BytesIO(open(path, 'rb').read())\n            result = self.read_table(buf, encoding='utf-16')\n            assert len(result) == 50\n\n    def test_unicode_encoding(self):\n        pth = tm.get_data_path('unicode_series.csv')\n\n        result = self.read_csv(pth, header=None, encoding='latin-1')\n        result = result.set_index(0)\n\n        got = result[1][1632]\n        expected = u('\\xc1 k\\xf6ldum klaka (Cold Fever) (1994)')\n\n        assert got == expected\n\n    def test_trailing_delimiters(self):\n        # #2442. grumble grumble\n        data = \"\"\"A,B,C\n1,2,3,\n4,5,6,\n7,8,9,\"\"\"\n        result = self.read_csv(StringIO(data), index_col=False)\n\n        expected = DataFrame({'A': [1, 4, 7], 'B': [2, 5, 8],\n                              'C': [3, 6, 9]})\n\n        tm.assert_frame_equal(result, expected)\n\n    def test_escapechar(self):\n        # http://stackoverflow.com/questions/13824840/feature-request-for-\n        # pandas-read-csv\n        data = '''SEARCH_TERM,ACTUAL_URL\n\"bra tv bord\",\"http://www.ikea.com/se/sv/catalog/categories/departments/living_room/10475/?se%7cps%7cnonbranded%7cvardagsrum%7cgoogle%7ctv_bord\"\n\"tv p\\xc3\\xa5 hjul\",\"http://www.ikea.com/se/sv/catalog/categories/departments/living_room/10475/?se%7cps%7cnonbranded%7cvardagsrum%7cgoogle%7ctv_bord\"\n\"SLAGBORD, \\\\\"Bergslagen\\\\\", IKEA:s 1700-tals serie\",\"http://www.ikea.com/se/sv/catalog/categories/departments/living_room/10475/?se%7cps%7cnonbranded%7cvardagsrum%7cgoogle%7ctv_bord\"'''  # noqa\n\n        result = self.read_csv(StringIO(data), escapechar='\\\\',\n                               quotechar='\"', encoding='utf-8')\n        assert result['SEARCH_TERM'][2] == ('SLAGBORD, \"Bergslagen\", '\n                                            'IKEA:s 1700-tals serie')\n        tm.assert_index_equal(result.columns,\n                              Index(['SEARCH_TERM', 'ACTUAL_URL']))\n\n    def test_int64_min_issues(self):\n        # #2599\n        data = 'A,B\\n0,0\\n0,'\n\n        result = self.read_csv(StringIO(data))\n        expected = DataFrame({'A': [0, 0], 'B': [0, np.nan]})\n\n        tm.assert_frame_equal(result, expected)\n\n    def test_parse_integers_above_fp_precision(self):\n        data = \"\"\"Numbers\n17007000002000191\n17007000002000191\n17007000002000191\n17007000002000191\n17007000002000192\n17007000002000192\n17007000002000192\n17007000002000192\n17007000002000192\n17007000002000194\"\"\"\n\n        result = self.read_csv(StringIO(data))\n        expected = DataFrame({'Numbers': [17007000002000191,\n                                          17007000002000191,\n                                          17007000002000191,\n                                          17007000002000191,\n                                          17007000002000192,\n                                          17007000002000192,\n                                          17007000002000192,\n                                          17007000002000192,\n                                          17007000002000192,\n                                          17007000002000194]})\n\n        assert np.array_equal(result['Numbers'], expected['Numbers'])\n\n    def test_chunks_have_consistent_numerical_type(self):\n        integers = [str(i) for i in range(499999)]\n        data = \"a\\n\" + \"\\n\".join(integers + [\"1.0\", \"2.0\"] + integers)\n\n        with tm.assert_produces_warning(False):\n            df = self.read_csv(StringIO(data))\n        # Assert that types were coerced.\n        assert type(df.a[0]) is np.float64\n        assert df.a.dtype == np.float\n\n    def test_warn_if_chunks_have_mismatched_type(self):\n        warning_type = False\n        integers = [str(i) for i in range(499999)]\n        data = \"a\\n\" + \"\\n\".join(integers + ['a', 'b'] + integers)\n\n        # see gh-3866: if chunks are different types and can't\n        # be coerced using numerical types, then issue warning.\n        if self.engine == 'c' and self.low_memory:\n            warning_type = DtypeWarning\n\n        with tm.assert_produces_warning(warning_type):\n            df = self.read_csv(StringIO(data))\n        assert df.a.dtype == np.object\n\n    def test_integer_overflow_bug(self):\n        # see gh-2601\n        data = \"65248E10 11\\n55555E55 22\\n\"\n\n        result = self.read_csv(StringIO(data), header=None, sep=' ')\n        assert result[0].dtype == np.float64\n\n        result = self.read_csv(StringIO(data), header=None, sep=r'\\s+')\n        assert result[0].dtype == np.float64\n\n    def test_catch_too_many_names(self):\n        # see gh-5156\n        data = \"\"\"\\\n1,2,3\n4,,6\n7,8,9\n10,11,12\\n\"\"\"\n        pytest.raises(ValueError, self.read_csv, StringIO(data),\n                      header=0, names=['a', 'b', 'c', 'd'])\n\n    def test_ignore_leading_whitespace(self):\n        # see gh-3374, gh-6607\n        data = ' a b c\\n 1 2 3\\n 4 5 6\\n 7 8 9'\n        result = self.read_table(StringIO(data), sep=r'\\s+')\n        expected = DataFrame({'a': [1, 4, 7], 'b': [2, 5, 8], 'c': [3, 6, 9]})\n        tm.assert_frame_equal(result, expected)\n\n    def test_chunk_begins_with_newline_whitespace(self):\n        # see gh-10022\n        data = '\\n hello\\nworld\\n'\n        result = self.read_csv(StringIO(data), header=None)\n        assert len(result) == 2\n\n        # see gh-9735: this issue is C parser-specific (bug when\n        # parsing whitespace and characters at chunk boundary)\n        if self.engine == 'c':\n            chunk1 = 'a' * (1024 * 256 - 2) + '\\na'\n            chunk2 = '\\n a'\n            result = self.read_csv(StringIO(chunk1 + chunk2), header=None)\n            expected = DataFrame(['a' * (1024 * 256 - 2), 'a', ' a'])\n            tm.assert_frame_equal(result, expected)\n\n    def test_empty_with_index(self):\n        # see gh-10184\n        data = 'x,y'\n        result = self.read_csv(StringIO(data), index_col=0)\n        expected = DataFrame([], columns=['y'], index=Index([], name='x'))\n        tm.assert_frame_equal(result, expected)\n\n    def test_empty_with_multiindex(self):\n        # see gh-10467\n        data = 'x,y,z'\n        result = self.read_csv(StringIO(data), index_col=['x', 'y'])\n        expected = DataFrame([], columns=['z'],\n                             index=MultiIndex.from_arrays(\n                                 [[]] * 2, names=['x', 'y']))\n        tm.assert_frame_equal(result, expected, check_index_type=False)\n\n    def test_empty_with_reversed_multiindex(self):\n        data = 'x,y,z'\n        result = self.read_csv(StringIO(data), index_col=[1, 0])\n        expected = DataFrame([], columns=['z'],\n                             index=MultiIndex.from_arrays(\n                                 [[]] * 2, names=['y', 'x']))\n        tm.assert_frame_equal(result, expected, check_index_type=False)\n\n    def test_float_parser(self):\n        # see gh-9565\n        data = '45e-1,4.5,45.,inf,-inf'\n        result = self.read_csv(StringIO(data), header=None)\n        expected = DataFrame([[float(s) for s in data.split(',')]])\n        tm.assert_frame_equal(result, expected)\n\n    def test_scientific_no_exponent(self):\n        # see gh-12215\n        df = DataFrame.from_items([('w', ['2e']), ('x', ['3E']),\n                                   ('y', ['42e']), ('z', ['632E'])])\n        data = df.to_csv(index=False)\n        for prec in self.float_precision_choices:\n            df_roundtrip = self.read_csv(\n                StringIO(data), float_precision=prec)\n            tm.assert_frame_equal(df_roundtrip, df)\n\n    def test_int64_overflow(self):\n        data = \"\"\"ID\n00013007854817840016671868\n00013007854817840016749251\n00013007854817840016754630\n00013007854817840016781876\n00013007854817840017028824\n00013007854817840017963235\n00013007854817840018860166\"\"\"\n\n        # 13007854817840016671868 > UINT64_MAX, so this\n        # will overflow and return object as the dtype.\n        result = self.read_csv(StringIO(data))\n        assert result['ID'].dtype == object\n\n        # 13007854817840016671868 > UINT64_MAX, so attempts\n        # to cast to either int64 or uint64 will result in\n        # an OverflowError being raised.\n        for conv in (np.int64, np.uint64):\n            pytest.raises(OverflowError, self.read_csv,\n                          StringIO(data), converters={'ID': conv})\n\n        # These numbers fall right inside the int64-uint64 range,\n        # so they should be parsed as string.\n        ui_max = np.iinfo(np.uint64).max\n        i_max = np.iinfo(np.int64).max\n        i_min = np.iinfo(np.int64).min\n\n        for x in [i_max, i_min, ui_max]:\n            result = self.read_csv(StringIO(str(x)), header=None)\n            expected = DataFrame([x])\n            tm.assert_frame_equal(result, expected)\n\n        # These numbers fall just outside the int64-uint64 range,\n        # so they should be parsed as string.\n        too_big = ui_max + 1\n        too_small = i_min - 1\n\n        for x in [too_big, too_small]:\n            result = self.read_csv(StringIO(str(x)), header=None)\n            expected = DataFrame([str(x)])\n            tm.assert_frame_equal(result, expected)\n\n        # No numerical dtype can hold both negative and uint64 values,\n        # so they should be cast as string.\n        data = '-1\\n' + str(2**63)\n        expected = DataFrame([str(-1), str(2**63)])\n        result = self.read_csv(StringIO(data), header=None)\n        tm.assert_frame_equal(result, expected)\n\n        data = str(2**63) + '\\n-1'\n        expected = DataFrame([str(2**63), str(-1)])\n        result = self.read_csv(StringIO(data), header=None)\n        tm.assert_frame_equal(result, expected)\n\n    def test_empty_with_nrows_chunksize(self):\n        # see gh-9535\n        expected = DataFrame([], columns=['foo', 'bar'])\n        result = self.read_csv(StringIO('foo,bar\\n'), nrows=10)\n        tm.assert_frame_equal(result, expected)\n\n        result = next(iter(self.read_csv(\n            StringIO('foo,bar\\n'), chunksize=10)))\n        tm.assert_frame_equal(result, expected)\n\n        with tm.assert_produces_warning(\n                FutureWarning, check_stacklevel=False):\n            result = self.read_csv(StringIO('foo,bar\\n'),\n                                   nrows=10, as_recarray=True)\n            result = DataFrame(result[2], columns=result[1],\n                               index=result[0])\n            tm.assert_frame_equal(DataFrame.from_records(\n                result), expected, check_index_type=False)\n\n        with tm.assert_produces_warning(\n                FutureWarning, check_stacklevel=False):\n            result = next(iter(self.read_csv(StringIO('foo,bar\\n'),\n                                             chunksize=10, as_recarray=True)))\n            result = DataFrame(result[2], columns=result[1], index=result[0])\n            tm.assert_frame_equal(DataFrame.from_records(result), expected,\n                                  check_index_type=False)\n\n    def test_eof_states(self):\n        # see gh-10728, gh-10548\n\n        # With skip_blank_lines = True\n        expected = DataFrame([[4, 5, 6]], columns=['a', 'b', 'c'])\n\n        # gh-10728: WHITESPACE_LINE\n        data = 'a,b,c\\n4,5,6\\n '\n        result = self.read_csv(StringIO(data))\n        tm.assert_frame_equal(result, expected)\n\n        # gh-10548: EAT_LINE_COMMENT\n        data = 'a,b,c\\n4,5,6\\n#comment'\n        result = self.read_csv(StringIO(data), comment='#')\n        tm.assert_frame_equal(result, expected)\n\n        # EAT_CRNL_NOP\n        data = 'a,b,c\\n4,5,6\\n\\r'\n        result = self.read_csv(StringIO(data))\n        tm.assert_frame_equal(result, expected)\n\n        # EAT_COMMENT\n        data = 'a,b,c\\n4,5,6#comment'\n        result = self.read_csv(StringIO(data), comment='#')\n        tm.assert_frame_equal(result, expected)\n\n        # SKIP_LINE\n        data = 'a,b,c\\n4,5,6\\nskipme'\n        result = self.read_csv(StringIO(data), skiprows=[2])\n        tm.assert_frame_equal(result, expected)\n\n        # With skip_blank_lines = False\n\n        # EAT_LINE_COMMENT\n        data = 'a,b,c\\n4,5,6\\n#comment'\n        result = self.read_csv(\n            StringIO(data), comment='#', skip_blank_lines=False)\n        expected = DataFrame([[4, 5, 6]], columns=['a', 'b', 'c'])\n        tm.assert_frame_equal(result, expected)\n\n        # IN_FIELD\n        data = 'a,b,c\\n4,5,6\\n '\n        result = self.read_csv(StringIO(data), skip_blank_lines=False)\n        expected = DataFrame(\n            [['4', 5, 6], [' ', None, None]], columns=['a', 'b', 'c'])\n        tm.assert_frame_equal(result, expected)\n\n        # EAT_CRNL\n        data = 'a,b,c\\n4,5,6\\n\\r'\n        result = self.read_csv(StringIO(data), skip_blank_lines=False)\n        expected = DataFrame(\n            [[4, 5, 6], [None, None, None]], columns=['a', 'b', 'c'])\n        tm.assert_frame_equal(result, expected)\n\n        # Should produce exceptions\n\n        # ESCAPED_CHAR\n        data = \"a,b,c\\n4,5,6\\n\\\\\"\n        pytest.raises(Exception, self.read_csv,\n                      StringIO(data), escapechar='\\\\')\n\n        # ESCAPE_IN_QUOTED_FIELD\n        data = 'a,b,c\\n4,5,6\\n\"\\\\'\n        pytest.raises(Exception, self.read_csv,\n                      StringIO(data), escapechar='\\\\')\n\n        # IN_QUOTED_FIELD\n        data = 'a,b,c\\n4,5,6\\n\"'\n        pytest.raises(Exception, self.read_csv,\n                      StringIO(data), escapechar='\\\\')\n\n    def test_uneven_lines_with_usecols(self):\n        # See gh-12203\n        csv = r\"\"\"a,b,c\n        0,1,2\n        3,4,5,6,7\n        8,9,10\n        \"\"\"\n\n        # make sure that an error is still thrown\n        # when the 'usecols' parameter is not provided\n        msg = r\"Expected \\d+ fields in line \\d+, saw \\d+\"\n        with tm.assert_raises_regex(ValueError, msg):\n            df = self.read_csv(StringIO(csv))\n\n        expected = DataFrame({\n            'a': [0, 3, 8],\n            'b': [1, 4, 9]\n        })\n\n        usecols = [0, 1]\n        df = self.read_csv(StringIO(csv), usecols=usecols)\n        tm.assert_frame_equal(df, expected)\n\n        usecols = ['a', 'b']\n        df = self.read_csv(StringIO(csv), usecols=usecols)\n        tm.assert_frame_equal(df, expected)\n\n    def test_read_empty_with_usecols(self):\n        # See gh-12493\n        names = ['Dummy', 'X', 'Dummy_2']\n        usecols = names[1:2]  # ['X']\n\n        # first, check to see that the response of\n        # parser when faced with no provided columns\n        # throws the correct error, with or without usecols\n        errmsg = \"No columns to parse from file\"\n\n        with tm.assert_raises_regex(EmptyDataError, errmsg):\n            self.read_csv(StringIO(''))\n\n        with tm.assert_raises_regex(EmptyDataError, errmsg):\n            self.read_csv(StringIO(''), usecols=usecols)\n\n        expected = DataFrame(columns=usecols, index=[0], dtype=np.float64)\n        df = self.read_csv(StringIO(',,'), names=names, usecols=usecols)\n        tm.assert_frame_equal(df, expected)\n\n        expected = DataFrame(columns=usecols)\n        df = self.read_csv(StringIO(''), names=names, usecols=usecols)\n        tm.assert_frame_equal(df, expected)\n\n    def test_trailing_spaces(self):\n        data = \"A B C  \\nrandom line with trailing spaces    \\nskip\\n1,2,3\\n1,2.,4.\\nrandom line with trailing tabs\\t\\t\\t\\n   \\n5.1,NaN,10.0\\n\"  # noqa\n        expected = DataFrame([[1., 2., 4.],\n                              [5.1, np.nan, 10.]])\n\n        # gh-8661, gh-8679: this should ignore six lines including\n        # lines with trailing whitespace and blank lines\n        df = self.read_csv(StringIO(data.replace(',', '  ')),\n                           header=None, delim_whitespace=True,\n                           skiprows=[0, 1, 2, 3, 5, 6], skip_blank_lines=True)\n        tm.assert_frame_equal(df, expected)\n        df = self.read_table(StringIO(data.replace(',', '  ')),\n                             header=None, delim_whitespace=True,\n                             skiprows=[0, 1, 2, 3, 5, 6],\n                             skip_blank_lines=True)\n        tm.assert_frame_equal(df, expected)\n\n        # gh-8983: test skipping set of rows after a row with trailing spaces\n        expected = DataFrame({\"A\": [1., 5.1], \"B\": [2., np.nan],\n                              \"C\": [4., 10]})\n        df = self.read_table(StringIO(data.replace(',', '  ')),\n                             delim_whitespace=True,\n                             skiprows=[1, 2, 3, 5, 6], skip_blank_lines=True)\n        tm.assert_frame_equal(df, expected)\n\n    def test_raise_on_sep_with_delim_whitespace(self):\n        # see gh-6607\n        data = 'a b c\\n1 2 3'\n        with tm.assert_raises_regex(ValueError,\n                                    'you can only specify one'):\n            self.read_table(StringIO(data), sep=r'\\s', delim_whitespace=True)\n\n    def test_single_char_leading_whitespace(self):\n        # see gh-9710\n        data = \"\"\"\\\nMyColumn\n   a\n   b\n   a\n   b\\n\"\"\"\n\n        expected = DataFrame({'MyColumn': list('abab')})\n\n        result = self.read_csv(StringIO(data), delim_whitespace=True,\n                               skipinitialspace=True)\n        tm.assert_frame_equal(result, expected)\n\n        result = self.read_csv(StringIO(data), skipinitialspace=True)\n        tm.assert_frame_equal(result, expected)\n\n    def test_empty_lines(self):\n        data = \"\"\"\\\nA,B,C\n1,2.,4.\n\n\n5.,NaN,10.0\n\n-70,.4,1\n\"\"\"\n        expected = np.array([[1., 2., 4.],\n                             [5., np.nan, 10.],\n                             [-70., .4, 1.]])\n        df = self.read_csv(StringIO(data))\n        tm.assert_numpy_array_equal(df.values, expected)\n        df = self.read_csv(StringIO(data.replace(',', '  ')), sep=r'\\s+')\n        tm.assert_numpy_array_equal(df.values, expected)\n        expected = np.array([[1., 2., 4.],\n                             [np.nan, np.nan, np.nan],\n                             [np.nan, np.nan, np.nan],\n                             [5., np.nan, 10.],\n                             [np.nan, np.nan, np.nan],\n                             [-70., .4, 1.]])\n        df = self.read_csv(StringIO(data), skip_blank_lines=False)\n        tm.assert_numpy_array_equal(df.values, expected)\n\n    def test_whitespace_lines(self):\n        data = \"\"\"\n\n\\t  \\t\\t\n  \\t\nA,B,C\n  \\t    1,2.,4.\n5.,NaN,10.0\n\"\"\"\n        expected = np.array([[1, 2., 4.],\n                             [5., np.nan, 10.]])\n        df = self.read_csv(StringIO(data))\n        tm.assert_numpy_array_equal(df.values, expected)\n\n    def test_regex_separator(self):\n        # see gh-6607\n        data = \"\"\"   A   B   C   D\na   1   2   3   4\nb   1   2   3   4\nc   1   2   3   4\n\"\"\"\n        df = self.read_table(StringIO(data), sep=r'\\s+')\n        expected = self.read_csv(StringIO(re.sub('[ ]+', ',', data)),\n                                 index_col=0)\n        assert expected.index.name is None\n        tm.assert_frame_equal(df, expected)\n\n        data = '    a b c\\n1 2 3 \\n4 5  6\\n 7 8 9'\n        result = self.read_table(StringIO(data), sep=r'\\s+')\n        expected = DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n                             columns=['a', 'b', 'c'])\n        tm.assert_frame_equal(result, expected)\n\n    @tm.capture_stdout\n    def test_verbose_import(self):\n        text = \"\"\"a,b,c,d\none,1,2,3\none,1,2,3\n,1,2,3\none,1,2,3\n,1,2,3\n,1,2,3\none,1,2,3\ntwo,1,2,3\"\"\"\n\n        # Engines are verbose in different ways.\n        self.read_csv(StringIO(text), verbose=True)\n        output = sys.stdout.getvalue()\n\n        if self.engine == 'c':\n            assert 'Tokenization took:' in output\n            assert 'Parser memory cleanup took:' in output\n        else:  # Python engine\n            assert output == 'Filled 3 NA values in column a\\n'\n\n        # Reset the stdout buffer.\n        sys.stdout = StringIO()\n\n        text = \"\"\"a,b,c,d\none,1,2,3\ntwo,1,2,3\nthree,1,2,3\nfour,1,2,3\nfive,1,2,3\n,1,2,3\nseven,1,2,3\neight,1,2,3\"\"\"\n\n        self.read_csv(StringIO(text), verbose=True, index_col=0)\n        output = sys.stdout.getvalue()\n\n        # Engines are verbose in different ways.\n        if self.engine == 'c':\n            assert 'Tokenization took:' in output\n            assert 'Parser memory cleanup took:' in output\n        else:  # Python engine\n            assert output == 'Filled 1 NA values in column a\\n'\n\n    def test_iteration_open_handle(self):\n        if PY3:\n            pytest.skip(\n                \"won't work in Python 3 {0}\".format(sys.version_info))\n\n        with tm.ensure_clean() as path:\n            with open(path, 'wb') as f:\n                f.write('AAA\\nBBB\\nCCC\\nDDD\\nEEE\\nFFF\\nGGG')\n\n            with open(path, 'rb') as f:\n                for line in f:\n                    if 'CCC' in line:\n                        break\n\n                if self.engine == 'c':\n                    pytest.raises(Exception, self.read_table,\n                                  f, squeeze=True, header=None)\n                else:\n                    result = self.read_table(f, squeeze=True, header=None)\n                    expected = Series(['DDD', 'EEE', 'FFF', 'GGG'], name=0)\n                    tm.assert_series_equal(result, expected)\n\n    def test_1000_sep_with_decimal(self):\n        data = \"\"\"A|B|C\n1|2,334.01|5\n10|13|10.\n\"\"\"\n        expected = DataFrame({\n            'A': [1, 10],\n            'B': [2334.01, 13],\n            'C': [5, 10.]\n        })\n\n        assert expected.A.dtype == 'int64'\n        assert expected.B.dtype == 'float'\n        assert expected.C.dtype == 'float'\n\n        df = self.read_csv(StringIO(data), sep='|', thousands=',', decimal='.')\n        tm.assert_frame_equal(df, expected)\n\n        df = self.read_table(StringIO(data), sep='|',\n                             thousands=',', decimal='.')\n        tm.assert_frame_equal(df, expected)\n\n        data_with_odd_sep = \"\"\"A|B|C\n1|2.334,01|5\n10|13|10,\n\"\"\"\n        df = self.read_csv(StringIO(data_with_odd_sep),\n                           sep='|', thousands='.', decimal=',')\n        tm.assert_frame_equal(df, expected)\n\n        df = self.read_table(StringIO(data_with_odd_sep),\n                             sep='|', thousands='.', decimal=',')\n        tm.assert_frame_equal(df, expected)\n\n    def test_euro_decimal_format(self):\n        data = \"\"\"Id;Number1;Number2;Text1;Text2;Number3\n1;1521,1541;187101,9543;ABC;poi;4,738797819\n2;121,12;14897,76;DEF;uyt;0,377320872\n3;878,158;108013,434;GHI;rez;2,735694704\"\"\"\n\n        df2 = self.read_csv(StringIO(data), sep=';', decimal=',')\n        assert df2['Number1'].dtype == float\n        assert df2['Number2'].dtype == float\n        assert df2['Number3'].dtype == float\n\n    def test_read_duplicate_names(self):\n        # See gh-7160\n        data = \"a,b,a\\n0,1,2\\n3,4,5\"\n        df = self.read_csv(StringIO(data))\n        expected = DataFrame([[0, 1, 2], [3, 4, 5]],\n                             columns=['a', 'b', 'a.1'])\n        tm.assert_frame_equal(df, expected)\n\n        data = \"0,1,2\\n3,4,5\"\n        df = self.read_csv(StringIO(data), names=[\"a\", \"b\", \"a\"])\n        expected = DataFrame([[0, 1, 2], [3, 4, 5]],\n                             columns=['a', 'b', 'a.1'])\n        tm.assert_frame_equal(df, expected)\n\n    def test_inf_parsing(self):\n        data = \"\"\"\\\n,A\na,inf\nb,-inf\nc,+Inf\nd,-Inf\ne,INF\nf,-INF\ng,+INf\nh,-INf\ni,inF\nj,-inF\"\"\"\n        inf = float('inf')\n        expected = Series([inf, -inf] * 5)\n\n        df = self.read_csv(StringIO(data), index_col=0)\n        tm.assert_almost_equal(df['A'].values, expected.values)\n\n        df = self.read_csv(StringIO(data), index_col=0, na_filter=False)\n        tm.assert_almost_equal(df['A'].values, expected.values)\n\n    def test_raise_on_no_columns(self):\n        # single newline\n        data = \"\\n\"\n        pytest.raises(EmptyDataError, self.read_csv, StringIO(data))\n\n        # test with more than a single newline\n        data = \"\\n\\n\\n\"\n        pytest.raises(EmptyDataError, self.read_csv, StringIO(data))\n\n    def test_compact_ints_use_unsigned(self):\n        # see gh-13323\n        data = 'a,b,c\\n1,9,258'\n\n        # sanity check\n        expected = DataFrame({\n            'a': np.array([1], dtype=np.int64),\n            'b': np.array([9], dtype=np.int64),\n            'c': np.array([258], dtype=np.int64),\n        })\n        out = self.read_csv(StringIO(data))\n        tm.assert_frame_equal(out, expected)\n\n        expected = DataFrame({\n            'a': np.array([1], dtype=np.int8),\n            'b': np.array([9], dtype=np.int8),\n            'c': np.array([258], dtype=np.int16),\n        })\n\n        # default behaviour for 'use_unsigned'\n        with tm.assert_produces_warning(\n                FutureWarning, check_stacklevel=False):\n            out = self.read_csv(StringIO(data), compact_ints=True)\n            tm.assert_frame_equal(out, expected)\n\n        with tm.assert_produces_warning(\n                FutureWarning, check_stacklevel=False):\n            out = self.read_csv(StringIO(data), compact_ints=True,\n                                use_unsigned=False)\n            tm.assert_frame_equal(out, expected)\n\n        expected = DataFrame({\n            'a': np.array([1], dtype=np.uint8),\n            'b': np.array([9], dtype=np.uint8),\n            'c': np.array([258], dtype=np.uint16),\n        })\n\n        with tm.assert_produces_warning(\n                FutureWarning, check_stacklevel=False):\n            out = self.read_csv(StringIO(data), compact_ints=True,\n                                use_unsigned=True)\n            tm.assert_frame_equal(out, expected)\n\n    def test_compact_ints_as_recarray(self):\n        data = ('0,1,0,0\\n'\n                '1,1,0,0\\n'\n                '0,1,0,1')\n\n        with tm.assert_produces_warning(\n                FutureWarning, check_stacklevel=False):\n            result = self.read_csv(StringIO(data), delimiter=',', header=None,\n                                   compact_ints=True, as_recarray=True)\n            ex_dtype = np.dtype([(str(i), 'i1') for i in range(4)])\n            assert result.dtype == ex_dtype\n\n        with tm.assert_produces_warning(\n                FutureWarning, check_stacklevel=False):\n            result = self.read_csv(StringIO(data), delimiter=',', header=None,\n                                   as_recarray=True, compact_ints=True,\n                                   use_unsigned=True)\n            ex_dtype = np.dtype([(str(i), 'u1') for i in range(4)])\n            assert result.dtype == ex_dtype\n\n    def test_as_recarray(self):\n        # basic test\n        with tm.assert_produces_warning(\n                FutureWarning, check_stacklevel=False):\n            data = 'a,b\\n1,a\\n2,b'\n            expected = np.array([(1, 'a'), (2, 'b')],\n                                dtype=[('a', '=i8'), ('b', 'O')])\n            out = self.read_csv(StringIO(data), as_recarray=True)\n            tm.assert_numpy_array_equal(out, expected)\n\n        # index_col ignored\n        with tm.assert_produces_warning(\n                FutureWarning, check_stacklevel=False):\n            data = 'a,b\\n1,a\\n2,b'\n            expected = np.array([(1, 'a'), (2, 'b')],\n                                dtype=[('a', '=i8'), ('b', 'O')])\n            out = self.read_csv(StringIO(data), as_recarray=True, index_col=0)\n            tm.assert_numpy_array_equal(out, expected)\n\n        # respects names\n        with tm.assert_produces_warning(\n                FutureWarning, check_stacklevel=False):\n            data = '1,a\\n2,b'\n            expected = np.array([(1, 'a'), (2, 'b')],\n                                dtype=[('a', '=i8'), ('b', 'O')])\n            out = self.read_csv(StringIO(data), names=['a', 'b'],\n                                header=None, as_recarray=True)\n            tm.assert_numpy_array_equal(out, expected)\n\n        # header order is respected even though it conflicts\n        # with the natural ordering of the column names\n        with tm.assert_produces_warning(\n                FutureWarning, check_stacklevel=False):\n            data = 'b,a\\n1,a\\n2,b'\n            expected = np.array([(1, 'a'), (2, 'b')],\n                                dtype=[('b', '=i8'), ('a', 'O')])\n            out = self.read_csv(StringIO(data), as_recarray=True)\n            tm.assert_numpy_array_equal(out, expected)\n\n        # overrides the squeeze parameter\n        with tm.assert_produces_warning(\n                FutureWarning, check_stacklevel=False):\n            data = 'a\\n1'\n            expected = np.array([(1,)], dtype=[('a', '=i8')])\n            out = self.read_csv(StringIO(data), as_recarray=True, squeeze=True)\n            tm.assert_numpy_array_equal(out, expected)\n\n        # does data conversions before doing recarray conversion\n        with tm.assert_produces_warning(\n                FutureWarning, check_stacklevel=False):\n            data = 'a,b\\n1,a\\n2,b'\n            conv = lambda x: int(x) + 1\n            expected = np.array([(2, 'a'), (3, 'b')],\n                                dtype=[('a', '=i8'), ('b', 'O')])\n            out = self.read_csv(StringIO(data), as_recarray=True,\n                                converters={'a': conv})\n            tm.assert_numpy_array_equal(out, expected)\n\n        # filters by usecols before doing recarray conversion\n        with tm.assert_produces_warning(\n                FutureWarning, check_stacklevel=False):\n            data = 'a,b\\n1,a\\n2,b'\n            expected = np.array([(1,), (2,)], dtype=[('a', '=i8')])\n            out = self.read_csv(StringIO(data), as_recarray=True,\n                                usecols=['a'])\n            tm.assert_numpy_array_equal(out, expected)\n\n    def test_memory_map(self):\n        mmap_file = os.path.join(self.dirpath, 'test_mmap.csv')\n        expected = DataFrame({\n            'a': [1, 2, 3],\n            'b': ['one', 'two', 'three'],\n            'c': ['I', 'II', 'III']\n        })\n\n        out = self.read_csv(mmap_file, memory_map=True)\n        tm.assert_frame_equal(out, expected)\n\n    def test_null_byte_char(self):\n        # see gh-2741\n        data = '\\x00,foo'\n        cols = ['a', 'b']\n\n        expected = DataFrame([[np.nan, 'foo']],\n                             columns=cols)\n\n        if self.engine == 'c':\n            out = self.read_csv(StringIO(data), names=cols)\n            tm.assert_frame_equal(out, expected)\n        else:\n            msg = \"NULL byte detected\"\n            with tm.assert_raises_regex(ParserError, msg):\n                self.read_csv(StringIO(data), names=cols)\n\n    def test_utf8_bom(self):\n        # see gh-4793\n        bom = u('\\ufeff')\n        utf8 = 'utf-8'\n\n        def _encode_data_with_bom(_data):\n            bom_data = (bom + _data).encode(utf8)\n            return BytesIO(bom_data)\n\n        # basic test\n        data = 'a\\n1'\n        expected = DataFrame({'a': [1]})\n\n        out = self.read_csv(_encode_data_with_bom(data),\n                            encoding=utf8)\n        tm.assert_frame_equal(out, expected)\n\n        # test with \"regular\" quoting\n        data = '\"a\"\\n1'\n        expected = DataFrame({'a': [1]})\n\n        out = self.read_csv(_encode_data_with_bom(data),\n                            encoding=utf8, quotechar='\"')\n        tm.assert_frame_equal(out, expected)\n\n        # test in a data row instead of header\n        data = 'b\\n1'\n        expected = DataFrame({'a': ['b', '1']})\n\n        out = self.read_csv(_encode_data_with_bom(data),\n                            encoding=utf8, names=['a'])\n        tm.assert_frame_equal(out, expected)\n\n        # test in empty data row with skipping\n        data = '\\n1'\n        expected = DataFrame({'a': [1]})\n\n        out = self.read_csv(_encode_data_with_bom(data),\n                            encoding=utf8, names=['a'],\n                            skip_blank_lines=True)\n        tm.assert_frame_equal(out, expected)\n\n        # test in empty data row without skipping\n        data = '\\n1'\n        expected = DataFrame({'a': [np.nan, 1.0]})\n\n        out = self.read_csv(_encode_data_with_bom(data),\n                            encoding=utf8, names=['a'],\n                            skip_blank_lines=False)\n        tm.assert_frame_equal(out, expected)\n\n    def test_temporary_file(self):\n        # see gh-13398\n        data1 = \"0 0\"\n\n        from tempfile import TemporaryFile\n        new_file = TemporaryFile(\"w+\")\n        new_file.write(data1)\n        new_file.flush()\n        new_file.seek(0)\n\n        result = self.read_csv(new_file, sep=r'\\s+', header=None)\n        new_file.close()\n        expected = DataFrame([[0, 0]])\n        tm.assert_frame_equal(result, expected)\n\n    def test_read_csv_utf_aliases(self):\n        # see gh issue 13549\n        expected = pd.DataFrame({'mb_num': [4.8], 'multibyte': ['test']})\n        for byte in [8, 16]:\n            for fmt in ['utf-{0}', 'utf_{0}', 'UTF-{0}', 'UTF_{0}']:\n                encoding = fmt.format(byte)\n                data = 'mb_num,multibyte\\n4.8,test'.encode(encoding)\n                result = self.read_csv(BytesIO(data), encoding=encoding)\n                tm.assert_frame_equal(result, expected)\n\n    def test_internal_eof_byte(self):\n        # see gh-5500\n        data = \"a,b\\n1\\x1a,2\"\n\n        expected = pd.DataFrame([[\"1\\x1a\", 2]], columns=['a', 'b'])\n        result = self.read_csv(StringIO(data))\n        tm.assert_frame_equal(result, expected)\n\n    def test_internal_eof_byte_to_file(self):\n        # see gh-16559\n        data = b'c1,c2\\r\\n\"test \\x1a    test\", test\\r\\n'\n        expected = pd.DataFrame([[\"test \\x1a    test\", \" test\"]],\n                                columns=[\"c1\", \"c2\"])\n\n        path = '__%s__.csv' % tm.rands(10)\n\n        with tm.ensure_clean(path) as path:\n            with open(path, \"wb\") as f:\n                f.write(data)\n\n            result = self.read_csv(path)\n            tm.assert_frame_equal(result, expected)\n\n    def test_sub_character(self):\n        # see gh-16893\n        dirpath = tm.get_data_path()\n        filename = os.path.join(dirpath, \"sub_char.csv\")\n\n        expected = DataFrame([[1, 2, 3]], columns=[\"a\", \"\\x1ab\", \"c\"])\n        result = self.read_csv(filename)\n\n        tm.assert_frame_equal(result, expected)\n\n    def test_file_handles(self):\n        # GH 14418 - don't close user provided file handles\n\n        fh = StringIO('a,b\\n1,2')\n        self.read_csv(fh)\n        assert not fh.closed\n\n        with open(self.csv1, 'r') as f:\n            self.read_csv(f)\n            assert not f.closed\n\n        # mmap not working with python engine\n        if self.engine != 'python':\n\n            import mmap\n            with open(self.csv1, 'r') as f:\n                m = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)\n                self.read_csv(m)\n                # closed attribute new in python 3.2\n                if PY3:\n                    assert not m.closed\n                m.close()\n\n    def test_invalid_file_buffer(self):\n        # see gh-15337\n\n        class InvalidBuffer(object):\n            pass\n\n        msg = \"Invalid file path or buffer object type\"\n\n        with tm.assert_raises_regex(ValueError, msg):\n            self.read_csv(InvalidBuffer())\n\n        # gh-16135: we want to ensure that \"tell\" and \"seek\"\n        # aren't actually being used when we call `read_csv`\n        #\n        # Thus, while the object may look \"invalid\" (these\n        # methods are attributes of the `StringIO` class),\n        # it is still a valid file-object for our purposes.\n        class NoSeekTellBuffer(StringIO):\n            def tell(self):\n                raise AttributeError(\"No tell method\")\n\n            def seek(self, pos, whence=0):\n                raise AttributeError(\"No seek method\")\n\n        data = \"a\\n1\"\n\n        expected = pd.DataFrame({\"a\": [1]})\n        result = self.read_csv(NoSeekTellBuffer(data))\n\n        tm.assert_frame_equal(result, expected)\n\n        if PY3:\n            from unittest import mock\n\n            with tm.assert_raises_regex(ValueError, msg):\n                self.read_csv(mock.Mock())\n\n    @tm.capture_stderr\n    def test_skip_bad_lines(self):\n        # see gh-15925\n        data = 'a\\n1\\n1,2,3\\n4\\n5,6,7'\n\n        with pytest.raises(ParserError):\n            self.read_csv(StringIO(data))\n\n        with pytest.raises(ParserError):\n            self.read_csv(StringIO(data), error_bad_lines=True)\n\n        expected = DataFrame({'a': [1, 4]})\n\n        out = self.read_csv(StringIO(data),\n                            error_bad_lines=False,\n                            warn_bad_lines=False)\n        tm.assert_frame_equal(out, expected)\n\n        val = sys.stderr.getvalue()\n        assert val == ''\n\n        # Reset the stderr buffer.\n        sys.stderr = StringIO()\n\n        out = self.read_csv(StringIO(data),\n                            error_bad_lines=False,\n                            warn_bad_lines=True)\n        tm.assert_frame_equal(out, expected)\n\n        val = sys.stderr.getvalue()\n        assert 'Skipping line 3' in val\n        assert 'Skipping line 5' in val\n"
    },
    {
      "filename": "pandas/tests/io/parser/mangle_dupes.py",
      "content": "# -*- coding: utf-8 -*-\n\n\"\"\"\nTests that duplicate columns are handled appropriately when parsed by the\nCSV engine. In general, the expected result is that they are either thoroughly\nde-duplicated (if mangling requested) or ignored otherwise.\n\"\"\"\n\nfrom pandas.compat import StringIO\n\n\nclass DupeColumnTests(object):\n    def test_basic(self):\n        # TODO: add test for condition \"mangle_dupe_cols=False\"\n        # once it is actually supported (gh-12935)\n        data = \"a,a,b,b,b\\n1,2,3,4,5\"\n\n        for method in (\"read_csv\", \"read_table\"):\n            # Check default behavior.\n            expected = [\"a\", \"a.1\", \"b\", \"b.1\", \"b.2\"]\n            df = getattr(self, method)(StringIO(data), sep=\",\")\n            assert list(df.columns) == expected\n\n            df = getattr(self, method)(StringIO(data), sep=\",\",\n                                       mangle_dupe_cols=True)\n            assert list(df.columns) == expected\n\n    def test_thorough_mangle(self):\n        # see gh-17060\n        data = \"a,a,a.1\\n1,2,3\"\n        df = self.read_csv(StringIO(data), sep=\",\", mangle_dupe_cols=True)\n        assert list(df.columns) == [\"a\", \"a.1\", \"a.1.1\"]\n\n        data = \"a,a,a.1,a.1.1,a.1.1.1,a.1.1.1.1\\n1,2,3,4,5,6\"\n        df = self.read_csv(StringIO(data), sep=\",\", mangle_dupe_cols=True)\n        assert list(df.columns) == [\"a\", \"a.1\", \"a.1.1\", \"a.1.1.1\",\n                                    \"a.1.1.1.1\", \"a.1.1.1.1.1\"]\n\n        data = \"a,a,a.3,a.1,a.2,a,a\\n1,2,3,4,5,6,7\"\n        df = self.read_csv(StringIO(data), sep=\",\", mangle_dupe_cols=True)\n        assert list(df.columns) == [\"a\", \"a.1\", \"a.3\", \"a.1.1\",\n                                    \"a.2\", \"a.2.1\", \"a.3.1\"]\n"
    },
    {
      "filename": "pandas/tests/io/parser/test_parsers.py",
      "content": "# -*- coding: utf-8 -*-\n\nimport os\nimport pandas.util.testing as tm\n\nfrom pandas import read_csv, read_table\nfrom pandas.core.common import AbstractMethodError\n\nfrom .common import ParserTests\nfrom .header import HeaderTests\nfrom .comment import CommentTests\nfrom .dialect import DialectTests\nfrom .quoting import QuotingTests\nfrom .usecols import UsecolsTests\nfrom .skiprows import SkipRowsTests\nfrom .index_col import IndexColTests\nfrom .na_values import NAvaluesTests\nfrom .converters import ConverterTests\nfrom .c_parser_only import CParserTests\nfrom .parse_dates import ParseDatesTests\nfrom .compression import CompressionTests\nfrom .mangle_dupes import DupeColumnTests\nfrom .multithread import MultithreadTests\nfrom .python_parser_only import PythonParserTests\nfrom .dtypes import DtypeTests\n\n\nclass BaseParser(CommentTests, CompressionTests,\n                 ConverterTests, DialectTests,\n                 DtypeTests, DupeColumnTests,\n                 HeaderTests, IndexColTests,\n                 MultithreadTests, NAvaluesTests,\n                 ParseDatesTests, ParserTests,\n                 SkipRowsTests, UsecolsTests,\n                 QuotingTests):\n\n    def read_csv(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def read_table(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def float_precision_choices(self):\n        raise AbstractMethodError(self)\n\n    def setup_method(self, method):\n        self.dirpath = tm.get_data_path()\n        self.csv1 = os.path.join(self.dirpath, 'test1.csv')\n        self.csv2 = os.path.join(self.dirpath, 'test2.csv')\n        self.xls1 = os.path.join(self.dirpath, 'test.xls')\n        self.csv_shiftjs = os.path.join(self.dirpath, 'sauron.SHIFT_JIS.csv')\n\n\nclass TestCParserHighMemory(BaseParser, CParserTests):\n    engine = 'c'\n    low_memory = False\n    float_precision_choices = [None, 'high', 'round_trip']\n\n    def read_csv(self, *args, **kwds):\n        kwds = kwds.copy()\n        kwds['engine'] = self.engine\n        kwds['low_memory'] = self.low_memory\n        return read_csv(*args, **kwds)\n\n    def read_table(self, *args, **kwds):\n        kwds = kwds.copy()\n        kwds['engine'] = self.engine\n        kwds['low_memory'] = self.low_memory\n        return read_table(*args, **kwds)\n\n\nclass TestCParserLowMemory(BaseParser, CParserTests):\n    engine = 'c'\n    low_memory = True\n    float_precision_choices = [None, 'high', 'round_trip']\n\n    def read_csv(self, *args, **kwds):\n        kwds = kwds.copy()\n        kwds['engine'] = self.engine\n        kwds['low_memory'] = self.low_memory\n        return read_csv(*args, **kwds)\n\n    def read_table(self, *args, **kwds):\n        kwds = kwds.copy()\n        kwds['engine'] = self.engine\n        kwds['low_memory'] = True\n        return read_table(*args, **kwds)\n\n\nclass TestPythonParser(BaseParser, PythonParserTests):\n    engine = 'python'\n    float_precision_choices = [None]\n\n    def read_csv(self, *args, **kwds):\n        kwds = kwds.copy()\n        kwds['engine'] = self.engine\n        return read_csv(*args, **kwds)\n\n    def read_table(self, *args, **kwds):\n        kwds = kwds.copy()\n        kwds['engine'] = self.engine\n        return read_table(*args, **kwds)\n"
    }
  ]
}