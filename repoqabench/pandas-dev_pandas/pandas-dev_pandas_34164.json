{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "34164",
  "issue_description": "# BUG: grouping with categorical interval columns\n\nVersions:\r\npandas 1.0.3\r\nnumpy 1.18.1\r\n\r\nThere is a bug in the 1.XXX pandas release that does not allow you to group by a categorical interval index column together with another column.\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\npd.set_option(\"use_inf_as_na\",True)\r\nt = pd.DataFrame({\"x\":np.random.randn(100), 'w':np.random.choice(list(\"ABC\"), 100)})\r\nqq = pd.qcut(t['x'], q=np.linspace(0,1,5))\r\n\r\n```\r\n\r\nThis works and gives the expected result:\r\n`\r\nt.groupby([qq])['x'].agg('mean')\r\n`\r\n\r\n`x\r\n(-10.001, -1.0]   -1.431893\r\n(-1.0, 0.0]       -0.423564\r\n(0.0, 1.0]         0.461174\r\n(1.0, 10.0]        1.662297\r\nName: x, dtype: float64\r\n`\r\n\r\n\r\n\r\nThis raises a TypeError:\r\n`\r\nt.groupby([qq,'w'])['x'].agg('mean')\r\n`\r\n\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-43-6d7782f17653> in <module>\r\n----> 1 t.groupby([qq,'w'])['x'].agg('mean')\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py in aggregate(self, func, *args, **kwargs)\r\n    245 \r\n    246         if isinstance(func, str):\r\n--> 247             return getattr(self, func)(*args, **kwargs)\r\n    248 \r\n    249         elif isinstance(func, abc.Iterable):\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in mean(self, *args, **kwargs)\r\n   1223         nv.validate_groupby_func(\"mean\", args, kwargs, [\"numeric_only\"])\r\n   1224         return self._cython_agg_general(\r\n-> 1225             \"mean\", alt=lambda x, axis: Series(x).mean(**kwargs), **kwargs\r\n   1226         )\r\n   1227 \r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in _cython_agg_general(self, how, alt, numeric_only, min_count)\r\n    907             raise DataError(\"No numeric types to aggregate\")\r\n    908 \r\n--> 909         return self._wrap_aggregated_output(output)\r\n    910 \r\n    911     def _python_agg_general(self, func, *args, **kwargs):\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py in _wrap_aggregated_output(self, output)\r\n    384             output=output, index=self.grouper.result_index\r\n    385         )\r\n--> 386         return self._reindex_output(result)._convert(datetime=True)\r\n    387 \r\n    388     def _wrap_transformed_output(\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in _reindex_output(self, output, fill_value)\r\n   2481         levels_list = [ping.group_index for ping in groupings]\r\n   2482         index, _ = MultiIndex.from_product(\r\n-> 2483             levels_list, names=self.grouper.names\r\n   2484         ).sortlevel()\r\n   2485 \r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/multi.py in from_product(cls, iterables, sortorder, names)\r\n    551 \r\n    552         codes = cartesian_product(codes)\r\n--> 553         return MultiIndex(levels, codes, sortorder=sortorder, names=names)\r\n    554 \r\n    555     @classmethod\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/multi.py in __new__(cls, levels, codes, sortorder, names, dtype, copy, name, verify_integrity, _set_identity)\r\n    278 \r\n    279         if verify_integrity:\r\n--> 280             new_codes = result._verify_integrity()\r\n    281             result._codes = new_codes\r\n    282 \r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/multi.py in _verify_integrity(self, codes, levels)\r\n    366 \r\n    367         codes = [\r\n--> 368             self._validate_codes(level, code) for level, code in zip(levels, codes)\r\n    369         ]\r\n    370         new_codes = FrozenList(codes)\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/multi.py in <listcomp>(.0)\r\n    366 \r\n    367         codes = [\r\n--> 368             self._validate_codes(level, code) for level, code in zip(levels, codes)\r\n    369         ]\r\n    370         new_codes = FrozenList(codes)\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/multi.py in _validate_codes(self, level, code)\r\n    302         to a level with missing values (NaN, NaT, None).\r\n    303         \"\"\"\r\n--> 304         null_mask = isna(level)\r\n    305         if np.any(null_mask):\r\n    306             code = np.where(null_mask[code], -1, code)\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/dtypes/missing.py in isna(obj)\r\n    124     Name: 1, dtype: bool\r\n    125     \"\"\"\r\n--> 126     return _isna(obj)\r\n    127 \r\n    128 \r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/dtypes/missing.py in _isna_old(obj)\r\n    181         return False\r\n    182     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\r\n--> 183         return _isna_ndarraylike_old(obj)\r\n    184     elif isinstance(obj, ABCGeneric):\r\n    185         return obj._constructor(obj._data.isna(func=_isna_old))\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/dtypes/missing.py in _isna_ndarraylike_old(obj)\r\n    281         else:\r\n    282             result = np.empty(shape, dtype=bool)\r\n--> 283             vec = libmissing.isnaobj_old(values.ravel())\r\n    284             result[:] = vec.reshape(shape)\r\n    285 \r\n\r\nTypeError: Argument 'arr' has incorrect type (expected numpy.ndarray, got Categorical)\r\n\r\n```\r\n",
  "issue_comments": [
    {
      "id": 628341983,
      "user": "dsaxton",
      "body": "This seems to work on master for me:\r\n```python\r\n[ins] In [1]: import numpy as np                                                                                                                                                                             \r\n\r\n[ins] In [2]: import pandas as pd                                                                                                                                                                            \r\n\r\n[ins] In [3]: pd.set_option(\"use_inf_as_na\",True)                                                                                                                                                            \r\n\r\n[ins] In [4]: t = pd.DataFrame({\"x\":np.random.randn(100), 'w':np.random.choice(list(\"ABC\"), 100)})                                                                                                           \r\n\r\n[ins] In [5]: qq = pd.qcut(t['x'], q=np.linspace(0,1,5))                                                                                                                                                     \r\n\r\n[ins] In [6]: t.groupby([qq,'w'])['x'].agg('mean')                                                                                                                                                           \r\nOut[6]: \r\nx                              w\r\n(-2.7649999999999997, -0.736]  A   -1.412247\r\n                               B   -1.319972\r\n                               C   -1.108550\r\n(-0.736, -0.114]               A   -0.351454\r\n                               B   -0.388151\r\n                               C   -0.404094\r\n(-0.114, 0.587]                A    0.134442\r\n                               B    0.235705\r\n                               C    0.406392\r\n(0.587, 1.864]                 A    1.056471\r\n                               B    0.973123\r\n                               C    1.189502\r\nName: x, dtype: float64\r\n```"
    },
    {
      "id": 629289786,
      "user": "antipisa",
      "body": "Here is my env:\r\n\r\n```\r\nPackage                Version   \r\n---------------------- ----------\r\nappnope                0.1.0     \r\nasn1crypto             1.2.0     \r\nattrs                  19.3.0    \r\nbackcall               0.1.0     \r\nbleach                 3.1.5     \r\ncertifi                2019.11.28\r\ncffi                   1.13.0    \r\nchardet                3.0.4     \r\nconda                  4.8.2     \r\nconda-package-handling 1.6.0     \r\ncryptography           2.8       \r\ndecorator              4.4.2     \r\ndefusedxml             0.6.0     \r\nentrypoints            0.3       \r\nidna                   2.8       \r\nimportlib-metadata     1.6.0     \r\nipykernel              5.2.1     \r\nipython                7.14.0    \r\nipython-genutils       0.2.0     \r\nipywidgets             7.5.1     \r\njedi                   0.17.0    \r\nJinja2                 2.11.2    \r\njsonschema             3.2.0     \r\njupyter                1.0.0     \r\njupyter-client         6.1.3     \r\njupyter-console        6.1.0     \r\njupyter-core           4.6.3     \r\nMarkupSafe             1.1.1     \r\nmistune                0.8.4     \r\nmkl-fft                1.0.15    \r\nmkl-random             1.1.0     \r\nmkl-service            2.3.0     \r\nnbconvert              5.6.1     \r\nnbformat               5.0.6     \r\nnotebook               6.0.3     \r\nnumpy                  1.18.1    \r\npackaging              20.3      \r\npandas                 1.0.3     \r\npandocfilters          1.4.2     \r\nparso                  0.7.0     \r\npexpect                4.8.0     \r\npickleshare            0.7.5     \r\npip                    19.3.1    \r\nprometheus-client      0.7.1     \r\nprompt-toolkit         3.0.5     \r\nptyprocess             0.6.0     \r\npycosat                0.6.3     \r\npycparser              2.19      \r\nPygments               2.6.1     \r\npyOpenSSL              19.0.0    \r\npyparsing              2.4.7     \r\npyq                    4.2.1     \r\npyrsistent             0.16.0    \r\nPySocks                1.7.1     \r\npython-dateutil        2.8.1     \r\npytz                   2020.1    \r\npyzmq                  19.0.1    \r\nqtconsole              4.7.4     \r\nQtPy                   1.9.0     \r\nrequests               2.22.0    \r\nruamel-yaml            0.15.46   \r\nSend2Trash             1.5.0     \r\nsetuptools             41.4.0    \r\nsix                    1.12.0    \r\nterminado              0.8.3     \r\ntestpath               0.4.4     \r\ntornado                6.0.4     \r\ntqdm                   4.36.1    \r\ntraitlets              4.3.3     \r\nurllib3                1.24.2    \r\nwcwidth                0.1.9     \r\nwebencodings           0.5.1     \r\nwheel                  0.33.6    \r\nwidgetsnbextension     3.5.1     \r\nzipp                   3.1.0     \r\n```"
    },
    {
      "id": 1079851859,
      "user": "banditelol",
      "body": "Also works for me on 1.3.5\r\n![image](https://user-images.githubusercontent.com/5263688/160269819-1e4eb488-d3ae-4e64-b0c4-e386c4f3898e.png)\r\nShould this issue be closed?"
    },
    {
      "id": 1079865283,
      "user": "jreback",
      "body": "would take a regression test to close"
    },
    {
      "id": 1079907259,
      "user": "banditelol",
      "body": "Do you mean writing a regression test for the correct behavior like this one, to make sure the behavior stays correct ?\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/dbc3afa7ed4a79dad285c950cef84834d5697ca4/pandas/tests/groupby/test_groupby.py#L108-L147\r\n"
    },
    {
      "id": 1079932335,
      "user": "jreback",
      "body": "yes"
    },
    {
      "id": 1080302926,
      "user": "banditelol",
      "body": "Noted, can I take on this issue?"
    },
    {
      "id": 1091900786,
      "user": "banditelol",
      "body": "After tweaking around on [notebook](https://colab.research.google.com/drive/1bNPCRzmSzAM85MW7k6209b4WP7vS9ExC?usp=sharing), it seems like I could replicate the problem with the condition:\r\n- Using pandas 1.0.3 (no problem on 1.0.4)\r\n- MultiIndexing dataframe\r\n- Using column with type of `categoricalDtype` not `IntervalDtype`\r\n- Using `observed=False` as `groupby`'s argument\r\nAnd it seems like the current test suite has already take this into account with these following lines:\r\nhttps://github.com/pandas-dev/pandas/blob/361021b56f3159afb71d690fac3a1f3b381b0da6/pandas/tests/groupby/test_categorical.py#L317-L353\r\nI want to test pandas version 1.0.3 against this unit test do you have any recommendation to do so?\r\n"
    },
    {
      "id": 1517051707,
      "user": "PrimeF",
      "body": "I've added a test for this older bug which, as stated above, was already fixed a while back."
    },
    {
      "id": 1583314584,
      "user": "mcgeestocks",
      "body": "Looks like [Closed PR#52818](https://github.com/pandas-dev/pandas/pull/52818) should have closed this task. Pinging @phofl for visibility.  "
    },
    {
      "id": 1825865098,
      "user": "aibanez7",
      "body": "Hello, \r\nCould this task be closed? \r\nThank you, "
    }
  ],
  "text_context": "# BUG: grouping with categorical interval columns\n\nVersions:\r\npandas 1.0.3\r\nnumpy 1.18.1\r\n\r\nThere is a bug in the 1.XXX pandas release that does not allow you to group by a categorical interval index column together with another column.\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\npd.set_option(\"use_inf_as_na\",True)\r\nt = pd.DataFrame({\"x\":np.random.randn(100), 'w':np.random.choice(list(\"ABC\"), 100)})\r\nqq = pd.qcut(t['x'], q=np.linspace(0,1,5))\r\n\r\n```\r\n\r\nThis works and gives the expected result:\r\n`\r\nt.groupby([qq])['x'].agg('mean')\r\n`\r\n\r\n`x\r\n(-10.001, -1.0]   -1.431893\r\n(-1.0, 0.0]       -0.423564\r\n(0.0, 1.0]         0.461174\r\n(1.0, 10.0]        1.662297\r\nName: x, dtype: float64\r\n`\r\n\r\n\r\n\r\nThis raises a TypeError:\r\n`\r\nt.groupby([qq,'w'])['x'].agg('mean')\r\n`\r\n\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-43-6d7782f17653> in <module>\r\n----> 1 t.groupby([qq,'w'])['x'].agg('mean')\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py in aggregate(self, func, *args, **kwargs)\r\n    245 \r\n    246         if isinstance(func, str):\r\n--> 247             return getattr(self, func)(*args, **kwargs)\r\n    248 \r\n    249         elif isinstance(func, abc.Iterable):\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in mean(self, *args, **kwargs)\r\n   1223         nv.validate_groupby_func(\"mean\", args, kwargs, [\"numeric_only\"])\r\n   1224         return self._cython_agg_general(\r\n-> 1225             \"mean\", alt=lambda x, axis: Series(x).mean(**kwargs), **kwargs\r\n   1226         )\r\n   1227 \r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in _cython_agg_general(self, how, alt, numeric_only, min_count)\r\n    907             raise DataError(\"No numeric types to aggregate\")\r\n    908 \r\n--> 909         return self._wrap_aggregated_output(output)\r\n    910 \r\n    911     def _python_agg_general(self, func, *args, **kwargs):\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py in _wrap_aggregated_output(self, output)\r\n    384             output=output, index=self.grouper.result_index\r\n    385         )\r\n--> 386         return self._reindex_output(result)._convert(datetime=True)\r\n    387 \r\n    388     def _wrap_transformed_output(\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in _reindex_output(self, output, fill_value)\r\n   2481         levels_list = [ping.group_index for ping in groupings]\r\n   2482         index, _ = MultiIndex.from_product(\r\n-> 2483             levels_list, names=self.grouper.names\r\n   2484         ).sortlevel()\r\n   2485 \r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/multi.py in from_product(cls, iterables, sortorder, names)\r\n    551 \r\n    552         codes = cartesian_product(codes)\r\n--> 553         return MultiIndex(levels, codes, sortorder=sortorder, names=names)\r\n    554 \r\n    555     @classmethod\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/multi.py in __new__(cls, levels, codes, sortorder, names, dtype, copy, name, verify_integrity, _set_identity)\r\n    278 \r\n    279         if verify_integrity:\r\n--> 280             new_codes = result._verify_integrity()\r\n    281             result._codes = new_codes\r\n    282 \r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/multi.py in _verify_integrity(self, codes, levels)\r\n    366 \r\n    367         codes = [\r\n--> 368             self._validate_codes(level, code) for level, code in zip(levels, codes)\r\n    369         ]\r\n    370         new_codes = FrozenList(codes)\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/multi.py in <listcomp>(.0)\r\n    366 \r\n    367         codes = [\r\n--> 368             self._validate_codes(level, code) for level, code in zip(levels, codes)\r\n    369         ]\r\n    370         new_codes = FrozenList(codes)\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/multi.py in _validate_codes(self, level, code)\r\n    302         to a level with missing values (NaN, NaT, None).\r\n    303         \"\"\"\r\n--> 304         null_mask = isna(level)\r\n    305         if np.any(null_mask):\r\n    306             code = np.where(null_mask[code], -1, code)\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/dtypes/missing.py in isna(obj)\r\n    124     Name: 1, dtype: bool\r\n    125     \"\"\"\r\n--> 126     return _isna(obj)\r\n    127 \r\n    128 \r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/dtypes/missing.py in _isna_old(obj)\r\n    181         return False\r\n    182     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\r\n--> 183         return _isna_ndarraylike_old(obj)\r\n    184     elif isinstance(obj, ABCGeneric):\r\n    185         return obj._constructor(obj._data.isna(func=_isna_old))\r\n\r\n~/miniconda3/lib/python3.7/site-packages/pandas/core/dtypes/missing.py in _isna_ndarraylike_old(obj)\r\n    281         else:\r\n    282             result = np.empty(shape, dtype=bool)\r\n--> 283             vec = libmissing.isnaobj_old(values.ravel())\r\n    284             result[:] = vec.reshape(shape)\r\n    285 \r\n\r\nTypeError: Argument 'arr' has incorrect type (expected numpy.ndarray, got Categorical)\r\n\r\n```\r\n\n\nThis seems to work on master for me:\r\n```python\r\n[ins] In [1]: import numpy as np                                                                                                                                                                             \r\n\r\n[ins] In [2]: import pandas as pd                                                                                                                                                                            \r\n\r\n[ins] In [3]: pd.set_option(\"use_inf_as_na\",True)                                                                                                                                                            \r\n\r\n[ins] In [4]: t = pd.DataFrame({\"x\":np.random.randn(100), 'w':np.random.choice(list(\"ABC\"), 100)})                                                                                                           \r\n\r\n[ins] In [5]: qq = pd.qcut(t['x'], q=np.linspace(0,1,5))                                                                                                                                                     \r\n\r\n[ins] In [6]: t.groupby([qq,'w'])['x'].agg('mean')                                                                                                                                                           \r\nOut[6]: \r\nx                              w\r\n(-2.7649999999999997, -0.736]  A   -1.412247\r\n                               B   -1.319972\r\n                               C   -1.108550\r\n(-0.736, -0.114]               A   -0.351454\r\n                               B   -0.388151\r\n                               C   -0.404094\r\n(-0.114, 0.587]                A    0.134442\r\n                               B    0.235705\r\n                               C    0.406392\r\n(0.587, 1.864]                 A    1.056471\r\n                               B    0.973123\r\n                               C    1.189502\r\nName: x, dtype: float64\r\n```\n\nHere is my env:\r\n\r\n```\r\nPackage                Version   \r\n---------------------- ----------\r\nappnope                0.1.0     \r\nasn1crypto             1.2.0     \r\nattrs                  19.3.0    \r\nbackcall               0.1.0     \r\nbleach                 3.1.5     \r\ncertifi                2019.11.28\r\ncffi                   1.13.0    \r\nchardet                3.0.4     \r\nconda                  4.8.2     \r\nconda-package-handling 1.6.0     \r\ncryptography           2.8       \r\ndecorator              4.4.2     \r\ndefusedxml             0.6.0     \r\nentrypoints            0.3       \r\nidna                   2.8       \r\nimportlib-metadata     1.6.0     \r\nipykernel              5.2.1     \r\nipython                7.14.0    \r\nipython-genutils       0.2.0     \r\nipywidgets             7.5.1     \r\njedi                   0.17.0    \r\nJinja2                 2.11.2    \r\njsonschema             3.2.0     \r\njupyter                1.0.0     \r\njupyter-client         6.1.3     \r\njupyter-console        6.1.0     \r\njupyter-core           4.6.3     \r\nMarkupSafe             1.1.1     \r\nmistune                0.8.4     \r\nmkl-fft                1.0.15    \r\nmkl-random             1.1.0     \r\nmkl-service            2.3.0     \r\nnbconvert              5.6.1     \r\nnbformat               5.0.6     \r\nnotebook               6.0.3     \r\nnumpy                  1.18.1    \r\npackaging              20.3      \r\npandas                 1.0.3     \r\npandocfilters          1.4.2     \r\nparso                  0.7.0     \r\npexpect                4.8.0     \r\npickleshare            0.7.5     \r\npip                    19.3.1    \r\nprometheus-client      0.7.1     \r\nprompt-toolkit         3.0.5     \r\nptyprocess             0.6.0     \r\npycosat                0.6.3     \r\npycparser              2.19      \r\nPygments               2.6.1     \r\npyOpenSSL              19.0.0    \r\npyparsing              2.4.7     \r\npyq                    4.2.1     \r\npyrsistent             0.16.0    \r\nPySocks                1.7.1     \r\npython-dateutil        2.8.1     \r\npytz                   2020.1    \r\npyzmq                  19.0.1    \r\nqtconsole              4.7.4     \r\nQtPy                   1.9.0     \r\nrequests               2.22.0    \r\nruamel-yaml            0.15.46   \r\nSend2Trash             1.5.0     \r\nsetuptools             41.4.0    \r\nsix                    1.12.0    \r\nterminado              0.8.3     \r\ntestpath               0.4.4     \r\ntornado                6.0.4     \r\ntqdm                   4.36.1    \r\ntraitlets              4.3.3     \r\nurllib3                1.24.2    \r\nwcwidth                0.1.9     \r\nwebencodings           0.5.1     \r\nwheel                  0.33.6    \r\nwidgetsnbextension     3.5.1     \r\nzipp                   3.1.0     \r\n```\n\nAlso works for me on 1.3.5\r\n![image](https://user-images.githubusercontent.com/5263688/160269819-1e4eb488-d3ae-4e64-b0c4-e386c4f3898e.png)\r\nShould this issue be closed?\n\nwould take a regression test to close\n\nDo you mean writing a regression test for the correct behavior like this one, to make sure the behavior stays correct ?\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/dbc3afa7ed4a79dad285c950cef84834d5697ca4/pandas/tests/groupby/test_groupby.py#L108-L147\r\n\n\nyes\n\nNoted, can I take on this issue?\n\nAfter tweaking around on [notebook](https://colab.research.google.com/drive/1bNPCRzmSzAM85MW7k6209b4WP7vS9ExC?usp=sharing), it seems like I could replicate the problem with the condition:\r\n- Using pandas 1.0.3 (no problem on 1.0.4)\r\n- MultiIndexing dataframe\r\n- Using column with type of `categoricalDtype` not `IntervalDtype`\r\n- Using `observed=False` as `groupby`'s argument\r\nAnd it seems like the current test suite has already take this into account with these following lines:\r\nhttps://github.com/pandas-dev/pandas/blob/361021b56f3159afb71d690fac3a1f3b381b0da6/pandas/tests/groupby/test_categorical.py#L317-L353\r\nI want to test pandas version 1.0.3 against this unit test do you have any recommendation to do so?\r\n\n\nI've added a test for this older bug which, as stated above, was already fixed a while back.\n\nLooks like [Closed PR#52818](https://github.com/pandas-dev/pandas/pull/52818) should have closed this task. Pinging @phofl for visibility.  \n\nHello, \r\nCould this task be closed? \r\nThank you, ",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/52818",
  "code_context": [
    {
      "filename": "pandas/tests/groupby/test_groupby.py",
      "content": "from datetime import datetime\nfrom decimal import Decimal\n\nimport numpy as np\nimport pytest\n\nfrom pandas.compat import IS64\nfrom pandas.errors import (\n    PerformanceWarning,\n    SpecificationError,\n)\n\nimport pandas as pd\nfrom pandas import (\n    Categorical,\n    DataFrame,\n    Grouper,\n    Index,\n    Interval,\n    MultiIndex,\n    RangeIndex,\n    Series,\n    Timedelta,\n    Timestamp,\n    date_range,\n    to_datetime,\n)\nimport pandas._testing as tm\nfrom pandas.core.arrays import BooleanArray\nimport pandas.core.common as com\nfrom pandas.tests.groupby import get_groupby_method_args\n\n\ndef test_repr():\n    # GH18203\n    result = repr(Grouper(key=\"A\", level=\"B\"))\n    expected = \"Grouper(key='A', level='B', axis=0, sort=False, dropna=True)\"\n    assert result == expected\n\n\ndef test_groupby_std_datetimelike():\n    # GH#48481\n    tdi = pd.timedelta_range(\"1 Day\", periods=10000)\n    ser = Series(tdi)\n    ser[::5] *= 2  # get different std for different groups\n\n    df = ser.to_frame(\"A\")\n\n    df[\"B\"] = ser + Timestamp(0)\n    df[\"C\"] = ser + Timestamp(0, tz=\"UTC\")\n    df.iloc[-1] = pd.NaT  # last group includes NaTs\n\n    gb = df.groupby(list(range(5)) * 2000)\n\n    result = gb.std()\n\n    # Note: this does not _exactly_ match what we would get if we did\n    # [gb.get_group(i).std() for i in gb.groups]\n    #  but it _does_ match the floating point error we get doing the\n    #  same operation on int64 data xref GH#51332\n    td1 = Timedelta(\"2887 days 11:21:02.326710176\")\n    td4 = Timedelta(\"2886 days 00:42:34.664668096\")\n    exp_ser = Series([td1 * 2, td1, td1, td1, td4], index=np.arange(5))\n    expected = DataFrame({\"A\": exp_ser, \"B\": exp_ser, \"C\": exp_ser})\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"dtype\", [\"int64\", \"int32\", \"float64\", \"float32\"])\ndef test_basic_aggregations(dtype):\n    data = Series(np.arange(9) // 3, index=np.arange(9), dtype=dtype)\n\n    index = np.arange(9)\n    np.random.shuffle(index)\n    data = data.reindex(index)\n\n    grouped = data.groupby(lambda x: x // 3, group_keys=False)\n\n    for k, v in grouped:\n        assert len(v) == 3\n\n    agged = grouped.aggregate(np.mean)\n    assert agged[1] == 1\n\n    tm.assert_series_equal(agged, grouped.agg(np.mean))  # shorthand\n    tm.assert_series_equal(agged, grouped.mean())\n    tm.assert_series_equal(grouped.agg(np.sum), grouped.sum())\n\n    expected = grouped.apply(lambda x: x * x.sum())\n    transformed = grouped.transform(lambda x: x * x.sum())\n    assert transformed[7] == 12\n    tm.assert_series_equal(transformed, expected)\n\n    value_grouped = data.groupby(data)\n    tm.assert_series_equal(\n        value_grouped.aggregate(np.mean), agged, check_index_type=False\n    )\n\n    # complex agg\n    agged = grouped.aggregate([np.mean, np.std])\n\n    msg = r\"nested renamer is not supported\"\n    with pytest.raises(SpecificationError, match=msg):\n        grouped.aggregate({\"one\": np.mean, \"two\": np.std})\n\n    group_constants = {0: 10, 1: 20, 2: 30}\n    msg = (\n        \"Pinning the groupby key to each group in SeriesGroupBy.agg is deprecated, \"\n        \"and cases that relied on it will raise in a future version\"\n    )\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        # GH#41090\n        agged = grouped.agg(lambda x: group_constants[x.name] + x.mean())\n    assert agged[1] == 21\n\n    # corner cases\n    msg = \"Must produce aggregated value\"\n    # exception raised is type Exception\n    with pytest.raises(Exception, match=msg):\n        grouped.aggregate(lambda x: x * 2)\n\n\ndef test_groupby_nonobject_dtype(mframe, df_mixed_floats):\n    key = mframe.index.codes[0]\n    grouped = mframe.groupby(key)\n    result = grouped.sum()\n\n    expected = mframe.groupby(key.astype(\"O\")).sum()\n    assert result.index.dtype == np.int8\n    assert expected.index.dtype == np.int64\n    tm.assert_frame_equal(result, expected, check_index_type=False)\n\n    # GH 3911, mixed frame non-conversion\n    df = df_mixed_floats.copy()\n    df[\"value\"] = range(len(df))\n\n    def max_value(group):\n        return group.loc[group[\"value\"].idxmax()]\n\n    msg = \"DataFrameGroupBy.apply operated on the grouping columns\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        applied = df.groupby(\"A\").apply(max_value)\n    result = applied.dtypes\n    expected = df.dtypes\n    tm.assert_series_equal(result, expected)\n\n\ndef test_inconsistent_return_type():\n    # GH5592\n    # inconsistent return type\n    df = DataFrame(\n        {\n            \"A\": [\"Tiger\", \"Tiger\", \"Tiger\", \"Lamb\", \"Lamb\", \"Pony\", \"Pony\"],\n            \"B\": Series(np.arange(7), dtype=\"int64\"),\n            \"C\": date_range(\"20130101\", periods=7),\n        }\n    )\n\n    def f_0(grp):\n        return grp.iloc[0]\n\n    expected = df.groupby(\"A\").first()[[\"B\"]]\n    msg = \"DataFrameGroupBy.apply operated on the grouping columns\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        result = df.groupby(\"A\").apply(f_0)[[\"B\"]]\n    tm.assert_frame_equal(result, expected)\n\n    def f_1(grp):\n        if grp.name == \"Tiger\":\n            return None\n        return grp.iloc[0]\n\n    msg = \"DataFrameGroupBy.apply operated on the grouping columns\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        result = df.groupby(\"A\").apply(f_1)[[\"B\"]]\n    e = expected.copy()\n    e.loc[\"Tiger\"] = np.nan\n    tm.assert_frame_equal(result, e)\n\n    def f_2(grp):\n        if grp.name == \"Pony\":\n            return None\n        return grp.iloc[0]\n\n    msg = \"DataFrameGroupBy.apply operated on the grouping columns\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        result = df.groupby(\"A\").apply(f_2)[[\"B\"]]\n    e = expected.copy()\n    e.loc[\"Pony\"] = np.nan\n    tm.assert_frame_equal(result, e)\n\n    # 5592 revisited, with datetimes\n    def f_3(grp):\n        if grp.name == \"Pony\":\n            return None\n        return grp.iloc[0]\n\n    msg = \"DataFrameGroupBy.apply operated on the grouping columns\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        result = df.groupby(\"A\").apply(f_3)[[\"C\"]]\n    e = df.groupby(\"A\").first()[[\"C\"]]\n    e.loc[\"Pony\"] = pd.NaT\n    tm.assert_frame_equal(result, e)\n\n    # scalar outputs\n    def f_4(grp):\n        if grp.name == \"Pony\":\n            return None\n        return grp.iloc[0].loc[\"C\"]\n\n    msg = \"DataFrameGroupBy.apply operated on the grouping columns\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        result = df.groupby(\"A\").apply(f_4)\n    e = df.groupby(\"A\").first()[\"C\"].copy()\n    e.loc[\"Pony\"] = np.nan\n    e.name = None\n    tm.assert_series_equal(result, e)\n\n\ndef test_pass_args_kwargs(ts, tsframe):\n    def f(x, q=None, axis=0):\n        return np.percentile(x, q, axis=axis)\n\n    g = lambda x: np.percentile(x, 80, axis=0)\n\n    # Series\n    ts_grouped = ts.groupby(lambda x: x.month)\n    agg_result = ts_grouped.agg(np.percentile, 80, axis=0)\n    apply_result = ts_grouped.apply(np.percentile, 80, axis=0)\n    trans_result = ts_grouped.transform(np.percentile, 80, axis=0)\n\n    agg_expected = ts_grouped.quantile(0.8)\n    trans_expected = ts_grouped.transform(g)\n\n    tm.assert_series_equal(apply_result, agg_expected)\n    tm.assert_series_equal(agg_result, agg_expected)\n    tm.assert_series_equal(trans_result, trans_expected)\n\n    agg_result = ts_grouped.agg(f, q=80)\n    apply_result = ts_grouped.apply(f, q=80)\n    trans_result = ts_grouped.transform(f, q=80)\n    tm.assert_series_equal(agg_result, agg_expected)\n    tm.assert_series_equal(apply_result, agg_expected)\n    tm.assert_series_equal(trans_result, trans_expected)\n\n    # DataFrame\n    for as_index in [True, False]:\n        df_grouped = tsframe.groupby(lambda x: x.month, as_index=as_index)\n        warn = None if as_index else FutureWarning\n        msg = \"A grouping .* was excluded from the result\"\n        with tm.assert_produces_warning(warn, match=msg):\n            agg_result = df_grouped.agg(np.percentile, 80, axis=0)\n        with tm.assert_produces_warning(warn, match=msg):\n            apply_result = df_grouped.apply(DataFrame.quantile, 0.8)\n        with tm.assert_produces_warning(warn, match=msg):\n            expected = df_grouped.quantile(0.8)\n        tm.assert_frame_equal(apply_result, expected, check_names=False)\n        tm.assert_frame_equal(agg_result, expected)\n\n        apply_result = df_grouped.apply(DataFrame.quantile, [0.4, 0.8])\n        with tm.assert_produces_warning(warn, match=msg):\n            expected_seq = df_grouped.quantile([0.4, 0.8])\n        tm.assert_frame_equal(apply_result, expected_seq, check_names=False)\n\n        with tm.assert_produces_warning(warn, match=msg):\n            agg_result = df_grouped.agg(f, q=80)\n        with tm.assert_produces_warning(warn, match=msg):\n            apply_result = df_grouped.apply(DataFrame.quantile, q=0.8)\n        tm.assert_frame_equal(agg_result, expected)\n        tm.assert_frame_equal(apply_result, expected, check_names=False)\n\n\n@pytest.mark.parametrize(\"as_index\", [True, False])\ndef test_pass_args_kwargs_duplicate_columns(tsframe, as_index):\n    # go through _aggregate_frame with self.axis == 0 and duplicate columns\n    tsframe.columns = [\"A\", \"B\", \"A\", \"C\"]\n    gb = tsframe.groupby(lambda x: x.month, as_index=as_index)\n\n    warn = None if as_index else FutureWarning\n    msg = \"A grouping .* was excluded from the result\"\n    with tm.assert_produces_warning(warn, match=msg):\n        res = gb.agg(np.percentile, 80, axis=0)\n\n    ex_data = {\n        1: tsframe[tsframe.index.month == 1].quantile(0.8),\n        2: tsframe[tsframe.index.month == 2].quantile(0.8),\n    }\n    expected = DataFrame(ex_data).T\n    if not as_index:\n        # TODO: try to get this more consistent?\n        expected.index = Index(range(2))\n\n    tm.assert_frame_equal(res, expected)\n\n\ndef test_len():\n    df = tm.makeTimeDataFrame()\n    grouped = df.groupby([lambda x: x.year, lambda x: x.month, lambda x: x.day])\n    assert len(grouped) == len(df)\n\n    grouped = df.groupby([lambda x: x.year, lambda x: x.month])\n    expected = len({(x.year, x.month) for x in df.index})\n    assert len(grouped) == expected\n\n    # issue 11016\n    df = DataFrame({\"a\": [np.nan] * 3, \"b\": [1, 2, 3]})\n    assert len(df.groupby(\"a\")) == 0\n    assert len(df.groupby(\"b\")) == 3\n    assert len(df.groupby([\"a\", \"b\"])) == 3\n\n\ndef test_basic_regression():\n    # regression\n    result = Series([1.0 * x for x in list(range(1, 10)) * 10])\n\n    data = np.random.random(1100) * 10.0\n    groupings = Series(data)\n\n    grouped = result.groupby(groupings)\n    grouped.mean()\n\n\n@pytest.mark.parametrize(\n    \"dtype\", [\"float64\", \"float32\", \"int64\", \"int32\", \"int16\", \"int8\"]\n)\ndef test_with_na_groups(dtype):\n    index = Index(np.arange(10))\n    values = Series(np.ones(10), index, dtype=dtype)\n    labels = Series(\n        [np.nan, \"foo\", \"bar\", \"bar\", np.nan, np.nan, \"bar\", \"bar\", np.nan, \"foo\"],\n        index=index,\n    )\n\n    # this SHOULD be an int\n    grouped = values.groupby(labels)\n    agged = grouped.agg(len)\n    expected = Series([4, 2], index=[\"bar\", \"foo\"])\n\n    tm.assert_series_equal(agged, expected, check_dtype=False)\n\n    # assert issubclass(agged.dtype.type, np.integer)\n\n    # explicitly return a float from my function\n    def f(x):\n        return float(len(x))\n\n    agged = grouped.agg(f)\n    expected = Series([4.0, 2.0], index=[\"bar\", \"foo\"])\n\n    tm.assert_series_equal(agged, expected)\n\n\ndef test_indices_concatenation_order():\n    # GH 2808\n\n    def f1(x):\n        y = x[(x.b % 2) == 1] ** 2\n        if y.empty:\n            multiindex = MultiIndex(levels=[[]] * 2, codes=[[]] * 2, names=[\"b\", \"c\"])\n            res = DataFrame(columns=[\"a\"], index=multiindex)\n            return res\n        else:\n            y = y.set_index([\"b\", \"c\"])\n            return y\n\n    def f2(x):\n        y = x[(x.b % 2) == 1] ** 2\n        if y.empty:\n            return DataFrame()\n        else:\n            y = y.set_index([\"b\", \"c\"])\n            return y\n\n    def f3(x):\n        y = x[(x.b % 2) == 1] ** 2\n        if y.empty:\n            multiindex = MultiIndex(\n                levels=[[]] * 2, codes=[[]] * 2, names=[\"foo\", \"bar\"]\n            )\n            res = DataFrame(columns=[\"a\", \"b\"], index=multiindex)\n            return res\n        else:\n            return y\n\n    df = DataFrame({\"a\": [1, 2, 2, 2], \"b\": range(4), \"c\": range(5, 9)})\n\n    df2 = DataFrame({\"a\": [3, 2, 2, 2], \"b\": range(4), \"c\": range(5, 9)})\n\n    # correct result\n    msg = \"DataFrameGroupBy.apply operated on the grouping columns\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        result1 = df.groupby(\"a\").apply(f1)\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        result2 = df2.groupby(\"a\").apply(f1)\n    tm.assert_frame_equal(result1, result2)\n\n    # should fail (not the same number of levels)\n    msg = \"Cannot concat indices that do not have the same number of levels\"\n    with pytest.raises(AssertionError, match=msg):\n        df.groupby(\"a\").apply(f2)\n    with pytest.raises(AssertionError, match=msg):\n        df2.groupby(\"a\").apply(f2)\n\n    # should fail (incorrect shape)\n    with pytest.raises(AssertionError, match=msg):\n        df.groupby(\"a\").apply(f3)\n    with pytest.raises(AssertionError, match=msg):\n        df2.groupby(\"a\").apply(f3)\n\n\ndef test_attr_wrapper(ts):\n    grouped = ts.groupby(lambda x: x.weekday())\n\n    result = grouped.std()\n    expected = grouped.agg(lambda x: np.std(x, ddof=1))\n    tm.assert_series_equal(result, expected)\n\n    # this is pretty cool\n    result = grouped.describe()\n    expected = {name: gp.describe() for name, gp in grouped}\n    expected = DataFrame(expected).T\n    tm.assert_frame_equal(result, expected)\n\n    # get attribute\n    result = grouped.dtype\n    expected = grouped.agg(lambda x: x.dtype)\n    tm.assert_series_equal(result, expected)\n\n    # make sure raises error\n    msg = \"'SeriesGroupBy' object has no attribute 'foo'\"\n    with pytest.raises(AttributeError, match=msg):\n        getattr(grouped, \"foo\")\n\n\ndef test_frame_groupby(tsframe):\n    grouped = tsframe.groupby(lambda x: x.weekday())\n\n    # aggregate\n    aggregated = grouped.aggregate(np.mean)\n    assert len(aggregated) == 5\n    assert len(aggregated.columns) == 4\n\n    # by string\n    tscopy = tsframe.copy()\n    tscopy[\"weekday\"] = [x.weekday() for x in tscopy.index]\n    stragged = tscopy.groupby(\"weekday\").aggregate(np.mean)\n    tm.assert_frame_equal(stragged, aggregated, check_names=False)\n\n    # transform\n    grouped = tsframe.head(30).groupby(lambda x: x.weekday())\n    transformed = grouped.transform(lambda x: x - x.mean())\n    assert len(transformed) == 30\n    assert len(transformed.columns) == 4\n\n    # transform propagate\n    transformed = grouped.transform(lambda x: x.mean())\n    for name, group in grouped:\n        mean = group.mean()\n        for idx in group.index:\n            tm.assert_series_equal(transformed.xs(idx), mean, check_names=False)\n\n    # iterate\n    for weekday, group in grouped:\n        assert group.index[0].weekday() == weekday\n\n    # groups / group_indices\n    groups = grouped.groups\n    indices = grouped.indices\n\n    for k, v in groups.items():\n        samething = tsframe.index.take(indices[k])\n        assert (samething == v).all()\n\n\ndef test_frame_groupby_columns(tsframe):\n    mapping = {\"A\": 0, \"B\": 0, \"C\": 1, \"D\": 1}\n    msg = \"DataFrame.groupby with axis=1 is deprecated\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        grouped = tsframe.groupby(mapping, axis=1)\n\n    # aggregate\n    aggregated = grouped.aggregate(np.mean)\n    assert len(aggregated) == len(tsframe)\n    assert len(aggregated.columns) == 2\n\n    # transform\n    tf = lambda x: x - x.mean()\n    msg = \"The 'axis' keyword in DataFrame.groupby is deprecated\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        groupedT = tsframe.T.groupby(mapping, axis=0)\n    tm.assert_frame_equal(groupedT.transform(tf).T, grouped.transform(tf))\n\n    # iterate\n    for k, v in grouped:\n        assert len(v.columns) == 2\n\n\ndef test_frame_set_name_single(df):\n    grouped = df.groupby(\"A\")\n\n    result = grouped.mean(numeric_only=True)\n    assert result.index.name == \"A\"\n\n    result = df.groupby(\"A\", as_index=False).mean(numeric_only=True)\n    assert result.index.name != \"A\"\n\n    result = grouped[[\"C\", \"D\"]].agg(np.mean)\n    assert result.index.name == \"A\"\n\n    result = grouped.agg({\"C\": np.mean, \"D\": np.std})\n    assert result.index.name == \"A\"\n\n    result = grouped[\"C\"].mean()\n    assert result.index.name == \"A\"\n    result = grouped[\"C\"].agg(np.mean)\n    assert result.index.name == \"A\"\n    result = grouped[\"C\"].agg([np.mean, np.std])\n    assert result.index.name == \"A\"\n\n    msg = r\"nested renamer is not supported\"\n    with pytest.raises(SpecificationError, match=msg):\n        grouped[\"C\"].agg({\"foo\": np.mean, \"bar\": np.std})\n\n\ndef test_multi_func(df):\n    col1 = df[\"A\"]\n    col2 = df[\"B\"]\n\n    grouped = df.groupby([col1.get, col2.get])\n    agged = grouped.mean(numeric_only=True)\n    expected = df.groupby([\"A\", \"B\"]).mean()\n\n    # TODO groupby get drops names\n    tm.assert_frame_equal(\n        agged.loc[:, [\"C\", \"D\"]], expected.loc[:, [\"C\", \"D\"]], check_names=False\n    )\n\n    # some \"groups\" with no data\n    df = DataFrame(\n        {\n            \"v1\": np.random.randn(6),\n            \"v2\": np.random.randn(6),\n            \"k1\": np.array([\"b\", \"b\", \"b\", \"a\", \"a\", \"a\"]),\n            \"k2\": np.array([\"1\", \"1\", \"1\", \"2\", \"2\", \"2\"]),\n        },\n        index=[\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"],\n    )\n    # only verify that it works for now\n    grouped = df.groupby([\"k1\", \"k2\"])\n    grouped.agg(np.sum)\n\n\ndef test_multi_key_multiple_functions(df):\n    grouped = df.groupby([\"A\", \"B\"])[\"C\"]\n\n    agged = grouped.agg([np.mean, np.std])\n    expected = DataFrame({\"mean\": grouped.agg(np.mean), \"std\": grouped.agg(np.std)})\n    tm.assert_frame_equal(agged, expected)\n\n\ndef test_frame_multi_key_function_list():\n    data = DataFrame(\n        {\n            \"A\": [\n                \"foo\",\n                \"foo\",\n                \"foo\",\n                \"foo\",\n                \"bar\",\n                \"bar\",\n                \"bar\",\n                \"bar\",\n                \"foo\",\n                \"foo\",\n                \"foo\",\n            ],\n            \"B\": [\n                \"one\",\n                \"one\",\n                \"one\",\n                \"two\",\n                \"one\",\n                \"one\",\n                \"one\",\n                \"two\",\n                \"two\",\n                \"two\",\n                \"one\",\n            ],\n            \"D\": np.random.randn(11),\n            \"E\": np.random.randn(11),\n            \"F\": np.random.randn(11),\n        }\n    )\n\n    grouped = data.groupby([\"A\", \"B\"])\n    funcs = [np.mean, np.std]\n    agged = grouped.agg(funcs)\n    expected = pd.concat(\n        [grouped[\"D\"].agg(funcs), grouped[\"E\"].agg(funcs), grouped[\"F\"].agg(funcs)],\n        keys=[\"D\", \"E\", \"F\"],\n        axis=1,\n    )\n    assert isinstance(agged.index, MultiIndex)\n    assert isinstance(expected.index, MultiIndex)\n    tm.assert_frame_equal(agged, expected)\n\n\ndef test_frame_multi_key_function_list_partial_failure():\n    data = DataFrame(\n        {\n            \"A\": [\n                \"foo\",\n                \"foo\",\n                \"foo\",\n                \"foo\",\n                \"bar\",\n                \"bar\",\n                \"bar\",\n                \"bar\",\n                \"foo\",\n                \"foo\",\n                \"foo\",\n            ],\n            \"B\": [\n                \"one\",\n                \"one\",\n                \"one\",\n                \"two\",\n                \"one\",\n                \"one\",\n                \"one\",\n                \"two\",\n                \"two\",\n                \"two\",\n                \"one\",\n            ],\n            \"C\": [\n                \"dull\",\n                \"dull\",\n                \"shiny\",\n                \"dull\",\n                \"dull\",\n                \"shiny\",\n                \"shiny\",\n                \"dull\",\n                \"shiny\",\n                \"shiny\",\n                \"shiny\",\n            ],\n            \"D\": np.random.randn(11),\n            \"E\": np.random.randn(11),\n            \"F\": np.random.randn(11),\n        }\n    )\n\n    grouped = data.groupby([\"A\", \"B\"])\n    funcs = [np.mean, np.std]\n    with pytest.raises(TypeError, match=\"Could not convert dullshinyshiny to numeric\"):\n        grouped.agg(funcs)\n\n\n@pytest.mark.parametrize(\"op\", [lambda x: x.sum(), lambda x: x.mean()])\ndef test_groupby_multiple_columns(df, op):\n    data = df\n    grouped = data.groupby([\"A\", \"B\"])\n\n    result1 = op(grouped)\n\n    keys = []\n    values = []\n    for n1, gp1 in data.groupby(\"A\"):\n        for n2, gp2 in gp1.groupby(\"B\"):\n            keys.append((n1, n2))\n            values.append(op(gp2.loc[:, [\"C\", \"D\"]]))\n\n    mi = MultiIndex.from_tuples(keys, names=[\"A\", \"B\"])\n    expected = pd.concat(values, axis=1).T\n    expected.index = mi\n\n    # a little bit crude\n    for col in [\"C\", \"D\"]:\n        result_col = op(grouped[col])\n        pivoted = result1[col]\n        exp = expected[col]\n        tm.assert_series_equal(result_col, exp)\n        tm.assert_series_equal(pivoted, exp)\n\n    # test single series works the same\n    result = data[\"C\"].groupby([data[\"A\"], data[\"B\"]]).mean()\n    expected = data.groupby([\"A\", \"B\"]).mean()[\"C\"]\n\n    tm.assert_series_equal(result, expected)\n\n\ndef test_as_index_select_column():\n    # GH 5764\n    df = DataFrame([[1, 2], [1, 4], [5, 6]], columns=[\"A\", \"B\"])\n    result = df.groupby(\"A\", as_index=False)[\"B\"].get_group(1)\n    expected = Series([2, 4], name=\"B\")\n    tm.assert_series_equal(result, expected)\n\n    result = df.groupby(\"A\", as_index=False, group_keys=True)[\"B\"].apply(\n        lambda x: x.cumsum()\n    )\n    expected = Series(\n        [2, 6, 6], name=\"B\", index=MultiIndex.from_tuples([(0, 0), (0, 1), (1, 2)])\n    )\n    tm.assert_series_equal(result, expected)\n\n\ndef test_groupby_as_index_select_column_sum_empty_df():\n    # GH 35246\n    df = DataFrame(columns=Index([\"A\", \"B\", \"C\"], name=\"alpha\"))\n    left = df.groupby(by=\"A\", as_index=False)[\"B\"].sum(numeric_only=False)\n\n    expected = DataFrame(columns=df.columns[:2], index=range(0))\n    # GH#50744 - Columns after selection shouldn't retain names\n    expected.columns.names = [None]\n    tm.assert_frame_equal(left, expected)\n\n\ndef test_groupby_as_index_agg(df):\n    grouped = df.groupby(\"A\", as_index=False)\n\n    # single-key\n\n    result = grouped[[\"C\", \"D\"]].agg(np.mean)\n    expected = grouped.mean(numeric_only=True)\n    tm.assert_frame_equal(result, expected)\n\n    result2 = grouped.agg({\"C\": np.mean, \"D\": np.sum})\n    expected2 = grouped.mean(numeric_only=True)\n    expected2[\"D\"] = grouped.sum()[\"D\"]\n    tm.assert_frame_equal(result2, expected2)\n\n    grouped = df.groupby(\"A\", as_index=True)\n\n    msg = r\"nested renamer is not supported\"\n    with pytest.raises(SpecificationError, match=msg):\n        grouped[\"C\"].agg({\"Q\": np.sum})\n\n    # multi-key\n\n    grouped = df.groupby([\"A\", \"B\"], as_index=False)\n\n    result = grouped.agg(np.mean)\n    expected = grouped.mean()\n    tm.assert_frame_equal(result, expected)\n\n    result2 = grouped.agg({\"C\": np.mean, \"D\": np.sum})\n    expected2 = grouped.mean()\n    expected2[\"D\"] = grouped.sum()[\"D\"]\n    tm.assert_frame_equal(result2, expected2)\n\n    expected3 = grouped[\"C\"].sum()\n    expected3 = DataFrame(expected3).rename(columns={\"C\": \"Q\"})\n    msg = \"Passing a dictionary to SeriesGroupBy.agg is deprecated\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        result3 = grouped[\"C\"].agg({\"Q\": np.sum})\n    tm.assert_frame_equal(result3, expected3)\n\n    # GH7115 & GH8112 & GH8582\n    df = DataFrame(np.random.randint(0, 100, (50, 3)), columns=[\"jim\", \"joe\", \"jolie\"])\n    ts = Series(np.random.randint(5, 10, 50), name=\"jim\")\n\n    gr = df.groupby(ts)\n    gr.nth(0)  # invokes set_selection_from_grouper internally\n    tm.assert_frame_equal(gr.apply(sum), df.groupby(ts).apply(sum))\n\n    for attr in [\"mean\", \"max\", \"count\", \"idxmax\", \"cumsum\", \"all\"]:\n        gr = df.groupby(ts, as_index=False)\n        left = getattr(gr, attr)()\n\n        gr = df.groupby(ts.values, as_index=True)\n        right = getattr(gr, attr)().reset_index(drop=True)\n\n        tm.assert_frame_equal(left, right)\n\n\ndef test_ops_not_as_index(reduction_func):\n    # GH 10355, 21090\n    # Using as_index=False should not modify grouped column\n\n    if reduction_func in (\"corrwith\", \"nth\", \"ngroup\"):\n        pytest.skip(f\"GH 5755: Test not applicable for {reduction_func}\")\n\n    df = DataFrame(np.random.randint(0, 5, size=(100, 2)), columns=[\"a\", \"b\"])\n    expected = getattr(df.groupby(\"a\"), reduction_func)()\n    if reduction_func == \"size\":\n        expected = expected.rename(\"size\")\n    expected = expected.reset_index()\n\n    if reduction_func != \"size\":\n        # 32 bit compat -> groupby preserves dtype whereas reset_index casts to int64\n        expected[\"a\"] = expected[\"a\"].astype(df[\"a\"].dtype)\n\n    g = df.groupby(\"a\", as_index=False)\n\n    result = getattr(g, reduction_func)()\n    tm.assert_frame_equal(result, expected)\n\n    result = g.agg(reduction_func)\n    tm.assert_frame_equal(result, expected)\n\n    result = getattr(g[\"b\"], reduction_func)()\n    tm.assert_frame_equal(result, expected)\n\n    result = g[\"b\"].agg(reduction_func)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_as_index_series_return_frame(df):\n    grouped = df.groupby(\"A\", as_index=False)\n    grouped2 = df.groupby([\"A\", \"B\"], as_index=False)\n\n    result = grouped[\"C\"].agg(np.sum)\n    expected = grouped.agg(np.sum).loc[:, [\"A\", \"C\"]]\n    assert isinstance(result, DataFrame)\n    tm.assert_frame_equal(result, expected)\n\n    result2 = grouped2[\"C\"].agg(np.sum)\n    expected2 = grouped2.agg(np.sum).loc[:, [\"A\", \"B\", \"C\"]]\n    assert isinstance(result2, DataFrame)\n    tm.assert_frame_equal(result2, expected2)\n\n    result = grouped[\"C\"].sum()\n    expected = grouped.sum().loc[:, [\"A\", \"C\"]]\n    assert isinstance(result, DataFrame)\n    tm.assert_frame_equal(result, expected)\n\n    result2 = grouped2[\"C\"].sum()\n    expected2 = grouped2.sum().loc[:, [\"A\", \"B\", \"C\"]]\n    assert isinstance(result2, DataFrame)\n    tm.assert_frame_equal(result2, expected2)\n\n\ndef test_as_index_series_column_slice_raises(df):\n    # GH15072\n    grouped = df.groupby(\"A\", as_index=False)\n    msg = r\"Column\\(s\\) C already selected\"\n\n    with pytest.raises(IndexError, match=msg):\n        grouped[\"C\"].__getitem__(\"D\")\n\n\ndef test_groupby_as_index_cython(df):\n    data = df\n\n    # single-key\n    grouped = data.groupby(\"A\", as_index=False)\n    result = grouped.mean(numeric_only=True)\n    expected = data.groupby([\"A\"]).mean(numeric_only=True)\n    expected.insert(0, \"A\", expected.index)\n    expected.index = RangeIndex(len(expected))\n    tm.assert_frame_equal(result, expected)\n\n    # multi-key\n    grouped = data.groupby([\"A\", \"B\"], as_index=False)\n    result = grouped.mean()\n    expected = data.groupby([\"A\", \"B\"]).mean()\n\n    arrays = list(zip(*expected.index.values))\n    expected.insert(0, \"A\", arrays[0])\n    expected.insert(1, \"B\", arrays[1])\n    expected.index = RangeIndex(len(expected))\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_as_index_series_scalar(df):\n    grouped = df.groupby([\"A\", \"B\"], as_index=False)\n\n    # GH #421\n\n    result = grouped[\"C\"].agg(len)\n    expected = grouped.agg(len).loc[:, [\"A\", \"B\", \"C\"]]\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_as_index_corner(df, ts):\n    msg = \"as_index=False only valid with DataFrame\"\n    with pytest.raises(TypeError, match=msg):\n        ts.groupby(lambda x: x.weekday(), as_index=False)\n\n    msg = \"as_index=False only valid for axis=0\"\n    depr_msg = \"DataFrame.groupby with axis=1 is deprecated\"\n    with pytest.raises(ValueError, match=msg):\n        with tm.assert_produces_warning(FutureWarning, match=depr_msg):\n            df.groupby(lambda x: x.lower(), as_index=False, axis=1)\n\n\ndef test_groupby_multiple_key():\n    df = tm.makeTimeDataFrame()\n    grouped = df.groupby([lambda x: x.year, lambda x: x.month, lambda x: x.day])\n    agged = grouped.sum()\n    tm.assert_almost_equal(df.values, agged.values)\n\n    depr_msg = \"DataFrame.groupby with axis=1 is deprecated\"\n    with tm.assert_produces_warning(FutureWarning, match=depr_msg):\n        grouped = df.T.groupby(\n            [lambda x: x.year, lambda x: x.month, lambda x: x.day], axis=1\n        )\n\n    agged = grouped.agg(lambda x: x.sum())\n    tm.assert_index_equal(agged.index, df.columns)\n    tm.assert_almost_equal(df.T.values, agged.values)\n\n    agged = grouped.agg(lambda x: x.sum())\n    tm.assert_almost_equal(df.T.values, agged.values)\n\n\ndef test_groupby_multi_corner(df):\n    # test that having an all-NA column doesn't mess you up\n    df = df.copy()\n    df[\"bad\"] = np.nan\n    agged = df.groupby([\"A\", \"B\"]).mean()\n\n    expected = df.groupby([\"A\", \"B\"]).mean()\n    expected[\"bad\"] = np.nan\n\n    tm.assert_frame_equal(agged, expected)\n\n\ndef test_raises_on_nuisance(df):\n    grouped = df.groupby(\"A\")\n    with pytest.raises(TypeError, match=\"Could not convert\"):\n        grouped.agg(np.mean)\n    with pytest.raises(TypeError, match=\"Could not convert\"):\n        grouped.mean()\n\n    df = df.loc[:, [\"A\", \"C\", \"D\"]]\n    df[\"E\"] = datetime.now()\n    grouped = df.groupby(\"A\")\n    msg = \"datetime64 type does not support sum operations\"\n    with pytest.raises(TypeError, match=msg):\n        grouped.agg(np.sum)\n    with pytest.raises(TypeError, match=msg):\n        grouped.sum()\n\n    # won't work with axis = 1\n    depr_msg = \"DataFrame.groupby with axis=1 is deprecated\"\n    with tm.assert_produces_warning(FutureWarning, match=depr_msg):\n        grouped = df.groupby({\"A\": 0, \"C\": 0, \"D\": 1, \"E\": 1}, axis=1)\n    msg = \"does not support reduction 'sum'\"\n    with pytest.raises(TypeError, match=msg):\n        grouped.agg(lambda x: x.sum(0, numeric_only=False))\n\n\n@pytest.mark.parametrize(\n    \"agg_function\",\n    [\"max\", \"min\"],\n)\ndef test_keep_nuisance_agg(df, agg_function):\n    # GH 38815\n    grouped = df.groupby(\"A\")\n    result = getattr(grouped, agg_function)()\n    expected = result.copy()\n    expected.loc[\"bar\", \"B\"] = getattr(df.loc[df[\"A\"] == \"bar\", \"B\"], agg_function)()\n    expected.loc[\"foo\", \"B\"] = getattr(df.loc[df[\"A\"] == \"foo\", \"B\"], agg_function)()\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"agg_function\",\n    [\"sum\", \"mean\", \"prod\", \"std\", \"var\", \"sem\", \"median\"],\n)\n@pytest.mark.parametrize(\"numeric_only\", [True, False])\ndef test_omit_nuisance_agg(df, agg_function, numeric_only):\n    # GH 38774, GH 38815\n    grouped = df.groupby(\"A\")\n\n    no_drop_nuisance = (\"var\", \"std\", \"sem\", \"mean\", \"prod\", \"median\")\n    if agg_function in no_drop_nuisance and not numeric_only:\n        # Added numeric_only as part of GH#46560; these do not drop nuisance\n        # columns when numeric_only is False\n        klass = ValueError if agg_function in (\"std\", \"sem\") else TypeError\n        msg = \"|\".join([\"[C|c]ould not convert\", \"can't multiply sequence\"])\n        with pytest.raises(klass, match=msg):\n            getattr(grouped, agg_function)(numeric_only=numeric_only)\n    else:\n        result = getattr(grouped, agg_function)(numeric_only=numeric_only)\n        if not numeric_only and agg_function == \"sum\":\n            # sum is successful on column B\n            columns = [\"A\", \"B\", \"C\", \"D\"]\n        else:\n            columns = [\"A\", \"C\", \"D\"]\n        expected = getattr(df.loc[:, columns].groupby(\"A\"), agg_function)(\n            numeric_only=numeric_only\n        )\n        tm.assert_frame_equal(result, expected)\n\n\ndef test_raise_on_nuisance_python_single(df):\n    # GH 38815\n    grouped = df.groupby(\"A\")\n    with pytest.raises(ValueError, match=\"could not convert\"):\n        grouped.skew()\n\n\ndef test_raise_on_nuisance_python_multiple(three_group):\n    grouped = three_group.groupby([\"A\", \"B\"])\n    with pytest.raises(TypeError, match=\"Could not convert\"):\n        grouped.agg(np.mean)\n    with pytest.raises(TypeError, match=\"Could not convert\"):\n        grouped.mean()\n\n\ndef test_empty_groups_corner(mframe):\n    # handle empty groups\n    df = DataFrame(\n        {\n            \"k1\": np.array([\"b\", \"b\", \"b\", \"a\", \"a\", \"a\"]),\n            \"k2\": np.array([\"1\", \"1\", \"1\", \"2\", \"2\", \"2\"]),\n            \"k3\": [\"foo\", \"bar\"] * 3,\n            \"v1\": np.random.randn(6),\n            \"v2\": np.random.randn(6),\n        }\n    )\n\n    grouped = df.groupby([\"k1\", \"k2\"])\n    result = grouped[[\"v1\", \"v2\"]].agg(np.mean)\n    expected = grouped.mean(numeric_only=True)\n    tm.assert_frame_equal(result, expected)\n\n    grouped = mframe[3:5].groupby(level=0)\n    agged = grouped.apply(lambda x: x.mean())\n    agged_A = grouped[\"A\"].apply(np.mean)\n    tm.assert_series_equal(agged[\"A\"], agged_A)\n    assert agged.index.name == \"first\"\n\n\ndef test_nonsense_func():\n    df = DataFrame([0])\n    msg = r\"unsupported operand type\\(s\\) for \\+: 'int' and 'str'\"\n    with pytest.raises(TypeError, match=msg):\n        df.groupby(lambda x: x + \"foo\")\n\n\ndef test_wrap_aggregated_output_multindex(mframe):\n    df = mframe.T\n    df[\"baz\", \"two\"] = \"peekaboo\"\n\n    keys = [np.array([0, 0, 1]), np.array([0, 0, 1])]\n    with pytest.raises(TypeError, match=\"Could not convert\"):\n        df.groupby(keys).agg(np.mean)\n    agged = df.drop(columns=(\"baz\", \"two\")).groupby(keys).agg(np.mean)\n    assert isinstance(agged.columns, MultiIndex)\n\n    def aggfun(ser):\n        if ser.name == (\"foo\", \"one\"):\n            raise TypeError(\"Test error message\")\n        return ser.sum()\n\n    with pytest.raises(TypeError, match=\"Test error message\"):\n        df.groupby(keys).aggregate(aggfun)\n\n\ndef test_groupby_level_apply(mframe):\n    result = mframe.groupby(level=0).count()\n    assert result.index.name == \"first\"\n    result = mframe.groupby(level=1).count()\n    assert result.index.name == \"second\"\n\n    result = mframe[\"A\"].groupby(level=0).count()\n    assert result.index.name == \"first\"\n\n\ndef test_groupby_level_mapper(mframe):\n    deleveled = mframe.reset_index()\n\n    mapper0 = {\"foo\": 0, \"bar\": 0, \"baz\": 1, \"qux\": 1}\n    mapper1 = {\"one\": 0, \"two\": 0, \"three\": 1}\n\n    result0 = mframe.groupby(mapper0, level=0).sum()\n    result1 = mframe.groupby(mapper1, level=1).sum()\n\n    mapped_level0 = np.array(\n        [mapper0.get(x) for x in deleveled[\"first\"]], dtype=np.int64\n    )\n    mapped_level1 = np.array(\n        [mapper1.get(x) for x in deleveled[\"second\"]], dtype=np.int64\n    )\n    expected0 = mframe.groupby(mapped_level0).sum()\n    expected1 = mframe.groupby(mapped_level1).sum()\n    expected0.index.name, expected1.index.name = \"first\", \"second\"\n\n    tm.assert_frame_equal(result0, expected0)\n    tm.assert_frame_equal(result1, expected1)\n\n\ndef test_groupby_level_nonmulti():\n    # GH 1313, GH 13901\n    s = Series([1, 2, 3, 10, 4, 5, 20, 6], Index([1, 2, 3, 1, 4, 5, 2, 6], name=\"foo\"))\n    expected = Series([11, 22, 3, 4, 5, 6], Index(range(1, 7), name=\"foo\"))\n\n    result = s.groupby(level=0).sum()\n    tm.assert_series_equal(result, expected)\n    result = s.groupby(level=[0]).sum()\n    tm.assert_series_equal(result, expected)\n    result = s.groupby(level=-1).sum()\n    tm.assert_series_equal(result, expected)\n    result = s.groupby(level=[-1]).sum()\n    tm.assert_series_equal(result, expected)\n\n    msg = \"level > 0 or level < -1 only valid with MultiIndex\"\n    with pytest.raises(ValueError, match=msg):\n        s.groupby(level=1)\n    with pytest.raises(ValueError, match=msg):\n        s.groupby(level=-2)\n    msg = \"No group keys passed!\"\n    with pytest.raises(ValueError, match=msg):\n        s.groupby(level=[])\n    msg = \"multiple levels only valid with MultiIndex\"\n    with pytest.raises(ValueError, match=msg):\n        s.groupby(level=[0, 0])\n    with pytest.raises(ValueError, match=msg):\n        s.groupby(level=[0, 1])\n    msg = \"level > 0 or level < -1 only valid with MultiIndex\"\n    with pytest.raises(ValueError, match=msg):\n        s.groupby(level=[1])\n\n\ndef test_groupby_complex():\n    # GH 12902\n    a = Series(data=np.arange(4) * (1 + 2j), index=[0, 0, 1, 1])\n    expected = Series((1 + 2j, 5 + 10j))\n\n    result = a.groupby(level=0).sum()\n    tm.assert_series_equal(result, expected)\n\n\ndef test_groupby_complex_numbers():\n    # GH 17927\n    df = DataFrame(\n        [\n            {\"a\": 1, \"b\": 1 + 1j},\n            {\"a\": 1, \"b\": 1 + 2j},\n            {\"a\": 4, \"b\": 1},\n        ]\n    )\n    expected = DataFrame(\n        np.array([1, 1, 1], dtype=np.int64),\n        index=Index([(1 + 1j), (1 + 2j), (1 + 0j)], name=\"b\"),\n        columns=Index([\"a\"], dtype=\"object\"),\n    )\n    result = df.groupby(\"b\", sort=False).count()\n    tm.assert_frame_equal(result, expected)\n\n    # Sorted by the magnitude of the complex numbers\n    expected.index = Index([(1 + 0j), (1 + 1j), (1 + 2j)], name=\"b\")\n    result = df.groupby(\"b\", sort=True).count()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_series_indexed_differently():\n    s1 = Series(\n        [5.0, -9.0, 4.0, 100.0, -5.0, 55.0, 6.7],\n        index=Index([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"]),\n    )\n    s2 = Series(\n        [1.0, 1.0, 4.0, 5.0, 5.0, 7.0], index=Index([\"a\", \"b\", \"d\", \"f\", \"g\", \"h\"])\n    )\n\n    grouped = s1.groupby(s2)\n    agged = grouped.mean()\n    exp = s1.groupby(s2.reindex(s1.index).get).mean()\n    tm.assert_series_equal(agged, exp)\n\n\ndef test_groupby_with_hier_columns():\n    tuples = list(\n        zip(\n            *[\n                [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n                [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n            ]\n        )\n    )\n    index = MultiIndex.from_tuples(tuples)\n    columns = MultiIndex.from_tuples(\n        [(\"A\", \"cat\"), (\"B\", \"dog\"), (\"B\", \"cat\"), (\"A\", \"dog\")]\n    )\n    df = DataFrame(np.random.randn(8, 4), index=index, columns=columns)\n\n    result = df.groupby(level=0).mean()\n    tm.assert_index_equal(result.columns, columns)\n\n    depr_msg = \"DataFrame.groupby with axis=1 is deprecated\"\n    with tm.assert_produces_warning(FutureWarning, match=depr_msg):\n        gb = df.groupby(level=0, axis=1)\n    result = gb.mean()\n    tm.assert_index_equal(result.index, df.index)\n\n    result = df.groupby(level=0).agg(np.mean)\n    tm.assert_index_equal(result.columns, columns)\n\n    result = df.groupby(level=0).apply(lambda x: x.mean())\n    tm.assert_index_equal(result.columns, columns)\n\n    with tm.assert_produces_warning(FutureWarning, match=depr_msg):\n        gb = df.groupby(level=0, axis=1)\n    result = gb.agg(lambda x: x.mean(1))\n    tm.assert_index_equal(result.columns, Index([\"A\", \"B\"]))\n    tm.assert_index_equal(result.index, df.index)\n\n    # add a nuisance column\n    sorted_columns, _ = columns.sortlevel(0)\n    df[\"A\", \"foo\"] = \"bar\"\n    result = df.groupby(level=0).mean(numeric_only=True)\n    tm.assert_index_equal(result.columns, df.columns[:-1])\n\n\ndef test_grouping_ndarray(df):\n    grouped = df.groupby(df[\"A\"].values)\n    result = grouped.sum()\n    expected = df.groupby(df[\"A\"].rename(None)).sum()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_wrong_multi_labels():\n    index = Index([0, 1, 2, 3, 4], name=\"index\")\n    data = DataFrame(\n        {\n            \"foo\": [\"foo1\", \"foo1\", \"foo2\", \"foo1\", \"foo3\"],\n            \"bar\": [\"bar1\", \"bar2\", \"bar2\", \"bar1\", \"bar1\"],\n            \"baz\": [\"baz1\", \"baz1\", \"baz1\", \"baz2\", \"baz2\"],\n            \"spam\": [\"spam2\", \"spam3\", \"spam2\", \"spam1\", \"spam1\"],\n            \"data\": [20, 30, 40, 50, 60],\n        },\n        index=index,\n    )\n\n    grouped = data.groupby([\"foo\", \"bar\", \"baz\", \"spam\"])\n\n    result = grouped.agg(np.mean)\n    expected = grouped.mean()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_series_with_name(df):\n    result = df.groupby(df[\"A\"]).mean(numeric_only=True)\n    result2 = df.groupby(df[\"A\"], as_index=False).mean(numeric_only=True)\n    assert result.index.name == \"A\"\n    assert \"A\" in result2\n\n    result = df.groupby([df[\"A\"], df[\"B\"]]).mean()\n    result2 = df.groupby([df[\"A\"], df[\"B\"]], as_index=False).mean()\n    assert result.index.names == (\"A\", \"B\")\n    assert \"A\" in result2\n    assert \"B\" in result2\n\n\ndef test_seriesgroupby_name_attr(df):\n    # GH 6265\n    result = df.groupby(\"A\")[\"C\"]\n    assert result.count().name == \"C\"\n    assert result.mean().name == \"C\"\n\n    testFunc = lambda x: np.sum(x) * 2\n    assert result.agg(testFunc).name == \"C\"\n\n\ndef test_consistency_name():\n    # GH 12363\n\n    df = DataFrame(\n        {\n            \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\n            \"B\": [\"one\", \"one\", \"two\", \"two\", \"two\", \"two\", \"one\", \"two\"],\n            \"C\": np.random.randn(8) + 1.0,\n            \"D\": np.arange(8),\n        }\n    )\n\n    expected = df.groupby([\"A\"]).B.count()\n    result = df.B.groupby(df.A).count()\n    tm.assert_series_equal(result, expected)\n\n\ndef test_groupby_name_propagation(df):\n    # GH 6124\n    def summarize(df, name=None):\n        return Series({\"count\": 1, \"mean\": 2, \"omissions\": 3}, name=name)\n\n    def summarize_random_name(df):\n        # Provide a different name for each Series.  In this case, groupby\n        # should not attempt to propagate the Series name since they are\n        # inconsistent.\n        return Series({\"count\": 1, \"mean\": 2, \"omissions\": 3}, name=df.iloc[0][\"A\"])\n\n    msg = \"DataFrameGroupBy.apply operated on the grouping columns\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        metrics = df.groupby(\"A\").apply(summarize)\n    assert metrics.columns.name is None\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        metrics = df.groupby(\"A\").apply(summarize, \"metrics\")\n    assert metrics.columns.name == \"metrics\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        metrics = df.groupby(\"A\").apply(summarize_random_name)\n    assert metrics.columns.name is None\n\n\ndef test_groupby_nonstring_columns():\n    df = DataFrame([np.arange(10) for x in range(10)])\n    grouped = df.groupby(0)\n    result = grouped.mean()\n    expected = df.groupby(df[0]).mean()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_mixed_type_columns():\n    # GH 13432, unorderable types in py3\n    df = DataFrame([[0, 1, 2]], columns=[\"A\", \"B\", 0])\n    expected = DataFrame([[1, 2]], columns=[\"B\", 0], index=Index([0], name=\"A\"))\n\n    result = df.groupby(\"A\").first()\n    tm.assert_frame_equal(result, expected)\n\n    result = df.groupby(\"A\").sum()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_cython_grouper_series_bug_noncontig():\n    arr = np.empty((100, 100))\n    arr.fill(np.nan)\n    obj = Series(arr[:, 0])\n    inds = np.tile(range(10), 10)\n\n    result = obj.groupby(inds).agg(Series.median)\n    assert result.isna().all()\n\n\ndef test_series_grouper_noncontig_index():\n    index = Index(tm.rands_array(10, 100))\n\n    values = Series(np.random.randn(50), index=index[::2])\n    labels = np.random.randint(0, 5, 50)\n\n    # it works!\n    grouped = values.groupby(labels)\n\n    # accessing the index elements causes segfault\n    f = lambda x: len(set(map(id, x.index)))\n    grouped.agg(f)\n\n\ndef test_convert_objects_leave_decimal_alone():\n    s = Series(range(5))\n    labels = np.array([\"a\", \"b\", \"c\", \"d\", \"e\"], dtype=\"O\")\n\n    def convert_fast(x):\n        return Decimal(str(x.mean()))\n\n    def convert_force_pure(x):\n        # base will be length 0\n        assert len(x.values.base) > 0\n        return Decimal(str(x.mean()))\n\n    grouped = s.groupby(labels)\n\n    result = grouped.agg(convert_fast)\n    assert result.dtype == np.object_\n    assert isinstance(result[0], Decimal)\n\n    result = grouped.agg(convert_force_pure)\n    assert result.dtype == np.object_\n    assert isinstance(result[0], Decimal)\n\n\ndef test_groupby_dtype_inference_empty():\n    # GH 6733\n    df = DataFrame({\"x\": [], \"range\": np.arange(0, dtype=\"int64\")})\n    assert df[\"x\"].dtype == np.float64\n\n    result = df.groupby(\"x\").first()\n    exp_index = Index([], name=\"x\", dtype=np.float64)\n    expected = DataFrame({\"range\": Series([], index=exp_index, dtype=\"int64\")})\n    tm.assert_frame_equal(result, expected, by_blocks=True)\n\n\ndef test_groupby_unit64_float_conversion():\n    # GH: 30859 groupby converts unit64 to floats sometimes\n    df = DataFrame({\"first\": [1], \"second\": [1], \"value\": [16148277970000000000]})\n    result = df.groupby([\"first\", \"second\"])[\"value\"].max()\n    expected = Series(\n        [16148277970000000000],\n        MultiIndex.from_product([[1], [1]], names=[\"first\", \"second\"]),\n        name=\"value\",\n    )\n    tm.assert_series_equal(result, expected)\n\n\ndef test_groupby_list_infer_array_like(df):\n    result = df.groupby(list(df[\"A\"])).mean(numeric_only=True)\n    expected = df.groupby(df[\"A\"]).mean(numeric_only=True)\n    tm.assert_frame_equal(result, expected, check_names=False)\n\n    with pytest.raises(KeyError, match=r\"^'foo'$\"):\n        df.groupby(list(df[\"A\"][:-1]))\n\n    # pathological case of ambiguity\n    df = DataFrame({\"foo\": [0, 1], \"bar\": [3, 4], \"val\": np.random.randn(2)})\n\n    result = df.groupby([\"foo\", \"bar\"]).mean()\n    expected = df.groupby([df[\"foo\"], df[\"bar\"]]).mean()[[\"val\"]]\n\n\ndef test_groupby_keys_same_size_as_index():\n    # GH 11185\n    freq = \"s\"\n    index = date_range(\n        start=Timestamp(\"2015-09-29T11:34:44-0700\"), periods=2, freq=freq\n    )\n    df = DataFrame([[\"A\", 10], [\"B\", 15]], columns=[\"metric\", \"values\"], index=index)\n    result = df.groupby([Grouper(level=0, freq=freq), \"metric\"]).mean()\n    expected = df.set_index([df.index, \"metric\"]).astype(float)\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_one_row():\n    # GH 11741\n    msg = r\"^'Z'$\"\n    df1 = DataFrame(np.random.randn(1, 4), columns=list(\"ABCD\"))\n    with pytest.raises(KeyError, match=msg):\n        df1.groupby(\"Z\")\n    df2 = DataFrame(np.random.randn(2, 4), columns=list(\"ABCD\"))\n    with pytest.raises(KeyError, match=msg):\n        df2.groupby(\"Z\")\n\n\ndef test_groupby_nat_exclude():\n    # GH 6992\n    df = DataFrame(\n        {\n            \"values\": np.random.randn(8),\n            \"dt\": [\n                np.nan,\n                Timestamp(\"2013-01-01\"),\n                np.nan,\n                Timestamp(\"2013-02-01\"),\n                np.nan,\n                Timestamp(\"2013-02-01\"),\n                np.nan,\n                Timestamp(\"2013-01-01\"),\n            ],\n            \"str\": [np.nan, \"a\", np.nan, \"a\", np.nan, \"a\", np.nan, \"b\"],\n        }\n    )\n    grouped = df.groupby(\"dt\")\n\n    expected = [Index([1, 7]), Index([3, 5])]\n    keys = sorted(grouped.groups.keys())\n    assert len(keys) == 2\n    for k, e in zip(keys, expected):\n        # grouped.groups keys are np.datetime64 with system tz\n        # not to be affected by tz, only compare values\n        tm.assert_index_equal(grouped.groups[k], e)\n\n    # confirm obj is not filtered\n    tm.assert_frame_equal(grouped.grouper.groupings[0].obj, df)\n    assert grouped.ngroups == 2\n\n    expected = {\n        Timestamp(\"2013-01-01 00:00:00\"): np.array([1, 7], dtype=np.intp),\n        Timestamp(\"2013-02-01 00:00:00\"): np.array([3, 5], dtype=np.intp),\n    }\n\n    for k in grouped.indices:\n        tm.assert_numpy_array_equal(grouped.indices[k], expected[k])\n\n    tm.assert_frame_equal(grouped.get_group(Timestamp(\"2013-01-01\")), df.iloc[[1, 7]])\n    tm.assert_frame_equal(grouped.get_group(Timestamp(\"2013-02-01\")), df.iloc[[3, 5]])\n\n    with pytest.raises(KeyError, match=r\"^NaT$\"):\n        grouped.get_group(pd.NaT)\n\n    nan_df = DataFrame(\n        {\"nan\": [np.nan, np.nan, np.nan], \"nat\": [pd.NaT, pd.NaT, pd.NaT]}\n    )\n    assert nan_df[\"nan\"].dtype == \"float64\"\n    assert nan_df[\"nat\"].dtype == \"datetime64[ns]\"\n\n    for key in [\"nan\", \"nat\"]:\n        grouped = nan_df.groupby(key)\n        assert grouped.groups == {}\n        assert grouped.ngroups == 0\n        assert grouped.indices == {}\n        with pytest.raises(KeyError, match=r\"^nan$\"):\n            grouped.get_group(np.nan)\n        with pytest.raises(KeyError, match=r\"^NaT$\"):\n            grouped.get_group(pd.NaT)\n\n\ndef test_groupby_two_group_keys_all_nan():\n    # GH #36842: Grouping over two group keys shouldn't raise an error\n    df = DataFrame({\"a\": [np.nan, np.nan], \"b\": [np.nan, np.nan], \"c\": [1, 2]})\n    result = df.groupby([\"a\", \"b\"]).indices\n    assert result == {}\n\n\ndef test_groupby_2d_malformed():\n    d = DataFrame(index=range(2))\n    d[\"group\"] = [\"g1\", \"g2\"]\n    d[\"zeros\"] = [0, 0]\n    d[\"ones\"] = [1, 1]\n    d[\"label\"] = [\"l1\", \"l2\"]\n    tmp = d.groupby([\"group\"]).mean(numeric_only=True)\n    res_values = np.array([[0.0, 1.0], [0.0, 1.0]])\n    tm.assert_index_equal(tmp.columns, Index([\"zeros\", \"ones\"]))\n    tm.assert_numpy_array_equal(tmp.values, res_values)\n\n\ndef test_int32_overflow():\n    B = np.concatenate((np.arange(10000), np.arange(10000), np.arange(5000)))\n    A = np.arange(25000)\n    df = DataFrame({\"A\": A, \"B\": B, \"C\": A, \"D\": B, \"E\": np.random.randn(25000)})\n\n    left = df.groupby([\"A\", \"B\", \"C\", \"D\"]).sum()\n    right = df.groupby([\"D\", \"C\", \"B\", \"A\"]).sum()\n    assert len(left) == len(right)\n\n\ndef test_groupby_sort_multi():\n    df = DataFrame(\n        {\n            \"a\": [\"foo\", \"bar\", \"baz\"],\n            \"b\": [3, 2, 1],\n            \"c\": [0, 1, 2],\n            \"d\": np.random.randn(3),\n        }\n    )\n\n    tups = [tuple(row) for row in df[[\"a\", \"b\", \"c\"]].values]\n    tups = com.asarray_tuplesafe(tups)\n    result = df.groupby([\"a\", \"b\", \"c\"], sort=True).sum()\n    tm.assert_numpy_array_equal(result.index.values, tups[[1, 2, 0]])\n\n    tups = [tuple(row) for row in df[[\"c\", \"a\", \"b\"]].values]\n    tups = com.asarray_tuplesafe(tups)\n    result = df.groupby([\"c\", \"a\", \"b\"], sort=True).sum()\n    tm.assert_numpy_array_equal(result.index.values, tups)\n\n    tups = [tuple(x) for x in df[[\"b\", \"c\", \"a\"]].values]\n    tups = com.asarray_tuplesafe(tups)\n    result = df.groupby([\"b\", \"c\", \"a\"], sort=True).sum()\n    tm.assert_numpy_array_equal(result.index.values, tups[[2, 1, 0]])\n\n    df = DataFrame(\n        {\"a\": [0, 1, 2, 0, 1, 2], \"b\": [0, 0, 0, 1, 1, 1], \"d\": np.random.randn(6)}\n    )\n    grouped = df.groupby([\"a\", \"b\"])[\"d\"]\n    result = grouped.sum()\n\n    def _check_groupby(df, result, keys, field, f=lambda x: x.sum()):\n        tups = [tuple(row) for row in df[keys].values]\n        tups = com.asarray_tuplesafe(tups)\n        expected = f(df.groupby(tups)[field])\n        for k, v in expected.items():\n            assert result[k] == v\n\n    _check_groupby(df, result, [\"a\", \"b\"], \"d\")\n\n\ndef test_dont_clobber_name_column():\n    df = DataFrame(\n        {\"key\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"], \"name\": [\"foo\", \"bar\", \"baz\"] * 2}\n    )\n\n    msg = \"DataFrameGroupBy.apply operated on the grouping columns\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        result = df.groupby(\"key\", group_keys=False).apply(lambda x: x)\n    tm.assert_frame_equal(result, df)\n\n\ndef test_skip_group_keys():\n    tsf = tm.makeTimeDataFrame()\n\n    grouped = tsf.groupby(lambda x: x.month, group_keys=False)\n    result = grouped.apply(lambda x: x.sort_values(by=\"A\")[:3])\n\n    pieces = [group.sort_values(by=\"A\")[:3] for key, group in grouped]\n\n    expected = pd.concat(pieces)\n    tm.assert_frame_equal(result, expected)\n\n    grouped = tsf[\"A\"].groupby(lambda x: x.month, group_keys=False)\n    result = grouped.apply(lambda x: x.sort_values()[:3])\n\n    pieces = [group.sort_values()[:3] for key, group in grouped]\n\n    expected = pd.concat(pieces)\n    tm.assert_series_equal(result, expected)\n\n\ndef test_no_nonsense_name(float_frame):\n    # GH #995\n    s = float_frame[\"C\"].copy()\n    s.name = None\n\n    result = s.groupby(float_frame[\"A\"]).agg(np.sum)\n    assert result.name is None\n\n\ndef test_multifunc_sum_bug():\n    # GH #1065\n    x = DataFrame(np.arange(9).reshape(3, 3))\n    x[\"test\"] = 0\n    x[\"fl\"] = [1.3, 1.5, 1.6]\n\n    grouped = x.groupby(\"test\")\n    result = grouped.agg({\"fl\": \"sum\", 2: \"size\"})\n    assert result[\"fl\"].dtype == np.float64\n\n\ndef test_handle_dict_return_value(df):\n    def f(group):\n        return {\"max\": group.max(), \"min\": group.min()}\n\n    def g(group):\n        return Series({\"max\": group.max(), \"min\": group.min()})\n\n    result = df.groupby(\"A\")[\"C\"].apply(f)\n    expected = df.groupby(\"A\")[\"C\"].apply(g)\n\n    assert isinstance(result, Series)\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"grouper\", [\"A\", [\"A\", \"B\"]])\ndef test_set_group_name(df, grouper):\n    def f(group):\n        assert group.name is not None\n        return group\n\n    def freduce(group):\n        assert group.name is not None\n        return group.sum()\n\n    def freducex(x):\n        return freduce(x)\n\n    grouped = df.groupby(grouper, group_keys=False)\n\n    # make sure all these work\n    msg = \"DataFrameGroupBy.apply operated on the grouping columns\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        grouped.apply(f)\n    grouped.aggregate(freduce)\n    grouped.aggregate({\"C\": freduce, \"D\": freduce})\n    grouped.transform(f)\n\n    grouped[\"C\"].apply(f)\n    grouped[\"C\"].aggregate(freduce)\n    grouped[\"C\"].aggregate([freduce, freducex])\n    grouped[\"C\"].transform(f)\n\n\ndef test_group_name_available_in_inference_pass():\n    # gh-15062\n    df = DataFrame({\"a\": [0, 0, 1, 1, 2, 2], \"b\": np.arange(6)})\n\n    names = []\n\n    def f(group):\n        names.append(group.name)\n        return group.copy()\n\n    msg = \"DataFrameGroupBy.apply operated on the grouping columns\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        df.groupby(\"a\", sort=False, group_keys=False).apply(f)\n\n    expected_names = [0, 1, 2]\n    assert names == expected_names\n\n\ndef test_no_dummy_key_names(df):\n    # see gh-1291\n    result = df.groupby(df[\"A\"].values).sum()\n    assert result.index.name is None\n\n    result = df.groupby([df[\"A\"].values, df[\"B\"].values]).sum()\n    assert result.index.names == (None, None)\n\n\ndef test_groupby_sort_multiindex_series():\n    # series multiindex groupby sort argument was not being passed through\n    # _compress_group_index\n    # GH 9444\n    index = MultiIndex(\n        levels=[[1, 2], [1, 2]],\n        codes=[[0, 0, 0, 0, 1, 1], [1, 1, 0, 0, 0, 0]],\n        names=[\"a\", \"b\"],\n    )\n    mseries = Series([0, 1, 2, 3, 4, 5], index=index)\n    index = MultiIndex(\n        levels=[[1, 2], [1, 2]], codes=[[0, 0, 1], [1, 0, 0]], names=[\"a\", \"b\"]\n    )\n    mseries_result = Series([0, 2, 4], index=index)\n\n    result = mseries.groupby(level=[\"a\", \"b\"], sort=False).first()\n    tm.assert_series_equal(result, mseries_result)\n    result = mseries.groupby(level=[\"a\", \"b\"], sort=True).first()\n    tm.assert_series_equal(result, mseries_result.sort_index())\n\n\ndef test_groupby_reindex_inside_function():\n    periods = 1000\n    ind = date_range(start=\"2012/1/1\", freq=\"5min\", periods=periods)\n    df = DataFrame({\"high\": np.arange(periods), \"low\": np.arange(periods)}, index=ind)\n\n    def agg_before(func, fix=False):\n        \"\"\"\n        Run an aggregate func on the subset of data.\n        \"\"\"\n\n        def _func(data):\n            d = data.loc[data.index.map(lambda x: x.hour < 11)].dropna()\n            if fix:\n                data[data.index[0]]\n            if len(d) == 0:\n                return None\n            return func(d)\n\n        return _func\n\n    grouped = df.groupby(lambda x: datetime(x.year, x.month, x.day))\n    closure_bad = grouped.agg({\"high\": agg_before(np.max)})\n    closure_good = grouped.agg({\"high\": agg_before(np.max, True)})\n\n    tm.assert_frame_equal(closure_bad, closure_good)\n\n\ndef test_groupby_multiindex_missing_pair():\n    # GH9049\n    df = DataFrame(\n        {\n            \"group1\": [\"a\", \"a\", \"a\", \"b\"],\n            \"group2\": [\"c\", \"c\", \"d\", \"c\"],\n            \"value\": [1, 1, 1, 5],\n        }\n    )\n    df = df.set_index([\"group1\", \"group2\"])\n    df_grouped = df.groupby(level=[\"group1\", \"group2\"], sort=True)\n\n    res = df_grouped.agg(\"sum\")\n    idx = MultiIndex.from_tuples(\n        [(\"a\", \"c\"), (\"a\", \"d\"), (\"b\", \"c\")], names=[\"group1\", \"group2\"]\n    )\n    exp = DataFrame([[2], [1], [5]], index=idx, columns=[\"value\"])\n\n    tm.assert_frame_equal(res, exp)\n\n\ndef test_groupby_multiindex_not_lexsorted():\n    # GH 11640\n\n    # define the lexsorted version\n    lexsorted_mi = MultiIndex.from_tuples(\n        [(\"a\", \"\"), (\"b1\", \"c1\"), (\"b2\", \"c2\")], names=[\"b\", \"c\"]\n    )\n    lexsorted_df = DataFrame([[1, 3, 4]], columns=lexsorted_mi)\n    assert lexsorted_df.columns._is_lexsorted()\n\n    # define the non-lexsorted version\n    not_lexsorted_df = DataFrame(\n        columns=[\"a\", \"b\", \"c\", \"d\"], data=[[1, \"b1\", \"c1\", 3], [1, \"b2\", \"c2\", 4]]\n    )\n    not_lexsorted_df = not_lexsorted_df.pivot_table(\n        index=\"a\", columns=[\"b\", \"c\"], values=\"d\"\n    )\n    not_lexsorted_df = not_lexsorted_df.reset_index()\n    assert not not_lexsorted_df.columns._is_lexsorted()\n\n    # compare the results\n    tm.assert_frame_equal(lexsorted_df, not_lexsorted_df)\n\n    expected = lexsorted_df.groupby(\"a\").mean()\n    with tm.assert_produces_warning(PerformanceWarning):\n        result = not_lexsorted_df.groupby(\"a\").mean()\n    tm.assert_frame_equal(expected, result)\n\n    # a transforming function should work regardless of sort\n    # GH 14776\n    df = DataFrame(\n        {\"x\": [\"a\", \"a\", \"b\", \"a\"], \"y\": [1, 1, 2, 2], \"z\": [1, 2, 3, 4]}\n    ).set_index([\"x\", \"y\"])\n    assert not df.index._is_lexsorted()\n\n    for level in [0, 1, [0, 1]]:\n        for sort in [False, True]:\n            result = df.groupby(level=level, sort=sort, group_keys=False).apply(\n                DataFrame.drop_duplicates\n            )\n            expected = df\n            tm.assert_frame_equal(expected, result)\n\n            result = (\n                df.sort_index()\n                .groupby(level=level, sort=sort, group_keys=False)\n                .apply(DataFrame.drop_duplicates)\n            )\n            expected = df.sort_index()\n            tm.assert_frame_equal(expected, result)\n\n\ndef test_index_label_overlaps_location():\n    # checking we don't have any label/location confusion in the\n    # wake of GH5375\n    df = DataFrame(list(\"ABCDE\"), index=[2, 0, 2, 1, 1])\n    g = df.groupby(list(\"ababb\"))\n    actual = g.filter(lambda x: len(x) > 2)\n    expected = df.iloc[[1, 3, 4]]\n    tm.assert_frame_equal(actual, expected)\n\n    ser = df[0]\n    g = ser.groupby(list(\"ababb\"))\n    actual = g.filter(lambda x: len(x) > 2)\n    expected = ser.take([1, 3, 4])\n    tm.assert_series_equal(actual, expected)\n\n    #  and again, with a generic Index of floats\n    df.index = df.index.astype(float)\n    g = df.groupby(list(\"ababb\"))\n    actual = g.filter(lambda x: len(x) > 2)\n    expected = df.iloc[[1, 3, 4]]\n    tm.assert_frame_equal(actual, expected)\n\n    ser = df[0]\n    g = ser.groupby(list(\"ababb\"))\n    actual = g.filter(lambda x: len(x) > 2)\n    expected = ser.take([1, 3, 4])\n    tm.assert_series_equal(actual, expected)\n\n\ndef test_transform_doesnt_clobber_ints():\n    # GH 7972\n    n = 6\n    x = np.arange(n)\n    df = DataFrame({\"a\": x // 2, \"b\": 2.0 * x, \"c\": 3.0 * x})\n    df2 = DataFrame({\"a\": x // 2 * 1.0, \"b\": 2.0 * x, \"c\": 3.0 * x})\n\n    gb = df.groupby(\"a\")\n    result = gb.transform(\"mean\")\n\n    gb2 = df2.groupby(\"a\")\n    expected = gb2.transform(\"mean\")\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"sort_column\",\n    [\"ints\", \"floats\", \"strings\", [\"ints\", \"floats\"], [\"ints\", \"strings\"]],\n)\n@pytest.mark.parametrize(\n    \"group_column\", [\"int_groups\", \"string_groups\", [\"int_groups\", \"string_groups\"]]\n)\ndef test_groupby_preserves_sort(sort_column, group_column):\n    # Test to ensure that groupby always preserves sort order of original\n    # object. Issue #8588 and #9651\n\n    df = DataFrame(\n        {\n            \"int_groups\": [3, 1, 0, 1, 0, 3, 3, 3],\n            \"string_groups\": [\"z\", \"a\", \"z\", \"a\", \"a\", \"g\", \"g\", \"g\"],\n            \"ints\": [8, 7, 4, 5, 2, 9, 1, 1],\n            \"floats\": [2.3, 5.3, 6.2, -2.4, 2.2, 1.1, 1.1, 5],\n            \"strings\": [\"z\", \"d\", \"a\", \"e\", \"word\", \"word2\", \"42\", \"47\"],\n        }\n    )\n\n    # Try sorting on different types and with different group types\n\n    df = df.sort_values(by=sort_column)\n    g = df.groupby(group_column)\n\n    def test_sort(x):\n        tm.assert_frame_equal(x, x.sort_values(by=sort_column))\n\n    msg = \"DataFrameGroupBy.apply operated on the grouping columns\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        g.apply(test_sort)\n\n\ndef test_pivot_table_values_key_error():\n    # This test is designed to replicate the error in issue #14938\n    df = DataFrame(\n        {\n            \"eventDate\": date_range(datetime.today(), periods=20, freq=\"M\").tolist(),\n            \"thename\": range(0, 20),\n        }\n    )\n\n    df[\"year\"] = df.set_index(\"eventDate\").index.year\n    df[\"month\"] = df.set_index(\"eventDate\").index.month\n\n    with pytest.raises(KeyError, match=\"'badname'\"):\n        df.reset_index().pivot_table(\n            index=\"year\", columns=\"month\", values=\"badname\", aggfunc=\"count\"\n        )\n\n\n@pytest.mark.parametrize(\"columns\", [\"C\", [\"C\"]])\n@pytest.mark.parametrize(\"keys\", [[\"A\"], [\"A\", \"B\"]])\n@pytest.mark.parametrize(\n    \"values\",\n    [\n        [True],\n        [0],\n        [0.0],\n        [\"a\"],\n        Categorical([0]),\n        [to_datetime(0)],\n        date_range(0, 1, 1, tz=\"US/Eastern\"),\n        pd.period_range(\"2016-01-01\", periods=3, freq=\"D\"),\n        pd.array([0], dtype=\"Int64\"),\n        pd.array([0], dtype=\"Float64\"),\n        pd.array([False], dtype=\"boolean\"),\n    ],\n    ids=[\n        \"bool\",\n        \"int\",\n        \"float\",\n        \"str\",\n        \"cat\",\n        \"dt64\",\n        \"dt64tz\",\n        \"period\",\n        \"Int64\",\n        \"Float64\",\n        \"boolean\",\n    ],\n)\n@pytest.mark.parametrize(\"method\", [\"attr\", \"agg\", \"apply\"])\n@pytest.mark.parametrize(\n    \"op\", [\"idxmax\", \"idxmin\", \"min\", \"max\", \"sum\", \"prod\", \"skew\"]\n)\ndef test_empty_groupby(\n    columns, keys, values, method, op, request, using_array_manager, dropna\n):\n    # GH8093 & GH26411\n    override_dtype = None\n\n    if (\n        isinstance(values, Categorical)\n        and len(keys) == 1\n        and op in [\"idxmax\", \"idxmin\"]\n    ):\n        mark = pytest.mark.xfail(\n            raises=ValueError, match=\"attempt to get arg(min|max) of an empty sequence\"\n        )\n        request.node.add_marker(mark)\n\n    if isinstance(values, BooleanArray) and op in [\"sum\", \"prod\"]:\n        # We expect to get Int64 back for these\n        override_dtype = \"Int64\"\n\n    if isinstance(values[0], bool) and op in (\"prod\", \"sum\"):\n        # sum/product of bools is an integer\n        override_dtype = \"int64\"\n\n    df = DataFrame({\"A\": values, \"B\": values, \"C\": values}, columns=list(\"ABC\"))\n\n    if hasattr(values, \"dtype\"):\n        # check that we did the construction right\n        assert (df.dtypes == values.dtype).all()\n\n    df = df.iloc[:0]\n\n    gb = df.groupby(keys, group_keys=False, dropna=dropna, observed=False)[columns]\n\n    def get_result(**kwargs):\n        if method == \"attr\":\n            return getattr(gb, op)(**kwargs)\n        else:\n            return getattr(gb, method)(op, **kwargs)\n\n    def get_categorical_invalid_expected():\n        # Categorical is special without 'observed=True', we get an NaN entry\n        #  corresponding to the unobserved group. If we passed observed=True\n        #  to groupby, expected would just be 'df.set_index(keys)[columns]'\n        #  as below\n        lev = Categorical([0], dtype=values.dtype)\n        if len(keys) != 1:\n            idx = MultiIndex.from_product([lev, lev], names=keys)\n        else:\n            # all columns are dropped, but we end up with one row\n            # Categorical is special without 'observed=True'\n            idx = Index(lev, name=keys[0])\n\n        expected = DataFrame([], columns=[], index=idx)\n        return expected\n\n    is_per = isinstance(df.dtypes[0], pd.PeriodDtype)\n    is_dt64 = df.dtypes[0].kind == \"M\"\n    is_cat = isinstance(values, Categorical)\n\n    if isinstance(values, Categorical) and not values.ordered and op in [\"min\", \"max\"]:\n        msg = f\"Cannot perform {op} with non-ordered Categorical\"\n        with pytest.raises(TypeError, match=msg):\n            get_result()\n\n        if isinstance(columns, list):\n            # i.e. DataframeGroupBy, not SeriesGroupBy\n            result = get_result(numeric_only=True)\n            expected = get_categorical_invalid_expected()\n            tm.assert_equal(result, expected)\n        return\n\n    if op in [\"prod\", \"sum\", \"skew\"]:\n        # ops that require more than just ordered-ness\n        if is_dt64 or is_cat or is_per:\n            # GH#41291\n            # datetime64 -> prod and sum are invalid\n            if is_dt64:\n                msg = \"datetime64 type does not support\"\n            elif is_per:\n                msg = \"Period type does not support\"\n            else:\n                msg = \"category type does not support\"\n            if op == \"skew\":\n                msg = \"|\".join([msg, \"does not support reduction 'skew'\"])\n            with pytest.raises(TypeError, match=msg):\n                get_result()\n\n            if not isinstance(columns, list):\n                # i.e. SeriesGroupBy\n                return\n            elif op == \"skew\":\n                # TODO: test the numeric_only=True case\n                return\n            else:\n                # i.e. op in [\"prod\", \"sum\"]:\n                # i.e. DataFrameGroupBy\n                # ops that require more than just ordered-ness\n                # GH#41291\n                result = get_result(numeric_only=True)\n\n                # with numeric_only=True, these are dropped, and we get\n                # an empty DataFrame back\n                expected = df.set_index(keys)[[]]\n                if is_cat:\n                    expected = get_categorical_invalid_expected()\n                tm.assert_equal(result, expected)\n                return\n\n    result = get_result()\n    expected = df.set_index(keys)[columns]\n    if op in [\"idxmax\", \"idxmin\"]:\n        expected = expected.astype(df.index.dtype)\n    if override_dtype is not None:\n        expected = expected.astype(override_dtype)\n    if len(keys) == 1:\n        expected.index.name = keys[0]\n    tm.assert_equal(result, expected)\n\n\ndef test_empty_groupby_apply_nonunique_columns():\n    # GH#44417\n    df = DataFrame(np.random.randn(0, 4))\n    df[3] = df[3].astype(np.int64)\n    df.columns = [0, 1, 2, 0]\n    gb = df.groupby(df[1], group_keys=False)\n    msg = \"DataFrameGroupBy.apply operated on the grouping columns\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        res = gb.apply(lambda x: x)\n    assert (res.dtypes == df.dtypes).all()\n\n\ndef test_tuple_as_grouping():\n    # https://github.com/pandas-dev/pandas/issues/18314\n    df = DataFrame(\n        {\n            (\"a\", \"b\"): [1, 1, 1, 1],\n            \"a\": [2, 2, 2, 2],\n            \"b\": [2, 2, 2, 2],\n            \"c\": [1, 1, 1, 1],\n        }\n    )\n\n    with pytest.raises(KeyError, match=r\"('a', 'b')\"):\n        df[[\"a\", \"b\", \"c\"]].groupby((\"a\", \"b\"))\n\n    result = df.groupby((\"a\", \"b\"))[\"c\"].sum()\n    expected = Series([4], name=\"c\", index=Index([1], name=(\"a\", \"b\")))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_tuple_correct_keyerror():\n    # https://github.com/pandas-dev/pandas/issues/18798\n    df = DataFrame(1, index=range(3), columns=MultiIndex.from_product([[1, 2], [3, 4]]))\n    with pytest.raises(KeyError, match=r\"^\\(7, 8\\)$\"):\n        df.groupby((7, 8)).mean()\n\n\ndef test_groupby_agg_ohlc_non_first():\n    # GH 21716\n    df = DataFrame(\n        [[1], [1]],\n        columns=Index([\"foo\"], name=\"mycols\"),\n        index=date_range(\"2018-01-01\", periods=2, freq=\"D\", name=\"dti\"),\n    )\n\n    expected = DataFrame(\n        [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]],\n        columns=MultiIndex.from_tuples(\n            (\n                (\"foo\", \"sum\", \"foo\"),\n                (\"foo\", \"ohlc\", \"open\"),\n                (\"foo\", \"ohlc\", \"high\"),\n                (\"foo\", \"ohlc\", \"low\"),\n                (\"foo\", \"ohlc\", \"close\"),\n            ),\n            names=[\"mycols\", None, None],\n        ),\n        index=date_range(\"2018-01-01\", periods=2, freq=\"D\", name=\"dti\"),\n    )\n\n    result = df.groupby(Grouper(freq=\"D\")).agg([\"sum\", \"ohlc\"])\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_multiindex_nat():\n    # GH 9236\n    values = [\n        (pd.NaT, \"a\"),\n        (datetime(2012, 1, 2), \"a\"),\n        (datetime(2012, 1, 2), \"b\"),\n        (datetime(2012, 1, 3), \"a\"),\n    ]\n    mi = MultiIndex.from_tuples(values, names=[\"date\", None])\n    ser = Series([3, 2, 2.5, 4], index=mi)\n\n    result = ser.groupby(level=1).mean()\n    expected = Series([3.0, 2.5], index=[\"a\", \"b\"])\n    tm.assert_series_equal(result, expected)\n\n\ndef test_groupby_empty_list_raises():\n    # GH 5289\n    values = zip(range(10), range(10))\n    df = DataFrame(values, columns=[\"apple\", \"b\"])\n    msg = \"Grouper and axis must be same length\"\n    with pytest.raises(ValueError, match=msg):\n        df.groupby([[]])\n\n\ndef test_groupby_multiindex_series_keys_len_equal_group_axis():\n    # GH 25704\n    index_array = [[\"x\", \"x\"], [\"a\", \"b\"], [\"k\", \"k\"]]\n    index_names = [\"first\", \"second\", \"third\"]\n    ri = MultiIndex.from_arrays(index_array, names=index_names)\n    s = Series(data=[1, 2], index=ri)\n    result = s.groupby([\"first\", \"third\"]).sum()\n\n    index_array = [[\"x\"], [\"k\"]]\n    index_names = [\"first\", \"third\"]\n    ei = MultiIndex.from_arrays(index_array, names=index_names)\n    expected = Series([3], index=ei)\n\n    tm.assert_series_equal(result, expected)\n\n\ndef test_groupby_groups_in_BaseGrouper():\n    # GH 26326\n    # Test if DataFrame grouped with a pandas.Grouper has correct groups\n    mi = MultiIndex.from_product([[\"A\", \"B\"], [\"C\", \"D\"]], names=[\"alpha\", \"beta\"])\n    df = DataFrame({\"foo\": [1, 2, 1, 2], \"bar\": [1, 2, 3, 4]}, index=mi)\n    result = df.groupby([Grouper(level=\"alpha\"), \"beta\"])\n    expected = df.groupby([\"alpha\", \"beta\"])\n    assert result.groups == expected.groups\n\n    result = df.groupby([\"beta\", Grouper(level=\"alpha\")])\n    expected = df.groupby([\"beta\", \"alpha\"])\n    assert result.groups == expected.groups\n\n\n@pytest.mark.parametrize(\"group_name\", [\"x\", [\"x\"]])\ndef test_groupby_axis_1(group_name):\n    # GH 27614\n    df = DataFrame(\n        np.arange(12).reshape(3, 4), index=[0, 1, 0], columns=[10, 20, 10, 20]\n    )\n    df.index.name = \"y\"\n    df.columns.name = \"x\"\n\n    depr_msg = \"DataFrame.groupby with axis=1 is deprecated\"\n    with tm.assert_produces_warning(FutureWarning, match=depr_msg):\n        gb = df.groupby(group_name, axis=1)\n\n    results = gb.sum()\n    expected = df.T.groupby(group_name).sum().T\n    tm.assert_frame_equal(results, expected)\n\n    # test on MI column\n    iterables = [[\"bar\", \"baz\", \"foo\"], [\"one\", \"two\"]]\n    mi = MultiIndex.from_product(iterables=iterables, names=[\"x\", \"x1\"])\n    df = DataFrame(np.arange(18).reshape(3, 6), index=[0, 1, 0], columns=mi)\n    with tm.assert_produces_warning(FutureWarning, match=depr_msg):\n        gb = df.groupby(group_name, axis=1)\n    results = gb.sum()\n    expected = df.T.groupby(group_name).sum().T\n    tm.assert_frame_equal(results, expected)\n\n\n@pytest.mark.parametrize(\n    \"op, expected\",\n    [\n        (\n            \"shift\",\n            {\n                \"time\": [\n                    None,\n                    None,\n                    Timestamp(\"2019-01-01 12:00:00\"),\n                    Timestamp(\"2019-01-01 12:30:00\"),\n                    None,\n                    None,\n                ]\n            },\n        ),\n        (\n            \"bfill\",\n            {\n                \"time\": [\n                    Timestamp(\"2019-01-01 12:00:00\"),\n                    Timestamp(\"2019-01-01 12:30:00\"),\n                    Timestamp(\"2019-01-01 14:00:00\"),\n                    Timestamp(\"2019-01-01 14:30:00\"),\n                    Timestamp(\"2019-01-01 14:00:00\"),\n                    Timestamp(\"2019-01-01 14:30:00\"),\n                ]\n            },\n        ),\n        (\n            \"ffill\",\n            {\n                \"time\": [\n                    Timestamp(\"2019-01-01 12:00:00\"),\n                    Timestamp(\"2019-01-01 12:30:00\"),\n                    Timestamp(\"2019-01-01 12:00:00\"),\n                    Timestamp(\"2019-01-01 12:30:00\"),\n                    Timestamp(\"2019-01-01 14:00:00\"),\n                    Timestamp(\"2019-01-01 14:30:00\"),\n                ]\n            },\n        ),\n    ],\n)\ndef test_shift_bfill_ffill_tz(tz_naive_fixture, op, expected):\n    # GH19995, GH27992: Check that timezone does not drop in shift, bfill, and ffill\n    tz = tz_naive_fixture\n    data = {\n        \"id\": [\"A\", \"B\", \"A\", \"B\", \"A\", \"B\"],\n        \"time\": [\n            Timestamp(\"2019-01-01 12:00:00\"),\n            Timestamp(\"2019-01-01 12:30:00\"),\n            None,\n            None,\n            Timestamp(\"2019-01-01 14:00:00\"),\n            Timestamp(\"2019-01-01 14:30:00\"),\n        ],\n    }\n    df = DataFrame(data).assign(time=lambda x: x.time.dt.tz_localize(tz))\n\n    grouped = df.groupby(\"id\")\n    result = getattr(grouped, op)()\n    expected = DataFrame(expected).assign(time=lambda x: x.time.dt.tz_localize(tz))\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_only_none_group():\n    # see GH21624\n    # this was crashing with \"ValueError: Length of passed values is 1, index implies 0\"\n    df = DataFrame({\"g\": [None], \"x\": 1})\n    actual = df.groupby(\"g\")[\"x\"].transform(\"sum\")\n    expected = Series([np.nan], name=\"x\")\n\n    tm.assert_series_equal(actual, expected)\n\n\ndef test_groupby_duplicate_index():\n    # GH#29189 the groupby call here used to raise\n    ser = Series([2, 5, 6, 8], index=[2.0, 4.0, 4.0, 5.0])\n    gb = ser.groupby(level=0)\n\n    result = gb.mean()\n    expected = Series([2, 5.5, 8], index=[2.0, 4.0, 5.0])\n    tm.assert_series_equal(result, expected)\n\n\ndef test_group_on_empty_multiindex(transformation_func, request):\n    # GH 47787\n    # With one row, those are transforms so the schema should be the same\n    df = DataFrame(\n        data=[[1, Timestamp(\"today\"), 3, 4]],\n        columns=[\"col_1\", \"col_2\", \"col_3\", \"col_4\"],\n    )\n    df[\"col_3\"] = df[\"col_3\"].astype(int)\n    df[\"col_4\"] = df[\"col_4\"].astype(int)\n    df = df.set_index([\"col_1\", \"col_2\"])\n    if transformation_func == \"fillna\":\n        args = (\"ffill\",)\n    else:\n        args = ()\n    result = df.iloc[:0].groupby([\"col_1\"]).transform(transformation_func, *args)\n    expected = df.groupby([\"col_1\"]).transform(transformation_func, *args).iloc[:0]\n    if transformation_func in (\"diff\", \"shift\"):\n        expected = expected.astype(int)\n    tm.assert_equal(result, expected)\n\n    result = (\n        df[\"col_3\"].iloc[:0].groupby([\"col_1\"]).transform(transformation_func, *args)\n    )\n    expected = (\n        df[\"col_3\"].groupby([\"col_1\"]).transform(transformation_func, *args).iloc[:0]\n    )\n    if transformation_func in (\"diff\", \"shift\"):\n        expected = expected.astype(int)\n    tm.assert_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"idx\",\n    [\n        Index([\"a\", \"a\"], name=\"foo\"),\n        MultiIndex.from_tuples(((\"a\", \"a\"), (\"a\", \"a\")), names=[\"foo\", \"bar\"]),\n    ],\n)\ndef test_dup_labels_output_shape(groupby_func, idx):\n    if groupby_func in {\"size\", \"ngroup\", \"cumcount\"}:\n        pytest.skip(f\"Not applicable for {groupby_func}\")\n\n    df = DataFrame([[1, 1]], columns=idx)\n    grp_by = df.groupby([0])\n\n    args = get_groupby_method_args(groupby_func, df)\n    result = getattr(grp_by, groupby_func)(*args)\n\n    assert result.shape == (1, 2)\n    tm.assert_index_equal(result.columns, idx)\n\n\ndef test_groupby_crash_on_nunique(axis):\n    # Fix following 30253\n    dti = date_range(\"2016-01-01\", periods=2, name=\"foo\")\n    df = DataFrame({(\"A\", \"B\"): [1, 2], (\"A\", \"C\"): [1, 3], (\"D\", \"B\"): [0, 0]})\n    df.columns.names = (\"bar\", \"baz\")\n    df.index = dti\n\n    axis_number = df._get_axis_number(axis)\n    if not axis_number:\n        df = df.T\n        msg = \"The 'axis' keyword in DataFrame.groupby is deprecated\"\n    else:\n        msg = \"DataFrame.groupby with axis=1 is deprecated\"\n\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        gb = df.groupby(axis=axis_number, level=0)\n    result = gb.nunique()\n\n    expected = DataFrame({\"A\": [1, 2], \"D\": [1, 1]}, index=dti)\n    expected.columns.name = \"bar\"\n    if not axis_number:\n        expected = expected.T\n\n    tm.assert_frame_equal(result, expected)\n\n    if axis_number == 0:\n        # same thing, but empty columns\n        with tm.assert_produces_warning(FutureWarning, match=msg):\n            gb2 = df[[]].groupby(axis=axis_number, level=0)\n        exp = expected[[]]\n    else:\n        # same thing, but empty rows\n        with tm.assert_produces_warning(FutureWarning, match=msg):\n            gb2 = df.loc[[]].groupby(axis=axis_number, level=0)\n        # default for empty when we can't infer a dtype is float64\n        exp = expected.loc[[]].astype(np.float64)\n\n    res = gb2.nunique()\n    tm.assert_frame_equal(res, exp)\n\n\ndef test_groupby_list_level():\n    # GH 9790\n    expected = DataFrame(np.arange(0, 9).reshape(3, 3), dtype=float)\n    result = expected.groupby(level=[0]).mean()\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"max_seq_items, expected\",\n    [\n        (5, \"{0: [0], 1: [1], 2: [2], 3: [3], 4: [4]}\"),\n        (4, \"{0: [0], 1: [1], 2: [2], 3: [3], ...}\"),\n        (1, \"{0: [0], ...}\"),\n    ],\n)\ndef test_groups_repr_truncates(max_seq_items, expected):\n    # GH 1135\n    df = DataFrame(np.random.randn(5, 1))\n    df[\"a\"] = df.index\n\n    with pd.option_context(\"display.max_seq_items\", max_seq_items):\n        result = df.groupby(\"a\").groups.__repr__()\n        assert result == expected\n\n        result = df.groupby(np.array(df.a)).groups.__repr__()\n        assert result == expected\n\n\ndef test_group_on_two_row_multiindex_returns_one_tuple_key():\n    # GH 18451\n    df = DataFrame([{\"a\": 1, \"b\": 2, \"c\": 99}, {\"a\": 1, \"b\": 2, \"c\": 88}])\n    df = df.set_index([\"a\", \"b\"])\n\n    grp = df.groupby([\"a\", \"b\"])\n    result = grp.indices\n    expected = {(1, 2): np.array([0, 1], dtype=np.int64)}\n\n    assert len(result) == 1\n    key = (1, 2)\n    assert (result[key] == expected[key]).all()\n\n\n@pytest.mark.parametrize(\n    \"klass, attr, value\",\n    [\n        (DataFrame, \"level\", \"a\"),\n        (DataFrame, \"as_index\", False),\n        (DataFrame, \"sort\", False),\n        (DataFrame, \"group_keys\", False),\n        (DataFrame, \"observed\", True),\n        (DataFrame, \"dropna\", False),\n        (Series, \"level\", \"a\"),\n        (Series, \"as_index\", False),\n        (Series, \"sort\", False),\n        (Series, \"group_keys\", False),\n        (Series, \"observed\", True),\n        (Series, \"dropna\", False),\n    ],\n)\ndef test_subsetting_columns_keeps_attrs(klass, attr, value):\n    # GH 9959 - When subsetting columns, don't drop attributes\n    df = DataFrame({\"a\": [1], \"b\": [2], \"c\": [3]})\n    if attr != \"axis\":\n        df = df.set_index(\"a\")\n\n    expected = df.groupby(\"a\", **{attr: value})\n    result = expected[[\"b\"]] if klass is DataFrame else expected[\"b\"]\n    assert getattr(result, attr) == getattr(expected, attr)\n\n\ndef test_subsetting_columns_axis_1():\n    # GH 37725\n    df = DataFrame({\"A\": [1], \"B\": [2], \"C\": [3]})\n    msg = \"DataFrame.groupby with axis=1 is deprecated\"\n    with tm.assert_produces_warning(FutureWarning, match=msg):\n        g = df.groupby([0, 0, 1], axis=1)\n    match = \"Cannot subset columns when using axis=1\"\n    with pytest.raises(ValueError, match=match):\n        g[[\"A\", \"B\"]].sum()\n\n\n@pytest.mark.parametrize(\"func\", [\"sum\", \"any\", \"shift\"])\ndef test_groupby_column_index_name_lost(func):\n    # GH: 29764 groupby loses index sometimes\n    expected = Index([\"a\"], name=\"idx\")\n    df = DataFrame([[1]], columns=expected)\n    df_grouped = df.groupby([1])\n    result = getattr(df_grouped, func)().columns\n    tm.assert_index_equal(result, expected)\n\n\ndef test_groupby_duplicate_columns():\n    # GH: 31735\n    df = DataFrame(\n        {\"A\": [\"f\", \"e\", \"g\", \"h\"], \"B\": [\"a\", \"b\", \"c\", \"d\"], \"C\": [1, 2, 3, 4]}\n    ).astype(object)\n    df.columns = [\"A\", \"B\", \"B\"]\n    result = df.groupby([0, 0, 0, 0]).min()\n    expected = DataFrame(\n        [[\"e\", \"a\", 1]], index=np.array([0]), columns=[\"A\", \"B\", \"B\"], dtype=object\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_series_with_tuple_name():\n    # GH 37755\n    ser = Series([1, 2, 3, 4], index=[1, 1, 2, 2], name=(\"a\", \"a\"))\n    ser.index.name = (\"b\", \"b\")\n    result = ser.groupby(level=0).last()\n    expected = Series([2, 4], index=[1, 2], name=(\"a\", \"a\"))\n    expected.index.name = (\"b\", \"b\")\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.xfail(not IS64, reason=\"GH#38778: fail on 32-bit system\")\n@pytest.mark.parametrize(\n    \"func, values\", [(\"sum\", [97.0, 98.0]), (\"mean\", [24.25, 24.5])]\n)\ndef test_groupby_numerical_stability_sum_mean(func, values):\n    # GH#38778\n    data = [1e16, 1e16, 97, 98, -5e15, -5e15, -5e15, -5e15]\n    df = DataFrame({\"group\": [1, 2] * 4, \"a\": data, \"b\": data})\n    result = getattr(df.groupby(\"group\"), func)()\n    expected = DataFrame({\"a\": values, \"b\": values}, index=Index([1, 2], name=\"group\"))\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.xfail(not IS64, reason=\"GH#38778: fail on 32-bit system\")\ndef test_groupby_numerical_stability_cumsum():\n    # GH#38934\n    data = [1e16, 1e16, 97, 98, -5e15, -5e15, -5e15, -5e15]\n    df = DataFrame({\"group\": [1, 2] * 4, \"a\": data, \"b\": data})\n    result = df.groupby(\"group\").cumsum()\n    exp_data = (\n        [1e16] * 2 + [1e16 + 96, 1e16 + 98] + [5e15 + 97, 5e15 + 98] + [97.0, 98.0]\n    )\n    expected = DataFrame({\"a\": exp_data, \"b\": exp_data})\n    tm.assert_frame_equal(result, expected, check_exact=True)\n\n\ndef test_groupby_cumsum_skipna_false():\n    # GH#46216 don't propagate np.nan above the diagonal\n    arr = np.random.randn(5, 5)\n    df = DataFrame(arr)\n    for i in range(5):\n        df.iloc[i, i] = np.nan\n\n    df[\"A\"] = 1\n    gb = df.groupby(\"A\")\n\n    res = gb.cumsum(skipna=False)\n\n    expected = df[[0, 1, 2, 3, 4]].cumsum(skipna=False)\n    tm.assert_frame_equal(res, expected)\n\n\ndef test_groupby_cumsum_timedelta64():\n    # GH#46216 don't ignore is_datetimelike in libgroupby.group_cumsum\n    dti = date_range(\"2016-01-01\", periods=5)\n    ser = Series(dti) - dti[0]\n    ser[2] = pd.NaT\n\n    df = DataFrame({\"A\": 1, \"B\": ser})\n    gb = df.groupby(\"A\")\n\n    res = gb.cumsum(numeric_only=False, skipna=True)\n    exp = DataFrame({\"B\": [ser[0], ser[1], pd.NaT, ser[4], ser[4] * 2]})\n    tm.assert_frame_equal(res, exp)\n\n    res = gb.cumsum(numeric_only=False, skipna=False)\n    exp = DataFrame({\"B\": [ser[0], ser[1], pd.NaT, pd.NaT, pd.NaT]})\n    tm.assert_frame_equal(res, exp)\n\n\ndef test_groupby_mean_duplicate_index(rand_series_with_duplicate_datetimeindex):\n    dups = rand_series_with_duplicate_datetimeindex\n    result = dups.groupby(level=0).mean()\n    expected = dups.groupby(dups.index).mean()\n    tm.assert_series_equal(result, expected)\n\n\ndef test_groupby_all_nan_groups_drop():\n    # GH 15036\n    s = Series([1, 2, 3], [np.nan, np.nan, np.nan])\n    result = s.groupby(s.index).sum()\n    expected = Series([], index=Index([], dtype=np.float64), dtype=np.int64)\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"numeric_only\", [True, False])\ndef test_groupby_empty_multi_column(as_index, numeric_only):\n    # GH 15106 & GH 41998\n    df = DataFrame(data=[], columns=[\"A\", \"B\", \"C\"])\n    gb = df.groupby([\"A\", \"B\"], as_index=as_index)\n    result = gb.sum(numeric_only=numeric_only)\n    if as_index:\n        index = MultiIndex([[], []], [[], []], names=[\"A\", \"B\"])\n        columns = [\"C\"] if not numeric_only else []\n    else:\n        index = RangeIndex(0)\n        columns = [\"A\", \"B\", \"C\"] if not numeric_only else [\"A\", \"B\"]\n    expected = DataFrame([], columns=columns, index=index)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_aggregation_non_numeric_dtype():\n    # GH #43108\n    df = DataFrame(\n        [[\"M\", [1]], [\"M\", [1]], [\"W\", [10]], [\"W\", [20]]], columns=[\"MW\", \"v\"]\n    )\n\n    expected = DataFrame(\n        {\n            \"v\": [[1, 1], [10, 20]],\n        },\n        index=Index([\"M\", \"W\"], dtype=\"object\", name=\"MW\"),\n    )\n\n    gb = df.groupby(by=[\"MW\"])\n    result = gb.sum()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_aggregation_multi_non_numeric_dtype():\n    # GH #42395\n    df = DataFrame(\n        {\n            \"x\": [1, 0, 1, 1, 0],\n            \"y\": [Timedelta(i, \"days\") for i in range(1, 6)],\n            \"z\": [Timedelta(i * 10, \"days\") for i in range(1, 6)],\n        }\n    )\n\n    expected = DataFrame(\n        {\n            \"y\": [Timedelta(i, \"days\") for i in range(7, 9)],\n            \"z\": [Timedelta(i * 10, \"days\") for i in range(7, 9)],\n        },\n        index=Index([0, 1], dtype=\"int64\", name=\"x\"),\n    )\n\n    gb = df.groupby(by=[\"x\"])\n    result = gb.sum()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_aggregation_numeric_with_non_numeric_dtype():\n    # GH #43108\n    df = DataFrame(\n        {\n            \"x\": [1, 0, 1, 1, 0],\n            \"y\": [Timedelta(i, \"days\") for i in range(1, 6)],\n            \"z\": list(range(1, 6)),\n        }\n    )\n\n    expected = DataFrame(\n        {\"y\": [Timedelta(7, \"days\"), Timedelta(8, \"days\")], \"z\": [7, 8]},\n        index=Index([0, 1], dtype=\"int64\", name=\"x\"),\n    )\n\n    gb = df.groupby(by=[\"x\"])\n    result = gb.sum()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_filtered_df_std():\n    # GH 16174\n    dicts = [\n        {\"filter_col\": False, \"groupby_col\": True, \"bool_col\": True, \"float_col\": 10.5},\n        {\"filter_col\": True, \"groupby_col\": True, \"bool_col\": True, \"float_col\": 20.5},\n        {\"filter_col\": True, \"groupby_col\": True, \"bool_col\": True, \"float_col\": 30.5},\n    ]\n    df = DataFrame(dicts)\n\n    df_filter = df[df[\"filter_col\"] == True]  # noqa:E712\n    dfgb = df_filter.groupby(\"groupby_col\")\n    result = dfgb.std()\n    expected = DataFrame(\n        [[0.0, 0.0, 7.071068]],\n        columns=[\"filter_col\", \"bool_col\", \"float_col\"],\n        index=Index([True], name=\"groupby_col\"),\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_datetime_categorical_multikey_groupby_indices():\n    # GH 26859\n    df = DataFrame(\n        {\n            \"a\": Series(list(\"abc\")),\n            \"b\": Series(\n                to_datetime([\"2018-01-01\", \"2018-02-01\", \"2018-03-01\"]),\n                dtype=\"category\",\n            ),\n            \"c\": Categorical.from_codes([-1, 0, 1], categories=[0, 1]),\n        }\n    )\n    result = df.groupby([\"a\", \"b\"], observed=False).indices\n    expected = {\n        (\"a\", Timestamp(\"2018-01-01 00:00:00\")): np.array([0]),\n        (\"b\", Timestamp(\"2018-02-01 00:00:00\")): np.array([1]),\n        (\"c\", Timestamp(\"2018-03-01 00:00:00\")): np.array([2]),\n    }\n    assert result == expected\n\n\ndef test_rolling_wrong_param_min_period():\n    # GH34037\n    name_l = [\"Alice\"] * 5 + [\"Bob\"] * 5\n    val_l = [np.nan, np.nan, 1, 2, 3] + [np.nan, 1, 2, 3, 4]\n    test_df = DataFrame([name_l, val_l]).T\n    test_df.columns = [\"name\", \"val\"]\n\n    result_error_msg = r\"__init__\\(\\) got an unexpected keyword argument 'min_period'\"\n    with pytest.raises(TypeError, match=result_error_msg):\n        test_df.groupby(\"name\")[\"val\"].rolling(window=2, min_period=1).sum()\n\n\ndef test_by_column_values_with_same_starting_value():\n    # GH29635\n    df = DataFrame(\n        {\n            \"Name\": [\"Thomas\", \"Thomas\", \"Thomas John\"],\n            \"Credit\": [1200, 1300, 900],\n            \"Mood\": [\"sad\", \"happy\", \"happy\"],\n        }\n    )\n    aggregate_details = {\"Mood\": Series.mode, \"Credit\": \"sum\"}\n\n    result = df.groupby([\"Name\"]).agg(aggregate_details)\n    expected_result = DataFrame(\n        {\n            \"Mood\": [[\"happy\", \"sad\"], \"happy\"],\n            \"Credit\": [2500, 900],\n            \"Name\": [\"Thomas\", \"Thomas John\"],\n        }\n    ).set_index(\"Name\")\n\n    tm.assert_frame_equal(result, expected_result)\n\n\ndef test_groupby_none_in_first_mi_level():\n    # GH#47348\n    arr = [[None, 1, 0, 1], [2, 3, 2, 3]]\n    ser = Series(1, index=MultiIndex.from_arrays(arr, names=[\"a\", \"b\"]))\n    result = ser.groupby(level=[0, 1]).sum()\n    expected = Series(\n        [1, 2], MultiIndex.from_tuples([(0.0, 2), (1.0, 3)], names=[\"a\", \"b\"])\n    )\n    tm.assert_series_equal(result, expected)\n\n\ndef test_groupby_none_column_name():\n    # GH#47348\n    df = DataFrame({None: [1, 1, 2, 2], \"b\": [1, 1, 2, 3], \"c\": [4, 5, 6, 7]})\n    result = df.groupby(by=[None]).sum()\n    expected = DataFrame({\"b\": [2, 5], \"c\": [9, 13]}, index=Index([1, 2], name=None))\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_single_element_list_grouping():\n    # GH 42795\n    df = DataFrame({\"a\": [1, 2], \"b\": [np.nan, 5], \"c\": [np.nan, 2]}, index=[\"x\", \"y\"])\n    result = [key for key, _ in df.groupby([\"a\"])]\n    expected = [(1,), (2,)]\n    assert result == expected\n\n\ndef test_groupby_string_dtype():\n    # GH 40148\n    df = DataFrame({\"str_col\": [\"a\", \"b\", \"c\", \"a\"], \"num_col\": [1, 2, 3, 2]})\n    df[\"str_col\"] = df[\"str_col\"].astype(\"string\")\n    expected = DataFrame(\n        {\n            \"str_col\": [\n                \"a\",\n                \"b\",\n                \"c\",\n            ],\n            \"num_col\": [1.5, 2.0, 3.0],\n        }\n    )\n    expected[\"str_col\"] = expected[\"str_col\"].astype(\"string\")\n    grouped = df.groupby(\"str_col\", as_index=False)\n    result = grouped.mean()\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"level_arg, multiindex\", [([0], False), ((0,), False), ([0], True), ((0,), True)]\n)\ndef test_single_element_listlike_level_grouping_deprecation(level_arg, multiindex):\n    # GH 51583\n    df = DataFrame({\"a\": [1, 2], \"b\": [3, 4], \"c\": [5, 6]}, index=[\"x\", \"y\"])\n    if multiindex:\n        df = df.set_index([\"a\", \"b\"])\n    depr_msg = (\n        \"Creating a Groupby object with a length-1 list-like \"\n        \"level parameter will yield indexes as tuples in a future version. \"\n        \"To keep indexes as scalars, create Groupby objects with \"\n        \"a scalar level parameter instead.\"\n    )\n    with tm.assert_produces_warning(FutureWarning, match=depr_msg):\n        [key for key, _ in df.groupby(level=level_arg)]\n\n\n@pytest.mark.parametrize(\"func\", [\"sum\", \"cumsum\", \"cumprod\", \"prod\"])\ndef test_groupby_avoid_casting_to_float(func):\n    # GH#37493\n    val = 922337203685477580\n    df = DataFrame({\"a\": 1, \"b\": [val]})\n    result = getattr(df.groupby(\"a\"), func)() - val\n    expected = DataFrame({\"b\": [0]}, index=Index([1], name=\"a\"))\n    if func in [\"cumsum\", \"cumprod\"]:\n        expected = expected.reset_index(drop=True)\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"func, val\", [(\"sum\", 3), (\"prod\", 2)])\ndef test_groupby_sum_support_mask(any_numeric_ea_dtype, func, val):\n    # GH#37493\n    df = DataFrame({\"a\": 1, \"b\": [1, 2, pd.NA]}, dtype=any_numeric_ea_dtype)\n    result = getattr(df.groupby(\"a\"), func)()\n    expected = DataFrame(\n        {\"b\": [val]},\n        index=Index([1], name=\"a\", dtype=any_numeric_ea_dtype),\n        dtype=any_numeric_ea_dtype,\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"val, dtype\", [(111, \"int\"), (222, \"uint\")])\ndef test_groupby_overflow(val, dtype):\n    # GH#37493\n    df = DataFrame({\"a\": 1, \"b\": [val, val]}, dtype=f\"{dtype}8\")\n    result = df.groupby(\"a\").sum()\n    expected = DataFrame(\n        {\"b\": [val * 2]},\n        index=Index([1], name=\"a\", dtype=f\"{dtype}8\"),\n        dtype=f\"{dtype}64\",\n    )\n    tm.assert_frame_equal(result, expected)\n\n    result = df.groupby(\"a\").cumsum()\n    expected = DataFrame({\"b\": [val, val * 2]}, dtype=f\"{dtype}64\")\n    tm.assert_frame_equal(result, expected)\n\n    result = df.groupby(\"a\").prod()\n    expected = DataFrame(\n        {\"b\": [val * val]},\n        index=Index([1], name=\"a\", dtype=f\"{dtype}8\"),\n        dtype=f\"{dtype}64\",\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"skipna, val\", [(True, 3), (False, pd.NA)])\ndef test_groupby_cumsum_mask(any_numeric_ea_dtype, skipna, val):\n    # GH#37493\n    df = DataFrame({\"a\": 1, \"b\": [1, pd.NA, 2]}, dtype=any_numeric_ea_dtype)\n    result = df.groupby(\"a\").cumsum(skipna=skipna)\n    expected = DataFrame(\n        {\"b\": [1, pd.NA, val]},\n        dtype=any_numeric_ea_dtype,\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"val_in, index, val_out\",\n    [\n        (\n            [1.0, 2.0, 3.0, 4.0, 5.0],\n            [\"foo\", \"foo\", \"bar\", \"baz\", \"blah\"],\n            [3.0, 4.0, 5.0, 3.0],\n        ),\n        (\n            [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\n            [\"foo\", \"foo\", \"bar\", \"baz\", \"blah\", \"blah\"],\n            [3.0, 4.0, 11.0, 3.0],\n        ),\n    ],\n)\ndef test_groupby_index_name_in_index_content(val_in, index, val_out):\n    # GH 48567\n    series = Series(data=val_in, name=\"values\", index=Index(index, name=\"blah\"))\n    result = series.groupby(\"blah\").sum()\n    expected = Series(\n        data=val_out,\n        name=\"values\",\n        index=Index([\"bar\", \"baz\", \"blah\", \"foo\"], name=\"blah\"),\n    )\n    tm.assert_series_equal(result, expected)\n\n    result = series.to_frame().groupby(\"blah\").sum()\n    expected = expected.to_frame()\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"n\", [1, 10, 32, 100, 1000])\ndef test_sum_of_booleans(n):\n    # GH 50347\n    df = DataFrame({\"groupby_col\": 1, \"bool\": [True] * n})\n    df[\"bool\"] = df[\"bool\"].eq(True)\n    result = df.groupby(\"groupby_col\").sum()\n    expected = DataFrame({\"bool\": [n]}, index=Index([1], name=\"groupby_col\"))\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.filterwarnings(\n    \"ignore:invalid value encountered in remainder:RuntimeWarning\"\n)\n@pytest.mark.parametrize(\"method\", [\"head\", \"tail\", \"nth\", \"first\", \"last\"])\ndef test_groupby_method_drop_na(method):\n    # GH 21755\n    df = DataFrame({\"A\": [\"a\", np.nan, \"b\", np.nan, \"c\"], \"B\": range(5)})\n\n    if method == \"nth\":\n        result = getattr(df.groupby(\"A\"), method)(n=0)\n    else:\n        result = getattr(df.groupby(\"A\"), method)()\n\n    if method in [\"first\", \"last\"]:\n        expected = DataFrame({\"B\": [0, 2, 4]}).set_index(\n            Series([\"a\", \"b\", \"c\"], name=\"A\")\n        )\n    else:\n        expected = DataFrame({\"A\": [\"a\", \"b\", \"c\"], \"B\": [0, 2, 4]}, index=[0, 2, 4])\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_reduce_period():\n    # GH#51040\n    pi = pd.period_range(\"2016-01-01\", periods=100, freq=\"D\")\n    grps = list(range(10)) * 10\n    ser = pi.to_series()\n    gb = ser.groupby(grps)\n\n    with pytest.raises(TypeError, match=\"Period type does not support sum operations\"):\n        gb.sum()\n    with pytest.raises(\n        TypeError, match=\"Period type does not support cumsum operations\"\n    ):\n        gb.cumsum()\n    with pytest.raises(TypeError, match=\"Period type does not support prod operations\"):\n        gb.prod()\n    with pytest.raises(\n        TypeError, match=\"Period type does not support cumprod operations\"\n    ):\n        gb.cumprod()\n\n    res = gb.max()\n    expected = ser[-10:]\n    expected.index = Index(range(10), dtype=np.int_)\n    tm.assert_series_equal(res, expected)\n\n    res = gb.min()\n    expected = ser[:10]\n    expected.index = Index(range(10), dtype=np.int_)\n    tm.assert_series_equal(res, expected)\n\n\ndef test_obj_with_exclusions_duplicate_columns():\n    # GH#50806\n    df = DataFrame([[0, 1, 2, 3]])\n    df.columns = [0, 1, 2, 0]\n    gb = df.groupby(df[1])\n    result = gb._obj_with_exclusions\n    expected = df.take([0, 2, 3], axis=1)\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"numeric_only\", [True, False])\ndef test_groupby_numeric_only_std_no_result(numeric_only):\n    # GH 51080\n    dicts_non_numeric = [{\"a\": \"foo\", \"b\": \"bar\"}, {\"a\": \"car\", \"b\": \"dar\"}]\n    df = DataFrame(dicts_non_numeric)\n    dfgb = df.groupby(\"a\", as_index=False, sort=False)\n\n    if numeric_only:\n        result = dfgb.std(numeric_only=True)\n        expected_df = DataFrame([\"foo\", \"car\"], columns=[\"a\"])\n        tm.assert_frame_equal(result, expected_df)\n    else:\n        with pytest.raises(\n            ValueError, match=\"could not convert string to float: 'bar'\"\n        ):\n            dfgb.std(numeric_only=numeric_only)\n\n\ndef test_grouping_with_categorical_interval_columns():\n    # GH#34164\n    df = DataFrame({\"x\": [0.1, 0.2, 0.3, -0.4, 0.5], \"w\": [\"a\", \"b\", \"a\", \"c\", \"a\"]})\n    qq = pd.qcut(df[\"x\"], q=np.linspace(0, 1, 5))\n    result = df.groupby([qq, \"w\"], observed=False)[\"x\"].agg(\"mean\")\n    categorical_index_level_1 = Categorical(\n        [\n            Interval(-0.401, 0.1, closed=\"right\"),\n            Interval(0.1, 0.2, closed=\"right\"),\n            Interval(0.2, 0.3, closed=\"right\"),\n            Interval(0.3, 0.5, closed=\"right\"),\n        ],\n        ordered=True,\n    )\n    index_level_2 = [\"a\", \"b\", \"c\"]\n    mi = MultiIndex.from_product(\n        [categorical_index_level_1, index_level_2], names=[\"x\", \"w\"]\n    )\n    expected = Series(\n        np.array(\n            [\n                0.1,\n                np.nan,\n                -0.4,\n                np.nan,\n                0.2,\n                np.nan,\n                0.3,\n                np.nan,\n                np.nan,\n                0.5,\n                np.nan,\n                np.nan,\n            ]\n        ),\n        index=mi,\n        name=\"x\",\n    )\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"bug_var\", [1, \"a\"])\ndef test_groupby_sum_on_nan_should_return_nan(bug_var):\n    # GH 24196\n    df = DataFrame({\"A\": [bug_var, bug_var, bug_var, np.nan]})\n    dfgb = df.groupby(lambda x: x)\n    result = dfgb.sum(min_count=1)\n\n    expected_df = DataFrame([bug_var, bug_var, bug_var, np.nan], columns=[\"A\"])\n    tm.assert_frame_equal(result, expected_df)\n"
    }
  ],
  "questions": [
    "Noted, can I take on this issue?",
    "After tweaking around on [notebook](https://colab.research.google.com/drive/1bNPCRzmSzAM85MW7k6209b4WP7vS9ExC?usp=sharing), it seems like I could replicate the problem with the condition:\r\n- Using pandas 1.0.3 (no problem on 1.0.4)\r\n- MultiIndexing dataframe\r\n- Using column with type of `categoricalDtype` not `IntervalDtype`\r\n- Using `observed=False` as `groupby`'s argument\r\nAnd it seems like the current test suite has already take this into account with these following lines:\r\nhttps://github.com/pandas-dev/pandas/blob/361021b56f3159afb71d690fac3a1f3b381b0da6/pandas/tests/groupby/test_categorical.py#L317-L353\r\nI want to test pandas version 1.0.3 against this unit test do you have any recommendation to do so?"
  ],
  "golden_answers": [
    "After tweaking around on [notebook](https://colab.research.google.com/drive/1bNPCRzmSzAM85MW7k6209b4WP7vS9ExC?usp=sharing), it seems like I could replicate the problem with the condition:\r\n- Using pandas 1.0.3 (no problem on 1.0.4)\r\n- MultiIndexing dataframe\r\n- Using column with type of `categoricalDtype` not `IntervalDtype`\r\n- Using `observed=False` as `groupby`'s argument\r\nAnd it seems like the current test suite has already take this into account with these following lines:\r\nhttps://github.com/pandas-dev/pandas/blob/361021b56f3159afb71d690fac3a1f3b381b0da6/pandas/tests/groupby/test_categorical.py#L317-L353\r\nI want to test pandas version 1.0.3 against this unit test do you have any recommendation to do so?",
    "Looks like [Closed PR#52818](https://github.com/pandas-dev/pandas/pull/52818) should have closed this task. Pinging @phofl for visibility."
  ],
  "questions_generated": [
    "What is the root cause of the TypeError encountered when attempting to group by a categorical interval column along with another column in pandas 1.0.3?",
    "Why does the groupby operation work when grouping by a single categorical interval column but fail with a TypeError when an additional column is added?",
    "In which part of the pandas codebase does the error occur when executing the groupby operation with a categorical interval and another column?",
    "What does the traceback suggest about the implementation of the 'isna' function when handling categorical data in a groupby operation?",
    "How does the current issue with grouping by categorical intervals affect the general functionality of groupby operations in pandas?"
  ],
  "golden_answers_generated": [
    "The TypeError arises because the pandas implementation for groupby operations does not handle categorical interval columns correctly when they are combined with other columns. Specifically, the error occurs during the reindexing of the output in the 'MultiIndex.from_product' method, where the 'isna' function is applied incorrectly to a Categorical object instead of a numpy.ndarray.",
    "The groupby operation works with a single categorical interval column because pandas can handle single categorical objects in isolation. However, when another column is added, it requires constructing a MultiIndex, and the current implementation fails to properly handle the categorical interval type in this context, leading to a TypeError during the construction of the MultiIndex.",
    "The error occurs in the 'pandas/core/groupby/groupby.py' file within the '_cython_agg_general' method, specifically when trying to wrap the aggregated output in the '_wrap_aggregated_output' method. The issue arises during the creation of a MultiIndex in the '_reindex_output' method, where the pandas internal method 'MultiIndex.from_product' is called, leading to the TypeError.",
    "The traceback suggests that the 'isna' function is not correctly handling categorical data during a groupby operation. The function '_isna_ndarraylike_old' is expecting a numpy.ndarray, but it receives a pandas Categorical object instead. This mismatch in expected type leads to a TypeError, indicating that the implementation needs to account for categorical types.",
    "The issue with grouping by categorical intervals affects the general functionality of groupby operations by limiting the ability to perform aggregations on datasets that include categorical interval columns combined with other columns. This restricts the analytical capabilities of pandas when dealing with mixed-type groupings and can lead to errors in data processing pipelines that rely on such operations."
  ]
}