{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "31687",
  "issue_description": "# pd.where OverflowError with large numbers\n\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame([[1.0, 2e25],[np.nan, 0.1]])\r\n\r\n# Works when applied to individual columns\r\nprint(df[0].where(pd.notnull(df[0]), None))\r\nprint(df[1].where(pd.notnull(df[1]), None))\r\n\r\n# Breaks for whole dataframe\r\nprint(df.where(pd.notnull(df), None))\r\n\r\n```\r\n#### Problem description\r\nThe above code does not work with 1.0.0, but used to work with at least 0.25.0.\r\nReplacing large floats with pd.where breaks \r\nRunning pd.where on a dataframe that contains large float values and the replacement value is of a different dtype throws `OverflowError: int too big to convert`:\r\n\r\n```\r\n    applied = getattr(b, f)(**kwargs)\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 1426, in where\r\n    return self._maybe_downcast(blocks, \"infer\")\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 514, in _maybe_downcast\r\n    return _extend_blocks([b.downcast(downcast) for b in blocks])\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 514, in <listcomp>\r\n    return _extend_blocks([b.downcast(downcast) for b in blocks])\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 552, in downcast\r\n    return self.split_and_operate(None, f, False)\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 496, in split_and_operate\r\n    nv = f(m, v, i)\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 549, in f\r\n    val = maybe_downcast_to_dtype(val, dtype=\"infer\")\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\", line 135, in maybe_downcast_to_dtype\r\n    converted = maybe_downcast_numeric(result, dtype, do_round)\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\", line 222, in maybe_downcast_numeric\r\n    new_result = trans(result).astype(dtype)\r\nOverflowError: int too big to convert\r\n```\r\n\r\nReplacing data one column at a time works.\r\n\r\n\r\n#### Expected Output\r\n```\r\nName: 1, dtype: float64\r\n      0      1\r\n0     1  2e+25\r\n1  None    0.1\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.3.final.0\r\npython-bits      : 64\r\nOS               : Windows\r\nOS-release       : 10\r\nmachine          : AMD64\r\nprocessor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : None\r\nLOCALE           : None.None\r\npandas           : 1.0.0\r\nnumpy            : 1.16.2\r\npytz             : 2018.9\r\ndateutil         : 2.8.0\r\npip              : 10.0.1\r\nsetuptools       : 39.1.0\r\nCython           : None\r\npytest           : 4.4.0\r\nhypothesis       : None\r\nsphinx           : 2.0.0\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : 4.3.3\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.11.1\r\nIPython          : None\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.3.3\r\nmatplotlib       : None\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : 2.6.2\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\npytest           : 4.4.0\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : None\r\nsqlalchemy       : 1.3.3\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : 1.2.0\r\nxlwt             : None\r\nxlsxwriter       : None\r\nnumba            : None\r\n</details>\r\n",
  "issue_comments": [
    {
      "id": 584146394,
      "user": "wailashi",
      "body": "Seems to be still broken on 1.0.1. "
    },
    {
      "id": 613123827,
      "user": "rmsilva1973",
      "body": "**print(df.where(pd.notnull(df), 'some_str'))** raises the same exception.\r\nHowever **print(df.where(pd.notnull(df), 1))** works.\r\n\r\nDebugging through the code, it seems on this line **new_result = trans(result).astype(dtype)**, the astype method is to blame. **dtype** here is **int64**"
    },
    {
      "id": 619389949,
      "user": "simonjayhawkins",
      "body": "> The above code does not work with 1.0.0, but used to work with at least 0.25.0.\r\n\r\nlooks like #29139 (i.e. 1.0.0)\r\n\r\n225cc9284d1abfbcc9d13203419516575a63cc7a is the first bad commit\r\ncommit 225cc9284d1abfbcc9d13203419516575a63cc7a\r\nAuthor: jbrockmendel <jbrockmendel@gmail.com>\r\nDate:   Thu Oct 24 05:10:07 2019 -0700\r\n\r\n    CLN: remove Block._try_coerce_arg (#29139)\r\n\r\ncc @jbrockmendel "
    },
    {
      "id": 643499144,
      "user": "eloyfelix",
      "body": "also affected by this bug"
    },
    {
      "id": 644838773,
      "user": "uchoa91",
      "body": "Same here. However got it using df.replace({pd.NaT: None}) instead of df.where(pd.notnull(df), None)"
    },
    {
      "id": 644842866,
      "user": "simonjayhawkins",
      "body": "@TomAugspurger adding the blocker tag as several users affected by this regression."
    },
    {
      "id": 644885307,
      "user": "jreback",
      "body": "@simonjayhawkins i suppose this is ok for 1.1 as a blocker, though generally we should not simply block on things, this would delay releases indefinitly which is a much worse problem."
    },
    {
      "id": 710030313,
      "user": "eloyfelix",
      "body": "still broken in 1.1.3"
    },
    {
      "id": 710083263,
      "user": "jreback",
      "body": "for all those commenting 'this is still broken' - well it's an open issue\n\nyou are welcome to propose a patch"
    },
    {
      "id": 710086659,
      "user": "eloyfelix",
      "body": "sorry, it was marked as a blocker for 1.1 and I was not sure about its status\r\n"
    },
    {
      "id": 726678024,
      "user": "skvrahul",
      "body": "An added observation is that this seems to still be breaking with numbers smaller than `sys.maxsize`\r\nTrying the same piece of code with a change in number:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport sys\r\n\r\ndf = pd.DataFrame([[1.0, sys.maxsize-5],[np.nan, 0.1]])\r\n\r\n# Works when applied to individual columns\r\nprint(df[0].where(pd.notnull(df[0]), None))\r\nprint(df[1].where(pd.notnull(df[1]), None))\r\n\r\n# This line still breaks\r\nprint(df.where(pd.notnull(df), None))\r\n```\r\n\r\nThis still causes the same error\r\n"
    },
    {
      "id": 736070222,
      "user": "suvayu",
      "body": "take"
    },
    {
      "id": 888003204,
      "user": "mroeschke",
      "body": "This looks to work on master now (`None` should coerce to `nan`). Could use a test.\r\n\r\n```\r\nIn [2]: import pandas as pd\r\n   ...: import numpy as np\r\n   ...:\r\n   ...: df = pd.DataFrame([[1.0, 2e25],[np.nan, 0.1]])\r\n   ...:\r\n   ...: # Works when applied to individual columns\r\n   ...: print(df[0].where(pd.notnull(df[0]), None))\r\n   ...: print(df[1].where(pd.notnull(df[1]), None))\r\n   ...:\r\n   ...: # Breaks for whole dataframe\r\n   ...: print(df.where(pd.notnull(df), None))\r\n0    1.0\r\n1    NaN\r\nName: 0, dtype: float64\r\n0    2.000000e+25\r\n1    1.000000e-01\r\nName: 1, dtype: float64\r\n     0             1\r\n0  1.0  2.000000e+25\r\n1  NaN  1.000000e-01\r\n```"
    },
    {
      "id": 1002681035,
      "user": "eloyfelix",
      "body": "edited:\r\nnot sure if it is related to the issue but as this other [issue](https://github.com/pandas-dev/pandas/issues/44485) points\r\n\r\n`df = df.where((pd.notnull(df)), None)` doesn't seem to work anymore\r\nand\r\n`df = df.replace({np.nan: None})` doesn't always work\r\n\r\n\r\n"
    },
    {
      "id": 1319335221,
      "user": "mliu08",
      "body": "take"
    },
    {
      "id": 1328237014,
      "user": "MarcoGorelli",
      "body": "Looks like this was fixed by https://github.com/pandas-dev/pandas/pull/39761\r\n\r\nhttps://www.kaggle.com/code/marcogorelli/pandas-regression-example/notebook?scriptVersionId=112217500"
    }
  ],
  "text_context": "# pd.where OverflowError with large numbers\n\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame([[1.0, 2e25],[np.nan, 0.1]])\r\n\r\n# Works when applied to individual columns\r\nprint(df[0].where(pd.notnull(df[0]), None))\r\nprint(df[1].where(pd.notnull(df[1]), None))\r\n\r\n# Breaks for whole dataframe\r\nprint(df.where(pd.notnull(df), None))\r\n\r\n```\r\n#### Problem description\r\nThe above code does not work with 1.0.0, but used to work with at least 0.25.0.\r\nReplacing large floats with pd.where breaks \r\nRunning pd.where on a dataframe that contains large float values and the replacement value is of a different dtype throws `OverflowError: int too big to convert`:\r\n\r\n```\r\n    applied = getattr(b, f)(**kwargs)\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 1426, in where\r\n    return self._maybe_downcast(blocks, \"infer\")\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 514, in _maybe_downcast\r\n    return _extend_blocks([b.downcast(downcast) for b in blocks])\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 514, in <listcomp>\r\n    return _extend_blocks([b.downcast(downcast) for b in blocks])\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 552, in downcast\r\n    return self.split_and_operate(None, f, False)\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 496, in split_and_operate\r\n    nv = f(m, v, i)\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 549, in f\r\n    val = maybe_downcast_to_dtype(val, dtype=\"infer\")\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\", line 135, in maybe_downcast_to_dtype\r\n    converted = maybe_downcast_numeric(result, dtype, do_round)\r\n  File \"***\\venv\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\", line 222, in maybe_downcast_numeric\r\n    new_result = trans(result).astype(dtype)\r\nOverflowError: int too big to convert\r\n```\r\n\r\nReplacing data one column at a time works.\r\n\r\n\r\n#### Expected Output\r\n```\r\nName: 1, dtype: float64\r\n      0      1\r\n0     1  2e+25\r\n1  None    0.1\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.3.final.0\r\npython-bits      : 64\r\nOS               : Windows\r\nOS-release       : 10\r\nmachine          : AMD64\r\nprocessor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : None\r\nLOCALE           : None.None\r\npandas           : 1.0.0\r\nnumpy            : 1.16.2\r\npytz             : 2018.9\r\ndateutil         : 2.8.0\r\npip              : 10.0.1\r\nsetuptools       : 39.1.0\r\nCython           : None\r\npytest           : 4.4.0\r\nhypothesis       : None\r\nsphinx           : 2.0.0\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : 4.3.3\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.11.1\r\nIPython          : None\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.3.3\r\nmatplotlib       : None\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : 2.6.2\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\npytest           : 4.4.0\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : None\r\nsqlalchemy       : 1.3.3\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : 1.2.0\r\nxlwt             : None\r\nxlsxwriter       : None\r\nnumba            : None\r\n</details>\r\n\n\nSeems to be still broken on 1.0.1. \n\n**print(df.where(pd.notnull(df), 'some_str'))** raises the same exception.\r\nHowever **print(df.where(pd.notnull(df), 1))** works.\r\n\r\nDebugging through the code, it seems on this line **new_result = trans(result).astype(dtype)**, the astype method is to blame. **dtype** here is **int64**\n\n> The above code does not work with 1.0.0, but used to work with at least 0.25.0.\r\n\r\nlooks like #29139 (i.e. 1.0.0)\r\n\r\n225cc9284d1abfbcc9d13203419516575a63cc7a is the first bad commit\r\ncommit 225cc9284d1abfbcc9d13203419516575a63cc7a\r\nAuthor: jbrockmendel <jbrockmendel@gmail.com>\r\nDate:   Thu Oct 24 05:10:07 2019 -0700\r\n\r\n    CLN: remove Block._try_coerce_arg (#29139)\r\n\r\ncc @jbrockmendel \n\nalso affected by this bug\n\nSame here. However got it using df.replace({pd.NaT: None}) instead of df.where(pd.notnull(df), None)\n\n@TomAugspurger adding the blocker tag as several users affected by this regression.\n\n@simonjayhawkins i suppose this is ok for 1.1 as a blocker, though generally we should not simply block on things, this would delay releases indefinitly which is a much worse problem.\n\nstill broken in 1.1.3\n\nfor all those commenting 'this is still broken' - well it's an open issue\n\nyou are welcome to propose a patch\n\nsorry, it was marked as a blocker for 1.1 and I was not sure about its status\r\n\n\nAn added observation is that this seems to still be breaking with numbers smaller than `sys.maxsize`\r\nTrying the same piece of code with a change in number:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport sys\r\n\r\ndf = pd.DataFrame([[1.0, sys.maxsize-5],[np.nan, 0.1]])\r\n\r\n# Works when applied to individual columns\r\nprint(df[0].where(pd.notnull(df[0]), None))\r\nprint(df[1].where(pd.notnull(df[1]), None))\r\n\r\n# This line still breaks\r\nprint(df.where(pd.notnull(df), None))\r\n```\r\n\r\nThis still causes the same error\r\n\n\ntake\n\nThis looks to work on master now (`None` should coerce to `nan`). Could use a test.\r\n\r\n```\r\nIn [2]: import pandas as pd\r\n   ...: import numpy as np\r\n   ...:\r\n   ...: df = pd.DataFrame([[1.0, 2e25],[np.nan, 0.1]])\r\n   ...:\r\n   ...: # Works when applied to individual columns\r\n   ...: print(df[0].where(pd.notnull(df[0]), None))\r\n   ...: print(df[1].where(pd.notnull(df[1]), None))\r\n   ...:\r\n   ...: # Breaks for whole dataframe\r\n   ...: print(df.where(pd.notnull(df), None))\r\n0    1.0\r\n1    NaN\r\nName: 0, dtype: float64\r\n0    2.000000e+25\r\n1    1.000000e-01\r\nName: 1, dtype: float64\r\n     0             1\r\n0  1.0  2.000000e+25\r\n1  NaN  1.000000e-01\r\n```\n\nedited:\r\nnot sure if it is related to the issue but as this other [issue](https://github.com/pandas-dev/pandas/issues/44485) points\r\n\r\n`df = df.where((pd.notnull(df)), None)` doesn't seem to work anymore\r\nand\r\n`df = df.replace({np.nan: None})` doesn't always work\r\n\r\n\r\n\n\ntake\n\nLooks like this was fixed by https://github.com/pandas-dev/pandas/pull/39761\r\n\r\nhttps://www.kaggle.com/code/marcogorelli/pandas-regression-example/notebook?scriptVersionId=112217500",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/39761",
  "code_context": [
    {
      "filename": "pandas/core/internals/blocks.py",
      "content": "from __future__ import annotations\n\nimport inspect\nimport re\nfrom typing import TYPE_CHECKING, Any, Callable, List, Optional, Type, Union, cast\n\nimport numpy as np\n\nfrom pandas._libs import (\n    Interval,\n    Period,\n    Timestamp,\n    algos as libalgos,\n    internals as libinternals,\n    lib,\n    writers,\n)\nfrom pandas._libs.internals import BlockPlacement\nfrom pandas._libs.tslibs import conversion\nfrom pandas._typing import ArrayLike, Dtype, DtypeObj, Shape\nfrom pandas.util._validators import validate_bool_kwarg\n\nfrom pandas.core.dtypes.cast import (\n    astype_dt64_to_dt64tz,\n    astype_nansafe,\n    can_hold_element,\n    convert_scalar_for_putitemlike,\n    find_common_type,\n    infer_dtype_from,\n    maybe_downcast_numeric,\n    maybe_downcast_to_dtype,\n    maybe_upcast,\n    soft_convert_objects,\n)\nfrom pandas.core.dtypes.common import (\n    DT64NS_DTYPE,\n    TD64NS_DTYPE,\n    is_categorical_dtype,\n    is_datetime64_dtype,\n    is_datetime64tz_dtype,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_integer,\n    is_list_like,\n    is_object_dtype,\n    is_sparse,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import CategoricalDtype, ExtensionDtype, PandasDtype\nfrom pandas.core.dtypes.generic import ABCDataFrame, ABCIndex, ABCPandasArray, ABCSeries\nfrom pandas.core.dtypes.missing import is_valid_na_for_dtype, isna\n\nimport pandas.core.algorithms as algos\nfrom pandas.core.array_algos.putmask import (\n    putmask_inplace,\n    putmask_smart,\n    putmask_without_repeat,\n)\nfrom pandas.core.array_algos.replace import (\n    compare_or_regex_search,\n    replace_regex,\n    should_use_regex,\n)\nfrom pandas.core.array_algos.transforms import shift\nfrom pandas.core.arrays import (\n    Categorical,\n    DatetimeArray,\n    ExtensionArray,\n    PandasArray,\n    TimedeltaArray,\n)\nfrom pandas.core.base import PandasObject\nimport pandas.core.common as com\nfrom pandas.core.construction import extract_array\nfrom pandas.core.indexers import (\n    check_setitem_lengths,\n    is_empty_indexer,\n    is_exact_shape_match,\n    is_scalar_indexer,\n)\nimport pandas.core.missing as missing\nfrom pandas.core.nanops import nanpercentile\n\nif TYPE_CHECKING:\n    from pandas import Float64Index, Index\n    from pandas.core.arrays._mixins import NDArrayBackedExtensionArray\n\n\nclass Block(PandasObject):\n    \"\"\"\n    Canonical n-dimensional unit of homogeneous dtype contained in a pandas\n    data structure\n\n    Index-ignorant; let the container take care of that\n    \"\"\"\n\n    values: Union[np.ndarray, ExtensionArray]\n\n    __slots__ = [\"_mgr_locs\", \"values\", \"ndim\"]\n    is_numeric = False\n    is_float = False\n    is_datetime = False\n    is_datetimetz = False\n    is_timedelta = False\n    is_bool = False\n    is_object = False\n    is_extension = False\n    _can_hold_na = False\n    _can_consolidate = True\n    _validate_ndim = True\n\n    @classmethod\n    def _simple_new(\n        cls, values: ArrayLike, placement: BlockPlacement, ndim: int\n    ) -> Block:\n        \"\"\"\n        Fastpath constructor, does *no* validation\n        \"\"\"\n        obj = object.__new__(cls)\n        obj.ndim = ndim\n        obj.values = values\n        obj._mgr_locs = placement\n        return obj\n\n    def __init__(self, values, placement, ndim: int):\n        \"\"\"\n        Parameters\n        ----------\n        values : np.ndarray or ExtensionArray\n        placement : BlockPlacement (or castable)\n        ndim : int\n            1 for SingleBlockManager/Series, 2 for BlockManager/DataFrame\n        \"\"\"\n        # TODO(EA2D): ndim will be unnecessary with 2D EAs\n        self.ndim = self._check_ndim(values, ndim)\n        self.mgr_locs = placement\n        self.values = self._maybe_coerce_values(values)\n\n        if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):\n            raise ValueError(\n                f\"Wrong number of items passed {len(self.values)}, \"\n                f\"placement implies {len(self.mgr_locs)}\"\n            )\n\n    def _maybe_coerce_values(self, values):\n        \"\"\"\n        Ensure we have correctly-typed values.\n\n        Parameters\n        ----------\n        values : np.ndarray, ExtensionArray, Index\n\n        Returns\n        -------\n        np.ndarray or ExtensionArray\n        \"\"\"\n        return values\n\n    def _check_ndim(self, values, ndim):\n        \"\"\"\n        ndim inference and validation.\n\n        Infers ndim from 'values' if not provided to __init__.\n        Validates that values.ndim and ndim are consistent if and only if\n        the class variable '_validate_ndim' is True.\n\n        Parameters\n        ----------\n        values : array-like\n        ndim : int or None\n\n        Returns\n        -------\n        ndim : int\n\n        Raises\n        ------\n        ValueError : the number of dimensions do not match\n        \"\"\"\n        if ndim is None:\n            ndim = values.ndim\n\n        if self._validate_ndim and values.ndim != ndim:\n            raise ValueError(\n                \"Wrong number of dimensions. \"\n                f\"values.ndim != ndim [{values.ndim} != {ndim}]\"\n            )\n        return ndim\n\n    @property\n    def _holder(self):\n        \"\"\"\n        The array-like that can hold the underlying values.\n\n        None for 'Block', overridden by subclasses that don't\n        use an ndarray.\n        \"\"\"\n        return None\n\n    @property\n    def _consolidate_key(self):\n        return self._can_consolidate, self.dtype.name\n\n    @property\n    def is_view(self) -> bool:\n        \"\"\" return a boolean if I am possibly a view \"\"\"\n        values = self.values\n        values = cast(np.ndarray, values)\n        return values.base is not None\n\n    @property\n    def is_categorical(self) -> bool:\n        return self._holder is Categorical\n\n    @property\n    def is_datelike(self) -> bool:\n        \"\"\" return True if I am a non-datelike \"\"\"\n        return self.is_datetime or self.is_timedelta\n\n    def external_values(self):\n        \"\"\"\n        The array that Series.values returns (public attribute).\n\n        This has some historical constraints, and is overridden in block\n        subclasses to return the correct array (e.g. period returns\n        object ndarray and datetimetz a datetime64[ns] ndarray instead of\n        proper extension array).\n        \"\"\"\n        return self.values\n\n    def internal_values(self):\n        \"\"\"\n        The array that Series._values returns (internal values).\n        \"\"\"\n        return self.values\n\n    def array_values(self) -> ExtensionArray:\n        \"\"\"\n        The array that Series.array returns. Always an ExtensionArray.\n        \"\"\"\n        return PandasArray(self.values)\n\n    def get_values(self, dtype: Optional[DtypeObj] = None) -> np.ndarray:\n        \"\"\"\n        return an internal format, currently just the ndarray\n        this is often overridden to handle to_dense like operations\n        \"\"\"\n        if is_object_dtype(dtype):\n            return self.values.astype(object)\n        return self.values\n\n    def get_block_values_for_json(self) -> np.ndarray:\n        \"\"\"\n        This is used in the JSON C code.\n        \"\"\"\n        # TODO(EA2D): reshape will be unnecessary with 2D EAs\n        return np.asarray(self.values).reshape(self.shape)\n\n    @property\n    def fill_value(self):\n        return np.nan\n\n    @property\n    def mgr_locs(self):\n        return self._mgr_locs\n\n    @mgr_locs.setter\n    def mgr_locs(self, new_mgr_locs):\n        if not isinstance(new_mgr_locs, libinternals.BlockPlacement):\n            new_mgr_locs = libinternals.BlockPlacement(new_mgr_locs)\n\n        self._mgr_locs = new_mgr_locs\n\n    def make_block(self, values, placement=None) -> Block:\n        \"\"\"\n        Create a new block, with type inference propagate any values that are\n        not specified\n        \"\"\"\n        if placement is None:\n            placement = self.mgr_locs\n        if self.is_extension:\n            values = _block_shape(values, ndim=self.ndim)\n\n        return make_block(values, placement=placement, ndim=self.ndim)\n\n    def make_block_same_class(self, values, placement=None, ndim=None) -> Block:\n        \"\"\" Wrap given values in a block of same type as self. \"\"\"\n        if placement is None:\n            placement = self.mgr_locs\n        if ndim is None:\n            ndim = self.ndim\n        return type(self)(values, placement=placement, ndim=ndim)\n\n    def __repr__(self) -> str:\n        # don't want to print out all of the items here\n        name = type(self).__name__\n        if self.ndim == 1:\n            result = f\"{name}: {len(self)} dtype: {self.dtype}\"\n        else:\n\n            shape = \" x \".join(str(s) for s in self.shape)\n            result = f\"{name}: {self.mgr_locs.indexer}, {shape}, dtype: {self.dtype}\"\n\n        return result\n\n    def __len__(self) -> int:\n        return len(self.values)\n\n    def __getstate__(self):\n        return self.mgr_locs.indexer, self.values\n\n    def __setstate__(self, state):\n        self.mgr_locs = libinternals.BlockPlacement(state[0])\n        self.values = state[1]\n        self.ndim = self.values.ndim\n\n    def _slice(self, slicer):\n        \"\"\" return a slice of my values \"\"\"\n\n        return self.values[slicer]\n\n    def getitem_block(self, slicer, new_mgr_locs=None) -> Block:\n        \"\"\"\n        Perform __getitem__-like, return result as block.\n\n        As of now, only supports slices that preserve dimensionality.\n        \"\"\"\n        if new_mgr_locs is None:\n            axis0_slicer = slicer[0] if isinstance(slicer, tuple) else slicer\n            new_mgr_locs = self.mgr_locs[axis0_slicer]\n        elif not isinstance(new_mgr_locs, BlockPlacement):\n            new_mgr_locs = BlockPlacement(new_mgr_locs)\n\n        new_values = self._slice(slicer)\n\n        if self._validate_ndim and new_values.ndim != self.ndim:\n            raise ValueError(\"Only same dim slicing is allowed\")\n\n        return type(self)._simple_new(new_values, new_mgr_locs, self.ndim)\n\n    @property\n    def shape(self) -> Shape:\n        return self.values.shape\n\n    @property\n    def dtype(self) -> DtypeObj:\n        return self.values.dtype\n\n    def iget(self, i):\n        return self.values[i]\n\n    def set_inplace(self, locs, values):\n        \"\"\"\n        Modify block values in-place with new item value.\n\n        Notes\n        -----\n        `set` never creates a new array or new Block, whereas `setitem` _may_\n        create a new array and always creates a new Block.\n        \"\"\"\n        self.values[locs] = values\n\n    def delete(self, loc) -> None:\n        \"\"\"\n        Delete given loc(-s) from block in-place.\n        \"\"\"\n        self.values = np.delete(self.values, loc, 0)\n        self.mgr_locs = self.mgr_locs.delete(loc)\n\n    def apply(self, func, **kwargs) -> List[Block]:\n        \"\"\"\n        apply the function to my values; return a block if we are not\n        one\n        \"\"\"\n        with np.errstate(all=\"ignore\"):\n            result = func(self.values, **kwargs)\n\n        return self._split_op_result(result)\n\n    def reduce(self, func, ignore_failures: bool = False) -> List[Block]:\n        # We will apply the function and reshape the result into a single-row\n        #  Block with the same mgr_locs; squeezing will be done at a higher level\n        assert self.ndim == 2\n\n        try:\n            result = func(self.values)\n        except (TypeError, NotImplementedError):\n            if ignore_failures:\n                return []\n            raise\n\n        if np.ndim(result) == 0:\n            # TODO(EA2D): special case not needed with 2D EAs\n            res_values = np.array([[result]])\n        else:\n            res_values = result.reshape(-1, 1)\n\n        nb = self.make_block(res_values)\n        return [nb]\n\n    def _split_op_result(self, result) -> List[Block]:\n        # See also: split_and_operate\n        if is_extension_array_dtype(result) and result.ndim > 1:\n            # TODO(EA2D): unnecessary with 2D EAs\n            # if we get a 2D ExtensionArray, we need to split it into 1D pieces\n            nbs = []\n            for i, loc in enumerate(self.mgr_locs):\n                vals = result[i]\n                block = self.make_block(values=vals, placement=[loc])\n                nbs.append(block)\n            return nbs\n\n        if not isinstance(result, Block):\n            result = self.make_block(result)\n\n        return [result]\n\n    def fillna(\n        self, value, limit=None, inplace: bool = False, downcast=None\n    ) -> List[Block]:\n        \"\"\"\n        fillna on the block with the value. If we fail, then convert to\n        ObjectBlock and try again\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        mask = isna(self.values)\n        mask = _extract_bool_array(mask)\n        if limit is not None:\n            limit = libalgos.validate_limit(None, limit=limit)\n            mask[mask.cumsum(self.ndim - 1) > limit] = False\n\n        if not self._can_hold_na:\n            if inplace:\n                return [self]\n            else:\n                return [self.copy()]\n\n        if self._can_hold_element(value):\n            nb = self if inplace else self.copy()\n            putmask_inplace(nb.values, mask, value)\n            # TODO: should be nb._maybe_downcast?\n            return self._maybe_downcast([nb], downcast)\n\n        # we can't process the value, but nothing to do\n        if not mask.any():\n            return [self] if inplace else [self.copy()]\n\n        # operate column-by-column\n        def f(mask, val, idx):\n            block = self.coerce_to_target_dtype(value)\n\n            # slice out our block\n            if idx is not None:\n                # i.e. self.ndim == 2\n                block = block.getitem_block(slice(idx, idx + 1))\n            return block.fillna(value, limit=limit, inplace=inplace, downcast=None)\n\n        return self.split_and_operate(None, f, inplace)\n\n    def _split(self) -> List[Block]:\n        \"\"\"\n        Split a block into a list of single-column blocks.\n        \"\"\"\n        assert self.ndim == 2\n\n        new_blocks = []\n        for i, ref_loc in enumerate(self.mgr_locs):\n            vals = self.values[slice(i, i + 1)]\n\n            nb = self.make_block(vals, [ref_loc])\n            new_blocks.append(nb)\n        return new_blocks\n\n    def split_and_operate(\n        self, mask, f, inplace: bool, ignore_failures: bool = False\n    ) -> List[Block]:\n        \"\"\"\n        split the block per-column, and apply the callable f\n        per-column, return a new block for each. Handle\n        masking which will not change a block unless needed.\n\n        Parameters\n        ----------\n        mask : 2-d boolean mask\n        f : callable accepting (1d-mask, 1d values, indexer)\n        inplace : bool\n        ignore_failures : bool, default False\n\n        Returns\n        -------\n        list of blocks\n        \"\"\"\n        if mask is None:\n            mask = np.broadcast_to(True, shape=self.shape)\n\n        new_values = self.values\n\n        def make_a_block(nv, ref_loc):\n            if isinstance(nv, list):\n                assert len(nv) == 1, nv\n                assert isinstance(nv[0], Block)\n                block = nv[0]\n            else:\n                # Put back the dimension that was taken from it and make\n                # a block out of the result.\n                nv = _block_shape(nv, ndim=self.ndim)\n                block = self.make_block(values=nv, placement=ref_loc)\n            return block\n\n        # ndim == 1\n        if self.ndim == 1:\n            if mask.any():\n                nv = f(mask, new_values, None)\n            else:\n                nv = new_values if inplace else new_values.copy()\n            block = make_a_block(nv, self.mgr_locs)\n            return [block]\n\n        # ndim > 1\n        new_blocks = []\n        for i, ref_loc in enumerate(self.mgr_locs):\n            m = mask[i]\n            v = new_values[i]\n\n            # need a new block\n            if m.any() or m.size == 0:\n                # Apply our function; we may ignore_failures if this is a\n                #  reduction that is dropping nuisance columns GH#37827\n                try:\n                    nv = f(m, v, i)\n                except TypeError:\n                    if ignore_failures:\n                        continue\n                    else:\n                        raise\n            else:\n                nv = v if inplace else v.copy()\n\n            block = make_a_block(nv, [ref_loc])\n            new_blocks.append(block)\n\n        return new_blocks\n\n    def _maybe_downcast(self, blocks: List[Block], downcast=None) -> List[Block]:\n\n        # no need to downcast our float\n        # unless indicated\n        if downcast is None and (self.is_float or self.is_datelike):\n            return blocks\n\n        return extend_blocks([b.downcast(downcast) for b in blocks])\n\n    def downcast(self, dtypes=None) -> List[Block]:\n        \"\"\" try to downcast each item to the dict of dtypes if present \"\"\"\n        # turn it off completely\n        if dtypes is False:\n            return [self]\n\n        values = self.values\n\n        if self.ndim == 1:\n\n            # try to cast all non-floats here\n            if dtypes is None:\n                dtypes = \"infer\"\n\n            nv = maybe_downcast_to_dtype(values, dtypes)\n            return [self.make_block(nv)]\n\n        # ndim > 1\n        if dtypes is None:\n            return [self]\n\n        if not (dtypes == \"infer\" or isinstance(dtypes, dict)):\n            raise ValueError(\n                \"downcast must have a dictionary or 'infer' as its argument\"\n            )\n        elif dtypes != \"infer\":\n            raise AssertionError(\"dtypes as dict is not supported yet\")\n\n        # operate column-by-column\n        # this is expensive as it splits the blocks items-by-item\n        def f(mask, val, idx):\n            val = maybe_downcast_to_dtype(val, dtype=\"infer\")\n            return val\n\n        return self.split_and_operate(None, f, False)\n\n    def astype(self, dtype, copy: bool = False, errors: str = \"raise\"):\n        \"\"\"\n        Coerce to the new dtype.\n\n        Parameters\n        ----------\n        dtype : str, dtype convertible\n        copy : bool, default False\n            copy if indicated\n        errors : str, {'raise', 'ignore'}, default 'raise'\n            - ``raise`` : allow exceptions to be raised\n            - ``ignore`` : suppress exceptions. On error return original object\n\n        Returns\n        -------\n        Block\n        \"\"\"\n        errors_legal_values = (\"raise\", \"ignore\")\n\n        if errors not in errors_legal_values:\n            invalid_arg = (\n                \"Expected value of kwarg 'errors' to be one of \"\n                f\"{list(errors_legal_values)}. Supplied value is '{errors}'\"\n            )\n            raise ValueError(invalid_arg)\n\n        if inspect.isclass(dtype) and issubclass(dtype, ExtensionDtype):\n            msg = (\n                f\"Expected an instance of {dtype.__name__}, \"\n                \"but got the class instead. Try instantiating 'dtype'.\"\n            )\n            raise TypeError(msg)\n\n        dtype = pandas_dtype(dtype)\n\n        try:\n            new_values = self._astype(dtype, copy=copy)\n        except (ValueError, TypeError):\n            # e.g. astype_nansafe can fail on object-dtype of strings\n            #  trying to convert to float\n            if errors == \"ignore\":\n                new_values = self.values\n            else:\n                raise\n\n        newb = self.make_block(new_values)\n        if newb.is_numeric and self.is_numeric:\n            if newb.shape != self.shape:\n                raise TypeError(\n                    f\"cannot set astype for copy = [{copy}] for dtype \"\n                    f\"({self.dtype.name} [{self.shape}]) to different shape \"\n                    f\"({newb.dtype.name} [{newb.shape}])\"\n                )\n        return newb\n\n    def _astype(self, dtype: DtypeObj, copy: bool) -> ArrayLike:\n        values = self.values\n\n        if is_datetime64tz_dtype(dtype) and is_datetime64_dtype(values.dtype):\n            return astype_dt64_to_dt64tz(values, dtype, copy, via_utc=True)\n\n        if is_dtype_equal(values.dtype, dtype):\n            if copy:\n                return values.copy()\n            return values\n\n        if isinstance(values, ExtensionArray):\n            values = values.astype(dtype, copy=copy)\n\n        else:\n            values = astype_nansafe(values, dtype, copy=copy)\n\n        return values\n\n    def convert(\n        self,\n        copy: bool = True,\n        datetime: bool = True,\n        numeric: bool = True,\n        timedelta: bool = True,\n    ) -> List[Block]:\n        \"\"\"\n        attempt to coerce any object types to better types return a copy\n        of the block (if copy = True) by definition we are not an ObjectBlock\n        here!\n        \"\"\"\n        return [self.copy()] if copy else [self]\n\n    def _can_hold_element(self, element: Any) -> bool:\n        \"\"\" require the same dtype as ourselves \"\"\"\n        raise NotImplementedError(\"Implemented on subclasses\")\n\n    def should_store(self, value: ArrayLike) -> bool:\n        \"\"\"\n        Should we set self.values[indexer] = value inplace or do we need to cast?\n\n        Parameters\n        ----------\n        value : np.ndarray or ExtensionArray\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return is_dtype_equal(value.dtype, self.dtype)\n\n    def to_native_types(self, na_rep=\"nan\", quoting=None, **kwargs):\n        \"\"\" convert to our native types format \"\"\"\n        values = self.values\n\n        mask = isna(values)\n        itemsize = writers.word_len(na_rep)\n\n        if not self.is_object and not quoting and itemsize:\n            values = values.astype(str)\n            if values.dtype.itemsize / np.dtype(\"U1\").itemsize < itemsize:\n                # enlarge for the na_rep\n                values = values.astype(f\"<U{itemsize}\")\n        else:\n            values = np.array(values, dtype=\"object\")\n\n        values[mask] = na_rep\n        return self.make_block(values)\n\n    # block actions #\n    def copy(self, deep: bool = True):\n        \"\"\" copy constructor \"\"\"\n        values = self.values\n        if deep:\n            values = values.copy()\n        return self.make_block_same_class(values, ndim=self.ndim)\n\n    def replace(\n        self,\n        to_replace,\n        value,\n        inplace: bool = False,\n        regex: bool = False,\n    ) -> List[Block]:\n        \"\"\"\n        replace the to_replace value with value, possible to create new\n        blocks here this is just a call to putmask. regex is not used here.\n        It is used in ObjectBlocks.  It is here for API compatibility.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        original_to_replace = to_replace\n\n        if not self._can_hold_element(to_replace):\n            # We cannot hold `to_replace`, so we know immediately that\n            #  replacing it is a no-op.\n            # Note: If to_replace were a list, NDFrame.replace would call\n            #  replace_list instead of replace.\n            return [self] if inplace else [self.copy()]\n\n        values = self.values\n\n        mask = missing.mask_missing(values, to_replace)\n        if not mask.any():\n            # Note: we get here with test_replace_extension_other incorrectly\n            #  bc _can_hold_element is incorrect.\n            return [self] if inplace else [self.copy()]\n\n        if not self._can_hold_element(value):\n            blk = self.astype(object)\n            return blk.replace(\n                to_replace=original_to_replace,\n                value=value,\n                inplace=True,\n                regex=regex,\n            )\n\n        blk = self if inplace else self.copy()\n        putmask_inplace(blk.values, mask, value)\n        blocks = blk.convert(numeric=False, copy=not inplace)\n        return blocks\n\n    def _replace_regex(\n        self,\n        to_replace,\n        value,\n        inplace: bool = False,\n        convert: bool = True,\n        mask=None,\n    ) -> List[Block]:\n        \"\"\"\n        Replace elements by the given value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        inplace : bool, default False\n            Perform inplace modification.\n        convert : bool, default True\n            If true, try to coerce any object types to better types.\n        mask : array-like of bool, optional\n            True indicate corresponding element is ignored.\n\n        Returns\n        -------\n        List[Block]\n        \"\"\"\n        if not self._can_hold_element(to_replace):\n            # i.e. only ObjectBlock, but could in principle include a\n            #  String ExtensionBlock\n            return [self] if inplace else [self.copy()]\n\n        rx = re.compile(to_replace)\n\n        new_values = self.values if inplace else self.values.copy()\n        replace_regex(new_values, rx, value, mask)\n\n        block = self.make_block(new_values)\n        if convert:\n            nbs = block.convert(numeric=False)\n        else:\n            nbs = [block]\n        return nbs\n\n    def _replace_list(\n        self,\n        src_list: List[Any],\n        dest_list: List[Any],\n        inplace: bool = False,\n        regex: bool = False,\n    ) -> List[Block]:\n        \"\"\"\n        See BlockManager._replace_list docstring.\n        \"\"\"\n        # TODO: dont special-case Categorical\n        if self.is_categorical and len(algos.unique(dest_list)) == 1:\n            # We likely got here by tiling value inside NDFrame.replace,\n            #  so un-tile here\n            return self.replace(src_list, dest_list[0], inplace, regex)\n\n        # Exclude anything that we know we won't contain\n        pairs = [\n            (x, y) for x, y in zip(src_list, dest_list) if self._can_hold_element(x)\n        ]\n        if not len(pairs):\n            # shortcut, nothing to replace\n            return [self] if inplace else [self.copy()]\n\n        src_len = len(pairs) - 1\n\n        if self.is_object:\n            # Calculate the mask once, prior to the call of comp\n            # in order to avoid repeating the same computations\n            mask = ~isna(self.values)\n            masks = [\n                compare_or_regex_search(self.values, s[0], regex=regex, mask=mask)\n                for s in pairs\n            ]\n        else:\n            # GH#38086 faster if we know we dont need to check for regex\n            masks = [missing.mask_missing(self.values, s[0]) for s in pairs]\n\n        masks = [_extract_bool_array(x) for x in masks]\n\n        rb = [self if inplace else self.copy()]\n        for i, (src, dest) in enumerate(pairs):\n            new_rb: List[\"Block\"] = []\n            for blk in rb:\n                m = masks[i]\n                convert = i == src_len  # only convert once at the end\n                result = blk._replace_coerce(\n                    to_replace=src,\n                    value=dest,\n                    mask=m,\n                    inplace=inplace,\n                    regex=regex,\n                )\n                if convert and blk.is_object:\n                    result = extend_blocks(\n                        [b.convert(numeric=False, copy=True) for b in result]\n                    )\n                new_rb.extend(result)\n            rb = new_rb\n        return rb\n\n    def setitem(self, indexer, value):\n        \"\"\"\n        Attempt self.values[indexer] = value, possibly creating a new array.\n\n        Parameters\n        ----------\n        indexer : tuple, list-like, array-like, slice\n            The subset of self.values to set\n        value : object\n            The value being set\n\n        Returns\n        -------\n        Block\n\n        Notes\n        -----\n        `indexer` is a direct slice/positional indexer. `value` must\n        be a compatible shape.\n        \"\"\"\n        transpose = self.ndim == 2\n\n        if isinstance(indexer, np.ndarray) and indexer.ndim > self.ndim:\n            raise ValueError(f\"Cannot set values with ndim > {self.ndim}\")\n\n        # coerce None values, if appropriate\n        if value is None:\n            if self.is_numeric:\n                value = np.nan\n\n        # coerce if block dtype can store value\n        values = self.values\n        if not self._can_hold_element(value):\n            # current dtype cannot store value, coerce to common dtype\n            return self.coerce_to_target_dtype(value).setitem(indexer, value)\n\n        if self.dtype.kind in [\"m\", \"M\"]:\n            arr = self.array_values().T\n            arr[indexer] = value\n            return self\n\n        # value must be storable at this moment\n        if is_extension_array_dtype(getattr(value, \"dtype\", None)):\n            # We need to be careful not to allow through strings that\n            #  can be parsed to EADtypes\n            is_ea_value = True\n            arr_value = value\n        else:\n            is_ea_value = False\n            arr_value = np.array(value)\n\n        if transpose:\n            values = values.T\n\n        # length checking\n        check_setitem_lengths(indexer, value, values)\n        exact_match = is_exact_shape_match(values, arr_value)\n        if is_empty_indexer(indexer, arr_value):\n            # GH#8669 empty indexers\n            pass\n\n        elif is_scalar_indexer(indexer, self.ndim):\n            # setting a single element for each dim and with a rhs that could\n            #  be e.g. a list; see GH#6043\n            values[indexer] = value\n\n        elif exact_match and is_categorical_dtype(arr_value.dtype):\n            # GH25495 - If the current dtype is not categorical,\n            # we need to create a new categorical block\n            values[indexer] = value\n            if values.ndim == 2:\n                # TODO(EA2D): special case not needed with 2D EAs\n                if values.shape[-1] != 1:\n                    # shouldn't get here (at least until 2D EAs)\n                    raise NotImplementedError\n                values = values[:, 0]\n            return self.make_block(Categorical(values, dtype=arr_value.dtype))\n\n        elif exact_match and is_ea_value:\n            # GH#32395 if we're going to replace the values entirely, just\n            #  substitute in the new array\n            return self.make_block(arr_value)\n\n        # if we are an exact match (ex-broadcasting),\n        # then use the resultant dtype\n        elif exact_match:\n            # We are setting _all_ of the array's values, so can cast to new dtype\n            values[indexer] = value\n\n            values = values.astype(arr_value.dtype, copy=False)\n\n        elif is_ea_value:\n            # GH#38952\n            if values.ndim == 1:\n                values[indexer] = value\n            else:\n                # TODO(EA2D): special case not needed with 2D EA\n                values[indexer] = value.to_numpy(values.dtype).reshape(-1, 1)\n\n        elif self.is_object and not is_ea_value and arr_value.dtype.kind in [\"m\", \"M\"]:\n            # https://github.com/numpy/numpy/issues/12550\n            #  numpy will incorrect cast to int if we're not careful\n            if is_list_like(value):\n                value = list(value)\n            else:\n                value = [value] * len(values[indexer])\n\n            values[indexer] = value\n\n        else:\n\n            values[indexer] = value\n\n        if transpose:\n            values = values.T\n        block = self.make_block(values)\n        return block\n\n    def putmask(self, mask, new) -> List[Block]:\n        \"\"\"\n        putmask the data to the block; it is possible that we may create a\n        new dtype of block\n\n        Return the resulting block(s).\n\n        Parameters\n        ----------\n        mask : np.ndarray[bool], SparseArray[bool], or BooleanArray\n        new : a ndarray/object\n\n        Returns\n        -------\n        List[Block]\n        \"\"\"\n        transpose = self.ndim == 2\n        mask = _extract_bool_array(mask)\n        assert not isinstance(new, (ABCIndex, ABCSeries, ABCDataFrame))\n\n        new_values = self.values  # delay copy if possible.\n        # if we are passed a scalar None, convert it here\n        if not is_list_like(new) and isna(new) and not self.is_object:\n            # FIXME: make sure we have compatible NA\n            new = self.fill_value\n\n        if self._can_hold_element(new):\n            if transpose:\n                new_values = new_values.T\n\n            putmask_without_repeat(new_values, mask, new)\n            return [self]\n\n        elif not mask.any():\n            return [self]\n\n        dtype, _ = infer_dtype_from(new)\n        if dtype.kind in [\"m\", \"M\"]:\n            # using putmask with object dtype will incorrect cast to object\n            # Having excluded self._can_hold_element, we know we cannot operate\n            #  in-place, so we are safe using `where`\n            return self.where(new, ~mask)\n\n        else:\n            # may need to upcast\n            if transpose:\n                mask = mask.T\n                if isinstance(new, np.ndarray):\n                    new = new.T\n\n            # operate column-by-column\n            def f(mask, val, idx):\n\n                if idx is None:\n                    # ndim==1 case.\n                    n = new\n                else:\n\n                    if isinstance(new, np.ndarray):\n                        n = np.squeeze(new[idx % new.shape[0]])\n                    else:\n                        n = np.array(new)\n\n                    # type of the new block\n                    dtype = find_common_type([n.dtype, val.dtype])\n\n                    # we need to explicitly astype here to make a copy\n                    n = n.astype(dtype)\n\n                nv = putmask_smart(val, mask, n)\n                return nv\n\n            new_blocks = self.split_and_operate(mask, f, True)\n            return new_blocks\n\n    def coerce_to_target_dtype(self, other) -> Block:\n        \"\"\"\n        coerce the current block to a dtype compat for other\n        we will return a block, possibly object, and not raise\n\n        we can also safely try to coerce to the same dtype\n        and will receive the same block\n        \"\"\"\n        # if we cannot then coerce to object\n        dtype, _ = infer_dtype_from(other, pandas_dtype=True)\n\n        new_dtype = find_common_type([self.dtype, dtype])\n\n        return self.astype(new_dtype, copy=False)\n\n    def interpolate(\n        self,\n        method: str = \"pad\",\n        axis: int = 0,\n        index: Optional[Index] = None,\n        inplace: bool = False,\n        limit: Optional[int] = None,\n        limit_direction: str = \"forward\",\n        limit_area: Optional[str] = None,\n        fill_value: Optional[Any] = None,\n        coerce: bool = False,\n        downcast: Optional[str] = None,\n        **kwargs,\n    ) -> List[Block]:\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        if not self._can_hold_na:\n            # If there are no NAs, then interpolate is a no-op\n            return [self] if inplace else [self.copy()]\n\n        # a fill na type method\n        try:\n            m = missing.clean_fill_method(method)\n        except ValueError:\n            m = None\n\n        if m is not None:\n            if fill_value is not None:\n                # similar to validate_fillna_kwargs\n                raise ValueError(\"Cannot pass both fill_value and method\")\n\n            return self._interpolate_with_fill(\n                method=m,\n                axis=axis,\n                inplace=inplace,\n                limit=limit,\n                limit_area=limit_area,\n                downcast=downcast,\n            )\n        # validate the interp method\n        m = missing.clean_interp_method(method, **kwargs)\n\n        assert index is not None  # for mypy\n\n        return self._interpolate(\n            method=m,\n            index=index,\n            axis=axis,\n            limit=limit,\n            limit_direction=limit_direction,\n            limit_area=limit_area,\n            fill_value=fill_value,\n            inplace=inplace,\n            downcast=downcast,\n            **kwargs,\n        )\n\n    def _interpolate_with_fill(\n        self,\n        method: str = \"pad\",\n        axis: int = 0,\n        inplace: bool = False,\n        limit: Optional[int] = None,\n        limit_area: Optional[str] = None,\n        downcast: Optional[str] = None,\n    ) -> List[Block]:\n        \"\"\" fillna but using the interpolate machinery \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        assert self._can_hold_na  # checked by caller\n\n        values = self.values if inplace else self.values.copy()\n\n        values = missing.interpolate_2d(\n            values,\n            method=method,\n            axis=axis,\n            limit=limit,\n            limit_area=limit_area,\n        )\n\n        blocks = [self.make_block_same_class(values, ndim=self.ndim)]\n        return self._maybe_downcast(blocks, downcast)\n\n    def _interpolate(\n        self,\n        method: str,\n        index: Index,\n        fill_value: Optional[Any] = None,\n        axis: int = 0,\n        limit: Optional[int] = None,\n        limit_direction: str = \"forward\",\n        limit_area: Optional[str] = None,\n        inplace: bool = False,\n        downcast: Optional[str] = None,\n        **kwargs,\n    ) -> List[Block]:\n        \"\"\" interpolate using scipy wrappers \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        data = self.values if inplace else self.values.copy()\n\n        # only deal with floats\n        if not self.is_float:\n            if self.dtype.kind not in [\"i\", \"u\"]:\n                return [self]\n            data = data.astype(np.float64)\n\n        if fill_value is None:\n            fill_value = self.fill_value\n\n        if method in (\"krogh\", \"piecewise_polynomial\", \"pchip\"):\n            if not index.is_monotonic:\n                raise ValueError(\n                    f\"{method} interpolation requires that the index be monotonic.\"\n                )\n        # process 1-d slices in the axis direction\n\n        def func(yvalues: np.ndarray) -> np.ndarray:\n\n            # process a 1-d slice, returning it\n            # should the axis argument be handled below in apply_along_axis?\n            # i.e. not an arg to missing.interpolate_1d\n            return missing.interpolate_1d(\n                xvalues=index,\n                yvalues=yvalues,\n                method=method,\n                limit=limit,\n                limit_direction=limit_direction,\n                limit_area=limit_area,\n                fill_value=fill_value,\n                bounds_error=False,\n                **kwargs,\n            )\n\n        # interp each column independently\n        interp_values = np.apply_along_axis(func, axis, data)\n\n        blocks = [self.make_block_same_class(interp_values)]\n        return self._maybe_downcast(blocks, downcast)\n\n    def take_nd(\n        self, indexer, axis: int, new_mgr_locs=None, fill_value=lib.no_default\n    ) -> Block:\n        \"\"\"\n        Take values according to indexer and return them as a block.bb\n\n        \"\"\"\n        # algos.take_nd dispatches for DatetimeTZBlock, CategoricalBlock\n        # so need to preserve types\n        # sparse is treated like an ndarray, but needs .get_values() shaping\n\n        values = self.values\n\n        if fill_value is lib.no_default:\n            fill_value = self.fill_value\n            allow_fill = False\n        else:\n            allow_fill = True\n\n        new_values = algos.take_nd(\n            values, indexer, axis=axis, allow_fill=allow_fill, fill_value=fill_value\n        )\n\n        # Called from three places in managers, all of which satisfy\n        #  this assertion\n        assert not (axis == 0 and new_mgr_locs is None)\n        if new_mgr_locs is None:\n            new_mgr_locs = self.mgr_locs\n\n        if not is_dtype_equal(new_values.dtype, self.dtype):\n            return self.make_block(new_values, new_mgr_locs)\n        else:\n            return self.make_block_same_class(new_values, new_mgr_locs)\n\n    def diff(self, n: int, axis: int = 1) -> List[Block]:\n        \"\"\" return block for the diff of the values \"\"\"\n        new_values = algos.diff(self.values, n, axis=axis, stacklevel=7)\n        return [self.make_block(values=new_values)]\n\n    def shift(self, periods: int, axis: int = 0, fill_value: Any = None) -> List[Block]:\n        \"\"\" shift the block by periods, possibly upcast \"\"\"\n        # convert integer to float if necessary. need to do a lot more than\n        # that, handle boolean etc also\n        new_values, fill_value = maybe_upcast(self.values, fill_value)\n\n        new_values = shift(new_values, periods, axis, fill_value)\n\n        return [self.make_block(new_values)]\n\n    def where(self, other, cond, errors=\"raise\", axis: int = 0) -> List[Block]:\n        \"\"\"\n        evaluate the block; return result block(s) from the result\n\n        Parameters\n        ----------\n        other : a ndarray/object\n        cond : np.ndarray[bool], SparseArray[bool], or BooleanArray\n        errors : str, {'raise', 'ignore'}, default 'raise'\n            - ``raise`` : allow exceptions to be raised\n            - ``ignore`` : suppress exceptions. On error return original object\n        axis : int, default 0\n\n        Returns\n        -------\n        List[Block]\n        \"\"\"\n        import pandas.core.computation.expressions as expressions\n\n        assert not isinstance(other, (ABCIndex, ABCSeries, ABCDataFrame))\n\n        assert errors in [\"raise\", \"ignore\"]\n        transpose = self.ndim == 2\n\n        values = self.values\n        orig_other = other\n        if transpose:\n            values = values.T\n\n        cond = _extract_bool_array(cond)\n\n        if is_valid_na_for_dtype(other, self.dtype) and not self.is_object:\n            other = self.fill_value\n\n        if cond.ravel(\"K\").all():\n            result = values\n        else:\n            # see if we can operate on the entire block, or need item-by-item\n            # or if we are a single block (ndim == 1)\n            if not self._can_hold_element(other):\n                # we cannot coerce, return a compat dtype\n                # we are explicitly ignoring errors\n                block = self.coerce_to_target_dtype(other)\n                blocks = block.where(orig_other, cond, errors=errors, axis=axis)\n                return self._maybe_downcast(blocks, \"infer\")\n\n            dtype, _ = infer_dtype_from(other, pandas_dtype=True)\n            if dtype.kind in [\"m\", \"M\"] and dtype.kind != values.dtype.kind:\n                # expressions.where would cast np.timedelta64 to int\n                if not is_list_like(other):\n                    other = [other] * (~cond).sum()\n                else:\n                    other = list(other)\n                result = values.copy()\n                np.putmask(result, ~cond, other)\n\n            else:\n                # convert datetime to datetime64, timedelta to timedelta64\n                other = convert_scalar_for_putitemlike(other, values.dtype)\n\n                # By the time we get here, we should have all Series/Index\n                #  args extracted to ndarray\n                result = expressions.where(cond, values, other)\n\n        if self._can_hold_na or self.ndim == 1:\n\n            if transpose:\n                result = result.T\n\n            return [self.make_block(result)]\n\n        # might need to separate out blocks\n        axis = cond.ndim - 1\n        cond = cond.swapaxes(axis, 0)\n        mask = np.array([cond[i].all() for i in range(cond.shape[0])], dtype=bool)\n\n        result_blocks: List[Block] = []\n        for m in [mask, ~mask]:\n            if m.any():\n                result = cast(np.ndarray, result)  # EABlock overrides where\n                taken = result.take(m.nonzero()[0], axis=axis)\n                r = maybe_downcast_numeric(taken, self.dtype)\n                nb = self.make_block(r.T, placement=self.mgr_locs[m])\n                result_blocks.append(nb)\n\n        return result_blocks\n\n    def _unstack(self, unstacker, fill_value, new_placement):\n        \"\"\"\n        Return a list of unstacked blocks of self\n\n        Parameters\n        ----------\n        unstacker : reshape._Unstacker\n        fill_value : int\n            Only used in ExtensionBlock._unstack\n\n        Returns\n        -------\n        blocks : list of Block\n            New blocks of unstacked values.\n        mask : array_like of bool\n            The mask of columns of `blocks` we should keep.\n        \"\"\"\n        new_values, mask = unstacker.get_new_values(\n            self.values.T, fill_value=fill_value\n        )\n\n        mask = mask.any(0)\n        # TODO: in all tests we have mask.all(); can we rely on that?\n\n        new_values = new_values.T[mask]\n        new_placement = new_placement[mask]\n\n        blocks = [make_block(new_values, placement=new_placement)]\n        return blocks, mask\n\n    def quantile(\n        self, qs: Float64Index, interpolation=\"linear\", axis: int = 0\n    ) -> Block:\n        \"\"\"\n        compute the quantiles of the\n\n        Parameters\n        ----------\n        qs : Float64Index\n            List of the quantiles to be computed.\n        interpolation : str, default 'linear'\n            Type of interpolation.\n        axis : int, default 0\n            Axis to compute.\n\n        Returns\n        -------\n        Block\n        \"\"\"\n        # We should always have ndim == 2 because Series dispatches to DataFrame\n        assert self.ndim == 2\n        assert axis == 1  # only ever called this way\n        assert is_list_like(qs)  # caller is responsible for this\n\n        values = self.get_values()\n\n        is_empty = values.shape[axis] == 0\n\n        if is_empty:\n            # create the array of na_values\n            # 2d len(values) * len(qs)\n            result = np.repeat(\n                np.array([self.fill_value] * len(qs)), len(values)\n            ).reshape(len(values), len(qs))\n        else:\n            # asarray needed for Sparse, see GH#24600\n            mask = np.asarray(isna(values))\n            result = nanpercentile(\n                values,\n                np.array(qs) * 100,\n                axis=axis,\n                na_value=self.fill_value,\n                mask=mask,\n                ndim=values.ndim,\n                interpolation=interpolation,\n            )\n\n            result = np.array(result, copy=False)\n            result = result.T\n\n        return make_block(result, placement=self.mgr_locs, ndim=2)\n\n    def _replace_coerce(\n        self,\n        to_replace,\n        value,\n        mask: np.ndarray,\n        inplace: bool = True,\n        regex: bool = False,\n    ) -> List[Block]:\n        \"\"\"\n        Replace value corresponding to the given boolean array with another\n        value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        mask : np.ndarray[bool]\n            True indicate corresponding element is ignored.\n        inplace : bool, default True\n            Perform inplace modification.\n        regex : bool, default False\n            If true, perform regular expression substitution.\n\n        Returns\n        -------\n        List[Block]\n        \"\"\"\n        if mask.any():\n            if not regex:\n                nb = self.coerce_to_target_dtype(value)\n                if nb is self and not inplace:\n                    nb = nb.copy()\n                putmask_inplace(nb.values, mask, value)\n                return [nb]\n            else:\n                regex = should_use_regex(regex, to_replace)\n                if regex:\n                    return self._replace_regex(\n                        to_replace,\n                        value,\n                        inplace=inplace,\n                        convert=False,\n                        mask=mask,\n                    )\n                return self.replace(to_replace, value, inplace=inplace, regex=False)\n        return [self]\n\n\nclass ExtensionBlock(Block):\n    \"\"\"\n    Block for holding extension types.\n\n    Notes\n    -----\n    This holds all 3rd-party extension array types. It's also the immediate\n    parent class for our internal extension types' blocks, CategoricalBlock.\n\n    ExtensionArrays are limited to 1-D.\n    \"\"\"\n\n    _can_consolidate = False\n    _validate_ndim = False\n    is_extension = True\n\n    values: ExtensionArray\n\n    def __init__(self, values, placement, ndim: int):\n        \"\"\"\n        Initialize a non-consolidatable block.\n\n        'ndim' may be inferred from 'placement'.\n\n        This will call continue to call __init__ for the other base\n        classes mixed in with this Mixin.\n        \"\"\"\n\n        # Placement must be converted to BlockPlacement so that we can check\n        # its length\n        if not isinstance(placement, libinternals.BlockPlacement):\n            placement = libinternals.BlockPlacement(placement)\n\n        # Maybe infer ndim from placement\n        if ndim is None:\n            if len(placement) != 1:\n                ndim = 1\n            else:\n                ndim = 2\n        super().__init__(values, placement, ndim=ndim)\n\n        if self.ndim == 2 and len(self.mgr_locs) != 1:\n            # TODO(EA2D): check unnecessary with 2D EAs\n            raise AssertionError(\"block.size != values.size\")\n\n    @property\n    def shape(self) -> Shape:\n        # TODO(EA2D): override unnecessary with 2D EAs\n        if self.ndim == 1:\n            return (len(self.values),)\n        return len(self.mgr_locs), len(self.values)\n\n    def iget(self, col):\n\n        if self.ndim == 2 and isinstance(col, tuple):\n            # TODO(EA2D): unnecessary with 2D EAs\n            col, loc = col\n            if not com.is_null_slice(col) and col != 0:\n                raise IndexError(f\"{self} only contains one item\")\n            elif isinstance(col, slice):\n                if col != slice(None):\n                    raise NotImplementedError(col)\n                return self.values[[loc]]\n            return self.values[loc]\n        else:\n            if col != 0:\n                raise IndexError(f\"{self} only contains one item\")\n            return self.values\n\n    def set_inplace(self, locs, values):\n        # NB: This is a misnomer, is supposed to be inplace but is not,\n        #  see GH#33457\n        assert locs.tolist() == [0]\n        self.values = values\n\n    def putmask(self, mask, new) -> List[Block]:\n        \"\"\"\n        See Block.putmask.__doc__\n        \"\"\"\n        mask = _extract_bool_array(mask)\n\n        new_values = self.values\n\n        if isinstance(new, (np.ndarray, ExtensionArray)) and len(new) == len(mask):\n            new = new[mask]\n\n        mask = safe_reshape(mask, new_values.shape)\n\n        new_values[mask] = new\n        return [self.make_block(values=new_values)]\n\n    def _maybe_coerce_values(self, values):\n        \"\"\"\n        Unbox to an extension array.\n\n        This will unbox an ExtensionArray stored in an Index or Series.\n        ExtensionArrays pass through. No dtype coercion is done.\n\n        Parameters\n        ----------\n        values : Index, Series, ExtensionArray\n\n        Returns\n        -------\n        ExtensionArray\n        \"\"\"\n        return extract_array(values)\n\n    @property\n    def _holder(self):\n        # For extension blocks, the holder is values-dependent.\n        return type(self.values)\n\n    @property\n    def fill_value(self):\n        # Used in reindex_indexer\n        return self.values.dtype.na_value\n\n    @property\n    def _can_hold_na(self):\n        # The default ExtensionArray._can_hold_na is True\n        return self._holder._can_hold_na\n\n    @property\n    def is_view(self) -> bool:\n        \"\"\"Extension arrays are never treated as views.\"\"\"\n        return False\n\n    @property\n    def is_numeric(self):\n        return self.values.dtype._is_numeric\n\n    def setitem(self, indexer, value):\n        \"\"\"\n        Attempt self.values[indexer] = value, possibly creating a new array.\n\n        This differs from Block.setitem by not allowing setitem to change\n        the dtype of the Block.\n\n        Parameters\n        ----------\n        indexer : tuple, list-like, array-like, slice\n            The subset of self.values to set\n        value : object\n            The value being set\n\n        Returns\n        -------\n        Block\n\n        Notes\n        -----\n        `indexer` is a direct slice/positional indexer. `value` must\n        be a compatible shape.\n        \"\"\"\n        if not self._can_hold_element(value):\n            # This is only relevant for DatetimeTZBlock, which has a\n            #  non-trivial `_can_hold_element`.\n            # https://github.com/pandas-dev/pandas/issues/24020\n            # Need a dedicated setitem until GH#24020 (type promotion in setitem\n            #  for extension arrays) is designed and implemented.\n            return self.astype(object).setitem(indexer, value)\n\n        if isinstance(indexer, tuple):\n            # TODO(EA2D): not needed with 2D EAs\n            # we are always 1-D\n            indexer = indexer[0]\n\n        check_setitem_lengths(indexer, value, self.values)\n        self.values[indexer] = value\n        return self\n\n    def get_values(self, dtype: Optional[DtypeObj] = None) -> np.ndarray:\n        # ExtensionArrays must be iterable, so this works.\n        # TODO(EA2D): reshape not needed with 2D EAs\n        return np.asarray(self.values).reshape(self.shape)\n\n    def array_values(self) -> ExtensionArray:\n        return self.values\n\n    def to_native_types(self, na_rep=\"nan\", quoting=None, **kwargs):\n        \"\"\"override to use ExtensionArray astype for the conversion\"\"\"\n        values = self.values\n        mask = isna(values)\n\n        values = np.asarray(values.astype(object))\n        values[mask] = na_rep\n\n        # TODO(EA2D): reshape not needed with 2D EAs\n        # we are expected to return a 2-d ndarray\n        return self.make_block(values)\n\n    def take_nd(\n        self, indexer, axis: int = 0, new_mgr_locs=None, fill_value=lib.no_default\n    ) -> Block:\n        \"\"\"\n        Take values according to indexer and return them as a block.\n        \"\"\"\n        if fill_value is lib.no_default:\n            fill_value = None\n\n        # TODO(EA2D): special case not needed with 2D EAs\n        # axis doesn't matter; we are really a single-dim object\n        # but are passed the axis depending on the calling routing\n        # if its REALLY axis 0, then this will be a reindex and not a take\n        new_values = self.values.take(indexer, fill_value=fill_value, allow_fill=True)\n\n        # Called from three places in managers, all of which satisfy\n        #  this assertion\n        assert not (self.ndim == 1 and new_mgr_locs is None)\n        if new_mgr_locs is None:\n            new_mgr_locs = self.mgr_locs\n\n        return self.make_block_same_class(new_values, new_mgr_locs)\n\n    def _can_hold_element(self, element: Any) -> bool:\n        # TODO: We may need to think about pushing this onto the array.\n        # We're doing the same as CategoricalBlock here.\n        return True\n\n    def _slice(self, slicer):\n        \"\"\"\n        Return a slice of my values.\n\n        Parameters\n        ----------\n        slicer : slice, ndarray[int], or a tuple of these\n            Valid (non-reducing) indexer for self.values.\n\n        Returns\n        -------\n        np.ndarray or ExtensionArray\n        \"\"\"\n        # return same dims as we currently have\n        if not isinstance(slicer, tuple) and self.ndim == 2:\n            # reached via getitem_block via _slice_take_blocks_ax0\n            # TODO(EA2D): won't be necessary with 2D EAs\n            slicer = (slicer, slice(None))\n\n        if isinstance(slicer, tuple) and len(slicer) == 2:\n            first = slicer[0]\n            if not isinstance(first, slice):\n                raise AssertionError(\n                    \"invalid slicing for a 1-ndim ExtensionArray\", first\n                )\n            # GH#32959 only full-slicers along fake-dim0 are valid\n            # TODO(EA2D): won't be necessary with 2D EAs\n            new_locs = self.mgr_locs[first]\n            if len(new_locs):\n                # effectively slice(None)\n                slicer = slicer[1]\n            else:\n                raise AssertionError(\n                    \"invalid slicing for a 1-ndim ExtensionArray\", slicer\n                )\n\n        return self.values[slicer]\n\n    def fillna(\n        self, value, limit=None, inplace: bool = False, downcast=None\n    ) -> List[Block]:\n        values = self.values if inplace else self.values.copy()\n        values = values.fillna(value=value, limit=limit)\n        return [\n            self.make_block_same_class(\n                values=values, placement=self.mgr_locs, ndim=self.ndim\n            )\n        ]\n\n    def interpolate(\n        self, method=\"pad\", axis=0, inplace=False, limit=None, fill_value=None, **kwargs\n    ):\n\n        values = self.values if inplace else self.values.copy()\n        return self.make_block_same_class(\n            values=values.fillna(value=fill_value, method=method, limit=limit),\n            placement=self.mgr_locs,\n        )\n\n    def diff(self, n: int, axis: int = 1) -> List[Block]:\n        if axis == 0 and n != 0:\n            # n==0 case will be a no-op so let is fall through\n            # Since we only have one column, the result will be all-NA.\n            #  Create this result by shifting along axis=0 past the length of\n            #  our values.\n            return super().diff(len(self.values), axis=0)\n        if axis == 1:\n            # TODO(EA2D): unnecessary with 2D EAs\n            # we are by definition 1D.\n            axis = 0\n        return super().diff(n, axis)\n\n    def shift(self, periods: int, axis: int = 0, fill_value: Any = None) -> List[Block]:\n        \"\"\"\n        Shift the block by `periods`.\n\n        Dispatches to underlying ExtensionArray and re-boxes in an\n        ExtensionBlock.\n        \"\"\"\n        return [\n            self.make_block_same_class(\n                self.values.shift(periods=periods, fill_value=fill_value),\n                placement=self.mgr_locs,\n                ndim=self.ndim,\n            )\n        ]\n\n    def where(self, other, cond, errors=\"raise\", axis: int = 0) -> List[Block]:\n\n        cond = _extract_bool_array(cond)\n        assert not isinstance(other, (ABCIndex, ABCSeries, ABCDataFrame))\n\n        if isinstance(other, np.ndarray) and other.ndim == 2:\n            # TODO(EA2D): unnecessary with 2D EAs\n            assert other.shape[1] == 1\n            other = other[:, 0]\n\n        if isinstance(cond, np.ndarray) and cond.ndim == 2:\n            # TODO(EA2D): unnecessary with 2D EAs\n            assert cond.shape[1] == 1\n            cond = cond[:, 0]\n\n        if lib.is_scalar(other) and isna(other):\n            # The default `other` for Series / Frame is np.nan\n            # we want to replace that with the correct NA value\n            # for the type\n            other = self.dtype.na_value\n\n        if is_sparse(self.values):\n            # TODO(SparseArray.__setitem__): remove this if condition\n            # We need to re-infer the type of the data after doing the\n            # where, for cases where the subtypes don't match\n            dtype = None\n        else:\n            dtype = self.dtype\n\n        result = self.values.copy()\n        icond = ~cond\n        if lib.is_scalar(other):\n            set_other = other\n        else:\n            set_other = other[icond]\n        try:\n            result[icond] = set_other\n        except (NotImplementedError, TypeError):\n            # NotImplementedError for class not implementing `__setitem__`\n            # TypeError for SparseArray, which implements just to raise\n            # a TypeError\n            result = self._holder._from_sequence(\n                np.where(cond, self.values, other), dtype=dtype\n            )\n\n        return [self.make_block_same_class(result, placement=self.mgr_locs)]\n\n    def _unstack(self, unstacker, fill_value, new_placement):\n        # ExtensionArray-safe unstack.\n        # We override ObjectBlock._unstack, which unstacks directly on the\n        # values of the array. For EA-backed blocks, this would require\n        # converting to a 2-D ndarray of objects.\n        # Instead, we unstack an ndarray of integer positions, followed by\n        # a `take` on the actual values.\n        n_rows = self.shape[-1]\n        dummy_arr = np.arange(n_rows)\n\n        new_values, mask = unstacker.get_new_values(dummy_arr, fill_value=-1)\n        mask = mask.any(0)\n        # TODO: in all tests we have mask.all(); can we rely on that?\n\n        blocks = [\n            self.make_block_same_class(\n                self.values.take(indices, allow_fill=True, fill_value=fill_value),\n                [place],\n            )\n            for indices, place in zip(new_values.T, new_placement)\n        ]\n        return blocks, mask\n\n\nclass HybridMixin:\n    \"\"\"\n    Mixin for Blocks backed (maybe indirectly) by ExtensionArrays.\n    \"\"\"\n\n    array_values: Callable\n\n    def _can_hold_element(self, element: Any) -> bool:\n        values = self.array_values()\n\n        try:\n            values._validate_setitem_value(element)\n            return True\n        except (ValueError, TypeError):\n            return False\n\n\nclass ObjectValuesExtensionBlock(HybridMixin, ExtensionBlock):\n    \"\"\"\n    Block providing backwards-compatibility for `.values`.\n\n    Used by PeriodArray and IntervalArray to ensure that\n    Series[T].values is an ndarray of objects.\n    \"\"\"\n\n    def external_values(self):\n        return self.values.astype(object)\n\n\nclass NumericBlock(Block):\n    __slots__ = ()\n    is_numeric = True\n\n    def _can_hold_element(self, element: Any) -> bool:\n        return can_hold_element(self.dtype, element)\n\n    @property\n    def _can_hold_na(self):\n        return self.dtype.kind not in [\"b\", \"i\", \"u\"]\n\n    @property\n    def is_bool(self):\n        return self.dtype.kind == \"b\"\n\n\nclass FloatBlock(NumericBlock):\n    __slots__ = ()\n    is_float = True\n\n    def to_native_types(\n        self, na_rep=\"\", float_format=None, decimal=\".\", quoting=None, **kwargs\n    ):\n        \"\"\" convert to our native types format \"\"\"\n        values = self.values\n\n        # see gh-13418: no special formatting is desired at the\n        # output (important for appropriate 'quoting' behaviour),\n        # so do not pass it through the FloatArrayFormatter\n        if float_format is None and decimal == \".\":\n            mask = isna(values)\n\n            if not quoting:\n                values = values.astype(str)\n            else:\n                values = np.array(values, dtype=\"object\")\n\n            values[mask] = na_rep\n            return self.make_block(values)\n\n        from pandas.io.formats.format import FloatArrayFormatter\n\n        formatter = FloatArrayFormatter(\n            values,\n            na_rep=na_rep,\n            float_format=float_format,\n            decimal=decimal,\n            quoting=quoting,\n            fixed_width=False,\n        )\n        res = formatter.get_result_as_array()\n        return self.make_block(res)\n\n\nclass DatetimeLikeBlockMixin(HybridMixin, Block):\n    \"\"\"Mixin class for DatetimeBlock, DatetimeTZBlock, and TimedeltaBlock.\"\"\"\n\n    @property\n    def _holder(self):\n        return DatetimeArray\n\n    @property\n    def fill_value(self):\n        return np.datetime64(\"NaT\", \"ns\")\n\n    def get_values(self, dtype: Optional[DtypeObj] = None) -> np.ndarray:\n        \"\"\"\n        return object dtype as boxed values, such as Timestamps/Timedelta\n        \"\"\"\n        if is_object_dtype(dtype):\n            # DTA/TDA constructor and astype can handle 2D\n            return self._holder(self.values).astype(object)\n        return self.values\n\n    def internal_values(self):\n        # Override to return DatetimeArray and TimedeltaArray\n        return self.array_values()\n\n    def array_values(self):\n        return self._holder._simple_new(self.values)\n\n    def iget(self, key):\n        # GH#31649 we need to wrap scalars in Timestamp/Timedelta\n        # TODO(EA2D): this can be removed if we ever have 2D EA\n        return self.array_values().reshape(self.shape)[key]\n\n    def diff(self, n: int, axis: int = 0) -> List[Block]:\n        \"\"\"\n        1st discrete difference.\n\n        Parameters\n        ----------\n        n : int\n            Number of periods to diff.\n        axis : int, default 0\n            Axis to diff upon.\n\n        Returns\n        -------\n        A list with a new TimeDeltaBlock.\n\n        Notes\n        -----\n        The arguments here are mimicking shift so they are called correctly\n        by apply.\n        \"\"\"\n        # TODO(EA2D): reshape not necessary with 2D EAs\n        values = self.array_values().reshape(self.shape)\n\n        new_values = values - values.shift(n, axis=axis)\n        return [\n            TimeDeltaBlock(new_values, placement=self.mgr_locs.indexer, ndim=self.ndim)\n        ]\n\n    def shift(self, periods: int, axis: int = 0, fill_value: Any = None) -> List[Block]:\n        # TODO(EA2D) this is unnecessary if these blocks are backed by 2D EAs\n        values = self.array_values()\n        new_values = values.shift(periods, fill_value=fill_value, axis=axis)\n        return [self.make_block_same_class(new_values)]\n\n    def to_native_types(self, na_rep=\"NaT\", **kwargs):\n        \"\"\" convert to our native types format \"\"\"\n        arr = self.array_values()\n\n        result = arr._format_native_types(na_rep=na_rep, **kwargs)\n        return self.make_block(result)\n\n    def putmask(self, mask, new) -> List[Block]:\n        mask = _extract_bool_array(mask)\n\n        if not self._can_hold_element(new):\n            return self.astype(object).putmask(mask, new)\n\n        # TODO(EA2D): reshape unnecessary with 2D EAs\n        arr = self.array_values().reshape(self.shape)\n        arr = cast(\"NDArrayBackedExtensionArray\", arr)\n        arr.T.putmask(mask, new)\n        return [self]\n\n    def where(self, other, cond, errors=\"raise\", axis: int = 0) -> List[Block]:\n        # TODO(EA2D): reshape unnecessary with 2D EAs\n        arr = self.array_values().reshape(self.shape)\n\n        cond = _extract_bool_array(cond)\n\n        try:\n            res_values = arr.T.where(cond, other).T\n        except (ValueError, TypeError):\n            return super().where(other, cond, errors=errors, axis=axis)\n\n        # TODO(EA2D): reshape not needed with 2D EAs\n        res_values = res_values.reshape(self.values.shape)\n        nb = self.make_block_same_class(res_values)\n        return [nb]\n\n\nclass DatetimeBlock(DatetimeLikeBlockMixin):\n    __slots__ = ()\n    is_datetime = True\n\n    @property\n    def _can_hold_na(self):\n        return True\n\n    def _maybe_coerce_values(self, values):\n        \"\"\"\n        Input validation for values passed to __init__. Ensure that\n        we have datetime64ns, coercing if necessary.\n\n        Parameters\n        ----------\n        values : array-like\n            Must be convertible to datetime64\n\n        Returns\n        -------\n        values : ndarray[datetime64ns]\n\n        Overridden by DatetimeTZBlock.\n        \"\"\"\n        if values.dtype != DT64NS_DTYPE:\n            values = conversion.ensure_datetime64ns(values)\n\n        if isinstance(values, DatetimeArray):\n            values = values._data\n\n        assert isinstance(values, np.ndarray), type(values)\n        return values\n\n    def set_inplace(self, locs, values):\n        \"\"\"\n        See Block.set.__doc__\n        \"\"\"\n        values = conversion.ensure_datetime64ns(values, copy=False)\n\n        self.values[locs] = values\n\n\nclass DatetimeTZBlock(ExtensionBlock, DatetimeBlock):\n    \"\"\" implement a datetime64 block with a tz attribute \"\"\"\n\n    values: DatetimeArray\n\n    __slots__ = ()\n    is_datetimetz = True\n    is_extension = True\n\n    internal_values = Block.internal_values\n    _can_hold_element = DatetimeBlock._can_hold_element\n    to_native_types = DatetimeBlock.to_native_types\n    diff = DatetimeBlock.diff\n    fill_value = np.datetime64(\"NaT\", \"ns\")\n    where = DatetimeBlock.where\n    putmask = DatetimeLikeBlockMixin.putmask\n\n    array_values = ExtensionBlock.array_values\n\n    @property\n    def _holder(self):\n        return DatetimeArray\n\n    def _maybe_coerce_values(self, values):\n        \"\"\"\n        Input validation for values passed to __init__. Ensure that\n        we have datetime64TZ, coercing if necessary.\n\n        Parameters\n        ----------\n        values : array-like\n            Must be convertible to datetime64\n\n        Returns\n        -------\n        values : DatetimeArray\n        \"\"\"\n        if not isinstance(values, self._holder):\n            values = self._holder(values)\n\n        if values.tz is None:\n            raise ValueError(\"cannot create a DatetimeTZBlock without a tz\")\n\n        return values\n\n    @property\n    def is_view(self) -> bool:\n        \"\"\" return a boolean if I am possibly a view \"\"\"\n        # check the ndarray values of the DatetimeIndex values\n        return self.values._data.base is not None\n\n    def get_values(self, dtype: Optional[DtypeObj] = None) -> np.ndarray:\n        \"\"\"\n        Returns an ndarray of values.\n\n        Parameters\n        ----------\n        dtype : np.dtype\n            Only `object`-like dtypes are respected here (not sure\n            why).\n\n        Returns\n        -------\n        values : ndarray\n            When ``dtype=object``, then and object-dtype ndarray of\n            boxed values is returned. Otherwise, an M8[ns] ndarray\n            is returned.\n\n            DatetimeArray is always 1-d. ``get_values`` will reshape\n            the return value to be the same dimensionality as the\n            block.\n        \"\"\"\n        values = self.values\n        if is_object_dtype(dtype):\n            values = values.astype(object)\n\n        # TODO(EA2D): reshape unnecessary with 2D EAs\n        # Ensure that our shape is correct for DataFrame.\n        # ExtensionArrays are always 1-D, even in a DataFrame when\n        # the analogous NumPy-backed column would be a 2-D ndarray.\n        return np.asarray(values).reshape(self.shape)\n\n    def external_values(self):\n        # NB: this is different from np.asarray(self.values), since that\n        #  return an object-dtype ndarray of Timestamps.\n        if self.is_datetimetz:\n            # avoid FutureWarning in .astype in casting from dt64t to dt64\n            return self.values._data\n        return np.asarray(self.values.astype(\"datetime64[ns]\", copy=False))\n\n    def fillna(\n        self, value, limit=None, inplace: bool = False, downcast=None\n    ) -> List[Block]:\n        # We support filling a DatetimeTZ with a `value` whose timezone\n        # is different by coercing to object.\n        if self._can_hold_element(value):\n            return super().fillna(value, limit, inplace, downcast)\n\n        # different timezones, or a non-tz\n        return self.astype(object).fillna(\n            value, limit=limit, inplace=inplace, downcast=downcast\n        )\n\n    def quantile(\n        self, qs: Float64Index, interpolation=\"linear\", axis: int = 0\n    ) -> Block:\n        assert axis == 1  # only ever called this way\n        naive = self.values.view(\"M8[ns]\")\n\n        # TODO(EA2D): kludge for 2D block with 1D values\n        naive = naive.reshape(self.shape)\n\n        blk = self.make_block(naive)\n        res_blk = blk.quantile(qs, interpolation=interpolation, axis=axis)\n\n        # TODO(EA2D): ravel is kludge for 2D block with 1D values, assumes column-like\n        aware = self._holder(res_blk.values.ravel(), dtype=self.dtype)\n        return self.make_block_same_class(aware, ndim=res_blk.ndim)\n\n    def _check_ndim(self, values, ndim):\n        \"\"\"\n        ndim inference and validation.\n\n        This is overridden by the DatetimeTZBlock to check the case of 2D\n        data (values.ndim == 2), which should only be allowed if ndim is\n        also 2.\n        The case of 1D array is still allowed with both ndim of 1 or 2, as\n        if the case for other EAs. Therefore, we are only checking\n        `values.ndim > ndim` instead of `values.ndim != ndim` as for\n        consolidated blocks.\n        \"\"\"\n        if ndim is None:\n            ndim = values.ndim\n\n        if values.ndim > ndim:\n            raise ValueError(\n                \"Wrong number of dimensions. \"\n                f\"values.ndim != ndim [{values.ndim} != {ndim}]\"\n            )\n        return ndim\n\n\nclass TimeDeltaBlock(DatetimeLikeBlockMixin):\n    __slots__ = ()\n    is_timedelta = True\n    _can_hold_na = True\n    is_numeric = False\n    fill_value = np.timedelta64(\"NaT\", \"ns\")\n\n    def _maybe_coerce_values(self, values):\n        if values.dtype != TD64NS_DTYPE:\n            # non-nano we will convert to nano\n            if values.dtype.kind != \"m\":\n                # caller is responsible for ensuring timedelta64 dtype\n                raise TypeError(values.dtype)  # pragma: no cover\n\n            values = TimedeltaArray._from_sequence(values)._data\n        if isinstance(values, TimedeltaArray):\n            values = values._data\n        assert isinstance(values, np.ndarray), type(values)\n        return values\n\n    @property\n    def _holder(self):\n        return TimedeltaArray\n\n    def fillna(\n        self, value, limit=None, inplace: bool = False, downcast=None\n    ) -> List[Block]:\n        # TODO(EA2D): if we operated on array_values, TDA.fillna would handle\n        #  raising here.\n        if is_integer(value):\n            # Deprecation GH#24694, GH#19233\n            raise TypeError(\n                \"Passing integers to fillna for timedelta64[ns] dtype is no \"\n                \"longer supported.  To obtain the old behavior, pass \"\n                \"`pd.Timedelta(seconds=n)` instead.\"\n            )\n        return super().fillna(value, limit=limit, inplace=inplace, downcast=downcast)\n\n\nclass ObjectBlock(Block):\n    __slots__ = ()\n    is_object = True\n    _can_hold_na = True\n\n    def _maybe_coerce_values(self, values):\n        if issubclass(values.dtype.type, str):\n            values = np.array(values, dtype=object)\n        return values\n\n    @property\n    def is_bool(self):\n        \"\"\"\n        we can be a bool if we have only bool values but are of type\n        object\n        \"\"\"\n        return lib.is_bool_array(self.values.ravel(\"K\"))\n\n    def reduce(self, func, ignore_failures: bool = False) -> List[Block]:\n        \"\"\"\n        For object-dtype, we operate column-wise.\n        \"\"\"\n        assert self.ndim == 2\n\n        values = self.values\n        if len(values) > 1:\n            # split_and_operate expects func with signature (mask, values, inplace)\n            def mask_func(mask, values, inplace):\n                if values.ndim == 1:\n                    values = values.reshape(1, -1)\n                return func(values)\n\n            return self.split_and_operate(\n                None, mask_func, False, ignore_failures=ignore_failures\n            )\n\n        try:\n            res = func(values)\n        except TypeError:\n            if not ignore_failures:\n                raise\n            return []\n\n        assert isinstance(res, np.ndarray)\n        assert res.ndim == 1\n        res = res.reshape(1, -1)\n        return [self.make_block_same_class(res)]\n\n    def convert(\n        self,\n        copy: bool = True,\n        datetime: bool = True,\n        numeric: bool = True,\n        timedelta: bool = True,\n    ) -> List[Block]:\n        \"\"\"\n        attempt to cast any object types to better types return a copy of\n        the block (if copy = True) by definition we ARE an ObjectBlock!!!!!\n        \"\"\"\n\n        # operate column-by-column\n        def f(mask, val, idx):\n            shape = val.shape\n            values = soft_convert_objects(\n                val.ravel(),\n                datetime=datetime,\n                numeric=numeric,\n                timedelta=timedelta,\n                copy=copy,\n            )\n            if isinstance(values, np.ndarray):\n                # TODO(EA2D): allow EA once reshape is supported\n                values = values.reshape(shape)\n\n            return values\n\n        if self.ndim == 2:\n            blocks = self.split_and_operate(None, f, False)\n        else:\n            values = f(None, self.values.ravel(), None)\n            blocks = [self.make_block(values)]\n\n        return blocks\n\n    def _maybe_downcast(self, blocks: List[Block], downcast=None) -> List[Block]:\n\n        if downcast is not None:\n            return blocks\n\n        # split and convert the blocks\n        return extend_blocks([b.convert(datetime=True, numeric=False) for b in blocks])\n\n    def _can_hold_element(self, element: Any) -> bool:\n        return True\n\n    def replace(\n        self,\n        to_replace,\n        value,\n        inplace: bool = False,\n        regex: bool = False,\n    ) -> List[Block]:\n        # Note: the checks we do in NDFrame.replace ensure we never get\n        #  here with listlike to_replace or value, as those cases\n        #  go through _replace_list\n\n        regex = should_use_regex(regex, to_replace)\n\n        if regex:\n            return self._replace_regex(to_replace, value, inplace=inplace)\n        else:\n            return super().replace(to_replace, value, inplace=inplace, regex=False)\n\n\nclass CategoricalBlock(ExtensionBlock):\n    __slots__ = ()\n\n    def replace(\n        self,\n        to_replace,\n        value,\n        inplace: bool = False,\n        regex: bool = False,\n    ) -> List[Block]:\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        result = self if inplace else self.copy()\n\n        result.values.replace(to_replace, value, inplace=True)\n        return [result]\n\n\n# -----------------------------------------------------------------\n# Constructor Helpers\n\n\ndef get_block_type(values, dtype: Optional[Dtype] = None):\n    \"\"\"\n    Find the appropriate Block subclass to use for the given values and dtype.\n\n    Parameters\n    ----------\n    values : ndarray-like\n    dtype : numpy or pandas dtype\n\n    Returns\n    -------\n    cls : class, subclass of Block\n    \"\"\"\n    # We use vtype and kind checks because they are much more performant\n    #  than is_foo_dtype\n    dtype = cast(np.dtype, pandas_dtype(dtype) if dtype else values.dtype)\n    vtype = dtype.type\n    kind = dtype.kind\n\n    cls: Type[Block]\n\n    if is_sparse(dtype):\n        # Need this first(ish) so that Sparse[datetime] is sparse\n        cls = ExtensionBlock\n    elif isinstance(dtype, CategoricalDtype):\n        cls = CategoricalBlock\n    elif vtype is Timestamp:\n        cls = DatetimeTZBlock\n    elif vtype is Interval or vtype is Period:\n        cls = ObjectValuesExtensionBlock\n    elif isinstance(dtype, ExtensionDtype):\n        # Note: need to be sure PandasArray is unwrapped before we get here\n        cls = ExtensionBlock\n\n    elif kind == \"M\":\n        cls = DatetimeBlock\n    elif kind == \"m\":\n        cls = TimeDeltaBlock\n    elif kind == \"f\":\n        cls = FloatBlock\n    elif kind in [\"c\", \"i\", \"u\", \"b\"]:\n        cls = NumericBlock\n    else:\n        cls = ObjectBlock\n    return cls\n\n\ndef make_block(\n    values, placement, klass=None, ndim=None, dtype: Optional[Dtype] = None\n) -> Block:\n    # Ensure that we don't allow PandasArray / PandasDtype in internals.\n    # For now, blocks should be backed by ndarrays when possible.\n    if isinstance(values, ABCPandasArray):\n        values = values.to_numpy()\n        if ndim and ndim > 1:\n            # TODO(EA2D): special case not needed with 2D EAs\n            values = np.atleast_2d(values)\n\n    if isinstance(dtype, PandasDtype):\n        dtype = dtype.numpy_dtype\n\n    if klass is None:\n        dtype = dtype or values.dtype\n        klass = get_block_type(values, dtype)\n\n    elif klass is DatetimeTZBlock and not is_datetime64tz_dtype(values.dtype):\n        # TODO: This is no longer hit internally; does it need to be retained\n        #  for e.g. pyarrow?\n        values = DatetimeArray._simple_new(values, dtype=dtype)\n\n    return klass(values, ndim=ndim, placement=placement)\n\n\n# -----------------------------------------------------------------\n\n\ndef extend_blocks(result, blocks=None) -> List[Block]:\n    \"\"\" return a new extended blocks, given the result \"\"\"\n    if blocks is None:\n        blocks = []\n    if isinstance(result, list):\n        for r in result:\n            if isinstance(r, list):\n                blocks.extend(r)\n            else:\n                blocks.append(r)\n    else:\n        assert isinstance(result, Block), type(result)\n        blocks.append(result)\n    return blocks\n\n\ndef _block_shape(values: ArrayLike, ndim: int = 1) -> ArrayLike:\n    \"\"\" guarantee the shape of the values to be at least 1 d \"\"\"\n    if values.ndim < ndim:\n        shape = values.shape\n        if not is_extension_array_dtype(values.dtype):\n            # TODO(EA2D): https://github.com/pandas-dev/pandas/issues/23023\n            # block.shape is incorrect for \"2D\" ExtensionArrays\n            # We can't, and don't need to, reshape.\n            # error: \"ExtensionArray\" has no attribute \"reshape\"\n            values = values.reshape(tuple((1,) + shape))  # type: ignore[attr-defined]\n    return values\n\n\ndef safe_reshape(arr: ArrayLike, new_shape: Shape) -> ArrayLike:\n    \"\"\"\n    Reshape `arr` to have shape `new_shape`, unless it is an ExtensionArray,\n    in which case it will be returned unchanged (see gh-13012).\n\n    Parameters\n    ----------\n    arr : np.ndarray or ExtensionArray\n    new_shape : Tuple[int]\n\n    Returns\n    -------\n    np.ndarray or ExtensionArray\n    \"\"\"\n    if not is_extension_array_dtype(arr.dtype):\n        # Note: this will include TimedeltaArray and tz-naive DatetimeArray\n        # TODO(EA2D): special case will be unnecessary with 2D EAs\n        arr = np.asarray(arr).reshape(new_shape)\n    return arr\n\n\ndef _extract_bool_array(mask: ArrayLike) -> np.ndarray:\n    \"\"\"\n    If we have a SparseArray or BooleanArray, convert it to ndarray[bool].\n    \"\"\"\n    if isinstance(mask, ExtensionArray):\n        # We could have BooleanArray, Sparse[bool], ...\n        #  Except for BooleanArray, this is equivalent to just\n        #  np.asarray(mask, dtype=bool)\n        mask = mask.to_numpy(dtype=bool, na_value=False)\n\n    assert isinstance(mask, np.ndarray), type(mask)\n    assert mask.dtype == bool, mask.dtype\n    return mask\n"
    },
    {
      "filename": "pandas/tests/indexing/test_indexing.py",
      "content": "\"\"\" test fancy indexing & misc \"\"\"\n\nfrom datetime import datetime\nimport re\nimport weakref\n\nimport numpy as np\nimport pytest\n\nfrom pandas.core.dtypes.common import is_float_dtype, is_integer_dtype\n\nimport pandas as pd\nfrom pandas import DataFrame, Index, NaT, Series, date_range, offsets, timedelta_range\nimport pandas._testing as tm\nfrom pandas.core.indexing import maybe_numeric_slice, non_reducing_slice\nfrom pandas.tests.indexing.common import _mklbl\nfrom pandas.tests.indexing.test_floats import gen_obj\n\n# ------------------------------------------------------------------------\n# Indexing test cases\n\n\nclass TestFancy:\n    \"\"\" pure get/set item & fancy indexing \"\"\"\n\n    def test_setitem_ndarray_1d(self):\n        # GH5508\n\n        # len of indexer vs length of the 1d ndarray\n        df = DataFrame(index=Index(np.arange(1, 11)))\n        df[\"foo\"] = np.zeros(10, dtype=np.float64)\n        df[\"bar\"] = np.zeros(10, dtype=complex)\n\n        # invalid\n        msg = \"Must have equal len keys and value when setting with an iterable\"\n        with pytest.raises(ValueError, match=msg):\n            df.loc[df.index[2:5], \"bar\"] = np.array([2.33j, 1.23 + 0.1j, 2.2, 1.0])\n\n        # valid\n        df.loc[df.index[2:6], \"bar\"] = np.array([2.33j, 1.23 + 0.1j, 2.2, 1.0])\n\n        result = df.loc[df.index[2:6], \"bar\"]\n        expected = Series(\n            [2.33j, 1.23 + 0.1j, 2.2, 1.0], index=[3, 4, 5, 6], name=\"bar\"\n        )\n        tm.assert_series_equal(result, expected)\n\n        # dtype getting changed?\n        df = DataFrame(index=Index(np.arange(1, 11)))\n        df[\"foo\"] = np.zeros(10, dtype=np.float64)\n        df[\"bar\"] = np.zeros(10, dtype=complex)\n\n        msg = \"Must have equal len keys and value when setting with an iterable\"\n        with pytest.raises(ValueError, match=msg):\n            df[2:5] = np.arange(1, 4) * 1j\n\n    def test_getitem_ndarray_3d(self, index, frame_or_series, indexer_sli):\n        # GH 25567\n        obj = gen_obj(frame_or_series, index)\n        idxr = indexer_sli(obj)\n        nd3 = np.random.randint(5, size=(2, 2, 2))\n\n        msgs = []\n        if frame_or_series is Series and indexer_sli in [tm.setitem, tm.iloc]:\n            msgs.append(r\"Wrong number of dimensions. values.ndim != ndim \\[3 != 1\\]\")\n        if frame_or_series is Series or indexer_sli is tm.iloc:\n            msgs.append(r\"Buffer has wrong number of dimensions \\(expected 1, got 3\\)\")\n        if indexer_sli is tm.loc or (\n            frame_or_series is Series and indexer_sli is tm.setitem\n        ):\n            msgs.append(\"Cannot index with multidimensional key\")\n        if frame_or_series is DataFrame and indexer_sli is tm.setitem:\n            msgs.append(\"Index data must be 1-dimensional\")\n        if isinstance(index, pd.IntervalIndex) and indexer_sli is tm.iloc:\n            msgs.append(\"Index data must be 1-dimensional\")\n        if len(index) == 0 or isinstance(index, pd.MultiIndex):\n            msgs.append(\"positional indexers are out-of-bounds\")\n        msg = \"|\".join(msgs)\n\n        potential_errors = (IndexError, ValueError, NotImplementedError)\n        with pytest.raises(potential_errors, match=msg):\n            with tm.assert_produces_warning(DeprecationWarning, check_stacklevel=False):\n                idxr[nd3]\n\n    def test_setitem_ndarray_3d(self, index, frame_or_series, indexer_sli):\n        # GH 25567\n        obj = gen_obj(frame_or_series, index)\n        idxr = indexer_sli(obj)\n        nd3 = np.random.randint(5, size=(2, 2, 2))\n\n        if indexer_sli.__name__ == \"iloc\":\n            err = ValueError\n            msg = f\"Cannot set values with ndim > {obj.ndim}\"\n        elif (\n            isinstance(index, pd.IntervalIndex)\n            and indexer_sli.__name__ == \"setitem\"\n            and obj.ndim == 1\n        ):\n            err = AttributeError\n            msg = (\n                \"'pandas._libs.interval.IntervalTree' object has no attribute 'get_loc'\"\n            )\n        else:\n            err = ValueError\n            msg = r\"Buffer has wrong number of dimensions \\(expected 1, got 3\\)|\"\n\n        with pytest.raises(err, match=msg):\n            idxr[nd3] = 0\n\n    def test_inf_upcast(self):\n        # GH 16957\n        # We should be able to use np.inf as a key\n        # np.inf should cause an index to convert to float\n\n        # Test with np.inf in rows\n        df = DataFrame(columns=[0])\n        df.loc[1] = 1\n        df.loc[2] = 2\n        df.loc[np.inf] = 3\n\n        # make sure we can look up the value\n        assert df.loc[np.inf, 0] == 3\n\n        result = df.index\n        expected = pd.Float64Index([1, 2, np.inf])\n        tm.assert_index_equal(result, expected)\n\n        # Test with np.inf in columns\n        df = DataFrame()\n        df.loc[0, 0] = 1\n        df.loc[1, 1] = 2\n        df.loc[0, np.inf] = 3\n\n        result = df.columns\n        expected = pd.Float64Index([0, 1, np.inf])\n        tm.assert_index_equal(result, expected)\n\n    def test_setitem_dtype_upcast(self):\n\n        # GH3216\n        df = DataFrame([{\"a\": 1}, {\"a\": 3, \"b\": 2}])\n        df[\"c\"] = np.nan\n        assert df[\"c\"].dtype == np.float64\n\n        df.loc[0, \"c\"] = \"foo\"\n        expected = DataFrame(\n            [{\"a\": 1, \"b\": np.nan, \"c\": \"foo\"}, {\"a\": 3, \"b\": 2, \"c\": np.nan}]\n        )\n        tm.assert_frame_equal(df, expected)\n\n        # GH10280\n        df = DataFrame(\n            np.arange(6, dtype=\"int64\").reshape(2, 3),\n            index=list(\"ab\"),\n            columns=[\"foo\", \"bar\", \"baz\"],\n        )\n\n        for val in [3.14, \"wxyz\"]:\n            left = df.copy()\n            left.loc[\"a\", \"bar\"] = val\n            right = DataFrame(\n                [[0, val, 2], [3, 4, 5]],\n                index=list(\"ab\"),\n                columns=[\"foo\", \"bar\", \"baz\"],\n            )\n\n            tm.assert_frame_equal(left, right)\n            assert is_integer_dtype(left[\"foo\"])\n            assert is_integer_dtype(left[\"baz\"])\n\n        left = DataFrame(\n            np.arange(6, dtype=\"int64\").reshape(2, 3) / 10.0,\n            index=list(\"ab\"),\n            columns=[\"foo\", \"bar\", \"baz\"],\n        )\n        left.loc[\"a\", \"bar\"] = \"wxyz\"\n\n        right = DataFrame(\n            [[0, \"wxyz\", 0.2], [0.3, 0.4, 0.5]],\n            index=list(\"ab\"),\n            columns=[\"foo\", \"bar\", \"baz\"],\n        )\n\n        tm.assert_frame_equal(left, right)\n        assert is_float_dtype(left[\"foo\"])\n        assert is_float_dtype(left[\"baz\"])\n\n    def test_dups_fancy_indexing(self):\n\n        # GH 3455\n\n        df = tm.makeCustomDataframe(10, 3)\n        df.columns = [\"a\", \"a\", \"b\"]\n        result = df[[\"b\", \"a\"]].columns\n        expected = Index([\"b\", \"a\", \"a\"])\n        tm.assert_index_equal(result, expected)\n\n        # across dtypes\n        df = DataFrame([[1, 2, 1.0, 2.0, 3.0, \"foo\", \"bar\"]], columns=list(\"aaaaaaa\"))\n        df.head()\n        str(df)\n        result = DataFrame([[1, 2, 1.0, 2.0, 3.0, \"foo\", \"bar\"]])\n        result.columns = list(\"aaaaaaa\")\n\n        # TODO(wesm): unused?\n        df_v = df.iloc[:, 4]  # noqa\n        res_v = result.iloc[:, 4]  # noqa\n\n        tm.assert_frame_equal(df, result)\n\n        # GH 3561, dups not in selected order\n        df = DataFrame(\n            {\"test\": [5, 7, 9, 11], \"test1\": [4.0, 5, 6, 7], \"other\": list(\"abcd\")},\n            index=[\"A\", \"A\", \"B\", \"C\"],\n        )\n        rows = [\"C\", \"B\"]\n        expected = DataFrame(\n            {\"test\": [11, 9], \"test1\": [7.0, 6], \"other\": [\"d\", \"c\"]}, index=rows\n        )\n        result = df.loc[rows]\n        tm.assert_frame_equal(result, expected)\n\n        result = df.loc[Index(rows)]\n        tm.assert_frame_equal(result, expected)\n\n        rows = [\"C\", \"B\", \"E\"]\n        with pytest.raises(KeyError, match=\"with any missing labels\"):\n            df.loc[rows]\n\n        # see GH5553, make sure we use the right indexer\n        rows = [\"F\", \"G\", \"H\", \"C\", \"B\", \"E\"]\n        with pytest.raises(KeyError, match=\"with any missing labels\"):\n            df.loc[rows]\n\n        # List containing only missing label\n        dfnu = DataFrame(np.random.randn(5, 3), index=list(\"AABCD\"))\n        with pytest.raises(\n            KeyError,\n            match=re.escape(\n                \"\\\"None of [Index(['E'], dtype='object')] are in the [index]\\\"\"\n            ),\n        ):\n            dfnu.loc[[\"E\"]]\n\n        # ToDo: check_index_type can be True after GH 11497\n\n        # GH 4619; duplicate indexer with missing label\n        df = DataFrame({\"A\": [0, 1, 2]})\n        with pytest.raises(KeyError, match=\"with any missing labels\"):\n            df.loc[[0, 8, 0]]\n\n        df = DataFrame({\"A\": list(\"abc\")})\n        with pytest.raises(KeyError, match=\"with any missing labels\"):\n            df.loc[[0, 8, 0]]\n\n        # non unique with non unique selector\n        df = DataFrame({\"test\": [5, 7, 9, 11]}, index=[\"A\", \"A\", \"B\", \"C\"])\n        with pytest.raises(KeyError, match=\"with any missing labels\"):\n            df.loc[[\"A\", \"A\", \"E\"]]\n\n    def test_dups_fancy_indexing2(self):\n        # GH 5835\n        # dups on index and missing values\n        df = DataFrame(np.random.randn(5, 5), columns=[\"A\", \"B\", \"B\", \"B\", \"A\"])\n\n        with pytest.raises(KeyError, match=\"with any missing labels\"):\n            df.loc[:, [\"A\", \"B\", \"C\"]]\n\n        # GH 6504, multi-axis indexing\n        df = DataFrame(\n            np.random.randn(9, 2), index=[1, 1, 1, 2, 2, 2, 3, 3, 3], columns=[\"a\", \"b\"]\n        )\n\n        expected = df.iloc[0:6]\n        result = df.loc[[1, 2]]\n        tm.assert_frame_equal(result, expected)\n\n        expected = df\n        result = df.loc[:, [\"a\", \"b\"]]\n        tm.assert_frame_equal(result, expected)\n\n        expected = df.iloc[0:6, :]\n        result = df.loc[[1, 2], [\"a\", \"b\"]]\n        tm.assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize(\"case\", [tm.getitem, tm.loc])\n    def test_duplicate_int_indexing(self, case):\n        # GH 17347\n        s = Series(range(3), index=[1, 1, 3])\n        expected = s[1]\n        result = case(s)[[1]]\n        tm.assert_series_equal(result, expected)\n\n    def test_indexing_mixed_frame_bug(self):\n\n        # GH3492\n        df = DataFrame(\n            {\"a\": {1: \"aaa\", 2: \"bbb\", 3: \"ccc\"}, \"b\": {1: 111, 2: 222, 3: 333}}\n        )\n\n        # this works, new column is created correctly\n        df[\"test\"] = df[\"a\"].apply(lambda x: \"_\" if x == \"aaa\" else x)\n\n        # this does not work, ie column test is not changed\n        idx = df[\"test\"] == \"_\"\n        temp = df.loc[idx, \"a\"].apply(lambda x: \"-----\" if x == \"aaa\" else x)\n        df.loc[idx, \"test\"] = temp\n        assert df.iloc[0, 2] == \"-----\"\n\n    def test_multitype_list_index_access(self):\n        # GH 10610\n        df = DataFrame(np.random.random((10, 5)), columns=[\"a\"] + [20, 21, 22, 23])\n\n        with pytest.raises(KeyError, match=re.escape(\"'[-8, 26] not in index'\")):\n            df[[22, 26, -8]]\n        assert df[21].shape[0] == df.shape[0]\n\n    def test_set_index_nan(self):\n\n        # GH 3586\n        df = DataFrame(\n            {\n                \"PRuid\": {\n                    17: \"nonQC\",\n                    18: \"nonQC\",\n                    19: \"nonQC\",\n                    20: \"10\",\n                    21: \"11\",\n                    22: \"12\",\n                    23: \"13\",\n                    24: \"24\",\n                    25: \"35\",\n                    26: \"46\",\n                    27: \"47\",\n                    28: \"48\",\n                    29: \"59\",\n                    30: \"10\",\n                },\n                \"QC\": {\n                    17: 0.0,\n                    18: 0.0,\n                    19: 0.0,\n                    20: np.nan,\n                    21: np.nan,\n                    22: np.nan,\n                    23: np.nan,\n                    24: 1.0,\n                    25: np.nan,\n                    26: np.nan,\n                    27: np.nan,\n                    28: np.nan,\n                    29: np.nan,\n                    30: np.nan,\n                },\n                \"data\": {\n                    17: 7.9544899999999998,\n                    18: 8.0142609999999994,\n                    19: 7.8591520000000008,\n                    20: 0.86140349999999999,\n                    21: 0.87853110000000001,\n                    22: 0.8427041999999999,\n                    23: 0.78587700000000005,\n                    24: 0.73062459999999996,\n                    25: 0.81668560000000001,\n                    26: 0.81927080000000008,\n                    27: 0.80705009999999999,\n                    28: 0.81440240000000008,\n                    29: 0.80140849999999997,\n                    30: 0.81307740000000006,\n                },\n                \"year\": {\n                    17: 2006,\n                    18: 2007,\n                    19: 2008,\n                    20: 1985,\n                    21: 1985,\n                    22: 1985,\n                    23: 1985,\n                    24: 1985,\n                    25: 1985,\n                    26: 1985,\n                    27: 1985,\n                    28: 1985,\n                    29: 1985,\n                    30: 1986,\n                },\n            }\n        ).reset_index()\n\n        result = (\n            df.set_index([\"year\", \"PRuid\", \"QC\"])\n            .reset_index()\n            .reindex(columns=df.columns)\n        )\n        tm.assert_frame_equal(result, df)\n\n    def test_multi_assign(self):\n\n        # GH 3626, an assignment of a sub-df to a df\n        df = DataFrame(\n            {\n                \"FC\": [\"a\", \"b\", \"a\", \"b\", \"a\", \"b\"],\n                \"PF\": [0, 0, 0, 0, 1, 1],\n                \"col1\": list(range(6)),\n                \"col2\": list(range(6, 12)),\n            }\n        )\n        df.iloc[1, 0] = np.nan\n        df2 = df.copy()\n\n        mask = ~df2.FC.isna()\n        cols = [\"col1\", \"col2\"]\n\n        dft = df2 * 2\n        dft.iloc[3, 3] = np.nan\n\n        expected = DataFrame(\n            {\n                \"FC\": [\"a\", np.nan, \"a\", \"b\", \"a\", \"b\"],\n                \"PF\": [0, 0, 0, 0, 1, 1],\n                \"col1\": Series([0, 1, 4, 6, 8, 10]),\n                \"col2\": [12, 7, 16, np.nan, 20, 22],\n            }\n        )\n\n        # frame on rhs\n        df2.loc[mask, cols] = dft.loc[mask, cols]\n        tm.assert_frame_equal(df2, expected)\n\n        df2.loc[mask, cols] = dft.loc[mask, cols]\n        tm.assert_frame_equal(df2, expected)\n\n        # with an ndarray on rhs\n        # coerces to float64 because values has float64 dtype\n        # GH 14001\n        expected = DataFrame(\n            {\n                \"FC\": [\"a\", np.nan, \"a\", \"b\", \"a\", \"b\"],\n                \"PF\": [0, 0, 0, 0, 1, 1],\n                \"col1\": [0.0, 1.0, 4.0, 6.0, 8.0, 10.0],\n                \"col2\": [12, 7, 16, np.nan, 20, 22],\n            }\n        )\n        df2 = df.copy()\n        df2.loc[mask, cols] = dft.loc[mask, cols].values\n        tm.assert_frame_equal(df2, expected)\n        df2.loc[mask, cols] = dft.loc[mask, cols].values\n        tm.assert_frame_equal(df2, expected)\n\n        # broadcasting on the rhs is required\n        df = DataFrame(\n            {\n                \"A\": [1, 2, 0, 0, 0],\n                \"B\": [0, 0, 0, 10, 11],\n                \"C\": [0, 0, 0, 10, 11],\n                \"D\": [3, 4, 5, 6, 7],\n            }\n        )\n\n        expected = df.copy()\n        mask = expected[\"A\"] == 0\n        for col in [\"A\", \"B\"]:\n            expected.loc[mask, col] = df[\"D\"]\n\n        df.loc[df[\"A\"] == 0, [\"A\", \"B\"]] = df[\"D\"]\n        tm.assert_frame_equal(df, expected)\n\n    def test_setitem_list(self):\n\n        # GH 6043\n        # iloc with a list\n        df = DataFrame(index=[0, 1], columns=[0])\n        df.iloc[1, 0] = [1, 2, 3]\n        df.iloc[1, 0] = [1, 2]\n\n        result = DataFrame(index=[0, 1], columns=[0])\n        result.iloc[1, 0] = [1, 2]\n\n        tm.assert_frame_equal(result, df)\n\n        # iloc with an object\n        class TO:\n            def __init__(self, value):\n                self.value = value\n\n            def __str__(self) -> str:\n                return f\"[{self.value}]\"\n\n            __repr__ = __str__\n\n            def __eq__(self, other) -> bool:\n                return self.value == other.value\n\n            def view(self):\n                return self\n\n        df = DataFrame(index=[0, 1], columns=[0])\n        df.iloc[1, 0] = TO(1)\n        df.iloc[1, 0] = TO(2)\n\n        result = DataFrame(index=[0, 1], columns=[0])\n        result.iloc[1, 0] = TO(2)\n\n        tm.assert_frame_equal(result, df)\n\n        # remains object dtype even after setting it back\n        df = DataFrame(index=[0, 1], columns=[0])\n        df.iloc[1, 0] = TO(1)\n        df.iloc[1, 0] = np.nan\n        result = DataFrame(index=[0, 1], columns=[0])\n\n        tm.assert_frame_equal(result, df)\n\n    def test_string_slice(self):\n        # GH 14424\n        # string indexing against datetimelike with object\n        # dtype should properly raises KeyError\n        df = DataFrame([1], Index([pd.Timestamp(\"2011-01-01\")], dtype=object))\n        assert df.index._is_all_dates\n        with pytest.raises(KeyError, match=\"'2011'\"):\n            df[\"2011\"]\n\n        with pytest.raises(KeyError, match=\"'2011'\"):\n            df.loc[\"2011\", 0]\n\n        df = DataFrame()\n        assert not df.index._is_all_dates\n        with pytest.raises(KeyError, match=\"'2011'\"):\n            df[\"2011\"]\n\n        with pytest.raises(KeyError, match=\"'2011'\"):\n            df.loc[\"2011\", 0]\n\n    def test_astype_assignment(self):\n\n        # GH4312 (iloc)\n        df_orig = DataFrame(\n            [[\"1\", \"2\", \"3\", \".4\", 5, 6.0, \"foo\"]], columns=list(\"ABCDEFG\")\n        )\n\n        df = df_orig.copy()\n        df.iloc[:, 0:2] = df.iloc[:, 0:2].astype(np.int64)\n        expected = DataFrame(\n            [[1, 2, \"3\", \".4\", 5, 6.0, \"foo\"]], columns=list(\"ABCDEFG\")\n        )\n        tm.assert_frame_equal(df, expected)\n\n        df = df_orig.copy()\n        df.iloc[:, 0:2] = df.iloc[:, 0:2]._convert(datetime=True, numeric=True)\n        expected = DataFrame(\n            [[1, 2, \"3\", \".4\", 5, 6.0, \"foo\"]], columns=list(\"ABCDEFG\")\n        )\n        tm.assert_frame_equal(df, expected)\n\n        # GH5702 (loc)\n        df = df_orig.copy()\n        df.loc[:, \"A\"] = df.loc[:, \"A\"].astype(np.int64)\n        expected = DataFrame(\n            [[1, \"2\", \"3\", \".4\", 5, 6.0, \"foo\"]], columns=list(\"ABCDEFG\")\n        )\n        tm.assert_frame_equal(df, expected)\n\n        df = df_orig.copy()\n        df.loc[:, [\"B\", \"C\"]] = df.loc[:, [\"B\", \"C\"]].astype(np.int64)\n        expected = DataFrame(\n            [[\"1\", 2, 3, \".4\", 5, 6.0, \"foo\"]], columns=list(\"ABCDEFG\")\n        )\n        tm.assert_frame_equal(df, expected)\n\n        # full replacements / no nans\n        df = DataFrame({\"A\": [1.0, 2.0, 3.0, 4.0]})\n        df.iloc[:, 0] = df[\"A\"].astype(np.int64)\n        expected = DataFrame({\"A\": [1, 2, 3, 4]})\n        tm.assert_frame_equal(df, expected)\n\n        df = DataFrame({\"A\": [1.0, 2.0, 3.0, 4.0]})\n        df.loc[:, \"A\"] = df[\"A\"].astype(np.int64)\n        expected = DataFrame({\"A\": [1, 2, 3, 4]})\n        tm.assert_frame_equal(df, expected)\n\n    @pytest.mark.parametrize(\"indexer\", [tm.getitem, tm.loc])\n    def test_index_type_coercion(self, indexer):\n\n        # GH 11836\n        # if we have an index type and set it with something that looks\n        # to numpy like the same, but is actually, not\n        # (e.g. setting with a float or string '0')\n        # then we need to coerce to object\n\n        # integer indexes\n        for s in [Series(range(5)), Series(range(5), index=range(1, 6))]:\n\n            assert s.index.is_integer()\n\n            s2 = s.copy()\n            indexer(s2)[0.1] = 0\n            assert s2.index.is_floating()\n            assert indexer(s2)[0.1] == 0\n\n            s2 = s.copy()\n            indexer(s2)[0.0] = 0\n            exp = s.index\n            if 0 not in s:\n                exp = Index(s.index.tolist() + [0])\n            tm.assert_index_equal(s2.index, exp)\n\n            s2 = s.copy()\n            indexer(s2)[\"0\"] = 0\n            assert s2.index.is_object()\n\n        for s in [Series(range(5), index=np.arange(5.0))]:\n\n            assert s.index.is_floating()\n\n            s2 = s.copy()\n            indexer(s2)[0.1] = 0\n            assert s2.index.is_floating()\n            assert indexer(s2)[0.1] == 0\n\n            s2 = s.copy()\n            indexer(s2)[0.0] = 0\n            tm.assert_index_equal(s2.index, s.index)\n\n            s2 = s.copy()\n            indexer(s2)[\"0\"] = 0\n            assert s2.index.is_object()\n\n\nclass TestMisc:\n    def test_float_index_to_mixed(self):\n        df = DataFrame({0.0: np.random.rand(10), 1.0: np.random.rand(10)})\n        df[\"a\"] = 10\n        tm.assert_frame_equal(\n            DataFrame({0.0: df[0.0], 1.0: df[1.0], \"a\": [10] * 10}), df\n        )\n\n    def test_float_index_non_scalar_assignment(self):\n        df = DataFrame({\"a\": [1, 2, 3], \"b\": [3, 4, 5]}, index=[1.0, 2.0, 3.0])\n        df.loc[df.index[:2]] = 1\n        expected = DataFrame({\"a\": [1, 1, 3], \"b\": [1, 1, 5]}, index=df.index)\n        tm.assert_frame_equal(expected, df)\n\n        df = DataFrame({\"a\": [1, 2, 3], \"b\": [3, 4, 5]}, index=[1.0, 2.0, 3.0])\n        df2 = df.copy()\n        df.loc[df.index] = df.loc[df.index]\n        tm.assert_frame_equal(df, df2)\n\n    def test_float_index_at_iat(self):\n        s = Series([1, 2, 3], index=[0.1, 0.2, 0.3])\n        for el, item in s.items():\n            assert s.at[el] == item\n        for i in range(len(s)):\n            assert s.iat[i] == i + 1\n\n    def test_rhs_alignment(self):\n        # GH8258, tests that both rows & columns are aligned to what is\n        # assigned to. covers both uniform data-type & multi-type cases\n        def run_tests(df, rhs, right_loc, right_iloc):\n            # label, index, slice\n            lbl_one, idx_one, slice_one = list(\"bcd\"), [1, 2, 3], slice(1, 4)\n            lbl_two, idx_two, slice_two = [\"joe\", \"jolie\"], [1, 2], slice(1, 3)\n\n            left = df.copy()\n            left.loc[lbl_one, lbl_two] = rhs\n            tm.assert_frame_equal(left, right_loc)\n\n            left = df.copy()\n            left.iloc[idx_one, idx_two] = rhs\n            tm.assert_frame_equal(left, right_iloc)\n\n            left = df.copy()\n            left.iloc[slice_one, slice_two] = rhs\n            tm.assert_frame_equal(left, right_iloc)\n\n        xs = np.arange(20).reshape(5, 4)\n        cols = [\"jim\", \"joe\", \"jolie\", \"joline\"]\n        df = DataFrame(xs, columns=cols, index=list(\"abcde\"), dtype=\"int64\")\n\n        # right hand side; permute the indices and multiplpy by -2\n        rhs = -2 * df.iloc[3:0:-1, 2:0:-1]\n\n        # expected `right` result; just multiply by -2\n        right_iloc = df.copy()\n        right_iloc[\"joe\"] = [1, 14, 10, 6, 17]\n        right_iloc[\"jolie\"] = [2, 13, 9, 5, 18]\n        right_iloc.iloc[1:4, 1:3] *= -2\n        right_loc = df.copy()\n        right_loc.iloc[1:4, 1:3] *= -2\n\n        # run tests with uniform dtypes\n        run_tests(df, rhs, right_loc, right_iloc)\n\n        # make frames multi-type & re-run tests\n        for frame in [df, rhs, right_loc, right_iloc]:\n            frame[\"joe\"] = frame[\"joe\"].astype(\"float64\")\n            frame[\"jolie\"] = frame[\"jolie\"].map(\"@{}\".format)\n        right_iloc[\"joe\"] = [1.0, \"@-28\", \"@-20\", \"@-12\", 17.0]\n        right_iloc[\"jolie\"] = [\"@2\", -26.0, -18.0, -10.0, \"@18\"]\n        run_tests(df, rhs, right_loc, right_iloc)\n\n    def test_str_label_slicing_with_negative_step(self):\n        SLC = pd.IndexSlice\n\n        def assert_slices_equivalent(l_slc, i_slc):\n            tm.assert_series_equal(s.loc[l_slc], s.iloc[i_slc])\n\n            if not idx.is_integer:\n                # For integer indices, .loc and plain getitem are position-based.\n                tm.assert_series_equal(s[l_slc], s.iloc[i_slc])\n                tm.assert_series_equal(s.loc[l_slc], s.iloc[i_slc])\n\n        for idx in [_mklbl(\"A\", 20), np.arange(20) + 100, np.linspace(100, 150, 20)]:\n            idx = Index(idx)\n            s = Series(np.arange(20), index=idx)\n            assert_slices_equivalent(SLC[idx[9] :: -1], SLC[9::-1])\n            assert_slices_equivalent(SLC[: idx[9] : -1], SLC[:8:-1])\n            assert_slices_equivalent(SLC[idx[13] : idx[9] : -1], SLC[13:8:-1])\n            assert_slices_equivalent(SLC[idx[9] : idx[13] : -1], SLC[:0])\n\n    def test_slice_with_zero_step_raises(self):\n        s = Series(np.arange(20), index=_mklbl(\"A\", 20))\n        with pytest.raises(ValueError, match=\"slice step cannot be zero\"):\n            s[::0]\n        with pytest.raises(ValueError, match=\"slice step cannot be zero\"):\n            s.loc[::0]\n\n    def test_indexing_assignment_dict_already_exists(self):\n        index = Index([-5, 0, 5], name=\"z\")\n        df = DataFrame({\"x\": [1, 2, 6], \"y\": [2, 2, 8]}, index=index)\n        expected = df.copy()\n        rhs = {\"x\": 9, \"y\": 99}\n        df.loc[5] = rhs\n        expected.loc[5] = [9, 99]\n        tm.assert_frame_equal(df, expected)\n\n        # GH#38335 same thing, mixed dtypes\n        df = DataFrame({\"x\": [1, 2, 6], \"y\": [2.0, 2.0, 8.0]}, index=index)\n        df.loc[5] = rhs\n        expected = DataFrame({\"x\": [1, 2, 9], \"y\": [2.0, 2.0, 99.0]}, index=index)\n        tm.assert_frame_equal(df, expected)\n\n    def test_indexing_dtypes_on_empty(self):\n        # Check that .iloc returns correct dtypes GH9983\n        df = DataFrame({\"a\": [1, 2, 3], \"b\": [\"b\", \"b2\", \"b3\"]})\n        df2 = df.iloc[[], :]\n\n        assert df2.loc[:, \"a\"].dtype == np.int64\n        tm.assert_series_equal(df2.loc[:, \"a\"], df2.iloc[:, 0])\n\n    @pytest.mark.parametrize(\"size\", [5, 999999, 1000000])\n    def test_range_in_series_indexing(self, size):\n        # range can cause an indexing error\n        # GH 11652\n        s = Series(index=range(size), dtype=np.float64)\n        s.loc[range(1)] = 42\n        tm.assert_series_equal(s.loc[range(1)], Series(42.0, index=[0]))\n\n        s.loc[range(2)] = 43\n        tm.assert_series_equal(s.loc[range(2)], Series(43.0, index=[0, 1]))\n\n    @pytest.mark.parametrize(\n        \"slc\",\n        [\n            pd.IndexSlice[:, :],\n            pd.IndexSlice[:, 1],\n            pd.IndexSlice[1, :],\n            pd.IndexSlice[[1], [1]],\n            pd.IndexSlice[1, [1]],\n            pd.IndexSlice[[1], 1],\n            pd.IndexSlice[1],\n            pd.IndexSlice[1, 1],\n            slice(None, None, None),\n            [0, 1],\n            np.array([0, 1]),\n            Series([0, 1]),\n        ],\n    )\n    def test_non_reducing_slice(self, slc):\n        df = DataFrame([[0, 1], [2, 3]])\n\n        tslice_ = non_reducing_slice(slc)\n        assert isinstance(df.loc[tslice_], DataFrame)\n\n    def test_list_slice(self):\n        # like dataframe getitem\n        slices = [[\"A\"], Series([\"A\"]), np.array([\"A\"])]\n        df = DataFrame({\"A\": [1, 2], \"B\": [3, 4]}, index=[\"A\", \"B\"])\n        expected = pd.IndexSlice[:, [\"A\"]]\n        for subset in slices:\n            result = non_reducing_slice(subset)\n            tm.assert_frame_equal(df.loc[result], df.loc[expected])\n\n    def test_maybe_numeric_slice(self):\n        df = DataFrame({\"A\": [1, 2], \"B\": [\"c\", \"d\"], \"C\": [True, False]})\n        result = maybe_numeric_slice(df, slice_=None)\n        expected = pd.IndexSlice[:, [\"A\"]]\n        assert result == expected\n\n        result = maybe_numeric_slice(df, None, include_bool=True)\n        expected = pd.IndexSlice[:, [\"A\", \"C\"]]\n        assert all(result[1] == expected[1])\n        result = maybe_numeric_slice(df, [1])\n        expected = [1]\n        assert result == expected\n\n    def test_partial_boolean_frame_indexing(self):\n        # GH 17170\n        df = DataFrame(\n            np.arange(9.0).reshape(3, 3), index=list(\"abc\"), columns=list(\"ABC\")\n        )\n        index_df = DataFrame(1, index=list(\"ab\"), columns=list(\"AB\"))\n        result = df[index_df.notnull()]\n        expected = DataFrame(\n            np.array([[0.0, 1.0, np.nan], [3.0, 4.0, np.nan], [np.nan] * 3]),\n            index=list(\"abc\"),\n            columns=list(\"ABC\"),\n        )\n        tm.assert_frame_equal(result, expected)\n\n    def test_no_reference_cycle(self):\n        df = DataFrame({\"a\": [0, 1], \"b\": [2, 3]})\n        for name in (\"loc\", \"iloc\", \"at\", \"iat\"):\n            getattr(df, name)\n        wr = weakref.ref(df)\n        del df\n        assert wr() is None\n\n    def test_label_indexing_on_nan(self):\n        # GH 32431\n        df = Series([1, \"{1,2}\", 1, None])\n        vc = df.value_counts(dropna=False)\n        result1 = vc.loc[np.nan]\n        result2 = vc[np.nan]\n\n        expected = 1\n        assert result1 == expected\n        assert result2 == expected\n\n\nclass TestDataframeNoneCoercion:\n    EXPECTED_SINGLE_ROW_RESULTS = [\n        # For numeric series, we should coerce to NaN.\n        ([1, 2, 3], [np.nan, 2, 3]),\n        ([1.0, 2.0, 3.0], [np.nan, 2.0, 3.0]),\n        # For datetime series, we should coerce to NaT.\n        (\n            [datetime(2000, 1, 1), datetime(2000, 1, 2), datetime(2000, 1, 3)],\n            [NaT, datetime(2000, 1, 2), datetime(2000, 1, 3)],\n        ),\n        # For objects, we should preserve the None value.\n        ([\"foo\", \"bar\", \"baz\"], [None, \"bar\", \"baz\"]),\n    ]\n\n    @pytest.mark.parametrize(\"expected\", EXPECTED_SINGLE_ROW_RESULTS)\n    def test_coercion_with_loc(self, expected):\n        start_data, expected_result = expected\n\n        start_dataframe = DataFrame({\"foo\": start_data})\n        start_dataframe.loc[0, [\"foo\"]] = None\n\n        expected_dataframe = DataFrame({\"foo\": expected_result})\n        tm.assert_frame_equal(start_dataframe, expected_dataframe)\n\n    @pytest.mark.parametrize(\"expected\", EXPECTED_SINGLE_ROW_RESULTS)\n    def test_coercion_with_setitem_and_dataframe(self, expected):\n        start_data, expected_result = expected\n\n        start_dataframe = DataFrame({\"foo\": start_data})\n        start_dataframe[start_dataframe[\"foo\"] == start_dataframe[\"foo\"][0]] = None\n\n        expected_dataframe = DataFrame({\"foo\": expected_result})\n        tm.assert_frame_equal(start_dataframe, expected_dataframe)\n\n    @pytest.mark.parametrize(\"expected\", EXPECTED_SINGLE_ROW_RESULTS)\n    def test_none_coercion_loc_and_dataframe(self, expected):\n        start_data, expected_result = expected\n\n        start_dataframe = DataFrame({\"foo\": start_data})\n        start_dataframe.loc[start_dataframe[\"foo\"] == start_dataframe[\"foo\"][0]] = None\n\n        expected_dataframe = DataFrame({\"foo\": expected_result})\n        tm.assert_frame_equal(start_dataframe, expected_dataframe)\n\n    def test_none_coercion_mixed_dtypes(self):\n        start_dataframe = DataFrame(\n            {\n                \"a\": [1, 2, 3],\n                \"b\": [1.0, 2.0, 3.0],\n                \"c\": [datetime(2000, 1, 1), datetime(2000, 1, 2), datetime(2000, 1, 3)],\n                \"d\": [\"a\", \"b\", \"c\"],\n            }\n        )\n        start_dataframe.iloc[0] = None\n\n        exp = DataFrame(\n            {\n                \"a\": [np.nan, 2, 3],\n                \"b\": [np.nan, 2.0, 3.0],\n                \"c\": [NaT, datetime(2000, 1, 2), datetime(2000, 1, 3)],\n                \"d\": [None, \"b\", \"c\"],\n            }\n        )\n        tm.assert_frame_equal(start_dataframe, exp)\n\n\nclass TestDatetimelikeCoercion:\n    def test_setitem_dt64_string_scalar(self, tz_naive_fixture, indexer_sli):\n        # dispatching _can_hold_element to underling DatetimeArray\n        tz = tz_naive_fixture\n\n        dti = date_range(\"2016-01-01\", periods=3, tz=tz)\n        ser = Series(dti)\n\n        values = ser._values\n\n        newval = \"2018-01-01\"\n        values._validate_setitem_value(newval)\n\n        indexer_sli(ser)[0] = newval\n\n        if tz is None:\n            # TODO(EA2D): we can make this no-copy in tz-naive case too\n            assert ser.dtype == dti.dtype\n            assert ser._values._data is values._data\n        else:\n            assert ser._values is values\n\n    @pytest.mark.parametrize(\"box\", [list, np.array, pd.array])\n    @pytest.mark.parametrize(\n        \"key\", [[0, 1], slice(0, 2), np.array([True, True, False])]\n    )\n    def test_setitem_dt64_string_values(self, tz_naive_fixture, indexer_sli, key, box):\n        # dispatching _can_hold_element to underling DatetimeArray\n        tz = tz_naive_fixture\n\n        if isinstance(key, slice) and indexer_sli is tm.loc:\n            key = slice(0, 1)\n\n        dti = date_range(\"2016-01-01\", periods=3, tz=tz)\n        ser = Series(dti)\n\n        values = ser._values\n\n        newvals = box([\"2019-01-01\", \"2010-01-02\"])\n        values._validate_setitem_value(newvals)\n\n        indexer_sli(ser)[key] = newvals\n\n        if tz is None:\n            # TODO(EA2D): we can make this no-copy in tz-naive case too\n            assert ser.dtype == dti.dtype\n            assert ser._values._data is values._data\n        else:\n            assert ser._values is values\n\n    @pytest.mark.parametrize(\"scalar\", [\"3 Days\", offsets.Hour(4)])\n    def test_setitem_td64_scalar(self, indexer_sli, scalar):\n        # dispatching _can_hold_element to underling TimedeltaArray\n        tdi = timedelta_range(\"1 Day\", periods=3)\n        ser = Series(tdi)\n\n        values = ser._values\n        values._validate_setitem_value(scalar)\n\n        indexer_sli(ser)[0] = scalar\n        assert ser._values._data is values._data\n\n    @pytest.mark.parametrize(\"box\", [list, np.array, pd.array])\n    @pytest.mark.parametrize(\n        \"key\", [[0, 1], slice(0, 2), np.array([True, True, False])]\n    )\n    def test_setitem_td64_string_values(self, indexer_sli, key, box):\n        # dispatching _can_hold_element to underling TimedeltaArray\n        if isinstance(key, slice) and indexer_sli is tm.loc:\n            key = slice(0, 1)\n\n        tdi = timedelta_range(\"1 Day\", periods=3)\n        ser = Series(tdi)\n\n        values = ser._values\n\n        newvals = box([\"10 Days\", \"44 hours\"])\n        values._validate_setitem_value(newvals)\n\n        indexer_sli(ser)[key] = newvals\n        assert ser._values._data is values._data\n\n\ndef test_extension_array_cross_section():\n    # A cross-section of a homogeneous EA should be an EA\n    df = DataFrame(\n        {\n            \"A\": pd.array([1, 2], dtype=\"Int64\"),\n            \"B\": pd.array([3, 4], dtype=\"Int64\"),\n        },\n        index=[\"a\", \"b\"],\n    )\n    expected = Series(pd.array([1, 3], dtype=\"Int64\"), index=[\"A\", \"B\"], name=\"a\")\n    result = df.loc[\"a\"]\n    tm.assert_series_equal(result, expected)\n\n    result = df.iloc[0]\n    tm.assert_series_equal(result, expected)\n\n\ndef test_extension_array_cross_section_converts():\n    # all numeric columns -> numeric series\n    df = DataFrame(\n        {\"A\": pd.array([1, 2], dtype=\"Int64\"), \"B\": np.array([1, 2])}, index=[\"a\", \"b\"]\n    )\n    result = df.loc[\"a\"]\n    expected = Series([1, 1], dtype=\"Int64\", index=[\"A\", \"B\"], name=\"a\")\n    tm.assert_series_equal(result, expected)\n\n    result = df.iloc[0]\n    tm.assert_series_equal(result, expected)\n\n    # mixed columns -> object series\n    df = DataFrame(\n        {\"A\": pd.array([1, 2], dtype=\"Int64\"), \"B\": np.array([\"a\", \"b\"])},\n        index=[\"a\", \"b\"],\n    )\n    result = df.loc[\"a\"]\n    expected = Series([1, \"a\"], dtype=object, index=[\"A\", \"B\"], name=\"a\")\n    tm.assert_series_equal(result, expected)\n\n    result = df.iloc[0]\n    tm.assert_series_equal(result, expected)\n\n\ndef test_setitem_with_bool_mask_and_values_matching_n_trues_in_length():\n    # GH 30567\n    ser = Series([None] * 10)\n    mask = [False] * 3 + [True] * 5 + [False] * 2\n    ser[mask] = range(5)\n    result = ser\n    expected = Series([None] * 3 + list(range(5)) + [None] * 2).astype(\"object\")\n    tm.assert_series_equal(result, expected)\n\n\ndef test_missing_labels_inside_loc_matched_in_error_message():\n    # GH34272\n    s = Series({\"a\": 1, \"b\": 2, \"c\": 3})\n    error_message_regex = \"missing_0.*missing_1.*missing_2\"\n    with pytest.raises(KeyError, match=error_message_regex):\n        s.loc[[\"a\", \"b\", \"missing_0\", \"c\", \"missing_1\", \"missing_2\"]]\n\n\ndef test_many_missing_labels_inside_loc_error_message_limited():\n    # GH34272\n    n = 10000\n    missing_labels = [f\"missing_{label}\" for label in range(n)]\n    s = Series({\"a\": 1, \"b\": 2, \"c\": 3})\n    # regex checks labels between 4 and 9995 are replaced with ellipses\n    error_message_regex = \"missing_4.*\\\\.\\\\.\\\\..*missing_9995\"\n    with pytest.raises(KeyError, match=error_message_regex):\n        s.loc[[\"a\", \"c\"] + missing_labels]\n\n\ndef test_long_text_missing_labels_inside_loc_error_message_limited():\n    # GH34272\n    s = Series({\"a\": 1, \"b\": 2, \"c\": 3})\n    missing_labels = [f\"long_missing_label_text_{i}\" * 5 for i in range(3)]\n    # regex checks for very long labels there are new lines between each\n    error_message_regex = \"long_missing_label_text_0.*\\\\\\\\n.*long_missing_label_text_1\"\n    with pytest.raises(KeyError, match=error_message_regex):\n        s.loc[[\"a\", \"c\"] + missing_labels]\n\n\ndef test_setitem_categorical():\n    # https://github.com/pandas-dev/pandas/issues/35369\n    df = DataFrame({\"h\": Series(list(\"mn\")).astype(\"category\")})\n    df.h = df.h.cat.reorder_categories([\"n\", \"m\"])\n    expected = DataFrame(\n        {\"h\": pd.Categorical([\"m\", \"n\"]).reorder_categories([\"n\", \"m\"])}\n    )\n    tm.assert_frame_equal(df, expected)\n"
    },
    {
      "filename": "pandas/tests/series/indexing/test_setitem.py",
      "content": "from datetime import date, datetime\n\nimport numpy as np\nimport pytest\n\nfrom pandas import (\n    DatetimeIndex,\n    Index,\n    MultiIndex,\n    NaT,\n    Series,\n    Timedelta,\n    Timestamp,\n    date_range,\n    period_range,\n)\nimport pandas._testing as tm\nfrom pandas.core.indexing import IndexingError\n\nfrom pandas.tseries.offsets import BDay\n\n\nclass TestSetitemDT64Values:\n    def test_setitem_none_nan(self):\n        series = Series(date_range(\"1/1/2000\", periods=10))\n        series[3] = None\n        assert series[3] is NaT\n\n        series[3:5] = None\n        assert series[4] is NaT\n\n        series[5] = np.nan\n        assert series[5] is NaT\n\n        series[5:7] = np.nan\n        assert series[6] is NaT\n\n    def test_setitem_multiindex_empty_slice(self):\n        # https://github.com/pandas-dev/pandas/issues/35878\n        idx = MultiIndex.from_tuples([(\"a\", 1), (\"b\", 2)])\n        result = Series([1, 2], index=idx)\n        expected = result.copy()\n        result.loc[[]] = 0\n        tm.assert_series_equal(result, expected)\n\n    def test_setitem_with_string_index(self):\n        # GH#23451\n        ser = Series([1, 2, 3], index=[\"Date\", \"b\", \"other\"])\n        ser[\"Date\"] = date.today()\n        assert ser.Date == date.today()\n        assert ser[\"Date\"] == date.today()\n\n    def test_setitem_tuple_with_datetimetz_values(self):\n        # GH#20441\n        arr = date_range(\"2017\", periods=4, tz=\"US/Eastern\")\n        index = [(0, 1), (0, 2), (0, 3), (0, 4)]\n        result = Series(arr, index=index)\n        expected = result.copy()\n        result[(0, 1)] = np.nan\n        expected.iloc[0] = np.nan\n        tm.assert_series_equal(result, expected)\n\n\nclass TestSetitemScalarIndexer:\n    def test_setitem_negative_out_of_bounds(self):\n        ser = Series(tm.rands_array(5, 10), index=tm.rands_array(10, 10))\n\n        msg = \"index -11 is out of bounds for axis 0 with size 10\"\n        with pytest.raises(IndexError, match=msg):\n            ser[-11] = \"foo\"\n\n    @pytest.mark.parametrize(\"indexer\", [tm.loc, tm.at])\n    @pytest.mark.parametrize(\"ser_index\", [0, 1])\n    def test_setitem_series_object_dtype(self, indexer, ser_index):\n        # GH#38303\n        ser = Series([0, 0], dtype=\"object\")\n        idxr = indexer(ser)\n        idxr[0] = Series([42], index=[ser_index])\n        expected = Series([Series([42], index=[ser_index]), 0], dtype=\"object\")\n        tm.assert_series_equal(ser, expected)\n\n    @pytest.mark.parametrize(\"index, exp_value\", [(0, 42.0), (1, np.nan)])\n    def test_setitem_series(self, index, exp_value):\n        # GH#38303\n        ser = Series([0, 0])\n        ser.loc[0] = Series([42], index=[index])\n        expected = Series([exp_value, 0])\n        tm.assert_series_equal(ser, expected)\n\n\nclass TestSetitemSlices:\n    def test_setitem_slice_float_raises(self, datetime_series):\n        msg = (\n            \"cannot do slice indexing on DatetimeIndex with these indexers \"\n            r\"\\[{key}\\] of type float\"\n        )\n        with pytest.raises(TypeError, match=msg.format(key=r\"4\\.0\")):\n            datetime_series[4.0:10.0] = 0\n\n        with pytest.raises(TypeError, match=msg.format(key=r\"4\\.5\")):\n            datetime_series[4.5:10.0] = 0\n\n    def test_setitem_slice(self):\n        ser = Series(range(10), index=list(range(10)))\n        ser[-12:] = 0\n        assert (ser == 0).all()\n\n        ser[:-12] = 5\n        assert (ser == 0).all()\n\n    def test_setitem_slice_integers(self):\n        ser = Series(np.random.randn(8), index=[2, 4, 6, 8, 10, 12, 14, 16])\n\n        ser[:4] = 0\n        assert (ser[:4] == 0).all()\n        assert not (ser[4:] == 0).any()\n\n\nclass TestSetitemBooleanMask:\n    def test_setitem_boolean(self, string_series):\n        mask = string_series > string_series.median()\n\n        # similar indexed series\n        result = string_series.copy()\n        result[mask] = string_series * 2\n        expected = string_series * 2\n        tm.assert_series_equal(result[mask], expected[mask])\n\n        # needs alignment\n        result = string_series.copy()\n        result[mask] = (string_series * 2)[0:5]\n        expected = (string_series * 2)[0:5].reindex_like(string_series)\n        expected[-mask] = string_series[mask]\n        tm.assert_series_equal(result[mask], expected[mask])\n\n    def test_setitem_boolean_corner(self, datetime_series):\n        ts = datetime_series\n        mask_shifted = ts.shift(1, freq=BDay()) > ts.median()\n\n        msg = (\n            r\"Unalignable boolean Series provided as indexer \\(index of \"\n            r\"the boolean Series and of the indexed object do not match\"\n        )\n        with pytest.raises(IndexingError, match=msg):\n            ts[mask_shifted] = 1\n\n        with pytest.raises(IndexingError, match=msg):\n            ts.loc[mask_shifted] = 1\n\n    def test_setitem_boolean_different_order(self, string_series):\n        ordered = string_series.sort_values()\n\n        copy = string_series.copy()\n        copy[ordered > 0] = 0\n\n        expected = string_series.copy()\n        expected[expected > 0] = 0\n\n        tm.assert_series_equal(copy, expected)\n\n    @pytest.mark.parametrize(\"func\", [list, np.array, Series])\n    def test_setitem_boolean_python_list(self, func):\n        # GH19406\n        ser = Series([None, \"b\", None])\n        mask = func([True, False, True])\n        ser[mask] = [\"a\", \"c\"]\n        expected = Series([\"a\", \"b\", \"c\"])\n        tm.assert_series_equal(ser, expected)\n\n    @pytest.mark.parametrize(\"value\", [None, NaT, np.nan])\n    def test_setitem_boolean_td64_values_cast_na(self, value):\n        # GH#18586\n        series = Series([0, 1, 2], dtype=\"timedelta64[ns]\")\n        mask = series == series[0]\n        series[mask] = value\n        expected = Series([NaT, 1, 2], dtype=\"timedelta64[ns]\")\n        tm.assert_series_equal(series, expected)\n\n    def test_setitem_boolean_nullable_int_types(self, any_nullable_numeric_dtype):\n        # GH: 26468\n        ser = Series([5, 6, 7, 8], dtype=any_nullable_numeric_dtype)\n        ser[ser > 6] = Series(range(4), dtype=any_nullable_numeric_dtype)\n        expected = Series([5, 6, 2, 3], dtype=any_nullable_numeric_dtype)\n        tm.assert_series_equal(ser, expected)\n\n        ser = Series([5, 6, 7, 8], dtype=any_nullable_numeric_dtype)\n        ser.loc[ser > 6] = Series(range(4), dtype=any_nullable_numeric_dtype)\n        tm.assert_series_equal(ser, expected)\n\n        ser = Series([5, 6, 7, 8], dtype=any_nullable_numeric_dtype)\n        loc_ser = Series(range(4), dtype=any_nullable_numeric_dtype)\n        ser.loc[ser > 6] = loc_ser.loc[loc_ser > 1]\n        tm.assert_series_equal(ser, expected)\n\n\nclass TestSetitemViewCopySemantics:\n    def test_setitem_invalidates_datetime_index_freq(self):\n        # GH#24096 altering a datetime64tz Series inplace invalidates the\n        #  `freq` attribute on the underlying DatetimeIndex\n\n        dti = date_range(\"20130101\", periods=3, tz=\"US/Eastern\")\n        ts = dti[1]\n        ser = Series(dti)\n        assert ser._values is not dti\n        assert ser._values._data.base is not dti._data._data.base\n        assert dti.freq == \"D\"\n        ser.iloc[1] = NaT\n        assert ser._values.freq is None\n\n        # check that the DatetimeIndex was not altered in place\n        assert ser._values is not dti\n        assert ser._values._data.base is not dti._data._data.base\n        assert dti[1] == ts\n        assert dti.freq == \"D\"\n\n    def test_dt64tz_setitem_does_not_mutate_dti(self):\n        # GH#21907, GH#24096\n        dti = date_range(\"2016-01-01\", periods=10, tz=\"US/Pacific\")\n        ts = dti[0]\n        ser = Series(dti)\n        assert ser._values is not dti\n        assert ser._values._data.base is not dti._data._data.base\n        assert ser._mgr.blocks[0].values is not dti\n        assert ser._mgr.blocks[0].values._data.base is not dti._data._data.base\n\n        ser[::3] = NaT\n        assert ser[0] is NaT\n        assert dti[0] == ts\n\n\nclass TestSetitemCallable:\n    def test_setitem_callable_key(self):\n        # GH#12533\n        ser = Series([1, 2, 3, 4], index=list(\"ABCD\"))\n        ser[lambda x: \"A\"] = -1\n\n        expected = Series([-1, 2, 3, 4], index=list(\"ABCD\"))\n        tm.assert_series_equal(ser, expected)\n\n    def test_setitem_callable_other(self):\n        # GH#13299\n        inc = lambda x: x + 1\n\n        ser = Series([1, 2, -1, 4])\n        ser[ser < 0] = inc\n\n        expected = Series([1, 2, inc, 4])\n        tm.assert_series_equal(ser, expected)\n\n\nclass TestSetitemCasting:\n    @pytest.mark.parametrize(\"unique\", [True, False])\n    @pytest.mark.parametrize(\"val\", [3, 3.0, \"3\"], ids=type)\n    def test_setitem_non_bool_into_bool(self, val, indexer_sli, unique):\n        # dont cast these 3-like values to bool\n        ser = Series([True, False])\n        if not unique:\n            ser.index = [1, 1]\n\n        indexer_sli(ser)[1] = val\n        assert type(ser.iloc[1]) == type(val)\n\n        expected = Series([True, val], dtype=object, index=ser.index)\n        if not unique and indexer_sli is not tm.iloc:\n            expected = Series([val, val], dtype=object, index=[1, 1])\n        tm.assert_series_equal(ser, expected)\n\n\nclass SetitemCastingEquivalents:\n    \"\"\"\n    Check each of several methods that _should_ be equivalent to `obj[key] = val`\n\n    We assume that\n        - obj.index is the default Index(range(len(obj)))\n        - the setitem does not expand the obj\n    \"\"\"\n\n    @pytest.fixture\n    def is_inplace(self):\n        \"\"\"\n        Indicate that we are not (yet) checking whether or not setting is inplace.\n        \"\"\"\n        return None\n\n    def check_indexer(self, obj, key, expected, val, indexer, is_inplace):\n        orig = obj\n        obj = obj.copy()\n        arr = obj._values\n\n        indexer(obj)[key] = val\n        tm.assert_series_equal(obj, expected)\n\n        self._check_inplace(is_inplace, orig, arr, obj)\n\n    def _check_inplace(self, is_inplace, orig, arr, obj):\n        if is_inplace is None:\n            # We are not (yet) checking whether setting is inplace or not\n            pass\n        elif is_inplace:\n            if arr.dtype.kind in [\"m\", \"M\"]:\n                # We may not have the same DTA/TDA, but will have the same\n                #  underlying data\n                assert arr._data is obj._values._data\n            else:\n                assert obj._values is arr\n        else:\n            # otherwise original array should be unchanged\n            tm.assert_equal(arr, orig._values)\n\n    def test_int_key(self, obj, key, expected, val, indexer_sli, is_inplace):\n        if not isinstance(key, int):\n            return\n\n        self.check_indexer(obj, key, expected, val, indexer_sli, is_inplace)\n\n        if indexer_sli is tm.loc:\n            self.check_indexer(obj, key, expected, val, tm.at, is_inplace)\n        elif indexer_sli is tm.iloc:\n            self.check_indexer(obj, key, expected, val, tm.iat, is_inplace)\n\n        rng = range(key, key + 1)\n        self.check_indexer(obj, rng, expected, val, indexer_sli, is_inplace)\n\n        if indexer_sli is not tm.loc:\n            # Note: no .loc because that handles slice edges differently\n            slc = slice(key, key + 1)\n            self.check_indexer(obj, slc, expected, val, indexer_sli, is_inplace)\n\n        ilkey = [key]\n        self.check_indexer(obj, ilkey, expected, val, indexer_sli, is_inplace)\n\n        indkey = np.array(ilkey)\n        self.check_indexer(obj, indkey, expected, val, indexer_sli, is_inplace)\n\n    def test_slice_key(self, obj, key, expected, val, indexer_sli, is_inplace):\n        if not isinstance(key, slice):\n            return\n\n        if indexer_sli is not tm.loc:\n            # Note: no .loc because that handles slice edges differently\n            self.check_indexer(obj, key, expected, val, indexer_sli, is_inplace)\n\n        ilkey = list(range(len(obj)))[key]\n        self.check_indexer(obj, ilkey, expected, val, indexer_sli, is_inplace)\n\n        indkey = np.array(ilkey)\n        self.check_indexer(obj, indkey, expected, val, indexer_sli, is_inplace)\n\n    def test_mask_key(self, obj, key, expected, val, indexer_sli):\n        # setitem with boolean mask\n        mask = np.zeros(obj.shape, dtype=bool)\n        mask[key] = True\n\n        obj = obj.copy()\n        indexer_sli(obj)[mask] = val\n        tm.assert_series_equal(obj, expected)\n\n    def test_series_where(self, obj, key, expected, val, is_inplace):\n        mask = np.zeros(obj.shape, dtype=bool)\n        mask[key] = True\n\n        orig = obj\n        obj = obj.copy()\n        arr = obj._values\n\n        res = obj.where(~mask, val)\n        tm.assert_series_equal(res, expected)\n\n        self._check_inplace(is_inplace, orig, arr, obj)\n\n    def test_index_where(self, obj, key, expected, val, request):\n        if Index(obj).dtype != obj.dtype:\n            pytest.skip(\"test not applicable for this dtype\")\n\n        mask = np.zeros(obj.shape, dtype=bool)\n        mask[key] = True\n\n        if obj.dtype == bool:\n            msg = \"Index/Series casting behavior inconsistent GH#38692\"\n            mark = pytest.mark.xfail(reason=msg)\n            request.node.add_marker(mark)\n\n        res = Index(obj).where(~mask, val)\n        tm.assert_index_equal(res, Index(expected))\n\n    def test_index_putmask(self, obj, key, expected, val):\n        if Index(obj).dtype != obj.dtype:\n            pytest.skip(\"test not applicable for this dtype\")\n\n        mask = np.zeros(obj.shape, dtype=bool)\n        mask[key] = True\n\n        res = Index(obj).putmask(mask, val)\n        tm.assert_index_equal(res, Index(expected))\n\n\n@pytest.mark.parametrize(\n    \"obj,expected,key\",\n    [\n        pytest.param(\n            # these induce dtype changes\n            Series([2, 3, 4, 5, 6, 7, 8, 9, 10]),\n            Series([np.nan, 3, np.nan, 5, np.nan, 7, np.nan, 9, np.nan]),\n            slice(None, None, 2),\n            id=\"int_series_slice_key_step\",\n        ),\n        pytest.param(\n            Series([True, True, False, False]),\n            Series([np.nan, True, np.nan, False], dtype=object),\n            slice(None, None, 2),\n            id=\"bool_series_slice_key_step\",\n        ),\n        pytest.param(\n            # these induce dtype changes\n            Series(np.arange(10)),\n            Series([np.nan, np.nan, np.nan, np.nan, np.nan, 5, 6, 7, 8, 9]),\n            slice(None, 5),\n            id=\"int_series_slice_key\",\n        ),\n        pytest.param(\n            # changes dtype GH#4463\n            Series([1, 2, 3]),\n            Series([np.nan, 2, 3]),\n            0,\n            id=\"int_series_int_key\",\n        ),\n        pytest.param(\n            # changes dtype GH#4463\n            Series([False]),\n            Series([np.nan], dtype=object),\n            # TODO: maybe go to float64 since we are changing the _whole_ Series?\n            0,\n            id=\"bool_series_int_key_change_all\",\n        ),\n        pytest.param(\n            # changes dtype GH#4463\n            Series([False, True]),\n            Series([np.nan, True], dtype=object),\n            0,\n            id=\"bool_series_int_key\",\n        ),\n    ],\n)\nclass TestSetitemCastingEquivalents(SetitemCastingEquivalents):\n    @pytest.fixture(params=[np.nan, np.float64(\"NaN\")])\n    def val(self, request):\n        \"\"\"\n        One python float NaN, one np.float64.  Only np.float64 has a `dtype`\n        attribute.\n        \"\"\"\n        return request.param\n\n\nclass TestSetitemWithExpansion:\n    def test_setitem_empty_series(self):\n        # GH#10193\n        key = Timestamp(\"2012-01-01\")\n        series = Series(dtype=object)\n        series[key] = 47\n        expected = Series(47, [key])\n        tm.assert_series_equal(series, expected)\n\n    def test_setitem_empty_series_datetimeindex_preserves_freq(self):\n        # GH#33573 our index should retain its freq\n        series = Series([], DatetimeIndex([], freq=\"D\"), dtype=object)\n        key = Timestamp(\"2012-01-01\")\n        series[key] = 47\n        expected = Series(47, DatetimeIndex([key], freq=\"D\"))\n        tm.assert_series_equal(series, expected)\n        assert series.index.freq == expected.index.freq\n\n    def test_setitem_empty_series_timestamp_preserves_dtype(self):\n        # GH 21881\n        timestamp = Timestamp(1412526600000000000)\n        series = Series([timestamp], index=[\"timestamp\"], dtype=object)\n        expected = series[\"timestamp\"]\n\n        series = Series([], dtype=object)\n        series[\"anything\"] = 300.0\n        series[\"timestamp\"] = timestamp\n        result = series[\"timestamp\"]\n        assert result == expected\n\n    @pytest.mark.parametrize(\n        \"td\",\n        [\n            Timedelta(\"9 days\"),\n            Timedelta(\"9 days\").to_timedelta64(),\n            Timedelta(\"9 days\").to_pytimedelta(),\n        ],\n    )\n    def test_append_timedelta_does_not_cast(self, td):\n        # GH#22717 inserting a Timedelta should _not_ cast to int64\n        expected = Series([\"x\", td], index=[0, \"td\"], dtype=object)\n\n        ser = Series([\"x\"])\n        ser[\"td\"] = td\n        tm.assert_series_equal(ser, expected)\n        assert isinstance(ser[\"td\"], Timedelta)\n\n        ser = Series([\"x\"])\n        ser.loc[\"td\"] = Timedelta(\"9 days\")\n        tm.assert_series_equal(ser, expected)\n        assert isinstance(ser[\"td\"], Timedelta)\n\n\ndef test_setitem_scalar_into_readonly_backing_data():\n    # GH#14359: test that you cannot mutate a read only buffer\n\n    array = np.zeros(5)\n    array.flags.writeable = False  # make the array immutable\n    series = Series(array)\n\n    for n in range(len(series)):\n        msg = \"assignment destination is read-only\"\n        with pytest.raises(ValueError, match=msg):\n            series[n] = 1\n\n        assert array[n] == 0\n\n\ndef test_setitem_slice_into_readonly_backing_data():\n    # GH#14359: test that you cannot mutate a read only buffer\n\n    array = np.zeros(5)\n    array.flags.writeable = False  # make the array immutable\n    series = Series(array)\n\n    msg = \"assignment destination is read-only\"\n    with pytest.raises(ValueError, match=msg):\n        series[1:3] = 1\n\n    assert not array.any()\n\n\nclass TestSetitemTimedelta64IntoNumeric(SetitemCastingEquivalents):\n    # timedelta64 should not be treated as integers when setting into\n    #  numeric Series\n\n    @pytest.fixture\n    def val(self):\n        td = np.timedelta64(4, \"ns\")\n        return td\n        # TODO: could also try np.full((1,), td)\n\n    @pytest.fixture(params=[complex, int, float])\n    def dtype(self, request):\n        return request.param\n\n    @pytest.fixture\n    def obj(self, dtype):\n        arr = np.arange(5).astype(dtype)\n        ser = Series(arr)\n        return ser\n\n    @pytest.fixture\n    def expected(self, dtype):\n        arr = np.arange(5).astype(dtype)\n        ser = Series(arr)\n        ser = ser.astype(object)\n        ser.values[0] = np.timedelta64(4, \"ns\")\n        return ser\n\n    @pytest.fixture\n    def key(self):\n        return 0\n\n    @pytest.fixture\n    def is_inplace(self):\n        \"\"\"\n        Indicate we do _not_ expect the setting to be done inplace.\n        \"\"\"\n        return False\n\n\nclass TestSetitemDT64IntoInt(SetitemCastingEquivalents):\n    # GH#39619 dont cast dt64 to int when doing this setitem\n\n    @pytest.fixture(params=[\"M8[ns]\", \"m8[ns]\"])\n    def dtype(self, request):\n        return request.param\n\n    @pytest.fixture\n    def scalar(self, dtype):\n        val = np.datetime64(\"2021-01-18 13:25:00\", \"ns\")\n        if dtype == \"m8[ns]\":\n            val = val - val\n        return val\n\n    @pytest.fixture\n    def expected(self, scalar):\n        expected = Series([scalar, scalar, 3], dtype=object)\n        assert isinstance(expected[0], type(scalar))\n        return expected\n\n    @pytest.fixture\n    def obj(self):\n        return Series([1, 2, 3])\n\n    @pytest.fixture\n    def key(self):\n        return slice(None, -1)\n\n    @pytest.fixture(params=[None, list, np.array])\n    def val(self, scalar, request):\n        box = request.param\n        if box is None:\n            return scalar\n        return box([scalar, scalar])\n\n    @pytest.fixture\n    def is_inplace(self):\n        return False\n\n\nclass TestSetitemNAPeriodDtype(SetitemCastingEquivalents):\n    # Setting compatible NA values into Series with PeriodDtype\n\n    @pytest.fixture\n    def expected(self, key):\n        exp = Series(period_range(\"2000-01-01\", periods=10, freq=\"D\"))\n        exp._values.view(\"i8\")[key] = NaT.value\n        assert exp[key] is NaT or all(x is NaT for x in exp[key])\n        return exp\n\n    @pytest.fixture\n    def obj(self):\n        return Series(period_range(\"2000-01-01\", periods=10, freq=\"D\"))\n\n    @pytest.fixture(params=[3, slice(3, 5)])\n    def key(self, request):\n        return request.param\n\n    @pytest.fixture(params=[None, np.nan])\n    def val(self, request):\n        return request.param\n\n    @pytest.fixture\n    def is_inplace(self):\n        return True\n\n\nclass TestSetitemNATimedelta64Dtype(SetitemCastingEquivalents):\n    # some nat-like values should be cast to timedelta64 when inserting\n    #  into a timedelta64 series.  Others should coerce to object\n    #  and retain their dtypes.\n\n    @pytest.fixture\n    def obj(self):\n        return Series([0, 1, 2], dtype=\"m8[ns]\")\n\n    @pytest.fixture(\n        params=[NaT, np.timedelta64(\"NaT\", \"ns\"), np.datetime64(\"NaT\", \"ns\")]\n    )\n    def val(self, request):\n        return request.param\n\n    @pytest.fixture\n    def is_inplace(self, val):\n        # cast to object iff val is datetime64(\"NaT\")\n        return val is NaT or val.dtype.kind == \"m\"\n\n    @pytest.fixture\n    def expected(self, obj, val, is_inplace):\n        dtype = obj.dtype if is_inplace else object\n        expected = Series([val] + list(obj[1:]), dtype=dtype)\n        return expected\n\n    @pytest.fixture\n    def key(self):\n        return 0\n\n\nclass TestSetitemMismatchedTZCastsToObject(SetitemCastingEquivalents):\n    # GH#24024\n    @pytest.fixture\n    def obj(self):\n        return Series(date_range(\"2000\", periods=2, tz=\"US/Central\"))\n\n    @pytest.fixture\n    def val(self):\n        return Timestamp(\"2000\", tz=\"US/Eastern\")\n\n    @pytest.fixture\n    def key(self):\n        return 0\n\n    @pytest.fixture\n    def expected(self):\n        expected = Series(\n            [\n                Timestamp(\"2000-01-01 00:00:00-05:00\", tz=\"US/Eastern\"),\n                Timestamp(\"2000-01-02 00:00:00-06:00\", tz=\"US/Central\"),\n            ],\n            dtype=object,\n        )\n        return expected\n\n\n@pytest.mark.parametrize(\n    \"obj,expected\",\n    [\n        # For numeric series, we should coerce to NaN.\n        (Series([1, 2, 3]), Series([np.nan, 2, 3])),\n        (Series([1.0, 2.0, 3.0]), Series([np.nan, 2.0, 3.0])),\n        # For datetime series, we should coerce to NaT.\n        (\n            Series([datetime(2000, 1, 1), datetime(2000, 1, 2), datetime(2000, 1, 3)]),\n            Series([NaT, datetime(2000, 1, 2), datetime(2000, 1, 3)]),\n        ),\n        # For objects, we should preserve the None value.\n        (Series([\"foo\", \"bar\", \"baz\"]), Series([None, \"bar\", \"baz\"])),\n    ],\n)\nclass TestSeriesNoneCoercion(SetitemCastingEquivalents):\n    @pytest.fixture\n    def key(self):\n        return 0\n\n    @pytest.fixture\n    def val(self):\n        return None\n\n    @pytest.fixture\n    def is_inplace(self, obj):\n        # This is specific to the 4 cases currently implemented for this class.\n        return obj.dtype.kind != \"i\"\n"
    }
  ],
  "questions": [],
  "golden_answers": [],
  "questions_generated": [
    "What is the main issue described in the pandas-dev/pandas repository regarding the 'pd.where' function?",
    "How does the 'pd.where' function behave differently when applied to individual columns versus the entire DataFrame in this scenario?",
    "What changes between pandas version 0.25.0 and 1.0.0 might have caused this OverflowError with 'pd.where'?",
    "Can you describe the role of 'maybe_downcast_to_dtype' in the 'pd.where' function and why it might be causing an OverflowError?",
    "What might be a potential workaround for this issue when using 'pd.where' with large numbers in a DataFrame?",
    "Why does the 'pd.where' function's behavior change when the replacement value has a different dtype than the DataFrame's large float values?"
  ],
  "golden_answers_generated": [
    "The main issue is that using the 'pd.where' function on a DataFrame containing large float values and replacing values with a different dtype leads to an 'OverflowError: int too big to convert'. This error occurs when the operation is applied to the whole DataFrame, but it works when applied to individual columns.",
    "When 'pd.where' is applied to individual columns of the DataFrame, it works correctly without throwing any errors. However, when applied to the entire DataFrame that contains large float values, it throws an OverflowError due to dtype conversion issues.",
    "The issue might be related to changes in how pandas handles dtype inference and conversion between versions 0.25.0 and 1.0.0. Specifically, the introduction of stricter dtype handling or changes in the internals of the 'where' method and related functions like 'maybe_downcast_to_dtype' and 'maybe_downcast_numeric' could have led to the OverflowError when dealing with large numbers.",
    "'maybe_downcast_to_dtype' is used within the 'pd.where' function to potentially downcast the result to a more appropriate dtype. In this issue, it attempts to convert the result to an integer dtype, but the large float values cause an OverflowError because they cannot be represented as integers. This function is part of the dtype handling process that leads to the error.",
    "A potential workaround is to apply 'pd.where' to each column individually instead of the entire DataFrame. This avoids the dtype conversion issues that lead to the OverflowError. Alternatively, ensuring that replacement values have a compatible dtype with the DataFrame's values might also prevent the error.",
    "The behavior changes because pandas attempts to infer a common dtype for the result when replacement values have a different dtype. If the inferred dtype cannot accommodate the large float values due to conversion constraints, such as converting to an integer dtype, it results in an OverflowError. The internal functions like 'maybe_downcast_to_dtype' and 'maybe_downcast_numeric' play a role in this dtype inference and conversion process."
  ]
}