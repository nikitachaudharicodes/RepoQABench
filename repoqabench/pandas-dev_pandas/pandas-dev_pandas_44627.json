{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "44627",
  "issue_description": "# DEPR Deprecate mapper argument in rename?\n\nxref https://github.com/pandas-dev/pandas/pull/40979#issuecomment-980063593\r\n\r\n> thanks @MarcoGorelli i realize now why this is convoluted, will be glad to remove this in 2.0\r\n\r\n@jreback not sure I understand what you mean - as in, to remove `mapper` in 2.0 and only support the `index=` and `columns=` calls? (and hence, introduce a deprecation warning in time for 1.4)",
  "issue_comments": [
    {
      "id": 980229636,
      "user": "jreback",
      "body": "no what u mean is to fix the structure of the impl here\nmaybe we just need to make the structure in generic a private function to avoid having to handle the signature like this"
    },
    {
      "id": 980249381,
      "user": "MarcoGorelli",
      "body": "OK, so rename `rename` in `pandas/core/generic.py` to `_rename`, and go back to the old signature for `Series.rename`, i.e.\r\n```\r\n    def rename(\r\n        self,\r\n        index=None,\r\n        *,\r\n        axis=None,\r\n        copy=True,\r\n        inplace=False,\r\n        level=None,\r\n        errors=\"ignore\",\r\n    ):\r\n```\r\n? Like this, `mypy` wouldn't complain about the incompatibility.\r\nWish I'd thought of this actually, it seems like a better solution than what I did.\r\n\r\nIf so, I think that could be done straight away, rather than having to wait for 2.0, it wouldn't alter anything user-facing"
    },
    {
      "id": 980256171,
      "user": "jreback",
      "body": "yeah i think that's right. its not worth trying to shoe-horn this in for methods which have a different signatture in series vs dataframe just to placate mypy, better to just have the impl be private."
    },
    {
      "id": 980616373,
      "user": "Varun270",
      "body": "@MarcoGorelli I tried to understand this issue and so far I understood that in `pandas/core/generic.py` we have to make the `rename` function private and make it `_rename`. Is that all? I am sure I am missing out on a lot."
    },
    {
      "id": 980635786,
      "user": "MarcoGorelli",
      "body": "Hey @Varun270 !\r\n\r\nSo, there's a few things to do:\r\n1. rename `rename` in `pandas/core/generic.py` to `_rename`\r\n2. update `rename` (in both `pandas/core/frame.py` and `pandas/core/series.py` to call `super()._rename`, rather than `super().rename`)\r\n3. revert some of the changes I made in #40979 so that the signature of `Series.rename` becomes:\r\n```python\r\n    def rename(\r\n        self,\r\n        index=None,\r\n        *,\r\n        axis=None,\r\n        copy=True,\r\n        inplace=False,\r\n        level=None,\r\n        errors=\"ignore\",\r\n    ) ->  Series | None:\r\n```\r\n\r\nIf you open a PR (even if you're not sure about everything) and tag me I can help you along with this"
    },
    {
      "id": 980842585,
      "user": "Varun270",
      "body": "> Hey @Varun270 !\r\n> \r\n> So, there's a few things to do:\r\n> \r\n> 1. rename `rename` in `pandas/core/generic.py` to `_rename`\r\n> 2. update `rename` (in both `pandas/core/frame.py` and `pandas/core/series.py` to call `super()._rename`, rather than `super().rename`)\r\n> 3. revert some of the changes I made in [TYP: Signature of \"rename\" incompatible with supertype \"NDFrame\" #40979](https://github.com/pandas-dev/pandas/pull/40979) so that the signature of `Series.rename` becomes:\r\n> \r\n> ```python\r\n>     def rename(\r\n>         self,\r\n>         index=None,\r\n>         *,\r\n>         axis=None,\r\n>         copy=True,\r\n>         inplace=False,\r\n>         level=None,\r\n>         errors=\"ignore\",\r\n>     ) ->  Series | None:\r\n> ```\r\n> \r\n> If you open a PR (even if you're not sure about everything) and tag me I can help you along with this\r\n\r\nI understood the first two points but will require your guidance on 3rd point as I don't know how can I revert changes done by someone else."
    },
    {
      "id": 980842782,
      "user": "Varun270",
      "body": "Currently in `Series.rename` the function rename looks like this -\r\n```py\r\ndef rename(\r\n        self,\r\n        index=None,\r\n        *,\r\n        axis=None,\r\n        copy=True,\r\n        inplace=False,\r\n        level=None,\r\n        errors=\"ignore\",\r\n    ):\r\n```\r\nSo after reverting it would look like this - \r\n```py\r\n    def rename(\r\n        self,\r\n        index=None,\r\n        *,\r\n        axis=None,\r\n        copy=True,\r\n        inplace=False,\r\n        level=None,\r\n        errors=\"ignore\",\r\n    ) ->  Series | None:\r\n\r\n```\r\nIs that what you are saying?"
    },
    {
      "id": 980911120,
      "user": "MarcoGorelli",
      "body": "> Currently in Series.rename the function rename looks like this -\r\n\r\nMake sure to pull the latest changes, it doesn't quite look like that at the moment. Anyway,\r\n\r\n> So after reverting it would look like this -\r\n\r\nYes, it should look like that at the end"
    },
    {
      "id": 981084244,
      "user": "Varun270",
      "body": "> > Currently in Series.rename the function rename looks like this -\r\n> \r\n> Make sure to pull the latest changes, it doesn't quite look like that at the moment. Anyway,\r\n> \r\n> > So after reverting it would look like this -\r\n> \r\n> Yes, it should look like that at the end\r\n\r\nOkay, so I guess I would have to revert the changes first then rename the `rename` functions. How am I supposed to do that? Like should I do the usual git revert commit_id?"
    },
    {
      "id": 981110806,
      "user": "MarcoGorelli",
      "body": "it's up to you how you do it"
    },
    {
      "id": 981291419,
      "user": "Varun270",
      "body": "Alright, will do it!"
    },
    {
      "id": 981655911,
      "user": "Varun270",
      "body": "@MarcoGorelli I found the commit that needed to be reverted but you have done some other changes in that commit apart from adding args to function `rename`. Here have a look at this - https://github.com/pandas-dev/pandas/pull/40979/commits/40ecf6e754451acb87f8a2f3b911a8cb9728cda1\r\nso should I proceed ahead since reverting it would revert some additional changes as well?"
    },
    {
      "id": 981659615,
      "user": "MarcoGorelli",
      "body": "What can be reverted are:\r\n- changes to `Series.rename` (can just revert them all, and change `super().rename` to `super()._rename`\r\n- the extra tests in `pandas/tests/series/methods/test_rename.py` are no longer necessary\r\n\r\nMight be easier to do this manually rather than reverting a commit using git. Up to you though, if you open a PR I'll take a look"
    },
    {
      "id": 981661705,
      "user": "Varun270",
      "body": "> What can be reverted are:\r\n> \r\n> * changes to `Series.rename` (can just revert them all, and change `super().rename` to `super()._rename`\r\n> * the extra tests in `pandas/tests/series/methods/test_rename.py` are no longer necessary\r\n> \r\n> Might be easier to do this manually rather than reverting a commit using git. Up to you though, if you open a PR I'll take a look\r\n\r\nYeah, even I was thinking that manually doing these changes would be much easier for me. Will create a PR."
    }
  ],
  "text_context": "# DEPR Deprecate mapper argument in rename?\n\nxref https://github.com/pandas-dev/pandas/pull/40979#issuecomment-980063593\r\n\r\n> thanks @MarcoGorelli i realize now why this is convoluted, will be glad to remove this in 2.0\r\n\r\n@jreback not sure I understand what you mean - as in, to remove `mapper` in 2.0 and only support the `index=` and `columns=` calls? (and hence, introduce a deprecation warning in time for 1.4)\n\nno what u mean is to fix the structure of the impl here\nmaybe we just need to make the structure in generic a private function to avoid having to handle the signature like this\n\nOK, so rename `rename` in `pandas/core/generic.py` to `_rename`, and go back to the old signature for `Series.rename`, i.e.\r\n```\r\n    def rename(\r\n        self,\r\n        index=None,\r\n        *,\r\n        axis=None,\r\n        copy=True,\r\n        inplace=False,\r\n        level=None,\r\n        errors=\"ignore\",\r\n    ):\r\n```\r\n? Like this, `mypy` wouldn't complain about the incompatibility.\r\nWish I'd thought of this actually, it seems like a better solution than what I did.\r\n\r\nIf so, I think that could be done straight away, rather than having to wait for 2.0, it wouldn't alter anything user-facing\n\nyeah i think that's right. its not worth trying to shoe-horn this in for methods which have a different signatture in series vs dataframe just to placate mypy, better to just have the impl be private.\n\n@MarcoGorelli I tried to understand this issue and so far I understood that in `pandas/core/generic.py` we have to make the `rename` function private and make it `_rename`. Is that all? I am sure I am missing out on a lot.\n\nHey @Varun270 !\r\n\r\nSo, there's a few things to do:\r\n1. rename `rename` in `pandas/core/generic.py` to `_rename`\r\n2. update `rename` (in both `pandas/core/frame.py` and `pandas/core/series.py` to call `super()._rename`, rather than `super().rename`)\r\n3. revert some of the changes I made in #40979 so that the signature of `Series.rename` becomes:\r\n```python\r\n    def rename(\r\n        self,\r\n        index=None,\r\n        *,\r\n        axis=None,\r\n        copy=True,\r\n        inplace=False,\r\n        level=None,\r\n        errors=\"ignore\",\r\n    ) ->  Series | None:\r\n```\r\n\r\nIf you open a PR (even if you're not sure about everything) and tag me I can help you along with this\n\n> Hey @Varun270 !\r\n> \r\n> So, there's a few things to do:\r\n> \r\n> 1. rename `rename` in `pandas/core/generic.py` to `_rename`\r\n> 2. update `rename` (in both `pandas/core/frame.py` and `pandas/core/series.py` to call `super()._rename`, rather than `super().rename`)\r\n> 3. revert some of the changes I made in [TYP: Signature of \"rename\" incompatible with supertype \"NDFrame\" #40979](https://github.com/pandas-dev/pandas/pull/40979) so that the signature of `Series.rename` becomes:\r\n> \r\n> ```python\r\n>     def rename(\r\n>         self,\r\n>         index=None,\r\n>         *,\r\n>         axis=None,\r\n>         copy=True,\r\n>         inplace=False,\r\n>         level=None,\r\n>         errors=\"ignore\",\r\n>     ) ->  Series | None:\r\n> ```\r\n> \r\n> If you open a PR (even if you're not sure about everything) and tag me I can help you along with this\r\n\r\nI understood the first two points but will require your guidance on 3rd point as I don't know how can I revert changes done by someone else.\n\nCurrently in `Series.rename` the function rename looks like this -\r\n```py\r\ndef rename(\r\n        self,\r\n        index=None,\r\n        *,\r\n        axis=None,\r\n        copy=True,\r\n        inplace=False,\r\n        level=None,\r\n        errors=\"ignore\",\r\n    ):\r\n```\r\nSo after reverting it would look like this - \r\n```py\r\n    def rename(\r\n        self,\r\n        index=None,\r\n        *,\r\n        axis=None,\r\n        copy=True,\r\n        inplace=False,\r\n        level=None,\r\n        errors=\"ignore\",\r\n    ) ->  Series | None:\r\n\r\n```\r\nIs that what you are saying?\n\n> Currently in Series.rename the function rename looks like this -\r\n\r\nMake sure to pull the latest changes, it doesn't quite look like that at the moment. Anyway,\r\n\r\n> So after reverting it would look like this -\r\n\r\nYes, it should look like that at the end\n\n> > Currently in Series.rename the function rename looks like this -\r\n> \r\n> Make sure to pull the latest changes, it doesn't quite look like that at the moment. Anyway,\r\n> \r\n> > So after reverting it would look like this -\r\n> \r\n> Yes, it should look like that at the end\r\n\r\nOkay, so I guess I would have to revert the changes first then rename the `rename` functions. How am I supposed to do that? Like should I do the usual git revert commit_id?\n\nit's up to you how you do it\n\nAlright, will do it!\n\n@MarcoGorelli I found the commit that needed to be reverted but you have done some other changes in that commit apart from adding args to function `rename`. Here have a look at this - https://github.com/pandas-dev/pandas/pull/40979/commits/40ecf6e754451acb87f8a2f3b911a8cb9728cda1\r\nso should I proceed ahead since reverting it would revert some additional changes as well?\n\nWhat can be reverted are:\r\n- changes to `Series.rename` (can just revert them all, and change `super().rename` to `super()._rename`\r\n- the extra tests in `pandas/tests/series/methods/test_rename.py` are no longer necessary\r\n\r\nMight be easier to do this manually rather than reverting a commit using git. Up to you though, if you open a PR I'll take a look\n\n> What can be reverted are:\r\n> \r\n> * changes to `Series.rename` (can just revert them all, and change `super().rename` to `super()._rename`\r\n> * the extra tests in `pandas/tests/series/methods/test_rename.py` are no longer necessary\r\n> \r\n> Might be easier to do this manually rather than reverting a commit using git. Up to you though, if you open a PR I'll take a look\r\n\r\nYeah, even I was thinking that manually doing these changes would be much easier for me. Will create a PR.",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/40979",
  "code_context": [
    {
      "filename": "pandas/core/frame.py",
      "content": "\"\"\"\nDataFrame\n---------\nAn efficient 2D container for potentially mixed-type time series or other\nlabeled data series.\n\nSimilar to its R counterpart, data.frame, except providing automatic data\nalignment and a host of useful data manipulation methods having to do with the\nlabeling information\n\"\"\"\nfrom __future__ import annotations\n\nimport collections\nfrom collections import abc\nimport datetime\nimport functools\nfrom io import StringIO\nimport itertools\nfrom textwrap import dedent\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Hashable,\n    Iterable,\n    Iterator,\n    Literal,\n    Sequence,\n    cast,\n    overload,\n)\nimport warnings\n\nimport numpy as np\nimport numpy.ma as ma\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import (\n    algos as libalgos,\n    lib,\n    properties,\n)\nfrom pandas._libs.hashtable import duplicated\nfrom pandas._libs.lib import no_default\nfrom pandas._typing import (\n    AggFuncType,\n    AnyArrayLike,\n    ArrayLike,\n    Axes,\n    Axis,\n    ColspaceArgType,\n    CompressionOptions,\n    Dtype,\n    DtypeObj,\n    FilePath,\n    FillnaOptions,\n    FloatFormatType,\n    FormattersType,\n    Frequency,\n    IndexKeyFunc,\n    IndexLabel,\n    Level,\n    PythonFuncType,\n    Renamer,\n    Scalar,\n    StorageOptions,\n    Suffixes,\n    TimedeltaConvertibleTypes,\n    TimestampConvertibleTypes,\n    ValueKeyFunc,\n    WriteBuffer,\n    npt,\n)\nfrom pandas.compat._optional import import_optional_dependency\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import (\n    Appender,\n    Substitution,\n    deprecate_kwarg,\n    deprecate_nonkeyword_arguments,\n    doc,\n    rewrite_axis_style_signature,\n)\nfrom pandas.util._exceptions import find_stack_level\nfrom pandas.util._validators import (\n    validate_ascending,\n    validate_axis_style_args,\n    validate_bool_kwarg,\n    validate_percentile,\n)\n\nfrom pandas.core.dtypes.cast import (\n    construct_1d_arraylike_from_scalar,\n    construct_2d_arraylike_from_scalar,\n    find_common_type,\n    infer_dtype_from_scalar,\n    invalidate_string_dtypes,\n    maybe_box_native,\n    maybe_downcast_to_dtype,\n    validate_numeric_casting,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_platform_int,\n    infer_dtype_from_object,\n    is_1d_only_ea_dtype,\n    is_1d_only_ea_obj,\n    is_bool_dtype,\n    is_dataclass,\n    is_datetime64_any_dtype,\n    is_dict_like,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float,\n    is_float_dtype,\n    is_hashable,\n    is_integer,\n    is_integer_dtype,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_scalar,\n    is_sequence,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import ExtensionDtype\nfrom pandas.core.dtypes.missing import (\n    isna,\n    notna,\n)\n\nfrom pandas.core import (\n    algorithms,\n    common as com,\n    generic,\n    nanops,\n    ops,\n)\nfrom pandas.core.accessor import CachedAccessor\nfrom pandas.core.apply import (\n    reconstruct_func,\n    relabel_result,\n)\nfrom pandas.core.array_algos.take import take_2d_multi\nfrom pandas.core.arraylike import OpsMixin\nfrom pandas.core.arrays import (\n    DatetimeArray,\n    ExtensionArray,\n    TimedeltaArray,\n)\nfrom pandas.core.arrays.sparse import SparseFrameAccessor\nfrom pandas.core.construction import (\n    extract_array,\n    sanitize_array,\n    sanitize_masked_array,\n)\nfrom pandas.core.generic import (\n    NDFrame,\n    _shared_docs,\n)\nfrom pandas.core.indexers import check_key_length\nfrom pandas.core.indexes.api import (\n    DatetimeIndex,\n    Index,\n    PeriodIndex,\n    default_index,\n    ensure_index,\n    ensure_index_from_sequences,\n)\nfrom pandas.core.indexes.multi import (\n    MultiIndex,\n    maybe_droplevels,\n)\nfrom pandas.core.indexing import (\n    check_bool_indexer,\n    convert_to_index_sliceable,\n)\nfrom pandas.core.internals import (\n    ArrayManager,\n    BlockManager,\n)\nfrom pandas.core.internals.construction import (\n    arrays_to_mgr,\n    dataclasses_to_dicts,\n    dict_to_mgr,\n    mgr_to_mgr,\n    ndarray_to_mgr,\n    nested_data_to_arrays,\n    rec_array_to_mgr,\n    reorder_arrays,\n    to_arrays,\n    treat_as_nested,\n)\nfrom pandas.core.reshape.melt import melt\nfrom pandas.core.series import Series\nfrom pandas.core.sorting import (\n    get_group_index,\n    lexsort_indexer,\n    nargsort,\n)\n\nfrom pandas.io.common import get_handle\nfrom pandas.io.formats import (\n    console,\n    format as fmt,\n)\nfrom pandas.io.formats.info import (\n    BaseInfo,\n    DataFrameInfo,\n)\nimport pandas.plotting\n\nif TYPE_CHECKING:\n\n    from pandas.core.groupby.generic import DataFrameGroupBy\n    from pandas.core.resample import Resampler\n\n    from pandas.io.formats.style import Styler\n\n# ---------------------------------------------------------------------\n# Docstring templates\n\n_shared_doc_kwargs = {\n    \"axes\": \"index, columns\",\n    \"klass\": \"DataFrame\",\n    \"axes_single_arg\": \"{0 or 'index', 1 or 'columns'}\",\n    \"axis\": \"\"\"axis : {0 or 'index', 1 or 'columns'}, default 0\n        If 0 or 'index': apply function to each column.\n        If 1 or 'columns': apply function to each row.\"\"\",\n    \"inplace\": \"\"\"\n    inplace : bool, default False\n        If True, performs operation inplace and returns None.\"\"\",\n    \"optional_by\": \"\"\"\n        by : str or list of str\n            Name or list of names to sort by.\n\n            - if `axis` is 0 or `'index'` then `by` may contain index\n              levels and/or column labels.\n            - if `axis` is 1 or `'columns'` then `by` may contain column\n              levels and/or index labels.\"\"\",\n    \"optional_labels\": \"\"\"labels : array-like, optional\n            New labels / index to conform the axis specified by 'axis' to.\"\"\",\n    \"optional_axis\": \"\"\"axis : int or str, optional\n            Axis to target. Can be either the axis name ('index', 'columns')\n            or number (0, 1).\"\"\",\n    \"replace_iloc\": \"\"\"\n    This differs from updating with ``.loc`` or ``.iloc``, which require\n    you to specify a location to update with some value.\"\"\",\n}\n\n_numeric_only_doc = \"\"\"numeric_only : bool or None, default None\n    Include only float, int, boolean data. If None, will attempt to use\n    everything, then use only numeric data\n\"\"\"\n\n_merge_doc = \"\"\"\nMerge DataFrame or named Series objects with a database-style join.\n\nA named Series object is treated as a DataFrame with a single named column.\n\nThe join is done on columns or indexes. If joining columns on\ncolumns, the DataFrame indexes *will be ignored*. Otherwise if joining indexes\non indexes or indexes on a column or columns, the index will be passed on.\nWhen performing a cross merge, no column specifications to merge on are\nallowed.\n\n.. warning::\n\n    If both key columns contain rows where the key is a null value, those\n    rows will be matched against each other. This is different from usual SQL\n    join behaviour and can lead to unexpected results.\n\nParameters\n----------%s\nright : DataFrame or named Series\n    Object to merge with.\nhow : {'left', 'right', 'outer', 'inner', 'cross'}, default 'inner'\n    Type of merge to be performed.\n\n    * left: use only keys from left frame, similar to a SQL left outer join;\n      preserve key order.\n    * right: use only keys from right frame, similar to a SQL right outer join;\n      preserve key order.\n    * outer: use union of keys from both frames, similar to a SQL full outer\n      join; sort keys lexicographically.\n    * inner: use intersection of keys from both frames, similar to a SQL inner\n      join; preserve the order of the left keys.\n    * cross: creates the cartesian product from both frames, preserves the order\n      of the left keys.\n\n      .. versionadded:: 1.2.0\n\non : label or list\n    Column or index level names to join on. These must be found in both\n    DataFrames. If `on` is None and not merging on indexes then this defaults\n    to the intersection of the columns in both DataFrames.\nleft_on : label or list, or array-like\n    Column or index level names to join on in the left DataFrame. Can also\n    be an array or list of arrays of the length of the left DataFrame.\n    These arrays are treated as if they are columns.\nright_on : label or list, or array-like\n    Column or index level names to join on in the right DataFrame. Can also\n    be an array or list of arrays of the length of the right DataFrame.\n    These arrays are treated as if they are columns.\nleft_index : bool, default False\n    Use the index from the left DataFrame as the join key(s). If it is a\n    MultiIndex, the number of keys in the other DataFrame (either the index\n    or a number of columns) must match the number of levels.\nright_index : bool, default False\n    Use the index from the right DataFrame as the join key. Same caveats as\n    left_index.\nsort : bool, default False\n    Sort the join keys lexicographically in the result DataFrame. If False,\n    the order of the join keys depends on the join type (how keyword).\nsuffixes : list-like, default is (\"_x\", \"_y\")\n    A length-2 sequence where each element is optionally a string\n    indicating the suffix to add to overlapping column names in\n    `left` and `right` respectively. Pass a value of `None` instead\n    of a string to indicate that the column name from `left` or\n    `right` should be left as-is, with no suffix. At least one of the\n    values must not be None.\ncopy : bool, default True\n    If False, avoid copy if possible.\nindicator : bool or str, default False\n    If True, adds a column to the output DataFrame called \"_merge\" with\n    information on the source of each row. The column can be given a different\n    name by providing a string argument. The column will have a Categorical\n    type with the value of \"left_only\" for observations whose merge key only\n    appears in the left DataFrame, \"right_only\" for observations\n    whose merge key only appears in the right DataFrame, and \"both\"\n    if the observation's merge key is found in both DataFrames.\n\nvalidate : str, optional\n    If specified, checks if merge is of specified type.\n\n    * \"one_to_one\" or \"1:1\": check if merge keys are unique in both\n      left and right datasets.\n    * \"one_to_many\" or \"1:m\": check if merge keys are unique in left\n      dataset.\n    * \"many_to_one\" or \"m:1\": check if merge keys are unique in right\n      dataset.\n    * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.\n\nReturns\n-------\nDataFrame\n    A DataFrame of the two merged objects.\n\nSee Also\n--------\nmerge_ordered : Merge with optional filling/interpolation.\nmerge_asof : Merge on nearest keys.\nDataFrame.join : Similar method using indices.\n\nNotes\n-----\nSupport for specifying index levels as the `on`, `left_on`, and\n`right_on` parameters was added in version 0.23.0\nSupport for merging named Series objects was added in version 0.24.0\n\nExamples\n--------\n>>> df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [1, 2, 3, 5]})\n>>> df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [5, 6, 7, 8]})\n>>> df1\n    lkey value\n0   foo      1\n1   bar      2\n2   baz      3\n3   foo      5\n>>> df2\n    rkey value\n0   foo      5\n1   bar      6\n2   baz      7\n3   foo      8\n\nMerge df1 and df2 on the lkey and rkey columns. The value columns have\nthe default suffixes, _x and _y, appended.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey')\n  lkey  value_x rkey  value_y\n0  foo        1  foo        5\n1  foo        1  foo        8\n2  foo        5  foo        5\n3  foo        5  foo        8\n4  bar        2  bar        6\n5  baz        3  baz        7\n\nMerge DataFrames df1 and df2 with specified left and right suffixes\nappended to any overlapping columns.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey',\n...           suffixes=('_left', '_right'))\n  lkey  value_left rkey  value_right\n0  foo           1  foo            5\n1  foo           1  foo            8\n2  foo           5  foo            5\n3  foo           5  foo            8\n4  bar           2  bar            6\n5  baz           3  baz            7\n\nMerge DataFrames df1 and df2, but raise an exception if the DataFrames have\nany overlapping columns.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))\nTraceback (most recent call last):\n...\nValueError: columns overlap but no suffix specified:\n    Index(['value'], dtype='object')\n\n>>> df1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\n>>> df2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\n>>> df1\n      a  b\n0   foo  1\n1   bar  2\n>>> df2\n      a  c\n0   foo  3\n1   baz  4\n\n>>> df1.merge(df2, how='inner', on='a')\n      a  b  c\n0   foo  1  3\n\n>>> df1.merge(df2, how='left', on='a')\n      a  b  c\n0   foo  1  3.0\n1   bar  2  NaN\n\n>>> df1 = pd.DataFrame({'left': ['foo', 'bar']})\n>>> df2 = pd.DataFrame({'right': [7, 8]})\n>>> df1\n    left\n0   foo\n1   bar\n>>> df2\n    right\n0   7\n1   8\n\n>>> df1.merge(df2, how='cross')\n   left  right\n0   foo      7\n1   foo      8\n2   bar      7\n3   bar      8\n\"\"\"\n\n\n# -----------------------------------------------------------------------\n# DataFrame class\n\n\nclass DataFrame(NDFrame, OpsMixin):\n    \"\"\"\n    Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\n    Data structure also contains labeled axes (rows and columns).\n    Arithmetic operations align on both row and column labels. Can be\n    thought of as a dict-like container for Series objects. The primary\n    pandas data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n        Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n        data is a dict, column order follows insertion-order. If a dict contains Series\n        which have an index defined, it is aligned by its index.\n\n        .. versionchanged:: 0.25.0\n           If data is a list of dicts, column order follows insertion-order.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing information part of input data and no index provided.\n    columns : Index or array-like\n        Column labels to use for resulting frame when data does not have them,\n        defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n        will perform column selection instead.\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is allowed. If None, infer.\n    copy : bool or None, default None\n        Copy data from inputs.\n        For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n        or 2d ndarray input, the default of None behaves like ``copy=False``.\n\n        .. versionchanged:: 1.3.0\n\n    See Also\n    --------\n    DataFrame.from_records : Constructor from tuples, also record arrays.\n    DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n    read_csv : Read a comma-separated values (csv) file into DataFrame.\n    read_table : Read general delimited file into DataFrame.\n    read_clipboard : Read text from clipboard into DataFrame.\n\n    Examples\n    --------\n    Constructing DataFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> df = pd.DataFrame(data=d)\n    >>> df\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> df.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> df = pd.DataFrame(data=d, dtype=np.int8)\n    >>> df.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing DataFrame from a dictionary including Series:\n\n    >>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n    >>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n       col1  col2\n    0     0   NaN\n    1     1   NaN\n    2     2   2.0\n    3     3   3.0\n\n    Constructing DataFrame from numpy ndarray:\n\n    >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> df2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n\n    Constructing DataFrame from a numpy ndarray that has labeled columns:\n\n    >>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n    ...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n    >>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n    ...\n    >>> df3\n       c  a\n    0  3  1\n    1  6  4\n    2  9  7\n\n    Constructing DataFrame from dataclass:\n\n    >>> from dataclasses import make_dataclass\n    >>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n    >>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n       x  y\n    0  0  0\n    1  0  3\n    2  2  3\n    \"\"\"\n\n    _internal_names_set = {\"columns\", \"index\"} | NDFrame._internal_names_set\n    _typ = \"dataframe\"\n    _HANDLED_TYPES = (Series, Index, ExtensionArray, np.ndarray)\n    _accessors: set[str] = {\"sparse\"}\n    _hidden_attrs: frozenset[str] = NDFrame._hidden_attrs | frozenset([])\n    _mgr: BlockManager | ArrayManager\n\n    @property\n    def _constructor(self) -> type[DataFrame]:\n        return DataFrame\n\n    _constructor_sliced: type[Series] = Series\n\n    # ----------------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self,\n        data=None,\n        index: Axes | None = None,\n        columns: Axes | None = None,\n        dtype: Dtype | None = None,\n        copy: bool | None = None,\n    ):\n\n        if copy is None:\n            if isinstance(data, dict) or data is None:\n                # retain pre-GH#38939 default behavior\n                copy = True\n            else:\n                copy = False\n\n        if data is None:\n            data = {}\n        if dtype is not None:\n            dtype = self._validate_dtype(dtype)\n\n        if isinstance(data, DataFrame):\n            data = data._mgr\n\n        if isinstance(data, (BlockManager, ArrayManager)):\n            # first check if a Manager is passed without any other arguments\n            # -> use fastpath (without checking Manager type)\n            if index is None and columns is None and dtype is None and not copy:\n                # GH#33357 fastpath\n                NDFrame.__init__(self, data)\n                return\n\n        manager = get_option(\"mode.data_manager\")\n\n        if isinstance(data, (BlockManager, ArrayManager)):\n            mgr = self._init_mgr(\n                data, axes={\"index\": index, \"columns\": columns}, dtype=dtype, copy=copy\n            )\n\n        elif isinstance(data, dict):\n            # GH#38939 de facto copy defaults to False only in non-dict cases\n            mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n        elif isinstance(data, ma.MaskedArray):\n            import numpy.ma.mrecords as mrecords\n\n            # masked recarray\n            if isinstance(data, mrecords.MaskedRecords):\n                mgr = rec_array_to_mgr(\n                    data,\n                    index,\n                    columns,\n                    dtype,\n                    copy,\n                    typ=manager,\n                )\n                warnings.warn(\n                    \"Support for MaskedRecords is deprecated and will be \"\n                    \"removed in a future version.  Pass \"\n                    \"{name: data[name] for name in data.dtype.names} instead.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n\n            # a masked array\n            else:\n                data = sanitize_masked_array(data)\n                mgr = ndarray_to_mgr(\n                    data,\n                    index,\n                    columns,\n                    dtype=dtype,\n                    copy=copy,\n                    typ=manager,\n                )\n\n        elif isinstance(data, (np.ndarray, Series, Index)):\n            if data.dtype.names:\n                # i.e. numpy structured array\n                data = cast(np.ndarray, data)\n                mgr = rec_array_to_mgr(\n                    data,\n                    index,\n                    columns,\n                    dtype,\n                    copy,\n                    typ=manager,\n                )\n            elif getattr(data, \"name\", None) is not None:\n                # i.e. Series/Index with non-None name\n                mgr = dict_to_mgr(\n                    # error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\n                    # attribute \"name\"\n                    {data.name: data},  # type: ignore[union-attr]\n                    index,\n                    columns,\n                    dtype=dtype,\n                    typ=manager,\n                )\n            else:\n                mgr = ndarray_to_mgr(\n                    data,\n                    index,\n                    columns,\n                    dtype=dtype,\n                    copy=copy,\n                    typ=manager,\n                )\n\n        # For data is list-like, or Iterable (will consume into list)\n        elif is_list_like(data):\n            if not isinstance(data, (abc.Sequence, ExtensionArray)):\n                data = list(data)\n            if len(data) > 0:\n                if is_dataclass(data[0]):\n                    data = dataclasses_to_dicts(data)\n                if treat_as_nested(data):\n                    if columns is not None:\n                        # error: Argument 1 to \"ensure_index\" has incompatible type\n                        # \"Collection[Any]\"; expected \"Union[Union[Union[ExtensionArray,\n                        # ndarray], Index, Series], Sequence[Any]]\"\n                        columns = ensure_index(columns)  # type: ignore[arg-type]\n                    arrays, columns, index = nested_data_to_arrays(\n                        # error: Argument 3 to \"nested_data_to_arrays\" has incompatible\n                        # type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\n                        data,\n                        columns,\n                        index,  # type: ignore[arg-type]\n                        dtype,\n                    )\n                    mgr = arrays_to_mgr(\n                        arrays,\n                        columns,\n                        index,\n                        dtype=dtype,\n                        typ=manager,\n                    )\n                else:\n                    mgr = ndarray_to_mgr(\n                        data,\n                        index,\n                        columns,\n                        dtype=dtype,\n                        copy=copy,\n                        typ=manager,\n                    )\n            else:\n                mgr = dict_to_mgr(\n                    {},\n                    index,\n                    columns,\n                    dtype=dtype,\n                    typ=manager,\n                )\n        # For data is scalar\n        else:\n            if index is None or columns is None:\n                raise ValueError(\"DataFrame constructor not properly called!\")\n\n            # Argument 1 to \"ensure_index\" has incompatible type \"Collection[Any]\";\n            # expected \"Union[Union[Union[ExtensionArray, ndarray],\n            # Index, Series], Sequence[Any]]\"\n            index = ensure_index(index)  # type: ignore[arg-type]\n            # Argument 1 to \"ensure_index\" has incompatible type \"Collection[Any]\";\n            # expected \"Union[Union[Union[ExtensionArray, ndarray],\n            # Index, Series], Sequence[Any]]\"\n            columns = ensure_index(columns)  # type: ignore[arg-type]\n\n            if not dtype:\n                dtype, _ = infer_dtype_from_scalar(data, pandas_dtype=True)\n\n            # For data is a scalar extension dtype\n            if isinstance(dtype, ExtensionDtype):\n                # TODO(EA2D): special case not needed with 2D EAs\n\n                values = [\n                    construct_1d_arraylike_from_scalar(data, len(index), dtype)\n                    for _ in range(len(columns))\n                ]\n                mgr = arrays_to_mgr(values, columns, index, dtype=None, typ=manager)\n            else:\n                arr2d = construct_2d_arraylike_from_scalar(\n                    data,\n                    len(index),\n                    len(columns),\n                    dtype,\n                    copy,\n                )\n\n                mgr = ndarray_to_mgr(\n                    arr2d,\n                    index,\n                    columns,\n                    dtype=arr2d.dtype,\n                    copy=False,\n                    typ=manager,\n                )\n\n        # ensure correct Manager type according to settings\n        mgr = mgr_to_mgr(mgr, typ=manager)\n\n        NDFrame.__init__(self, mgr)\n\n    # ----------------------------------------------------------------------\n\n    @property\n    def axes(self) -> list[Index]:\n        \"\"\"\n        Return a list representing the axes of the DataFrame.\n\n        It has the row axis labels and column axis labels as the only members.\n        They are returned in that order.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.axes\n        [RangeIndex(start=0, stop=2, step=1), Index(['col1', 'col2'],\n        dtype='object')]\n        \"\"\"\n        return [self.index, self.columns]\n\n    @property\n    def shape(self) -> tuple[int, int]:\n        \"\"\"\n        Return a tuple representing the dimensionality of the DataFrame.\n\n        See Also\n        --------\n        ndarray.shape : Tuple of array dimensions.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.shape\n        (2, 2)\n\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4],\n        ...                    'col3': [5, 6]})\n        >>> df.shape\n        (2, 3)\n        \"\"\"\n        return len(self.index), len(self.columns)\n\n    @property\n    def _is_homogeneous_type(self) -> bool:\n        \"\"\"\n        Whether all the columns in a DataFrame have the same type.\n\n        Returns\n        -------\n        bool\n\n        See Also\n        --------\n        Index._is_homogeneous_type : Whether the object has a single\n            dtype.\n        MultiIndex._is_homogeneous_type : Whether all the levels of a\n            MultiIndex have the same dtype.\n\n        Examples\n        --------\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3, 4]})._is_homogeneous_type\n        True\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.0]})._is_homogeneous_type\n        False\n\n        Items with the same type but different sizes are considered\n        different types.\n\n        >>> DataFrame({\n        ...    \"A\": np.array([1, 2], dtype=np.int32),\n        ...    \"B\": np.array([1, 2], dtype=np.int64)})._is_homogeneous_type\n        False\n        \"\"\"\n        if isinstance(self._mgr, ArrayManager):\n            return len({arr.dtype for arr in self._mgr.arrays}) == 1\n        if self._mgr.any_extension_types:\n            return len({block.dtype for block in self._mgr.blocks}) == 1\n        else:\n            return not self._is_mixed_type\n\n    @property\n    def _can_fast_transpose(self) -> bool:\n        \"\"\"\n        Can we transpose this DataFrame without creating any new array objects.\n        \"\"\"\n        if isinstance(self._mgr, ArrayManager):\n            return False\n        blocks = self._mgr.blocks\n        if len(blocks) != 1:\n            return False\n\n        dtype = blocks[0].dtype\n        # TODO(EA2D) special case would be unnecessary with 2D EAs\n        return not is_1d_only_ea_dtype(dtype)\n\n    # error: Return type \"Union[ndarray, DatetimeArray, TimedeltaArray]\" of\n    # \"_values\" incompatible with return type \"ndarray\" in supertype \"NDFrame\"\n    @property\n    def _values(  # type: ignore[override]\n        self,\n    ) -> np.ndarray | DatetimeArray | TimedeltaArray:\n        \"\"\"\n        Analogue to ._values that may return a 2D ExtensionArray.\n        \"\"\"\n        self._consolidate_inplace()\n\n        mgr = self._mgr\n\n        if isinstance(mgr, ArrayManager):\n            if len(mgr.arrays) == 1 and not is_1d_only_ea_obj(mgr.arrays[0]):\n                # error: Item \"ExtensionArray\" of \"Union[ndarray, ExtensionArray]\"\n                # has no attribute \"reshape\"\n                return mgr.arrays[0].reshape(-1, 1)  # type: ignore[union-attr]\n            return self.values\n\n        blocks = mgr.blocks\n        if len(blocks) != 1:\n            return self.values\n\n        arr = blocks[0].values\n        if arr.ndim == 1:\n            # non-2D ExtensionArray\n            return self.values\n\n        # more generally, whatever we allow in NDArrayBackedExtensionBlock\n        arr = cast(\"np.ndarray | DatetimeArray | TimedeltaArray\", arr)\n        return arr.T\n\n    # ----------------------------------------------------------------------\n    # Rendering Methods\n\n    def _repr_fits_vertical_(self) -> bool:\n        \"\"\"\n        Check length against max_rows.\n        \"\"\"\n        max_rows = get_option(\"display.max_rows\")\n        return len(self) <= max_rows\n\n    def _repr_fits_horizontal_(self, ignore_width: bool = False) -> bool:\n        \"\"\"\n        Check if full repr fits in horizontal boundaries imposed by the display\n        options width and max_columns.\n\n        In case of non-interactive session, no boundaries apply.\n\n        `ignore_width` is here so ipynb+HTML output can behave the way\n        users expect. display.max_columns remains in effect.\n        GH3541, GH3573\n        \"\"\"\n        width, height = console.get_console_size()\n        max_columns = get_option(\"display.max_columns\")\n        nb_columns = len(self.columns)\n\n        # exceed max columns\n        if (max_columns and nb_columns > max_columns) or (\n            (not ignore_width) and width and nb_columns > (width // 2)\n        ):\n            return False\n\n        # used by repr_html under IPython notebook or scripts ignore terminal\n        # dims\n        if ignore_width or not console.in_interactive_session():\n            return True\n\n        if get_option(\"display.width\") is not None or console.in_ipython_frontend():\n            # check at least the column row for excessive width\n            max_rows = 1\n        else:\n            max_rows = get_option(\"display.max_rows\")\n\n        # when auto-detecting, so width=None and not in ipython front end\n        # check whether repr fits horizontal by actually checking\n        # the width of the rendered repr\n        buf = StringIO()\n\n        # only care about the stuff we'll actually print out\n        # and to_string on entire frame may be expensive\n        d = self\n\n        if max_rows is not None:  # unlimited rows\n            # min of two, where one may be None\n            d = d.iloc[: min(max_rows, len(d))]\n        else:\n            return True\n\n        d.to_string(buf=buf)\n        value = buf.getvalue()\n        repr_width = max(len(line) for line in value.split(\"\\n\"))\n\n        return repr_width < width\n\n    def _info_repr(self) -> bool:\n        \"\"\"\n        True if the repr should show the info view.\n        \"\"\"\n        info_repr_option = get_option(\"display.large_repr\") == \"info\"\n        return info_repr_option and not (\n            self._repr_fits_horizontal_() and self._repr_fits_vertical_()\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular DataFrame.\n        \"\"\"\n        if self._info_repr():\n            buf = StringIO()\n            self.info(buf=buf)\n            return buf.getvalue()\n\n        repr_params = fmt.get_dataframe_repr_params()\n        return self.to_string(**repr_params)\n\n    def _repr_html_(self) -> str | None:\n        \"\"\"\n        Return a html representation for a particular DataFrame.\n\n        Mainly for IPython notebook.\n        \"\"\"\n        if self._info_repr():\n            buf = StringIO()\n            self.info(buf=buf)\n            # need to escape the <class>, should be the first line.\n            val = buf.getvalue().replace(\"<\", r\"&lt;\", 1)\n            val = val.replace(\">\", r\"&gt;\", 1)\n            return \"<pre>\" + val + \"</pre>\"\n\n        if get_option(\"display.notebook_repr_html\"):\n            max_rows = get_option(\"display.max_rows\")\n            min_rows = get_option(\"display.min_rows\")\n            max_cols = get_option(\"display.max_columns\")\n            show_dimensions = get_option(\"display.show_dimensions\")\n\n            formatter = fmt.DataFrameFormatter(\n                self,\n                columns=None,\n                col_space=None,\n                na_rep=\"NaN\",\n                formatters=None,\n                float_format=None,\n                sparsify=None,\n                justify=None,\n                index_names=True,\n                header=True,\n                index=True,\n                bold_rows=True,\n                escape=True,\n                max_rows=max_rows,\n                min_rows=min_rows,\n                max_cols=max_cols,\n                show_dimensions=show_dimensions,\n                decimal=\".\",\n            )\n            return fmt.DataFrameRenderer(formatter).to_html(notebook=True)\n        else:\n            return None\n\n    @overload\n    def to_string(\n        self,\n        buf: None = ...,\n        columns: Sequence[str] | None = ...,\n        col_space: int | list[int] | dict[Hashable, int] | None = ...,\n        header: bool | Sequence[str] = ...,\n        index: bool = ...,\n        na_rep: str = ...,\n        formatters: fmt.FormattersType | None = ...,\n        float_format: fmt.FloatFormatType | None = ...,\n        sparsify: bool | None = ...,\n        index_names: bool = ...,\n        justify: str | None = ...,\n        max_rows: int | None = ...,\n        max_cols: int | None = ...,\n        show_dimensions: bool = ...,\n        decimal: str = ...,\n        line_width: int | None = ...,\n        min_rows: int | None = ...,\n        max_colwidth: int | None = ...,\n        encoding: str | None = ...,\n    ) -> str:\n        ...\n\n    @overload\n    def to_string(\n        self,\n        buf: FilePath | WriteBuffer[str],\n        columns: Sequence[str] | None = ...,\n        col_space: int | list[int] | dict[Hashable, int] | None = ...,\n        header: bool | Sequence[str] = ...,\n        index: bool = ...,\n        na_rep: str = ...,\n        formatters: fmt.FormattersType | None = ...,\n        float_format: fmt.FloatFormatType | None = ...,\n        sparsify: bool | None = ...,\n        index_names: bool = ...,\n        justify: str | None = ...,\n        max_rows: int | None = ...,\n        max_cols: int | None = ...,\n        show_dimensions: bool = ...,\n        decimal: str = ...,\n        line_width: int | None = ...,\n        min_rows: int | None = ...,\n        max_colwidth: int | None = ...,\n        encoding: str | None = ...,\n    ) -> None:\n        ...\n\n    @Substitution(\n        header_type=\"bool or sequence of strings\",\n        header=\"Write out the column names. If a list of strings \"\n        \"is given, it is assumed to be aliases for the \"\n        \"column names\",\n        col_space_type=\"int, list or dict of int\",\n        col_space=\"The minimum width of each column. If a list of ints is given \"\n        \"every integers corresponds with one column. If a dict is given, the key \"\n        \"references the column, while the value defines the space to use.\",\n    )\n    @Substitution(shared_params=fmt.common_docstring, returns=fmt.return_docstring)\n    def to_string(\n        self,\n        buf: FilePath | WriteBuffer[str] | None = None,\n        columns: Sequence[str] | None = None,\n        col_space: int | list[int] | dict[Hashable, int] | None = None,\n        header: bool | Sequence[str] = True,\n        index: bool = True,\n        na_rep: str = \"NaN\",\n        formatters: fmt.FormattersType | None = None,\n        float_format: fmt.FloatFormatType | None = None,\n        sparsify: bool | None = None,\n        index_names: bool = True,\n        justify: str | None = None,\n        max_rows: int | None = None,\n        max_cols: int | None = None,\n        show_dimensions: bool = False,\n        decimal: str = \".\",\n        line_width: int | None = None,\n        min_rows: int | None = None,\n        max_colwidth: int | None = None,\n        encoding: str | None = None,\n    ) -> str | None:\n        \"\"\"\n        Render a DataFrame to a console-friendly tabular output.\n        %(shared_params)s\n        line_width : int, optional\n            Width to wrap a line in characters.\n        min_rows : int, optional\n            The number of rows to display in the console in a truncated repr\n            (when number of rows is above `max_rows`).\n        max_colwidth : int, optional\n            Max width to truncate each column in characters. By default, no limit.\n\n            .. versionadded:: 1.0.0\n        encoding : str, default \"utf-8\"\n            Set character encoding.\n\n            .. versionadded:: 1.0\n        %(returns)s\n        See Also\n        --------\n        to_html : Convert DataFrame to HTML.\n\n        Examples\n        --------\n        >>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n        >>> df = pd.DataFrame(d)\n        >>> print(df.to_string())\n           col1  col2\n        0     1     4\n        1     2     5\n        2     3     6\n        \"\"\"\n        from pandas import option_context\n\n        with option_context(\"display.max_colwidth\", max_colwidth):\n            formatter = fmt.DataFrameFormatter(\n                self,\n                columns=columns,\n                col_space=col_space,\n                na_rep=na_rep,\n                formatters=formatters,\n                float_format=float_format,\n                sparsify=sparsify,\n                justify=justify,\n                index_names=index_names,\n                header=header,\n                index=index,\n                min_rows=min_rows,\n                max_rows=max_rows,\n                max_cols=max_cols,\n                show_dimensions=show_dimensions,\n                decimal=decimal,\n            )\n            return fmt.DataFrameRenderer(formatter).to_string(\n                buf=buf,\n                encoding=encoding,\n                line_width=line_width,\n            )\n\n    # ----------------------------------------------------------------------\n\n    @property\n    def style(self) -> Styler:\n        \"\"\"\n        Returns a Styler object.\n\n        Contains methods for building a styled HTML representation of the DataFrame.\n\n        See Also\n        --------\n        io.formats.style.Styler : Helps style a DataFrame or Series according to the\n            data with HTML and CSS.\n        \"\"\"\n        from pandas.io.formats.style import Styler\n\n        return Styler(self)\n\n    _shared_docs[\n        \"items\"\n    ] = r\"\"\"\n        Iterate over (column name, Series) pairs.\n\n        Iterates over the DataFrame columns, returning a tuple with\n        the column name and the content as a Series.\n\n        Yields\n        ------\n        label : object\n            The column names for the DataFrame being iterated over.\n        content : Series\n            The column entries belonging to each label, as a Series.\n\n        See Also\n        --------\n        DataFrame.iterrows : Iterate over DataFrame rows as\n            (index, Series) pairs.\n        DataFrame.itertuples : Iterate over DataFrame rows as namedtuples\n            of the values.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],\n        ...                   'population': [1864, 22000, 80000]},\n        ...                   index=['panda', 'polar', 'koala'])\n        >>> df\n                species   population\n        panda   bear      1864\n        polar   bear      22000\n        koala   marsupial 80000\n        >>> for label, content in df.items():\n        ...     print(f'label: {label}')\n        ...     print(f'content: {content}', sep='\\n')\n        ...\n        label: species\n        content:\n        panda         bear\n        polar         bear\n        koala    marsupial\n        Name: species, dtype: object\n        label: population\n        content:\n        panda     1864\n        polar    22000\n        koala    80000\n        Name: population, dtype: int64\n        \"\"\"\n\n    @Appender(_shared_docs[\"items\"])\n    def items(self) -> Iterable[tuple[Hashable, Series]]:\n        if self.columns.is_unique and hasattr(self, \"_item_cache\"):\n            for k in self.columns:\n                yield k, self._get_item_cache(k)\n        else:\n            for i, k in enumerate(self.columns):\n                yield k, self._ixs(i, axis=1)\n\n    @Appender(_shared_docs[\"items\"])\n    def iteritems(self) -> Iterable[tuple[Hashable, Series]]:\n        yield from self.items()\n\n    def iterrows(self) -> Iterable[tuple[Hashable, Series]]:\n        \"\"\"\n        Iterate over DataFrame rows as (index, Series) pairs.\n\n        Yields\n        ------\n        index : label or tuple of label\n            The index of the row. A tuple for a `MultiIndex`.\n        data : Series\n            The data of the row as a Series.\n\n        See Also\n        --------\n        DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values.\n        DataFrame.items : Iterate over (column name, Series) pairs.\n\n        Notes\n        -----\n        1. Because ``iterrows`` returns a Series for each row,\n           it does **not** preserve dtypes across the rows (dtypes are\n           preserved across columns for DataFrames). For example,\n\n           >>> df = pd.DataFrame([[1, 1.5]], columns=['int', 'float'])\n           >>> row = next(df.iterrows())[1]\n           >>> row\n           int      1.0\n           float    1.5\n           Name: 0, dtype: float64\n           >>> print(row['int'].dtype)\n           float64\n           >>> print(df['int'].dtype)\n           int64\n\n           To preserve dtypes while iterating over the rows, it is better\n           to use :meth:`itertuples` which returns namedtuples of the values\n           and which is generally faster than ``iterrows``.\n\n        2. You should **never modify** something you are iterating over.\n           This is not guaranteed to work in all cases. Depending on the\n           data types, the iterator returns a copy and not a view, and writing\n           to it will have no effect.\n        \"\"\"\n        columns = self.columns\n        klass = self._constructor_sliced\n        for k, v in zip(self.index, self.values):\n            s = klass(v, index=columns, name=k)\n            yield k, s\n\n    def itertuples(\n        self, index: bool = True, name: str | None = \"Pandas\"\n    ) -> Iterable[tuple[Any, ...]]:\n        \"\"\"\n        Iterate over DataFrame rows as namedtuples.\n\n        Parameters\n        ----------\n        index : bool, default True\n            If True, return the index as the first element of the tuple.\n        name : str or None, default \"Pandas\"\n            The name of the returned namedtuples or None to return regular\n            tuples.\n\n        Returns\n        -------\n        iterator\n            An object to iterate over namedtuples for each row in the\n            DataFrame with the first field possibly being the index and\n            following fields being the column values.\n\n        See Also\n        --------\n        DataFrame.iterrows : Iterate over DataFrame rows as (index, Series)\n            pairs.\n        DataFrame.items : Iterate over (column name, Series) pairs.\n\n        Notes\n        -----\n        The column names will be renamed to positional names if they are\n        invalid Python identifiers, repeated, or start with an underscore.\n        On python versions < 3.7 regular tuples are returned for DataFrames\n        with a large number of columns (>254).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]},\n        ...                   index=['dog', 'hawk'])\n        >>> df\n              num_legs  num_wings\n        dog          4          0\n        hawk         2          2\n        >>> for row in df.itertuples():\n        ...     print(row)\n        ...\n        Pandas(Index='dog', num_legs=4, num_wings=0)\n        Pandas(Index='hawk', num_legs=2, num_wings=2)\n\n        By setting the `index` parameter to False we can remove the index\n        as the first element of the tuple:\n\n        >>> for row in df.itertuples(index=False):\n        ...     print(row)\n        ...\n        Pandas(num_legs=4, num_wings=0)\n        Pandas(num_legs=2, num_wings=2)\n\n        With the `name` parameter set we set a custom name for the yielded\n        namedtuples:\n\n        >>> for row in df.itertuples(name='Animal'):\n        ...     print(row)\n        ...\n        Animal(Index='dog', num_legs=4, num_wings=0)\n        Animal(Index='hawk', num_legs=2, num_wings=2)\n        \"\"\"\n        arrays = []\n        fields = list(self.columns)\n        if index:\n            arrays.append(self.index)\n            fields.insert(0, \"Index\")\n\n        # use integer indexing because of possible duplicate column names\n        arrays.extend(self.iloc[:, k] for k in range(len(self.columns)))\n\n        if name is not None:\n            # https://github.com/python/mypy/issues/9046\n            # error: namedtuple() expects a string literal as the first argument\n            itertuple = collections.namedtuple(  # type: ignore[misc]\n                name, fields, rename=True\n            )\n            return map(itertuple._make, zip(*arrays))\n\n        # fallback to regular tuples\n        return zip(*arrays)\n\n    def __len__(self) -> int:\n        \"\"\"\n        Returns length of info axis, but here we use the index.\n        \"\"\"\n        return len(self.index)\n\n    @overload\n    def dot(self, other: Series) -> Series:\n        ...\n\n    @overload\n    def dot(self, other: DataFrame | Index | ArrayLike) -> DataFrame:\n        ...\n\n    def dot(self, other: AnyArrayLike | DataFrame) -> DataFrame | Series:\n        \"\"\"\n        Compute the matrix multiplication between the DataFrame and other.\n\n        This method computes the matrix product between the DataFrame and the\n        values of an other Series, DataFrame or a numpy array.\n\n        It can also be called using ``self @ other`` in Python >= 3.5.\n\n        Parameters\n        ----------\n        other : Series, DataFrame or array-like\n            The other object to compute the matrix product with.\n\n        Returns\n        -------\n        Series or DataFrame\n            If other is a Series, return the matrix product between self and\n            other as a Series. If other is a DataFrame or a numpy.array, return\n            the matrix product of self and other in a DataFrame of a np.array.\n\n        See Also\n        --------\n        Series.dot: Similar method for Series.\n\n        Notes\n        -----\n        The dimensions of DataFrame and other must be compatible in order to\n        compute the matrix multiplication. In addition, the column names of\n        DataFrame and the index of other must contain the same values, as they\n        will be aligned prior to the multiplication.\n\n        The dot method for Series computes the inner product, instead of the\n        matrix product here.\n\n        Examples\n        --------\n        Here we multiply a DataFrame with a Series.\n\n        >>> df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]])\n        >>> s = pd.Series([1, 1, 2, 1])\n        >>> df.dot(s)\n        0    -4\n        1     5\n        dtype: int64\n\n        Here we multiply a DataFrame with another DataFrame.\n\n        >>> other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]])\n        >>> df.dot(other)\n            0   1\n        0   1   4\n        1   2   2\n\n        Note that the dot method give the same result as @\n\n        >>> df @ other\n            0   1\n        0   1   4\n        1   2   2\n\n        The dot method works also if other is an np.array.\n\n        >>> arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]])\n        >>> df.dot(arr)\n            0   1\n        0   1   4\n        1   2   2\n\n        Note how shuffling of the objects does not change the result.\n\n        >>> s2 = s.reindex([1, 0, 2, 3])\n        >>> df.dot(s2)\n        0    -4\n        1     5\n        dtype: int64\n        \"\"\"\n        if isinstance(other, (Series, DataFrame)):\n            common = self.columns.union(other.index)\n            if len(common) > len(self.columns) or len(common) > len(other.index):\n                raise ValueError(\"matrices are not aligned\")\n\n            left = self.reindex(columns=common, copy=False)\n            right = other.reindex(index=common, copy=False)\n            lvals = left.values\n            rvals = right._values\n        else:\n            left = self\n            lvals = self.values\n            rvals = np.asarray(other)\n            if lvals.shape[1] != rvals.shape[0]:\n                raise ValueError(\n                    f\"Dot product shape mismatch, {lvals.shape} vs {rvals.shape}\"\n                )\n\n        if isinstance(other, DataFrame):\n            return self._constructor(\n                np.dot(lvals, rvals), index=left.index, columns=other.columns\n            )\n        elif isinstance(other, Series):\n            return self._constructor_sliced(np.dot(lvals, rvals), index=left.index)\n        elif isinstance(rvals, (np.ndarray, Index)):\n            result = np.dot(lvals, rvals)\n            if result.ndim == 2:\n                return self._constructor(result, index=left.index)\n            else:\n                return self._constructor_sliced(result, index=left.index)\n        else:  # pragma: no cover\n            raise TypeError(f\"unsupported type: {type(other)}\")\n\n    @overload\n    def __matmul__(self, other: Series) -> Series:\n        ...\n\n    @overload\n    def __matmul__(\n        self, other: AnyArrayLike | DataFrame | Series\n    ) -> DataFrame | Series:\n        ...\n\n    def __matmul__(\n        self, other: AnyArrayLike | DataFrame | Series\n    ) -> DataFrame | Series:\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(other)\n\n    def __rmatmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        try:\n            return self.T.dot(np.transpose(other)).T\n        except ValueError as err:\n            if \"shape mismatch\" not in str(err):\n                raise\n            # GH#21581 give exception message for original shapes\n            msg = f\"shapes {np.shape(other)} and {self.shape} not aligned\"\n            raise ValueError(msg) from err\n\n    # ----------------------------------------------------------------------\n    # IO methods (to / from other formats)\n\n    @classmethod\n    def from_dict(\n        cls,\n        data,\n        orient: str = \"columns\",\n        dtype: Dtype | None = None,\n        columns=None,\n    ) -> DataFrame:\n        \"\"\"\n        Construct DataFrame from dict of array-like or dicts.\n\n        Creates DataFrame object from dictionary by columns or by index\n        allowing dtype specification.\n\n        Parameters\n        ----------\n        data : dict\n            Of the form {field : array-like} or {field : dict}.\n        orient : {'columns', 'index', 'tight'}, default 'columns'\n            The \"orientation\" of the data. If the keys of the passed dict\n            should be the columns of the resulting DataFrame, pass 'columns'\n            (default). Otherwise if the keys should be rows, pass 'index'.\n            If 'tight', assume a dict with keys ['index', 'columns', 'data',\n            'index_names', 'column_names'].\n\n            .. versionadded:: 1.4.0\n               'tight' as an allowed value for the ``orient`` argument\n\n        dtype : dtype, default None\n            Data type to force, otherwise infer.\n        columns : list, default None\n            Column labels to use when ``orient='index'``. Raises a ValueError\n            if used with ``orient='columns'`` or ``orient='tight'``.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.from_records : DataFrame from structured ndarray, sequence\n            of tuples or dicts, or DataFrame.\n        DataFrame : DataFrame object creation using constructor.\n        DataFrame.to_dict : Convert the DataFrame to a dictionary.\n\n        Examples\n        --------\n        By default the keys of the dict become the DataFrame columns:\n\n        >>> data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}\n        >>> pd.DataFrame.from_dict(data)\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n\n        Specify ``orient='index'`` to create the DataFrame using dictionary\n        keys as rows:\n\n        >>> data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']}\n        >>> pd.DataFrame.from_dict(data, orient='index')\n               0  1  2  3\n        row_1  3  2  1  0\n        row_2  a  b  c  d\n\n        When using the 'index' orientation, the column names can be\n        specified manually:\n\n        >>> pd.DataFrame.from_dict(data, orient='index',\n        ...                        columns=['A', 'B', 'C', 'D'])\n               A  B  C  D\n        row_1  3  2  1  0\n        row_2  a  b  c  d\n\n        Specify ``orient='tight'`` to create the DataFrame using a 'tight'\n        format:\n\n        >>> data = {'index': [('a', 'b'), ('a', 'c')],\n        ...         'columns': [('x', 1), ('y', 2)],\n        ...         'data': [[1, 3], [2, 4]],\n        ...         'index_names': ['n1', 'n2'],\n        ...         'column_names': ['z1', 'z2']}\n        >>> pd.DataFrame.from_dict(data, orient='tight')\n        z1     x  y\n        z2     1  2\n        n1 n2\n        a  b   1  3\n           c   2  4\n        \"\"\"\n        index = None\n        orient = orient.lower()\n        if orient == \"index\":\n            if len(data) > 0:\n                # TODO speed up Series case\n                if isinstance(list(data.values())[0], (Series, dict)):\n                    data = _from_nested_dict(data)\n                else:\n                    data, index = list(data.values()), list(data.keys())\n        elif orient == \"columns\" or orient == \"tight\":\n            if columns is not None:\n                raise ValueError(f\"cannot use columns parameter with orient='{orient}'\")\n        else:  # pragma: no cover\n            raise ValueError(\"only recognize index or columns for orient\")\n\n        if orient != \"tight\":\n            return cls(data, index=index, columns=columns, dtype=dtype)\n        else:\n            realdata = data[\"data\"]\n\n            def create_index(indexlist, namelist):\n                index: Index\n                if len(namelist) > 1:\n                    index = MultiIndex.from_tuples(indexlist, names=namelist)\n                else:\n                    index = Index(indexlist, name=namelist[0])\n                return index\n\n            index = create_index(data[\"index\"], data[\"index_names\"])\n            columns = create_index(data[\"columns\"], data[\"column_names\"])\n            return cls(realdata, index=index, columns=columns, dtype=dtype)\n\n    def to_numpy(\n        self,\n        dtype: npt.DTypeLike | None = None,\n        copy: bool = False,\n        na_value=lib.no_default,\n    ) -> np.ndarray:\n        \"\"\"\n        Convert the DataFrame to a NumPy array.\n\n        By default, the dtype of the returned array will be the common NumPy\n        dtype of all types in the DataFrame. For example, if the dtypes are\n        ``float16`` and ``float32``, the results dtype will be ``float32``.\n        This may require copying data and coercing values, which may be\n        expensive.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to pass to :meth:`numpy.asarray`.\n        copy : bool, default False\n            Whether to ensure that the returned value is not a view on\n            another array. Note that ``copy=False`` does not *ensure* that\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n            a copy is made, even if not strictly necessary.\n        na_value : Any, optional\n            The value to use for missing values. The default value depends\n            on `dtype` and the dtypes of the DataFrame columns.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        numpy.ndarray\n\n        See Also\n        --------\n        Series.to_numpy : Similar method for Series.\n\n        Examples\n        --------\n        >>> pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy()\n        array([[1, 3],\n               [2, 4]])\n\n        With heterogeneous data, the lowest common type will have to\n        be used.\n\n        >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]})\n        >>> df.to_numpy()\n        array([[1. , 3. ],\n               [2. , 4.5]])\n\n        For a mix of numeric and non-numeric types, the output array will\n        have object dtype.\n\n        >>> df['C'] = pd.date_range('2000', periods=2)\n        >>> df.to_numpy()\n        array([[1, 3.0, Timestamp('2000-01-01 00:00:00')],\n               [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object)\n        \"\"\"\n        self._consolidate_inplace()\n        if dtype is not None:\n            dtype = np.dtype(dtype)\n        result = self._mgr.as_array(dtype=dtype, copy=copy, na_value=na_value)\n        if result.dtype is not dtype:\n            result = np.array(result, dtype=dtype, copy=False)\n\n        return result\n\n    def to_dict(self, orient: str = \"dict\", into=dict):\n        \"\"\"\n        Convert the DataFrame to a dictionary.\n\n        The type of the key-value pairs can be customized with the parameters\n        (see below).\n\n        Parameters\n        ----------\n        orient : str {'dict', 'list', 'series', 'split', 'records', 'index'}\n            Determines the type of the values of the dictionary.\n\n            - 'dict' (default) : dict like {column -> {index -> value}}\n            - 'list' : dict like {column -> [values]}\n            - 'series' : dict like {column -> Series(values)}\n            - 'split' : dict like\n              {'index' -> [index], 'columns' -> [columns], 'data' -> [values]}\n            - 'tight' : dict like\n              {'index' -> [index], 'columns' -> [columns], 'data' -> [values],\n              'index_names' -> [index.names], 'column_names' -> [column.names]}\n            - 'records' : list like\n              [{column -> value}, ... , {column -> value}]\n            - 'index' : dict like {index -> {column -> value}}\n\n            Abbreviations are allowed. `s` indicates `series` and `sp`\n            indicates `split`.\n\n            .. versionadded:: 1.4.0\n                'tight' as an allowed value for the ``orient`` argument\n\n        into : class, default dict\n            The collections.abc.Mapping subclass used for all Mappings\n            in the return value.  Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        Returns\n        -------\n        dict, list or collections.abc.Mapping\n            Return a collections.abc.Mapping object representing the DataFrame.\n            The resulting transformation depends on the `orient` parameter.\n\n        See Also\n        --------\n        DataFrame.from_dict: Create a DataFrame from a dictionary.\n        DataFrame.to_json: Convert a DataFrame to JSON format.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2],\n        ...                    'col2': [0.5, 0.75]},\n        ...                   index=['row1', 'row2'])\n        >>> df\n              col1  col2\n        row1     1  0.50\n        row2     2  0.75\n        >>> df.to_dict()\n        {'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}}\n\n        You can specify the return orientation.\n\n        >>> df.to_dict('series')\n        {'col1': row1    1\n                 row2    2\n        Name: col1, dtype: int64,\n        'col2': row1    0.50\n                row2    0.75\n        Name: col2, dtype: float64}\n\n        >>> df.to_dict('split')\n        {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],\n         'data': [[1, 0.5], [2, 0.75]]}\n\n        >>> df.to_dict('records')\n        [{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}]\n\n        >>> df.to_dict('index')\n        {'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}}\n\n        >>> df.to_dict('tight')\n        {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],\n         'data': [[1, 0.5], [2, 0.75]], 'index_names': [None], 'column_names': [None]}\n\n        You can also specify the mapping type.\n\n        >>> from collections import OrderedDict, defaultdict\n        >>> df.to_dict(into=OrderedDict)\n        OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])),\n                     ('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))])\n\n        If you want a `defaultdict`, you need to initialize it:\n\n        >>> dd = defaultdict(list)\n        >>> df.to_dict('records', into=dd)\n        [defaultdict(<class 'list'>, {'col1': 1, 'col2': 0.5}),\n         defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})]\n        \"\"\"\n        if not self.columns.is_unique:\n            warnings.warn(\n                \"DataFrame columns are not unique, some columns will be omitted.\",\n                UserWarning,\n                stacklevel=find_stack_level(),\n            )\n        # GH16122\n        into_c = com.standardize_mapping(into)\n\n        orient = orient.lower()\n        # GH32515\n        if orient.startswith((\"d\", \"l\", \"s\", \"r\", \"i\")) and orient not in {\n            \"dict\",\n            \"list\",\n            \"series\",\n            \"split\",\n            \"records\",\n            \"index\",\n        }:\n            warnings.warn(\n                \"Using short name for 'orient' is deprecated. Only the \"\n                \"options: ('dict', list, 'series', 'split', 'records', 'index') \"\n                \"will be used in a future version. Use one of the above \"\n                \"to silence this warning.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n            if orient.startswith(\"d\"):\n                orient = \"dict\"\n            elif orient.startswith(\"l\"):\n                orient = \"list\"\n            elif orient.startswith(\"sp\"):\n                orient = \"split\"\n            elif orient.startswith(\"s\"):\n                orient = \"series\"\n            elif orient.startswith(\"r\"):\n                orient = \"records\"\n            elif orient.startswith(\"i\"):\n                orient = \"index\"\n\n        if orient == \"dict\":\n            return into_c((k, v.to_dict(into)) for k, v in self.items())\n\n        elif orient == \"list\":\n            return into_c((k, v.tolist()) for k, v in self.items())\n\n        elif orient == \"split\":\n            return into_c(\n                (\n                    (\"index\", self.index.tolist()),\n                    (\"columns\", self.columns.tolist()),\n                    (\n                        \"data\",\n                        [\n                            list(map(maybe_box_native, t))\n                            for t in self.itertuples(index=False, name=None)\n                        ],\n                    ),\n                )\n            )\n\n        elif orient == \"tight\":\n            return into_c(\n                (\n                    (\"index\", self.index.tolist()),\n                    (\"columns\", self.columns.tolist()),\n                    (\n                        \"data\",\n                        [\n                            list(map(maybe_box_native, t))\n                            for t in self.itertuples(index=False, name=None)\n                        ],\n                    ),\n                    (\"index_names\", list(self.index.names)),\n                    (\"column_names\", list(self.columns.names)),\n                )\n            )\n\n        elif orient == \"series\":\n            return into_c((k, v) for k, v in self.items())\n\n        elif orient == \"records\":\n            columns = self.columns.tolist()\n            rows = (\n                dict(zip(columns, row))\n                for row in self.itertuples(index=False, name=None)\n            )\n            return [\n                into_c((k, maybe_box_native(v)) for k, v in row.items()) for row in rows\n            ]\n\n        elif orient == \"index\":\n            if not self.index.is_unique:\n                raise ValueError(\"DataFrame index must be unique for orient='index'.\")\n            return into_c(\n                (t[0], dict(zip(self.columns, t[1:])))\n                for t in self.itertuples(name=None)\n            )\n\n        else:\n            raise ValueError(f\"orient '{orient}' not understood\")\n\n    def to_gbq(\n        self,\n        destination_table: str,\n        project_id: str | None = None,\n        chunksize: int | None = None,\n        reauth: bool = False,\n        if_exists: str = \"fail\",\n        auth_local_webserver: bool = False,\n        table_schema: list[dict[str, str]] | None = None,\n        location: str | None = None,\n        progress_bar: bool = True,\n        credentials=None,\n    ) -> None:\n        \"\"\"\n        Write a DataFrame to a Google BigQuery table.\n\n        This function requires the `pandas-gbq package\n        <https://pandas-gbq.readthedocs.io>`__.\n\n        See the `How to authenticate with Google BigQuery\n        <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html>`__\n        guide for authentication instructions.\n\n        Parameters\n        ----------\n        destination_table : str\n            Name of table to be written, in the form ``dataset.tablename``.\n        project_id : str, optional\n            Google BigQuery Account project ID. Optional when available from\n            the environment.\n        chunksize : int, optional\n            Number of rows to be inserted in each chunk from the dataframe.\n            Set to ``None`` to load the whole dataframe at once.\n        reauth : bool, default False\n            Force Google BigQuery to re-authenticate the user. This is useful\n            if multiple accounts are used.\n        if_exists : str, default 'fail'\n            Behavior when the destination table exists. Value can be one of:\n\n            ``'fail'``\n                If table exists raise pandas_gbq.gbq.TableCreationError.\n            ``'replace'``\n                If table exists, drop it, recreate it, and insert data.\n            ``'append'``\n                If table exists, insert data. Create if does not exist.\n        auth_local_webserver : bool, default False\n            Use the `local webserver flow`_ instead of the `console flow`_\n            when getting user credentials.\n\n            .. _local webserver flow:\n                https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server\n            .. _console flow:\n                https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console\n\n            *New in version 0.2.0 of pandas-gbq*.\n        table_schema : list of dicts, optional\n            List of BigQuery table fields to which according DataFrame\n            columns conform to, e.g. ``[{'name': 'col1', 'type':\n            'STRING'},...]``. If schema is not provided, it will be\n            generated according to dtypes of DataFrame columns. See\n            BigQuery API documentation on available names of a field.\n\n            *New in version 0.3.1 of pandas-gbq*.\n        location : str, optional\n            Location where the load job should run. See the `BigQuery locations\n            documentation\n            <https://cloud.google.com/bigquery/docs/dataset-locations>`__ for a\n            list of available locations. The location must match that of the\n            target dataset.\n\n            *New in version 0.5.0 of pandas-gbq*.\n        progress_bar : bool, default True\n            Use the library `tqdm` to show the progress bar for the upload,\n            chunk by chunk.\n\n            *New in version 0.5.0 of pandas-gbq*.\n        credentials : google.auth.credentials.Credentials, optional\n            Credentials for accessing Google APIs. Use this parameter to\n            override default credentials, such as to use Compute Engine\n            :class:`google.auth.compute_engine.Credentials` or Service\n            Account :class:`google.oauth2.service_account.Credentials`\n            directly.\n\n            *New in version 0.8.0 of pandas-gbq*.\n\n        See Also\n        --------\n        pandas_gbq.to_gbq : This function in the pandas-gbq library.\n        read_gbq : Read a DataFrame from Google BigQuery.\n        \"\"\"\n        from pandas.io import gbq\n\n        gbq.to_gbq(\n            self,\n            destination_table,\n            project_id=project_id,\n            chunksize=chunksize,\n            reauth=reauth,\n            if_exists=if_exists,\n            auth_local_webserver=auth_local_webserver,\n            table_schema=table_schema,\n            location=location,\n            progress_bar=progress_bar,\n            credentials=credentials,\n        )\n\n    @classmethod\n    def from_records(\n        cls,\n        data,\n        index=None,\n        exclude=None,\n        columns=None,\n        coerce_float: bool = False,\n        nrows: int | None = None,\n    ) -> DataFrame:\n        \"\"\"\n        Convert structured or record ndarray to DataFrame.\n\n        Creates a DataFrame object from a structured ndarray, sequence of\n        tuples or dicts, or DataFrame.\n\n        Parameters\n        ----------\n        data : structured ndarray, sequence of tuples or dicts, or DataFrame\n            Structured input data.\n        index : str, list of fields, array-like\n            Field of array to use as the index, alternately a specific set of\n            input labels to use.\n        exclude : sequence, default None\n            Columns or fields to exclude.\n        columns : sequence, default None\n            Column names to use. If the passed data do not have names\n            associated with them, this argument provides names for the\n            columns. Otherwise this argument indicates the order of the columns\n            in the result (any names not found in the data will become all-NA\n            columns).\n        coerce_float : bool, default False\n            Attempt to convert values of non-string, non-numeric objects (like\n            decimal.Decimal) to floating point, useful for SQL result sets.\n        nrows : int, default None\n            Number of rows to read if data is an iterator.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.from_dict : DataFrame from dict of array-like or dicts.\n        DataFrame : DataFrame object creation using constructor.\n\n        Examples\n        --------\n        Data can be provided as a structured ndarray:\n\n        >>> data = np.array([(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')],\n        ...                 dtype=[('col_1', 'i4'), ('col_2', 'U1')])\n        >>> pd.DataFrame.from_records(data)\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n\n        Data can be provided as a list of dicts:\n\n        >>> data = [{'col_1': 3, 'col_2': 'a'},\n        ...         {'col_1': 2, 'col_2': 'b'},\n        ...         {'col_1': 1, 'col_2': 'c'},\n        ...         {'col_1': 0, 'col_2': 'd'}]\n        >>> pd.DataFrame.from_records(data)\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n\n        Data can be provided as a list of tuples with corresponding columns:\n\n        >>> data = [(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')]\n        >>> pd.DataFrame.from_records(data, columns=['col_1', 'col_2'])\n           col_1 col_2\n        0      3     a\n        1      2     b\n        2      1     c\n        3      0     d\n        \"\"\"\n        result_index = None\n\n        # Make a copy of the input columns so we can modify it\n        if columns is not None:\n            columns = ensure_index(columns)\n\n        def maybe_reorder(\n            arrays: list[ArrayLike], arr_columns: Index, columns: Index, index\n        ) -> tuple[list[ArrayLike], Index, Index | None]:\n            \"\"\"\n            If our desired 'columns' do not match the data's pre-existing 'arr_columns',\n            we re-order our arrays.  This is like a pre-emptive (cheap) reindex.\n            \"\"\"\n            if len(arrays):\n                length = len(arrays[0])\n            else:\n                length = 0\n\n            result_index = None\n            if len(arrays) == 0 and index is None and length == 0:\n                # for backward compat use an object Index instead of RangeIndex\n                result_index = Index([])\n\n            arrays, arr_columns = reorder_arrays(arrays, arr_columns, columns, length)\n            return arrays, arr_columns, result_index\n\n        if is_iterator(data):\n            if nrows == 0:\n                return cls()\n\n            try:\n                first_row = next(data)\n            except StopIteration:\n                return cls(index=index, columns=columns)\n\n            dtype = None\n            if hasattr(first_row, \"dtype\") and first_row.dtype.names:\n                dtype = first_row.dtype\n\n            values = [first_row]\n\n            if nrows is None:\n                values += data\n            else:\n                values.extend(itertools.islice(data, nrows - 1))\n\n            if dtype is not None:\n                data = np.array(values, dtype=dtype)\n            else:\n                data = values\n\n        if isinstance(data, dict):\n            if columns is None:\n                columns = arr_columns = ensure_index(sorted(data))\n                arrays = [data[k] for k in columns]\n            else:\n                arrays = []\n                arr_columns_list = []\n                for k, v in data.items():\n                    if k in columns:\n                        arr_columns_list.append(k)\n                        arrays.append(v)\n\n                arr_columns = Index(arr_columns_list)\n                arrays, arr_columns, result_index = maybe_reorder(\n                    arrays, arr_columns, columns, index\n                )\n\n        elif isinstance(data, (np.ndarray, DataFrame)):\n            arrays, columns = to_arrays(data, columns)\n            arr_columns = columns\n        else:\n            arrays, arr_columns = to_arrays(data, columns)\n            if coerce_float:\n                for i, arr in enumerate(arrays):\n                    if arr.dtype == object:\n                        # error: Argument 1 to \"maybe_convert_objects\" has\n                        # incompatible type \"Union[ExtensionArray, ndarray]\";\n                        # expected \"ndarray\"\n                        arrays[i] = lib.maybe_convert_objects(\n                            arr,  # type: ignore[arg-type]\n                            try_float=True,\n                        )\n\n            arr_columns = ensure_index(arr_columns)\n            if columns is None:\n                columns = arr_columns\n            else:\n                arrays, arr_columns, result_index = maybe_reorder(\n                    arrays, arr_columns, columns, index\n                )\n\n        if exclude is None:\n            exclude = set()\n        else:\n            exclude = set(exclude)\n\n        if index is not None:\n            if isinstance(index, str) or not hasattr(index, \"__iter__\"):\n                i = columns.get_loc(index)\n                exclude.add(index)\n                if len(arrays) > 0:\n                    result_index = Index(arrays[i], name=index)\n                else:\n                    result_index = Index([], name=index)\n            else:\n                try:\n                    index_data = [arrays[arr_columns.get_loc(field)] for field in index]\n                except (KeyError, TypeError):\n                    # raised by get_loc, see GH#29258\n                    result_index = index\n                else:\n                    result_index = ensure_index_from_sequences(index_data, names=index)\n                    exclude.update(index)\n\n        if any(exclude):\n            arr_exclude = [x for x in exclude if x in arr_columns]\n            to_remove = [arr_columns.get_loc(col) for col in arr_exclude]\n            arrays = [v for i, v in enumerate(arrays) if i not in to_remove]\n\n            columns = columns.drop(exclude)\n\n        manager = get_option(\"mode.data_manager\")\n        mgr = arrays_to_mgr(arrays, columns, result_index, typ=manager)\n\n        return cls(mgr)\n\n    def to_records(\n        self, index=True, column_dtypes=None, index_dtypes=None\n    ) -> np.recarray:\n        \"\"\"\n        Convert DataFrame to a NumPy record array.\n\n        Index will be included as the first field of the record array if\n        requested.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Include index in resulting record array, stored in 'index'\n            field or using the index label, if set.\n        column_dtypes : str, type, dict, default None\n            If a string or type, the data type to store all columns. If\n            a dictionary, a mapping of column names and indices (zero-indexed)\n            to specific data types.\n        index_dtypes : str, type, dict, default None\n            If a string or type, the data type to store all index levels. If\n            a dictionary, a mapping of index level names and indices\n            (zero-indexed) to specific data types.\n\n            This mapping is applied only if `index=True`.\n\n        Returns\n        -------\n        numpy.recarray\n            NumPy ndarray with the DataFrame labels as fields and each row\n            of the DataFrame as entries.\n\n        See Also\n        --------\n        DataFrame.from_records: Convert structured or record ndarray\n            to DataFrame.\n        numpy.recarray: An ndarray that allows field access using\n            attributes, analogous to typed columns in a\n            spreadsheet.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]},\n        ...                   index=['a', 'b'])\n        >>> df\n           A     B\n        a  1  0.50\n        b  2  0.75\n        >>> df.to_records()\n        rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n                  dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')])\n\n        If the DataFrame index has no label then the recarray field name\n        is set to 'index'. If the index has a label then this is used as the\n        field name:\n\n        >>> df.index = df.index.rename(\"I\")\n        >>> df.to_records()\n        rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n                  dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')])\n\n        The index can be excluded from the record array:\n\n        >>> df.to_records(index=False)\n        rec.array([(1, 0.5 ), (2, 0.75)],\n                  dtype=[('A', '<i8'), ('B', '<f8')])\n\n        Data types can be specified for the columns:\n\n        >>> df.to_records(column_dtypes={\"A\": \"int32\"})\n        rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n                  dtype=[('I', 'O'), ('A', '<i4'), ('B', '<f8')])\n\n        As well as for the index:\n\n        >>> df.to_records(index_dtypes=\"<S2\")\n        rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n                  dtype=[('I', 'S2'), ('A', '<i8'), ('B', '<f8')])\n\n        >>> index_dtypes = f\"<S{df.index.str.len().max()}\"\n        >>> df.to_records(index_dtypes=index_dtypes)\n        rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n                  dtype=[('I', 'S1'), ('A', '<i8'), ('B', '<f8')])\n        \"\"\"\n        if index:\n            if isinstance(self.index, MultiIndex):\n                # array of tuples to numpy cols. copy copy copy\n                ix_vals = list(map(np.array, zip(*self.index._values)))\n            else:\n                # error: List item 0 has incompatible type \"ArrayLike\"; expected\n                # \"ndarray\"\n                ix_vals = [self.index.values]  # type: ignore[list-item]\n\n            arrays = ix_vals + [\n                np.asarray(self.iloc[:, i]) for i in range(len(self.columns))\n            ]\n\n            index_names = list(self.index.names)\n\n            if isinstance(self.index, MultiIndex):\n                count = 0\n                for i, n in enumerate(index_names):\n                    if n is None:\n                        index_names[i] = f\"level_{count}\"\n                        count += 1\n            elif index_names[0] is None:\n                index_names = [\"index\"]\n\n            names = [str(name) for name in itertools.chain(index_names, self.columns)]\n        else:\n            arrays = [np.asarray(self.iloc[:, i]) for i in range(len(self.columns))]\n            names = [str(c) for c in self.columns]\n            index_names = []\n\n        index_len = len(index_names)\n        formats = []\n\n        for i, v in enumerate(arrays):\n            index = i\n\n            # When the names and arrays are collected, we\n            # first collect those in the DataFrame's index,\n            # followed by those in its columns.\n            #\n            # Thus, the total length of the array is:\n            # len(index_names) + len(DataFrame.columns).\n            #\n            # This check allows us to see whether we are\n            # handling a name / array in the index or column.\n            if index < index_len:\n                dtype_mapping = index_dtypes\n                name = index_names[index]\n            else:\n                index -= index_len\n                dtype_mapping = column_dtypes\n                name = self.columns[index]\n\n            # We have a dictionary, so we get the data type\n            # associated with the index or column (which can\n            # be denoted by its name in the DataFrame or its\n            # position in DataFrame's array of indices or\n            # columns, whichever is applicable.\n            if is_dict_like(dtype_mapping):\n                if name in dtype_mapping:\n                    dtype_mapping = dtype_mapping[name]\n                elif index in dtype_mapping:\n                    dtype_mapping = dtype_mapping[index]\n                else:\n                    dtype_mapping = None\n\n            # If no mapping can be found, use the array's\n            # dtype attribute for formatting.\n            #\n            # A valid dtype must either be a type or\n            # string naming a type.\n            if dtype_mapping is None:\n                formats.append(v.dtype)\n            elif isinstance(dtype_mapping, (type, np.dtype, str)):\n                formats.append(dtype_mapping)\n            else:\n                element = \"row\" if i < index_len else \"column\"\n                msg = f\"Invalid dtype {dtype_mapping} specified for {element} {name}\"\n                raise ValueError(msg)\n\n        return np.rec.fromarrays(arrays, dtype={\"names\": names, \"formats\": formats})\n\n    @classmethod\n    def _from_arrays(\n        cls,\n        arrays,\n        columns,\n        index,\n        dtype: Dtype | None = None,\n        verify_integrity: bool = True,\n    ) -> DataFrame:\n        \"\"\"\n        Create DataFrame from a list of arrays corresponding to the columns.\n\n        Parameters\n        ----------\n        arrays : list-like of arrays\n            Each array in the list corresponds to one column, in order.\n        columns : list-like, Index\n            The column names for the resulting DataFrame.\n        index : list-like, Index\n            The rows labels for the resulting DataFrame.\n        dtype : dtype, optional\n            Optional dtype to enforce for all arrays.\n        verify_integrity : bool, default True\n            Validate and homogenize all input. If set to False, it is assumed\n            that all elements of `arrays` are actual arrays how they will be\n            stored in a block (numpy ndarray or ExtensionArray), have the same\n            length as and are aligned with the index, and that `columns` and\n            `index` are ensured to be an Index object.\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n\n        manager = get_option(\"mode.data_manager\")\n        columns = ensure_index(columns)\n        if len(columns) != len(arrays):\n            raise ValueError(\"len(columns) must match len(arrays)\")\n        mgr = arrays_to_mgr(\n            arrays,\n            columns,\n            index,\n            dtype=dtype,\n            verify_integrity=verify_integrity,\n            typ=manager,\n        )\n        return cls(mgr)\n\n    @doc(storage_options=generic._shared_docs[\"storage_options\"])\n    @deprecate_kwarg(old_arg_name=\"fname\", new_arg_name=\"path\")\n    def to_stata(\n        self,\n        path: FilePath | WriteBuffer[bytes],\n        convert_dates: dict[Hashable, str] | None = None,\n        write_index: bool = True,\n        byteorder: str | None = None,\n        time_stamp: datetime.datetime | None = None,\n        data_label: str | None = None,\n        variable_labels: dict[Hashable, str] | None = None,\n        version: int | None = 114,\n        convert_strl: Sequence[Hashable] | None = None,\n        compression: CompressionOptions = \"infer\",\n        storage_options: StorageOptions = None,\n        *,\n        value_labels: dict[Hashable, dict[float | int, str]] | None = None,\n    ) -> None:\n        \"\"\"\n        Export DataFrame object to Stata dta format.\n\n        Writes the DataFrame to a Stata dataset file.\n        \"dta\" files contain a Stata dataset.\n\n        Parameters\n        ----------\n        path : str, path object, or buffer\n            String, path object (implementing ``os.PathLike[str]``), or file-like\n            object implementing a binary ``write()`` function.\n\n            .. versionchanged:: 1.0.0\n\n            Previously this was \"fname\"\n\n        convert_dates : dict\n            Dictionary mapping columns containing datetime types to stata\n            internal format to use when writing the dates. Options are 'tc',\n            'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer\n            or a name. Datetime columns that do not have a conversion type\n            specified will be converted to 'tc'. Raises NotImplementedError if\n            a datetime column has timezone information.\n        write_index : bool\n            Write the index to Stata dataset.\n        byteorder : str\n            Can be \">\", \"<\", \"little\", or \"big\". default is `sys.byteorder`.\n        time_stamp : datetime\n            A datetime to use as file creation date.  Default is the current\n            time.\n        data_label : str, optional\n            A label for the data set.  Must be 80 characters or smaller.\n        variable_labels : dict\n            Dictionary containing columns as keys and variable labels as\n            values. Each label must be 80 characters or smaller.\n        version : {{114, 117, 118, 119, None}}, default 114\n            Version to use in the output dta file. Set to None to let pandas\n            decide between 118 or 119 formats depending on the number of\n            columns in the frame. Version 114 can be read by Stata 10 and\n            later. Version 117 can be read by Stata 13 or later. Version 118\n            is supported in Stata 14 and later. Version 119 is supported in\n            Stata 15 and later. Version 114 limits string variables to 244\n            characters or fewer while versions 117 and later allow strings\n            with lengths up to 2,000,000 characters. Versions 118 and 119\n            support Unicode characters, and version 119 supports more than\n            32,767 variables.\n\n            Version 119 should usually only be used when the number of\n            variables exceeds the capacity of dta format 118. Exporting\n            smaller datasets in format 119 may have unintended consequences,\n            and, as of November 2020, Stata SE cannot read version 119 files.\n\n            .. versionchanged:: 1.0.0\n\n                Added support for formats 118 and 119.\n\n        convert_strl : list, optional\n            List of column names to convert to string columns to Stata StrL\n            format. Only available if version is 117.  Storing strings in the\n            StrL format can produce smaller dta files if strings have more than\n            8 characters and values are repeated.\n        compression : str or dict, default 'infer'\n            For on-the-fly compression of the output dta. If string, specifies\n            compression mode. If dict, value at key 'method' specifies\n            compression mode. Compression mode must be one of {{'infer', 'gzip',\n            'bz2', 'zip', 'xz', None}}. If compression mode is 'infer' and\n            `fname` is path-like, then detect compression from the following\n            extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n            compression). If dict and compression mode is one of {{'zip',\n            'gzip', 'bz2'}}, or inferred as one of the above, other entries\n            passed as additional compression options.\n\n            .. versionadded:: 1.1.0\n\n        {storage_options}\n\n            .. versionadded:: 1.2.0\n\n        value_labels : dict of dicts\n            Dictionary containing columns as keys and dictionaries of column value\n            to labels as values. Labels for a single variable must be 32,000\n            characters or smaller.\n\n            .. versionadded:: 1.4.0\n\n        Raises\n        ------\n        NotImplementedError\n            * If datetimes contain timezone information\n            * Column dtype is not representable in Stata\n        ValueError\n            * Columns listed in convert_dates are neither datetime64[ns]\n              or datetime.datetime\n            * Column listed in convert_dates is not in DataFrame\n            * Categorical label contains more than 32,000 characters\n\n        See Also\n        --------\n        read_stata : Import Stata data files.\n        io.stata.StataWriter : Low-level writer for Stata data files.\n        io.stata.StataWriter117 : Low-level writer for version 117 files.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({{'animal': ['falcon', 'parrot', 'falcon',\n        ...                               'parrot'],\n        ...                    'speed': [350, 18, 361, 15]}})\n        >>> df.to_stata('animals.dta')  # doctest: +SKIP\n        \"\"\"\n        if version not in (114, 117, 118, 119, None):\n            raise ValueError(\"Only formats 114, 117, 118 and 119 are supported.\")\n        if version == 114:\n            if convert_strl is not None:\n                raise ValueError(\"strl is not supported in format 114\")\n            from pandas.io.stata import StataWriter as statawriter\n        elif version == 117:\n            # mypy: Name 'statawriter' already defined (possibly by an import)\n            from pandas.io.stata import (  # type: ignore[no-redef]\n                StataWriter117 as statawriter,\n            )\n        else:  # versions 118 and 119\n            # mypy: Name 'statawriter' already defined (possibly by an import)\n            from pandas.io.stata import (  # type: ignore[no-redef]\n                StataWriterUTF8 as statawriter,\n            )\n\n        kwargs: dict[str, Any] = {}\n        if version is None or version >= 117:\n            # strl conversion is only supported >= 117\n            kwargs[\"convert_strl\"] = convert_strl\n        if version is None or version >= 118:\n            # Specifying the version is only supported for UTF8 (118 or 119)\n            kwargs[\"version\"] = version\n\n        # mypy: Too many arguments for \"StataWriter\"\n        writer = statawriter(  # type: ignore[call-arg]\n            path,\n            self,\n            convert_dates=convert_dates,\n            byteorder=byteorder,\n            time_stamp=time_stamp,\n            data_label=data_label,\n            write_index=write_index,\n            variable_labels=variable_labels,\n            compression=compression,\n            storage_options=storage_options,\n            value_labels=value_labels,\n            **kwargs,\n        )\n        writer.write_file()\n\n    @deprecate_kwarg(old_arg_name=\"fname\", new_arg_name=\"path\")\n    def to_feather(self, path: FilePath | WriteBuffer[bytes], **kwargs) -> None:\n        \"\"\"\n        Write a DataFrame to the binary Feather format.\n\n        Parameters\n        ----------\n        path : str, path object, file-like object\n            String, path object (implementing ``os.PathLike[str]``), or file-like\n            object implementing a binary ``write()`` function. If a string or a path,\n            it will be used as Root Directory path when writing a partitioned dataset.\n        **kwargs :\n            Additional keywords passed to :func:`pyarrow.feather.write_feather`.\n            Starting with pyarrow 0.17, this includes the `compression`,\n            `compression_level`, `chunksize` and `version` keywords.\n\n            .. versionadded:: 1.1.0\n\n        Notes\n        -----\n        This function writes the dataframe as a `feather file\n        <https://arrow.apache.org/docs/python/feather.html>`_. Requires a default\n        index. For saving the DataFrame with your custom index use a method that\n        supports custom indices e.g. `to_parquet`.\n        \"\"\"\n        from pandas.io.feather_format import to_feather\n\n        to_feather(self, path, **kwargs)\n\n    @doc(\n        Series.to_markdown,\n        klass=_shared_doc_kwargs[\"klass\"],\n        storage_options=_shared_docs[\"storage_options\"],\n        examples=\"\"\"Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     data={\"animal_1\": [\"elk\", \"pig\"], \"animal_2\": [\"dog\", \"quetzal\"]}\n        ... )\n        >>> print(df.to_markdown())\n        |    | animal_1   | animal_2   |\n        |---:|:-----------|:-----------|\n        |  0 | elk        | dog        |\n        |  1 | pig        | quetzal    |\n\n        Output markdown with a tabulate option.\n\n        >>> print(df.to_markdown(tablefmt=\"grid\"))\n        +----+------------+------------+\n        |    | animal_1   | animal_2   |\n        +====+============+============+\n        |  0 | elk        | dog        |\n        +----+------------+------------+\n        |  1 | pig        | quetzal    |\n        +----+------------+------------+\"\"\",\n    )\n    def to_markdown(\n        self,\n        buf: IO[str] | str | None = None,\n        mode: str = \"wt\",\n        index: bool = True,\n        storage_options: StorageOptions = None,\n        **kwargs,\n    ) -> str | None:\n        if \"showindex\" in kwargs:\n            warnings.warn(\n                \"'showindex' is deprecated. Only 'index' will be used \"\n                \"in a future version. Use 'index' to silence this warning.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        kwargs.setdefault(\"headers\", \"keys\")\n        kwargs.setdefault(\"tablefmt\", \"pipe\")\n        kwargs.setdefault(\"showindex\", index)\n        tabulate = import_optional_dependency(\"tabulate\")\n        result = tabulate.tabulate(self, **kwargs)\n        if buf is None:\n            return result\n\n        with get_handle(buf, mode, storage_options=storage_options) as handles:\n            handles.handle.write(result)\n        return None\n\n    @doc(storage_options=generic._shared_docs[\"storage_options\"])\n    @deprecate_kwarg(old_arg_name=\"fname\", new_arg_name=\"path\")\n    def to_parquet(\n        self,\n        path: FilePath | WriteBuffer[bytes] | None = None,\n        engine: str = \"auto\",\n        compression: str | None = \"snappy\",\n        index: bool | None = None,\n        partition_cols: list[str] | None = None,\n        storage_options: StorageOptions = None,\n        **kwargs,\n    ) -> bytes | None:\n        \"\"\"\n        Write a DataFrame to the binary parquet format.\n\n        This function writes the dataframe as a `parquet file\n        <https://parquet.apache.org/>`_. You can choose different parquet\n        backends, and have the option of compression. See\n        :ref:`the user guide <io.parquet>` for more details.\n\n        Parameters\n        ----------\n        path : str, path object, file-like object, or None, default None\n            String, path object (implementing ``os.PathLike[str]``), or file-like\n            object implementing a binary ``write()`` function. If None, the result is\n            returned as bytes. If a string or path, it will be used as Root Directory\n            path when writing a partitioned dataset.\n\n            .. versionchanged:: 1.2.0\n\n            Previously this was \"fname\"\n\n        engine : {{'auto', 'pyarrow', 'fastparquet'}}, default 'auto'\n            Parquet library to use. If 'auto', then the option\n            ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n            behavior is to try 'pyarrow', falling back to 'fastparquet' if\n            'pyarrow' is unavailable.\n        compression : {{'snappy', 'gzip', 'brotli', None}}, default 'snappy'\n            Name of the compression to use. Use ``None`` for no compression.\n        index : bool, default None\n            If ``True``, include the dataframe's index(es) in the file output.\n            If ``False``, they will not be written to the file.\n            If ``None``, similar to ``True`` the dataframe's index(es)\n            will be saved. However, instead of being saved as values,\n            the RangeIndex will be stored as a range in the metadata so it\n            doesn't require much space and is faster. Other indexes will\n            be included as columns in the file output.\n        partition_cols : list, optional, default None\n            Column names by which to partition the dataset.\n            Columns are partitioned in the order they are given.\n            Must be None if path is not a string.\n        {storage_options}\n\n            .. versionadded:: 1.2.0\n\n        **kwargs\n            Additional arguments passed to the parquet library. See\n            :ref:`pandas io <io.parquet>` for more details.\n\n        Returns\n        -------\n        bytes if no path argument is provided else None\n\n        See Also\n        --------\n        read_parquet : Read a parquet file.\n        DataFrame.to_csv : Write a csv file.\n        DataFrame.to_sql : Write to a sql table.\n        DataFrame.to_hdf : Write to hdf.\n\n        Notes\n        -----\n        This function requires either the `fastparquet\n        <https://pypi.org/project/fastparquet>`_ or `pyarrow\n        <https://arrow.apache.org/docs/python/>`_ library.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(data={{'col1': [1, 2], 'col2': [3, 4]}})\n        >>> df.to_parquet('df.parquet.gzip',\n        ...               compression='gzip')  # doctest: +SKIP\n        >>> pd.read_parquet('df.parquet.gzip')  # doctest: +SKIP\n           col1  col2\n        0     1     3\n        1     2     4\n\n        If you want to get a buffer to the parquet content you can use a io.BytesIO\n        object, as long as you don't use partition_cols, which creates multiple files.\n\n        >>> import io\n        >>> f = io.BytesIO()\n        >>> df.to_parquet(f)\n        >>> f.seek(0)\n        0\n        >>> content = f.read()\n        \"\"\"\n        from pandas.io.parquet import to_parquet\n\n        return to_parquet(\n            self,\n            path,\n            engine,\n            compression=compression,\n            index=index,\n            partition_cols=partition_cols,\n            storage_options=storage_options,\n            **kwargs,\n        )\n\n    @Substitution(\n        header_type=\"bool\",\n        header=\"Whether to print column labels, default True\",\n        col_space_type=\"str or int, list or dict of int or str\",\n        col_space=\"The minimum width of each column in CSS length \"\n        \"units.  An int is assumed to be px units.\\n\\n\"\n        \"            .. versionadded:: 0.25.0\\n\"\n        \"                Ability to use str\",\n    )\n    @Substitution(shared_params=fmt.common_docstring, returns=fmt.return_docstring)\n    def to_html(\n        self,\n        buf: FilePath | WriteBuffer[str] | None = None,\n        columns: Sequence[str] | None = None,\n        col_space: ColspaceArgType | None = None,\n        header: bool | Sequence[str] = True,\n        index: bool = True,\n        na_rep: str = \"NaN\",\n        formatters: FormattersType | None = None,\n        float_format: FloatFormatType | None = None,\n        sparsify: bool | None = None,\n        index_names: bool = True,\n        justify: str | None = None,\n        max_rows: int | None = None,\n        max_cols: int | None = None,\n        show_dimensions: bool | str = False,\n        decimal: str = \".\",\n        bold_rows: bool = True,\n        classes: str | list | tuple | None = None,\n        escape: bool = True,\n        notebook: bool = False,\n        border: int | None = None,\n        table_id: str | None = None,\n        render_links: bool = False,\n        encoding: str | None = None,\n    ):\n        \"\"\"\n        Render a DataFrame as an HTML table.\n        %(shared_params)s\n        bold_rows : bool, default True\n            Make the row labels bold in the output.\n        classes : str or list or tuple, default None\n            CSS class(es) to apply to the resulting html table.\n        escape : bool, default True\n            Convert the characters <, >, and & to HTML-safe sequences.\n        notebook : {True, False}, default False\n            Whether the generated HTML is for IPython Notebook.\n        border : int\n            A ``border=border`` attribute is included in the opening\n            `<table>` tag. Default ``pd.options.display.html.border``.\n        table_id : str, optional\n            A css id is included in the opening `<table>` tag if specified.\n        render_links : bool, default False\n            Convert URLs to HTML links.\n        encoding : str, default \"utf-8\"\n            Set character encoding.\n\n            .. versionadded:: 1.0\n        %(returns)s\n        See Also\n        --------\n        to_string : Convert DataFrame to a string.\n        \"\"\"\n        if justify is not None and justify not in fmt._VALID_JUSTIFY_PARAMETERS:\n            raise ValueError(\"Invalid value for justify parameter\")\n\n        formatter = fmt.DataFrameFormatter(\n            self,\n            columns=columns,\n            col_space=col_space,\n            na_rep=na_rep,\n            header=header,\n            index=index,\n            formatters=formatters,\n            float_format=float_format,\n            bold_rows=bold_rows,\n            sparsify=sparsify,\n            justify=justify,\n            index_names=index_names,\n            escape=escape,\n            decimal=decimal,\n            max_rows=max_rows,\n            max_cols=max_cols,\n            show_dimensions=show_dimensions,\n        )\n        # TODO: a generic formatter wld b in DataFrameFormatter\n        return fmt.DataFrameRenderer(formatter).to_html(\n            buf=buf,\n            classes=classes,\n            notebook=notebook,\n            border=border,\n            encoding=encoding,\n            table_id=table_id,\n            render_links=render_links,\n        )\n\n    @doc(storage_options=generic._shared_docs[\"storage_options\"])\n    def to_xml(\n        self,\n        path_or_buffer: FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None = None,\n        index: bool = True,\n        root_name: str | None = \"data\",\n        row_name: str | None = \"row\",\n        na_rep: str | None = None,\n        attr_cols: str | list[str] | None = None,\n        elem_cols: str | list[str] | None = None,\n        namespaces: dict[str | None, str] | None = None,\n        prefix: str | None = None,\n        encoding: str = \"utf-8\",\n        xml_declaration: bool | None = True,\n        pretty_print: bool | None = True,\n        parser: str | None = \"lxml\",\n        stylesheet: FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None = None,\n        compression: CompressionOptions = \"infer\",\n        storage_options: StorageOptions = None,\n    ) -> str | None:\n        \"\"\"\n        Render a DataFrame to an XML document.\n\n        .. versionadded:: 1.3.0\n\n        Parameters\n        ----------\n        path_or_buffer : str, path object, file-like object, or None, default None\n            String, path object (implementing ``os.PathLike[str]``), or file-like\n            object implementing a ``write()`` function. If None, the result is returned\n            as a string.\n        index : bool, default True\n            Whether to include index in XML document.\n        root_name : str, default 'data'\n            The name of root element in XML document.\n        row_name : str, default 'row'\n            The name of row element in XML document.\n        na_rep : str, optional\n            Missing data representation.\n        attr_cols : list-like, optional\n            List of columns to write as attributes in row element.\n            Hierarchical columns will be flattened with underscore\n            delimiting the different levels.\n        elem_cols : list-like, optional\n            List of columns to write as children in row element. By default,\n            all columns output as children of row element. Hierarchical\n            columns will be flattened with underscore delimiting the\n            different levels.\n        namespaces : dict, optional\n            All namespaces to be defined in root element. Keys of dict\n            should be prefix names and values of dict corresponding URIs.\n            Default namespaces should be given empty string key. For\n            example, ::\n\n                namespaces = {{\"\": \"https://example.com\"}}\n\n        prefix : str, optional\n            Namespace prefix to be used for every element and/or attribute\n            in document. This should be one of the keys in ``namespaces``\n            dict.\n        encoding : str, default 'utf-8'\n            Encoding of the resulting document.\n        xml_declaration : bool, default True\n            Whether to include the XML declaration at start of document.\n        pretty_print : bool, default True\n            Whether output should be pretty printed with indentation and\n            line breaks.\n        parser : {{'lxml','etree'}}, default 'lxml'\n            Parser module to use for building of tree. Only 'lxml' and\n            'etree' are supported. With 'lxml', the ability to use XSLT\n            stylesheet is supported.\n        stylesheet : str, path object or file-like object, optional\n            A URL, file-like object, or a raw string containing an XSLT\n            script used to transform the raw XML output. Script should use\n            layout of elements and attributes from original output. This\n            argument requires ``lxml`` to be installed. Only XSLT 1.0\n            scripts and not later versions is currently supported.\n        compression : {{'infer', 'gzip', 'bz2', 'zip', 'xz', None}}, default 'infer'\n            For on-the-fly decompression of on-disk data. If 'infer', then use\n            gzip, bz2, zip or xz if path_or_buffer is a string ending in\n            '.gz', '.bz2', '.zip', or 'xz', respectively, and no decompression\n            otherwise. If using 'zip', the ZIP file must contain only one data\n            file to be read in. Set to None for no decompression.\n        {storage_options}\n\n        Returns\n        -------\n        None or str\n            If ``io`` is None, returns the resulting XML format as a\n            string. Otherwise returns None.\n\n        See Also\n        --------\n        to_json : Convert the pandas object to a JSON string.\n        to_html : Convert DataFrame to a html.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({{'shape': ['square', 'circle', 'triangle'],\n        ...                    'degrees': [360, 360, 180],\n        ...                    'sides': [4, np.nan, 3]}})\n\n        >>> df.to_xml()  # doctest: +SKIP\n        <?xml version='1.0' encoding='utf-8'?>\n        <data>\n          <row>\n            <index>0</index>\n            <shape>square</shape>\n            <degrees>360</degrees>\n            <sides>4.0</sides>\n          </row>\n          <row>\n            <index>1</index>\n            <shape>circle</shape>\n            <degrees>360</degrees>\n            <sides/>\n          </row>\n          <row>\n            <index>2</index>\n            <shape>triangle</shape>\n            <degrees>180</degrees>\n            <sides>3.0</sides>\n          </row>\n        </data>\n\n        >>> df.to_xml(attr_cols=[\n        ...           'index', 'shape', 'degrees', 'sides'\n        ...           ])  # doctest: +SKIP\n        <?xml version='1.0' encoding='utf-8'?>\n        <data>\n          <row index=\"0\" shape=\"square\" degrees=\"360\" sides=\"4.0\"/>\n          <row index=\"1\" shape=\"circle\" degrees=\"360\"/>\n          <row index=\"2\" shape=\"triangle\" degrees=\"180\" sides=\"3.0\"/>\n        </data>\n\n        >>> df.to_xml(namespaces={{\"doc\": \"https://example.com\"}},\n        ...           prefix=\"doc\")  # doctest: +SKIP\n        <?xml version='1.0' encoding='utf-8'?>\n        <doc:data xmlns:doc=\"https://example.com\">\n          <doc:row>\n            <doc:index>0</doc:index>\n            <doc:shape>square</doc:shape>\n            <doc:degrees>360</doc:degrees>\n            <doc:sides>4.0</doc:sides>\n          </doc:row>\n          <doc:row>\n            <doc:index>1</doc:index>\n            <doc:shape>circle</doc:shape>\n            <doc:degrees>360</doc:degrees>\n            <doc:sides/>\n          </doc:row>\n          <doc:row>\n            <doc:index>2</doc:index>\n            <doc:shape>triangle</doc:shape>\n            <doc:degrees>180</doc:degrees>\n            <doc:sides>3.0</doc:sides>\n          </doc:row>\n        </doc:data>\n        \"\"\"\n\n        from pandas.io.formats.xml import (\n            EtreeXMLFormatter,\n            LxmlXMLFormatter,\n        )\n\n        lxml = import_optional_dependency(\"lxml.etree\", errors=\"ignore\")\n\n        TreeBuilder: type[EtreeXMLFormatter] | type[LxmlXMLFormatter]\n\n        if parser == \"lxml\":\n            if lxml is not None:\n                TreeBuilder = LxmlXMLFormatter\n            else:\n                raise ImportError(\n                    \"lxml not found, please install or use the etree parser.\"\n                )\n\n        elif parser == \"etree\":\n            TreeBuilder = EtreeXMLFormatter\n\n        else:\n            raise ValueError(\"Values for parser can only be lxml or etree.\")\n\n        xml_formatter = TreeBuilder(\n            self,\n            path_or_buffer=path_or_buffer,\n            index=index,\n            root_name=root_name,\n            row_name=row_name,\n            na_rep=na_rep,\n            attr_cols=attr_cols,\n            elem_cols=elem_cols,\n            namespaces=namespaces,\n            prefix=prefix,\n            encoding=encoding,\n            xml_declaration=xml_declaration,\n            pretty_print=pretty_print,\n            stylesheet=stylesheet,\n            compression=compression,\n            storage_options=storage_options,\n        )\n\n        return xml_formatter.write_output()\n\n    # ----------------------------------------------------------------------\n    @Substitution(\n        klass=\"DataFrame\",\n        type_sub=\" and columns\",\n        max_cols_sub=dedent(\n            \"\"\"\\\n            max_cols : int, optional\n                When to switch from the verbose to the truncated output. If the\n                DataFrame has more than `max_cols` columns, the truncated output\n                is used. By default, the setting in\n                ``pandas.options.display.max_info_columns`` is used.\"\"\"\n        ),\n        show_counts_sub=dedent(\n            \"\"\"\\\n            show_counts : bool, optional\n                Whether to show the non-null counts. By default, this is shown\n                only if the DataFrame is smaller than\n                ``pandas.options.display.max_info_rows`` and\n                ``pandas.options.display.max_info_columns``. A value of True always\n                shows the counts, and False never shows the counts.\n            null_counts : bool, optional\n                .. deprecated:: 1.2.0\n                    Use show_counts instead.\"\"\"\n        ),\n        examples_sub=dedent(\n            \"\"\"\\\n            >>> int_values = [1, 2, 3, 4, 5]\n            >>> text_values = ['alpha', 'beta', 'gamma', 'delta', 'epsilon']\n            >>> float_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n            >>> df = pd.DataFrame({\"int_col\": int_values, \"text_col\": text_values,\n            ...                   \"float_col\": float_values})\n            >>> df\n                int_col text_col  float_col\n            0        1    alpha       0.00\n            1        2     beta       0.25\n            2        3    gamma       0.50\n            3        4    delta       0.75\n            4        5  epsilon       1.00\n\n            Prints information of all columns:\n\n            >>> df.info(verbose=True)\n            <class 'pandas.core.frame.DataFrame'>\n            RangeIndex: 5 entries, 0 to 4\n            Data columns (total 3 columns):\n             #   Column     Non-Null Count  Dtype\n            ---  ------     --------------  -----\n             0   int_col    5 non-null      int64\n             1   text_col   5 non-null      object\n             2   float_col  5 non-null      float64\n            dtypes: float64(1), int64(1), object(1)\n            memory usage: 248.0+ bytes\n\n            Prints a summary of columns count and its dtypes but not per column\n            information:\n\n            >>> df.info(verbose=False)\n            <class 'pandas.core.frame.DataFrame'>\n            RangeIndex: 5 entries, 0 to 4\n            Columns: 3 entries, int_col to float_col\n            dtypes: float64(1), int64(1), object(1)\n            memory usage: 248.0+ bytes\n\n            Pipe output of DataFrame.info to buffer instead of sys.stdout, get\n            buffer content and writes to a text file:\n\n            >>> import io\n            >>> buffer = io.StringIO()\n            >>> df.info(buf=buffer)\n            >>> s = buffer.getvalue()\n            >>> with open(\"df_info.txt\", \"w\",\n            ...           encoding=\"utf-8\") as f:  # doctest: +SKIP\n            ...     f.write(s)\n            260\n\n            The `memory_usage` parameter allows deep introspection mode, specially\n            useful for big DataFrames and fine-tune memory optimization:\n\n            >>> random_strings_array = np.random.choice(['a', 'b', 'c'], 10 ** 6)\n            >>> df = pd.DataFrame({\n            ...     'column_1': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n            ...     'column_2': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n            ...     'column_3': np.random.choice(['a', 'b', 'c'], 10 ** 6)\n            ... })\n            >>> df.info()\n            <class 'pandas.core.frame.DataFrame'>\n            RangeIndex: 1000000 entries, 0 to 999999\n            Data columns (total 3 columns):\n             #   Column    Non-Null Count    Dtype\n            ---  ------    --------------    -----\n             0   column_1  1000000 non-null  object\n             1   column_2  1000000 non-null  object\n             2   column_3  1000000 non-null  object\n            dtypes: object(3)\n            memory usage: 22.9+ MB\n\n            >>> df.info(memory_usage='deep')\n            <class 'pandas.core.frame.DataFrame'>\n            RangeIndex: 1000000 entries, 0 to 999999\n            Data columns (total 3 columns):\n             #   Column    Non-Null Count    Dtype\n            ---  ------    --------------    -----\n             0   column_1  1000000 non-null  object\n             1   column_2  1000000 non-null  object\n             2   column_3  1000000 non-null  object\n            dtypes: object(3)\n            memory usage: 165.9 MB\"\"\"\n        ),\n        see_also_sub=dedent(\n            \"\"\"\\\n            DataFrame.describe: Generate descriptive statistics of DataFrame\n                columns.\n            DataFrame.memory_usage: Memory usage of DataFrame columns.\"\"\"\n        ),\n        version_added_sub=\"\",\n    )\n    @doc(BaseInfo.render)\n    def info(\n        self,\n        verbose: bool | None = None,\n        buf: WriteBuffer[str] | None = None,\n        max_cols: int | None = None,\n        memory_usage: bool | str | None = None,\n        show_counts: bool | None = None,\n        null_counts: bool | None = None,\n    ) -> None:\n        if null_counts is not None:\n            if show_counts is not None:\n                raise ValueError(\"null_counts used with show_counts. Use show_counts.\")\n            warnings.warn(\n                \"null_counts is deprecated. Use show_counts instead\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            show_counts = null_counts\n        info = DataFrameInfo(\n            data=self,\n            memory_usage=memory_usage,\n        )\n        info.render(\n            buf=buf,\n            max_cols=max_cols,\n            verbose=verbose,\n            show_counts=show_counts,\n        )\n\n    def memory_usage(self, index: bool = True, deep: bool = False) -> Series:\n        \"\"\"\n        Return the memory usage of each column in bytes.\n\n        The memory usage can optionally include the contribution of\n        the index and elements of `object` dtype.\n\n        This value is displayed in `DataFrame.info` by default. This can be\n        suppressed by setting ``pandas.options.display.memory_usage`` to False.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the DataFrame's\n            index in returned Series. If ``index=True``, the memory usage of\n            the index is the first item in the output.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned values.\n\n        Returns\n        -------\n        Series\n            A Series whose index is the original column names and whose values\n            is the memory usage of each column in bytes.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of an\n            ndarray.\n        Series.memory_usage : Bytes consumed by a Series.\n        Categorical : Memory-efficient array for string values with\n            many repeated values.\n        DataFrame.info : Concise summary of a DataFrame.\n\n        Examples\n        --------\n        >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool']\n        >>> data = dict([(t, np.ones(shape=5000, dtype=int).astype(t))\n        ...              for t in dtypes])\n        >>> df = pd.DataFrame(data)\n        >>> df.head()\n           int64  float64            complex128  object  bool\n        0      1      1.0              1.0+0.0j       1  True\n        1      1      1.0              1.0+0.0j       1  True\n        2      1      1.0              1.0+0.0j       1  True\n        3      1      1.0              1.0+0.0j       1  True\n        4      1      1.0              1.0+0.0j       1  True\n\n        >>> df.memory_usage()\n        Index           128\n        int64         40000\n        float64       40000\n        complex128    80000\n        object        40000\n        bool           5000\n        dtype: int64\n\n        >>> df.memory_usage(index=False)\n        int64         40000\n        float64       40000\n        complex128    80000\n        object        40000\n        bool           5000\n        dtype: int64\n\n        The memory footprint of `object` dtype columns is ignored by default:\n\n        >>> df.memory_usage(deep=True)\n        Index            128\n        int64          40000\n        float64        40000\n        complex128     80000\n        object        180000\n        bool            5000\n        dtype: int64\n\n        Use a Categorical for efficient storage of an object-dtype column with\n        many repeated values.\n\n        >>> df['object'].astype('category').memory_usage(deep=True)\n        5244\n        \"\"\"\n        result = self._constructor_sliced(\n            [c.memory_usage(index=False, deep=deep) for col, c in self.items()],\n            index=self.columns,\n        )\n        if index:\n            result = self._constructor_sliced(\n                self.index.memory_usage(deep=deep), index=[\"Index\"]\n            ).append(result)\n        return result\n\n    def transpose(self, *args, copy: bool = False) -> DataFrame:\n        \"\"\"\n        Transpose index and columns.\n\n        Reflect the DataFrame over its main diagonal by writing rows as columns\n        and vice-versa. The property :attr:`.T` is an accessor to the method\n        :meth:`transpose`.\n\n        Parameters\n        ----------\n        *args : tuple, optional\n            Accepted for compatibility with NumPy.\n        copy : bool, default False\n            Whether to copy the data after transposing, even for DataFrames\n            with a single dtype.\n\n            Note that a copy is always required for mixed dtype DataFrames,\n            or for DataFrames with any extension types.\n\n        Returns\n        -------\n        DataFrame\n            The transposed DataFrame.\n\n        See Also\n        --------\n        numpy.transpose : Permute the dimensions of a given array.\n\n        Notes\n        -----\n        Transposing a DataFrame with mixed dtypes will result in a homogeneous\n        DataFrame with the `object` dtype. In such a case, a copy of the data\n        is always made.\n\n        Examples\n        --------\n        **Square DataFrame with homogeneous dtype**\n\n        >>> d1 = {'col1': [1, 2], 'col2': [3, 4]}\n        >>> df1 = pd.DataFrame(data=d1)\n        >>> df1\n           col1  col2\n        0     1     3\n        1     2     4\n\n        >>> df1_transposed = df1.T # or df1.transpose()\n        >>> df1_transposed\n              0  1\n        col1  1  2\n        col2  3  4\n\n        When the dtype is homogeneous in the original DataFrame, we get a\n        transposed DataFrame with the same dtype:\n\n        >>> df1.dtypes\n        col1    int64\n        col2    int64\n        dtype: object\n        >>> df1_transposed.dtypes\n        0    int64\n        1    int64\n        dtype: object\n\n        **Non-square DataFrame with mixed dtypes**\n\n        >>> d2 = {'name': ['Alice', 'Bob'],\n        ...       'score': [9.5, 8],\n        ...       'employed': [False, True],\n        ...       'kids': [0, 0]}\n        >>> df2 = pd.DataFrame(data=d2)\n        >>> df2\n            name  score  employed  kids\n        0  Alice    9.5     False     0\n        1    Bob    8.0      True     0\n\n        >>> df2_transposed = df2.T # or df2.transpose()\n        >>> df2_transposed\n                      0     1\n        name      Alice   Bob\n        score       9.5   8.0\n        employed  False  True\n        kids          0     0\n\n        When the DataFrame has mixed dtypes, we get a transposed DataFrame with\n        the `object` dtype:\n\n        >>> df2.dtypes\n        name         object\n        score       float64\n        employed       bool\n        kids          int64\n        dtype: object\n        >>> df2_transposed.dtypes\n        0    object\n        1    object\n        dtype: object\n        \"\"\"\n        nv.validate_transpose(args, {})\n        # construct the args\n\n        dtypes = list(self.dtypes)\n\n        if self._can_fast_transpose:\n            # Note: tests pass without this, but this improves perf quite a bit.\n            new_vals = self._values.T\n            if copy:\n                new_vals = new_vals.copy()\n\n            result = self._constructor(new_vals, index=self.columns, columns=self.index)\n\n        elif (\n            self._is_homogeneous_type and dtypes and is_extension_array_dtype(dtypes[0])\n        ):\n            # We have EAs with the same dtype. We can preserve that dtype in transpose.\n            dtype = dtypes[0]\n            arr_type = dtype.construct_array_type()\n            values = self.values\n\n            new_values = [arr_type._from_sequence(row, dtype=dtype) for row in values]\n            result = type(self)._from_arrays(\n                new_values, index=self.columns, columns=self.index\n            )\n\n        else:\n            new_arr = self.values.T\n            if copy:\n                new_arr = new_arr.copy()\n            result = self._constructor(new_arr, index=self.columns, columns=self.index)\n\n        return result.__finalize__(self, method=\"transpose\")\n\n    @property\n    def T(self) -> DataFrame:\n        return self.transpose()\n\n    # ----------------------------------------------------------------------\n    # Indexing Methods\n\n    def _ixs(self, i: int, axis: int = 0):\n        \"\"\"\n        Parameters\n        ----------\n        i : int\n        axis : int\n\n        Notes\n        -----\n        If slice passed, the resulting data will be a view.\n        \"\"\"\n        # irow\n        if axis == 0:\n            new_values = self._mgr.fast_xs(i)\n\n            # if we are a copy, mark as such\n            copy = isinstance(new_values, np.ndarray) and new_values.base is None\n            result = self._constructor_sliced(\n                new_values,\n                index=self.columns,\n                name=self.index[i],\n                dtype=new_values.dtype,\n            )\n            result._set_is_copy(self, copy=copy)\n            return result\n\n        # icol\n        else:\n            label = self.columns[i]\n\n            values = self._mgr.iget(i)\n            result = self._box_col_values(values, i)\n\n            # this is a cached value, mark it so\n            result._set_as_cached(label, self)\n            return result\n\n    def _get_column_array(self, i: int) -> ArrayLike:\n        \"\"\"\n        Get the values of the i'th column (ndarray or ExtensionArray, as stored\n        in the Block)\n        \"\"\"\n        return self._mgr.iget_values(i)\n\n    def _iter_column_arrays(self) -> Iterator[ArrayLike]:\n        \"\"\"\n        Iterate over the arrays of all columns in order.\n        This returns the values as stored in the Block (ndarray or ExtensionArray).\n        \"\"\"\n        for i in range(len(self.columns)):\n            yield self._get_column_array(i)\n\n    def __getitem__(self, key):\n        key = lib.item_from_zerodim(key)\n        key = com.apply_if_callable(key, self)\n\n        if is_hashable(key):\n            # shortcut if the key is in columns\n            if self.columns.is_unique and key in self.columns:\n                if isinstance(self.columns, MultiIndex):\n                    return self._getitem_multilevel(key)\n                return self._get_item_cache(key)\n\n        # Do we have a slicer (on rows)?\n        indexer = convert_to_index_sliceable(self, key)\n        if indexer is not None:\n            if isinstance(indexer, np.ndarray):\n                indexer = lib.maybe_indices_to_slice(\n                    indexer.astype(np.intp, copy=False), len(self)\n                )\n                if isinstance(indexer, np.ndarray):\n                    # GH#43223 If we can not convert, use take\n                    return self.take(indexer, axis=0)\n            # either we have a slice or we have a string that can be converted\n            #  to a slice for partial-string date indexing\n            return self._slice(indexer, axis=0)\n\n        # Do we have a (boolean) DataFrame?\n        if isinstance(key, DataFrame):\n            return self.where(key)\n\n        # Do we have a (boolean) 1d indexer?\n        if com.is_bool_indexer(key):\n            return self._getitem_bool_array(key)\n\n        # We are left with two options: a single key, and a collection of keys,\n        # We interpret tuples as collections only for non-MultiIndex\n        is_single_key = isinstance(key, tuple) or not is_list_like(key)\n\n        if is_single_key:\n            if self.columns.nlevels > 1:\n                return self._getitem_multilevel(key)\n            indexer = self.columns.get_loc(key)\n            if is_integer(indexer):\n                indexer = [indexer]\n        else:\n            if is_iterator(key):\n                key = list(key)\n            indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n\n        # take() does not accept boolean indexers\n        if getattr(indexer, \"dtype\", None) == bool:\n            indexer = np.where(indexer)[0]\n\n        data = self._take_with_is_copy(indexer, axis=1)\n\n        if is_single_key:\n            # What does looking for a single key in a non-unique index return?\n            # The behavior is inconsistent. It returns a Series, except when\n            # - the key itself is repeated (test on data.shape, #9519), or\n            # - we have a MultiIndex on columns (test on self.columns, #21309)\n            if data.shape[1] == 1 and not isinstance(self.columns, MultiIndex):\n                # GH#26490 using data[key] can cause RecursionError\n                return data._get_item_cache(key)\n\n        return data\n\n    def _getitem_bool_array(self, key):\n        # also raises Exception if object array with NA values\n        # warning here just in case -- previously __setitem__ was\n        # reindexing but __getitem__ was not; it seems more reasonable to\n        # go with the __setitem__ behavior since that is more consistent\n        # with all other indexing behavior\n        if isinstance(key, Series) and not key.index.equals(self.index):\n            warnings.warn(\n                \"Boolean Series key will be reindexed to match DataFrame index.\",\n                UserWarning,\n                stacklevel=find_stack_level(),\n            )\n        elif len(key) != len(self.index):\n            raise ValueError(\n                f\"Item wrong length {len(key)} instead of {len(self.index)}.\"\n            )\n\n        # check_bool_indexer will throw exception if Series key cannot\n        # be reindexed to match DataFrame rows\n        key = check_bool_indexer(self.index, key)\n        indexer = key.nonzero()[0]\n        return self._take_with_is_copy(indexer, axis=0)\n\n    def _getitem_multilevel(self, key):\n        # self.columns is a MultiIndex\n        loc = self.columns.get_loc(key)\n        if isinstance(loc, (slice, np.ndarray)):\n            new_columns = self.columns[loc]\n            result_columns = maybe_droplevels(new_columns, key)\n            if self._is_mixed_type:\n                result = self.reindex(columns=new_columns)\n                result.columns = result_columns\n            else:\n                new_values = self.values[:, loc]\n                result = self._constructor(\n                    new_values, index=self.index, columns=result_columns\n                )\n                result = result.__finalize__(self)\n\n            # If there is only one column being returned, and its name is\n            # either an empty string, or a tuple with an empty string as its\n            # first element, then treat the empty string as a placeholder\n            # and return the column as if the user had provided that empty\n            # string in the key. If the result is a Series, exclude the\n            # implied empty string from its name.\n            if len(result.columns) == 1:\n                top = result.columns[0]\n                if isinstance(top, tuple):\n                    top = top[0]\n                if top == \"\":\n                    result = result[\"\"]\n                    if isinstance(result, Series):\n                        result = self._constructor_sliced(\n                            result, index=self.index, name=key\n                        )\n\n            result._set_is_copy(self)\n            return result\n        else:\n            # loc is neither a slice nor ndarray, so must be an int\n            return self._ixs(loc, axis=1)\n\n    def _get_value(self, index, col, takeable: bool = False) -> Scalar:\n        \"\"\"\n        Quickly retrieve single value at passed column and index.\n\n        Parameters\n        ----------\n        index : row label\n        col : column label\n        takeable : interpret the index/col as indexers, default False\n\n        Returns\n        -------\n        scalar\n\n        Notes\n        -----\n        Assumes that both `self.index._index_as_unique` and\n        `self.columns._index_as_unique`; Caller is responsible for checking.\n        \"\"\"\n        if takeable:\n            series = self._ixs(col, axis=1)\n            return series._values[index]\n\n        series = self._get_item_cache(col)\n        engine = self.index._engine\n\n        if not isinstance(self.index, MultiIndex):\n            # CategoricalIndex: Trying to use the engine fastpath may give incorrect\n            #  results if our categories are integers that dont match our codes\n            # IntervalIndex: IntervalTree has no get_loc\n            row = self.index.get_loc(index)\n            return series._values[row]\n\n        # For MultiIndex going through engine effectively restricts us to\n        #  same-length tuples; see test_get_set_value_no_partial_indexing\n        loc = engine.get_loc(index)\n        return series._values[loc]\n\n    def __setitem__(self, key, value):\n        key = com.apply_if_callable(key, self)\n\n        # see if we can slice the rows\n        indexer = convert_to_index_sliceable(self, key)\n        if indexer is not None:\n            # either we have a slice or we have a string that can be converted\n            #  to a slice for partial-string date indexing\n            return self._setitem_slice(indexer, value)\n\n        if isinstance(key, DataFrame) or getattr(key, \"ndim\", None) == 2:\n            self._setitem_frame(key, value)\n        elif isinstance(key, (Series, np.ndarray, list, Index)):\n            self._setitem_array(key, value)\n        elif isinstance(value, DataFrame):\n            self._set_item_frame_value(key, value)\n        elif (\n            is_list_like(value)\n            and not self.columns.is_unique\n            and 1 < len(self.columns.get_indexer_for([key])) == len(value)\n        ):\n            # Column to set is duplicated\n            self._setitem_array([key], value)\n        else:\n            # set column\n            self._set_item(key, value)\n\n    def _setitem_slice(self, key: slice, value):\n        # NB: we can't just use self.loc[key] = value because that\n        #  operates on labels and we need to operate positional for\n        #  backwards-compat, xref GH#31469\n        self._check_setitem_copy()\n        self.iloc[key] = value\n\n    def _setitem_array(self, key, value):\n        # also raises Exception if object array with NA values\n        if com.is_bool_indexer(key):\n            # bool indexer is indexing along rows\n            if len(key) != len(self.index):\n                raise ValueError(\n                    f\"Item wrong length {len(key)} instead of {len(self.index)}!\"\n                )\n            key = check_bool_indexer(self.index, key)\n            indexer = key.nonzero()[0]\n            self._check_setitem_copy()\n            if isinstance(value, DataFrame):\n                # GH#39931 reindex since iloc does not align\n                value = value.reindex(self.index.take(indexer))\n            self.iloc[indexer] = value\n\n        else:\n            # Note: unlike self.iloc[:, indexer] = value, this will\n            #  never try to overwrite values inplace\n\n            if isinstance(value, DataFrame):\n                check_key_length(self.columns, key, value)\n                for k1, k2 in zip(key, value.columns):\n                    self[k1] = value[k2]\n\n            elif not is_list_like(value):\n                for col in key:\n                    self[col] = value\n\n            elif isinstance(value, np.ndarray) and value.ndim == 2:\n                self._iset_not_inplace(key, value)\n\n            elif np.ndim(value) > 1:\n                # list of lists\n                value = DataFrame(value).values\n                return self._setitem_array(key, value)\n\n            else:\n                self._iset_not_inplace(key, value)\n\n    def _iset_not_inplace(self, key, value):\n        # GH#39510 when setting with df[key] = obj with a list-like key and\n        #  list-like value, we iterate over those listlikes and set columns\n        #  one at a time.  This is different from dispatching to\n        #  `self.loc[:, key]= value`  because loc.__setitem__ may overwrite\n        #  data inplace, whereas this will insert new arrays.\n\n        def igetitem(obj, i: int):\n            # Note: we catch DataFrame obj before getting here, but\n            #  hypothetically would return obj.iloc[:, i]\n            if isinstance(obj, np.ndarray):\n                return obj[..., i]\n            else:\n                return obj[i]\n\n        if self.columns.is_unique:\n            if np.shape(value)[-1] != len(key):\n                raise ValueError(\"Columns must be same length as key\")\n\n            for i, col in enumerate(key):\n                self[col] = igetitem(value, i)\n\n        else:\n\n            ilocs = self.columns.get_indexer_non_unique(key)[0]\n            if (ilocs < 0).any():\n                # key entries not in self.columns\n                raise NotImplementedError\n\n            if np.shape(value)[-1] != len(ilocs):\n                raise ValueError(\"Columns must be same length as key\")\n\n            assert np.ndim(value) <= 2\n\n            orig_columns = self.columns\n\n            # Using self.iloc[:, i] = ... may set values inplace, which\n            #  by convention we do not do in __setitem__\n            try:\n                self.columns = Index(range(len(self.columns)))\n                for i, iloc in enumerate(ilocs):\n                    self[iloc] = igetitem(value, i)\n            finally:\n                self.columns = orig_columns\n\n    def _setitem_frame(self, key, value):\n        # support boolean setting with DataFrame input, e.g.\n        # df[df > df2] = 0\n        if isinstance(key, np.ndarray):\n            if key.shape != self.shape:\n                raise ValueError(\"Array conditional must be same shape as self\")\n            key = self._constructor(key, **self._construct_axes_dict())\n\n        if key.size and not is_bool_dtype(key.values):\n            raise TypeError(\n                \"Must pass DataFrame or 2-d ndarray with boolean values only\"\n            )\n\n        self._check_inplace_setting(value)\n        self._check_setitem_copy()\n        self._where(-key, value, inplace=True)\n\n    def _set_item_frame_value(self, key, value: DataFrame) -> None:\n        self._ensure_valid_index(value)\n\n        # align columns\n        if key in self.columns:\n            loc = self.columns.get_loc(key)\n            cols = self.columns[loc]\n            len_cols = 1 if is_scalar(cols) else len(cols)\n            if len_cols != len(value.columns):\n                raise ValueError(\"Columns must be same length as key\")\n\n            # align right-hand-side columns if self.columns\n            # is multi-index and self[key] is a sub-frame\n            if isinstance(self.columns, MultiIndex) and isinstance(\n                loc, (slice, Series, np.ndarray, Index)\n            ):\n                cols = maybe_droplevels(cols, key)\n                if len(cols) and not cols.equals(value.columns):\n                    value = value.reindex(cols, axis=1)\n\n        # now align rows\n        arraylike = _reindex_for_setitem(value, self.index)\n        self._set_item_mgr(key, arraylike)\n\n    def _iset_item_mgr(\n        self, loc: int | slice | np.ndarray, value, inplace: bool = False\n    ) -> None:\n        # when called from _set_item_mgr loc can be anything returned from get_loc\n        self._mgr.iset(loc, value, inplace=inplace)\n        self._clear_item_cache()\n\n    def _set_item_mgr(self, key, value: ArrayLike) -> None:\n        try:\n            loc = self._info_axis.get_loc(key)\n        except KeyError:\n            # This item wasn't present, just insert at end\n            self._mgr.insert(len(self._info_axis), key, value)\n        else:\n            self._iset_item_mgr(loc, value)\n\n        # check if we are modifying a copy\n        # try to set first as we want an invalid\n        # value exception to occur first\n        if len(self):\n            self._check_setitem_copy()\n\n    def _iset_item(self, loc: int, value) -> None:\n        arraylike = self._sanitize_column(value)\n        self._iset_item_mgr(loc, arraylike, inplace=True)\n\n        # check if we are modifying a copy\n        # try to set first as we want an invalid\n        # value exception to occur first\n        if len(self):\n            self._check_setitem_copy()\n\n    def _set_item(self, key, value) -> None:\n        \"\"\"\n        Add series to DataFrame in specified column.\n\n        If series is a numpy-array (not a Series/TimeSeries), it must be the\n        same length as the DataFrames index or an error will be thrown.\n\n        Series/TimeSeries will be conformed to the DataFrames index to\n        ensure homogeneity.\n        \"\"\"\n        value = self._sanitize_column(value)\n\n        if (\n            key in self.columns\n            and value.ndim == 1\n            and not is_extension_array_dtype(value)\n        ):\n            # broadcast across multiple columns if necessary\n            if not self.columns.is_unique or isinstance(self.columns, MultiIndex):\n                existing_piece = self[key]\n                if isinstance(existing_piece, DataFrame):\n                    value = np.tile(value, (len(existing_piece.columns), 1)).T\n\n        self._set_item_mgr(key, value)\n\n    def _set_value(\n        self, index: IndexLabel, col, value: Scalar, takeable: bool = False\n    ) -> None:\n        \"\"\"\n        Put single value at passed column and index.\n\n        Parameters\n        ----------\n        index : Label\n            row label\n        col : Label\n            column label\n        value : scalar\n        takeable : bool, default False\n            Sets whether or not index/col interpreted as indexers\n        \"\"\"\n        try:\n            if takeable:\n                series = self._ixs(col, axis=1)\n                series._set_value(index, value, takeable=True)\n                return\n\n            series = self._get_item_cache(col)\n            loc = self.index.get_loc(index)\n            validate_numeric_casting(series.dtype, value)\n\n            series._values[loc] = value\n            # Note: trying to use series._set_value breaks tests in\n            #  tests.frame.indexing.test_indexing and tests.indexing.test_partial\n        except (KeyError, TypeError):\n            # set using a non-recursive method & reset the cache\n            if takeable:\n                self.iloc[index, col] = value\n            else:\n                self.loc[index, col] = value\n            self._item_cache.pop(col, None)\n\n    def _ensure_valid_index(self, value) -> None:\n        \"\"\"\n        Ensure that if we don't have an index, that we can create one from the\n        passed value.\n        \"\"\"\n        # GH5632, make sure that we are a Series convertible\n        if not len(self.index) and is_list_like(value) and len(value):\n            if not isinstance(value, DataFrame):\n                try:\n                    value = Series(value)\n                except (ValueError, NotImplementedError, TypeError) as err:\n                    raise ValueError(\n                        \"Cannot set a frame with no defined index \"\n                        \"and a value that cannot be converted to a Series\"\n                    ) from err\n\n            # GH31368 preserve name of index\n            index_copy = value.index.copy()\n            if self.index.name is not None:\n                index_copy.name = self.index.name\n\n            self._mgr = self._mgr.reindex_axis(index_copy, axis=1, fill_value=np.nan)\n\n    def _box_col_values(self, values, loc: int) -> Series:\n        \"\"\"\n        Provide boxed values for a column.\n        \"\"\"\n        # Lookup in columns so that if e.g. a str datetime was passed\n        #  we attach the Timestamp object as the name.\n        name = self.columns[loc]\n        klass = self._constructor_sliced\n        return klass(values, index=self.index, name=name, fastpath=True)\n\n    # ----------------------------------------------------------------------\n    # Lookup Caching\n\n    def _clear_item_cache(self) -> None:\n        self._item_cache.clear()\n\n    def _get_item_cache(self, item: Hashable) -> Series:\n        \"\"\"Return the cached item, item represents a label indexer.\"\"\"\n        cache = self._item_cache\n        res = cache.get(item)\n        if res is None:\n            # All places that call _get_item_cache have unique columns,\n            #  pending resolution of GH#33047\n\n            loc = self.columns.get_loc(item)\n            values = self._mgr.iget(loc)\n            res = self._box_col_values(values, loc).__finalize__(self)\n\n            cache[item] = res\n            res._set_as_cached(item, self)\n\n            # for a chain\n            res._is_copy = self._is_copy\n        return res\n\n    def _reset_cacher(self) -> None:\n        # no-op for DataFrame\n        pass\n\n    def _maybe_cache_changed(self, item, value: Series, inplace: bool) -> None:\n        \"\"\"\n        The object has called back to us saying maybe it has changed.\n        \"\"\"\n        loc = self._info_axis.get_loc(item)\n        arraylike = value._values\n        self._mgr.iset(loc, arraylike, inplace=inplace)\n\n    # ----------------------------------------------------------------------\n    # Unsorted\n\n    def query(self, expr: str, inplace: bool = False, **kwargs):\n        \"\"\"\n        Query the columns of a DataFrame with a boolean expression.\n\n        Parameters\n        ----------\n        expr : str\n            The query string to evaluate.\n\n            You can refer to variables\n            in the environment by prefixing them with an '@' character like\n            ``@a + b``.\n\n            You can refer to column names that are not valid Python variable names\n            by surrounding them in backticks. Thus, column names containing spaces\n            or punctuations (besides underscores) or starting with digits must be\n            surrounded by backticks. (For example, a column named \"Area (cm^2)\" would\n            be referenced as ```Area (cm^2)```). Column names which are Python keywords\n            (like \"list\", \"for\", \"import\", etc) cannot be used.\n\n            For example, if one of your columns is called ``a a`` and you want\n            to sum it with ``b``, your query should be ```a a` + b``.\n\n            .. versionadded:: 0.25.0\n                Backtick quoting introduced.\n\n            .. versionadded:: 1.0.0\n                Expanding functionality of backtick quoting for more than only spaces.\n\n        inplace : bool\n            Whether the query should modify the data in place or return\n            a modified copy.\n        **kwargs\n            See the documentation for :func:`eval` for complete details\n            on the keyword arguments accepted by :meth:`DataFrame.query`.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame resulting from the provided query expression or\n            None if ``inplace=True``.\n\n        See Also\n        --------\n        eval : Evaluate a string describing operations on\n            DataFrame columns.\n        DataFrame.eval : Evaluate a string describing operations on\n            DataFrame columns.\n\n        Notes\n        -----\n        The result of the evaluation of this expression is first passed to\n        :attr:`DataFrame.loc` and if that fails because of a\n        multidimensional key (e.g., a DataFrame) then the result will be passed\n        to :meth:`DataFrame.__getitem__`.\n\n        This method uses the top-level :func:`eval` function to\n        evaluate the passed query.\n\n        The :meth:`~pandas.DataFrame.query` method uses a slightly\n        modified Python syntax by default. For example, the ``&`` and ``|``\n        (bitwise) operators have the precedence of their boolean cousins,\n        :keyword:`and` and :keyword:`or`. This *is* syntactically valid Python,\n        however the semantics are different.\n\n        You can change the semantics of the expression by passing the keyword\n        argument ``parser='python'``. This enforces the same semantics as\n        evaluation in Python space. Likewise, you can pass ``engine='python'``\n        to evaluate an expression using Python itself as a backend. This is not\n        recommended as it is inefficient compared to using ``numexpr`` as the\n        engine.\n\n        The :attr:`DataFrame.index` and\n        :attr:`DataFrame.columns` attributes of the\n        :class:`~pandas.DataFrame` instance are placed in the query namespace\n        by default, which allows you to treat both the index and columns of the\n        frame as a column in the frame.\n        The identifier ``index`` is used for the frame index; you can also\n        use the name of the index to identify it in a query. Please note that\n        Python keywords may not be used as identifiers.\n\n        For further details and examples see the ``query`` documentation in\n        :ref:`indexing <indexing.query>`.\n\n        *Backtick quoted variables*\n\n        Backtick quoted variables are parsed as literal Python code and\n        are converted internally to a Python valid identifier.\n        This can lead to the following problems.\n\n        During parsing a number of disallowed characters inside the backtick\n        quoted string are replaced by strings that are allowed as a Python identifier.\n        These characters include all operators in Python, the space character, the\n        question mark, the exclamation mark, the dollar sign, and the euro sign.\n        For other characters that fall outside the ASCII range (U+0001..U+007F)\n        and those that are not further specified in PEP 3131,\n        the query parser will raise an error.\n        This excludes whitespace different than the space character,\n        but also the hashtag (as it is used for comments) and the backtick\n        itself (backtick can also not be escaped).\n\n        In a special case, quotes that make a pair around a backtick can\n        confuse the parser.\n        For example, ```it's` > `that's``` will raise an error,\n        as it forms a quoted string (``'s > `that'``) with a backtick inside.\n\n        See also the Python documentation about lexical analysis\n        (https://docs.python.org/3/reference/lexical_analysis.html)\n        in combination with the source code in :mod:`pandas.core.computation.parsing`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': range(1, 6),\n        ...                    'B': range(10, 0, -2),\n        ...                    'C C': range(10, 5, -1)})\n        >>> df\n           A   B  C C\n        0  1  10   10\n        1  2   8    9\n        2  3   6    8\n        3  4   4    7\n        4  5   2    6\n        >>> df.query('A > B')\n           A  B  C C\n        4  5  2    6\n\n        The previous expression is equivalent to\n\n        >>> df[df.A > df.B]\n           A  B  C C\n        4  5  2    6\n\n        For columns with spaces in their name, you can use backtick quoting.\n\n        >>> df.query('B == `C C`')\n           A   B  C C\n        0  1  10   10\n\n        The previous expression is equivalent to\n\n        >>> df[df.B == df['C C']]\n           A   B  C C\n        0  1  10   10\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if not isinstance(expr, str):\n            msg = f\"expr must be a string to be evaluated, {type(expr)} given\"\n            raise ValueError(msg)\n        kwargs[\"level\"] = kwargs.pop(\"level\", 0) + 1\n        kwargs[\"target\"] = None\n        res = self.eval(expr, **kwargs)\n\n        try:\n            result = self.loc[res]\n        except ValueError:\n            # when res is multi-dimensional loc raises, but this is sometimes a\n            # valid query\n            result = self[res]\n\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    def eval(self, expr: str, inplace: bool = False, **kwargs):\n        \"\"\"\n        Evaluate a string describing operations on DataFrame columns.\n\n        Operates on columns only, not specific rows or elements.  This allows\n        `eval` to run arbitrary code, which can make you vulnerable to code\n        injection if you pass user input to this function.\n\n        Parameters\n        ----------\n        expr : str\n            The expression string to evaluate.\n        inplace : bool, default False\n            If the expression contains an assignment, whether to perform the\n            operation inplace and mutate the existing DataFrame. Otherwise,\n            a new DataFrame is returned.\n        **kwargs\n            See the documentation for :func:`eval` for complete details\n            on the keyword arguments accepted by\n            :meth:`~pandas.DataFrame.query`.\n\n        Returns\n        -------\n        ndarray, scalar, pandas object, or None\n            The result of the evaluation or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.query : Evaluates a boolean expression to query the columns\n            of a frame.\n        DataFrame.assign : Can evaluate an expression or function to create new\n            values for a column.\n        eval : Evaluate a Python expression as a string using various\n            backends.\n\n        Notes\n        -----\n        For more details see the API documentation for :func:`~eval`.\n        For detailed examples see :ref:`enhancing performance with eval\n        <enhancingperf.eval>`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': range(1, 6), 'B': range(10, 0, -2)})\n        >>> df\n           A   B\n        0  1  10\n        1  2   8\n        2  3   6\n        3  4   4\n        4  5   2\n        >>> df.eval('A + B')\n        0    11\n        1    10\n        2     9\n        3     8\n        4     7\n        dtype: int64\n\n        Assignment is allowed though by default the original DataFrame is not\n        modified.\n\n        >>> df.eval('C = A + B')\n           A   B   C\n        0  1  10  11\n        1  2   8  10\n        2  3   6   9\n        3  4   4   8\n        4  5   2   7\n        >>> df\n           A   B\n        0  1  10\n        1  2   8\n        2  3   6\n        3  4   4\n        4  5   2\n\n        Use ``inplace=True`` to modify the original DataFrame.\n\n        >>> df.eval('C = A + B', inplace=True)\n        >>> df\n           A   B   C\n        0  1  10  11\n        1  2   8  10\n        2  3   6   9\n        3  4   4   8\n        4  5   2   7\n\n        Multiple columns can be assigned to using multi-line expressions:\n\n        >>> df.eval(\n        ...     '''\n        ... C = A + B\n        ... D = A - B\n        ... '''\n        ... )\n           A   B   C  D\n        0  1  10  11 -9\n        1  2   8  10 -6\n        2  3   6   9 -3\n        3  4   4   8  0\n        4  5   2   7  3\n        \"\"\"\n        from pandas.core.computation.eval import eval as _eval\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        resolvers = kwargs.pop(\"resolvers\", None)\n        kwargs[\"level\"] = kwargs.pop(\"level\", 0) + 1\n        if resolvers is None:\n            index_resolvers = self._get_index_resolvers()\n            column_resolvers = self._get_cleaned_column_resolvers()\n            resolvers = column_resolvers, index_resolvers\n        if \"target\" not in kwargs:\n            kwargs[\"target\"] = self\n        kwargs[\"resolvers\"] = kwargs.get(\"resolvers\", ()) + tuple(resolvers)\n\n        return _eval(expr, inplace=inplace, **kwargs)\n\n    def select_dtypes(self, include=None, exclude=None) -> DataFrame:\n        \"\"\"\n        Return a subset of the DataFrame's columns based on the column dtypes.\n\n        Parameters\n        ----------\n        include, exclude : scalar or list-like\n            A selection of dtypes or strings to be included/excluded. At least\n            one of these parameters must be supplied.\n\n        Returns\n        -------\n        DataFrame\n            The subset of the frame including the dtypes in ``include`` and\n            excluding the dtypes in ``exclude``.\n\n        Raises\n        ------\n        ValueError\n            * If both of ``include`` and ``exclude`` are empty\n            * If ``include`` and ``exclude`` have overlapping elements\n            * If any kind of string dtype is passed in.\n\n        See Also\n        --------\n        DataFrame.dtypes: Return Series with the data type of each column.\n\n        Notes\n        -----\n        * To select all *numeric* types, use ``np.number`` or ``'number'``\n        * To select strings you must use the ``object`` dtype, but note that\n          this will return *all* object dtype columns\n        * See the `numpy dtype hierarchy\n          <https://numpy.org/doc/stable/reference/arrays.scalars.html>`__\n        * To select datetimes, use ``np.datetime64``, ``'datetime'`` or\n          ``'datetime64'``\n        * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or\n          ``'timedelta64'``\n        * To select Pandas categorical dtypes, use ``'category'``\n        * To select Pandas datetimetz dtypes, use ``'datetimetz'`` (new in\n          0.20.0) or ``'datetime64[ns, tz]'``\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'a': [1, 2] * 3,\n        ...                    'b': [True, False] * 3,\n        ...                    'c': [1.0, 2.0] * 3})\n        >>> df\n                a      b  c\n        0       1   True  1.0\n        1       2  False  2.0\n        2       1   True  1.0\n        3       2  False  2.0\n        4       1   True  1.0\n        5       2  False  2.0\n\n        >>> df.select_dtypes(include='bool')\n           b\n        0  True\n        1  False\n        2  True\n        3  False\n        4  True\n        5  False\n\n        >>> df.select_dtypes(include=['float64'])\n           c\n        0  1.0\n        1  2.0\n        2  1.0\n        3  2.0\n        4  1.0\n        5  2.0\n\n        >>> df.select_dtypes(exclude=['int64'])\n               b    c\n        0   True  1.0\n        1  False  2.0\n        2   True  1.0\n        3  False  2.0\n        4   True  1.0\n        5  False  2.0\n        \"\"\"\n        if not is_list_like(include):\n            include = (include,) if include is not None else ()\n        if not is_list_like(exclude):\n            exclude = (exclude,) if exclude is not None else ()\n\n        selection = (frozenset(include), frozenset(exclude))\n\n        if not any(selection):\n            raise ValueError(\"at least one of include or exclude must be nonempty\")\n\n        # convert the myriad valid dtypes object to a single representation\n        def check_int_infer_dtype(dtypes):\n            converted_dtypes = []\n            for dtype in dtypes:\n                # Numpy maps int to different types (int32, in64) on Windows and Linux\n                # see https://github.com/numpy/numpy/issues/9464\n                if (isinstance(dtype, str) and dtype == \"int\") or (dtype is int):\n                    converted_dtypes.append(np.int32)\n                    # error: Argument 1 to \"append\" of \"list\" has incompatible type\n                    # \"Type[signedinteger[Any]]\"; expected \"Type[signedinteger[Any]]\"\n                    converted_dtypes.append(np.int64)  # type: ignore[arg-type]\n                elif dtype == \"float\" or dtype is float:\n                    # GH#42452 : np.dtype(\"float\") coerces to np.float64 from Numpy 1.20\n                    converted_dtypes.extend(\n                        [np.float64, np.float32]  # type: ignore[list-item]\n                    )\n                else:\n                    # error: Argument 1 to \"append\" of \"list\" has incompatible type\n                    # \"Union[dtype[Any], ExtensionDtype]\"; expected\n                    # \"Type[signedinteger[Any]]\"\n                    converted_dtypes.append(\n                        infer_dtype_from_object(dtype)  # type: ignore[arg-type]\n                    )\n            return frozenset(converted_dtypes)\n\n        include = check_int_infer_dtype(include)\n        exclude = check_int_infer_dtype(exclude)\n\n        for dtypes in (include, exclude):\n            invalidate_string_dtypes(dtypes)\n\n        # can't both include AND exclude!\n        if not include.isdisjoint(exclude):\n            raise ValueError(f\"include and exclude overlap on {(include & exclude)}\")\n\n        def dtype_predicate(dtype: DtypeObj, dtypes_set) -> bool:\n            return issubclass(dtype.type, tuple(dtypes_set)) or (\n                np.number in dtypes_set and getattr(dtype, \"_is_numeric\", False)\n            )\n\n        def predicate(arr: ArrayLike) -> bool:\n            dtype = arr.dtype\n            if include:\n                if not dtype_predicate(dtype, include):\n                    return False\n\n            if exclude:\n                if dtype_predicate(dtype, exclude):\n                    return False\n\n            return True\n\n        mgr = self._mgr._get_data_subset(predicate)\n        return type(self)(mgr).__finalize__(self)\n\n    def insert(\n        self,\n        loc: int,\n        column: Hashable,\n        value: Scalar | AnyArrayLike,\n        allow_duplicates: bool = False,\n    ) -> None:\n        \"\"\"\n        Insert column into DataFrame at specified location.\n\n        Raises a ValueError if `column` is already contained in the DataFrame,\n        unless `allow_duplicates` is set to True.\n\n        Parameters\n        ----------\n        loc : int\n            Insertion index. Must verify 0 <= loc <= len(columns).\n        column : str, number, or hashable object\n            Label of the inserted column.\n        value : Scalar, Series, or array-like\n        allow_duplicates : bool, optional default False\n\n        See Also\n        --------\n        Index.insert : Insert new item by index.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n        >>> df\n           col1  col2\n        0     1     3\n        1     2     4\n        >>> df.insert(1, \"newcol\", [99, 99])\n        >>> df\n           col1  newcol  col2\n        0     1      99     3\n        1     2      99     4\n        >>> df.insert(0, \"col1\", [100, 100], allow_duplicates=True)\n        >>> df\n           col1  col1  newcol  col2\n        0   100     1      99     3\n        1   100     2      99     4\n\n        Notice that pandas uses index alignment in case of `value` from type `Series`:\n\n        >>> df.insert(0, \"col0\", pd.Series([5, 6], index=[1, 2]))\n        >>> df\n           col0  col1  col1  newcol  col2\n        0   NaN   100     1      99     3\n        1   5.0   100     2      99     4\n        \"\"\"\n        if allow_duplicates and not self.flags.allows_duplicate_labels:\n            raise ValueError(\n                \"Cannot specify 'allow_duplicates=True' when \"\n                \"'self.flags.allows_duplicate_labels' is False.\"\n            )\n        if not allow_duplicates and column in self.columns:\n            # Should this be a different kind of error??\n            raise ValueError(f\"cannot insert {column}, already exists\")\n        if not isinstance(loc, int):\n            raise TypeError(\"loc must be int\")\n\n        value = self._sanitize_column(value)\n        self._mgr.insert(loc, column, value)\n\n    def assign(self, **kwargs) -> DataFrame:\n        r\"\"\"\n        Assign new columns to a DataFrame.\n\n        Returns a new object with all original columns in addition to new ones.\n        Existing columns that are re-assigned will be overwritten.\n\n        Parameters\n        ----------\n        **kwargs : dict of {str: callable or Series}\n            The column names are keywords. If the values are\n            callable, they are computed on the DataFrame and\n            assigned to the new columns. The callable must not\n            change input DataFrame (though pandas doesn't check it).\n            If the values are not callable, (e.g. a Series, scalar, or array),\n            they are simply assigned.\n\n        Returns\n        -------\n        DataFrame\n            A new DataFrame with the new columns in addition to\n            all the existing columns.\n\n        Notes\n        -----\n        Assigning multiple columns within the same ``assign`` is possible.\n        Later items in '\\*\\*kwargs' may refer to newly created or modified\n        columns in 'df'; items are computed and assigned into 'df' in order.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'temp_c': [17.0, 25.0]},\n        ...                   index=['Portland', 'Berkeley'])\n        >>> df\n                  temp_c\n        Portland    17.0\n        Berkeley    25.0\n\n        Where the value is a callable, evaluated on `df`:\n\n        >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)\n                  temp_c  temp_f\n        Portland    17.0    62.6\n        Berkeley    25.0    77.0\n\n        Alternatively, the same behavior can be achieved by directly\n        referencing an existing Series or sequence:\n\n        >>> df.assign(temp_f=df['temp_c'] * 9 / 5 + 32)\n                  temp_c  temp_f\n        Portland    17.0    62.6\n        Berkeley    25.0    77.0\n\n        You can create multiple columns within the same assign where one\n        of the columns depends on another one defined within the same assign:\n\n        >>> df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32,\n        ...           temp_k=lambda x: (x['temp_f'] +  459.67) * 5 / 9)\n                  temp_c  temp_f  temp_k\n        Portland    17.0    62.6  290.15\n        Berkeley    25.0    77.0  298.15\n        \"\"\"\n        data = self.copy()\n\n        for k, v in kwargs.items():\n            data[k] = com.apply_if_callable(v, data)\n        return data\n\n    def _sanitize_column(self, value) -> ArrayLike:\n        \"\"\"\n        Ensures new columns (which go into the BlockManager as new blocks) are\n        always copied and converted into an array.\n\n        Parameters\n        ----------\n        value : scalar, Series, or array-like\n\n        Returns\n        -------\n        numpy.ndarray or ExtensionArray\n        \"\"\"\n        self._ensure_valid_index(value)\n\n        # We should never get here with DataFrame value\n        if isinstance(value, Series):\n            return _reindex_for_setitem(value, self.index)\n\n        if is_list_like(value):\n            com.require_length_match(value, self.index)\n        return sanitize_array(value, self.index, copy=True, allow_2d=True)\n\n    @property\n    def _series(self):\n        return {\n            item: Series(\n                self._mgr.iget(idx), index=self.index, name=item, fastpath=True\n            )\n            for idx, item in enumerate(self.columns)\n        }\n\n    def lookup(\n        self, row_labels: Sequence[IndexLabel], col_labels: Sequence[IndexLabel]\n    ) -> np.ndarray:\n        \"\"\"\n        Label-based \"fancy indexing\" function for DataFrame.\n        Given equal-length arrays of row and column labels, return an\n        array of the values corresponding to each (row, col) pair.\n\n        .. deprecated:: 1.2.0\n            DataFrame.lookup is deprecated,\n            use DataFrame.melt and DataFrame.loc instead.\n            For further details see\n            :ref:`Looking up values by index/column labels <indexing.lookup>`.\n\n        Parameters\n        ----------\n        row_labels : sequence\n            The row labels to use for lookup.\n        col_labels : sequence\n            The column labels to use for lookup.\n\n        Returns\n        -------\n        numpy.ndarray\n            The found values.\n        \"\"\"\n        msg = (\n            \"The 'lookup' method is deprecated and will be \"\n            \"removed in a future version. \"\n            \"You can use DataFrame.melt and DataFrame.loc \"\n            \"as a substitute.\"\n        )\n        warnings.warn(msg, FutureWarning, stacklevel=find_stack_level())\n\n        n = len(row_labels)\n        if n != len(col_labels):\n            raise ValueError(\"Row labels must have same size as column labels\")\n        if not (self.index.is_unique and self.columns.is_unique):\n            # GH#33041\n            raise ValueError(\"DataFrame.lookup requires unique index and columns\")\n\n        thresh = 1000\n        if not self._is_mixed_type or n > thresh:\n            values = self.values\n            ridx = self.index.get_indexer(row_labels)\n            cidx = self.columns.get_indexer(col_labels)\n            if (ridx == -1).any():\n                raise KeyError(\"One or more row labels was not found\")\n            if (cidx == -1).any():\n                raise KeyError(\"One or more column labels was not found\")\n            flat_index = ridx * len(self.columns) + cidx\n            result = values.flat[flat_index]\n        else:\n            result = np.empty(n, dtype=\"O\")\n            for i, (r, c) in enumerate(zip(row_labels, col_labels)):\n                result[i] = self._get_value(r, c)\n\n        if is_object_dtype(result):\n            result = lib.maybe_convert_objects(result)\n\n        return result\n\n    # ----------------------------------------------------------------------\n    # Reindexing and alignment\n\n    def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy):\n        frame = self\n\n        columns = axes[\"columns\"]\n        if columns is not None:\n            frame = frame._reindex_columns(\n                columns, method, copy, level, fill_value, limit, tolerance\n            )\n\n        index = axes[\"index\"]\n        if index is not None:\n            frame = frame._reindex_index(\n                index, method, copy, level, fill_value, limit, tolerance\n            )\n\n        return frame\n\n    def _reindex_index(\n        self,\n        new_index,\n        method,\n        copy: bool,\n        level: Level,\n        fill_value=np.nan,\n        limit=None,\n        tolerance=None,\n    ):\n        new_index, indexer = self.index.reindex(\n            new_index, method=method, level=level, limit=limit, tolerance=tolerance\n        )\n        return self._reindex_with_indexers(\n            {0: [new_index, indexer]},\n            copy=copy,\n            fill_value=fill_value,\n            allow_dups=False,\n        )\n\n    def _reindex_columns(\n        self,\n        new_columns,\n        method,\n        copy: bool,\n        level: Level,\n        fill_value=None,\n        limit=None,\n        tolerance=None,\n    ):\n        new_columns, indexer = self.columns.reindex(\n            new_columns, method=method, level=level, limit=limit, tolerance=tolerance\n        )\n        return self._reindex_with_indexers(\n            {1: [new_columns, indexer]},\n            copy=copy,\n            fill_value=fill_value,\n            allow_dups=False,\n        )\n\n    def _reindex_multi(\n        self, axes: dict[str, Index], copy: bool, fill_value\n    ) -> DataFrame:\n        \"\"\"\n        We are guaranteed non-Nones in the axes.\n        \"\"\"\n\n        new_index, row_indexer = self.index.reindex(axes[\"index\"])\n        new_columns, col_indexer = self.columns.reindex(axes[\"columns\"])\n\n        if row_indexer is not None and col_indexer is not None:\n            # Fastpath. By doing two 'take's at once we avoid making an\n            #  unnecessary copy.\n            # We only get here with `not self._is_mixed_type`, which (almost)\n            #  ensures that self.values is cheap. It may be worth making this\n            #  condition more specific.\n            indexer = row_indexer, col_indexer\n            new_values = take_2d_multi(self.values, indexer, fill_value=fill_value)\n            return self._constructor(new_values, index=new_index, columns=new_columns)\n        else:\n            return self._reindex_with_indexers(\n                {0: [new_index, row_indexer], 1: [new_columns, col_indexer]},\n                copy=copy,\n                fill_value=fill_value,\n            )\n\n    @doc(NDFrame.align, **_shared_doc_kwargs)\n    def align(\n        self,\n        other,\n        join: str = \"outer\",\n        axis: Axis | None = None,\n        level: Level | None = None,\n        copy: bool = True,\n        fill_value=None,\n        method: str | None = None,\n        limit=None,\n        fill_axis: Axis = 0,\n        broadcast_axis: Axis | None = None,\n    ) -> DataFrame:\n        return super().align(\n            other,\n            join=join,\n            axis=axis,\n            level=level,\n            copy=copy,\n            fill_value=fill_value,\n            method=method,\n            limit=limit,\n            fill_axis=fill_axis,\n            broadcast_axis=broadcast_axis,\n        )\n\n    @overload\n    def set_axis(\n        self, labels, axis: Axis = ..., inplace: Literal[False] = ...\n    ) -> DataFrame:\n        ...\n\n    @overload\n    def set_axis(self, labels, axis: Axis, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def set_axis(self, labels, *, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def set_axis(\n        self, labels, axis: Axis = ..., inplace: bool = ...\n    ) -> DataFrame | None:\n        ...\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"labels\"])\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\n        Change the row labels.\n\n        >>> df.set_axis(['a', 'b', 'c'], axis='index')\n           A  B\n        a  1  4\n        b  2  5\n        c  3  6\n\n        Change the column labels.\n\n        >>> df.set_axis(['I', 'II'], axis='columns')\n           I  II\n        0  1   4\n        1  2   5\n        2  3   6\n\n        Now, update the labels inplace.\n\n        >>> df.set_axis(['i', 'ii'], axis='columns', inplace=True)\n        >>> df\n           i  ii\n        0  1   4\n        1  2   5\n        2  3   6\n        \"\"\"\n    )\n    @Substitution(\n        **_shared_doc_kwargs,\n        extended_summary_sub=\" column or\",\n        axis_description_sub=\", and 1 identifies the columns\",\n        see_also_sub=\" or columns\",\n    )\n    @Appender(NDFrame.set_axis.__doc__)\n    def set_axis(self, labels, axis: Axis = 0, inplace: bool = False):\n        return super().set_axis(labels, axis=axis, inplace=inplace)\n\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(NDFrame.reindex.__doc__)\n    @rewrite_axis_style_signature(\n        \"labels\",\n        [\n            (\"method\", None),\n            (\"copy\", True),\n            (\"level\", None),\n            (\"fill_value\", np.nan),\n            (\"limit\", None),\n            (\"tolerance\", None),\n        ],\n    )\n    def reindex(self, *args, **kwargs) -> DataFrame:\n        axes = validate_axis_style_args(self, args, kwargs, \"labels\", \"reindex\")\n        kwargs.update(axes)\n        # Pop these, since the values are in `kwargs` under different names\n        kwargs.pop(\"axis\", None)\n        kwargs.pop(\"labels\", None)\n        return super().reindex(**kwargs)\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"labels\"])\n    def drop(\n        self,\n        labels=None,\n        axis: Axis = 0,\n        index=None,\n        columns=None,\n        level: Level | None = None,\n        inplace: bool = False,\n        errors: str = \"raise\",\n    ):\n        \"\"\"\n        Drop specified labels from rows or columns.\n\n        Remove rows or columns by specifying label names and corresponding\n        axis, or by specifying directly index or column names. When using a\n        multi-index, labels on different levels can be removed by specifying\n        the level. See the `user guide <advanced.shown_levels>`\n        for more information about the now unused levels.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index or column labels to drop. A tuple will be used as a single\n            label and not treated as a list-like.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Whether to drop labels from the index (0 or 'index') or\n            columns (1 or 'columns').\n        index : single label or list-like\n            Alternative to specifying axis (``labels, axis=0``\n            is equivalent to ``index=labels``).\n        columns : single label or list-like\n            Alternative to specifying axis (``labels, axis=1``\n            is equivalent to ``columns=labels``).\n        level : int or level name, optional\n            For MultiIndex, level from which the labels will be removed.\n        inplace : bool, default False\n            If False, return a copy. Otherwise, do operation\n            inplace and return None.\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and only existing labels are\n            dropped.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame without the removed index or column labels or\n            None if ``inplace=True``.\n\n        Raises\n        ------\n        KeyError\n            If any of the labels is not found in the selected axis.\n\n        See Also\n        --------\n        DataFrame.loc : Label-location based indexer for selection by label.\n        DataFrame.dropna : Return DataFrame with labels on given axis omitted\n            where (all or any) data are missing.\n        DataFrame.drop_duplicates : Return DataFrame with duplicate rows\n            removed, optionally only considering certain columns.\n        Series.drop : Return Series with specified index labels removed.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n        ...                   columns=['A', 'B', 'C', 'D'])\n        >>> df\n           A  B   C   D\n        0  0  1   2   3\n        1  4  5   6   7\n        2  8  9  10  11\n\n        Drop columns\n\n        >>> df.drop(['B', 'C'], axis=1)\n           A   D\n        0  0   3\n        1  4   7\n        2  8  11\n\n        >>> df.drop(columns=['B', 'C'])\n           A   D\n        0  0   3\n        1  4   7\n        2  8  11\n\n        Drop a row by index\n\n        >>> df.drop([0, 1])\n           A  B   C   D\n        2  8  9  10  11\n\n        Drop columns and/or rows of MultiIndex DataFrame\n\n        >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n        ...                              ['speed', 'weight', 'length']],\n        ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n        ...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n        ...                         [250, 150], [1.5, 0.8], [320, 250],\n        ...                         [1, 0.8], [0.3, 0.2]])\n        >>> df\n                        big     small\n        lama    speed   45.0    30.0\n                weight  200.0   100.0\n                length  1.5     1.0\n        cow     speed   30.0    20.0\n                weight  250.0   150.0\n                length  1.5     0.8\n        falcon  speed   320.0   250.0\n                weight  1.0     0.8\n                length  0.3     0.2\n\n        Drop a specific index combination from the MultiIndex\n        DataFrame, i.e., drop the combination ``'falcon'`` and\n        ``'weight'``, which deletes only the corresponding row\n\n        >>> df.drop(index=('falcon', 'weight'))\n                        big     small\n        lama    speed   45.0    30.0\n                weight  200.0   100.0\n                length  1.5     1.0\n        cow     speed   30.0    20.0\n                weight  250.0   150.0\n                length  1.5     0.8\n        falcon  speed   320.0   250.0\n                length  0.3     0.2\n\n        >>> df.drop(index='cow', columns='small')\n                        big\n        lama    speed   45.0\n                weight  200.0\n                length  1.5\n        falcon  speed   320.0\n                weight  1.0\n                length  0.3\n\n        >>> df.drop(index='length', level=1)\n                        big     small\n        lama    speed   45.0    30.0\n                weight  200.0   100.0\n        cow     speed   30.0    20.0\n                weight  250.0   150.0\n        falcon  speed   320.0   250.0\n                weight  1.0     0.8\n        \"\"\"\n        return super().drop(\n            labels=labels,\n            axis=axis,\n            index=index,\n            columns=columns,\n            level=level,\n            inplace=inplace,\n            errors=errors,\n        )\n\n    def rename(\n        self,\n        mapper: Renamer | None = None,\n        *,\n        index: Renamer | None = None,\n        columns: Renamer | None = None,\n        axis: Axis | None = None,\n        copy: bool = True,\n        inplace: bool = False,\n        level: Level | None = None,\n        errors: str = \"ignore\",\n    ) -> DataFrame | None:\n        \"\"\"\n        Alter axes labels.\n\n        Function / dict values must be unique (1-to-1). Labels not contained in\n        a dict / Series will be left as-is. Extra labels listed don't throw an\n        error.\n\n        See the :ref:`user guide <basics.rename>` for more.\n\n        Parameters\n        ----------\n        mapper : dict-like or function\n            Dict-like or function transformations to apply to\n            that axis' values. Use either ``mapper`` and ``axis`` to\n            specify the axis to target with ``mapper``, or ``index`` and\n            ``columns``.\n        index : dict-like or function\n            Alternative to specifying axis (``mapper, axis=0``\n            is equivalent to ``index=mapper``).\n        columns : dict-like or function\n            Alternative to specifying axis (``mapper, axis=1``\n            is equivalent to ``columns=mapper``).\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis to target with ``mapper``. Can be either the axis name\n            ('index', 'columns') or number (0, 1). The default is 'index'.\n        copy : bool, default True\n            Also copy underlying data.\n        inplace : bool, default False\n            Whether to return a new DataFrame. If True then value of copy is\n            ignored.\n        level : int or level name, default None\n            In case of a MultiIndex, only rename labels in the specified\n            level.\n        errors : {'ignore', 'raise'}, default 'ignore'\n            If 'raise', raise a `KeyError` when a dict-like `mapper`, `index`,\n            or `columns` contains labels that are not present in the Index\n            being transformed.\n            If 'ignore', existing keys will be renamed and extra keys will be\n            ignored.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with the renamed axis labels or None if ``inplace=True``.\n\n        Raises\n        ------\n        KeyError\n            If any of the labels is not found in the selected axis and\n            \"errors='raise'\".\n\n        See Also\n        --------\n        DataFrame.rename_axis : Set the name of the axis.\n\n        Examples\n        --------\n        ``DataFrame.rename`` supports two calling conventions\n\n        * ``(index=index_mapper, columns=columns_mapper, ...)``\n        * ``(mapper, axis={'index', 'columns'}, ...)``\n\n        We *highly* recommend using keyword arguments to clarify your\n        intent.\n\n        Rename columns using a mapping:\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n        >>> df.rename(columns={\"A\": \"a\", \"B\": \"c\"})\n           a  c\n        0  1  4\n        1  2  5\n        2  3  6\n\n        Rename index using a mapping:\n\n        >>> df.rename(index={0: \"x\", 1: \"y\", 2: \"z\"})\n           A  B\n        x  1  4\n        y  2  5\n        z  3  6\n\n        Cast index labels to a different type:\n\n        >>> df.index\n        RangeIndex(start=0, stop=3, step=1)\n        >>> df.rename(index=str).index\n        Index(['0', '1', '2'], dtype='object')\n\n        >>> df.rename(columns={\"A\": \"a\", \"B\": \"b\", \"C\": \"c\"}, errors=\"raise\")\n        Traceback (most recent call last):\n        KeyError: ['C'] not found in axis\n\n        Using axis-style parameters:\n\n        >>> df.rename(str.lower, axis='columns')\n           a  b\n        0  1  4\n        1  2  5\n        2  3  6\n\n        >>> df.rename({1: 2, 2: 4}, axis='index')\n           A  B\n        0  1  4\n        2  2  5\n        4  3  6\n        \"\"\"\n        return super().rename(\n            mapper=mapper,\n            index=index,\n            columns=columns,\n            axis=axis,\n            copy=copy,\n            inplace=inplace,\n            level=level,\n            errors=errors,\n        )\n\n    @overload\n    def fillna(\n        self,\n        value=...,\n        method: FillnaOptions | None = ...,\n        axis: Axis | None = ...,\n        inplace: Literal[False] = ...,\n        limit=...,\n        downcast=...,\n    ) -> DataFrame:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        method: FillnaOptions | None,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        method: FillnaOptions | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        method: FillnaOptions | None,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        *,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        method: FillnaOptions | None,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value=...,\n        method: FillnaOptions | None = ...,\n        axis: Axis | None = ...,\n        inplace: bool = ...,\n        limit=...,\n        downcast=...,\n    ) -> DataFrame | None:\n        ...\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"value\"])\n    @doc(NDFrame.fillna, **_shared_doc_kwargs)\n    def fillna(\n        self,\n        value: object | ArrayLike | None = None,\n        method: FillnaOptions | None = None,\n        axis: Axis | None = None,\n        inplace: bool = False,\n        limit=None,\n        downcast=None,\n    ) -> DataFrame | None:\n        return super().fillna(\n            value=value,\n            method=method,\n            axis=axis,\n            inplace=inplace,\n            limit=limit,\n            downcast=downcast,\n        )\n\n    def pop(self, item: Hashable) -> Series:\n        \"\"\"\n        Return item and drop from frame. Raise KeyError if not found.\n\n        Parameters\n        ----------\n        item : label\n            Label of column to be popped.\n\n        Returns\n        -------\n        Series\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n        ...                    ('parrot', 'bird', 24.0),\n        ...                    ('lion', 'mammal', 80.5),\n        ...                    ('monkey', 'mammal', np.nan)],\n        ...                   columns=('name', 'class', 'max_speed'))\n        >>> df\n             name   class  max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        >>> df.pop('class')\n        0      bird\n        1      bird\n        2    mammal\n        3    mammal\n        Name: class, dtype: object\n\n        >>> df\n             name  max_speed\n        0  falcon      389.0\n        1  parrot       24.0\n        2    lion       80.5\n        3  monkey        NaN\n        \"\"\"\n        return super().pop(item=item)\n\n    @doc(NDFrame.replace, **_shared_doc_kwargs)\n    def replace(\n        self,\n        to_replace=None,\n        value=None,\n        inplace: bool = False,\n        limit=None,\n        regex: bool = False,\n        method: str = \"pad\",\n    ):\n        return super().replace(\n            to_replace=to_replace,\n            value=value,\n            inplace=inplace,\n            limit=limit,\n            regex=regex,\n            method=method,\n        )\n\n    def _replace_columnwise(\n        self, mapping: dict[Hashable, tuple[Any, Any]], inplace: bool, regex\n    ):\n        \"\"\"\n        Dispatch to Series.replace column-wise.\n\n        Parameters\n        ----------\n        mapping : dict\n            of the form {col: (target, value)}\n        inplace : bool\n        regex : bool or same types as `to_replace` in DataFrame.replace\n\n        Returns\n        -------\n        DataFrame or None\n        \"\"\"\n        # Operate column-wise\n        res = self if inplace else self.copy()\n        ax = self.columns\n\n        for i in range(len(ax)):\n            if ax[i] in mapping:\n                ser = self.iloc[:, i]\n\n                target, value = mapping[ax[i]]\n                newobj = ser.replace(target, value, regex=regex)\n\n                res.iloc[:, i] = newobj\n\n        if inplace:\n            return\n        return res.__finalize__(self)\n\n    @doc(NDFrame.shift, klass=_shared_doc_kwargs[\"klass\"])\n    def shift(\n        self,\n        periods=1,\n        freq: Frequency | None = None,\n        axis: Axis = 0,\n        fill_value=lib.no_default,\n    ) -> DataFrame:\n        axis = self._get_axis_number(axis)\n\n        ncols = len(self.columns)\n        if axis == 1 and periods != 0 and fill_value is lib.no_default and ncols > 0:\n            # We will infer fill_value to match the closest column\n\n            # Use a column that we know is valid for our column's dtype GH#38434\n            label = self.columns[0]\n\n            if periods > 0:\n                result = self.iloc[:, :-periods]\n                for col in range(min(ncols, abs(periods))):\n                    # TODO(EA2D): doing this in a loop unnecessary with 2D EAs\n                    # Define filler inside loop so we get a copy\n                    filler = self.iloc[:, 0].shift(len(self))\n                    result.insert(0, label, filler, allow_duplicates=True)\n            else:\n                result = self.iloc[:, -periods:]\n                for col in range(min(ncols, abs(periods))):\n                    # Define filler inside loop so we get a copy\n                    filler = self.iloc[:, -1].shift(len(self))\n                    result.insert(\n                        len(result.columns), label, filler, allow_duplicates=True\n                    )\n\n            result.columns = self.columns.copy()\n            return result\n\n        return super().shift(\n            periods=periods, freq=freq, axis=axis, fill_value=fill_value\n        )\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"keys\"])\n    def set_index(\n        self,\n        keys,\n        drop: bool = True,\n        append: bool = False,\n        inplace: bool = False,\n        verify_integrity: bool = False,\n    ):\n        \"\"\"\n        Set the DataFrame index using existing columns.\n\n        Set the DataFrame index (row labels) using one or more existing\n        columns or arrays (of the correct length). The index can replace the\n        existing index or expand on it.\n\n        Parameters\n        ----------\n        keys : label or array-like or list of labels/arrays\n            This parameter can be either a single column key, a single array of\n            the same length as the calling DataFrame, or a list containing an\n            arbitrary combination of column keys and arrays. Here, \"array\"\n            encompasses :class:`Series`, :class:`Index`, ``np.ndarray``, and\n            instances of :class:`~collections.abc.Iterator`.\n        drop : bool, default True\n            Delete columns to be used as the new index.\n        append : bool, default False\n            Whether to append columns to existing index.\n        inplace : bool, default False\n            If True, modifies the DataFrame in place (do not create a new object).\n        verify_integrity : bool, default False\n            Check the new index for duplicates. Otherwise defer the check until\n            necessary. Setting to False will improve the performance of this\n            method.\n\n        Returns\n        -------\n        DataFrame or None\n            Changed row labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.reset_index : Opposite of set_index.\n        DataFrame.reindex : Change to new indices or expand indices.\n        DataFrame.reindex_like : Change to same indices as other DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'month': [1, 4, 7, 10],\n        ...                    'year': [2012, 2014, 2013, 2014],\n        ...                    'sale': [55, 40, 84, 31]})\n        >>> df\n           month  year  sale\n        0      1  2012    55\n        1      4  2014    40\n        2      7  2013    84\n        3     10  2014    31\n\n        Set the index to become the 'month' column:\n\n        >>> df.set_index('month')\n               year  sale\n        month\n        1      2012    55\n        4      2014    40\n        7      2013    84\n        10     2014    31\n\n        Create a MultiIndex using columns 'year' and 'month':\n\n        >>> df.set_index(['year', 'month'])\n                    sale\n        year  month\n        2012  1     55\n        2014  4     40\n        2013  7     84\n        2014  10    31\n\n        Create a MultiIndex using an Index and a column:\n\n        >>> df.set_index([pd.Index([1, 2, 3, 4]), 'year'])\n                 month  sale\n           year\n        1  2012  1      55\n        2  2014  4      40\n        3  2013  7      84\n        4  2014  10     31\n\n        Create a MultiIndex using two Series:\n\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> df.set_index([s, s**2])\n              month  year  sale\n        1 1       1  2012    55\n        2 4       4  2014    40\n        3 9       7  2013    84\n        4 16     10  2014    31\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        self._check_inplace_and_allows_duplicate_labels(inplace)\n        if not isinstance(keys, list):\n            keys = [keys]\n\n        err_msg = (\n            'The parameter \"keys\" may be a column key, one-dimensional '\n            \"array, or a list containing only valid column keys and \"\n            \"one-dimensional arrays.\"\n        )\n\n        missing: list[Hashable] = []\n        for col in keys:\n            if isinstance(col, (Index, Series, np.ndarray, list, abc.Iterator)):\n                # arrays are fine as long as they are one-dimensional\n                # iterators get converted to list below\n                if getattr(col, \"ndim\", 1) != 1:\n                    raise ValueError(err_msg)\n            else:\n                # everything else gets tried as a key; see GH 24969\n                try:\n                    found = col in self.columns\n                except TypeError as err:\n                    raise TypeError(\n                        f\"{err_msg}. Received column of type {type(col)}\"\n                    ) from err\n                else:\n                    if not found:\n                        missing.append(col)\n\n        if missing:\n            raise KeyError(f\"None of {missing} are in the columns\")\n\n        if inplace:\n            frame = self\n        else:\n            frame = self.copy()\n\n        arrays = []\n        names: list[Hashable] = []\n        if append:\n            names = list(self.index.names)\n            if isinstance(self.index, MultiIndex):\n                for i in range(self.index.nlevels):\n                    arrays.append(self.index._get_level_values(i))\n            else:\n                arrays.append(self.index)\n\n        to_remove: list[Hashable] = []\n        for col in keys:\n            if isinstance(col, MultiIndex):\n                for n in range(col.nlevels):\n                    arrays.append(col._get_level_values(n))\n                names.extend(col.names)\n            elif isinstance(col, (Index, Series)):\n                # if Index then not MultiIndex (treated above)\n\n                # error: Argument 1 to \"append\" of \"list\" has incompatible type\n                #  \"Union[Index, Series]\"; expected \"Index\"\n                arrays.append(col)  # type:ignore[arg-type]\n                names.append(col.name)\n            elif isinstance(col, (list, np.ndarray)):\n                # error: Argument 1 to \"append\" of \"list\" has incompatible type\n                # \"Union[List[Any], ndarray]\"; expected \"Index\"\n                arrays.append(col)  # type: ignore[arg-type]\n                names.append(None)\n            elif isinstance(col, abc.Iterator):\n                # error: Argument 1 to \"append\" of \"list\" has incompatible type\n                # \"List[Any]\"; expected \"Index\"\n                arrays.append(list(col))  # type: ignore[arg-type]\n                names.append(None)\n            # from here, col can only be a column label\n            else:\n                arrays.append(frame[col]._values)\n                names.append(col)\n                if drop:\n                    to_remove.append(col)\n\n            if len(arrays[-1]) != len(self):\n                # check newest element against length of calling frame, since\n                # ensure_index_from_sequences would not raise for append=False.\n                raise ValueError(\n                    f\"Length mismatch: Expected {len(self)} rows, \"\n                    f\"received array of length {len(arrays[-1])}\"\n                )\n\n        index = ensure_index_from_sequences(arrays, names)\n\n        if verify_integrity and not index.is_unique:\n            duplicates = index[index.duplicated()].unique()\n            raise ValueError(f\"Index has duplicate keys: {duplicates}\")\n\n        # use set to handle duplicate column names gracefully in case of drop\n        for c in set(to_remove):\n            del frame[c]\n\n        # clear up memory usage\n        index._cleanup()\n\n        frame.index = index\n\n        if not inplace:\n            return frame\n\n    @overload\n    def reset_index(\n        self,\n        level: Hashable | Sequence[Hashable] | None = ...,\n        drop: bool = ...,\n        inplace: Literal[False] = ...,\n        col_level: Hashable = ...,\n        col_fill: Hashable = ...,\n    ) -> DataFrame:\n        ...\n\n    @overload\n    def reset_index(\n        self,\n        level: Hashable | Sequence[Hashable] | None,\n        drop: bool,\n        inplace: Literal[True],\n        col_level: Hashable = ...,\n        col_fill: Hashable = ...,\n    ) -> None:\n        ...\n\n    @overload\n    def reset_index(\n        self,\n        *,\n        drop: bool,\n        inplace: Literal[True],\n        col_level: Hashable = ...,\n        col_fill: Hashable = ...,\n    ) -> None:\n        ...\n\n    @overload\n    def reset_index(\n        self,\n        level: Hashable | Sequence[Hashable] | None,\n        *,\n        inplace: Literal[True],\n        col_level: Hashable = ...,\n        col_fill: Hashable = ...,\n    ) -> None:\n        ...\n\n    @overload\n    def reset_index(\n        self,\n        *,\n        inplace: Literal[True],\n        col_level: Hashable = ...,\n        col_fill: Hashable = ...,\n    ) -> None:\n        ...\n\n    @overload\n    def reset_index(\n        self,\n        level: Hashable | Sequence[Hashable] | None = ...,\n        drop: bool = ...,\n        inplace: bool = ...,\n        col_level: Hashable = ...,\n        col_fill: Hashable = ...,\n    ) -> DataFrame | None:\n        ...\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"level\"])\n    def reset_index(\n        self,\n        level: Hashable | Sequence[Hashable] | None = None,\n        drop: bool = False,\n        inplace: bool = False,\n        col_level: Hashable = 0,\n        col_fill: Hashable = \"\",\n    ) -> DataFrame | None:\n        \"\"\"\n        Reset the index, or a level of it.\n\n        Reset the index of the DataFrame, and use the default one instead.\n        If the DataFrame has a MultiIndex, this method can remove one or more\n        levels.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default None\n            Only remove the given levels from the index. Removes all levels by\n            default.\n        drop : bool, default False\n            Do not try to insert index into dataframe columns. This resets\n            the index to the default integer index.\n        inplace : bool, default False\n            Modify the DataFrame in place (do not create a new object).\n        col_level : int or str, default 0\n            If the columns have multiple levels, determines which level the\n            labels are inserted into. By default it is inserted into the first\n            level.\n        col_fill : object, default ''\n            If the columns have multiple levels, determines how the other\n            levels are named. If None then the index name is repeated.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with the new index or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.set_index : Opposite of reset_index.\n        DataFrame.reindex : Change to new indices or expand indices.\n        DataFrame.reindex_like : Change to same indices as other DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('bird', 389.0),\n        ...                    ('bird', 24.0),\n        ...                    ('mammal', 80.5),\n        ...                    ('mammal', np.nan)],\n        ...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n        ...                   columns=('class', 'max_speed'))\n        >>> df\n                 class  max_speed\n        falcon    bird      389.0\n        parrot    bird       24.0\n        lion    mammal       80.5\n        monkey  mammal        NaN\n\n        When we reset the index, the old index is added as a column, and a\n        new sequential index is used:\n\n        >>> df.reset_index()\n            index   class  max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        We can use the `drop` parameter to avoid the old index being added as\n        a column:\n\n        >>> df.reset_index(drop=True)\n            class  max_speed\n        0    bird      389.0\n        1    bird       24.0\n        2  mammal       80.5\n        3  mammal        NaN\n\n        You can also use `reset_index` with `MultiIndex`.\n\n        >>> index = pd.MultiIndex.from_tuples([('bird', 'falcon'),\n        ...                                    ('bird', 'parrot'),\n        ...                                    ('mammal', 'lion'),\n        ...                                    ('mammal', 'monkey')],\n        ...                                   names=['class', 'name'])\n        >>> columns = pd.MultiIndex.from_tuples([('speed', 'max'),\n        ...                                      ('species', 'type')])\n        >>> df = pd.DataFrame([(389.0, 'fly'),\n        ...                    ( 24.0, 'fly'),\n        ...                    ( 80.5, 'run'),\n        ...                    (np.nan, 'jump')],\n        ...                   index=index,\n        ...                   columns=columns)\n        >>> df\n                       speed species\n                         max    type\n        class  name\n        bird   falcon  389.0     fly\n               parrot   24.0     fly\n        mammal lion     80.5     run\n               monkey    NaN    jump\n\n        If the index has multiple levels, we can reset a subset of them:\n\n        >>> df.reset_index(level='class')\n                 class  speed species\n                          max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        If we are not dropping the index, by default, it is placed in the top\n        level. We can place it in another level:\n\n        >>> df.reset_index(level='class', col_level=1)\n                        speed species\n                 class    max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        When the index is inserted under another level, we can specify under\n        which one with the parameter `col_fill`:\n\n        >>> df.reset_index(level='class', col_level=1, col_fill='species')\n                      species  speed species\n                        class    max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n\n        If we specify a nonexistent level for `col_fill`, it is created:\n\n        >>> df.reset_index(level='class', col_level=1, col_fill='genus')\n                        genus  speed species\n                        class    max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        self._check_inplace_and_allows_duplicate_labels(inplace)\n        if inplace:\n            new_obj = self\n        else:\n            new_obj = self.copy()\n\n        new_index = default_index(len(new_obj))\n        if level is not None:\n            if not isinstance(level, (tuple, list)):\n                level = [level]\n            level = [self.index._get_level_number(lev) for lev in level]\n            if len(level) < self.index.nlevels:\n                new_index = self.index.droplevel(level)\n\n        if not drop:\n            to_insert: Iterable[tuple[Any, Any | None]]\n            if isinstance(self.index, MultiIndex):\n                names = [\n                    (n if n is not None else f\"level_{i}\")\n                    for i, n in enumerate(self.index.names)\n                ]\n                to_insert = zip(self.index.levels, self.index.codes)\n            else:\n                default = \"index\" if \"index\" not in self else \"level_0\"\n                names = [default] if self.index.name is None else [self.index.name]\n                to_insert = ((self.index, None),)\n\n            multi_col = isinstance(self.columns, MultiIndex)\n            for i, (lev, lab) in reversed(list(enumerate(to_insert))):\n                if level is not None and i not in level:\n                    continue\n                name = names[i]\n                if multi_col:\n                    col_name = list(name) if isinstance(name, tuple) else [name]\n                    if col_fill is None:\n                        if len(col_name) not in (1, self.columns.nlevels):\n                            raise ValueError(\n                                \"col_fill=None is incompatible \"\n                                f\"with incomplete column name {name}\"\n                            )\n                        col_fill = col_name[0]\n\n                    lev_num = self.columns._get_level_number(col_level)\n                    name_lst = [col_fill] * lev_num + col_name\n                    missing = self.columns.nlevels - len(name_lst)\n                    name_lst += [col_fill] * missing\n                    name = tuple(name_lst)\n\n                # to ndarray and maybe infer different dtype\n                level_values = lev._values\n                if level_values.dtype == np.object_:\n                    level_values = lib.maybe_convert_objects(level_values)\n\n                if lab is not None:\n                    # if we have the codes, extract the values with a mask\n                    level_values = algorithms.take(\n                        level_values, lab, allow_fill=True, fill_value=lev._na_value\n                    )\n\n                new_obj.insert(0, name, level_values)\n\n        new_obj.index = new_index\n        if not inplace:\n            return new_obj\n\n        return None\n\n    # ----------------------------------------------------------------------\n    # Reindex-based selection methods\n\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs[\"klass\"])\n    def isna(self) -> DataFrame:\n        result = self._constructor(self._mgr.isna(func=isna))\n        return result.__finalize__(self, method=\"isna\")\n\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs[\"klass\"])\n    def isnull(self) -> DataFrame:\n        return self.isna()\n\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs[\"klass\"])\n    def notna(self) -> DataFrame:\n        return ~self.isna()\n\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs[\"klass\"])\n    def notnull(self) -> DataFrame:\n        return ~self.isna()\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def dropna(\n        self,\n        axis: Axis = 0,\n        how: str = \"any\",\n        thresh=None,\n        subset: IndexLabel = None,\n        inplace: bool = False,\n    ):\n        \"\"\"\n        Remove missing values.\n\n        See the :ref:`User Guide <missing_data>` for more on which values are\n        considered missing, and how to work with missing data.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Determine if rows or columns which contain missing values are\n            removed.\n\n            * 0, or 'index' : Drop rows which contain missing values.\n            * 1, or 'columns' : Drop columns which contain missing value.\n\n            .. versionchanged:: 1.0.0\n\n               Pass tuple or list to drop on multiple axes.\n               Only a single axis is allowed.\n\n        how : {'any', 'all'}, default 'any'\n            Determine if row or column is removed from DataFrame, when we have\n            at least one NA or all NA.\n\n            * 'any' : If any NA values are present, drop that row or column.\n            * 'all' : If all values are NA, drop that row or column.\n\n        thresh : int, optional\n            Require that many non-NA values.\n        subset : column label or sequence of labels, optional\n            Labels along other axis to consider, e.g. if you are dropping rows\n            these would be a list of columns to include.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with NA entries dropped from it or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.isna: Indicate missing values.\n        DataFrame.notna : Indicate existing (non-missing) values.\n        DataFrame.fillna : Replace missing values.\n        Series.dropna : Drop missing values.\n        Index.dropna : Drop missing indices.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'],\n        ...                    \"toy\": [np.nan, 'Batmobile', 'Bullwhip'],\n        ...                    \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"),\n        ...                             pd.NaT]})\n        >>> df\n               name        toy       born\n        0    Alfred        NaN        NaT\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Drop the rows where at least one element is missing.\n\n        >>> df.dropna()\n             name        toy       born\n        1  Batman  Batmobile 1940-04-25\n\n        Drop the columns where at least one element is missing.\n\n        >>> df.dropna(axis='columns')\n               name\n        0    Alfred\n        1    Batman\n        2  Catwoman\n\n        Drop the rows where all elements are missing.\n\n        >>> df.dropna(how='all')\n               name        toy       born\n        0    Alfred        NaN        NaT\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Keep only the rows with at least 2 non-NA values.\n\n        >>> df.dropna(thresh=2)\n               name        toy       born\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Define in which columns to look for missing values.\n\n        >>> df.dropna(subset=['name', 'toy'])\n               name        toy       born\n        1    Batman  Batmobile 1940-04-25\n        2  Catwoman   Bullwhip        NaT\n\n        Keep the DataFrame with valid entries in the same variable.\n\n        >>> df.dropna(inplace=True)\n        >>> df\n             name        toy       born\n        1  Batman  Batmobile 1940-04-25\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if isinstance(axis, (tuple, list)):\n            # GH20987\n            raise TypeError(\"supplying multiple axes to axis is no longer supported.\")\n\n        axis = self._get_axis_number(axis)\n        agg_axis = 1 - axis\n\n        agg_obj = self\n        if subset is not None:\n            # subset needs to be list\n            if not is_list_like(subset):\n                subset = [subset]\n            ax = self._get_axis(agg_axis)\n            indices = ax.get_indexer_for(subset)\n            check = indices == -1\n            if check.any():\n                raise KeyError(np.array(subset)[check].tolist())\n            agg_obj = self.take(indices, axis=agg_axis)\n\n        count = agg_obj.count(axis=agg_axis)\n\n        if thresh is not None:\n            mask = count >= thresh\n        elif how == \"any\":\n            mask = count == len(agg_obj._get_axis(agg_axis))\n        elif how == \"all\":\n            mask = count > 0\n        else:\n            if how is not None:\n                raise ValueError(f\"invalid how option: {how}\")\n            else:\n                raise TypeError(\"must specify how or thresh\")\n\n        result = self.loc(axis=axis)[mask]\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"subset\"])\n    def drop_duplicates(\n        self,\n        subset: Hashable | Sequence[Hashable] | None = None,\n        keep: Literal[\"first\"] | Literal[\"last\"] | Literal[False] = \"first\",\n        inplace: bool = False,\n        ignore_index: bool = False,\n    ) -> DataFrame | None:\n        \"\"\"\n        Return DataFrame with duplicate rows removed.\n\n        Considering certain columns is optional. Indexes, including time indexes\n        are ignored.\n\n        Parameters\n        ----------\n        subset : column label or sequence of labels, optional\n            Only consider certain columns for identifying duplicates, by\n            default use all of the columns.\n        keep : {'first', 'last', False}, default 'first'\n            Determines which duplicates (if any) to keep.\n            - ``first`` : Drop duplicates except for the first occurrence.\n            - ``last`` : Drop duplicates except for the last occurrence.\n            - False : Drop all duplicates.\n        inplace : bool, default False\n            Whether to drop duplicates in place or to return a copy.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, …, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        Returns\n        -------\n        DataFrame or None\n            DataFrame with duplicates removed or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.value_counts: Count unique combinations of columns.\n\n        Examples\n        --------\n        Consider dataset containing ramen rating.\n\n        >>> df = pd.DataFrame({\n        ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n        ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n        ...     'rating': [4, 4, 3.5, 15, 5]\n        ... })\n        >>> df\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        1  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        3  Indomie  pack    15.0\n        4  Indomie  pack     5.0\n\n        By default, it removes duplicate rows based on all columns.\n\n        >>> df.drop_duplicates()\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        3  Indomie  pack    15.0\n        4  Indomie  pack     5.0\n\n        To remove duplicates on specific column(s), use ``subset``.\n\n        >>> df.drop_duplicates(subset=['brand'])\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n\n        To remove duplicates and keep last occurrences, use ``keep``.\n\n        >>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n            brand style  rating\n        1  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        4  Indomie  pack     5.0\n        \"\"\"\n        if self.empty:\n            return self.copy()\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        ignore_index = validate_bool_kwarg(ignore_index, \"ignore_index\")\n        duplicated = self.duplicated(subset, keep=keep)\n\n        result = self[-duplicated]\n        if ignore_index:\n            result.index = default_index(len(result))\n\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    def duplicated(\n        self,\n        subset: Hashable | Sequence[Hashable] | None = None,\n        keep: Literal[\"first\"] | Literal[\"last\"] | Literal[False] = \"first\",\n    ) -> Series:\n        \"\"\"\n        Return boolean Series denoting duplicate rows.\n\n        Considering certain columns is optional.\n\n        Parameters\n        ----------\n        subset : column label or sequence of labels, optional\n            Only consider certain columns for identifying duplicates, by\n            default use all of the columns.\n        keep : {'first', 'last', False}, default 'first'\n            Determines which duplicates (if any) to mark.\n\n            - ``first`` : Mark duplicates as ``True`` except for the first occurrence.\n            - ``last`` : Mark duplicates as ``True`` except for the last occurrence.\n            - False : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series\n            Boolean series for each duplicated rows.\n\n        See Also\n        --------\n        Index.duplicated : Equivalent method on index.\n        Series.duplicated : Equivalent method on Series.\n        Series.drop_duplicates : Remove duplicate values from Series.\n        DataFrame.drop_duplicates : Remove duplicate values from DataFrame.\n\n        Examples\n        --------\n        Consider dataset containing ramen rating.\n\n        >>> df = pd.DataFrame({\n        ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n        ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n        ...     'rating': [4, 4, 3.5, 15, 5]\n        ... })\n        >>> df\n            brand style  rating\n        0  Yum Yum   cup     4.0\n        1  Yum Yum   cup     4.0\n        2  Indomie   cup     3.5\n        3  Indomie  pack    15.0\n        4  Indomie  pack     5.0\n\n        By default, for each set of duplicated values, the first occurrence\n        is set on False and all others on True.\n\n        >>> df.duplicated()\n        0    False\n        1     True\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True.\n\n        >>> df.duplicated(keep='last')\n        0     True\n        1    False\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        By setting ``keep`` on False, all duplicates are True.\n\n        >>> df.duplicated(keep=False)\n        0     True\n        1     True\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        To find duplicates on specific column(s), use ``subset``.\n\n        >>> df.duplicated(subset=['brand'])\n        0    False\n        1     True\n        2    False\n        3     True\n        4     True\n        dtype: bool\n        \"\"\"\n\n        if self.empty:\n            return self._constructor_sliced(dtype=bool)\n\n        def f(vals) -> tuple[np.ndarray, int]:\n            labels, shape = algorithms.factorize(vals, size_hint=len(self))\n            return labels.astype(\"i8\", copy=False), len(shape)\n\n        if subset is None:\n            # https://github.com/pandas-dev/pandas/issues/28770\n            # Incompatible types in assignment (expression has type \"Index\", variable\n            # has type \"Sequence[Any]\")\n            subset = self.columns  # type: ignore[assignment]\n        elif (\n            not np.iterable(subset)\n            or isinstance(subset, str)\n            or isinstance(subset, tuple)\n            and subset in self.columns\n        ):\n            subset = (subset,)\n\n        #  needed for mypy since can't narrow types using np.iterable\n        subset = cast(Sequence, subset)\n\n        # Verify all columns in subset exist in the queried dataframe\n        # Otherwise, raise a KeyError, same as if you try to __getitem__ with a\n        # key that doesn't exist.\n        diff = Index(subset).difference(self.columns)\n        if not diff.empty:\n            raise KeyError(diff)\n\n        vals = (col.values for name, col in self.items() if name in subset)\n        labels, shape = map(list, zip(*map(f, vals)))\n\n        ids = get_group_index(\n            labels,\n            # error: Argument 1 to \"tuple\" has incompatible type \"List[_T]\";\n            # expected \"Iterable[int]\"\n            tuple(shape),  # type: ignore[arg-type]\n            sort=False,\n            xnull=False,\n        )\n        result = self._constructor_sliced(duplicated(ids, keep), index=self.index)\n        return result.__finalize__(self, method=\"duplicated\")\n\n    # ----------------------------------------------------------------------\n    # Sorting\n    # TODO: Just move the sort_values doc here.\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"by\"])\n    @Substitution(**_shared_doc_kwargs)\n    @Appender(NDFrame.sort_values.__doc__)\n    # error: Signature of \"sort_values\" incompatible with supertype \"NDFrame\"\n    def sort_values(  # type: ignore[override]\n        self,\n        by,\n        axis: Axis = 0,\n        ascending=True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        ignore_index: bool = False,\n        key: ValueKeyFunc = None,\n    ):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        axis = self._get_axis_number(axis)\n        ascending = validate_ascending(ascending)\n        if not isinstance(by, list):\n            by = [by]\n        if is_sequence(ascending) and len(by) != len(ascending):\n            raise ValueError(\n                f\"Length of ascending ({len(ascending)}) != length of by ({len(by)})\"\n            )\n        if len(by) > 1:\n\n            keys = [self._get_label_or_level_values(x, axis=axis) for x in by]\n\n            # need to rewrap columns in Series to apply key function\n            if key is not None:\n                # error: List comprehension has incompatible type List[Series];\n                # expected List[ndarray]\n                keys = [\n                    Series(k, name=name)  # type: ignore[misc]\n                    for (k, name) in zip(keys, by)\n                ]\n\n            indexer = lexsort_indexer(\n                keys, orders=ascending, na_position=na_position, key=key\n            )\n        elif len(by):\n            # len(by) == 1\n\n            by = by[0]\n            k = self._get_label_or_level_values(by, axis=axis)\n\n            # need to rewrap column in Series to apply key function\n            if key is not None:\n                # error: Incompatible types in assignment (expression has type\n                # \"Series\", variable has type \"ndarray\")\n                k = Series(k, name=by)  # type: ignore[assignment]\n\n            if isinstance(ascending, (tuple, list)):\n                ascending = ascending[0]\n\n            indexer = nargsort(\n                k, kind=kind, ascending=ascending, na_position=na_position, key=key\n            )\n        else:\n            return self.copy()\n\n        new_data = self._mgr.take(\n            indexer, axis=self._get_block_manager_axis(axis), verify=False\n        )\n\n        if ignore_index:\n            new_data.set_axis(\n                self._get_block_manager_axis(axis), default_index(len(indexer))\n            )\n\n        result = self._constructor(new_data)\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_values\")\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def sort_index(\n        self,\n        axis: Axis = 0,\n        level: Level | None = None,\n        ascending: bool | int | Sequence[bool | int] = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        sort_remaining: bool = True,\n        ignore_index: bool = False,\n        key: IndexKeyFunc = None,\n    ):\n        \"\"\"\n        Sort object by labels (along an axis).\n\n        Returns a new DataFrame sorted by label if `inplace` argument is\n        ``False``, otherwise updates the original DataFrame and returns None.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis along which to sort.  The value 0 identifies the rows,\n            and 1 identifies the columns.\n        level : int or level name or list of ints or list of level names\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list-like of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level individually.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. `mergesort` and `stable` are the only stable algorithms. For\n            DataFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'last'}, default 'last'\n            Puts NaNs at the beginning if `first`; `last` puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, …, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape. For MultiIndex\n            inputs, the key is applied *per level*.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        DataFrame or None\n            The original DataFrame sorted by the labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.sort_index : Sort Series by the index.\n        DataFrame.sort_values : Sort DataFrame by the value.\n        Series.sort_values : Sort Series by the value.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150],\n        ...                   columns=['A'])\n        >>> df.sort_index()\n             A\n        1    4\n        29   2\n        100  1\n        150  5\n        234  3\n\n        By default, it sorts in ascending order, to sort in descending order,\n        use ``ascending=False``\n\n        >>> df.sort_index(ascending=False)\n             A\n        234  3\n        150  5\n        100  1\n        29   2\n        1    4\n\n        A key function can be specified which is applied to the index before\n        sorting. For a ``MultiIndex`` this is applied to each level separately.\n\n        >>> df = pd.DataFrame({\"a\": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd'])\n        >>> df.sort_index(key=lambda x: x.str.lower())\n           a\n        A  1\n        b  2\n        C  3\n        d  4\n        \"\"\"\n        return super().sort_index(\n            axis,\n            level,\n            ascending,\n            inplace,\n            kind,\n            na_position,\n            sort_remaining,\n            ignore_index,\n            key,\n        )\n\n    def value_counts(\n        self,\n        subset: Sequence[Hashable] | None = None,\n        normalize: bool = False,\n        sort: bool = True,\n        ascending: bool = False,\n        dropna: bool = True,\n    ):\n        \"\"\"\n        Return a Series containing counts of unique rows in the DataFrame.\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        subset : list-like, optional\n            Columns to use when counting unique combinations.\n        normalize : bool, default False\n            Return proportions rather than frequencies.\n        sort : bool, default True\n            Sort by frequencies.\n        ascending : bool, default False\n            Sort in ascending order.\n        dropna : bool, default True\n            Don’t include counts of rows that contain NA values.\n\n            .. versionadded:: 1.3.0\n\n        Returns\n        -------\n        Series\n\n        See Also\n        --------\n        Series.value_counts: Equivalent method on Series.\n\n        Notes\n        -----\n        The returned Series will have a MultiIndex with one level per input\n        column. By default, rows that contain any NA values are omitted from\n        the result. By default, the resulting Series will be in descending\n        order so that the first element is the most frequently-occurring row.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [2, 4, 4, 6],\n        ...                    'num_wings': [2, 0, 0, 0]},\n        ...                   index=['falcon', 'dog', 'cat', 'ant'])\n        >>> df\n                num_legs  num_wings\n        falcon         2          2\n        dog            4          0\n        cat            4          0\n        ant            6          0\n\n        >>> df.value_counts()\n        num_legs  num_wings\n        4         0            2\n        2         2            1\n        6         0            1\n        dtype: int64\n\n        >>> df.value_counts(sort=False)\n        num_legs  num_wings\n        2         2            1\n        4         0            2\n        6         0            1\n        dtype: int64\n\n        >>> df.value_counts(ascending=True)\n        num_legs  num_wings\n        2         2            1\n        6         0            1\n        4         0            2\n        dtype: int64\n\n        >>> df.value_counts(normalize=True)\n        num_legs  num_wings\n        4         0            0.50\n        2         2            0.25\n        6         0            0.25\n        dtype: float64\n\n        With `dropna` set to `False` we can also count rows with NA values.\n\n        >>> df = pd.DataFrame({'first_name': ['John', 'Anne', 'John', 'Beth'],\n        ...                    'middle_name': ['Smith', pd.NA, pd.NA, 'Louise']})\n        >>> df\n          first_name middle_name\n        0       John       Smith\n        1       Anne        <NA>\n        2       John        <NA>\n        3       Beth      Louise\n\n        >>> df.value_counts()\n        first_name  middle_name\n        Beth        Louise         1\n        John        Smith          1\n        dtype: int64\n\n        >>> df.value_counts(dropna=False)\n        first_name  middle_name\n        Anne        NaN            1\n        Beth        Louise         1\n        John        Smith          1\n                    NaN            1\n        dtype: int64\n        \"\"\"\n        if subset is None:\n            subset = self.columns.tolist()\n\n        counts = self.groupby(subset, dropna=dropna).grouper.size()\n\n        if sort:\n            counts = counts.sort_values(ascending=ascending)\n        if normalize:\n            counts /= counts.sum()\n\n        # Force MultiIndex for single column\n        if len(subset) == 1:\n            counts.index = MultiIndex.from_arrays(\n                [counts.index], names=[counts.index.name]\n            )\n\n        return counts\n\n    def nlargest(self, n: int, columns: IndexLabel, keep: str = \"first\") -> DataFrame:\n        \"\"\"\n        Return the first `n` rows ordered by `columns` in descending order.\n\n        Return the first `n` rows with the largest values in `columns`, in\n        descending order. The columns that are not specified are returned as\n        well, but not used for ordering.\n\n        This method is equivalent to\n        ``df.sort_values(columns, ascending=False).head(n)``, but more\n        performant.\n\n        Parameters\n        ----------\n        n : int\n            Number of rows to return.\n        columns : label or list of labels\n            Column label(s) to order by.\n        keep : {'first', 'last', 'all'}, default 'first'\n            Where there are duplicate values:\n\n            - ``first`` : prioritize the first occurrence(s)\n            - ``last`` : prioritize the last occurrence(s)\n            - ``all`` : do not drop any duplicates, even it means\n              selecting more than `n` items.\n\n        Returns\n        -------\n        DataFrame\n            The first `n` rows ordered by the given columns in descending\n            order.\n\n        See Also\n        --------\n        DataFrame.nsmallest : Return the first `n` rows ordered by `columns` in\n            ascending order.\n        DataFrame.sort_values : Sort DataFrame by the values.\n        DataFrame.head : Return the first `n` rows without re-ordering.\n\n        Notes\n        -----\n        This function cannot be used with all column types. For example, when\n        specifying columns with `object` or `category` dtypes, ``TypeError`` is\n        raised.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n        ...                                   434000, 434000, 337000, 11300,\n        ...                                   11300, 11300],\n        ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n        ...                            17036, 182, 38, 311],\n        ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n        ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n        ...                   index=[\"Italy\", \"France\", \"Malta\",\n        ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n        ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n        >>> df\n                  population      GDP alpha-2\n        Italy       59000000  1937894      IT\n        France      65000000  2583560      FR\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n        Iceland       337000    17036      IS\n        Nauru          11300      182      NR\n        Tuvalu         11300       38      TV\n        Anguilla       11300      311      AI\n\n        In the following example, we will use ``nlargest`` to select the three\n        rows having the largest values in column \"population\".\n\n        >>> df.nlargest(3, 'population')\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Malta       434000    12011      MT\n\n        When using ``keep='last'``, ties are resolved in reverse order:\n\n        >>> df.nlargest(3, 'population', keep='last')\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Brunei      434000    12128      BN\n\n        When using ``keep='all'``, all duplicate items are maintained:\n\n        >>> df.nlargest(3, 'population', keep='all')\n                  population      GDP alpha-2\n        France      65000000  2583560      FR\n        Italy       59000000  1937894      IT\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n\n        To order by the largest values in column \"population\" and then \"GDP\",\n        we can specify multiple columns like in the next example.\n\n        >>> df.nlargest(3, ['population', 'GDP'])\n                population      GDP alpha-2\n        France    65000000  2583560      FR\n        Italy     59000000  1937894      IT\n        Brunei      434000    12128      BN\n        \"\"\"\n        return algorithms.SelectNFrame(self, n=n, keep=keep, columns=columns).nlargest()\n\n    def nsmallest(self, n: int, columns: IndexLabel, keep: str = \"first\") -> DataFrame:\n        \"\"\"\n        Return the first `n` rows ordered by `columns` in ascending order.\n\n        Return the first `n` rows with the smallest values in `columns`, in\n        ascending order. The columns that are not specified are returned as\n        well, but not used for ordering.\n\n        This method is equivalent to\n        ``df.sort_values(columns, ascending=True).head(n)``, but more\n        performant.\n\n        Parameters\n        ----------\n        n : int\n            Number of items to retrieve.\n        columns : list or str\n            Column name or names to order by.\n        keep : {'first', 'last', 'all'}, default 'first'\n            Where there are duplicate values:\n\n            - ``first`` : take the first occurrence.\n            - ``last`` : take the last occurrence.\n            - ``all`` : do not drop any duplicates, even it means\n              selecting more than `n` items.\n\n        Returns\n        -------\n        DataFrame\n\n        See Also\n        --------\n        DataFrame.nlargest : Return the first `n` rows ordered by `columns` in\n            descending order.\n        DataFrame.sort_values : Sort DataFrame by the values.\n        DataFrame.head : Return the first `n` rows without re-ordering.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n        ...                                   434000, 434000, 337000, 337000,\n        ...                                   11300, 11300],\n        ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n        ...                            17036, 182, 38, 311],\n        ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n        ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n        ...                   index=[\"Italy\", \"France\", \"Malta\",\n        ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n        ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n        >>> df\n                  population      GDP alpha-2\n        Italy       59000000  1937894      IT\n        France      65000000  2583560      FR\n        Malta         434000    12011      MT\n        Maldives      434000     4520      MV\n        Brunei        434000    12128      BN\n        Iceland       337000    17036      IS\n        Nauru         337000      182      NR\n        Tuvalu         11300       38      TV\n        Anguilla       11300      311      AI\n\n        In the following example, we will use ``nsmallest`` to select the\n        three rows having the smallest values in column \"population\".\n\n        >>> df.nsmallest(3, 'population')\n                  population    GDP alpha-2\n        Tuvalu         11300     38      TV\n        Anguilla       11300    311      AI\n        Iceland       337000  17036      IS\n\n        When using ``keep='last'``, ties are resolved in reverse order:\n\n        >>> df.nsmallest(3, 'population', keep='last')\n                  population  GDP alpha-2\n        Anguilla       11300  311      AI\n        Tuvalu         11300   38      TV\n        Nauru         337000  182      NR\n\n        When using ``keep='all'``, all duplicate items are maintained:\n\n        >>> df.nsmallest(3, 'population', keep='all')\n                  population    GDP alpha-2\n        Tuvalu         11300     38      TV\n        Anguilla       11300    311      AI\n        Iceland       337000  17036      IS\n        Nauru         337000    182      NR\n\n        To order by the smallest values in column \"population\" and then \"GDP\", we can\n        specify multiple columns like in the next example.\n\n        >>> df.nsmallest(3, ['population', 'GDP'])\n                  population  GDP alpha-2\n        Tuvalu         11300   38      TV\n        Anguilla       11300  311      AI\n        Nauru         337000  182      NR\n        \"\"\"\n        return algorithms.SelectNFrame(\n            self, n=n, keep=keep, columns=columns\n        ).nsmallest()\n\n    @doc(\n        Series.swaplevel,\n        klass=_shared_doc_kwargs[\"klass\"],\n        extra_params=dedent(\n            \"\"\"axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to swap levels on. 0 or 'index' for row-wise, 1 or\n            'columns' for column-wise.\"\"\"\n        ),\n        examples=dedent(\n            \"\"\"Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"Grade\": [\"A\", \"B\", \"A\", \"C\"]},\n        ...     index=[\n        ...         [\"Final exam\", \"Final exam\", \"Coursework\", \"Coursework\"],\n        ...         [\"History\", \"Geography\", \"History\", \"Geography\"],\n        ...         [\"January\", \"February\", \"March\", \"April\"],\n        ...     ],\n        ... )\n        >>> df\n                                            Grade\n        Final exam  History     January      A\n                    Geography   February     B\n        Coursework  History     March        A\n                    Geography   April        C\n\n        In the following example, we will swap the levels of the indices.\n        Here, we will swap the levels column-wise, but levels can be swapped row-wise\n        in a similar manner. Note that column-wise is the default behaviour.\n        By not supplying any arguments for i and j, we swap the last and second to\n        last indices.\n\n        >>> df.swaplevel()\n                                            Grade\n        Final exam  January     History         A\n                    February    Geography       B\n        Coursework  March       History         A\n                    April       Geography       C\n\n        By supplying one argument, we can choose which index to swap the last\n        index with. We can for example swap the first index with the last one as\n        follows.\n\n        >>> df.swaplevel(0)\n                                            Grade\n        January     History     Final exam      A\n        February    Geography   Final exam      B\n        March       History     Coursework      A\n        April       Geography   Coursework      C\n\n        We can also define explicitly which indices we want to swap by supplying values\n        for both i and j. Here, we for example swap the first and second indices.\n\n        >>> df.swaplevel(0, 1)\n                                            Grade\n        History     Final exam  January         A\n        Geography   Final exam  February        B\n        History     Coursework  March           A\n        Geography   Coursework  April           C\"\"\"\n        ),\n    )\n    def swaplevel(self, i: Axis = -2, j: Axis = -1, axis: Axis = 0) -> DataFrame:\n        result = self.copy()\n\n        axis = self._get_axis_number(axis)\n\n        if not isinstance(result._get_axis(axis), MultiIndex):  # pragma: no cover\n            raise TypeError(\"Can only swap levels on a hierarchical axis.\")\n\n        if axis == 0:\n            assert isinstance(result.index, MultiIndex)\n            result.index = result.index.swaplevel(i, j)\n        else:\n            assert isinstance(result.columns, MultiIndex)\n            result.columns = result.columns.swaplevel(i, j)\n        return result\n\n    def reorder_levels(self, order: Sequence[Axis], axis: Axis = 0) -> DataFrame:\n        \"\"\"\n        Rearrange index levels using input order. May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int or list of str\n            List representing new level order. Reference level by number\n            (position) or by key (label).\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Where to reorder levels.\n\n        Returns\n        -------\n        DataFrame\n\n        Examples\n        --------\n        >>> data = {\n        ...     \"class\": [\"Mammals\", \"Mammals\", \"Reptiles\"],\n        ...     \"diet\": [\"Omnivore\", \"Carnivore\", \"Carnivore\"],\n        ...     \"species\": [\"Humans\", \"Dogs\", \"Snakes\"],\n        ... }\n        >>> df = pd.DataFrame(data, columns=[\"class\", \"diet\", \"species\"])\n        >>> df = df.set_index([\"class\", \"diet\"])\n        >>> df\n                                          species\n        class      diet\n        Mammals    Omnivore                Humans\n                   Carnivore                 Dogs\n        Reptiles   Carnivore               Snakes\n\n        Let's reorder the levels of the index:\n\n        >>> df.reorder_levels([\"diet\", \"class\"])\n                                          species\n        diet      class\n        Omnivore  Mammals                  Humans\n        Carnivore Mammals                    Dogs\n                  Reptiles                 Snakes\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        if not isinstance(self._get_axis(axis), MultiIndex):  # pragma: no cover\n            raise TypeError(\"Can only reorder levels on a hierarchical axis.\")\n\n        result = self.copy()\n\n        if axis == 0:\n            assert isinstance(result.index, MultiIndex)\n            result.index = result.index.reorder_levels(order)\n        else:\n            assert isinstance(result.columns, MultiIndex)\n            result.columns = result.columns.reorder_levels(order)\n        return result\n\n    # ----------------------------------------------------------------------\n    # Arithmetic Methods\n\n    def _cmp_method(self, other, op):\n        axis = 1  # only relevant for Series other case\n\n        self, other = ops.align_method_FRAME(self, other, axis, flex=False, level=None)\n\n        # See GH#4537 for discussion of scalar op behavior\n        new_data = self._dispatch_frame_op(other, op, axis=axis)\n        return self._construct_result(new_data)\n\n    def _arith_method(self, other, op):\n        if ops.should_reindex_frame_op(self, other, op, 1, 1, None, None):\n            return ops.frame_arith_method_with_reindex(self, other, op)\n\n        axis = 1  # only relevant for Series other case\n        other = ops.maybe_prepare_scalar_for_op(other, (self.shape[axis],))\n\n        self, other = ops.align_method_FRAME(self, other, axis, flex=True, level=None)\n\n        new_data = self._dispatch_frame_op(other, op, axis=axis)\n        return self._construct_result(new_data)\n\n    _logical_method = _arith_method\n\n    def _dispatch_frame_op(self, right, func: Callable, axis: int | None = None):\n        \"\"\"\n        Evaluate the frame operation func(left, right) by evaluating\n        column-by-column, dispatching to the Series implementation.\n\n        Parameters\n        ----------\n        right : scalar, Series, or DataFrame\n        func : arithmetic or comparison operator\n        axis : {None, 0, 1}\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        # Get the appropriate array-op to apply to each column/block's values.\n        array_op = ops.get_array_op(func)\n\n        right = lib.item_from_zerodim(right)\n        if not is_list_like(right):\n            # i.e. scalar, faster than checking np.ndim(right) == 0\n            with np.errstate(all=\"ignore\"):\n                bm = self._mgr.apply(array_op, right=right)\n            return self._constructor(bm)\n\n        elif isinstance(right, DataFrame):\n            assert self.index.equals(right.index)\n            assert self.columns.equals(right.columns)\n            # TODO: The previous assertion `assert right._indexed_same(self)`\n            #  fails in cases with empty columns reached via\n            #  _frame_arith_method_with_reindex\n\n            # TODO operate_blockwise expects a manager of the same type\n            with np.errstate(all=\"ignore\"):\n                bm = self._mgr.operate_blockwise(\n                    # error: Argument 1 to \"operate_blockwise\" of \"ArrayManager\" has\n                    # incompatible type \"Union[ArrayManager, BlockManager]\"; expected\n                    # \"ArrayManager\"\n                    # error: Argument 1 to \"operate_blockwise\" of \"BlockManager\" has\n                    # incompatible type \"Union[ArrayManager, BlockManager]\"; expected\n                    # \"BlockManager\"\n                    right._mgr,  # type: ignore[arg-type]\n                    array_op,\n                )\n            return self._constructor(bm)\n\n        elif isinstance(right, Series) and axis == 1:\n            # axis=1 means we want to operate row-by-row\n            assert right.index.equals(self.columns)\n\n            right = right._values\n            # maybe_align_as_frame ensures we do not have an ndarray here\n            assert not isinstance(right, np.ndarray)\n\n            with np.errstate(all=\"ignore\"):\n                arrays = [\n                    array_op(_left, _right)\n                    for _left, _right in zip(self._iter_column_arrays(), right)\n                ]\n\n        elif isinstance(right, Series):\n            assert right.index.equals(self.index)  # Handle other cases later\n            right = right._values\n\n            with np.errstate(all=\"ignore\"):\n                arrays = [array_op(left, right) for left in self._iter_column_arrays()]\n\n        else:\n            # Remaining cases have less-obvious dispatch rules\n            raise NotImplementedError(right)\n\n        return type(self)._from_arrays(\n            arrays, self.columns, self.index, verify_integrity=False\n        )\n\n    def _combine_frame(self, other: DataFrame, func, fill_value=None):\n        # at this point we have `self._indexed_same(other)`\n\n        if fill_value is None:\n            # since _arith_op may be called in a loop, avoid function call\n            #  overhead if possible by doing this check once\n            _arith_op = func\n\n        else:\n\n            def _arith_op(left, right):\n                # for the mixed_type case where we iterate over columns,\n                # _arith_op(left, right) is equivalent to\n                # left._binop(right, func, fill_value=fill_value)\n                left, right = ops.fill_binop(left, right, fill_value)\n                return func(left, right)\n\n        new_data = self._dispatch_frame_op(other, _arith_op)\n        return new_data\n\n    def _construct_result(self, result) -> DataFrame:\n        \"\"\"\n        Wrap the result of an arithmetic, comparison, or logical operation.\n\n        Parameters\n        ----------\n        result : DataFrame\n\n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        out = self._constructor(result, copy=False)\n        # Pin columns instead of passing to constructor for compat with\n        #  non-unique columns case\n        out.columns = self.columns\n        out.index = self.index\n        return out\n\n    def __divmod__(self, other) -> tuple[DataFrame, DataFrame]:\n        # Naive implementation, room for optimization\n        div = self // other\n        mod = self - div * other\n        return div, mod\n\n    def __rdivmod__(self, other) -> tuple[DataFrame, DataFrame]:\n        # Naive implementation, room for optimization\n        div = other // self\n        mod = other - div * self\n        return div, mod\n\n    # ----------------------------------------------------------------------\n    # Combination-Related\n\n    @doc(\n        _shared_docs[\"compare\"],\n        \"\"\"\nReturns\n-------\nDataFrame\n    DataFrame that shows the differences stacked side by side.\n\n    The resulting index will be a MultiIndex with 'self' and 'other'\n    stacked alternately at the inner level.\n\nRaises\n------\nValueError\n    When the two DataFrames don't have identical labels or shape.\n\nSee Also\n--------\nSeries.compare : Compare with another Series and show differences.\nDataFrame.equals : Test whether two objects contain the same elements.\n\nNotes\n-----\nMatching NaNs will not appear as a difference.\n\nCan only compare identically-labeled\n(i.e. same shape, identical row and column labels) DataFrames\n\nExamples\n--------\n>>> df = pd.DataFrame(\n...     {{\n...         \"col1\": [\"a\", \"a\", \"b\", \"b\", \"a\"],\n...         \"col2\": [1.0, 2.0, 3.0, np.nan, 5.0],\n...         \"col3\": [1.0, 2.0, 3.0, 4.0, 5.0]\n...     }},\n...     columns=[\"col1\", \"col2\", \"col3\"],\n... )\n>>> df\n  col1  col2  col3\n0    a   1.0   1.0\n1    a   2.0   2.0\n2    b   3.0   3.0\n3    b   NaN   4.0\n4    a   5.0   5.0\n\n>>> df2 = df.copy()\n>>> df2.loc[0, 'col1'] = 'c'\n>>> df2.loc[2, 'col3'] = 4.0\n>>> df2\n  col1  col2  col3\n0    c   1.0   1.0\n1    a   2.0   2.0\n2    b   3.0   4.0\n3    b   NaN   4.0\n4    a   5.0   5.0\n\nAlign the differences on columns\n\n>>> df.compare(df2)\n  col1       col3\n  self other self other\n0    a     c  NaN   NaN\n2  NaN   NaN  3.0   4.0\n\nStack the differences on rows\n\n>>> df.compare(df2, align_axis=0)\n        col1  col3\n0 self     a   NaN\n  other    c   NaN\n2 self   NaN   3.0\n  other  NaN   4.0\n\nKeep the equal values\n\n>>> df.compare(df2, keep_equal=True)\n  col1       col3\n  self other self other\n0    a     c  1.0   1.0\n2    b     b  3.0   4.0\n\nKeep all original rows and columns\n\n>>> df.compare(df2, keep_shape=True)\n  col1       col2       col3\n  self other self other self other\n0    a     c  NaN   NaN  NaN   NaN\n1  NaN   NaN  NaN   NaN  NaN   NaN\n2  NaN   NaN  NaN   NaN  3.0   4.0\n3  NaN   NaN  NaN   NaN  NaN   NaN\n4  NaN   NaN  NaN   NaN  NaN   NaN\n\nKeep all original rows and columns and also all original values\n\n>>> df.compare(df2, keep_shape=True, keep_equal=True)\n  col1       col2       col3\n  self other self other self other\n0    a     c  1.0   1.0  1.0   1.0\n1    a     a  2.0   2.0  2.0   2.0\n2    b     b  3.0   3.0  3.0   4.0\n3    b     b  NaN   NaN  4.0   4.0\n4    a     a  5.0   5.0  5.0   5.0\n\"\"\",\n        klass=_shared_doc_kwargs[\"klass\"],\n    )\n    def compare(\n        self,\n        other: DataFrame,\n        align_axis: Axis = 1,\n        keep_shape: bool = False,\n        keep_equal: bool = False,\n    ) -> DataFrame:\n        return super().compare(\n            other=other,\n            align_axis=align_axis,\n            keep_shape=keep_shape,\n            keep_equal=keep_equal,\n        )\n\n    def combine(\n        self, other: DataFrame, func, fill_value=None, overwrite: bool = True\n    ) -> DataFrame:\n        \"\"\"\n        Perform column-wise combine with another DataFrame.\n\n        Combines a DataFrame with `other` DataFrame using `func`\n        to element-wise combine columns. The row and column indexes of the\n        resulting DataFrame will be the union of the two.\n\n        Parameters\n        ----------\n        other : DataFrame\n            The DataFrame to merge column-wise.\n        func : function\n            Function that takes two series as inputs and return a Series or a\n            scalar. Used to merge the two dataframes column by columns.\n        fill_value : scalar value, default None\n            The value to fill NaNs with prior to passing any column to the\n            merge func.\n        overwrite : bool, default True\n            If True, columns in `self` that do not exist in `other` will be\n            overwritten with NaNs.\n\n        Returns\n        -------\n        DataFrame\n            Combination of the provided DataFrames.\n\n        See Also\n        --------\n        DataFrame.combine_first : Combine two DataFrame objects and default to\n            non-null values in frame calling the method.\n\n        Examples\n        --------\n        Combine using a simple function that chooses the smaller column.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2\n        >>> df1.combine(df2, take_smaller)\n           A  B\n        0  0  3\n        1  0  3\n\n        Example using a true element-wise combine function.\n\n        >>> df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine(df2, np.minimum)\n           A  B\n        0  1  2\n        1  0  3\n\n        Using `fill_value` fills Nones prior to passing the column to the\n        merge function.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine(df2, take_smaller, fill_value=-5)\n           A    B\n        0  0 -5.0\n        1  0  4.0\n\n        However, if the same element in both dataframes is None, that None\n        is preserved\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]})\n        >>> df1.combine(df2, take_smaller, fill_value=-5)\n            A    B\n        0  0 -5.0\n        1  0  3.0\n\n        Example that demonstrates the use of `overwrite` and behavior when\n        the axis differ between the dataframes.\n\n        >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2])\n        >>> df1.combine(df2, take_smaller)\n             A    B     C\n        0  NaN  NaN   NaN\n        1  NaN  3.0 -10.0\n        2  NaN  3.0   1.0\n\n        >>> df1.combine(df2, take_smaller, overwrite=False)\n             A    B     C\n        0  0.0  NaN   NaN\n        1  0.0  3.0 -10.0\n        2  NaN  3.0   1.0\n\n        Demonstrating the preference of the passed in dataframe.\n\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2])\n        >>> df2.combine(df1, take_smaller)\n           A    B   C\n        0  0.0  NaN NaN\n        1  0.0  3.0 NaN\n        2  NaN  3.0 NaN\n\n        >>> df2.combine(df1, take_smaller, overwrite=False)\n             A    B   C\n        0  0.0  NaN NaN\n        1  0.0  3.0 1.0\n        2  NaN  3.0 1.0\n        \"\"\"\n        other_idxlen = len(other.index)  # save for compare\n\n        this, other = self.align(other, copy=False)\n        new_index = this.index\n\n        if other.empty and len(new_index) == len(self.index):\n            return self.copy()\n\n        if self.empty and len(other) == other_idxlen:\n            return other.copy()\n\n        # sorts if possible\n        new_columns = this.columns.union(other.columns)\n        do_fill = fill_value is not None\n        result = {}\n        for col in new_columns:\n            series = this[col]\n            otherSeries = other[col]\n\n            this_dtype = series.dtype\n            other_dtype = otherSeries.dtype\n\n            this_mask = isna(series)\n            other_mask = isna(otherSeries)\n\n            # don't overwrite columns unnecessarily\n            # DO propagate if this column is not in the intersection\n            if not overwrite and other_mask.all():\n                result[col] = this[col].copy()\n                continue\n\n            if do_fill:\n                series = series.copy()\n                otherSeries = otherSeries.copy()\n                series[this_mask] = fill_value\n                otherSeries[other_mask] = fill_value\n\n            if col not in self.columns:\n                # If self DataFrame does not have col in other DataFrame,\n                # try to promote series, which is all NaN, as other_dtype.\n                new_dtype = other_dtype\n                try:\n                    series = series.astype(new_dtype, copy=False)\n                except ValueError:\n                    # e.g. new_dtype is integer types\n                    pass\n            else:\n                # if we have different dtypes, possibly promote\n                new_dtype = find_common_type([this_dtype, other_dtype])\n                series = series.astype(new_dtype, copy=False)\n                otherSeries = otherSeries.astype(new_dtype, copy=False)\n\n            arr = func(series, otherSeries)\n            if isinstance(new_dtype, np.dtype):\n                # if new_dtype is an EA Dtype, then `func` is expected to return\n                # the correct dtype without any additional casting\n                arr = maybe_downcast_to_dtype(arr, new_dtype)\n\n            result[col] = arr\n\n        # convert_objects just in case\n        return self._constructor(result, index=new_index, columns=new_columns)\n\n    def combine_first(self, other: DataFrame) -> DataFrame:\n        \"\"\"\n        Update null elements with value in the same location in `other`.\n\n        Combine two DataFrame objects by filling null values in one DataFrame\n        with non-null values from other DataFrame. The row and column indexes\n        of the resulting DataFrame will be the union of the two.\n\n        Parameters\n        ----------\n        other : DataFrame\n            Provided DataFrame to use to fill null values.\n\n        Returns\n        -------\n        DataFrame\n            The result of combining the provided DataFrame with the other object.\n\n        See Also\n        --------\n        DataFrame.combine : Perform series-wise operation on two DataFrames\n            using a given function.\n\n        Examples\n        --------\n        >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]})\n        >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n        >>> df1.combine_first(df2)\n             A    B\n        0  1.0  3.0\n        1  0.0  4.0\n\n        Null values still persist if the location of that null value\n        does not exist in `other`\n\n        >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]})\n        >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2])\n        >>> df1.combine_first(df2)\n             A    B    C\n        0  NaN  4.0  NaN\n        1  0.0  3.0  1.0\n        2  NaN  3.0  1.0\n        \"\"\"\n        import pandas.core.computation.expressions as expressions\n\n        def combiner(x, y):\n            mask = extract_array(isna(x))\n\n            x_values = extract_array(x, extract_numpy=True)\n            y_values = extract_array(y, extract_numpy=True)\n\n            # If the column y in other DataFrame is not in first DataFrame,\n            # just return y_values.\n            if y.name not in self.columns:\n                return y_values\n\n            return expressions.where(mask, y_values, x_values)\n\n        combined = self.combine(other, combiner, overwrite=False)\n\n        dtypes = {\n            col: find_common_type([self.dtypes[col], other.dtypes[col]])\n            for col in self.columns.intersection(other.columns)\n            if not is_dtype_equal(combined.dtypes[col], self.dtypes[col])\n        }\n\n        if dtypes:\n            combined = combined.astype(dtypes)\n\n        return combined\n\n    def update(\n        self,\n        other,\n        join: str = \"left\",\n        overwrite: bool = True,\n        filter_func=None,\n        errors: str = \"ignore\",\n    ) -> None:\n        \"\"\"\n        Modify in place using non-NA values from another DataFrame.\n\n        Aligns on indices. There is no return value.\n\n        Parameters\n        ----------\n        other : DataFrame, or object coercible into a DataFrame\n            Should have at least one matching index/column label\n            with the original DataFrame. If a Series is passed,\n            its name attribute must be set, and that will be\n            used as the column name to align with the original DataFrame.\n        join : {'left'}, default 'left'\n            Only left join is implemented, keeping the index and columns of the\n            original object.\n        overwrite : bool, default True\n            How to handle non-NA values for overlapping keys:\n\n            * True: overwrite original DataFrame's values\n              with values from `other`.\n            * False: only update values that are NA in\n              the original DataFrame.\n\n        filter_func : callable(1d-array) -> bool 1d-array, optional\n            Can choose to replace values other than NA. Return True for values\n            that should be updated.\n        errors : {'raise', 'ignore'}, default 'ignore'\n            If 'raise', will raise a ValueError if the DataFrame and `other`\n            both contain non-NA data in the same place.\n\n        Returns\n        -------\n        None : method directly changes calling object\n\n        Raises\n        ------\n        ValueError\n            * When `errors='raise'` and there's overlapping non-NA data.\n            * When `errors` is not either `'ignore'` or `'raise'`\n        NotImplementedError\n            * If `join != 'left'`\n\n        See Also\n        --------\n        dict.update : Similar method for dictionaries.\n        DataFrame.merge : For column(s)-on-column(s) operations.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2, 3],\n        ...                    'B': [400, 500, 600]})\n        >>> new_df = pd.DataFrame({'B': [4, 5, 6],\n        ...                        'C': [7, 8, 9]})\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  1  4\n        1  2  5\n        2  3  6\n\n        The DataFrame's length does not increase as a result of the update,\n        only values at matching index/column labels are updated.\n\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_df = pd.DataFrame({'B': ['d', 'e', 'f', 'g', 'h', 'i']})\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  a  d\n        1  b  e\n        2  c  f\n\n        For Series, its name attribute must be set.\n\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_column = pd.Series(['d', 'e'], name='B', index=[0, 2])\n        >>> df.update(new_column)\n        >>> df\n           A  B\n        0  a  d\n        1  b  y\n        2  c  e\n        >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n        ...                    'B': ['x', 'y', 'z']})\n        >>> new_df = pd.DataFrame({'B': ['d', 'e']}, index=[1, 2])\n        >>> df.update(new_df)\n        >>> df\n           A  B\n        0  a  x\n        1  b  d\n        2  c  e\n\n        If `other` contains NaNs the corresponding values are not updated\n        in the original dataframe.\n\n        >>> df = pd.DataFrame({'A': [1, 2, 3],\n        ...                    'B': [400, 500, 600]})\n        >>> new_df = pd.DataFrame({'B': [4, np.nan, 6]})\n        >>> df.update(new_df)\n        >>> df\n           A      B\n        0  1    4.0\n        1  2  500.0\n        2  3    6.0\n        \"\"\"\n        import pandas.core.computation.expressions as expressions\n\n        # TODO: Support other joins\n        if join != \"left\":  # pragma: no cover\n            raise NotImplementedError(\"Only left join is supported\")\n        if errors not in [\"ignore\", \"raise\"]:\n            raise ValueError(\"The parameter errors must be either 'ignore' or 'raise'\")\n\n        if not isinstance(other, DataFrame):\n            other = DataFrame(other)\n\n        other = other.reindex_like(self)\n\n        for col in self.columns:\n            this = self[col]._values\n            that = other[col]._values\n            if filter_func is not None:\n                with np.errstate(all=\"ignore\"):\n                    mask = ~filter_func(this) | isna(that)\n            else:\n                if errors == \"raise\":\n                    mask_this = notna(that)\n                    mask_that = notna(this)\n                    if any(mask_this & mask_that):\n                        raise ValueError(\"Data overlaps.\")\n\n                if overwrite:\n                    mask = isna(that)\n                else:\n                    mask = notna(this)\n\n            # don't overwrite columns unnecessarily\n            if mask.all():\n                continue\n\n            self[col] = expressions.where(mask, this, that)\n\n    # ----------------------------------------------------------------------\n    # Data reshaping\n    @Appender(\n        \"\"\"\nExamples\n--------\n>>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n...                               'Parrot', 'Parrot'],\n...                    'Max Speed': [380., 370., 24., 26.]})\n>>> df\n   Animal  Max Speed\n0  Falcon      380.0\n1  Falcon      370.0\n2  Parrot       24.0\n3  Parrot       26.0\n>>> df.groupby(['Animal']).mean()\n        Max Speed\nAnimal\nFalcon      375.0\nParrot       25.0\n\n**Hierarchical Indexes**\n\nWe can groupby different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> df = pd.DataFrame({'Max Speed': [390., 350., 30., 20.]},\n...                   index=index)\n>>> df\n                Max Speed\nAnimal Type\nFalcon Captive      390.0\n       Wild         350.0\nParrot Captive       30.0\n       Wild          20.0\n>>> df.groupby(level=0).mean()\n        Max Speed\nAnimal\nFalcon      370.0\nParrot       25.0\n>>> df.groupby(level=\"Type\").mean()\n         Max Speed\nType\nCaptive      210.0\nWild         185.0\n\nWe can also choose to include NA in group keys or not by setting\n`dropna` parameter, the default setting is `True`.\n\n>>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n>>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> df.groupby(by=[\"b\"]).sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\n\n>>> df.groupby(by=[\"b\"], dropna=False).sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\nNaN 1   4\n\n>>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\n>>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> df.groupby(by=\"a\").sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\n\n>>> df.groupby(by=\"a\", dropna=False).sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\nNaN 12.3   33.0\n\"\"\"\n    )\n    @Appender(_shared_docs[\"groupby\"] % _shared_doc_kwargs)\n    def groupby(\n        self,\n        by=None,\n        axis: Axis = 0,\n        level: Level | None = None,\n        as_index: bool = True,\n        sort: bool = True,\n        group_keys: bool = True,\n        squeeze: bool | lib.NoDefault = no_default,\n        observed: bool = False,\n        dropna: bool = True,\n    ) -> DataFrameGroupBy:\n        from pandas.core.groupby.generic import DataFrameGroupBy\n\n        if squeeze is not no_default:\n            warnings.warn(\n                (\n                    \"The `squeeze` parameter is deprecated and \"\n                    \"will be removed in a future version.\"\n                ),\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n        else:\n            squeeze = False\n\n        if level is None and by is None:\n            raise TypeError(\"You have to supply one of 'by' and 'level'\")\n        axis = self._get_axis_number(axis)\n\n        # https://github.com/python/mypy/issues/7642\n        # error: Argument \"squeeze\" to \"DataFrameGroupBy\" has incompatible type\n        # \"Union[bool, NoDefault]\"; expected \"bool\"\n        return DataFrameGroupBy(\n            obj=self,\n            keys=by,\n            axis=axis,\n            level=level,\n            as_index=as_index,\n            sort=sort,\n            group_keys=group_keys,\n            squeeze=squeeze,  # type: ignore[arg-type]\n            observed=observed,\n            dropna=dropna,\n        )\n\n    _shared_docs[\n        \"pivot\"\n    ] = \"\"\"\n        Return reshaped DataFrame organized by given index / column values.\n\n        Reshape data (produce a \"pivot\" table) based on column values. Uses\n        unique values from specified `index` / `columns` to form axes of the\n        resulting DataFrame. This function does not support data\n        aggregation, multiple values will result in a MultiIndex in the\n        columns. See the :ref:`User Guide <reshaping>` for more on reshaping.\n\n        Parameters\n        ----------%s\n        index : str or object or a list of str, optional\n            Column to use to make new frame's index. If None, uses\n            existing index.\n\n            .. versionchanged:: 1.1.0\n               Also accept list of index names.\n\n        columns : str or object or a list of str\n            Column to use to make new frame's columns.\n\n            .. versionchanged:: 1.1.0\n               Also accept list of columns names.\n\n        values : str, object or a list of the previous, optional\n            Column(s) to use for populating new frame's values. If not\n            specified, all remaining columns will be used and the result will\n            have hierarchically indexed columns.\n\n        Returns\n        -------\n        DataFrame\n            Returns reshaped DataFrame.\n\n        Raises\n        ------\n        ValueError:\n            When there are any `index`, `columns` combinations with multiple\n            values. `DataFrame.pivot_table` when you need to aggregate.\n\n        See Also\n        --------\n        DataFrame.pivot_table : Generalization of pivot that can handle\n            duplicate values for one index/column pair.\n        DataFrame.unstack : Pivot based on the index values instead of a\n            column.\n        wide_to_long : Wide panel to long format. Less flexible but more\n            user-friendly than melt.\n\n        Notes\n        -----\n        For finer-tuned control, see hierarchical indexing documentation along\n        with the related stack/unstack methods.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n        ...                            'two'],\n        ...                    'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n        ...                    'baz': [1, 2, 3, 4, 5, 6],\n        ...                    'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\n        >>> df\n            foo   bar  baz  zoo\n        0   one   A    1    x\n        1   one   B    2    y\n        2   one   C    3    z\n        3   two   A    4    q\n        4   two   B    5    w\n        5   two   C    6    t\n\n        >>> df.pivot(index='foo', columns='bar', values='baz')\n        bar  A   B   C\n        foo\n        one  1   2   3\n        two  4   5   6\n\n        >>> df.pivot(index='foo', columns='bar')['baz']\n        bar  A   B   C\n        foo\n        one  1   2   3\n        two  4   5   6\n\n        >>> df.pivot(index='foo', columns='bar', values=['baz', 'zoo'])\n              baz       zoo\n        bar   A  B  C   A  B  C\n        foo\n        one   1  2  3   x  y  z\n        two   4  5  6   q  w  t\n\n        You could also assign a list of column names or a list of index names.\n\n        >>> df = pd.DataFrame({\n        ...        \"lev1\": [1, 1, 1, 2, 2, 2],\n        ...        \"lev2\": [1, 1, 2, 1, 1, 2],\n        ...        \"lev3\": [1, 2, 1, 2, 1, 2],\n        ...        \"lev4\": [1, 2, 3, 4, 5, 6],\n        ...        \"values\": [0, 1, 2, 3, 4, 5]})\n        >>> df\n            lev1 lev2 lev3 lev4 values\n        0   1    1    1    1    0\n        1   1    1    2    2    1\n        2   1    2    1    3    2\n        3   2    1    2    4    3\n        4   2    1    1    5    4\n        5   2    2    2    6    5\n\n        >>> df.pivot(index=\"lev1\", columns=[\"lev2\", \"lev3\"],values=\"values\")\n        lev2    1         2\n        lev3    1    2    1    2\n        lev1\n        1     0.0  1.0  2.0  NaN\n        2     4.0  3.0  NaN  5.0\n\n        >>> df.pivot(index=[\"lev1\", \"lev2\"], columns=[\"lev3\"],values=\"values\")\n              lev3    1    2\n        lev1  lev2\n           1     1  0.0  1.0\n                 2  2.0  NaN\n           2     1  4.0  3.0\n                 2  NaN  5.0\n\n        A ValueError is raised if there are any duplicates.\n\n        >>> df = pd.DataFrame({\"foo\": ['one', 'one', 'two', 'two'],\n        ...                    \"bar\": ['A', 'A', 'B', 'C'],\n        ...                    \"baz\": [1, 2, 3, 4]})\n        >>> df\n           foo bar  baz\n        0  one   A    1\n        1  one   A    2\n        2  two   B    3\n        3  two   C    4\n\n        Notice that the first two rows are the same for our `index`\n        and `columns` arguments.\n\n        >>> df.pivot(index='foo', columns='bar', values='baz')\n        Traceback (most recent call last):\n           ...\n        ValueError: Index contains duplicate entries, cannot reshape\n        \"\"\"\n\n    @Substitution(\"\")\n    @Appender(_shared_docs[\"pivot\"])\n    def pivot(self, index=None, columns=None, values=None) -> DataFrame:\n        from pandas.core.reshape.pivot import pivot\n\n        return pivot(self, index=index, columns=columns, values=values)\n\n    _shared_docs[\n        \"pivot_table\"\n    ] = \"\"\"\n        Create a spreadsheet-style pivot table as a DataFrame.\n\n        The levels in the pivot table will be stored in MultiIndex objects\n        (hierarchical indexes) on the index and columns of the result DataFrame.\n\n        Parameters\n        ----------%s\n        values : column to aggregate, optional\n        index : column, Grouper, array, or list of the previous\n            If an array is passed, it must be the same length as the data. The\n            list can contain any of the other types (except list).\n            Keys to group by on the pivot table index.  If an array is passed,\n            it is being used as the same manner as column values.\n        columns : column, Grouper, array, or list of the previous\n            If an array is passed, it must be the same length as the data. The\n            list can contain any of the other types (except list).\n            Keys to group by on the pivot table column.  If an array is passed,\n            it is being used as the same manner as column values.\n        aggfunc : function, list of functions, dict, default numpy.mean\n            If list of functions passed, the resulting pivot table will have\n            hierarchical columns whose top level are the function names\n            (inferred from the function objects themselves)\n            If dict is passed, the key is column to aggregate and value\n            is function or list of functions.\n        fill_value : scalar, default None\n            Value to replace missing values with (in the resulting pivot table,\n            after aggregation).\n        margins : bool, default False\n            Add all row / columns (e.g. for subtotal / grand totals).\n        dropna : bool, default True\n            Do not include columns whose entries are all NaN.\n        margins_name : str, default 'All'\n            Name of the row / column that will contain the totals\n            when margins is True.\n        observed : bool, default False\n            This only applies if any of the groupers are Categoricals.\n            If True: only show observed values for categorical groupers.\n            If False: show all values for categorical groupers.\n\n            .. versionchanged:: 0.25.0\n\n        sort : bool, default True\n            Specifies if the result should be sorted.\n\n            .. versionadded:: 1.3.0\n\n        Returns\n        -------\n        DataFrame\n            An Excel style pivot table.\n\n        See Also\n        --------\n        DataFrame.pivot : Pivot without aggregation that can handle\n            non-numeric data.\n        DataFrame.melt: Unpivot a DataFrame from wide to long format,\n            optionally leaving identifiers set.\n        wide_to_long : Wide panel to long format. Less flexible but more\n            user-friendly than melt.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\",\n        ...                          \"bar\", \"bar\", \"bar\", \"bar\"],\n        ...                    \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\",\n        ...                          \"one\", \"one\", \"two\", \"two\"],\n        ...                    \"C\": [\"small\", \"large\", \"large\", \"small\",\n        ...                          \"small\", \"large\", \"small\", \"small\",\n        ...                          \"large\"],\n        ...                    \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7],\n        ...                    \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]})\n        >>> df\n             A    B      C  D  E\n        0  foo  one  small  1  2\n        1  foo  one  large  2  4\n        2  foo  one  large  2  5\n        3  foo  two  small  3  5\n        4  foo  two  small  3  6\n        5  bar  one  large  4  6\n        6  bar  one  small  5  8\n        7  bar  two  small  6  9\n        8  bar  two  large  7  9\n\n        This first example aggregates values by taking the sum.\n\n        >>> table = pd.pivot_table(df, values='D', index=['A', 'B'],\n        ...                     columns=['C'], aggfunc=np.sum)\n        >>> table\n        C        large  small\n        A   B\n        bar one    4.0    5.0\n            two    7.0    6.0\n        foo one    4.0    1.0\n            two    NaN    6.0\n\n        We can also fill missing values using the `fill_value` parameter.\n\n        >>> table = pd.pivot_table(df, values='D', index=['A', 'B'],\n        ...                     columns=['C'], aggfunc=np.sum, fill_value=0)\n        >>> table\n        C        large  small\n        A   B\n        bar one      4      5\n            two      7      6\n        foo one      4      1\n            two      0      6\n\n        The next example aggregates by taking the mean across multiple columns.\n\n        >>> table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n        ...                     aggfunc={'D': np.mean,\n        ...                              'E': np.mean})\n        >>> table\n                        D         E\n        A   C\n        bar large  5.500000  7.500000\n            small  5.500000  8.500000\n        foo large  2.000000  4.500000\n            small  2.333333  4.333333\n\n        We can also calculate multiple types of aggregations for any given\n        value column.\n\n        >>> table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n        ...                     aggfunc={'D': np.mean,\n        ...                              'E': [min, max, np.mean]})\n        >>> table\n                        D    E\n                    mean  max      mean  min\n        A   C\n        bar large  5.500000  9.0  7.500000  6.0\n            small  5.500000  9.0  8.500000  8.0\n        foo large  2.000000  5.0  4.500000  4.0\n            small  2.333333  6.0  4.333333  2.0\n        \"\"\"\n\n    @Substitution(\"\")\n    @Appender(_shared_docs[\"pivot_table\"])\n    def pivot_table(\n        self,\n        values=None,\n        index=None,\n        columns=None,\n        aggfunc=\"mean\",\n        fill_value=None,\n        margins=False,\n        dropna=True,\n        margins_name=\"All\",\n        observed=False,\n        sort=True,\n    ) -> DataFrame:\n        from pandas.core.reshape.pivot import pivot_table\n\n        return pivot_table(\n            self,\n            values=values,\n            index=index,\n            columns=columns,\n            aggfunc=aggfunc,\n            fill_value=fill_value,\n            margins=margins,\n            dropna=dropna,\n            margins_name=margins_name,\n            observed=observed,\n            sort=sort,\n        )\n\n    def stack(self, level: Level = -1, dropna: bool = True):\n        \"\"\"\n        Stack the prescribed level(s) from columns to index.\n\n        Return a reshaped DataFrame or Series having a multi-level\n        index with one or more new inner-most levels compared to the current\n        DataFrame. The new inner-most levels are created by pivoting the\n        columns of the current dataframe:\n\n          - if the columns have a single level, the output is a Series;\n          - if the columns have multiple levels, the new index\n            level(s) is (are) taken from the prescribed level(s) and\n            the output is a DataFrame.\n\n        Parameters\n        ----------\n        level : int, str, list, default -1\n            Level(s) to stack from the column axis onto the index\n            axis, defined as one index or label, or a list of indices\n            or labels.\n        dropna : bool, default True\n            Whether to drop rows in the resulting Frame/Series with\n            missing values. Stacking a column level onto the index\n            axis can create combinations of index and column values\n            that are missing from the original dataframe. See Examples\n            section.\n\n        Returns\n        -------\n        DataFrame or Series\n            Stacked dataframe or series.\n\n        See Also\n        --------\n        DataFrame.unstack : Unstack prescribed level(s) from index axis\n             onto column axis.\n        DataFrame.pivot : Reshape dataframe from long format to wide\n             format.\n        DataFrame.pivot_table : Create a spreadsheet-style pivot table\n             as a DataFrame.\n\n        Notes\n        -----\n        The function is named by analogy with a collection of books\n        being reorganized from being side by side on a horizontal\n        position (the columns of the dataframe) to being stacked\n        vertically on top of each other (in the index of the\n        dataframe).\n\n        Examples\n        --------\n        **Single level columns**\n\n        >>> df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=['weight', 'height'])\n\n        Stacking a dataframe with a single level column axis returns a Series:\n\n        >>> df_single_level_cols\n             weight height\n        cat       0      1\n        dog       2      3\n        >>> df_single_level_cols.stack()\n        cat  weight    0\n             height    1\n        dog  weight    2\n             height    3\n        dtype: int64\n\n        **Multi level columns: simple case**\n\n        >>> multicol1 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n        ...                                        ('weight', 'pounds')])\n        >>> df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=multicol1)\n\n        Stacking a dataframe with a multi-level column axis:\n\n        >>> df_multi_level_cols1\n             weight\n                 kg    pounds\n        cat       1        2\n        dog       2        4\n        >>> df_multi_level_cols1.stack()\n                    weight\n        cat kg           1\n            pounds       2\n        dog kg           2\n            pounds       4\n\n        **Missing values**\n\n        >>> multicol2 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n        ...                                        ('height', 'm')])\n        >>> df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=multicol2)\n\n        It is common to have missing values when stacking a dataframe\n        with multi-level columns, as the stacked dataframe typically\n        has more values than the original dataframe. Missing values\n        are filled with NaNs:\n\n        >>> df_multi_level_cols2\n            weight height\n                kg      m\n        cat    1.0    2.0\n        dog    3.0    4.0\n        >>> df_multi_level_cols2.stack()\n                height  weight\n        cat kg     NaN     1.0\n            m      2.0     NaN\n        dog kg     NaN     3.0\n            m      4.0     NaN\n\n        **Prescribing the level(s) to be stacked**\n\n        The first parameter controls which level or levels are stacked:\n\n        >>> df_multi_level_cols2.stack(0)\n                     kg    m\n        cat height  NaN  2.0\n            weight  1.0  NaN\n        dog height  NaN  4.0\n            weight  3.0  NaN\n        >>> df_multi_level_cols2.stack([0, 1])\n        cat  height  m     2.0\n             weight  kg    1.0\n        dog  height  m     4.0\n             weight  kg    3.0\n        dtype: float64\n\n        **Dropping missing values**\n\n        >>> df_multi_level_cols3 = pd.DataFrame([[None, 1.0], [2.0, 3.0]],\n        ...                                     index=['cat', 'dog'],\n        ...                                     columns=multicol2)\n\n        Note that rows where all values are missing are dropped by\n        default but this behaviour can be controlled via the dropna\n        keyword parameter:\n\n        >>> df_multi_level_cols3\n            weight height\n                kg      m\n        cat    NaN    1.0\n        dog    2.0    3.0\n        >>> df_multi_level_cols3.stack(dropna=False)\n                height  weight\n        cat kg     NaN     NaN\n            m      1.0     NaN\n        dog kg     NaN     2.0\n            m      3.0     NaN\n        >>> df_multi_level_cols3.stack(dropna=True)\n                height  weight\n        cat m      1.0     NaN\n        dog kg     NaN     2.0\n            m      3.0     NaN\n        \"\"\"\n        from pandas.core.reshape.reshape import (\n            stack,\n            stack_multiple,\n        )\n\n        if isinstance(level, (tuple, list)):\n            result = stack_multiple(self, level, dropna=dropna)\n        else:\n            result = stack(self, level, dropna=dropna)\n\n        return result.__finalize__(self, method=\"stack\")\n\n    def explode(\n        self,\n        column: IndexLabel,\n        ignore_index: bool = False,\n    ) -> DataFrame:\n        \"\"\"\n        Transform each element of a list-like to a row, replicating index values.\n\n        .. versionadded:: 0.25.0\n\n        Parameters\n        ----------\n        column : IndexLabel\n            Column(s) to explode.\n            For multiple columns, specify a non-empty list with each element\n            be str or tuple, and all specified columns their list-like data\n            on same row of the frame must have matching length.\n\n            .. versionadded:: 1.3.0\n                Multi-column explode\n\n        ignore_index : bool, default False\n            If True, the resulting index will be labeled 0, 1, …, n - 1.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        DataFrame\n            Exploded lists to rows of the subset columns;\n            index will be duplicated for these rows.\n\n        Raises\n        ------\n        ValueError :\n            * If columns of the frame are not unique.\n            * If specified columns to explode is empty list.\n            * If specified columns to explode have not matching count of\n              elements rowwise in the frame.\n\n        See Also\n        --------\n        DataFrame.unstack : Pivot a level of the (necessarily hierarchical)\n            index labels.\n        DataFrame.melt : Unpivot a DataFrame from wide format to long format.\n        Series.explode : Explode a DataFrame from list-like columns to long format.\n\n        Notes\n        -----\n        This routine will explode list-likes including lists, tuples, sets,\n        Series, and np.ndarray. The result dtype of the subset rows will\n        be object. Scalars will be returned unchanged, and empty list-likes will\n        result in a np.nan for that row. In addition, the ordering of rows in the\n        output will be non-deterministic when exploding sets.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [[0, 1, 2], 'foo', [], [3, 4]],\n        ...                    'B': 1,\n        ...                    'C': [['a', 'b', 'c'], np.nan, [], ['d', 'e']]})\n        >>> df\n                   A  B          C\n        0  [0, 1, 2]  1  [a, b, c]\n        1        foo  1        NaN\n        2         []  1         []\n        3     [3, 4]  1     [d, e]\n\n        Single-column explode.\n\n        >>> df.explode('A')\n             A  B          C\n        0    0  1  [a, b, c]\n        0    1  1  [a, b, c]\n        0    2  1  [a, b, c]\n        1  foo  1        NaN\n        2  NaN  1         []\n        3    3  1     [d, e]\n        3    4  1     [d, e]\n\n        Multi-column explode.\n\n        >>> df.explode(list('AC'))\n             A  B    C\n        0    0  1    a\n        0    1  1    b\n        0    2  1    c\n        1  foo  1  NaN\n        2  NaN  1  NaN\n        3    3  1    d\n        3    4  1    e\n        \"\"\"\n        if not self.columns.is_unique:\n            raise ValueError(\"columns must be unique\")\n\n        columns: list[Hashable]\n        if is_scalar(column) or isinstance(column, tuple):\n            columns = [column]\n        elif isinstance(column, list) and all(\n            map(lambda c: is_scalar(c) or isinstance(c, tuple), column)\n        ):\n            if not column:\n                raise ValueError(\"column must be nonempty\")\n            if len(column) > len(set(column)):\n                raise ValueError(\"column must be unique\")\n            columns = column\n        else:\n            raise ValueError(\"column must be a scalar, tuple, or list thereof\")\n\n        df = self.reset_index(drop=True)\n        if len(columns) == 1:\n            result = df[columns[0]].explode()\n        else:\n            mylen = lambda x: len(x) if is_list_like(x) else -1\n            counts0 = self[columns[0]].apply(mylen)\n            for c in columns[1:]:\n                if not all(counts0 == self[c].apply(mylen)):\n                    raise ValueError(\"columns must have matching element counts\")\n            result = DataFrame({c: df[c].explode() for c in columns})\n        result = df.drop(columns, axis=1).join(result)\n        if ignore_index:\n            result.index = default_index(len(result))\n        else:\n            result.index = self.index.take(result.index)\n        result = result.reindex(columns=self.columns, copy=False)\n\n        return result\n\n    def unstack(self, level: Level = -1, fill_value=None):\n        \"\"\"\n        Pivot a level of the (necessarily hierarchical) index labels.\n\n        Returns a DataFrame having a new level of column labels whose inner-most level\n        consists of the pivoted index labels.\n\n        If the index is not a MultiIndex, the output will be a Series\n        (the analogue of stack when the columns are not a MultiIndex).\n\n        Parameters\n        ----------\n        level : int, str, or list of these, default -1 (last level)\n            Level(s) of index to unstack, can pass level name.\n        fill_value : int, str or dict\n            Replace NaN with this value if the unstack produces missing values.\n\n        Returns\n        -------\n        Series or DataFrame\n\n        See Also\n        --------\n        DataFrame.pivot : Pivot a table based on column values.\n        DataFrame.stack : Pivot a level of the column labels (inverse operation\n            from `unstack`).\n\n        Examples\n        --------\n        >>> index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),\n        ...                                    ('two', 'a'), ('two', 'b')])\n        >>> s = pd.Series(np.arange(1.0, 5.0), index=index)\n        >>> s\n        one  a   1.0\n             b   2.0\n        two  a   3.0\n             b   4.0\n        dtype: float64\n\n        >>> s.unstack(level=-1)\n             a   b\n        one  1.0  2.0\n        two  3.0  4.0\n\n        >>> s.unstack(level=0)\n           one  two\n        a  1.0   3.0\n        b  2.0   4.0\n\n        >>> df = s.unstack(level=0)\n        >>> df.unstack()\n        one  a  1.0\n             b  2.0\n        two  a  3.0\n             b  4.0\n        dtype: float64\n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n\n        result = unstack(self, level, fill_value)\n\n        return result.__finalize__(self, method=\"unstack\")\n\n    @Appender(_shared_docs[\"melt\"] % {\"caller\": \"df.melt(\", \"other\": \"melt\"})\n    def melt(\n        self,\n        id_vars=None,\n        value_vars=None,\n        var_name=None,\n        value_name=\"value\",\n        col_level: Level | None = None,\n        ignore_index: bool = True,\n    ) -> DataFrame:\n\n        return melt(\n            self,\n            id_vars=id_vars,\n            value_vars=value_vars,\n            var_name=var_name,\n            value_name=value_name,\n            col_level=col_level,\n            ignore_index=ignore_index,\n        )\n\n    # ----------------------------------------------------------------------\n    # Time series-related\n\n    @doc(\n        Series.diff,\n        klass=\"Dataframe\",\n        extra_params=\"axis : {0 or 'index', 1 or 'columns'}, default 0\\n    \"\n        \"Take difference over rows (0) or columns (1).\\n\",\n        other_klass=\"Series\",\n        examples=dedent(\n            \"\"\"\n        Difference with previous row\n\n        >>> df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6],\n        ...                    'b': [1, 1, 2, 3, 5, 8],\n        ...                    'c': [1, 4, 9, 16, 25, 36]})\n        >>> df\n           a  b   c\n        0  1  1   1\n        1  2  1   4\n        2  3  2   9\n        3  4  3  16\n        4  5  5  25\n        5  6  8  36\n\n        >>> df.diff()\n             a    b     c\n        0  NaN  NaN   NaN\n        1  1.0  0.0   3.0\n        2  1.0  1.0   5.0\n        3  1.0  1.0   7.0\n        4  1.0  2.0   9.0\n        5  1.0  3.0  11.0\n\n        Difference with previous column\n\n        >>> df.diff(axis=1)\n            a  b   c\n        0 NaN  0   0\n        1 NaN -1   3\n        2 NaN -1   7\n        3 NaN -1  13\n        4 NaN  0  20\n        5 NaN  2  28\n\n        Difference with 3rd previous row\n\n        >>> df.diff(periods=3)\n             a    b     c\n        0  NaN  NaN   NaN\n        1  NaN  NaN   NaN\n        2  NaN  NaN   NaN\n        3  3.0  2.0  15.0\n        4  3.0  4.0  21.0\n        5  3.0  6.0  27.0\n\n        Difference with following row\n\n        >>> df.diff(periods=-1)\n             a    b     c\n        0 -1.0  0.0  -3.0\n        1 -1.0 -1.0  -5.0\n        2 -1.0 -1.0  -7.0\n        3 -1.0 -2.0  -9.0\n        4 -1.0 -3.0 -11.0\n        5  NaN  NaN   NaN\n\n        Overflow in input dtype\n\n        >>> df = pd.DataFrame({'a': [1, 0]}, dtype=np.uint8)\n        >>> df.diff()\n               a\n        0    NaN\n        1  255.0\"\"\"\n        ),\n    )\n    def diff(self, periods: int = 1, axis: Axis = 0) -> DataFrame:\n        if not lib.is_integer(periods):\n            if not (\n                is_float(periods)\n                # error: \"int\" has no attribute \"is_integer\"\n                and periods.is_integer()  # type: ignore[attr-defined]\n            ):\n                raise ValueError(\"periods must be an integer\")\n            periods = int(periods)\n\n        axis = self._get_axis_number(axis)\n        if axis == 1 and periods != 0:\n            return self - self.shift(periods, axis=axis)\n\n        new_data = self._mgr.diff(n=periods, axis=axis)\n        return self._constructor(new_data).__finalize__(self, \"diff\")\n\n    # ----------------------------------------------------------------------\n    # Function application\n\n    def _gotitem(\n        self,\n        key: IndexLabel,\n        ndim: int,\n        subset: DataFrame | Series | None = None,\n    ) -> DataFrame | Series:\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : {1, 2}\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n        if subset is None:\n            subset = self\n        elif subset.ndim == 1:  # is Series\n            return subset\n\n        # TODO: _shallow_copy(subset)?\n        return subset[key]\n\n    _agg_summary_and_see_also_doc = dedent(\n        \"\"\"\n    The aggregation operations are always performed over an axis, either the\n    index (default) or the column axis. This behavior is different from\n    `numpy` aggregation functions (`mean`, `median`, `prod`, `sum`, `std`,\n    `var`), where the default is to compute the aggregation of the flattened\n    array, e.g., ``numpy.mean(arr_2d)`` as opposed to\n    ``numpy.mean(arr_2d, axis=0)``.\n\n    `agg` is an alias for `aggregate`. Use the alias.\n\n    See Also\n    --------\n    DataFrame.apply : Perform any type of operations.\n    DataFrame.transform : Perform transformation type operations.\n    core.groupby.GroupBy : Perform operations over groups.\n    core.resample.Resampler : Perform operations over resampled bins.\n    core.window.Rolling : Perform operations over rolling window.\n    core.window.Expanding : Perform operations over expanding window.\n    core.window.ExponentialMovingWindow : Perform operation over exponential weighted\n        window.\n    \"\"\"\n    )\n\n    _agg_examples_doc = dedent(\n        \"\"\"\n    Examples\n    --------\n    >>> df = pd.DataFrame([[1, 2, 3],\n    ...                    [4, 5, 6],\n    ...                    [7, 8, 9],\n    ...                    [np.nan, np.nan, np.nan]],\n    ...                   columns=['A', 'B', 'C'])\n\n    Aggregate these functions over the rows.\n\n    >>> df.agg(['sum', 'min'])\n            A     B     C\n    sum  12.0  15.0  18.0\n    min   1.0   2.0   3.0\n\n    Different aggregations per column.\n\n    >>> df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})\n            A    B\n    sum  12.0  NaN\n    min   1.0  2.0\n    max   NaN  8.0\n\n    Aggregate different functions over the columns and rename the index of the resulting\n    DataFrame.\n\n    >>> df.agg(x=('A', max), y=('B', 'min'), z=('C', np.mean))\n         A    B    C\n    x  7.0  NaN  NaN\n    y  NaN  2.0  NaN\n    z  NaN  NaN  6.0\n\n    Aggregate over the columns.\n\n    >>> df.agg(\"mean\", axis=\"columns\")\n    0    2.0\n    1    5.0\n    2    8.0\n    3    NaN\n    dtype: float64\n    \"\"\"\n    )\n\n    @doc(\n        _shared_docs[\"aggregate\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        axis=_shared_doc_kwargs[\"axis\"],\n        see_also=_agg_summary_and_see_also_doc,\n        examples=_agg_examples_doc,\n    )\n    def aggregate(self, func=None, axis: Axis = 0, *args, **kwargs):\n        from pandas.core.apply import frame_apply\n\n        axis = self._get_axis_number(axis)\n\n        relabeling, func, columns, order = reconstruct_func(func, **kwargs)\n\n        op = frame_apply(self, func=func, axis=axis, args=args, kwargs=kwargs)\n        result = op.agg()\n\n        if relabeling:\n            # This is to keep the order to columns occurrence unchanged, and also\n            # keep the order of new columns occurrence unchanged\n\n            # For the return values of reconstruct_func, if relabeling is\n            # False, columns and order will be None.\n            assert columns is not None\n            assert order is not None\n\n            result_in_dict = relabel_result(result, func, columns, order)\n            result = DataFrame(result_in_dict, index=columns)\n\n        return result\n\n    agg = aggregate\n\n    @doc(\n        _shared_docs[\"transform\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        axis=_shared_doc_kwargs[\"axis\"],\n    )\n    def transform(\n        self, func: AggFuncType, axis: Axis = 0, *args, **kwargs\n    ) -> DataFrame:\n        from pandas.core.apply import frame_apply\n\n        op = frame_apply(self, func=func, axis=axis, args=args, kwargs=kwargs)\n        result = op.transform()\n        assert isinstance(result, DataFrame)\n        return result\n\n    def apply(\n        self,\n        func: AggFuncType,\n        axis: Axis = 0,\n        raw: bool = False,\n        result_type=None,\n        args=(),\n        **kwargs,\n    ):\n        \"\"\"\n        Apply a function along an axis of the DataFrame.\n\n        Objects passed to the function are Series objects whose index is\n        either the DataFrame's index (``axis=0``) or the DataFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to apply to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': apply function to each column.\n            * 1 or 'columns': apply function to each row.\n\n        raw : bool, default False\n            Determines if row or column is passed as a Series or ndarray object:\n\n            * ``False`` : passes each row or column as a Series to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just applying a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Series if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the DataFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Series\n            of those. However if the apply function returns a Series these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/series.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Series or DataFrame\n            Result of applying ``func`` along the given axis of the\n            DataFrame.\n\n        See Also\n        --------\n        DataFrame.applymap: For elementwise operations.\n        DataFrame.aggregate: Only perform aggregating type operations.\n        DataFrame.transform: Only perform transforming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n        for more details.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> df\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(df)``):\n\n        >>> df.apply(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> df.apply(np.sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> df.apply(np.sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Series\n\n        >>> df.apply(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Series inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Series index.\n\n        >>> df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        \"\"\"\n        from pandas.core.apply import frame_apply\n\n        op = frame_apply(\n            self,\n            func=func,\n            axis=axis,\n            raw=raw,\n            result_type=result_type,\n            args=args,\n            kwargs=kwargs,\n        )\n        return op.apply().__finalize__(self, method=\"apply\")\n\n    def applymap(\n        self, func: PythonFuncType, na_action: str | None = None, **kwargs\n    ) -> DataFrame:\n        \"\"\"\n        Apply a function to a Dataframe elementwise.\n\n        This method applies a function that accepts and returns a scalar\n        to every element of a DataFrame.\n\n        Parameters\n        ----------\n        func : callable\n            Python function, returns a single value from a single value.\n        na_action : {None, 'ignore'}, default None\n            If ‘ignore’, propagate NaN values, without passing them to func.\n\n            .. versionadded:: 1.2\n\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n            .. versionadded:: 1.3.0\n\n        Returns\n        -------\n        DataFrame\n            Transformed DataFrame.\n\n        See Also\n        --------\n        DataFrame.apply : Apply a function along input axis of DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])\n        >>> df\n               0      1\n        0  1.000  2.120\n        1  3.356  4.567\n\n        >>> df.applymap(lambda x: len(str(x)))\n           0  1\n        0  3  4\n        1  5  5\n\n        Like Series.map, NA values can be ignored:\n\n        >>> df_copy = df.copy()\n        >>> df_copy.iloc[0, 0] = pd.NA\n        >>> df_copy.applymap(lambda x: len(str(x)), na_action='ignore')\n              0  1\n        0  <NA>  4\n        1     5  5\n\n        Note that a vectorized version of `func` often exists, which will\n        be much faster. You could square each number elementwise.\n\n        >>> df.applymap(lambda x: x**2)\n                   0          1\n        0   1.000000   4.494400\n        1  11.262736  20.857489\n\n        But it's better to avoid applymap in that case.\n\n        >>> df ** 2\n                   0          1\n        0   1.000000   4.494400\n        1  11.262736  20.857489\n        \"\"\"\n        if na_action not in {\"ignore\", None}:\n            raise ValueError(\n                f\"na_action must be 'ignore' or None. Got {repr(na_action)}\"\n            )\n        ignore_na = na_action == \"ignore\"\n        func = functools.partial(func, **kwargs)\n\n        # if we have a dtype == 'M8[ns]', provide boxed values\n        def infer(x):\n            if x.empty:\n                return lib.map_infer(x, func, ignore_na=ignore_na)\n            return lib.map_infer(x.astype(object)._values, func, ignore_na=ignore_na)\n\n        return self.apply(infer).__finalize__(self, \"applymap\")\n\n    # ----------------------------------------------------------------------\n    # Merging / joining methods\n\n    def append(\n        self,\n        other,\n        ignore_index: bool = False,\n        verify_integrity: bool = False,\n        sort: bool = False,\n    ) -> DataFrame:\n        \"\"\"\n        Append rows of `other` to the end of caller, returning a new object.\n\n        Columns in `other` that are not in the caller are added as new columns.\n\n        Parameters\n        ----------\n        other : DataFrame or Series/dict-like object, or list of these\n            The data to append.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, …, n - 1.\n        verify_integrity : bool, default False\n            If True, raise ValueError on creating index with duplicates.\n        sort : bool, default False\n            Sort columns if the columns of `self` and `other` are not aligned.\n\n            .. versionchanged:: 1.0.0\n\n                Changed to not sort by default.\n\n        Returns\n        -------\n        DataFrame\n            A new DataFrame consisting of the rows of caller and the rows of `other`.\n\n        See Also\n        --------\n        concat : General function to concatenate DataFrame or Series objects.\n\n        Notes\n        -----\n        If a list of dict/series is passed and the keys are all contained in\n        the DataFrame's index, the order of the columns in the resulting\n        DataFrame will be unchanged.\n\n        Iteratively appending rows to a DataFrame can be more computationally\n        intensive than a single concatenate. A better solution is to append\n        those rows to a list and then concatenate the list with the original\n        DataFrame all at once.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'), index=['x', 'y'])\n        >>> df\n           A  B\n        x  1  2\n        y  3  4\n        >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'), index=['x', 'y'])\n        >>> df.append(df2)\n           A  B\n        x  1  2\n        y  3  4\n        x  5  6\n        y  7  8\n\n        With `ignore_index` set to True:\n\n        >>> df.append(df2, ignore_index=True)\n           A  B\n        0  1  2\n        1  3  4\n        2  5  6\n        3  7  8\n\n        The following, while not recommended methods for generating DataFrames,\n        show two ways to generate a DataFrame from multiple data sources.\n\n        Less efficient:\n\n        >>> df = pd.DataFrame(columns=['A'])\n        >>> for i in range(5):\n        ...     df = df.append({'A': i}, ignore_index=True)\n        >>> df\n           A\n        0  0\n        1  1\n        2  2\n        3  3\n        4  4\n\n        More efficient:\n\n        >>> pd.concat([pd.DataFrame([i], columns=['A']) for i in range(5)],\n        ...           ignore_index=True)\n           A\n        0  0\n        1  1\n        2  2\n        3  3\n        4  4\n        \"\"\"\n        combined_columns = None\n        if isinstance(other, (Series, dict)):\n            if isinstance(other, dict):\n                if not ignore_index:\n                    raise TypeError(\"Can only append a dict if ignore_index=True\")\n                other = Series(other)\n            if other.name is None and not ignore_index:\n                raise TypeError(\n                    \"Can only append a Series if ignore_index=True \"\n                    \"or if the Series has a name\"\n                )\n\n            index = Index([other.name], name=self.index.name)\n            idx_diff = other.index.difference(self.columns)\n            combined_columns = self.columns.append(idx_diff)\n            row_df = other.to_frame().T\n            # infer_objects is needed for\n            #  test_append_empty_frame_to_series_with_dateutil_tz\n            other = row_df.infer_objects().rename_axis(index.names, copy=False)\n        elif isinstance(other, list):\n            if not other:\n                pass\n            elif not isinstance(other[0], DataFrame):\n                other = DataFrame(other)\n                if self.index.name is not None and not ignore_index:\n                    other.index.name = self.index.name\n\n        from pandas.core.reshape.concat import concat\n\n        if isinstance(other, (list, tuple)):\n            to_concat = [self, *other]\n        else:\n            to_concat = [self, other]\n\n        result = concat(\n            to_concat,\n            ignore_index=ignore_index,\n            verify_integrity=verify_integrity,\n            sort=sort,\n        )\n        if (\n            combined_columns is not None\n            and not sort\n            and not combined_columns.equals(result.columns)\n        ):\n            # TODO: reindexing here is a kludge bc union_indexes does not\n            #  pass sort to index.union, xref #43375\n            # combined_columns.equals check is necessary for preserving dtype\n            #  in test_crosstab_normalize\n            result = result.reindex(combined_columns, axis=1)\n        return result.__finalize__(self, method=\"append\")\n\n    def join(\n        self,\n        other: DataFrame | Series,\n        on: IndexLabel | None = None,\n        how: str = \"left\",\n        lsuffix: str = \"\",\n        rsuffix: str = \"\",\n        sort: bool = False,\n    ) -> DataFrame:\n        \"\"\"\n        Join columns of another DataFrame.\n\n        Join columns with `other` DataFrame either on index or on a key\n        column. Efficiently join multiple DataFrame objects by index at once by\n        passing a list.\n\n        Parameters\n        ----------\n        other : DataFrame, Series, or list of DataFrame\n            Index should be similar to one of the columns in this one. If a\n            Series is passed, its name attribute must be set, and that will be\n            used as the column name in the resulting joined DataFrame.\n        on : str, list of str, or array-like, optional\n            Column or index level name(s) in the caller to join on the index\n            in `other`, otherwise joins index-on-index. If multiple\n            values given, the `other` DataFrame must have a MultiIndex. Can\n            pass an array as the join key if it is not already contained in\n            the calling DataFrame. Like an Excel VLOOKUP operation.\n        how : {'left', 'right', 'outer', 'inner'}, default 'left'\n            How to handle the operation of the two objects.\n\n            * left: use calling frame's index (or column if on is specified)\n            * right: use `other`'s index.\n            * outer: form union of calling frame's index (or column if on is\n              specified) with `other`'s index, and sort it.\n              lexicographically.\n            * inner: form intersection of calling frame's index (or column if\n              on is specified) with `other`'s index, preserving the order\n              of the calling's one.\n            * cross: creates the cartesian product from both frames, preserves the order\n              of the left keys.\n\n              .. versionadded:: 1.2.0\n\n        lsuffix : str, default ''\n            Suffix to use from left frame's overlapping columns.\n        rsuffix : str, default ''\n            Suffix to use from right frame's overlapping columns.\n        sort : bool, default False\n            Order result DataFrame lexicographically by the join key. If False,\n            the order of the join key depends on the join type (how keyword).\n\n        Returns\n        -------\n        DataFrame\n            A dataframe containing columns from both the caller and `other`.\n\n        See Also\n        --------\n        DataFrame.merge : For column(s)-on-column(s) operations.\n\n        Notes\n        -----\n        Parameters `on`, `lsuffix`, and `rsuffix` are not supported when\n        passing a list of `DataFrame` objects.\n\n        Support for specifying index levels as the `on` parameter was added\n        in version 0.23.0.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],\n        ...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n\n        >>> df\n          key   A\n        0  K0  A0\n        1  K1  A1\n        2  K2  A2\n        3  K3  A3\n        4  K4  A4\n        5  K5  A5\n\n        >>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],\n        ...                       'B': ['B0', 'B1', 'B2']})\n\n        >>> other\n          key   B\n        0  K0  B0\n        1  K1  B1\n        2  K2  B2\n\n        Join DataFrames using their indexes.\n\n        >>> df.join(other, lsuffix='_caller', rsuffix='_other')\n          key_caller   A key_other    B\n        0         K0  A0        K0   B0\n        1         K1  A1        K1   B1\n        2         K2  A2        K2   B2\n        3         K3  A3       NaN  NaN\n        4         K4  A4       NaN  NaN\n        5         K5  A5       NaN  NaN\n\n        If we want to join using the key columns, we need to set key to be\n        the index in both `df` and `other`. The joined DataFrame will have\n        key as its index.\n\n        >>> df.set_index('key').join(other.set_index('key'))\n              A    B\n        key\n        K0   A0   B0\n        K1   A1   B1\n        K2   A2   B2\n        K3   A3  NaN\n        K4   A4  NaN\n        K5   A5  NaN\n\n        Another option to join using the key columns is to use the `on`\n        parameter. DataFrame.join always uses `other`'s index but we can use\n        any column in `df`. This method preserves the original DataFrame's\n        index in the result.\n\n        >>> df.join(other.set_index('key'), on='key')\n          key   A    B\n        0  K0  A0   B0\n        1  K1  A1   B1\n        2  K2  A2   B2\n        3  K3  A3  NaN\n        4  K4  A4  NaN\n        5  K5  A5  NaN\n        \"\"\"\n        return self._join_compat(\n            other, on=on, how=how, lsuffix=lsuffix, rsuffix=rsuffix, sort=sort\n        )\n\n    def _join_compat(\n        self,\n        other: DataFrame | Series,\n        on: IndexLabel | None = None,\n        how: str = \"left\",\n        lsuffix: str = \"\",\n        rsuffix: str = \"\",\n        sort: bool = False,\n    ):\n        from pandas.core.reshape.concat import concat\n        from pandas.core.reshape.merge import merge\n\n        if isinstance(other, Series):\n            if other.name is None:\n                raise ValueError(\"Other Series must have a name\")\n            other = DataFrame({other.name: other})\n\n        if isinstance(other, DataFrame):\n            if how == \"cross\":\n                return merge(\n                    self,\n                    other,\n                    how=how,\n                    on=on,\n                    suffixes=(lsuffix, rsuffix),\n                    sort=sort,\n                )\n            return merge(\n                self,\n                other,\n                left_on=on,\n                how=how,\n                left_index=on is None,\n                right_index=True,\n                suffixes=(lsuffix, rsuffix),\n                sort=sort,\n            )\n        else:\n            if on is not None:\n                raise ValueError(\n                    \"Joining multiple DataFrames only supported for joining on index\"\n                )\n\n            frames = [self] + list(other)\n\n            can_concat = all(df.index.is_unique for df in frames)\n\n            # join indexes only using concat\n            if can_concat:\n                if how == \"left\":\n                    res = concat(\n                        frames, axis=1, join=\"outer\", verify_integrity=True, sort=sort\n                    )\n                    return res.reindex(self.index, copy=False)\n                else:\n                    return concat(\n                        frames, axis=1, join=how, verify_integrity=True, sort=sort\n                    )\n\n            joined = frames[0]\n\n            for frame in frames[1:]:\n                joined = merge(\n                    joined, frame, how=how, left_index=True, right_index=True\n                )\n\n            return joined\n\n    @Substitution(\"\")\n    @Appender(_merge_doc, indents=2)\n    def merge(\n        self,\n        right: DataFrame | Series,\n        how: str = \"inner\",\n        on: IndexLabel | None = None,\n        left_on: IndexLabel | None = None,\n        right_on: IndexLabel | None = None,\n        left_index: bool = False,\n        right_index: bool = False,\n        sort: bool = False,\n        suffixes: Suffixes = (\"_x\", \"_y\"),\n        copy: bool = True,\n        indicator: bool = False,\n        validate: str | None = None,\n    ) -> DataFrame:\n        from pandas.core.reshape.merge import merge\n\n        return merge(\n            self,\n            right,\n            how=how,\n            on=on,\n            left_on=left_on,\n            right_on=right_on,\n            left_index=left_index,\n            right_index=right_index,\n            sort=sort,\n            suffixes=suffixes,\n            copy=copy,\n            indicator=indicator,\n            validate=validate,\n        )\n\n    def round(\n        self, decimals: int | dict[IndexLabel, int] | Series = 0, *args, **kwargs\n    ) -> DataFrame:\n        \"\"\"\n        Round a DataFrame to a variable number of decimal places.\n\n        Parameters\n        ----------\n        decimals : int, dict, Series\n            Number of decimal places to round each column to. If an int is\n            given, round each column to the same number of places.\n            Otherwise dict and Series round to variable numbers of places.\n            Column names should be in the keys if `decimals` is a\n            dict-like, or in the index if `decimals` is a Series. Any\n            columns not included in `decimals` will be left as is. Elements\n            of `decimals` which are not columns of the input will be\n            ignored.\n        *args\n            Additional keywords have no effect but might be accepted for\n            compatibility with numpy.\n        **kwargs\n            Additional keywords have no effect but might be accepted for\n            compatibility with numpy.\n\n        Returns\n        -------\n        DataFrame\n            A DataFrame with the affected columns rounded to the specified\n            number of decimal places.\n\n        See Also\n        --------\n        numpy.around : Round a numpy array to the given number of decimals.\n        Series.round : Round a Series to the given number of decimals.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df\n            dogs  cats\n        0  0.21  0.32\n        1  0.01  0.67\n        2  0.66  0.03\n        3  0.21  0.18\n\n        By providing an integer each column is rounded to the same number\n        of decimal places\n\n        >>> df.round(1)\n            dogs  cats\n        0   0.2   0.3\n        1   0.0   0.7\n        2   0.7   0.0\n        3   0.2   0.2\n\n        With a dict, the number of places for specific columns can be\n        specified with the column names as key and the number of decimal\n        places as value\n\n        >>> df.round({'dogs': 1, 'cats': 0})\n            dogs  cats\n        0   0.2   0.0\n        1   0.0   1.0\n        2   0.7   0.0\n        3   0.2   0.0\n\n        Using a Series, the number of places for specific columns can be\n        specified with the column names as index and the number of\n        decimal places as value\n\n        >>> decimals = pd.Series([0, 1], index=['cats', 'dogs'])\n        >>> df.round(decimals)\n            dogs  cats\n        0   0.2   0.0\n        1   0.0   1.0\n        2   0.7   0.0\n        3   0.2   0.0\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        def _dict_round(df: DataFrame, decimals):\n            for col, vals in df.items():\n                try:\n                    yield _series_round(vals, decimals[col])\n                except KeyError:\n                    yield vals\n\n        def _series_round(ser: Series, decimals: int):\n            if is_integer_dtype(ser.dtype) or is_float_dtype(ser.dtype):\n                return ser.round(decimals)\n            return ser\n\n        nv.validate_round(args, kwargs)\n\n        if isinstance(decimals, (dict, Series)):\n            if isinstance(decimals, Series) and not decimals.index.is_unique:\n                raise ValueError(\"Index of decimals must be unique\")\n            if is_dict_like(decimals) and not all(\n                is_integer(value) for _, value in decimals.items()\n            ):\n                raise TypeError(\"Values in decimals must be integers\")\n            new_cols = list(_dict_round(self, decimals))\n        elif is_integer(decimals):\n            # Dispatch to Series.round\n            new_cols = [_series_round(v, decimals) for _, v in self.items()]\n        else:\n            raise TypeError(\"decimals must be an integer, a dict-like or a Series\")\n\n        if len(new_cols) > 0:\n            return self._constructor(\n                concat(new_cols, axis=1), index=self.index, columns=self.columns\n            )\n        else:\n            return self\n\n    # ----------------------------------------------------------------------\n    # Statistical methods, etc.\n\n    def corr(\n        self,\n        method: str | Callable[[np.ndarray, np.ndarray], float] = \"pearson\",\n        min_periods: int = 1,\n    ) -> DataFrame:\n        \"\"\"\n        Compute pairwise correlation of columns, excluding NA/null values.\n\n        Parameters\n        ----------\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method of correlation:\n\n            * pearson : standard correlation coefficient\n            * kendall : Kendall Tau correlation coefficient\n            * spearman : Spearman rank correlation\n            * callable: callable with input two 1d ndarrays\n                and returning a float. Note that the returned matrix from corr\n                will have 1 along the diagonals and will be symmetric\n                regardless of the callable's behavior.\n        min_periods : int, optional\n            Minimum number of observations required per pair of columns\n            to have a valid result. Currently only available for Pearson\n            and Spearman correlation.\n\n        Returns\n        -------\n        DataFrame\n            Correlation matrix.\n\n        See Also\n        --------\n        DataFrame.corrwith : Compute pairwise correlation with another\n            DataFrame or Series.\n        Series.corr : Compute the correlation between two Series.\n\n        Examples\n        --------\n        >>> def histogram_intersection(a, b):\n        ...     v = np.minimum(a, b).sum().round(decimals=1)\n        ...     return v\n        >>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df.corr(method=histogram_intersection)\n              dogs  cats\n        dogs   1.0   0.3\n        cats   0.3   1.0\n        \"\"\"\n        numeric_df = self._get_numeric_data()\n        cols = numeric_df.columns\n        idx = cols.copy()\n        mat = numeric_df.to_numpy(dtype=float, na_value=np.nan, copy=False)\n\n        if method == \"pearson\":\n            correl = libalgos.nancorr(mat, minp=min_periods)\n        elif method == \"spearman\":\n            correl = libalgos.nancorr_spearman(mat, minp=min_periods)\n        elif method == \"kendall\" or callable(method):\n            if min_periods is None:\n                min_periods = 1\n            mat = mat.T\n            corrf = nanops.get_corr_func(method)\n            K = len(cols)\n            correl = np.empty((K, K), dtype=float)\n            mask = np.isfinite(mat)\n            for i, ac in enumerate(mat):\n                for j, bc in enumerate(mat):\n                    if i > j:\n                        continue\n\n                    valid = mask[i] & mask[j]\n                    if valid.sum() < min_periods:\n                        c = np.nan\n                    elif i == j:\n                        c = 1.0\n                    elif not valid.all():\n                        c = corrf(ac[valid], bc[valid])\n                    else:\n                        c = corrf(ac, bc)\n                    correl[i, j] = c\n                    correl[j, i] = c\n        else:\n            raise ValueError(\n                \"method must be either 'pearson', \"\n                \"'spearman', 'kendall', or a callable, \"\n                f\"'{method}' was supplied\"\n            )\n\n        return self._constructor(correl, index=idx, columns=cols)\n\n    def cov(self, min_periods: int | None = None, ddof: int | None = 1) -> DataFrame:\n        \"\"\"\n        Compute pairwise covariance of columns, excluding NA/null values.\n\n        Compute the pairwise covariance among the series of a DataFrame.\n        The returned data frame is the `covariance matrix\n        <https://en.wikipedia.org/wiki/Covariance_matrix>`__ of the columns\n        of the DataFrame.\n\n        Both NA and null values are automatically excluded from the\n        calculation. (See the note below about bias from missing values.)\n        A threshold can be set for the minimum number of\n        observations for each value created. Comparisons with observations\n        below this threshold will be returned as ``NaN``.\n\n        This method is generally used for the analysis of time series data to\n        understand the relationship between different measures\n        across time.\n\n        Parameters\n        ----------\n        min_periods : int, optional\n            Minimum number of observations required per pair of columns\n            to have a valid result.\n\n        ddof : int, default 1\n            Delta degrees of freedom.  The divisor used in calculations\n            is ``N - ddof``, where ``N`` represents the number of elements.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        DataFrame\n            The covariance matrix of the series of the DataFrame.\n\n        See Also\n        --------\n        Series.cov : Compute covariance with another Series.\n        core.window.ExponentialMovingWindow.cov: Exponential weighted sample covariance.\n        core.window.Expanding.cov : Expanding sample covariance.\n        core.window.Rolling.cov : Rolling sample covariance.\n\n        Notes\n        -----\n        Returns the covariance matrix of the DataFrame's time series.\n        The covariance is normalized by N-ddof.\n\n        For DataFrames that have Series that are missing data (assuming that\n        data is `missing at random\n        <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random>`__)\n        the returned covariance matrix will be an unbiased estimate\n        of the variance and covariance between the member Series.\n\n        However, for many applications this estimate may not be acceptable\n        because the estimate covariance matrix is not guaranteed to be positive\n        semi-definite. This could lead to estimate correlations having\n        absolute values which are greater than one, and/or a non-invertible\n        covariance matrix. See `Estimation of covariance matrices\n        <https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_\n        matrices>`__ for more details.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)],\n        ...                   columns=['dogs', 'cats'])\n        >>> df.cov()\n                  dogs      cats\n        dogs  0.666667 -1.000000\n        cats -1.000000  1.666667\n\n        >>> np.random.seed(42)\n        >>> df = pd.DataFrame(np.random.randn(1000, 5),\n        ...                   columns=['a', 'b', 'c', 'd', 'e'])\n        >>> df.cov()\n                  a         b         c         d         e\n        a  0.998438 -0.020161  0.059277 -0.008943  0.014144\n        b -0.020161  1.059352 -0.008543 -0.024738  0.009826\n        c  0.059277 -0.008543  1.010670 -0.001486 -0.000271\n        d -0.008943 -0.024738 -0.001486  0.921297 -0.013692\n        e  0.014144  0.009826 -0.000271 -0.013692  0.977795\n\n        **Minimum number of periods**\n\n        This method also supports an optional ``min_periods`` keyword\n        that specifies the required minimum number of non-NA observations for\n        each column pair in order to have a valid result:\n\n        >>> np.random.seed(42)\n        >>> df = pd.DataFrame(np.random.randn(20, 3),\n        ...                   columns=['a', 'b', 'c'])\n        >>> df.loc[df.index[:5], 'a'] = np.nan\n        >>> df.loc[df.index[5:10], 'b'] = np.nan\n        >>> df.cov(min_periods=12)\n                  a         b         c\n        a  0.316741       NaN -0.150812\n        b       NaN  1.248003  0.191417\n        c -0.150812  0.191417  0.895202\n        \"\"\"\n        numeric_df = self._get_numeric_data()\n        cols = numeric_df.columns\n        idx = cols.copy()\n        mat = numeric_df.to_numpy(dtype=float, na_value=np.nan, copy=False)\n\n        if notna(mat).all():\n            if min_periods is not None and min_periods > len(mat):\n                base_cov = np.empty((mat.shape[1], mat.shape[1]))\n                base_cov.fill(np.nan)\n            else:\n                base_cov = np.cov(mat.T, ddof=ddof)\n            base_cov = base_cov.reshape((len(cols), len(cols)))\n        else:\n            base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\n\n        return self._constructor(base_cov, index=idx, columns=cols)\n\n    def corrwith(self, other, axis: Axis = 0, drop=False, method=\"pearson\") -> Series:\n        \"\"\"\n        Compute pairwise correlation.\n\n        Pairwise correlation is computed between rows or columns of\n        DataFrame with rows or columns of Series or DataFrame. DataFrames\n        are first aligned along both axes before computing the\n        correlations.\n\n        Parameters\n        ----------\n        other : DataFrame, Series\n            Object with which to compute correlations.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' to compute column-wise, 1 or 'columns' for\n            row-wise.\n        drop : bool, default False\n            Drop missing indices from result.\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method of correlation:\n\n            * pearson : standard correlation coefficient\n            * kendall : Kendall Tau correlation coefficient\n            * spearman : Spearman rank correlation\n            * callable: callable with input two 1d ndarrays\n                and returning a float.\n\n        Returns\n        -------\n        Series\n            Pairwise correlations.\n\n        See Also\n        --------\n        DataFrame.corr : Compute pairwise correlation of columns.\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        this = self._get_numeric_data()\n\n        if isinstance(other, Series):\n            return this.apply(lambda x: other.corr(x, method=method), axis=axis)\n\n        other = other._get_numeric_data()\n        left, right = this.align(other, join=\"inner\", copy=False)\n\n        if axis == 1:\n            left = left.T\n            right = right.T\n\n        if method == \"pearson\":\n            # mask missing values\n            left = left + right * 0\n            right = right + left * 0\n\n            # demeaned data\n            ldem = left - left.mean()\n            rdem = right - right.mean()\n\n            num = (ldem * rdem).sum()\n            dom = (left.count() - 1) * left.std() * right.std()\n\n            correl = num / dom\n\n        elif method in [\"kendall\", \"spearman\"] or callable(method):\n\n            def c(x):\n                return nanops.nancorr(x[0], x[1], method=method)\n\n            correl = self._constructor_sliced(\n                map(c, zip(left.values.T, right.values.T)), index=left.columns\n            )\n\n        else:\n            raise ValueError(\n                f\"Invalid method {method} was passed, \"\n                \"valid methods are: 'pearson', 'kendall', \"\n                \"'spearman', or callable\"\n            )\n\n        if not drop:\n            # Find non-matching labels along the given axis\n            # and append missing correlations (GH 22375)\n            raxis = 1 if axis == 0 else 0\n            result_index = this._get_axis(raxis).union(other._get_axis(raxis))\n            idx_diff = result_index.difference(correl.index)\n\n            if len(idx_diff) > 0:\n                correl = correl.append(Series([np.nan] * len(idx_diff), index=idx_diff))\n\n        return correl\n\n    # ----------------------------------------------------------------------\n    # ndarray-like stats methods\n\n    def count(\n        self, axis: Axis = 0, level: Level | None = None, numeric_only: bool = False\n    ):\n        \"\"\"\n        Count non-NA cells for each column or row.\n\n        The values `None`, `NaN`, `NaT`, and optionally `numpy.inf` (depending\n        on `pandas.options.mode.use_inf_as_na`) are considered NA.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            If 0 or 'index' counts are generated for each column.\n            If 1 or 'columns' counts are generated for each row.\n        level : int or str, optional\n            If the axis is a `MultiIndex` (hierarchical), count along a\n            particular `level`, collapsing into a `DataFrame`.\n            A `str` specifies the level name.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n        Returns\n        -------\n        Series or DataFrame\n            For each column/row the number of non-NA/null entries.\n            If `level` is specified returns a `DataFrame`.\n\n        See Also\n        --------\n        Series.count: Number of non-NA elements in a Series.\n        DataFrame.value_counts: Count unique combinations of columns.\n        DataFrame.shape: Number of DataFrame rows and columns (including NA\n            elements).\n        DataFrame.isna: Boolean same-sized DataFrame showing places of NA\n            elements.\n\n        Examples\n        --------\n        Constructing DataFrame from a dictionary:\n\n        >>> df = pd.DataFrame({\"Person\":\n        ...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n        ...                    \"Age\": [24., np.nan, 21., 33, 26],\n        ...                    \"Single\": [False, True, True, True, False]})\n        >>> df\n           Person   Age  Single\n        0    John  24.0   False\n        1    Myla   NaN    True\n        2   Lewis  21.0    True\n        3    John  33.0    True\n        4    Myla  26.0   False\n\n        Notice the uncounted NA values:\n\n        >>> df.count()\n        Person    5\n        Age       4\n        Single    5\n        dtype: int64\n\n        Counts for each **row**:\n\n        >>> df.count(axis='columns')\n        0    3\n        1    2\n        2    3\n        3    3\n        4    3\n        dtype: int64\n        \"\"\"\n        axis = self._get_axis_number(axis)\n        if level is not None:\n            warnings.warn(\n                \"Using the level keyword in DataFrame and Series aggregations is \"\n                \"deprecated and will be removed in a future version. Use groupby \"\n                \"instead. df.count(level=1) should use df.groupby(level=1).count().\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            return self._count_level(level, axis=axis, numeric_only=numeric_only)\n\n        if numeric_only:\n            frame = self._get_numeric_data()\n        else:\n            frame = self\n\n        # GH #423\n        if len(frame._get_axis(axis)) == 0:\n            result = self._constructor_sliced(0, index=frame._get_agg_axis(axis))\n        else:\n            if frame._is_mixed_type or frame._mgr.any_extension_types:\n                # the or any_extension_types is really only hit for single-\n                # column frames with an extension array\n                result = notna(frame).sum(axis=axis)\n            else:\n                # GH13407\n                series_counts = notna(frame).sum(axis=axis)\n                counts = series_counts.values\n                result = self._constructor_sliced(\n                    counts, index=frame._get_agg_axis(axis)\n                )\n\n        return result.astype(\"int64\")\n\n    def _count_level(self, level: Level, axis: int = 0, numeric_only: bool = False):\n        if numeric_only:\n            frame = self._get_numeric_data()\n        else:\n            frame = self\n\n        count_axis = frame._get_axis(axis)\n        agg_axis = frame._get_agg_axis(axis)\n\n        if not isinstance(count_axis, MultiIndex):\n            raise TypeError(\n                f\"Can only count levels on hierarchical {self._get_axis_name(axis)}.\"\n            )\n\n        # Mask NaNs: Mask rows or columns where the index level is NaN, and all\n        # values in the DataFrame that are NaN\n        if frame._is_mixed_type:\n            # Since we have mixed types, calling notna(frame.values) might\n            # upcast everything to object\n            values_mask = notna(frame).values\n        else:\n            # But use the speedup when we have homogeneous dtypes\n            values_mask = notna(frame.values)\n\n        index_mask = notna(count_axis.get_level_values(level=level))\n        if axis == 1:\n            mask = index_mask & values_mask\n        else:\n            mask = index_mask.reshape(-1, 1) & values_mask\n\n        if isinstance(level, str):\n            level = count_axis._get_level_number(level)\n\n        level_name = count_axis._names[level]\n        level_index = count_axis.levels[level]._rename(name=level_name)\n        level_codes = ensure_platform_int(count_axis.codes[level])\n        counts = lib.count_level_2d(mask, level_codes, len(level_index), axis=axis)\n\n        if axis == 1:\n            result = self._constructor(counts, index=agg_axis, columns=level_index)\n        else:\n            result = self._constructor(counts, index=level_index, columns=agg_axis)\n\n        return result\n\n    def _reduce(\n        self,\n        op,\n        name: str,\n        *,\n        axis: Axis = 0,\n        skipna: bool = True,\n        numeric_only: bool | None = None,\n        filter_type=None,\n        **kwds,\n    ):\n\n        assert filter_type is None or filter_type == \"bool\", filter_type\n        out_dtype = \"bool\" if filter_type == \"bool\" else None\n\n        if numeric_only is None and name in [\"mean\", \"median\"]:\n            own_dtypes = [arr.dtype for arr in self._mgr.arrays]\n\n            dtype_is_dt = np.array(\n                [is_datetime64_any_dtype(dtype) for dtype in own_dtypes],\n                dtype=bool,\n            )\n            if dtype_is_dt.any():\n                warnings.warn(\n                    \"DataFrame.mean and DataFrame.median with numeric_only=None \"\n                    \"will include datetime64 and datetime64tz columns in a \"\n                    \"future version.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n                # Non-copy equivalent to\n                #  dt64_cols = self.dtypes.apply(is_datetime64_any_dtype)\n                #  cols = self.columns[~dt64_cols]\n                #  self = self[cols]\n                predicate = lambda x: not is_datetime64_any_dtype(x.dtype)\n                mgr = self._mgr._get_data_subset(predicate)\n                self = type(self)(mgr)\n\n        # TODO: Make other agg func handle axis=None properly GH#21597\n        axis = self._get_axis_number(axis)\n        labels = self._get_agg_axis(axis)\n        assert axis in [0, 1]\n\n        def func(values: np.ndarray):\n            # We only use this in the case that operates on self.values\n            return op(values, axis=axis, skipna=skipna, **kwds)\n\n        def blk_func(values, axis=1):\n            if isinstance(values, ExtensionArray):\n                if not is_1d_only_ea_obj(values) and not isinstance(\n                    self._mgr, ArrayManager\n                ):\n                    return values._reduce(name, axis=1, skipna=skipna, **kwds)\n                return values._reduce(name, skipna=skipna, **kwds)\n            else:\n                return op(values, axis=axis, skipna=skipna, **kwds)\n\n        def _get_data() -> DataFrame:\n            if filter_type is None:\n                data = self._get_numeric_data()\n            else:\n                # GH#25101, GH#24434\n                assert filter_type == \"bool\"\n                data = self._get_bool_data()\n            return data\n\n        if numeric_only is not None or axis == 0:\n            # For numeric_only non-None and axis non-None, we know\n            #  which blocks to use and no try/except is needed.\n            #  For numeric_only=None only the case with axis==0 and no object\n            #  dtypes are unambiguous can be handled with BlockManager.reduce\n            # Case with EAs see GH#35881\n            df = self\n            if numeric_only is True:\n                df = _get_data()\n            if axis == 1:\n                df = df.T\n                axis = 0\n\n            ignore_failures = numeric_only is None\n\n            # After possibly _get_data and transposing, we are now in the\n            #  simple case where we can use BlockManager.reduce\n            res, _ = df._mgr.reduce(blk_func, ignore_failures=ignore_failures)\n            out = df._constructor(res).iloc[0]\n            if out_dtype is not None:\n                out = out.astype(out_dtype)\n            if axis == 0 and len(self) == 0 and name in [\"sum\", \"prod\"]:\n                # Even if we are object dtype, follow numpy and return\n                #  float64, see test_apply_funcs_over_empty\n                out = out.astype(np.float64)\n\n            if numeric_only is None and out.shape[0] != df.shape[1]:\n                # columns have been dropped GH#41480\n                arg_name = \"numeric_only\"\n                if name in [\"all\", \"any\"]:\n                    arg_name = \"bool_only\"\n                warnings.warn(\n                    \"Dropping of nuisance columns in DataFrame reductions \"\n                    f\"(with '{arg_name}=None') is deprecated; in a future \"\n                    \"version this will raise TypeError.  Select only valid \"\n                    \"columns before calling the reduction.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n\n            return out\n\n        assert numeric_only is None\n\n        data = self\n        values = data.values\n\n        try:\n            result = func(values)\n\n        except TypeError:\n            # e.g. in nanops trying to convert strs to float\n\n            data = _get_data()\n            labels = data._get_agg_axis(axis)\n\n            values = data.values\n            with np.errstate(all=\"ignore\"):\n                result = func(values)\n\n            # columns have been dropped GH#41480\n            arg_name = \"numeric_only\"\n            if name in [\"all\", \"any\"]:\n                arg_name = \"bool_only\"\n            warnings.warn(\n                \"Dropping of nuisance columns in DataFrame reductions \"\n                f\"(with '{arg_name}=None') is deprecated; in a future \"\n                \"version this will raise TypeError.  Select only valid \"\n                \"columns before calling the reduction.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        if hasattr(result, \"dtype\"):\n            if filter_type == \"bool\" and notna(result).all():\n                result = result.astype(np.bool_)\n            elif filter_type is None and is_object_dtype(result.dtype):\n                try:\n                    result = result.astype(np.float64)\n                except (ValueError, TypeError):\n                    # try to coerce to the original dtypes item by item if we can\n                    pass\n\n        result = self._constructor_sliced(result, index=labels)\n        return result\n\n    def nunique(self, axis: Axis = 0, dropna: bool = True) -> Series:\n        \"\"\"\n        Count number of distinct elements in specified axis.\n\n        Return Series with number of distinct elements. Can ignore NaN\n        values.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for\n            column-wise.\n        dropna : bool, default True\n            Don't include NaN in the counts.\n\n        Returns\n        -------\n        Series\n\n        See Also\n        --------\n        Series.nunique: Method nunique for Series.\n        DataFrame.count: Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [4, 5, 6], 'B': [4, 1, 1]})\n        >>> df.nunique()\n        A    3\n        B    2\n        dtype: int64\n\n        >>> df.nunique(axis=1)\n        0    1\n        1    2\n        2    2\n        dtype: int64\n        \"\"\"\n        return self.apply(Series.nunique, axis=axis, dropna=dropna)\n\n    def idxmin(self, axis: Axis = 0, skipna: bool = True) -> Series:\n        \"\"\"\n        Return index of first occurrence of minimum over requested axis.\n\n        NA/null values are excluded.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n\n        Returns\n        -------\n        Series\n            Indexes of minima along the specified axis.\n\n        Raises\n        ------\n        ValueError\n            * If the row/column is empty\n\n        See Also\n        --------\n        Series.idxmin : Return index of the minimum element.\n\n        Notes\n        -----\n        This method is the DataFrame version of ``ndarray.argmin``.\n\n        Examples\n        --------\n        Consider a dataset containing food consumption in Argentina.\n\n        >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n        ...                    'co2_emissions': [37.2, 19.66, 1712]},\n        ...                    index=['Pork', 'Wheat Products', 'Beef'])\n\n        >>> df\n                        consumption  co2_emissions\n        Pork                  10.51         37.20\n        Wheat Products       103.11         19.66\n        Beef                  55.48       1712.00\n\n        By default, it returns the index for the minimum value in each column.\n\n        >>> df.idxmin()\n        consumption                Pork\n        co2_emissions    Wheat Products\n        dtype: object\n\n        To return the index for the minimum value in each row, use ``axis=\"columns\"``.\n\n        >>> df.idxmin(axis=\"columns\")\n        Pork                consumption\n        Wheat Products    co2_emissions\n        Beef                consumption\n        dtype: object\n        \"\"\"\n        axis = self._get_axis_number(axis)\n\n        res = self._reduce(\n            nanops.nanargmin, \"argmin\", axis=axis, skipna=skipna, numeric_only=False\n        )\n        indices = res._values\n\n        # indices will always be np.ndarray since axis is not None and\n        # values is a 2d array for DataFrame\n        # error: Item \"int\" of \"Union[int, Any]\" has no attribute \"__iter__\"\n        assert isinstance(indices, np.ndarray)  # for mypy\n\n        index = self._get_axis(axis)\n        result = [index[i] if i >= 0 else np.nan for i in indices]\n        return self._constructor_sliced(result, index=self._get_agg_axis(axis))\n\n    def idxmax(self, axis: Axis = 0, skipna: bool = True) -> Series:\n        \"\"\"\n        Return index of first occurrence of maximum over requested axis.\n\n        NA/null values are excluded.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is NA, the result\n            will be NA.\n\n        Returns\n        -------\n        Series\n            Indexes of maxima along the specified axis.\n\n        Raises\n        ------\n        ValueError\n            * If the row/column is empty\n\n        See Also\n        --------\n        Series.idxmax : Return index of the maximum element.\n\n        Notes\n        -----\n        This method is the DataFrame version of ``ndarray.argmax``.\n\n        Examples\n        --------\n        Consider a dataset containing food consumption in Argentina.\n\n        >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n        ...                    'co2_emissions': [37.2, 19.66, 1712]},\n        ...                    index=['Pork', 'Wheat Products', 'Beef'])\n\n        >>> df\n                        consumption  co2_emissions\n        Pork                  10.51         37.20\n        Wheat Products       103.11         19.66\n        Beef                  55.48       1712.00\n\n        By default, it returns the index for the maximum value in each column.\n\n        >>> df.idxmax()\n        consumption     Wheat Products\n        co2_emissions             Beef\n        dtype: object\n\n        To return the index for the maximum value in each row, use ``axis=\"columns\"``.\n\n        >>> df.idxmax(axis=\"columns\")\n        Pork              co2_emissions\n        Wheat Products     consumption\n        Beef              co2_emissions\n        dtype: object\n        \"\"\"\n        axis = self._get_axis_number(axis)\n\n        res = self._reduce(\n            nanops.nanargmax, \"argmax\", axis=axis, skipna=skipna, numeric_only=False\n        )\n        indices = res._values\n\n        # indices will always be np.ndarray since axis is not None and\n        # values is a 2d array for DataFrame\n        # error: Item \"int\" of \"Union[int, Any]\" has no attribute \"__iter__\"\n        assert isinstance(indices, np.ndarray)  # for mypy\n\n        index = self._get_axis(axis)\n        result = [index[i] if i >= 0 else np.nan for i in indices]\n        return self._constructor_sliced(result, index=self._get_agg_axis(axis))\n\n    def _get_agg_axis(self, axis_num: int) -> Index:\n        \"\"\"\n        Let's be explicit about this.\n        \"\"\"\n        if axis_num == 0:\n            return self.columns\n        elif axis_num == 1:\n            return self.index\n        else:\n            raise ValueError(f\"Axis must be 0 or 1 (got {repr(axis_num)})\")\n\n    def mode(\n        self, axis: Axis = 0, numeric_only: bool = False, dropna: bool = True\n    ) -> DataFrame:\n        \"\"\"\n        Get the mode(s) of each element along the selected axis.\n\n        The mode of a set of values is the value that appears most often.\n        It can be multiple values.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to iterate over while searching for the mode:\n\n            * 0 or 'index' : get mode of each column\n            * 1 or 'columns' : get mode of each row.\n\n        numeric_only : bool, default False\n            If True, only apply to numeric columns.\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n        Returns\n        -------\n        DataFrame\n            The modes of each column or row.\n\n        See Also\n        --------\n        Series.mode : Return the highest frequency value in a Series.\n        Series.value_counts : Return the counts of values in a Series.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([('bird', 2, 2),\n        ...                    ('mammal', 4, np.nan),\n        ...                    ('arthropod', 8, 0),\n        ...                    ('bird', 2, np.nan)],\n        ...                   index=('falcon', 'horse', 'spider', 'ostrich'),\n        ...                   columns=('species', 'legs', 'wings'))\n        >>> df\n                   species  legs  wings\n        falcon        bird     2    2.0\n        horse       mammal     4    NaN\n        spider   arthropod     8    0.0\n        ostrich       bird     2    NaN\n\n        By default, missing values are not considered, and the mode of wings\n        are both 0 and 2. Because the resulting DataFrame has two rows,\n        the second row of ``species`` and ``legs`` contains ``NaN``.\n\n        >>> df.mode()\n          species  legs  wings\n        0    bird   2.0    0.0\n        1     NaN   NaN    2.0\n\n        Setting ``dropna=False`` ``NaN`` values are considered and they can be\n        the mode (like for wings).\n\n        >>> df.mode(dropna=False)\n          species  legs  wings\n        0    bird     2    NaN\n\n        Setting ``numeric_only=True``, only the mode of numeric columns is\n        computed, and columns of other types are ignored.\n\n        >>> df.mode(numeric_only=True)\n           legs  wings\n        0   2.0    0.0\n        1   NaN    2.0\n\n        To compute the mode over columns and not rows, use the axis parameter:\n\n        >>> df.mode(axis='columns', numeric_only=True)\n                   0    1\n        falcon   2.0  NaN\n        horse    4.0  NaN\n        spider   0.0  8.0\n        ostrich  2.0  NaN\n        \"\"\"\n        data = self if not numeric_only else self._get_numeric_data()\n\n        def f(s):\n            return s.mode(dropna=dropna)\n\n        data = data.apply(f, axis=axis)\n        # Ensure index is type stable (should always use int index)\n        if data.empty:\n            data.index = default_index(0)\n\n        return data\n\n    def quantile(\n        self,\n        q=0.5,\n        axis: Axis = 0,\n        numeric_only: bool = True,\n        interpolation: str = \"linear\",\n    ):\n        \"\"\"\n        Return values at the given quantile over requested axis.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            Value between 0 <= q <= 1, the quantile(s) to compute.\n        axis : {0, 1, 'index', 'columns'}, default 0\n            Equals 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n        numeric_only : bool, default True\n            If False, the quantile of datetime and timedelta data will be\n            computed as well.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            This optional parameter specifies the interpolation method to use,\n            when the desired quantile lies between two data points `i` and `j`:\n\n            * linear: `i + (j - i) * fraction`, where `fraction` is the\n              fractional part of the index surrounded by `i` and `j`.\n            * lower: `i`.\n            * higher: `j`.\n            * nearest: `i` or `j` whichever is nearest.\n            * midpoint: (`i` + `j`) / 2.\n\n        Returns\n        -------\n        Series or DataFrame\n\n            If ``q`` is an array, a DataFrame will be returned where the\n              index is ``q``, the columns are the columns of self, and the\n              values are the quantiles.\n            If ``q`` is a float, a Series will be returned where the\n              index is the columns of self and the values are the quantiles.\n\n        See Also\n        --------\n        core.window.Rolling.quantile: Rolling quantile.\n        numpy.percentile: Numpy function to compute the percentile.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]),\n        ...                   columns=['a', 'b'])\n        >>> df.quantile(.1)\n        a    1.3\n        b    3.7\n        Name: 0.1, dtype: float64\n        >>> df.quantile([.1, .5])\n               a     b\n        0.1  1.3   3.7\n        0.5  2.5  55.0\n\n        Specifying `numeric_only=False` will also compute the quantile of\n        datetime and timedelta data.\n\n        >>> df = pd.DataFrame({'A': [1, 2],\n        ...                    'B': [pd.Timestamp('2010'),\n        ...                          pd.Timestamp('2011')],\n        ...                    'C': [pd.Timedelta('1 days'),\n        ...                          pd.Timedelta('2 days')]})\n        >>> df.quantile(0.5, numeric_only=False)\n        A                    1.5\n        B    2010-07-02 12:00:00\n        C        1 days 12:00:00\n        Name: 0.5, dtype: object\n        \"\"\"\n        validate_percentile(q)\n\n        if not is_list_like(q):\n            # BlockManager.quantile expects listlike, so we wrap and unwrap here\n            res = self.quantile(\n                [q], axis=axis, numeric_only=numeric_only, interpolation=interpolation\n            )\n            return res.iloc[0]\n\n        q = Index(q, dtype=np.float64)\n        data = self._get_numeric_data() if numeric_only else self\n        axis = self._get_axis_number(axis)\n\n        if axis == 1:\n            data = data.T\n\n        if len(data.columns) == 0:\n            # GH#23925 _get_numeric_data may have dropped all columns\n            cols = Index([], name=self.columns.name)\n            if is_list_like(q):\n                return self._constructor([], index=q, columns=cols)\n            return self._constructor_sliced([], index=cols, name=q, dtype=np.float64)\n\n        res = data._mgr.quantile(qs=q, axis=1, interpolation=interpolation)\n\n        result = self._constructor(res)\n        return result\n\n    @doc(NDFrame.asfreq, **_shared_doc_kwargs)\n    def asfreq(\n        self,\n        freq: Frequency,\n        method=None,\n        how: str | None = None,\n        normalize: bool = False,\n        fill_value=None,\n    ) -> DataFrame:\n        return super().asfreq(\n            freq=freq,\n            method=method,\n            how=how,\n            normalize=normalize,\n            fill_value=fill_value,\n        )\n\n    @doc(NDFrame.resample, **_shared_doc_kwargs)\n    def resample(\n        self,\n        rule,\n        axis=0,\n        closed: str | None = None,\n        label: str | None = None,\n        convention: str = \"start\",\n        kind: str | None = None,\n        loffset=None,\n        base: int | None = None,\n        on=None,\n        level=None,\n        origin: str | TimestampConvertibleTypes = \"start_day\",\n        offset: TimedeltaConvertibleTypes | None = None,\n    ) -> Resampler:\n        return super().resample(\n            rule=rule,\n            axis=axis,\n            closed=closed,\n            label=label,\n            convention=convention,\n            kind=kind,\n            loffset=loffset,\n            base=base,\n            on=on,\n            level=level,\n            origin=origin,\n            offset=offset,\n        )\n\n    def to_timestamp(\n        self,\n        freq: Frequency | None = None,\n        how: str = \"start\",\n        axis: Axis = 0,\n        copy: bool = True,\n    ) -> DataFrame:\n        \"\"\"\n        Cast to DatetimeIndex of timestamps, at *beginning* of period.\n\n        Parameters\n        ----------\n        freq : str, default frequency of PeriodIndex\n            Desired frequency.\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to convert (the index by default).\n        copy : bool, default True\n            If False then underlying input data is not copied.\n\n        Returns\n        -------\n        DataFrame with DatetimeIndex\n        \"\"\"\n        new_obj = self.copy(deep=copy)\n\n        axis_name = self._get_axis_name(axis)\n        old_ax = getattr(self, axis_name)\n        if not isinstance(old_ax, PeriodIndex):\n            raise TypeError(f\"unsupported Type {type(old_ax).__name__}\")\n\n        new_ax = old_ax.to_timestamp(freq=freq, how=how)\n\n        setattr(new_obj, axis_name, new_ax)\n        return new_obj\n\n    def to_period(\n        self, freq: Frequency | None = None, axis: Axis = 0, copy: bool = True\n    ) -> DataFrame:\n        \"\"\"\n        Convert DataFrame from DatetimeIndex to PeriodIndex.\n\n        Convert DataFrame from DatetimeIndex to PeriodIndex with desired\n        frequency (inferred from index if not passed).\n\n        Parameters\n        ----------\n        freq : str, default\n            Frequency of the PeriodIndex.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to convert (the index by default).\n        copy : bool, default True\n            If False then underlying input data is not copied.\n\n        Returns\n        -------\n        DataFrame with PeriodIndex\n\n        Examples\n        --------\n        >>> idx = pd.to_datetime(\n        ...     [\n        ...         \"2001-03-31 00:00:00\",\n        ...         \"2002-05-31 00:00:00\",\n        ...         \"2003-08-31 00:00:00\",\n        ...     ]\n        ... )\n\n        >>> idx\n        DatetimeIndex(['2001-03-31', '2002-05-31', '2003-08-31'],\n        dtype='datetime64[ns]', freq=None)\n\n        >>> idx.to_period(\"M\")\n        PeriodIndex(['2001-03', '2002-05', '2003-08'], dtype='period[M]')\n\n        For the yearly frequency\n\n        >>> idx.to_period(\"Y\")\n        PeriodIndex(['2001', '2002', '2003'], dtype='period[A-DEC]')\n        \"\"\"\n        new_obj = self.copy(deep=copy)\n\n        axis_name = self._get_axis_name(axis)\n        old_ax = getattr(self, axis_name)\n        if not isinstance(old_ax, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(old_ax).__name__}\")\n\n        new_ax = old_ax.to_period(freq=freq)\n\n        setattr(new_obj, axis_name, new_ax)\n        return new_obj\n\n    def isin(self, values) -> DataFrame:\n        \"\"\"\n        Whether each element in the DataFrame is contained in values.\n\n        Parameters\n        ----------\n        values : iterable, Series, DataFrame or dict\n            The result will only be true at a location if all the\n            labels match. If `values` is a Series, that's the index. If\n            `values` is a dict, the keys must be the column names,\n            which must match. If `values` is a DataFrame,\n            then both the index and column labels must match.\n\n        Returns\n        -------\n        DataFrame\n            DataFrame of booleans showing whether each element in the DataFrame\n            is contained in values.\n\n        See Also\n        --------\n        DataFrame.eq: Equality test for DataFrame.\n        Series.isin: Equivalent method on Series.\n        Series.str.contains: Test if pattern or regex is contained within a\n            string of a Series or Index.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]},\n        ...                   index=['falcon', 'dog'])\n        >>> df\n                num_legs  num_wings\n        falcon         2          2\n        dog            4          0\n\n        When ``values`` is a list check whether every value in the DataFrame\n        is present in the list (which animals have 0 or 2 legs or wings)\n\n        >>> df.isin([0, 2])\n                num_legs  num_wings\n        falcon      True       True\n        dog        False       True\n\n        To check if ``values`` is *not* in the DataFrame, use the ``~`` operator:\n\n        >>> ~df.isin([0, 2])\n                num_legs  num_wings\n        falcon     False      False\n        dog         True      False\n\n        When ``values`` is a dict, we can pass values to check for each\n        column separately:\n\n        >>> df.isin({'num_wings': [0, 3]})\n                num_legs  num_wings\n        falcon     False      False\n        dog        False       True\n\n        When ``values`` is a Series or DataFrame the index and column must\n        match. Note that 'falcon' does not match based on the number of legs\n        in other.\n\n        >>> other = pd.DataFrame({'num_legs': [8, 3], 'num_wings': [0, 2]},\n        ...                      index=['spider', 'falcon'])\n        >>> df.isin(other)\n                num_legs  num_wings\n        falcon     False       True\n        dog        False      False\n        \"\"\"\n        if isinstance(values, dict):\n            from pandas.core.reshape.concat import concat\n\n            values = collections.defaultdict(list, values)\n            return concat(\n                (\n                    self.iloc[:, [i]].isin(values[col])\n                    for i, col in enumerate(self.columns)\n                ),\n                axis=1,\n            )\n        elif isinstance(values, Series):\n            if not values.index.is_unique:\n                raise ValueError(\"cannot compute isin with a duplicate axis.\")\n            return self.eq(values.reindex_like(self), axis=\"index\")\n        elif isinstance(values, DataFrame):\n            if not (values.columns.is_unique and values.index.is_unique):\n                raise ValueError(\"cannot compute isin with a duplicate axis.\")\n            return self.eq(values.reindex_like(self))\n        else:\n            if not is_list_like(values):\n                raise TypeError(\n                    \"only list-like or dict-like objects are allowed \"\n                    \"to be passed to DataFrame.isin(), \"\n                    f\"you passed a '{type(values).__name__}'\"\n                )\n            return self._constructor(\n                algorithms.isin(self.values.ravel(), values).reshape(self.shape),\n                self.index,\n                self.columns,\n            )\n\n    # ----------------------------------------------------------------------\n    # Add index and columns\n    _AXIS_ORDERS = [\"index\", \"columns\"]\n    _AXIS_TO_AXIS_NUMBER: dict[Axis, int] = {\n        **NDFrame._AXIS_TO_AXIS_NUMBER,\n        1: 1,\n        \"columns\": 1,\n    }\n    _AXIS_LEN = len(_AXIS_ORDERS)\n    _info_axis_number = 1\n    _info_axis_name = \"columns\"\n\n    index: Index = properties.AxisProperty(\n        axis=1, doc=\"The index (row labels) of the DataFrame.\"\n    )\n    columns: Index = properties.AxisProperty(\n        axis=0, doc=\"The column labels of the DataFrame.\"\n    )\n\n    @property\n    def _AXIS_NUMBERS(self) -> dict[str, int]:\n        \"\"\".. deprecated:: 1.1.0\"\"\"\n        super()._AXIS_NUMBERS\n        return {\"index\": 0, \"columns\": 1}\n\n    @property\n    def _AXIS_NAMES(self) -> dict[int, str]:\n        \"\"\".. deprecated:: 1.1.0\"\"\"\n        super()._AXIS_NAMES\n        return {0: \"index\", 1: \"columns\"}\n\n    # ----------------------------------------------------------------------\n    # Add plotting methods to DataFrame\n    plot = CachedAccessor(\"plot\", pandas.plotting.PlotAccessor)\n    hist = pandas.plotting.hist_frame\n    boxplot = pandas.plotting.boxplot_frame\n    sparse = CachedAccessor(\"sparse\", SparseFrameAccessor)\n\n    # ----------------------------------------------------------------------\n    # Internal Interface Methods\n\n    def _to_dict_of_blocks(self, copy: bool = True):\n        \"\"\"\n        Return a dict of dtype -> Constructor Types that\n        each is a homogeneous dtype.\n\n        Internal ONLY - only works for BlockManager\n        \"\"\"\n        mgr = self._mgr\n        # convert to BlockManager if needed -> this way support ArrayManager as well\n        mgr = mgr_to_mgr(mgr, \"block\")\n        mgr = cast(BlockManager, mgr)\n        return {\n            k: self._constructor(v).__finalize__(self)\n            for k, v, in mgr.to_dict(copy=copy).items()\n        }\n\n    @property\n    def values(self) -> np.ndarray:\n        \"\"\"\n        Return a Numpy representation of the DataFrame.\n\n        .. warning::\n\n           We recommend using :meth:`DataFrame.to_numpy` instead.\n\n        Only the values in the DataFrame will be returned, the axes labels\n        will be removed.\n\n        Returns\n        -------\n        numpy.ndarray\n            The values of the DataFrame.\n\n        See Also\n        --------\n        DataFrame.to_numpy : Recommended alternative to this method.\n        DataFrame.index : Retrieve the index labels.\n        DataFrame.columns : Retrieving the column names.\n\n        Notes\n        -----\n        The dtype will be a lower-common-denominator dtype (implicit\n        upcasting); that is to say if the dtypes (even of numeric types)\n        are mixed, the one that accommodates all will be chosen. Use this\n        with care if you are not dealing with the blocks.\n\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\n        float32.  If dtypes are int32 and uint8, dtype will be upcast to\n        int32. By :func:`numpy.find_common_type` convention, mixing int64\n        and uint64 will result in a float64 dtype.\n\n        Examples\n        --------\n        A DataFrame where all columns are the same type (e.g., int64) results\n        in an array of the same type.\n\n        >>> df = pd.DataFrame({'age':    [ 3,  29],\n        ...                    'height': [94, 170],\n        ...                    'weight': [31, 115]})\n        >>> df\n           age  height  weight\n        0    3      94      31\n        1   29     170     115\n        >>> df.dtypes\n        age       int64\n        height    int64\n        weight    int64\n        dtype: object\n        >>> df.values\n        array([[  3,  94,  31],\n               [ 29, 170, 115]])\n\n        A DataFrame with mixed type columns(e.g., str/object, int64, float32)\n        results in an ndarray of the broadest type that accommodates these\n        mixed types (e.g., object).\n\n        >>> df2 = pd.DataFrame([('parrot',   24.0, 'second'),\n        ...                     ('lion',     80.5, 1),\n        ...                     ('monkey', np.nan, None)],\n        ...                   columns=('name', 'max_speed', 'rank'))\n        >>> df2.dtypes\n        name          object\n        max_speed    float64\n        rank          object\n        dtype: object\n        >>> df2.values\n        array([['parrot', 24.0, 'second'],\n               ['lion', 80.5, 1],\n               ['monkey', nan, None]], dtype=object)\n        \"\"\"\n        self._consolidate_inplace()\n        return self._mgr.as_array()\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def ffill(\n        self: DataFrame,\n        axis: None | Axis = None,\n        inplace: bool = False,\n        limit: None | int = None,\n        downcast=None,\n    ) -> DataFrame | None:\n        return super().ffill(axis, inplace, limit, downcast)\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def bfill(\n        self: DataFrame,\n        axis: None | Axis = None,\n        inplace: bool = False,\n        limit: None | int = None,\n        downcast=None,\n    ) -> DataFrame | None:\n        return super().bfill(axis, inplace, limit, downcast)\n\n    @deprecate_nonkeyword_arguments(\n        version=None, allowed_args=[\"self\", \"lower\", \"upper\"]\n    )\n    def clip(\n        self: DataFrame,\n        lower=None,\n        upper=None,\n        axis: Axis | None = None,\n        inplace: bool = False,\n        *args,\n        **kwargs,\n    ) -> DataFrame | None:\n        return super().clip(lower, upper, axis, inplace, *args, **kwargs)\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"method\"])\n    def interpolate(\n        self: DataFrame,\n        method: str = \"linear\",\n        axis: Axis = 0,\n        limit: int | None = None,\n        inplace: bool = False,\n        limit_direction: str | None = None,\n        limit_area: str | None = None,\n        downcast: str | None = None,\n        **kwargs,\n    ) -> DataFrame | None:\n        return super().interpolate(\n            method,\n            axis,\n            limit,\n            inplace,\n            limit_direction,\n            limit_area,\n            downcast,\n            **kwargs,\n        )\n\n    @deprecate_nonkeyword_arguments(\n        version=None, allowed_args=[\"self\", \"cond\", \"other\"]\n    )\n    def where(\n        self,\n        cond,\n        other=np.nan,\n        inplace=False,\n        axis=None,\n        level=None,\n        errors=lib.no_default,\n        try_cast=lib.no_default,\n    ):\n        return super().where(cond, other, inplace, axis, level, errors, try_cast)\n\n    @deprecate_nonkeyword_arguments(\n        version=None, allowed_args=[\"self\", \"cond\", \"other\"]\n    )\n    def mask(\n        self,\n        cond,\n        other=np.nan,\n        inplace=False,\n        axis=None,\n        level=None,\n        errors=lib.no_default,\n        try_cast=lib.no_default,\n    ):\n        return super().mask(cond, other, inplace, axis, level, errors, try_cast)\n\n\nDataFrame._add_numeric_operations()\n\nops.add_flex_arithmetic_methods(DataFrame)\n\n\ndef _from_nested_dict(data) -> collections.defaultdict:\n    new_data: collections.defaultdict = collections.defaultdict(dict)\n    for index, s in data.items():\n        for col, v in s.items():\n            new_data[col][index] = v\n    return new_data\n\n\ndef _reindex_for_setitem(value: DataFrame | Series, index: Index) -> ArrayLike:\n    # reindex if necessary\n\n    if value.index.equals(index) or not len(index):\n        return value._values.copy()\n\n    # GH#4107\n    try:\n        reindexed_value = value.reindex(index)._values\n    except ValueError as err:\n        # raised in MultiIndex.from_tuples, see test_insert_error_msmgs\n        if not value.index.is_unique:\n            # duplicate axis\n            raise err\n\n        raise TypeError(\n            \"incompatible index of inserted column with frame index\"\n        ) from err\n    return reindexed_value\n"
    },
    {
      "filename": "pandas/core/groupby/groupby.py",
      "content": "\"\"\"\nProvide the groupby split-apply-combine paradigm. Define the GroupBy\nclass providing the base-class of operations.\n\nThe SeriesGroupBy and DataFrameGroupBy sub-class\n(defined in pandas.core.groupby.generic)\nexpose these user-facing objects to provide specific functionality.\n\"\"\"\nfrom __future__ import annotations\n\nfrom contextlib import contextmanager\nimport datetime\nfrom functools import (\n    partial,\n    wraps,\n)\nimport inspect\nfrom textwrap import dedent\nimport types\nfrom typing import (\n    Callable,\n    Hashable,\n    Iterable,\n    Iterator,\n    List,\n    Literal,\n    Mapping,\n    Sequence,\n    TypeVar,\n    Union,\n    cast,\n    final,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config.config import option_context\n\nfrom pandas._libs import (\n    Timestamp,\n    lib,\n)\nimport pandas._libs.groupby as libgroupby\nfrom pandas._typing import (\n    ArrayLike,\n    IndexLabel,\n    NDFrameT,\n    PositionalIndexer,\n    RandomState,\n    Scalar,\n    T,\n    npt,\n)\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import (\n    Appender,\n    Substitution,\n    cache_readonly,\n    doc,\n)\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.core.dtypes.common import (\n    is_bool_dtype,\n    is_datetime64_dtype,\n    is_float_dtype,\n    is_integer,\n    is_integer_dtype,\n    is_numeric_dtype,\n    is_object_dtype,\n    is_scalar,\n    is_timedelta64_dtype,\n)\nfrom pandas.core.dtypes.missing import (\n    isna,\n    notna,\n)\n\nfrom pandas.core import nanops\nfrom pandas.core._numba import executor\nimport pandas.core.algorithms as algorithms\nfrom pandas.core.arrays import (\n    BaseMaskedArray,\n    BooleanArray,\n    Categorical,\n    ExtensionArray,\n)\nfrom pandas.core.base import (\n    DataError,\n    PandasObject,\n    SelectionMixin,\n)\nimport pandas.core.common as com\nfrom pandas.core.frame import DataFrame\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.groupby import (\n    base,\n    numba_,\n    ops,\n)\nfrom pandas.core.groupby.indexing import GroupByIndexingMixin\nfrom pandas.core.indexes.api import (\n    CategoricalIndex,\n    Index,\n    MultiIndex,\n)\nfrom pandas.core.internals.blocks import ensure_block_shape\nimport pandas.core.sample as sample\nfrom pandas.core.series import Series\nfrom pandas.core.sorting import get_group_index_sorter\nfrom pandas.core.util.numba_ import (\n    NUMBA_FUNC_CACHE,\n    maybe_use_numba,\n)\n\n_common_see_also = \"\"\"\n        See Also\n        --------\n        Series.%(name)s : Apply a function %(name)s to a Series.\n        DataFrame.%(name)s : Apply a function %(name)s\n            to each row or column of a DataFrame.\n\"\"\"\n\n_apply_docs = {\n    \"template\": \"\"\"\n    Apply function ``func`` group-wise and combine the results together.\n\n    The function passed to ``apply`` must take a {input} as its first\n    argument and return a DataFrame, Series or scalar. ``apply`` will\n    then take care of combining the results back together into a single\n    dataframe or series. ``apply`` is therefore a highly flexible\n    grouping method.\n\n    While ``apply`` is a very flexible method, its downside is that\n    using it can be quite a bit slower than using more specific methods\n    like ``agg`` or ``transform``. Pandas offers a wide range of method that will\n    be much faster than using ``apply`` for their specific purposes, so try to\n    use them before reaching for ``apply``.\n\n    Parameters\n    ----------\n    func : callable\n        A callable that takes a {input} as its first argument, and\n        returns a dataframe, a series or a scalar. In addition the\n        callable may take positional and keyword arguments.\n    args, kwargs : tuple and dict\n        Optional positional and keyword arguments to pass to ``func``.\n\n    Returns\n    -------\n    applied : Series or DataFrame\n\n    See Also\n    --------\n    pipe : Apply function to the full GroupBy object instead of to each\n        group.\n    aggregate : Apply aggregate function to the GroupBy object.\n    transform : Apply function column-by-column to the GroupBy object.\n    Series.apply : Apply a function to a Series.\n    DataFrame.apply : Apply a function to each row or column of a DataFrame.\n\n    Notes\n    -----\n\n    .. versionchanged:: 1.3.0\n\n        The resulting dtype will reflect the return value of the passed ``func``,\n        see the examples below.\n\n    Functions that mutate the passed object can produce unexpected\n    behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n    for more details.\n\n    Examples\n    --------\n    {examples}\n    \"\"\",\n    \"dataframe_examples\": \"\"\"\n    >>> df = pd.DataFrame({'A': 'a a b'.split(),\n    ...                    'B': [1,2,3],\n    ...                    'C': [4,6,5]})\n    >>> g = df.groupby('A')\n\n    Notice that ``g`` has two groups, ``a`` and ``b``.\n    Calling `apply` in various ways, we can get different grouping results:\n\n    Example 1: below the function passed to `apply` takes a DataFrame as\n    its argument and returns a DataFrame. `apply` combines the result for\n    each group together into a new DataFrame:\n\n    >>> g[['B', 'C']].apply(lambda x: x / x.sum())\n              B    C\n    0  0.333333  0.4\n    1  0.666667  0.6\n    2  1.000000  1.0\n\n    Example 2: The function passed to `apply` takes a DataFrame as\n    its argument and returns a Series.  `apply` combines the result for\n    each group together into a new DataFrame.\n\n    .. versionchanged:: 1.3.0\n\n        The resulting dtype will reflect the return value of the passed ``func``.\n\n    >>> g[['B', 'C']].apply(lambda x: x.astype(float).max() - x.min())\n         B    C\n    A\n    a  1.0  2.0\n    b  0.0  0.0\n\n    Example 3: The function passed to `apply` takes a DataFrame as\n    its argument and returns a scalar. `apply` combines the result for\n    each group together into a Series, including setting the index as\n    appropriate:\n\n    >>> g.apply(lambda x: x.C.max() - x.B.min())\n    A\n    a    5\n    b    2\n    dtype: int64\"\"\",\n    \"series_examples\": \"\"\"\n    >>> s = pd.Series([0, 1, 2], index='a a b'.split())\n    >>> g = s.groupby(s.index)\n\n    From ``s`` above we can see that ``g`` has two groups, ``a`` and ``b``.\n    Calling `apply` in various ways, we can get different grouping results:\n\n    Example 1: The function passed to `apply` takes a Series as\n    its argument and returns a Series.  `apply` combines the result for\n    each group together into a new Series.\n\n    .. versionchanged:: 1.3.0\n\n        The resulting dtype will reflect the return value of the passed ``func``.\n\n    >>> g.apply(lambda x: x*2 if x.name == 'a' else x/2)\n    a    0.0\n    a    2.0\n    b    1.0\n    dtype: float64\n\n    Example 2: The function passed to `apply` takes a Series as\n    its argument and returns a scalar. `apply` combines the result for\n    each group together into a Series, including setting the index as\n    appropriate:\n\n    >>> g.apply(lambda x: x.max() - x.min())\n    a    1\n    b    0\n    dtype: int64\"\"\",\n}\n\n_groupby_agg_method_template = \"\"\"\nCompute {fname} of group values.\n\nParameters\n----------\nnumeric_only : bool, default {no}\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data.\nmin_count : int, default {mc}\n    The required number of valid values to perform the operation. If fewer\n    than ``min_count`` non-NA values are present the result will be NA.\n\nReturns\n-------\nSeries or DataFrame\n    Computed {fname} of values within each group.\n\"\"\"\n\n_pipe_template = \"\"\"\nApply a function `func` with arguments to this %(klass)s object and return\nthe function's result.\n\nUse `.pipe` when you want to improve readability by chaining together\nfunctions that expect Series, DataFrames, GroupBy or Resampler objects.\nInstead of writing\n\n>>> h(g(f(df.groupby('group')), arg1=a), arg2=b, arg3=c)  # doctest: +SKIP\n\nYou can write\n\n>>> (df.groupby('group')\n...    .pipe(f)\n...    .pipe(g, arg1=a)\n...    .pipe(h, arg2=b, arg3=c))  # doctest: +SKIP\n\nwhich is much more readable.\n\nParameters\n----------\nfunc : callable or tuple of (callable, str)\n    Function to apply to this %(klass)s object or, alternatively,\n    a `(callable, data_keyword)` tuple where `data_keyword` is a\n    string indicating the keyword of `callable` that expects the\n    %(klass)s object.\nargs : iterable, optional\n       Positional arguments passed into `func`.\nkwargs : dict, optional\n         A dictionary of keyword arguments passed into `func`.\n\nReturns\n-------\nobject : the return type of `func`.\n\nSee Also\n--------\nSeries.pipe : Apply a function with arguments to a series.\nDataFrame.pipe: Apply a function with arguments to a dataframe.\napply : Apply function to each group instead of to the\n    full %(klass)s object.\n\nNotes\n-----\nSee more `here\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#piping-function-calls>`_\n\nExamples\n--------\n%(examples)s\n\"\"\"\n\n_transform_template = \"\"\"\nCall function producing a like-indexed %(klass)s on each group and\nreturn a %(klass)s having the same indexes as the original object\nfilled with the transformed values.\n\nParameters\n----------\nf : function\n    Function to apply to each group.\n\n    Can also accept a Numba JIT function with\n    ``engine='numba'`` specified.\n\n    If the ``'numba'`` engine is chosen, the function must be\n    a user defined function with ``values`` and ``index`` as the\n    first and second arguments respectively in the function signature.\n    Each group's index will be passed to the user defined function\n    and optionally available for use.\n\n    .. versionchanged:: 1.1.0\n*args\n    Positional arguments to pass to func.\nengine : str, default None\n    * ``'cython'`` : Runs the function through C-extensions from cython.\n    * ``'numba'`` : Runs the function through JIT compiled code from numba.\n    * ``None`` : Defaults to ``'cython'`` or the global setting ``compute.use_numba``\n\n    .. versionadded:: 1.1.0\nengine_kwargs : dict, default None\n    * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n    * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n      and ``parallel`` dictionary keys. The values must either be ``True`` or\n      ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n      ``{'nopython': True, 'nogil': False, 'parallel': False}`` and will be\n      applied to the function\n\n    .. versionadded:: 1.1.0\n**kwargs\n    Keyword arguments to be passed into func.\n\nReturns\n-------\n%(klass)s\n\nSee Also\n--------\n%(klass)s.groupby.apply : Apply function ``func`` group-wise and combine\n    the results together.\n%(klass)s.groupby.aggregate : Aggregate using one or more\n    operations over the specified axis.\n%(klass)s.transform : Call ``func`` on self producing a %(klass)s with\n    transformed values.\n\nNotes\n-----\nEach group is endowed the attribute 'name' in case you need to know\nwhich group you are working on.\n\nThe current implementation imposes three requirements on f:\n\n* f must return a value that either has the same shape as the input\n  subframe or can be broadcast to the shape of the input subframe.\n  For example, if `f` returns a scalar it will be broadcast to have the\n  same shape as the input subframe.\n* if this is a DataFrame, f must support application column-by-column\n  in the subframe. If f also supports application to the entire subframe,\n  then a fast path is used starting from the second chunk.\n* f must not mutate groups. Mutation is not supported and may\n  produce unexpected results. See :ref:`gotchas.udf-mutation` for more details.\n\nWhen using ``engine='numba'``, there will be no \"fall back\" behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\n.. versionchanged:: 1.3.0\n\n    The resulting dtype will reflect the return value of the passed ``func``,\n    see the examples below.\n\nExamples\n--------\n\n>>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n...                           'foo', 'bar'],\n...                    'B' : ['one', 'one', 'two', 'three',\n...                           'two', 'two'],\n...                    'C' : [1, 5, 5, 2, 5, 5],\n...                    'D' : [2.0, 5., 8., 1., 2., 9.]})\n>>> grouped = df.groupby('A')\n>>> grouped.transform(lambda x: (x - x.mean()) / x.std())\n          C         D\n0 -1.154701 -0.577350\n1  0.577350  0.000000\n2  0.577350  1.154701\n3 -1.154701 -1.000000\n4  0.577350 -0.577350\n5  0.577350  1.000000\n\nBroadcast result of the transformation\n\n>>> grouped.transform(lambda x: x.max() - x.min())\n   C    D\n0  4  6.0\n1  3  8.0\n2  4  6.0\n3  3  8.0\n4  4  6.0\n5  3  8.0\n\n.. versionchanged:: 1.3.0\n\n    The resulting dtype will reflect the return value of the passed ``func``,\n    for example:\n\n>>> grouped[['C', 'D']].transform(lambda x: x.astype(int).max())\n   C  D\n0  5  8\n1  5  9\n2  5  8\n3  5  9\n4  5  8\n5  5  9\n\"\"\"\n\n_agg_template = \"\"\"\nAggregate using one or more operations over the specified axis.\n\nParameters\n----------\nfunc : function, str, list or dict\n    Function to use for aggregating the data. If a function, must either\n    work when passed a {klass} or when passed to {klass}.apply.\n\n    Accepted combinations are:\n\n    - function\n    - string function name\n    - list of functions and/or function names, e.g. ``[np.sum, 'mean']``\n    - dict of axis labels -> functions, function names or list of such.\n\n    Can also accept a Numba JIT function with\n    ``engine='numba'`` specified. Only passing a single function is supported\n    with this engine.\n\n    If the ``'numba'`` engine is chosen, the function must be\n    a user defined function with ``values`` and ``index`` as the\n    first and second arguments respectively in the function signature.\n    Each group's index will be passed to the user defined function\n    and optionally available for use.\n\n    .. versionchanged:: 1.1.0\n*args\n    Positional arguments to pass to func.\nengine : str, default None\n    * ``'cython'`` : Runs the function through C-extensions from cython.\n    * ``'numba'`` : Runs the function through JIT compiled code from numba.\n    * ``None`` : Defaults to ``'cython'`` or globally setting ``compute.use_numba``\n\n    .. versionadded:: 1.1.0\nengine_kwargs : dict, default None\n    * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n    * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n      and ``parallel`` dictionary keys. The values must either be ``True`` or\n      ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n      ``{{'nopython': True, 'nogil': False, 'parallel': False}}`` and will be\n      applied to the function\n\n    .. versionadded:: 1.1.0\n**kwargs\n    Keyword arguments to be passed into func.\n\nReturns\n-------\n{klass}\n\nSee Also\n--------\n{klass}.groupby.apply : Apply function func group-wise\n    and combine the results together.\n{klass}.groupby.transform : Aggregate using one or more\n    operations over the specified axis.\n{klass}.aggregate : Transforms the Series on each group\n    based on the given function.\n\nNotes\n-----\nWhen using ``engine='numba'``, there will be no \"fall back\" behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\nFunctions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\nfor more details.\n\n.. versionchanged:: 1.3.0\n\n    The resulting dtype will reflect the return value of the passed ``func``,\n    see the examples below.\n{examples}\"\"\"\n\n\n@final\nclass GroupByPlot(PandasObject):\n    \"\"\"\n    Class implementing the .plot attribute for groupby objects.\n    \"\"\"\n\n    def __init__(self, groupby: GroupBy):\n        self._groupby = groupby\n\n    def __call__(self, *args, **kwargs):\n        def f(self):\n            return self.plot(*args, **kwargs)\n\n        f.__name__ = \"plot\"\n        return self._groupby.apply(f)\n\n    def __getattr__(self, name: str):\n        def attr(*args, **kwargs):\n            def f(self):\n                return getattr(self.plot, name)(*args, **kwargs)\n\n            return self._groupby.apply(f)\n\n        return attr\n\n\n_KeysArgType = Union[\n    Hashable,\n    List[Hashable],\n    Callable[[Hashable], Hashable],\n    List[Callable[[Hashable], Hashable]],\n    Mapping[Hashable, Hashable],\n]\n\n\nclass BaseGroupBy(PandasObject, SelectionMixin[NDFrameT], GroupByIndexingMixin):\n    _group_selection: IndexLabel | None = None\n    _apply_allowlist: frozenset[str] = frozenset()\n    _hidden_attrs = PandasObject._hidden_attrs | {\n        \"as_index\",\n        \"axis\",\n        \"dropna\",\n        \"exclusions\",\n        \"grouper\",\n        \"group_keys\",\n        \"keys\",\n        \"level\",\n        \"mutated\",\n        \"obj\",\n        \"observed\",\n        \"sort\",\n        \"squeeze\",\n    }\n\n    axis: int\n    grouper: ops.BaseGrouper\n    group_keys: bool\n\n    @final\n    def __len__(self) -> int:\n        return len(self.groups)\n\n    @final\n    def __repr__(self) -> str:\n        # TODO: Better repr for GroupBy object\n        return object.__repr__(self)\n\n    @final\n    @property\n    def groups(self) -> dict[Hashable, np.ndarray]:\n        \"\"\"\n        Dict {group name -> group labels}.\n        \"\"\"\n        return self.grouper.groups\n\n    @final\n    @property\n    def ngroups(self) -> int:\n        return self.grouper.ngroups\n\n    @final\n    @property\n    def indices(self):\n        \"\"\"\n        Dict {group name -> group indices}.\n        \"\"\"\n        return self.grouper.indices\n\n    @final\n    def _get_indices(self, names):\n        \"\"\"\n        Safe get multiple indices, translate keys for\n        datelike to underlying repr.\n        \"\"\"\n\n        def get_converter(s):\n            # possibly convert to the actual key types\n            # in the indices, could be a Timestamp or a np.datetime64\n            if isinstance(s, datetime.datetime):\n                return lambda key: Timestamp(key)\n            elif isinstance(s, np.datetime64):\n                return lambda key: Timestamp(key).asm8\n            else:\n                return lambda key: key\n\n        if len(names) == 0:\n            return []\n\n        if len(self.indices) > 0:\n            index_sample = next(iter(self.indices))\n        else:\n            index_sample = None  # Dummy sample\n\n        name_sample = names[0]\n        if isinstance(index_sample, tuple):\n            if not isinstance(name_sample, tuple):\n                msg = \"must supply a tuple to get_group with multiple grouping keys\"\n                raise ValueError(msg)\n            if not len(name_sample) == len(index_sample):\n                try:\n                    # If the original grouper was a tuple\n                    return [self.indices[name] for name in names]\n                except KeyError as err:\n                    # turns out it wasn't a tuple\n                    msg = (\n                        \"must supply a same-length tuple to get_group \"\n                        \"with multiple grouping keys\"\n                    )\n                    raise ValueError(msg) from err\n\n            converters = [get_converter(s) for s in index_sample]\n            names = (tuple(f(n) for f, n in zip(converters, name)) for name in names)\n\n        else:\n            converter = get_converter(index_sample)\n            names = (converter(name) for name in names)\n\n        return [self.indices.get(name, []) for name in names]\n\n    @final\n    def _get_index(self, name):\n        \"\"\"\n        Safe get index, translate keys for datelike to underlying repr.\n        \"\"\"\n        return self._get_indices([name])[0]\n\n    @final\n    @cache_readonly\n    def _selected_obj(self):\n        # Note: _selected_obj is always just `self.obj` for SeriesGroupBy\n\n        if self._selection is None or isinstance(self.obj, Series):\n            if self._group_selection is not None:\n                return self.obj[self._group_selection]\n            return self.obj\n        else:\n            return self.obj[self._selection]\n\n    @final\n    def _dir_additions(self) -> set[str]:\n        return self.obj._dir_additions() | self._apply_allowlist\n\n    @Substitution(\n        klass=\"GroupBy\",\n        examples=dedent(\n            \"\"\"\\\n        >>> df = pd.DataFrame({'A': 'a b a b'.split(), 'B': [1, 2, 3, 4]})\n        >>> df\n           A  B\n        0  a  1\n        1  b  2\n        2  a  3\n        3  b  4\n\n        To get the difference between each groups maximum and minimum value in one\n        pass, you can do\n\n        >>> df.groupby('A').pipe(lambda x: x.max() - x.min())\n           B\n        A\n        a  2\n        b  2\"\"\"\n        ),\n    )\n    @Appender(_pipe_template)\n    def pipe(\n        self,\n        func: Callable[..., T] | tuple[Callable[..., T], str],\n        *args,\n        **kwargs,\n    ) -> T:\n        return com.pipe(self, func, *args, **kwargs)\n\n    plot = property(GroupByPlot)\n\n    @final\n    def get_group(self, name, obj=None) -> DataFrame | Series:\n        \"\"\"\n        Construct DataFrame from group with provided name.\n\n        Parameters\n        ----------\n        name : object\n            The name of the group to get as a DataFrame.\n        obj : DataFrame, default None\n            The DataFrame to take the DataFrame out of.  If\n            it is None, the object groupby was called on will\n            be used.\n\n        Returns\n        -------\n        group : same type as obj\n        \"\"\"\n        if obj is None:\n            obj = self._selected_obj\n\n        inds = self._get_index(name)\n        if not len(inds):\n            raise KeyError(name)\n\n        return obj._take_with_is_copy(inds, axis=self.axis)\n\n    @final\n    def __iter__(self) -> Iterator[tuple[Hashable, NDFrameT]]:\n        \"\"\"\n        Groupby iterator.\n\n        Returns\n        -------\n        Generator yielding sequence of (name, subsetted object)\n        for each group\n        \"\"\"\n        return self.grouper.get_iterator(self.obj, axis=self.axis)\n\n\n# To track operations that expand dimensions, like ohlc\nOutputFrameOrSeries = TypeVar(\"OutputFrameOrSeries\", bound=NDFrame)\n\n\nclass GroupBy(BaseGroupBy[NDFrameT]):\n    \"\"\"\n    Class for grouping and aggregating relational data.\n\n    See aggregate, transform, and apply functions on this object.\n\n    It's easiest to use obj.groupby(...) to use GroupBy, but you can also do:\n\n    ::\n\n        grouped = groupby(obj, ...)\n\n    Parameters\n    ----------\n    obj : pandas object\n    axis : int, default 0\n    level : int, default None\n        Level of MultiIndex\n    groupings : list of Grouping objects\n        Most users should ignore this\n    exclusions : array-like, optional\n        List of columns to exclude\n    name : str\n        Most users should ignore this\n\n    Returns\n    -------\n    **Attributes**\n    groups : dict\n        {group name -> group labels}\n    len(grouped) : int\n        Number of groups\n\n    Notes\n    -----\n    After grouping, see aggregate, apply, and transform functions. Here are\n    some other brief notes about usage. When grouping by multiple groups, the\n    result index will be a MultiIndex (hierarchical) by default.\n\n    Iteration produces (key, group) tuples, i.e. chunking the data by group. So\n    you can write code like:\n\n    ::\n\n        grouped = obj.groupby(keys, axis=axis)\n        for key, group in grouped:\n            # do something with the data\n\n    Function calls on GroupBy, if not specially implemented, \"dispatch\" to the\n    grouped data. So if you group a DataFrame and wish to invoke the std()\n    method on each group, you can simply do:\n\n    ::\n\n        df.groupby(mapper).std()\n\n    rather than\n\n    ::\n\n        df.groupby(mapper).aggregate(np.std)\n\n    You can pass arguments to these \"wrapped\" functions, too.\n\n    See the online documentation for full exposition on these topics and much\n    more\n    \"\"\"\n\n    grouper: ops.BaseGrouper\n    as_index: bool\n\n    @final\n    def __init__(\n        self,\n        obj: NDFrameT,\n        keys: _KeysArgType | None = None,\n        axis: int = 0,\n        level: IndexLabel | None = None,\n        grouper: ops.BaseGrouper | None = None,\n        exclusions: frozenset[Hashable] | None = None,\n        selection: IndexLabel | None = None,\n        as_index: bool = True,\n        sort: bool = True,\n        group_keys: bool = True,\n        squeeze: bool = False,\n        observed: bool = False,\n        mutated: bool = False,\n        dropna: bool = True,\n    ):\n\n        self._selection = selection\n\n        assert isinstance(obj, NDFrame), type(obj)\n\n        self.level = level\n\n        if not as_index:\n            if not isinstance(obj, DataFrame):\n                raise TypeError(\"as_index=False only valid with DataFrame\")\n            if axis != 0:\n                raise ValueError(\"as_index=False only valid for axis=0\")\n\n        self.as_index = as_index\n        self.keys = keys\n        self.sort = sort\n        self.group_keys = group_keys\n        self.squeeze = squeeze\n        self.observed = observed\n        self.mutated = mutated\n        self.dropna = dropna\n\n        if grouper is None:\n            from pandas.core.groupby.grouper import get_grouper\n\n            grouper, exclusions, obj = get_grouper(\n                obj,\n                keys,\n                axis=axis,\n                level=level,\n                sort=sort,\n                observed=observed,\n                mutated=self.mutated,\n                dropna=self.dropna,\n            )\n\n        self.obj = obj\n        self.axis = obj._get_axis_number(axis)\n        self.grouper = grouper\n        self.exclusions = frozenset(exclusions) if exclusions else frozenset()\n\n    def __getattr__(self, attr: str):\n        if attr in self._internal_names_set:\n            return object.__getattribute__(self, attr)\n        if attr in self.obj:\n            return self[attr]\n\n        raise AttributeError(\n            f\"'{type(self).__name__}' object has no attribute '{attr}'\"\n        )\n\n    @final\n    def _make_wrapper(self, name: str) -> Callable:\n        assert name in self._apply_allowlist\n\n        with self._group_selection_context():\n            # need to setup the selection\n            # as are not passed directly but in the grouper\n            f = getattr(self._obj_with_exclusions, name)\n            if not isinstance(f, types.MethodType):\n                return self.apply(lambda self: getattr(self, name))\n\n        f = getattr(type(self._obj_with_exclusions), name)\n        sig = inspect.signature(f)\n\n        def wrapper(*args, **kwargs):\n            # a little trickery for aggregation functions that need an axis\n            # argument\n            if \"axis\" in sig.parameters:\n                if kwargs.get(\"axis\", None) is None:\n                    kwargs[\"axis\"] = self.axis\n\n            def curried(x):\n                return f(x, *args, **kwargs)\n\n            # preserve the name so we can detect it when calling plot methods,\n            # to avoid duplicates\n            curried.__name__ = name\n\n            # special case otherwise extra plots are created when catching the\n            # exception below\n            if name in base.plotting_methods:\n                return self.apply(curried)\n\n            return self._python_apply_general(curried, self._obj_with_exclusions)\n\n        wrapper.__name__ = name\n        return wrapper\n\n    # -----------------------------------------------------------------\n    # Selection\n\n    @final\n    def _set_group_selection(self) -> None:\n        \"\"\"\n        Create group based selection.\n\n        Used when selection is not passed directly but instead via a grouper.\n\n        NOTE: this should be paired with a call to _reset_group_selection\n        \"\"\"\n        # This is a no-op for SeriesGroupBy\n        grp = self.grouper\n        if not (\n            self.as_index\n            and grp.groupings is not None\n            and self.obj.ndim > 1\n            and self._group_selection is None\n        ):\n            return\n\n        groupers = [g.name for g in grp.groupings if g.level is None and g.in_axis]\n\n        if len(groupers):\n            # GH12839 clear selected obj cache when group selection changes\n            ax = self.obj._info_axis\n            self._group_selection = ax.difference(Index(groupers), sort=False).tolist()\n            self._reset_cache(\"_selected_obj\")\n\n    @final\n    def _reset_group_selection(self) -> None:\n        \"\"\"\n        Clear group based selection.\n\n        Used for methods needing to return info on each group regardless of\n        whether a group selection was previously set.\n        \"\"\"\n        if self._group_selection is not None:\n            # GH12839 clear cached selection too when changing group selection\n            self._group_selection = None\n            self._reset_cache(\"_selected_obj\")\n\n    @contextmanager\n    def _group_selection_context(self) -> Iterator[GroupBy]:\n        \"\"\"\n        Set / reset the _group_selection_context.\n        \"\"\"\n        self._set_group_selection()\n        try:\n            yield self\n        finally:\n            self._reset_group_selection()\n\n    def _iterate_slices(self) -> Iterable[Series]:\n        raise AbstractMethodError(self)\n\n    # -----------------------------------------------------------------\n    # Dispatch/Wrapping\n\n    @final\n    def _concat_objects(self, values, not_indexed_same: bool = False):\n        from pandas.core.reshape.concat import concat\n\n        def reset_identity(values):\n            # reset the identities of the components\n            # of the values to prevent aliasing\n            for v in com.not_none(*values):\n                ax = v._get_axis(self.axis)\n                ax._reset_identity()\n            return values\n\n        if not not_indexed_same:\n            result = concat(values, axis=self.axis)\n\n            ax = self._selected_obj._get_axis(self.axis)\n            if self.dropna:\n                labels = self.grouper.group_info[0]\n                mask = labels != -1\n                ax = ax[mask]\n\n            # this is a very unfortunate situation\n            # we can't use reindex to restore the original order\n            # when the ax has duplicates\n            # so we resort to this\n            # GH 14776, 30667\n            if ax.has_duplicates and not result.axes[self.axis].equals(ax):\n                indexer, _ = result.index.get_indexer_non_unique(ax._values)\n                indexer = algorithms.unique1d(indexer)\n                result = result.take(indexer, axis=self.axis)\n            else:\n                result = result.reindex(ax, axis=self.axis, copy=False)\n\n        elif self.group_keys:\n\n            values = reset_identity(values)\n            if self.as_index:\n\n                # possible MI return case\n                group_keys = self.grouper.result_index\n                group_levels = self.grouper.levels\n                group_names = self.grouper.names\n\n                result = concat(\n                    values,\n                    axis=self.axis,\n                    keys=group_keys,\n                    levels=group_levels,\n                    names=group_names,\n                    sort=False,\n                )\n            else:\n\n                # GH5610, returns a MI, with the first level being a\n                # range index\n                keys = list(range(len(values)))\n                result = concat(values, axis=self.axis, keys=keys)\n        else:\n            values = reset_identity(values)\n            result = concat(values, axis=self.axis)\n\n        name = self.obj.name if self.obj.ndim == 1 else self._selection\n        if isinstance(result, Series) and name is not None:\n\n            result.name = name\n\n        return result\n\n    @final\n    def _set_result_index_ordered(\n        self, result: OutputFrameOrSeries\n    ) -> OutputFrameOrSeries:\n        # set the result index on the passed values object and\n        # return the new object, xref 8046\n\n        if self.grouper.is_monotonic:\n            # shortcut if we have an already ordered grouper\n            result.set_axis(self.obj._get_axis(self.axis), axis=self.axis, inplace=True)\n            return result\n\n        # row order is scrambled => sort the rows by position in original index\n        original_positions = Index(\n            np.concatenate(self._get_indices(self.grouper.result_index))\n        )\n        result.set_axis(original_positions, axis=self.axis, inplace=True)\n        result = result.sort_index(axis=self.axis)\n\n        dropped_rows = len(result.index) < len(self.obj.index)\n\n        if dropped_rows:\n            # get index by slicing original index according to original positions\n            # slice drops attrs => use set_axis when no rows were dropped\n            sorted_indexer = result.index\n            result.index = self._selected_obj.index[sorted_indexer]\n        else:\n            result.set_axis(self.obj._get_axis(self.axis), axis=self.axis, inplace=True)\n\n        return result\n\n    def _indexed_output_to_ndframe(\n        self, result: Mapping[base.OutputKey, ArrayLike]\n    ) -> Series | DataFrame:\n        raise AbstractMethodError(self)\n\n    @final\n    def _wrap_aggregated_output(\n        self,\n        output: Series | DataFrame | Mapping[base.OutputKey, ArrayLike],\n        qs: npt.NDArray[np.float64] | None = None,\n    ):\n        \"\"\"\n        Wraps the output of GroupBy aggregations into the expected result.\n\n        Parameters\n        ----------\n        output : Series, DataFrame, or Mapping[base.OutputKey, ArrayLike]\n           Data to wrap.\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n\n        if isinstance(output, (Series, DataFrame)):\n            # We get here (for DataFrameGroupBy) if we used Manager.grouped_reduce,\n            #  in which case our columns are already set correctly.\n            # ATM we do not get here for SeriesGroupBy; when we do, we will\n            #  need to require that result.name already match self.obj.name\n            result = output\n        else:\n            result = self._indexed_output_to_ndframe(output)\n\n        if not self.as_index:\n            # `not self.as_index` is only relevant for DataFrameGroupBy,\n            #   enforced in __init__\n            self._insert_inaxis_grouper_inplace(result)\n            result = result._consolidate()\n            index = Index(range(self.grouper.ngroups))\n\n        else:\n            index = self.grouper.result_index\n\n        if qs is not None:\n            # We get here with len(qs) != 1 and not self.as_index\n            #  in test_pass_args_kwargs\n            index = _insert_quantile_level(index, qs)\n\n        result.index = index\n\n        if self.axis == 1:\n            # Only relevant for DataFrameGroupBy, no-op for SeriesGroupBy\n            result = result.T\n            if result.index.equals(self.obj.index):\n                # Retain e.g. DatetimeIndex/TimedeltaIndex freq\n                result.index = self.obj.index.copy()\n                # TODO: Do this more systematically\n\n        return self._reindex_output(result, qs=qs)\n\n    @final\n    def _wrap_transformed_output(\n        self, output: Mapping[base.OutputKey, ArrayLike]\n    ) -> Series | DataFrame:\n        \"\"\"\n        Wraps the output of GroupBy transformations into the expected result.\n\n        Parameters\n        ----------\n        output : Mapping[base.OutputKey, ArrayLike]\n            Data to wrap.\n\n        Returns\n        -------\n        Series or DataFrame\n            Series for SeriesGroupBy, DataFrame for DataFrameGroupBy\n        \"\"\"\n        if isinstance(output, (Series, DataFrame)):\n            result = output\n        else:\n            result = self._indexed_output_to_ndframe(output)\n\n        if self.axis == 1:\n            # Only relevant for DataFrameGroupBy\n            result = result.T\n            result.columns = self.obj.columns\n\n        result.index = self.obj.index\n        return result\n\n    def _wrap_applied_output(self, data, values: list, not_indexed_same: bool = False):\n        raise AbstractMethodError(self)\n\n    def _resolve_numeric_only(self, numeric_only: bool | lib.NoDefault) -> bool:\n        \"\"\"\n        Determine subclass-specific default value for 'numeric_only'.\n\n        For SeriesGroupBy we want the default to be False (to match Series behavior).\n        For DataFrameGroupBy we want it to be True (for backwards-compat).\n\n        Parameters\n        ----------\n        numeric_only : bool or lib.no_default\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        # GH#41291\n        if numeric_only is lib.no_default:\n            # i.e. not explicitly passed by user\n            if self.obj.ndim == 2:\n                # i.e. DataFrameGroupBy\n                numeric_only = True\n                # GH#42395 GH#43108 GH#43154\n                # Regression from 1.2.5 to 1.3 caused object columns to be dropped\n                if self.axis:\n                    obj = self._obj_with_exclusions.T\n                else:\n                    obj = self._obj_with_exclusions\n                check = obj._get_numeric_data()\n                if len(obj.columns) and not len(check.columns) and not obj.empty:\n                    numeric_only = False\n                    # TODO: v1.4+ Add FutureWarning\n\n            else:\n                numeric_only = False\n\n        # error: Incompatible return value type (got \"Union[bool, NoDefault]\",\n        # expected \"bool\")\n        return numeric_only  # type: ignore[return-value]\n\n    # -----------------------------------------------------------------\n    # numba\n\n    @final\n    def _numba_prep(self, func, data):\n        if not callable(func):\n            raise NotImplementedError(\n                \"Numba engine can only be used with a single function.\"\n            )\n        ids, _, ngroups = self.grouper.group_info\n        sorted_index = get_group_index_sorter(ids, ngroups)\n        sorted_ids = algorithms.take_nd(ids, sorted_index, allow_fill=False)\n\n        sorted_data = data.take(sorted_index, axis=self.axis).to_numpy()\n        sorted_index_data = data.index.take(sorted_index).to_numpy()\n\n        starts, ends = lib.generate_slices(sorted_ids, ngroups)\n        return (\n            starts,\n            ends,\n            sorted_index_data,\n            sorted_data,\n        )\n\n    def _numba_agg_general(\n        self,\n        func: Callable,\n        engine_kwargs: dict[str, bool] | None,\n        numba_cache_key_str: str,\n    ):\n        \"\"\"\n        Perform groupby with a standard numerical aggregation function (e.g. mean)\n        with Numba.\n        \"\"\"\n        if not self.as_index:\n            raise NotImplementedError(\n                \"as_index=False is not supported. Use .reset_index() instead.\"\n            )\n        if self.axis == 1:\n            raise NotImplementedError(\"axis=1 is not supported.\")\n\n        with self._group_selection_context():\n            data = self._selected_obj\n        df = data if data.ndim == 2 else data.to_frame()\n        starts, ends, sorted_index, sorted_data = self._numba_prep(func, df)\n        aggregator = executor.generate_shared_aggregator(\n            func, engine_kwargs, numba_cache_key_str\n        )\n        result = aggregator(sorted_data, starts, ends, 0)\n\n        cache_key = (func, numba_cache_key_str)\n        if cache_key not in NUMBA_FUNC_CACHE:\n            NUMBA_FUNC_CACHE[cache_key] = aggregator\n\n        index = self.grouper.result_index\n        if data.ndim == 1:\n            result_kwargs = {\"name\": data.name}\n            result = result.ravel()\n        else:\n            result_kwargs = {\"columns\": data.columns}\n        return data._constructor(result, index=index, **result_kwargs)\n\n    @final\n    def _transform_with_numba(self, data, func, *args, engine_kwargs=None, **kwargs):\n        \"\"\"\n        Perform groupby transform routine with the numba engine.\n\n        This routine mimics the data splitting routine of the DataSplitter class\n        to generate the indices of each group in the sorted data and then passes the\n        data and indices into a Numba jitted function.\n        \"\"\"\n        starts, ends, sorted_index, sorted_data = self._numba_prep(func, data)\n\n        numba_transform_func = numba_.generate_numba_transform_func(\n            kwargs, func, engine_kwargs\n        )\n        result = numba_transform_func(\n            sorted_data,\n            sorted_index,\n            starts,\n            ends,\n            len(data.columns),\n            *args,\n        )\n\n        cache_key = (func, \"groupby_transform\")\n        if cache_key not in NUMBA_FUNC_CACHE:\n            NUMBA_FUNC_CACHE[cache_key] = numba_transform_func\n\n        # result values needs to be resorted to their original positions since we\n        # evaluated the data sorted by group\n        return result.take(np.argsort(sorted_index), axis=0)\n\n    @final\n    def _aggregate_with_numba(self, data, func, *args, engine_kwargs=None, **kwargs):\n        \"\"\"\n        Perform groupby aggregation routine with the numba engine.\n\n        This routine mimics the data splitting routine of the DataSplitter class\n        to generate the indices of each group in the sorted data and then passes the\n        data and indices into a Numba jitted function.\n        \"\"\"\n        starts, ends, sorted_index, sorted_data = self._numba_prep(func, data)\n\n        numba_agg_func = numba_.generate_numba_agg_func(kwargs, func, engine_kwargs)\n        result = numba_agg_func(\n            sorted_data,\n            sorted_index,\n            starts,\n            ends,\n            len(data.columns),\n            *args,\n        )\n\n        cache_key = (func, \"groupby_agg\")\n        if cache_key not in NUMBA_FUNC_CACHE:\n            NUMBA_FUNC_CACHE[cache_key] = numba_agg_func\n\n        return result\n\n    # -----------------------------------------------------------------\n    # apply/agg/transform\n\n    @Appender(\n        _apply_docs[\"template\"].format(\n            input=\"dataframe\", examples=_apply_docs[\"dataframe_examples\"]\n        )\n    )\n    def apply(self, func, *args, **kwargs):\n\n        func = com.is_builtin_func(func)\n\n        # this is needed so we don't try and wrap strings. If we could\n        # resolve functions to their callable functions prior, this\n        # wouldn't be needed\n        if args or kwargs:\n            if callable(func):\n\n                @wraps(func)\n                def f(g):\n                    with np.errstate(all=\"ignore\"):\n                        return func(g, *args, **kwargs)\n\n            elif hasattr(nanops, \"nan\" + func):\n                # TODO: should we wrap this in to e.g. _is_builtin_func?\n                f = getattr(nanops, \"nan\" + func)\n\n            else:\n                raise ValueError(\n                    \"func must be a callable if args or kwargs are supplied\"\n                )\n        elif isinstance(func, str):\n            if hasattr(self, func):\n                res = getattr(self, func)\n                if callable(res):\n                    return res()\n                return res\n\n            else:\n                raise TypeError(f\"apply func should be callable, not '{func}'\")\n        else:\n\n            f = func\n\n        # ignore SettingWithCopy here in case the user mutates\n        with option_context(\"mode.chained_assignment\", None):\n            try:\n                result = self._python_apply_general(f, self._selected_obj)\n            except TypeError:\n                # gh-20949\n                # try again, with .apply acting as a filtering\n                # operation, by excluding the grouping column\n                # This would normally not be triggered\n                # except if the udf is trying an operation that\n                # fails on *some* columns, e.g. a numeric operation\n                # on a string grouper column\n\n                with self._group_selection_context():\n                    return self._python_apply_general(f, self._selected_obj)\n\n        return result\n\n    @final\n    def _python_apply_general(\n        self,\n        f: Callable,\n        data: DataFrame | Series,\n        not_indexed_same: bool | None = None,\n    ) -> DataFrame | Series:\n        \"\"\"\n        Apply function f in python space\n\n        Parameters\n        ----------\n        f : callable\n            Function to apply\n        data : Series or DataFrame\n            Data to apply f to\n        not_indexed_same: bool, optional\n            When specified, overrides the value of not_indexed_same. Apply behaves\n            differently when the result index is equal to the input index, but\n            this can be coincidental leading to value-dependent behavior.\n\n        Returns\n        -------\n        Series or DataFrame\n            data after applying f\n        \"\"\"\n        values, mutated = self.grouper.apply(f, data, self.axis)\n\n        if not_indexed_same is None:\n            not_indexed_same = mutated or self.mutated\n\n        return self._wrap_applied_output(\n            data, values, not_indexed_same=not_indexed_same\n        )\n\n    @final\n    def _python_agg_general(self, func, *args, **kwargs):\n        func = com.is_builtin_func(func)\n        f = lambda x: func(x, *args, **kwargs)\n\n        # iterate through \"columns\" ex exclusions to populate output dict\n        output: dict[base.OutputKey, ArrayLike] = {}\n\n        if self.ngroups == 0:\n            # agg_series below assumes ngroups > 0\n            return self._python_apply_general(f, self._selected_obj)\n\n        for idx, obj in enumerate(self._iterate_slices()):\n            name = obj.name\n\n            try:\n                # if this function is invalid for this dtype, we will ignore it.\n                result = self.grouper.agg_series(obj, f)\n            except TypeError:\n                warn_dropping_nuisance_columns_deprecated(type(self), \"agg\")\n                continue\n\n            key = base.OutputKey(label=name, position=idx)\n            output[key] = result\n\n        if not output:\n            return self._python_apply_general(f, self._selected_obj)\n\n        return self._wrap_aggregated_output(output)\n\n    @final\n    def _agg_general(\n        self,\n        numeric_only: bool = True,\n        min_count: int = -1,\n        *,\n        alias: str,\n        npfunc: Callable,\n    ):\n\n        with self._group_selection_context():\n            # try a cython aggregation if we can\n            result = self._cython_agg_general(\n                how=alias,\n                alt=npfunc,\n                numeric_only=numeric_only,\n                min_count=min_count,\n            )\n            return result.__finalize__(self.obj, method=\"groupby\")\n\n    def _agg_py_fallback(\n        self, values: ArrayLike, ndim: int, alt: Callable\n    ) -> ArrayLike:\n        \"\"\"\n        Fallback to pure-python aggregation if _cython_operation raises\n        NotImplementedError.\n        \"\"\"\n        # We get here with a) EADtypes and b) object dtype\n\n        if values.ndim == 1:\n            # For DataFrameGroupBy we only get here with ExtensionArray\n            ser = Series(values)\n        else:\n            # We only get here with values.dtype == object\n            # TODO: special case not needed with ArrayManager\n            df = DataFrame(values.T)\n            # bc we split object blocks in grouped_reduce, we have only 1 col\n            # otherwise we'd have to worry about block-splitting GH#39329\n            assert df.shape[1] == 1\n            # Avoid call to self.values that can occur in DataFrame\n            #  reductions; see GH#28949\n            ser = df.iloc[:, 0]\n\n        # We do not get here with UDFs, so we know that our dtype\n        #  should always be preserved by the implemented aggregations\n        # TODO: Is this exactly right; see WrappedCythonOp get_result_dtype?\n        res_values = self.grouper.agg_series(ser, alt, preserve_dtype=True)\n\n        if isinstance(values, Categorical):\n            # Because we only get here with known dtype-preserving\n            #  reductions, we cast back to Categorical.\n            # TODO: if we ever get \"rank\" working, exclude it here.\n            res_values = type(values)._from_sequence(res_values, dtype=values.dtype)\n\n        # If we are DataFrameGroupBy and went through a SeriesGroupByPath\n        # then we need to reshape\n        # GH#32223 includes case with IntegerArray values, ndarray res_values\n        # test_groupby_duplicate_columns with object dtype values\n        return ensure_block_shape(res_values, ndim=ndim)\n\n    @final\n    def _cython_agg_general(\n        self, how: str, alt: Callable, numeric_only: bool, min_count: int = -1\n    ):\n        # Note: we never get here with how=\"ohlc\" for DataFrameGroupBy;\n        #  that goes through SeriesGroupBy\n\n        data = self._get_data_to_aggregate()\n        is_ser = data.ndim == 1\n\n        if numeric_only:\n            if is_ser and not is_numeric_dtype(self._selected_obj.dtype):\n                # GH#41291 match Series behavior\n                kwd_name = \"numeric_only\"\n                if how in [\"any\", \"all\"]:\n                    kwd_name = \"bool_only\"\n                raise NotImplementedError(\n                    f\"{type(self).__name__}.{how} does not implement {kwd_name}.\"\n                )\n            elif not is_ser:\n                data = data.get_numeric_data(copy=False)\n\n        def array_func(values: ArrayLike) -> ArrayLike:\n            try:\n                result = self.grouper._cython_operation(\n                    \"aggregate\", values, how, axis=data.ndim - 1, min_count=min_count\n                )\n            except NotImplementedError:\n                # generally if we have numeric_only=False\n                # and non-applicable functions\n                # try to python agg\n                # TODO: shouldn't min_count matter?\n                result = self._agg_py_fallback(values, ndim=data.ndim, alt=alt)\n\n            return result\n\n        # TypeError -> we may have an exception in trying to aggregate\n        #  continue and exclude the block\n        new_mgr = data.grouped_reduce(array_func, ignore_failures=True)\n\n        if not is_ser and len(new_mgr) < len(data):\n            warn_dropping_nuisance_columns_deprecated(type(self), how)\n\n        res = self._wrap_agged_manager(new_mgr)\n        if is_ser:\n            res.index = self.grouper.result_index\n            return self._reindex_output(res)\n        else:\n            return res\n\n    def _cython_transform(\n        self, how: str, numeric_only: bool = True, axis: int = 0, **kwargs\n    ):\n        raise AbstractMethodError(self)\n\n    @final\n    def _transform(self, func, *args, engine=None, engine_kwargs=None, **kwargs):\n\n        if maybe_use_numba(engine):\n            # TODO: tests with self._selected_obj.ndim == 1 on DataFrameGroupBy\n            with self._group_selection_context():\n                data = self._selected_obj\n            df = data if data.ndim == 2 else data.to_frame()\n            result = self._transform_with_numba(\n                df, func, *args, engine_kwargs=engine_kwargs, **kwargs\n            )\n            if self.obj.ndim == 2:\n                return cast(DataFrame, self.obj)._constructor(\n                    result, index=data.index, columns=data.columns\n                )\n            else:\n                return cast(Series, self.obj)._constructor(\n                    result.ravel(), index=data.index, name=data.name\n                )\n\n        # optimized transforms\n        func = com.get_cython_func(func) or func\n\n        if not isinstance(func, str):\n            return self._transform_general(func, *args, **kwargs)\n\n        elif func not in base.transform_kernel_allowlist:\n            msg = f\"'{func}' is not a valid function name for transform(name)\"\n            raise ValueError(msg)\n        elif func in base.cythonized_kernels or func in base.transformation_kernels:\n            # cythonized transform or canned \"agg+broadcast\"\n            return getattr(self, func)(*args, **kwargs)\n\n        else:\n            # i.e. func in base.reduction_kernels\n\n            # GH#30918 Use _transform_fast only when we know func is an aggregation\n            # If func is a reduction, we need to broadcast the\n            # result to the whole group. Compute func result\n            # and deal with possible broadcasting below.\n            # Temporarily set observed for dealing with categoricals.\n            with com.temp_setattr(self, \"observed\", True):\n                result = getattr(self, func)(*args, **kwargs)\n\n            if self._can_use_transform_fast(result):\n                return self._wrap_transform_fast_result(result)\n\n            # only reached for DataFrameGroupBy\n            return self._transform_general(func, *args, **kwargs)\n\n    @final\n    def _wrap_transform_fast_result(self, result: NDFrameT) -> NDFrameT:\n        \"\"\"\n        Fast transform path for aggregations.\n        \"\"\"\n        obj = self._obj_with_exclusions\n\n        # for each col, reshape to size of original frame by take operation\n        ids, _, _ = self.grouper.group_info\n        result = result.reindex(self.grouper.result_index, copy=False)\n\n        if self.obj.ndim == 1:\n            # i.e. SeriesGroupBy\n            out = algorithms.take_nd(result._values, ids)\n            output = obj._constructor(out, index=obj.index, name=obj.name)\n        else:\n            output = result.take(ids, axis=0)\n            output.index = obj.index\n        return output\n\n    # -----------------------------------------------------------------\n    # Utilities\n\n    @final\n    def _apply_filter(self, indices, dropna):\n        if len(indices) == 0:\n            indices = np.array([], dtype=\"int64\")\n        else:\n            indices = np.sort(np.concatenate(indices))\n        if dropna:\n            filtered = self._selected_obj.take(indices, axis=self.axis)\n        else:\n            mask = np.empty(len(self._selected_obj.index), dtype=bool)\n            mask.fill(False)\n            mask[indices.astype(int)] = True\n            # mask fails to broadcast when passed to where; broadcast manually.\n            mask = np.tile(mask, list(self._selected_obj.shape[1:]) + [1]).T\n            filtered = self._selected_obj.where(mask)  # Fill with NaNs.\n        return filtered\n\n    @final\n    def _cumcount_array(self, ascending: bool = True) -> np.ndarray:\n        \"\"\"\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Notes\n        -----\n        this is currently implementing sort=False\n        (though the default is sort=True) for groupby in general\n        \"\"\"\n        ids, _, ngroups = self.grouper.group_info\n        sorter = get_group_index_sorter(ids, ngroups)\n        ids, count = ids[sorter], len(ids)\n\n        if count == 0:\n            return np.empty(0, dtype=np.int64)\n\n        run = np.r_[True, ids[:-1] != ids[1:]]\n        rep = np.diff(np.r_[np.nonzero(run)[0], count])\n        out = (~run).cumsum()\n\n        if ascending:\n            out -= np.repeat(out[run], rep)\n        else:\n            out = np.repeat(out[np.r_[run[1:], True]], rep) - out\n\n        rev = np.empty(count, dtype=np.intp)\n        rev[sorter] = np.arange(count, dtype=np.intp)\n        return out[rev].astype(np.int64, copy=False)\n\n    # -----------------------------------------------------------------\n\n    @final\n    @property\n    def _obj_1d_constructor(self) -> type[Series]:\n        # GH28330 preserve subclassed Series/DataFrames\n        if isinstance(self.obj, DataFrame):\n            return self.obj._constructor_sliced\n        assert isinstance(self.obj, Series)\n        return self.obj._constructor\n\n    @final\n    def _bool_agg(self, val_test: Literal[\"any\", \"all\"], skipna: bool):\n        \"\"\"\n        Shared func to call any / all Cython GroupBy implementations.\n        \"\"\"\n\n        def objs_to_bool(vals: ArrayLike) -> tuple[np.ndarray, type]:\n            if is_object_dtype(vals.dtype):\n                # GH#37501: don't raise on pd.NA when skipna=True\n                if skipna:\n                    func = np.vectorize(lambda x: bool(x) if not isna(x) else True)\n                    vals = func(vals)\n                else:\n                    vals = vals.astype(bool, copy=False)\n\n                vals = cast(np.ndarray, vals)\n            elif isinstance(vals, BaseMaskedArray):\n                vals = vals._data.astype(bool, copy=False)\n            else:\n                vals = vals.astype(bool, copy=False)\n\n            return vals.view(np.int8), bool\n\n        def result_to_bool(\n            result: np.ndarray,\n            inference: type,\n            nullable: bool = False,\n        ) -> ArrayLike:\n            if nullable:\n                return BooleanArray(result.astype(bool, copy=False), result == -1)\n            else:\n                return result.astype(inference, copy=False)\n\n        return self._get_cythonized_result(\n            libgroupby.group_any_all,\n            numeric_only=False,\n            cython_dtype=np.dtype(np.int8),\n            needs_mask=True,\n            needs_nullable=True,\n            pre_processing=objs_to_bool,\n            post_processing=result_to_bool,\n            val_test=val_test,\n            skipna=skipna,\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def any(self, skipna: bool = True):\n        \"\"\"\n        Return True if any value in the group is truthful, else False.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Flag to ignore nan values during truth testing.\n\n        Returns\n        -------\n        Series or DataFrame\n            DataFrame or Series of boolean values, where a value is True if any element\n            is True within its respective group, False otherwise.\n        \"\"\"\n        return self._bool_agg(\"any\", skipna)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def all(self, skipna: bool = True):\n        \"\"\"\n        Return True if all values in the group are truthful, else False.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Flag to ignore nan values during truth testing.\n\n        Returns\n        -------\n        Series or DataFrame\n            DataFrame or Series of boolean values, where a value is True if all elements\n            are True within its respective group, False otherwise.\n        \"\"\"\n        return self._bool_agg(\"all\", skipna)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def count(self) -> Series | DataFrame:\n        \"\"\"\n        Compute count of group, excluding missing values.\n\n        Returns\n        -------\n        Series or DataFrame\n            Count of values within each group.\n        \"\"\"\n        data = self._get_data_to_aggregate()\n        ids, _, ngroups = self.grouper.group_info\n        mask = ids != -1\n\n        is_series = data.ndim == 1\n\n        def hfunc(bvalues: ArrayLike) -> ArrayLike:\n            # TODO(EA2D): reshape would not be necessary with 2D EAs\n            if bvalues.ndim == 1:\n                # EA\n                masked = mask & ~isna(bvalues).reshape(1, -1)\n            else:\n                masked = mask & ~isna(bvalues)\n\n            counted = lib.count_level_2d(masked, labels=ids, max_bin=ngroups, axis=1)\n            if is_series:\n                assert counted.ndim == 2\n                assert counted.shape[0] == 1\n                return counted[0]\n            return counted\n\n        new_mgr = data.grouped_reduce(hfunc)\n\n        # If we are grouping on categoricals we want unobserved categories to\n        # return zero, rather than the default of NaN which the reindexing in\n        # _wrap_agged_manager() returns. GH 35028\n        with com.temp_setattr(self, \"observed\", True):\n            result = self._wrap_agged_manager(new_mgr)\n\n        if result.ndim == 1:\n            result.index = self.grouper.result_index\n\n        return self._reindex_output(result, fill_value=0)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def mean(\n        self,\n        numeric_only: bool | lib.NoDefault = lib.no_default,\n        engine: str = \"cython\",\n        engine_kwargs: dict[str, bool] | None = None,\n    ):\n        \"\"\"\n        Compute mean of groups, excluding missing values.\n\n        Parameters\n        ----------\n        numeric_only : bool, default True\n            Include only float, int, boolean columns. If None, will attempt to use\n            everything, then use only numeric data.\n\n        engine : str, default None\n            * ``'cython'`` : Runs the operation through C-extensions from cython.\n            * ``'numba'`` : Runs the operation through JIT compiled code from numba.\n            * ``None`` : Defaults to ``'cython'`` or globally setting\n              ``compute.use_numba``\n\n            .. versionadded:: 1.4.0\n\n        engine_kwargs : dict, default None\n            * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n            * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n              and ``parallel`` dictionary keys. The values must either be ``True`` or\n              ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n              ``{{'nopython': True, 'nogil': False, 'parallel': False}}``\n\n            .. versionadded:: 1.4.0\n\n        Returns\n        -------\n        pandas.Series or pandas.DataFrame\n        %(see_also)s\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5],\n        ...                    'C': [1, 2, 1, 1, 2]}, columns=['A', 'B', 'C'])\n\n        Groupby one column and return the mean of the remaining columns in\n        each group.\n\n        >>> df.groupby('A').mean()\n             B         C\n        A\n        1  3.0  1.333333\n        2  4.0  1.500000\n\n        Groupby two columns and return the mean of the remaining column.\n\n        >>> df.groupby(['A', 'B']).mean()\n                 C\n        A B\n        1 2.0  2.0\n          4.0  1.0\n        2 3.0  1.0\n          5.0  2.0\n\n        Groupby one column and return the mean of only particular column in\n        the group.\n\n        >>> df.groupby('A')['B'].mean()\n        A\n        1    3.0\n        2    4.0\n        Name: B, dtype: float64\n        \"\"\"\n        numeric_only = self._resolve_numeric_only(numeric_only)\n\n        if maybe_use_numba(engine):\n            from pandas.core._numba.kernels import sliding_mean\n\n            return self._numba_agg_general(sliding_mean, engine_kwargs, \"groupby_mean\")\n        else:\n            result = self._cython_agg_general(\n                \"mean\",\n                alt=lambda x: Series(x).mean(numeric_only=numeric_only),\n                numeric_only=numeric_only,\n            )\n            return result.__finalize__(self.obj, method=\"groupby\")\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def median(self, numeric_only: bool | lib.NoDefault = lib.no_default):\n        \"\"\"\n        Compute median of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Parameters\n        ----------\n        numeric_only : bool, default True\n            Include only float, int, boolean columns. If None, will attempt to use\n            everything, then use only numeric data.\n\n        Returns\n        -------\n        Series or DataFrame\n            Median of values within each group.\n        \"\"\"\n        numeric_only = self._resolve_numeric_only(numeric_only)\n\n        result = self._cython_agg_general(\n            \"median\",\n            alt=lambda x: Series(x).median(numeric_only=numeric_only),\n            numeric_only=numeric_only,\n        )\n        return result.__finalize__(self.obj, method=\"groupby\")\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def std(self, ddof: int = 1):\n        \"\"\"\n        Compute standard deviation of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        Returns\n        -------\n        Series or DataFrame\n            Standard deviation of values within each group.\n        \"\"\"\n        return self._get_cythonized_result(\n            libgroupby.group_var,\n            needs_counts=True,\n            cython_dtype=np.dtype(np.float64),\n            post_processing=lambda vals, inference: np.sqrt(vals),\n            ddof=ddof,\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def var(self, ddof: int = 1):\n        \"\"\"\n        Compute variance of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        Returns\n        -------\n        Series or DataFrame\n            Variance of values within each group.\n        \"\"\"\n        if ddof == 1:\n            numeric_only = self._resolve_numeric_only(lib.no_default)\n            return self._cython_agg_general(\n                \"var\", alt=lambda x: Series(x).var(ddof=ddof), numeric_only=numeric_only\n            )\n        else:\n            func = lambda x: x.var(ddof=ddof)\n            with self._group_selection_context():\n                return self._python_agg_general(func)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def sem(self, ddof: int = 1):\n        \"\"\"\n        Compute standard error of the mean of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : int, default 1\n            Degrees of freedom.\n\n        Returns\n        -------\n        Series or DataFrame\n            Standard error of the mean of values within each group.\n        \"\"\"\n        result = self.std(ddof=ddof)\n        if result.ndim == 1:\n            result /= np.sqrt(self.count())\n        else:\n            cols = result.columns.difference(self.exclusions).unique()\n            counts = self.count()\n            result_ilocs = result.columns.get_indexer_for(cols)\n            count_ilocs = counts.columns.get_indexer_for(cols)\n            result.iloc[:, result_ilocs] /= np.sqrt(counts.iloc[:, count_ilocs])\n        return result\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def size(self) -> DataFrame | Series:\n        \"\"\"\n        Compute group sizes.\n\n        Returns\n        -------\n        DataFrame or Series\n            Number of rows in each group as a Series if as_index is True\n            or a DataFrame if as_index is False.\n        \"\"\"\n        result = self.grouper.size()\n\n        # GH28330 preserve subclassed Series/DataFrames through calls\n        if issubclass(self.obj._constructor, Series):\n            result = self._obj_1d_constructor(result, name=self.obj.name)\n        else:\n            result = self._obj_1d_constructor(result)\n\n        if not self.as_index:\n            # Item \"None\" of \"Optional[Series]\" has no attribute \"reset_index\"\n            result = result.rename(\"size\").reset_index()  # type: ignore[union-attr]\n\n        return self._reindex_output(result, fill_value=0)\n\n    @final\n    @doc(_groupby_agg_method_template, fname=\"sum\", no=True, mc=0)\n    def sum(\n        self, numeric_only: bool | lib.NoDefault = lib.no_default, min_count: int = 0\n    ):\n        numeric_only = self._resolve_numeric_only(numeric_only)\n\n        # If we are grouping on categoricals we want unobserved categories to\n        # return zero, rather than the default of NaN which the reindexing in\n        # _agg_general() returns. GH #31422\n        with com.temp_setattr(self, \"observed\", True):\n            result = self._agg_general(\n                numeric_only=numeric_only,\n                min_count=min_count,\n                alias=\"add\",\n                npfunc=np.sum,\n            )\n\n        return self._reindex_output(result, fill_value=0)\n\n    @final\n    @doc(_groupby_agg_method_template, fname=\"prod\", no=True, mc=0)\n    def prod(\n        self, numeric_only: bool | lib.NoDefault = lib.no_default, min_count: int = 0\n    ):\n        numeric_only = self._resolve_numeric_only(numeric_only)\n\n        return self._agg_general(\n            numeric_only=numeric_only, min_count=min_count, alias=\"prod\", npfunc=np.prod\n        )\n\n    @final\n    @doc(_groupby_agg_method_template, fname=\"min\", no=False, mc=-1)\n    def min(self, numeric_only: bool = False, min_count: int = -1):\n        return self._agg_general(\n            numeric_only=numeric_only, min_count=min_count, alias=\"min\", npfunc=np.min\n        )\n\n    @final\n    @doc(_groupby_agg_method_template, fname=\"max\", no=False, mc=-1)\n    def max(self, numeric_only: bool = False, min_count: int = -1):\n        return self._agg_general(\n            numeric_only=numeric_only, min_count=min_count, alias=\"max\", npfunc=np.max\n        )\n\n    @final\n    @doc(_groupby_agg_method_template, fname=\"first\", no=False, mc=-1)\n    def first(self, numeric_only: bool = False, min_count: int = -1):\n        def first_compat(obj: NDFrameT, axis: int = 0):\n            def first(x: Series):\n                \"\"\"Helper function for first item that isn't NA.\"\"\"\n                arr = x.array[notna(x.array)]\n                if not len(arr):\n                    return np.nan\n                return arr[0]\n\n            if isinstance(obj, DataFrame):\n                return obj.apply(first, axis=axis)\n            elif isinstance(obj, Series):\n                return first(obj)\n            else:  # pragma: no cover\n                raise TypeError(type(obj))\n\n        return self._agg_general(\n            numeric_only=numeric_only,\n            min_count=min_count,\n            alias=\"first\",\n            npfunc=first_compat,\n        )\n\n    @final\n    @doc(_groupby_agg_method_template, fname=\"last\", no=False, mc=-1)\n    def last(self, numeric_only: bool = False, min_count: int = -1):\n        def last_compat(obj: NDFrameT, axis: int = 0):\n            def last(x: Series):\n                \"\"\"Helper function for last item that isn't NA.\"\"\"\n                arr = x.array[notna(x.array)]\n                if not len(arr):\n                    return np.nan\n                return arr[-1]\n\n            if isinstance(obj, DataFrame):\n                return obj.apply(last, axis=axis)\n            elif isinstance(obj, Series):\n                return last(obj)\n            else:  # pragma: no cover\n                raise TypeError(type(obj))\n\n        return self._agg_general(\n            numeric_only=numeric_only,\n            min_count=min_count,\n            alias=\"last\",\n            npfunc=last_compat,\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def ohlc(self) -> DataFrame:\n        \"\"\"\n        Compute open, high, low and close values of a group, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Returns\n        -------\n        DataFrame\n            Open, high, low and close values within each group.\n        \"\"\"\n        if self.obj.ndim == 1:\n            # self._iterate_slices() yields only self._selected_obj\n            obj = self._selected_obj\n\n            is_numeric = is_numeric_dtype(obj.dtype)\n            if not is_numeric:\n                raise DataError(\"No numeric types to aggregate\")\n\n            res_values = self.grouper._cython_operation(\n                \"aggregate\", obj._values, \"ohlc\", axis=0, min_count=-1\n            )\n\n            agg_names = [\"open\", \"high\", \"low\", \"close\"]\n            result = self.obj._constructor_expanddim(\n                res_values, index=self.grouper.result_index, columns=agg_names\n            )\n            return self._reindex_output(result)\n\n        return self._apply_to_column_groupbys(\n            lambda x: x.ohlc(), self._obj_with_exclusions\n        )\n\n    @doc(DataFrame.describe)\n    def describe(self, **kwargs):\n        with self._group_selection_context():\n            result = self.apply(lambda x: x.describe(**kwargs))\n            if self.axis == 1:\n                return result.T\n            return result.unstack()\n\n    @final\n    def resample(self, rule, *args, **kwargs):\n        \"\"\"\n        Provide resampling when using a TimeGrouper.\n\n        Given a grouper, the function resamples it according to a string\n        \"string\" -> \"frequency\".\n\n        See the :ref:`frequency aliases <timeseries.offset_aliases>`\n        documentation for more details.\n\n        Parameters\n        ----------\n        rule : str or DateOffset\n            The offset string or object representing target grouper conversion.\n        *args, **kwargs\n            Possible arguments are `how`, `fill_method`, `limit`, `kind` and\n            `on`, and other arguments of `TimeGrouper`.\n\n        Returns\n        -------\n        Grouper\n            Return a new grouper with our resampler appended.\n\n        See Also\n        --------\n        Grouper : Specify a frequency to resample with when\n            grouping by a key.\n        DatetimeIndex.resample : Frequency conversion and resampling of\n            time series.\n\n        Examples\n        --------\n        >>> idx = pd.date_range('1/1/2000', periods=4, freq='T')\n        >>> df = pd.DataFrame(data=4 * [range(2)],\n        ...                   index=idx,\n        ...                   columns=['a', 'b'])\n        >>> df.iloc[2, 0] = 5\n        >>> df\n                            a  b\n        2000-01-01 00:00:00  0  1\n        2000-01-01 00:01:00  0  1\n        2000-01-01 00:02:00  5  1\n        2000-01-01 00:03:00  0  1\n\n        Downsample the DataFrame into 3 minute bins and sum the values of\n        the timestamps falling into a bin.\n\n        >>> df.groupby('a').resample('3T').sum()\n                                 a  b\n        a\n        0   2000-01-01 00:00:00  0  2\n            2000-01-01 00:03:00  0  1\n        5   2000-01-01 00:00:00  5  1\n\n        Upsample the series into 30 second bins.\n\n        >>> df.groupby('a').resample('30S').sum()\n                            a  b\n        a\n        0   2000-01-01 00:00:00  0  1\n            2000-01-01 00:00:30  0  0\n            2000-01-01 00:01:00  0  1\n            2000-01-01 00:01:30  0  0\n            2000-01-01 00:02:00  0  0\n            2000-01-01 00:02:30  0  0\n            2000-01-01 00:03:00  0  1\n        5   2000-01-01 00:02:00  5  1\n\n        Resample by month. Values are assigned to the month of the period.\n\n        >>> df.groupby('a').resample('M').sum()\n                    a  b\n        a\n        0   2000-01-31  0  3\n        5   2000-01-31  5  1\n\n        Downsample the series into 3 minute bins as above, but close the right\n        side of the bin interval.\n\n        >>> df.groupby('a').resample('3T', closed='right').sum()\n                                 a  b\n        a\n        0   1999-12-31 23:57:00  0  1\n            2000-01-01 00:00:00  0  2\n        5   2000-01-01 00:00:00  5  1\n\n        Downsample the series into 3 minute bins and close the right side of\n        the bin interval, but label each bin using the right edge instead of\n        the left.\n\n        >>> df.groupby('a').resample('3T', closed='right', label='right').sum()\n                                 a  b\n        a\n        0   2000-01-01 00:00:00  0  1\n            2000-01-01 00:03:00  0  2\n        5   2000-01-01 00:03:00  5  1\n        \"\"\"\n        from pandas.core.resample import get_resampler_for_grouping\n\n        return get_resampler_for_grouping(self, rule, *args, **kwargs)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def rolling(self, *args, **kwargs):\n        \"\"\"\n        Return a rolling grouper, providing rolling functionality per group.\n        \"\"\"\n        from pandas.core.window import RollingGroupby\n\n        return RollingGroupby(\n            self._selected_obj,\n            *args,\n            _grouper=self.grouper,\n            _as_index=self.as_index,\n            **kwargs,\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def expanding(self, *args, **kwargs):\n        \"\"\"\n        Return an expanding grouper, providing expanding\n        functionality per group.\n        \"\"\"\n        from pandas.core.window import ExpandingGroupby\n\n        return ExpandingGroupby(\n            self._selected_obj,\n            *args,\n            _grouper=self.grouper,\n            **kwargs,\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def ewm(self, *args, **kwargs):\n        \"\"\"\n        Return an ewm grouper, providing ewm functionality per group.\n        \"\"\"\n        from pandas.core.window import ExponentialMovingWindowGroupby\n\n        return ExponentialMovingWindowGroupby(\n            self._selected_obj,\n            *args,\n            _grouper=self.grouper,\n            **kwargs,\n        )\n\n    @final\n    def _fill(self, direction: Literal[\"ffill\", \"bfill\"], limit=None):\n        \"\"\"\n        Shared function for `pad` and `backfill` to call Cython method.\n\n        Parameters\n        ----------\n        direction : {'ffill', 'bfill'}\n            Direction passed to underlying Cython function. `bfill` will cause\n            values to be filled backwards. `ffill` and any other values will\n            default to a forward fill\n        limit : int, default None\n            Maximum number of consecutive values to fill. If `None`, this\n            method will convert to -1 prior to passing to Cython\n\n        Returns\n        -------\n        `Series` or `DataFrame` with filled values\n\n        See Also\n        --------\n        pad : Returns Series with minimum number of char in object.\n        backfill : Backward fill the missing values in the dataset.\n        \"\"\"\n        # Need int value for Cython\n        if limit is None:\n            limit = -1\n\n        ids, _, _ = self.grouper.group_info\n        sorted_labels = np.argsort(ids, kind=\"mergesort\").astype(np.intp, copy=False)\n        if direction == \"bfill\":\n            sorted_labels = sorted_labels[::-1]\n\n        col_func = partial(\n            libgroupby.group_fillna_indexer,\n            labels=ids,\n            sorted_labels=sorted_labels,\n            direction=direction,\n            limit=limit,\n            dropna=self.dropna,\n        )\n\n        def blk_func(values: ArrayLike) -> ArrayLike:\n            mask = isna(values)\n            if values.ndim == 1:\n                indexer = np.empty(values.shape, dtype=np.intp)\n                col_func(out=indexer, mask=mask)\n                return algorithms.take_nd(values, indexer)\n\n            else:\n                # We broadcast algorithms.take_nd analogous to\n                #  np.take_along_axis\n\n                # Note: we only get here with backfill/pad,\n                #  so if we have a dtype that cannot hold NAs,\n                #  then there will be no -1s in indexer, so we can use\n                #  the original dtype (no need to ensure_dtype_can_hold_na)\n                if isinstance(values, np.ndarray):\n                    out = np.empty(values.shape, dtype=values.dtype)\n                else:\n                    out = type(values)._empty(values.shape, dtype=values.dtype)\n\n                for i in range(len(values)):\n                    # call group_fillna_indexer column-wise\n                    indexer = np.empty(values.shape[1], dtype=np.intp)\n                    col_func(out=indexer, mask=mask[i])\n                    out[i, :] = algorithms.take_nd(values[i], indexer)\n                return out\n\n        obj = self._obj_with_exclusions\n        if self.axis == 1:\n            obj = obj.T\n        mgr = obj._mgr\n        res_mgr = mgr.apply(blk_func)\n\n        new_obj = obj._constructor(res_mgr)\n        if isinstance(new_obj, Series):\n            new_obj.name = obj.name\n\n        return self._wrap_transformed_output(new_obj)\n\n    @final\n    @Substitution(name=\"groupby\")\n    def pad(self, limit=None):\n        \"\"\"\n        Forward fill the values.\n\n        Parameters\n        ----------\n        limit : int, optional\n            Limit of how many values to fill.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object with missing values filled.\n\n        See Also\n        --------\n        Series.pad: Returns Series with minimum number of char in object.\n        DataFrame.pad: Object with missing values filled or None if inplace=True.\n        Series.fillna: Fill NaN values of a Series.\n        DataFrame.fillna: Fill NaN values of a DataFrame.\n        \"\"\"\n        return self._fill(\"ffill\", limit=limit)\n\n    ffill = pad\n\n    @final\n    @Substitution(name=\"groupby\")\n    def backfill(self, limit=None):\n        \"\"\"\n        Backward fill the values.\n\n        Parameters\n        ----------\n        limit : int, optional\n            Limit of how many values to fill.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object with missing values filled.\n\n        See Also\n        --------\n        Series.backfill :  Backward fill the missing values in the dataset.\n        DataFrame.backfill:  Backward fill the missing values in the dataset.\n        Series.fillna: Fill NaN values of a Series.\n        DataFrame.fillna: Fill NaN values of a DataFrame.\n        \"\"\"\n        return self._fill(\"bfill\", limit=limit)\n\n    bfill = backfill\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def nth(\n        self,\n        n: PositionalIndexer | tuple,\n        dropna: Literal[\"any\", \"all\", None] = None,\n    ) -> NDFrameT:\n        \"\"\"\n        Take the nth row from each group if n is an int, otherwise a subset of rows.\n\n        If dropna, will take the nth non-null row, dropna is either\n        'all' or 'any'; this is equivalent to calling dropna(how=dropna)\n        before the groupby.\n\n        Parameters\n        ----------\n        n : int, slice or list of ints and slices\n            A single nth value for the row or a list of nth values or slices.\n\n            .. versionchanged:: 1.4.0\n                Added slice and lists containiing slices.\n\n        dropna : {'any', 'all', None}, default None\n            Apply the specified dropna operation before counting which row is\n            the nth row. Only supported if n is an int.\n\n        Returns\n        -------\n        Series or DataFrame\n            N-th value within each group.\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5]}, columns=['A', 'B'])\n        >>> g = df.groupby('A')\n        >>> g.nth(0)\n             B\n        A\n        1  NaN\n        2  3.0\n        >>> g.nth(1)\n             B\n        A\n        1  2.0\n        2  5.0\n        >>> g.nth(-1)\n             B\n        A\n        1  4.0\n        2  5.0\n        >>> g.nth([0, 1])\n             B\n        A\n        1  NaN\n        1  2.0\n        2  3.0\n        2  5.0\n        >>> g.nth(slice(None, -1))\n             B\n        A\n        1  NaN\n        1  2.0\n        2  3.0\n\n        Specifying `dropna` allows count ignoring ``NaN``\n\n        >>> g.nth(0, dropna='any')\n             B\n        A\n        1  2.0\n        2  3.0\n\n        NaNs denote group exhausted when using dropna\n\n        >>> g.nth(3, dropna='any')\n            B\n        A\n        1 NaN\n        2 NaN\n\n        Specifying `as_index=False` in `groupby` keeps the original index.\n\n        >>> df.groupby('A', as_index=False).nth(1)\n           A    B\n        1  1  2.0\n        4  2  5.0\n        \"\"\"\n        if not dropna:\n            with self._group_selection_context():\n                mask = self._make_mask_from_positional_indexer(n)\n\n                ids, _, _ = self.grouper.group_info\n\n                # Drop NA values in grouping\n                mask = mask & (ids != -1)\n\n                out = self._mask_selected_obj(mask)\n                if not self.as_index:\n                    return out\n\n                result_index = self.grouper.result_index\n                if self.axis == 0:\n                    out.index = result_index[ids[mask]]\n                    if not self.observed and isinstance(result_index, CategoricalIndex):\n                        out = out.reindex(result_index)\n\n                    out = self._reindex_output(out)\n                else:\n                    out.columns = result_index[ids[mask]]\n\n                return out.sort_index(axis=self.axis) if self.sort else out\n\n        # dropna is truthy\n        if not is_integer(n):\n            raise ValueError(\"dropna option only supported for an integer argument\")\n\n        if dropna not in [\"any\", \"all\"]:\n            # Note: when agg-ing picker doesn't raise this, just returns NaN\n            raise ValueError(\n                \"For a DataFrame or Series groupby.nth, dropna must be \"\n                \"either None, 'any' or 'all', \"\n                f\"(was passed {dropna}).\"\n            )\n\n        # old behaviour, but with all and any support for DataFrames.\n        # modified in GH 7559 to have better perf\n        n = cast(int, n)\n        max_len = n if n >= 0 else -1 - n\n        dropped = self.obj.dropna(how=dropna, axis=self.axis)\n\n        # get a new grouper for our dropped obj\n        if self.keys is None and self.level is None:\n\n            # we don't have the grouper info available\n            # (e.g. we have selected out\n            # a column that is not in the current object)\n            axis = self.grouper.axis\n            grouper = axis[axis.isin(dropped.index)]\n\n        else:\n\n            # create a grouper with the original parameters, but on dropped\n            # object\n            from pandas.core.groupby.grouper import get_grouper\n\n            grouper, _, _ = get_grouper(\n                dropped,\n                key=self.keys,\n                axis=self.axis,\n                level=self.level,\n                sort=self.sort,\n                mutated=self.mutated,\n            )\n\n        grb = dropped.groupby(\n            grouper, as_index=self.as_index, sort=self.sort, axis=self.axis\n        )\n        sizes, result = grb.size(), grb.nth(n)\n        mask = (sizes < max_len)._values\n\n        # set the results which don't meet the criteria\n        if len(result) and mask.any():\n            result.loc[mask] = np.nan\n\n        # reset/reindex to the original groups\n        if len(self.obj) == len(dropped) or len(result) == len(\n            self.grouper.result_index\n        ):\n            result.index = self.grouper.result_index\n        else:\n            result = result.reindex(self.grouper.result_index)\n\n        return result\n\n    @final\n    def quantile(self, q=0.5, interpolation: str = \"linear\"):\n        \"\"\"\n        Return group values at the given quantile, a la numpy.percentile.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            Value(s) between 0 and 1 providing the quantile(s) to compute.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            Method to use when the desired quantile falls between two points.\n\n        Returns\n        -------\n        Series or DataFrame\n            Return type determined by caller of GroupBy object.\n\n        See Also\n        --------\n        Series.quantile : Similar method for Series.\n        DataFrame.quantile : Similar method for DataFrame.\n        numpy.percentile : NumPy method to compute qth percentile.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([\n        ...     ['a', 1], ['a', 2], ['a', 3],\n        ...     ['b', 1], ['b', 3], ['b', 5]\n        ... ], columns=['key', 'val'])\n        >>> df.groupby('key').quantile()\n            val\n        key\n        a    2.0\n        b    3.0\n        \"\"\"\n\n        def pre_processor(vals: ArrayLike) -> tuple[np.ndarray, np.dtype | None]:\n            if is_object_dtype(vals):\n                raise TypeError(\n                    \"'quantile' cannot be performed against 'object' dtypes!\"\n                )\n\n            inference: np.dtype | None = None\n            if is_integer_dtype(vals.dtype):\n                if isinstance(vals, ExtensionArray):\n                    out = vals.to_numpy(dtype=float, na_value=np.nan)\n                else:\n                    out = vals\n                inference = np.dtype(np.int64)\n            elif is_bool_dtype(vals.dtype) and isinstance(vals, ExtensionArray):\n                out = vals.to_numpy(dtype=float, na_value=np.nan)\n            elif is_datetime64_dtype(vals.dtype):\n                inference = np.dtype(\"datetime64[ns]\")\n                out = np.asarray(vals).astype(float)\n            elif is_timedelta64_dtype(vals.dtype):\n                inference = np.dtype(\"timedelta64[ns]\")\n                out = np.asarray(vals).astype(float)\n            elif isinstance(vals, ExtensionArray) and is_float_dtype(vals):\n                inference = np.dtype(np.float64)\n                out = vals.to_numpy(dtype=float, na_value=np.nan)\n            else:\n                out = np.asarray(vals)\n\n            return out, inference\n\n        def post_processor(vals: np.ndarray, inference: np.dtype | None) -> np.ndarray:\n            if inference:\n                # Check for edge case\n                if not (\n                    is_integer_dtype(inference)\n                    and interpolation in {\"linear\", \"midpoint\"}\n                ):\n                    vals = vals.astype(inference)\n\n            return vals\n\n        orig_scalar = is_scalar(q)\n        if orig_scalar:\n            q = [q]\n\n        qs = np.array(q, dtype=np.float64)\n        ids, _, ngroups = self.grouper.group_info\n        nqs = len(qs)\n\n        func = partial(\n            libgroupby.group_quantile, labels=ids, qs=qs, interpolation=interpolation\n        )\n\n        # Put '-1' (NaN) labels as the last group so it does not interfere\n        # with the calculations. Note: length check avoids failure on empty\n        # labels. In that case, the value doesn't matter\n        na_label_for_sorting = ids.max() + 1 if len(ids) > 0 else 0\n        labels_for_lexsort = np.where(ids == -1, na_label_for_sorting, ids)\n\n        def blk_func(values: ArrayLike) -> ArrayLike:\n            mask = isna(values)\n            vals, inference = pre_processor(values)\n\n            ncols = 1\n            if vals.ndim == 2:\n                ncols = vals.shape[0]\n                shaped_labels = np.broadcast_to(\n                    labels_for_lexsort, (ncols, len(labels_for_lexsort))\n                )\n            else:\n                shaped_labels = labels_for_lexsort\n\n            out = np.empty((ncols, ngroups, nqs), dtype=np.float64)\n\n            # Get an index of values sorted by values and then labels\n            order = (vals, shaped_labels)\n            sort_arr = np.lexsort(order).astype(np.intp, copy=False)\n\n            if vals.ndim == 1:\n                func(out[0], values=vals, mask=mask, sort_indexer=sort_arr)\n            else:\n                for i in range(ncols):\n                    func(out[i], values=vals[i], mask=mask[i], sort_indexer=sort_arr[i])\n\n            if vals.ndim == 1:\n                out = out.ravel(\"K\")\n            else:\n                out = out.reshape(ncols, ngroups * nqs)\n            return post_processor(out, inference)\n\n        obj = self._obj_with_exclusions\n        is_ser = obj.ndim == 1\n        mgr = self._get_data_to_aggregate()\n\n        res_mgr = mgr.grouped_reduce(blk_func, ignore_failures=True)\n        if not is_ser and len(res_mgr.items) != len(mgr.items):\n            warn_dropping_nuisance_columns_deprecated(type(self), \"quantile\")\n\n            if len(res_mgr.items) == 0:\n                # re-call grouped_reduce to get the desired exception message\n                mgr.grouped_reduce(blk_func, ignore_failures=False)\n                # grouped_reduce _should_ raise, so this should not be reached\n                raise TypeError(  # pragma: no cover\n                    \"All columns were dropped in grouped_reduce\"\n                )\n\n        if is_ser:\n            res = self._wrap_agged_manager(res_mgr)\n        else:\n            res = obj._constructor(res_mgr)\n\n        if orig_scalar:\n            # Avoid expensive MultiIndex construction\n            return self._wrap_aggregated_output(res)\n        return self._wrap_aggregated_output(res, qs=qs)\n\n    @final\n    @Substitution(name=\"groupby\")\n    def ngroup(self, ascending: bool = True):\n        \"\"\"\n        Number each group from 0 to the number of groups - 1.\n\n        This is the enumerative complement of cumcount.  Note that the\n        numbers given to the groups match the order in which the groups\n        would be seen when iterating over the groupby object, not the\n        order they are first observed.\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from number of group - 1 to 0.\n\n        Returns\n        -------\n        Series\n            Unique numbers for each group.\n\n        See Also\n        --------\n        .cumcount : Number the rows in each group.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": list(\"aaabba\")})\n        >>> df\n           A\n        0  a\n        1  a\n        2  a\n        3  b\n        4  b\n        5  a\n        >>> df.groupby('A').ngroup()\n        0    0\n        1    0\n        2    0\n        3    1\n        4    1\n        5    0\n        dtype: int64\n        >>> df.groupby('A').ngroup(ascending=False)\n        0    1\n        1    1\n        2    1\n        3    0\n        4    0\n        5    1\n        dtype: int64\n        >>> df.groupby([\"A\", [1,1,2,3,2,1]]).ngroup()\n        0    0\n        1    0\n        2    1\n        3    3\n        4    2\n        5    0\n        dtype: int64\n        \"\"\"\n        with self._group_selection_context():\n            index = self._selected_obj.index\n            result = self._obj_1d_constructor(\n                self.grouper.group_info[0], index, dtype=np.int64\n            )\n            if not ascending:\n                result = self.ngroups - 1 - result\n            return result\n\n    @final\n    @Substitution(name=\"groupby\")\n    def cumcount(self, ascending: bool = True):\n        \"\"\"\n        Number each item in each group from 0 to the length of that group - 1.\n\n        Essentially this is equivalent to\n\n        .. code-block:: python\n\n            self.apply(lambda x: pd.Series(np.arange(len(x)), x.index))\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Returns\n        -------\n        Series\n            Sequence number of each element within each group.\n\n        See Also\n        --------\n        .ngroup : Number the groups themselves.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([['a'], ['a'], ['a'], ['b'], ['b'], ['a']],\n        ...                   columns=['A'])\n        >>> df\n           A\n        0  a\n        1  a\n        2  a\n        3  b\n        4  b\n        5  a\n        >>> df.groupby('A').cumcount()\n        0    0\n        1    1\n        2    2\n        3    0\n        4    1\n        5    3\n        dtype: int64\n        >>> df.groupby('A').cumcount(ascending=False)\n        0    3\n        1    2\n        2    1\n        3    1\n        4    0\n        5    0\n        dtype: int64\n        \"\"\"\n        with self._group_selection_context():\n            index = self._selected_obj._get_axis(self.axis)\n            cumcounts = self._cumcount_array(ascending=ascending)\n            return self._obj_1d_constructor(cumcounts, index)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def rank(\n        self,\n        method: str = \"average\",\n        ascending: bool = True,\n        na_option: str = \"keep\",\n        pct: bool = False,\n        axis: int = 0,\n    ):\n        \"\"\"\n        Provide the rank of values within each group.\n\n        Parameters\n        ----------\n        method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n            * average: average rank of group.\n            * min: lowest rank in group.\n            * max: highest rank in group.\n            * first: ranks assigned in order they appear in the array.\n            * dense: like 'min', but rank always increases by 1 between groups.\n        ascending : bool, default True\n            False for ranks by high (1) to low (N).\n        na_option : {'keep', 'top', 'bottom'}, default 'keep'\n            * keep: leave NA values where they are.\n            * top: smallest rank if ascending.\n            * bottom: smallest rank if descending.\n        pct : bool, default False\n            Compute percentage rank of data within each group.\n        axis : int, default 0\n            The axis of the object over which to compute the rank.\n\n        Returns\n        -------\n        DataFrame with ranking of values within each group\n        %(see_also)s\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\n        ...         \"group\": [\"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"b\"],\n        ...         \"value\": [2, 4, 2, 3, 5, 1, 2, 4, 1, 5],\n        ...     }\n        ... )\n        >>> df\n          group  value\n        0     a      2\n        1     a      4\n        2     a      2\n        3     a      3\n        4     a      5\n        5     b      1\n        6     b      2\n        7     b      4\n        8     b      1\n        9     b      5\n        >>> for method in ['average', 'min', 'max', 'dense', 'first']:\n        ...     df[f'{method}_rank'] = df.groupby('group')['value'].rank(method)\n        >>> df\n          group  value  average_rank  min_rank  max_rank  dense_rank  first_rank\n        0     a      2           1.5       1.0       2.0         1.0         1.0\n        1     a      4           4.0       4.0       4.0         3.0         4.0\n        2     a      2           1.5       1.0       2.0         1.0         2.0\n        3     a      3           3.0       3.0       3.0         2.0         3.0\n        4     a      5           5.0       5.0       5.0         4.0         5.0\n        5     b      1           1.5       1.0       2.0         1.0         1.0\n        6     b      2           3.0       3.0       3.0         2.0         3.0\n        7     b      4           4.0       4.0       4.0         3.0         4.0\n        8     b      1           1.5       1.0       2.0         1.0         2.0\n        9     b      5           5.0       5.0       5.0         4.0         5.0\n        \"\"\"\n        if na_option not in {\"keep\", \"top\", \"bottom\"}:\n            msg = \"na_option must be one of 'keep', 'top', or 'bottom'\"\n            raise ValueError(msg)\n\n        kwargs = {\n            \"ties_method\": method,\n            \"ascending\": ascending,\n            \"na_option\": na_option,\n            \"pct\": pct,\n        }\n        if axis != 0:\n            # DataFrame uses different keyword name\n            kwargs[\"method\"] = kwargs.pop(\"ties_method\")\n            return self.apply(lambda x: x.rank(axis=axis, numeric_only=False, **kwargs))\n\n        return self._cython_transform(\n            \"rank\",\n            numeric_only=False,\n            axis=axis,\n            **kwargs,\n        )\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def cumprod(self, axis=0, *args, **kwargs):\n        \"\"\"\n        Cumulative product for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        nv.validate_groupby_func(\"cumprod\", args, kwargs, [\"numeric_only\", \"skipna\"])\n        if axis != 0:\n            return self.apply(lambda x: x.cumprod(axis=axis, **kwargs))\n\n        return self._cython_transform(\"cumprod\", **kwargs)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def cumsum(self, axis=0, *args, **kwargs):\n        \"\"\"\n        Cumulative sum for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        nv.validate_groupby_func(\"cumsum\", args, kwargs, [\"numeric_only\", \"skipna\"])\n        if axis != 0:\n            return self.apply(lambda x: x.cumsum(axis=axis, **kwargs))\n\n        return self._cython_transform(\"cumsum\", **kwargs)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def cummin(self, axis=0, **kwargs):\n        \"\"\"\n        Cumulative min for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        skipna = kwargs.get(\"skipna\", True)\n        if axis != 0:\n            return self.apply(lambda x: np.minimum.accumulate(x, axis))\n\n        return self._cython_transform(\"cummin\", numeric_only=False, skipna=skipna)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def cummax(self, axis=0, **kwargs):\n        \"\"\"\n        Cumulative max for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        skipna = kwargs.get(\"skipna\", True)\n        if axis != 0:\n            return self.apply(lambda x: np.maximum.accumulate(x, axis))\n\n        return self._cython_transform(\"cummax\", numeric_only=False, skipna=skipna)\n\n    @final\n    def _get_cythonized_result(\n        self,\n        base_func: Callable,\n        cython_dtype: np.dtype,\n        numeric_only: bool | lib.NoDefault = lib.no_default,\n        needs_counts: bool = False,\n        needs_nullable: bool = False,\n        needs_mask: bool = False,\n        pre_processing=None,\n        post_processing=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Get result for Cythonized functions.\n\n        Parameters\n        ----------\n        base_func : callable, Cythonized function to be called\n        cython_dtype : np.dtype\n            Type of the array that will be modified by the Cython call.\n        numeric_only : bool, default True\n            Whether only numeric datatypes should be computed\n        needs_counts : bool, default False\n            Whether the counts should be a part of the Cython call\n        needs_mask : bool, default False\n            Whether boolean mask needs to be part of the Cython call\n            signature\n        needs_nullable : bool, default False\n            Whether a bool specifying if the input is nullable is part\n            of the Cython call signature\n        pre_processing : function, default None\n            Function to be applied to `values` prior to passing to Cython.\n            Function should return a tuple where the first element is the\n            values to be passed to Cython and the second element is an optional\n            type which the values should be converted to after being returned\n            by the Cython operation. This function is also responsible for\n            raising a TypeError if the values have an invalid type. Raises\n            if `needs_values` is False.\n        post_processing : function, default None\n            Function to be applied to result of Cython function. Should accept\n            an array of values as the first argument and type inferences as its\n            second argument, i.e. the signature should be\n            (ndarray, Type). If `needs_nullable=True`, a third argument should be\n            `nullable`, to allow for processing specific to nullable values.\n        **kwargs : dict\n            Extra arguments to be passed back to Cython funcs\n\n        Returns\n        -------\n        `Series` or `DataFrame`  with filled values\n        \"\"\"\n        numeric_only = self._resolve_numeric_only(numeric_only)\n\n        if post_processing and not callable(post_processing):\n            raise ValueError(\"'post_processing' must be a callable!\")\n        if pre_processing and not callable(pre_processing):\n            raise ValueError(\"'pre_processing' must be a callable!\")\n\n        grouper = self.grouper\n\n        ids, _, ngroups = grouper.group_info\n\n        how = base_func.__name__\n        base_func = partial(base_func, labels=ids)\n\n        def blk_func(values: ArrayLike) -> ArrayLike:\n            values = values.T\n            ncols = 1 if values.ndim == 1 else values.shape[1]\n\n            result: ArrayLike\n            result = np.zeros(ngroups * ncols, dtype=cython_dtype)\n            result = result.reshape((ngroups, ncols))\n\n            func = partial(base_func, out=result)\n\n            inferences = None\n\n            if needs_counts:\n                counts = np.zeros(self.ngroups, dtype=np.int64)\n                func = partial(func, counts=counts)\n\n            vals = values\n            if pre_processing:\n                vals, inferences = pre_processing(vals)\n\n            vals = vals.astype(cython_dtype, copy=False)\n            if vals.ndim == 1:\n                vals = vals.reshape((-1, 1))\n            func = partial(func, values=vals)\n\n            if needs_mask:\n                mask = isna(values).view(np.uint8)\n                if mask.ndim == 1:\n                    mask = mask.reshape(-1, 1)\n                func = partial(func, mask=mask)\n\n            if needs_nullable:\n                is_nullable = isinstance(values, BaseMaskedArray)\n                func = partial(func, nullable=is_nullable)\n\n            func(**kwargs)  # Call func to modify indexer values in place\n\n            if values.ndim == 1:\n                assert result.shape[1] == 1, result.shape\n                result = result[:, 0]\n\n            if post_processing:\n                pp_kwargs = {}\n                if needs_nullable:\n                    pp_kwargs[\"nullable\"] = isinstance(values, BaseMaskedArray)\n\n                result = post_processing(result, inferences, **pp_kwargs)\n\n            return result.T\n\n        obj = self._obj_with_exclusions\n\n        # Operate block-wise instead of column-by-column\n        is_ser = obj.ndim == 1\n        mgr = self._get_data_to_aggregate()\n\n        if numeric_only:\n            mgr = mgr.get_numeric_data()\n\n        res_mgr = mgr.grouped_reduce(blk_func, ignore_failures=True)\n\n        if not is_ser and len(res_mgr.items) != len(mgr.items):\n            howstr = how.replace(\"group_\", \"\")\n            warn_dropping_nuisance_columns_deprecated(type(self), howstr)\n\n            if len(res_mgr.items) == 0:\n                # We re-call grouped_reduce to get the right exception message\n                mgr.grouped_reduce(blk_func, ignore_failures=False)\n                # grouped_reduce _should_ raise, so this should not be reached\n                raise TypeError(  # pragma: no cover\n                    \"All columns were dropped in grouped_reduce\"\n                )\n\n        if is_ser:\n            out = self._wrap_agged_manager(res_mgr)\n        else:\n            out = obj._constructor(res_mgr)\n\n        return self._wrap_aggregated_output(out)\n\n    @final\n    @Substitution(name=\"groupby\")\n    def shift(self, periods=1, freq=None, axis=0, fill_value=None):\n        \"\"\"\n        Shift each group by periods observations.\n\n        If freq is passed, the index will be increased using the periods and the freq.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods to shift.\n        freq : str, optional\n            Frequency string.\n        axis : axis to shift, default 0\n            Shift direction.\n        fill_value : optional\n            The scalar value to use for newly introduced missing values.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object shifted within each group.\n\n        See Also\n        --------\n        Index.shift : Shift values of Index.\n        tshift : Shift the time index, using the index’s frequency\n            if available.\n        \"\"\"\n        if freq is not None or axis != 0:\n            return self.apply(lambda x: x.shift(periods, freq, axis, fill_value))\n\n        ids, _, ngroups = self.grouper.group_info\n        res_indexer = np.zeros(len(ids), dtype=np.int64)\n\n        libgroupby.group_shift_indexer(res_indexer, ids, ngroups, periods)\n\n        obj = self._obj_with_exclusions\n\n        res = obj._reindex_with_indexers(\n            {self.axis: (obj.axes[self.axis], res_indexer)},\n            fill_value=fill_value,\n            allow_dups=True,\n        )\n        return res\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Appender(_common_see_also)\n    def pct_change(self, periods=1, fill_method=\"pad\", limit=None, freq=None, axis=0):\n        \"\"\"\n        Calculate pct_change of each value to previous entry in group.\n\n        Returns\n        -------\n        Series or DataFrame\n            Percentage changes within each group.\n        \"\"\"\n        # TODO(GH#23918): Remove this conditional for SeriesGroupBy when\n        #  GH#23918 is fixed\n        if freq is not None or axis != 0:\n            return self.apply(\n                lambda x: x.pct_change(\n                    periods=periods,\n                    fill_method=fill_method,\n                    limit=limit,\n                    freq=freq,\n                    axis=axis,\n                )\n            )\n        if fill_method is None:  # GH30463\n            fill_method = \"pad\"\n            limit = 0\n        filled = getattr(self, fill_method)(limit=limit)\n        fill_grp = filled.groupby(self.grouper.codes, axis=self.axis)\n        shifted = fill_grp.shift(periods=periods, freq=freq, axis=self.axis)\n        return (filled / shifted) - 1\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def head(self, n=5):\n        \"\"\"\n        Return first n rows of each group.\n\n        Similar to ``.apply(lambda x: x.head(n))``, but it returns a subset of rows\n        from the original DataFrame with original index and order preserved\n        (``as_index`` flag is ignored).\n\n        Parameters\n        ----------\n        n : int\n            If positive: number of entries to include from start of each group.\n            If negative: number of entries to exclude from end of each group.\n\n        Returns\n        -------\n        Series or DataFrame\n            Subset of original Series or DataFrame as determined by n.\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([[1, 2], [1, 4], [5, 6]],\n        ...                   columns=['A', 'B'])\n        >>> df.groupby('A').head(1)\n           A  B\n        0  1  2\n        2  5  6\n        >>> df.groupby('A').head(-1)\n           A  B\n        0  1  2\n        \"\"\"\n        self._reset_group_selection()\n        mask = self._make_mask_from_positional_indexer(slice(None, n))\n        return self._mask_selected_obj(mask)\n\n    @final\n    @Substitution(name=\"groupby\")\n    @Substitution(see_also=_common_see_also)\n    def tail(self, n=5):\n        \"\"\"\n        Return last n rows of each group.\n\n        Similar to ``.apply(lambda x: x.tail(n))``, but it returns a subset of rows\n        from the original DataFrame with original index and order preserved\n        (``as_index`` flag is ignored).\n\n        Parameters\n        ----------\n        n : int\n            If positive: number of entries to include from end of each group.\n            If negative: number of entries to exclude from start of each group.\n\n        Returns\n        -------\n        Series or DataFrame\n            Subset of original Series or DataFrame as determined by n.\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([['a', 1], ['a', 2], ['b', 1], ['b', 2]],\n        ...                   columns=['A', 'B'])\n        >>> df.groupby('A').tail(1)\n           A  B\n        1  a  2\n        3  b  2\n        >>> df.groupby('A').tail(-1)\n           A  B\n        1  a  2\n        3  b  2\n        \"\"\"\n        self._reset_group_selection()\n        if n:\n            mask = self._make_mask_from_positional_indexer(slice(-n, None))\n        else:\n            mask = self._make_mask_from_positional_indexer([])\n\n        return self._mask_selected_obj(mask)\n\n    @final\n    def _mask_selected_obj(self, mask: np.ndarray) -> NDFrameT:\n        \"\"\"\n        Return _selected_obj with mask applied to the correct axis.\n\n        Parameters\n        ----------\n        mask : np.ndarray\n            Boolean mask to apply.\n\n        Returns\n        -------\n        Series or DataFrame\n            Filtered _selected_obj.\n        \"\"\"\n        if self.axis == 0:\n            return self._selected_obj[mask]\n        else:\n            return self._selected_obj.iloc[:, mask]\n\n    @final\n    def _reindex_output(\n        self,\n        output: OutputFrameOrSeries,\n        fill_value: Scalar = np.NaN,\n        qs: npt.NDArray[np.float64] | None = None,\n    ) -> OutputFrameOrSeries:\n        \"\"\"\n        If we have categorical groupers, then we might want to make sure that\n        we have a fully re-indexed output to the levels. This means expanding\n        the output space to accommodate all values in the cartesian product of\n        our groups, regardless of whether they were observed in the data or\n        not. This will expand the output space if there are missing groups.\n\n        The method returns early without modifying the input if the number of\n        groupings is less than 2, self.observed == True or none of the groupers\n        are categorical.\n\n        Parameters\n        ----------\n        output : Series or DataFrame\n            Object resulting from grouping and applying an operation.\n        fill_value : scalar, default np.NaN\n            Value to use for unobserved categories if self.observed is False.\n        qs : np.ndarray[float64] or None, default None\n            quantile values, only relevant for quantile.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object (potentially) re-indexed to include all possible groups.\n        \"\"\"\n        groupings = self.grouper.groupings\n        if len(groupings) == 1:\n            return output\n\n        # if we only care about the observed values\n        # we are done\n        elif self.observed:\n            return output\n\n        # reindexing only applies to a Categorical grouper\n        elif not any(\n            isinstance(ping.grouping_vector, (Categorical, CategoricalIndex))\n            for ping in groupings\n        ):\n            return output\n\n        levels_list = [ping.group_index for ping in groupings]\n        names = self.grouper.names\n        if qs is not None:\n            # error: Argument 1 to \"append\" of \"list\" has incompatible type\n            # \"ndarray[Any, dtype[floating[_64Bit]]]\"; expected \"Index\"\n            levels_list.append(qs)  # type: ignore[arg-type]\n            names = names + [None]\n        index, _ = MultiIndex.from_product(levels_list, names=names).sortlevel()\n\n        if self.as_index:\n            d = {\n                self.obj._get_axis_name(self.axis): index,\n                \"copy\": False,\n                \"fill_value\": fill_value,\n            }\n            return output.reindex(**d)\n\n        # GH 13204\n        # Here, the categorical in-axis groupers, which need to be fully\n        # expanded, are columns in `output`. An idea is to do:\n        # output = output.set_index(self.grouper.names)\n        #                .reindex(index).reset_index()\n        # but special care has to be taken because of possible not-in-axis\n        # groupers.\n        # So, we manually select and drop the in-axis grouper columns,\n        # reindex `output`, and then reset the in-axis grouper columns.\n\n        # Select in-axis groupers\n        in_axis_grps = (\n            (i, ping.name) for (i, ping) in enumerate(groupings) if ping.in_axis\n        )\n        g_nums, g_names = zip(*in_axis_grps)\n\n        output = output.drop(labels=list(g_names), axis=1)\n\n        # Set a temp index and reindex (possibly expanding)\n        output = output.set_index(self.grouper.result_index).reindex(\n            index, copy=False, fill_value=fill_value\n        )\n\n        # Reset in-axis grouper columns\n        # (using level numbers `g_nums` because level names may not be unique)\n        output = output.reset_index(level=g_nums)\n\n        return output.reset_index(drop=True)\n\n    @final\n    def sample(\n        self,\n        n: int | None = None,\n        frac: float | None = None,\n        replace: bool = False,\n        weights: Sequence | Series | None = None,\n        random_state: RandomState | None = None,\n    ):\n        \"\"\"\n        Return a random sample of items from each group.\n\n        You can use `random_state` for reproducibility.\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        n : int, optional\n            Number of items to return for each group. Cannot be used with\n            `frac` and must be no larger than the smallest group unless\n            `replace` is True. Default is one if `frac` is None.\n        frac : float, optional\n            Fraction of items to return. Cannot be used with `n`.\n        replace : bool, default False\n            Allow or disallow sampling of the same row more than once.\n        weights : list-like, optional\n            Default None results in equal probability weighting.\n            If passed a list-like then values must have the same length as\n            the underlying DataFrame or Series object and will be used as\n            sampling probabilities after normalization within each group.\n            Values must be non-negative with at least one positive element\n            within each group.\n        random_state : int, array-like, BitGenerator, np.random.RandomState,\n            np.random.Generator, optional. If int, array-like, or BitGenerator, seed for\n            random number generator. If np.random.RandomState or np.random.Generator,\n            use as given.\n\n            .. versionchanged:: 1.4.0\n\n                np.random.Generator objects now accepted\n\n        Returns\n        -------\n        Series or DataFrame\n            A new object of same type as caller containing items randomly\n            sampled within each group from the caller object.\n\n        See Also\n        --------\n        DataFrame.sample: Generate random samples from a DataFrame object.\n        numpy.random.choice: Generate a random sample from a given 1-D numpy\n            array.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"a\": [\"red\"] * 2 + [\"blue\"] * 2 + [\"black\"] * 2, \"b\": range(6)}\n        ... )\n        >>> df\n               a  b\n        0    red  0\n        1    red  1\n        2   blue  2\n        3   blue  3\n        4  black  4\n        5  black  5\n\n        Select one row at random for each distinct value in column a. The\n        `random_state` argument can be used to guarantee reproducibility:\n\n        >>> df.groupby(\"a\").sample(n=1, random_state=1)\n               a  b\n        4  black  4\n        2   blue  2\n        1    red  1\n\n        Set `frac` to sample fixed proportions rather than counts:\n\n        >>> df.groupby(\"a\")[\"b\"].sample(frac=0.5, random_state=2)\n        5    5\n        2    2\n        0    0\n        Name: b, dtype: int64\n\n        Control sample probabilities within groups by setting weights:\n\n        >>> df.groupby(\"a\").sample(\n        ...     n=1,\n        ...     weights=[1, 1, 1, 0, 0, 1],\n        ...     random_state=1,\n        ... )\n               a  b\n        5  black  5\n        2   blue  2\n        0    red  0\n        \"\"\"\n        size = sample.process_sampling_size(n, frac, replace)\n        if weights is not None:\n            weights_arr = sample.preprocess_weights(\n                self._selected_obj, weights, axis=self.axis\n            )\n\n        random_state = com.random_state(random_state)\n\n        group_iterator = self.grouper.get_iterator(self._selected_obj, self.axis)\n\n        sampled_indices = []\n        for labels, obj in group_iterator:\n            grp_indices = self.indices[labels]\n            group_size = len(grp_indices)\n            if size is not None:\n                sample_size = size\n            else:\n                assert frac is not None\n                sample_size = round(frac * group_size)\n\n            grp_sample = sample.sample(\n                group_size,\n                size=sample_size,\n                replace=replace,\n                weights=None if weights is None else weights_arr[grp_indices],\n                random_state=random_state,\n            )\n            sampled_indices.append(grp_indices[grp_sample])\n\n        sampled_indices = np.concatenate(sampled_indices)\n        return self._selected_obj.take(sampled_indices, axis=self.axis)\n\n\n@doc(GroupBy)\ndef get_groupby(\n    obj: NDFrame,\n    by: _KeysArgType | None = None,\n    axis: int = 0,\n    level=None,\n    grouper: ops.BaseGrouper | None = None,\n    exclusions=None,\n    selection=None,\n    as_index: bool = True,\n    sort: bool = True,\n    group_keys: bool = True,\n    squeeze: bool = False,\n    observed: bool = False,\n    mutated: bool = False,\n    dropna: bool = True,\n) -> GroupBy:\n\n    klass: type[GroupBy]\n    if isinstance(obj, Series):\n        from pandas.core.groupby.generic import SeriesGroupBy\n\n        klass = SeriesGroupBy\n    elif isinstance(obj, DataFrame):\n        from pandas.core.groupby.generic import DataFrameGroupBy\n\n        klass = DataFrameGroupBy\n    else:  # pragma: no cover\n        raise TypeError(f\"invalid type: {obj}\")\n\n    return klass(\n        obj=obj,\n        keys=by,\n        axis=axis,\n        level=level,\n        grouper=grouper,\n        exclusions=exclusions,\n        selection=selection,\n        as_index=as_index,\n        sort=sort,\n        group_keys=group_keys,\n        squeeze=squeeze,\n        observed=observed,\n        mutated=mutated,\n        dropna=dropna,\n    )\n\n\ndef _insert_quantile_level(idx: Index, qs: npt.NDArray[np.float64]) -> MultiIndex:\n    \"\"\"\n    Insert the sequence 'qs' of quantiles as the inner-most level of a MultiIndex.\n\n    The quantile level in the MultiIndex is a repeated copy of 'qs'.\n\n    Parameters\n    ----------\n    idx : Index\n    qs : np.ndarray[float64]\n\n    Returns\n    -------\n    MultiIndex\n    \"\"\"\n    nqs = len(qs)\n\n    if idx._is_multi:\n        idx = cast(MultiIndex, idx)\n        lev_codes, lev = Index(qs).factorize()\n        levels = list(idx.levels) + [lev]\n        codes = [np.repeat(x, nqs) for x in idx.codes] + [np.tile(lev_codes, len(idx))]\n        mi = MultiIndex(levels=levels, codes=codes, names=idx.names + [None])\n    else:\n        mi = MultiIndex.from_product([idx, qs])\n    return mi\n\n\ndef warn_dropping_nuisance_columns_deprecated(cls, how: str) -> None:\n    warnings.warn(\n        \"Dropping invalid columns in \"\n        f\"{cls.__name__}.{how} is deprecated. \"\n        \"In a future version, a TypeError will be raised. \"\n        f\"Before calling .{how}, select only columns which \"\n        \"should be valid for the function.\",\n        FutureWarning,\n        stacklevel=find_stack_level(),\n    )\n"
    },
    {
      "filename": "pandas/core/series.py",
      "content": "\"\"\"\nData structure for 1-dimensional cross-sectional and time series data\n\"\"\"\nfrom __future__ import annotations\n\nfrom textwrap import dedent\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Hashable,\n    Iterable,\n    Literal,\n    Sequence,\n    Union,\n    cast,\n    overload,\n)\nimport warnings\nimport weakref\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import (\n    lib,\n    properties,\n    reshape,\n    tslibs,\n)\nfrom pandas._libs.lib import no_default\nfrom pandas._typing import (\n    AggFuncType,\n    ArrayLike,\n    Axis,\n    Dtype,\n    DtypeObj,\n    FillnaOptions,\n    IndexKeyFunc,\n    SingleManager,\n    StorageOptions,\n    TimedeltaConvertibleTypes,\n    TimestampConvertibleTypes,\n    ValueKeyFunc,\n    npt,\n)\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import InvalidIndexError\nfrom pandas.util._decorators import (\n    Appender,\n    Substitution,\n    deprecate_nonkeyword_arguments,\n    doc,\n)\nfrom pandas.util._exceptions import find_stack_level\nfrom pandas.util._validators import (\n    validate_ascending,\n    validate_bool_kwarg,\n    validate_percentile,\n)\n\nfrom pandas.core.dtypes.cast import (\n    convert_dtypes,\n    maybe_box_native,\n    maybe_cast_pointwise_result,\n    validate_numeric_casting,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_platform_int,\n    is_dict_like,\n    is_integer,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_scalar,\n    pandas_dtype,\n    validate_all_hashable,\n)\nfrom pandas.core.dtypes.generic import ABCDataFrame\nfrom pandas.core.dtypes.inference import is_hashable\nfrom pandas.core.dtypes.missing import (\n    isna,\n    na_value_for_dtype,\n    notna,\n    remove_na_arraylike,\n)\n\nfrom pandas.core import (\n    algorithms,\n    base,\n    generic,\n    missing,\n    nanops,\n    ops,\n)\nfrom pandas.core.accessor import CachedAccessor\nfrom pandas.core.apply import SeriesApply\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.arrays.categorical import CategoricalAccessor\nfrom pandas.core.arrays.sparse import SparseAccessor\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    create_series_with_explicit_dtype,\n    extract_array,\n    is_empty_data,\n    sanitize_array,\n)\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.indexers import (\n    deprecate_ndim_indexing,\n    unpack_1tuple,\n)\nfrom pandas.core.indexes.accessors import CombinedDatetimelikeProperties\nfrom pandas.core.indexes.api import (\n    CategoricalIndex,\n    DatetimeIndex,\n    Float64Index,\n    Index,\n    MultiIndex,\n    PeriodIndex,\n    TimedeltaIndex,\n    default_index,\n    ensure_index,\n)\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexing import check_bool_indexer\nfrom pandas.core.internals import (\n    SingleArrayManager,\n    SingleBlockManager,\n)\nfrom pandas.core.shared_docs import _shared_docs\nfrom pandas.core.sorting import (\n    ensure_key_mapped,\n    nargsort,\n)\nfrom pandas.core.strings import StringMethods\nfrom pandas.core.tools.datetimes import to_datetime\n\nimport pandas.io.formats.format as fmt\nimport pandas.plotting\n\nif TYPE_CHECKING:\n\n    from pandas._typing import (\n        NumpySorter,\n        NumpyValueArrayLike,\n    )\n\n    from pandas.core.frame import DataFrame\n    from pandas.core.groupby.generic import SeriesGroupBy\n    from pandas.core.resample import Resampler\n\n__all__ = [\"Series\"]\n\n_shared_doc_kwargs = {\n    \"axes\": \"index\",\n    \"klass\": \"Series\",\n    \"axes_single_arg\": \"{0 or 'index'}\",\n    \"axis\": \"\"\"axis : {0 or 'index'}\n        Parameter needed for compatibility with DataFrame.\"\"\",\n    \"inplace\": \"\"\"inplace : bool, default False\n        If True, performs operation inplace and returns None.\"\"\",\n    \"unique\": \"np.ndarray\",\n    \"duplicated\": \"Series\",\n    \"optional_by\": \"\",\n    \"optional_mapper\": \"\",\n    \"optional_labels\": \"\",\n    \"optional_axis\": \"\",\n    \"replace_iloc\": \"\"\"\n    This differs from updating with ``.loc`` or ``.iloc``, which require\n    you to specify a location to update with some value.\"\"\",\n}\n\n\ndef _coerce_method(converter):\n    \"\"\"\n    Install the scalar coercion methods.\n    \"\"\"\n\n    def wrapper(self):\n        if len(self) == 1:\n            return converter(self.iloc[0])\n        raise TypeError(f\"cannot convert the series to {converter}\")\n\n    wrapper.__name__ = f\"__{converter.__name__}__\"\n    return wrapper\n\n\n# ----------------------------------------------------------------------\n# Series class\n\n\nclass Series(base.IndexOpsMixin, generic.NDFrame):\n    \"\"\"\n    One-dimensional ndarray with axis labels (including time series).\n\n    Labels need not be unique but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for performing operations involving the index. Statistical\n    methods from ndarray have been overridden to automatically exclude\n    missing data (currently represented as NaN).\n\n    Operations between Series (+, -, /, \\\\*, \\\\*\\\\*) align values based on their\n    associated index values-- they need not be the same length. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Series. If data is a dict, argument order is\n        maintained.\n    index : array-like or Index (1d)\n        Values must be hashable and have the same length as `data`.\n        Non-unique index values are allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n        and index is None, then the keys in the data are used as the index. If the\n        index is not None, the resulting Series is reindexed with the index values.\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\n        Data type for the output Series. If not specified, this will be\n        inferred from `data`.\n        See the :ref:`user guide <basics.dtypes>` for more usages.\n    name : str, optional\n        The name to give to the Series.\n    copy : bool, default False\n        Copy input data. Only affects Series or 1d ndarray input. See examples.\n\n    Examples\n    --------\n    Constructing Series from a dictionary with an Index specified\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = pd.Series(data=d, index=['a', 'b', 'c'])\n    >>> ser\n    a   1\n    b   2\n    c   3\n    dtype: int64\n\n    The keys of the dictionary match with the Index values, hence the Index\n    values have no effect.\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = pd.Series(data=d, index=['x', 'y', 'z'])\n    >>> ser\n    x   NaN\n    y   NaN\n    z   NaN\n    dtype: float64\n\n    Note that the Index is first build with the keys from the dictionary.\n    After this the Series is reindexed with the given Index values, hence we\n    get all NaN as a result.\n\n    Constructing Series from a list with `copy=False`.\n\n    >>> r = [1, 2]\n    >>> ser = pd.Series(r, copy=False)\n    >>> ser.iloc[0] = 999\n    >>> r\n    [1, 2]\n    >>> ser\n    0    999\n    1      2\n    dtype: int64\n\n    Due to input data type the Series has a `copy` of\n    the original data even though `copy=False`, so\n    the data is unchanged.\n\n    Constructing Series from a 1d ndarray with `copy=False`.\n\n    >>> r = np.array([1, 2])\n    >>> ser = pd.Series(r, copy=False)\n    >>> ser.iloc[0] = 999\n    >>> r\n    array([999,   2])\n    >>> ser\n    0    999\n    1      2\n    dtype: int64\n\n    Due to input data type the Series has a `view` on\n    the original data, so\n    the data is changed as well.\n    \"\"\"\n\n    _typ = \"series\"\n    _HANDLED_TYPES = (Index, ExtensionArray, np.ndarray)\n\n    _name: Hashable\n    _metadata: list[str] = [\"name\"]\n    _internal_names_set = {\"index\"} | generic.NDFrame._internal_names_set\n    _accessors = {\"dt\", \"cat\", \"str\", \"sparse\"}\n    _hidden_attrs = (\n        base.IndexOpsMixin._hidden_attrs\n        | generic.NDFrame._hidden_attrs\n        | frozenset([\"compress\", \"ptp\"])\n    )\n\n    # Override cache_readonly bc Series is mutable\n    # error: Incompatible types in assignment (expression has type \"property\",\n    # base class \"IndexOpsMixin\" defined the type as \"Callable[[IndexOpsMixin], bool]\")\n    hasnans = property(  # type: ignore[assignment]\n        # error: \"Callable[[IndexOpsMixin], bool]\" has no attribute \"fget\"\n        base.IndexOpsMixin.hasnans.fget,  # type: ignore[attr-defined]\n        doc=base.IndexOpsMixin.hasnans.__doc__,\n    )\n    _mgr: SingleManager\n    div: Callable[[Series, Any], Series]\n    rdiv: Callable[[Series, Any], Series]\n\n    # ----------------------------------------------------------------------\n    # Constructors\n\n    def __init__(\n        self,\n        data=None,\n        index=None,\n        dtype: Dtype | None = None,\n        name=None,\n        copy: bool = False,\n        fastpath: bool = False,\n    ):\n\n        if (\n            isinstance(data, (SingleBlockManager, SingleArrayManager))\n            and index is None\n            and dtype is None\n            and copy is False\n        ):\n            # GH#33357 called with just the SingleBlockManager\n            NDFrame.__init__(self, data)\n            self.name = name\n            return\n\n        # we are called internally, so short-circuit\n        if fastpath:\n\n            # data is an ndarray, index is defined\n            if not isinstance(data, (SingleBlockManager, SingleArrayManager)):\n                manager = get_option(\"mode.data_manager\")\n                if manager == \"block\":\n                    data = SingleBlockManager.from_array(data, index)\n                elif manager == \"array\":\n                    data = SingleArrayManager.from_array(data, index)\n            if copy:\n                data = data.copy()\n            if index is None:\n                index = data.index\n\n        else:\n\n            name = ibase.maybe_extract_name(name, data, type(self))\n\n            if is_empty_data(data) and dtype is None:\n                # gh-17261\n                warnings.warn(\n                    \"The default dtype for empty Series will be 'object' instead \"\n                    \"of 'float64' in a future version. Specify a dtype explicitly \"\n                    \"to silence this warning.\",\n                    FutureWarning,\n                    stacklevel=find_stack_level(),\n                )\n                # uncomment the line below when removing the FutureWarning\n                # dtype = np.dtype(object)\n\n            if index is not None:\n                index = ensure_index(index)\n\n            if data is None:\n                data = {}\n            if dtype is not None:\n                dtype = self._validate_dtype(dtype)\n\n            if isinstance(data, MultiIndex):\n                raise NotImplementedError(\n                    \"initializing a Series from a MultiIndex is not supported\"\n                )\n            elif isinstance(data, Index):\n\n                if dtype is not None:\n                    # astype copies\n                    data = data.astype(dtype)\n                else:\n                    # GH#24096 we need to ensure the index remains immutable\n                    data = data._values.copy()\n                copy = False\n\n            elif isinstance(data, np.ndarray):\n                if len(data.dtype):\n                    # GH#13296 we are dealing with a compound dtype, which\n                    #  should be treated as 2D\n                    raise ValueError(\n                        \"Cannot construct a Series from an ndarray with \"\n                        \"compound dtype.  Use DataFrame instead.\"\n                    )\n            elif isinstance(data, Series):\n                if index is None:\n                    index = data.index\n                else:\n                    data = data.reindex(index, copy=copy)\n                    copy = False\n                data = data._mgr\n            elif is_dict_like(data):\n                data, index = self._init_dict(data, index, dtype)\n                dtype = None\n                copy = False\n            elif isinstance(data, (SingleBlockManager, SingleArrayManager)):\n                if index is None:\n                    index = data.index\n                elif not data.index.equals(index) or copy:\n                    # GH#19275 SingleBlockManager input should only be called\n                    # internally\n                    raise AssertionError(\n                        \"Cannot pass both SingleBlockManager \"\n                        \"`data` argument and a different \"\n                        \"`index` argument. `copy` must be False.\"\n                    )\n\n            elif isinstance(data, ExtensionArray):\n                pass\n            else:\n                data = com.maybe_iterable_to_list(data)\n\n            if index is None:\n                if not is_list_like(data):\n                    data = [data]\n                index = default_index(len(data))\n            elif is_list_like(data):\n                com.require_length_match(data, index)\n\n            # create/copy the manager\n            if isinstance(data, (SingleBlockManager, SingleArrayManager)):\n                if dtype is not None:\n                    data = data.astype(dtype=dtype, errors=\"ignore\", copy=copy)\n                elif copy:\n                    data = data.copy()\n            else:\n                data = sanitize_array(data, index, dtype, copy)\n\n                manager = get_option(\"mode.data_manager\")\n                if manager == \"block\":\n                    data = SingleBlockManager.from_array(data, index)\n                elif manager == \"array\":\n                    data = SingleArrayManager.from_array(data, index)\n\n        generic.NDFrame.__init__(self, data)\n        self.name = name\n        self._set_axis(0, index, fastpath=True)\n\n    def _init_dict(\n        self, data, index: Index | None = None, dtype: DtypeObj | None = None\n    ):\n        \"\"\"\n        Derive the \"_mgr\" and \"index\" attributes of a new Series from a\n        dictionary input.\n\n        Parameters\n        ----------\n        data : dict or dict-like\n            Data used to populate the new Series.\n        index : Index or None, default None\n            Index for the new Series: if None, use dict keys.\n        dtype : np.dtype, ExtensionDtype, or None, default None\n            The dtype for the new Series: if None, infer from data.\n\n        Returns\n        -------\n        _data : BlockManager for the new Series\n        index : index for the new Series\n        \"\"\"\n        keys: Index | tuple\n\n        # Looking for NaN in dict doesn't work ({np.nan : 1}[float('nan')]\n        # raises KeyError), so we iterate the entire dict, and align\n        if data:\n            # GH:34717, issue was using zip to extract key and values from data.\n            # using generators in effects the performance.\n            # Below is the new way of extracting the keys and values\n\n            keys = tuple(data.keys())\n            values = list(data.values())  # Generating list of values- faster way\n        elif index is not None:\n            # fastpath for Series(data=None). Just use broadcasting a scalar\n            # instead of reindexing.\n            values = na_value_for_dtype(pandas_dtype(dtype), compat=False)\n            keys = index\n        else:\n            keys, values = (), []\n\n        # Input is now list-like, so rely on \"standard\" construction:\n\n        # TODO: passing np.float64 to not break anything yet. See GH-17261\n        s = create_series_with_explicit_dtype(\n            # error: Argument \"index\" to \"create_series_with_explicit_dtype\" has\n            # incompatible type \"Tuple[Any, ...]\"; expected \"Union[ExtensionArray,\n            # ndarray, Index, None]\"\n            values,\n            index=keys,  # type: ignore[arg-type]\n            dtype=dtype,\n            dtype_if_empty=np.float64,\n        )\n\n        # Now we just make sure the order is respected, if any\n        if data and index is not None:\n            s = s.reindex(index, copy=False)\n        return s._mgr, s.index\n\n    # ----------------------------------------------------------------------\n\n    @property\n    def _constructor(self) -> type[Series]:\n        return Series\n\n    @property\n    def _constructor_expanddim(self) -> type[DataFrame]:\n        \"\"\"\n        Used when a manipulation result has one higher dimension as the\n        original, such as Series.to_frame()\n        \"\"\"\n        from pandas.core.frame import DataFrame\n\n        return DataFrame\n\n    # types\n    @property\n    def _can_hold_na(self) -> bool:\n        return self._mgr._can_hold_na\n\n    def _set_axis(self, axis: int, labels, fastpath: bool = False) -> None:\n        \"\"\"\n        Override generic, we want to set the _typ here.\n\n        This is called from the cython code when we set the `index` attribute\n        directly, e.g. `series.index = [1, 2, 3]`.\n        \"\"\"\n        if not fastpath:\n            labels = ensure_index(labels)\n\n        if labels._is_all_dates:\n            deep_labels = labels\n            if isinstance(labels, CategoricalIndex):\n                deep_labels = labels.categories\n\n            if not isinstance(\n                deep_labels, (DatetimeIndex, PeriodIndex, TimedeltaIndex)\n            ):\n                try:\n                    labels = DatetimeIndex(labels)\n                    # need to set here because we changed the index\n                    if fastpath:\n                        self._mgr.set_axis(axis, labels)\n                except (tslibs.OutOfBoundsDatetime, ValueError):\n                    # labels may exceeds datetime bounds,\n                    # or not be a DatetimeIndex\n                    pass\n\n        if not fastpath:\n            # The ensure_index call above ensures we have an Index object\n            self._mgr.set_axis(axis, labels)\n\n    # ndarray compatibility\n    @property\n    def dtype(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._mgr.dtype\n\n    @property\n    def dtypes(self) -> DtypeObj:\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        # DataFrame compatibility\n        return self.dtype\n\n    @property\n    def name(self) -> Hashable:\n        \"\"\"\n        Return the name of the Series.\n\n        The name of a Series becomes its index or column name if it is used\n        to form a DataFrame. It is also used whenever displaying the Series\n        using the interpreter.\n\n        Returns\n        -------\n        label (hashable object)\n            The name of the Series, also the column name if part of a DataFrame.\n\n        See Also\n        --------\n        Series.rename : Sets the Series name when given a scalar input.\n        Index.name : Corresponding Index property.\n\n        Examples\n        --------\n        The Series name can be set initially when calling the constructor.\n\n        >>> s = pd.Series([1, 2, 3], dtype=np.int64, name='Numbers')\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Numbers, dtype: int64\n        >>> s.name = \"Integers\"\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: Integers, dtype: int64\n\n        The name of a Series within a DataFrame is its column name.\n\n        >>> df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],\n        ...                   columns=[\"Odd Numbers\", \"Even Numbers\"])\n        >>> df\n           Odd Numbers  Even Numbers\n        0            1             2\n        1            3             4\n        2            5             6\n        >>> df[\"Even Numbers\"].name\n        'Even Numbers'\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value: Hashable) -> None:\n        validate_all_hashable(value, error_name=f\"{type(self).__name__}.name\")\n        object.__setattr__(self, \"_name\", value)\n\n    @property\n    def values(self):\n        \"\"\"\n        Return Series as ndarray or ndarray-like depending on the dtype.\n\n        .. warning::\n\n           We recommend using :attr:`Series.array` or\n           :meth:`Series.to_numpy`, depending on whether you need\n           a reference to the underlying data or a NumPy array.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n\n        See Also\n        --------\n        Series.array : Reference to the underlying data.\n        Series.to_numpy : A NumPy array representing the underlying data.\n\n        Examples\n        --------\n        >>> pd.Series([1, 2, 3]).values\n        array([1, 2, 3])\n\n        >>> pd.Series(list('aabc')).values\n        array(['a', 'a', 'b', 'c'], dtype=object)\n\n        >>> pd.Series(list('aabc')).astype('category').values\n        ['a', 'a', 'b', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n\n        Timezone aware datetime data is converted to UTC:\n\n        >>> pd.Series(pd.date_range('20130101', periods=3,\n        ...                         tz='US/Eastern')).values\n        array(['2013-01-01T05:00:00.000000000',\n               '2013-01-02T05:00:00.000000000',\n               '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')\n        \"\"\"\n        return self._mgr.external_values()\n\n    @property\n    def _values(self):\n        \"\"\"\n        Return the internal repr of this data (defined by Block.interval_values).\n        This are the values as stored in the Block (ndarray or ExtensionArray\n        depending on the Block class), with datetime64[ns] and timedelta64[ns]\n        wrapped in ExtensionArrays to match Index._values behavior.\n\n        Differs from the public ``.values`` for certain data types, because of\n        historical backwards compatibility of the public attribute (e.g. period\n        returns object ndarray and datetimetz a datetime64[ns] ndarray for\n        ``.values`` while it returns an ExtensionArray for ``._values`` in those\n        cases).\n\n        Differs from ``.array`` in that this still returns the numpy array if\n        the Block is backed by a numpy array (except for datetime64 and\n        timedelta64 dtypes), while ``.array`` ensures to always return an\n        ExtensionArray.\n\n        Overview:\n\n        dtype       | values        | _values       | array         |\n        ----------- | ------------- | ------------- | ------------- |\n        Numeric     | ndarray       | ndarray       | PandasArray   |\n        Category    | Categorical   | Categorical   | Categorical   |\n        dt64[ns]    | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        dt64[ns tz] | ndarray[M8ns] | DatetimeArray | DatetimeArray |\n        td64[ns]    | ndarray[m8ns] | TimedeltaArray| ndarray[m8ns] |\n        Period      | ndarray[obj]  | PeriodArray   | PeriodArray   |\n        Nullable    | EA            | EA            | EA            |\n\n        \"\"\"\n        return self._mgr.internal_values()\n\n    # error: Decorated property not supported\n    @Appender(base.IndexOpsMixin.array.__doc__)  # type: ignore[misc]\n    @property\n    def array(self) -> ExtensionArray:\n        return self._mgr.array_values()\n\n    # ops\n    def ravel(self, order=\"C\"):\n        \"\"\"\n        Return the flattened underlying data as an ndarray.\n\n        Returns\n        -------\n        numpy.ndarray or ndarray-like\n            Flattened data of the Series.\n\n        See Also\n        --------\n        numpy.ndarray.ravel : Return a flattened array.\n        \"\"\"\n        return self._values.ravel(order=order)\n\n    def __len__(self) -> int:\n        \"\"\"\n        Return the length of the Series.\n        \"\"\"\n        return len(self._mgr)\n\n    def view(self, dtype: Dtype | None = None) -> Series:\n        \"\"\"\n        Create a new view of the Series.\n\n        This function will return a new Series with a view of the same\n        underlying values in memory, optionally reinterpreted with a new data\n        type. The new data type must preserve the same size in bytes as to not\n        cause index misalignment.\n\n        Parameters\n        ----------\n        dtype : data type\n            Data type object or one of their string representations.\n\n        Returns\n        -------\n        Series\n            A new Series object as a view of the same data in memory.\n\n        See Also\n        --------\n        numpy.ndarray.view : Equivalent numpy function to create a new view of\n            the same data in memory.\n\n        Notes\n        -----\n        Series are instantiated with ``dtype=float64`` by default. While\n        ``numpy.ndarray.view()`` will return a view with the same data type as\n        the original array, ``Series.view()`` (without specified dtype)\n        will try using ``float64`` and may fail if the original data type size\n        in bytes is not the same.\n\n        Examples\n        --------\n        >>> s = pd.Series([-2, -1, 0, 1, 2], dtype='int8')\n        >>> s\n        0   -2\n        1   -1\n        2    0\n        3    1\n        4    2\n        dtype: int8\n\n        The 8 bit signed integer representation of `-1` is `0b11111111`, but\n        the same bytes represent 255 if read as an 8 bit unsigned integer:\n\n        >>> us = s.view('uint8')\n        >>> us\n        0    254\n        1    255\n        2      0\n        3      1\n        4      2\n        dtype: uint8\n\n        The views share the same underlying values:\n\n        >>> us[0] = 128\n        >>> s\n        0   -128\n        1     -1\n        2      0\n        3      1\n        4      2\n        dtype: int8\n        \"\"\"\n        # self.array instead of self._values so we piggyback on PandasArray\n        #  implementation\n        res_values = self.array.view(dtype)\n        res_ser = self._constructor(res_values, index=self.index)\n        return res_ser.__finalize__(self, method=\"view\")\n\n    # ----------------------------------------------------------------------\n    # NDArray Compat\n    _HANDLED_TYPES = (Index, ExtensionArray, np.ndarray)\n\n    def __array__(self, dtype: npt.DTypeLike | None = None) -> np.ndarray:\n        \"\"\"\n        Return the values as a NumPy array.\n\n        Users should not call this directly. Rather, it is invoked by\n        :func:`numpy.array` and :func:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to use for the resulting NumPy array. By default,\n            the dtype is inferred from the data.\n\n        Returns\n        -------\n        numpy.ndarray\n            The values in the series converted to a :class:`numpy.ndarray`\n            with the specified `dtype`.\n\n        See Also\n        --------\n        array : Create a new array from data.\n        Series.array : Zero-copy view to the array backing the Series.\n        Series.to_numpy : Series method for similar behavior.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1, 2, 3])\n        >>> np.asarray(ser)\n        array([1, 2, 3])\n\n        For timezone-aware data, the timezones may be retained with\n        ``dtype='object'``\n\n        >>> tzser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\n        >>> np.asarray(tzser, dtype=\"object\")\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\n              dtype=object)\n\n        Or the values may be localized to UTC and the tzinfo discarded with\n        ``dtype='datetime64[ns]'``\n\n        >>> np.asarray(tzser, dtype=\"datetime64[ns]\")  # doctest: +ELLIPSIS\n        array(['1999-12-31T23:00:00.000000000', ...],\n              dtype='datetime64[ns]')\n        \"\"\"\n        return np.asarray(self._values, dtype)\n\n    # ----------------------------------------------------------------------\n    # Unary Methods\n\n    # coercion\n    __float__ = _coerce_method(float)\n    __long__ = _coerce_method(int)\n    __int__ = _coerce_method(int)\n\n    # ----------------------------------------------------------------------\n\n    # indexers\n    @property\n    def axes(self) -> list[Index]:\n        \"\"\"\n        Return a list of the row axis labels.\n        \"\"\"\n        return [self.index]\n\n    # ----------------------------------------------------------------------\n    # Indexing Methods\n\n    @Appender(generic.NDFrame.take.__doc__)\n    def take(self, indices, axis=0, is_copy=None, **kwargs) -> Series:\n        if is_copy is not None:\n            warnings.warn(\n                \"is_copy is deprecated and will be removed in a future version. \"\n                \"'take' always returns a copy, so there is no need to specify this.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n        nv.validate_take((), kwargs)\n\n        indices = ensure_platform_int(indices)\n        new_index = self.index.take(indices)\n        new_values = self._values.take(indices)\n\n        result = self._constructor(new_values, index=new_index, fastpath=True)\n        return result.__finalize__(self, method=\"take\")\n\n    def _take_with_is_copy(self, indices, axis=0) -> Series:\n        \"\"\"\n        Internal version of the `take` method that sets the `_is_copy`\n        attribute to keep track of the parent dataframe (using in indexing\n        for the SettingWithCopyWarning). For Series this does the same\n        as the public take (it never sets `_is_copy`).\n\n        See the docstring of `take` for full explanation of the parameters.\n        \"\"\"\n        return self.take(indices=indices, axis=axis)\n\n    def _ixs(self, i: int, axis: int = 0):\n        \"\"\"\n        Return the i-th value or values in the Series by location.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        scalar (int) or Series (slice, sequence)\n        \"\"\"\n        return self._values[i]\n\n    def _slice(self, slobj: slice, axis: int = 0) -> Series:\n        # axis kwarg is retained for compat with NDFrame method\n        #  _slice is *always* positional\n        return self._get_values(slobj)\n\n    def __getitem__(self, key):\n        key = com.apply_if_callable(key, self)\n\n        if key is Ellipsis:\n            return self\n\n        key_is_scalar = is_scalar(key)\n        if isinstance(key, (list, tuple)):\n            key = unpack_1tuple(key)\n\n        if is_integer(key) and self.index._should_fallback_to_positional:\n            return self._values[key]\n\n        elif key_is_scalar:\n            return self._get_value(key)\n\n        if is_hashable(key):\n            # Otherwise index.get_value will raise InvalidIndexError\n            try:\n                # For labels that don't resolve as scalars like tuples and frozensets\n                result = self._get_value(key)\n\n                return result\n\n            except (KeyError, TypeError):\n                if isinstance(key, tuple) and isinstance(self.index, MultiIndex):\n                    # We still have the corner case where a tuple is a key\n                    # in the first level of our MultiIndex\n                    return self._get_values_tuple(key)\n\n        if is_iterator(key):\n            key = list(key)\n\n        if com.is_bool_indexer(key):\n            key = check_bool_indexer(self.index, key)\n            key = np.asarray(key, dtype=bool)\n            return self._get_values(key)\n\n        return self._get_with(key)\n\n    def _get_with(self, key):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            # _convert_slice_indexer to determine if this slice is positional\n            #  or label based, and if the latter, convert to positional\n            slobj = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            return self._slice(slobj)\n        elif isinstance(key, ABCDataFrame):\n            raise TypeError(\n                \"Indexing a Series with DataFrame is not \"\n                \"supported, use the appropriate DataFrame column\"\n            )\n        elif isinstance(key, tuple):\n            return self._get_values_tuple(key)\n\n        elif not is_list_like(key):\n            # e.g. scalars that aren't recognized by lib.is_scalar, GH#32684\n            return self.loc[key]\n\n        if not isinstance(key, (list, np.ndarray, ExtensionArray, Series, Index)):\n            key = list(key)\n\n        if isinstance(key, Index):\n            key_type = key.inferred_type\n        else:\n            key_type = lib.infer_dtype(key, skipna=False)\n\n        # Note: The key_type == \"boolean\" case should be caught by the\n        #  com.is_bool_indexer check in __getitem__\n        if key_type == \"integer\":\n            # We need to decide whether to treat this as a positional indexer\n            #  (i.e. self.iloc) or label-based (i.e. self.loc)\n            if not self.index._should_fallback_to_positional:\n                return self.loc[key]\n            else:\n                return self.iloc[key]\n\n        # handle the dup indexing case GH#4246\n        return self.loc[key]\n\n    def _get_values_tuple(self, key):\n        # mpl hackaround\n        if com.any_none(*key):\n            result = self._get_values(key)\n            deprecate_ndim_indexing(result, stacklevel=find_stack_level())\n            return result\n\n        if not isinstance(self.index, MultiIndex):\n            raise KeyError(\"key of type tuple not found and not a MultiIndex\")\n\n        # If key is contained, would have returned by now\n        indexer, new_index = self.index.get_loc_level(key)\n        return self._constructor(self._values[indexer], index=new_index).__finalize__(\n            self\n        )\n\n    def _get_values(self, indexer):\n        try:\n            new_mgr = self._mgr.getitem_mgr(indexer)\n            return self._constructor(new_mgr).__finalize__(self)\n        except ValueError:\n            # mpl compat if we look up e.g. ser[:, np.newaxis];\n            #  see tests.series.timeseries.test_mpl_compat_hack\n            # the asarray is needed to avoid returning a 2D DatetimeArray\n            return np.asarray(self._values[indexer])\n\n    def _get_value(self, label, takeable: bool = False):\n        \"\"\"\n        Quickly retrieve single value at passed index label.\n\n        Parameters\n        ----------\n        label : object\n        takeable : interpret the index as indexers, default False\n\n        Returns\n        -------\n        scalar value\n        \"\"\"\n        if takeable:\n            return self._values[label]\n\n        # Similar to Index.get_value, but we do not fall back to positional\n        loc = self.index.get_loc(label)\n        return self.index._get_values_for_loc(self, loc, label)\n\n    def __setitem__(self, key, value) -> None:\n        key = com.apply_if_callable(key, self)\n        cacher_needs_updating = self._check_is_chained_assignment_possible()\n\n        if key is Ellipsis:\n            key = slice(None)\n\n        if isinstance(key, slice):\n            indexer = self.index._convert_slice_indexer(key, kind=\"getitem\")\n            return self._set_values(indexer, value)\n\n        try:\n            self._set_with_engine(key, value)\n        except (KeyError, ValueError):\n            if is_integer(key) and self.index.inferred_type != \"integer\":\n                # positional setter\n                if not self.index._should_fallback_to_positional:\n                    # GH#33469\n                    warnings.warn(\n                        \"Treating integers as positional in Series.__setitem__ \"\n                        \"with a Float64Index is deprecated. In a future version, \"\n                        \"`series[an_int] = val` will insert a new key into the \"\n                        \"Series. Use `series.iloc[an_int] = val` to treat the \"\n                        \"key as positional.\",\n                        FutureWarning,\n                        stacklevel=find_stack_level(),\n                    )\n                # this is equivalent to self._values[key] = value\n                self._mgr.setitem_inplace(key, value)\n            else:\n                # GH#12862 adding a new key to the Series\n                self.loc[key] = value\n\n        except (InvalidIndexError, TypeError) as err:\n            if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):\n                # cases with MultiIndex don't get here bc they raise KeyError\n                raise KeyError(\n                    \"key of type tuple not found and not a MultiIndex\"\n                ) from err\n\n            if com.is_bool_indexer(key):\n                key = check_bool_indexer(self.index, key)\n                key = np.asarray(key, dtype=bool)\n\n                if (\n                    is_list_like(value)\n                    and len(value) != len(self)\n                    and not isinstance(value, Series)\n                    and not is_object_dtype(self.dtype)\n                ):\n                    # Series will be reindexed to have matching length inside\n                    #  _where call below\n                    # GH#44265\n                    indexer = key.nonzero()[0]\n                    self._set_values(indexer, value)\n                    return\n\n                # otherwise with listlike other we interpret series[mask] = other\n                #  as series[mask] = other[mask]\n                try:\n                    self._where(~key, value, inplace=True)\n                except InvalidIndexError:\n                    # test_where_dups\n                    self.iloc[key] = value\n                return\n\n            else:\n                self._set_with(key, value)\n\n        if cacher_needs_updating:\n            self._maybe_update_cacher()\n\n    def _set_with_engine(self, key, value) -> None:\n        loc = self.index.get_loc(key)\n        # error: Argument 1 to \"validate_numeric_casting\" has incompatible type\n        # \"Union[dtype, ExtensionDtype]\"; expected \"dtype\"\n        validate_numeric_casting(self.dtype, value)  # type: ignore[arg-type]\n        # this is equivalent to self._values[key] = value\n        self._mgr.setitem_inplace(loc, value)\n\n    def _set_with(self, key, value):\n        # other: fancy integer or otherwise\n        assert not isinstance(key, tuple)\n\n        if is_scalar(key):\n            key = [key]\n        elif is_iterator(key):\n            # Without this, the call to infer_dtype will consume the generator\n            key = list(key)\n\n        key_type = lib.infer_dtype(key, skipna=False)\n\n        # Note: key_type == \"boolean\" should not occur because that\n        #  should be caught by the is_bool_indexer check in __setitem__\n        if key_type == \"integer\":\n            if not self.index._should_fallback_to_positional:\n                self._set_labels(key, value)\n            else:\n                self._set_values(key, value)\n        else:\n            self.loc[key] = value\n\n    def _set_labels(self, key, value) -> None:\n        key = com.asarray_tuplesafe(key)\n        indexer: np.ndarray = self.index.get_indexer(key)\n        mask = indexer == -1\n        if mask.any():\n            raise KeyError(f\"{key[mask]} not in index\")\n        self._set_values(indexer, value)\n\n    def _set_values(self, key, value) -> None:\n        if isinstance(key, (Index, Series)):\n            key = key._values\n\n        self._mgr = self._mgr.setitem(indexer=key, value=value)\n        self._maybe_update_cacher()\n\n    def _set_value(self, label, value, takeable: bool = False):\n        \"\"\"\n        Quickly set single value at passed label.\n\n        If label is not contained, a new object is created with the label\n        placed at the end of the result index.\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed.\n        value : object\n            Scalar value.\n        takeable : interpret the index as indexers, default False\n        \"\"\"\n        if not takeable:\n            try:\n                loc = self.index.get_loc(label)\n            except KeyError:\n                # set using a non-recursive method\n                self.loc[label] = value\n                return\n        else:\n            loc = label\n\n        self._set_values(loc, value)\n\n    # ----------------------------------------------------------------------\n    # Lookup Caching\n\n    @property\n    def _is_cached(self) -> bool:\n        \"\"\"Return boolean indicating if self is cached or not.\"\"\"\n        return getattr(self, \"_cacher\", None) is not None\n\n    def _get_cacher(self):\n        \"\"\"return my cacher or None\"\"\"\n        cacher = getattr(self, \"_cacher\", None)\n        if cacher is not None:\n            cacher = cacher[1]()\n        return cacher\n\n    def _reset_cacher(self) -> None:\n        \"\"\"\n        Reset the cacher.\n        \"\"\"\n        if hasattr(self, \"_cacher\"):\n            # should only get here with self.ndim == 1\n            del self._cacher\n\n    def _set_as_cached(self, item, cacher) -> None:\n        \"\"\"\n        Set the _cacher attribute on the calling object with a weakref to\n        cacher.\n        \"\"\"\n        self._cacher = (item, weakref.ref(cacher))\n\n    def _clear_item_cache(self) -> None:\n        # no-op for Series\n        pass\n\n    def _check_is_chained_assignment_possible(self) -> bool:\n        \"\"\"\n        See NDFrame._check_is_chained_assignment_possible.__doc__\n        \"\"\"\n        if self._is_view and self._is_cached:\n            ref = self._get_cacher()\n            if ref is not None and ref._is_mixed_type:\n                self._check_setitem_copy(t=\"referent\", force=True)\n            return True\n        return super()._check_is_chained_assignment_possible()\n\n    def _maybe_update_cacher(\n        self, clear: bool = False, verify_is_copy: bool = True, inplace: bool = False\n    ) -> None:\n        \"\"\"\n        See NDFrame._maybe_update_cacher.__doc__\n        \"\"\"\n        cacher = getattr(self, \"_cacher\", None)\n        if cacher is not None:\n            assert self.ndim == 1\n            ref: DataFrame = cacher[1]()\n\n            # we are trying to reference a dead referent, hence\n            # a copy\n            if ref is None:\n                del self._cacher\n            elif len(self) == len(ref) and self.name in ref.columns:\n                # GH#42530 self.name must be in ref.columns\n                # to ensure column still in dataframe\n                # otherwise, either self or ref has swapped in new arrays\n                ref._maybe_cache_changed(cacher[0], self, inplace=inplace)\n            else:\n                # GH#33675 we have swapped in a new array, so parent\n                #  reference to self is now invalid\n                ref._item_cache.pop(cacher[0], None)\n\n        super()._maybe_update_cacher(\n            clear=clear, verify_is_copy=verify_is_copy, inplace=inplace\n        )\n\n    # ----------------------------------------------------------------------\n    # Unsorted\n\n    @property\n    def _is_mixed_type(self):\n        return False\n\n    def repeat(self, repeats, axis=None) -> Series:\n        \"\"\"\n        Repeat elements of a Series.\n\n        Returns a new Series where each element of the current Series\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            Series.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        Series\n            Newly created Series with repeated elements.\n\n        See Also\n        --------\n        Index.repeat : Equivalent function for Index.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s\n        0    a\n        1    b\n        2    c\n        dtype: object\n        >>> s.repeat(2)\n        0    a\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        dtype: object\n        >>> s.repeat([1, 2, 3])\n        0    a\n        1    b\n        1    b\n        2    c\n        2    c\n        2    c\n        dtype: object\n        \"\"\"\n        nv.validate_repeat((), {\"axis\": axis})\n        new_index = self.index.repeat(repeats)\n        new_values = self._values.repeat(repeats)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"repeat\"\n        )\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"level\"])\n    def reset_index(self, level=None, drop=False, name=lib.no_default, inplace=False):\n        \"\"\"\n        Generate a new DataFrame or Series with the index reset.\n\n        This is useful when the index needs to be treated as a column, or\n        when the index is meaningless and needs to be reset to the default\n        before another operation.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default optional\n            For a Series with a MultiIndex, only remove the specified levels\n            from the index. Removes all levels by default.\n        drop : bool, default False\n            Just reset the index, without inserting it as a column in\n            the new DataFrame.\n        name : object, optional\n            The name to use for the column containing the original Series\n            values. Uses ``self.name`` by default. This argument is ignored\n            when `drop` is True.\n        inplace : bool, default False\n            Modify the Series in place (do not create a new object).\n\n        Returns\n        -------\n        Series or DataFrame or None\n            When `drop` is False (the default), a DataFrame is returned.\n            The newly created columns will come first in the DataFrame,\n            followed by the original Series values.\n            When `drop` is True, a `Series` is returned.\n            In either case, if ``inplace=True``, no value is returned.\n\n        See Also\n        --------\n        DataFrame.reset_index: Analogous function for DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4], name='foo',\n        ...               index=pd.Index(['a', 'b', 'c', 'd'], name='idx'))\n\n        Generate a DataFrame with default index.\n\n        >>> s.reset_index()\n          idx  foo\n        0   a    1\n        1   b    2\n        2   c    3\n        3   d    4\n\n        To specify the name of the new column use `name`.\n\n        >>> s.reset_index(name='values')\n          idx  values\n        0   a       1\n        1   b       2\n        2   c       3\n        3   d       4\n\n        To generate a new Series with the default set `drop` to True.\n\n        >>> s.reset_index(drop=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        To update the Series in place, without generating a new one\n        set `inplace` to True. Note that it also requires ``drop=True``.\n\n        >>> s.reset_index(inplace=True, drop=True)\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        The `level` parameter is interesting for Series with a multi-level\n        index.\n\n        >>> arrays = [np.array(['bar', 'bar', 'baz', 'baz']),\n        ...           np.array(['one', 'two', 'one', 'two'])]\n        >>> s2 = pd.Series(\n        ...     range(4), name='foo',\n        ...     index=pd.MultiIndex.from_arrays(arrays,\n        ...                                     names=['a', 'b']))\n\n        To remove a specific level from the Index, use `level`.\n\n        >>> s2.reset_index(level='a')\n               a  foo\n        b\n        one  bar    0\n        two  bar    1\n        one  baz    2\n        two  baz    3\n\n        If `level` is not set, all levels are removed from the Index.\n\n        >>> s2.reset_index()\n             a    b  foo\n        0  bar  one    0\n        1  bar  two    1\n        2  baz  one    2\n        3  baz  two    3\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if drop:\n            new_index = default_index(len(self))\n            if level is not None:\n                if not isinstance(level, (tuple, list)):\n                    level = [level]\n                level = [self.index._get_level_number(lev) for lev in level]\n                if len(level) < self.index.nlevels:\n                    new_index = self.index.droplevel(level)\n\n            if inplace:\n                self.index = new_index\n            else:\n                return self._constructor(\n                    self._values.copy(), index=new_index\n                ).__finalize__(self, method=\"reset_index\")\n        elif inplace:\n            raise TypeError(\n                \"Cannot reset_index inplace on a Series to create a DataFrame\"\n            )\n        else:\n            if name is lib.no_default:\n                # For backwards compatibility, keep columns as [0] instead of\n                #  [None] when self.name is None\n                if self.name is None:\n                    name = 0\n                else:\n                    name = self.name\n\n            df = self.to_frame(name)\n            return df.reset_index(level=level, drop=drop)\n\n    # ----------------------------------------------------------------------\n    # Rendering Methods\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return a string representation for a particular Series.\n        \"\"\"\n        repr_params = fmt.get_series_repr_params()\n        return self.to_string(**repr_params)\n\n    def to_string(\n        self,\n        buf=None,\n        na_rep=\"NaN\",\n        float_format=None,\n        header=True,\n        index=True,\n        length=False,\n        dtype=False,\n        name=False,\n        max_rows=None,\n        min_rows=None,\n    ):\n        \"\"\"\n        Render a string representation of the Series.\n\n        Parameters\n        ----------\n        buf : StringIO-like, optional\n            Buffer to write to.\n        na_rep : str, optional\n            String representation of NaN to use, default 'NaN'.\n        float_format : one-parameter function, optional\n            Formatter function to apply to columns' elements if they are\n            floats, default None.\n        header : bool, default True\n            Add the Series header (index name).\n        index : bool, optional\n            Add index (row) labels, default True.\n        length : bool, default False\n            Add the Series length.\n        dtype : bool, default False\n            Add the Series dtype.\n        name : bool, default False\n            Add the Series name if not None.\n        max_rows : int, optional\n            Maximum number of rows to show before truncating. If None, show\n            all.\n        min_rows : int, optional\n            The number of rows to display in a truncated repr (when number\n            of rows is above `max_rows`).\n\n        Returns\n        -------\n        str or None\n            String representation of Series if ``buf=None``, otherwise None.\n        \"\"\"\n        formatter = fmt.SeriesFormatter(\n            self,\n            name=name,\n            length=length,\n            header=header,\n            index=index,\n            dtype=dtype,\n            na_rep=na_rep,\n            float_format=float_format,\n            min_rows=min_rows,\n            max_rows=max_rows,\n        )\n        result = formatter.to_string()\n\n        # catch contract violations\n        if not isinstance(result, str):\n            raise AssertionError(\n                \"result must be of type str, type \"\n                f\"of result is {repr(type(result).__name__)}\"\n            )\n\n        if buf is None:\n            return result\n        else:\n            try:\n                buf.write(result)\n            except AttributeError:\n                with open(buf, \"w\") as f:\n                    f.write(result)\n\n    @doc(\n        klass=_shared_doc_kwargs[\"klass\"],\n        storage_options=generic._shared_docs[\"storage_options\"],\n        examples=dedent(\n            \"\"\"Examples\n            --------\n            >>> s = pd.Series([\"elk\", \"pig\", \"dog\", \"quetzal\"], name=\"animal\")\n            >>> print(s.to_markdown())\n            |    | animal   |\n            |---:|:---------|\n            |  0 | elk      |\n            |  1 | pig      |\n            |  2 | dog      |\n            |  3 | quetzal  |\n\n            Output markdown with a tabulate option.\n\n            >>> print(s.to_markdown(tablefmt=\"grid\"))\n            +----+----------+\n            |    | animal   |\n            +====+==========+\n            |  0 | elk      |\n            +----+----------+\n            |  1 | pig      |\n            +----+----------+\n            |  2 | dog      |\n            +----+----------+\n            |  3 | quetzal  |\n            +----+----------+\"\"\"\n        ),\n    )\n    def to_markdown(\n        self,\n        buf: IO[str] | None = None,\n        mode: str = \"wt\",\n        index: bool = True,\n        storage_options: StorageOptions = None,\n        **kwargs,\n    ) -> str | None:\n        \"\"\"\n        Print {klass} in Markdown-friendly format.\n\n        .. versionadded:: 1.0.0\n\n        Parameters\n        ----------\n        buf : str, Path or StringIO-like, optional, default None\n            Buffer to write to. If None, the output is returned as a string.\n        mode : str, optional\n            Mode in which file is opened, \"wt\" by default.\n        index : bool, optional, default True\n            Add index (row) labels.\n\n            .. versionadded:: 1.1.0\n        {storage_options}\n\n            .. versionadded:: 1.2.0\n\n        **kwargs\n            These parameters will be passed to `tabulate \\\n                <https://pypi.org/project/tabulate>`_.\n\n        Returns\n        -------\n        str\n            {klass} in Markdown-friendly format.\n\n        Notes\n        -----\n        Requires the `tabulate <https://pypi.org/project/tabulate>`_ package.\n\n        {examples}\n        \"\"\"\n        return self.to_frame().to_markdown(\n            buf, mode, index, storage_options=storage_options, **kwargs\n        )\n\n    # ----------------------------------------------------------------------\n\n    def items(self) -> Iterable[tuple[Hashable, Any]]:\n        \"\"\"\n        Lazily iterate over (index, value) tuples.\n\n        This method returns an iterable tuple (index, value). This is\n        convenient if you want to create a lazy iterator.\n\n        Returns\n        -------\n        iterable\n            Iterable of tuples containing the (index, value) pairs from a\n            Series.\n\n        See Also\n        --------\n        DataFrame.items : Iterate over (column name, Series) pairs.\n        DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs.\n\n        Examples\n        --------\n        >>> s = pd.Series(['A', 'B', 'C'])\n        >>> for index, value in s.items():\n        ...     print(f\"Index : {index}, Value : {value}\")\n        Index : 0, Value : A\n        Index : 1, Value : B\n        Index : 2, Value : C\n        \"\"\"\n        return zip(iter(self.index), iter(self))\n\n    @Appender(items.__doc__)\n    def iteritems(self) -> Iterable[tuple[Hashable, Any]]:\n        return self.items()\n\n    # ----------------------------------------------------------------------\n    # Misc public methods\n\n    def keys(self) -> Index:\n        \"\"\"\n        Return alias for index.\n\n        Returns\n        -------\n        Index\n            Index of the Series.\n        \"\"\"\n        return self.index\n\n    def to_dict(self, into=dict):\n        \"\"\"\n        Convert Series to {label -> value} dict or dict-like object.\n\n        Parameters\n        ----------\n        into : class, default dict\n            The collections.abc.Mapping subclass to use as the return\n            object. Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Key-value representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.to_dict()\n        {0: 1, 1: 2, 2: 3, 3: 4}\n        >>> from collections import OrderedDict, defaultdict\n        >>> s.to_dict(OrderedDict)\n        OrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])\n        >>> dd = defaultdict(list)\n        >>> s.to_dict(dd)\n        defaultdict(<class 'list'>, {0: 1, 1: 2, 2: 3, 3: 4})\n        \"\"\"\n        # GH16122\n        into_c = com.standardize_mapping(into)\n        return into_c((k, maybe_box_native(v)) for k, v in self.items())\n\n    def to_frame(self, name: Hashable = lib.no_default) -> DataFrame:\n        \"\"\"\n        Convert Series to DataFrame.\n\n        Parameters\n        ----------\n        name : object, default None\n            The passed name should substitute for the series name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([\"a\", \"b\", \"c\"],\n        ...               name=\"vals\")\n        >>> s.to_frame()\n          vals\n        0    a\n        1    b\n        2    c\n        \"\"\"\n        columns: Index\n        if name is lib.no_default:\n            name = self.name\n            if name is None:\n                # default to [0], same as we would get with DataFrame(self)\n                columns = default_index(1)\n            else:\n                columns = Index([name])\n        else:\n            columns = Index([name])\n\n        mgr = self._mgr.to_2d_mgr(columns)\n        return self._constructor_expanddim(mgr)\n\n    def _set_name(self, name, inplace=False) -> Series:\n        \"\"\"\n        Set the Series name.\n\n        Parameters\n        ----------\n        name : str\n        inplace : bool\n            Whether to modify `self` directly or return a copy.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        ser = self if inplace else self.copy()\n        ser.name = name\n        return ser\n\n    @Appender(\n        \"\"\"\nExamples\n--------\n>>> ser = pd.Series([390., 350., 30., 20.],\n...                 index=['Falcon', 'Falcon', 'Parrot', 'Parrot'], name=\"Max Speed\")\n>>> ser\nFalcon    390.0\nFalcon    350.0\nParrot     30.0\nParrot     20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby([\"a\", \"b\", \"a\", \"b\"]).mean()\na    210.0\nb    185.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(ser > 100).mean()\nMax Speed\nFalse     25.0\nTrue     370.0\nName: Max Speed, dtype: float64\n\n**Grouping by Indexes**\n\nWe can groupby different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> ser = pd.Series([390., 350., 30., 20.], index=index, name=\"Max Speed\")\n>>> ser\nAnimal  Type\nFalcon  Captive    390.0\n        Wild       350.0\nParrot  Captive     30.0\n        Wild        20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nAnimal\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=\"Type\").mean()\nType\nCaptive    210.0\nWild       185.0\nName: Max Speed, dtype: float64\n\nWe can also choose to include `NA` in group keys or not by defining\n`dropna` parameter, the default setting is `True`.\n\n>>> ser = pd.Series([1, 2, 3, 3], index=[\"a\", 'a', 'b', np.nan])\n>>> ser.groupby(level=0).sum()\na    3\nb    3\ndtype: int64\n\n>>> ser.groupby(level=0, dropna=False).sum()\na    3\nb    3\nNaN  3\ndtype: int64\n\n>>> arrays = ['Falcon', 'Falcon', 'Parrot', 'Parrot']\n>>> ser = pd.Series([390., 350., 30., 20.], index=arrays, name=\"Max Speed\")\n>>> ser.groupby([\"a\", \"b\", \"a\", np.nan]).mean()\na    210.0\nb    350.0\nName: Max Speed, dtype: float64\n\n>>> ser.groupby([\"a\", \"b\", \"a\", np.nan], dropna=False).mean()\na    210.0\nb    350.0\nNaN   20.0\nName: Max Speed, dtype: float64\n\"\"\"\n    )\n    @Appender(generic._shared_docs[\"groupby\"] % _shared_doc_kwargs)\n    def groupby(\n        self,\n        by=None,\n        axis=0,\n        level=None,\n        as_index: bool = True,\n        sort: bool = True,\n        group_keys: bool = True,\n        squeeze: bool | lib.NoDefault = no_default,\n        observed: bool = False,\n        dropna: bool = True,\n    ) -> SeriesGroupBy:\n        from pandas.core.groupby.generic import SeriesGroupBy\n\n        if squeeze is not no_default:\n            warnings.warn(\n                (\n                    \"The `squeeze` parameter is deprecated and \"\n                    \"will be removed in a future version.\"\n                ),\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n        else:\n            squeeze = False\n\n        if level is None and by is None:\n            raise TypeError(\"You have to supply one of 'by' and 'level'\")\n        axis = self._get_axis_number(axis)\n\n        # error: Argument \"squeeze\" to \"SeriesGroupBy\" has incompatible type\n        # \"Union[bool, NoDefault]\"; expected \"bool\"\n        return SeriesGroupBy(\n            obj=self,\n            keys=by,\n            axis=axis,\n            level=level,\n            as_index=as_index,\n            sort=sort,\n            group_keys=group_keys,\n            squeeze=squeeze,  # type: ignore[arg-type]\n            observed=observed,\n            dropna=dropna,\n        )\n\n    # ----------------------------------------------------------------------\n    # Statistics, overridden ndarray methods\n\n    # TODO: integrate bottleneck\n\n    def count(self, level=None):\n        \"\"\"\n        Return number of non-NA/null observations in the Series.\n\n        Parameters\n        ----------\n        level : int or level name, default None\n            If the axis is a MultiIndex (hierarchical), count along a\n            particular level, collapsing into a smaller Series.\n\n        Returns\n        -------\n        int or Series (if level specified)\n            Number of non-null values in the Series.\n\n        See Also\n        --------\n        DataFrame.count : Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.0, 1.0, np.nan])\n        >>> s.count()\n        2\n        \"\"\"\n        if level is None:\n            return notna(self._values).sum().astype(\"int64\")\n        else:\n            warnings.warn(\n                \"Using the level keyword in DataFrame and Series aggregations is \"\n                \"deprecated and will be removed in a future version. Use groupby \"\n                \"instead. ser.count(level=1) should use ser.groupby(level=1).count().\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            if not isinstance(self.index, MultiIndex):\n                raise ValueError(\"Series.count level is only valid with a MultiIndex\")\n\n        index = self.index\n        assert isinstance(index, MultiIndex)  # for mypy\n\n        if isinstance(level, str):\n            level = index._get_level_number(level)\n\n        lev = index.levels[level]\n        level_codes = np.array(index.codes[level], subok=False, copy=True)\n\n        mask = level_codes == -1\n        if mask.any():\n            level_codes[mask] = cnt = len(lev)\n            lev = lev.insert(cnt, lev._na_value)\n\n        obs = level_codes[notna(self._values)]\n        out = np.bincount(obs, minlength=len(lev) or None)\n        return self._constructor(out, index=lev, dtype=\"int64\").__finalize__(\n            self, method=\"count\"\n        )\n\n    def mode(self, dropna=True) -> Series:\n        \"\"\"\n        Return the mode(s) of the Series.\n\n        The mode is the value that appears most often. There can be multiple modes.\n\n        Always returns Series even if only one value is returned.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n        Returns\n        -------\n        Series\n            Modes of the Series in sorted order.\n        \"\"\"\n        # TODO: Add option for bins like value_counts()\n        return algorithms.mode(self, dropna=dropna)\n\n    def unique(self) -> ArrayLike:\n        \"\"\"\n        Return unique values of Series object.\n\n        Uniques are returned in order of appearance. Hash table-based unique,\n        therefore does NOT sort.\n\n        Returns\n        -------\n        ndarray or ExtensionArray\n            The unique values returned as a NumPy array. See Notes.\n\n        See Also\n        --------\n        unique : Top-level unique method for any 1-d array-like object.\n        Index.unique : Return Index with unique values from an Index object.\n\n        Notes\n        -----\n        Returns the unique values as a NumPy array. In case of an\n        extension-array backed Series, a new\n        :class:`~api.extensions.ExtensionArray` of that type with just\n        the unique values is returned. This includes\n\n            * Categorical\n            * Period\n            * Datetime with Timezone\n            * Interval\n            * Sparse\n            * IntegerNA\n\n        See Examples section.\n\n        Examples\n        --------\n        >>> pd.Series([2, 1, 3, 3], name='A').unique()\n        array([2, 1, 3])\n\n        >>> pd.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()\n        array(['2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\n\n        >>> pd.Series([pd.Timestamp('2016-01-01', tz='US/Eastern')\n        ...            for _ in range(3)]).unique()\n        <DatetimeArray>\n        ['2016-01-01 00:00:00-05:00']\n        Length: 1, dtype: datetime64[ns, US/Eastern]\n\n        An Categorical will return categories in the order of\n        appearance and with the same dtype.\n\n        >>> pd.Series(pd.Categorical(list('baabc'))).unique()\n        ['b', 'a', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> pd.Series(pd.Categorical(list('baabc'), categories=list('abc'),\n        ...                          ordered=True)).unique()\n        ['b', 'a', 'c']\n        Categories (3, object): ['a' < 'b' < 'c']\n        \"\"\"\n        return super().unique()\n\n    @overload\n    def drop_duplicates(self, keep=..., inplace: Literal[False] = ...) -> Series:\n        ...\n\n    @overload\n    def drop_duplicates(self, keep, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def drop_duplicates(self, *, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def drop_duplicates(self, keep=..., inplace: bool = ...) -> Series | None:\n        ...\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def drop_duplicates(self, keep=\"first\", inplace=False) -> Series | None:\n        \"\"\"\n        Return Series with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        inplace : bool, default ``False``\n            If ``True``, performs operation inplace and returns None.\n\n        Returns\n        -------\n        Series or None\n            Series with duplicates dropped or None if ``inplace=True``.\n\n        See Also\n        --------\n        Index.drop_duplicates : Equivalent method on Index.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Series.duplicated : Related method on Series, indicating duplicate\n            Series values.\n\n        Examples\n        --------\n        Generate a Series with duplicated entries.\n\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'],\n        ...               name='animal')\n        >>> s\n        0      lama\n        1       cow\n        2      lama\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        With the 'keep' parameter, the selection behaviour of duplicated values\n        can be changed. The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> s.drop_duplicates()\n        0      lama\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n\n        The value 'last' for parameter 'keep' keeps the last occurrence for\n        each set of duplicated entries.\n\n        >>> s.drop_duplicates(keep='last')\n        1       cow\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        The value ``False`` for parameter 'keep' discards all sets of\n        duplicated entries. Setting the value of 'inplace' to ``True`` performs\n        the operation inplace and returns ``None``.\n\n        >>> s.drop_duplicates(keep=False, inplace=True)\n        >>> s\n        1       cow\n        3    beetle\n        5     hippo\n        Name: animal, dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        result = super().drop_duplicates(keep=keep)\n        if inplace:\n            self._update_inplace(result)\n            return None\n        else:\n            return result\n\n    def duplicated(self, keep=\"first\") -> Series:\n        \"\"\"\n        Indicate duplicate Series values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        Series. Either all duplicates, all except the first or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            Method to handle dropping duplicates:\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series[bool]\n            Series indicating whether each value has occurred in the\n            preceding values.\n\n        See Also\n        --------\n        Index.duplicated : Equivalent method on pandas.Index.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Series.drop_duplicates : Remove duplicate values from Series.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set on False and all others on True:\n\n        >>> animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> animals.duplicated()\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        which is equivalent to\n\n        >>> animals.duplicated(keep='first')\n        0    False\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> animals.duplicated(keep='last')\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> animals.duplicated(keep=False)\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        dtype: bool\n        \"\"\"\n        res = self._duplicated(keep=keep)\n        result = self._constructor(res, index=self.index)\n        return result.__finalize__(self, method=\"duplicated\")\n\n    def idxmin(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the minimum value.\n\n        If multiple values equal the minimum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmin. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the minimum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmin : Return indices of the minimum values\n            along the given axis.\n        DataFrame.idxmin : Return index of first occurrence of minimum\n            over requested axis.\n        Series.idxmax : Return index *label* of the first occurrence\n            of maximum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmin``. This method\n        returns the label of the minimum, while ``ndarray.argmin`` returns\n        the position. To get the position, use ``series.values.argmin()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 1],\n        ...               index=['A', 'B', 'C', 'D'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    1.0\n        dtype: float64\n\n        >>> s.idxmin()\n        'A'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmin(skipna=False)\n        nan\n        \"\"\"\n        i = self.argmin(axis, skipna, *args, **kwargs)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def idxmax(self, axis=0, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the row label of the maximum value.\n\n        If multiple values equal the maximum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            For compatibility with DataFrame.idxmax. Redundant for application\n            on Series.\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Index\n            Label of the maximum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        numpy.argmax : Return indices of the maximum values\n            along the given axis.\n        DataFrame.idxmax : Return index of first occurrence of maximum\n            over requested axis.\n        Series.idxmin : Return index *label* of the first occurrence\n            of minimum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmax``. This method\n        returns the label of the maximum, while ``ndarray.argmax`` returns\n        the position. To get the position, use ``series.values.argmax()``.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=[1, None, 4, 3, 4],\n        ...               index=['A', 'B', 'C', 'D', 'E'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    3.0\n        E    4.0\n        dtype: float64\n\n        >>> s.idxmax()\n        'C'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmax(skipna=False)\n        nan\n        \"\"\"\n        i = self.argmax(axis, skipna, *args, **kwargs)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def round(self, decimals=0, *args, **kwargs) -> Series:\n        \"\"\"\n        Round each value in a Series to the given number of decimals.\n\n        Parameters\n        ----------\n        decimals : int, default 0\n            Number of decimal places to round to. If decimals is negative,\n            it specifies the number of positions to the left of the decimal point.\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with NumPy.\n\n        Returns\n        -------\n        Series\n            Rounded values of the Series.\n\n        See Also\n        --------\n        numpy.around : Round values of an np.array.\n        DataFrame.round : Round values of a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.1, 1.3, 2.7])\n        >>> s.round()\n        0    0.0\n        1    1.0\n        2    3.0\n        dtype: float64\n        \"\"\"\n        nv.validate_round(args, kwargs)\n        result = self._values.round(decimals)\n        result = self._constructor(result, index=self.index).__finalize__(\n            self, method=\"round\"\n        )\n\n        return result\n\n    def quantile(self, q=0.5, interpolation=\"linear\"):\n        \"\"\"\n        Return value at the given quantile.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            The quantile(s) to compute, which can lie in range: 0 <= q <= 1.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            This optional parameter specifies the interpolation method to use,\n            when the desired quantile lies between two data points `i` and `j`:\n\n                * linear: `i + (j - i) * fraction`, where `fraction` is the\n                  fractional part of the index surrounded by `i` and `j`.\n                * lower: `i`.\n                * higher: `j`.\n                * nearest: `i` or `j` whichever is nearest.\n                * midpoint: (`i` + `j`) / 2.\n\n        Returns\n        -------\n        float or Series\n            If ``q`` is an array, a Series will be returned where the\n            index is ``q`` and the values are the quantiles, otherwise\n            a float will be returned.\n\n        See Also\n        --------\n        core.window.Rolling.quantile : Calculate the rolling quantile.\n        numpy.percentile : Returns the q-th percentile(s) of the array elements.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.quantile(.5)\n        2.5\n        >>> s.quantile([.25, .5, .75])\n        0.25    1.75\n        0.50    2.50\n        0.75    3.25\n        dtype: float64\n        \"\"\"\n        validate_percentile(q)\n\n        # We dispatch to DataFrame so that core.internals only has to worry\n        #  about 2D cases.\n        df = self.to_frame()\n\n        result = df.quantile(q=q, interpolation=interpolation, numeric_only=False)\n        if result.ndim == 2:\n            result = result.iloc[:, 0]\n\n        if is_list_like(q):\n            result.name = self.name\n            return self._constructor(result, index=Float64Index(q), name=self.name)\n        else:\n            # scalar\n            return result.iloc[0]\n\n    def corr(self, other, method=\"pearson\", min_periods=None) -> float:\n        \"\"\"\n        Compute correlation with `other` Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the correlation.\n        method : {'pearson', 'kendall', 'spearman'} or callable\n            Method used to compute correlation:\n\n            - pearson : Standard correlation coefficient\n            - kendall : Kendall Tau correlation coefficient\n            - spearman : Spearman rank correlation\n            - callable: Callable with input two 1d ndarrays and returning a float.\n\n            .. warning::\n                Note that the returned matrix from corr will have 1 along the\n                diagonals and will be symmetric regardless of the callable's\n                behavior.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n\n        Returns\n        -------\n        float\n            Correlation with other.\n\n        See Also\n        --------\n        DataFrame.corr : Compute pairwise correlation between columns.\n        DataFrame.corrwith : Compute pairwise correlation with another\n            DataFrame or Series.\n\n        Examples\n        --------\n        >>> def histogram_intersection(a, b):\n        ...     v = np.minimum(a, b).sum().round(decimals=1)\n        ...     return v\n        >>> s1 = pd.Series([.2, .0, .6, .2])\n        >>> s2 = pd.Series([.3, .6, .0, .1])\n        >>> s1.corr(s2, method=histogram_intersection)\n        0.3\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n\n        if method in [\"pearson\", \"spearman\", \"kendall\"] or callable(method):\n            return nanops.nancorr(\n                this.values, other.values, method=method, min_periods=min_periods\n            )\n\n        raise ValueError(\n            \"method must be either 'pearson', \"\n            \"'spearman', 'kendall', or a callable, \"\n            f\"'{method}' was supplied\"\n        )\n\n    def cov(\n        self,\n        other: Series,\n        min_periods: int | None = None,\n        ddof: int | None = 1,\n    ) -> float:\n        \"\"\"\n        Compute covariance with Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the covariance.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n        ddof : int, default 1\n            Delta degrees of freedom.  The divisor used in calculations\n            is ``N - ddof``, where ``N`` represents the number of elements.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        float\n            Covariance between Series and other normalized by N-1\n            (unbiased estimator).\n\n        See Also\n        --------\n        DataFrame.cov : Compute pairwise covariance of columns.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([0.90010907, 0.13484424, 0.62036035])\n        >>> s2 = pd.Series([0.12528585, 0.26962463, 0.51111198])\n        >>> s1.cov(s2)\n        -0.01685762652715874\n        \"\"\"\n        this, other = self.align(other, join=\"inner\", copy=False)\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancov(\n            this.values, other.values, min_periods=min_periods, ddof=ddof\n        )\n\n    @doc(\n        klass=\"Series\",\n        extra_params=\"\",\n        other_klass=\"DataFrame\",\n        examples=dedent(\n            \"\"\"\n        Difference with previous row\n\n        >>> s = pd.Series([1, 1, 2, 3, 5, 8])\n        >>> s.diff()\n        0    NaN\n        1    0.0\n        2    1.0\n        3    1.0\n        4    2.0\n        5    3.0\n        dtype: float64\n\n        Difference with 3rd previous row\n\n        >>> s.diff(periods=3)\n        0    NaN\n        1    NaN\n        2    NaN\n        3    2.0\n        4    4.0\n        5    6.0\n        dtype: float64\n\n        Difference with following row\n\n        >>> s.diff(periods=-1)\n        0    0.0\n        1   -1.0\n        2   -1.0\n        3   -2.0\n        4   -3.0\n        5    NaN\n        dtype: float64\n\n        Overflow in input dtype\n\n        >>> s = pd.Series([1, 0], dtype=np.uint8)\n        >>> s.diff()\n        0      NaN\n        1    255.0\n        dtype: float64\"\"\"\n        ),\n    )\n    def diff(self, periods: int = 1) -> Series:\n        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of a {klass} element compared with another\n        element in the {klass} (default is element in previous row).\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative\n            values.\n        {extra_params}\n        Returns\n        -------\n        {klass}\n            First differences of the Series.\n\n        See Also\n        --------\n        {klass}.pct_change: Percent change over given number of periods.\n        {klass}.shift: Shift index by desired number of periods with an\n            optional time freq.\n        {other_klass}.diff: First discrete difference of object.\n\n        Notes\n        -----\n        For boolean dtypes, this uses :meth:`operator.xor` rather than\n        :meth:`operator.sub`.\n        The result is calculated according to current dtype in {klass},\n        however dtype of the result is always float64.\n\n        Examples\n        --------\n        {examples}\n        \"\"\"\n        result = algorithms.diff(self._values, periods)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"diff\"\n        )\n\n    def autocorr(self, lag=1) -> float:\n        \"\"\"\n        Compute the lag-N autocorrelation.\n\n        This method computes the Pearson correlation between\n        the Series and its shifted self.\n\n        Parameters\n        ----------\n        lag : int, default 1\n            Number of lags to apply before performing autocorrelation.\n\n        Returns\n        -------\n        float\n            The Pearson correlation between self and self.shift(lag).\n\n        See Also\n        --------\n        Series.corr : Compute the correlation between two Series.\n        Series.shift : Shift index by desired number of periods.\n        DataFrame.corr : Compute pairwise correlation of columns.\n        DataFrame.corrwith : Compute pairwise correlation between rows or\n            columns of two DataFrame objects.\n\n        Notes\n        -----\n        If the Pearson correlation is not well defined return 'NaN'.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.25, 0.5, 0.2, -0.05])\n        >>> s.autocorr()  # doctest: +ELLIPSIS\n        0.10355...\n        >>> s.autocorr(lag=2)  # doctest: +ELLIPSIS\n        -0.99999...\n\n        If the Pearson correlation is not well defined, then 'NaN' is returned.\n\n        >>> s = pd.Series([1, 0, 0, 0])\n        >>> s.autocorr()\n        nan\n        \"\"\"\n        return self.corr(self.shift(lag))\n\n    def dot(self, other):\n        \"\"\"\n        Compute the dot product between the Series and the columns of other.\n\n        This method computes the dot product between the Series and another\n        one, or the Series and each columns of a DataFrame, or the Series and\n        each columns of an array.\n\n        It can also be called using `self @ other` in Python >= 3.5.\n\n        Parameters\n        ----------\n        other : Series, DataFrame or array-like\n            The other object to compute the dot product with its columns.\n\n        Returns\n        -------\n        scalar, Series or numpy.ndarray\n            Return the dot product of the Series and other if other is a\n            Series, the Series of the dot product of Series and each rows of\n            other if other is a DataFrame or a numpy.ndarray between the Series\n            and each columns of the numpy array.\n\n        See Also\n        --------\n        DataFrame.dot: Compute the matrix product with the DataFrame.\n        Series.mul: Multiplication of series and other, element-wise.\n\n        Notes\n        -----\n        The Series and other has to share the same index if other is a Series\n        or a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0, 1, 2, 3])\n        >>> other = pd.Series([-1, 2, -3, 4])\n        >>> s.dot(other)\n        8\n        >>> s @ other\n        8\n        >>> df = pd.DataFrame([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(df)\n        0    24\n        1    14\n        dtype: int64\n        >>> arr = np.array([[0, 1], [-2, 3], [4, -5], [6, 7]])\n        >>> s.dot(arr)\n        array([24, 14])\n        \"\"\"\n        if isinstance(other, (Series, ABCDataFrame)):\n            common = self.index.union(other.index)\n            if len(common) > len(self.index) or len(common) > len(other.index):\n                raise ValueError(\"matrices are not aligned\")\n\n            left = self.reindex(index=common, copy=False)\n            right = other.reindex(index=common, copy=False)\n            lvals = left.values\n            rvals = right.values\n        else:\n            lvals = self.values\n            rvals = np.asarray(other)\n            if lvals.shape[0] != rvals.shape[0]:\n                raise Exception(\n                    f\"Dot product shape mismatch, {lvals.shape} vs {rvals.shape}\"\n                )\n\n        if isinstance(other, ABCDataFrame):\n            return self._constructor(\n                np.dot(lvals, rvals), index=other.columns\n            ).__finalize__(self, method=\"dot\")\n        elif isinstance(other, Series):\n            return np.dot(lvals, rvals)\n        elif isinstance(rvals, np.ndarray):\n            return np.dot(lvals, rvals)\n        else:  # pragma: no cover\n            raise TypeError(f\"unsupported type: {type(other)}\")\n\n    def __matmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(other)\n\n    def __rmatmul__(self, other):\n        \"\"\"\n        Matrix multiplication using binary `@` operator in Python>=3.5.\n        \"\"\"\n        return self.dot(np.transpose(other))\n\n    @doc(base.IndexOpsMixin.searchsorted, klass=\"Series\")\n    # Signature of \"searchsorted\" incompatible with supertype \"IndexOpsMixin\"\n    def searchsorted(  # type: ignore[override]\n        self,\n        value: NumpyValueArrayLike | ExtensionArray,\n        side: Literal[\"left\", \"right\"] = \"left\",\n        sorter: NumpySorter = None,\n    ) -> npt.NDArray[np.intp] | np.intp:\n        return base.IndexOpsMixin.searchsorted(self, value, side=side, sorter=sorter)\n\n    # -------------------------------------------------------------------\n    # Combination\n\n    def append(\n        self, to_append, ignore_index: bool = False, verify_integrity: bool = False\n    ):\n        \"\"\"\n        Concatenate two or more Series.\n\n        Parameters\n        ----------\n        to_append : Series or list/tuple of Series\n            Series to append with self.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, …, n - 1.\n        verify_integrity : bool, default False\n            If True, raise Exception on creating index with duplicates.\n\n        Returns\n        -------\n        Series\n            Concatenated Series.\n\n        See Also\n        --------\n        concat : General function to concatenate DataFrame or Series objects.\n\n        Notes\n        -----\n        Iteratively appending to a Series can be more computationally intensive\n        than a single concatenate. A better solution is to append values to a\n        list and then concatenate the list with the original Series all at\n        once.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, 2, 3])\n        >>> s2 = pd.Series([4, 5, 6])\n        >>> s3 = pd.Series([4, 5, 6], index=[3, 4, 5])\n        >>> s1.append(s2)\n        0    1\n        1    2\n        2    3\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s1.append(s3)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `ignore_index` set to True:\n\n        >>> s1.append(s2, ignore_index=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `verify_integrity` set to True:\n\n        >>> s1.append(s2, verify_integrity=True)\n        Traceback (most recent call last):\n        ...\n        ValueError: Indexes have overlapping values: [0, 1, 2]\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        if isinstance(to_append, (list, tuple)):\n            to_concat = [self]\n            to_concat.extend(to_append)\n        else:\n            to_concat = [self, to_append]\n        if any(isinstance(x, (ABCDataFrame,)) for x in to_concat[1:]):\n            msg = \"to_append should be a Series or list/tuple of Series, got DataFrame\"\n            raise TypeError(msg)\n        return concat(\n            to_concat, ignore_index=ignore_index, verify_integrity=verify_integrity\n        )\n\n    def _binop(self, other: Series, func, level=None, fill_value=None):\n        \"\"\"\n        Perform generic binary operation with optional fill value.\n\n        Parameters\n        ----------\n        other : Series\n        func : binary operator\n        fill_value : float or object\n            Value to substitute for NA/null values. If both Series are NA in a\n            location, the result will be NA regardless of the passed fill value.\n        level : int or level name, default None\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level.\n\n        Returns\n        -------\n        Series\n        \"\"\"\n        if not isinstance(other, Series):\n            raise AssertionError(\"Other operand must be Series\")\n\n        this = self\n\n        if not self.index.equals(other.index):\n            this, other = self.align(other, level=level, join=\"outer\", copy=False)\n\n        this_vals, other_vals = ops.fill_binop(this._values, other._values, fill_value)\n\n        with np.errstate(all=\"ignore\"):\n            result = func(this_vals, other_vals)\n\n        name = ops.get_op_result_name(self, other)\n        return this._construct_result(result, name)\n\n    def _construct_result(\n        self, result: ArrayLike | tuple[ArrayLike, ArrayLike], name: Hashable\n    ) -> Series | tuple[Series, Series]:\n        \"\"\"\n        Construct an appropriately-labelled Series from the result of an op.\n\n        Parameters\n        ----------\n        result : ndarray or ExtensionArray\n        name : Label\n\n        Returns\n        -------\n        Series\n            In the case of __divmod__ or __rdivmod__, a 2-tuple of Series.\n        \"\"\"\n        if isinstance(result, tuple):\n            # produced by divmod or rdivmod\n\n            res1 = self._construct_result(result[0], name=name)\n            res2 = self._construct_result(result[1], name=name)\n\n            # GH#33427 assertions to keep mypy happy\n            assert isinstance(res1, Series)\n            assert isinstance(res2, Series)\n            return (res1, res2)\n\n        # We do not pass dtype to ensure that the Series constructor\n        #  does inference in the case where `result` has object-dtype.\n        out = self._constructor(result, index=self.index)\n        out = out.__finalize__(self)\n\n        # Set the result's name after __finalize__ is called because __finalize__\n        #  would set it back to self.name\n        out.name = name\n        return out\n\n    @doc(\n        generic._shared_docs[\"compare\"],\n        \"\"\"\nReturns\n-------\nSeries or DataFrame\n    If axis is 0 or 'index' the result will be a Series.\n    The resulting index will be a MultiIndex with 'self' and 'other'\n    stacked alternately at the inner level.\n\n    If axis is 1 or 'columns' the result will be a DataFrame.\n    It will have two columns namely 'self' and 'other'.\n\nSee Also\n--------\nDataFrame.compare : Compare with another DataFrame and show differences.\n\nNotes\n-----\nMatching NaNs will not appear as a difference.\n\nExamples\n--------\n>>> s1 = pd.Series([\"a\", \"b\", \"c\", \"d\", \"e\"])\n>>> s2 = pd.Series([\"a\", \"a\", \"c\", \"b\", \"e\"])\n\nAlign the differences on columns\n\n>>> s1.compare(s2)\n  self other\n1    b     a\n3    d     b\n\nStack the differences on indices\n\n>>> s1.compare(s2, align_axis=0)\n1  self     b\n   other    a\n3  self     d\n   other    b\ndtype: object\n\nKeep all original rows\n\n>>> s1.compare(s2, keep_shape=True)\n  self other\n0  NaN   NaN\n1    b     a\n2  NaN   NaN\n3    d     b\n4  NaN   NaN\n\nKeep all original rows and also all original values\n\n>>> s1.compare(s2, keep_shape=True, keep_equal=True)\n  self other\n0    a     a\n1    b     a\n2    c     c\n3    d     b\n4    e     e\n\"\"\",\n        klass=_shared_doc_kwargs[\"klass\"],\n    )\n    def compare(\n        self,\n        other: Series,\n        align_axis: Axis = 1,\n        keep_shape: bool = False,\n        keep_equal: bool = False,\n    ) -> DataFrame | Series:\n        return super().compare(\n            other=other,\n            align_axis=align_axis,\n            keep_shape=keep_shape,\n            keep_equal=keep_equal,\n        )\n\n    def combine(self, other, func, fill_value=None) -> Series:\n        \"\"\"\n        Combine the Series with a Series or scalar according to `func`.\n\n        Combine the Series and `other` using `func` to perform elementwise\n        selection for combined Series.\n        `fill_value` is assumed when value is missing at some index\n        from one of the two objects being combined.\n\n        Parameters\n        ----------\n        other : Series or scalar\n            The value(s) to be combined with the `Series`.\n        func : function\n            Function that takes two scalars as inputs and returns an element.\n        fill_value : scalar, optional\n            The value to assume when an index is missing from\n            one Series or the other. The default specifies to use the\n            appropriate NaN value for the underlying dtype of the Series.\n\n        Returns\n        -------\n        Series\n            The result of combining the Series with the other object.\n\n        See Also\n        --------\n        Series.combine_first : Combine Series values, choosing the calling\n            Series' values first.\n\n        Examples\n        --------\n        Consider 2 Datasets ``s1`` and ``s2`` containing\n        highest clocked speeds of different birds.\n\n        >>> s1 = pd.Series({'falcon': 330.0, 'eagle': 160.0})\n        >>> s1\n        falcon    330.0\n        eagle     160.0\n        dtype: float64\n        >>> s2 = pd.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})\n        >>> s2\n        falcon    345.0\n        eagle     200.0\n        duck       30.0\n        dtype: float64\n\n        Now, to combine the two datasets and view the highest speeds\n        of the birds across the two datasets\n\n        >>> s1.combine(s2, max)\n        duck        NaN\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n\n        In the previous example, the resulting value for duck is missing,\n        because the maximum of a NaN and a float is a NaN.\n        So, in the example, we set ``fill_value=0``,\n        so the maximum value returned will be the value from some dataset.\n\n        >>> s1.combine(s2, max, fill_value=0)\n        duck       30.0\n        eagle     200.0\n        falcon    345.0\n        dtype: float64\n        \"\"\"\n        if fill_value is None:\n            fill_value = na_value_for_dtype(self.dtype, compat=False)\n\n        if isinstance(other, Series):\n            # If other is a Series, result is based on union of Series,\n            # so do this element by element\n            new_index = self.index.union(other.index)\n            new_name = ops.get_op_result_name(self, other)\n            new_values = np.empty(len(new_index), dtype=object)\n            for i, idx in enumerate(new_index):\n                lv = self.get(idx, fill_value)\n                rv = other.get(idx, fill_value)\n                with np.errstate(all=\"ignore\"):\n                    new_values[i] = func(lv, rv)\n        else:\n            # Assume that other is a scalar, so apply the function for\n            # each element in the Series\n            new_index = self.index\n            new_values = np.empty(len(new_index), dtype=object)\n            with np.errstate(all=\"ignore\"):\n                new_values[:] = [func(lv, other) for lv in self._values]\n            new_name = self.name\n\n        # try_float=False is to match agg_series\n        npvalues = lib.maybe_convert_objects(new_values, try_float=False)\n        res_values = maybe_cast_pointwise_result(npvalues, self.dtype, same_dtype=False)\n        return self._constructor(res_values, index=new_index, name=new_name)\n\n    def combine_first(self, other) -> Series:\n        \"\"\"\n        Update null elements with value in the same location in 'other'.\n\n        Combine two Series objects by filling null values in one Series with\n        non-null values from the other Series. Result index will be the union\n        of the two indexes.\n\n        Parameters\n        ----------\n        other : Series\n            The value(s) to be used for filling null values.\n\n        Returns\n        -------\n        Series\n            The result of combining the provided Series with the other object.\n\n        See Also\n        --------\n        Series.combine : Perform element-wise operation on two Series\n            using a given function.\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, np.nan])\n        >>> s2 = pd.Series([3, 4, 5])\n        >>> s1.combine_first(s2)\n        0    1.0\n        1    4.0\n        2    5.0\n        dtype: float64\n\n        Null values still persist if the location of that null value\n        does not exist in `other`\n\n        >>> s1 = pd.Series({'falcon': np.nan, 'eagle': 160.0})\n        >>> s2 = pd.Series({'eagle': 200.0, 'duck': 30.0})\n        >>> s1.combine_first(s2)\n        duck       30.0\n        eagle     160.0\n        falcon      NaN\n        dtype: float64\n        \"\"\"\n        new_index = self.index.union(other.index)\n        this = self.reindex(new_index, copy=False)\n        other = other.reindex(new_index, copy=False)\n        if this.dtype.kind == \"M\" and other.dtype.kind != \"M\":\n            other = to_datetime(other)\n\n        return this.where(notna(this), other)\n\n    def update(self, other) -> None:\n        \"\"\"\n        Modify Series in place using values from passed Series.\n\n        Uses non-NA values from passed Series to make updates. Aligns\n        on index.\n\n        Parameters\n        ----------\n        other : Series, or object coercible into Series\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s.update(pd.Series(['d', 'e'], index=[0, 2]))\n        >>> s\n        0    d\n        1    b\n        2    e\n        dtype: object\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, 5, 6, 7, 8]))\n        >>> s\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        If ``other`` contains NaNs the corresponding values are not updated\n        in the original Series.\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update(pd.Series([4, np.nan, 6]))\n        >>> s\n        0    4\n        1    2\n        2    6\n        dtype: int64\n\n        ``other`` can also be a non-Series object type\n        that is coercible into a Series\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update([4, np.nan, 6])\n        >>> s\n        0    4\n        1    2\n        2    6\n        dtype: int64\n\n        >>> s = pd.Series([1, 2, 3])\n        >>> s.update({1: 9})\n        >>> s\n        0    1\n        1    9\n        2    3\n        dtype: int64\n        \"\"\"\n\n        if not isinstance(other, Series):\n            other = Series(other)\n\n        other = other.reindex_like(self)\n        mask = notna(other)\n\n        self._mgr = self._mgr.putmask(mask=mask, new=other)\n        self._maybe_update_cacher()\n\n    # ----------------------------------------------------------------------\n    # Reindexing, sorting\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def sort_values(\n        self,\n        axis=0,\n        ascending: bool | int | Sequence[bool | int] = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        ignore_index: bool = False,\n        key: ValueKeyFunc = None,\n    ):\n        \"\"\"\n        Sort by the values.\n\n        Sort a Series in ascending or descending order by some\n        criterion.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            Axis to direct sorting. The value 'index' is accepted for\n            compatibility with DataFrame.sort_values.\n        ascending : bool or list of bools, default True\n            If True, sort values in ascending order, otherwise descending.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. 'mergesort' and 'stable' are the only stable  algorithms.\n        na_position : {'first' or 'last'}, default 'last'\n            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n            the end.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, …, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the series values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect a\n            ``Series`` and return an array-like.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series or None\n            Series ordered by values or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.sort_index : Sort by the Series indices.\n        DataFrame.sort_values : Sort DataFrame by the values along either axis.\n        DataFrame.sort_index : Sort DataFrame by indices.\n\n        Examples\n        --------\n        >>> s = pd.Series([np.nan, 1, 3, 10, 5])\n        >>> s\n        0     NaN\n        1     1.0\n        2     3.0\n        3     10.0\n        4     5.0\n        dtype: float64\n\n        Sort values ascending order (default behaviour)\n\n        >>> s.sort_values(ascending=True)\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        0     NaN\n        dtype: float64\n\n        Sort values descending order\n\n        >>> s.sort_values(ascending=False)\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values inplace\n\n        >>> s.sort_values(ascending=False, inplace=True)\n        >>> s\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        dtype: float64\n\n        Sort values putting NAs first\n\n        >>> s.sort_values(na_position='first')\n        0     NaN\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        dtype: float64\n\n        Sort a series of strings\n\n        >>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n        >>> s\n        0    z\n        1    b\n        2    d\n        3    a\n        4    c\n        dtype: object\n\n        >>> s.sort_values()\n        3    a\n        1    b\n        4    c\n        2    d\n        0    z\n        dtype: object\n\n        Sort using a key function. Your `key` function will be\n        given the ``Series`` of values and should return an array-like.\n\n        >>> s = pd.Series(['a', 'B', 'c', 'D', 'e'])\n        >>> s.sort_values()\n        1    B\n        3    D\n        0    a\n        2    c\n        4    e\n        dtype: object\n        >>> s.sort_values(key=lambda x: x.str.lower())\n        0    a\n        1    B\n        2    c\n        3    D\n        4    e\n        dtype: object\n\n        NumPy ufuncs work well here. For example, we can\n        sort by the ``sin`` of the value\n\n        >>> s = pd.Series([-4, -2, 0, 2, 4])\n        >>> s.sort_values(key=np.sin)\n        1   -2\n        4    4\n        2    0\n        0   -4\n        3    2\n        dtype: int64\n\n        More complicated user-defined functions can be used,\n        as long as they expect a Series and return an array-like\n\n        >>> s.sort_values(key=lambda x: (np.tan(x.cumsum())))\n        0   -4\n        3    2\n        4    4\n        1   -2\n        2    0\n        dtype: int64\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n\n        # GH 5856/5853\n        if inplace and self._is_cached:\n            raise ValueError(\n                \"This Series is a view of some other array, to \"\n                \"sort in-place you must create a copy\"\n            )\n\n        if is_list_like(ascending):\n            ascending = cast(Sequence[Union[bool, int]], ascending)\n            if len(ascending) != 1:\n                raise ValueError(\n                    f\"Length of ascending ({len(ascending)}) must be 1 for Series\"\n                )\n            ascending = ascending[0]\n\n        ascending = validate_ascending(ascending)\n\n        if na_position not in [\"first\", \"last\"]:\n            raise ValueError(f\"invalid na_position: {na_position}\")\n\n        # GH 35922. Make sorting stable by leveraging nargsort\n        values_to_sort = ensure_key_mapped(self, key)._values if key else self._values\n        sorted_index = nargsort(values_to_sort, kind, bool(ascending), na_position)\n\n        result = self._constructor(\n            self._values[sorted_index], index=self.index[sorted_index]\n        )\n\n        if ignore_index:\n            result.index = default_index(len(sorted_index))\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result.__finalize__(self, method=\"sort_values\")\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def sort_index(\n        self,\n        axis=0,\n        level=None,\n        ascending: bool | int | Sequence[bool | int] = True,\n        inplace: bool = False,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        sort_remaining: bool = True,\n        ignore_index: bool = False,\n        key: IndexKeyFunc = None,\n    ):\n        \"\"\"\n        Sort Series by index labels.\n\n        Returns a new Series sorted by label if `inplace` argument is\n        ``False``, otherwise updates the original series and returns None.\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Axis to direct sorting. This can only be 0 for Series.\n        level : int, optional\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list-like of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level individually.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            information. 'mergesort' and 'stable' are the only stable algorithms. For\n            DataFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'last'}, default 'last'\n            If 'first' puts NaNs at the beginning, 'last' puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, …, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series or None\n            The original Series sorted by the labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.sort_index: Sort DataFrame by the index.\n        DataFrame.sort_values: Sort DataFrame by the value.\n        Series.sort_values : Sort Series by the value.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])\n        >>> s.sort_index()\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        Sort Descending\n\n        >>> s.sort_index(ascending=False)\n        4    d\n        3    a\n        2    b\n        1    c\n        dtype: object\n\n        Sort Inplace\n\n        >>> s.sort_index(inplace=True)\n        >>> s\n        1    c\n        2    b\n        3    a\n        4    d\n        dtype: object\n\n        By default NaNs are put at the end, but use `na_position` to place\n        them at the beginning\n\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])\n        >>> s.sort_index(na_position='first')\n        NaN     d\n         1.0    c\n         2.0    b\n         3.0    a\n        dtype: object\n\n        Specify index level to sort\n\n        >>> arrays = [np.array(['qux', 'qux', 'foo', 'foo',\n        ...                     'baz', 'baz', 'bar', 'bar']),\n        ...           np.array(['two', 'one', 'two', 'one',\n        ...                     'two', 'one', 'two', 'one'])]\n        >>> s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)\n        >>> s.sort_index(level=1)\n        bar  one    8\n        baz  one    6\n        foo  one    4\n        qux  one    2\n        bar  two    7\n        baz  two    5\n        foo  two    3\n        qux  two    1\n        dtype: int64\n\n        Does not sort by remaining levels when sorting by levels\n\n        >>> s.sort_index(level=1, sort_remaining=False)\n        qux  one    2\n        foo  one    4\n        baz  one    6\n        bar  one    8\n        qux  two    1\n        foo  two    3\n        baz  two    5\n        bar  two    7\n        dtype: int64\n\n        Apply a key function before sorting\n\n        >>> s = pd.Series([1, 2, 3, 4], index=['A', 'b', 'C', 'd'])\n        >>> s.sort_index(key=lambda x : x.str.lower())\n        A    1\n        b    2\n        C    3\n        d    4\n        dtype: int64\n        \"\"\"\n\n        return super().sort_index(\n            axis,\n            level,\n            ascending,\n            inplace,\n            kind,\n            na_position,\n            sort_remaining,\n            ignore_index,\n            key,\n        )\n\n    def argsort(self, axis=0, kind=\"quicksort\", order=None) -> Series:\n        \"\"\"\n        Return the integer indices that would sort the Series values.\n\n        Override ndarray.argsort. Argsorts the value, omitting NA/null values,\n        and places the result in the same locations as the non-NA values.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Has no effect but is accepted for compatibility with numpy.\n        kind : {'mergesort', 'quicksort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See :func:`numpy.sort` for more\n            information. 'mergesort' and 'stable' are the only stable algorithms.\n        order : None\n            Has no effect but is accepted for compatibility with numpy.\n\n        Returns\n        -------\n        Series[np.intp]\n            Positions of values within the sort order with -1 indicating\n            nan values.\n\n        See Also\n        --------\n        numpy.ndarray.argsort : Returns the indices that would sort this array.\n        \"\"\"\n        values = self._values\n        mask = isna(values)\n\n        if mask.any():\n            result = np.full(len(self), -1, dtype=np.intp)\n            notmask = ~mask\n            result[notmask] = np.argsort(values[notmask], kind=kind)\n        else:\n            result = np.argsort(values, kind=kind)\n\n        res = self._constructor(result, index=self.index, name=self.name, dtype=np.intp)\n        return res.__finalize__(self, method=\"argsort\")\n\n    def nlargest(self, n=5, keep=\"first\") -> Series:\n        \"\"\"\n        Return the largest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many descending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n              of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n              order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n              size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` largest values in the Series, sorted in decreasing order.\n\n        See Also\n        --------\n        Series.nsmallest: Get the `n` smallest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values(ascending=False).head(n)`` for small `n`\n        relative to the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Malta\": 434000, \"Maldives\": 434000,\n        ...                         \"Brunei\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Montserrat      5200\n        dtype: int64\n\n        The `n` largest elements where ``n=5`` by default.\n\n        >>> s.nlargest()\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3``. Default `keep` value is 'first'\n        so Malta will be kept.\n\n        >>> s.nlargest(3)\n        France    65000000\n        Italy     59000000\n        Malta       434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` and keeping the last duplicates.\n        Brunei will be kept since it is the last with value 434000 based on\n        the index order.\n\n        >>> s.nlargest(3, keep='last')\n        France      65000000\n        Italy       59000000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has five elements due to the three duplicates.\n\n        >>> s.nlargest(3, keep='all')\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldives      434000\n        Brunei        434000\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nlargest()\n\n    def nsmallest(self, n: int = 5, keep: str = \"first\") -> Series:\n        \"\"\"\n        Return the smallest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many ascending sorted values.\n        keep : {'first', 'last', 'all'}, default 'first'\n            When there are duplicate values that cannot all fit in a\n            Series of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n              of appearance.\n            - ``last`` : return the last `n` occurrences in reverse\n              order of appearance.\n            - ``all`` : keep all occurrences. This can result in a Series of\n              size larger than `n`.\n\n        Returns\n        -------\n        Series\n            The `n` smallest values in the Series, sorted in increasing order.\n\n        See Also\n        --------\n        Series.nlargest: Get the `n` largest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values().head(n)`` for small `n` relative to\n        the size of the ``Series`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Brunei\": 434000, \"Malta\": 434000,\n        ...                         \"Maldives\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n        >>> s = pd.Series(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Brunei        434000\n        Malta         434000\n        Maldives      434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Montserrat      5200\n        dtype: int64\n\n        The `n` smallest elements where ``n=5`` by default.\n\n        >>> s.nsmallest()\n        Montserrat    5200\n        Nauru        11300\n        Tuvalu       11300\n        Anguilla     11300\n        Iceland     337000\n        dtype: int64\n\n        The `n` smallest elements where ``n=3``. Default `keep` value is\n        'first' so Nauru and Tuvalu will be kept.\n\n        >>> s.nsmallest(3)\n        Montserrat   5200\n        Nauru       11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` and keeping the last\n        duplicates. Anguilla and Tuvalu will be kept since they are the last\n        with value 11300 based on the index order.\n\n        >>> s.nsmallest(3, keep='last')\n        Montserrat   5200\n        Anguilla    11300\n        Tuvalu      11300\n        dtype: int64\n\n        The `n` smallest elements where ``n=3`` with all duplicates kept. Note\n        that the returned Series has four elements due to the three duplicates.\n\n        >>> s.nsmallest(3, keep='all')\n        Montserrat   5200\n        Nauru       11300\n        Tuvalu      11300\n        Anguilla    11300\n        dtype: int64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nsmallest()\n\n    @doc(\n        klass=_shared_doc_kwargs[\"klass\"],\n        extra_params=dedent(\n            \"\"\"copy : bool, default True\n            Whether to copy underlying data.\"\"\"\n        ),\n        examples=dedent(\n            \"\"\"Examples\n        --------\n        >>> s = pd.Series(\n        ...     [\"A\", \"B\", \"A\", \"C\"],\n        ...     index=[\n        ...         [\"Final exam\", \"Final exam\", \"Coursework\", \"Coursework\"],\n        ...         [\"History\", \"Geography\", \"History\", \"Geography\"],\n        ...         [\"January\", \"February\", \"March\", \"April\"],\n        ...     ],\n        ... )\n        >>> s\n        Final exam  History     January      A\n                    Geography   February     B\n        Coursework  History     March        A\n                    Geography   April        C\n        dtype: object\n\n        In the following example, we will swap the levels of the indices.\n        Here, we will swap the levels column-wise, but levels can be swapped row-wise\n        in a similar manner. Note that column-wise is the default behaviour.\n        By not supplying any arguments for i and j, we swap the last and second to\n        last indices.\n\n        >>> s.swaplevel()\n        Final exam  January     History         A\n                    February    Geography       B\n        Coursework  March       History         A\n                    April       Geography       C\n        dtype: object\n\n        By supplying one argument, we can choose which index to swap the last\n        index with. We can for example swap the first index with the last one as\n        follows.\n\n        >>> s.swaplevel(0)\n        January     History     Final exam      A\n        February    Geography   Final exam      B\n        March       History     Coursework      A\n        April       Geography   Coursework      C\n        dtype: object\n\n        We can also define explicitly which indices we want to swap by supplying values\n        for both i and j. Here, we for example swap the first and second indices.\n\n        >>> s.swaplevel(0, 1)\n        History     Final exam  January         A\n        Geography   Final exam  February        B\n        History     Coursework  March           A\n        Geography   Coursework  April           C\n        dtype: object\"\"\"\n        ),\n    )\n    def swaplevel(self, i=-2, j=-1, copy=True) -> Series:\n        \"\"\"\n        Swap levels i and j in a :class:`MultiIndex`.\n\n        Default is to swap the two innermost levels of the index.\n\n        Parameters\n        ----------\n        i, j : int or str\n            Levels of the indices to be swapped. Can pass level name as string.\n        {extra_params}\n\n        Returns\n        -------\n        {klass}\n            {klass} with levels swapped in MultiIndex.\n\n        {examples}\n        \"\"\"\n        assert isinstance(self.index, MultiIndex)\n        new_index = self.index.swaplevel(i, j)\n        return self._constructor(self._values, index=new_index, copy=copy).__finalize__(\n            self, method=\"swaplevel\"\n        )\n\n    def reorder_levels(self, order) -> Series:\n        \"\"\"\n        Rearrange index levels using input order.\n\n        May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int representing new level order\n            Reference level by number or key.\n\n        Returns\n        -------\n        type of caller (new object)\n        \"\"\"\n        if not isinstance(self.index, MultiIndex):  # pragma: no cover\n            raise Exception(\"Can only reorder levels on a hierarchical axis.\")\n\n        result = self.copy()\n        assert isinstance(result.index, MultiIndex)\n        result.index = result.index.reorder_levels(order)\n        return result\n\n    def explode(self, ignore_index: bool = False) -> Series:\n        \"\"\"\n        Transform each element of a list-like to a row.\n\n        .. versionadded:: 0.25.0\n\n        Parameters\n        ----------\n        ignore_index : bool, default False\n            If True, the resulting index will be labeled 0, 1, …, n - 1.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        Series\n            Exploded lists to rows; index will be duplicated for these rows.\n\n        See Also\n        --------\n        Series.str.split : Split string values on specified separator.\n        Series.unstack : Unstack, a.k.a. pivot, Series with MultiIndex\n            to produce DataFrame.\n        DataFrame.melt : Unpivot a DataFrame from wide format to long format.\n        DataFrame.explode : Explode a DataFrame from list-like\n            columns to long format.\n\n        Notes\n        -----\n        This routine will explode list-likes including lists, tuples, sets,\n        Series, and np.ndarray. The result dtype of the subset rows will\n        be object. Scalars will be returned unchanged, and empty list-likes will\n        result in a np.nan for that row. In addition, the ordering of elements in\n        the output will be non-deterministic when exploding sets.\n\n        Examples\n        --------\n        >>> s = pd.Series([[1, 2, 3], 'foo', [], [3, 4]])\n        >>> s\n        0    [1, 2, 3]\n        1          foo\n        2           []\n        3       [3, 4]\n        dtype: object\n\n        >>> s.explode()\n        0      1\n        0      2\n        0      3\n        1    foo\n        2    NaN\n        3      3\n        3      4\n        dtype: object\n        \"\"\"\n        if not len(self) or not is_object_dtype(self):\n            result = self.copy()\n            return result.reset_index(drop=True) if ignore_index else result\n\n        values, counts = reshape.explode(np.asarray(self._values))\n\n        if ignore_index:\n            index = default_index(len(values))\n        else:\n            index = self.index.repeat(counts)\n\n        return self._constructor(values, index=index, name=self.name)\n\n    def unstack(self, level=-1, fill_value=None) -> DataFrame:\n        \"\"\"\n        Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.\n\n        Parameters\n        ----------\n        level : int, str, or list of these, default last level\n            Level(s) to unstack, can pass level name.\n        fill_value : scalar value, default None\n            Value to use when replacing NaN values.\n\n        Returns\n        -------\n        DataFrame\n            Unstacked Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4],\n        ...               index=pd.MultiIndex.from_product([['one', 'two'],\n        ...                                                 ['a', 'b']]))\n        >>> s\n        one  a    1\n             b    2\n        two  a    3\n             b    4\n        dtype: int64\n\n        >>> s.unstack(level=-1)\n             a  b\n        one  1  2\n        two  3  4\n\n        >>> s.unstack(level=0)\n           one  two\n        a    1    3\n        b    2    4\n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n\n        return unstack(self, level, fill_value)\n\n    # ----------------------------------------------------------------------\n    # function application\n\n    def map(self, arg, na_action=None) -> Series:\n        \"\"\"\n        Map values of Series according to input correspondence.\n\n        Used for substituting each value in a Series with another value,\n        that may be derived from a function, a ``dict`` or\n        a :class:`Series`.\n\n        Parameters\n        ----------\n        arg : function, collections.abc.Mapping subclass or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}, default None\n            If 'ignore', propagate NaN values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        Series\n            Same index as caller.\n\n        See Also\n        --------\n        Series.apply : For applying more complex functions on a Series.\n        DataFrame.apply : Apply a function row-/column-wise.\n        DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n\n        Notes\n        -----\n        When ``arg`` is a dictionary, values in Series that are not in the\n        dictionary (as keys) are converted to ``NaN``. However, if the\n        dictionary is a ``dict`` subclass that defines ``__missing__`` (i.e.\n        provides a method for default values), then this default is used\n        rather than ``NaN``.\n\n        Examples\n        --------\n        >>> s = pd.Series(['cat', 'dog', np.nan, 'rabbit'])\n        >>> s\n        0      cat\n        1      dog\n        2      NaN\n        3   rabbit\n        dtype: object\n\n        ``map`` accepts a ``dict`` or a ``Series``. Values that are not found\n        in the ``dict`` are converted to ``NaN``, unless the dict has a default\n        value (e.g. ``defaultdict``):\n\n        >>> s.map({'cat': 'kitten', 'dog': 'puppy'})\n        0   kitten\n        1    puppy\n        2      NaN\n        3      NaN\n        dtype: object\n\n        It also accepts a function:\n\n        >>> s.map('I am a {}'.format)\n        0       I am a cat\n        1       I am a dog\n        2       I am a nan\n        3    I am a rabbit\n        dtype: object\n\n        To avoid applying the function to missing values (and keep them as\n        ``NaN``) ``na_action='ignore'`` can be used:\n\n        >>> s.map('I am a {}'.format, na_action='ignore')\n        0     I am a cat\n        1     I am a dog\n        2            NaN\n        3  I am a rabbit\n        dtype: object\n        \"\"\"\n        new_values = self._map_values(arg, na_action=na_action)\n        return self._constructor(new_values, index=self.index).__finalize__(\n            self, method=\"map\"\n        )\n\n    def _gotitem(self, key, ndim, subset=None) -> Series:\n        \"\"\"\n        Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : {1, 2}\n            Requested ndim of result.\n        subset : object, default None\n            Subset to act on.\n        \"\"\"\n        return self\n\n    _agg_see_also_doc = dedent(\n        \"\"\"\n    See Also\n    --------\n    Series.apply : Invoke function on a Series.\n    Series.transform : Transform function producing a Series with like indexes.\n    \"\"\"\n    )\n\n    _agg_examples_doc = dedent(\n        \"\"\"\n    Examples\n    --------\n    >>> s = pd.Series([1, 2, 3, 4])\n    >>> s\n    0    1\n    1    2\n    2    3\n    3    4\n    dtype: int64\n\n    >>> s.agg('min')\n    1\n\n    >>> s.agg(['min', 'max'])\n    min   1\n    max   4\n    dtype: int64\n    \"\"\"\n    )\n\n    @doc(\n        generic._shared_docs[\"aggregate\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        axis=_shared_doc_kwargs[\"axis\"],\n        see_also=_agg_see_also_doc,\n        examples=_agg_examples_doc,\n    )\n    def aggregate(self, func=None, axis=0, *args, **kwargs):\n        # Validate the axis parameter\n        self._get_axis_number(axis)\n\n        # if func is None, will switch to user-provided \"named aggregation\" kwargs\n        if func is None:\n            func = dict(kwargs.items())\n\n        op = SeriesApply(self, func, convert_dtype=False, args=args, kwargs=kwargs)\n        result = op.agg()\n        return result\n\n    agg = aggregate\n\n    @doc(\n        _shared_docs[\"transform\"],\n        klass=_shared_doc_kwargs[\"klass\"],\n        axis=_shared_doc_kwargs[\"axis\"],\n    )\n    def transform(\n        self, func: AggFuncType, axis: Axis = 0, *args, **kwargs\n    ) -> DataFrame | Series:\n        # Validate axis argument\n        self._get_axis_number(axis)\n        result = SeriesApply(\n            self, func=func, convert_dtype=True, args=args, kwargs=kwargs\n        ).transform()\n        return result\n\n    def apply(\n        self,\n        func: AggFuncType,\n        convert_dtype: bool = True,\n        args: tuple[Any, ...] = (),\n        **kwargs,\n    ) -> DataFrame | Series:\n        \"\"\"\n        Invoke function on values of Series.\n\n        Can be ufunc (a NumPy function that applies to the entire Series)\n        or a Python function that only works on single values.\n\n        Parameters\n        ----------\n        func : function\n            Python function or NumPy ufunc to apply.\n        convert_dtype : bool, default True\n            Try to find better dtype for elementwise function results. If\n            False, leave as dtype=object. Note that the dtype is always\n            preserved for some extension array dtypes, such as Categorical.\n        args : tuple\n            Positional arguments passed to func after the series value.\n        **kwargs\n            Additional keyword arguments passed to func.\n\n        Returns\n        -------\n        Series or DataFrame\n            If func returns a Series object the result will be a DataFrame.\n\n        See Also\n        --------\n        Series.map: For element-wise operations.\n        Series.agg: Only perform aggregating type operations.\n        Series.transform: Only perform transforming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n        for more details.\n\n        Examples\n        --------\n        Create a series with typical summer temperatures for each city.\n\n        >>> s = pd.Series([20, 21, 12],\n        ...               index=['London', 'New York', 'Helsinki'])\n        >>> s\n        London      20\n        New York    21\n        Helsinki    12\n        dtype: int64\n\n        Square the values by defining a function and passing it as an\n        argument to ``apply()``.\n\n        >>> def square(x):\n        ...     return x ** 2\n        >>> s.apply(square)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Square the values by passing an anonymous function as an\n        argument to ``apply()``.\n\n        >>> s.apply(lambda x: x ** 2)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Define a custom function that needs additional positional\n        arguments and pass these additional arguments using the\n        ``args`` keyword.\n\n        >>> def subtract_custom_value(x, custom_value):\n        ...     return x - custom_value\n\n        >>> s.apply(subtract_custom_value, args=(5,))\n        London      15\n        New York    16\n        Helsinki     7\n        dtype: int64\n\n        Define a custom function that takes keyword arguments\n        and pass these arguments to ``apply``.\n\n        >>> def add_custom_values(x, **kwargs):\n        ...     for month in kwargs:\n        ...         x += kwargs[month]\n        ...     return x\n\n        >>> s.apply(add_custom_values, june=30, july=20, august=25)\n        London      95\n        New York    96\n        Helsinki    87\n        dtype: int64\n\n        Use a function from the Numpy library.\n\n        >>> s.apply(np.log)\n        London      2.995732\n        New York    3.044522\n        Helsinki    2.484907\n        dtype: float64\n        \"\"\"\n        return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n\n    def _reduce(\n        self,\n        op,\n        name: str,\n        *,\n        axis=0,\n        skipna=True,\n        numeric_only=None,\n        filter_type=None,\n        **kwds,\n    ):\n        \"\"\"\n        Perform a reduction operation.\n\n        If we have an ndarray as a value, then simply perform the operation,\n        otherwise delegate to the object.\n        \"\"\"\n        delegate = self._values\n\n        if axis is not None:\n            self._get_axis_number(axis)\n\n        if isinstance(delegate, ExtensionArray):\n            # dispatch to ExtensionArray interface\n            return delegate._reduce(name, skipna=skipna, **kwds)\n\n        else:\n            # dispatch to numpy arrays\n            if numeric_only:\n                kwd_name = \"numeric_only\"\n                if name in [\"any\", \"all\"]:\n                    kwd_name = \"bool_only\"\n                raise NotImplementedError(\n                    f\"Series.{name} does not implement {kwd_name}.\"\n                )\n            with np.errstate(all=\"ignore\"):\n                return op(delegate, skipna=skipna, **kwds)\n\n    def _reindex_indexer(\n        self, new_index: Index | None, indexer: npt.NDArray[np.intp] | None, copy: bool\n    ) -> Series:\n        # Note: new_index is None iff indexer is None\n        # if not None, indexer is np.intp\n        if indexer is None:\n            if copy:\n                return self.copy()\n            return self\n\n        new_values = algorithms.take_nd(\n            self._values, indexer, allow_fill=True, fill_value=None\n        )\n        return self._constructor(new_values, index=new_index)\n\n    def _needs_reindex_multi(self, axes, method, level) -> bool:\n        \"\"\"\n        Check if we do need a multi reindex; this is for compat with\n        higher dims.\n        \"\"\"\n        return False\n\n    # error: Cannot determine type of 'align'\n    @doc(\n        NDFrame.align,  # type: ignore[has-type]\n        klass=_shared_doc_kwargs[\"klass\"],\n        axes_single_arg=_shared_doc_kwargs[\"axes_single_arg\"],\n    )\n    def align(\n        self,\n        other,\n        join=\"outer\",\n        axis=None,\n        level=None,\n        copy=True,\n        fill_value=None,\n        method=None,\n        limit=None,\n        fill_axis=0,\n        broadcast_axis=None,\n    ):\n        return super().align(\n            other,\n            join=join,\n            axis=axis,\n            level=level,\n            copy=copy,\n            fill_value=fill_value,\n            method=method,\n            limit=limit,\n            fill_axis=fill_axis,\n            broadcast_axis=broadcast_axis,\n        )\n\n    def rename(\n        self,\n        mapper=None,\n        *,\n        index=None,\n        columns=None,\n        axis=None,\n        copy=True,\n        inplace=False,\n        level=None,\n        errors=\"ignore\",\n    ) -> Series | None:\n        \"\"\"\n        Alter Series index labels or name.\n\n        Function / dict values must be unique (1-to-1). Labels not contained in\n        a dict / Series will be left as-is. Extra labels listed don't throw an\n        error.\n\n        Alternatively, change ``Series.name`` with a scalar value.\n\n        See the :ref:`user guide <basics.rename>` for more.\n\n        Parameters\n        ----------\n        axis : {0 or \"index\"}\n            Unused. Accepted for compatibility with DataFrame method only.\n        mapper : scalar, hashable sequence, dict-like or function, optional\n            Functions or dict-like are transformations to apply to\n            the index.\n            Scalar or hashable sequence-like will alter the ``Series.name``\n            attribute.\n\n        **kwargs\n            Additional keyword arguments passed to the function. Only the\n            \"inplace\" keyword is used.\n\n        Returns\n        -------\n        Series or None\n            Series with index labels or name altered or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.rename : Corresponding DataFrame method.\n        Series.rename_axis : Set the name of the axis.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n        >>> s.rename(\"my_name\")  # scalar, changes Series.name\n        0    1\n        1    2\n        2    3\n        Name: my_name, dtype: int64\n        >>> s.rename(lambda x: x ** 2)  # function, changes labels\n        0    1\n        1    2\n        4    3\n        dtype: int64\n        >>> s.rename({1: 3, 2: 5})  # mapping, changes labels\n        0    1\n        3    2\n        5    3\n        dtype: int64\n        \"\"\"\n        if axis is not None:\n            # Make sure we raise if an invalid 'axis' is passed.\n            axis = self._get_axis_number(axis)\n\n        if index is not None and mapper is not None:\n            raise TypeError(\"Cannot specify both 'mapper' and 'index'\")\n        if mapper is None:\n            mapper = index\n        if callable(mapper) or is_dict_like(mapper):\n            return super().rename(\n                mapper, copy=copy, inplace=inplace, level=level, errors=errors\n            )\n        else:\n            return self._set_name(mapper, inplace=inplace)\n\n    @overload\n    def set_axis(\n        self, labels, axis: Axis = ..., inplace: Literal[False] = ...\n    ) -> Series:\n        ...\n\n    @overload\n    def set_axis(self, labels, axis: Axis, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def set_axis(self, labels, *, inplace: Literal[True]) -> None:\n        ...\n\n    @overload\n    def set_axis(self, labels, axis: Axis = ..., inplace: bool = ...) -> Series | None:\n        ...\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"labels\"])\n    @Appender(\n        \"\"\"\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        dtype: int64\n\n        >>> s.set_axis(['a', 'b', 'c'], axis=0)\n        a    1\n        b    2\n        c    3\n        dtype: int64\n    \"\"\"\n    )\n    @Substitution(\n        **_shared_doc_kwargs,\n        extended_summary_sub=\"\",\n        axis_description_sub=\"\",\n        see_also_sub=\"\",\n    )\n    @Appender(generic.NDFrame.set_axis.__doc__)\n    def set_axis(self, labels, axis: Axis = 0, inplace: bool = False):\n        return super().set_axis(labels, axis=axis, inplace=inplace)\n\n    # error: Cannot determine type of 'reindex'\n    @doc(\n        NDFrame.reindex,  # type: ignore[has-type]\n        klass=_shared_doc_kwargs[\"klass\"],\n        axes=_shared_doc_kwargs[\"axes\"],\n        optional_labels=_shared_doc_kwargs[\"optional_labels\"],\n        optional_axis=_shared_doc_kwargs[\"optional_axis\"],\n    )\n    def reindex(self, index=None, **kwargs):\n        return super().reindex(index=index, **kwargs)\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"labels\"])\n    def drop(\n        self,\n        labels=None,\n        axis=0,\n        index=None,\n        columns=None,\n        level=None,\n        inplace=False,\n        errors=\"raise\",\n    ) -> Series:\n        \"\"\"\n        Return Series with specified index labels removed.\n\n        Remove elements of a Series based on specifying the index labels.\n        When using a multi-index, labels on different levels can be removed\n        by specifying the level.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index labels to drop.\n        axis : 0, default 0\n            Redundant for application on Series.\n        index : single label or list-like\n            Redundant for application on Series, but 'index' can be used instead\n            of 'labels'.\n        columns : single label or list-like\n            No change is made to the Series; use 'index' or 'labels' instead.\n        level : int or level name, optional\n            For MultiIndex, level for which the labels will be removed.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and only existing labels are dropped.\n\n        Returns\n        -------\n        Series or None\n            Series with specified index labels removed or None if ``inplace=True``.\n\n        Raises\n        ------\n        KeyError\n            If none of the labels are found in the index.\n\n        See Also\n        --------\n        Series.reindex : Return only specified index labels of Series.\n        Series.dropna : Return series without null values.\n        Series.drop_duplicates : Return Series with duplicate values removed.\n        DataFrame.drop : Drop specified labels from rows or columns.\n\n        Examples\n        --------\n        >>> s = pd.Series(data=np.arange(3), index=['A', 'B', 'C'])\n        >>> s\n        A  0\n        B  1\n        C  2\n        dtype: int64\n\n        Drop labels B en C\n\n        >>> s.drop(labels=['B', 'C'])\n        A  0\n        dtype: int64\n\n        Drop 2nd level label in MultiIndex Series\n\n        >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n        ...                              ['speed', 'weight', 'length']],\n        ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> s = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...               index=midx)\n        >>> s\n        lama    speed      45.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        dtype: float64\n\n        >>> s.drop(labels='weight', level=1)\n        lama    speed      45.0\n                length      1.2\n        cow     speed      30.0\n                length      1.5\n        falcon  speed     320.0\n                length      0.3\n        dtype: float64\n        \"\"\"\n        return super().drop(\n            labels=labels,\n            axis=axis,\n            index=index,\n            columns=columns,\n            level=level,\n            inplace=inplace,\n            errors=errors,\n        )\n\n    @overload\n    def fillna(\n        self,\n        value=...,\n        method: FillnaOptions | None = ...,\n        axis: Axis | None = ...,\n        inplace: Literal[False] = ...,\n        limit=...,\n        downcast=...,\n    ) -> Series:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        method: FillnaOptions | None,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        method: FillnaOptions | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        *,\n        method: FillnaOptions | None,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        *,\n        axis: Axis | None,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value,\n        method: FillnaOptions | None,\n        *,\n        inplace: Literal[True],\n        limit=...,\n        downcast=...,\n    ) -> None:\n        ...\n\n    @overload\n    def fillna(\n        self,\n        value=...,\n        method: FillnaOptions | None = ...,\n        axis: Axis | None = ...,\n        inplace: bool = ...,\n        limit=...,\n        downcast=...,\n    ) -> Series | None:\n        ...\n\n    # error: Cannot determine type of 'fillna'\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"value\"])\n    @doc(NDFrame.fillna, **_shared_doc_kwargs)  # type: ignore[has-type]\n    def fillna(\n        self,\n        value: object | ArrayLike | None = None,\n        method: FillnaOptions | None = None,\n        axis=None,\n        inplace=False,\n        limit=None,\n        downcast=None,\n    ) -> Series | None:\n        return super().fillna(\n            value=value,\n            method=method,\n            axis=axis,\n            inplace=inplace,\n            limit=limit,\n            downcast=downcast,\n        )\n\n    def pop(self, item: Hashable) -> Any:\n        \"\"\"\n        Return item and drops from series. Raise KeyError if not found.\n\n        Parameters\n        ----------\n        item : label\n            Index of the element that needs to be removed.\n\n        Returns\n        -------\n        Value that is popped from series.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1,2,3])\n\n        >>> ser.pop(0)\n        1\n\n        >>> ser\n        1    2\n        2    3\n        dtype: int64\n        \"\"\"\n        return super().pop(item=item)\n\n    # error: Cannot determine type of 'replace'\n    @doc(\n        NDFrame.replace,  # type: ignore[has-type]\n        klass=_shared_doc_kwargs[\"klass\"],\n        inplace=_shared_doc_kwargs[\"inplace\"],\n        replace_iloc=_shared_doc_kwargs[\"replace_iloc\"],\n    )\n    def replace(\n        self,\n        to_replace=None,\n        value=None,\n        inplace=False,\n        limit=None,\n        regex=False,\n        method=\"pad\",\n    ):\n        return super().replace(\n            to_replace=to_replace,\n            value=value,\n            inplace=inplace,\n            limit=limit,\n            regex=regex,\n            method=method,\n        )\n\n    def _replace_single(self, to_replace, method: str, inplace: bool, limit):\n        \"\"\"\n        Replaces values in a Series using the fill method specified when no\n        replacement value is given in the replace method\n        \"\"\"\n\n        result = self if inplace else self.copy()\n\n        values = result._values\n        mask = missing.mask_missing(values, to_replace)\n\n        if isinstance(values, ExtensionArray):\n            # dispatch to the EA's _pad_mask_inplace method\n            values._fill_mask_inplace(method, limit, mask)\n        else:\n            fill_f = missing.get_fill_func(method)\n            values, _ = fill_f(values, limit=limit, mask=mask)\n\n        if inplace:\n            return\n        return result\n\n    # error: Cannot determine type of 'shift'\n    @doc(NDFrame.shift, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def shift(self, periods=1, freq=None, axis=0, fill_value=None) -> Series:\n        return super().shift(\n            periods=periods, freq=freq, axis=axis, fill_value=fill_value\n        )\n\n    def memory_usage(self, index: bool = True, deep: bool = False) -> int:\n        \"\"\"\n        Return the memory usage of the Series.\n\n        The memory usage can optionally include the contribution of\n        the index and of elements of `object` dtype.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the Series index.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned value.\n\n        Returns\n        -------\n        int\n            Bytes of memory consumed.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\n            array.\n        DataFrame.memory_usage : Bytes consumed by a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(range(3))\n        >>> s.memory_usage()\n        152\n\n        Not including the index gives the size of the rest of the data, which\n        is necessarily smaller:\n\n        >>> s.memory_usage(index=False)\n        24\n\n        The memory footprint of `object` values is ignored by default:\n\n        >>> s = pd.Series([\"a\", \"b\"])\n        >>> s.values\n        array(['a', 'b'], dtype=object)\n        >>> s.memory_usage()\n        144\n        >>> s.memory_usage(deep=True)\n        244\n        \"\"\"\n        v = self._memory_usage(deep=deep)\n        if index:\n            v += self.index.memory_usage(deep=deep)\n        return v\n\n    def isin(self, values) -> Series:\n        \"\"\"\n        Whether elements in Series are contained in `values`.\n\n        Return a boolean Series showing whether each element in the Series\n        matches an element in the passed sequence of `values` exactly.\n\n        Parameters\n        ----------\n        values : set or list-like\n            The sequence of values to test. Passing in a single string will\n            raise a ``TypeError``. Instead, turn a single string into a\n            list of one element.\n\n        Returns\n        -------\n        Series\n            Series of booleans indicating if each element is in values.\n\n        Raises\n        ------\n        TypeError\n          * If `values` is a string\n\n        See Also\n        --------\n        DataFrame.isin : Equivalent method on DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama',\n        ...                'hippo'], name='animal')\n        >>> s.isin(['cow', 'lama'])\n        0     True\n        1     True\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n\n        To invert the boolean values, use the ``~`` operator:\n\n        >>> ~s.isin(['cow', 'lama'])\n        0    False\n        1    False\n        2    False\n        3     True\n        4    False\n        5     True\n        Name: animal, dtype: bool\n\n        Passing a single string as ``s.isin('lama')`` will raise an error. Use\n        a list of one element instead:\n\n        >>> s.isin(['lama'])\n        0     True\n        1    False\n        2     True\n        3    False\n        4     True\n        5    False\n        Name: animal, dtype: bool\n\n        Strings and integers are distinct and are therefore not comparable:\n\n        >>> pd.Series([1]).isin(['1'])\n        0    False\n        dtype: bool\n        >>> pd.Series([1.1]).isin(['1.1'])\n        0    False\n        dtype: bool\n        \"\"\"\n        result = algorithms.isin(self._values, values)\n        return self._constructor(result, index=self.index).__finalize__(\n            self, method=\"isin\"\n        )\n\n    def between(self, left, right, inclusive=\"both\") -> Series:\n        \"\"\"\n        Return boolean Series equivalent to left <= series <= right.\n\n        This function returns a boolean vector containing `True` wherever the\n        corresponding Series element is between the boundary values `left` and\n        `right`. NA values are treated as `False`.\n\n        Parameters\n        ----------\n        left : scalar or list-like\n            Left boundary.\n        right : scalar or list-like\n            Right boundary.\n        inclusive : {\"both\", \"neither\", \"left\", \"right\"}\n            Include boundaries. Whether to set each bound as closed or open.\n\n            .. versionchanged:: 1.3.0\n\n        Returns\n        -------\n        Series\n            Series representing whether each element is between left and\n            right (inclusive).\n\n        See Also\n        --------\n        Series.gt : Greater than of series and other.\n        Series.lt : Less than of series and other.\n\n        Notes\n        -----\n        This function is equivalent to ``(left <= ser) & (ser <= right)``\n\n        Examples\n        --------\n        >>> s = pd.Series([2, 0, 4, 8, np.nan])\n\n        Boundary values are included by default:\n\n        >>> s.between(1, 4)\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        With `inclusive` set to ``\"neither\"`` boundary values are excluded:\n\n        >>> s.between(1, 4, inclusive=\"neither\")\n        0     True\n        1    False\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        `left` and `right` can be any scalar value:\n\n        >>> s = pd.Series(['Alice', 'Bob', 'Carol', 'Eve'])\n        >>> s.between('Anna', 'Daniel')\n        0    False\n        1     True\n        2     True\n        3    False\n        dtype: bool\n        \"\"\"\n        if inclusive is True or inclusive is False:\n            warnings.warn(\n                \"Boolean inputs to the `inclusive` argument are deprecated in \"\n                \"favour of `both` or `neither`.\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n            if inclusive:\n                inclusive = \"both\"\n            else:\n                inclusive = \"neither\"\n        if inclusive == \"both\":\n            lmask = self >= left\n            rmask = self <= right\n        elif inclusive == \"left\":\n            lmask = self >= left\n            rmask = self < right\n        elif inclusive == \"right\":\n            lmask = self > left\n            rmask = self <= right\n        elif inclusive == \"neither\":\n            lmask = self > left\n            rmask = self < right\n        else:\n            raise ValueError(\n                \"Inclusive has to be either string of 'both',\"\n                \"'left', 'right', or 'neither'.\"\n            )\n\n        return lmask & rmask\n\n    # ----------------------------------------------------------------------\n    # Convert to types that support pd.NA\n\n    def _convert_dtypes(\n        self,\n        infer_objects: bool = True,\n        convert_string: bool = True,\n        convert_integer: bool = True,\n        convert_boolean: bool = True,\n        convert_floating: bool = True,\n    ) -> Series:\n        input_series = self\n        if infer_objects:\n            input_series = input_series.infer_objects()\n            if is_object_dtype(input_series):\n                input_series = input_series.copy()\n\n        if convert_string or convert_integer or convert_boolean or convert_floating:\n            inferred_dtype = convert_dtypes(\n                input_series._values,\n                convert_string,\n                convert_integer,\n                convert_boolean,\n                convert_floating,\n            )\n            result = input_series.astype(inferred_dtype)\n        else:\n            result = input_series.copy()\n        return result\n\n    # error: Cannot determine type of 'isna'\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def isna(self) -> Series:\n        return generic.NDFrame.isna(self)\n\n    # error: Cannot determine type of 'isna'\n    @doc(NDFrame.isna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def isnull(self) -> Series:\n        return super().isnull()\n\n    # error: Cannot determine type of 'notna'\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def notna(self) -> Series:\n        return super().notna()\n\n    # error: Cannot determine type of 'notna'\n    @doc(NDFrame.notna, klass=_shared_doc_kwargs[\"klass\"])  # type: ignore[has-type]\n    def notnull(self) -> Series:\n        return super().notnull()\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def dropna(self, axis=0, inplace=False, how=None):\n        \"\"\"\n        Return a new Series with missing values removed.\n\n        See the :ref:`User Guide <missing_data>` for more on which values are\n        considered missing, and how to work with missing data.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            There is only one axis to drop values from.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        how : str, optional\n            Not in use. Kept for compatibility.\n\n        Returns\n        -------\n        Series or None\n            Series with NA entries dropped from it or None if ``inplace=True``.\n\n        See Also\n        --------\n        Series.isna: Indicate missing values.\n        Series.notna : Indicate existing (non-missing) values.\n        Series.fillna : Replace missing values.\n        DataFrame.dropna : Drop rows or columns which contain NA values.\n        Index.dropna : Drop missing indices.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1., 2., np.nan])\n        >>> ser\n        0    1.0\n        1    2.0\n        2    NaN\n        dtype: float64\n\n        Drop NA values from a Series.\n\n        >>> ser.dropna()\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Keep the Series with valid entries in the same variable.\n\n        >>> ser.dropna(inplace=True)\n        >>> ser\n        0    1.0\n        1    2.0\n        dtype: float64\n\n        Empty strings are not considered NA values. ``None`` is considered an\n        NA value.\n\n        >>> ser = pd.Series([np.NaN, 2, pd.NaT, '', None, 'I stay'])\n        >>> ser\n        0       NaN\n        1         2\n        2       NaT\n        3\n        4      None\n        5    I stay\n        dtype: object\n        >>> ser.dropna()\n        1         2\n        3\n        5    I stay\n        dtype: object\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        # Validate the axis parameter\n        self._get_axis_number(axis or 0)\n\n        if self._can_hold_na:\n            result = remove_na_arraylike(self)\n            if inplace:\n                self._update_inplace(result)\n            else:\n                return result\n        else:\n            if inplace:\n                # do nothing\n                pass\n            else:\n                return self.copy()\n\n    # ----------------------------------------------------------------------\n    # Time series-oriented methods\n\n    # error: Cannot determine type of 'asfreq'\n    @doc(NDFrame.asfreq, **_shared_doc_kwargs)  # type: ignore[has-type]\n    def asfreq(\n        self,\n        freq,\n        method=None,\n        how: str | None = None,\n        normalize: bool = False,\n        fill_value=None,\n    ) -> Series:\n        return super().asfreq(\n            freq=freq,\n            method=method,\n            how=how,\n            normalize=normalize,\n            fill_value=fill_value,\n        )\n\n    # error: Cannot determine type of 'resample'\n    @doc(NDFrame.resample, **_shared_doc_kwargs)  # type: ignore[has-type]\n    def resample(\n        self,\n        rule,\n        axis=0,\n        closed: str | None = None,\n        label: str | None = None,\n        convention: str = \"start\",\n        kind: str | None = None,\n        loffset=None,\n        base: int | None = None,\n        on=None,\n        level=None,\n        origin: str | TimestampConvertibleTypes = \"start_day\",\n        offset: TimedeltaConvertibleTypes | None = None,\n    ) -> Resampler:\n        return super().resample(\n            rule=rule,\n            axis=axis,\n            closed=closed,\n            label=label,\n            convention=convention,\n            kind=kind,\n            loffset=loffset,\n            base=base,\n            on=on,\n            level=level,\n            origin=origin,\n            offset=offset,\n        )\n\n    def to_timestamp(self, freq=None, how=\"start\", copy=True) -> Series:\n        \"\"\"\n        Cast to DatetimeIndex of Timestamps, at *beginning* of period.\n\n        Parameters\n        ----------\n        freq : str, default frequency of PeriodIndex\n            Desired frequency.\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series with DatetimeIndex\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        if not isinstance(self.index, PeriodIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\n        new_index = self.index.to_timestamp(freq=freq, how=how)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_timestamp\"\n        )\n\n    def to_period(self, freq=None, copy=True) -> Series:\n        \"\"\"\n        Convert Series from DatetimeIndex to PeriodIndex.\n\n        Parameters\n        ----------\n        freq : str, default None\n            Frequency associated with the PeriodIndex.\n        copy : bool, default True\n            Whether or not to return a copy.\n\n        Returns\n        -------\n        Series\n            Series with index converted to PeriodIndex.\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\n        new_index = self.index.to_period(freq=freq)\n        return self._constructor(new_values, index=new_index).__finalize__(\n            self, method=\"to_period\"\n        )\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def ffill(\n        self: Series,\n        axis: None | Axis = None,\n        inplace: bool = False,\n        limit: None | int = None,\n        downcast=None,\n    ) -> Series | None:\n        return super().ffill(axis, inplace, limit, downcast)\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n    def bfill(\n        self: Series,\n        axis: None | Axis = None,\n        inplace: bool = False,\n        limit: None | int = None,\n        downcast=None,\n    ) -> Series | None:\n        return super().bfill(axis, inplace, limit, downcast)\n\n    @deprecate_nonkeyword_arguments(\n        version=None, allowed_args=[\"self\", \"lower\", \"upper\"]\n    )\n    def clip(\n        self: Series,\n        lower=None,\n        upper=None,\n        axis: Axis | None = None,\n        inplace: bool = False,\n        *args,\n        **kwargs,\n    ) -> Series | None:\n        return super().clip(lower, upper, axis, inplace, *args, **kwargs)\n\n    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"method\"])\n    def interpolate(\n        self: Series,\n        method: str = \"linear\",\n        axis: Axis = 0,\n        limit: int | None = None,\n        inplace: bool = False,\n        limit_direction: str | None = None,\n        limit_area: str | None = None,\n        downcast: str | None = None,\n        **kwargs,\n    ) -> Series | None:\n        return super().interpolate(\n            method,\n            axis,\n            limit,\n            inplace,\n            limit_direction,\n            limit_area,\n            downcast,\n            **kwargs,\n        )\n\n    @deprecate_nonkeyword_arguments(\n        version=None, allowed_args=[\"self\", \"cond\", \"other\"]\n    )\n    def where(\n        self,\n        cond,\n        other=np.nan,\n        inplace=False,\n        axis=None,\n        level=None,\n        errors=lib.no_default,\n        try_cast=lib.no_default,\n    ):\n        return super().where(cond, other, inplace, axis, level, errors, try_cast)\n\n    @deprecate_nonkeyword_arguments(\n        version=None, allowed_args=[\"self\", \"cond\", \"other\"]\n    )\n    def mask(\n        self,\n        cond,\n        other=np.nan,\n        inplace=False,\n        axis=None,\n        level=None,\n        errors=lib.no_default,\n        try_cast=lib.no_default,\n    ):\n        return super().mask(cond, other, inplace, axis, level, errors, try_cast)\n\n    # ----------------------------------------------------------------------\n    # Add index\n    _AXIS_ORDERS = [\"index\"]\n    _AXIS_LEN = len(_AXIS_ORDERS)\n    _info_axis_number = 0\n    _info_axis_name = \"index\"\n\n    index: Index = properties.AxisProperty(\n        axis=0, doc=\"The index (axis labels) of the Series.\"\n    )\n\n    # ----------------------------------------------------------------------\n    # Accessor Methods\n    # ----------------------------------------------------------------------\n    str = CachedAccessor(\"str\", StringMethods)\n    dt = CachedAccessor(\"dt\", CombinedDatetimelikeProperties)\n    cat = CachedAccessor(\"cat\", CategoricalAccessor)\n    plot = CachedAccessor(\"plot\", pandas.plotting.PlotAccessor)\n    sparse = CachedAccessor(\"sparse\", SparseAccessor)\n\n    # ----------------------------------------------------------------------\n    # Add plotting methods to Series\n    hist = pandas.plotting.hist_series\n\n    # ----------------------------------------------------------------------\n    # Template-Based Arithmetic/Comparison Methods\n\n    def _cmp_method(self, other, op):\n        res_name = ops.get_op_result_name(self, other)\n\n        if isinstance(other, Series) and not self._indexed_same(other):\n            raise ValueError(\"Can only compare identically-labeled Series objects\")\n\n        lvalues = self._values\n        rvalues = extract_array(other, extract_numpy=True, extract_range=True)\n\n        with np.errstate(all=\"ignore\"):\n            res_values = ops.comparison_op(lvalues, rvalues, op)\n\n        return self._construct_result(res_values, name=res_name)\n\n    def _logical_method(self, other, op):\n        res_name = ops.get_op_result_name(self, other)\n        self, other = ops.align_method_SERIES(self, other, align_asobject=True)\n\n        lvalues = self._values\n        rvalues = extract_array(other, extract_numpy=True, extract_range=True)\n\n        res_values = ops.logical_op(lvalues, rvalues, op)\n        return self._construct_result(res_values, name=res_name)\n\n    def _arith_method(self, other, op):\n        self, other = ops.align_method_SERIES(self, other)\n        return base.IndexOpsMixin._arith_method(self, other, op)\n\n\nSeries._add_numeric_operations()\n\n# Add arithmetic!\nops.add_flex_arithmetic_methods(Series)\n"
    },
    {
      "filename": "pandas/io/json/_normalize.py",
      "content": "# ---------------------------------------------------------------------\n# JSON normalization routines\nfrom __future__ import annotations\n\nfrom collections import (\n    abc,\n    defaultdict,\n)\nimport copy\nfrom typing import (\n    Any,\n    DefaultDict,\n    Iterable,\n)\n\nimport numpy as np\n\nfrom pandas._libs.writers import convert_json_to_lines\nfrom pandas._typing import Scalar\nfrom pandas.util._decorators import deprecate\n\nimport pandas as pd\nfrom pandas import DataFrame\n\n\ndef convert_to_line_delimits(s: str) -> str:\n    \"\"\"\n    Helper function that converts JSON lists to line delimited JSON.\n    \"\"\"\n    # Determine we have a JSON list to turn to lines otherwise just return the\n    # json object, only lists can\n    if not s[0] == \"[\" and s[-1] == \"]\":\n        return s\n    s = s[1:-1]\n\n    return convert_json_to_lines(s)\n\n\ndef nested_to_record(\n    ds,\n    prefix: str = \"\",\n    sep: str = \".\",\n    level: int = 0,\n    max_level: int | None = None,\n):\n    \"\"\"\n    A simplified json_normalize\n\n    Converts a nested dict into a flat dict (\"record\"), unlike json_normalize,\n    it does not attempt to extract a subset of the data.\n\n    Parameters\n    ----------\n    ds : dict or list of dicts\n    prefix: the prefix, optional, default: \"\"\n    sep : str, default '.'\n        Nested records will generate names separated by sep,\n        e.g., for sep='.', { 'foo' : { 'bar' : 0 } } -> foo.bar\n    level: int, optional, default: 0\n        The number of levels in the json string.\n\n    max_level: int, optional, default: None\n        The max depth to normalize.\n\n        .. versionadded:: 0.25.0\n\n    Returns\n    -------\n    d - dict or list of dicts, matching `ds`\n\n    Examples\n    --------\n    >>> nested_to_record(\n    ...     dict(flat1=1, dict1=dict(c=1, d=2), nested=dict(e=dict(c=1, d=2), d=2))\n    ... )\n    {\\\n'flat1': 1, \\\n'dict1.c': 1, \\\n'dict1.d': 2, \\\n'nested.e.c': 1, \\\n'nested.e.d': 2, \\\n'nested.d': 2\\\n}\n    \"\"\"\n    singleton = False\n    if isinstance(ds, dict):\n        ds = [ds]\n        singleton = True\n    new_ds = []\n    for d in ds:\n        new_d = copy.deepcopy(d)\n        for k, v in d.items():\n            # each key gets renamed with prefix\n            if not isinstance(k, str):\n                k = str(k)\n            if level == 0:\n                newkey = k\n            else:\n                newkey = prefix + sep + k\n\n            # flatten if type is dict and\n            # current dict level  < maximum level provided and\n            # only dicts gets recurse-flattened\n            # only at level>1 do we rename the rest of the keys\n            if not isinstance(v, dict) or (\n                max_level is not None and level >= max_level\n            ):\n                if level != 0:  # so we skip copying for top level, common case\n                    v = new_d.pop(k)\n                    new_d[newkey] = v\n                continue\n            else:\n                v = new_d.pop(k)\n                new_d.update(nested_to_record(v, newkey, sep, level + 1, max_level))\n        new_ds.append(new_d)\n\n    if singleton:\n        return new_ds[0]\n    return new_ds\n\n\ndef _normalise_json(\n    data: Any,\n    key_string: str,\n    normalized_dict: dict[str, Any],\n    separator: str,\n) -> dict[str, Any]:\n    \"\"\"\n    Main recursive function\n    Designed for the most basic use case of pd.json_normalize(data)\n    intended as a performance improvement, see #15621\n\n    Parameters\n    ----------\n    data : Any\n        Type dependent on types contained within nested Json\n    key_string : str\n        New key (with separator(s) in) for data\n    normalized_dict : dict\n        The new normalized/flattened Json dict\n    separator : str, default '.'\n        Nested records will generate names separated by sep,\n        e.g., for sep='.', { 'foo' : { 'bar' : 0 } } -> foo.bar\n    \"\"\"\n    if isinstance(data, dict):\n        for key, value in data.items():\n            new_key = f\"{key_string}{separator}{key}\"\n            _normalise_json(\n                data=value,\n                # to avoid adding the separator to the start of every key\n                # GH#43831 avoid adding key if key_string blank\n                key_string=new_key\n                if new_key[: len(separator)] != separator\n                else new_key[len(separator) :],\n                normalized_dict=normalized_dict,\n                separator=separator,\n            )\n    else:\n        normalized_dict[key_string] = data\n    return normalized_dict\n\n\ndef _normalise_json_ordered(data: dict[str, Any], separator: str) -> dict[str, Any]:\n    \"\"\"\n    Order the top level keys and then recursively go to depth\n\n    Parameters\n    ----------\n    data : dict or list of dicts\n    separator : str, default '.'\n        Nested records will generate names separated by sep,\n        e.g., for sep='.', { 'foo' : { 'bar' : 0 } } -> foo.bar\n\n    Returns\n    -------\n    dict or list of dicts, matching `normalised_json_object`\n    \"\"\"\n    top_dict_ = {k: v for k, v in data.items() if not isinstance(v, dict)}\n    nested_dict_ = _normalise_json(\n        data={k: v for k, v in data.items() if isinstance(v, dict)},\n        key_string=\"\",\n        normalized_dict={},\n        separator=separator,\n    )\n    return {**top_dict_, **nested_dict_}\n\n\ndef _simple_json_normalize(\n    ds: dict | list[dict],\n    sep: str = \".\",\n) -> dict | list[dict] | Any:\n    \"\"\"\n    A optimized basic json_normalize\n\n    Converts a nested dict into a flat dict (\"record\"), unlike\n    json_normalize and nested_to_record it doesn't do anything clever.\n    But for the most basic use cases it enhances performance.\n    E.g. pd.json_normalize(data)\n\n    Parameters\n    ----------\n    ds : dict or list of dicts\n    sep : str, default '.'\n        Nested records will generate names separated by sep,\n        e.g., for sep='.', { 'foo' : { 'bar' : 0 } } -> foo.bar\n\n    Returns\n    -------\n    frame : DataFrame\n    d - dict or list of dicts, matching `normalised_json_object`\n\n    Examples\n    --------\n    >>> _simple_json_normalize(\n    ...     {\n    ...         \"flat1\": 1,\n    ...         \"dict1\": {\"c\": 1, \"d\": 2},\n    ...         \"nested\": {\"e\": {\"c\": 1, \"d\": 2}, \"d\": 2},\n    ...     }\n    ... )\n    {\\\n'flat1': 1, \\\n'dict1.c': 1, \\\n'dict1.d': 2, \\\n'nested.e.c': 1, \\\n'nested.e.d': 2, \\\n'nested.d': 2\\\n}\n\n    \"\"\"\n    normalised_json_object = {}\n    # expect a dictionary, as most jsons are. However, lists are perfectly valid\n    if isinstance(ds, dict):\n        normalised_json_object = _normalise_json_ordered(data=ds, separator=sep)\n    elif isinstance(ds, list):\n        normalised_json_list = [_simple_json_normalize(row, sep=sep) for row in ds]\n        return normalised_json_list\n    return normalised_json_object\n\n\ndef _json_normalize(\n    data: dict | list[dict],\n    record_path: str | list | None = None,\n    meta: str | list[str | list[str]] | None = None,\n    meta_prefix: str | None = None,\n    record_prefix: str | None = None,\n    errors: str = \"raise\",\n    sep: str = \".\",\n    max_level: int | None = None,\n) -> DataFrame:\n    \"\"\"\n    Normalize semi-structured JSON data into a flat table.\n\n    Parameters\n    ----------\n    data : dict or list of dicts\n        Unserialized JSON objects.\n    record_path : str or list of str, default None\n        Path in each object to list of records. If not passed, data will be\n        assumed to be an array of records.\n    meta : list of paths (str or list of str), default None\n        Fields to use as metadata for each record in resulting table.\n    meta_prefix : str, default None\n        If True, prefix records with dotted (?) path, e.g. foo.bar.field if\n        meta is ['foo', 'bar'].\n    record_prefix : str, default None\n        If True, prefix records with dotted (?) path, e.g. foo.bar.field if\n        path to records is ['foo', 'bar'].\n    errors : {'raise', 'ignore'}, default 'raise'\n        Configures error handling.\n\n        * 'ignore' : will ignore KeyError if keys listed in meta are not\n          always present.\n        * 'raise' : will raise KeyError if keys listed in meta are not\n          always present.\n    sep : str, default '.'\n        Nested records will generate names separated by sep.\n        e.g., for sep='.', {'foo': {'bar': 0}} -> foo.bar.\n    max_level : int, default None\n        Max number of levels(depth of dict) to normalize.\n        if None, normalizes all levels.\n\n        .. versionadded:: 0.25.0\n\n    Returns\n    -------\n    frame : DataFrame\n    Normalize semi-structured JSON data into a flat table.\n\n    Examples\n    --------\n    >>> data = [\n    ...     {\"id\": 1, \"name\": {\"first\": \"Coleen\", \"last\": \"Volk\"}},\n    ...     {\"name\": {\"given\": \"Mark\", \"family\": \"Regner\"}},\n    ...     {\"id\": 2, \"name\": \"Faye Raker\"},\n    ... ]\n    >>> pd.json_normalize(data)\n        id name.first name.last name.given name.family        name\n    0  1.0     Coleen      Volk        NaN         NaN         NaN\n    1  NaN        NaN       NaN       Mark      Regner         NaN\n    2  2.0        NaN       NaN        NaN         NaN  Faye Raker\n\n    >>> data = [\n    ...     {\n    ...         \"id\": 1,\n    ...         \"name\": \"Cole Volk\",\n    ...         \"fitness\": {\"height\": 130, \"weight\": 60},\n    ...     },\n    ...     {\"name\": \"Mark Reg\", \"fitness\": {\"height\": 130, \"weight\": 60}},\n    ...     {\n    ...         \"id\": 2,\n    ...         \"name\": \"Faye Raker\",\n    ...         \"fitness\": {\"height\": 130, \"weight\": 60},\n    ...     },\n    ... ]\n    >>> pd.json_normalize(data, max_level=0)\n        id        name                        fitness\n    0  1.0   Cole Volk  {'height': 130, 'weight': 60}\n    1  NaN    Mark Reg  {'height': 130, 'weight': 60}\n    2  2.0  Faye Raker  {'height': 130, 'weight': 60}\n\n    Normalizes nested data up to level 1.\n\n    >>> data = [\n    ...     {\n    ...         \"id\": 1,\n    ...         \"name\": \"Cole Volk\",\n    ...         \"fitness\": {\"height\": 130, \"weight\": 60},\n    ...     },\n    ...     {\"name\": \"Mark Reg\", \"fitness\": {\"height\": 130, \"weight\": 60}},\n    ...     {\n    ...         \"id\": 2,\n    ...         \"name\": \"Faye Raker\",\n    ...         \"fitness\": {\"height\": 130, \"weight\": 60},\n    ...     },\n    ... ]\n    >>> pd.json_normalize(data, max_level=1)\n        id        name  fitness.height  fitness.weight\n    0  1.0   Cole Volk             130              60\n    1  NaN    Mark Reg             130              60\n    2  2.0  Faye Raker             130              60\n\n    >>> data = [\n    ...     {\n    ...         \"state\": \"Florida\",\n    ...         \"shortname\": \"FL\",\n    ...         \"info\": {\"governor\": \"Rick Scott\"},\n    ...         \"counties\": [\n    ...             {\"name\": \"Dade\", \"population\": 12345},\n    ...             {\"name\": \"Broward\", \"population\": 40000},\n    ...             {\"name\": \"Palm Beach\", \"population\": 60000},\n    ...         ],\n    ...     },\n    ...     {\n    ...         \"state\": \"Ohio\",\n    ...         \"shortname\": \"OH\",\n    ...         \"info\": {\"governor\": \"John Kasich\"},\n    ...         \"counties\": [\n    ...             {\"name\": \"Summit\", \"population\": 1234},\n    ...             {\"name\": \"Cuyahoga\", \"population\": 1337},\n    ...         ],\n    ...     },\n    ... ]\n    >>> result = pd.json_normalize(\n    ...     data, \"counties\", [\"state\", \"shortname\", [\"info\", \"governor\"]]\n    ... )\n    >>> result\n             name  population    state shortname info.governor\n    0        Dade       12345   Florida    FL    Rick Scott\n    1     Broward       40000   Florida    FL    Rick Scott\n    2  Palm Beach       60000   Florida    FL    Rick Scott\n    3      Summit        1234   Ohio       OH    John Kasich\n    4    Cuyahoga        1337   Ohio       OH    John Kasich\n\n    >>> data = {\"A\": [1, 2]}\n    >>> pd.json_normalize(data, \"A\", record_prefix=\"Prefix.\")\n        Prefix.0\n    0          1\n    1          2\n\n    Returns normalized data with columns prefixed with the given string.\n    \"\"\"\n\n    def _pull_field(\n        js: dict[str, Any], spec: list | str, extract_record: bool = False\n    ) -> Scalar | Iterable:\n        \"\"\"Internal function to pull field\"\"\"\n        result = js\n        try:\n            if isinstance(spec, list):\n                for field in spec:\n                    if result is None:\n                        raise KeyError(field)\n                    result = result[field]\n            else:\n                result = result[spec]\n        except KeyError as e:\n            if extract_record:\n                raise KeyError(\n                    f\"Key {e} not found. If specifying a record_path, all elements of \"\n                    f\"data should have the path.\"\n                ) from e\n            elif errors == \"ignore\":\n                return np.nan\n            else:\n                raise KeyError(\n                    f\"Key {e} not found. To replace missing values of {e} with \"\n                    f\"np.nan, pass in errors='ignore'\"\n                ) from e\n\n        return result\n\n    def _pull_records(js: dict[str, Any], spec: list | str) -> list:\n        \"\"\"\n        Internal function to pull field for records, and similar to\n        _pull_field, but require to return list. And will raise error\n        if has non iterable value.\n        \"\"\"\n        result = _pull_field(js, spec, extract_record=True)\n\n        # GH 31507 GH 30145, GH 26284 if result is not list, raise TypeError if not\n        # null, otherwise return an empty list\n        if not isinstance(result, list):\n            if pd.isnull(result):\n                result = []\n            else:\n                raise TypeError(\n                    f\"{js} has non list value {result} for path {spec}. \"\n                    \"Must be list or null.\"\n                )\n        return result\n\n    if isinstance(data, list) and not data:\n        return DataFrame()\n    elif isinstance(data, dict):\n        # A bit of a hackjob\n        data = [data]\n    elif isinstance(data, abc.Iterable) and not isinstance(data, str):\n        # GH35923 Fix pd.json_normalize to not skip the first element of a\n        # generator input\n        data = list(data)\n    else:\n        raise NotImplementedError\n\n    # check to see if a simple recursive function is possible to\n    # improve performance (see #15621) but only for cases such\n    # as pd.Dataframe(data) or pd.Dataframe(data, sep)\n    if (\n        record_path is None\n        and meta is None\n        and meta_prefix is None\n        and record_prefix is None\n        and max_level is None\n    ):\n        return DataFrame(_simple_json_normalize(data, sep=sep))\n\n    if record_path is None:\n        if any([isinstance(x, dict) for x in y.values()] for y in data):\n            # naive normalization, this is idempotent for flat records\n            # and potentially will inflate the data considerably for\n            # deeply nested structures:\n            #  {VeryLong: { b: 1,c:2}} -> {VeryLong.b:1 ,VeryLong.c:@}\n            #\n            # TODO: handle record value which are lists, at least error\n            #       reasonably\n            data = nested_to_record(data, sep=sep, max_level=max_level)\n        return DataFrame(data)\n    elif not isinstance(record_path, list):\n        record_path = [record_path]\n\n    if meta is None:\n        meta = []\n    elif not isinstance(meta, list):\n        meta = [meta]\n\n    _meta = [m if isinstance(m, list) else [m] for m in meta]\n\n    # Disastrously inefficient for now\n    records: list = []\n    lengths = []\n\n    meta_vals: DefaultDict = defaultdict(list)\n    meta_keys = [sep.join(val) for val in _meta]\n\n    def _recursive_extract(data, path, seen_meta, level=0):\n        if isinstance(data, dict):\n            data = [data]\n        if len(path) > 1:\n            for obj in data:\n                for val, key in zip(_meta, meta_keys):\n                    if level + 1 == len(val):\n                        seen_meta[key] = _pull_field(obj, val[-1])\n\n                _recursive_extract(obj[path[0]], path[1:], seen_meta, level=level + 1)\n        else:\n            for obj in data:\n                recs = _pull_records(obj, path[0])\n                recs = [\n                    nested_to_record(r, sep=sep, max_level=max_level)\n                    if isinstance(r, dict)\n                    else r\n                    for r in recs\n                ]\n\n                # For repeating the metadata later\n                lengths.append(len(recs))\n                for val, key in zip(_meta, meta_keys):\n                    if level + 1 > len(val):\n                        meta_val = seen_meta[key]\n                    else:\n                        meta_val = _pull_field(obj, val[level:])\n                    meta_vals[key].append(meta_val)\n                records.extend(recs)\n\n    _recursive_extract(data, record_path, {}, level=0)\n\n    result = DataFrame(records)\n\n    if record_prefix is not None:\n        # Incompatible types in assignment (expression has type \"Optional[DataFrame]\",\n        # variable has type \"DataFrame\")\n        result = result.rename(  # type: ignore[assignment]\n            columns=lambda x: f\"{record_prefix}{x}\"\n        )\n\n    # Data types, a problem\n    for k, v in meta_vals.items():\n        if meta_prefix is not None:\n            k = meta_prefix + k\n\n        if k in result:\n            raise ValueError(\n                f\"Conflicting metadata name {k}, need distinguishing prefix \"\n            )\n        result[k] = np.array(v, dtype=object).repeat(lengths)\n    return result\n\n\njson_normalize = deprecate(\n    \"pandas.io.json.json_normalize\", _json_normalize, \"1.0.0\", \"pandas.json_normalize\"\n)\n"
    },
    {
      "filename": "pandas/tests/series/methods/test_rename.py",
      "content": "from datetime import datetime\n\nimport numpy as np\nimport pytest\n\nfrom pandas import (\n    Index,\n    MultiIndex,\n    Series,\n)\nimport pandas._testing as tm\n\n\nclass TestRename:\n    def test_rename(self, datetime_series):\n        ts = datetime_series\n        renamer = lambda x: x.strftime(\"%Y%m%d\")\n        renamed = ts.rename(renamer)\n        assert renamed.index[0] == renamer(ts.index[0])\n\n        # dict\n        rename_dict = dict(zip(ts.index, renamed.index))\n        renamed2 = ts.rename(rename_dict)\n        tm.assert_series_equal(renamed, renamed2)\n\n    def test_rename_partial_dict(self):\n        # partial dict\n        ser = Series(np.arange(4), index=[\"a\", \"b\", \"c\", \"d\"], dtype=\"int64\")\n        renamed = ser.rename({\"b\": \"foo\", \"d\": \"bar\"})\n        tm.assert_index_equal(renamed.index, Index([\"a\", \"foo\", \"c\", \"bar\"]))\n\n    def test_rename_retain_index_name(self):\n        # index with name\n        renamer = Series(\n            np.arange(4), index=Index([\"a\", \"b\", \"c\", \"d\"], name=\"name\"), dtype=\"int64\"\n        )\n        renamed = renamer.rename({})\n        assert renamed.index.name == renamer.index.name\n\n    def test_rename_by_series(self):\n        ser = Series(range(5), name=\"foo\")\n        renamer = Series({1: 10, 2: 20})\n        result = ser.rename(renamer)\n        expected = Series(range(5), index=[0, 10, 20, 3, 4], name=\"foo\")\n        tm.assert_series_equal(result, expected)\n\n    def test_rename_set_name(self):\n        ser = Series(range(4), index=list(\"abcd\"))\n        for name in [\"foo\", 123, 123.0, datetime(2001, 11, 11), (\"foo\",)]:\n            result = ser.rename(name)\n            assert result.name == name\n            tm.assert_numpy_array_equal(result.index.values, ser.index.values)\n            assert ser.name is None\n\n    def test_rename_set_name_inplace(self):\n        ser = Series(range(3), index=list(\"abc\"))\n        for name in [\"foo\", 123, 123.0, datetime(2001, 11, 11), (\"foo\",)]:\n            ser.rename(name, inplace=True)\n            assert ser.name == name\n\n            exp = np.array([\"a\", \"b\", \"c\"], dtype=np.object_)\n            tm.assert_numpy_array_equal(ser.index.values, exp)\n\n    def test_rename_axis_supported(self):\n        # Supporting axis for compatibility, detailed in GH-18589\n        ser = Series(range(5))\n        ser.rename({}, axis=0)\n        ser.rename({}, axis=\"index\")\n\n        with pytest.raises(ValueError, match=\"No axis named 5\"):\n            ser.rename({}, axis=5)\n\n    def test_rename_inplace(self, datetime_series):\n        renamer = lambda x: x.strftime(\"%Y%m%d\")\n        expected = renamer(datetime_series.index[0])\n\n        datetime_series.rename(renamer, inplace=True)\n        assert datetime_series.index[0] == expected\n\n    def test_rename_with_custom_indexer(self):\n        # GH 27814\n        class MyIndexer:\n            pass\n\n        ix = MyIndexer()\n        ser = Series([1, 2, 3]).rename(ix)\n        assert ser.name is ix\n\n    def test_rename_with_custom_indexer_inplace(self):\n        # GH 27814\n        class MyIndexer:\n            pass\n\n        ix = MyIndexer()\n        ser = Series([1, 2, 3])\n        ser.rename(ix, inplace=True)\n        assert ser.name is ix\n\n    def test_rename_callable(self):\n        # GH 17407\n        ser = Series(range(1, 6), index=Index(range(2, 7), name=\"IntIndex\"))\n        result = ser.rename(str)\n        expected = ser.rename(lambda i: str(i))\n        tm.assert_series_equal(result, expected)\n\n        assert result.name == expected.name\n\n    def test_rename_method_and_index(self):\n        # GH 40977\n        ser = Series([1, 2])\n        with pytest.raises(TypeError, match=\"Cannot specify both 'mapper' and 'index'\"):\n            ser.rename(str, index=str)\n\n    def test_rename_none(self):\n        # GH 40977\n        ser = Series([1, 2], name=\"foo\")\n        result = ser.rename(None)\n        expected = Series([1, 2])\n        tm.assert_series_equal(result, expected)\n\n    def test_rename_series_with_multiindex(self):\n        # issue #43659\n        arrays = [\n            [\"bar\", \"baz\", \"baz\", \"foo\", \"qux\"],\n            [\"one\", \"one\", \"two\", \"two\", \"one\"],\n        ]\n\n        index = MultiIndex.from_arrays(arrays, names=[\"first\", \"second\"])\n        ser = Series(np.ones(5), index=index)\n        result = ser.rename(index={\"one\": \"yes\"}, level=\"second\", errors=\"raise\")\n\n        arrays_expected = [\n            [\"bar\", \"baz\", \"baz\", \"foo\", \"qux\"],\n            [\"yes\", \"yes\", \"two\", \"two\", \"yes\"],\n        ]\n\n        index_expected = MultiIndex.from_arrays(\n            arrays_expected, names=[\"first\", \"second\"]\n        )\n        series_expected = Series(np.ones(5), index=index_expected)\n\n        tm.assert_series_equal(result, series_expected)\n"
    }
  ],
  "questions": [
    "OK, so rename `rename` in `pandas/core/generic.py` to `_rename`, and go back to the old signature for `Series.rename`, i.e.\r\n```\r\n    def rename(\r\n        self,\r\n        index=None,\r\n        *,\r\n        axis=None,\r\n        copy=True,\r\n        inplace=False,\r\n        level=None,\r\n        errors=\"ignore\",\r\n    ):\r\n```\r\n? Like this, `mypy` wouldn't complain about the incompatibility.\r\nWish I'd thought of this actually, it seems like a better solution than what I did.\r\n\r\nIf so, I think that could be done straight away, rather than having to wait for 2.0, it wouldn't alter anything user-facing",
    "@MarcoGorelli I tried to understand this issue and so far I understood that in `pandas/core/generic.py` we have to make the `rename` function private and make it `_rename`. Is that all? I am sure I am missing out on a lot.",
    "Currently in `Series.rename` the function rename looks like this -\r\n```py\r\ndef rename(\r\n        self,\r\n        index=None,\r\n        *,\r\n        axis=None,\r\n        copy=True,\r\n        inplace=False,\r\n        level=None,\r\n        errors=\"ignore\",\r\n    ):\r\n```\r\nSo after reverting it would look like this - \r\n```py\r\n    def rename(\r\n        self,\r\n        index=None,\r\n        *,\r\n        axis=None,\r\n        copy=True,\r\n        inplace=False,\r\n        level=None,\r\n        errors=\"ignore\",\r\n    ) ->  Series | None:\r\n\r\n```\r\nIs that what you are saying?",
    "@MarcoGorelli I found the commit that needed to be reverted but you have done some other changes in that commit apart from adding args to function `rename`. Here have a look at this - https://github.com/pandas-dev/pandas/pull/40979/commits/40ecf6e754451acb87f8a2f3b911a8cb9728cda1\r\nso should I proceed ahead since reverting it would revert some additional changes as well?",
    "xref https://github.com/pandas-dev/pandas/pull/40979#issuecomment-980063593\r\n\r\n> thanks @MarcoGorelli i realize now why this is convoluted, will be glad to remove this in 2.0\r\n\r\n@jreback not sure I understand what you mean - as in, to remove `mapper` in 2.0 and only support the `index=` and `columns=` calls? (and hence, introduce a deprecation warning in time for 1.4)"
  ],
  "golden_answers": [
    "@MarcoGorelli I tried to understand this issue and so far I understood that in `pandas/core/generic.py` we have to make the `rename` function private and make it `_rename`. Is that all? I am sure I am missing out on a lot.",
    "Hey @Varun270 !\r\n\r\nSo, there's a few things to do:\r\n1. rename `rename` in `pandas/core/generic.py` to `_rename`\r\n2. update `rename` (in both `pandas/core/frame.py` and `pandas/core/series.py` to call `super()._rename`, rather than `super().rename`)\r\n3. revert some of the changes I made in #40979 so that the signature of `Series.rename` becomes:\r\n```python\r\n    def rename(\r\n        self,\r\n        index=None,\r\n        *,\r\n        axis=None,\r\n        copy=True,\r\n        inplace=False,\r\n        level=None,\r\n        errors=\"ignore\",\r\n    ) ->  Series | None:\r\n```\r\n\r\nIf you open a PR (even if you're not sure about everything) and tag me I can help you along with this",
    "> Currently in Series.rename the function rename looks like this -\r\n\r\nMake sure to pull the latest changes, it doesn't quite look like that at the moment. Anyway,\r\n\r\n> So after reverting it would look like this -\r\n\r\nYes, it should look like that at the end",
    "What can be reverted are:\r\n- changes to `Series.rename` (can just revert them all, and change `super().rename` to `super()._rename`\r\n- the extra tests in `pandas/tests/series/methods/test_rename.py` are no longer necessary\r\n\r\nMight be easier to do this manually rather than reverting a commit using git. Up to you though, if you open a PR I'll take a look",
    "no what u mean is to fix the structure of the impl here\nmaybe we just need to make the structure in generic a private function to avoid having to handle the signature like this"
  ],
  "questions_generated": [
    "What is the proposed change to the 'rename' function in 'pandas/core/generic.py' and why?",
    "Why is the 'mapper' argument being considered for deprecation in the 'rename' method and what alternative is suggested?",
    "What changes need to be reverted from pull request #40979 regarding the 'Series.rename' method?",
    "How should the 'rename' function in 'pandas/core/frame.py' and 'pandas/core/series.py' be updated according to the discussion?",
    "What is the rationale behind making the implementation of 'rename' a private function rather than addressing signature differences directly in the public methods?"
  ],
  "golden_answers_generated": [
    "The proposed change is to make the 'rename' function private by renaming it to '_rename'. This is suggested to avoid handling different signatures for the 'rename' method in Series and DataFrame, which would placate mypy by ensuring that the implementation details do not interfere with the public API.",
    "The 'mapper' argument is considered convoluted and is proposed for removal in version 2.0 of pandas. The alternative suggested is to support only 'index=' and 'columns=' calls, which would simplify the method's usage and structure.",
    "The changes to be reverted involve restoring the signature of 'Series.rename' to its original form, which includes parameters like 'index', 'axis', 'copy', 'inplace', 'level', and 'errors'. This reversion allows the method to maintain its compatibility and functionality without the need for workaround solutions for the method signature.",
    "The 'rename' functions in 'pandas/core/frame.py' and 'pandas/core/series.py' should be updated to call 'super()._rename' instead of 'super().rename'. This change aligns with making the 'rename' implementation private in the generic module and using the updated function name.",
    "The rationale is that addressing signature differences directly in the public 'rename' methods would require complex handling to satisfy mypy checks, which could complicate the codebase. By making the implementation private, the complexity can be managed internally without affecting the public API, ensuring cleaner and more maintainable code."
  ]
}