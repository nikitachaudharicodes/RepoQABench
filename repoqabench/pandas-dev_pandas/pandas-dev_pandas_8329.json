{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "8329",
  "issue_description": "# CLN: start using numpy-1.7 API\n\nNow that numpy-1.6 support is dropped we could proceed to use np-1.7 API.\r\n\r\nThis involves fixing a macro or two deep down in pandas internals, but otherwise should be pretty straightforward. As a bonus we can get rid of these annoying build warnings:\r\n\r\n```\r\ngcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -Ipandas/src/klib -Ipandas/src -I/home/immerrr/.conda/envs/pandas/lib/python2.7/site-packages/numpy/core/include -I/home/immerrr/.conda/envs/pandas/include/python2.7 -c pandas/index.c -o build/temp.linux-x86_64-2.7/pandas/index.o\r\nIn file included from /home/immerrr/.conda/envs/pandas/lib/python2.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1761:0,\r\n                 from /home/immerrr/.conda/envs/pandas/lib/python2.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:17,\r\n                 from /home/immerrr/.conda/envs/pandas/lib/python2.7/site-packages/numpy/core/include/numpy/arrayobject.h:4,\r\n                 from pandas/index.c:244:\r\n/home/immerrr/.conda/envs/pandas/lib/python2.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n```\r\nxref https://github.com/pydata/numexpr/pull/228/files\r\n",
  "issue_comments": [
    {
      "id": 56261407,
      "user": "jreback",
      "body": "@immerrr would be gr8 (esp for 0.15)!\n"
    },
    {
      "id": 56279394,
      "user": "immerrr",
      "body": "I was going to suggest it for the hackathon, are you expecting anyone who's into low-level stuff there?\n"
    },
    {
      "id": 56279472,
      "user": "jreback",
      "body": "hmm, interesting, I will add this up their too. As their are going to be people who could hack on NumPy, but we'd rather co-op them to pandas :)\n"
    },
    {
      "id": 57056131,
      "user": "devanshmehta",
      "body": "I am trying to work on this bug. But half difficulty finding a starting point. It will be nice if you can point me in right direction. I have the forked the panda repo and added upstream.\n"
    },
    {
      "id": 57056189,
      "user": "jreback",
      "body": "are you at the bloomberg hackathon?\n"
    },
    {
      "id": 57056287,
      "user": "devanshmehta",
      "body": "Yes\n"
    },
    {
      "id": 57058950,
      "user": "jreback",
      "body": "@devanshmehta can you come over to my table?\n"
    },
    {
      "id": 57917964,
      "user": "immerrr",
      "body": "Ok, I gave this one a shot and it's nowhere near easy. After figuring out simple stuff, you run into two huge obstacles.  One is that cython doesn't yet support numpy-1.7 api. Pandas has its own `numpy.pxd`, but it too has the necessary macros commented out with a description saying:\n\n```\n    # dtype PyArray_DESCR(ndarray) wrong refcount semantics\n```\n\nThat one can be figured out with some interaction with Cython guys (and probably backported there), but then there are rolling functions.\n\nSome of those functions in algos module do pure magic: they create one a single ndarray to refer to the window, pass it to python-level function and then alter its `arr.data` field to skip to the next element without incurring overhead of creating/destroying ndarrays. In np-1.7 api referring to data must be done via `PyArray_DATA(arr)` macro and it's probably not guaranteed to be assignable, so that optimization potential goes out of the window. I'm not sure if there's a way around this that doesn't lose performance.\n"
    },
    {
      "id": 57918001,
      "user": "jreback",
      "body": "I should have mentioned\n@devanshmehta can u post the branch u did (even of not complete)\n"
    },
    {
      "id": 57918100,
      "user": "immerrr",
      "body": "Here's mine: https://github.com/immerrr/pandas/commit/71454db08b9c22f2745da8cfb807f11ed9b7333c\n"
    },
    {
      "id": 59136980,
      "user": "devanshmehta",
      "body": "Apologize for the delayed response. I did not work on it after the hackathon. Will post the over the branch. \n"
    },
    {
      "id": 59137831,
      "user": "devanshmehta",
      "body": "https://github.com/devanshmehta/pandas\n\nHere is the branch. Based on the conversation with Mark who was also present during the hackathon. He said there was particular macro which need to be defined before using the numpy api. The cython files cannot contain #define macro, so we created a c header file which contained the #define macro required by numpy and tried to include it in front of numpy api. After doing this there were few compilation error in both c and python files. I was trying to get rid of the compilation error by using the new numpy api. However my work is till incomplete and gives some compilation error.\n"
    },
    {
      "id": 59158401,
      "user": "immerrr",
      "body": "Thanks, @devanshmehta . I'll look into this.\n"
    },
    {
      "id": 61456961,
      "user": "immerrr",
      "body": "So I attempted to resolve this once more, but again hit some sort of a wall.\n\nFor the reasons outlined [here](https://www.mail-archive.com/numpy-discussion@scipy.org/msg45121.html) it's not permitted to modify data pointers in living arrays and a lot of `reduce.pyx` code should be basically rewritten from scratch in a way that will decrease its performance (according to the mailing list message, negligibly).\n"
    },
    {
      "id": 301696852,
      "user": "gfyoung",
      "body": "@jreback : I think you can close this now that #15206 has been issued."
    },
    {
      "id": 301739258,
      "user": "jreback",
      "body": "@gfyoung your reference seems to be off, and this is certainly not closed. we have warning messages in the c-code. need to set a macro."
    },
    {
      "id": 301782737,
      "user": "gfyoung",
      "body": "@jreback : I meant to refer to the issue where we are dropping `numpy < 1.9`.  Certainly that should encapsulate this one?"
    },
    {
      "id": 301791030,
      "user": "jreback",
      "body": "this refers to the c api\r\nwe are actually using the pre 1.7 one\r\nthat's the point of setting the macro\r\n\r\nso this issue needs to be fixed before (though of course will work when compiling on later numpy anyhow as they haven't removed the old api)"
    },
    {
      "id": 404691135,
      "user": "jbrockmendel",
      "body": "According to the cython docs this warning should be ignored.  I take that to mean we jus have to wait for them to fix it there (no indication if/when that will be).  This is not actionable.  Closing."
    },
    {
      "id": 404691488,
      "user": "gfyoung",
      "body": "@jbrockmendel : Could you add a link for future reference?"
    },
    {
      "id": 404694278,
      "user": "jreback",
      "body": "i believe u can set a flag about the api to turn off the old api "
    },
    {
      "id": 404709009,
      "user": "jbrockmendel",
      "body": "https://cython.readthedocs.io/en/latest/src/reference/compilation.html#configuring-the-c-build\r\n\r\n> Despite this, you will still get warnings like the following from the compiler, because Cython is using a deprecated Numpy API:\r\n>\r\n>> .../include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n>\r\n> For the time being, it is just a warning that you can ignore.\r\n\r\n@jreback \r\n> i believe u can set a flag about the api to turn off the old api\r\n\r\nThe issue isn't pandas using the old API, it is cython using it.  As long as cython-generated C is using the old API, we're not getting rid of those warnings.  The relevant flag would be to add to `setup.macros` the entry `('NPY_NO_DEPRECATED_API', 'NPY_1_7_API_VERSION')`, but doing so causes compile-time errors."
    }
  ],
  "text_context": "# CLN: start using numpy-1.7 API\n\nNow that numpy-1.6 support is dropped we could proceed to use np-1.7 API.\r\n\r\nThis involves fixing a macro or two deep down in pandas internals, but otherwise should be pretty straightforward. As a bonus we can get rid of these annoying build warnings:\r\n\r\n```\r\ngcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -Ipandas/src/klib -Ipandas/src -I/home/immerrr/.conda/envs/pandas/lib/python2.7/site-packages/numpy/core/include -I/home/immerrr/.conda/envs/pandas/include/python2.7 -c pandas/index.c -o build/temp.linux-x86_64-2.7/pandas/index.o\r\nIn file included from /home/immerrr/.conda/envs/pandas/lib/python2.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1761:0,\r\n                 from /home/immerrr/.conda/envs/pandas/lib/python2.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:17,\r\n                 from /home/immerrr/.conda/envs/pandas/lib/python2.7/site-packages/numpy/core/include/numpy/arrayobject.h:4,\r\n                 from pandas/index.c:244:\r\n/home/immerrr/.conda/envs/pandas/lib/python2.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n```\r\nxref https://github.com/pydata/numexpr/pull/228/files\r\n\n\n@immerrr would be gr8 (esp for 0.15)!\n\n\nI was going to suggest it for the hackathon, are you expecting anyone who's into low-level stuff there?\n\n\nhmm, interesting, I will add this up their too. As their are going to be people who could hack on NumPy, but we'd rather co-op them to pandas :)\n\n\nI am trying to work on this bug. But half difficulty finding a starting point. It will be nice if you can point me in right direction. I have the forked the panda repo and added upstream.\n\n\nare you at the bloomberg hackathon?\n\n\nYes\n\n\n@devanshmehta can you come over to my table?\n\n\nOk, I gave this one a shot and it's nowhere near easy. After figuring out simple stuff, you run into two huge obstacles.  One is that cython doesn't yet support numpy-1.7 api. Pandas has its own `numpy.pxd`, but it too has the necessary macros commented out with a description saying:\n\n```\n    # dtype PyArray_DESCR(ndarray) wrong refcount semantics\n```\n\nThat one can be figured out with some interaction with Cython guys (and probably backported there), but then there are rolling functions.\n\nSome of those functions in algos module do pure magic: they create one a single ndarray to refer to the window, pass it to python-level function and then alter its `arr.data` field to skip to the next element without incurring overhead of creating/destroying ndarrays. In np-1.7 api referring to data must be done via `PyArray_DATA(arr)` macro and it's probably not guaranteed to be assignable, so that optimization potential goes out of the window. I'm not sure if there's a way around this that doesn't lose performance.\n\n\nI should have mentioned\n@devanshmehta can u post the branch u did (even of not complete)\n\n\nHere's mine: https://github.com/immerrr/pandas/commit/71454db08b9c22f2745da8cfb807f11ed9b7333c\n\n\nApologize for the delayed response. I did not work on it after the hackathon. Will post the over the branch. \n\n\nhttps://github.com/devanshmehta/pandas\n\nHere is the branch. Based on the conversation with Mark who was also present during the hackathon. He said there was particular macro which need to be defined before using the numpy api. The cython files cannot contain #define macro, so we created a c header file which contained the #define macro required by numpy and tried to include it in front of numpy api. After doing this there were few compilation error in both c and python files. I was trying to get rid of the compilation error by using the new numpy api. However my work is till incomplete and gives some compilation error.\n\n\nThanks, @devanshmehta . I'll look into this.\n\n\nSo I attempted to resolve this once more, but again hit some sort of a wall.\n\nFor the reasons outlined [here](https://www.mail-archive.com/numpy-discussion@scipy.org/msg45121.html) it's not permitted to modify data pointers in living arrays and a lot of `reduce.pyx` code should be basically rewritten from scratch in a way that will decrease its performance (according to the mailing list message, negligibly).\n\n\n@jreback : I think you can close this now that #15206 has been issued.\n\n@gfyoung your reference seems to be off, and this is certainly not closed. we have warning messages in the c-code. need to set a macro.\n\n@jreback : I meant to refer to the issue where we are dropping `numpy < 1.9`.  Certainly that should encapsulate this one?\n\nthis refers to the c api\r\nwe are actually using the pre 1.7 one\r\nthat's the point of setting the macro\r\n\r\nso this issue needs to be fixed before (though of course will work when compiling on later numpy anyhow as they haven't removed the old api)\n\nAccording to the cython docs this warning should be ignored.  I take that to mean we jus have to wait for them to fix it there (no indication if/when that will be).  This is not actionable.  Closing.\n\n@jbrockmendel : Could you add a link for future reference?\n\ni believe u can set a flag about the api to turn off the old api \n\nhttps://cython.readthedocs.io/en/latest/src/reference/compilation.html#configuring-the-c-build\r\n\r\n> Despite this, you will still get warnings like the following from the compiler, because Cython is using a deprecated Numpy API:\r\n>\r\n>> .../include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n>\r\n> For the time being, it is just a warning that you can ignore.\r\n\r\n@jreback \r\n> i believe u can set a flag about the api to turn off the old api\r\n\r\nThe issue isn't pandas using the old API, it is cython using it.  As long as cython-generated C is using the old API, we're not getting rid of those warnings.  The relevant flag would be to add to `setup.macros` the entry `('NPY_NO_DEPRECATED_API', 'NPY_1_7_API_VERSION')`, but doing so causes compile-time errors.",
  "pr_link": "https://github.com/pydata/numexpr/pull/228",
  "code_context": [
    {
      "filename": "numexpr/interpreter.cpp",
      "content": "/*********************************************************************\n  Numexpr - Fast numerical array expression evaluator for NumPy.\n\n      License: MIT\n      Author:  See AUTHORS.txt\n\n  See LICENSE.txt for details about copyright and rights to use.\n**********************************************************************/\n\n#include \"module.hpp\"\n#include <numpy/npy_cpu.h>\n#include <math.h>\n#include <string.h>\n#include <assert.h>\n#include <vector>\n\n#include \"numexpr_config.hpp\"\n#include \"complex_functions.hpp\"\n#include \"interpreter.hpp\"\n#include \"numexpr_object.hpp\"\n\n#ifdef _MSC_VER\n/* Some missing symbols and functions for Win */\n#define fmax max\n#define fmin min\n#define INFINITY (DBL_MAX+DBL_MAX)\n#define NAN (INFINITY-INFINITY)\n#endif\n\n#ifndef SIZE_MAX\n#define SIZE_MAX ((size_t)-1)\n#endif\n\n#define RETURN_TYPE char*\n\n// AVAILABLE(Haystack, Haystack_Len, J, Needle_Len)\n//     A macro that returns nonzero if there are at least Needle_Len\n//     bytes left starting at Haystack[J].\n//     Haystack is 'unsigned char *', Haystack_Len, J, and Needle_Len\n//     are 'size_t'; Haystack_Len is an lvalue.  For NUL-terminated\n//     searches, Haystack_Len can be modified each iteration to avoid\n//     having to compute the end of Haystack up front.\n\n#define AVAILABLE(Haystack, Haystack_Len, J, Needle_Len)   \\\n  ((Haystack_Len) >= (J) + (Needle_Len))\n\n#include \"str-two-way.hpp\"\n\n#ifdef DEBUG\n#define DEBUG_TEST 1\n#else\n#define DEBUG_TEST 0\n#endif\n\n\nusing namespace std;\n\n// Global state\nthread_data th_params;\n\n/* This file and interp_body should really be generated from a description of\n   the opcodes -- there's too much repetition here for manually editing */\n\n\n/* bit of a misnomer; includes the return value. */\n#define NUMEXPR_MAX_ARGS 4\n\nstatic char op_signature_table[][NUMEXPR_MAX_ARGS] = {\n#define Tb 'b'\n#define Ti 'i'\n#define Tl 'l'\n#define Tf 'f'\n#define Td 'd'\n#define Tc 'c'\n#define Ts 's'\n#define Tn 'n'\n#define T0 0\n#define OPCODE(n, e, ex, rt, a1, a2, a3) {rt, a1, a2, a3},\n#include \"opcodes.hpp\"\n#undef OPCODE\n#undef Tb\n#undef Ti\n#undef Tl\n#undef Tf\n#undef Td\n#undef Tc\n#undef Ts\n#undef Tn\n#undef T0\n};\n\n/* returns the sig of the nth op, '\\0' if no more ops -1 on failure */\nstatic int\nop_signature(int op, unsigned int n) {\n    if (n >= NUMEXPR_MAX_ARGS) {\n        return 0;\n    }\n    if (op < 0 || op > OP_END) {\n        return -1;\n    }\n    return op_signature_table[op][n];\n}\n\n\n\n/*\n   To add a function to the lookup table, add to FUNC_CODES (first\n   group is 1-arg functions, second is 2-arg functions), also to\n   functions_f or functions_ff as appropriate. Finally, use add_func\n   down below to add to funccodes. Functions with more arguments\n   aren't implemented at present, but should be easy; just copy the 1-\n   or 2-arg case.\n\n   Some functions (for example, sqrt) are repeated in this table that\n   are opcodes, but there's no problem with that as the compiler\n   selects opcodes over functions, and this makes it easier to compare\n   opcode vs. function speeds.\n*/\n\ntypedef float (*FuncFFPtr)(float);\n\n#ifdef _WIN32\nFuncFFPtr functions_ff[] = {\n#define FUNC_FF(fop, s, f, f_win32, ...) f_win32,\n#include \"functions.hpp\"\n#undef FUNC_FF\n};\n#else\nFuncFFPtr functions_ff[] = {\n#define FUNC_FF(fop, s, f, ...) f,\n#include \"functions.hpp\"\n#undef FUNC_FF\n};\n#endif\n\n#ifdef USE_VML\n/* Fake vsConj function just for casting purposes inside numexpr */\nstatic void vsConj(MKL_INT n, const float* x1, float* dest)\n{\n    MKL_INT j;\n    for (j=0; j<n; j++) {\n        dest[j] = x1[j];\n    };\n};\n#endif\n\n#ifdef USE_VML\ntypedef void (*FuncFFPtr_vml)(MKL_INT, const float*, float*);\nFuncFFPtr_vml functions_ff_vml[] = {\n#define FUNC_FF(fop, s, f, f_win32, f_vml) f_vml,\n#include \"functions.hpp\"\n#undef FUNC_FF\n};\n#endif\n\ntypedef float (*FuncFFFPtr)(float, float);\n\n#ifdef _WIN32\nFuncFFFPtr functions_fff[] = {\n#define FUNC_FFF(fop, s, f, f_win32, ...) f_win32,\n#include \"functions.hpp\"\n#undef FUNC_FFF\n};\n#else\nFuncFFFPtr functions_fff[] = {\n#define FUNC_FFF(fop, s, f, ...) f,\n#include \"functions.hpp\"\n#undef FUNC_FFF\n};\n#endif\n\n#ifdef USE_VML\n/* fmod not available in VML */\nstatic void vsfmod(MKL_INT n, const float* x1, const float* x2, float* dest)\n{\n    MKL_INT j;\n    for(j=0; j < n; j++) {\n    dest[j] = fmod(x1[j], x2[j]);\n    };\n};\n\ntypedef void (*FuncFFFPtr_vml)(MKL_INT, const float*, const float*, float*);\nFuncFFFPtr_vml functions_fff_vml[] = {\n#define FUNC_FFF(fop, s, f, f_win32, f_vml) f_vml,\n#include \"functions.hpp\"\n#undef FUNC_FFF\n};\n#endif\n\ntypedef double (*FuncDDPtr)(double);\n\nFuncDDPtr functions_dd[] = {\n#define FUNC_DD(fop, s, f, ...) f,\n#include \"functions.hpp\"\n#undef FUNC_DD\n};\n\n#ifdef USE_VML\n/* Fake vdConj function just for casting purposes inside numexpr */\nstatic void vdConj(MKL_INT n, const double* x1, double* dest)\n{\n    MKL_INT j;\n    for (j=0; j<n; j++) {\n        dest[j] = x1[j];\n    };\n};\n#endif\n\n#ifdef USE_VML\ntypedef void (*FuncDDPtr_vml)(MKL_INT, const double*, double*);\nFuncDDPtr_vml functions_dd_vml[] = {\n#define FUNC_DD(fop, s, f, f_vml) f_vml,\n#include \"functions.hpp\"\n#undef FUNC_DD\n};\n#endif\n\ntypedef double (*FuncDDDPtr)(double, double);\n\nFuncDDDPtr functions_ddd[] = {\n#define FUNC_DDD(fop, s, f, ...) f,\n#include \"functions.hpp\"\n#undef FUNC_DDD\n};\n\n#ifdef USE_VML\n/* fmod not available in VML */\nstatic void vdfmod(MKL_INT n, const double* x1, const double* x2, double* dest)\n{\n    MKL_INT j;\n    for(j=0; j < n; j++) {\n    dest[j] = fmod(x1[j], x2[j]);\n    };\n};\n\ntypedef void (*FuncDDDPtr_vml)(MKL_INT, const double*, const double*, double*);\nFuncDDDPtr_vml functions_ddd_vml[] = {\n#define FUNC_DDD(fop, s, f, f_vml) f_vml,\n#include \"functions.hpp\"\n#undef FUNC_DDD\n};\n#endif\n\n\n\ntypedef void (*FuncCCPtr)(npy_cdouble*, npy_cdouble*);\n\nFuncCCPtr functions_cc[] = {\n#define FUNC_CC(fop, s, f, ...) f,\n#include \"functions.hpp\"\n#undef FUNC_CC\n};\n\n#ifdef USE_VML\n/* complex expm1 not available in VML */\nstatic void vzExpm1(MKL_INT n, const MKL_Complex16* x1, MKL_Complex16* dest)\n{\n    MKL_INT j;\n    vzExp(n, x1, dest);\n    for (j=0; j<n; j++) {\n    dest[j].real -= 1.0;\n    };\n};\n\nstatic void vzLog1p(MKL_INT n, const MKL_Complex16* x1, MKL_Complex16* dest)\n{\n    MKL_INT j;\n    for (j=0; j<n; j++) {\n    dest[j].real = x1[j].real + 1;\n    dest[j].imag = x1[j].imag;\n    };\n    vzLn(n, dest, dest);\n};\n\n/* Use this instead of native vzAbs in VML as it seems to work badly */\nstatic void vzAbs_(MKL_INT n, const MKL_Complex16* x1, MKL_Complex16* dest)\n{\n    MKL_INT j;\n    for (j=0; j<n; j++) {\n        dest[j].real = sqrt(x1[j].real*x1[j].real + x1[j].imag*x1[j].imag);\n    dest[j].imag = 0;\n    };\n};\n\ntypedef void (*FuncCCPtr_vml)(MKL_INT, const MKL_Complex16[], MKL_Complex16[]);\n\nFuncCCPtr_vml functions_cc_vml[] = {\n#define FUNC_CC(fop, s, f, f_vml) f_vml,\n#include \"functions.hpp\"\n#undef FUNC_CC\n};\n#endif\n\n\ntypedef void (*FuncCCCPtr)(npy_cdouble*, npy_cdouble*, npy_cdouble*);\n\nFuncCCCPtr functions_ccc[] = {\n#define FUNC_CCC(fop, s, f) f,\n#include \"functions.hpp\"\n#undef FUNC_CCC\n};\n\n\nchar\nget_return_sig(PyObject* program)\n{\n    int sig;\n    char last_opcode;\n    Py_ssize_t end = PyBytes_Size(program);\n    char *program_str = PyBytes_AS_STRING(program);\n\n    do {\n        end -= 4;\n        if (end < 0) return 'X';\n        last_opcode = program_str[end];\n    }\n    while (last_opcode == OP_NOOP);\n\n    sig = op_signature(last_opcode, 0);\n    if (sig <= 0) {\n        return 'X';\n    } else {\n        return (char)sig;\n    }\n}\n\nstatic int\ntypecode_from_char(char c)\n{\n    switch (c) {\n        case 'b': return NPY_BOOL;\n        case 'i': return NPY_INT;\n        case 'l': return NPY_LONGLONG;\n        case 'f': return NPY_FLOAT;\n        case 'd': return NPY_DOUBLE;\n        case 'c': return NPY_CDOUBLE;\n        case 's': return NPY_STRING;\n        default:\n            PyErr_SetString(PyExc_TypeError, \"signature value not in 'bilfdcs'\");\n            return -1;\n    }\n}\n\nstatic int\nlast_opcode(PyObject *program_object) {\n    Py_ssize_t n;\n    unsigned char *program;\n    PyBytes_AsStringAndSize(program_object, (char **)&program, &n);\n    return program[n-4];\n}\n\nstatic int\nget_reduction_axis(PyObject* program) {\n    Py_ssize_t end = PyBytes_Size(program);\n    int axis = ((unsigned char *)PyBytes_AS_STRING(program))[end-1];\n    if (axis != 255 && axis >= NPY_MAXDIMS)\n        axis = NPY_MAXDIMS - axis;\n    return axis;\n}\n\n\n\nint\ncheck_program(NumExprObject *self)\n{\n    unsigned char *program;\n    Py_ssize_t prog_len, n_buffers, n_inputs;\n    int pc, arg, argloc, argno, sig;\n    char *fullsig, *signature;\n\n    if (PyBytes_AsStringAndSize(self->program, (char **)&program,\n                                &prog_len) < 0) {\n        PyErr_Format(PyExc_RuntimeError, \"invalid program: can't read program\");\n        return -1;\n    }\n    if (prog_len % 4 != 0) {\n        PyErr_Format(PyExc_RuntimeError, \"invalid program: prog_len mod 4 != 0\");\n        return -1;\n    }\n    if (PyBytes_AsStringAndSize(self->fullsig, (char **)&fullsig,\n                                &n_buffers) < 0) {\n        PyErr_Format(PyExc_RuntimeError, \"invalid program: can't read fullsig\");\n        return -1;\n    }\n    if (PyBytes_AsStringAndSize(self->signature, (char **)&signature,\n                                &n_inputs) < 0) {\n        PyErr_Format(PyExc_RuntimeError, \"invalid program: can't read signature\");\n        return -1;\n    }\n    if (n_buffers > 255) {\n        PyErr_Format(PyExc_RuntimeError, \"invalid program: too many buffers\");\n        return -1;\n    }\n    for (pc = 0; pc < prog_len; pc += 4) {\n        unsigned int op = program[pc];\n        if (op == OP_NOOP) {\n            continue;\n        }\n        if ((op >= OP_REDUCTION) && pc != prog_len-4) {\n                PyErr_Format(PyExc_RuntimeError,\n                    \"invalid program: reduction operations must occur last\");\n                return -1;\n        }\n        for (argno = 0; ; argno++) {\n            sig = op_signature(op, argno);\n            if (sig == -1) {\n                PyErr_Format(PyExc_RuntimeError, \"invalid program: illegal opcode at %i (%d)\", pc, op);\n                return -1;\n            }\n            if (sig == 0) break;\n            if (argno < 3) {\n                argloc = pc+argno+1;\n            }\n            if (argno >= 3) {\n                if (pc + 1 >= prog_len) {\n                    PyErr_Format(PyExc_RuntimeError, \"invalid program: double opcode (%c) at end (%i)\", pc, sig);\n                    return -1;\n                }\n                argloc = pc+argno+2;\n            }\n            arg = program[argloc];\n\n            if (sig != 'n' && ((arg >= n_buffers) || (arg < 0))) {\n                PyErr_Format(PyExc_RuntimeError, \"invalid program: buffer out of range (%i) at %i\", arg, argloc);\n                return -1;\n            }\n            if (sig == 'n') {\n                if (op == OP_FUNC_FFN) {\n                    if (arg < 0 || arg >= FUNC_FF_LAST) {\n                        PyErr_Format(PyExc_RuntimeError, \"invalid program: funccode out of range (%i) at %i\", arg, argloc);\n                        return -1;\n                    }\n                } else if (op == OP_FUNC_FFFN) {\n                    if (arg < 0 || arg >= FUNC_FFF_LAST) {\n                        PyErr_Format(PyExc_RuntimeError, \"invalid program: funccode out of range (%i) at %i\", arg, argloc);\n                        return -1;\n                    }\n                } else if (op == OP_FUNC_DDN) {\n                    if (arg < 0 || arg >= FUNC_DD_LAST) {\n                        PyErr_Format(PyExc_RuntimeError, \"invalid program: funccode out of range (%i) at %i\", arg, argloc);\n                        return -1;\n                    }\n                } else if (op == OP_FUNC_DDDN) {\n                    if (arg < 0 || arg >= FUNC_DDD_LAST) {\n                        PyErr_Format(PyExc_RuntimeError, \"invalid program: funccode out of range (%i) at %i\", arg, argloc);\n                        return -1;\n                    }\n                } else if (op == OP_FUNC_CCN) {\n                    if (arg < 0 || arg >= FUNC_CC_LAST) {\n                        PyErr_Format(PyExc_RuntimeError, \"invalid program: funccode out of range (%i) at %i\", arg, argloc);\n                        return -1;\n                    }\n                } else if (op == OP_FUNC_CCCN) {\n                    if (arg < 0 || arg >= FUNC_CCC_LAST) {\n                        PyErr_Format(PyExc_RuntimeError, \"invalid program: funccode out of range (%i) at %i\", arg, argloc);\n                        return -1;\n                    }\n                } else if (op >= OP_REDUCTION) {\n                    ;\n                } else {\n                    PyErr_Format(PyExc_RuntimeError, \"invalid program: internal checker errror processing %i\", argloc);\n                    return -1;\n                }\n            /* The next is to avoid problems with the ('i','l') duality,\n               specially in 64-bit platforms */\n            } else if (((sig == 'l') && (fullsig[arg] == 'i')) ||\n                       ((sig == 'i') && (fullsig[arg] == 'l'))) {\n              ;\n            } else if (sig != fullsig[arg]) {\n                PyErr_Format(PyExc_RuntimeError,\n                \"invalid : opcode signature doesn't match buffer (%c vs %c) at %i\", sig, fullsig[arg], argloc);\n                return -1;\n            }\n        }\n    }\n    return 0;\n}\n\n\n\n\nstruct index_data {\n    int count;\n    int size;\n    int findex;\n    npy_intp *shape;\n    npy_intp *strides;\n    int *index;\n    char *buffer;\n};\n\n// BOUNDS_CHECK is used in interp_body.cpp\n#define DO_BOUNDS_CHECK 1\n\n#if DO_BOUNDS_CHECK\n#define BOUNDS_CHECK(arg) if ((arg) >= params.r_end) { \\\n        *pc_error = pc;                                                 \\\n        return -2;                                                      \\\n    }\n#else\n#define BOUNDS_CHECK(arg)\n#endif\n\nint\nstringcmp(const char *s1, const char *s2, npy_intp maxlen1, npy_intp maxlen2)\n{\n    npy_intp maxlen, nextpos;\n    /* Point to this when the end of a string is found,\n       to simulate infinte trailing NULL characters. */\n    const char null = 0;\n\n    // First check if some of the operands is the empty string and if so,\n    // just check that the first char of the other is the NULL one.\n    // Fixes #121\n    if (maxlen2 == 0) return *s1 != null;\n    if (maxlen1 == 0) return *s2 != null;\n\n    maxlen = (maxlen1 > maxlen2) ? maxlen1 : maxlen2;\n    for (nextpos = 1;  nextpos <= maxlen;  nextpos++) {\n        if (*s1 < *s2)\n            return -1;\n        if (*s1 > *s2)\n            return +1;\n        s1 = (nextpos >= maxlen1) ? &null : s1+1;\n        s2 = (nextpos >= maxlen2) ? &null : s2+1;\n    }\n    return 0;\n}\n\n\n/* contains(str1, str2) function for string columns.\n\n   Based on Newlib/strstr.c.                        */\n\nint\nstringcontains(const char *haystack_start, const char *needle_start,  npy_intp max_haystack_len, npy_intp max_needle_len)\n{\n    // needle_len - Length of needle.\n    // haystack_len - Known minimum length of haystack.\n    size_t needle_len = min((size_t)max_needle_len, strlen(needle_start));\n    size_t haystack_len = min((size_t)max_haystack_len, strlen(haystack_start));\n\n    const char *haystack = haystack_start;\n    const char *needle = needle_start;\n    bool ok = true; /* needle is prefix of haystack. */\n\n    if(haystack_len<needle_len)\n        return 0;\n\n    size_t si = 0;\n    while (*haystack && *needle && si < needle_len)\n    {\n      ok &= *haystack++ == *needle++;\n      si++;\n    }\n    if (ok)\n    {\n      return 1;\n    }\n\n    if (needle_len < LONG_NEEDLE_THRESHOLD)\n    {\n        char *res = two_way_short_needle ((const unsigned char *) haystack_start,\n                                     haystack_len,\n                                     (const unsigned char *) needle_start, needle_len) ;\n        int ptrcomp = res != NULL;\n        return ptrcomp;\n    }\n\n    char* res = two_way_long_needle ((const unsigned char *) haystack, haystack_len,\n                              (const unsigned char *) needle, needle_len);\n    int ptrcomp2 = res != NULL ? 1 : 0;\n    return ptrcomp2;\n}\n\n\n/* Get space for VM temporary registers */\nint get_temps_space(const vm_params& params, char **mem, size_t block_size)\n{\n    int r, k = 1 + params.n_inputs + params.n_constants;\n\n    for (r = k; r < k + params.n_temps; r++) {\n        mem[r] = (char *)malloc(block_size * params.memsizes[r]);\n        if (mem[r] == NULL) {\n            return -1;\n        }\n    }\n    return 0;\n}\n\n/* Free space for VM temporary registers */\nvoid free_temps_space(const vm_params& params, char **mem)\n{\n    int r, k = 1 + params.n_inputs + params.n_constants;\n\n    for (r = k; r < k + params.n_temps; r++) {\n        free(mem[r]);\n    }\n}\n\n/* Serial/parallel task iterator version of the VM engine */\nint vm_engine_iter_task(NpyIter *iter, npy_intp *memsteps,\n                    const vm_params& params,\n                    int *pc_error, char **errmsg)\n{\n    char **mem = params.mem;\n    NpyIter_IterNextFunc *iternext;\n    npy_intp block_size, *size_ptr;\n    char **iter_dataptr;\n    npy_intp *iter_strides;\n\n    iternext = NpyIter_GetIterNext(iter, errmsg);\n    if (iternext == NULL) {\n        return -1;\n    }\n\n    size_ptr = NpyIter_GetInnerLoopSizePtr(iter);\n    iter_dataptr = NpyIter_GetDataPtrArray(iter);\n    iter_strides = NpyIter_GetInnerStrideArray(iter);\n\n    /*\n     * First do all the blocks with a compile-time fixed size.\n     * This makes a big difference (30-50% on some tests).\n     */\n    block_size = *size_ptr;\n    while (block_size == BLOCK_SIZE1) {\n#define REDUCTION_INNER_LOOP\n#define BLOCK_SIZE BLOCK_SIZE1\n#include \"interp_body.cpp\"\n#undef BLOCK_SIZE\n#undef REDUCTION_INNER_LOOP\n        iternext(iter);\n        block_size = *size_ptr;\n    }\n\n    /* Then finish off the rest */\n    if (block_size > 0) do {\n#define REDUCTION_INNER_LOOP\n#define BLOCK_SIZE block_size\n#include \"interp_body.cpp\"\n#undef BLOCK_SIZE\n#undef REDUCTION_INNER_LOOP\n    } while (iternext(iter));\n\n    return 0;\n}\n\nstatic int\nvm_engine_iter_outer_reduce_task(NpyIter *iter, npy_intp *memsteps,\n                const vm_params& params, int *pc_error, char **errmsg)\n{\n    char **mem = params.mem;\n    NpyIter_IterNextFunc *iternext;\n    npy_intp block_size, *size_ptr;\n    char **iter_dataptr;\n    npy_intp *iter_strides;\n\n    iternext = NpyIter_GetIterNext(iter, errmsg);\n    if (iternext == NULL) {\n        return -1;\n    }\n\n    size_ptr = NpyIter_GetInnerLoopSizePtr(iter);\n    iter_dataptr = NpyIter_GetDataPtrArray(iter);\n    iter_strides = NpyIter_GetInnerStrideArray(iter);\n\n    /*\n     * First do all the blocks with a compile-time fixed size.\n     * This makes a big difference (30-50% on some tests).\n     */\n    block_size = *size_ptr;\n    while (block_size == BLOCK_SIZE1) {\n#define BLOCK_SIZE BLOCK_SIZE1\n#define NO_OUTPUT_BUFFERING // Because it's a reduction\n#include \"interp_body.cpp\"\n#undef NO_OUTPUT_BUFFERING\n#undef BLOCK_SIZE\n        iternext(iter);\n        block_size = *size_ptr;\n    }\n\n    /* Then finish off the rest */\n    if (block_size > 0) do {\n#define BLOCK_SIZE block_size\n#define NO_OUTPUT_BUFFERING // Because it's a reduction\n#include \"interp_body.cpp\"\n#undef NO_OUTPUT_BUFFERING\n#undef BLOCK_SIZE\n    } while (iternext(iter));\n\n    return 0;\n}\n\n/* Parallel iterator version of VM engine */\nstatic int\nvm_engine_iter_parallel(NpyIter *iter, const vm_params& params,\n                        bool need_output_buffering, int *pc_error,\n                        char **errmsg)\n{\n    int i, ret = -1;\n    npy_intp numblocks, taskfactor;\n\n    if (errmsg == NULL) {\n        return -1;\n    }\n\n    /* Ensure only one parallel job is running at a time (otherwise\n       the global th_params get corrupted). */\n    Py_BEGIN_ALLOW_THREADS;\n    pthread_mutex_lock(&gs.parallel_mutex);\n    Py_END_ALLOW_THREADS;\n\n    /* Populate parameters for worker threads */\n    NpyIter_GetIterIndexRange(iter, &th_params.start, &th_params.vlen);\n    /*\n     * Try to make it so each thread gets 16 tasks.  This is a compromise\n     * between 1 task per thread and one block per task.\n     */\n    taskfactor = 16*BLOCK_SIZE1*gs.nthreads;\n    numblocks = (th_params.vlen - th_params.start + taskfactor - 1) /\n                            taskfactor;\n    th_params.block_size = numblocks * BLOCK_SIZE1;\n\n    th_params.params = params;\n    th_params.need_output_buffering = need_output_buffering;\n    th_params.ret_code = 0;\n    th_params.pc_error = pc_error;\n    th_params.errmsg = errmsg;\n    th_params.iter[0] = iter;\n    /* Make one copy for each additional thread */\n    for (i = 1; i < gs.nthreads; ++i) {\n        th_params.iter[i] = NpyIter_Copy(iter);\n        if (th_params.iter[i] == NULL) {\n            --i;\n            for (; i > 0; --i) {\n                NpyIter_Deallocate(th_params.iter[i]);\n            }\n            goto end;\n        }\n    }\n    th_params.memsteps[0] = params.memsteps;\n    /* Make one copy of memsteps for each additional thread */\n    for (i = 1; i < gs.nthreads; ++i) {\n        th_params.memsteps[i] = PyMem_New(npy_intp,\n                    1 + params.n_inputs + params.n_constants + params.n_temps);\n        if (th_params.memsteps[i] == NULL) {\n            --i;\n            for (; i > 0; --i) {\n                PyMem_Del(th_params.memsteps[i]);\n            }\n            for (i = 0; i < gs.nthreads; ++i) {\n                NpyIter_Deallocate(th_params.iter[i]);\n            }\n            goto end;\n        }\n        memcpy(th_params.memsteps[i], th_params.memsteps[0],\n                sizeof(npy_intp) *\n                (1 + params.n_inputs + params.n_constants + params.n_temps));\n    }\n\n    Py_BEGIN_ALLOW_THREADS;\n\n    /* Synchronization point for all threads (wait for initialization) */\n    pthread_mutex_lock(&gs.count_threads_mutex);\n    if (gs.count_threads < gs.nthreads) {\n        gs.count_threads++;\n        pthread_cond_wait(&gs.count_threads_cv, &gs.count_threads_mutex);\n    }\n    else {\n        pthread_cond_broadcast(&gs.count_threads_cv);\n    }\n    pthread_mutex_unlock(&gs.count_threads_mutex);\n\n    /* Synchronization point for all threads (wait for finalization) */\n    pthread_mutex_lock(&gs.count_threads_mutex);\n    if (gs.count_threads > 0) {\n        gs.count_threads--;\n        pthread_cond_wait(&gs.count_threads_cv, &gs.count_threads_mutex);\n    }\n    else {\n        pthread_cond_broadcast(&gs.count_threads_cv);\n    }\n    pthread_mutex_unlock(&gs.count_threads_mutex);\n\n    Py_END_ALLOW_THREADS;\n\n    /* Deallocate all the iterator and memsteps copies */\n    for (i = 1; i < gs.nthreads; ++i) {\n        NpyIter_Deallocate(th_params.iter[i]);\n        PyMem_Del(th_params.memsteps[i]);\n    }\n\n    ret = th_params.ret_code;\n\nend:\n    pthread_mutex_unlock(&gs.parallel_mutex);\n    return ret;\n}\n\nstatic int\nrun_interpreter(NumExprObject *self, NpyIter *iter, NpyIter *reduce_iter,\n                     bool reduction_outer_loop, bool need_output_buffering,\n                     int *pc_error)\n{\n    int r;\n    Py_ssize_t plen;\n    vm_params params;\n    char *errmsg = NULL;\n\n    *pc_error = -1;\n    if (PyBytes_AsStringAndSize(self->program, (char **)&(params.program),\n                                &plen) < 0) {\n        return -1;\n    }\n\n    params.prog_len = (int)plen;\n    params.output = NULL;\n    params.inputs = NULL;\n    params.index_data = NULL;\n    params.n_inputs = self->n_inputs;\n    params.n_constants = self->n_constants;\n    params.n_temps = self->n_temps;\n    params.mem = self->mem;\n    params.memsteps = self->memsteps;\n    params.memsizes = self->memsizes;\n    params.r_end = (int)PyBytes_Size(self->fullsig);\n    params.out_buffer = NULL;\n\n    if ((gs.nthreads == 1) || gs.force_serial) {\n        // Can do it as one \"task\"\n        if (reduce_iter == NULL) {\n            // Allocate memory for output buffering if needed\n            vector<char> out_buffer(need_output_buffering ?\n                                (self->memsizes[0] * BLOCK_SIZE1) : 0);\n            params.out_buffer = need_output_buffering ? &out_buffer[0] : NULL;\n            // Reset the iterator to allocate its buffers\n            if(NpyIter_Reset(iter, NULL) != NPY_SUCCEED) {\n                return -1;\n            }\n            get_temps_space(params, params.mem, BLOCK_SIZE1);\n            Py_BEGIN_ALLOW_THREADS;\n            r = vm_engine_iter_task(iter, params.memsteps,\n                                        params, pc_error, &errmsg);\n            Py_END_ALLOW_THREADS;\n            free_temps_space(params, params.mem);\n        }\n        else {\n            if (reduction_outer_loop) {\n                char **dataptr;\n                NpyIter_IterNextFunc *iternext;\n\n                dataptr = NpyIter_GetDataPtrArray(reduce_iter);\n                iternext = NpyIter_GetIterNext(reduce_iter, NULL);\n                if (iternext == NULL) {\n                    return -1;\n                }\n\n                get_temps_space(params, params.mem, BLOCK_SIZE1);\n                Py_BEGIN_ALLOW_THREADS;\n                do {\n                    r = NpyIter_ResetBasePointers(iter, dataptr, &errmsg);\n                    if (r >= 0) {\n                        r = vm_engine_iter_outer_reduce_task(iter,\n                                                params.memsteps, params,\n                                                pc_error, &errmsg);\n                    }\n                    if (r < 0) {\n                        break;\n                    }\n                } while (iternext(reduce_iter));\n                Py_END_ALLOW_THREADS;\n                free_temps_space(params, params.mem);\n            }\n            else {\n                char **dataptr;\n                NpyIter_IterNextFunc *iternext;\n\n                dataptr = NpyIter_GetDataPtrArray(iter);\n                iternext = NpyIter_GetIterNext(iter, NULL);\n                if (iternext == NULL) {\n                    return -1;\n                }\n\n                get_temps_space(params, params.mem, BLOCK_SIZE1);\n                Py_BEGIN_ALLOW_THREADS;\n                do {\n                    r = NpyIter_ResetBasePointers(reduce_iter, dataptr,\n                                                                    &errmsg);\n                    if (r >= 0) {\n                        r = vm_engine_iter_task(reduce_iter, params.memsteps,\n                                                params, pc_error, &errmsg);\n                    }\n                    if (r < 0) {\n                        break;\n                    }\n                } while (iternext(iter));\n                Py_END_ALLOW_THREADS;\n                free_temps_space(params, params.mem);\n            }\n        }\n    }\n    else {\n        if (reduce_iter == NULL) {\n            r = vm_engine_iter_parallel(iter, params, need_output_buffering,\n                        pc_error, &errmsg);\n        }\n        else {\n            errmsg = \"Parallel engine doesn't support reduction yet\";\n            r = -1;\n        }\n    }\n\n    if (r < 0 && errmsg != NULL) {\n        PyErr_SetString(PyExc_RuntimeError, errmsg);\n    }\n\n    return 0;\n}\n\nstatic int\nrun_interpreter_const(NumExprObject *self, char *output, int *pc_error)\n{\n    vm_params params;\n    Py_ssize_t plen;\n    char **mem;\n    npy_intp *memsteps;\n\n    *pc_error = -1;\n    if (PyBytes_AsStringAndSize(self->program, (char **)&(params.program),\n                                &plen) < 0) {\n        return -1;\n    }\n    if (self->n_inputs != 0) {\n        return -1;\n    }\n    params.prog_len = (int)plen;\n    params.output = output;\n    params.inputs = NULL;\n    params.index_data = NULL;\n    params.n_inputs = self->n_inputs;\n    params.n_constants = self->n_constants;\n    params.n_temps = self->n_temps;\n    params.mem = self->mem;\n    memsteps = self->memsteps;\n    params.memsizes = self->memsizes;\n    params.r_end = (int)PyBytes_Size(self->fullsig);\n\n    mem = params.mem;\n    get_temps_space(params, mem, 1);\n#define SINGLE_ITEM_CONST_LOOP\n#define BLOCK_SIZE 1\n#define NO_OUTPUT_BUFFERING // Because it's constant\n#include \"interp_body.cpp\"\n#undef NO_OUTPUT_BUFFERING\n#undef BLOCK_SIZE\n#undef SINGLE_ITEM_CONST_LOOP\n    free_temps_space(params, mem);\n\n    return 0;\n}\n\nPyObject *\nNumExpr_run(NumExprObject *self, PyObject *args, PyObject *kwds)\n{\n    PyArrayObject *operands[NPY_MAXARGS];\n    PyArray_Descr *dtypes[NPY_MAXARGS], **dtypes_tmp;\n    PyObject *tmp, *ret;\n    npy_uint32 op_flags[NPY_MAXARGS];\n    NPY_CASTING casting = NPY_SAFE_CASTING;\n    NPY_ORDER order = NPY_KEEPORDER;\n    unsigned int i, n_inputs;\n    int r, pc_error = 0;\n    int reduction_axis = -1;\n    npy_intp reduction_size = 1;\n    int ex_uses_vml = 0, is_reduction = 0;\n    bool reduction_outer_loop = false, need_output_buffering = false;\n\n    // To specify axes when doing a reduction\n    int op_axes_values[NPY_MAXARGS][NPY_MAXDIMS],\n         op_axes_reduction_values[NPY_MAXARGS];\n    int *op_axes_ptrs[NPY_MAXDIMS];\n    int oa_ndim = 0;\n    int **op_axes = NULL;\n\n    NpyIter *iter = NULL, *reduce_iter = NULL;\n\n    // Check whether we need to restart threads\n    if (!gs.init_threads_done || gs.pid != getpid()) {\n        numexpr_set_nthreads(gs.nthreads);\n    }\n\n    // Don't force serial mode by default\n    gs.force_serial = 0;\n\n    // Check whether there's a reduction as the final step\n    is_reduction = last_opcode(self->program) > OP_REDUCTION;\n\n    n_inputs = (int)PyTuple_Size(args);\n    if (PyBytes_Size(self->signature) != n_inputs) {\n        return PyErr_Format(PyExc_ValueError,\n                            \"number of inputs doesn't match program\");\n    }\n    else if (n_inputs+1 > NPY_MAXARGS) {\n        return PyErr_Format(PyExc_ValueError,\n                            \"too many inputs\");\n    }\n\n    memset(operands, 0, sizeof(operands));\n    memset(dtypes, 0, sizeof(dtypes));\n\n    if (kwds) {\n        tmp = PyDict_GetItemString(kwds, \"casting\"); // borrowed ref\n        if (tmp != NULL && !PyArray_CastingConverter(tmp, &casting)) {\n            return NULL;\n        }\n        tmp = PyDict_GetItemString(kwds, \"order\"); // borrowed ref\n        if (tmp != NULL && !PyArray_OrderConverter(tmp, &order)) {\n            return NULL;\n        }\n        tmp = PyDict_GetItemString(kwds, \"ex_uses_vml\"); // borrowed ref\n        if (tmp == NULL) {\n            return PyErr_Format(PyExc_ValueError,\n                                \"ex_uses_vml parameter is required\");\n        }\n        if (tmp == Py_True) {\n            ex_uses_vml = 1;\n        }\n            // borrowed ref\n        operands[0] = (PyArrayObject *)PyDict_GetItemString(kwds, \"out\");\n        if (operands[0] != NULL) {\n            if ((PyObject *)operands[0] == Py_None) {\n                operands[0] = NULL;\n            }\n            else if (!PyArray_Check(operands[0])) {\n                return PyErr_Format(PyExc_ValueError,\n                                    \"out keyword parameter is not an array\");\n            }\n            else {\n                Py_INCREF(operands[0]);\n            }\n        }\n    }\n\n    for (i = 0; i < n_inputs; i++) {\n        PyObject *o = PyTuple_GET_ITEM(args, i); // borrowed ref\n        PyObject *a;\n        char c = PyBytes_AS_STRING(self->signature)[i];\n        int typecode = typecode_from_char(c);\n        // Convert it if it's not an array\n        if (!PyArray_Check(o)) {\n            if (typecode == -1) goto fail;\n            a = PyArray_FROM_OTF(o, typecode, NPY_ARRAY_NOTSWAPPED);\n        }\n        else {\n            Py_INCREF(o);\n            a = o;\n        }\n        operands[i+1] = (PyArrayObject *)a;\n        dtypes[i+1] = PyArray_DescrFromType(typecode);\n\n        if (operands[0] != NULL) {\n            // Check for the case where \"out\" is one of the inputs\n            // TODO: Probably should deal with the general overlap case,\n            //       but NumPy ufuncs don't do that yet either.\n            if (PyArray_DATA(operands[0]) == PyArray_DATA(operands[i+1])) {\n                need_output_buffering = true;\n            }\n        }\n\n        if (operands[i+1] == NULL || dtypes[i+1] == NULL) {\n            goto fail;\n        }\n        op_flags[i+1] = NPY_ITER_READONLY|\n#ifdef USE_VML\n                        (ex_uses_vml ? (NPY_ITER_CONTIG|NPY_ITER_ALIGNED) : 0)|\n#endif\n#ifndef USE_UNALIGNED_ACCESS\n                        NPY_ITER_ALIGNED|\n#endif\n                        NPY_ITER_NBO\n                        ;\n    }\n\n    if (is_reduction) {\n        // A reduction can not result in a string,\n        // so we don't need to worry about item sizes here.\n        char retsig = get_return_sig(self->program);\n        reduction_axis = get_reduction_axis(self->program);\n\n        // Need to set up op_axes for the non-reduction part\n        if (reduction_axis != 255) {\n            // Get the number of broadcast dimensions\n            for (i = 0; i < n_inputs; ++i) {\n                int ndim = PyArray_NDIM(operands[i+1]);\n                if (ndim > oa_ndim) {\n                    oa_ndim = ndim;\n                }\n            }\n            if (reduction_axis < 0 || reduction_axis >= oa_ndim) {\n                PyErr_Format(PyExc_ValueError,\n                        \"reduction axis is out of bounds\");\n                goto fail;\n            }\n            // Fill in the op_axes\n            op_axes_ptrs[0] = NULL;\n            op_axes_reduction_values[0] = -1;\n            for (i = 0; i < n_inputs; ++i) {\n                int j = 0, idim, ndim = PyArray_NDIM(operands[i+1]);\n                for (idim = 0; idim < oa_ndim-ndim; ++idim) {\n                    if (idim != reduction_axis) {\n                        op_axes_values[i+1][j++] = -1;\n                    }\n                    else {\n                        op_axes_reduction_values[i+1] = -1;\n                    }\n                }\n                for (idim = oa_ndim-ndim; idim < oa_ndim; ++idim) {\n                    if (idim != reduction_axis) {\n                        op_axes_values[i+1][j++] = idim-(oa_ndim-ndim);\n                    }\n                    else {\n                        npy_intp size = PyArray_DIM(operands[i+1],\n                                                    idim-(oa_ndim-ndim));\n                        if (size > reduction_size) {\n                            reduction_size = size;\n                        }\n                        op_axes_reduction_values[i+1] = idim-(oa_ndim-ndim);\n                    }\n                }\n                op_axes_ptrs[i+1] = op_axes_values[i+1];\n            }\n            // op_axes has one less than the broadcast dimensions\n            --oa_ndim;\n            if (oa_ndim > 0) {\n                op_axes = op_axes_ptrs;\n            }\n            else {\n                reduction_size = 1;\n            }\n        }\n        // A full reduction can be done without nested iteration\n        if (oa_ndim == 0) {\n            if (operands[0] == NULL) {\n                npy_intp dim = 1;\n                operands[0] = (PyArrayObject *)PyArray_SimpleNew(0, &dim,\n                                            typecode_from_char(retsig));\n                if (!operands[0])\n                    goto fail;\n            } else if (PyArray_SIZE(operands[0]) != 1) {\n                PyErr_Format(PyExc_ValueError,\n                        \"out argument must have size 1 for a full reduction\");\n                goto fail;\n            }\n        }\n\n        dtypes[0] = PyArray_DescrFromType(typecode_from_char(retsig));\n\n        op_flags[0] = NPY_ITER_READWRITE|\n                      NPY_ITER_ALLOCATE|\n                      // Copy, because it can't buffer the reduction\n                      NPY_ITER_UPDATEIFCOPY|\n                      NPY_ITER_NBO|\n#ifndef USE_UNALIGNED_ACCESS\n                      NPY_ITER_ALIGNED|\n#endif\n                      (oa_ndim == 0 ? 0 : NPY_ITER_NO_BROADCAST);\n    }\n    else {\n        char retsig = get_return_sig(self->program);\n        if (retsig != 's') {\n            dtypes[0] = PyArray_DescrFromType(typecode_from_char(retsig));\n        } else {\n            /* Since the *only* supported operation returning a string\n             * is a copy, the size of returned strings\n             * can be directly gotten from the first (and only)\n             * input/constant/temporary. */\n            if (n_inputs > 0) {  // input, like in 'a' where a -> 'foo'\n                dtypes[0] = PyArray_DESCR(operands[1]);\n                Py_INCREF(dtypes[0]);\n            } else {  // constant, like in '\"foo\"'\n                dtypes[0] = PyArray_DescrNewFromType(NPY_STRING);\n                dtypes[0]->elsize = (int)self->memsizes[1];\n            }  // no string temporaries, so no third case\n        }\n        if (dtypes[0] == NULL) {\n            goto fail;\n        }\n        op_flags[0] = NPY_ITER_WRITEONLY|\n                      NPY_ITER_ALLOCATE|\n                      NPY_ITER_CONTIG|\n                      NPY_ITER_NBO|\n#ifndef USE_UNALIGNED_ACCESS\n                      NPY_ITER_ALIGNED|\n#endif\n                      NPY_ITER_NO_BROADCAST;\n    }\n\n    // Check for empty arrays in expression\n    if (n_inputs > 0) {\n        char retsig = get_return_sig(self->program);\n\n        // Check length for all inputs\n        int zeroi, zerolen = 0;\n        for (i=0; i < n_inputs; i++) {\n            if (PyArray_SIZE(operands[i+1]) == 0) {\n                zerolen = 1;\n                zeroi = i+1;\n                break;\n            }\n        }\n\n        if (zerolen != 0) {\n            // Allocate the output\n            int ndim = PyArray_NDIM(operands[zeroi]);\n            npy_intp *dims = PyArray_DIMS(operands[zeroi]);\n            operands[0] = (PyArrayObject *)PyArray_SimpleNew(ndim, dims,\n                                              typecode_from_char(retsig));\n            if (operands[0] == NULL) {\n                goto fail;\n            }\n\n            ret = (PyObject *)operands[0];\n            Py_INCREF(ret);\n            goto cleanup_and_exit;\n        }\n    }\n\n\n    /* A case with a single constant output */\n    if (n_inputs == 0) {\n        char retsig = get_return_sig(self->program);\n\n        /* Allocate the output */\n        if (operands[0] == NULL) {\n            npy_intp dim = 1;\n            operands[0] = (PyArrayObject *)PyArray_SimpleNew(0, &dim,\n                                        typecode_from_char(retsig));\n            if (operands[0] == NULL) {\n                goto fail;\n            }\n        }\n        else {\n            PyArrayObject *a;\n            if (PyArray_SIZE(operands[0]) != 1) {\n                PyErr_SetString(PyExc_ValueError,\n                        \"output for a constant expression must have size 1\");\n                goto fail;\n            }\n            else if (!PyArray_ISWRITEABLE(operands[0])) {\n                PyErr_SetString(PyExc_ValueError,\n                        \"output is not writeable\");\n                goto fail;\n            }\n            Py_INCREF(dtypes[0]);\n            a = (PyArrayObject *)PyArray_FromArray(operands[0], dtypes[0],\n                                        NPY_ARRAY_ALIGNED|NPY_ARRAY_UPDATEIFCOPY);\n            if (a == NULL) {\n                goto fail;\n            }\n            Py_DECREF(operands[0]);\n            operands[0] = a;\n        }\n\n        r = run_interpreter_const(self, PyArray_BYTES(operands[0]), &pc_error);\n\n        ret = (PyObject *)operands[0];\n        Py_INCREF(ret);\n        goto cleanup_and_exit;\n    }\n\n\n    /* Allocate the iterator or nested iterators */\n    if (reduction_size == 1) {\n        /* When there's no reduction, reduction_size is 1 as well */\n        iter = NpyIter_AdvancedNew(n_inputs+1, operands,\n                            NPY_ITER_BUFFERED|\n                            NPY_ITER_REDUCE_OK|\n                            NPY_ITER_RANGED|\n                            NPY_ITER_DELAY_BUFALLOC|\n                            NPY_ITER_EXTERNAL_LOOP,\n                            order, casting,\n                            op_flags, dtypes,\n                            -1, NULL, NULL,\n                            BLOCK_SIZE1);\n        if (iter == NULL) {\n            goto fail;\n        }\n    } else {\n        npy_uint32 op_flags_outer[NPY_MAXDIMS];\n        /* The outer loop is unbuffered */\n        op_flags_outer[0] = NPY_ITER_READWRITE|\n                            NPY_ITER_ALLOCATE|\n                            NPY_ITER_NO_BROADCAST;\n        for (i = 0; i < n_inputs; ++i) {\n            op_flags_outer[i+1] = NPY_ITER_READONLY;\n        }\n        /* Arbitrary threshold for which is the inner loop...benchmark? */\n        if (reduction_size < 64) {\n            reduction_outer_loop = true;\n            iter = NpyIter_AdvancedNew(n_inputs+1, operands,\n                                NPY_ITER_BUFFERED|\n                                NPY_ITER_RANGED|\n                                NPY_ITER_DELAY_BUFALLOC|\n                                NPY_ITER_EXTERNAL_LOOP,\n                                order, casting,\n                                op_flags, dtypes,\n                                oa_ndim, op_axes, NULL,\n                                BLOCK_SIZE1);\n            if (iter == NULL) {\n                goto fail;\n            }\n\n            /* If the output was allocated, get it for the second iterator */\n            if (operands[0] == NULL) {\n                operands[0] = NpyIter_GetOperandArray(iter)[0];\n                Py_INCREF(operands[0]);\n            }\n\n            op_axes[0] = &op_axes_reduction_values[0];\n            for (i = 0; i < n_inputs; ++i) {\n                op_axes[i+1] = &op_axes_reduction_values[i+1];\n            }\n            op_flags_outer[0] &= ~NPY_ITER_NO_BROADCAST;\n            reduce_iter = NpyIter_AdvancedNew(n_inputs+1, operands,\n                                NPY_ITER_REDUCE_OK,\n                                order, casting,\n                                op_flags_outer, NULL,\n                                1, op_axes, NULL,\n                                0);\n            if (reduce_iter == NULL) {\n                goto fail;\n            }\n        }\n        else {\n            PyArray_Descr *dtypes_outer[NPY_MAXDIMS];\n\n            /* If the output is being allocated, need to specify its dtype */\n            dtypes_outer[0] = dtypes[0];\n            for (i = 0; i < n_inputs; ++i) {\n                dtypes_outer[i+1] = NULL;\n            }\n            iter = NpyIter_AdvancedNew(n_inputs+1, operands,\n                                NPY_ITER_RANGED,\n                                order, casting,\n                                op_flags_outer, dtypes_outer,\n                                oa_ndim, op_axes, NULL,\n                                0);\n            if (iter == NULL) {\n                goto fail;\n            }\n\n            /* If the output was allocated, get it for the second iterator */\n            if (operands[0] == NULL) {\n                operands[0] = NpyIter_GetOperandArray(iter)[0];\n                Py_INCREF(operands[0]);\n            }\n\n            op_axes[0] = &op_axes_reduction_values[0];\n            for (i = 0; i < n_inputs; ++i) {\n                op_axes[i+1] = &op_axes_reduction_values[i+1];\n            }\n            op_flags[0] &= ~NPY_ITER_NO_BROADCAST;\n            reduce_iter = NpyIter_AdvancedNew(n_inputs+1, operands,\n                                NPY_ITER_BUFFERED|\n                                NPY_ITER_REDUCE_OK|\n                                NPY_ITER_DELAY_BUFALLOC|\n                                NPY_ITER_EXTERNAL_LOOP,\n                                order, casting,\n                                op_flags, dtypes,\n                                1, op_axes, NULL,\n                                BLOCK_SIZE1);\n            if (reduce_iter == NULL) {\n                goto fail;\n            }\n        }\n    }\n\n    /* Initialize the output to the reduction unit */\n    if (is_reduction) {\n        PyArrayObject *a = NpyIter_GetOperandArray(iter)[0];\n        PyObject *fill;\n        int op = last_opcode(self->program);\n        if (op < OP_PROD) {\n            /* sum identity is 0 */\n            fill = PyLong_FromLong(0);\n        } else if (op >= OP_PROD && op < OP_MIN) {\n            /* product identity is 1 */\n            fill = PyLong_FromLong(1);\n        } else if (PyArray_DESCR(a)->kind == 'f') {\n            /* floating point min/max identity is NaN */\n            fill = PyFloat_FromDouble(NAN);\n        } else if (op >= OP_MIN && op < OP_MAX) {\n            /* integer min identity */\n            fill = PyLong_FromLong(LONG_MAX);\n        } else {\n            /* integer max identity */\n            fill = PyLong_FromLong(LONG_MIN);\n        }\n        PyArray_FillWithScalar(a, fill);\n        Py_DECREF(fill);\n    }\n\n    /* Get the sizes of all the operands */\n    dtypes_tmp = NpyIter_GetDescrArray(iter);\n    for (i = 0; i < n_inputs+1; ++i) {\n        self->memsizes[i] = dtypes_tmp[i]->elsize;\n    }\n\n    /* For small calculations, just use 1 thread */\n    if (NpyIter_GetIterSize(iter) < 2*BLOCK_SIZE1) {\n        gs.force_serial = 1;\n    }\n\n    /* Reductions do not support parallel execution yet */\n    if (is_reduction) {\n        gs.force_serial = 1;\n    }\n\n    r = run_interpreter(self, iter, reduce_iter,\n                             reduction_outer_loop, need_output_buffering,\n                             &pc_error);\n\n    if (r < 0) {\n        if (r == -1) {\n            if (!PyErr_Occurred()) {\n                PyErr_SetString(PyExc_RuntimeError,\n                                \"an error occurred while running the program\");\n            }\n        } else if (r == -2) {\n            PyErr_Format(PyExc_RuntimeError,\n                         \"bad argument at pc=%d\", pc_error);\n        } else if (r == -3) {\n            PyErr_Format(PyExc_RuntimeError,\n                         \"bad opcode at pc=%d\", pc_error);\n        } else {\n            PyErr_SetString(PyExc_RuntimeError,\n                            \"unknown error occurred while running the program\");\n        }\n        goto fail;\n    }\n\n    /* Get the output from the iterator */\n    ret = (PyObject *)NpyIter_GetOperandArray(iter)[0];\n    Py_INCREF(ret);\n\n    NpyIter_Deallocate(iter);\n    if (reduce_iter != NULL) {\n        NpyIter_Deallocate(reduce_iter);\n    }\ncleanup_and_exit:\n    for (i = 0; i < n_inputs+1; i++) {\n        Py_XDECREF(operands[i]);\n        Py_XDECREF(dtypes[i]);\n    }\n\n    return ret;\nfail:\n    for (i = 0; i < n_inputs+1; i++) {\n        Py_XDECREF(operands[i]);\n        Py_XDECREF(dtypes[i]);\n    }\n    if (iter != NULL) {\n        NpyIter_Deallocate(iter);\n    }\n    if (reduce_iter != NULL) {\n        NpyIter_Deallocate(reduce_iter);\n    }\n\n    return NULL;\n}\n\n/*\nLocal Variables:\n   c-basic-offset: 4\nEnd:\n*/\n"
    },
    {
      "filename": "numexpr/module.cpp",
      "content": "// Numexpr - Fast numerical array expression evaluator for NumPy.\n//\n//      License: MIT\n//      Author:  See AUTHORS.txt\n//\n//  See LICENSE.txt for details about copyright and rights to use.\n//\n// module.cpp contains the CPython-specific module exposure.\n\n#define DO_NUMPY_IMPORT_ARRAY\n\n#include \"module.hpp\"\n#include <structmember.h>\n#include <vector>\n\n#include \"interpreter.hpp\"\n#include \"numexpr_object.hpp\"\n\nusing namespace std;\n\n// Global state. The file interpreter.hpp also has some global state\n// in its 'th_params' variable\nglobal_state gs;\n\n\n/* Do the worker job for a certain thread */\nvoid *th_worker(void *tidptr)\n{\n    int tid = *(int *)tidptr;\n    /* Parameters for threads */\n    npy_intp start;\n    npy_intp vlen;\n    npy_intp block_size;\n    NpyIter *iter;\n    vm_params params;\n    int *pc_error;\n    int ret;\n    int n_inputs;\n    int n_constants;\n    int n_temps;\n    size_t memsize;\n    char **mem;\n    npy_intp *memsteps;\n    npy_intp istart, iend;\n    char **errmsg;\n    // For output buffering if needed\n    vector<char> out_buffer;\n\n    while (1) {\n\n        /* Sentinels have to be initialised yet */\n        gs.init_sentinels_done = 0;\n\n        /* Meeting point for all threads (wait for initialization) */\n        pthread_mutex_lock(&gs.count_threads_mutex);\n        if (gs.count_threads < gs.nthreads) {\n            gs.count_threads++;\n            pthread_cond_wait(&gs.count_threads_cv, &gs.count_threads_mutex);\n        }\n        else {\n            pthread_cond_broadcast(&gs.count_threads_cv);\n        }\n        pthread_mutex_unlock(&gs.count_threads_mutex);\n\n        /* Check if thread has been asked to return */\n        if (gs.end_threads) {\n            return(0);\n        }\n\n        /* Get parameters for this thread before entering the main loop */\n        start = th_params.start;\n        vlen = th_params.vlen;\n        block_size = th_params.block_size;\n        params = th_params.params;\n        pc_error = th_params.pc_error;\n\n        // If output buffering is needed, allocate it\n        if (th_params.need_output_buffering) {\n            out_buffer.resize(params.memsizes[0] * BLOCK_SIZE1);\n            params.out_buffer = &out_buffer[0];\n        } else {\n            params.out_buffer = NULL;\n        }\n\n        /* Populate private data for each thread */\n        n_inputs = params.n_inputs;\n        n_constants = params.n_constants;\n        n_temps = params.n_temps;\n        memsize = (1+n_inputs+n_constants+n_temps) * sizeof(char *);\n        /* XXX malloc seems thread safe for POSIX, but for Win? */\n        mem = (char **)malloc(memsize);\n        memcpy(mem, params.mem, memsize);\n\n        errmsg = th_params.errmsg;\n\n        params.mem = mem;\n\n        /* Loop over blocks */\n        pthread_mutex_lock(&gs.count_mutex);\n        if (!gs.init_sentinels_done) {\n            /* Set sentinels and other global variables */\n            gs.gindex = start;\n            istart = gs.gindex;\n            iend = istart + block_size;\n            if (iend > vlen) {\n                iend = vlen;\n            }\n            gs.init_sentinels_done = 1;  /* sentinels have been initialised */\n            gs.giveup = 0;            /* no giveup initially */\n        } else {\n            gs.gindex += block_size;\n            istart = gs.gindex;\n            iend = istart + block_size;\n            if (iend > vlen) {\n                iend = vlen;\n            }\n        }\n        /* Grab one of the iterators */\n        iter = th_params.iter[tid];\n        if (iter == NULL) {\n            th_params.ret_code = -1;\n            gs.giveup = 1;\n        }\n        memsteps = th_params.memsteps[tid];\n        /* Get temporary space for each thread */\n        ret = get_temps_space(params, mem, BLOCK_SIZE1);\n        if (ret < 0) {\n            /* Propagate error to main thread */\n            th_params.ret_code = ret;\n            gs.giveup = 1;\n        }\n        pthread_mutex_unlock(&gs.count_mutex);\n\n        while (istart < vlen && !gs.giveup) {\n            /* Reset the iterator to the range for this task */\n            ret = NpyIter_ResetToIterIndexRange(iter, istart, iend,\n                                                errmsg);\n            /* Execute the task */\n            if (ret >= 0) {\n                ret = vm_engine_iter_task(iter, memsteps, params, pc_error, errmsg);\n            }\n\n            if (ret < 0) {\n                pthread_mutex_lock(&gs.count_mutex);\n                gs.giveup = 1;\n                /* Propagate error to main thread */\n                th_params.ret_code = ret;\n                pthread_mutex_unlock(&gs.count_mutex);\n                break;\n            }\n\n            pthread_mutex_lock(&gs.count_mutex);\n            gs.gindex += block_size;\n            istart = gs.gindex;\n            iend = istart + block_size;\n            if (iend > vlen) {\n                iend = vlen;\n            }\n            pthread_mutex_unlock(&gs.count_mutex);\n        }\n\n        /* Meeting point for all threads (wait for finalization) */\n        pthread_mutex_lock(&gs.count_threads_mutex);\n        if (gs.count_threads > 0) {\n            gs.count_threads--;\n            pthread_cond_wait(&gs.count_threads_cv, &gs.count_threads_mutex);\n        }\n        else {\n            pthread_cond_broadcast(&gs.count_threads_cv);\n        }\n        pthread_mutex_unlock(&gs.count_threads_mutex);\n\n        /* Release resources */\n        free_temps_space(params, mem);\n        free(mem);\n\n    }  /* closes while(1) */\n\n    /* This should never be reached, but anyway */\n    return(0);\n}\n\n/* Initialize threads */\nint init_threads(void)\n{\n    int tid, rc;\n\n    /* Initialize mutex and condition variable objects */\n    pthread_mutex_init(&gs.count_mutex, NULL);\n    pthread_mutex_init(&gs.parallel_mutex, NULL);\n\n    /* Barrier initialization */\n    pthread_mutex_init(&gs.count_threads_mutex, NULL);\n    pthread_cond_init(&gs.count_threads_cv, NULL);\n    gs.count_threads = 0;      /* Reset threads counter */\n\n    /* Finally, create the threads */\n    for (tid = 0; tid < gs.nthreads; tid++) {\n        gs.tids[tid] = tid;\n        rc = pthread_create(&gs.threads[tid], NULL, th_worker,\n                            (void *)&gs.tids[tid]);\n        if (rc) {\n            fprintf(stderr,\n                    \"ERROR; return code from pthread_create() is %d\\n\", rc);\n            fprintf(stderr, \"\\tError detail: %s\\n\", strerror(rc));\n            exit(-1);\n        }\n    }\n\n    gs.init_threads_done = 1;                 /* Initialization done! */\n    gs.pid = (int)getpid();                   /* save the PID for this process */\n\n    return(0);\n}\n\n/* Set the number of threads in numexpr's VM */\nint numexpr_set_nthreads(int nthreads_new)\n{\n    int nthreads_old = gs.nthreads;\n    int t, rc;\n    void *status;\n\n    if (nthreads_new > MAX_THREADS) {\n        fprintf(stderr,\n                \"Error.  nthreads cannot be larger than MAX_THREADS (%d)\",\n                MAX_THREADS);\n        return -1;\n    }\n    else if (nthreads_new <= 0) {\n        fprintf(stderr, \"Error.  nthreads must be a positive integer\");\n        return -1;\n    }\n\n    /* Only join threads if they are not initialized or if our PID is\n       different from that in pid var (probably means that we are a\n       subprocess, and thus threads are non-existent). */\n    if (gs.nthreads > 1 && gs.init_threads_done && gs.pid == getpid()) {\n        /* Tell all existing threads to finish */\n        gs.end_threads = 1;\n        pthread_mutex_lock(&gs.count_threads_mutex);\n        if (gs.count_threads < gs.nthreads) {\n            gs.count_threads++;\n            pthread_cond_wait(&gs.count_threads_cv, &gs.count_threads_mutex);\n        }\n        else {\n            pthread_cond_broadcast(&gs.count_threads_cv);\n        }\n        pthread_mutex_unlock(&gs.count_threads_mutex);\n\n        /* Join exiting threads */\n        for (t=0; t<gs.nthreads; t++) {\n            rc = pthread_join(gs.threads[t], &status);\n            if (rc) {\n                fprintf(stderr,\n                        \"ERROR; return code from pthread_join() is %d\\n\",\n                        rc);\n                fprintf(stderr, \"\\tError detail: %s\\n\", strerror(rc));\n                exit(-1);\n            }\n        }\n        gs.init_threads_done = 0;\n        gs.end_threads = 0;\n    }\n\n    /* Launch a new pool of threads (if necessary) */\n    gs.nthreads = nthreads_new;\n    if (gs.nthreads > 1 && (!gs.init_threads_done || gs.pid != getpid())) {\n        init_threads();\n    }\n\n    return nthreads_old;\n}\n\n\n#ifdef USE_VML\n\nstatic PyObject *\n_get_vml_version(PyObject *self, PyObject *args)\n{\n    int len=198;\n    char buf[198];\n    mkl_get_version_string(buf, len);\n    return Py_BuildValue(\"s\", buf);\n}\n\nstatic PyObject *\n_set_vml_accuracy_mode(PyObject *self, PyObject *args)\n{\n    int mode_in, mode_old;\n    if (!PyArg_ParseTuple(args, \"i\", &mode_in))\n    return NULL;\n    mode_old = vmlGetMode() & VML_ACCURACY_MASK;\n    vmlSetMode((mode_in & VML_ACCURACY_MASK) | VML_ERRMODE_IGNORE );\n    return Py_BuildValue(\"i\", mode_old);\n}\n\nstatic PyObject *\n_set_vml_num_threads(PyObject *self, PyObject *args)\n{\n    int max_num_threads;\n    if (!PyArg_ParseTuple(args, \"i\", &max_num_threads))\n    return NULL;\n    mkl_domain_set_num_threads(max_num_threads, MKL_DOMAIN_VML);\n    Py_RETURN_NONE;\n}\n\n#endif\n\nstatic PyObject *\n_set_num_threads(PyObject *self, PyObject *args)\n{\n    int num_threads, nthreads_old;\n    if (!PyArg_ParseTuple(args, \"i\", &num_threads))\n    return NULL;\n    nthreads_old = numexpr_set_nthreads(num_threads);\n    return Py_BuildValue(\"i\", nthreads_old);\n}\n\nstatic PyMethodDef module_methods[] = {\n#ifdef USE_VML\n    {\"_get_vml_version\", _get_vml_version, METH_VARARGS,\n     \"Get the VML/MKL library version.\"},\n    {\"_set_vml_accuracy_mode\", _set_vml_accuracy_mode, METH_VARARGS,\n     \"Set accuracy mode for VML functions.\"},\n    {\"_set_vml_num_threads\", _set_vml_num_threads, METH_VARARGS,\n     \"Suggests a maximum number of threads to be used in VML operations.\"},\n#endif\n    {\"_set_num_threads\", _set_num_threads, METH_VARARGS,\n     \"Suggests a maximum number of threads to be used in operations.\"},\n    {NULL}\n};\n\nstatic int\nadd_symbol(PyObject *d, const char *sname, int name, const char* routine_name)\n{\n    PyObject *o, *s;\n    int r;\n\n    if (!sname) {\n        return 0;\n    }\n\n    o = PyLong_FromLong(name);\n    s = PyBytes_FromString(sname);\n    if (!s) {\n        PyErr_SetString(PyExc_RuntimeError, routine_name);\n        return -1;\n    }\n    r = PyDict_SetItem(d, s, o);\n    Py_XDECREF(o);\n    return r;\n}\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n#if PY_MAJOR_VERSION >= 3\n\n/* XXX: handle the \"global_state\" state via moduledef */\nstatic struct PyModuleDef moduledef = {\n        PyModuleDef_HEAD_INIT,\n        \"interpreter\",\n        NULL,\n        -1,                 /* sizeof(struct global_state), */\n        module_methods,\n        NULL,\n        NULL,               /* module_traverse, */\n        NULL,               /* module_clear, */\n        NULL\n};\n\n#define INITERROR return NULL\n\nPyObject *\nPyInit_interpreter(void)\n\n#else\n#define INITERROR return\n\nPyMODINIT_FUNC\ninitinterpreter()\n#endif\n{\n    PyObject *m, *d;\n\n    if (PyType_Ready(&NumExprType) < 0)\n        INITERROR;\n\n#if PY_MAJOR_VERSION >= 3\n    m = PyModule_Create(&moduledef);\n#else\n    m = Py_InitModule3(\"interpreter\", module_methods, NULL);\n#endif\n\n    if (m == NULL)\n        INITERROR;\n\n    Py_INCREF(&NumExprType);\n    PyModule_AddObject(m, \"NumExpr\", (PyObject *)&NumExprType);\n\n    import_array();\n\n    d = PyDict_New();\n    if (!d) INITERROR;\n\n#define OPCODE(n, name, sname, ...)                              \\\n    if (add_symbol(d, sname, name, \"add_op\") < 0) { INITERROR; }\n#include \"opcodes.hpp\"\n#undef OPCODE\n\n    if (PyModule_AddObject(m, \"opcodes\", d) < 0) INITERROR;\n\n    d = PyDict_New();\n    if (!d) INITERROR;\n\n#define add_func(name, sname)                           \\\n    if (add_symbol(d, sname, name, \"add_func\") < 0) { INITERROR; }\n#define FUNC_FF(name, sname, ...)  add_func(name, sname);\n#define FUNC_FFF(name, sname, ...) add_func(name, sname);\n#define FUNC_DD(name, sname, ...)  add_func(name, sname);\n#define FUNC_DDD(name, sname, ...) add_func(name, sname);\n#define FUNC_CC(name, sname, ...)  add_func(name, sname);\n#define FUNC_CCC(name, sname, ...) add_func(name, sname);\n#include \"functions.hpp\"\n#undef FUNC_CCC\n#undef FUNC_CC\n#undef FUNC_DDD\n#undef FUNC_DD\n#undef FUNC_DD\n#undef FUNC_FFF\n#undef FUNC_FF\n#undef add_func\n\n    if (PyModule_AddObject(m, \"funccodes\", d) < 0) INITERROR;\n\n    if (PyModule_AddObject(m, \"allaxes\", PyLong_FromLong(255)) < 0) INITERROR;\n    if (PyModule_AddObject(m, \"maxdims\", PyLong_FromLong(NPY_MAXDIMS)) < 0) INITERROR;\n\n#if PY_MAJOR_VERSION >= 3\n    return m;\n#endif\n}\n\n#ifdef __cplusplus\n}  // extern \"C\"\n#endif\n"
    },
    {
      "filename": "numexpr/module.hpp",
      "content": "#ifndef NUMEXPR_MODULE_HPP\r\n#define NUMEXPR_MODULE_HPP\r\n\r\n// Deal with the clunky numpy import mechanism\r\n// by inverting the logic of the NO_IMPORT_ARRAY symbol.\r\n#define PY_ARRAY_UNIQUE_SYMBOL numexpr_ARRAY_API\r\n#ifndef DO_NUMPY_IMPORT_ARRAY\r\n#  define NO_IMPORT_ARRAY\r\n#endif\r\n\r\n#define NPY_NO_DEPRECATED_API NPY_API_VERSION\r\n\r\n#include <Python.h>\r\n#include <numpy/ndarrayobject.h>\r\n#include <numpy/arrayscalars.h>\r\n\r\n#include \"numexpr_config.hpp\"\r\n\r\nstruct global_state {\r\n    /* Global variables for threads */\r\n    int nthreads;                    /* number of desired threads in pool */\r\n    int init_threads_done;           /* pool of threads initialized? */\r\n    int end_threads;                 /* should exisiting threads end? */\r\n    pthread_t threads[MAX_THREADS];  /* opaque structure for threads */\r\n    int tids[MAX_THREADS];           /* ID per each thread */\r\n    npy_intp gindex;                 /* global index for all threads */\r\n    int init_sentinels_done;         /* sentinels initialized? */\r\n    int giveup;                      /* should parallel code giveup? */\r\n    int force_serial;                /* force serial code instead of parallel? */\r\n    int pid;                         /* the PID for this process */\r\n\r\n    /* Synchronization variables for threadpool state */\r\n    pthread_mutex_t count_mutex;\r\n    int count_threads;\r\n    pthread_mutex_t count_threads_mutex;\r\n    pthread_cond_t count_threads_cv;\r\n\r\n    /* Mutual exclusion for access to global thread params (th_params) */\r\n    pthread_mutex_t parallel_mutex;\r\n\r\n    global_state() {\r\n        nthreads = 1;\r\n        init_threads_done = 0;\r\n        end_threads = 0;\r\n        pid = 0;\r\n    }\r\n};\r\n\r\nextern global_state gs;\r\n\r\nint numexpr_set_nthreads(int nthreads_new);\r\n\r\n#endif // NUMEXPR_MODULE_HPP\r\n"
    }
  ],
  "questions": [
    "@devanshmehta can you come over to my table?",
    "@jbrockmendel : Could you add a link for future reference?"
  ],
  "golden_answers": [
    "Ok, I gave this one a shot and it's nowhere near easy. After figuring out simple stuff, you run into two huge obstacles.  One is that cython doesn't yet support numpy-1.7 api. Pandas has its own `numpy.pxd`, but it too has the necessary macros commented out with a description saying:\n\n```\n    # dtype PyArray_DESCR(ndarray) wrong refcount semantics\n```\n\nThat one can be figured out with some interaction with Cython guys (and probably backported there), but then there are rolling functions.\n\nSome of those functions in algos module do pure magic: they create one a single ndarray to refer to the window, pass it to python-level function and then alter its `arr.data` field to skip to the next element without incurring overhead of creating/destroying ndarrays. In np-1.7 api referring to data must be done via `PyArray_DATA(arr)` macro and it's probably not guaranteed to be assignable, so that optimization potential goes out of the window. I'm not sure if there's a way around this that doesn't lose performance.",
    "https://cython.readthedocs.io/en/latest/src/reference/compilation.html#configuring-the-c-build\r\n\r\n> Despite this, you will still get warnings like the following from the compiler, because Cython is using a deprecated Numpy API:\r\n>\r\n>> .../include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n>\r\n> For the time being, it is just a warning that you can ignore.\r\n\r\n@jreback \r\n> i believe u can set a flag about the api to turn off the old api\r\n\r\nThe issue isn't pandas using the old API, it is cython using it.  As long as cython-generated C is using the old API, we're not getting rid of those warnings.  The relevant flag would be to add to `setup.macros` the entry `('NPY_NO_DEPRECATED_API', 'NPY_1_7_API_VERSION')`, but doing so causes compile-time errors."
  ],
  "questions_generated": [
    "What is the primary motivation for transitioning to the NumPy 1.7 API in the pandas repository?",
    "What challenges are identified in the issue description regarding the transition to NumPy 1.7 API?",
    "What specific warning is generated when using the deprecated NumPy API, and how can it be avoided?",
    "Why might the optimization of rolling functions in the algos module be problematic with the NumPy 1.7 API?",
    "What role does the 'numpy.pxd' file play in this issue, and what is the limitation mentioned?",
    "How does the issue description suggest that the transition to NumPy 1.7 API could impact performance?",
    "What is the significance of the xref link provided in the issue description, and how does it relate to the problem being addressed?"
  ],
  "golden_answers_generated": [
    "The primary motivation for transitioning to the NumPy 1.7 API in the pandas repository is to eliminate support for NumPy 1.6, which allows the usage of newer API features available in NumPy 1.7. This transition also helps in getting rid of build warnings related to the use of deprecated APIs.",
    "The challenges identified include the need to fix certain macros in the pandas internals and the fact that Cython doesn't yet fully support the NumPy 1.7 API. Additionally, there are issues with certain rolling functions in the algos module that perform optimizations by altering the `arr.data` field, which may not be possible with the new API.",
    "The specific warning generated is: '#warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]'. This warning can be avoided by defining the macro `NPY_NO_DEPRECATED_API` with the value `NPY_1_7_API_VERSION` to prevent the use of deprecated API features.",
    "The optimization of rolling functions in the algos module is problematic with the NumPy 1.7 API because these functions manipulate the `arr.data` field directly to skip elements, which is not guaranteed to be assignable in the new API. This could lead to a loss of performance since these optimizations may not be feasible under the new constraints.",
    "The 'numpy.pxd' file in the pandas repository acts as an interface to define NumPy's C API for use in Cython. The limitation mentioned is that the necessary macros for the NumPy 1.7 API are commented out due to incorrect reference count semantics, which indicates an incomplete or unsupported integration in Cython.",
    "The issue description suggests that transitioning to the NumPy 1.7 API could impact performance negatively due to the inability to perform certain optimizations. Specifically, it mentions that manipulating the `arr.data` field directly in rolling functions, which is a performance optimization, may not be possible, leading to increased overhead from creating and destroying ndarrays.",
    "The xref link provided in the issue description points to a pull request in the numexpr repository that deals with similar issues related to the use of deprecated NumPy APIs. It is significant because it offers a reference or precedent for addressing similar problems, potentially providing insights or solutions for handling the transition to the NumPy 1.7 API in the pandas repository."
  ]
}