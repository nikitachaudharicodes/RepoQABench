{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "17978",
  "issue_description": "# Investigate using Hypothesis for some tests\n\nHypothesis recently added support for generating pandas objects: http://hypothesis.readthedocs.io/en/latest/numpy.html#pandas\r\n\r\nThis could be useful for testing some of the higher-level methods like `melt`, `merge`, etc.\r\n\r\nAre there any objections to adding some hypothesis-based tests to our test suite? Is anyone familiar with hypothesis and interested in putting together examples of how we could use it?",
  "issue_comments": [
    {
      "id": 339308124,
      "user": "TomAugspurger",
      "body": "As another example, I could see Hypothesis being *very* useful for something like https://github.com/pandas-dev/pandas/issues/13432 (buggy index set ops for duplicates, non monotonic, categorical)"
    },
    {
      "id": 339376521,
      "user": "shoyer",
      "body": "This sounds like it's definitely worth trying! Is there a guarantee that the test cases it produces will always be the same? (Non-reproducible tests could be pretty painful.)"
    },
    {
      "id": 339380367,
      "user": "TomAugspurger",
      "body": "I believe that once a failing test case is found it's cached (somehow, not\nsure on the details.)\n\nOn Wed, Oct 25, 2017 at 10:49 AM, Stephan Hoyer <notifications@github.com>\nwrote:\n\n> This sounds like it's definitely worth trying! Is there a guarantee that\n> the test cases it produces will always be the same? (Non-reproducible tests\n> could be pretty painful.)\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/17978#issuecomment-339376521>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABQHIvYHXq8I4yFc8irTLznyt0jUidU7ks5sv1h7gaJpZM4QF4u7>\n> .\n>\n"
    },
    {
      "id": 339437146,
      "user": "jbrockmendel",
      "body": "A test database is kept in a .hypothesis directory.  Not sure if that plays nicely with CI."
    },
    {
      "id": 347246713,
      "user": "jbrockmendel",
      "body": "@TomAugspurger did this idea ever go anywhere?  I'm thinking this might be a useful for testing offsets."
    },
    {
      "id": 347252543,
      "user": "TomAugspurger",
      "body": "Don't think so, go for it!"
    },
    {
      "id": 376609978,
      "user": "sushobhit27",
      "body": "Hi, Can I work on this issue. I have recently implemented some unit test cases using hypothesis and therefore have lil bit of exp which I think can be used here.\r\nInfact I have zeroed in some files, where it can be used easily like pandas/tests/tools/test_numeric.py, e.g\r\n\r\n![image](https://user-images.githubusercontent.com/31987769/37984121-0dc4695e-3213-11e8-973b-6834195bffd2.png)\r\n"
    },
    {
      "id": 376633459,
      "user": "TomAugspurger",
      "body": "That seems reasonable. I think the main thing is ensuring this doesn't\nincrease our test times too much.\n\nOn Tue, Mar 27, 2018 at 12:33 PM, sushobhit27 <notifications@github.com>\nwrote:\n\n> Hi, Can I work on this issue. I have recently implemented some unit test\n> cases using hypothesis and therefore have lil bit of exp which I think can\n> be used here.\n> Infact I have zeroed in some files, where it can be used easily like\n> pandas/tests/tools/test_numeric.py, e.g\n>\n> [image: image]\n> <https://user-images.githubusercontent.com/31987769/37984121-0dc4695e-3213-11e8-973b-6834195bffd2.png>\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/17978#issuecomment-376609978>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABQHIh1OvooYADMoaghVQl0p3DGKp1iqks5tinf9gaJpZM4QF4u7>\n> .\n>\n"
    },
    {
      "id": 376746973,
      "user": "sushobhit27",
      "body": "Hypothesis test run time depends on mainly 3 things:\r\nnumber of examples to run for a test case. (is configurable per test case)\r\nrange of the test data like in aforementioned example (bounded by min_size and max_size, therefore this is also configurable)\r\nhow internally hypothesis generates different datasets from the range (called shrinking mechanism AFAIK).\r\nWe can play with first two options to have some influence on test run time, however it will surely gonna take more time than current pytest test cases as here test cases are constant."
    },
    {
      "id": 378167254,
      "user": "sushobhit27",
      "body": "@TomAugspurger, I created a pull request https://github.com/pandas-dev/pandas/pull/20590 for this issue by adding hypothesis based test cases in just one file. However travis CI build failed with error \"ImportError: No module named 'hypothesis'\". Could you please check how it (hypothesis module) can be available, so that tests can be run."
    },
    {
      "id": 412075653,
      "user": "Zac-HD",
      "body": "Hi all - I'm a maintainer of Hypothesis and happy user of Pandas, so hopefully I can help with this :smile: \r\n\r\n* Hypothesis is *not* deterministic by default - though you can derandomize it, we don't recommend this.\r\n* For obvious reasons though, we put a *lot* of effort into ensuring that [test failures are always reproducible](https://hypothesis.readthedocs.io/en/latest/reproducing.html).  Short version: we try to print a call you can copy-paste to get the error plus the seed value if necessary.  If it looks like that might fail, we have a decorator to force everything to the right internal state and print that code with the base64 data it needs to reproduce everything.\r\n* Failing examples are also persisted to disk, so the test will fail immediately if you rerun the test suite before fixing the bug no matter how unlikely it was to find.\r\n* Test runtime is a reasonable concern; there's not usually much overhead but running a test tens or hundreds of times is inherently slower than running it once!  The usual solution is some combination of \"don't use it for everything\", \"turn down the `max_examples` setting in CI\", and \"just deal with it\".  \r\n* All hypothesis tests automatically apply a pytest marker to themselves too, so you can include or exclude them with `-m hypothesis` or `-m \"not hypothesis\"`.  So it would be easy to only run them with the slow tests, for example.\r\n\r\nAnd my question to Pandas maintainers: what's the best way to add this as a dependency?  Hypothesis can be installed from PyPI or from conda-forge, so it's down to whatever would work best in your CI."
    },
    {
      "id": 412077958,
      "user": "TomAugspurger",
      "body": "Thanks @Zac-HD , we have https://github.com/pandas-dev/pandas/pull/20590 that's seemingly ready to go. Need to find the time to take a look again."
    },
    {
      "id": 412078319,
      "user": "TomAugspurger",
      "body": "Actually, @Zac-HD would you mind taking a look in https://github.com/pandas-dev/pandas/pull/20590? One of the issues we're facing is that none of the pandas maintainers have really used hypothesis before. We're trying to define some best practices in that PR, since others will mimic it. You may be the ideal person to set those standards for pandas :)"
    },
    {
      "id": 412090546,
      "user": "Zac-HD",
      "body": "I have three high-level concerns about #20590:\r\n\r\n1. Configuration can be centralised in `conftest.py`, which makes it easier to edit or just override with a command-line argument than using explicit decorators everywhere.\r\n\r\n2. I would ditch the `util/_hypothesis.py` module, and just the Hypothesis api directly.  We've put a lot of thought into making it clear, flexible, and orthogonal - and it has much better performance and error reporting than anything I know of downstream.  If there's anything Pandas-specific that would be really helpful, we would be happy to add it to `hypothesis.extra.pandas` upstream so everyone can use it!\r\n\r\n3. Pandas would probably get much more value out of Hypothesis by using it for integration-style tests, i.e. for more complex invariants where there are more interacting parts.  In Hypothesis' own test suite we still use traditional unit tests (often with `pytest.mark.parametrize`) to check input validation, because it's just not worth the runtime.\r\n   #18761 has some interesting stuff for that, though it too could be much simpler with more idiomatic use of Hypothesis!\r\n\r\nSo rather than tread on toes by suggesting detailed edits to those PRs, I might open a *third* one this weekend incorporating some of both and some of my own notes."
    }
  ],
  "text_context": "# Investigate using Hypothesis for some tests\n\nHypothesis recently added support for generating pandas objects: http://hypothesis.readthedocs.io/en/latest/numpy.html#pandas\r\n\r\nThis could be useful for testing some of the higher-level methods like `melt`, `merge`, etc.\r\n\r\nAre there any objections to adding some hypothesis-based tests to our test suite? Is anyone familiar with hypothesis and interested in putting together examples of how we could use it?\n\nAs another example, I could see Hypothesis being *very* useful for something like https://github.com/pandas-dev/pandas/issues/13432 (buggy index set ops for duplicates, non monotonic, categorical)\n\nThis sounds like it's definitely worth trying! Is there a guarantee that the test cases it produces will always be the same? (Non-reproducible tests could be pretty painful.)\n\nI believe that once a failing test case is found it's cached (somehow, not\nsure on the details.)\n\nOn Wed, Oct 25, 2017 at 10:49 AM, Stephan Hoyer <notifications@github.com>\nwrote:\n\n> This sounds like it's definitely worth trying! Is there a guarantee that\n> the test cases it produces will always be the same? (Non-reproducible tests\n> could be pretty painful.)\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/17978#issuecomment-339376521>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABQHIvYHXq8I4yFc8irTLznyt0jUidU7ks5sv1h7gaJpZM4QF4u7>\n> .\n>\n\n\nA test database is kept in a .hypothesis directory.  Not sure if that plays nicely with CI.\n\n@TomAugspurger did this idea ever go anywhere?  I'm thinking this might be a useful for testing offsets.\n\nDon't think so, go for it!\n\nHi, Can I work on this issue. I have recently implemented some unit test cases using hypothesis and therefore have lil bit of exp which I think can be used here.\r\nInfact I have zeroed in some files, where it can be used easily like pandas/tests/tools/test_numeric.py, e.g\r\n\r\n![image](https://user-images.githubusercontent.com/31987769/37984121-0dc4695e-3213-11e8-973b-6834195bffd2.png)\r\n\n\nThat seems reasonable. I think the main thing is ensuring this doesn't\nincrease our test times too much.\n\nOn Tue, Mar 27, 2018 at 12:33 PM, sushobhit27 <notifications@github.com>\nwrote:\n\n> Hi, Can I work on this issue. I have recently implemented some unit test\n> cases using hypothesis and therefore have lil bit of exp which I think can\n> be used here.\n> Infact I have zeroed in some files, where it can be used easily like\n> pandas/tests/tools/test_numeric.py, e.g\n>\n> [image: image]\n> <https://user-images.githubusercontent.com/31987769/37984121-0dc4695e-3213-11e8-973b-6834195bffd2.png>\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/17978#issuecomment-376609978>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABQHIh1OvooYADMoaghVQl0p3DGKp1iqks5tinf9gaJpZM4QF4u7>\n> .\n>\n\n\nHypothesis test run time depends on mainly 3 things:\r\nnumber of examples to run for a test case. (is configurable per test case)\r\nrange of the test data like in aforementioned example (bounded by min_size and max_size, therefore this is also configurable)\r\nhow internally hypothesis generates different datasets from the range (called shrinking mechanism AFAIK).\r\nWe can play with first two options to have some influence on test run time, however it will surely gonna take more time than current pytest test cases as here test cases are constant.\n\n@TomAugspurger, I created a pull request https://github.com/pandas-dev/pandas/pull/20590 for this issue by adding hypothesis based test cases in just one file. However travis CI build failed with error \"ImportError: No module named 'hypothesis'\". Could you please check how it (hypothesis module) can be available, so that tests can be run.\n\nHi all - I'm a maintainer of Hypothesis and happy user of Pandas, so hopefully I can help with this :smile: \r\n\r\n* Hypothesis is *not* deterministic by default - though you can derandomize it, we don't recommend this.\r\n* For obvious reasons though, we put a *lot* of effort into ensuring that [test failures are always reproducible](https://hypothesis.readthedocs.io/en/latest/reproducing.html).  Short version: we try to print a call you can copy-paste to get the error plus the seed value if necessary.  If it looks like that might fail, we have a decorator to force everything to the right internal state and print that code with the base64 data it needs to reproduce everything.\r\n* Failing examples are also persisted to disk, so the test will fail immediately if you rerun the test suite before fixing the bug no matter how unlikely it was to find.\r\n* Test runtime is a reasonable concern; there's not usually much overhead but running a test tens or hundreds of times is inherently slower than running it once!  The usual solution is some combination of \"don't use it for everything\", \"turn down the `max_examples` setting in CI\", and \"just deal with it\".  \r\n* All hypothesis tests automatically apply a pytest marker to themselves too, so you can include or exclude them with `-m hypothesis` or `-m \"not hypothesis\"`.  So it would be easy to only run them with the slow tests, for example.\r\n\r\nAnd my question to Pandas maintainers: what's the best way to add this as a dependency?  Hypothesis can be installed from PyPI or from conda-forge, so it's down to whatever would work best in your CI.\n\nThanks @Zac-HD , we have https://github.com/pandas-dev/pandas/pull/20590 that's seemingly ready to go. Need to find the time to take a look again.\n\nActually, @Zac-HD would you mind taking a look in https://github.com/pandas-dev/pandas/pull/20590? One of the issues we're facing is that none of the pandas maintainers have really used hypothesis before. We're trying to define some best practices in that PR, since others will mimic it. You may be the ideal person to set those standards for pandas :)\n\nI have three high-level concerns about #20590:\r\n\r\n1. Configuration can be centralised in `conftest.py`, which makes it easier to edit or just override with a command-line argument than using explicit decorators everywhere.\r\n\r\n2. I would ditch the `util/_hypothesis.py` module, and just the Hypothesis api directly.  We've put a lot of thought into making it clear, flexible, and orthogonal - and it has much better performance and error reporting than anything I know of downstream.  If there's anything Pandas-specific that would be really helpful, we would be happy to add it to `hypothesis.extra.pandas` upstream so everyone can use it!\r\n\r\n3. Pandas would probably get much more value out of Hypothesis by using it for integration-style tests, i.e. for more complex invariants where there are more interacting parts.  In Hypothesis' own test suite we still use traditional unit tests (often with `pytest.mark.parametrize`) to check input validation, because it's just not worth the runtime.\r\n   #18761 has some interesting stuff for that, though it too could be much simpler with more idiomatic use of Hypothesis!\r\n\r\nSo rather than tread on toes by suggesting detailed edits to those PRs, I might open a *third* one this weekend incorporating some of both and some of my own notes.",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/20590",
  "code_context": [
    {
      "filename": "pandas/tests/reshape/test_util.py",
      "content": "\nimport numpy as np\nfrom pandas import date_range, Index\nimport pandas.util.testing as tm\nfrom pandas.core.reshape.util import cartesian_product\n\nimport string\nfrom datetime import date\nfrom dateutil import relativedelta\n\nfrom pandas.util import _hypothesis as hp\n\nNO_OF_EXAMPLES_PER_TEST_CASE = 20\n\n\nclass TestCartesianProduct(object):\n\n    @hp.settings(max_examples=20)\n    @hp.given(hp.st.lists(hp.st.text(string.ascii_letters,\n                                     min_size=1, max_size=1),\n                          min_size=1, max_size=3),\n              hp.get_seq((int,), False, 1, 2))\n    def test_simple(self, x, y):\n        result1, result2 = cartesian_product([x, y])\n        expected1 = np.array([item1 for item1 in x for item2 in y])\n        expected2 = np.array([item2 for item1 in x for item2 in y])\n\n        tm.assert_numpy_array_equal(result1, expected1)\n        tm.assert_numpy_array_equal(result2, expected2)\n\n    @hp.settings(max_examples=20)\n    @hp.given(hp.st.dates(min_value=date(1900, 1, 1),\n                          max_value=date(2100, 1, 1)))\n    def test_datetimeindex(self, d):\n        # regression test for GitHub issue #6439\n        # make sure that the ordering on datetimeindex is consistent\n        n = d + relativedelta.relativedelta(days=1)\n        x = date_range(d, periods=2)\n        result1, result2 = [Index(y).day for y in cartesian_product([x, x])]\n        expected1 = Index([d.day, d.day, n.day, n.day])\n        expected2 = Index([d.day, n.day, d.day, n.day])\n\n        tm.assert_index_equal(result1, expected1)\n        tm.assert_index_equal(result2, expected2)\n\n    @hp.settings(max_examples=20)\n    @hp.given(hp.st.lists(hp.st.nothing()),\n              hp.get_seq((int,), False, min_size=1, max_size=10),\n              hp.get_seq((str,), False, min_size=1, max_size=10))\n    def test_empty(self, empty_list, list_of_int, list_of_str):\n        # product of empty factors\n        X = [empty_list, list_of_int, empty_list]\n        Y = [empty_list, empty_list, list_of_str]\n\n        for x, y in zip(X, Y):\n            expected1 = np.array([], dtype=np.asarray(x).dtype)\n            expected2 = np.array([], dtype=np.asarray(y).dtype)\n            result1, result2 = cartesian_product([x, y])\n            tm.assert_numpy_array_equal(result1, expected1)\n            tm.assert_numpy_array_equal(result2, expected2)\n\n        # empty product (empty input):\n        result = cartesian_product(empty_list)\n        expected = []\n        assert result == expected\n\n    @hp.settings(max_examples=20)\n    @hp.given(hp.st.integers(),\n              hp.st.text(string.ascii_letters, min_size=1),\n              hp.get_seq((int, str), True, min_size=1),\n              hp.st.builds(lambda *x: list(x), hp.st.integers(),\n                           hp.st.text(string.ascii_letters, min_size=1),\n                           hp.st.lists(hp.st.integers(), min_size=1)))\n    def test_invalid_input(self, number, text, seq, mixed_seq):\n\n        invalid_inputs = [number,\n                          text,\n                          seq,\n                          mixed_seq]\n\n        msg = \"Input must be a list-like of list-likes\"\n        for X in invalid_inputs:\n            tm.assert_raises_regex(TypeError, msg, cartesian_product, X=X)\n"
    },
    {
      "filename": "pandas/util/_hypothesis.py",
      "content": "\"\"\"\nThis module houses utility functions to generate hypothesis strategies which\n can be used to generate random input test data for various test cases.\nIt is for internal use by different test case files like pandas/test/test*.py\n files only and should not be used beyond this purpose.\nFor more information on hypothesis, check\n(http://hypothesis.readthedocs.io/en/latest/).\n\"\"\"\nimport string\nfrom hypothesis import (given,  # noqa:F401\n                        settings,   # noqa:F401\n                        assume, # noqa:F401\n                        strategies as st,\n                        )\n\n\ndef get_elements(elem_type):\n    \"\"\"\n    Helper function to return hypothesis strategy whose elements depends on\n    the input data-type.\n    Currently only four types are supported namely, bool, int, float and str.\n\n    Parameters\n    ----------\n    elem_type: type\n        type of the elements for the strategy.\n\n    Returns\n    -------\n    hypothesis strategy.\n\n    Examples\n    --------\n    >>> strat = get_elements(str)\n    >>> strat.example()\n    'KWAo'\n\n    >>> strat.example()\n    'OfAlBH'\n\n    >>> strat = get_elements(int)\n    >>> strat.example()\n    31911\n\n    >>> strat.example()\n    25288\n\n    >>> strat = get_elements(float)\n    >>> strat.example()\n    nan\n\n    >>> strat.example()\n    inf\n\n    >>> strat.example()\n    -2.2250738585072014e-308\n\n    >>> strat.example()\n    0.5\n\n    >>> strat.example()\n    1.7976931348623157e+308\n\n    >>> strat = get_elements(bool)\n    >>> strat.example()\n    True\n\n    >>> strat.example()\n    True\n\n    >>> strat.example()\n    False\n    \"\"\"\n    strategy = st.nothing()\n    if elem_type == bool:\n        strategy = st.booleans()\n    elif elem_type == int:\n        strategy = st.integers()\n    elif elem_type == float:\n        strategy = st.floats()\n    elif elem_type == str:\n        strategy = st.text(string.ascii_letters, max_size=10)\n    return strategy\n\n\n@st.composite\ndef get_seq(draw, types, mixed=False, min_size=None, max_size=None,\n            transform_func=None):\n    \"\"\"\n    Helper function to generate strategy for creating lists.\n    What constitute in the generated list is driven by the different\n    parameters.\n\n    Parameters\n    ----------\n    types: iterable sequence like tuple or list\n        types which can be in the generated list.\n    mixed: bool\n        if True, list will contains elements from all types listed in arg,\n        otherwise it will have elements only from types[0].\n    min_size: int\n        minimum size of the list.\n    max_size: int\n        maximum size of the list.\n    transform_func: callable\n        a callable which can be applied to whole list after it has been\n         generated. It can think of as providing functionality of filter\n         and map function.\n\n    Returns\n    -------\n    hypothesis lists strategy.\n\n    Examples\n    --------\n    >>> seq_strategy = get_seq((int, str, bool), mixed=True, min_size=1,\n...    max_size=5)\n\n    >>> seq_strategy.example()\n    ['lkYMSn', -2501, 35, 'J']\n\n    >>> seq_strategy.example()\n    [True]\n\n    >>> seq_strategy.example()\n    ['dRWgQYrBrW', True, False, 'gmsujJVDBM', 'Z']\n\n    >>> seq_strategy = get_seq((int, bool),\n...                             mixed=False,\n...                             min_size=1,\n...                             max_size=5,\n...                             transform_func=lambda seq:\n...                             [str(x) for x in seq])\n\n    >>> seq_strategy.example()\n    ['9552', '124', '-24024']\n\n    >>> seq_strategy.example()\n    ['-1892']\n\n    >>> seq_strategy.example()\n    ['22', '66', '14785', '-26312', '32']\n    \"\"\"\n    if min_size is None:\n        min_size = draw(st.integers(min_value=0, max_value=100))\n\n    if max_size is None:\n        max_size = draw(st.integers(min_value=min_size, max_value=100))\n\n    assert min_size <= max_size, \\\n        'max_size must be greater than equal to min_size'\n\n    elem_strategies = []\n    for elem_type in types:\n        elem_strategies.append(get_elements(elem_type))\n        if not mixed:\n            break\n    if transform_func:\n        strategy = draw(st.lists(st.one_of(elem_strategies),\n                                 min_size=min_size,\n                                 max_size=max_size).map(transform_func))\n    else:\n        strategy = draw(st.lists(st.one_of(elem_strategies),\n                                 min_size=min_size,\n                                 max_size=max_size))\n    return strategy\n"
    }
  ],
  "questions": [],
  "golden_answers": [],
  "questions_generated": [
    "What is the purpose of integrating Hypothesis into the pandas test suite?",
    "How does Hypothesis ensure reproducibility of failing test cases in the pandas test suite?",
    "In the provided code context, how does the 'test_simple' method utilize Hypothesis for testing?",
    "What concerns were raised regarding the use of Hypothesis in the pandas test suite, and how might they be addressed?",
    "What is the significance of the 'test_datetimeindex' method in the context of the pandas issue #6439?"
  ],
  "golden_answers_generated": [
    "The purpose of integrating Hypothesis into the pandas test suite is to leverage its support for generating pandas objects, which can be useful for testing higher-level methods such as 'melt' and 'merge'. Hypothesis can help discover edge cases and potential bugs by generating a wide range of test inputs, including those that might not have been considered manually.",
    "Hypothesis ensures reproducibility of failing test cases by caching them. When a failing test case is found, it is stored in a test database located in a '.hypothesis' directory. This allows for consistent re-running of the same failing case to aid in debugging and resolving issues.",
    "In the 'test_simple' method, Hypothesis is used to generate random lists of single-character strings and sequences of integers. The method uses these generated inputs to test the 'cartesian_product' function from the pandas library. It ensures that the outputs match the expected results by comparing them with manually constructed arrays using assertions.",
    "One concern raised was the potential increase in test times due to the use of Hypothesis. This can be addressed by setting a limit on the number of examples generated per test case, as shown in the code context where 'max_examples=20' is specified. This helps control the test execution time while still benefiting from Hypothesis's random input generation.",
    "The 'test_datetimeindex' method is a regression test for the GitHub issue #6439, which involves ensuring consistent ordering of 'DatetimeIndex'. Hypothesis generates random dates within a specified range, and the test checks that the 'cartesian_product' function maintains consistent ordering when applied to date ranges. This helps verify that the issue is resolved and that the function behaves as expected with datetime inputs."
  ]
}