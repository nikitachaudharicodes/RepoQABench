{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "27800",
  "issue_description": "# BUG: aggregation on ordered categorical column drops grouping index or crashes, depending on context\n\n### Code Sample\r\nBuild the model data frame:\r\n```python\r\ndf = pd.DataFrame({\r\n    'nr': [1,2,3,4,5,6,7,8], \r\n    'cat_ord': list('aabbccdd'), \r\n    'cat':list('aaaabbbb')\r\n})\r\ndf = df.astype({'cat': 'category', 'cat_ord': 'category'})\r\ndf['cat_ord'] = df['cat_ord'].cat.as_ordered()\r\n```\r\nWhen grouping, single aggregations on a _numeric column_ work:\r\n```python\r\ndf.groupby('cat').agg({'nr': 'min'})\r\n```\r\n```\r\n\tnr\r\ncat\t\r\na\t1\r\nb\t5\r\n```\r\n**Single aggregations** on an _ordered categorical column_ work, but drop the grouping index:\r\n```python\r\ndf.groupby('cat').agg({'cat_ord': 'min'})\r\n```\r\n```\r\n   cat_ord\r\n0        a\r\n1        c\r\n```\r\n**Combined single aggregations** on a _numeric_ and an _ordered categorical_ column work:\r\n```python\r\ndf.groupby('cat').agg({'nr': 'min', 'cat_ord': 'min'})\r\n```\r\n```\r\n       nr cat_ord\r\ncat\t\t\r\na\t1\ta\r\nb\t5\tc\r\n```\r\n**Multiple aggregations** on an _ordered categorical_ column work, but drop the grouping index:\r\n```python\r\ndf.groupby('cat').agg({'cat_ord': ['min', 'max']})\r\n```\r\n```\r\n          cat_ord\r\n      min     max\r\n0\ta\tb\r\n1\tc\td\r\n```\r\n**Combined aggregations** on a _numeric_ (single) and an _ordered categorical_ column (multiple) fail with a **TypeError**:\r\n```python\r\ndf.groupby('cat').agg({'nr': 'min', 'cat_ord': ['min', 'max']})\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-148-b446de510106> in <module>\r\n----> 1 df.groupby('cat').agg({'nr': 'min', 'cat_ord': ['min', 'max']})\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py in aggregate(self, arg, *args, **kwargs)\r\n   1453     @Appender(_shared_docs[\"aggregate\"])\r\n   1454     def aggregate(self, arg=None, *args, **kwargs):\r\n-> 1455         return super().aggregate(arg, *args, **kwargs)\r\n   1456 \r\n   1457     agg = aggregate\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py in aggregate(self, func, *args, **kwargs)\r\n    227         func = _maybe_mangle_lambdas(func)\r\n    228 \r\n--> 229         result, how = self._aggregate(func, _level=_level, *args, **kwargs)\r\n    230         if how is None:\r\n    231             return result\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py in _aggregate(self, arg, *args, **kwargs)\r\n    528                 # return a MI DataFrame\r\n    529 \r\n--> 530                 return concat([result[k] for k in keys], keys=keys, axis=1), True\r\n    531 \r\n    532             elif isinstance(self, ABCSeries) and is_any_series():\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\r\n    256     )\r\n    257 \r\n--> 258     return op.get_result()\r\n    259 \r\n    260 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py in get_result(self)\r\n    466                     obj_labels = mgr.axes[ax]\r\n    467                     if not new_labels.equals(obj_labels):\r\n--> 468                         indexers[ax] = obj_labels.reindex(new_labels)[1]\r\n    469 \r\n    470                 mgrs_indexers.append((obj._data, indexers))\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\category.py in reindex(self, target, method, level, limit, tolerance)\r\n    616                 # coerce to a regular index here!\r\n    617                 result = Index(np.array(self), name=self.name)\r\n--> 618                 new_target, indexer, _ = result._reindex_non_unique(np.array(target))\r\n    619             else:\r\n    620 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py in _reindex_non_unique(self, target)\r\n   3434 \r\n   3435         target = ensure_index(target)\r\n-> 3436         indexer, missing = self.get_indexer_non_unique(target)\r\n   3437         check = indexer != -1\r\n   3438         new_labels = self.take(indexer[check])\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py in get_indexer_non_unique(self, target)\r\n   4792             tgt_values = target._ndarray_values\r\n   4793 \r\n-> 4794         indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\r\n   4795         return ensure_platform_int(indexer), missing\r\n   4796 \r\n\r\npandas\\_libs\\index.pyx in pandas._libs.index.IndexEngine.get_indexer_non_unique()\r\n\r\nTypeError: '<' not supported between instances of 'str' and 'int'\r\n```\r\n**Combined aggregations** on a _numeric_ (multiple) and an _ordered categorical_ column (single) also fail with the same **TypeError**:\r\n```python\r\ndf.groupby('cat').agg({'nr': ['min', 'max'], 'cat_ord': 'min'})\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-164-b1d70184bd81> in <module>\r\n----> 1 df.groupby('cat').agg({'nr': ['min', 'max'], 'cat_ord': 'min'})\r\n\r\n...\r\n\r\npandas\\_libs\\index.pyx in pandas._libs.index.IndexEngine.get_indexer_non_unique()\r\n\r\nTypeError: '<' not supported between instances of 'str' and 'int'\r\n```\r\n\r\n### Problem description\r\n\r\nAggregations on ordered categoricals drop the grouping index, or crash, as shown above.\r\n\r\nThis makes it hard to calculate combined aggregations over big data sets correctly and efficiently.\r\n\r\n### Expected Output\r\n\r\nAggregations on ordered categoricals should work as on non-categorical columns.\r\n\r\n### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.3.final.0\r\npython-bits      : 64\r\nOS               : Windows\r\nOS-release       : 10\r\nmachine          : AMD64\r\nprocessor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : None\r\nLOCALE           : None.None\r\n\r\npandas           : 0.25.0\r\nnumpy            : 1.16.4\r\npytz             : 2019.1\r\ndateutil         : 2.8.0\r\npip              : 19.1.1\r\nsetuptools       : 41.0.1\r\nCython           : 0.29.12\r\npytest           : 5.0.1\r\nhypothesis       : None\r\nsphinx           : 2.1.2\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : 1.1.8\r\nlxml.etree       : 4.3.4\r\nhtml5lib         : 1.0.1\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10.1\r\nIPython          : 7.7.0\r\npandas_datareader: None\r\nbs4              : 4.7.1\r\nbottleneck       : 1.2.1\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.3.4\r\nmatplotlib       : 3.1.1\r\nnumexpr          : 2.6.9\r\nodfpy            : None\r\nopenpyxl         : 2.6.2\r\npandas_gbq       : None\r\npyarrow          : 0.11.1\r\npytables         : None\r\ns3fs             : None\r\nscipy            : 1.3.0\r\nsqlalchemy       : 1.3.5\r\ntables           : 3.5.2\r\nxarray           : None\r\nxlrd             : 1.2.0\r\nxlwt             : 1.3.0\r\nxlsxwriter       : 1.1.8\r\n</details>\r\n",
  "issue_comments": [
    {
      "id": 519275408,
      "user": "TomAugspurger",
      "body": "Thanks for the report. Are you interested in debugging to see where things go wrong?"
    },
    {
      "id": 520638917,
      "user": "bongolegend",
      "body": "@kpflugshaupt  I ran your code samples and did not get any errors, on pandas 0.24.2. Some later update must have broken this. I can look into this.\r\n\r\n@TomAugspurger is there an easy way to see what changes between 0.24.2 and 0.25.0 would have affected this functionality? The whatsnew docs (could be a lot of combing)?"
    },
    {
      "id": 520906397,
      "user": "kpflugshaupt",
      "body": "Hi @ncernek ,\r\n\r\nI also cannot recall seeing this ever before 0.25.0, so the upgrade has probably introduced it.\r\n\r\nAs to how to tackle this: I would (given time) debug the call, and either catch where it's going wrong (compare to working call), or list all the visited files and cross-reference the whatsnew file.\r\n\r\nThanks for looking into this! I may also have a go, but not in the next days -- too much work.\r\n\r\nCheers\r\nKaspar"
    },
    {
      "id": 520910654,
      "user": "TomAugspurger",
      "body": "You can also use git bisect to find the faulty commit. Might be able to look through PRs in the git log that mention groupby or categorical to narrow down the bisect range."
    },
    {
      "id": 520962286,
      "user": "bongolegend",
      "body": "I went the `git bisect` route. I wrote a single failing test for this condition \r\n```\r\ndf.groupby(\"cat\").agg({\"nr\": \"min\", \"cat_ord\": [\"min\", \"max\"]})\r\n```\r\nResult of `git bisect`\r\n```\r\nd0292fe1e12f1d460e52df4da4250bef32324579 is the first bad commit\r\ncommit d0292fe1e12f1d460e52df4da4250bef32324579\r\nAuthor: Jeff Reback <jeff@reback.net>\r\nDate:   Thu Jun 27 18:13:27 2019 -0500\r\n\r\n    BUG: preserve categorical & sparse types when grouping / pivot (#27071)\r\n```\r\n\r\n[link to that PR](https://github.com/pandas-dev/pandas/pull/27071/files) \r\n\r\nLooks like a lot of thinking went into that PR, maybe those of you (e.g. @jreback ) who worked on it want to look into this bug further?  "
    },
    {
      "id": 521407718,
      "user": "TomAugspurger",
      "body": "Thanks @ncernek. At least for this specific one, it's from how concat handles a mix of categorical & non-categorical indexes\r\n\r\n```pytb\r\nIn [2]: a = pd.DataFrame({\"A\": [1, 2]})\r\n\r\nIn [3]: b = pd.DataFrame({\"B\": [3, 4]}, index=pd.CategoricalIndex(['a', 'b']))\r\n\r\nIn [4]: pd.concat([a, b], axis=1)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-4-a98a78ec2995> in <module>\r\n----> 1 pd.concat([a, b], axis=1)\r\n\r\n~/sandbox/pandas/pandas/core/reshape/concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\r\n    256     )\r\n    257\r\n--> 258     return op.get_result()\r\n    259\r\n    260\r\n\r\n~/sandbox/pandas/pandas/core/reshape/concat.py in get_result(self)\r\n    466                     obj_labels = mgr.axes[ax]\r\n    467                     if not new_labels.equals(obj_labels):\r\n--> 468                         indexers[ax] = obj_labels.reindex(new_labels)[1]\r\n    469\r\n    470                 mgrs_indexers.append((obj._data, indexers))\r\n\r\n~/sandbox/pandas/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance)\r\n    615                 # coerce to a regular index here!\r\n    616                 result = Index(np.array(self), name=self.name)\r\n--> 617                 new_target, indexer, _ = result._reindex_non_unique(np.array(target))\r\n    618             else:\r\n    619\r\n\r\n~/sandbox/pandas/pandas/core/indexes/base.py in _reindex_non_unique(self, target)\r\n   3388\r\n   3389         target = ensure_index(target)\r\n-> 3390         indexer, missing = self.get_indexer_non_unique(target)\r\n   3391         check = indexer != -1\r\n   3392         new_labels = self.take(indexer[check])\r\n\r\n~/sandbox/pandas/pandas/core/indexes/base.py in get_indexer_non_unique(self, target)\r\n   4751             tgt_values = target._ndarray_values\r\n   4752\r\n-> 4753         indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\r\n   4754         return ensure_platform_int(indexer), missing\r\n   4755\r\n\r\n~/sandbox/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_indexer_non_unique()\r\n    305             # increasing, then use binary search for each starget\r\n    306             for starget in stargets:\r\n--> 307                 start = values.searchsorted(starget, side='left')\r\n    308                 end = values.searchsorted(starget, side='right')\r\n    309                 if start != end:\r\n\r\nTypeError: '<' not supported between instances of 'str' and 'int'\r\n\r\nIn [5]: a = pd.DataFrame({\"A\": [1, 2]})\r\n\r\nIn [6]: b = pd.DataFrame({\"B\": [3, 4]}, index=pd.CategoricalIndex(['a', 'b']))\r\n\r\nIn [7]: pd.concat([a, b], axis=1)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-a98a78ec2995> in <module>\r\n----> 1 pd.concat([a, b], axis=1)\r\n\r\n~/sandbox/pandas/pandas/core/reshape/concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\r\n    256     )\r\n    257\r\n--> 258     return op.get_result()\r\n    259\r\n    260\r\n\r\n~/sandbox/pandas/pandas/core/reshape/concat.py in get_result(self)\r\n    466                     obj_labels = mgr.axes[ax]\r\n    467                     if not new_labels.equals(obj_labels):\r\n--> 468                         indexers[ax] = obj_labels.reindex(new_labels)[1]\r\n    469\r\n    470                 mgrs_indexers.append((obj._data, indexers))\r\n\r\n~/sandbox/pandas/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance)\r\n    615                 # coerce to a regular index here!\r\n    616                 result = Index(np.array(self), name=self.name)\r\n--> 617                 new_target, indexer, _ = result._reindex_non_unique(np.array(target))\r\n    618             else:\r\n    619\r\n\r\n~/sandbox/pandas/pandas/core/indexes/base.py in _reindex_non_unique(self, target)\r\n   3388\r\n   3389         target = ensure_index(target)\r\n-> 3390         indexer, missing = self.get_indexer_non_unique(target)\r\n   3391         check = indexer != -1\r\n   3392         new_labels = self.take(indexer[check])\r\n\r\n~/sandbox/pandas/pandas/core/indexes/base.py in get_indexer_non_unique(self, target)\r\n   4751             tgt_values = target._ndarray_values\r\n   4752\r\n-> 4753         indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\r\n   4754         return ensure_platform_int(indexer), missing\r\n   4755\r\n\r\n~/sandbox/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_indexer_non_unique()\r\n    305             # increasing, then use binary search for each starget\r\n    306             for starget in stargets:\r\n--> 307                 start = values.searchsorted(starget, side='left')\r\n    308                 end = values.searchsorted(starget, side='right')\r\n    309                 if start != end:\r\n\r\n```\r\n\r\nI think that should coerce both to object-dtype index before going too far."
    },
    {
      "id": 560056436,
      "user": "BaruchYoussin",
      "body": "Here is another bug which is probably another face of this bug:\r\n\r\nAggregating a categorical column in a groupby yields a Categorical object rather than a Series.\r\n\r\nHere is the code:\r\n\r\n```\r\ndf = pd.DataFrame({\r\n    'nr': [1,2,3,4,5,6,7,8], \r\n    'cat_ord': list('aabbccdd'), \r\n    'cat':list('aaaabbbb')\r\n})\r\ndf = df.astype({'cat': 'category', 'cat_ord': 'category'})\r\ndf['cat_ord'] = df['cat_ord'].cat.as_ordered()\r\ndf\r\n```\r\n\r\n>    nr cat_ord cat\r\n> 0   1       a   a\r\n> 1   2       a   a\r\n> 2   3       b   a\r\n> 3   4       b   a\r\n> \r\n\r\n`df.groupby('cat_ord')['cat'].first()`\r\n\r\n> [a, a, b, b]\r\n> Categories (2, object): [a, b]\r\n\r\n`type(df.groupby('cat_ord')['cat'].first())`\r\n\r\n> <class 'pandas.core.arrays.categorical.Categorical'>\r\n"
    },
    {
      "id": 560071583,
      "user": "BaruchYoussin",
      "body": "I suspect that this bug was introduced by the following change in 0.25.\r\n\r\nAggregation in groupby in Pandas 0.24 dropped the categorical dtype, replacing it by object which was a string.\r\n\r\nVersion 0.25 introduced additional code to restore the original dtype.  It seems to me that it is this code which is to blame for this bug.\r\n\r\nI also forgot to mention that the aggregation of categorical columns take huge amount of time on large datasets, much much more than for other column types.  For this reason I bypassed it entirely in my code, aggregating string columns instead, despite the memory cost."
    },
    {
      "id": 650836754,
      "user": "mroeschke",
      "body": "Looks like the examples work on master. Could use tests\r\n\r\n```\r\nIn [125]: df = pd.DataFrame({\r\n     ...:     'nr': [1,2,3,4,5,6,7,8],\r\n     ...:     'cat_ord': list('aabbccdd'),\r\n     ...:     'cat':list('aaaabbbb')\r\n     ...: })\r\n     ...: df = df.astype({'cat': 'category', 'cat_ord': 'category'})\r\n     ...: df['cat_ord'] = df['cat_ord'].cat.as_ordered()\r\n\r\nIn [126]: df.groupby('cat').agg({'nr': 'min', 'cat_ord': ['min', 'max']})\r\n     ...:\r\nOut[126]:\r\n     nr cat_ord\r\n    min     min max\r\ncat\r\na     1       a   b\r\nb     5       c   d\r\n\r\nIn [127]: df.groupby('cat').agg({'nr': ['min', 'max'], 'cat_ord': 'min'})\r\n     ...:\r\nOut[127]:\r\n     nr     cat_ord\r\n    min max     min\r\ncat\r\na     1   4       a\r\nb     5   8       c\r\n```"
    },
    {
      "id": 652258022,
      "user": "kpflugshaupt",
      "body": "**This seems solved on the current productive release.**\r\n\r\nIn my testing, all cases from the initial report came out correctly, with no regressions.\r\n\r\n### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.8.3.final.0\r\npython-bits      : 64\r\nOS               : Windows\r\nOS-release       : 10\r\nmachine          : AMD64\r\nprocessor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : None\r\nLOCALE           : German_Switzerland.1252\r\npandas           : 1.0.5\r\nnumpy            : 1.19.0\r\npytz             : 2020.1\r\ndateutil         : 2.8.1\r\npip              : 20.1.1\r\nsetuptools       : 46.0.0\r\nCython           : None\r\npytest           : None\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : 1.2.9\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.11.2\r\nIPython          : 7.16.1\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : None\r\nmatplotlib       : None\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : 0.17.1\r\npytables         : None\r\npytest           : None\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : None\r\nsqlalchemy       : None\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : 1.2.9\r\nnumba            : None\r\n</details>\r\n\r\nWell done, whoever fixed this!\r\n\r\n@mroeschke: Should I close this, or would you wait for more positive test results? (I don't have an installation of the development branch, so cannot test on that)"
    },
    {
      "id": 652280973,
      "user": "BaruchYoussin",
      "body": "Works fine on 1.0.4."
    },
    {
      "id": 652532060,
      "user": "mroeschke",
      "body": "@kpflugshaupt we'll want to add a regression test to ensure that this issue doesn't occur again. Are you interested in submitted a PR with that test?"
    },
    {
      "id": 654668556,
      "user": "kpflugshaupt",
      "body": "> \r\n> \r\n> @kpflugshaupt we'll want to add a regression test to ensure that this issue doesn't occur again. Are you interested in submitted a PR with that test?\r\n\r\n@mroeschke Not sure I'll have the time -- work & family tend to get in the way. Possibly in the week starting July 20th. I'll try!"
    },
    {
      "id": 668268479,
      "user": "mathurk1",
      "body": "take"
    }
  ],
  "text_context": "# BUG: aggregation on ordered categorical column drops grouping index or crashes, depending on context\n\n### Code Sample\r\nBuild the model data frame:\r\n```python\r\ndf = pd.DataFrame({\r\n    'nr': [1,2,3,4,5,6,7,8], \r\n    'cat_ord': list('aabbccdd'), \r\n    'cat':list('aaaabbbb')\r\n})\r\ndf = df.astype({'cat': 'category', 'cat_ord': 'category'})\r\ndf['cat_ord'] = df['cat_ord'].cat.as_ordered()\r\n```\r\nWhen grouping, single aggregations on a _numeric column_ work:\r\n```python\r\ndf.groupby('cat').agg({'nr': 'min'})\r\n```\r\n```\r\n\tnr\r\ncat\t\r\na\t1\r\nb\t5\r\n```\r\n**Single aggregations** on an _ordered categorical column_ work, but drop the grouping index:\r\n```python\r\ndf.groupby('cat').agg({'cat_ord': 'min'})\r\n```\r\n```\r\n   cat_ord\r\n0        a\r\n1        c\r\n```\r\n**Combined single aggregations** on a _numeric_ and an _ordered categorical_ column work:\r\n```python\r\ndf.groupby('cat').agg({'nr': 'min', 'cat_ord': 'min'})\r\n```\r\n```\r\n       nr cat_ord\r\ncat\t\t\r\na\t1\ta\r\nb\t5\tc\r\n```\r\n**Multiple aggregations** on an _ordered categorical_ column work, but drop the grouping index:\r\n```python\r\ndf.groupby('cat').agg({'cat_ord': ['min', 'max']})\r\n```\r\n```\r\n          cat_ord\r\n      min     max\r\n0\ta\tb\r\n1\tc\td\r\n```\r\n**Combined aggregations** on a _numeric_ (single) and an _ordered categorical_ column (multiple) fail with a **TypeError**:\r\n```python\r\ndf.groupby('cat').agg({'nr': 'min', 'cat_ord': ['min', 'max']})\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-148-b446de510106> in <module>\r\n----> 1 df.groupby('cat').agg({'nr': 'min', 'cat_ord': ['min', 'max']})\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py in aggregate(self, arg, *args, **kwargs)\r\n   1453     @Appender(_shared_docs[\"aggregate\"])\r\n   1454     def aggregate(self, arg=None, *args, **kwargs):\r\n-> 1455         return super().aggregate(arg, *args, **kwargs)\r\n   1456 \r\n   1457     agg = aggregate\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py in aggregate(self, func, *args, **kwargs)\r\n    227         func = _maybe_mangle_lambdas(func)\r\n    228 \r\n--> 229         result, how = self._aggregate(func, _level=_level, *args, **kwargs)\r\n    230         if how is None:\r\n    231             return result\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py in _aggregate(self, arg, *args, **kwargs)\r\n    528                 # return a MI DataFrame\r\n    529 \r\n--> 530                 return concat([result[k] for k in keys], keys=keys, axis=1), True\r\n    531 \r\n    532             elif isinstance(self, ABCSeries) and is_any_series():\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\r\n    256     )\r\n    257 \r\n--> 258     return op.get_result()\r\n    259 \r\n    260 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py in get_result(self)\r\n    466                     obj_labels = mgr.axes[ax]\r\n    467                     if not new_labels.equals(obj_labels):\r\n--> 468                         indexers[ax] = obj_labels.reindex(new_labels)[1]\r\n    469 \r\n    470                 mgrs_indexers.append((obj._data, indexers))\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\category.py in reindex(self, target, method, level, limit, tolerance)\r\n    616                 # coerce to a regular index here!\r\n    617                 result = Index(np.array(self), name=self.name)\r\n--> 618                 new_target, indexer, _ = result._reindex_non_unique(np.array(target))\r\n    619             else:\r\n    620 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py in _reindex_non_unique(self, target)\r\n   3434 \r\n   3435         target = ensure_index(target)\r\n-> 3436         indexer, missing = self.get_indexer_non_unique(target)\r\n   3437         check = indexer != -1\r\n   3438         new_labels = self.take(indexer[check])\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py in get_indexer_non_unique(self, target)\r\n   4792             tgt_values = target._ndarray_values\r\n   4793 \r\n-> 4794         indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\r\n   4795         return ensure_platform_int(indexer), missing\r\n   4796 \r\n\r\npandas\\_libs\\index.pyx in pandas._libs.index.IndexEngine.get_indexer_non_unique()\r\n\r\nTypeError: '<' not supported between instances of 'str' and 'int'\r\n```\r\n**Combined aggregations** on a _numeric_ (multiple) and an _ordered categorical_ column (single) also fail with the same **TypeError**:\r\n```python\r\ndf.groupby('cat').agg({'nr': ['min', 'max'], 'cat_ord': 'min'})\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-164-b1d70184bd81> in <module>\r\n----> 1 df.groupby('cat').agg({'nr': ['min', 'max'], 'cat_ord': 'min'})\r\n\r\n...\r\n\r\npandas\\_libs\\index.pyx in pandas._libs.index.IndexEngine.get_indexer_non_unique()\r\n\r\nTypeError: '<' not supported between instances of 'str' and 'int'\r\n```\r\n\r\n### Problem description\r\n\r\nAggregations on ordered categoricals drop the grouping index, or crash, as shown above.\r\n\r\nThis makes it hard to calculate combined aggregations over big data sets correctly and efficiently.\r\n\r\n### Expected Output\r\n\r\nAggregations on ordered categoricals should work as on non-categorical columns.\r\n\r\n### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.3.final.0\r\npython-bits      : 64\r\nOS               : Windows\r\nOS-release       : 10\r\nmachine          : AMD64\r\nprocessor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : None\r\nLOCALE           : None.None\r\n\r\npandas           : 0.25.0\r\nnumpy            : 1.16.4\r\npytz             : 2019.1\r\ndateutil         : 2.8.0\r\npip              : 19.1.1\r\nsetuptools       : 41.0.1\r\nCython           : 0.29.12\r\npytest           : 5.0.1\r\nhypothesis       : None\r\nsphinx           : 2.1.2\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : 1.1.8\r\nlxml.etree       : 4.3.4\r\nhtml5lib         : 1.0.1\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10.1\r\nIPython          : 7.7.0\r\npandas_datareader: None\r\nbs4              : 4.7.1\r\nbottleneck       : 1.2.1\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : 4.3.4\r\nmatplotlib       : 3.1.1\r\nnumexpr          : 2.6.9\r\nodfpy            : None\r\nopenpyxl         : 2.6.2\r\npandas_gbq       : None\r\npyarrow          : 0.11.1\r\npytables         : None\r\ns3fs             : None\r\nscipy            : 1.3.0\r\nsqlalchemy       : 1.3.5\r\ntables           : 3.5.2\r\nxarray           : None\r\nxlrd             : 1.2.0\r\nxlwt             : 1.3.0\r\nxlsxwriter       : 1.1.8\r\n</details>\r\n\n\nThanks for the report. Are you interested in debugging to see where things go wrong?\n\n@kpflugshaupt  I ran your code samples and did not get any errors, on pandas 0.24.2. Some later update must have broken this. I can look into this.\r\n\r\n@TomAugspurger is there an easy way to see what changes between 0.24.2 and 0.25.0 would have affected this functionality? The whatsnew docs (could be a lot of combing)?\n\nHi @ncernek ,\r\n\r\nI also cannot recall seeing this ever before 0.25.0, so the upgrade has probably introduced it.\r\n\r\nAs to how to tackle this: I would (given time) debug the call, and either catch where it's going wrong (compare to working call), or list all the visited files and cross-reference the whatsnew file.\r\n\r\nThanks for looking into this! I may also have a go, but not in the next days -- too much work.\r\n\r\nCheers\r\nKaspar\n\nYou can also use git bisect to find the faulty commit. Might be able to look through PRs in the git log that mention groupby or categorical to narrow down the bisect range.\n\nI went the `git bisect` route. I wrote a single failing test for this condition \r\n```\r\ndf.groupby(\"cat\").agg({\"nr\": \"min\", \"cat_ord\": [\"min\", \"max\"]})\r\n```\r\nResult of `git bisect`\r\n```\r\nd0292fe1e12f1d460e52df4da4250bef32324579 is the first bad commit\r\ncommit d0292fe1e12f1d460e52df4da4250bef32324579\r\nAuthor: Jeff Reback <jeff@reback.net>\r\nDate:   Thu Jun 27 18:13:27 2019 -0500\r\n\r\n    BUG: preserve categorical & sparse types when grouping / pivot (#27071)\r\n```\r\n\r\n[link to that PR](https://github.com/pandas-dev/pandas/pull/27071/files) \r\n\r\nLooks like a lot of thinking went into that PR, maybe those of you (e.g. @jreback ) who worked on it want to look into this bug further?  \n\nThanks @ncernek. At least for this specific one, it's from how concat handles a mix of categorical & non-categorical indexes\r\n\r\n```pytb\r\nIn [2]: a = pd.DataFrame({\"A\": [1, 2]})\r\n\r\nIn [3]: b = pd.DataFrame({\"B\": [3, 4]}, index=pd.CategoricalIndex(['a', 'b']))\r\n\r\nIn [4]: pd.concat([a, b], axis=1)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-4-a98a78ec2995> in <module>\r\n----> 1 pd.concat([a, b], axis=1)\r\n\r\n~/sandbox/pandas/pandas/core/reshape/concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\r\n    256     )\r\n    257\r\n--> 258     return op.get_result()\r\n    259\r\n    260\r\n\r\n~/sandbox/pandas/pandas/core/reshape/concat.py in get_result(self)\r\n    466                     obj_labels = mgr.axes[ax]\r\n    467                     if not new_labels.equals(obj_labels):\r\n--> 468                         indexers[ax] = obj_labels.reindex(new_labels)[1]\r\n    469\r\n    470                 mgrs_indexers.append((obj._data, indexers))\r\n\r\n~/sandbox/pandas/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance)\r\n    615                 # coerce to a regular index here!\r\n    616                 result = Index(np.array(self), name=self.name)\r\n--> 617                 new_target, indexer, _ = result._reindex_non_unique(np.array(target))\r\n    618             else:\r\n    619\r\n\r\n~/sandbox/pandas/pandas/core/indexes/base.py in _reindex_non_unique(self, target)\r\n   3388\r\n   3389         target = ensure_index(target)\r\n-> 3390         indexer, missing = self.get_indexer_non_unique(target)\r\n   3391         check = indexer != -1\r\n   3392         new_labels = self.take(indexer[check])\r\n\r\n~/sandbox/pandas/pandas/core/indexes/base.py in get_indexer_non_unique(self, target)\r\n   4751             tgt_values = target._ndarray_values\r\n   4752\r\n-> 4753         indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\r\n   4754         return ensure_platform_int(indexer), missing\r\n   4755\r\n\r\n~/sandbox/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_indexer_non_unique()\r\n    305             # increasing, then use binary search for each starget\r\n    306             for starget in stargets:\r\n--> 307                 start = values.searchsorted(starget, side='left')\r\n    308                 end = values.searchsorted(starget, side='right')\r\n    309                 if start != end:\r\n\r\nTypeError: '<' not supported between instances of 'str' and 'int'\r\n\r\nIn [5]: a = pd.DataFrame({\"A\": [1, 2]})\r\n\r\nIn [6]: b = pd.DataFrame({\"B\": [3, 4]}, index=pd.CategoricalIndex(['a', 'b']))\r\n\r\nIn [7]: pd.concat([a, b], axis=1)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-a98a78ec2995> in <module>\r\n----> 1 pd.concat([a, b], axis=1)\r\n\r\n~/sandbox/pandas/pandas/core/reshape/concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\r\n    256     )\r\n    257\r\n--> 258     return op.get_result()\r\n    259\r\n    260\r\n\r\n~/sandbox/pandas/pandas/core/reshape/concat.py in get_result(self)\r\n    466                     obj_labels = mgr.axes[ax]\r\n    467                     if not new_labels.equals(obj_labels):\r\n--> 468                         indexers[ax] = obj_labels.reindex(new_labels)[1]\r\n    469\r\n    470                 mgrs_indexers.append((obj._data, indexers))\r\n\r\n~/sandbox/pandas/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance)\r\n    615                 # coerce to a regular index here!\r\n    616                 result = Index(np.array(self), name=self.name)\r\n--> 617                 new_target, indexer, _ = result._reindex_non_unique(np.array(target))\r\n    618             else:\r\n    619\r\n\r\n~/sandbox/pandas/pandas/core/indexes/base.py in _reindex_non_unique(self, target)\r\n   3388\r\n   3389         target = ensure_index(target)\r\n-> 3390         indexer, missing = self.get_indexer_non_unique(target)\r\n   3391         check = indexer != -1\r\n   3392         new_labels = self.take(indexer[check])\r\n\r\n~/sandbox/pandas/pandas/core/indexes/base.py in get_indexer_non_unique(self, target)\r\n   4751             tgt_values = target._ndarray_values\r\n   4752\r\n-> 4753         indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\r\n   4754         return ensure_platform_int(indexer), missing\r\n   4755\r\n\r\n~/sandbox/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_indexer_non_unique()\r\n    305             # increasing, then use binary search for each starget\r\n    306             for starget in stargets:\r\n--> 307                 start = values.searchsorted(starget, side='left')\r\n    308                 end = values.searchsorted(starget, side='right')\r\n    309                 if start != end:\r\n\r\n```\r\n\r\nI think that should coerce both to object-dtype index before going too far.\n\nHere is another bug which is probably another face of this bug:\r\n\r\nAggregating a categorical column in a groupby yields a Categorical object rather than a Series.\r\n\r\nHere is the code:\r\n\r\n```\r\ndf = pd.DataFrame({\r\n    'nr': [1,2,3,4,5,6,7,8], \r\n    'cat_ord': list('aabbccdd'), \r\n    'cat':list('aaaabbbb')\r\n})\r\ndf = df.astype({'cat': 'category', 'cat_ord': 'category'})\r\ndf['cat_ord'] = df['cat_ord'].cat.as_ordered()\r\ndf\r\n```\r\n\r\n>    nr cat_ord cat\r\n> 0   1       a   a\r\n> 1   2       a   a\r\n> 2   3       b   a\r\n> 3   4       b   a\r\n> \r\n\r\n`df.groupby('cat_ord')['cat'].first()`\r\n\r\n> [a, a, b, b]\r\n> Categories (2, object): [a, b]\r\n\r\n`type(df.groupby('cat_ord')['cat'].first())`\r\n\r\n> <class 'pandas.core.arrays.categorical.Categorical'>\r\n\n\nI suspect that this bug was introduced by the following change in 0.25.\r\n\r\nAggregation in groupby in Pandas 0.24 dropped the categorical dtype, replacing it by object which was a string.\r\n\r\nVersion 0.25 introduced additional code to restore the original dtype.  It seems to me that it is this code which is to blame for this bug.\r\n\r\nI also forgot to mention that the aggregation of categorical columns take huge amount of time on large datasets, much much more than for other column types.  For this reason I bypassed it entirely in my code, aggregating string columns instead, despite the memory cost.\n\nLooks like the examples work on master. Could use tests\r\n\r\n```\r\nIn [125]: df = pd.DataFrame({\r\n     ...:     'nr': [1,2,3,4,5,6,7,8],\r\n     ...:     'cat_ord': list('aabbccdd'),\r\n     ...:     'cat':list('aaaabbbb')\r\n     ...: })\r\n     ...: df = df.astype({'cat': 'category', 'cat_ord': 'category'})\r\n     ...: df['cat_ord'] = df['cat_ord'].cat.as_ordered()\r\n\r\nIn [126]: df.groupby('cat').agg({'nr': 'min', 'cat_ord': ['min', 'max']})\r\n     ...:\r\nOut[126]:\r\n     nr cat_ord\r\n    min     min max\r\ncat\r\na     1       a   b\r\nb     5       c   d\r\n\r\nIn [127]: df.groupby('cat').agg({'nr': ['min', 'max'], 'cat_ord': 'min'})\r\n     ...:\r\nOut[127]:\r\n     nr     cat_ord\r\n    min max     min\r\ncat\r\na     1   4       a\r\nb     5   8       c\r\n```\n\n**This seems solved on the current productive release.**\r\n\r\nIn my testing, all cases from the initial report came out correctly, with no regressions.\r\n\r\n### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.8.3.final.0\r\npython-bits      : 64\r\nOS               : Windows\r\nOS-release       : 10\r\nmachine          : AMD64\r\nprocessor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : None\r\nLOCALE           : German_Switzerland.1252\r\npandas           : 1.0.5\r\nnumpy            : 1.19.0\r\npytz             : 2020.1\r\ndateutil         : 2.8.1\r\npip              : 20.1.1\r\nsetuptools       : 46.0.0\r\nCython           : None\r\npytest           : None\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : 1.2.9\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.11.2\r\nIPython          : 7.16.1\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : None\r\nmatplotlib       : None\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : 0.17.1\r\npytables         : None\r\npytest           : None\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : None\r\nsqlalchemy       : None\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : 1.2.9\r\nnumba            : None\r\n</details>\r\n\r\nWell done, whoever fixed this!\r\n\r\n@mroeschke: Should I close this, or would you wait for more positive test results? (I don't have an installation of the development branch, so cannot test on that)\n\nWorks fine on 1.0.4.\n\n@kpflugshaupt we'll want to add a regression test to ensure that this issue doesn't occur again. Are you interested in submitted a PR with that test?\n\n> \r\n> \r\n> @kpflugshaupt we'll want to add a regression test to ensure that this issue doesn't occur again. Are you interested in submitted a PR with that test?\r\n\r\n@mroeschke Not sure I'll have the time -- work & family tend to get in the way. Possibly in the week starting July 20th. I'll try!\n\ntake",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/27071",
  "code_context": [
    {
      "filename": "pandas/core/groupby/generic.py",
      "content": "\"\"\"\nDefine the SeriesGroupBy and DataFrameGroupBy\nclasses that hold the groupby interfaces (and some implementations).\n\nThese are user facing as the result of the ``df.groupby(...)`` operations,\nwhich here returns a DataFrameGroupBy object.\n\"\"\"\n\nfrom collections import OrderedDict, abc, namedtuple\nimport copy\nfrom functools import partial\nfrom textwrap import dedent\nimport typing\nfrom typing import Any, Callable, FrozenSet, Iterator, List, Type, Union\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import Timestamp, lib\nfrom pandas.compat import PY36\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import Appender, Substitution\n\nfrom pandas.core.dtypes.cast import (\n    maybe_convert_objects, maybe_downcast_to_dtype)\nfrom pandas.core.dtypes.common import (\n    ensure_int64, ensure_platform_int, is_bool, is_datetimelike,\n    is_integer_dtype, is_interval_dtype, is_numeric_dtype, is_object_dtype,\n    is_scalar)\nfrom pandas.core.dtypes.missing import isna, notna\n\nfrom pandas._typing import FrameOrSeries\nimport pandas.core.algorithms as algorithms\nfrom pandas.core.base import DataError, SpecificationError\nimport pandas.core.common as com\nfrom pandas.core.frame import DataFrame\nfrom pandas.core.generic import NDFrame, _shared_docs\nfrom pandas.core.groupby import base\nfrom pandas.core.groupby.groupby import (\n    GroupBy, _apply_docs, _transform_template)\nfrom pandas.core.index import Index, MultiIndex\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.internals import BlockManager, make_block\nfrom pandas.core.series import Series\nfrom pandas.core.sparse.frame import SparseDataFrame\n\nfrom pandas.plotting import boxplot_frame_groupby\n\nNamedAgg = namedtuple(\"NamedAgg\", [\"column\", \"aggfunc\"])\n# TODO(typing) the return value on this callable should be any *scalar*.\nAggScalar = Union[str, Callable[..., Any]]\n\n\ndef whitelist_method_generator(base_class: Type[GroupBy],\n                               klass: Type[FrameOrSeries],\n                               whitelist: FrozenSet[str],\n                               ) -> Iterator[str]:\n    \"\"\"\n    Yields all GroupBy member defs for DataFrame/Series names in whitelist.\n\n    Parameters\n    ----------\n    base_class : Groupby class\n        base class\n    klass : DataFrame or Series class\n        class where members are defined.\n    whitelist : frozenset\n        Set of names of klass methods to be constructed\n\n    Returns\n    -------\n    The generator yields a sequence of strings, each suitable for exec'ing,\n    that define implementations of the named methods for DataFrameGroupBy\n    or SeriesGroupBy.\n\n    Since we don't want to override methods explicitly defined in the\n    base class, any such name is skipped.\n    \"\"\"\n    property_wrapper_template = \\\n        \"\"\"@property\ndef %(name)s(self) :\n    \\\"\"\"%(doc)s\\\"\"\"\n    return self.__getattr__('%(name)s')\"\"\"\n\n    for name in whitelist:\n        # don't override anything that was explicitly defined\n        # in the base class\n        if hasattr(base_class, name):\n            continue\n        # ugly, but we need the name string itself in the method.\n        f = getattr(klass, name)\n        doc = f.__doc__\n        doc = doc if type(doc) == str else ''\n        wrapper_template = property_wrapper_template\n        params = {'name': name, 'doc': doc}\n        yield wrapper_template % params\n\n\nclass NDFrameGroupBy(GroupBy):\n\n    def _iterate_slices(self):\n        if self.axis == 0:\n            # kludge\n            if self._selection is None:\n                slice_axis = self.obj.columns\n            else:\n                slice_axis = self._selection_list\n            slicer = lambda x: self.obj[x]\n        else:\n            slice_axis = self.obj.index\n            slicer = self.obj.xs\n\n        for val in slice_axis:\n            if val in self.exclusions:\n                continue\n            yield val, slicer(val)\n\n    def _cython_agg_general(self, how, alt=None, numeric_only=True,\n                            min_count=-1):\n        new_items, new_blocks = self._cython_agg_blocks(\n            how, alt=alt, numeric_only=numeric_only, min_count=min_count)\n        return self._wrap_agged_blocks(new_items, new_blocks)\n\n    _block_agg_axis = 0\n\n    def _cython_agg_blocks(self, how, alt=None, numeric_only=True,\n                           min_count=-1):\n        # TODO: the actual managing of mgr_locs is a PITA\n        # here, it should happen via BlockManager.combine\n\n        data, agg_axis = self._get_data_to_aggregate()\n\n        if numeric_only:\n            data = data.get_numeric_data(copy=False)\n\n        new_blocks = []\n        new_items = []\n        deleted_items = []\n        for block in data.blocks:\n\n            locs = block.mgr_locs.as_array\n            try:\n                result, _ = self.grouper.aggregate(\n                    block.values, how, axis=agg_axis, min_count=min_count)\n            except NotImplementedError:\n                # generally if we have numeric_only=False\n                # and non-applicable functions\n                # try to python agg\n\n                if alt is None:\n                    # we cannot perform the operation\n                    # in an alternate way, exclude the block\n                    deleted_items.append(locs)\n                    continue\n\n                # call our grouper again with only this block\n                from pandas.core.groupby.groupby import groupby\n\n                obj = self.obj[data.items[locs]]\n                s = groupby(obj, self.grouper)\n                try:\n                    result = s.aggregate(lambda x: alt(x, axis=self.axis))\n                except TypeError:\n                    # we may have an exception in trying to aggregate\n                    # continue and exclude the block\n                    pass\n\n            finally:\n\n                dtype = block.values.dtype\n\n                # see if we can cast the block back to the original dtype\n                result = block._try_coerce_and_cast_result(result, dtype=dtype)\n                newb = block.make_block(result)\n\n            new_items.append(locs)\n            new_blocks.append(newb)\n\n        if len(new_blocks) == 0:\n            raise DataError('No numeric types to aggregate')\n\n        # reset the locs in the blocks to correspond to our\n        # current ordering\n        indexer = np.concatenate(new_items)\n        new_items = data.items.take(np.sort(indexer))\n\n        if len(deleted_items):\n\n            # we need to adjust the indexer to account for the\n            # items we have removed\n            # really should be done in internals :<\n\n            deleted = np.concatenate(deleted_items)\n            ai = np.arange(len(data))\n            mask = np.zeros(len(data))\n            mask[deleted] = 1\n            indexer = (ai - mask.cumsum())[indexer]\n\n        offset = 0\n        for b in new_blocks:\n            loc = len(b.mgr_locs)\n            b.mgr_locs = indexer[offset:(offset + loc)]\n            offset += loc\n\n        return new_items, new_blocks\n\n    def aggregate(self, func, *args, **kwargs):\n        _level = kwargs.pop('_level', None)\n\n        relabeling = func is None and _is_multi_agg_with_relabel(**kwargs)\n        if relabeling:\n            func, columns, order = _normalize_keyword_aggregation(kwargs)\n\n            kwargs = {}\n        elif func is None:\n            # nicer error message\n            raise TypeError(\"Must provide 'func' or tuples of \"\n                            \"'(column, aggfunc).\")\n\n        result, how = self._aggregate(func, _level=_level, *args, **kwargs)\n        if how is None:\n            return result\n\n        if result is None:\n\n            # grouper specific aggregations\n            if self.grouper.nkeys > 1:\n                return self._python_agg_general(func, *args, **kwargs)\n            else:\n\n                # try to treat as if we are passing a list\n                try:\n                    assert not args and not kwargs\n                    result = self._aggregate_multiple_funcs(\n                        [func], _level=_level, _axis=self.axis)\n\n                    result.columns = Index(\n                        result.columns.levels[0],\n                        name=self._selected_obj.columns.name)\n\n                    if isinstance(self.obj, SparseDataFrame):\n                        # Backwards compat for groupby.agg() with sparse\n                        # values. concat no longer converts DataFrame[Sparse]\n                        # to SparseDataFrame, so we do it here.\n                        result = SparseDataFrame(result._data)\n                except Exception:\n                    result = self._aggregate_generic(func, *args, **kwargs)\n\n        if not self.as_index:\n            self._insert_inaxis_grouper_inplace(result)\n            result.index = np.arange(len(result))\n\n        if relabeling:\n            result = result[order]\n            result.columns = columns\n\n        return result._convert(datetime=True)\n\n    agg = aggregate\n\n    def _aggregate_generic(self, func, *args, **kwargs):\n        if self.grouper.nkeys != 1:\n            raise AssertionError('Number of keys must be 1')\n\n        axis = self.axis\n        obj = self._obj_with_exclusions\n\n        result = OrderedDict()\n        if axis != obj._info_axis_number:\n            try:\n                for name, data in self:\n                    result[name] = self._try_cast(func(data, *args, **kwargs),\n                                                  data)\n            except Exception:\n                return self._aggregate_item_by_item(func, *args, **kwargs)\n        else:\n            for name in self.indices:\n                try:\n                    data = self.get_group(name, obj=obj)\n                    result[name] = self._try_cast(func(data, *args, **kwargs),\n                                                  data)\n                except Exception:\n                    wrapper = lambda x: func(x, *args, **kwargs)\n                    result[name] = data.apply(wrapper, axis=axis)\n\n        return self._wrap_generic_output(result, obj)\n\n    def _wrap_aggregated_output(self, output, names=None):\n        raise AbstractMethodError(self)\n\n    def _aggregate_item_by_item(self, func, *args, **kwargs):\n        # only for axis==0\n\n        obj = self._obj_with_exclusions\n        result = OrderedDict()\n        cannot_agg = []\n        errors = None\n        for item in obj:\n            try:\n                data = obj[item]\n                colg = SeriesGroupBy(data, selection=item,\n                                     grouper=self.grouper)\n\n                cast = self._transform_should_cast(func)\n\n                result[item] = colg.aggregate(func, *args, **kwargs)\n                if cast:\n                    result[item] = self._try_cast(result[item], data)\n\n            except ValueError:\n                cannot_agg.append(item)\n                continue\n            except TypeError as e:\n                cannot_agg.append(item)\n                errors = e\n                continue\n\n        result_columns = obj.columns\n        if cannot_agg:\n            result_columns = result_columns.drop(cannot_agg)\n\n            # GH6337\n            if not len(result_columns) and errors is not None:\n                raise errors\n\n        return DataFrame(result, columns=result_columns)\n\n    def _decide_output_index(self, output, labels):\n        if len(output) == len(labels):\n            output_keys = labels\n        else:\n            output_keys = sorted(output)\n            try:\n                output_keys.sort()\n            except Exception:  # pragma: no cover\n                pass\n\n            if isinstance(labels, MultiIndex):\n                output_keys = MultiIndex.from_tuples(output_keys,\n                                                     names=labels.names)\n\n        return output_keys\n\n    def _wrap_applied_output(self, keys, values, not_indexed_same=False):\n        from pandas.core.index import _all_indexes_same\n\n        if len(keys) == 0:\n            return DataFrame(index=keys)\n\n        key_names = self.grouper.names\n\n        # GH12824.\n        def first_not_none(values):\n            try:\n                return next(com._not_none(*values))\n            except StopIteration:\n                return None\n\n        v = first_not_none(values)\n\n        if v is None:\n            # GH9684. If all values are None, then this will throw an error.\n            # We'd prefer it return an empty dataframe.\n            return DataFrame()\n        elif isinstance(v, DataFrame):\n            return self._concat_objects(keys, values,\n                                        not_indexed_same=not_indexed_same)\n        elif self.grouper.groupings is not None:\n            if len(self.grouper.groupings) > 1:\n                key_index = self.grouper.result_index\n\n            else:\n                ping = self.grouper.groupings[0]\n                if len(keys) == ping.ngroups:\n                    key_index = ping.group_index\n                    key_index.name = key_names[0]\n\n                    key_lookup = Index(keys)\n                    indexer = key_lookup.get_indexer(key_index)\n\n                    # reorder the values\n                    values = [values[i] for i in indexer]\n                else:\n\n                    key_index = Index(keys, name=key_names[0])\n\n                # don't use the key indexer\n                if not self.as_index:\n                    key_index = None\n\n            # make Nones an empty object\n            v = first_not_none(values)\n            if v is None:\n                return DataFrame()\n            elif isinstance(v, NDFrame):\n                values = [\n                    x if x is not None else\n                    v._constructor(**v._construct_axes_dict())\n                    for x in values\n                ]\n\n            v = values[0]\n\n            if isinstance(v, (np.ndarray, Index, Series)):\n                if isinstance(v, Series):\n                    applied_index = self._selected_obj._get_axis(self.axis)\n                    all_indexed_same = _all_indexes_same([\n                        x.index for x in values\n                    ])\n                    singular_series = (len(values) == 1 and\n                                       applied_index.nlevels == 1)\n\n                    # GH3596\n                    # provide a reduction (Frame -> Series) if groups are\n                    # unique\n                    if self.squeeze:\n                        # assign the name to this series\n                        if singular_series:\n                            values[0].name = keys[0]\n\n                            # GH2893\n                            # we have series in the values array, we want to\n                            # produce a series:\n                            # if any of the sub-series are not indexed the same\n                            # OR we don't have a multi-index and we have only a\n                            # single values\n                            return self._concat_objects(\n                                keys, values, not_indexed_same=not_indexed_same\n                            )\n\n                        # still a series\n                        # path added as of GH 5545\n                        elif all_indexed_same:\n                            from pandas.core.reshape.concat import concat\n                            return concat(values)\n\n                    if not all_indexed_same:\n                        # GH 8467\n                        return self._concat_objects(\n                            keys, values, not_indexed_same=True,\n                        )\n\n                try:\n                    if self.axis == 0:\n                        # GH6124 if the list of Series have a consistent name,\n                        # then propagate that name to the result.\n                        index = v.index.copy()\n                        if index.name is None:\n                            # Only propagate the series name to the result\n                            # if all series have a consistent name.  If the\n                            # series do not have a consistent name, do\n                            # nothing.\n                            names = {v.name for v in values}\n                            if len(names) == 1:\n                                index.name = list(names)[0]\n\n                        # normally use vstack as its faster than concat\n                        # and if we have mi-columns\n                        if (isinstance(v.index, MultiIndex) or\n                                key_index is None or\n                                isinstance(key_index, MultiIndex)):\n                            stacked_values = np.vstack([\n                                np.asarray(v) for v in values\n                            ])\n                            result = DataFrame(stacked_values, index=key_index,\n                                               columns=index)\n                        else:\n                            # GH5788 instead of stacking; concat gets the\n                            # dtypes correct\n                            from pandas.core.reshape.concat import concat\n                            result = concat(values, keys=key_index,\n                                            names=key_index.names,\n                                            axis=self.axis).unstack()\n                            result.columns = index\n                    else:\n                        stacked_values = np.vstack([np.asarray(v)\n                                                    for v in values])\n                        result = DataFrame(stacked_values.T, index=v.index,\n                                           columns=key_index)\n\n                except (ValueError, AttributeError):\n                    # GH1738: values is list of arrays of unequal lengths fall\n                    # through to the outer else caluse\n                    return Series(values, index=key_index,\n                                  name=self._selection_name)\n\n                # if we have date/time like in the original, then coerce dates\n                # as we are stacking can easily have object dtypes here\n                so = self._selected_obj\n                if so.ndim == 2 and so.dtypes.apply(is_datetimelike).any():\n                    result = _recast_datetimelike_result(result)\n                else:\n                    result = result._convert(datetime=True)\n\n                return self._reindex_output(result)\n\n            # values are not series or array-like but scalars\n            else:\n                # only coerce dates if we find at least 1 datetime\n                coerce = any(isinstance(x, Timestamp) for x in values)\n                # self._selection_name not passed through to Series as the\n                # result should not take the name of original selection\n                # of columns\n                return (Series(values, index=key_index)\n                        ._convert(datetime=True,\n                                  coerce=coerce))\n\n        else:\n            # Handle cases like BinGrouper\n            return self._concat_objects(keys, values,\n                                        not_indexed_same=not_indexed_same)\n\n    def _transform_general(self, func, *args, **kwargs):\n        from pandas.core.reshape.concat import concat\n\n        applied = []\n        obj = self._obj_with_exclusions\n        gen = self.grouper.get_iterator(obj, axis=self.axis)\n        fast_path, slow_path = self._define_paths(func, *args, **kwargs)\n\n        path = None\n        for name, group in gen:\n            object.__setattr__(group, 'name', name)\n\n            if path is None:\n                # Try slow path and fast path.\n                try:\n                    path, res = self._choose_path(fast_path, slow_path, group)\n                except TypeError:\n                    return self._transform_item_by_item(obj, fast_path)\n                except ValueError:\n                    msg = 'transform must return a scalar value for each group'\n                    raise ValueError(msg)\n            else:\n                res = path(group)\n\n            if isinstance(res, Series):\n\n                # we need to broadcast across the\n                # other dimension; this will preserve dtypes\n                # GH14457\n                if not np.prod(group.shape):\n                    continue\n                elif res.index.is_(obj.index):\n                    r = concat([res] * len(group.columns), axis=1)\n                    r.columns = group.columns\n                    r.index = group.index\n                else:\n                    r = DataFrame(\n                        np.concatenate([res.values] * len(group.index)\n                                       ).reshape(group.shape),\n                        columns=group.columns, index=group.index)\n\n                applied.append(r)\n            else:\n                applied.append(res)\n\n        concat_index = obj.columns if self.axis == 0 else obj.index\n        concatenated = concat(applied, join_axes=[concat_index],\n                              axis=self.axis, verify_integrity=False)\n        return self._set_result_index_ordered(concatenated)\n\n    @Substitution(klass='DataFrame', selected='')\n    @Appender(_transform_template)\n    def transform(self, func, *args, **kwargs):\n\n        # optimized transforms\n        func = self._is_cython_func(func) or func\n        if isinstance(func, str):\n            if func in base.cython_transforms:\n                # cythonized transform\n                return getattr(self, func)(*args, **kwargs)\n            else:\n                # cythonized aggregation and merge\n                result = getattr(self, func)(*args, **kwargs)\n        else:\n            return self._transform_general(func, *args, **kwargs)\n\n        # a reduction transform\n        if not isinstance(result, DataFrame):\n            return self._transform_general(func, *args, **kwargs)\n\n        obj = self._obj_with_exclusions\n\n        # nuiscance columns\n        if not result.columns.equals(obj.columns):\n            return self._transform_general(func, *args, **kwargs)\n\n        return self._transform_fast(result, obj, func)\n\n    def _transform_fast(self, result, obj, func_nm):\n        \"\"\"\n        Fast transform path for aggregations\n        \"\"\"\n        # if there were groups with no observations (Categorical only?)\n        # try casting data to original dtype\n        cast = self._transform_should_cast(func_nm)\n\n        # for each col, reshape to to size of original frame\n        # by take operation\n        ids, _, ngroup = self.grouper.group_info\n        output = []\n        for i, _ in enumerate(result.columns):\n            res = algorithms.take_1d(result.iloc[:, i].values, ids)\n            if cast:\n                res = self._try_cast(res, obj.iloc[:, i])\n            output.append(res)\n\n        return DataFrame._from_arrays(output, columns=result.columns,\n                                      index=obj.index)\n\n    def _define_paths(self, func, *args, **kwargs):\n        if isinstance(func, str):\n            fast_path = lambda group: getattr(group, func)(*args, **kwargs)\n            slow_path = lambda group: group.apply(\n                lambda x: getattr(x, func)(*args, **kwargs), axis=self.axis)\n        else:\n            fast_path = lambda group: func(group, *args, **kwargs)\n            slow_path = lambda group: group.apply(\n                lambda x: func(x, *args, **kwargs), axis=self.axis)\n        return fast_path, slow_path\n\n    def _choose_path(self, fast_path, slow_path, group):\n        path = slow_path\n        res = slow_path(group)\n\n        # if we make it here, test if we can use the fast path\n        try:\n            res_fast = fast_path(group)\n\n            # verify fast path does not change columns (and names), otherwise\n            # its results cannot be joined with those of the slow path\n            if res_fast.columns != group.columns:\n                return path, res\n            # verify numerical equality with the slow path\n            if res.shape == res_fast.shape:\n                res_r = res.values.ravel()\n                res_fast_r = res_fast.values.ravel()\n                mask = notna(res_r)\n                if (res_r[mask] == res_fast_r[mask]).all():\n                    path = fast_path\n        except Exception:\n            pass\n        return path, res\n\n    def _transform_item_by_item(self, obj, wrapper):\n        # iterate through columns\n        output = {}\n        inds = []\n        for i, col in enumerate(obj):\n            try:\n                output[col] = self[col].transform(wrapper)\n                inds.append(i)\n            except Exception:\n                pass\n\n        if len(output) == 0:  # pragma: no cover\n            raise TypeError('Transform function invalid for data types')\n\n        columns = obj.columns\n        if len(output) < len(obj.columns):\n            columns = columns.take(inds)\n\n        return DataFrame(output, index=obj.index, columns=columns)\n\n    def filter(self, func, dropna=True, *args, **kwargs):  # noqa\n        \"\"\"\n        Return a copy of a DataFrame excluding elements from groups that\n        do not satisfy the boolean criterion specified by func.\n\n        Parameters\n        ----------\n        f : function\n            Function to apply to each subframe. Should return True or False.\n        dropna : Drop groups that do not pass the filter. True by default;\n            if False, groups that evaluate False are filled with NaNs.\n\n        Returns\n        -------\n        filtered : DataFrame\n\n        Notes\n        -----\n        Each subframe is endowed the attribute 'name' in case you need to know\n        which group you are working on.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n        ...                           'foo', 'bar'],\n        ...                    'B' : [1, 2, 3, 4, 5, 6],\n        ...                    'C' : [2.0, 5., 8., 1., 2., 9.]})\n        >>> grouped = df.groupby('A')\n        >>> grouped.filter(lambda x: x['B'].mean() > 3.)\n             A  B    C\n        1  bar  2  5.0\n        3  bar  4  1.0\n        5  bar  6  9.0\n        \"\"\"\n\n        indices = []\n\n        obj = self._selected_obj\n        gen = self.grouper.get_iterator(obj, axis=self.axis)\n\n        for name, group in gen:\n            object.__setattr__(group, 'name', name)\n\n            res = func(group, *args, **kwargs)\n\n            try:\n                res = res.squeeze()\n            except AttributeError:  # allow e.g., scalars and frames to pass\n                pass\n\n            # interpret the result of the filter\n            if is_bool(res) or (is_scalar(res) and isna(res)):\n                if res and notna(res):\n                    indices.append(self._get_index(name))\n            else:\n                # non scalars aren't allowed\n                raise TypeError(\"filter function returned a %s, \"\n                                \"but expected a scalar bool\" %\n                                type(res).__name__)\n\n        return self._apply_filter(indices, dropna)\n\n\nclass SeriesGroupBy(GroupBy):\n    #\n    # Make class defs of attributes on SeriesGroupBy whitelist\n\n    _apply_whitelist = base.series_apply_whitelist\n    for _def_str in whitelist_method_generator(\n            GroupBy, Series, _apply_whitelist):\n        exec(_def_str)\n\n    @property\n    def _selection_name(self):\n        \"\"\"\n        since we are a series, we by definition only have\n        a single name, but may be the result of a selection or\n        the name of our object\n        \"\"\"\n        if self._selection is None:\n            return self.obj.name\n        else:\n            return self._selection\n\n    _agg_see_also_doc = dedent(\"\"\"\n    See Also\n    --------\n    pandas.Series.groupby.apply\n    pandas.Series.groupby.transform\n    pandas.Series.aggregate\n    \"\"\")\n\n    _agg_examples_doc = dedent(\"\"\"\n    Examples\n    --------\n    >>> s = pd.Series([1, 2, 3, 4])\n\n    >>> s\n    0    1\n    1    2\n    2    3\n    3    4\n    dtype: int64\n\n    >>> s.groupby([1, 1, 2, 2]).min()\n    1    1\n    2    3\n    dtype: int64\n\n    >>> s.groupby([1, 1, 2, 2]).agg('min')\n    1    1\n    2    3\n    dtype: int64\n\n    >>> s.groupby([1, 1, 2, 2]).agg(['min', 'max'])\n       min  max\n    1    1    2\n    2    3    4\n\n    The output column names can be controlled by passing\n    the desired column names and aggregations as keyword arguments.\n\n    >>> s.groupby([1, 1, 2, 2]).agg(\n    ...     minimum='min',\n    ...     maximum='max',\n    ... )\n       minimum  maximum\n    1        1        2\n    2        3        4\n    \"\"\")\n\n    @Appender(_apply_docs['template']\n              .format(input='series',\n                      examples=_apply_docs['series_examples']))\n    def apply(self, func, *args, **kwargs):\n        return super().apply(func, *args, **kwargs)\n\n    @Substitution(see_also=_agg_see_also_doc,\n                  examples=_agg_examples_doc,\n                  versionadded='',\n                  klass='Series',\n                  axis='')\n    @Appender(_shared_docs['aggregate'])\n    def aggregate(self, func_or_funcs=None, *args, **kwargs):\n        _level = kwargs.pop('_level', None)\n\n        relabeling = func_or_funcs is None\n        columns = None\n        no_arg_message = (\"Must provide 'func_or_funcs' or named \"\n                          \"aggregation **kwargs.\")\n        if relabeling:\n            columns = list(kwargs)\n            if not PY36:\n                # sort for 3.5 and earlier\n                columns = list(sorted(columns))\n\n            func_or_funcs = [kwargs[col] for col in columns]\n            kwargs = {}\n            if not columns:\n                raise TypeError(no_arg_message)\n\n        if isinstance(func_or_funcs, str):\n            return getattr(self, func_or_funcs)(*args, **kwargs)\n\n        if isinstance(func_or_funcs, abc.Iterable):\n            # Catch instances of lists / tuples\n            # but not the class list / tuple itself.\n            ret = self._aggregate_multiple_funcs(func_or_funcs,\n                                                 (_level or 0) + 1)\n            if relabeling:\n                ret.columns = columns\n        else:\n            cyfunc = self._is_cython_func(func_or_funcs)\n            if cyfunc and not args and not kwargs:\n                return getattr(self, cyfunc)()\n\n            if self.grouper.nkeys > 1:\n                return self._python_agg_general(func_or_funcs, *args, **kwargs)\n\n            try:\n                return self._python_agg_general(func_or_funcs, *args, **kwargs)\n            except Exception:\n                result = self._aggregate_named(func_or_funcs, *args, **kwargs)\n\n            index = Index(sorted(result), name=self.grouper.names[0])\n            ret = Series(result, index=index)\n\n        if not self.as_index:  # pragma: no cover\n            print('Warning, ignoring as_index=True')\n\n        # _level handled at higher\n        if not _level and isinstance(ret, dict):\n            from pandas import concat\n            ret = concat(ret, axis=1)\n        return ret\n\n    agg = aggregate\n\n    def _aggregate_multiple_funcs(self, arg, _level):\n        if isinstance(arg, dict):\n\n            # show the deprecation, but only if we\n            # have not shown a higher level one\n            # GH 15931\n            if isinstance(self._selected_obj, Series) and _level <= 1:\n                msg = dedent(\"\"\"\\\n                using a dict on a Series for aggregation\n                is deprecated and will be removed in a future version. Use \\\n                named aggregation instead.\n\n                    >>> grouper.agg(name_1=func_1, name_2=func_2)\n                \"\"\")\n                warnings.warn(msg, FutureWarning, stacklevel=3)\n\n            columns = list(arg.keys())\n            arg = arg.items()\n        elif any(isinstance(x, (tuple, list)) for x in arg):\n            arg = [(x, x) if not isinstance(x, (tuple, list)) else x\n                   for x in arg]\n\n            # indicated column order\n            columns = next(zip(*arg))\n        else:\n            # list of functions / function names\n            columns = []\n            for f in arg:\n                columns.append(com.get_callable_name(f) or f)\n\n            arg = zip(columns, arg)\n\n        results = OrderedDict()\n        for name, func in arg:\n            obj = self\n            if name in results:\n                raise SpecificationError(\n                    'Function names must be unique, found multiple named '\n                    '{}'.format(name))\n\n            # reset the cache so that we\n            # only include the named selection\n            if name in self._selected_obj:\n                obj = copy.copy(obj)\n                obj._reset_cache()\n                obj._selection = name\n            results[name] = obj.aggregate(func)\n\n        if any(isinstance(x, DataFrame) for x in results.values()):\n            # let higher level handle\n            if _level:\n                return results\n\n        return DataFrame(results, columns=columns)\n\n    def _wrap_output(self, output, index, names=None):\n        \"\"\" common agg/transform wrapping logic \"\"\"\n        output = output[self._selection_name]\n\n        if names is not None:\n            return DataFrame(output, index=index, columns=names)\n        else:\n            name = self._selection_name\n            if name is None:\n                name = self._selected_obj.name\n            return Series(output, index=index, name=name)\n\n    def _wrap_aggregated_output(self, output, names=None):\n        result = self._wrap_output(output=output,\n                                   index=self.grouper.result_index,\n                                   names=names)\n        return self._reindex_output(result)._convert(datetime=True)\n\n    def _wrap_transformed_output(self, output, names=None):\n        return self._wrap_output(output=output,\n                                 index=self.obj.index,\n                                 names=names)\n\n    def _wrap_applied_output(self, keys, values, not_indexed_same=False):\n        if len(keys) == 0:\n            # GH #6265\n            return Series([], name=self._selection_name, index=keys)\n\n        def _get_index():\n            if self.grouper.nkeys > 1:\n                index = MultiIndex.from_tuples(keys, names=self.grouper.names)\n            else:\n                index = Index(keys, name=self.grouper.names[0])\n            return index\n\n        if isinstance(values[0], dict):\n            # GH #823 #24880\n            index = _get_index()\n            result = self._reindex_output(DataFrame(values, index=index))\n            # if self.observed is False,\n            # keep all-NaN rows created while re-indexing\n            result = result.stack(dropna=self.observed)\n            result.name = self._selection_name\n            return result\n\n        if isinstance(values[0], Series):\n            return self._concat_objects(keys, values,\n                                        not_indexed_same=not_indexed_same)\n        elif isinstance(values[0], DataFrame):\n            # possible that Series -> DataFrame by applied function\n            return self._concat_objects(keys, values,\n                                        not_indexed_same=not_indexed_same)\n        else:\n            # GH #6265 #24880\n            result = Series(data=values,\n                            index=_get_index(),\n                            name=self._selection_name)\n            return self._reindex_output(result)\n\n    def _aggregate_named(self, func, *args, **kwargs):\n        result = OrderedDict()\n\n        for name, group in self:\n            group.name = name\n            output = func(group, *args, **kwargs)\n            if isinstance(output, (Series, Index, np.ndarray)):\n                raise Exception('Must produce aggregated value')\n            result[name] = self._try_cast(output, group)\n\n        return result\n\n    @Substitution(klass='Series', selected='A.')\n    @Appender(_transform_template)\n    def transform(self, func, *args, **kwargs):\n        func = self._is_cython_func(func) or func\n\n        # if string function\n        if isinstance(func, str):\n            if func in base.cython_transforms:\n                # cythonized transform\n                return getattr(self, func)(*args, **kwargs)\n            else:\n                # cythonized aggregation and merge\n                return self._transform_fast(\n                    lambda: getattr(self, func)(*args, **kwargs), func)\n\n        # reg transform\n        klass = self._selected_obj.__class__\n        results = []\n        wrapper = lambda x: func(x, *args, **kwargs)\n        for name, group in self:\n            object.__setattr__(group, 'name', name)\n            res = wrapper(group)\n\n            if hasattr(res, 'values'):\n                res = res.values\n\n            indexer = self._get_index(name)\n            s = klass(res, indexer)\n            results.append(s)\n\n        # check for empty \"results\" to avoid concat ValueError\n        if results:\n            from pandas.core.reshape.concat import concat\n            result = concat(results).sort_index()\n        else:\n            result = Series()\n\n        # we will only try to coerce the result type if\n        # we have a numeric dtype, as these are *always* udfs\n        # the cython take a different path (and casting)\n        dtype = self._selected_obj.dtype\n        if is_numeric_dtype(dtype):\n            result = maybe_downcast_to_dtype(result, dtype)\n\n        result.name = self._selected_obj.name\n        result.index = self._selected_obj.index\n        return result\n\n    def _transform_fast(self, func, func_nm):\n        \"\"\"\n        fast version of transform, only applicable to\n        builtin/cythonizable functions\n        \"\"\"\n        if isinstance(func, str):\n            func = getattr(self, func)\n\n        ids, _, ngroup = self.grouper.group_info\n        cast = self._transform_should_cast(func_nm)\n        out = algorithms.take_1d(func()._values, ids)\n        if cast:\n            out = self._try_cast(out, self.obj)\n        return Series(out, index=self.obj.index, name=self.obj.name)\n\n    def filter(self, func, dropna=True, *args, **kwargs):  # noqa\n        \"\"\"\n        Return a copy of a Series excluding elements from groups that\n        do not satisfy the boolean criterion specified by func.\n\n        Parameters\n        ----------\n        func : function\n            To apply to each group. Should return True or False.\n        dropna : Drop groups that do not pass the filter. True by default;\n            if False, groups that evaluate False are filled with NaNs.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n        ...                           'foo', 'bar'],\n        ...                    'B' : [1, 2, 3, 4, 5, 6],\n        ...                    'C' : [2.0, 5., 8., 1., 2., 9.]})\n        >>> grouped = df.groupby('A')\n        >>> df.groupby('A').B.filter(lambda x: x.mean() > 3.)\n        1    2\n        3    4\n        5    6\n        Name: B, dtype: int64\n\n        Returns\n        -------\n        filtered : Series\n        \"\"\"\n        if isinstance(func, str):\n            wrapper = lambda x: getattr(x, func)(*args, **kwargs)\n        else:\n            wrapper = lambda x: func(x, *args, **kwargs)\n\n        # Interpret np.nan as False.\n        def true_and_notna(x, *args, **kwargs):\n            b = wrapper(x, *args, **kwargs)\n            return b and notna(b)\n\n        try:\n            indices = [self._get_index(name) for name, group in self\n                       if true_and_notna(group)]\n        except ValueError:\n            raise TypeError(\"the filter must return a boolean result\")\n        except TypeError:\n            raise TypeError(\"the filter must return a boolean result\")\n\n        filtered = self._apply_filter(indices, dropna)\n        return filtered\n\n    def nunique(self, dropna=True):\n        \"\"\"\n        Return number of unique elements in the group.\n\n        Returns\n        -------\n        Series\n            Number of unique values within each group.\n        \"\"\"\n        ids, _, _ = self.grouper.group_info\n\n        val = self.obj.get_values()\n\n        try:\n            sorter = np.lexsort((val, ids))\n        except TypeError:  # catches object dtypes\n            msg = 'val.dtype must be object, got {}'.format(val.dtype)\n            assert val.dtype == object, msg\n            val, _ = algorithms.factorize(val, sort=False)\n            sorter = np.lexsort((val, ids))\n            _isna = lambda a: a == -1\n        else:\n            _isna = isna\n\n        ids, val = ids[sorter], val[sorter]\n\n        # group boundaries are where group ids change\n        # unique observations are where sorted values change\n        idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, val[1:] != val[:-1]]\n\n        # 1st item of each group is a new unique observation\n        mask = _isna(val)\n        if dropna:\n            inc[idx] = 1\n            inc[mask] = 0\n        else:\n            inc[mask & np.r_[False, mask[:-1]]] = 0\n            inc[idx] = 1\n\n        out = np.add.reduceat(inc, idx).astype('int64', copy=False)\n        if len(ids):\n            # NaN/NaT group exists if the head of ids is -1,\n            # so remove it from res and exclude its index from idx\n            if ids[0] == -1:\n                res = out[1:]\n                idx = idx[np.flatnonzero(idx)]\n            else:\n                res = out\n        else:\n            res = out[1:]\n        ri = self.grouper.result_index\n\n        # we might have duplications among the bins\n        if len(res) != len(ri):\n            res, out = np.zeros(len(ri), dtype=out.dtype), res\n            res[ids[idx]] = out\n\n        return Series(res,\n                      index=ri,\n                      name=self._selection_name)\n\n    @Appender(Series.describe.__doc__)\n    def describe(self, **kwargs):\n        result = self.apply(lambda x: x.describe(**kwargs))\n        if self.axis == 1:\n            return result.T\n        return result.unstack()\n\n    def value_counts(self, normalize=False, sort=True, ascending=False,\n                     bins=None, dropna=True):\n\n        from pandas.core.reshape.tile import cut\n        from pandas.core.reshape.merge import _get_join_indexers\n\n        if bins is not None and not np.iterable(bins):\n            # scalar bins cannot be done at top level\n            # in a backward compatible way\n            return self.apply(Series.value_counts,\n                              normalize=normalize,\n                              sort=sort,\n                              ascending=ascending,\n                              bins=bins)\n\n        ids, _, _ = self.grouper.group_info\n        val = self.obj.get_values()\n\n        # groupby removes null keys from groupings\n        mask = ids != -1\n        ids, val = ids[mask], val[mask]\n\n        if bins is None:\n            lab, lev = algorithms.factorize(val, sort=True)\n            llab = lambda lab, inc: lab[inc]\n        else:\n\n            # lab is a Categorical with categories an IntervalIndex\n            lab = cut(Series(val), bins, include_lowest=True)\n            lev = lab.cat.categories\n            lab = lev.take(lab.cat.codes)\n            llab = lambda lab, inc: lab[inc]._multiindex.codes[-1]\n\n        if is_interval_dtype(lab):\n            # TODO: should we do this inside II?\n            sorter = np.lexsort((lab.left, lab.right, ids))\n        else:\n            sorter = np.lexsort((lab, ids))\n\n        ids, lab = ids[sorter], lab[sorter]\n\n        # group boundaries are where group ids change\n        idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n\n        # new values are where sorted labels change\n        lchanges = llab(lab, slice(1, None)) != llab(lab, slice(None, -1))\n        inc = np.r_[True, lchanges]\n        inc[idx] = True  # group boundaries are also new values\n        out = np.diff(np.nonzero(np.r_[inc, True])[0])  # value counts\n\n        # num. of times each group should be repeated\n        rep = partial(np.repeat, repeats=np.add.reduceat(inc, idx))\n\n        # multi-index components\n        labels = list(map(rep, self.grouper.recons_labels)) + [llab(lab, inc)]\n        levels = [ping.group_index for ping in self.grouper.groupings] + [lev]\n        names = self.grouper.names + [self._selection_name]\n\n        if dropna:\n            mask = labels[-1] != -1\n            if mask.all():\n                dropna = False\n            else:\n                out, labels = out[mask], [label[mask] for label in labels]\n\n        if normalize:\n            out = out.astype('float')\n            d = np.diff(np.r_[idx, len(ids)])\n            if dropna:\n                m = ids[lab == -1]\n                np.add.at(d, m, -1)\n                acc = rep(d)[mask]\n            else:\n                acc = rep(d)\n            out /= acc\n\n        if sort and bins is None:\n            cat = ids[inc][mask] if dropna else ids[inc]\n            sorter = np.lexsort((out if ascending else -out, cat))\n            out, labels[-1] = out[sorter], labels[-1][sorter]\n\n        if bins is None:\n            mi = MultiIndex(levels=levels, codes=labels, names=names,\n                            verify_integrity=False)\n\n            if is_integer_dtype(out):\n                out = ensure_int64(out)\n            return Series(out, index=mi, name=self._selection_name)\n\n        # for compat. with libgroupby.value_counts need to ensure every\n        # bin is present at every index level, null filled with zeros\n        diff = np.zeros(len(out), dtype='bool')\n        for lab in labels[:-1]:\n            diff |= np.r_[True, lab[1:] != lab[:-1]]\n\n        ncat, nbin = diff.sum(), len(levels[-1])\n\n        left = [np.repeat(np.arange(ncat), nbin),\n                np.tile(np.arange(nbin), ncat)]\n\n        right = [diff.cumsum() - 1, labels[-1]]\n\n        _, idx = _get_join_indexers(left, right, sort=False, how='left')\n        out = np.where(idx != -1, out[idx], 0)\n\n        if sort:\n            sorter = np.lexsort((out if ascending else -out, left[0]))\n            out, left[-1] = out[sorter], left[-1][sorter]\n\n        # build the multi-index w/ full levels\n        codes = list(map(lambda lab: np.repeat(lab[diff], nbin), labels[:-1]))\n        codes.append(left[-1])\n\n        mi = MultiIndex(levels=levels, codes=codes, names=names,\n                        verify_integrity=False)\n\n        if is_integer_dtype(out):\n            out = ensure_int64(out)\n        return Series(out, index=mi, name=self._selection_name)\n\n    def count(self):\n        \"\"\"\n        Compute count of group, excluding missing values.\n\n        Returns\n        -------\n        Series\n            Count of values within each group.\n        \"\"\"\n        ids, _, ngroups = self.grouper.group_info\n        val = self.obj.get_values()\n\n        mask = (ids != -1) & ~isna(val)\n        ids = ensure_platform_int(ids)\n        minlength = ngroups or 0\n        out = np.bincount(ids[mask], minlength=minlength)\n\n        return Series(out,\n                      index=self.grouper.result_index,\n                      name=self._selection_name,\n                      dtype='int64')\n\n    def _apply_to_column_groupbys(self, func):\n        \"\"\" return a pass thru \"\"\"\n        return func(self)\n\n    def pct_change(self, periods=1, fill_method='pad', limit=None, freq=None):\n        \"\"\"Calculate pct_change of each value to previous entry in group\"\"\"\n        # TODO: Remove this conditional when #23918 is fixed\n        if freq:\n            return self.apply(lambda x: x.pct_change(periods=periods,\n                                                     fill_method=fill_method,\n                                                     limit=limit, freq=freq))\n        filled = getattr(self, fill_method)(limit=limit)\n        fill_grp = filled.groupby(self.grouper.labels)\n        shifted = fill_grp.shift(periods=periods, freq=freq)\n\n        return (filled / shifted) - 1\n\n\nclass DataFrameGroupBy(NDFrameGroupBy):\n\n    _apply_whitelist = base.dataframe_apply_whitelist\n\n    #\n    # Make class defs of attributes on DataFrameGroupBy whitelist.\n    for _def_str in whitelist_method_generator(\n            GroupBy, DataFrame, _apply_whitelist):\n        exec(_def_str)\n\n    _block_agg_axis = 1\n\n    _agg_see_also_doc = dedent(\"\"\"\n    See Also\n    --------\n    pandas.DataFrame.groupby.apply\n    pandas.DataFrame.groupby.transform\n    pandas.DataFrame.aggregate\n    \"\"\")\n\n    _agg_examples_doc = dedent(\"\"\"\n    Examples\n    --------\n\n    >>> df = pd.DataFrame({'A': [1, 1, 2, 2],\n    ...                    'B': [1, 2, 3, 4],\n    ...                    'C': np.random.randn(4)})\n\n    >>> df\n       A  B         C\n    0  1  1  0.362838\n    1  1  2  0.227877\n    2  2  3  1.267767\n    3  2  4 -0.562860\n\n    The aggregation is for each column.\n\n    >>> df.groupby('A').agg('min')\n       B         C\n    A\n    1  1  0.227877\n    2  3 -0.562860\n\n    Multiple aggregations\n\n    >>> df.groupby('A').agg(['min', 'max'])\n        B             C\n      min max       min       max\n    A\n    1   1   2  0.227877  0.362838\n    2   3   4 -0.562860  1.267767\n\n    Select a column for aggregation\n\n    >>> df.groupby('A').B.agg(['min', 'max'])\n       min  max\n    A\n    1    1    2\n    2    3    4\n\n    Different aggregations per column\n\n    >>> df.groupby('A').agg({'B': ['min', 'max'], 'C': 'sum'})\n        B             C\n      min max       sum\n    A\n    1   1   2  0.590716\n    2   3   4  0.704907\n\n    To control the output names with different aggregations per column,\n    pandas supports \"named aggregation\"\n\n    >>> df.groupby(\"A\").agg(\n    ...     b_min=pd.NamedAgg(column=\"B\", aggfunc=\"min\"),\n    ...     c_sum=pd.NamedAgg(column=\"C\", aggfunc=\"sum\"))\n       b_min     c_sum\n    A\n    1      1 -1.956929\n    2      3 -0.322183\n\n    - The keywords are the *output* column names\n    - The values are tuples whose first element is the column to select\n      and the second element is the aggregation to apply to that column.\n      Pandas provides the ``pandas.NamedAgg`` namedtuple with the fields\n      ``['column', 'aggfunc']`` to make it clearer what the arguments are.\n      As usual, the aggregation can be a callable or a string alias.\n\n    See :ref:`groupby.aggregate.named` for more.\n    \"\"\")\n\n    @Substitution(see_also=_agg_see_also_doc,\n                  examples=_agg_examples_doc,\n                  versionadded='',\n                  klass='DataFrame',\n                  axis='')\n    @Appender(_shared_docs['aggregate'])\n    def aggregate(self, arg=None, *args, **kwargs):\n        return super().aggregate(arg, *args, **kwargs)\n\n    agg = aggregate\n\n    def _gotitem(self, key, ndim, subset=None):\n        \"\"\"\n        sub-classes to define\n        return a sliced object\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : 1,2\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n\n        if ndim == 2:\n            if subset is None:\n                subset = self.obj\n            return DataFrameGroupBy(subset, self.grouper, selection=key,\n                                    grouper=self.grouper,\n                                    exclusions=self.exclusions,\n                                    as_index=self.as_index,\n                                    observed=self.observed)\n        elif ndim == 1:\n            if subset is None:\n                subset = self.obj[key]\n            return SeriesGroupBy(subset, selection=key,\n                                 grouper=self.grouper,\n                                 observed=self.observed)\n\n        raise AssertionError(\"invalid ndim for _gotitem\")\n\n    def _wrap_generic_output(self, result, obj):\n        result_index = self.grouper.levels[0]\n\n        if self.axis == 0:\n            return DataFrame(result, index=obj.columns,\n                             columns=result_index).T\n        else:\n            return DataFrame(result, index=obj.index,\n                             columns=result_index)\n\n    def _get_data_to_aggregate(self):\n        obj = self._obj_with_exclusions\n        if self.axis == 1:\n            return obj.T._data, 1\n        else:\n            return obj._data, 1\n\n    def _insert_inaxis_grouper_inplace(self, result):\n        # zip in reverse so we can always insert at loc 0\n        izip = zip(* map(reversed, (\n            self.grouper.names,\n            self.grouper.get_group_levels(),\n            [grp.in_axis for grp in self.grouper.groupings])))\n\n        for name, lev, in_axis in izip:\n            if in_axis:\n                result.insert(0, name, lev)\n\n    def _wrap_aggregated_output(self, output, names=None):\n        agg_axis = 0 if self.axis == 1 else 1\n        agg_labels = self._obj_with_exclusions._get_axis(agg_axis)\n\n        output_keys = self._decide_output_index(output, agg_labels)\n\n        if not self.as_index:\n            result = DataFrame(output, columns=output_keys)\n            self._insert_inaxis_grouper_inplace(result)\n            result = result._consolidate()\n        else:\n            index = self.grouper.result_index\n            result = DataFrame(output, index=index, columns=output_keys)\n\n        if self.axis == 1:\n            result = result.T\n\n        return self._reindex_output(result)._convert(datetime=True)\n\n    def _wrap_transformed_output(self, output, names=None):\n        return DataFrame(output, index=self.obj.index)\n\n    def _wrap_agged_blocks(self, items, blocks):\n        if not self.as_index:\n            index = np.arange(blocks[0].values.shape[-1])\n            mgr = BlockManager(blocks, [items, index])\n            result = DataFrame(mgr)\n\n            self._insert_inaxis_grouper_inplace(result)\n            result = result._consolidate()\n        else:\n            index = self.grouper.result_index\n            mgr = BlockManager(blocks, [items, index])\n            result = DataFrame(mgr)\n\n        if self.axis == 1:\n            result = result.T\n\n        return self._reindex_output(result)._convert(datetime=True)\n\n    def _iterate_column_groupbys(self):\n        for i, colname in enumerate(self._selected_obj.columns):\n            yield colname, SeriesGroupBy(self._selected_obj.iloc[:, i],\n                                         selection=colname,\n                                         grouper=self.grouper,\n                                         exclusions=self.exclusions)\n\n    def _apply_to_column_groupbys(self, func):\n        from pandas.core.reshape.concat import concat\n        return concat(\n            (func(col_groupby) for _, col_groupby\n             in self._iterate_column_groupbys()),\n            keys=self._selected_obj.columns, axis=1)\n\n    def count(self):\n        \"\"\"\n        Compute count of group, excluding missing values.\n\n        Returns\n        -------\n        DataFrame\n            Count of values within each group.\n        \"\"\"\n        from pandas.core.dtypes.missing import _isna_ndarraylike as _isna\n\n        data, _ = self._get_data_to_aggregate()\n        ids, _, ngroups = self.grouper.group_info\n        mask = ids != -1\n\n        val = ((mask & ~_isna(np.atleast_2d(blk.get_values())))\n               for blk in data.blocks)\n        loc = (blk.mgr_locs for blk in data.blocks)\n\n        counter = partial(\n            lib.count_level_2d, labels=ids, max_bin=ngroups, axis=1)\n        blk = map(make_block, map(counter, val), loc)\n\n        return self._wrap_agged_blocks(data.items, list(blk))\n\n    def nunique(self, dropna=True):\n        \"\"\"\n        Return DataFrame with number of distinct observations per group for\n        each column.\n\n        .. versionadded:: 0.20.0\n\n        Parameters\n        ----------\n        dropna : boolean, default True\n            Don't include NaN in the counts.\n\n        Returns\n        -------\n        nunique: DataFrame\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',\n        ...                           'ham', 'ham'],\n        ...                    'value1': [1, 5, 5, 2, 5, 5],\n        ...                    'value2': list('abbaxy')})\n        >>> df\n             id  value1 value2\n        0  spam       1      a\n        1   egg       5      b\n        2   egg       5      b\n        3  spam       2      a\n        4   ham       5      x\n        5   ham       5      y\n\n        >>> df.groupby('id').nunique()\n            id  value1  value2\n        id\n        egg    1       1       1\n        ham    1       1       2\n        spam   1       2       1\n\n        Check for rows with the same id but conflicting values:\n\n        >>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())\n             id  value1 value2\n        0  spam       1      a\n        3  spam       2      a\n        4   ham       5      x\n        5   ham       5      y\n        \"\"\"\n\n        obj = self._selected_obj\n\n        def groupby_series(obj, col=None):\n            return SeriesGroupBy(obj,\n                                 selection=col,\n                                 grouper=self.grouper).nunique(dropna=dropna)\n\n        if isinstance(obj, Series):\n            results = groupby_series(obj)\n        else:\n            from pandas.core.reshape.concat import concat\n            results = [groupby_series(obj[col], col) for col in obj.columns]\n            results = concat(results, axis=1)\n            results.columns.names = obj.columns.names\n\n        if not self.as_index:\n            results.index = ibase.default_index(len(results))\n        return results\n\n    boxplot = boxplot_frame_groupby\n\n\ndef _is_multi_agg_with_relabel(**kwargs):\n    \"\"\"\n    Check whether kwargs passed to .agg look like multi-agg with relabeling.\n\n    Parameters\n    ----------\n    **kwargs : dict\n\n    Returns\n    -------\n    bool\n\n    Examples\n    --------\n    >>> _is_multi_agg_with_relabel(a='max')\n    False\n    >>> _is_multi_agg_with_relabel(a_max=('a', 'max'),\n    ...                            a_min=('a', 'min'))\n    True\n    >>> _is_multi_agg_with_relabel()\n    False\n    \"\"\"\n    return all(\n        isinstance(v, tuple) and len(v) == 2\n        for v in kwargs.values()\n    ) and kwargs\n\n\ndef _normalize_keyword_aggregation(kwargs):\n    \"\"\"\n    Normalize user-provided \"named aggregation\" kwargs.\n\n    Transforms from the new ``Dict[str, NamedAgg]`` style kwargs\n    to the old OrderedDict[str, List[scalar]]].\n\n    Parameters\n    ----------\n    kwargs : dict\n\n    Returns\n    -------\n    aggspec : dict\n        The transformed kwargs.\n    columns : List[str]\n        The user-provided keys.\n    order : List[Tuple[str, str]]\n        Pairs of the input and output column names.\n\n    Examples\n    --------\n    >>> _normalize_keyword_aggregation({'output': ('input', 'sum')})\n    (OrderedDict([('input', ['sum'])]), ('output',), [('input', 'sum')])\n    \"\"\"\n    if not PY36:\n        kwargs = OrderedDict(sorted(kwargs.items()))\n\n    # Normalize the aggregation functions as Dict[column, List[func]],\n    # process normally, then fixup the names.\n    # TODO(Py35): When we drop python 3.5, change this to\n    # defaultdict(list)\n    aggspec = OrderedDict()  # type: typing.OrderedDict[str, List[AggScalar]]\n    order = []\n    columns, pairs = list(zip(*kwargs.items()))\n\n    for name, (column, aggfunc) in zip(columns, pairs):\n        if column in aggspec:\n            aggspec[column].append(aggfunc)\n        else:\n            aggspec[column] = [aggfunc]\n        order.append((column,\n                      com.get_callable_name(aggfunc) or aggfunc))\n    return aggspec, columns, order\n\n\ndef _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n    \"\"\"\n    If we have date/time like in the original, then coerce dates\n    as we are stacking can easily have object dtypes here.\n\n    Parameters\n    ----------\n    result : DataFrame\n\n    Returns\n    -------\n    DataFrame\n\n    Notes\n    -----\n    - Assumes Groupby._selected_obj has ndim==2 and at least one\n    datetimelike column\n    \"\"\"\n    result = result.copy()\n\n    obj_cols = [idx for idx in range(len(result.columns))\n                if is_object_dtype(result.dtypes[idx])]\n\n    # See GH#26285\n    for n in obj_cols:\n        converted = maybe_convert_objects(result.iloc[:, n].values,\n                                          convert_numeric=False)\n\n        result.iloc[:, n] = converted\n    return result\n"
    },
    {
      "filename": "pandas/core/groupby/groupby.py",
      "content": "\"\"\"\nProvide the groupby split-apply-combine paradigm. Define the GroupBy\nclass providing the base-class of operations.\n\nThe SeriesGroupBy and DataFrameGroupBy sub-class\n(defined in pandas.core.groupby.generic)\nexpose these user-facing objects to provide specific functionailty.\n\"\"\"\n\nimport collections\nfrom contextlib import contextmanager\nimport datetime\nfrom functools import partial, wraps\nimport types\nfrom typing import FrozenSet, List, Optional, Tuple, Type, Union\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config.config import option_context\n\nfrom pandas._libs import Timestamp\nimport pandas._libs.groupby as libgroupby\nfrom pandas.compat import set_function_name\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import Appender, Substitution, cache_readonly\nfrom pandas.util._validators import validate_kwargs\n\nfrom pandas.core.dtypes.cast import maybe_downcast_to_dtype\nfrom pandas.core.dtypes.common import (\n    ensure_float, is_datetime64tz_dtype, is_extension_array_dtype,\n    is_numeric_dtype, is_scalar)\nfrom pandas.core.dtypes.missing import isna, notna\n\nfrom pandas.api.types import (\n    is_datetime64_dtype, is_integer_dtype, is_object_dtype)\nimport pandas.core.algorithms as algorithms\nfrom pandas.core.arrays import Categorical\nfrom pandas.core.base import (\n    DataError, GroupByError, PandasObject, SelectionMixin, SpecificationError)\nimport pandas.core.common as com\nfrom pandas.core.frame import DataFrame\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.groupby import base\nfrom pandas.core.index import CategoricalIndex, Index, MultiIndex\nfrom pandas.core.series import Series\nfrom pandas.core.sorting import get_group_index_sorter\n\n_common_see_also = \"\"\"\n        See Also\n        --------\n        Series.%(name)s\n        DataFrame.%(name)s\n\"\"\"\n\n_apply_docs = dict(\n    template=\"\"\"\n    Apply function `func`  group-wise and combine the results together.\n\n    The function passed to `apply` must take a {input} as its first\n    argument and return a DataFrame, Series or scalar. `apply` will\n    then take care of combining the results back together into a single\n    dataframe or series. `apply` is therefore a highly flexible\n    grouping method.\n\n    While `apply` is a very flexible method, its downside is that\n    using it can be quite a bit slower than using more specific methods\n    like `agg` or `transform`. Pandas offers a wide range of method that will\n    be much faster than using `apply` for their specific purposes, so try to\n    use them before reaching for `apply`.\n\n    Parameters\n    ----------\n    func : callable\n        A callable that takes a {input} as its first argument, and\n        returns a dataframe, a series or a scalar. In addition the\n        callable may take positional and keyword arguments.\n    args, kwargs : tuple and dict\n        Optional positional and keyword arguments to pass to `func`.\n\n    Returns\n    -------\n    applied : Series or DataFrame\n\n    See Also\n    --------\n    pipe : Apply function to the full GroupBy object instead of to each\n        group.\n    aggregate : Apply aggregate function to the GroupBy object.\n    transform : Apply function column-by-column to the GroupBy object.\n    Series.apply : Apply a function to a Series.\n    DataFrame.apply : Apply a function to each row or column of a DataFrame.\n    \"\"\",\n    dataframe_examples=\"\"\"\n    >>> df = pd.DataFrame({'A': 'a a b'.split(),\n                           'B': [1,2,3],\n                           'C': [4,6, 5]})\n    >>> g = df.groupby('A')\n\n    Notice that ``g`` has two groups, ``a`` and ``b``.\n    Calling `apply` in various ways, we can get different grouping results:\n\n    Example 1: below the function passed to `apply` takes a DataFrame as\n    its argument and returns a DataFrame. `apply` combines the result for\n    each group together into a new DataFrame:\n\n    >>> g[['B', 'C']].apply(lambda x: x / x.sum())\n              B    C\n    0  0.333333  0.4\n    1  0.666667  0.6\n    2  1.000000  1.0\n\n    Example 2: The function passed to `apply` takes a DataFrame as\n    its argument and returns a Series.  `apply` combines the result for\n    each group together into a new DataFrame:\n\n    >>> g[['B', 'C']].apply(lambda x: x.max() - x.min())\n       B  C\n    A\n    a  1  2\n    b  0  0\n\n    Example 3: The function passed to `apply` takes a DataFrame as\n    its argument and returns a scalar. `apply` combines the result for\n    each group together into a Series, including setting the index as\n    appropriate:\n\n    >>> g.apply(lambda x: x.C.max() - x.B.min())\n    A\n    a    5\n    b    2\n    dtype: int64\n    \"\"\",\n    series_examples=\"\"\"\n    >>> s = pd.Series([0, 1, 2], index='a a b'.split())\n    >>> g = s.groupby(s.index)\n\n    From ``s`` above we can see that ``g`` has two groups, ``a`` and ``b``.\n    Calling `apply` in various ways, we can get different grouping results:\n\n    Example 1: The function passed to `apply` takes a Series as\n    its argument and returns a Series.  `apply` combines the result for\n    each group together into a new Series:\n\n    >>> g.apply(lambda x:  x*2 if x.name == 'b' else x/2)\n    0    0.0\n    1    0.5\n    2    4.0\n    dtype: float64\n\n    Example 2: The function passed to `apply` takes a Series as\n    its argument and returns a scalar. `apply` combines the result for\n    each group together into a Series, including setting the index as\n    appropriate:\n\n    >>> g.apply(lambda x: x.max() - x.min())\n    a    1\n    b    0\n    dtype: int64\n\n    Notes\n    -----\n    In the current implementation `apply` calls `func` twice on the\n    first group to decide whether it can take a fast or slow code\n    path. This can lead to unexpected behavior if `func` has\n    side-effects, as they will take effect twice for the first\n    group.\n\n    Examples\n    --------\n    {examples}\n    \"\"\")\n\n_pipe_template = \"\"\"\nApply a function `func` with arguments to this %(klass)s object and return\nthe function's result.\n\n%(versionadded)s\n\nUse `.pipe` when you want to improve readability by chaining together\nfunctions that expect Series, DataFrames, GroupBy or Resampler objects.\nInstead of writing\n\n>>> h(g(f(df.groupby('group')), arg1=a), arg2=b, arg3=c)\n\nYou can write\n\n>>> (df.groupby('group')\n...    .pipe(f)\n...    .pipe(g, arg1=a)\n...    .pipe(h, arg2=b, arg3=c))\n\nwhich is much more readable.\n\nParameters\n----------\nfunc : callable or tuple of (callable, string)\n    Function to apply to this %(klass)s object or, alternatively,\n    a `(callable, data_keyword)` tuple where `data_keyword` is a\n    string indicating the keyword of `callable` that expects the\n    %(klass)s object.\nargs : iterable, optional\n       positional arguments passed into `func`.\nkwargs : dict, optional\n         a dictionary of keyword arguments passed into `func`.\n\nReturns\n-------\nobject : the return type of `func`.\n\nSee Also\n--------\nSeries.pipe : Apply a function with arguments to a series.\nDataFrame.pipe: Apply a function with arguments to a dataframe.\napply : Apply function to each group instead of to the\n    full %(klass)s object.\n\nNotes\n-----\nSee more `here\n<http://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#piping-function-calls>`_\n\nExamples\n--------\n%(examples)s\n\"\"\"\n\n_transform_template = \"\"\"\nCall function producing a like-indexed %(klass)s on each group and\nreturn a %(klass)s having the same indexes as the original object\nfilled with the transformed values\n\nParameters\n----------\nf : function\n    Function to apply to each group\n\nReturns\n-------\n%(klass)s\n\nSee Also\n--------\naggregate, transform\n\nNotes\n-----\nEach group is endowed the attribute 'name' in case you need to know\nwhich group you are working on.\n\nThe current implementation imposes three requirements on f:\n\n* f must return a value that either has the same shape as the input\n  subframe or can be broadcast to the shape of the input subframe.\n  For example, f returns a scalar it will be broadcast to have the\n  same shape as the input subframe.\n* if this is a DataFrame, f must support application column-by-column\n  in the subframe. If f also supports application to the entire subframe,\n  then a fast path is used starting from the second chunk.\n* f must not mutate groups. Mutation is not supported and may\n  produce unexpected results.\n\nExamples\n--------\n\n# Same shape\n>>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n...                           'foo', 'bar'],\n...                    'B' : ['one', 'one', 'two', 'three',\n...                          'two', 'two'],\n...                    'C' : [1, 5, 5, 2, 5, 5],\n...                    'D' : [2.0, 5., 8., 1., 2., 9.]})\n>>> grouped = df.groupby('A')\n>>> grouped.transform(lambda x: (x - x.mean()) / x.std())\n          C         D\n0 -1.154701 -0.577350\n1  0.577350  0.000000\n2  0.577350  1.154701\n3 -1.154701 -1.000000\n4  0.577350 -0.577350\n5  0.577350  1.000000\n\n# Broadcastable\n>>> grouped.transform(lambda x: x.max() - x.min())\n   C    D\n0  4  6.0\n1  3  8.0\n2  4  6.0\n3  3  8.0\n4  4  6.0\n5  3  8.0\n\"\"\"\n\n\nclass GroupByPlot(PandasObject):\n    \"\"\"\n    Class implementing the .plot attribute for groupby objects.\n    \"\"\"\n\n    def __init__(self, groupby):\n        self._groupby = groupby\n\n    def __call__(self, *args, **kwargs):\n        def f(self):\n            return self.plot(*args, **kwargs)\n        f.__name__ = 'plot'\n        return self._groupby.apply(f)\n\n    def __getattr__(self, name):\n        def attr(*args, **kwargs):\n            def f(self):\n                return getattr(self.plot, name)(*args, **kwargs)\n            return self._groupby.apply(f)\n        return attr\n\n\n@contextmanager\ndef _group_selection_context(groupby):\n    \"\"\"\n    Set / reset the _group_selection_context.\n    \"\"\"\n    groupby._set_group_selection()\n    yield groupby\n    groupby._reset_group_selection()\n\n\nclass _GroupBy(PandasObject, SelectionMixin):\n    _group_selection = None\n    _apply_whitelist = frozenset()  # type: FrozenSet[str]\n\n    def __init__(self, obj, keys=None, axis=0, level=None,\n                 grouper=None, exclusions=None, selection=None, as_index=True,\n                 sort=True, group_keys=True, squeeze=False,\n                 observed=False, **kwargs):\n\n        self._selection = selection\n\n        if isinstance(obj, NDFrame):\n            obj._consolidate_inplace()\n\n        self.level = level\n\n        if not as_index:\n            if not isinstance(obj, DataFrame):\n                raise TypeError('as_index=False only valid with DataFrame')\n            if axis != 0:\n                raise ValueError('as_index=False only valid for axis=0')\n\n        self.as_index = as_index\n        self.keys = keys\n        self.sort = sort\n        self.group_keys = group_keys\n        self.squeeze = squeeze\n        self.observed = observed\n        self.mutated = kwargs.pop('mutated', False)\n\n        if grouper is None:\n            from pandas.core.groupby.grouper import _get_grouper\n            grouper, exclusions, obj = _get_grouper(obj, keys,\n                                                    axis=axis,\n                                                    level=level,\n                                                    sort=sort,\n                                                    observed=observed,\n                                                    mutated=self.mutated)\n\n        self.obj = obj\n        self.axis = obj._get_axis_number(axis)\n        self.grouper = grouper\n        self.exclusions = set(exclusions) if exclusions else set()\n\n        # we accept no other args\n        validate_kwargs('group', kwargs, {})\n\n    def __len__(self):\n        return len(self.groups)\n\n    def __repr__(self):\n        # TODO: Better repr for GroupBy object\n        return object.__repr__(self)\n\n    def _assure_grouper(self):\n        \"\"\"\n        We create the grouper on instantiation sub-classes may have a\n        different policy.\n        \"\"\"\n        pass\n\n    @property\n    def groups(self):\n        \"\"\"\n        Dict {group name -> group labels}.\n        \"\"\"\n        self._assure_grouper()\n        return self.grouper.groups\n\n    @property\n    def ngroups(self):\n        self._assure_grouper()\n        return self.grouper.ngroups\n\n    @property\n    def indices(self):\n        \"\"\"\n        Dict {group name -> group indices}.\n        \"\"\"\n        self._assure_grouper()\n        return self.grouper.indices\n\n    def _get_indices(self, names):\n        \"\"\"\n        Safe get multiple indices, translate keys for\n        datelike to underlying repr.\n        \"\"\"\n\n        def get_converter(s):\n            # possibly convert to the actual key types\n            # in the indices, could be a Timestamp or a np.datetime64\n            if isinstance(s, (Timestamp, datetime.datetime)):\n                return lambda key: Timestamp(key)\n            elif isinstance(s, np.datetime64):\n                return lambda key: Timestamp(key).asm8\n            else:\n                return lambda key: key\n\n        if len(names) == 0:\n            return []\n\n        if len(self.indices) > 0:\n            index_sample = next(iter(self.indices))\n        else:\n            index_sample = None     # Dummy sample\n\n        name_sample = names[0]\n        if isinstance(index_sample, tuple):\n            if not isinstance(name_sample, tuple):\n                msg = (\"must supply a tuple to get_group with multiple\"\n                       \" grouping keys\")\n                raise ValueError(msg)\n            if not len(name_sample) == len(index_sample):\n                try:\n                    # If the original grouper was a tuple\n                    return [self.indices[name] for name in names]\n                except KeyError:\n                    # turns out it wasn't a tuple\n                    msg = (\"must supply a same-length tuple to get_group\"\n                           \" with multiple grouping keys\")\n                    raise ValueError(msg)\n\n            converters = [get_converter(s) for s in index_sample]\n            names = (tuple(f(n) for f, n in zip(converters, name))\n                     for name in names)\n\n        else:\n            converter = get_converter(index_sample)\n            names = (converter(name) for name in names)\n\n        return [self.indices.get(name, []) for name in names]\n\n    def _get_index(self, name):\n        \"\"\"\n        Safe get index, translate keys for datelike to underlying repr.\n        \"\"\"\n        return self._get_indices([name])[0]\n\n    @cache_readonly\n    def _selected_obj(self):\n\n        if self._selection is None or isinstance(self.obj, Series):\n            if self._group_selection is not None:\n                return self.obj[self._group_selection]\n            return self.obj\n        else:\n            return self.obj[self._selection]\n\n    def _reset_group_selection(self):\n        \"\"\"\n        Clear group based selection.\n\n        Used for methods needing to return info on each group regardless of\n        whether a group selection was previously set.\n        \"\"\"\n        if self._group_selection is not None:\n            # GH12839 clear cached selection too when changing group selection\n            self._group_selection = None\n            self._reset_cache('_selected_obj')\n\n    def _set_group_selection(self):\n        \"\"\"\n        Create group based selection.\n\n        Used when selection is not passed directly but instead via a grouper.\n\n        NOTE: this should be paired with a call to _reset_group_selection\n        \"\"\"\n        grp = self.grouper\n        if not (self.as_index and\n                getattr(grp, 'groupings', None) is not None and\n                self.obj.ndim > 1 and\n                self._group_selection is None):\n            return\n\n        ax = self.obj._info_axis\n        groupers = [g.name for g in grp.groupings\n                    if g.level is None and g.in_axis]\n\n        if len(groupers):\n            # GH12839 clear selected obj cache when group selection changes\n            self._group_selection = ax.difference(Index(groupers),\n                                                  sort=False).tolist()\n            self._reset_cache('_selected_obj')\n\n    def _set_result_index_ordered(self, result):\n        # set the result index on the passed values object and\n        # return the new object, xref 8046\n\n        # the values/counts are repeated according to the group index\n        # shortcut if we have an already ordered grouper\n        if not self.grouper.is_monotonic:\n            index = Index(np.concatenate(\n                self._get_indices(self.grouper.result_index)))\n            result.set_axis(index, axis=self.axis, inplace=True)\n            result = result.sort_index(axis=self.axis)\n\n        result.set_axis(self.obj._get_axis(self.axis), axis=self.axis,\n                        inplace=True)\n        return result\n\n    def _dir_additions(self):\n        return self.obj._dir_additions() | self._apply_whitelist\n\n    def __getattr__(self, attr):\n        if attr in self._internal_names_set:\n            return object.__getattribute__(self, attr)\n        if attr in self.obj:\n            return self[attr]\n        if hasattr(self.obj, attr):\n            return self._make_wrapper(attr)\n\n        raise AttributeError(\"%r object has no attribute %r\" %\n                             (type(self).__name__, attr))\n\n    @Substitution(klass='GroupBy',\n                  versionadded='.. versionadded:: 0.21.0',\n                  examples=\"\"\"\\\n>>> df = pd.DataFrame({'A': 'a b a b'.split(), 'B': [1, 2, 3, 4]})\n>>> df\n   A  B\n0  a  1\n1  b  2\n2  a  3\n3  b  4\n\nTo get the difference between each groups maximum and minimum value in one\npass, you can do\n\n>>> df.groupby('A').pipe(lambda x: x.max() - x.min())\n   B\nA\na  2\nb  2\"\"\")\n    @Appender(_pipe_template)\n    def pipe(self, func, *args, **kwargs):\n        return com._pipe(self, func, *args, **kwargs)\n\n    plot = property(GroupByPlot)\n\n    def _make_wrapper(self, name):\n        if name not in self._apply_whitelist:\n            is_callable = callable(getattr(self._selected_obj, name, None))\n            kind = ' callable ' if is_callable else ' '\n            msg = (\"Cannot access{0}attribute {1!r} of {2!r} objects, try \"\n                   \"using the 'apply' method\".format(kind, name,\n                                                     type(self).__name__))\n            raise AttributeError(msg)\n\n        self._set_group_selection()\n\n        # need to setup the selection\n        # as are not passed directly but in the grouper\n        f = getattr(self._selected_obj, name)\n        if not isinstance(f, types.MethodType):\n            return self.apply(lambda self: getattr(self, name))\n\n        f = getattr(type(self._selected_obj), name)\n\n        def wrapper(*args, **kwargs):\n            # a little trickery for aggregation functions that need an axis\n            # argument\n            kwargs_with_axis = kwargs.copy()\n            if ('axis' not in kwargs_with_axis or\n                    kwargs_with_axis['axis'] is None):\n                kwargs_with_axis['axis'] = self.axis\n\n            def curried_with_axis(x):\n                return f(x, *args, **kwargs_with_axis)\n\n            def curried(x):\n                return f(x, *args, **kwargs)\n\n            # preserve the name so we can detect it when calling plot methods,\n            # to avoid duplicates\n            curried.__name__ = curried_with_axis.__name__ = name\n\n            # special case otherwise extra plots are created when catching the\n            # exception below\n            if name in base.plotting_methods:\n                return self.apply(curried)\n\n            try:\n                return self.apply(curried_with_axis)\n            except Exception:\n                try:\n                    return self.apply(curried)\n                except Exception:\n\n                    # related to : GH3688\n                    # try item-by-item\n                    # this can be called recursively, so need to raise\n                    # ValueError\n                    # if we don't have this method to indicated to aggregate to\n                    # mark this column as an error\n                    try:\n                        return self._aggregate_item_by_item(name,\n                                                            *args, **kwargs)\n                    except (AttributeError):\n                        raise ValueError\n\n        return wrapper\n\n    def get_group(self, name, obj=None):\n        \"\"\"\n        Construct DataFrame from group with provided name.\n\n        Parameters\n        ----------\n        name : object\n            the name of the group to get as a DataFrame\n        obj : DataFrame, default None\n            the DataFrame to take the DataFrame out of.  If\n            it is None, the object groupby was called on will\n            be used\n\n        Returns\n        -------\n        group : same type as obj\n        \"\"\"\n        if obj is None:\n            obj = self._selected_obj\n\n        inds = self._get_index(name)\n        if not len(inds):\n            raise KeyError(name)\n\n        return obj._take(inds, axis=self.axis)\n\n    def __iter__(self):\n        \"\"\"\n        Groupby iterator.\n\n        Returns\n        -------\n        Generator yielding sequence of (name, subsetted object)\n        for each group\n        \"\"\"\n        return self.grouper.get_iterator(self.obj, axis=self.axis)\n\n    @Appender(_apply_docs['template']\n              .format(input=\"dataframe\",\n                      examples=_apply_docs['dataframe_examples']))\n    def apply(self, func, *args, **kwargs):\n\n        func = self._is_builtin_func(func)\n\n        # this is needed so we don't try and wrap strings. If we could\n        # resolve functions to their callable functions prior, this\n        # wouldn't be needed\n        if args or kwargs:\n            if callable(func):\n\n                @wraps(func)\n                def f(g):\n                    with np.errstate(all='ignore'):\n                        return func(g, *args, **kwargs)\n            else:\n                raise ValueError('func must be a callable if args or '\n                                 'kwargs are supplied')\n        else:\n            f = func\n\n        # ignore SettingWithCopy here in case the user mutates\n        with option_context('mode.chained_assignment', None):\n            try:\n                result = self._python_apply_general(f)\n            except Exception:\n\n                # gh-20949\n                # try again, with .apply acting as a filtering\n                # operation, by excluding the grouping column\n                # This would normally not be triggered\n                # except if the udf is trying an operation that\n                # fails on *some* columns, e.g. a numeric operation\n                # on a string grouper column\n\n                with _group_selection_context(self):\n                    return self._python_apply_general(f)\n\n        return result\n\n    def _python_apply_general(self, f):\n        keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n                                                   self.axis)\n\n        return self._wrap_applied_output(\n            keys,\n            values,\n            not_indexed_same=mutated or self.mutated)\n\n    def _iterate_slices(self):\n        yield self._selection_name, self._selected_obj\n\n    def transform(self, func, *args, **kwargs):\n        raise AbstractMethodError(self)\n\n    def _cumcount_array(self, ascending=True):\n        \"\"\"\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Notes\n        -----\n        this is currently implementing sort=False\n        (though the default is sort=True) for groupby in general\n        \"\"\"\n        ids, _, ngroups = self.grouper.group_info\n        sorter = get_group_index_sorter(ids, ngroups)\n        ids, count = ids[sorter], len(ids)\n\n        if count == 0:\n            return np.empty(0, dtype=np.int64)\n\n        run = np.r_[True, ids[:-1] != ids[1:]]\n        rep = np.diff(np.r_[np.nonzero(run)[0], count])\n        out = (~run).cumsum()\n\n        if ascending:\n            out -= np.repeat(out[run], rep)\n        else:\n            out = np.repeat(out[np.r_[run[1:], True]], rep) - out\n\n        rev = np.empty(count, dtype=np.intp)\n        rev[sorter] = np.arange(count, dtype=np.intp)\n        return out[rev].astype(np.int64, copy=False)\n\n    def _try_cast(self, result, obj, numeric_only=False):\n        \"\"\"\n        Try to cast the result to our obj original type,\n        we may have roundtripped through object in the mean-time.\n\n        If numeric_only is True, then only try to cast numerics\n        and not datetimelikes.\n\n        \"\"\"\n        if obj.ndim > 1:\n            dtype = obj._values.dtype\n        else:\n            dtype = obj.dtype\n\n        if not is_scalar(result):\n            if is_datetime64tz_dtype(dtype):\n                # GH 23683\n                # Prior results _may_ have been generated in UTC.\n                # Ensure we localize to UTC first before converting\n                # to the target timezone\n                try:\n                    result = obj._values._from_sequence(\n                        result, dtype='datetime64[ns, UTC]'\n                    )\n                    result = result.astype(dtype)\n                except TypeError:\n                    # _try_cast was called at a point where the result\n                    # was already tz-aware\n                    pass\n            elif is_extension_array_dtype(dtype):\n                # The function can return something of any type, so check\n                # if the type is compatible with the calling EA.\n\n                # return the same type (Series) as our caller\n                try:\n                    result = obj._values._from_sequence(result, dtype=dtype)\n                except Exception:\n                    # https://github.com/pandas-dev/pandas/issues/22850\n                    # pandas has no control over what 3rd-party ExtensionArrays\n                    # do in _values_from_sequence. We still want ops to work\n                    # though, so we catch any regular Exception.\n                    pass\n            elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:\n                result = maybe_downcast_to_dtype(result, dtype)\n\n        return result\n\n    def _transform_should_cast(self, func_nm):\n        \"\"\"\n        Parameters\n        ----------\n        func_nm: str\n            The name of the aggregation function being performed\n\n        Returns\n        -------\n        bool\n            Whether transform should attempt to cast the result of aggregation\n        \"\"\"\n        return (self.size().fillna(0) > 0).any() and (\n            func_nm not in base.cython_cast_blacklist)\n\n    def _cython_transform(self, how, numeric_only=True, **kwargs):\n        output = collections.OrderedDict()\n        for name, obj in self._iterate_slices():\n            is_numeric = is_numeric_dtype(obj.dtype)\n            if numeric_only and not is_numeric:\n                continue\n\n            try:\n                result, names = self.grouper.transform(obj.values, how,\n                                                       **kwargs)\n            except NotImplementedError:\n                continue\n            except AssertionError as e:\n                raise GroupByError(str(e))\n            if self._transform_should_cast(how):\n                output[name] = self._try_cast(result, obj)\n            else:\n                output[name] = result\n\n        if len(output) == 0:\n            raise DataError('No numeric types to aggregate')\n\n        return self._wrap_transformed_output(output, names)\n\n    def _cython_agg_general(self, how, alt=None, numeric_only=True,\n                            min_count=-1):\n        output = {}\n        for name, obj in self._iterate_slices():\n            is_numeric = is_numeric_dtype(obj.dtype)\n            if numeric_only and not is_numeric:\n                continue\n\n            try:\n                result, names = self.grouper.aggregate(obj.values, how,\n                                                       min_count=min_count)\n            except AssertionError as e:\n                raise GroupByError(str(e))\n            output[name] = self._try_cast(result, obj)\n\n        if len(output) == 0:\n            raise DataError('No numeric types to aggregate')\n\n        return self._wrap_aggregated_output(output, names)\n\n    def _python_agg_general(self, func, *args, **kwargs):\n        func = self._is_builtin_func(func)\n        f = lambda x: func(x, *args, **kwargs)\n\n        # iterate through \"columns\" ex exclusions to populate output dict\n        output = {}\n        for name, obj in self._iterate_slices():\n            try:\n                result, counts = self.grouper.agg_series(obj, f)\n                output[name] = self._try_cast(result, obj, numeric_only=True)\n            except TypeError:\n                continue\n\n        if len(output) == 0:\n            return self._python_apply_general(f)\n\n        if self.grouper._filter_empty_groups:\n\n            mask = counts.ravel() > 0\n            for name, result in output.items():\n\n                # since we are masking, make sure that we have a float object\n                values = result\n                if is_numeric_dtype(values.dtype):\n                    values = ensure_float(values)\n\n                output[name] = self._try_cast(values[mask], result)\n\n        return self._wrap_aggregated_output(output)\n\n    def _wrap_applied_output(self, *args, **kwargs):\n        raise AbstractMethodError(self)\n\n    def _concat_objects(self, keys, values, not_indexed_same=False):\n        from pandas.core.reshape.concat import concat\n\n        def reset_identity(values):\n            # reset the identities of the components\n            # of the values to prevent aliasing\n            for v in com._not_none(*values):\n                ax = v._get_axis(self.axis)\n                ax._reset_identity()\n            return values\n\n        if not not_indexed_same:\n            result = concat(values, axis=self.axis)\n            ax = self._selected_obj._get_axis(self.axis)\n\n            if isinstance(result, Series):\n                result = result.reindex(ax)\n            else:\n\n                # this is a very unfortunate situation\n                # we have a multi-index that is NOT lexsorted\n                # and we have a result which is duplicated\n                # we can't reindex, so we resort to this\n                # GH 14776\n                if isinstance(ax, MultiIndex) and not ax.is_unique:\n                    indexer = algorithms.unique1d(\n                        result.index.get_indexer_for(ax.values))\n                    result = result.take(indexer, axis=self.axis)\n                else:\n                    result = result.reindex(ax, axis=self.axis)\n\n        elif self.group_keys:\n\n            values = reset_identity(values)\n            if self.as_index:\n\n                # possible MI return case\n                group_keys = keys\n                group_levels = self.grouper.levels\n                group_names = self.grouper.names\n\n                result = concat(values, axis=self.axis, keys=group_keys,\n                                levels=group_levels, names=group_names,\n                                sort=False)\n            else:\n\n                # GH5610, returns a MI, with the first level being a\n                # range index\n                keys = list(range(len(values)))\n                result = concat(values, axis=self.axis, keys=keys)\n        else:\n            values = reset_identity(values)\n            result = concat(values, axis=self.axis)\n\n        if (isinstance(result, Series) and\n                getattr(self, '_selection_name', None) is not None):\n\n            result.name = self._selection_name\n\n        return result\n\n    def _apply_filter(self, indices, dropna):\n        if len(indices) == 0:\n            indices = np.array([], dtype='int64')\n        else:\n            indices = np.sort(np.concatenate(indices))\n        if dropna:\n            filtered = self._selected_obj.take(indices, axis=self.axis)\n        else:\n            mask = np.empty(len(self._selected_obj.index), dtype=bool)\n            mask.fill(False)\n            mask[indices.astype(int)] = True\n            # mask fails to broadcast when passed to where; broadcast manually.\n            mask = np.tile(mask, list(self._selected_obj.shape[1:]) + [1]).T\n            filtered = self._selected_obj.where(mask)  # Fill with NaNs.\n        return filtered\n\n\nclass GroupBy(_GroupBy):\n\n    \"\"\"\n    Class for grouping and aggregating relational data.\n\n    See aggregate, transform, and apply functions on this object.\n\n    It's easiest to use obj.groupby(...) to use GroupBy, but you can also do:\n\n    ::\n\n        grouped = groupby(obj, ...)\n\n    Parameters\n    ----------\n    obj : pandas object\n    axis : int, default 0\n    level : int, default None\n        Level of MultiIndex\n    groupings : list of Grouping objects\n        Most users should ignore this\n    exclusions : array-like, optional\n        List of columns to exclude\n    name : string\n        Most users should ignore this\n\n    Returns\n    -------\n    **Attributes**\n    groups : dict\n        {group name -> group labels}\n    len(grouped) : int\n        Number of groups\n\n    Notes\n    -----\n    After grouping, see aggregate, apply, and transform functions. Here are\n    some other brief notes about usage. When grouping by multiple groups, the\n    result index will be a MultiIndex (hierarchical) by default.\n\n    Iteration produces (key, group) tuples, i.e. chunking the data by group. So\n    you can write code like:\n\n    ::\n\n        grouped = obj.groupby(keys, axis=axis)\n        for key, group in grouped:\n            # do something with the data\n\n    Function calls on GroupBy, if not specially implemented, \"dispatch\" to the\n    grouped data. So if you group a DataFrame and wish to invoke the std()\n    method on each group, you can simply do:\n\n    ::\n\n        df.groupby(mapper).std()\n\n    rather than\n\n    ::\n\n        df.groupby(mapper).aggregate(np.std)\n\n    You can pass arguments to these \"wrapped\" functions, too.\n\n    See the online documentation for full exposition on these topics and much\n    more\n    \"\"\"\n    def _bool_agg(self, val_test, skipna):\n        \"\"\"\n        Shared func to call any / all Cython GroupBy implementations.\n        \"\"\"\n\n        def objs_to_bool(vals: np.ndarray) -> Tuple[np.ndarray, Type]:\n            if is_object_dtype(vals):\n                vals = np.array([bool(x) for x in vals])\n            else:\n                vals = vals.astype(np.bool)\n\n            return vals.view(np.uint8), np.bool\n\n        def result_to_bool(result: np.ndarray, inference: Type) -> np.ndarray:\n            return result.astype(inference, copy=False)\n\n        return self._get_cythonized_result('group_any_all', self.grouper,\n                                           aggregate=True,\n                                           cython_dtype=np.uint8,\n                                           needs_values=True,\n                                           needs_mask=True,\n                                           pre_processing=objs_to_bool,\n                                           post_processing=result_to_bool,\n                                           val_test=val_test, skipna=skipna)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def any(self, skipna=True):\n        \"\"\"\n        Return True if any value in the group is truthful, else False.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Flag to ignore nan values during truth testing\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return self._bool_agg('any', skipna)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def all(self, skipna=True):\n        \"\"\"\n        Return True if all values in the group are truthful, else False.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Flag to ignore nan values during truth testing\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return self._bool_agg('all', skipna)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def count(self):\n        \"\"\"\n        Compute count of group, excluding missing values.\n\n        Returns\n        -------\n        Series or DataFrame\n            Count of values within each group.\n        \"\"\"\n\n        # defined here for API doc\n        raise NotImplementedError\n\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def mean(self, *args, **kwargs):\n        \"\"\"\n        Compute mean of groups, excluding missing values.\n\n        Returns\n        -------\n        pandas.Series or pandas.DataFrame\n        %(see_also)s\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5],\n        ...                    'C': [1, 2, 1, 1, 2]}, columns=['A', 'B', 'C'])\n\n        Groupby one column and return the mean of the remaining columns in\n        each group.\n\n        >>> df.groupby('A').mean()\n             B         C\n        A\n        1  3.0  1.333333\n        2  4.0  1.500000\n\n        Groupby two columns and return the mean of the remaining column.\n\n        >>> df.groupby(['A', 'B']).mean()\n               C\n        A B\n        1 2.0  2\n          4.0  1\n        2 3.0  1\n          5.0  2\n\n        Groupby one column and return the mean of only particular column in\n        the group.\n\n        >>> df.groupby('A')['B'].mean()\n        A\n        1    3.0\n        2    4.0\n        Name: B, dtype: float64\n        \"\"\"\n        nv.validate_groupby_func('mean', args, kwargs, ['numeric_only'])\n        try:\n            return self._cython_agg_general(\n                'mean', alt=lambda x, axis: Series(x).mean(**kwargs), **kwargs)\n        except GroupByError:\n            raise\n        except Exception:  # pragma: no cover\n            with _group_selection_context(self):\n                f = lambda x: x.mean(axis=self.axis, **kwargs)\n                return self._python_agg_general(f)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def median(self, **kwargs):\n        \"\"\"\n        Compute median of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Returns\n        -------\n        Series or DataFrame\n            Median of values within each group.\n        \"\"\"\n        try:\n            return self._cython_agg_general(\n                'median',\n                alt=lambda x,\n                axis: Series(x).median(axis=axis, **kwargs),\n                **kwargs)\n        except GroupByError:\n            raise\n        except Exception:  # pragma: no cover\n\n            def f(x):\n                if isinstance(x, np.ndarray):\n                    x = Series(x)\n                return x.median(axis=self.axis, **kwargs)\n            with _group_selection_context(self):\n                return self._python_agg_general(f)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def std(self, ddof=1, *args, **kwargs):\n        \"\"\"\n        Compute standard deviation of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : integer, default 1\n            degrees of freedom\n\n        Returns\n        -------\n        Series or DataFrame\n            Standard deviation of values within each group.\n        \"\"\"\n\n        # TODO: implement at Cython level?\n        nv.validate_groupby_func('std', args, kwargs)\n        return np.sqrt(self.var(ddof=ddof, **kwargs))\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def var(self, ddof=1, *args, **kwargs):\n        \"\"\"\n        Compute variance of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : integer, default 1\n            degrees of freedom\n\n        Returns\n        -------\n        Series or DataFrame\n            Variance of values within each group.\n        \"\"\"\n        nv.validate_groupby_func('var', args, kwargs)\n        if ddof == 1:\n            try:\n                return self._cython_agg_general(\n                    'var',\n                    alt=lambda x, axis: Series(x).var(ddof=ddof, **kwargs),\n                    **kwargs)\n            except Exception:\n                f = lambda x: x.var(ddof=ddof, **kwargs)\n                with _group_selection_context(self):\n                    return self._python_agg_general(f)\n        else:\n            f = lambda x: x.var(ddof=ddof, **kwargs)\n            with _group_selection_context(self):\n                return self._python_agg_general(f)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def sem(self, ddof=1):\n        \"\"\"\n        Compute standard error of the mean of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : integer, default 1\n            degrees of freedom\n\n        Returns\n        -------\n        Series or DataFrame\n            Standard error of the mean of values within each group.\n        \"\"\"\n        return self.std(ddof=ddof) / np.sqrt(self.count())\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def size(self):\n        \"\"\"\n        Compute group sizes.\n\n        Returns\n        -------\n        Series\n            Number of rows in each group.\n        \"\"\"\n        result = self.grouper.size()\n\n        if isinstance(self.obj, Series):\n            result.name = getattr(self.obj, 'name', None)\n        return result\n\n    @classmethod\n    def _add_numeric_operations(cls):\n        \"\"\"\n        Add numeric operations to the GroupBy generically.\n        \"\"\"\n\n        def groupby_function(name, alias, npfunc,\n                             numeric_only=True,\n                             min_count=-1):\n\n            _local_template = \"\"\"\n            Compute %(f)s of group values.\n\n            Returns\n            -------\n            Series or DataFrame\n                Computed %(f)s of values within each group.\n            \"\"\"\n\n            @Substitution(name='groupby', f=name)\n            @Appender(_common_see_also)\n            @Appender(_local_template)\n            def f(self, **kwargs):\n                if 'numeric_only' not in kwargs:\n                    kwargs['numeric_only'] = numeric_only\n                if 'min_count' not in kwargs:\n                    kwargs['min_count'] = min_count\n\n                self._set_group_selection()\n\n                # try a cython aggregation if we can\n                try:\n                    return self._cython_agg_general(\n                        alias, alt=npfunc, **kwargs)\n                except AssertionError as e:\n                    raise SpecificationError(str(e))\n                except Exception:\n                    pass\n\n                # apply a non-cython aggregation\n                result = self.aggregate(\n                    lambda x: npfunc(x, axis=self.axis))\n\n                # coerce the resulting columns if we can\n                if isinstance(result, DataFrame):\n                    for col in result.columns:\n                        result[col] = self._try_cast(\n                            result[col], self.obj[col])\n                else:\n                    result = self._try_cast(\n                        result, self.obj)\n\n                return result\n\n            set_function_name(f, name, cls)\n\n            return f\n\n        def first_compat(x, axis=0):\n\n            def first(x):\n                x = x.to_numpy()\n\n                x = x[notna(x)]\n                if len(x) == 0:\n                    return np.nan\n                return x[0]\n\n            if isinstance(x, DataFrame):\n                return x.apply(first, axis=axis)\n            else:\n                return first(x)\n\n        def last_compat(x, axis=0):\n\n            def last(x):\n                x = x.to_numpy()\n                x = x[notna(x)]\n                if len(x) == 0:\n                    return np.nan\n                return x[-1]\n\n            if isinstance(x, DataFrame):\n                return x.apply(last, axis=axis)\n            else:\n                return last(x)\n\n        cls.sum = groupby_function('sum', 'add', np.sum, min_count=0)\n        cls.prod = groupby_function('prod', 'prod', np.prod, min_count=0)\n        cls.min = groupby_function('min', 'min', np.min, numeric_only=False)\n        cls.max = groupby_function('max', 'max', np.max, numeric_only=False)\n        cls.first = groupby_function('first', 'first', first_compat,\n                                     numeric_only=False)\n        cls.last = groupby_function('last', 'last', last_compat,\n                                    numeric_only=False)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def ohlc(self):\n        \"\"\"\n        Compute sum of values, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Returns\n        -------\n        DataFrame\n            Open, high, low and close values within each group.\n        \"\"\"\n\n        return self._apply_to_column_groupbys(\n            lambda x: x._cython_agg_general('ohlc'))\n\n    @Appender(DataFrame.describe.__doc__)\n    def describe(self, **kwargs):\n        with _group_selection_context(self):\n            result = self.apply(lambda x: x.describe(**kwargs))\n            if self.axis == 1:\n                return result.T\n            return result.unstack()\n\n    def resample(self, rule, *args, **kwargs):\n        \"\"\"\n        Provide resampling when using a TimeGrouper.\n\n        Given a grouper, the function resamples it according to a string\n        \"string\" -> \"frequency\".\n\n        See the :ref:`frequency aliases <timeseries.offset_aliases>`\n        documentation for more details.\n\n        Parameters\n        ----------\n        rule : str or DateOffset\n            The offset string or object representing target grouper conversion.\n        *args, **kwargs\n            Possible arguments are `how`, `fill_method`, `limit`, `kind` and\n            `on`, and other arguments of `TimeGrouper`.\n\n        Returns\n        -------\n        Grouper\n            Return a new grouper with our resampler appended.\n\n        See Also\n        --------\n        Grouper : Specify a frequency to resample with when\n            grouping by a key.\n        DatetimeIndex.resample : Frequency conversion and resampling of\n            time series.\n\n        Examples\n        --------\n        >>> idx = pd.date_range('1/1/2000', periods=4, freq='T')\n        >>> df = pd.DataFrame(data=4 * [range(2)],\n        ...                   index=idx,\n        ...                   columns=['a', 'b'])\n        >>> df.iloc[2, 0] = 5\n        >>> df\n                            a  b\n        2000-01-01 00:00:00  0  1\n        2000-01-01 00:01:00  0  1\n        2000-01-01 00:02:00  5  1\n        2000-01-01 00:03:00  0  1\n\n        Downsample the DataFrame into 3 minute bins and sum the values of\n        the timestamps falling into a bin.\n\n        >>> df.groupby('a').resample('3T').sum()\n                                 a  b\n        a\n        0   2000-01-01 00:00:00  0  2\n            2000-01-01 00:03:00  0  1\n        5   2000-01-01 00:00:00  5  1\n\n        Upsample the series into 30 second bins.\n\n        >>> df.groupby('a').resample('30S').sum()\n                            a  b\n        a\n        0   2000-01-01 00:00:00  0  1\n            2000-01-01 00:00:30  0  0\n            2000-01-01 00:01:00  0  1\n            2000-01-01 00:01:30  0  0\n            2000-01-01 00:02:00  0  0\n            2000-01-01 00:02:30  0  0\n            2000-01-01 00:03:00  0  1\n        5   2000-01-01 00:02:00  5  1\n\n        Resample by month. Values are assigned to the month of the period.\n\n        >>> df.groupby('a').resample('M').sum()\n                    a  b\n        a\n        0   2000-01-31  0  3\n        5   2000-01-31  5  1\n\n        Downsample the series into 3 minute bins as above, but close the right\n        side of the bin interval.\n\n        >>> df.groupby('a').resample('3T', closed='right').sum()\n                                 a  b\n        a\n        0   1999-12-31 23:57:00  0  1\n            2000-01-01 00:00:00  0  2\n        5   2000-01-01 00:00:00  5  1\n\n        Downsample the series into 3 minute bins and close the right side of\n        the bin interval, but label each bin using the right edge instead of\n        the left.\n\n        >>> df.groupby('a').resample('3T', closed='right', label='right').sum()\n                                 a  b\n        a\n        0   2000-01-01 00:00:00  0  1\n            2000-01-01 00:03:00  0  2\n        5   2000-01-01 00:03:00  5  1\n\n        Add an offset of twenty seconds.\n\n        >>> df.groupby('a').resample('3T', loffset='20s').sum()\n                               a  b\n        a\n        0   2000-01-01 00:00:20  0  2\n            2000-01-01 00:03:20  0  1\n        5   2000-01-01 00:00:20  5  1\n        \"\"\"\n        from pandas.core.resample import get_resampler_for_grouping\n        return get_resampler_for_grouping(self, rule, *args, **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def rolling(self, *args, **kwargs):\n        \"\"\"\n        Return a rolling grouper, providing rolling functionality per group.\n        \"\"\"\n        from pandas.core.window import RollingGroupby\n        return RollingGroupby(self, *args, **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def expanding(self, *args, **kwargs):\n        \"\"\"\n        Return an expanding grouper, providing expanding\n        functionality per group.\n        \"\"\"\n        from pandas.core.window import ExpandingGroupby\n        return ExpandingGroupby(self, *args, **kwargs)\n\n    def _fill(self, direction, limit=None):\n        \"\"\"\n        Shared function for `pad` and `backfill` to call Cython method.\n\n        Parameters\n        ----------\n        direction : {'ffill', 'bfill'}\n            Direction passed to underlying Cython function. `bfill` will cause\n            values to be filled backwards. `ffill` and any other values will\n            default to a forward fill\n        limit : int, default None\n            Maximum number of consecutive values to fill. If `None`, this\n            method will convert to -1 prior to passing to Cython\n\n        Returns\n        -------\n        `Series` or `DataFrame` with filled values\n\n        See Also\n        --------\n        pad\n        backfill\n        \"\"\"\n        # Need int value for Cython\n        if limit is None:\n            limit = -1\n\n        return self._get_cythonized_result('group_fillna_indexer',\n                                           self.grouper, needs_mask=True,\n                                           cython_dtype=np.int64,\n                                           result_is_index=True,\n                                           direction=direction, limit=limit)\n\n    @Substitution(name='groupby')\n    def pad(self, limit=None):\n        \"\"\"\n        Forward fill the values.\n\n        Parameters\n        ----------\n        limit : integer, optional\n            limit of how many values to fill\n\n        Returns\n        -------\n        Series or DataFrame\n            Object with missing values filled.\n\n        See Also\n        --------\n        Series.pad\n        DataFrame.pad\n        Series.fillna\n        DataFrame.fillna\n        \"\"\"\n        return self._fill('ffill', limit=limit)\n    ffill = pad\n\n    @Substitution(name='groupby')\n    def backfill(self, limit=None):\n        \"\"\"\n        Backward fill the values.\n\n        Parameters\n        ----------\n        limit : integer, optional\n            limit of how many values to fill\n\n        Returns\n        -------\n        Series or DataFrame\n            Object with missing values filled.\n\n        See Also\n        --------\n        Series.backfill\n        DataFrame.backfill\n        Series.fillna\n        DataFrame.fillna\n        \"\"\"\n        return self._fill('bfill', limit=limit)\n    bfill = backfill\n\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def nth(self,\n            n: Union[int, List[int]],\n            dropna: Optional[str] = None) -> DataFrame:\n        \"\"\"\n        Take the nth row from each group if n is an int, or a subset of rows\n        if n is a list of ints.\n\n        If dropna, will take the nth non-null row, dropna is either\n        'all' or 'any'; this is equivalent to calling dropna(how=dropna)\n        before the groupby.\n\n        Parameters\n        ----------\n        n : int or list of ints\n            a single nth value for the row or a list of nth values\n        dropna : None or str, optional\n            apply the specified dropna operation before counting which row is\n            the nth row. Needs to be None, 'any' or 'all'\n\n        Returns\n        -------\n        Series or DataFrame\n            N-th value within each group.\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5]}, columns=['A', 'B'])\n        >>> g = df.groupby('A')\n        >>> g.nth(0)\n             B\n        A\n        1  NaN\n        2  3.0\n        >>> g.nth(1)\n             B\n        A\n        1  2.0\n        2  5.0\n        >>> g.nth(-1)\n             B\n        A\n        1  4.0\n        2  5.0\n        >>> g.nth([0, 1])\n             B\n        A\n        1  NaN\n        1  2.0\n        2  3.0\n        2  5.0\n\n        Specifying `dropna` allows count ignoring ``NaN``\n\n        >>> g.nth(0, dropna='any')\n             B\n        A\n        1  2.0\n        2  3.0\n\n        NaNs denote group exhausted when using dropna\n\n        >>> g.nth(3, dropna='any')\n            B\n        A\n        1 NaN\n        2 NaN\n\n        Specifying `as_index=False` in `groupby` keeps the original index.\n\n        >>> df.groupby('A', as_index=False).nth(1)\n           A    B\n        1  1  2.0\n        4  2  5.0\n        \"\"\"\n\n        valid_containers = (set, list, tuple)\n        if not isinstance(n, (valid_containers, int)):\n            raise TypeError(\"n needs to be an int or a list/set/tuple of ints\")\n\n        if not dropna:\n\n            if isinstance(n, int):\n                nth_values = [n]\n            elif isinstance(n, valid_containers):\n                nth_values = list(set(n))\n\n            nth_array = np.array(nth_values, dtype=np.intp)\n            self._set_group_selection()\n\n            mask_left = np.in1d(self._cumcount_array(), nth_array)\n            mask_right = np.in1d(self._cumcount_array(ascending=False) + 1,\n                                 -nth_array)\n            mask = mask_left | mask_right\n\n            ids, _, _ = self.grouper.group_info\n\n            # Drop NA values in grouping\n            mask = mask & (ids != -1)\n\n            out = self._selected_obj[mask]\n            if not self.as_index:\n                return out\n\n            out.index = self.grouper.result_index[ids[mask]]\n\n            return out.sort_index() if self.sort else out\n\n        # dropna is truthy\n        if isinstance(n, valid_containers):\n            raise ValueError(\n                \"dropna option with a list of nth values is not supported\")\n\n        if dropna not in ['any', 'all']:\n            if isinstance(self._selected_obj, Series) and dropna is True:\n                warnings.warn(\"the dropna={dropna} keyword is deprecated,\"\n                              \"use dropna='all' instead. \"\n                              \"For a Series groupby, dropna must be \"\n                              \"either None, 'any' or 'all'.\".format(\n                                  dropna=dropna),\n                              FutureWarning,\n                              stacklevel=2)\n                dropna = 'all'\n            else:\n                # Note: when agg-ing picker doesn't raise this,\n                # just returns NaN\n                raise ValueError(\"For a DataFrame groupby, dropna must be \"\n                                 \"either None, 'any' or 'all', \"\n                                 \"(was passed {dropna}).\".format(\n                                     dropna=dropna))\n\n        # old behaviour, but with all and any support for DataFrames.\n        # modified in GH 7559 to have better perf\n        max_len = n if n >= 0 else - 1 - n\n        dropped = self.obj.dropna(how=dropna, axis=self.axis)\n\n        # get a new grouper for our dropped obj\n        if self.keys is None and self.level is None:\n\n            # we don't have the grouper info available\n            # (e.g. we have selected out\n            # a column that is not in the current object)\n            axis = self.grouper.axis\n            grouper = axis[axis.isin(dropped.index)]\n\n        else:\n\n            # create a grouper with the original parameters, but on dropped\n            # object\n            from pandas.core.groupby.grouper import _get_grouper\n            grouper, _, _ = _get_grouper(dropped, key=self.keys,\n                                         axis=self.axis, level=self.level,\n                                         sort=self.sort,\n                                         mutated=self.mutated)\n\n        grb = dropped.groupby(\n            grouper, as_index=self.as_index, sort=self.sort)\n        sizes, result = grb.size(), grb.nth(n)\n        mask = (sizes < max_len).values\n\n        # set the results which don't meet the criteria\n        if len(result) and mask.any():\n            result.loc[mask] = np.nan\n\n        # reset/reindex to the original groups\n        if (len(self.obj) == len(dropped) or\n                len(result) == len(self.grouper.result_index)):\n            result.index = self.grouper.result_index\n        else:\n            result = result.reindex(self.grouper.result_index)\n\n        return result\n\n    def quantile(self, q=0.5, interpolation='linear'):\n        \"\"\"\n        Return group values at the given quantile, a la numpy.percentile.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            Value(s) between 0 and 1 providing the quantile(s) to compute.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            Method to use when the desired quantile falls between two points.\n\n        Returns\n        -------\n        Series or DataFrame\n            Return type determined by caller of GroupBy object.\n\n        See Also\n        --------\n        Series.quantile : Similar method for Series.\n        DataFrame.quantile : Similar method for DataFrame.\n        numpy.percentile : NumPy method to compute qth percentile.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([\n        ...     ['a', 1], ['a', 2], ['a', 3],\n        ...     ['b', 1], ['b', 3], ['b', 5]\n        ... ], columns=['key', 'val'])\n        >>> df.groupby('key').quantile()\n            val\n        key\n        a    2.0\n        b    3.0\n        \"\"\"\n\n        def pre_processor(\n                vals: np.ndarray\n        ) -> Tuple[np.ndarray, Optional[Type]]:\n            if is_object_dtype(vals):\n                raise TypeError(\"'quantile' cannot be performed against \"\n                                \"'object' dtypes!\")\n\n            inference = None\n            if is_integer_dtype(vals):\n                inference = np.int64\n            elif is_datetime64_dtype(vals):\n                inference = 'datetime64[ns]'\n                vals = vals.astype(np.float)\n\n            return vals, inference\n\n        def post_processor(\n                vals: np.ndarray,\n                inference: Optional[Type]\n        ) -> np.ndarray:\n            if inference:\n                # Check for edge case\n                if not (is_integer_dtype(inference) and\n                        interpolation in {'linear', 'midpoint'}):\n                    vals = vals.astype(inference)\n\n            return vals\n\n        return self._get_cythonized_result('group_quantile', self.grouper,\n                                           aggregate=True,\n                                           needs_values=True,\n                                           needs_mask=True,\n                                           cython_dtype=np.float64,\n                                           pre_processing=pre_processor,\n                                           post_processing=post_processor,\n                                           q=q, interpolation=interpolation)\n\n    @Substitution(name='groupby')\n    def ngroup(self, ascending=True):\n        \"\"\"\n        Number each group from 0 to the number of groups - 1.\n\n        This is the enumerative complement of cumcount.  Note that the\n        numbers given to the groups match the order in which the groups\n        would be seen when iterating over the groupby object, not the\n        order they are first observed.\n\n        .. versionadded:: 0.20.2\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from number of group - 1 to 0.\n\n        Returns\n        -------\n        Series\n            Unique numbers for each group.\n\n        See Also\n        --------\n        .cumcount : Number the rows in each group.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame({\"A\": list(\"aaabba\")})\n        >>> df\n           A\n        0  a\n        1  a\n        2  a\n        3  b\n        4  b\n        5  a\n        >>> df.groupby('A').ngroup()\n        0    0\n        1    0\n        2    0\n        3    1\n        4    1\n        5    0\n        dtype: int64\n        >>> df.groupby('A').ngroup(ascending=False)\n        0    1\n        1    1\n        2    1\n        3    0\n        4    0\n        5    1\n        dtype: int64\n        >>> df.groupby([\"A\", [1,1,2,3,2,1]]).ngroup()\n        0    0\n        1    0\n        2    1\n        3    3\n        4    2\n        5    0\n        dtype: int64\n        \"\"\"\n\n        with _group_selection_context(self):\n            index = self._selected_obj.index\n            result = Series(self.grouper.group_info[0], index)\n            if not ascending:\n                result = self.ngroups - 1 - result\n            return result\n\n    @Substitution(name='groupby')\n    def cumcount(self, ascending=True):\n        \"\"\"\n        Number each item in each group from 0 to the length of that group - 1.\n\n        Essentially this is equivalent to\n\n        >>> self.apply(lambda x: pd.Series(np.arange(len(x)), x.index))\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Returns\n        -------\n        Series\n            Sequence number of each element within each group.\n\n        See Also\n        --------\n        .ngroup : Number the groups themselves.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([['a'], ['a'], ['a'], ['b'], ['b'], ['a']],\n        ...                   columns=['A'])\n        >>> df\n           A\n        0  a\n        1  a\n        2  a\n        3  b\n        4  b\n        5  a\n        >>> df.groupby('A').cumcount()\n        0    0\n        1    1\n        2    2\n        3    0\n        4    1\n        5    3\n        dtype: int64\n        >>> df.groupby('A').cumcount(ascending=False)\n        0    3\n        1    2\n        2    1\n        3    1\n        4    0\n        5    0\n        dtype: int64\n        \"\"\"\n\n        with _group_selection_context(self):\n            index = self._selected_obj.index\n            cumcounts = self._cumcount_array(ascending=ascending)\n            return Series(cumcounts, index)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def rank(self, method='average', ascending=True, na_option='keep',\n             pct=False, axis=0):\n        \"\"\"\n        Provide the rank of values within each group.\n\n        Parameters\n        ----------\n        method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n            * average: average rank of group\n            * min: lowest rank in group\n            * max: highest rank in group\n            * first: ranks assigned in order they appear in the array\n            * dense: like 'min', but rank always increases by 1 between groups\n        ascending : boolean, default True\n            False for ranks by high (1) to low (N)\n        na_option :  {'keep', 'top', 'bottom'}, default 'keep'\n            * keep: leave NA values where they are\n            * top: smallest rank if ascending\n            * bottom: smallest rank if descending\n        pct : boolean, default False\n            Compute percentage rank of data within each group\n        axis : int, default 0\n            The axis of the object over which to compute the rank.\n\n        Returns\n        -------\n        DataFrame with ranking of values within each group\n        \"\"\"\n        if na_option not in {'keep', 'top', 'bottom'}:\n            msg = \"na_option must be one of 'keep', 'top', or 'bottom'\"\n            raise ValueError(msg)\n        return self._cython_transform('rank', numeric_only=False,\n                                      ties_method=method, ascending=ascending,\n                                      na_option=na_option, pct=pct, axis=axis)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def cumprod(self, axis=0, *args, **kwargs):\n        \"\"\"\n        Cumulative product for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        nv.validate_groupby_func('cumprod', args, kwargs,\n                                 ['numeric_only', 'skipna'])\n        if axis != 0:\n            return self.apply(lambda x: x.cumprod(axis=axis, **kwargs))\n\n        return self._cython_transform('cumprod', **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def cumsum(self, axis=0, *args, **kwargs):\n        \"\"\"\n        Cumulative sum for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        nv.validate_groupby_func('cumsum', args, kwargs,\n                                 ['numeric_only', 'skipna'])\n        if axis != 0:\n            return self.apply(lambda x: x.cumsum(axis=axis, **kwargs))\n\n        return self._cython_transform('cumsum', **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def cummin(self, axis=0, **kwargs):\n        \"\"\"\n        Cumulative min for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        if axis != 0:\n            return self.apply(lambda x: np.minimum.accumulate(x, axis))\n\n        return self._cython_transform('cummin', numeric_only=False)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def cummax(self, axis=0, **kwargs):\n        \"\"\"\n        Cumulative max for each group.\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        if axis != 0:\n            return self.apply(lambda x: np.maximum.accumulate(x, axis))\n\n        return self._cython_transform('cummax', numeric_only=False)\n\n    def _get_cythonized_result(self, how, grouper, aggregate=False,\n                               cython_dtype=None, needs_values=False,\n                               needs_mask=False, needs_ngroups=False,\n                               result_is_index=False,\n                               pre_processing=None, post_processing=None,\n                               **kwargs):\n        \"\"\"\n        Get result for Cythonized functions.\n\n        Parameters\n        ----------\n        how : str, Cythonized function name to be called\n        grouper : Grouper object containing pertinent group info\n        aggregate : bool, default False\n            Whether the result should be aggregated to match the number of\n            groups\n        cython_dtype : default None\n            Type of the array that will be modified by the Cython call. If\n            `None`, the type will be inferred from the values of each slice\n        needs_values : bool, default False\n            Whether the values should be a part of the Cython call\n            signature\n        needs_mask : bool, default False\n            Whether boolean mask needs to be part of the Cython call\n            signature\n        needs_ngroups : bool, default False\n            Whether number of groups is part of the Cython call signature\n        result_is_index : bool, default False\n            Whether the result of the Cython operation is an index of\n            values to be retrieved, instead of the actual values themselves\n        pre_processing : function, default None\n            Function to be applied to `values` prior to passing to Cython.\n            Function should return a tuple where the first element is the\n            values to be passed to Cython and the second element is an optional\n            type which the values should be converted to after being returned\n            by the Cython operation. Raises if `needs_values` is False.\n        post_processing : function, default None\n            Function to be applied to result of Cython function. Should accept\n            an array of values as the first argument and type inferences as its\n            second argument, i.e. the signature should be\n            (ndarray, Type).\n        **kwargs : dict\n            Extra arguments to be passed back to Cython funcs\n\n        Returns\n        -------\n        `Series` or `DataFrame`  with filled values\n        \"\"\"\n        if result_is_index and aggregate:\n            raise ValueError(\"'result_is_index' and 'aggregate' cannot both \"\n                             \"be True!\")\n        if post_processing:\n            if not callable(pre_processing):\n                raise ValueError(\"'post_processing' must be a callable!\")\n        if pre_processing:\n            if not callable(pre_processing):\n                raise ValueError(\"'pre_processing' must be a callable!\")\n            if not needs_values:\n                raise ValueError(\"Cannot use 'pre_processing' without \"\n                                 \"specifying 'needs_values'!\")\n\n        labels, _, ngroups = grouper.group_info\n        output = collections.OrderedDict()\n        base_func = getattr(libgroupby, how)\n\n        for name, obj in self._iterate_slices():\n            if aggregate:\n                result_sz = ngroups\n            else:\n                result_sz = len(obj.values)\n\n            if not cython_dtype:\n                cython_dtype = obj.values.dtype\n\n            result = np.zeros(result_sz, dtype=cython_dtype)\n            func = partial(base_func, result, labels)\n            inferences = None\n\n            if needs_values:\n                vals = obj.values\n                if pre_processing:\n                    vals, inferences = pre_processing(vals)\n                func = partial(func, vals)\n\n            if needs_mask:\n                mask = isna(obj.values).view(np.uint8)\n                func = partial(func, mask)\n\n            if needs_ngroups:\n                func = partial(func, ngroups)\n\n            func(**kwargs)  # Call func to modify indexer values in place\n\n            if result_is_index:\n                result = algorithms.take_nd(obj.values, result)\n\n            if post_processing:\n                result = post_processing(result, inferences)\n\n            output[name] = result\n\n        if aggregate:\n            return self._wrap_aggregated_output(output)\n        else:\n            return self._wrap_transformed_output(output)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def shift(self, periods=1, freq=None, axis=0, fill_value=None):\n        \"\"\"\n        Shift each group by periods observations.\n\n        Parameters\n        ----------\n        periods : integer, default 1\n            number of periods to shift\n        freq : frequency string\n        axis : axis to shift, default 0\n        fill_value : optional\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        Series or DataFrame\n            Object shifted within each group.\n        \"\"\"\n\n        if freq is not None or axis != 0 or not isna(fill_value):\n            return self.apply(lambda x: x.shift(periods, freq,\n                                                axis, fill_value))\n\n        return self._get_cythonized_result('group_shift_indexer',\n                                           self.grouper, cython_dtype=np.int64,\n                                           needs_ngroups=True,\n                                           result_is_index=True,\n                                           periods=periods)\n\n    @Substitution(name='groupby')\n    @Appender(_common_see_also)\n    def pct_change(self, periods=1, fill_method='pad', limit=None, freq=None,\n                   axis=0):\n        \"\"\"\n        Calculate pct_change of each value to previous entry in group.\n\n        Returns\n        -------\n        Series or DataFrame\n            Percentage changes within each group.\n        \"\"\"\n        if freq is not None or axis != 0:\n            return self.apply(lambda x: x.pct_change(periods=periods,\n                                                     fill_method=fill_method,\n                                                     limit=limit, freq=freq,\n                                                     axis=axis))\n        filled = getattr(self, fill_method)(limit=limit)\n        fill_grp = filled.groupby(self.grouper.labels)\n        shifted = fill_grp.shift(periods=periods, freq=freq)\n        return (filled / shifted) - 1\n\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def head(self, n=5):\n        \"\"\"\n        Return first n rows of each group.\n\n        Essentially equivalent to ``.apply(lambda x: x.head(n))``,\n        except ignores as_index flag.\n\n        Returns\n        -------\n        Series or DataFrame\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([[1, 2], [1, 4], [5, 6]],\n        ...                   columns=['A', 'B'])\n        >>> df.groupby('A', as_index=False).head(1)\n           A  B\n        0  1  2\n        2  5  6\n        >>> df.groupby('A').head(1)\n           A  B\n        0  1  2\n        2  5  6\n        \"\"\"\n        self._reset_group_selection()\n        mask = self._cumcount_array() < n\n        return self._selected_obj[mask]\n\n    @Substitution(name='groupby')\n    @Substitution(see_also=_common_see_also)\n    def tail(self, n=5):\n        \"\"\"\n        Return last n rows of each group.\n\n        Essentially equivalent to ``.apply(lambda x: x.tail(n))``,\n        except ignores as_index flag.\n\n        Returns\n        -------\n        Series or DataFrame\n        %(see_also)s\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([['a', 1], ['a', 2], ['b', 1], ['b', 2]],\n        ...                   columns=['A', 'B'])\n        >>> df.groupby('A').tail(1)\n           A  B\n        1  a  2\n        3  b  2\n        >>> df.groupby('A').head(1)\n           A  B\n        0  a  1\n        2  b  1\n        \"\"\"\n        self._reset_group_selection()\n        mask = self._cumcount_array(ascending=False) < n\n        return self._selected_obj[mask]\n\n    def _reindex_output(self, output):\n        \"\"\"\n        If we have categorical groupers, then we might want to make sure that\n        we have a fully re-indexed output to the levels. This means expanding\n        the output space to accommodate all values in the cartesian product of\n        our groups, regardless of whether they were observed in the data or\n        not. This will expand the output space if there are missing groups.\n\n        The method returns early without modifying the input if the number of\n        groupings is less than 2, self.observed == True or none of the groupers\n        are categorical.\n\n        Parameters\n        ----------\n        output: Series or DataFrame\n            Object resulting from grouping and applying an operation.\n\n        Returns\n        -------\n        Series or DataFrame\n            Object (potentially) re-indexed to include all possible groups.\n        \"\"\"\n        groupings = self.grouper.groupings\n        if groupings is None:\n            return output\n        elif len(groupings) == 1:\n            return output\n\n        # if we only care about the observed values\n        # we are done\n        elif self.observed:\n            return output\n\n        # reindexing only applies to a Categorical grouper\n        elif not any(isinstance(ping.grouper, (Categorical, CategoricalIndex))\n                     for ping in groupings):\n            return output\n\n        levels_list = [ping.group_index for ping in groupings]\n        index, _ = MultiIndex.from_product(\n            levels_list, names=self.grouper.names).sortlevel()\n\n        if self.as_index:\n            d = {self.obj._get_axis_name(self.axis): index, 'copy': False}\n            return output.reindex(**d)\n\n        # GH 13204\n        # Here, the categorical in-axis groupers, which need to be fully\n        # expanded, are columns in `output`. An idea is to do:\n        # output = output.set_index(self.grouper.names)\n        #                .reindex(index).reset_index()\n        # but special care has to be taken because of possible not-in-axis\n        # groupers.\n        # So, we manually select and drop the in-axis grouper columns,\n        # reindex `output`, and then reset the in-axis grouper columns.\n\n        # Select in-axis groupers\n        in_axis_grps = ((i, ping.name) for (i, ping)\n                        in enumerate(groupings) if ping.in_axis)\n        g_nums, g_names = zip(*in_axis_grps)\n\n        output = output.drop(labels=list(g_names), axis=1)\n\n        # Set a temp index and reindex (possibly expanding)\n        output = output.set_index(self.grouper.result_index\n                                  ).reindex(index, copy=False)\n\n        # Reset in-axis grouper columns\n        # (using level numbers `g_nums` because level names may not be unique)\n        output = output.reset_index(level=g_nums)\n\n        return output.reset_index(drop=True)\n\n\nGroupBy._add_numeric_operations()\n\n\n@Appender(GroupBy.__doc__)\ndef groupby(obj, by, **kwds):\n    if isinstance(obj, Series):\n        from pandas.core.groupby.generic import SeriesGroupBy\n        klass = SeriesGroupBy\n    elif isinstance(obj, DataFrame):\n        from pandas.core.groupby.generic import DataFrameGroupBy\n        klass = DataFrameGroupBy\n    else:  # pragma: no cover\n        raise TypeError('invalid type: {}'.format(obj))\n\n    return klass(obj, by, **kwds)\n"
    },
    {
      "filename": "pandas/core/groupby/ops.py",
      "content": "\"\"\"\nProvide classes to perform the groupby aggregate operations.\n\nThese are not exposed to the user and provide implementations of the grouping\noperations, primarily in cython. These classes (BaseGrouper and BinGrouper)\nare contained *in* the SeriesGroupBy and DataFrameGroupBy objects.\n\"\"\"\n\nimport collections\n\nimport numpy as np\n\nfrom pandas._libs import NaT, iNaT, lib\nimport pandas._libs.groupby as libgroupby\nimport pandas._libs.reduction as reduction\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import cache_readonly\n\nfrom pandas.core.dtypes.common import (\n    ensure_float64, ensure_int64, ensure_int_or_float, ensure_object,\n    ensure_platform_int, is_bool_dtype, is_categorical_dtype, is_complex_dtype,\n    is_datetime64_any_dtype, is_integer_dtype, is_numeric_dtype, is_sparse,\n    is_timedelta64_dtype, needs_i8_conversion)\nfrom pandas.core.dtypes.missing import _maybe_fill, isna\n\nimport pandas.core.algorithms as algorithms\nfrom pandas.core.base import SelectionMixin\nimport pandas.core.common as com\nfrom pandas.core.frame import DataFrame\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.groupby import base\nfrom pandas.core.index import Index, MultiIndex, ensure_index\nfrom pandas.core.series import Series\nfrom pandas.core.sorting import (\n    compress_group_index, decons_obs_group_ids, get_flattened_iterator,\n    get_group_index, get_group_index_sorter, get_indexer_dict)\n\n\ndef generate_bins_generic(values, binner, closed):\n    \"\"\"\n    Generate bin edge offsets and bin labels for one array using another array\n    which has bin edge values. Both arrays must be sorted.\n\n    Parameters\n    ----------\n    values : array of values\n    binner : a comparable array of values representing bins into which to bin\n        the first array. Note, 'values' end-points must fall within 'binner'\n        end-points.\n    closed : which end of bin is closed; left (default), right\n\n    Returns\n    -------\n    bins : array of offsets (into 'values' argument) of bins.\n        Zero and last edge are excluded in result, so for instance the first\n        bin is values[0:bin[0]] and the last is values[bin[-1]:]\n    \"\"\"\n    lenidx = len(values)\n    lenbin = len(binner)\n\n    if lenidx <= 0 or lenbin <= 0:\n        raise ValueError(\"Invalid length for values or for binner\")\n\n    # check binner fits data\n    if values[0] < binner[0]:\n        raise ValueError(\"Values falls before first bin\")\n\n    if values[lenidx - 1] > binner[lenbin - 1]:\n        raise ValueError(\"Values falls after last bin\")\n\n    bins = np.empty(lenbin - 1, dtype=np.int64)\n\n    j = 0  # index into values\n    bc = 0  # bin count\n\n    # linear scan, presume nothing about values/binner except that it fits ok\n    for i in range(0, lenbin - 1):\n        r_bin = binner[i + 1]\n\n        # count values in current bin, advance to next bin\n        while j < lenidx and (values[j] < r_bin or\n                              (closed == 'right' and values[j] == r_bin)):\n            j += 1\n\n        bins[bc] = j\n        bc += 1\n\n    return bins\n\n\nclass BaseGrouper:\n    \"\"\"\n    This is an internal Grouper class, which actually holds\n    the generated groups\n\n    Parameters\n    ----------\n    axis : int\n        the axis to group\n    groupings : array of grouping\n        all the grouping instances to handle in this grouper\n        for example for grouper list to groupby, need to pass the list\n    sort : boolean, default True\n        whether this grouper will give sorted result or not\n    group_keys : boolean, default True\n    mutated : boolean, default False\n    indexer : intp array, optional\n        the indexer created by Grouper\n        some groupers (TimeGrouper) will sort its axis and its\n        group_info is also sorted, so need the indexer to reorder\n\n    \"\"\"\n\n    def __init__(self, axis, groupings, sort=True, group_keys=True,\n                 mutated=False, indexer=None):\n        self._filter_empty_groups = self.compressed = len(groupings) != 1\n        self.axis = axis\n        self.groupings = groupings\n        self.sort = sort\n        self.group_keys = group_keys\n        self.mutated = mutated\n        self.indexer = indexer\n\n    @property\n    def shape(self):\n        return tuple(ping.ngroups for ping in self.groupings)\n\n    def __iter__(self):\n        return iter(self.indices)\n\n    @property\n    def nkeys(self):\n        return len(self.groupings)\n\n    def get_iterator(self, data, axis=0):\n        \"\"\"\n        Groupby iterator\n\n        Returns\n        -------\n        Generator yielding sequence of (name, subsetted object)\n        for each group\n        \"\"\"\n        splitter = self._get_splitter(data, axis=axis)\n        keys = self._get_group_keys()\n        for key, (i, group) in zip(keys, splitter):\n            yield key, group\n\n    def _get_splitter(self, data, axis=0):\n        comp_ids, _, ngroups = self.group_info\n        return get_splitter(data, comp_ids, ngroups, axis=axis)\n\n    def _get_grouper(self):\n        \"\"\"\n        We are a grouper as part of another's groupings.\n\n        We have a specific method of grouping, so cannot\n        convert to a Index for our grouper.\n        \"\"\"\n        return self.groupings[0].grouper\n\n    def _get_group_keys(self):\n        if len(self.groupings) == 1:\n            return self.levels[0]\n        else:\n            comp_ids, _, ngroups = self.group_info\n\n            # provide \"flattened\" iterator for multi-group setting\n            return get_flattened_iterator(comp_ids,\n                                          ngroups,\n                                          self.levels,\n                                          self.labels)\n\n    def apply(self, f, data, axis=0):\n        mutated = self.mutated\n        splitter = self._get_splitter(data, axis=axis)\n        group_keys = self._get_group_keys()\n        result_values = None\n\n        # oh boy\n        f_name = com.get_callable_name(f)\n        if (f_name not in base.plotting_methods and\n                hasattr(splitter, 'fast_apply') and axis == 0):\n            try:\n                result_values, mutated = splitter.fast_apply(f, group_keys)\n\n                # If the fast apply path could be used we can return here.\n                # Otherwise we need to fall back to the slow implementation.\n                if len(result_values) == len(group_keys):\n                    return group_keys, result_values, mutated\n\n            except reduction.InvalidApply:\n                # Cannot fast apply on MultiIndex (_has_complex_internals).\n                # This Exception is also raised if `f` triggers an exception\n                # but it is preferable to raise the exception in Python.\n                pass\n            except Exception:\n                # raise this error to the caller\n                pass\n\n        for key, (i, group) in zip(group_keys, splitter):\n            object.__setattr__(group, 'name', key)\n\n            # result_values is None if fast apply path wasn't taken\n            # or fast apply aborted with an unexpected exception.\n            # In either case, initialize the result list and perform\n            # the slow iteration.\n            if result_values is None:\n                result_values = []\n\n            # If result_values is not None we're in the case that the\n            # fast apply loop was broken prematurely but we have\n            # already the result for the first group which we can reuse.\n            elif i == 0:\n                continue\n\n            # group might be modified\n            group_axes = _get_axes(group)\n            res = f(group)\n            if not _is_indexed_like(res, group_axes):\n                mutated = True\n            result_values.append(res)\n\n        return group_keys, result_values, mutated\n\n    @cache_readonly\n    def indices(self):\n        \"\"\" dict {group name -> group indices} \"\"\"\n        if len(self.groupings) == 1:\n            return self.groupings[0].indices\n        else:\n            label_list = [ping.labels for ping in self.groupings]\n            keys = [com.values_from_object(ping.group_index)\n                    for ping in self.groupings]\n            return get_indexer_dict(label_list, keys)\n\n    @property\n    def labels(self):\n        return [ping.labels for ping in self.groupings]\n\n    @property\n    def levels(self):\n        return [ping.group_index for ping in self.groupings]\n\n    @property\n    def names(self):\n        return [ping.name for ping in self.groupings]\n\n    def size(self):\n        \"\"\"\n        Compute group sizes\n\n        \"\"\"\n        ids, _, ngroup = self.group_info\n        ids = ensure_platform_int(ids)\n        if ngroup:\n            out = np.bincount(ids[ids != -1], minlength=ngroup)\n        else:\n            out = []\n        return Series(out,\n                      index=self.result_index,\n                      dtype='int64')\n\n    @cache_readonly\n    def groups(self):\n        \"\"\" dict {group name -> group labels} \"\"\"\n        if len(self.groupings) == 1:\n            return self.groupings[0].groups\n        else:\n            to_groupby = zip(*(ping.grouper for ping in self.groupings))\n            to_groupby = Index(to_groupby)\n            return self.axis.groupby(to_groupby)\n\n    @cache_readonly\n    def is_monotonic(self):\n        # return if my group orderings are monotonic\n        return Index(self.group_info[0]).is_monotonic\n\n    @cache_readonly\n    def group_info(self):\n        comp_ids, obs_group_ids = self._get_compressed_labels()\n\n        ngroups = len(obs_group_ids)\n        comp_ids = ensure_int64(comp_ids)\n        return comp_ids, obs_group_ids, ngroups\n\n    @cache_readonly\n    def label_info(self):\n        # return the labels of items in original grouped axis\n        labels, _, _ = self.group_info\n        if self.indexer is not None:\n            sorter = np.lexsort((labels, self.indexer))\n            labels = labels[sorter]\n        return labels\n\n    def _get_compressed_labels(self):\n        all_labels = [ping.labels for ping in self.groupings]\n        if len(all_labels) > 1:\n            group_index = get_group_index(all_labels, self.shape,\n                                          sort=True, xnull=True)\n            return compress_group_index(group_index, sort=self.sort)\n\n        ping = self.groupings[0]\n        return ping.labels, np.arange(len(ping.group_index))\n\n    @cache_readonly\n    def ngroups(self):\n        return len(self.result_index)\n\n    @property\n    def recons_labels(self):\n        comp_ids, obs_ids, _ = self.group_info\n        labels = (ping.labels for ping in self.groupings)\n        return decons_obs_group_ids(\n            comp_ids, obs_ids, self.shape, labels, xnull=True)\n\n    @cache_readonly\n    def result_index(self):\n        if not self.compressed and len(self.groupings) == 1:\n            return self.groupings[0].result_index.rename(self.names[0])\n\n        codes = self.recons_labels\n        levels = [ping.result_index for ping in self.groupings]\n        result = MultiIndex(levels=levels,\n                            codes=codes,\n                            verify_integrity=False,\n                            names=self.names)\n        return result\n\n    def get_group_levels(self):\n        if not self.compressed and len(self.groupings) == 1:\n            return [self.groupings[0].result_index]\n\n        name_list = []\n        for ping, labels in zip(self.groupings, self.recons_labels):\n            labels = ensure_platform_int(labels)\n            levels = ping.result_index.take(labels)\n\n            name_list.append(levels)\n\n        return name_list\n\n    # ------------------------------------------------------------\n    # Aggregation functions\n\n    _cython_functions = {\n        'aggregate': {\n            'add': 'group_add',\n            'prod': 'group_prod',\n            'min': 'group_min',\n            'max': 'group_max',\n            'mean': 'group_mean',\n            'median': {\n                'name': 'group_median'\n            },\n            'var': 'group_var',\n            'first': {\n                'name': 'group_nth',\n                'f': lambda func, a, b, c, d, e: func(a, b, c, d, 1, -1)\n            },\n            'last': 'group_last',\n            'ohlc': 'group_ohlc',\n        },\n\n        'transform': {\n            'cumprod': 'group_cumprod',\n            'cumsum': 'group_cumsum',\n            'cummin': 'group_cummin',\n            'cummax': 'group_cummax',\n            'rank': {\n                'name': 'group_rank',\n                'f': lambda func, a, b, c, d, e, **kwargs: func(\n                    a, b, c, e,\n                    kwargs.get('ties_method', 'average'),\n                    kwargs.get('ascending', True),\n                    kwargs.get('pct', False),\n                    kwargs.get('na_option', 'keep')\n                )\n            }\n        }\n    }\n\n    _cython_arity = {\n        'ohlc': 4,  # OHLC\n    }\n\n    _name_functions = {\n        'ohlc': lambda *args: ['open', 'high', 'low', 'close']\n    }\n\n    def _is_builtin_func(self, arg):\n        \"\"\"\n        if we define an builtin function for this argument, return it,\n        otherwise return the arg\n        \"\"\"\n        return SelectionMixin._builtin_table.get(arg, arg)\n\n    def _get_cython_function(self, kind, how, values, is_numeric):\n\n        dtype_str = values.dtype.name\n\n        def get_func(fname):\n            # see if there is a fused-type version of function\n            # only valid for numeric\n            f = getattr(libgroupby, fname, None)\n            if f is not None and is_numeric:\n                return f\n\n            # otherwise find dtype-specific version, falling back to object\n            for dt in [dtype_str, 'object']:\n                f = getattr(libgroupby, \"{fname}_{dtype_str}\".format(\n                    fname=fname, dtype_str=dt), None)\n                if f is not None:\n                    return f\n\n        ftype = self._cython_functions[kind][how]\n\n        if isinstance(ftype, dict):\n            func = afunc = get_func(ftype['name'])\n\n            # a sub-function\n            f = ftype.get('f')\n            if f is not None:\n\n                def wrapper(*args, **kwargs):\n                    return f(afunc, *args, **kwargs)\n\n                # need to curry our sub-function\n                func = wrapper\n\n        else:\n            func = get_func(ftype)\n\n        if func is None:\n            raise NotImplementedError(\n                \"function is not implemented for this dtype: \"\n                \"[how->{how},dtype->{dtype_str}]\".format(how=how,\n                                                         dtype_str=dtype_str))\n\n        return func\n\n    def _cython_operation(self, kind, values, how, axis, min_count=-1,\n                          **kwargs):\n        assert kind in ['transform', 'aggregate']\n\n        # can we do this operation with our cython functions\n        # if not raise NotImplementedError\n\n        # we raise NotImplemented if this is an invalid operation\n        # entirely, e.g. adding datetimes\n\n        # categoricals are only 1d, so we\n        # are not setup for dim transforming\n        if is_categorical_dtype(values) or is_sparse(values):\n            raise NotImplementedError(\n                \"{} are not support in cython ops\".format(values.dtype))\n        elif is_datetime64_any_dtype(values):\n            if how in ['add', 'prod', 'cumsum', 'cumprod']:\n                raise NotImplementedError(\n                    \"datetime64 type does not support {} \"\n                    \"operations\".format(how))\n        elif is_timedelta64_dtype(values):\n            if how in ['prod', 'cumprod']:\n                raise NotImplementedError(\n                    \"timedelta64 type does not support {} \"\n                    \"operations\".format(how))\n\n        arity = self._cython_arity.get(how, 1)\n\n        vdim = values.ndim\n        swapped = False\n        if vdim == 1:\n            values = values[:, None]\n            out_shape = (self.ngroups, arity)\n        else:\n            if axis > 0:\n                swapped = True\n                values = values.swapaxes(0, axis)\n            if arity > 1:\n                raise NotImplementedError(\"arity of more than 1 is not \"\n                                          \"supported for the 'how' argument\")\n            out_shape = (self.ngroups,) + values.shape[1:]\n\n        is_datetimelike = needs_i8_conversion(values.dtype)\n        is_numeric = is_numeric_dtype(values.dtype)\n\n        if is_datetimelike:\n            values = values.view('int64')\n            is_numeric = True\n        elif is_bool_dtype(values.dtype):\n            values = ensure_float64(values)\n        elif is_integer_dtype(values):\n            # we use iNaT for the missing value on ints\n            # so pre-convert to guard this condition\n            if (values == iNaT).any():\n                values = ensure_float64(values)\n            else:\n                values = ensure_int_or_float(values)\n        elif is_numeric and not is_complex_dtype(values):\n            values = ensure_float64(values)\n        else:\n            values = values.astype(object)\n\n        try:\n            func = self._get_cython_function(\n                kind, how, values, is_numeric)\n        except NotImplementedError:\n            if is_numeric:\n                values = ensure_float64(values)\n                func = self._get_cython_function(\n                    kind, how, values, is_numeric)\n            else:\n                raise\n\n        if how == 'rank':\n            out_dtype = 'float'\n        else:\n            if is_numeric:\n                out_dtype = '{kind}{itemsize}'.format(\n                    kind=values.dtype.kind, itemsize=values.dtype.itemsize)\n            else:\n                out_dtype = 'object'\n\n        labels, _, _ = self.group_info\n\n        if kind == 'aggregate':\n            result = _maybe_fill(np.empty(out_shape, dtype=out_dtype),\n                                 fill_value=np.nan)\n            counts = np.zeros(self.ngroups, dtype=np.int64)\n            result = self._aggregate(\n                result, counts, values, labels, func, is_numeric,\n                is_datetimelike, min_count)\n        elif kind == 'transform':\n            result = _maybe_fill(np.empty_like(values, dtype=out_dtype),\n                                 fill_value=np.nan)\n\n            # TODO: min_count\n            result = self._transform(\n                result, values, labels, func, is_numeric, is_datetimelike,\n                **kwargs)\n\n        if is_integer_dtype(result) and not is_datetimelike:\n            mask = result == iNaT\n            if mask.any():\n                result = result.astype('float64')\n                result[mask] = np.nan\n\n        if (kind == 'aggregate' and\n                self._filter_empty_groups and not counts.all()):\n            if result.ndim == 2:\n                try:\n                    result = lib.row_bool_subset(\n                        result, (counts > 0).view(np.uint8))\n                except ValueError:\n                    result = lib.row_bool_subset_object(\n                        ensure_object(result),\n                        (counts > 0).view(np.uint8))\n            else:\n                result = result[counts > 0]\n\n        if vdim == 1 and arity == 1:\n            result = result[:, 0]\n\n        if how in self._name_functions:\n            # TODO\n            names = self._name_functions[how]()\n        else:\n            names = None\n\n        if swapped:\n            result = result.swapaxes(0, axis)\n\n        return result, names\n\n    def aggregate(self, values, how, axis=0, min_count=-1):\n        return self._cython_operation('aggregate', values, how, axis,\n                                      min_count=min_count)\n\n    def transform(self, values, how, axis=0, **kwargs):\n        return self._cython_operation('transform', values, how, axis, **kwargs)\n\n    def _aggregate(self, result, counts, values, comp_ids, agg_func,\n                   is_numeric, is_datetimelike, min_count=-1):\n        if values.ndim > 3:\n            # punting for now\n            raise NotImplementedError(\"number of dimensions is currently \"\n                                      \"limited to 3\")\n        elif values.ndim > 2:\n            for i, chunk in enumerate(values.transpose(2, 0, 1)):\n\n                chunk = chunk.squeeze()\n                agg_func(result[:, :, i], counts, chunk, comp_ids,\n                         min_count)\n        else:\n            agg_func(result, counts, values, comp_ids, min_count)\n\n        return result\n\n    def _transform(self, result, values, comp_ids, transform_func,\n                   is_numeric, is_datetimelike, **kwargs):\n\n        comp_ids, _, ngroups = self.group_info\n        if values.ndim > 3:\n            # punting for now\n            raise NotImplementedError(\"number of dimensions is currently \"\n                                      \"limited to 3\")\n        elif values.ndim > 2:\n            for i, chunk in enumerate(values.transpose(2, 0, 1)):\n\n                transform_func(result[:, :, i], values,\n                               comp_ids, ngroups, is_datetimelike, **kwargs)\n        else:\n            transform_func(result, values, comp_ids, ngroups, is_datetimelike,\n                           **kwargs)\n\n        return result\n\n    def agg_series(self, obj, func):\n        try:\n            return self._aggregate_series_fast(obj, func)\n        except Exception:\n            return self._aggregate_series_pure_python(obj, func)\n\n    def _aggregate_series_fast(self, obj, func):\n        func = self._is_builtin_func(func)\n\n        if obj.index._has_complex_internals:\n            raise TypeError('Incompatible index for Cython grouper')\n\n        group_index, _, ngroups = self.group_info\n\n        # avoids object / Series creation overhead\n        dummy = obj._get_values(slice(None, 0))\n        indexer = get_group_index_sorter(group_index, ngroups)\n        obj = obj._take(indexer)\n        group_index = algorithms.take_nd(\n            group_index, indexer, allow_fill=False)\n        grouper = reduction.SeriesGrouper(obj, func, group_index, ngroups,\n                                          dummy)\n        result, counts = grouper.get_result()\n        return result, counts\n\n    def _aggregate_series_pure_python(self, obj, func):\n\n        group_index, _, ngroups = self.group_info\n\n        counts = np.zeros(ngroups, dtype=int)\n        result = None\n\n        splitter = get_splitter(obj, group_index, ngroups, axis=self.axis)\n\n        for label, group in splitter:\n            res = func(group)\n            if result is None:\n                if (isinstance(res, (Series, Index, np.ndarray))):\n                    raise ValueError('Function does not reduce')\n                result = np.empty(ngroups, dtype='O')\n\n            counts[label] = group.shape[0]\n            result[label] = res\n\n        result = lib.maybe_convert_objects(result, try_float=0)\n        return result, counts\n\n\nclass BinGrouper(BaseGrouper):\n\n    \"\"\"\n    This is an internal Grouper class\n\n    Parameters\n    ----------\n    bins : the split index of binlabels to group the item of axis\n    binlabels : the label list\n    filter_empty : boolean, default False\n    mutated : boolean, default False\n    indexer : a intp array\n\n    Examples\n    --------\n    bins: [2, 4, 6, 8, 10]\n    binlabels: DatetimeIndex(['2005-01-01', '2005-01-03',\n        '2005-01-05', '2005-01-07', '2005-01-09'],\n        dtype='datetime64[ns]', freq='2D')\n\n    the group_info, which contains the label of each item in grouped\n    axis, the index of label in label list, group number, is\n\n    (array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4]), array([0, 1, 2, 3, 4]), 5)\n\n    means that, the grouped axis has 10 items, can be grouped into 5\n    labels, the first and second items belong to the first label, the\n    third and forth items belong to the second label, and so on\n\n    \"\"\"\n\n    def __init__(self, bins, binlabels, filter_empty=False, mutated=False,\n                 indexer=None):\n        self.bins = ensure_int64(bins)\n        self.binlabels = ensure_index(binlabels)\n        self._filter_empty_groups = filter_empty\n        self.mutated = mutated\n        self.indexer = indexer\n\n    @cache_readonly\n    def groups(self):\n        \"\"\" dict {group name -> group labels} \"\"\"\n\n        # this is mainly for compat\n        # GH 3881\n        result = {key: value for key, value in zip(self.binlabels, self.bins)\n                  if key is not NaT}\n        return result\n\n    @property\n    def nkeys(self):\n        return 1\n\n    def _get_grouper(self):\n        \"\"\"\n        We are a grouper as part of another's groupings.\n\n        We have a specific method of grouping, so cannot\n        convert to a Index for our grouper.\n        \"\"\"\n        return self\n\n    def get_iterator(self, data, axis=0):\n        \"\"\"\n        Groupby iterator\n\n        Returns\n        -------\n        Generator yielding sequence of (name, subsetted object)\n        for each group\n        \"\"\"\n        if isinstance(data, NDFrame):\n            slicer = lambda start, edge: data._slice(\n                slice(start, edge), axis=axis)\n            length = len(data.axes[axis])\n        else:\n            slicer = lambda start, edge: data[slice(start, edge)]\n            length = len(data)\n\n        start = 0\n        for edge, label in zip(self.bins, self.binlabels):\n            if label is not NaT:\n                yield label, slicer(start, edge)\n            start = edge\n\n        if start < length:\n            yield self.binlabels[-1], slicer(start, None)\n\n    @cache_readonly\n    def indices(self):\n        indices = collections.defaultdict(list)\n\n        i = 0\n        for label, bin in zip(self.binlabels, self.bins):\n            if i < bin:\n                if label is not NaT:\n                    indices[label] = list(range(i, bin))\n                i = bin\n        return indices\n\n    @cache_readonly\n    def group_info(self):\n        ngroups = self.ngroups\n        obs_group_ids = np.arange(ngroups)\n        rep = np.diff(np.r_[0, self.bins])\n\n        rep = ensure_platform_int(rep)\n        if ngroups == len(self.bins):\n            comp_ids = np.repeat(np.arange(ngroups), rep)\n        else:\n            comp_ids = np.repeat(np.r_[-1, np.arange(ngroups)], rep)\n\n        return (comp_ids.astype('int64', copy=False),\n                obs_group_ids.astype('int64', copy=False),\n                ngroups)\n\n    @cache_readonly\n    def result_index(self):\n        if len(self.binlabels) != 0 and isna(self.binlabels[0]):\n            return self.binlabels[1:]\n\n        return self.binlabels\n\n    @property\n    def levels(self):\n        return [self.binlabels]\n\n    @property\n    def names(self):\n        return [self.binlabels.name]\n\n    @property\n    def groupings(self):\n        from pandas.core.groupby.grouper import Grouping\n        return [Grouping(lvl, lvl, in_axis=False, level=None, name=name)\n                for lvl, name in zip(self.levels, self.names)]\n\n    def agg_series(self, obj, func):\n        dummy = obj[:0]\n        grouper = reduction.SeriesBinGrouper(obj, func, self.bins, dummy)\n        return grouper.get_result()\n\n\ndef _get_axes(group):\n    if isinstance(group, Series):\n        return [group.index]\n    else:\n        return group.axes\n\n\ndef _is_indexed_like(obj, axes):\n    if isinstance(obj, Series):\n        if len(axes) > 1:\n            return False\n        return obj.index.equals(axes[0])\n    elif isinstance(obj, DataFrame):\n        return obj.index.equals(axes[0])\n\n    return False\n\n\n# ----------------------------------------------------------------------\n# Splitting / application\n\n\nclass DataSplitter:\n\n    def __init__(self, data, labels, ngroups, axis=0):\n        self.data = data\n        self.labels = ensure_int64(labels)\n        self.ngroups = ngroups\n\n        self.axis = axis\n\n    @cache_readonly\n    def slabels(self):\n        # Sorted labels\n        return algorithms.take_nd(self.labels, self.sort_idx, allow_fill=False)\n\n    @cache_readonly\n    def sort_idx(self):\n        # Counting sort indexer\n        return get_group_index_sorter(self.labels, self.ngroups)\n\n    def __iter__(self):\n        sdata = self._get_sorted_data()\n\n        if self.ngroups == 0:\n            # we are inside a generator, rather than raise StopIteration\n            # we merely return signal the end\n            return\n\n        starts, ends = lib.generate_slices(self.slabels, self.ngroups)\n\n        for i, (start, end) in enumerate(zip(starts, ends)):\n            # Since I'm now compressing the group ids, it's now not \"possible\"\n            # to produce empty slices because such groups would not be observed\n            # in the data\n            # if start >= end:\n            #     raise AssertionError('Start %s must be less than end %s'\n            #                          % (str(start), str(end)))\n            yield i, self._chop(sdata, slice(start, end))\n\n    def _get_sorted_data(self):\n        return self.data._take(self.sort_idx, axis=self.axis)\n\n    def _chop(self, sdata, slice_obj):\n        return sdata.iloc[slice_obj]\n\n    def apply(self, f):\n        raise AbstractMethodError(self)\n\n\nclass SeriesSplitter(DataSplitter):\n\n    def _chop(self, sdata, slice_obj):\n        return sdata._get_values(slice_obj)\n\n\nclass FrameSplitter(DataSplitter):\n\n    def fast_apply(self, f, names):\n        # must return keys::list, values::list, mutated::bool\n        try:\n            starts, ends = lib.generate_slices(self.slabels, self.ngroups)\n        except Exception:\n            # fails when all -1\n            return [], True\n\n        sdata = self._get_sorted_data()\n        return reduction.apply_frame_axis0(sdata, f, names, starts, ends)\n\n    def _chop(self, sdata, slice_obj):\n        if self.axis == 0:\n            return sdata.iloc[slice_obj]\n        else:\n            return sdata._slice(slice_obj, axis=1)  # .loc[:, slice_obj]\n\n\ndef get_splitter(data, *args, **kwargs):\n    if isinstance(data, Series):\n        klass = SeriesSplitter\n    elif isinstance(data, DataFrame):\n        klass = FrameSplitter\n\n    return klass(data, *args, **kwargs)\n"
    },
    {
      "filename": "pandas/core/internals/blocks.py",
      "content": "from datetime import date, datetime, timedelta\nimport functools\nimport inspect\nimport re\nfrom typing import Any, List\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import lib, tslib, tslibs\nimport pandas._libs.internals as libinternals\nfrom pandas._libs.tslibs import Timedelta, conversion, is_null_datetimelike\nfrom pandas.util._validators import validate_bool_kwarg\n\nfrom pandas.core.dtypes.cast import (\n    astype_nansafe, find_common_type, infer_dtype_from,\n    infer_dtype_from_scalar, maybe_convert_objects, maybe_downcast_to_dtype,\n    maybe_infer_dtype_type, maybe_promote, maybe_upcast, soft_convert_objects)\nfrom pandas.core.dtypes.common import (\n    _NS_DTYPE, _TD_DTYPE, ensure_platform_int, is_bool_dtype, is_categorical,\n    is_categorical_dtype, is_datetime64_dtype, is_datetime64tz_dtype,\n    is_dtype_equal, is_extension_array_dtype, is_extension_type,\n    is_float_dtype, is_integer, is_integer_dtype, is_interval_dtype,\n    is_list_like, is_numeric_v_string_like, is_object_dtype, is_period_dtype,\n    is_re, is_re_compilable, is_sparse, is_timedelta64_dtype, pandas_dtype)\nimport pandas.core.dtypes.concat as _concat\nfrom pandas.core.dtypes.dtypes import CategoricalDtype, ExtensionDtype\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame, ABCDatetimeIndex, ABCExtensionArray, ABCIndexClass,\n    ABCPandasArray, ABCSeries)\nfrom pandas.core.dtypes.missing import (\n    _isna_compat, array_equivalent, isna, notna)\n\nimport pandas.core.algorithms as algos\nfrom pandas.core.arrays import (\n    Categorical, DatetimeArray, ExtensionArray, PandasDtype, TimedeltaArray)\nfrom pandas.core.base import PandasObject\nimport pandas.core.common as com\nfrom pandas.core.indexing import check_setitem_lengths\nfrom pandas.core.internals.arrays import extract_array\nimport pandas.core.missing as missing\nfrom pandas.core.nanops import nanpercentile\n\nfrom pandas.io.formats.printing import pprint_thing\n\n\nclass Block(PandasObject):\n    \"\"\"\n    Canonical n-dimensional unit of homogeneous dtype contained in a pandas\n    data structure\n\n    Index-ignorant; let the container take care of that\n    \"\"\"\n    __slots__ = ['_mgr_locs', 'values', 'ndim']\n    is_numeric = False\n    is_float = False\n    is_integer = False\n    is_complex = False\n    is_datetime = False\n    is_datetimetz = False\n    is_timedelta = False\n    is_bool = False\n    is_object = False\n    is_categorical = False\n    is_extension = False\n    _can_hold_na = False\n    _can_consolidate = True\n    _verify_integrity = True\n    _validate_ndim = True\n    _ftype = 'dense'\n    _concatenator = staticmethod(np.concatenate)\n\n    def __init__(self, values, placement, ndim=None):\n        self.ndim = self._check_ndim(values, ndim)\n        self.mgr_locs = placement\n        self.values = values\n\n        if (self._validate_ndim and self.ndim and\n                len(self.mgr_locs) != len(self.values)):\n            raise ValueError(\n                'Wrong number of items passed {val}, placement implies '\n                '{mgr}'.format(val=len(self.values), mgr=len(self.mgr_locs)))\n\n    def _check_ndim(self, values, ndim):\n        \"\"\"\n        ndim inference and validation.\n\n        Infers ndim from 'values' if not provided to __init__.\n        Validates that values.ndim and ndim are consistent if and only if\n        the class variable '_validate_ndim' is True.\n\n        Parameters\n        ----------\n        values : array-like\n        ndim : int or None\n\n        Returns\n        -------\n        ndim : int\n\n        Raises\n        ------\n        ValueError : the number of dimensions do not match\n        \"\"\"\n        if ndim is None:\n            ndim = values.ndim\n\n        if self._validate_ndim and values.ndim != ndim:\n            msg = (\"Wrong number of dimensions. values.ndim != ndim \"\n                   \"[{} != {}]\")\n            raise ValueError(msg.format(values.ndim, ndim))\n\n        return ndim\n\n    @property\n    def _holder(self):\n        \"\"\"The array-like that can hold the underlying values.\n\n        None for 'Block', overridden by subclasses that don't\n        use an ndarray.\n        \"\"\"\n        return None\n\n    @property\n    def _consolidate_key(self):\n        return (self._can_consolidate, self.dtype.name)\n\n    @property\n    def _is_single_block(self):\n        return self.ndim == 1\n\n    @property\n    def is_view(self):\n        \"\"\" return a boolean if I am possibly a view \"\"\"\n        return self.values.base is not None\n\n    @property\n    def is_datelike(self):\n        \"\"\" return True if I am a non-datelike \"\"\"\n        return self.is_datetime or self.is_timedelta\n\n    def is_categorical_astype(self, dtype):\n        \"\"\"\n        validate that we have a astypeable to categorical,\n        returns a boolean if we are a categorical\n        \"\"\"\n        if dtype is Categorical or dtype is CategoricalDtype:\n            # this is a pd.Categorical, but is not\n            # a valid type for astypeing\n            raise TypeError(\"invalid type {0} for astype\".format(dtype))\n\n        elif is_categorical_dtype(dtype):\n            return True\n\n        return False\n\n    def external_values(self, dtype=None):\n        \"\"\" return an outside world format, currently just the ndarray \"\"\"\n        return self.values\n\n    def internal_values(self, dtype=None):\n        \"\"\" return an internal format, currently just the ndarray\n        this should be the pure internal API format\n        \"\"\"\n        return self.values\n\n    def formatting_values(self):\n        \"\"\"Return the internal values used by the DataFrame/SeriesFormatter\"\"\"\n        return self.internal_values()\n\n    def get_values(self, dtype=None):\n        \"\"\"\n        return an internal format, currently just the ndarray\n        this is often overridden to handle to_dense like operations\n        \"\"\"\n        if is_object_dtype(dtype):\n            return self.values.astype(object)\n        return self.values\n\n    def to_dense(self):\n        return self.values.view()\n\n    @property\n    def fill_value(self):\n        return np.nan\n\n    @property\n    def mgr_locs(self):\n        return self._mgr_locs\n\n    @mgr_locs.setter\n    def mgr_locs(self, new_mgr_locs):\n        if not isinstance(new_mgr_locs, libinternals.BlockPlacement):\n            new_mgr_locs = libinternals.BlockPlacement(new_mgr_locs)\n\n        self._mgr_locs = new_mgr_locs\n\n    @property\n    def array_dtype(self):\n        \"\"\" the dtype to return if I want to construct this block as an\n        array\n        \"\"\"\n        return self.dtype\n\n    def make_block(self, values, placement=None, ndim=None):\n        \"\"\"\n        Create a new block, with type inference propagate any values that are\n        not specified\n        \"\"\"\n        if placement is None:\n            placement = self.mgr_locs\n        if ndim is None:\n            ndim = self.ndim\n\n        return make_block(values, placement=placement, ndim=ndim)\n\n    def make_block_same_class(self, values, placement=None, ndim=None,\n                              dtype=None):\n        \"\"\" Wrap given values in a block of same type as self. \"\"\"\n        if dtype is not None:\n            # issue 19431 fastparquet is passing this\n            warnings.warn(\"dtype argument is deprecated, will be removed \"\n                          \"in a future release.\", DeprecationWarning)\n        if placement is None:\n            placement = self.mgr_locs\n        return make_block(values, placement=placement, ndim=ndim,\n                          klass=self.__class__, dtype=dtype)\n\n    def __repr__(self):\n        # don't want to print out all of the items here\n        name = pprint_thing(self.__class__.__name__)\n        if self._is_single_block:\n\n            result = '{name}: {len} dtype: {dtype}'.format(\n                name=name, len=len(self), dtype=self.dtype)\n\n        else:\n\n            shape = ' x '.join(pprint_thing(s) for s in self.shape)\n            result = '{name}: {index}, {shape}, dtype: {dtype}'.format(\n                name=name, index=pprint_thing(self.mgr_locs.indexer),\n                shape=shape, dtype=self.dtype)\n\n        return result\n\n    def __len__(self):\n        return len(self.values)\n\n    def __getstate__(self):\n        return self.mgr_locs.indexer, self.values\n\n    def __setstate__(self, state):\n        self.mgr_locs = libinternals.BlockPlacement(state[0])\n        self.values = state[1]\n        self.ndim = self.values.ndim\n\n    def _slice(self, slicer):\n        \"\"\" return a slice of my values \"\"\"\n        return self.values[slicer]\n\n    def getitem_block(self, slicer, new_mgr_locs=None):\n        \"\"\"\n        Perform __getitem__-like, return result as block.\n\n        As of now, only supports slices that preserve dimensionality.\n        \"\"\"\n        if new_mgr_locs is None:\n            if isinstance(slicer, tuple):\n                axis0_slicer = slicer[0]\n            else:\n                axis0_slicer = slicer\n            new_mgr_locs = self.mgr_locs[axis0_slicer]\n\n        new_values = self._slice(slicer)\n\n        if self._validate_ndim and new_values.ndim != self.ndim:\n            raise ValueError(\"Only same dim slicing is allowed\")\n\n        return self.make_block_same_class(new_values, new_mgr_locs)\n\n    @property\n    def shape(self):\n        return self.values.shape\n\n    @property\n    def dtype(self):\n        return self.values.dtype\n\n    @property\n    def ftype(self):\n        if getattr(self.values, '_pandas_ftype', False):\n            dtype = self.dtype.subtype\n        else:\n            dtype = self.dtype\n        return \"{dtype}:{ftype}\".format(dtype=dtype, ftype=self._ftype)\n\n    def merge(self, other):\n        return _merge_blocks([self, other])\n\n    def concat_same_type(self, to_concat, placement=None):\n        \"\"\"\n        Concatenate list of single blocks of the same type.\n        \"\"\"\n        values = self._concatenator([blk.values for blk in to_concat],\n                                    axis=self.ndim - 1)\n        return self.make_block_same_class(\n            values, placement=placement or slice(0, len(values), 1))\n\n    def iget(self, i):\n        return self.values[i]\n\n    def set(self, locs, values):\n        \"\"\"\n        Modify Block in-place with new item value\n\n        Returns\n        -------\n        None\n        \"\"\"\n        self.values[locs] = values\n\n    def delete(self, loc):\n        \"\"\"\n        Delete given loc(-s) from block in-place.\n        \"\"\"\n        self.values = np.delete(self.values, loc, 0)\n        self.mgr_locs = self.mgr_locs.delete(loc)\n\n    def apply(self, func, **kwargs):\n        \"\"\" apply the function to my values; return a block if we are not\n        one\n        \"\"\"\n        with np.errstate(all='ignore'):\n            result = func(self.values, **kwargs)\n        if not isinstance(result, Block):\n            result = self.make_block(values=_block_shape(result,\n                                                         ndim=self.ndim))\n\n        return result\n\n    def fillna(self, value, limit=None, inplace=False, downcast=None):\n        \"\"\" fillna on the block with the value. If we fail, then convert to\n        ObjectBlock and try again\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n\n        mask = isna(self.values)\n        if limit is not None:\n            if not is_integer(limit):\n                raise ValueError('Limit must be an integer')\n            if limit < 1:\n                raise ValueError('Limit must be greater than 0')\n            if self.ndim > 2:\n                raise NotImplementedError(\"number of dimensions for 'fillna' \"\n                                          \"is currently limited to 2\")\n            mask[mask.cumsum(self.ndim - 1) > limit] = False\n\n        if not self._can_hold_na:\n            if inplace:\n                return self\n            else:\n                return self.copy()\n\n        # fillna, but if we cannot coerce, then try again as an ObjectBlock\n        try:\n            values, _ = self._try_coerce_args(self.values, value)\n            blocks = self.putmask(mask, value, inplace=inplace)\n            blocks = [b.make_block(values=self._try_coerce_result(b.values))\n                      for b in blocks]\n            return self._maybe_downcast(blocks, downcast)\n        except (TypeError, ValueError):\n\n            # we can't process the value, but nothing to do\n            if not mask.any():\n                return self if inplace else self.copy()\n\n            # operate column-by-column\n            def f(m, v, i):\n                block = self.coerce_to_target_dtype(value)\n\n                # slice out our block\n                if i is not None:\n                    block = block.getitem_block(slice(i, i + 1))\n                return block.fillna(value,\n                                    limit=limit,\n                                    inplace=inplace,\n                                    downcast=None)\n\n            return self.split_and_operate(mask, f, inplace)\n\n    def split_and_operate(self, mask, f, inplace):\n        \"\"\"\n        split the block per-column, and apply the callable f\n        per-column, return a new block for each. Handle\n        masking which will not change a block unless needed.\n\n        Parameters\n        ----------\n        mask : 2-d boolean mask\n        f : callable accepting (1d-mask, 1d values, indexer)\n        inplace : boolean\n\n        Returns\n        -------\n        list of blocks\n        \"\"\"\n\n        if mask is None:\n            mask = np.ones(self.shape, dtype=bool)\n        new_values = self.values\n\n        def make_a_block(nv, ref_loc):\n            if isinstance(nv, list):\n                assert len(nv) == 1, nv\n                assert isinstance(nv[0], Block)\n                block = nv[0]\n            else:\n                # Put back the dimension that was taken from it and make\n                # a block out of the result.\n                nv = _block_shape(nv, ndim=self.ndim)\n                block = self.make_block(values=nv,\n                                        placement=ref_loc)\n            return block\n\n        # ndim == 1\n        if self.ndim == 1:\n            if mask.any():\n                nv = f(mask, new_values, None)\n            else:\n                nv = new_values if inplace else new_values.copy()\n            block = make_a_block(nv, self.mgr_locs)\n            return [block]\n\n        # ndim > 1\n        new_blocks = []\n        for i, ref_loc in enumerate(self.mgr_locs):\n            m = mask[i]\n            v = new_values[i]\n\n            # need a new block\n            if m.any():\n                nv = f(m, v, i)\n            else:\n                nv = v if inplace else v.copy()\n\n            block = make_a_block(nv, [ref_loc])\n            new_blocks.append(block)\n\n        return new_blocks\n\n    def _maybe_downcast(self, blocks, downcast=None):\n\n        # no need to downcast our float\n        # unless indicated\n        if downcast is None and self.is_float:\n            return blocks\n        elif downcast is None and (self.is_timedelta or self.is_datetime):\n            return blocks\n\n        if not isinstance(blocks, list):\n            blocks = [blocks]\n        return _extend_blocks([b.downcast(downcast) for b in blocks])\n\n    def downcast(self, dtypes=None):\n        \"\"\" try to downcast each item to the dict of dtypes if present \"\"\"\n\n        # turn it off completely\n        if dtypes is False:\n            return self\n\n        values = self.values\n\n        # single block handling\n        if self._is_single_block:\n\n            # try to cast all non-floats here\n            if dtypes is None:\n                dtypes = 'infer'\n\n            nv = maybe_downcast_to_dtype(values, dtypes)\n            return self.make_block(nv)\n\n        # ndim > 1\n        if dtypes is None:\n            return self\n\n        if not (dtypes == 'infer' or isinstance(dtypes, dict)):\n            raise ValueError(\"downcast must have a dictionary or 'infer' as \"\n                             \"its argument\")\n\n        # operate column-by-column\n        # this is expensive as it splits the blocks items-by-item\n        def f(m, v, i):\n\n            if dtypes == 'infer':\n                dtype = 'infer'\n            else:\n                raise AssertionError(\"dtypes as dict is not supported yet\")\n\n            if dtype is not None:\n                v = maybe_downcast_to_dtype(v, dtype)\n            return v\n\n        return self.split_and_operate(None, f, False)\n\n    def astype(self, dtype, copy=False, errors='raise', values=None, **kwargs):\n        return self._astype(dtype, copy=copy, errors=errors, values=values,\n                            **kwargs)\n\n    def _astype(self, dtype, copy=False, errors='raise', values=None,\n                **kwargs):\n        \"\"\"Coerce to the new type\n\n        Parameters\n        ----------\n        dtype : str, dtype convertible\n        copy : boolean, default False\n            copy if indicated\n        errors : str, {'raise', 'ignore'}, default 'ignore'\n            - ``raise`` : allow exceptions to be raised\n            - ``ignore`` : suppress exceptions. On error return original object\n\n        Returns\n        -------\n        Block\n        \"\"\"\n        errors_legal_values = ('raise', 'ignore')\n\n        if errors not in errors_legal_values:\n            invalid_arg = (\"Expected value of kwarg 'errors' to be one of {}. \"\n                           \"Supplied value is '{}'\".format(\n                               list(errors_legal_values), errors))\n            raise ValueError(invalid_arg)\n\n        if (inspect.isclass(dtype) and\n                issubclass(dtype, ExtensionDtype)):\n            msg = (\"Expected an instance of {}, but got the class instead. \"\n                   \"Try instantiating 'dtype'.\".format(dtype.__name__))\n            raise TypeError(msg)\n\n        # may need to convert to categorical\n        if self.is_categorical_astype(dtype):\n\n            # deprecated 17636\n            if ('categories' in kwargs or 'ordered' in kwargs):\n                if isinstance(dtype, CategoricalDtype):\n                    raise TypeError(\n                        \"Cannot specify a CategoricalDtype and also \"\n                        \"`categories` or `ordered`. Use \"\n                        \"`dtype=CategoricalDtype(categories, ordered)`\"\n                        \" instead.\")\n                warnings.warn(\"specifying 'categories' or 'ordered' in \"\n                              \".astype() is deprecated; pass a \"\n                              \"CategoricalDtype instead\",\n                              FutureWarning, stacklevel=7)\n\n            categories = kwargs.get('categories', None)\n            ordered = kwargs.get('ordered', None)\n            if com._any_not_none(categories, ordered):\n                dtype = CategoricalDtype(categories, ordered)\n\n            if is_categorical_dtype(self.values):\n                # GH 10696/18593: update an existing categorical efficiently\n                return self.make_block(self.values.astype(dtype, copy=copy))\n\n            return self.make_block(Categorical(self.values, dtype=dtype))\n\n        dtype = pandas_dtype(dtype)\n\n        # astype processing\n        if is_dtype_equal(self.dtype, dtype):\n            if copy:\n                return self.copy()\n            return self\n\n        try:\n            # force the copy here\n            if values is None:\n\n                if self.is_extension:\n                    values = self.values.astype(dtype)\n                else:\n                    if issubclass(dtype.type, str):\n\n                        # use native type formatting for datetime/tz/timedelta\n                        if self.is_datelike:\n                            values = self.to_native_types()\n\n                        # astype formatting\n                        else:\n                            values = self.get_values()\n\n                    else:\n                        values = self.get_values(dtype=dtype)\n\n                    # _astype_nansafe works fine with 1-d only\n                    values = astype_nansafe(\n                        values.ravel(), dtype, copy=True, **kwargs)\n\n                # TODO(extension)\n                # should we make this attribute?\n                try:\n                    values = values.reshape(self.shape)\n                except AttributeError:\n                    pass\n\n            newb = make_block(values, placement=self.mgr_locs,\n                              ndim=self.ndim)\n        except Exception:  # noqa: E722\n            if errors == 'raise':\n                raise\n            newb = self.copy() if copy else self\n\n        if newb.is_numeric and self.is_numeric:\n            if newb.shape != self.shape:\n                raise TypeError(\n                    \"cannot set astype for copy = [{copy}] for dtype \"\n                    \"({dtype} [{shape}]) to different shape \"\n                    \"({newb_dtype} [{newb_shape}])\".format(\n                        copy=copy, dtype=self.dtype.name,\n                        shape=self.shape, newb_dtype=newb.dtype.name,\n                        newb_shape=newb.shape))\n        return newb\n\n    def convert(self, copy=True, **kwargs):\n        \"\"\" attempt to coerce any object types to better types return a copy\n        of the block (if copy = True) by definition we are not an ObjectBlock\n        here!\n        \"\"\"\n\n        return self.copy() if copy else self\n\n    def _can_hold_element(self, element):\n        \"\"\" require the same dtype as ourselves \"\"\"\n        dtype = self.values.dtype.type\n        tipo = maybe_infer_dtype_type(element)\n        if tipo is not None:\n            return issubclass(tipo.type, dtype)\n        return isinstance(element, dtype)\n\n    def _try_cast_result(self, result, dtype=None):\n        \"\"\" try to cast the result to our original type, we may have\n        roundtripped thru object in the mean-time\n        \"\"\"\n        if dtype is None:\n            dtype = self.dtype\n\n        if self.is_integer or self.is_bool or self.is_datetime:\n            pass\n        elif self.is_float and result.dtype == self.dtype:\n            # protect against a bool/object showing up here\n            if isinstance(dtype, str) and dtype == 'infer':\n                return result\n\n            # This is only reached via Block.setitem, where dtype is always\n            #  either \"infer\", self.dtype, or values.dtype.\n            assert dtype == self.dtype, (dtype, self.dtype)\n            return result\n\n        # may need to change the dtype here\n        return maybe_downcast_to_dtype(result, dtype)\n\n    def _try_coerce_args(self, values, other):\n        \"\"\" provide coercion to our input arguments \"\"\"\n\n        if np.any(notna(other)) and not self._can_hold_element(other):\n            # coercion issues\n            # let higher levels handle\n            raise TypeError(\"cannot convert {} to an {}\".format(\n                type(other).__name__,\n                type(self).__name__.lower().replace('Block', '')))\n\n        return values, other\n\n    def _try_coerce_result(self, result):\n        \"\"\" reverse of try_coerce_args \"\"\"\n        return result\n\n    def _try_coerce_and_cast_result(self, result, dtype=None):\n        result = self._try_coerce_result(result)\n        result = self._try_cast_result(result, dtype=dtype)\n        return result\n\n    def to_native_types(self, slicer=None, na_rep='nan', quoting=None,\n                        **kwargs):\n        \"\"\" convert to our native types format, slicing if desired \"\"\"\n\n        values = self.get_values()\n\n        if slicer is not None:\n            values = values[:, slicer]\n        mask = isna(values)\n\n        if not self.is_object and not quoting:\n            values = values.astype(str)\n        else:\n            values = np.array(values, dtype='object')\n\n        values[mask] = na_rep\n        return values\n\n    # block actions ####\n    def copy(self, deep=True):\n        \"\"\" copy constructor \"\"\"\n        values = self.values\n        if deep:\n            values = values.copy()\n        return self.make_block_same_class(values, ndim=self.ndim)\n\n    def replace(self, to_replace, value, inplace=False, filter=None,\n                regex=False, convert=True):\n        \"\"\"replace the to_replace value with value, possible to create new\n        blocks here this is just a call to putmask. regex is not used here.\n        It is used in ObjectBlocks.  It is here for API compatibility.\n        \"\"\"\n\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        original_to_replace = to_replace\n\n        # try to replace, if we raise an error, convert to ObjectBlock and\n        # retry\n        try:\n            values, to_replace = self._try_coerce_args(self.values,\n                                                       to_replace)\n            mask = missing.mask_missing(values, to_replace)\n            if filter is not None:\n                filtered_out = ~self.mgr_locs.isin(filter)\n                mask[filtered_out.nonzero()[0]] = False\n\n            blocks = self.putmask(mask, value, inplace=inplace)\n            if convert:\n                blocks = [b.convert(by_item=True, numeric=False,\n                                    copy=not inplace) for b in blocks]\n            return blocks\n        except (TypeError, ValueError):\n            # GH 22083, TypeError or ValueError occurred within error handling\n            # causes infinite loop. Cast and retry only if not objectblock.\n            if is_object_dtype(self):\n                raise\n\n            # try again with a compatible block\n            block = self.astype(object)\n            return block.replace(to_replace=original_to_replace,\n                                 value=value,\n                                 inplace=inplace,\n                                 filter=filter,\n                                 regex=regex,\n                                 convert=convert)\n\n    def _replace_single(self, *args, **kwargs):\n        \"\"\" no-op on a non-ObjectBlock \"\"\"\n        return self if kwargs['inplace'] else self.copy()\n\n    def setitem(self, indexer, value):\n        \"\"\"Set the value inplace, returning a a maybe different typed block.\n\n        Parameters\n        ----------\n        indexer : tuple, list-like, array-like, slice\n            The subset of self.values to set\n        value : object\n            The value being set\n\n        Returns\n        -------\n        Block\n\n        Notes\n        -----\n        `indexer` is a direct slice/positional indexer. `value` must\n        be a compatible shape.\n        \"\"\"\n        # coerce None values, if appropriate\n        if value is None:\n            if self.is_numeric:\n                value = np.nan\n\n        # coerce if block dtype can store value\n        values = self.values\n        try:\n            values, value = self._try_coerce_args(values, value)\n            # can keep its own dtype\n            if hasattr(value, 'dtype') and is_dtype_equal(values.dtype,\n                                                          value.dtype):\n                dtype = self.dtype\n            else:\n                dtype = 'infer'\n\n        except (TypeError, ValueError):\n            # current dtype cannot store value, coerce to common dtype\n            find_dtype = False\n\n            if hasattr(value, 'dtype'):\n                dtype = value.dtype\n                find_dtype = True\n\n            elif lib.is_scalar(value):\n                if isna(value):\n                    # NaN promotion is handled in latter path\n                    dtype = False\n                else:\n                    dtype, _ = infer_dtype_from_scalar(value,\n                                                       pandas_dtype=True)\n                    find_dtype = True\n            else:\n                dtype = 'infer'\n\n            if find_dtype:\n                dtype = find_common_type([values.dtype, dtype])\n                if not is_dtype_equal(self.dtype, dtype):\n                    b = self.astype(dtype)\n                    return b.setitem(indexer, value)\n\n        # value must be storeable at this moment\n        arr_value = np.array(value)\n\n        # cast the values to a type that can hold nan (if necessary)\n        if not self._can_hold_element(value):\n            dtype, _ = maybe_promote(arr_value.dtype)\n            values = values.astype(dtype)\n\n        transf = (lambda x: x.T) if self.ndim == 2 else (lambda x: x)\n        values = transf(values)\n\n        # length checking\n        check_setitem_lengths(indexer, value, values)\n\n        def _is_scalar_indexer(indexer):\n            # return True if we are all scalar indexers\n\n            if arr_value.ndim == 1:\n                if not isinstance(indexer, tuple):\n                    indexer = tuple([indexer])\n                    return any(isinstance(idx, np.ndarray) and len(idx) == 0\n                               for idx in indexer)\n            return False\n\n        def _is_empty_indexer(indexer):\n            # return a boolean if we have an empty indexer\n\n            if is_list_like(indexer) and not len(indexer):\n                return True\n            if arr_value.ndim == 1:\n                if not isinstance(indexer, tuple):\n                    indexer = tuple([indexer])\n                return any(isinstance(idx, np.ndarray) and len(idx) == 0\n                           for idx in indexer)\n            return False\n\n        # empty indexers\n        # 8669 (empty)\n        if _is_empty_indexer(indexer):\n            pass\n\n        # setting a single element for each dim and with a rhs that could\n        # be say a list\n        # GH 6043\n        elif _is_scalar_indexer(indexer):\n            values[indexer] = value\n\n        # if we are an exact match (ex-broadcasting),\n        # then use the resultant dtype\n        elif (len(arr_value.shape) and\n              arr_value.shape[0] == values.shape[0] and\n              np.prod(arr_value.shape) == np.prod(values.shape)):\n            values[indexer] = value\n            try:\n                values = values.astype(arr_value.dtype)\n            except ValueError:\n                pass\n\n        # set\n        else:\n            values[indexer] = value\n\n        # coerce and try to infer the dtypes of the result\n        values = self._try_coerce_and_cast_result(values, dtype)\n        block = self.make_block(transf(values))\n        return block\n\n    def putmask(self, mask, new, align=True, inplace=False, axis=0,\n                transpose=False):\n        \"\"\" putmask the data to the block; it is possible that we may create a\n        new dtype of block\n\n        return the resulting block(s)\n\n        Parameters\n        ----------\n        mask  : the condition to respect\n        new : a ndarray/object\n        align : boolean, perform alignment on other/cond, default is True\n        inplace : perform inplace modification, default is False\n        axis : int\n        transpose : boolean\n            Set to True if self is stored with axes reversed\n\n        Returns\n        -------\n        a list of new blocks, the result of the putmask\n        \"\"\"\n\n        new_values = self.values if inplace else self.values.copy()\n\n        new = getattr(new, 'values', new)\n        mask = getattr(mask, 'values', mask)\n\n        # if we are passed a scalar None, convert it here\n        if not is_list_like(new) and isna(new) and not self.is_object:\n            new = self.fill_value\n\n        if self._can_hold_element(new):\n            _, new = self._try_coerce_args(new_values, new)\n\n            if transpose:\n                new_values = new_values.T\n\n            # If the default repeat behavior in np.putmask would go in the\n            # wrong direction, then explicitly repeat and reshape new instead\n            if getattr(new, 'ndim', 0) >= 1:\n                if self.ndim - 1 == new.ndim and axis == 1:\n                    new = np.repeat(\n                        new, new_values.shape[-1]).reshape(self.shape)\n                new = new.astype(new_values.dtype)\n\n            # we require exact matches between the len of the\n            # values we are setting (or is compat). np.putmask\n            # doesn't check this and will simply truncate / pad\n            # the output, but we want sane error messages\n            #\n            # TODO: this prob needs some better checking\n            # for 2D cases\n            if ((is_list_like(new) and\n                 np.any(mask[mask]) and\n                 getattr(new, 'ndim', 1) == 1)):\n\n                if not (mask.shape[-1] == len(new) or\n                        mask[mask].shape[-1] == len(new) or\n                        len(new) == 1):\n                    raise ValueError(\"cannot assign mismatch \"\n                                     \"length to masked array\")\n\n            np.putmask(new_values, mask, new)\n\n        # maybe upcast me\n        elif mask.any():\n            if transpose:\n                mask = mask.T\n                if isinstance(new, np.ndarray):\n                    new = new.T\n                axis = new_values.ndim - axis - 1\n\n            # Pseudo-broadcast\n            if getattr(new, 'ndim', 0) >= 1:\n                if self.ndim - 1 == new.ndim:\n                    new_shape = list(new.shape)\n                    new_shape.insert(axis, 1)\n                    new = new.reshape(tuple(new_shape))\n\n            # operate column-by-column\n            def f(m, v, i):\n\n                if i is None:\n                    # ndim==1 case.\n                    n = new\n                else:\n\n                    if isinstance(new, np.ndarray):\n                        n = np.squeeze(new[i % new.shape[0]])\n                    else:\n                        n = np.array(new)\n\n                    # type of the new block\n                    dtype, _ = maybe_promote(n.dtype)\n\n                    # we need to explicitly astype here to make a copy\n                    n = n.astype(dtype)\n\n                nv = _putmask_smart(v, m, n)\n                return nv\n\n            new_blocks = self.split_and_operate(mask, f, inplace)\n            return new_blocks\n\n        if inplace:\n            return [self]\n\n        if transpose:\n            new_values = new_values.T\n\n        return [self.make_block(new_values)]\n\n    def coerce_to_target_dtype(self, other):\n        \"\"\"\n        coerce the current block to a dtype compat for other\n        we will return a block, possibly object, and not raise\n\n        we can also safely try to coerce to the same dtype\n        and will receive the same block\n        \"\"\"\n\n        # if we cannot then coerce to object\n        dtype, _ = infer_dtype_from(other, pandas_dtype=True)\n\n        if is_dtype_equal(self.dtype, dtype):\n            return self\n\n        if self.is_bool or is_object_dtype(dtype) or is_bool_dtype(dtype):\n            # we don't upcast to bool\n            return self.astype(object)\n\n        elif ((self.is_float or self.is_complex) and\n              (is_integer_dtype(dtype) or is_float_dtype(dtype))):\n            # don't coerce float/complex to int\n            return self\n\n        elif (self.is_datetime or\n              is_datetime64_dtype(dtype) or\n              is_datetime64tz_dtype(dtype)):\n\n            # not a datetime\n            if not ((is_datetime64_dtype(dtype) or\n                     is_datetime64tz_dtype(dtype)) and self.is_datetime):\n                return self.astype(object)\n\n            # don't upcast timezone with different timezone or no timezone\n            mytz = getattr(self.dtype, 'tz', None)\n            othertz = getattr(dtype, 'tz', None)\n\n            if str(mytz) != str(othertz):\n                return self.astype(object)\n\n            raise AssertionError(\"possible recursion in \"\n                                 \"coerce_to_target_dtype: {} {}\".format(\n                                     self, other))\n\n        elif (self.is_timedelta or is_timedelta64_dtype(dtype)):\n\n            # not a timedelta\n            if not (is_timedelta64_dtype(dtype) and self.is_timedelta):\n                return self.astype(object)\n\n            raise AssertionError(\"possible recursion in \"\n                                 \"coerce_to_target_dtype: {} {}\".format(\n                                     self, other))\n\n        try:\n            return self.astype(dtype)\n        except (ValueError, TypeError, OverflowError):\n            pass\n\n        return self.astype(object)\n\n    def interpolate(self, method='pad', axis=0, index=None, values=None,\n                    inplace=False, limit=None, limit_direction='forward',\n                    limit_area=None, fill_value=None, coerce=False,\n                    downcast=None, **kwargs):\n\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n\n        def check_int_bool(self, inplace):\n            # Only FloatBlocks will contain NaNs.\n            # timedelta subclasses IntBlock\n            if (self.is_bool or self.is_integer) and not self.is_timedelta:\n                if inplace:\n                    return self\n                else:\n                    return self.copy()\n\n        # a fill na type method\n        try:\n            m = missing.clean_fill_method(method)\n        except ValueError:\n            m = None\n\n        if m is not None:\n            r = check_int_bool(self, inplace)\n            if r is not None:\n                return r\n            return self._interpolate_with_fill(method=m, axis=axis,\n                                               inplace=inplace, limit=limit,\n                                               fill_value=fill_value,\n                                               coerce=coerce,\n                                               downcast=downcast)\n        # validate the interp method\n        m = missing.clean_interp_method(method, **kwargs)\n\n        r = check_int_bool(self, inplace)\n        if r is not None:\n            return r\n        return self._interpolate(method=m, index=index, values=values,\n                                 axis=axis, limit=limit,\n                                 limit_direction=limit_direction,\n                                 limit_area=limit_area,\n                                 fill_value=fill_value, inplace=inplace,\n                                 downcast=downcast, **kwargs)\n\n    def _interpolate_with_fill(self, method='pad', axis=0, inplace=False,\n                               limit=None, fill_value=None, coerce=False,\n                               downcast=None):\n        \"\"\" fillna but using the interpolate machinery \"\"\"\n\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n\n        # if we are coercing, then don't force the conversion\n        # if the block can't hold the type\n        if coerce:\n            if not self._can_hold_na:\n                if inplace:\n                    return [self]\n                else:\n                    return [self.copy()]\n\n        values = self.values if inplace else self.values.copy()\n        values, fill_value = self._try_coerce_args(values, fill_value)\n        values = missing.interpolate_2d(values, method=method, axis=axis,\n                                        limit=limit, fill_value=fill_value,\n                                        dtype=self.dtype)\n        values = self._try_coerce_result(values)\n\n        blocks = [self.make_block_same_class(values, ndim=self.ndim)]\n        return self._maybe_downcast(blocks, downcast)\n\n    def _interpolate(self, method=None, index=None, values=None,\n                     fill_value=None, axis=0, limit=None,\n                     limit_direction='forward', limit_area=None,\n                     inplace=False, downcast=None, **kwargs):\n        \"\"\" interpolate using scipy wrappers \"\"\"\n\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        data = self.values if inplace else self.values.copy()\n\n        # only deal with floats\n        if not self.is_float:\n            if not self.is_integer:\n                return self\n            data = data.astype(np.float64)\n\n        if fill_value is None:\n            fill_value = self.fill_value\n\n        if method in ('krogh', 'piecewise_polynomial', 'pchip'):\n            if not index.is_monotonic:\n                raise ValueError(\"{0} interpolation requires that the \"\n                                 \"index be monotonic.\".format(method))\n        # process 1-d slices in the axis direction\n\n        def func(x):\n\n            # process a 1-d slice, returning it\n            # should the axis argument be handled below in apply_along_axis?\n            # i.e. not an arg to missing.interpolate_1d\n            return missing.interpolate_1d(index, x, method=method, limit=limit,\n                                          limit_direction=limit_direction,\n                                          limit_area=limit_area,\n                                          fill_value=fill_value,\n                                          bounds_error=False, **kwargs)\n\n        # interp each column independently\n        interp_values = np.apply_along_axis(func, axis, data)\n\n        blocks = [self.make_block_same_class(interp_values)]\n        return self._maybe_downcast(blocks, downcast)\n\n    def take_nd(self, indexer, axis, new_mgr_locs=None, fill_tuple=None):\n        \"\"\"\n        Take values according to indexer and return them as a block.bb\n\n        \"\"\"\n\n        # algos.take_nd dispatches for DatetimeTZBlock, CategoricalBlock\n        # so need to preserve types\n        # sparse is treated like an ndarray, but needs .get_values() shaping\n\n        values = self.values\n\n        if fill_tuple is None:\n            fill_value = self.fill_value\n            new_values = algos.take_nd(values, indexer, axis=axis,\n                                       allow_fill=False, fill_value=fill_value)\n        else:\n            fill_value = fill_tuple[0]\n            new_values = algos.take_nd(values, indexer, axis=axis,\n                                       allow_fill=True, fill_value=fill_value)\n\n        if new_mgr_locs is None:\n            if axis == 0:\n                slc = libinternals.indexer_as_slice(indexer)\n                if slc is not None:\n                    new_mgr_locs = self.mgr_locs[slc]\n                else:\n                    new_mgr_locs = self.mgr_locs[indexer]\n            else:\n                new_mgr_locs = self.mgr_locs\n\n        if not is_dtype_equal(new_values.dtype, self.dtype):\n            return self.make_block(new_values, new_mgr_locs)\n        else:\n            return self.make_block_same_class(new_values, new_mgr_locs)\n\n    def diff(self, n, axis=1):\n        \"\"\" return block for the diff of the values \"\"\"\n        new_values = algos.diff(self.values, n, axis=axis)\n        return [self.make_block(values=new_values)]\n\n    def shift(self, periods, axis=0, fill_value=None):\n        \"\"\" shift the block by periods, possibly upcast \"\"\"\n\n        # convert integer to float if necessary. need to do a lot more than\n        # that, handle boolean etc also\n        new_values, fill_value = maybe_upcast(self.values, fill_value)\n\n        # make sure array sent to np.roll is c_contiguous\n        f_ordered = new_values.flags.f_contiguous\n        if f_ordered:\n            new_values = new_values.T\n            axis = new_values.ndim - axis - 1\n\n        if np.prod(new_values.shape):\n            new_values = np.roll(new_values, ensure_platform_int(periods),\n                                 axis=axis)\n\n        axis_indexer = [slice(None)] * self.ndim\n        if periods > 0:\n            axis_indexer[axis] = slice(None, periods)\n        else:\n            axis_indexer[axis] = slice(periods, None)\n        new_values[tuple(axis_indexer)] = fill_value\n\n        # restore original order\n        if f_ordered:\n            new_values = new_values.T\n\n        return [self.make_block(new_values)]\n\n    def where(self, other, cond, align=True, errors='raise',\n              try_cast=False, axis=0, transpose=False):\n        \"\"\"\n        evaluate the block; return result block(s) from the result\n\n        Parameters\n        ----------\n        other : a ndarray/object\n        cond  : the condition to respect\n        align : boolean, perform alignment on other/cond\n        errors : str, {'raise', 'ignore'}, default 'raise'\n            - ``raise`` : allow exceptions to be raised\n            - ``ignore`` : suppress exceptions. On error return original object\n\n        axis : int\n        transpose : boolean\n            Set to True if self is stored with axes reversed\n\n        Returns\n        -------\n        a new block(s), the result of the func\n        \"\"\"\n        import pandas.core.computation.expressions as expressions\n        assert errors in ['raise', 'ignore']\n\n        values = self.values\n        orig_other = other\n        if transpose:\n            values = values.T\n\n        other = getattr(other, '_values', getattr(other, 'values', other))\n        cond = getattr(cond, 'values', cond)\n\n        # If the default broadcasting would go in the wrong direction, then\n        # explicitly reshape other instead\n        if getattr(other, 'ndim', 0) >= 1:\n            if values.ndim - 1 == other.ndim and axis == 1:\n                other = other.reshape(tuple(other.shape + (1, )))\n            elif transpose and values.ndim == self.ndim - 1:\n                cond = cond.T\n\n        if not hasattr(cond, 'shape'):\n            raise ValueError(\"where must have a condition that is ndarray \"\n                             \"like\")\n\n        # our where function\n        def func(cond, values, other):\n            if cond.ravel().all():\n                return values\n\n            values, other = self._try_coerce_args(values, other)\n\n            try:\n                return self._try_coerce_result(expressions.where(\n                    cond, values, other))\n            except Exception as detail:\n                if errors == 'raise':\n                    raise TypeError(\n                        'Could not operate [{other!r}] with block values '\n                        '[{detail!s}]'.format(other=other, detail=detail))\n                else:\n                    # return the values\n                    result = np.empty(values.shape, dtype='float64')\n                    result.fill(np.nan)\n                    return result\n\n        # see if we can operate on the entire block, or need item-by-item\n        # or if we are a single block (ndim == 1)\n        try:\n            result = func(cond, values, other)\n        except TypeError:\n\n            # we cannot coerce, return a compat dtype\n            # we are explicitly ignoring errors\n            block = self.coerce_to_target_dtype(other)\n            blocks = block.where(orig_other, cond, align=align,\n                                 errors=errors,\n                                 try_cast=try_cast, axis=axis,\n                                 transpose=transpose)\n            return self._maybe_downcast(blocks, 'infer')\n\n        if self._can_hold_na or self.ndim == 1:\n\n            if transpose:\n                result = result.T\n\n            # try to cast if requested\n            if try_cast:\n                result = self._try_cast_result(result)\n\n            return self.make_block(result)\n\n        # might need to separate out blocks\n        axis = cond.ndim - 1\n        cond = cond.swapaxes(axis, 0)\n        mask = np.array([cond[i].all() for i in range(cond.shape[0])],\n                        dtype=bool)\n\n        result_blocks = []\n        for m in [mask, ~mask]:\n            if m.any():\n                r = self._try_cast_result(result.take(m.nonzero()[0],\n                                                      axis=axis))\n                result_blocks.append(\n                    self.make_block(r.T, placement=self.mgr_locs[m]))\n\n        return result_blocks\n\n    def equals(self, other):\n        if self.dtype != other.dtype or self.shape != other.shape:\n            return False\n        return array_equivalent(self.values, other.values)\n\n    def _unstack(self, unstacker_func, new_columns, n_rows, fill_value):\n        \"\"\"Return a list of unstacked blocks of self\n\n        Parameters\n        ----------\n        unstacker_func : callable\n            Partially applied unstacker.\n        new_columns : Index\n            All columns of the unstacked BlockManager.\n        n_rows : int\n            Only used in ExtensionBlock.unstack\n        fill_value : int\n            Only used in ExtensionBlock.unstack\n\n        Returns\n        -------\n        blocks : list of Block\n            New blocks of unstacked values.\n        mask : array_like of bool\n            The mask of columns of `blocks` we should keep.\n        \"\"\"\n        unstacker = unstacker_func(self.values.T)\n        new_items = unstacker.get_new_columns()\n        new_placement = new_columns.get_indexer(new_items)\n        new_values, mask = unstacker.get_new_values()\n\n        mask = mask.any(0)\n        new_values = new_values.T[mask]\n        new_placement = new_placement[mask]\n\n        blocks = [make_block(new_values, placement=new_placement)]\n        return blocks, mask\n\n    def quantile(self, qs, interpolation='linear', axis=0):\n        \"\"\"\n        compute the quantiles of the\n\n        Parameters\n        ----------\n        qs: a scalar or list of the quantiles to be computed\n        interpolation: type of interpolation, default 'linear'\n        axis: axis to compute, default 0\n\n        Returns\n        -------\n        Block\n        \"\"\"\n        # We should always have ndim == 2 becase Series dispatches to DataFrame\n        assert self.ndim == 2\n\n        if self.is_datetimetz:\n            # TODO: cleanup this special case.\n            # We need to operate on i8 values for datetimetz\n            # but `Block.get_values()` returns an ndarray of objects\n            # right now. We need an API for \"values to do numeric-like ops on\"\n            values = self.values.asi8\n\n            # TODO: NonConsolidatableMixin shape\n            # Usual shape inconsistencies for ExtensionBlocks\n            values = values[None, :]\n        else:\n            values = self.get_values()\n            values, _ = self._try_coerce_args(values, values)\n\n        is_empty = values.shape[axis] == 0\n        orig_scalar = not is_list_like(qs)\n        if orig_scalar:\n            # make list-like, unpack later\n            qs = [qs]\n\n        if is_empty:\n            # create the array of na_values\n            # 2d len(values) * len(qs)\n            result = np.repeat(np.array([self.fill_value] * len(qs)),\n                               len(values)).reshape(len(values),\n                                                    len(qs))\n        else:\n            # asarray needed for Sparse, see GH#24600\n            # TODO: Why self.values and not values?\n            mask = np.asarray(isna(self.values))\n            result = nanpercentile(values, np.array(qs) * 100,\n                                   axis=axis, na_value=self.fill_value,\n                                   mask=mask, ndim=self.ndim,\n                                   interpolation=interpolation)\n\n            result = np.array(result, copy=False)\n            result = result.T\n\n        if orig_scalar and not lib.is_scalar(result):\n            # result could be scalar in case with is_empty and self.ndim == 1\n            assert result.shape[-1] == 1, result.shape\n            result = result[..., 0]\n            result = lib.item_from_zerodim(result)\n\n        ndim = getattr(result, 'ndim', None) or 0\n        result = self._try_coerce_result(result)\n        return make_block(result,\n                          placement=np.arange(len(result)),\n                          ndim=ndim)\n\n    def _replace_coerce(self, to_replace, value, inplace=True, regex=False,\n                        convert=False, mask=None):\n        \"\"\"\n        Replace value corresponding to the given boolean array with another\n        value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        inplace : bool, default False\n            Perform inplace modification.\n        regex : bool, default False\n            If true, perform regular expression substitution.\n        convert : bool, default True\n            If true, try to coerce any object types to better types.\n        mask : array-like of bool, optional\n            True indicate corresponding element is ignored.\n\n        Returns\n        -------\n        A new block if there is anything to replace or the original block.\n        \"\"\"\n\n        if mask.any():\n            if not regex:\n                self = self.coerce_to_target_dtype(value)\n                return self.putmask(mask, value, inplace=inplace)\n            else:\n                return self._replace_single(to_replace, value, inplace=inplace,\n                                            regex=regex,\n                                            convert=convert,\n                                            mask=mask)\n        return self\n\n\nclass NonConsolidatableMixIn:\n    \"\"\" hold methods for the nonconsolidatable blocks \"\"\"\n    _can_consolidate = False\n    _verify_integrity = False\n    _validate_ndim = False\n\n    def __init__(self, values, placement, ndim=None):\n        \"\"\"Initialize a non-consolidatable block.\n\n        'ndim' may be inferred from 'placement'.\n\n        This will call continue to call __init__ for the other base\n        classes mixed in with this Mixin.\n        \"\"\"\n        # Placement must be converted to BlockPlacement so that we can check\n        # its length\n        if not isinstance(placement, libinternals.BlockPlacement):\n            placement = libinternals.BlockPlacement(placement)\n\n        # Maybe infer ndim from placement\n        if ndim is None:\n            if len(placement) != 1:\n                ndim = 1\n            else:\n                ndim = 2\n        super().__init__(values, placement, ndim=ndim)\n\n    @property\n    def shape(self):\n        if self.ndim == 1:\n            return (len(self.values)),\n        return (len(self.mgr_locs), len(self.values))\n\n    def iget(self, col):\n\n        if self.ndim == 2 and isinstance(col, tuple):\n            col, loc = col\n            if not com.is_null_slice(col) and col != 0:\n                raise IndexError(\"{0} only contains one item\".format(self))\n            return self.values[loc]\n        else:\n            if col != 0:\n                raise IndexError(\"{0} only contains one item\".format(self))\n            return self.values\n\n    def should_store(self, value):\n        return isinstance(value, self._holder)\n\n    def set(self, locs, values, check=False):\n        assert locs.tolist() == [0]\n        self.values = values\n\n    def putmask(self, mask, new, align=True, inplace=False, axis=0,\n                transpose=False):\n        \"\"\"\n        putmask the data to the block; we must be a single block and not\n        generate other blocks\n\n        return the resulting block\n\n        Parameters\n        ----------\n        mask  : the condition to respect\n        new : a ndarray/object\n        align : boolean, perform alignment on other/cond, default is True\n        inplace : perform inplace modification, default is False\n\n        Returns\n        -------\n        a new block, the result of the putmask\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n\n        # use block's copy logic.\n        # .values may be an Index which does shallow copy by default\n        new_values = self.values if inplace else self.copy().values\n        new_values, new = self._try_coerce_args(new_values, new)\n\n        if isinstance(new, np.ndarray) and len(new) == len(mask):\n            new = new[mask]\n\n        mask = _safe_reshape(mask, new_values.shape)\n\n        new_values[mask] = new\n        new_values = self._try_coerce_result(new_values)\n        return [self.make_block(values=new_values)]\n\n    def _try_cast_result(self, result, dtype=None):\n        return result\n\n    def _get_unstack_items(self, unstacker, new_columns):\n        \"\"\"\n        Get the placement, values, and mask for a Block unstack.\n\n        This is shared between ObjectBlock and ExtensionBlock. They\n        differ in that ObjectBlock passes the values, while ExtensionBlock\n        passes the dummy ndarray of positions to be used by a take\n        later.\n\n        Parameters\n        ----------\n        unstacker : pandas.core.reshape.reshape._Unstacker\n        new_columns : Index\n            All columns of the unstacked BlockManager.\n\n        Returns\n        -------\n        new_placement : ndarray[int]\n            The placement of the new columns in `new_columns`.\n        new_values : Union[ndarray, ExtensionArray]\n            The first return value from _Unstacker.get_new_values.\n        mask : ndarray[bool]\n            The second return value from _Unstacker.get_new_values.\n        \"\"\"\n        # shared with ExtensionBlock\n        new_items = unstacker.get_new_columns()\n        new_placement = new_columns.get_indexer(new_items)\n        new_values, mask = unstacker.get_new_values()\n\n        mask = mask.any(0)\n        return new_placement, new_values, mask\n\n\nclass ExtensionBlock(NonConsolidatableMixIn, Block):\n    \"\"\"Block for holding extension types.\n\n    Notes\n    -----\n    This holds all 3rd-party extension array types. It's also the immediate\n    parent class for our internal extension types' blocks, CategoricalBlock.\n\n    ExtensionArrays are limited to 1-D.\n    \"\"\"\n    is_extension = True\n\n    def __init__(self, values, placement, ndim=None):\n        values = self._maybe_coerce_values(values)\n        super().__init__(values, placement, ndim)\n\n    def _maybe_coerce_values(self, values):\n        \"\"\"Unbox to an extension array.\n\n        This will unbox an ExtensionArray stored in an Index or Series.\n        ExtensionArrays pass through. No dtype coercion is done.\n\n        Parameters\n        ----------\n        values : Index, Series, ExtensionArray\n\n        Returns\n        -------\n        ExtensionArray\n        \"\"\"\n        if isinstance(values, (ABCIndexClass, ABCSeries)):\n            values = values._values\n        return values\n\n    @property\n    def _holder(self):\n        # For extension blocks, the holder is values-dependent.\n        return type(self.values)\n\n    @property\n    def fill_value(self):\n        # Used in reindex_indexer\n        return self.values.dtype.na_value\n\n    @property\n    def _can_hold_na(self):\n        # The default ExtensionArray._can_hold_na is True\n        return self._holder._can_hold_na\n\n    @property\n    def is_view(self):\n        \"\"\"Extension arrays are never treated as views.\"\"\"\n        return False\n\n    @property\n    def is_numeric(self):\n        return self.values.dtype._is_numeric\n\n    def setitem(self, indexer, value):\n        \"\"\"Set the value inplace, returning a same-typed block.\n\n        This differs from Block.setitem by not allowing setitem to change\n        the dtype of the Block.\n\n        Parameters\n        ----------\n        indexer : tuple, list-like, array-like, slice\n            The subset of self.values to set\n        value : object\n            The value being set\n\n        Returns\n        -------\n        Block\n\n        Notes\n        -----\n        `indexer` is a direct slice/positional indexer. `value` must\n        be a compatible shape.\n        \"\"\"\n        if isinstance(indexer, tuple):\n            # we are always 1-D\n            indexer = indexer[0]\n\n        check_setitem_lengths(indexer, value, self.values)\n        self.values[indexer] = value\n        return self\n\n    def get_values(self, dtype=None):\n        # ExtensionArrays must be iterable, so this works.\n        values = np.asarray(self.values)\n        if values.ndim == self.ndim - 1:\n            values = values.reshape((1,) + values.shape)\n        return values\n\n    def to_dense(self):\n        return np.asarray(self.values)\n\n    def take_nd(self, indexer, axis=0, new_mgr_locs=None, fill_tuple=None):\n        \"\"\"\n        Take values according to indexer and return them as a block.\n        \"\"\"\n        if fill_tuple is None:\n            fill_value = None\n        else:\n            fill_value = fill_tuple[0]\n\n        # axis doesn't matter; we are really a single-dim object\n        # but are passed the axis depending on the calling routing\n        # if its REALLY axis 0, then this will be a reindex and not a take\n        new_values = self.values.take(indexer, fill_value=fill_value,\n                                      allow_fill=True)\n\n        if self.ndim == 1 and new_mgr_locs is None:\n            new_mgr_locs = [0]\n        else:\n            if new_mgr_locs is None:\n                new_mgr_locs = self.mgr_locs\n\n        return self.make_block_same_class(new_values, new_mgr_locs)\n\n    def _can_hold_element(self, element):\n        # XXX: We may need to think about pushing this onto the array.\n        # We're doing the same as CategoricalBlock here.\n        return True\n\n    def _slice(self, slicer):\n        \"\"\" return a slice of my values \"\"\"\n\n        # slice the category\n        # return same dims as we currently have\n\n        if isinstance(slicer, tuple) and len(slicer) == 2:\n            if not com.is_null_slice(slicer[0]):\n                raise AssertionError(\"invalid slicing for a 1-ndim \"\n                                     \"categorical\")\n            slicer = slicer[1]\n\n        return self.values[slicer]\n\n    def _try_cast_result(self, result, dtype=None):\n        \"\"\"\n        if we have an operation that operates on for example floats\n        we want to try to cast back to our EA here if possible\n\n        result could be a 2-D numpy array, e.g. the result of\n        a numeric operation; but it must be shape (1, X) because\n        we by-definition operate on the ExtensionBlocks one-by-one\n\n        result could also be an EA Array itself, in which case it\n        is already a 1-D array\n        \"\"\"\n        try:\n\n            result = self._holder._from_sequence(\n                result.ravel(), dtype=dtype)\n        except Exception:\n            pass\n\n        return result\n\n    def formatting_values(self):\n        # Deprecating the ability to override _formatting_values.\n        # Do the warning here, it's only user in pandas, since we\n        # have to check if the subclass overrode it.\n        fv = getattr(type(self.values), '_formatting_values', None)\n        if fv and fv != ExtensionArray._formatting_values:\n            msg = (\n                \"'ExtensionArray._formatting_values' is deprecated. \"\n                \"Specify 'ExtensionArray._formatter' instead.\"\n            )\n            warnings.warn(msg, DeprecationWarning, stacklevel=10)\n            return self.values._formatting_values()\n\n        return self.values\n\n    def concat_same_type(self, to_concat, placement=None):\n        \"\"\"\n        Concatenate list of single blocks of the same type.\n        \"\"\"\n        values = self._holder._concat_same_type(\n            [blk.values for blk in to_concat])\n        placement = placement or slice(0, len(values), 1)\n        return self.make_block_same_class(values, ndim=self.ndim,\n                                          placement=placement)\n\n    def fillna(self, value, limit=None, inplace=False, downcast=None):\n        values = self.values if inplace else self.values.copy()\n        values = values.fillna(value=value, limit=limit)\n        return [self.make_block_same_class(values=values,\n                                           placement=self.mgr_locs,\n                                           ndim=self.ndim)]\n\n    def interpolate(self, method='pad', axis=0, inplace=False, limit=None,\n                    fill_value=None, **kwargs):\n\n        values = self.values if inplace else self.values.copy()\n        return self.make_block_same_class(\n            values=values.fillna(value=fill_value, method=method,\n                                 limit=limit),\n            placement=self.mgr_locs)\n\n    def shift(self,\n              periods: int,\n              axis: libinternals.BlockPlacement = 0,\n              fill_value: Any = None) -> List['ExtensionBlock']:\n        \"\"\"\n        Shift the block by `periods`.\n\n        Dispatches to underlying ExtensionArray and re-boxes in an\n        ExtensionBlock.\n        \"\"\"\n        return [\n            self.make_block_same_class(\n                self.values.shift(periods=periods, fill_value=fill_value),\n                placement=self.mgr_locs, ndim=self.ndim)\n        ]\n\n    def where(self, other, cond, align=True, errors='raise',\n              try_cast=False, axis=0, transpose=False):\n        if isinstance(other, ABCDataFrame):\n            # ExtensionArrays are 1-D, so if we get here then\n            # `other` should be a DataFrame with a single column.\n            assert other.shape[1] == 1\n            other = other.iloc[:, 0]\n\n        other = extract_array(other, extract_numpy=True)\n\n        if isinstance(cond, ABCDataFrame):\n            assert cond.shape[1] == 1\n            cond = cond.iloc[:, 0]\n\n        cond = extract_array(cond, extract_numpy=True)\n\n        if lib.is_scalar(other) and isna(other):\n            # The default `other` for Series / Frame is np.nan\n            # we want to replace that with the correct NA value\n            # for the type\n            other = self.dtype.na_value\n\n        if is_sparse(self.values):\n            # TODO(SparseArray.__setitem__): remove this if condition\n            # We need to re-infer the type of the data after doing the\n            # where, for cases where the subtypes don't match\n            dtype = None\n        else:\n            dtype = self.dtype\n\n        try:\n            result = self.values.copy()\n            icond = ~cond\n            if lib.is_scalar(other):\n                result[icond] = other\n            else:\n                result[icond] = other[icond]\n        except (NotImplementedError, TypeError):\n            # NotImplementedError for class not implementing `__setitem__`\n            # TypeError for SparseArray, which implements just to raise\n            # a TypeError\n            result = self._holder._from_sequence(\n                np.where(cond, self.values, other),\n                dtype=dtype,\n            )\n\n        return self.make_block_same_class(result, placement=self.mgr_locs)\n\n    @property\n    def _ftype(self):\n        return getattr(self.values, '_pandas_ftype', Block._ftype)\n\n    def _unstack(self, unstacker_func, new_columns, n_rows, fill_value):\n        # ExtensionArray-safe unstack.\n        # We override ObjectBlock._unstack, which unstacks directly on the\n        # values of the array. For EA-backed blocks, this would require\n        # converting to a 2-D ndarray of objects.\n        # Instead, we unstack an ndarray of integer positions, followed by\n        # a `take` on the actual values.\n        dummy_arr = np.arange(n_rows)\n        dummy_unstacker = functools.partial(unstacker_func, fill_value=-1)\n        unstacker = dummy_unstacker(dummy_arr)\n\n        new_placement, new_values, mask = self._get_unstack_items(\n            unstacker, new_columns\n        )\n\n        blocks = [\n            self.make_block_same_class(\n                self.values.take(indices, allow_fill=True,\n                                 fill_value=fill_value),\n                [place])\n            for indices, place in zip(new_values.T, new_placement)\n        ]\n        return blocks, mask\n\n\nclass ObjectValuesExtensionBlock(ExtensionBlock):\n    \"\"\"\n    Block providing backwards-compatibility for `.values`.\n\n    Used by PeriodArray and IntervalArray to ensure that\n    Series[T].values is an ndarray of objects.\n    \"\"\"\n\n    def external_values(self, dtype=None):\n        return self.values.astype(object)\n\n\nclass NumericBlock(Block):\n    __slots__ = ()\n    is_numeric = True\n    _can_hold_na = True\n\n\nclass FloatOrComplexBlock(NumericBlock):\n    __slots__ = ()\n\n    def equals(self, other):\n        if self.dtype != other.dtype or self.shape != other.shape:\n            return False\n        left, right = self.values, other.values\n        return ((left == right) | (np.isnan(left) & np.isnan(right))).all()\n\n\nclass FloatBlock(FloatOrComplexBlock):\n    __slots__ = ()\n    is_float = True\n\n    def _can_hold_element(self, element):\n        tipo = maybe_infer_dtype_type(element)\n        if tipo is not None:\n            return (issubclass(tipo.type, (np.floating, np.integer)) and\n                    not issubclass(tipo.type, (np.datetime64, np.timedelta64)))\n        return (\n            isinstance(\n                element, (float, int, np.floating, np.int_)) and\n            not isinstance(element, (bool, np.bool_, datetime, timedelta,\n                                     np.datetime64, np.timedelta64)))\n\n    def to_native_types(self, slicer=None, na_rep='', float_format=None,\n                        decimal='.', quoting=None, **kwargs):\n        \"\"\" convert to our native types format, slicing if desired \"\"\"\n\n        values = self.values\n        if slicer is not None:\n            values = values[:, slicer]\n\n        # see gh-13418: no special formatting is desired at the\n        # output (important for appropriate 'quoting' behaviour),\n        # so do not pass it through the FloatArrayFormatter\n        if float_format is None and decimal == '.':\n            mask = isna(values)\n\n            if not quoting:\n                values = values.astype(str)\n            else:\n                values = np.array(values, dtype='object')\n\n            values[mask] = na_rep\n            return values\n\n        from pandas.io.formats.format import FloatArrayFormatter\n        formatter = FloatArrayFormatter(values, na_rep=na_rep,\n                                        float_format=float_format,\n                                        decimal=decimal, quoting=quoting,\n                                        fixed_width=False)\n        return formatter.get_result_as_array()\n\n    def should_store(self, value):\n        # when inserting a column should not coerce integers to floats\n        # unnecessarily\n        return (issubclass(value.dtype.type, np.floating) and\n                value.dtype == self.dtype)\n\n\nclass ComplexBlock(FloatOrComplexBlock):\n    __slots__ = ()\n    is_complex = True\n\n    def _can_hold_element(self, element):\n        tipo = maybe_infer_dtype_type(element)\n        if tipo is not None:\n            return issubclass(tipo.type,\n                              (np.floating, np.integer, np.complexfloating))\n        return (\n            isinstance(\n                element,\n                (float, int, complex, np.float_, np.int_)) and\n            not isinstance(element, (bool, np.bool_)))\n\n    def should_store(self, value):\n        return issubclass(value.dtype.type, np.complexfloating)\n\n\nclass IntBlock(NumericBlock):\n    __slots__ = ()\n    is_integer = True\n    _can_hold_na = False\n\n    def _can_hold_element(self, element):\n        tipo = maybe_infer_dtype_type(element)\n        if tipo is not None:\n            return (issubclass(tipo.type, np.integer) and\n                    not issubclass(tipo.type, (np.datetime64,\n                                               np.timedelta64)) and\n                    self.dtype.itemsize >= tipo.itemsize)\n        return is_integer(element)\n\n    def should_store(self, value):\n        return is_integer_dtype(value) and value.dtype == self.dtype\n\n\nclass DatetimeLikeBlockMixin:\n    \"\"\"Mixin class for DatetimeBlock, DatetimeTZBlock, and TimedeltaBlock.\"\"\"\n\n    @property\n    def _holder(self):\n        return DatetimeArray\n\n    @property\n    def fill_value(self):\n        return tslibs.iNaT\n\n    def get_values(self, dtype=None):\n        \"\"\"\n        return object dtype as boxed values, such as Timestamps/Timedelta\n        \"\"\"\n        if is_object_dtype(dtype):\n            values = self.values.ravel()\n            result = self._holder(values).astype(object)\n            return result.reshape(self.values.shape)\n        return self.values\n\n\nclass DatetimeBlock(DatetimeLikeBlockMixin, Block):\n    __slots__ = ()\n    is_datetime = True\n\n    def __init__(self, values, placement, ndim=None):\n        values = self._maybe_coerce_values(values)\n        super().__init__(values, placement=placement, ndim=ndim)\n\n    @property\n    def _can_hold_na(self):\n        return True\n\n    def _maybe_coerce_values(self, values):\n        \"\"\"Input validation for values passed to __init__. Ensure that\n        we have datetime64ns, coercing if necessary.\n\n        Parameters\n        ----------\n        values : array-like\n            Must be convertible to datetime64\n\n        Returns\n        -------\n        values : ndarray[datetime64ns]\n\n        Overridden by DatetimeTZBlock.\n        \"\"\"\n        if values.dtype != _NS_DTYPE:\n            values = conversion.ensure_datetime64ns(values)\n\n        if isinstance(values, DatetimeArray):\n            values = values._data\n\n        assert isinstance(values, np.ndarray), type(values)\n        return values\n\n    def _astype(self, dtype, **kwargs):\n        \"\"\"\n        these automatically copy, so copy=True has no effect\n        raise on an except if raise == True\n        \"\"\"\n        dtype = pandas_dtype(dtype)\n\n        # if we are passed a datetime64[ns, tz]\n        if is_datetime64tz_dtype(dtype):\n            values = self.values\n            if getattr(values, 'tz', None) is None:\n                values = DatetimeArray(values).tz_localize('UTC')\n            values = values.tz_convert(dtype.tz)\n            return self.make_block(values)\n\n        # delegate\n        return super()._astype(dtype=dtype, **kwargs)\n\n    def _can_hold_element(self, element):\n        tipo = maybe_infer_dtype_type(element)\n        if tipo is not None:\n            return tipo == _NS_DTYPE or tipo == np.int64\n        return (is_integer(element) or isinstance(element, datetime) or\n                isna(element))\n\n    def _try_coerce_args(self, values, other):\n        \"\"\"\n        Coerce values and other to dtype 'i8'. NaN and NaT convert to\n        the smallest i8, and will correctly round-trip to NaT if converted\n        back in _try_coerce_result. values is always ndarray-like, other\n        may not be\n\n        Parameters\n        ----------\n        values : ndarray-like\n        other : ndarray-like or scalar\n\n        Returns\n        -------\n        base-type values, base-type other\n        \"\"\"\n\n        values = values.view('i8')\n\n        if isinstance(other, bool):\n            raise TypeError\n        elif is_null_datetimelike(other):\n            other = tslibs.iNaT\n        elif isinstance(other, (datetime, np.datetime64, date)):\n            other = self._box_func(other)\n            if getattr(other, 'tz') is not None:\n                raise TypeError(\"cannot coerce a Timestamp with a tz on a \"\n                                \"naive Block\")\n            other = other.asm8.view('i8')\n        elif hasattr(other, 'dtype') and is_datetime64_dtype(other):\n            other = other.astype('i8', copy=False).view('i8')\n        else:\n            # coercion issues\n            # let higher levels handle\n            raise TypeError(other)\n\n        return values, other\n\n    def _try_coerce_result(self, result):\n        \"\"\" reverse of try_coerce_args \"\"\"\n        if isinstance(result, np.ndarray):\n            if result.dtype.kind in ['i', 'f']:\n                result = result.astype('M8[ns]')\n\n        elif isinstance(result, (np.integer, np.float, np.datetime64)):\n            result = self._box_func(result)\n        return result\n\n    @property\n    def _box_func(self):\n        return tslibs.Timestamp\n\n    def to_native_types(self, slicer=None, na_rep=None, date_format=None,\n                        quoting=None, **kwargs):\n        \"\"\" convert to our native types format, slicing if desired \"\"\"\n\n        values = self.values\n        i8values = self.values.view('i8')\n\n        if slicer is not None:\n            values = values[..., slicer]\n            i8values = i8values[..., slicer]\n\n        from pandas.io.formats.format import _get_format_datetime64_from_values\n        fmt = _get_format_datetime64_from_values(values, date_format)\n\n        result = tslib.format_array_from_datetime(\n            i8values.ravel(), tz=getattr(self.values, 'tz', None),\n            format=fmt, na_rep=na_rep).reshape(i8values.shape)\n        return np.atleast_2d(result)\n\n    def should_store(self, value):\n        return (issubclass(value.dtype.type, np.datetime64) and\n                not is_datetime64tz_dtype(value) and\n                not is_extension_array_dtype(value))\n\n    def set(self, locs, values):\n        \"\"\"\n        Modify Block in-place with new item value\n\n        Returns\n        -------\n        None\n        \"\"\"\n        values = conversion.ensure_datetime64ns(values, copy=False)\n\n        self.values[locs] = values\n\n    def external_values(self):\n        return np.asarray(self.values.astype('datetime64[ns]', copy=False))\n\n\nclass DatetimeTZBlock(ExtensionBlock, DatetimeBlock):\n    \"\"\" implement a datetime64 block with a tz attribute \"\"\"\n    __slots__ = ()\n    is_datetimetz = True\n    is_extension = True\n\n    @property\n    def _holder(self):\n        return DatetimeArray\n\n    def _maybe_coerce_values(self, values):\n        \"\"\"Input validation for values passed to __init__. Ensure that\n        we have datetime64TZ, coercing if necessary.\n\n        Parameters\n        ----------\n        values : array-like\n            Must be convertible to datetime64\n\n        Returns\n        -------\n        values : DatetimeArray\n        \"\"\"\n        if not isinstance(values, self._holder):\n            values = self._holder(values)\n\n        if values.tz is None:\n            raise ValueError(\"cannot create a DatetimeTZBlock without a tz\")\n\n        return values\n\n    @property\n    def is_view(self):\n        \"\"\" return a boolean if I am possibly a view \"\"\"\n        # check the ndarray values of the DatetimeIndex values\n        return self.values._data.base is not None\n\n    def copy(self, deep=True):\n        \"\"\" copy constructor \"\"\"\n        values = self.values\n        if deep:\n            values = values.copy()\n        return self.make_block_same_class(values)\n\n    def get_values(self, dtype=None):\n        \"\"\"\n        Returns an ndarray of values.\n\n        Parameters\n        ----------\n        dtype : np.dtype\n            Only `object`-like dtypes are respected here (not sure\n            why).\n\n        Returns\n        -------\n        values : ndarray\n            When ``dtype=object``, then and object-dtype ndarray of\n            boxed values is returned. Otherwise, an M8[ns] ndarray\n            is returned.\n\n            DatetimeArray is always 1-d. ``get_values`` will reshape\n            the return value to be the same dimensionality as the\n            block.\n        \"\"\"\n        values = self.values\n        if is_object_dtype(dtype):\n            values = values._box_values(values._data)\n\n        values = np.asarray(values)\n\n        if self.ndim == 2:\n            # Ensure that our shape is correct for DataFrame.\n            # ExtensionArrays are always 1-D, even in a DataFrame when\n            # the analogous NumPy-backed column would be a 2-D ndarray.\n            values = values.reshape(1, -1)\n        return values\n\n    def to_dense(self):\n        # we request M8[ns] dtype here, even though it discards tzinfo,\n        # as lots of code (e.g. anything using values_from_object)\n        # expects that behavior.\n        return np.asarray(self.values, dtype=_NS_DTYPE)\n\n    def _slice(self, slicer):\n        \"\"\" return a slice of my values \"\"\"\n        if isinstance(slicer, tuple):\n            col, loc = slicer\n            if not com.is_null_slice(col) and col != 0:\n                raise IndexError(\"{0} only contains one item\".format(self))\n            return self.values[loc]\n        return self.values[slicer]\n\n    def _try_coerce_args(self, values, other):\n        \"\"\"\n        localize and return i8 for the values\n\n        Parameters\n        ----------\n        values : ndarray-like\n        other : ndarray-like or scalar\n\n        Returns\n        -------\n        base-type values, base-type other\n        \"\"\"\n        # asi8 is a view, needs copy\n        values = _block_shape(values.view(\"i8\"), ndim=self.ndim)\n\n        if isinstance(other, ABCSeries):\n            other = self._holder(other)\n\n        if isinstance(other, bool):\n            raise TypeError\n        elif is_datetime64_dtype(other):\n            # add the tz back\n            other = self._holder(other, dtype=self.dtype)\n\n        elif is_null_datetimelike(other):\n            other = tslibs.iNaT\n        elif isinstance(other, self._holder):\n            if other.tz != self.values.tz:\n                raise ValueError(\"incompatible or non tz-aware value\")\n            other = _block_shape(other.asi8, ndim=self.ndim)\n        elif isinstance(other, (np.datetime64, datetime, date)):\n            other = tslibs.Timestamp(other)\n            tz = getattr(other, 'tz', None)\n\n            # test we can have an equal time zone\n            if tz is None or str(tz) != str(self.values.tz):\n                raise ValueError(\"incompatible or non tz-aware value\")\n            other = other.value\n        else:\n            raise TypeError(other)\n\n        return values, other\n\n    def _try_coerce_result(self, result):\n        \"\"\" reverse of try_coerce_args \"\"\"\n        if isinstance(result, np.ndarray):\n            if result.dtype.kind in ['i', 'f']:\n                result = result.astype('M8[ns]')\n\n        elif isinstance(result, (np.integer, np.float, np.datetime64)):\n            result = self._box_func(result)\n\n        if isinstance(result, np.ndarray):\n            # allow passing of > 1dim if its trivial\n\n            if result.ndim > 1:\n                result = result.reshape(np.prod(result.shape))\n            # GH#24096 new values invalidates a frequency\n            result = self._holder._simple_new(result, freq=None,\n                                              dtype=self.values.dtype)\n\n        return result\n\n    @property\n    def _box_func(self):\n        return lambda x: tslibs.Timestamp(x, tz=self.dtype.tz)\n\n    def diff(self, n, axis=0):\n        \"\"\"1st discrete difference\n\n        Parameters\n        ----------\n        n : int, number of periods to diff\n        axis : int, axis to diff upon. default 0\n\n        Returns\n        -------\n        A list with a new TimeDeltaBlock.\n\n        Notes\n        -----\n        The arguments here are mimicking shift so they are called correctly\n        by apply.\n        \"\"\"\n        if axis == 0:\n            # Cannot currently calculate diff across multiple blocks since this\n            # function is invoked via apply\n            raise NotImplementedError\n        new_values = (self.values - self.shift(n, axis=axis)[0].values).asi8\n\n        # Reshape the new_values like how algos.diff does for timedelta data\n        new_values = new_values.reshape(1, len(new_values))\n        new_values = new_values.astype('timedelta64[ns]')\n        return [TimeDeltaBlock(new_values, placement=self.mgr_locs.indexer)]\n\n    def concat_same_type(self, to_concat, placement=None):\n        # need to handle concat([tz1, tz2]) here, since DatetimeArray\n        # only handles cases where all the tzs are the same.\n        # Instead of placing the condition here, it could also go into the\n        # is_uniform_join_units check, but I'm not sure what is better.\n        if len({x.dtype for x in to_concat}) > 1:\n            values = _concat._concat_datetime([x.values for x in to_concat])\n            placement = placement or slice(0, len(values), 1)\n\n            if self.ndim > 1:\n                values = np.atleast_2d(values)\n            return ObjectBlock(values, ndim=self.ndim, placement=placement)\n        return super().concat_same_type(to_concat, placement)\n\n    def fillna(self, value, limit=None, inplace=False, downcast=None):\n        # We support filling a DatetimeTZ with a `value` whose timezone\n        # is different by coercing to object.\n        try:\n            return super().fillna(value, limit, inplace, downcast)\n        except (ValueError, TypeError):\n            # different timezones, or a non-tz\n            return self.astype(object).fillna(\n                value, limit=limit, inplace=inplace, downcast=downcast\n            )\n\n    def setitem(self, indexer, value):\n        # https://github.com/pandas-dev/pandas/issues/24020\n        # Need a dedicated setitem until #24020 (type promotion in setitem\n        # for extension arrays) is designed and implemented.\n        try:\n            return super().setitem(indexer, value)\n        except (ValueError, TypeError):\n            newb = make_block(self.values.astype(object),\n                              placement=self.mgr_locs,\n                              klass=ObjectBlock)\n            return newb.setitem(indexer, value)\n\n    def equals(self, other):\n        # override for significant performance improvement\n        if self.dtype != other.dtype or self.shape != other.shape:\n            return False\n        return (self.values.view('i8') == other.values.view('i8')).all()\n\n\nclass TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):\n    __slots__ = ()\n    is_timedelta = True\n    _can_hold_na = True\n    is_numeric = False\n\n    def __init__(self, values, placement, ndim=None):\n        if values.dtype != _TD_DTYPE:\n            values = conversion.ensure_timedelta64ns(values)\n        if isinstance(values, TimedeltaArray):\n            values = values._data\n        assert isinstance(values, np.ndarray), type(values)\n        super().__init__(values, placement=placement, ndim=ndim)\n\n    @property\n    def _holder(self):\n        return TimedeltaArray\n\n    @property\n    def _box_func(self):\n        return lambda x: Timedelta(x, unit='ns')\n\n    def _can_hold_element(self, element):\n        tipo = maybe_infer_dtype_type(element)\n        if tipo is not None:\n            return issubclass(tipo.type, (np.timedelta64, np.int64))\n        return is_integer(element) or isinstance(\n            element, (timedelta, np.timedelta64, np.int64))\n\n    def fillna(self, value, **kwargs):\n\n        # allow filling with integers to be\n        # interpreted as nanoseconds\n        if is_integer(value) and not isinstance(value, np.timedelta64):\n            # Deprecation GH#24694, GH#19233\n            warnings.warn(\"Passing integers to fillna is deprecated, will \"\n                          \"raise a TypeError in a future version.  To retain \"\n                          \"the old behavior, pass pd.Timedelta(seconds=n) \"\n                          \"instead.\",\n                          FutureWarning, stacklevel=6)\n            value = Timedelta(value, unit='s')\n        return super().fillna(value, **kwargs)\n\n    def _try_coerce_args(self, values, other):\n        \"\"\"\n        Coerce values and other to int64, with null values converted to\n        iNaT. values is always ndarray-like, other may not be\n\n        Parameters\n        ----------\n        values : ndarray-like\n        other : ndarray-like or scalar\n\n        Returns\n        -------\n        base-type values, base-type other\n        \"\"\"\n        values = values.view('i8')\n\n        if isinstance(other, bool):\n            raise TypeError\n        elif is_null_datetimelike(other):\n            other = tslibs.iNaT\n        elif isinstance(other, (timedelta, np.timedelta64)):\n            other = Timedelta(other).value\n        elif hasattr(other, 'dtype') and is_timedelta64_dtype(other):\n            other = other.astype('i8', copy=False).view('i8')\n        else:\n            # coercion issues\n            # let higher levels handle\n            raise TypeError(other)\n\n        return values, other\n\n    def _try_coerce_result(self, result):\n        \"\"\" reverse of try_coerce_args / try_operate \"\"\"\n        if isinstance(result, np.ndarray):\n            mask = isna(result)\n            if result.dtype.kind in ['i', 'f']:\n                result = result.astype('m8[ns]')\n            result[mask] = tslibs.iNaT\n\n        elif isinstance(result, (np.integer, np.float)):\n            result = self._box_func(result)\n\n        return result\n\n    def should_store(self, value):\n        return (issubclass(value.dtype.type, np.timedelta64) and\n                not is_extension_array_dtype(value))\n\n    def to_native_types(self, slicer=None, na_rep=None, quoting=None,\n                        **kwargs):\n        \"\"\" convert to our native types format, slicing if desired \"\"\"\n\n        values = self.values\n        if slicer is not None:\n            values = values[:, slicer]\n        mask = isna(values)\n\n        rvalues = np.empty(values.shape, dtype=object)\n        if na_rep is None:\n            na_rep = 'NaT'\n        rvalues[mask] = na_rep\n        imask = (~mask).ravel()\n\n        # FIXME:\n        # should use the formats.format.Timedelta64Formatter here\n        # to figure what format to pass to the Timedelta\n        # e.g. to not show the decimals say\n        rvalues.flat[imask] = np.array([Timedelta(val)._repr_base(format='all')\n                                        for val in values.ravel()[imask]],\n                                       dtype=object)\n        return rvalues\n\n    def external_values(self, dtype=None):\n        return np.asarray(self.values.astype(\"timedelta64[ns]\", copy=False))\n\n\nclass BoolBlock(NumericBlock):\n    __slots__ = ()\n    is_bool = True\n    _can_hold_na = False\n\n    def _can_hold_element(self, element):\n        tipo = maybe_infer_dtype_type(element)\n        if tipo is not None:\n            return issubclass(tipo.type, np.bool_)\n        return isinstance(element, (bool, np.bool_))\n\n    def should_store(self, value):\n        return (issubclass(value.dtype.type, np.bool_) and not\n                is_extension_array_dtype(value))\n\n    def replace(self, to_replace, value, inplace=False, filter=None,\n                regex=False, convert=True):\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        to_replace_values = np.atleast_1d(to_replace)\n        if not np.can_cast(to_replace_values, bool):\n            return self\n        return super().replace(to_replace, value, inplace=inplace,\n                               filter=filter, regex=regex, convert=convert)\n\n\nclass ObjectBlock(Block):\n    __slots__ = ()\n    is_object = True\n    _can_hold_na = True\n\n    def __init__(self, values, placement=None, ndim=2):\n        if issubclass(values.dtype.type, str):\n            values = np.array(values, dtype=object)\n\n        super().__init__(values, ndim=ndim, placement=placement)\n\n    @property\n    def is_bool(self):\n        \"\"\" we can be a bool if we have only bool values but are of type\n        object\n        \"\"\"\n        return lib.is_bool_array(self.values.ravel())\n\n    # TODO: Refactor when convert_objects is removed since there will be 1 path\n    def convert(self, *args, **kwargs):\n        \"\"\" attempt to coerce any object types to better types return a copy of\n        the block (if copy = True) by definition we ARE an ObjectBlock!!!!!\n\n        can return multiple blocks!\n        \"\"\"\n\n        if args:\n            raise NotImplementedError\n        by_item = kwargs.get('by_item', True)\n\n        new_inputs = ['coerce', 'datetime', 'numeric', 'timedelta']\n        new_style = False\n        for kw in new_inputs:\n            new_style |= kw in kwargs\n\n        if new_style:\n            fn = soft_convert_objects\n            fn_inputs = new_inputs\n        else:\n            fn = maybe_convert_objects\n            fn_inputs = ['convert_dates', 'convert_numeric',\n                         'convert_timedeltas']\n        fn_inputs += ['copy']\n\n        fn_kwargs = {key: kwargs[key] for key in fn_inputs if key in kwargs}\n\n        # operate column-by-column\n        def f(m, v, i):\n            shape = v.shape\n            values = fn(v.ravel(), **fn_kwargs)\n            try:\n                values = values.reshape(shape)\n            except (AttributeError, NotImplementedError):\n                pass\n\n            values = _block_shape(values, ndim=self.ndim)\n            return values\n\n        if by_item and not self._is_single_block:\n            blocks = self.split_and_operate(None, f, False)\n        else:\n            values = f(None, self.values.ravel(), None)\n            blocks = [make_block(values, ndim=self.ndim,\n                                 placement=self.mgr_locs)]\n\n        return blocks\n\n    def set(self, locs, values):\n        \"\"\"\n        Modify Block in-place with new item value\n\n        Returns\n        -------\n        None\n        \"\"\"\n        try:\n            self.values[locs] = values\n        except (ValueError):\n\n            # broadcasting error\n            # see GH6171\n            new_shape = list(values.shape)\n            new_shape[0] = len(self.items)\n            self.values = np.empty(tuple(new_shape), dtype=self.dtype)\n            self.values.fill(np.nan)\n            self.values[locs] = values\n\n    def _maybe_downcast(self, blocks, downcast=None):\n\n        if downcast is not None:\n            return blocks\n\n        # split and convert the blocks\n        return _extend_blocks([b.convert(datetime=True, numeric=False)\n                               for b in blocks])\n\n    def _can_hold_element(self, element):\n        return True\n\n    def _try_coerce_args(self, values, other):\n        \"\"\" provide coercion to our input arguments \"\"\"\n\n        if isinstance(other, ABCDatetimeIndex):\n            # May get a DatetimeIndex here. Unbox it.\n            other = other.array\n\n        if isinstance(other, DatetimeArray):\n            # hit in pandas/tests/indexing/test_coercion.py\n            # ::TestWhereCoercion::test_where_series_datetime64[datetime64tz]\n            # when falling back to ObjectBlock.where\n            other = other.astype(object)\n\n        return values, other\n\n    def should_store(self, value):\n        return not (issubclass(value.dtype.type,\n                               (np.integer, np.floating, np.complexfloating,\n                                np.datetime64, np.bool_)) or\n                    # TODO(ExtensionArray): remove is_extension_type\n                    # when all extension arrays have been ported.\n                    is_extension_type(value) or\n                    is_extension_array_dtype(value))\n\n    def replace(self, to_replace, value, inplace=False, filter=None,\n                regex=False, convert=True):\n        to_rep_is_list = is_list_like(to_replace)\n        value_is_list = is_list_like(value)\n        both_lists = to_rep_is_list and value_is_list\n        either_list = to_rep_is_list or value_is_list\n\n        result_blocks = []\n        blocks = [self]\n\n        if not either_list and is_re(to_replace):\n            return self._replace_single(to_replace, value, inplace=inplace,\n                                        filter=filter, regex=True,\n                                        convert=convert)\n        elif not (either_list or regex):\n            return super().replace(to_replace, value, inplace=inplace,\n                                   filter=filter, regex=regex, convert=convert)\n        elif both_lists:\n            for to_rep, v in zip(to_replace, value):\n                result_blocks = []\n                for b in blocks:\n                    result = b._replace_single(to_rep, v, inplace=inplace,\n                                               filter=filter, regex=regex,\n                                               convert=convert)\n                    result_blocks = _extend_blocks(result, result_blocks)\n                blocks = result_blocks\n            return result_blocks\n\n        elif to_rep_is_list and regex:\n            for to_rep in to_replace:\n                result_blocks = []\n                for b in blocks:\n                    result = b._replace_single(to_rep, value, inplace=inplace,\n                                               filter=filter, regex=regex,\n                                               convert=convert)\n                    result_blocks = _extend_blocks(result, result_blocks)\n                blocks = result_blocks\n            return result_blocks\n\n        return self._replace_single(to_replace, value, inplace=inplace,\n                                    filter=filter, convert=convert,\n                                    regex=regex)\n\n    def _replace_single(self, to_replace, value, inplace=False, filter=None,\n                        regex=False, convert=True, mask=None):\n        \"\"\"\n        Replace elements by the given value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        inplace : bool, default False\n            Perform inplace modification.\n        filter : list, optional\n        regex : bool, default False\n            If true, perform regular expression substitution.\n        convert : bool, default True\n            If true, try to coerce any object types to better types.\n        mask : array-like of bool, optional\n            True indicate corresponding element is ignored.\n\n        Returns\n        -------\n        a new block, the result after replacing\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n\n        # to_replace is regex compilable\n        to_rep_re = regex and is_re_compilable(to_replace)\n\n        # regex is regex compilable\n        regex_re = is_re_compilable(regex)\n\n        # only one will survive\n        if to_rep_re and regex_re:\n            raise AssertionError('only one of to_replace and regex can be '\n                                 'regex compilable')\n\n        # if regex was passed as something that can be a regex (rather than a\n        # boolean)\n        if regex_re:\n            to_replace = regex\n\n        regex = regex_re or to_rep_re\n\n        # try to get the pattern attribute (compiled re) or it's a string\n        try:\n            pattern = to_replace.pattern\n        except AttributeError:\n            pattern = to_replace\n\n        # if the pattern is not empty and to_replace is either a string or a\n        # regex\n        if regex and pattern:\n            rx = re.compile(to_replace)\n        else:\n            # if the thing to replace is not a string or compiled regex call\n            # the superclass method -> to_replace is some kind of object\n            return super().replace(to_replace, value, inplace=inplace,\n                                   filter=filter, regex=regex)\n\n        new_values = self.values if inplace else self.values.copy()\n\n        # deal with replacing values with objects (strings) that match but\n        # whose replacement is not a string (numeric, nan, object)\n        if isna(value) or not isinstance(value, str):\n\n            def re_replacer(s):\n                try:\n                    return value if rx.search(s) is not None else s\n                except TypeError:\n                    return s\n        else:\n            # value is guaranteed to be a string here, s can be either a string\n            # or null if it's null it gets returned\n            def re_replacer(s):\n                try:\n                    return rx.sub(value, s)\n                except TypeError:\n                    return s\n\n        f = np.vectorize(re_replacer, otypes=[self.dtype])\n\n        if filter is None:\n            filt = slice(None)\n        else:\n            filt = self.mgr_locs.isin(filter).nonzero()[0]\n\n        if mask is None:\n            new_values[filt] = f(new_values[filt])\n        else:\n            new_values[filt][mask] = f(new_values[filt][mask])\n\n        # convert\n        block = self.make_block(new_values)\n        if convert:\n            block = block.convert(by_item=True, numeric=False)\n        return block\n\n    def _replace_coerce(self, to_replace, value, inplace=True, regex=False,\n                        convert=False, mask=None):\n        \"\"\"\n        Replace value corresponding to the given boolean array with another\n        value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        inplace : bool, default False\n            Perform inplace modification.\n        regex : bool, default False\n            If true, perform regular expression substitution.\n        convert : bool, default True\n            If true, try to coerce any object types to better types.\n        mask : array-like of bool, optional\n            True indicate corresponding element is ignored.\n\n        Returns\n        -------\n        A new block if there is anything to replace or the original block.\n        \"\"\"\n        if mask.any():\n            block = super()._replace_coerce(\n                to_replace=to_replace, value=value, inplace=inplace,\n                regex=regex, convert=convert, mask=mask)\n            if convert:\n                block = [b.convert(by_item=True, numeric=False, copy=True)\n                         for b in block]\n            return block\n        return self\n\n\nclass CategoricalBlock(ExtensionBlock):\n    __slots__ = ()\n    is_categorical = True\n    _verify_integrity = True\n    _can_hold_na = True\n    _concatenator = staticmethod(_concat._concat_categorical)\n\n    def __init__(self, values, placement, ndim=None):\n        from pandas.core.arrays.categorical import _maybe_to_categorical\n\n        # coerce to categorical if we can\n        super().__init__(_maybe_to_categorical(values),\n                         placement=placement,\n                         ndim=ndim)\n\n    @property\n    def _holder(self):\n        return Categorical\n\n    @property\n    def array_dtype(self):\n        \"\"\" the dtype to return if I want to construct this block as an\n        array\n        \"\"\"\n        return np.object_\n\n    def _try_coerce_result(self, result):\n        \"\"\" reverse of try_coerce_args \"\"\"\n\n        # GH12564: CategoricalBlock is 1-dim only\n        # while returned results could be any dim\n        if ((not is_categorical_dtype(result)) and\n                isinstance(result, np.ndarray)):\n            result = _block_shape(result, ndim=self.ndim)\n\n        return result\n\n    def to_dense(self):\n        # Categorical.get_values returns a DatetimeIndex for datetime\n        # categories, so we can't simply use `np.asarray(self.values)` like\n        # other types.\n        return self.values.get_values()\n\n    def to_native_types(self, slicer=None, na_rep='', quoting=None, **kwargs):\n        \"\"\" convert to our native types format, slicing if desired \"\"\"\n\n        values = self.values\n        if slicer is not None:\n            # Categorical is always one dimension\n            values = values[slicer]\n        mask = isna(values)\n        values = np.array(values, dtype='object')\n        values[mask] = na_rep\n\n        # we are expected to return a 2-d ndarray\n        return values.reshape(1, len(values))\n\n    def concat_same_type(self, to_concat, placement=None):\n        \"\"\"\n        Concatenate list of single blocks of the same type.\n\n        Note that this CategoricalBlock._concat_same_type *may* not\n        return a CategoricalBlock. When the categories in `to_concat`\n        differ, this will return an object ndarray.\n\n        If / when we decide we don't like that behavior:\n\n        1. Change Categorical._concat_same_type to use union_categoricals\n        2. Delete this method.\n        \"\"\"\n        values = self._concatenator([blk.values for blk in to_concat],\n                                    axis=self.ndim - 1)\n        # not using self.make_block_same_class as values can be object dtype\n        return make_block(\n            values, placement=placement or slice(0, len(values), 1),\n            ndim=self.ndim)\n\n    def where(self, other, cond, align=True, errors='raise',\n              try_cast=False, axis=0, transpose=False):\n        # TODO(CategoricalBlock.where):\n        # This can all be deleted in favor of ExtensionBlock.where once\n        # we enforce the deprecation.\n        object_msg = (\n            \"Implicitly converting categorical to object-dtype ndarray. \"\n            \"One or more of the values in 'other' are not present in this \"\n            \"categorical's categories. A future version of pandas will raise \"\n            \"a ValueError when 'other' contains different categories.\\n\\n\"\n            \"To preserve the current behavior, add the new categories to \"\n            \"the categorical before calling 'where', or convert the \"\n            \"categorical to a different dtype.\"\n        )\n        try:\n            # Attempt to do preserve categorical dtype.\n            result = super().where(\n                other, cond, align, errors, try_cast, axis, transpose\n            )\n        except (TypeError, ValueError):\n            warnings.warn(object_msg, FutureWarning, stacklevel=6)\n            result = self.astype(object).where(other, cond, align=align,\n                                               errors=errors,\n                                               try_cast=try_cast,\n                                               axis=axis, transpose=transpose)\n        return result\n\n\n# -----------------------------------------------------------------\n# Constructor Helpers\n\ndef get_block_type(values, dtype=None):\n    \"\"\"\n    Find the appropriate Block subclass to use for the given values and dtype.\n\n    Parameters\n    ----------\n    values : ndarray-like\n    dtype : numpy or pandas dtype\n\n    Returns\n    -------\n    cls : class, subclass of Block\n    \"\"\"\n    dtype = dtype or values.dtype\n    vtype = dtype.type\n\n    if is_sparse(dtype):\n        # Need this first(ish) so that Sparse[datetime] is sparse\n        cls = ExtensionBlock\n    elif is_categorical(values):\n        cls = CategoricalBlock\n    elif issubclass(vtype, np.datetime64):\n        assert not is_datetime64tz_dtype(values)\n        cls = DatetimeBlock\n    elif is_datetime64tz_dtype(values):\n        cls = DatetimeTZBlock\n    elif is_interval_dtype(dtype) or is_period_dtype(dtype):\n        cls = ObjectValuesExtensionBlock\n    elif is_extension_array_dtype(values):\n        cls = ExtensionBlock\n    elif issubclass(vtype, np.floating):\n        cls = FloatBlock\n    elif issubclass(vtype, np.timedelta64):\n        assert issubclass(vtype, np.integer)\n        cls = TimeDeltaBlock\n    elif issubclass(vtype, np.complexfloating):\n        cls = ComplexBlock\n    elif issubclass(vtype, np.integer):\n        cls = IntBlock\n    elif dtype == np.bool_:\n        cls = BoolBlock\n    else:\n        cls = ObjectBlock\n    return cls\n\n\ndef make_block(values, placement, klass=None, ndim=None, dtype=None,\n               fastpath=None):\n    # Ensure that we don't allow PandasArray / PandasDtype in internals.\n    # For now, blocks should be backed by ndarrays when possible.\n    if isinstance(values, ABCPandasArray):\n        values = values.to_numpy()\n        if ndim and ndim > 1:\n            values = np.atleast_2d(values)\n\n    if isinstance(dtype, PandasDtype):\n        dtype = dtype.numpy_dtype\n\n    if fastpath is not None:\n        # GH#19265 pyarrow is passing this\n        warnings.warn(\"fastpath argument is deprecated, will be removed \"\n                      \"in a future release.\", DeprecationWarning)\n    if klass is None:\n        dtype = dtype or values.dtype\n        klass = get_block_type(values, dtype)\n\n    elif klass is DatetimeTZBlock and not is_datetime64tz_dtype(values):\n        # TODO: This is no longer hit internally; does it need to be retained\n        #  for e.g. pyarrow?\n        values = DatetimeArray._simple_new(values, dtype=dtype)\n\n    return klass(values, ndim=ndim, placement=placement)\n\n\n# -----------------------------------------------------------------\n\ndef _extend_blocks(result, blocks=None):\n    \"\"\" return a new extended blocks, givin the result \"\"\"\n    from pandas.core.internals import BlockManager\n    if blocks is None:\n        blocks = []\n    if isinstance(result, list):\n        for r in result:\n            if isinstance(r, list):\n                blocks.extend(r)\n            else:\n                blocks.append(r)\n    elif isinstance(result, BlockManager):\n        blocks.extend(result.blocks)\n    else:\n        blocks.append(result)\n    return blocks\n\n\ndef _block_shape(values, ndim=1, shape=None):\n    \"\"\" guarantee the shape of the values to be at least 1 d \"\"\"\n    if values.ndim < ndim:\n        if shape is None:\n            shape = values.shape\n        if not is_extension_array_dtype(values):\n            # TODO: https://github.com/pandas-dev/pandas/issues/23023\n            # block.shape is incorrect for \"2D\" ExtensionArrays\n            # We can't, and don't need to, reshape.\n            values = values.reshape(tuple((1, ) + shape))\n    return values\n\n\ndef _merge_blocks(blocks, dtype=None, _can_consolidate=True):\n\n    if len(blocks) == 1:\n        return blocks[0]\n\n    if _can_consolidate:\n\n        if dtype is None:\n            if len({b.dtype for b in blocks}) != 1:\n                raise AssertionError(\"_merge_blocks are invalid!\")\n            dtype = blocks[0].dtype\n\n        # FIXME: optimization potential in case all mgrs contain slices and\n        # combination of those slices is a slice, too.\n        new_mgr_locs = np.concatenate([b.mgr_locs.as_array for b in blocks])\n        new_values = np.vstack([b.values for b in blocks])\n\n        argsort = np.argsort(new_mgr_locs)\n        new_values = new_values[argsort]\n        new_mgr_locs = new_mgr_locs[argsort]\n\n        return make_block(new_values, placement=new_mgr_locs)\n\n    # no merge\n    return blocks\n\n\ndef _safe_reshape(arr, new_shape):\n    \"\"\"\n    If possible, reshape `arr` to have shape `new_shape`,\n    with a couple of exceptions (see gh-13012):\n\n    1) If `arr` is a ExtensionArray or Index, `arr` will be\n       returned as is.\n    2) If `arr` is a Series, the `_values` attribute will\n       be reshaped and returned.\n\n    Parameters\n    ----------\n    arr : array-like, object to be reshaped\n    new_shape : int or tuple of ints, the new shape\n    \"\"\"\n    if isinstance(arr, ABCSeries):\n        arr = arr._values\n    if not isinstance(arr, ABCExtensionArray):\n        arr = arr.reshape(new_shape)\n    return arr\n\n\ndef _putmask_smart(v, m, n):\n    \"\"\"\n    Return a new ndarray, try to preserve dtype if possible.\n\n    Parameters\n    ----------\n    v : `values`, updated in-place (array like)\n    m : `mask`, applies to both sides (array like)\n    n : `new values` either scalar or an array like aligned with `values`\n\n    Returns\n    -------\n    values : ndarray with updated values\n        this *may* be a copy of the original\n\n    See Also\n    --------\n    ndarray.putmask\n    \"\"\"\n\n    # we cannot use np.asarray() here as we cannot have conversions\n    # that numpy does when numeric are mixed with strings\n\n    # n should be the length of the mask or a scalar here\n    if not is_list_like(n):\n        n = np.repeat(n, len(m))\n\n    # see if we are only masking values that if putted\n    # will work in the current dtype\n    try:\n        nn = n[m]\n\n        # make sure that we have a nullable type\n        # if we have nulls\n        if not _isna_compat(v, nn[0]):\n            raise ValueError\n\n        # we ignore ComplexWarning here\n        with warnings.catch_warnings(record=True):\n            warnings.simplefilter(\"ignore\", np.ComplexWarning)\n            nn_at = nn.astype(v.dtype)\n\n        # avoid invalid dtype comparisons\n        # between numbers & strings\n\n        # only compare integers/floats\n        # don't compare integers to datetimelikes\n        if (not is_numeric_v_string_like(nn, nn_at) and\n            (is_float_dtype(nn.dtype) or\n             is_integer_dtype(nn.dtype) and\n             is_float_dtype(nn_at.dtype) or\n             is_integer_dtype(nn_at.dtype))):\n\n            comp = (nn == nn_at)\n            if is_list_like(comp) and comp.all():\n                nv = v.copy()\n                nv[m] = nn_at\n                return nv\n    except (ValueError, IndexError, TypeError, OverflowError):\n        pass\n\n    n = np.asarray(n)\n\n    def _putmask_preserve(nv, n):\n        try:\n            nv[m] = n[m]\n        except (IndexError, ValueError):\n            nv[m] = n\n        return nv\n\n    # preserves dtype if possible\n    if v.dtype.kind == n.dtype.kind:\n        return _putmask_preserve(v, n)\n\n    # change the dtype if needed\n    dtype, _ = maybe_promote(n.dtype)\n\n    if is_extension_type(v.dtype) and is_object_dtype(dtype):\n        v = v.get_values(dtype)\n    else:\n        v = v.astype(dtype)\n\n    return _putmask_preserve(v, n)\n"
    },
    {
      "filename": "pandas/core/internals/construction.py",
      "content": "\"\"\"\nFunctions for preparing various inputs passed to the DataFrame or Series\nconstructors before passing them to a BlockManager.\n\"\"\"\nfrom collections import OrderedDict, abc\n\nimport numpy as np\nimport numpy.ma as ma\n\nfrom pandas._libs import lib\nfrom pandas._libs.tslibs import IncompatibleFrequency, OutOfBoundsDatetime\nfrom pandas.compat import raise_with_traceback\n\nfrom pandas.core.dtypes.cast import (\n    construct_1d_arraylike_from_scalar, construct_1d_ndarray_preserving_na,\n    construct_1d_object_array_from_listlike, infer_dtype_from_scalar,\n    maybe_cast_to_datetime, maybe_cast_to_integer_array, maybe_castable,\n    maybe_convert_platform, maybe_infer_to_datetimelike, maybe_upcast)\nfrom pandas.core.dtypes.common import (\n    is_categorical_dtype, is_datetime64tz_dtype, is_dtype_equal,\n    is_extension_array_dtype, is_extension_type, is_float_dtype,\n    is_integer_dtype, is_iterator, is_list_like, is_object_dtype, pandas_dtype)\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame, ABCDatetimeIndex, ABCIndexClass, ABCPandasArray,\n    ABCPeriodIndex, ABCSeries, ABCTimedeltaIndex)\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core import algorithms, common as com\nfrom pandas.core.arrays import Categorical, ExtensionArray, period_array\nfrom pandas.core.index import (\n    Index, _get_objs_combined_axis, _union_indexes, ensure_index)\nfrom pandas.core.indexes import base as ibase\nfrom pandas.core.internals import (\n    create_block_manager_from_arrays, create_block_manager_from_blocks)\nfrom pandas.core.internals.arrays import extract_array\n\n# ---------------------------------------------------------------------\n# BlockManager Interface\n\n\ndef arrays_to_mgr(arrays, arr_names, index, columns, dtype=None):\n    \"\"\"\n    Segregate Series based on type and coerce into matrices.\n\n    Needs to handle a lot of exceptional cases.\n    \"\"\"\n    # figure out the index, if necessary\n    if index is None:\n        index = extract_index(arrays)\n    else:\n        index = ensure_index(index)\n\n    # don't force copy because getting jammed in an ndarray anyway\n    arrays = _homogenize(arrays, index, dtype)\n\n    # from BlockManager perspective\n    axes = [ensure_index(columns), index]\n\n    return create_block_manager_from_arrays(arrays, arr_names, axes)\n\n\ndef masked_rec_array_to_mgr(data, index, columns, dtype, copy):\n    \"\"\"\n    Extract from a masked rec array and create the manager.\n    \"\"\"\n\n    # essentially process a record array then fill it\n    fill_value = data.fill_value\n    fdata = ma.getdata(data)\n    if index is None:\n        index = get_names_from_index(fdata)\n        if index is None:\n            index = ibase.default_index(len(data))\n    index = ensure_index(index)\n\n    if columns is not None:\n        columns = ensure_index(columns)\n    arrays, arr_columns = to_arrays(fdata, columns)\n\n    # fill if needed\n    new_arrays = []\n    for fv, arr, col in zip(fill_value, arrays, arr_columns):\n        mask = ma.getmaskarray(data[col])\n        if mask.any():\n            arr, fv = maybe_upcast(arr, fill_value=fv, copy=True)\n            arr[mask] = fv\n        new_arrays.append(arr)\n\n    # create the manager\n    arrays, arr_columns = reorder_arrays(new_arrays, arr_columns, columns)\n    if columns is None:\n        columns = arr_columns\n\n    mgr = arrays_to_mgr(arrays, arr_columns, index, columns, dtype)\n\n    if copy:\n        mgr = mgr.copy()\n    return mgr\n\n\n# ---------------------------------------------------------------------\n# DataFrame Constructor Interface\n\ndef init_ndarray(values, index, columns, dtype=None, copy=False):\n    # input must be a ndarray, list, Series, index\n\n    if isinstance(values, ABCSeries):\n        if columns is None:\n            if values.name is not None:\n                columns = [values.name]\n        if index is None:\n            index = values.index\n        else:\n            values = values.reindex(index)\n\n        # zero len case (GH #2234)\n        if not len(values) and columns is not None and len(columns):\n            values = np.empty((0, 1), dtype=object)\n\n    # we could have a categorical type passed or coerced to 'category'\n    # recast this to an arrays_to_mgr\n    if (is_categorical_dtype(getattr(values, 'dtype', None)) or\n            is_categorical_dtype(dtype)):\n\n        if not hasattr(values, 'dtype'):\n            values = prep_ndarray(values, copy=copy)\n            values = values.ravel()\n        elif copy:\n            values = values.copy()\n\n        index, columns = _get_axes(len(values), 1, index, columns)\n        return arrays_to_mgr([values], columns, index, columns,\n                             dtype=dtype)\n    elif is_extension_array_dtype(values):\n        # GH#19157\n        if columns is None:\n            columns = [0]\n        return arrays_to_mgr([values], columns, index, columns,\n                             dtype=dtype)\n\n    # by definition an array here\n    # the dtypes will be coerced to a single dtype\n    values = prep_ndarray(values, copy=copy)\n\n    if dtype is not None:\n        if not is_dtype_equal(values.dtype, dtype):\n            try:\n                values = values.astype(dtype)\n            except Exception as orig:\n                e = ValueError(\"failed to cast to '{dtype}' (Exception \"\n                               \"was: {orig})\".format(dtype=dtype,\n                                                     orig=orig))\n                raise_with_traceback(e)\n\n    index, columns = _get_axes(*values.shape, index=index, columns=columns)\n    values = values.T\n\n    # if we don't have a dtype specified, then try to convert objects\n    # on the entire block; this is to convert if we have datetimelike's\n    # embedded in an object type\n    if dtype is None and is_object_dtype(values):\n\n        if values.ndim == 2 and values.shape[0] != 1:\n            # transpose and separate blocks\n\n            dvals_list = [maybe_infer_to_datetimelike(row) for row in values]\n            for n in range(len(dvals_list)):\n                if isinstance(dvals_list[n], np.ndarray):\n                    dvals_list[n] = dvals_list[n].reshape(1, -1)\n\n            from pandas.core.internals.blocks import make_block\n\n            # TODO: What about re-joining object columns?\n            block_values = [make_block(dvals_list[n], placement=[n])\n                            for n in range(len(dvals_list))]\n\n        else:\n            datelike_vals = maybe_infer_to_datetimelike(values)\n            block_values = [datelike_vals]\n    else:\n        block_values = [values]\n\n    return create_block_manager_from_blocks(block_values, [columns, index])\n\n\ndef init_dict(data, index, columns, dtype=None):\n    \"\"\"\n    Segregate Series based on type and coerce into matrices.\n    Needs to handle a lot of exceptional cases.\n    \"\"\"\n    if columns is not None:\n        from pandas.core.series import Series\n        arrays = Series(data, index=columns, dtype=object)\n        data_names = arrays.index\n\n        missing = arrays.isnull()\n        if index is None:\n            # GH10856\n            # raise ValueError if only scalars in dict\n            index = extract_index(arrays[~missing])\n        else:\n            index = ensure_index(index)\n\n        # no obvious \"empty\" int column\n        if missing.any() and not is_integer_dtype(dtype):\n            if dtype is None or np.issubdtype(dtype, np.flexible):\n                # GH#1783\n                nan_dtype = object\n            else:\n                nan_dtype = dtype\n            val = construct_1d_arraylike_from_scalar(np.nan, len(index),\n                                                     nan_dtype)\n            arrays.loc[missing] = [val] * missing.sum()\n\n    else:\n        keys = com.dict_keys_to_ordered_list(data)\n        columns = data_names = Index(keys)\n        arrays = (com.maybe_iterable_to_list(data[k]) for k in keys)\n        # GH#24096 need copy to be deep for datetime64tz case\n        # TODO: See if we can avoid these copies\n        arrays = [arr if not isinstance(arr, ABCIndexClass) else arr._data\n                  for arr in arrays]\n        arrays = [arr if not is_datetime64tz_dtype(arr) else\n                  arr.copy() for arr in arrays]\n    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)\n\n\n# ---------------------------------------------------------------------\n\ndef prep_ndarray(values, copy=True):\n    if not isinstance(values, (np.ndarray, ABCSeries, Index)):\n        if len(values) == 0:\n            return np.empty((0, 0), dtype=object)\n\n        def convert(v):\n            return maybe_convert_platform(v)\n\n        # we could have a 1-dim or 2-dim list here\n        # this is equiv of np.asarray, but does object conversion\n        # and platform dtype preservation\n        try:\n            if is_list_like(values[0]) or hasattr(values[0], 'len'):\n                values = np.array([convert(v) for v in values])\n            elif isinstance(values[0], np.ndarray) and values[0].ndim == 0:\n                # GH#21861\n                values = np.array([convert(v) for v in values])\n            else:\n                values = convert(values)\n        except (ValueError, TypeError):\n            values = convert(values)\n\n    else:\n\n        # drop subclass info, do not copy data\n        values = np.asarray(values)\n        if copy:\n            values = values.copy()\n\n    if values.ndim == 1:\n        values = values.reshape((values.shape[0], 1))\n    elif values.ndim != 2:\n        raise ValueError('Must pass 2-d input')\n\n    return values\n\n\ndef _homogenize(data, index, dtype=None):\n    oindex = None\n    homogenized = []\n\n    for val in data:\n        if isinstance(val, ABCSeries):\n            if dtype is not None:\n                val = val.astype(dtype)\n            if val.index is not index:\n                # Forces alignment. No need to copy data since we\n                # are putting it into an ndarray later\n                val = val.reindex(index, copy=False)\n        else:\n            if isinstance(val, dict):\n                if oindex is None:\n                    oindex = index.astype('O')\n\n                if isinstance(index, (ABCDatetimeIndex, ABCTimedeltaIndex)):\n                    val = com.dict_compat(val)\n                else:\n                    val = dict(val)\n                val = lib.fast_multiget(val, oindex.values, default=np.nan)\n            val = sanitize_array(val, index, dtype=dtype, copy=False,\n                                 raise_cast_failure=False)\n\n        homogenized.append(val)\n\n    return homogenized\n\n\ndef extract_index(data):\n    index = None\n    if len(data) == 0:\n        index = Index([])\n    elif len(data) > 0:\n        raw_lengths = []\n        indexes = []\n\n        have_raw_arrays = False\n        have_series = False\n        have_dicts = False\n\n        for val in data:\n            if isinstance(val, ABCSeries):\n                have_series = True\n                indexes.append(val.index)\n            elif isinstance(val, dict):\n                have_dicts = True\n                indexes.append(list(val.keys()))\n            elif is_list_like(val) and getattr(val, 'ndim', 1) == 1:\n                have_raw_arrays = True\n                raw_lengths.append(len(val))\n\n        if not indexes and not raw_lengths:\n            raise ValueError('If using all scalar values, you must pass'\n                             ' an index')\n\n        if have_series or have_dicts:\n            index = _union_indexes(indexes)\n\n        if have_raw_arrays:\n            lengths = list(set(raw_lengths))\n            if len(lengths) > 1:\n                raise ValueError('arrays must all be same length')\n\n            if have_dicts:\n                raise ValueError('Mixing dicts with non-Series may lead to '\n                                 'ambiguous ordering.')\n\n            if have_series:\n                if lengths[0] != len(index):\n                    msg = ('array length {length} does not match index '\n                           'length {idx_len}'\n                           .format(length=lengths[0], idx_len=len(index)))\n                    raise ValueError(msg)\n            else:\n                index = ibase.default_index(lengths[0])\n\n    return ensure_index(index)\n\n\ndef reorder_arrays(arrays, arr_columns, columns):\n    # reorder according to the columns\n    if (columns is not None and len(columns) and arr_columns is not None and\n            len(arr_columns)):\n        indexer = ensure_index(arr_columns).get_indexer(columns)\n        arr_columns = ensure_index([arr_columns[i] for i in indexer])\n        arrays = [arrays[i] for i in indexer]\n    return arrays, arr_columns\n\n\ndef get_names_from_index(data):\n    has_some_name = any(getattr(s, 'name', None) is not None for s in data)\n    if not has_some_name:\n        return ibase.default_index(len(data))\n\n    index = list(range(len(data)))\n    count = 0\n    for i, s in enumerate(data):\n        n = getattr(s, 'name', None)\n        if n is not None:\n            index[i] = n\n        else:\n            index[i] = 'Unnamed {count}'.format(count=count)\n            count += 1\n\n    return index\n\n\ndef _get_axes(N, K, index, columns):\n    # helper to create the axes as indexes\n    # return axes or defaults\n\n    if index is None:\n        index = ibase.default_index(N)\n    else:\n        index = ensure_index(index)\n\n    if columns is None:\n        columns = ibase.default_index(K)\n    else:\n        columns = ensure_index(columns)\n    return index, columns\n\n\n# ---------------------------------------------------------------------\n# Conversion of Inputs to Arrays\n\ndef to_arrays(data, columns, coerce_float=False, dtype=None):\n    \"\"\"\n    Return list of arrays, columns.\n    \"\"\"\n    if isinstance(data, ABCDataFrame):\n        if columns is not None:\n            arrays = [data._ixs(i, axis=1).values\n                      for i, col in enumerate(data.columns) if col in columns]\n        else:\n            columns = data.columns\n            arrays = [data._ixs(i, axis=1).values for i in range(len(columns))]\n\n        return arrays, columns\n\n    if not len(data):\n        if isinstance(data, np.ndarray):\n            columns = data.dtype.names\n            if columns is not None:\n                return [[]] * len(columns), columns\n        return [], []  # columns if columns is not None else []\n    if isinstance(data[0], (list, tuple)):\n        return _list_to_arrays(data, columns, coerce_float=coerce_float,\n                               dtype=dtype)\n    elif isinstance(data[0], abc.Mapping):\n        return _list_of_dict_to_arrays(data, columns,\n                                       coerce_float=coerce_float, dtype=dtype)\n    elif isinstance(data[0], ABCSeries):\n        return _list_of_series_to_arrays(data, columns,\n                                         coerce_float=coerce_float,\n                                         dtype=dtype)\n    elif isinstance(data[0], Categorical):\n        if columns is None:\n            columns = ibase.default_index(len(data))\n        return data, columns\n    elif (isinstance(data, (np.ndarray, ABCSeries, Index)) and\n          data.dtype.names is not None):\n\n        columns = list(data.dtype.names)\n        arrays = [data[k] for k in columns]\n        return arrays, columns\n    else:\n        # last ditch effort\n        data = [tuple(x) for x in data]\n        return _list_to_arrays(data, columns, coerce_float=coerce_float,\n                               dtype=dtype)\n\n\ndef _list_to_arrays(data, columns, coerce_float=False, dtype=None):\n    if len(data) > 0 and isinstance(data[0], tuple):\n        content = list(lib.to_object_array_tuples(data).T)\n    else:\n        # list of lists\n        content = list(lib.to_object_array(data).T)\n    # gh-26429 do not raise user-facing AssertionError\n    try:\n        result = _convert_object_array(content, columns, dtype=dtype,\n                                       coerce_float=coerce_float)\n    except AssertionError as e:\n        raise ValueError(e) from e\n    return result\n\n\ndef _list_of_series_to_arrays(data, columns, coerce_float=False, dtype=None):\n    if columns is None:\n        columns = _get_objs_combined_axis(data, sort=False)\n\n    indexer_cache = {}\n\n    aligned_values = []\n    for s in data:\n        index = getattr(s, 'index', None)\n        if index is None:\n            index = ibase.default_index(len(s))\n\n        if id(index) in indexer_cache:\n            indexer = indexer_cache[id(index)]\n        else:\n            indexer = indexer_cache[id(index)] = index.get_indexer(columns)\n\n        values = com.values_from_object(s)\n        aligned_values.append(algorithms.take_1d(values, indexer))\n\n    values = np.vstack(aligned_values)\n\n    if values.dtype == np.object_:\n        content = list(values.T)\n        return _convert_object_array(content, columns, dtype=dtype,\n                                     coerce_float=coerce_float)\n    else:\n        return values.T, columns\n\n\ndef _list_of_dict_to_arrays(data, columns, coerce_float=False, dtype=None):\n    if columns is None:\n        gen = (list(x.keys()) for x in data)\n        sort = not any(isinstance(d, OrderedDict) for d in data)\n        columns = lib.fast_unique_multiple_list_gen(gen, sort=sort)\n\n    # assure that they are of the base dict class and not of derived\n    # classes\n    data = [(type(d) is dict) and d or dict(d) for d in data]\n\n    content = list(lib.dicts_to_array(data, list(columns)).T)\n    return _convert_object_array(content, columns, dtype=dtype,\n                                 coerce_float=coerce_float)\n\n\ndef _convert_object_array(content, columns, coerce_float=False, dtype=None):\n    if columns is None:\n        columns = ibase.default_index(len(content))\n    else:\n        if len(columns) != len(content):  # pragma: no cover\n            # caller's responsibility to check for this...\n            raise AssertionError('{col:d} columns passed, passed data had '\n                                 '{con} columns'.format(col=len(columns),\n                                                        con=len(content)))\n\n    # provide soft conversion of object dtypes\n    def convert(arr):\n        if dtype != object and dtype != np.object:\n            arr = lib.maybe_convert_objects(arr, try_float=coerce_float)\n            arr = maybe_cast_to_datetime(arr, dtype)\n        return arr\n\n    arrays = [convert(arr) for arr in content]\n\n    return arrays, columns\n\n\n# ---------------------------------------------------------------------\n# Series-Based\n\ndef sanitize_index(data, index, copy=False):\n    \"\"\"\n    Sanitize an index type to return an ndarray of the underlying, pass\n    through a non-Index.\n    \"\"\"\n\n    if index is None:\n        return data\n\n    if len(data) != len(index):\n        raise ValueError('Length of values does not match length of index')\n\n    if isinstance(data, ABCIndexClass) and not copy:\n        pass\n    elif isinstance(data, (ABCPeriodIndex, ABCDatetimeIndex)):\n        data = data._values\n        if copy:\n            data = data.copy()\n\n    elif isinstance(data, np.ndarray):\n\n        # coerce datetimelike types\n        if data.dtype.kind in ['M', 'm']:\n            data = sanitize_array(data, index, copy=copy)\n\n    return data\n\n\ndef sanitize_array(data, index, dtype=None, copy=False,\n                   raise_cast_failure=False):\n    \"\"\"\n    Sanitize input data to an ndarray, copy if specified, coerce to the\n    dtype if specified.\n    \"\"\"\n    if dtype is not None:\n        dtype = pandas_dtype(dtype)\n\n    if isinstance(data, ma.MaskedArray):\n        mask = ma.getmaskarray(data)\n        if mask.any():\n            data, fill_value = maybe_upcast(data, copy=True)\n            data.soften_mask()  # set hardmask False if it was True\n            data[mask] = fill_value\n        else:\n            data = data.copy()\n\n    data = extract_array(data, extract_numpy=True)\n\n    # GH#846\n    if isinstance(data, np.ndarray):\n\n        if dtype is not None:\n            subarr = np.array(data, copy=False)\n\n            # possibility of nan -> garbage\n            if is_float_dtype(data.dtype) and is_integer_dtype(dtype):\n                try:\n                    subarr = _try_cast(data, True, dtype, copy,\n                                       True)\n                except ValueError:\n                    if copy:\n                        subarr = data.copy()\n            else:\n                subarr = _try_cast(data, True, dtype, copy, raise_cast_failure)\n        elif isinstance(data, Index):\n            # don't coerce Index types\n            # e.g. indexes can have different conversions (so don't fast path\n            # them)\n            # GH#6140\n            subarr = sanitize_index(data, index, copy=copy)\n        else:\n\n            # we will try to copy be-definition here\n            subarr = _try_cast(data, True, dtype, copy, raise_cast_failure)\n\n    elif isinstance(data, ExtensionArray):\n        if isinstance(data, ABCPandasArray):\n            # We don't want to let people put our PandasArray wrapper\n            # (the output of Series/Index.array), into a Series. So\n            # we explicitly unwrap it here.\n            subarr = data.to_numpy()\n        else:\n            subarr = data\n\n        # everything else in this block must also handle ndarray's,\n        # because we've unwrapped PandasArray into an ndarray.\n\n        if dtype is not None:\n            subarr = data.astype(dtype)\n\n        if copy:\n            subarr = data.copy()\n        return subarr\n\n    elif isinstance(data, (list, tuple)) and len(data) > 0:\n        if dtype is not None:\n            try:\n                subarr = _try_cast(data, False, dtype, copy,\n                                   raise_cast_failure)\n            except Exception:\n                if raise_cast_failure:  # pragma: no cover\n                    raise\n                subarr = np.array(data, dtype=object, copy=copy)\n                subarr = lib.maybe_convert_objects(subarr)\n\n        else:\n            subarr = maybe_convert_platform(data)\n\n        subarr = maybe_cast_to_datetime(subarr, dtype)\n\n    elif isinstance(data, range):\n        # GH#16804\n        arr = np.arange(data.start, data.stop, data.step, dtype='int64')\n        subarr = _try_cast(arr, False, dtype, copy, raise_cast_failure)\n    else:\n        subarr = _try_cast(data, False, dtype, copy, raise_cast_failure)\n\n    # scalar like, GH\n    if getattr(subarr, 'ndim', 0) == 0:\n        if isinstance(data, list):  # pragma: no cover\n            subarr = np.array(data, dtype=object)\n        elif index is not None:\n            value = data\n\n            # figure out the dtype from the value (upcast if necessary)\n            if dtype is None:\n                dtype, value = infer_dtype_from_scalar(value)\n            else:\n                # need to possibly convert the value here\n                value = maybe_cast_to_datetime(value, dtype)\n\n            subarr = construct_1d_arraylike_from_scalar(\n                value, len(index), dtype)\n\n        else:\n            return subarr.item()\n\n    # the result that we want\n    elif subarr.ndim == 1:\n        if index is not None:\n\n            # a 1-element ndarray\n            if len(subarr) != len(index) and len(subarr) == 1:\n                subarr = construct_1d_arraylike_from_scalar(\n                    subarr[0], len(index), subarr.dtype)\n\n    elif subarr.ndim > 1:\n        if isinstance(data, np.ndarray):\n            raise Exception('Data must be 1-dimensional')\n        else:\n            subarr = com.asarray_tuplesafe(data, dtype=dtype)\n\n    # This is to prevent mixed-type Series getting all casted to\n    # NumPy string type, e.g. NaN --> '-1#IND'.\n    if issubclass(subarr.dtype.type, str):\n        # GH#16605\n        # If not empty convert the data to dtype\n        # GH#19853: If data is a scalar, subarr has already the result\n        if not lib.is_scalar(data):\n            if not np.all(isna(data)):\n                data = np.array(data, dtype=dtype, copy=False)\n            subarr = np.array(data, dtype=object, copy=copy)\n\n    if (not (is_extension_array_dtype(subarr.dtype) or\n             is_extension_array_dtype(dtype)) and\n            is_object_dtype(subarr.dtype) and\n            not is_object_dtype(dtype)):\n        inferred = lib.infer_dtype(subarr, skipna=False)\n        if inferred == 'period':\n            try:\n                subarr = period_array(subarr)\n            except IncompatibleFrequency:\n                pass\n\n    return subarr\n\n\ndef _try_cast(arr, take_fast_path, dtype, copy, raise_cast_failure):\n\n    # perf shortcut as this is the most common case\n    if take_fast_path:\n        if maybe_castable(arr) and not copy and dtype is None:\n            return arr\n\n    try:\n        # GH#15832: Check if we are requesting a numeric dype and\n        # that we can convert the data to the requested dtype.\n        if is_integer_dtype(dtype):\n            subarr = maybe_cast_to_integer_array(arr, dtype)\n\n        subarr = maybe_cast_to_datetime(arr, dtype)\n        # Take care in creating object arrays (but iterators are not\n        # supported):\n        if is_object_dtype(dtype) and (is_list_like(subarr) and\n                                       not (is_iterator(subarr) or\n                                       isinstance(subarr, np.ndarray))):\n            subarr = construct_1d_object_array_from_listlike(subarr)\n        elif not is_extension_type(subarr):\n            subarr = construct_1d_ndarray_preserving_na(subarr, dtype,\n                                                        copy=copy)\n    except OutOfBoundsDatetime:\n        # in case of out of bound datetime64 -> always raise\n        raise\n    except (ValueError, TypeError):\n        if is_categorical_dtype(dtype):\n            # We *do* allow casting to categorical, since we know\n            # that Categorical is the only array type for 'category'.\n            subarr = Categorical(arr, dtype.categories,\n                                 ordered=dtype.ordered)\n        elif is_extension_array_dtype(dtype):\n            # create an extension array from its dtype\n            array_type = dtype.construct_array_type()._from_sequence\n            subarr = array_type(arr, dtype=dtype, copy=copy)\n        elif dtype is not None and raise_cast_failure:\n            raise\n        else:\n            subarr = np.array(arr, dtype=object, copy=copy)\n    return subarr\n"
    },
    {
      "filename": "pandas/core/nanops.py",
      "content": "import functools\nimport itertools\nimport operator\nfrom typing import Any, Optional, Tuple, Union\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import iNaT, lib, tslibs\nfrom pandas.compat._optional import import_optional_dependency\n\nfrom pandas.core.dtypes.cast import _int64_max, maybe_upcast_putmask\nfrom pandas.core.dtypes.common import (\n    _get_dtype, is_any_int_dtype, is_bool_dtype, is_complex, is_complex_dtype,\n    is_datetime64_dtype, is_datetime64tz_dtype, is_datetime_or_timedelta_dtype,\n    is_float, is_float_dtype, is_integer, is_integer_dtype, is_numeric_dtype,\n    is_object_dtype, is_scalar, is_timedelta64_dtype, pandas_dtype)\nfrom pandas.core.dtypes.dtypes import DatetimeTZDtype\nfrom pandas.core.dtypes.missing import isna, na_value_for_dtype, notna\n\nimport pandas.core.common as com\n\nbn = import_optional_dependency(\"bottleneck\",\n                                raise_on_missing=False,\n                                on_version=\"warn\")\n_BOTTLENECK_INSTALLED = bn is not None\n_USE_BOTTLENECK = False\n\n\ndef set_use_bottleneck(v=True):\n    # set/unset to use bottleneck\n    global _USE_BOTTLENECK\n    if _BOTTLENECK_INSTALLED:\n        _USE_BOTTLENECK = v\n\n\nset_use_bottleneck(get_option('compute.use_bottleneck'))\n\n\nclass disallow:\n\n    def __init__(self, *dtypes):\n        super().__init__()\n        self.dtypes = tuple(pandas_dtype(dtype).type for dtype in dtypes)\n\n    def check(self, obj):\n        return hasattr(obj, 'dtype') and issubclass(obj.dtype.type,\n                                                    self.dtypes)\n\n    def __call__(self, f):\n        @functools.wraps(f)\n        def _f(*args, **kwargs):\n            obj_iter = itertools.chain(args, kwargs.values())\n            if any(self.check(obj) for obj in obj_iter):\n                msg = 'reduction operation {name!r} not allowed for this dtype'\n                raise TypeError(msg.format(name=f.__name__.replace('nan', '')))\n            try:\n                with np.errstate(invalid='ignore'):\n                    return f(*args, **kwargs)\n            except ValueError as e:\n                # we want to transform an object array\n                # ValueError message to the more typical TypeError\n                # e.g. this is normally a disallowed function on\n                # object arrays that contain strings\n                if is_object_dtype(args[0]):\n                    raise TypeError(e)\n                raise\n\n        return _f\n\n\nclass bottleneck_switch:\n\n    def __init__(self, name=None, **kwargs):\n        self.name = name\n        self.kwargs = kwargs\n\n    def __call__(self, alt):\n        bn_name = self.name or alt.__name__\n\n        try:\n            bn_func = getattr(bn, bn_name)\n        except (AttributeError, NameError):  # pragma: no cover\n            bn_func = None\n\n        @functools.wraps(alt)\n        def f(values, axis=None, skipna=True, **kwds):\n            if len(self.kwargs) > 0:\n                for k, v in self.kwargs.items():\n                    if k not in kwds:\n                        kwds[k] = v\n            try:\n                if values.size == 0 and kwds.get('min_count') is None:\n                    # We are empty, returning NA for our type\n                    # Only applies for the default `min_count` of None\n                    # since that affects how empty arrays are handled.\n                    # TODO(GH-18976) update all the nanops methods to\n                    # correctly handle empty inputs and remove this check.\n                    # It *may* just be `var`\n                    return _na_for_min_count(values, axis)\n\n                if (_USE_BOTTLENECK and skipna and\n                        _bn_ok_dtype(values.dtype, bn_name)):\n                    result = bn_func(values, axis=axis, **kwds)\n\n                    # prefer to treat inf/-inf as NA, but must compute the func\n                    # twice :(\n                    if _has_infs(result):\n                        result = alt(values, axis=axis, skipna=skipna, **kwds)\n                else:\n                    result = alt(values, axis=axis, skipna=skipna, **kwds)\n            except Exception:\n                try:\n                    result = alt(values, axis=axis, skipna=skipna, **kwds)\n                except ValueError as e:\n                    # we want to transform an object array\n                    # ValueError message to the more typical TypeError\n                    # e.g. this is normally a disallowed function on\n                    # object arrays that contain strings\n\n                    if is_object_dtype(values):\n                        raise TypeError(e)\n                    raise\n\n            return result\n\n        return f\n\n\ndef _bn_ok_dtype(dt, name):\n    # Bottleneck chokes on datetime64\n    if (not is_object_dtype(dt) and\n            not (is_datetime_or_timedelta_dtype(dt) or\n                 is_datetime64tz_dtype(dt))):\n\n        # GH 15507\n        # bottleneck does not properly upcast during the sum\n        # so can overflow\n\n        # GH 9422\n        # further we also want to preserve NaN when all elements\n        # are NaN, unlinke bottleneck/numpy which consider this\n        # to be 0\n        if name in ['nansum', 'nanprod']:\n            return False\n\n        return True\n    return False\n\n\ndef _has_infs(result):\n    if isinstance(result, np.ndarray):\n        if result.dtype == 'f8':\n            return lib.has_infs_f8(result.ravel())\n        elif result.dtype == 'f4':\n            return lib.has_infs_f4(result.ravel())\n    try:\n        return np.isinf(result).any()\n    except (TypeError, NotImplementedError):\n        # if it doesn't support infs, then it can't have infs\n        return False\n\n\ndef _get_fill_value(dtype, fill_value=None, fill_value_typ=None):\n    \"\"\" return the correct fill value for the dtype of the values \"\"\"\n    if fill_value is not None:\n        return fill_value\n    if _na_ok_dtype(dtype):\n        if fill_value_typ is None:\n            return np.nan\n        else:\n            if fill_value_typ == '+inf':\n                return np.inf\n            else:\n                return -np.inf\n    else:\n        if fill_value_typ is None:\n            return tslibs.iNaT\n        else:\n            if fill_value_typ == '+inf':\n                # need the max int here\n                return _int64_max\n            else:\n                return tslibs.iNaT\n\n\ndef _maybe_get_mask(values: np.ndarray, skipna: bool,\n                    mask: Optional[np.ndarray]) -> Optional[np.ndarray]:\n    \"\"\" This function will compute a mask iff it is necessary. Otherwise,\n    return the provided mask (potentially None) when a mask does not need to be\n    computed.\n\n    A mask is never necessary if the values array is of boolean or integer\n    dtypes, as these are incapable of storing NaNs. If passing a NaN-capable\n    dtype that is interpretable as either boolean or integer data (eg,\n    timedelta64), a mask must be provided.\n\n    If the skipna parameter is False, a new mask will not be computed.\n\n    The mask is computed using isna() by default. Setting invert=True selects\n    notna() as the masking function.\n\n    Parameters\n    ----------\n    values : ndarray\n        input array to potentially compute mask for\n    skipna : bool\n        boolean for whether NaNs should be skipped\n    mask : Optional[ndarray]\n        nan-mask if known\n\n    Returns\n    -------\n    Optional[np.ndarray]\n\n    \"\"\"\n\n    if mask is None:\n        if is_bool_dtype(values.dtype) or is_integer_dtype(values.dtype):\n            # Boolean data cannot contain nulls, so signal via mask being None\n            return None\n\n        if skipna:\n            mask = isna(values)\n\n    return mask\n\n\ndef _get_values(values: np.ndarray, skipna: bool, fill_value: Any = None,\n                fill_value_typ: str = None, mask: Optional[np.ndarray] = None\n                ) -> Tuple[np.ndarray, Optional[np.ndarray], np.dtype,\n                           np.dtype, Any]:\n    \"\"\" Utility to get the values view, mask, dtype, dtype_max, and fill_value.\n\n    If both mask and fill_value/fill_value_typ are not None and skipna is True,\n    the values array will be copied.\n\n    For input arrays of boolean or integer dtypes, copies will only occur if a\n    precomputed mask, a fill_value/fill_value_typ, and skipna=True are\n    provided.\n\n    Parameters\n    ----------\n    values : ndarray\n        input array to potentially compute mask for\n    skipna : bool\n        boolean for whether NaNs should be skipped\n    fill_value : Any\n        value to fill NaNs with\n    fill_value_typ : str\n        Set to '+inf' or '-inf' to handle dtype-specific infinities\n    mask : Optional[np.ndarray]\n        nan-mask if known\n\n    Returns\n    -------\n    values : ndarray\n        Potential copy of input value array\n    mask : Optional[ndarray[bool]]\n        Mask for values, if deemed necessary to compute\n    dtype : dtype\n        dtype for values\n    dtype_max : dtype\n        platform independent dtype\n    fill_value : Any\n        fill value used\n    \"\"\"\n    mask = _maybe_get_mask(values, skipna, mask)\n\n    if is_datetime64tz_dtype(values):\n        # com.values_from_object returns M8[ns] dtype instead of tz-aware,\n        #  so this case must be handled separately from the rest\n        dtype = values.dtype\n        values = getattr(values, \"_values\", values)\n    else:\n        values = com.values_from_object(values)\n        dtype = values.dtype\n\n    if is_datetime_or_timedelta_dtype(values) or is_datetime64tz_dtype(values):\n        # changing timedelta64/datetime64 to int64 needs to happen after\n        #  finding `mask` above\n        values = getattr(values, \"asi8\", values)\n        values = values.view(np.int64)\n\n    dtype_ok = _na_ok_dtype(dtype)\n\n    # get our fill value (in case we need to provide an alternative\n    # dtype for it)\n    fill_value = _get_fill_value(dtype, fill_value=fill_value,\n                                 fill_value_typ=fill_value_typ)\n\n    copy = (mask is not None) and (fill_value is not None)\n\n    if skipna and copy:\n        values = values.copy()\n        if dtype_ok:\n            np.putmask(values, mask, fill_value)\n\n        # promote if needed\n        else:\n            values, changed = maybe_upcast_putmask(values, mask, fill_value)\n\n    # return a platform independent precision dtype\n    dtype_max = dtype\n    if is_integer_dtype(dtype) or is_bool_dtype(dtype):\n        dtype_max = np.int64\n    elif is_float_dtype(dtype):\n        dtype_max = np.float64\n\n    return values, mask, dtype, dtype_max, fill_value\n\n\ndef _isfinite(values):\n    if is_datetime_or_timedelta_dtype(values):\n        return isna(values)\n    if (is_complex_dtype(values) or is_float_dtype(values) or\n            is_integer_dtype(values) or is_bool_dtype(values)):\n        return ~np.isfinite(values)\n    return ~np.isfinite(values.astype('float64'))\n\n\ndef _na_ok_dtype(dtype):\n    # TODO: what about datetime64tz?  PeriodDtype?\n    return not issubclass(dtype.type,\n                          (np.integer, np.timedelta64, np.datetime64))\n\n\ndef _wrap_results(result, dtype, fill_value=None):\n    \"\"\" wrap our results if needed \"\"\"\n\n    if is_datetime64_dtype(dtype) or is_datetime64tz_dtype(dtype):\n        if fill_value is None:\n            # GH#24293\n            fill_value = iNaT\n        if not isinstance(result, np.ndarray):\n            tz = getattr(dtype, 'tz', None)\n            assert not isna(fill_value), \"Expected non-null fill_value\"\n            if result == fill_value:\n                result = np.nan\n            result = tslibs.Timestamp(result, tz=tz)\n        else:\n            result = result.view(dtype)\n    elif is_timedelta64_dtype(dtype):\n        if not isinstance(result, np.ndarray):\n            if result == fill_value:\n                result = np.nan\n\n            # raise if we have a timedelta64[ns] which is too large\n            if np.fabs(result) > _int64_max:\n                raise ValueError(\"overflow in timedelta operation\")\n\n            result = tslibs.Timedelta(result, unit='ns')\n        else:\n            result = result.astype('i8').view(dtype)\n\n    return result\n\n\ndef _na_for_min_count(values, axis):\n    \"\"\"Return the missing value for `values`\n\n    Parameters\n    ----------\n    values : ndarray\n    axis : int or None\n        axis for the reduction\n\n    Returns\n    -------\n    result : scalar or ndarray\n        For 1-D values, returns a scalar of the correct missing type.\n        For 2-D values, returns a 1-D array where each element is missing.\n    \"\"\"\n    # we either return np.nan or pd.NaT\n    if is_numeric_dtype(values):\n        values = values.astype('float64')\n    fill_value = na_value_for_dtype(values.dtype)\n\n    if values.ndim == 1:\n        return fill_value\n    else:\n        result_shape = (values.shape[:axis] +\n                        values.shape[axis + 1:])\n        result = np.empty(result_shape, dtype=values.dtype)\n        result.fill(fill_value)\n        return result\n\n\ndef nanany(values, axis=None, skipna=True, mask=None):\n    \"\"\"\n    Check if any elements along an axis evaluate to True.\n\n    Parameters\n    ----------\n    values : ndarray\n    axis : int, optional\n    skipna : bool, default True\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : bool\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, 2])\n    >>> nanops.nanany(s)\n    True\n\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([np.nan])\n    >>> nanops.nanany(s)\n    False\n    \"\"\"\n    values, _, _, _, _ = _get_values(values, skipna, fill_value=False,\n                                     mask=mask)\n    return values.any(axis)\n\n\ndef nanall(values, axis=None, skipna=True, mask=None):\n    \"\"\"\n    Check if all elements along an axis evaluate to True.\n\n    Parameters\n    ----------\n    values : ndarray\n    axis: int, optional\n    skipna : bool, default True\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : bool\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, 2, np.nan])\n    >>> nanops.nanall(s)\n    True\n\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, 0])\n    >>> nanops.nanall(s)\n    False\n    \"\"\"\n    values, _, _, _, _ = _get_values(values, skipna, fill_value=True,\n                                     mask=mask)\n    return values.all(axis)\n\n\n@disallow('M8')\ndef nansum(values, axis=None, skipna=True, min_count=0, mask=None):\n    \"\"\"\n    Sum the elements along an axis ignoring NaNs\n\n    Parameters\n    ----------\n    values : ndarray[dtype]\n    axis: int, optional\n    skipna : bool, default True\n    min_count: int, default 0\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : dtype\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, 2, np.nan])\n    >>> nanops.nansum(s)\n    3.0\n    \"\"\"\n    values, mask, dtype, dtype_max, _ = _get_values(values, skipna,\n                                                    fill_value=0, mask=mask)\n    dtype_sum = dtype_max\n    if is_float_dtype(dtype):\n        dtype_sum = dtype\n    elif is_timedelta64_dtype(dtype):\n        dtype_sum = np.float64\n    the_sum = values.sum(axis, dtype=dtype_sum)\n    the_sum = _maybe_null_out(the_sum, axis, mask, values.shape,\n                              min_count=min_count)\n\n    return _wrap_results(the_sum, dtype)\n\n\n@disallow('M8', DatetimeTZDtype)\n@bottleneck_switch()\ndef nanmean(values, axis=None, skipna=True, mask=None):\n    \"\"\"\n    Compute the mean of the element along an axis ignoring NaNs\n\n    Parameters\n    ----------\n    values : ndarray\n    axis: int, optional\n    skipna : bool, default True\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : float\n        Unless input is a float array, in which case use the same\n        precision as the input array.\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, 2, np.nan])\n    >>> nanops.nanmean(s)\n    1.5\n    \"\"\"\n    values, mask, dtype, dtype_max, _ = _get_values(values, skipna,\n                                                    fill_value=0, mask=mask)\n    dtype_sum = dtype_max\n    dtype_count = np.float64\n    if (is_integer_dtype(dtype) or is_timedelta64_dtype(dtype) or\n            is_datetime64_dtype(dtype) or is_datetime64tz_dtype(dtype)):\n        dtype_sum = np.float64\n    elif is_float_dtype(dtype):\n        dtype_sum = dtype\n        dtype_count = dtype\n    count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n    the_sum = _ensure_numeric(values.sum(axis, dtype=dtype_sum))\n\n    if axis is not None and getattr(the_sum, 'ndim', False):\n        with np.errstate(all=\"ignore\"):\n            # suppress division by zero warnings\n            the_mean = the_sum / count\n        ct_mask = count == 0\n        if ct_mask.any():\n            the_mean[ct_mask] = np.nan\n    else:\n        the_mean = the_sum / count if count > 0 else np.nan\n\n    return _wrap_results(the_mean, dtype)\n\n\n@disallow('M8')\n@bottleneck_switch()\ndef nanmedian(values, axis=None, skipna=True, mask=None):\n    \"\"\"\n    Parameters\n    ----------\n    values : ndarray\n    axis: int, optional\n    skipna : bool, default True\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : float\n        Unless input is a float array, in which case use the same\n        precision as the input array.\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, np.nan, 2, 2])\n    >>> nanops.nanmedian(s)\n    2.0\n    \"\"\"\n    def get_median(x):\n        mask = notna(x)\n        if not skipna and not mask.all():\n            return np.nan\n        return np.nanmedian(x[mask])\n\n    values, mask, dtype, dtype_max, _ = _get_values(values, skipna, mask=mask)\n    if not is_float_dtype(values):\n        values = values.astype('f8')\n        if mask is not None:\n            values[mask] = np.nan\n\n    if axis is None:\n        values = values.ravel()\n\n    notempty = values.size\n\n    # an array from a frame\n    if values.ndim > 1:\n\n        # there's a non-empty array to apply over otherwise numpy raises\n        if notempty:\n            if not skipna:\n                return _wrap_results(\n                    np.apply_along_axis(get_median, axis, values), dtype)\n\n            # fastpath for the skipna case\n            return _wrap_results(np.nanmedian(values, axis), dtype)\n\n        # must return the correct shape, but median is not defined for the\n        # empty set so return nans of shape \"everything but the passed axis\"\n        # since \"axis\" is where the reduction would occur if we had a nonempty\n        # array\n        shp = np.array(values.shape)\n        dims = np.arange(values.ndim)\n        ret = np.empty(shp[dims != axis])\n        ret.fill(np.nan)\n        return _wrap_results(ret, dtype)\n\n    # otherwise return a scalar value\n    return _wrap_results(get_median(values) if notempty else np.nan, dtype)\n\n\ndef _get_counts_nanvar(value_counts: Tuple[int], mask: Optional[np.ndarray],\n                       axis: Optional[int], ddof: int,\n                       dtype=float) -> Tuple[Union[int, np.ndarray],\n                                             Union[int, np.ndarray]]:\n    \"\"\" Get the count of non-null values along an axis, accounting\n    for degrees of freedom.\n\n    Parameters\n    ----------\n    values_shape : Tuple[int]\n        shape tuple from values ndarray, used if mask is None\n    mask : Optional[ndarray[bool]]\n        locations in values that should be considered missing\n    axis : Optional[int]\n        axis to count along\n    ddof : int\n        degrees of freedom\n    dtype : type, optional\n        type to use for count\n\n    Returns\n    -------\n    count : scalar or array\n    d : scalar or array\n    \"\"\"\n    dtype = _get_dtype(dtype)\n    count = _get_counts(value_counts, mask, axis, dtype=dtype)\n    d = count - dtype.type(ddof)\n\n    # always return NaN, never inf\n    if is_scalar(count):\n        if count <= ddof:\n            count = np.nan\n            d = np.nan\n    else:\n        mask2 = count <= ddof  # type: np.ndarray\n        if mask2.any():\n            np.putmask(d, mask2, np.nan)\n            np.putmask(count, mask2, np.nan)\n    return count, d\n\n\n@disallow('M8')\n@bottleneck_switch(ddof=1)\ndef nanstd(values, axis=None, skipna=True, ddof=1, mask=None):\n    \"\"\"\n    Compute the standard deviation along given axis while ignoring NaNs\n\n    Parameters\n    ----------\n    values : ndarray\n    axis: int, optional\n    skipna : bool, default True\n    ddof : int, default 1\n        Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n        where N represents the number of elements.\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : float\n        Unless input is a float array, in which case use the same\n        precision as the input array.\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, np.nan, 2, 3])\n    >>> nanops.nanstd(s)\n    1.0\n    \"\"\"\n    result = np.sqrt(nanvar(values, axis=axis, skipna=skipna, ddof=ddof,\n                            mask=mask))\n    return _wrap_results(result, values.dtype)\n\n\n@disallow('M8')\n@bottleneck_switch(ddof=1)\ndef nanvar(values, axis=None, skipna=True, ddof=1, mask=None):\n    \"\"\"\n    Compute the variance along given axis while ignoring NaNs\n\n    Parameters\n    ----------\n    values : ndarray\n    axis: int, optional\n    skipna : bool, default True\n    ddof : int, default 1\n        Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n        where N represents the number of elements.\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : float\n        Unless input is a float array, in which case use the same\n        precision as the input array.\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, np.nan, 2, 3])\n    >>> nanops.nanvar(s)\n    1.0\n    \"\"\"\n    values = com.values_from_object(values)\n    dtype = values.dtype\n    mask = _maybe_get_mask(values, skipna, mask)\n    if is_any_int_dtype(values):\n        values = values.astype('f8')\n        if mask is not None:\n            values[mask] = np.nan\n\n    if is_float_dtype(values):\n        count, d = _get_counts_nanvar(values.shape, mask, axis, ddof,\n                                      values.dtype)\n    else:\n        count, d = _get_counts_nanvar(values.shape, mask, axis, ddof)\n\n    if skipna and mask is not None:\n        values = values.copy()\n        np.putmask(values, mask, 0)\n\n    # xref GH10242\n    # Compute variance via two-pass algorithm, which is stable against\n    # cancellation errors and relatively accurate for small numbers of\n    # observations.\n    #\n    # See https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n    avg = _ensure_numeric(values.sum(axis=axis, dtype=np.float64)) / count\n    if axis is not None:\n        avg = np.expand_dims(avg, axis)\n    sqr = _ensure_numeric((avg - values) ** 2)\n    if mask is not None:\n        np.putmask(sqr, mask, 0)\n    result = sqr.sum(axis=axis, dtype=np.float64) / d\n\n    # Return variance as np.float64 (the datatype used in the accumulator),\n    # unless we were dealing with a float array, in which case use the same\n    # precision as the original values array.\n    if is_float_dtype(dtype):\n        result = result.astype(dtype)\n    return _wrap_results(result, values.dtype)\n\n\n@disallow('M8', 'm8')\ndef nansem(values, axis=None, skipna=True, ddof=1, mask=None):\n    \"\"\"\n    Compute the standard error in the mean along given axis while ignoring NaNs\n\n    Parameters\n    ----------\n    values : ndarray\n    axis: int, optional\n    skipna : bool, default True\n    ddof : int, default 1\n        Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n        where N represents the number of elements.\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : float64\n        Unless input is a float array, in which case use the same\n        precision as the input array.\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, np.nan, 2, 3])\n    >>> nanops.nansem(s)\n     0.5773502691896258\n    \"\"\"\n\n    # This checks if non-numeric-like data is passed with numeric_only=False\n    # and raises a TypeError otherwise\n    nanvar(values, axis, skipna, ddof=ddof, mask=mask)\n\n    mask = _maybe_get_mask(values, skipna, mask)\n    if not is_float_dtype(values.dtype):\n        values = values.astype('f8')\n\n    count, _ = _get_counts_nanvar(values.shape, mask, axis, ddof, values.dtype)\n    var = nanvar(values, axis, skipna, ddof=ddof)\n\n    return np.sqrt(var) / np.sqrt(count)\n\n\ndef _nanminmax(meth, fill_value_typ):\n\n    @bottleneck_switch(name='nan' + meth)\n    def reduction(values, axis=None, skipna=True, mask=None):\n\n        values, mask, dtype, dtype_max, fill_value = _get_values(\n            values, skipna, fill_value_typ=fill_value_typ, mask=mask)\n\n        if ((axis is not None and values.shape[axis] == 0) or\n                values.size == 0):\n            try:\n                result = getattr(values, meth)(axis, dtype=dtype_max)\n                result.fill(np.nan)\n            except (AttributeError, TypeError,\n                    ValueError, np.core._internal.AxisError):\n                result = np.nan\n        else:\n            result = getattr(values, meth)(axis)\n\n        result = _wrap_results(result, dtype, fill_value)\n        return _maybe_null_out(result, axis, mask, values.shape)\n\n    return reduction\n\n\nnanmin = _nanminmax('min', fill_value_typ='+inf')\nnanmax = _nanminmax('max', fill_value_typ='-inf')\n\n\n@disallow('O')\ndef nanargmax(values, axis=None, skipna=True, mask=None):\n    \"\"\"\n    Parameters\n    ----------\n    values : ndarray\n    axis: int, optional\n    skipna : bool, default True\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : int\n        The index of max value in specified axis or -1 in the NA case\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, 2, 3, np.nan, 4])\n    >>> nanops.nanargmax(s)\n    4\n    \"\"\"\n    values, mask, dtype, _, _ = _get_values(\n        values, True, fill_value_typ='-inf', mask=mask)\n    result = values.argmax(axis)\n    result = _maybe_arg_null_out(result, axis, mask, skipna)\n    return result\n\n\n@disallow('O')\ndef nanargmin(values, axis=None, skipna=True, mask=None):\n    \"\"\"\n    Parameters\n    ----------\n    values : ndarray\n    axis: int, optional\n    skipna : bool, default True\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : int\n        The index of min value in specified axis or -1 in the NA case\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, 2, 3, np.nan, 4])\n    >>> nanops.nanargmin(s)\n    0\n    \"\"\"\n    values, mask, dtype, _, _ = _get_values(\n        values, True, fill_value_typ='+inf', mask=mask)\n    result = values.argmin(axis)\n    result = _maybe_arg_null_out(result, axis, mask, skipna)\n    return result\n\n\n@disallow('M8', 'm8')\ndef nanskew(values, axis=None, skipna=True, mask=None):\n    \"\"\" Compute the sample skewness.\n\n    The statistic computed here is the adjusted Fisher-Pearson standardized\n    moment coefficient G1. The algorithm computes this coefficient directly\n    from the second and third central moment.\n\n    Parameters\n    ----------\n    values : ndarray\n    axis: int, optional\n    skipna : bool, default True\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : float64\n        Unless input is a float array, in which case use the same\n        precision as the input array.\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1,np.nan, 1, 2])\n    >>> nanops.nanskew(s)\n    1.7320508075688787\n    \"\"\"\n    values = com.values_from_object(values)\n    mask = _maybe_get_mask(values, skipna, mask)\n    if not is_float_dtype(values.dtype):\n        values = values.astype('f8')\n        count = _get_counts(values.shape, mask, axis)\n    else:\n        count = _get_counts(values.shape, mask, axis, dtype=values.dtype)\n\n    if skipna and mask is not None:\n        values = values.copy()\n        np.putmask(values, mask, 0)\n\n    mean = values.sum(axis, dtype=np.float64) / count\n    if axis is not None:\n        mean = np.expand_dims(mean, axis)\n\n    adjusted = values - mean\n    if skipna and mask is not None:\n        np.putmask(adjusted, mask, 0)\n    adjusted2 = adjusted ** 2\n    adjusted3 = adjusted2 * adjusted\n    m2 = adjusted2.sum(axis, dtype=np.float64)\n    m3 = adjusted3.sum(axis, dtype=np.float64)\n\n    # floating point error\n    #\n    # #18044 in _libs/windows.pyx calc_skew follow this behavior\n    # to fix the fperr to treat m2 <1e-14 as zero\n    m2 = _zero_out_fperr(m2)\n    m3 = _zero_out_fperr(m3)\n\n    with np.errstate(invalid='ignore', divide='ignore'):\n        result = (count * (count - 1) ** 0.5 / (count - 2)) * (m3 / m2 ** 1.5)\n\n    dtype = values.dtype\n    if is_float_dtype(dtype):\n        result = result.astype(dtype)\n\n    if isinstance(result, np.ndarray):\n        result = np.where(m2 == 0, 0, result)\n        result[count < 3] = np.nan\n        return result\n    else:\n        result = 0 if m2 == 0 else result\n        if count < 3:\n            return np.nan\n        return result\n\n\n@disallow('M8', 'm8')\ndef nankurt(values, axis=None, skipna=True, mask=None):\n    \"\"\"\n    Compute the sample excess kurtosis\n\n    The statistic computed here is the adjusted Fisher-Pearson standardized\n    moment coefficient G2, computed directly from the second and fourth\n    central moment.\n\n    Parameters\n    ----------\n    values : ndarray\n    axis: int, optional\n    skipna : bool, default True\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : float64\n        Unless input is a float array, in which case use the same\n        precision as the input array.\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1,np.nan, 1, 3, 2])\n    >>> nanops.nankurt(s)\n    -1.2892561983471076\n    \"\"\"\n    values = com.values_from_object(values)\n    mask = _maybe_get_mask(values, skipna, mask)\n    if not is_float_dtype(values.dtype):\n        values = values.astype('f8')\n        count = _get_counts(values.shape, mask, axis)\n    else:\n        count = _get_counts(values.shape, mask, axis, dtype=values.dtype)\n\n    if skipna and mask is not None:\n        values = values.copy()\n        np.putmask(values, mask, 0)\n\n    mean = values.sum(axis, dtype=np.float64) / count\n    if axis is not None:\n        mean = np.expand_dims(mean, axis)\n\n    adjusted = values - mean\n    if skipna and mask is not None:\n        np.putmask(adjusted, mask, 0)\n    adjusted2 = adjusted ** 2\n    adjusted4 = adjusted2 ** 2\n    m2 = adjusted2.sum(axis, dtype=np.float64)\n    m4 = adjusted4.sum(axis, dtype=np.float64)\n\n    with np.errstate(invalid='ignore', divide='ignore'):\n        adj = 3 * (count - 1) ** 2 / ((count - 2) * (count - 3))\n        numer = count * (count + 1) * (count - 1) * m4\n        denom = (count - 2) * (count - 3) * m2 ** 2\n\n    # floating point error\n    #\n    # #18044 in _libs/windows.pyx calc_kurt follow this behavior\n    # to fix the fperr to treat denom <1e-14 as zero\n    numer = _zero_out_fperr(numer)\n    denom = _zero_out_fperr(denom)\n\n    if not isinstance(denom, np.ndarray):\n        # if ``denom`` is a scalar, check these corner cases first before\n        # doing division\n        if count < 4:\n            return np.nan\n        if denom == 0:\n            return 0\n\n    with np.errstate(invalid='ignore', divide='ignore'):\n        result = numer / denom - adj\n\n    dtype = values.dtype\n    if is_float_dtype(dtype):\n        result = result.astype(dtype)\n\n    if isinstance(result, np.ndarray):\n        result = np.where(denom == 0, 0, result)\n        result[count < 4] = np.nan\n\n    return result\n\n\n@disallow('M8', 'm8')\ndef nanprod(values, axis=None, skipna=True, min_count=0, mask=None):\n    \"\"\"\n    Parameters\n    ----------\n    values : ndarray[dtype]\n    axis: int, optional\n    skipna : bool, default True\n    min_count: int, default 0\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : dtype\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, 2, 3, np.nan])\n    >>> nanops.nanprod(s)\n    6.0\n\n    Returns\n    -------\n    The product of all elements on a given axis. ( NaNs are treated as 1)\n    \"\"\"\n    mask = _maybe_get_mask(values, skipna, mask)\n\n    if skipna and mask is not None:\n        values = values.copy()\n        values[mask] = 1\n    result = values.prod(axis)\n    return _maybe_null_out(result, axis, mask, values.shape,\n                           min_count=min_count)\n\n\ndef _maybe_arg_null_out(result: np.ndarray, axis: Optional[int],\n                        mask: Optional[np.ndarray],\n                        skipna: bool) -> Union[np.ndarray, int]:\n    # helper function for nanargmin/nanargmax\n    if mask is None:\n        return result\n\n    if axis is None or not getattr(result, 'ndim', False):\n        if skipna:\n            if mask.all():\n                result = -1\n        else:\n            if mask.any():\n                result = -1\n    else:\n        if skipna:\n            na_mask = mask.all(axis)\n        else:\n            na_mask = mask.any(axis)\n        if na_mask.any():\n            result[na_mask] = -1\n    return result\n\n\ndef _get_counts(values_shape: Tuple[int], mask: Optional[np.ndarray],\n                axis: Optional[int], dtype=float) -> Union[int, np.ndarray]:\n    \"\"\" Get the count of non-null values along an axis\n\n    Parameters\n    ----------\n    values_shape : Tuple[int]\n        shape tuple from values ndarray, used if mask is None\n    mask : Optional[ndarray[bool]]\n        locations in values that should be considered missing\n    axis : Optional[int]\n        axis to count along\n    dtype : type, optional\n        type to use for count\n\n    Returns\n    -------\n    count : scalar or array\n    \"\"\"\n    dtype = _get_dtype(dtype)\n    if axis is None:\n        if mask is not None:\n            n = mask.size - mask.sum()\n        else:\n            n = np.prod(values_shape)\n        return dtype.type(n)\n\n    if mask is not None:\n        count = mask.shape[axis] - mask.sum(axis)\n    else:\n        count = values_shape[axis]\n\n    if is_scalar(count):\n        return dtype.type(count)\n    try:\n        return count.astype(dtype)\n    except AttributeError:\n        return np.array(count, dtype=dtype)\n\n\ndef _maybe_null_out(result: np.ndarray, axis: Optional[int],\n                    mask: Optional[np.ndarray], shape: Tuple,\n                    min_count: int = 1) -> np.ndarray:\n    if (mask is not None and axis is not None and\n            getattr(result, 'ndim', False)):\n        null_mask = (mask.shape[axis] - mask.sum(axis) - min_count) < 0\n        if np.any(null_mask):\n            if is_numeric_dtype(result):\n                if np.iscomplexobj(result):\n                    result = result.astype('c16')\n                else:\n                    result = result.astype('f8')\n                result[null_mask] = np.nan\n            else:\n                # GH12941, use None to auto cast null\n                result[null_mask] = None\n    elif result is not tslibs.NaT:\n        if mask is not None:\n            null_mask = mask.size - mask.sum()\n        else:\n            null_mask = np.prod(shape)\n        if null_mask < min_count:\n            result = np.nan\n\n    return result\n\n\ndef _zero_out_fperr(arg):\n    # #18044 reference this behavior to fix rolling skew/kurt issue\n    if isinstance(arg, np.ndarray):\n        with np.errstate(invalid='ignore'):\n            return np.where(np.abs(arg) < 1e-14, 0, arg)\n    else:\n        return arg.dtype.type(0) if np.abs(arg) < 1e-14 else arg\n\n\n@disallow('M8', 'm8')\ndef nancorr(a, b, method='pearson', min_periods=None):\n    \"\"\"\n    a, b: ndarrays\n    \"\"\"\n    if len(a) != len(b):\n        raise AssertionError('Operands to nancorr must have same size')\n\n    if min_periods is None:\n        min_periods = 1\n\n    valid = notna(a) & notna(b)\n    if not valid.all():\n        a = a[valid]\n        b = b[valid]\n\n    if len(a) < min_periods:\n        return np.nan\n\n    f = get_corr_func(method)\n    return f(a, b)\n\n\ndef get_corr_func(method):\n    if method in ['kendall', 'spearman']:\n        from scipy.stats import kendalltau, spearmanr\n    elif callable(method):\n        return method\n\n    def _pearson(a, b):\n        return np.corrcoef(a, b)[0, 1]\n\n    def _kendall(a, b):\n        rs = kendalltau(a, b)\n        if isinstance(rs, tuple):\n            return rs[0]\n        return rs\n\n    def _spearman(a, b):\n        return spearmanr(a, b)[0]\n\n    _cor_methods = {\n        'pearson': _pearson,\n        'kendall': _kendall,\n        'spearman': _spearman\n    }\n    return _cor_methods[method]\n\n\n@disallow('M8', 'm8')\ndef nancov(a, b, min_periods=None):\n    if len(a) != len(b):\n        raise AssertionError('Operands to nancov must have same size')\n\n    if min_periods is None:\n        min_periods = 1\n\n    valid = notna(a) & notna(b)\n    if not valid.all():\n        a = a[valid]\n        b = b[valid]\n\n    if len(a) < min_periods:\n        return np.nan\n\n    return np.cov(a, b)[0, 1]\n\n\ndef _ensure_numeric(x):\n    if isinstance(x, np.ndarray):\n        if is_integer_dtype(x) or is_bool_dtype(x):\n            x = x.astype(np.float64)\n        elif is_object_dtype(x):\n            try:\n                x = x.astype(np.complex128)\n            except (TypeError, ValueError):\n                x = x.astype(np.float64)\n            else:\n                if not np.any(x.imag):\n                    x = x.real\n    elif not (is_float(x) or is_integer(x) or is_complex(x)):\n        try:\n            x = float(x)\n        except Exception:\n            try:\n                x = complex(x)\n            except Exception:\n                raise TypeError('Could not convert {value!s} to numeric'\n                                .format(value=x))\n    return x\n\n# NA-friendly array comparisons\n\n\ndef make_nancomp(op):\n    def f(x, y):\n        xmask = isna(x)\n        ymask = isna(y)\n        mask = xmask | ymask\n\n        with np.errstate(all='ignore'):\n            result = op(x, y)\n\n        if mask.any():\n            if is_bool_dtype(result):\n                result = result.astype('O')\n            np.putmask(result, mask, np.nan)\n\n        return result\n\n    return f\n\n\nnangt = make_nancomp(operator.gt)\nnange = make_nancomp(operator.ge)\nnanlt = make_nancomp(operator.lt)\nnanle = make_nancomp(operator.le)\nnaneq = make_nancomp(operator.eq)\nnanne = make_nancomp(operator.ne)\n\n\ndef _nanpercentile_1d(values, mask, q, na_value, interpolation):\n    \"\"\"\n    Wraper for np.percentile that skips missing values, specialized to\n    1-dimensional case.\n\n    Parameters\n    ----------\n    values : array over which to find quantiles\n    mask : ndarray[bool]\n        locations in values that should be considered missing\n    q : scalar or array of quantile indices to find\n    na_value : scalar\n        value to return for empty or all-null values\n    interpolation : str\n\n    Returns\n    -------\n    quantiles : scalar or array\n    \"\"\"\n    # mask is Union[ExtensionArray, ndarray]\n    values = values[~mask]\n\n    if len(values) == 0:\n        if lib.is_scalar(q):\n            return na_value\n        else:\n            return np.array([na_value] * len(q),\n                            dtype=values.dtype)\n\n    return np.percentile(values, q, interpolation=interpolation)\n\n\ndef nanpercentile(values, q, axis, na_value, mask, ndim, interpolation):\n    \"\"\"\n    Wraper for np.percentile that skips missing values.\n\n    Parameters\n    ----------\n    values : array over which to find quantiles\n    q : scalar or array of quantile indices to find\n    axis : {0, 1}\n    na_value : scalar\n        value to return for empty or all-null values\n    mask : ndarray[bool]\n        locations in values that should be considered missing\n    ndim : {1, 2}\n    interpolation : str\n\n    Returns\n    -------\n    quantiles : scalar or array\n    \"\"\"\n    if not lib.is_scalar(mask) and mask.any():\n        if ndim == 1:\n            return _nanpercentile_1d(values, mask, q, na_value,\n                                     interpolation=interpolation)\n        else:\n            # for nonconsolidatable blocks mask is 1D, but values 2D\n            if mask.ndim < values.ndim:\n                mask = mask.reshape(values.shape)\n            if axis == 0:\n                values = values.T\n                mask = mask.T\n            result = [_nanpercentile_1d(val, m, q, na_value,\n                                        interpolation=interpolation)\n                      for (val, m) in zip(list(values), list(mask))]\n            result = np.array(result, dtype=values.dtype, copy=False).T\n            return result\n    else:\n        return np.percentile(values, q, axis=axis, interpolation=interpolation)\n"
    },
    {
      "filename": "pandas/tests/extension/base/groupby.py",
      "content": "import pytest\n\nimport pandas as pd\nimport pandas.util.testing as tm\n\nfrom .base import BaseExtensionTests\n\n\nclass BaseGroupbyTests(BaseExtensionTests):\n    \"\"\"Groupby-specific tests.\"\"\"\n\n    def test_grouping_grouper(self, data_for_grouping):\n        df = pd.DataFrame({\n            \"A\": [\"B\", \"B\", None, None, \"A\", \"A\", \"B\", \"C\"],\n            \"B\": data_for_grouping\n        })\n        gr1 = df.groupby(\"A\").grouper.groupings[0]\n        gr2 = df.groupby(\"B\").grouper.groupings[0]\n\n        tm.assert_numpy_array_equal(gr1.grouper, df.A.values)\n        tm.assert_extension_array_equal(gr2.grouper, data_for_grouping)\n\n    @pytest.mark.parametrize('as_index', [True, False])\n    def test_groupby_extension_agg(self, as_index, data_for_grouping):\n        df = pd.DataFrame({\"A\": [1, 1, 2, 2, 3, 3, 1, 4],\n                           \"B\": data_for_grouping})\n        result = df.groupby(\"B\", as_index=as_index).A.mean()\n        _, index = pd.factorize(data_for_grouping, sort=True)\n\n        index = pd.Index(index, name=\"B\")\n        expected = pd.Series([3, 1, 4], index=index, name=\"A\")\n        if as_index:\n            self.assert_series_equal(result, expected)\n        else:\n            expected = expected.reset_index()\n            self.assert_frame_equal(result, expected)\n\n    def test_groupby_extension_no_sort(self, data_for_grouping):\n        df = pd.DataFrame({\"A\": [1, 1, 2, 2, 3, 3, 1, 4],\n                           \"B\": data_for_grouping})\n        result = df.groupby(\"B\", sort=False).A.mean()\n        _, index = pd.factorize(data_for_grouping, sort=False)\n\n        index = pd.Index(index, name=\"B\")\n        expected = pd.Series([1, 3, 4], index=index, name=\"A\")\n        self.assert_series_equal(result, expected)\n\n    def test_groupby_extension_transform(self, data_for_grouping):\n        valid = data_for_grouping[~data_for_grouping.isna()]\n        df = pd.DataFrame({\"A\": [1, 1, 3, 3, 1, 4],\n                           \"B\": valid})\n\n        result = df.groupby(\"B\").A.transform(len)\n        expected = pd.Series([3, 3, 2, 2, 3, 1], name=\"A\")\n\n        self.assert_series_equal(result, expected)\n\n    def test_groupby_extension_apply(\n            self, data_for_grouping, groupby_apply_op):\n        df = pd.DataFrame({\"A\": [1, 1, 2, 2, 3, 3, 1, 4],\n                           \"B\": data_for_grouping})\n        df.groupby(\"B\").apply(groupby_apply_op)\n        df.groupby(\"B\").A.apply(groupby_apply_op)\n        df.groupby(\"A\").apply(groupby_apply_op)\n        df.groupby(\"A\").B.apply(groupby_apply_op)\n\n    def test_groupby_apply_identity(self, data_for_grouping):\n        df = pd.DataFrame({\"A\": [1, 1, 2, 2, 3, 3, 1, 4],\n                           \"B\": data_for_grouping})\n        result = df.groupby('A').B.apply(lambda x: x.array)\n        expected = pd.Series([df.B.iloc[[0, 1, 6]].array,\n                              df.B.iloc[[2, 3]].array,\n                              df.B.iloc[[4, 5]].array,\n                              df.B.iloc[[7]].array],\n                             index=pd.Index([1, 2, 3, 4], name='A'),\n                             name='B')\n        self.assert_series_equal(result, expected)\n\n    def test_in_numeric_groupby(self, data_for_grouping):\n        df = pd.DataFrame({\"A\": [1, 1, 2, 2, 3, 3, 1, 4],\n                           \"B\": data_for_grouping,\n                           \"C\": [1, 1, 1, 1, 1, 1, 1, 1]})\n        result = df.groupby(\"A\").sum().columns\n\n        if data_for_grouping.dtype._is_numeric:\n            expected = pd.Index(['B', 'C'])\n        else:\n            expected = pd.Index(['C'])\n\n        tm.assert_index_equal(result, expected)\n"
    },
    {
      "filename": "pandas/tests/extension/decimal/test_decimal.py",
      "content": "import decimal\nimport math\nimport operator\n\nimport numpy as np\nimport pytest\n\nimport pandas as pd\nfrom pandas.tests.extension import base\nimport pandas.util.testing as tm\n\nfrom .array import DecimalArray, DecimalDtype, make_data, to_decimal\n\n\n@pytest.fixture\ndef dtype():\n    return DecimalDtype()\n\n\n@pytest.fixture\ndef data():\n    return DecimalArray(make_data())\n\n\n@pytest.fixture\ndef data_for_twos():\n    return DecimalArray([decimal.Decimal(2) for _ in range(100)])\n\n\n@pytest.fixture\ndef data_missing():\n    return DecimalArray([decimal.Decimal('NaN'), decimal.Decimal(1)])\n\n\n@pytest.fixture\ndef data_for_sorting():\n    return DecimalArray([decimal.Decimal('1'),\n                         decimal.Decimal('2'),\n                         decimal.Decimal('0')])\n\n\n@pytest.fixture\ndef data_missing_for_sorting():\n    return DecimalArray([decimal.Decimal('1'),\n                         decimal.Decimal('NaN'),\n                         decimal.Decimal('0')])\n\n\n@pytest.fixture\ndef na_cmp():\n    return lambda x, y: x.is_nan() and y.is_nan()\n\n\n@pytest.fixture\ndef na_value():\n    return decimal.Decimal(\"NaN\")\n\n\n@pytest.fixture\ndef data_for_grouping():\n    b = decimal.Decimal('1.0')\n    a = decimal.Decimal('0.0')\n    c = decimal.Decimal('2.0')\n    na = decimal.Decimal('NaN')\n    return DecimalArray([b, b, na, na, a, a, b, c])\n\n\nclass BaseDecimal:\n\n    def assert_series_equal(self, left, right, *args, **kwargs):\n        def convert(x):\n            # need to convert array([Decimal(NaN)], dtype='object') to np.NaN\n            # because Series[object].isnan doesn't recognize decimal(NaN) as\n            # NA.\n            try:\n                return math.isnan(x)\n            except TypeError:\n                return False\n\n        if left.dtype == 'object':\n            left_na = left.apply(convert)\n        else:\n            left_na = left.isna()\n        if right.dtype == 'object':\n            right_na = right.apply(convert)\n        else:\n            right_na = right.isna()\n\n        tm.assert_series_equal(left_na, right_na)\n        return tm.assert_series_equal(left[~left_na],\n                                      right[~right_na],\n                                      *args, **kwargs)\n\n    def assert_frame_equal(self, left, right, *args, **kwargs):\n        # TODO(EA): select_dtypes\n        tm.assert_index_equal(\n            left.columns, right.columns,\n            exact=kwargs.get('check_column_type', 'equiv'),\n            check_names=kwargs.get('check_names', True),\n            check_exact=kwargs.get('check_exact', False),\n            check_categorical=kwargs.get('check_categorical', True),\n            obj='{obj}.columns'.format(obj=kwargs.get('obj', 'DataFrame')))\n\n        decimals = (left.dtypes == 'decimal').index\n\n        for col in decimals:\n            self.assert_series_equal(left[col], right[col],\n                                     *args, **kwargs)\n\n        left = left.drop(columns=decimals)\n        right = right.drop(columns=decimals)\n        tm.assert_frame_equal(left, right, *args, **kwargs)\n\n\nclass TestDtype(BaseDecimal, base.BaseDtypeTests):\n    def test_hashable(self, dtype):\n        pass\n\n\nclass TestInterface(BaseDecimal, base.BaseInterfaceTests):\n    pass\n\n\nclass TestConstructors(BaseDecimal, base.BaseConstructorsTests):\n\n    @pytest.mark.skip(reason=\"not implemented constructor from dtype\")\n    def test_from_dtype(self, data):\n        # construct from our dtype & string dtype\n        pass\n\n\nclass TestReshaping(BaseDecimal, base.BaseReshapingTests):\n    pass\n\n\nclass TestGetitem(BaseDecimal, base.BaseGetitemTests):\n\n    def test_take_na_value_other_decimal(self):\n        arr = DecimalArray([decimal.Decimal('1.0'),\n                            decimal.Decimal('2.0')])\n        result = arr.take([0, -1], allow_fill=True,\n                          fill_value=decimal.Decimal('-1.0'))\n        expected = DecimalArray([decimal.Decimal('1.0'),\n                                 decimal.Decimal('-1.0')])\n        self.assert_extension_array_equal(result, expected)\n\n\nclass TestMissing(BaseDecimal, base.BaseMissingTests):\n    pass\n\n\nclass Reduce:\n\n    def check_reduce(self, s, op_name, skipna):\n\n        if skipna or op_name in ['median', 'skew', 'kurt']:\n            with pytest.raises(NotImplementedError):\n                getattr(s, op_name)(skipna=skipna)\n\n        else:\n            result = getattr(s, op_name)(skipna=skipna)\n            expected = getattr(np.asarray(s), op_name)()\n            tm.assert_almost_equal(result, expected)\n\n\nclass TestNumericReduce(Reduce, base.BaseNumericReduceTests):\n    pass\n\n\nclass TestBooleanReduce(Reduce, base.BaseBooleanReduceTests):\n    pass\n\n\nclass TestMethods(BaseDecimal, base.BaseMethodsTests):\n    @pytest.mark.parametrize('dropna', [True, False])\n    @pytest.mark.xfail(reason=\"value_counts not implemented yet.\")\n    def test_value_counts(self, all_data, dropna):\n        all_data = all_data[:10]\n        if dropna:\n            other = np.array(all_data[~all_data.isna()])\n        else:\n            other = all_data\n\n        result = pd.Series(all_data).value_counts(dropna=dropna).sort_index()\n        expected = pd.Series(other).value_counts(dropna=dropna).sort_index()\n\n        tm.assert_series_equal(result, expected)\n\n\nclass TestCasting(BaseDecimal, base.BaseCastingTests):\n    pass\n\n\nclass TestGroupby(BaseDecimal, base.BaseGroupbyTests):\n\n    @pytest.mark.xfail(\n        reason=\"needs to correctly define __eq__ to handle nans, xref #27081.\")\n    def test_groupby_apply_identity(self, data_for_grouping):\n        super().test_groupby_apply_identity(data_for_grouping)\n\n\nclass TestSetitem(BaseDecimal, base.BaseSetitemTests):\n    pass\n\n\nclass TestPrinting(BaseDecimal, base.BasePrintingTests):\n\n    def test_series_repr(self, data):\n        # Overriding this base test to explicitly test that\n        # the custom _formatter is used\n        ser = pd.Series(data)\n        assert data.dtype.name in repr(ser)\n        assert \"Decimal: \" in repr(ser)\n\n\n# TODO(extension)\n@pytest.mark.xfail(reason=(\n    \"raising AssertionError as this is not implemented, \"\n    \"though easy enough to do\"))\ndef test_series_constructor_coerce_data_to_extension_dtype_raises():\n    xpr = (\"Cannot cast data to extension dtype 'decimal'. Pass the \"\n           \"extension array directly.\")\n    with pytest.raises(ValueError, match=xpr):\n        pd.Series([0, 1, 2], dtype=DecimalDtype())\n\n\ndef test_series_constructor_with_dtype():\n    arr = DecimalArray([decimal.Decimal('10.0')])\n    result = pd.Series(arr, dtype=DecimalDtype())\n    expected = pd.Series(arr)\n    tm.assert_series_equal(result, expected)\n\n    result = pd.Series(arr, dtype='int64')\n    expected = pd.Series([10])\n    tm.assert_series_equal(result, expected)\n\n\ndef test_dataframe_constructor_with_dtype():\n    arr = DecimalArray([decimal.Decimal('10.0')])\n\n    result = pd.DataFrame({\"A\": arr}, dtype=DecimalDtype())\n    expected = pd.DataFrame({\"A\": arr})\n    tm.assert_frame_equal(result, expected)\n\n    arr = DecimalArray([decimal.Decimal('10.0')])\n    result = pd.DataFrame({\"A\": arr}, dtype='int64')\n    expected = pd.DataFrame({\"A\": [10]})\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"frame\", [True, False])\ndef test_astype_dispatches(frame):\n    # This is a dtype-specific test that ensures Series[decimal].astype\n    # gets all the way through to ExtensionArray.astype\n    # Designing a reliable smoke test that works for arbitrary data types\n    # is difficult.\n    data = pd.Series(DecimalArray([decimal.Decimal(2)]), name='a')\n    ctx = decimal.Context()\n    ctx.prec = 5\n\n    if frame:\n        data = data.to_frame()\n\n    result = data.astype(DecimalDtype(ctx))\n\n    if frame:\n        result = result['a']\n\n    assert result.dtype.context.prec == ctx.prec\n\n\nclass TestArithmeticOps(BaseDecimal, base.BaseArithmeticOpsTests):\n\n    def check_opname(self, s, op_name, other, exc=None):\n        super().check_opname(s, op_name, other, exc=None)\n\n    def test_arith_series_with_array(self, data, all_arithmetic_operators):\n        op_name = all_arithmetic_operators\n        s = pd.Series(data)\n\n        context = decimal.getcontext()\n        divbyzerotrap = context.traps[decimal.DivisionByZero]\n        invalidoptrap = context.traps[decimal.InvalidOperation]\n        context.traps[decimal.DivisionByZero] = 0\n        context.traps[decimal.InvalidOperation] = 0\n\n        # Decimal supports ops with int, but not float\n        other = pd.Series([int(d * 100) for d in data])\n        self.check_opname(s, op_name, other)\n\n        if \"mod\" not in op_name:\n            self.check_opname(s, op_name, s * 2)\n\n        self.check_opname(s, op_name, 0)\n        self.check_opname(s, op_name, 5)\n        context.traps[decimal.DivisionByZero] = divbyzerotrap\n        context.traps[decimal.InvalidOperation] = invalidoptrap\n\n    def _check_divmod_op(self, s, op, other, exc=NotImplementedError):\n        # We implement divmod\n        super()._check_divmod_op(s, op, other, exc=None)\n\n    def test_error(self):\n        pass\n\n\nclass TestComparisonOps(BaseDecimal, base.BaseComparisonOpsTests):\n\n    def check_opname(self, s, op_name, other, exc=None):\n        super().check_opname(s, op_name, other, exc=None)\n\n    def _compare_other(self, s, data, op_name, other):\n        self.check_opname(s, op_name, other)\n\n    def test_compare_scalar(self, data, all_compare_operators):\n        op_name = all_compare_operators\n        s = pd.Series(data)\n        self._compare_other(s, data, op_name, 0.5)\n\n    def test_compare_array(self, data, all_compare_operators):\n        op_name = all_compare_operators\n        s = pd.Series(data)\n\n        alter = np.random.choice([-1, 0, 1], len(data))\n        # Randomly double, halve or keep same value\n        other = pd.Series(data) * [decimal.Decimal(pow(2.0, i))\n                                   for i in alter]\n        self._compare_other(s, data, op_name, other)\n\n\nclass DecimalArrayWithoutFromSequence(DecimalArray):\n    \"\"\"Helper class for testing error handling in _from_sequence.\"\"\"\n    def _from_sequence(cls, scalars, dtype=None, copy=False):\n        raise KeyError(\"For the test\")\n\n\nclass DecimalArrayWithoutCoercion(DecimalArrayWithoutFromSequence):\n    @classmethod\n    def _create_arithmetic_method(cls, op):\n        return cls._create_method(op, coerce_to_dtype=False)\n\n\nDecimalArrayWithoutCoercion._add_arithmetic_ops()\n\n\ndef test_combine_from_sequence_raises():\n    # https://github.com/pandas-dev/pandas/issues/22850\n    ser = pd.Series(DecimalArrayWithoutFromSequence([\n        decimal.Decimal(\"1.0\"),\n        decimal.Decimal(\"2.0\")\n    ]))\n    result = ser.combine(ser, operator.add)\n\n    # note: object dtype\n    expected = pd.Series([decimal.Decimal(\"2.0\"),\n                          decimal.Decimal(\"4.0\")], dtype=\"object\")\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"class_\", [DecimalArrayWithoutFromSequence,\n                                    DecimalArrayWithoutCoercion])\ndef test_scalar_ops_from_sequence_raises(class_):\n    # op(EA, EA) should return an EA, or an ndarray if it's not possible\n    # to return an EA with the return values.\n    arr = class_([\n        decimal.Decimal(\"1.0\"),\n        decimal.Decimal(\"2.0\")\n    ])\n    result = arr + arr\n    expected = np.array([decimal.Decimal(\"2.0\"), decimal.Decimal(\"4.0\")],\n                        dtype=\"object\")\n    tm.assert_numpy_array_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"reverse, expected_div, expected_mod\", [\n    (False, [0, 1, 1, 2], [1, 0, 1, 0]),\n    (True, [2, 1, 0, 0], [0, 0, 2, 2]),\n])\ndef test_divmod_array(reverse, expected_div, expected_mod):\n    # https://github.com/pandas-dev/pandas/issues/22930\n    arr = to_decimal([1, 2, 3, 4])\n    if reverse:\n        div, mod = divmod(2, arr)\n    else:\n        div, mod = divmod(arr, 2)\n    expected_div = to_decimal(expected_div)\n    expected_mod = to_decimal(expected_mod)\n\n    tm.assert_extension_array_equal(div, expected_div)\n    tm.assert_extension_array_equal(mod, expected_mod)\n\n\ndef test_formatting_values_deprecated():\n    class DecimalArray2(DecimalArray):\n        def _formatting_values(self):\n            return np.array(self)\n\n    ser = pd.Series(DecimalArray2([decimal.Decimal('1.0')]))\n\n    with tm.assert_produces_warning(DeprecationWarning,\n                                    check_stacklevel=False):\n        repr(ser)\n"
    },
    {
      "filename": "pandas/tests/groupby/test_categorical.py",
      "content": "from collections import OrderedDict\nfrom datetime import datetime\n\nimport numpy as np\nimport pytest\n\nfrom pandas.compat import PY37\n\nimport pandas as pd\nfrom pandas import (\n    Categorical, CategoricalIndex, DataFrame, Index, MultiIndex, Series, qcut)\nimport pandas.util.testing as tm\nfrom pandas.util.testing import (\n    assert_equal, assert_frame_equal, assert_series_equal)\n\n\ndef cartesian_product_for_groupers(result, args, names):\n    \"\"\" Reindex to a cartesian production for the groupers,\n    preserving the nature (Categorical) of each grouper \"\"\"\n\n    def f(a):\n        if isinstance(a, (CategoricalIndex, Categorical)):\n            categories = a.categories\n            a = Categorical.from_codes(np.arange(len(categories)),\n                                       categories=categories,\n                                       ordered=a.ordered)\n        return a\n\n    index = MultiIndex.from_product(map(f, args), names=names)\n    return result.reindex(index).sort_index()\n\n\ndef test_apply_use_categorical_name(df):\n    cats = qcut(df.C, 4)\n\n    def get_stats(group):\n        return {'min': group.min(),\n                'max': group.max(),\n                'count': group.count(),\n                'mean': group.mean()}\n\n    result = df.groupby(cats, observed=False).D.apply(get_stats)\n    assert result.index.names[0] == 'C'\n\n\ndef test_basic():\n\n    cats = Categorical([\"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\"],\n                       categories=[\"a\", \"b\", \"c\", \"d\"], ordered=True)\n    data = DataFrame({\"a\": [1, 1, 1, 2, 2, 2, 3, 4, 5], \"b\": cats})\n\n    exp_index = CategoricalIndex(list('abcd'), name='b', ordered=True)\n    expected = DataFrame({'a': [1, 2, 4, np.nan]}, index=exp_index)\n    result = data.groupby(\"b\", observed=False).mean()\n    tm.assert_frame_equal(result, expected)\n\n    cat1 = Categorical([\"a\", \"a\", \"b\", \"b\"],\n                       categories=[\"a\", \"b\", \"z\"], ordered=True)\n    cat2 = Categorical([\"c\", \"d\", \"c\", \"d\"],\n                       categories=[\"c\", \"d\", \"y\"], ordered=True)\n    df = DataFrame({\"A\": cat1, \"B\": cat2, \"values\": [1, 2, 3, 4]})\n\n    # single grouper\n    gb = df.groupby(\"A\", observed=False)\n    exp_idx = CategoricalIndex(['a', 'b', 'z'], name='A', ordered=True)\n    expected = DataFrame({'values': Series([3, 7, 0], index=exp_idx)})\n    result = gb.sum()\n    tm.assert_frame_equal(result, expected)\n\n    # GH 8623\n    x = DataFrame([[1, 'John P. Doe'], [2, 'Jane Dove'],\n                   [1, 'John P. Doe']],\n                  columns=['person_id', 'person_name'])\n    x['person_name'] = Categorical(x.person_name)\n\n    g = x.groupby(['person_id'], observed=False)\n    result = g.transform(lambda x: x)\n    tm.assert_frame_equal(result, x[['person_name']])\n\n    result = x.drop_duplicates('person_name')\n    expected = x.iloc[[0, 1]]\n    tm.assert_frame_equal(result, expected)\n\n    def f(x):\n        return x.drop_duplicates('person_name').iloc[0]\n\n    result = g.apply(f)\n    expected = x.iloc[[0, 1]].copy()\n    expected.index = Index([1, 2], name='person_id')\n    expected['person_name'] = expected['person_name'].astype('object')\n    tm.assert_frame_equal(result, expected)\n\n    # GH 9921\n    # Monotonic\n    df = DataFrame({\"a\": [5, 15, 25]})\n    c = pd.cut(df.a, bins=[0, 10, 20, 30, 40])\n\n    result = df.a.groupby(c, observed=False).transform(sum)\n    tm.assert_series_equal(result, df['a'])\n\n    tm.assert_series_equal(\n        df.a.groupby(c, observed=False).transform(lambda xs: np.sum(xs)),\n        df['a'])\n    tm.assert_frame_equal(\n        df.groupby(c, observed=False).transform(sum),\n        df[['a']])\n    tm.assert_frame_equal(\n        df.groupby(c, observed=False).transform(lambda xs: np.max(xs)),\n        df[['a']])\n\n    # Filter\n    tm.assert_series_equal(\n        df.a.groupby(c, observed=False).filter(np.all),\n        df['a'])\n    tm.assert_frame_equal(\n        df.groupby(c, observed=False).filter(np.all),\n        df)\n\n    # Non-monotonic\n    df = DataFrame({\"a\": [5, 15, 25, -5]})\n    c = pd.cut(df.a, bins=[-10, 0, 10, 20, 30, 40])\n\n    result = df.a.groupby(c, observed=False).transform(sum)\n    tm.assert_series_equal(result, df['a'])\n\n    tm.assert_series_equal(\n        df.a.groupby(c, observed=False).transform(lambda xs: np.sum(xs)),\n        df['a'])\n    tm.assert_frame_equal(\n        df.groupby(c, observed=False).transform(sum),\n        df[['a']])\n    tm.assert_frame_equal(\n        df.groupby(c, observed=False).transform(lambda xs: np.sum(xs)),\n        df[['a']])\n\n    # GH 9603\n    df = DataFrame({'a': [1, 0, 0, 0]})\n    c = pd.cut(df.a, [0, 1, 2, 3, 4], labels=Categorical(list('abcd')))\n    result = df.groupby(c, observed=False).apply(len)\n\n    exp_index = CategoricalIndex(\n        c.values.categories, ordered=c.values.ordered)\n    expected = Series([1, 0, 0, 0], index=exp_index)\n    expected.index.name = 'a'\n    tm.assert_series_equal(result, expected)\n\n    # more basic\n    levels = ['foo', 'bar', 'baz', 'qux']\n    codes = np.random.randint(0, 4, size=100)\n\n    cats = Categorical.from_codes(codes, levels, ordered=True)\n\n    data = DataFrame(np.random.randn(100, 4))\n\n    result = data.groupby(cats, observed=False).mean()\n\n    expected = data.groupby(np.asarray(cats), observed=False).mean()\n    exp_idx = CategoricalIndex(levels, categories=cats.categories,\n                               ordered=True)\n    expected = expected.reindex(exp_idx)\n\n    assert_frame_equal(result, expected)\n\n    grouped = data.groupby(cats, observed=False)\n    desc_result = grouped.describe()\n\n    idx = cats.codes.argsort()\n    ord_labels = np.asarray(cats).take(idx)\n    ord_data = data.take(idx)\n\n    exp_cats = Categorical(ord_labels, ordered=True,\n                           categories=['foo', 'bar', 'baz', 'qux'])\n    expected = ord_data.groupby(\n        exp_cats, sort=False, observed=False).describe()\n    assert_frame_equal(desc_result, expected)\n\n    # GH 10460\n    expc = Categorical.from_codes(np.arange(4).repeat(8),\n                                  levels, ordered=True)\n    exp = CategoricalIndex(expc)\n    tm.assert_index_equal((desc_result.stack().index\n                           .get_level_values(0)), exp)\n    exp = Index(['count', 'mean', 'std', 'min', '25%', '50%',\n                 '75%', 'max'] * 4)\n    tm.assert_index_equal((desc_result.stack().index\n                           .get_level_values(1)), exp)\n\n\ndef test_level_get_group(observed):\n    # GH15155\n    df = DataFrame(data=np.arange(2, 22, 2),\n                   index=MultiIndex(\n                       levels=[CategoricalIndex([\"a\", \"b\"]), range(10)],\n                       codes=[[0] * 5 + [1] * 5, range(10)],\n                       names=[\"Index1\", \"Index2\"]))\n    g = df.groupby(level=[\"Index1\"], observed=observed)\n\n    # expected should equal test.loc[[\"a\"]]\n    # GH15166\n    expected = DataFrame(data=np.arange(2, 12, 2),\n                         index=MultiIndex(levels=[CategoricalIndex(\n                             [\"a\", \"b\"]), range(5)],\n        codes=[[0] * 5, range(5)],\n        names=[\"Index1\", \"Index2\"]))\n    result = g.get_group('a')\n\n    assert_frame_equal(result, expected)\n\n\n@pytest.mark.xfail(PY37, reason=\"flaky on 3.7, xref gh-21636\", strict=False)\n@pytest.mark.parametrize('ordered', [True, False])\ndef test_apply(ordered):\n    # GH 10138\n\n    dense = Categorical(list('abc'), ordered=ordered)\n\n    # 'b' is in the categories but not in the list\n    missing = Categorical(\n        list('aaa'), categories=['a', 'b'], ordered=ordered)\n    values = np.arange(len(dense))\n    df = DataFrame({'missing': missing,\n                    'dense': dense,\n                    'values': values})\n    grouped = df.groupby(['missing', 'dense'], observed=True)\n\n    # missing category 'b' should still exist in the output index\n    idx = MultiIndex.from_arrays(\n        [missing, dense], names=['missing', 'dense'])\n    expected = DataFrame([0, 1, 2.],\n                         index=idx,\n                         columns=['values'])\n\n    result = grouped.apply(lambda x: np.mean(x))\n    assert_frame_equal(result, expected)\n\n    # we coerce back to ints\n    expected = expected.astype('int')\n    result = grouped.mean()\n    assert_frame_equal(result, expected)\n\n    result = grouped.agg(np.mean)\n    assert_frame_equal(result, expected)\n\n    # but for transform we should still get back the original index\n    idx = MultiIndex.from_arrays([missing, dense],\n                                 names=['missing', 'dense'])\n    expected = Series(1, index=idx)\n    result = grouped.apply(lambda x: 1)\n    assert_series_equal(result, expected)\n\n\ndef test_observed(observed):\n    # multiple groupers, don't re-expand the output space\n    # of the grouper\n    # gh-14942 (implement)\n    # gh-10132 (back-compat)\n    # gh-8138 (back-compat)\n    # gh-8869\n\n    cat1 = Categorical([\"a\", \"a\", \"b\", \"b\"],\n                       categories=[\"a\", \"b\", \"z\"], ordered=True)\n    cat2 = Categorical([\"c\", \"d\", \"c\", \"d\"],\n                       categories=[\"c\", \"d\", \"y\"], ordered=True)\n    df = DataFrame({\"A\": cat1, \"B\": cat2, \"values\": [1, 2, 3, 4]})\n    df['C'] = ['foo', 'bar'] * 2\n\n    # multiple groupers with a non-cat\n    gb = df.groupby(['A', 'B', 'C'], observed=observed)\n    exp_index = MultiIndex.from_arrays(\n        [cat1, cat2, ['foo', 'bar'] * 2],\n        names=['A', 'B', 'C'])\n    expected = DataFrame({'values': Series(\n        [1, 2, 3, 4], index=exp_index)}).sort_index()\n    result = gb.sum()\n    if not observed:\n        expected = cartesian_product_for_groupers(\n            expected,\n            [cat1, cat2, ['foo', 'bar']],\n            list('ABC'))\n\n    tm.assert_frame_equal(result, expected)\n\n    gb = df.groupby(['A', 'B'], observed=observed)\n    exp_index = MultiIndex.from_arrays(\n        [cat1, cat2],\n        names=['A', 'B'])\n    expected = DataFrame({'values': [1, 2, 3, 4]},\n                         index=exp_index)\n    result = gb.sum()\n    if not observed:\n        expected = cartesian_product_for_groupers(\n            expected,\n            [cat1, cat2],\n            list('AB'))\n\n    tm.assert_frame_equal(result, expected)\n\n    # https://github.com/pandas-dev/pandas/issues/8138\n    d = {'cat':\n         Categorical([\"a\", \"b\", \"a\", \"b\"], categories=[\"a\", \"b\", \"c\"],\n                     ordered=True),\n         'ints': [1, 1, 2, 2],\n         'val': [10, 20, 30, 40]}\n    df = DataFrame(d)\n\n    # Grouping on a single column\n    groups_single_key = df.groupby(\"cat\", observed=observed)\n    result = groups_single_key.mean()\n\n    exp_index = CategoricalIndex(list('ab'), name=\"cat\",\n                                 categories=list('abc'),\n                                 ordered=True)\n    expected = DataFrame({\"ints\": [1.5, 1.5], \"val\": [20., 30]},\n                         index=exp_index)\n    if not observed:\n        index = CategoricalIndex(list('abc'), name=\"cat\",\n                                 categories=list('abc'),\n                                 ordered=True)\n        expected = expected.reindex(index)\n\n    tm.assert_frame_equal(result, expected)\n\n    # Grouping on two columns\n    groups_double_key = df.groupby([\"cat\", \"ints\"], observed=observed)\n    result = groups_double_key.agg('mean')\n    expected = DataFrame(\n        {\"val\": [10, 30, 20, 40],\n         \"cat\": Categorical(['a', 'a', 'b', 'b'],\n                            categories=['a', 'b', 'c'],\n                            ordered=True),\n         \"ints\": [1, 2, 1, 2]}).set_index([\"cat\", \"ints\"])\n    if not observed:\n        expected = cartesian_product_for_groupers(\n            expected,\n            [df.cat.values, [1, 2]],\n            ['cat', 'ints'])\n\n    tm.assert_frame_equal(result, expected)\n\n    # GH 10132\n    for key in [('a', 1), ('b', 2), ('b', 1), ('a', 2)]:\n        c, i = key\n        result = groups_double_key.get_group(key)\n        expected = df[(df.cat == c) & (df.ints == i)]\n        assert_frame_equal(result, expected)\n\n    # gh-8869\n    # with as_index\n    d = {'foo': [10, 8, 4, 8, 4, 1, 1], 'bar': [10, 20, 30, 40, 50, 60, 70],\n         'baz': ['d', 'c', 'e', 'a', 'a', 'd', 'c']}\n    df = DataFrame(d)\n    cat = pd.cut(df['foo'], np.linspace(0, 10, 3))\n    df['range'] = cat\n    groups = df.groupby(['range', 'baz'], as_index=False, observed=observed)\n    result = groups.agg('mean')\n\n    groups2 = df.groupby(['range', 'baz'], as_index=True, observed=observed)\n    expected = groups2.agg('mean').reset_index()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_observed_codes_remap(observed):\n    d = {'C1': [3, 3, 4, 5], 'C2': [1, 2, 3, 4], 'C3': [10, 100, 200, 34]}\n    df = DataFrame(d)\n    values = pd.cut(df['C1'], [1, 2, 3, 6])\n    values.name = \"cat\"\n    groups_double_key = df.groupby([values, 'C2'], observed=observed)\n\n    idx = MultiIndex.from_arrays([values, [1, 2, 3, 4]],\n                                 names=[\"cat\", \"C2\"])\n    expected = DataFrame({\"C1\": [3, 3, 4, 5],\n                          \"C3\": [10, 100, 200, 34]}, index=idx)\n    if not observed:\n        expected = cartesian_product_for_groupers(\n            expected,\n            [values.values, [1, 2, 3, 4]],\n            ['cat', 'C2'])\n\n    result = groups_double_key.agg('mean')\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_observed_perf():\n    # we create a cartesian product, so this is\n    # non-performant if we don't use observed values\n    # gh-14942\n    df = DataFrame({\n        'cat': np.random.randint(0, 255, size=30000),\n        'int_id': np.random.randint(0, 255, size=30000),\n        'other_id': np.random.randint(0, 10000, size=30000),\n        'foo': 0})\n    df['cat'] = df.cat.astype(str).astype('category')\n\n    grouped = df.groupby(['cat', 'int_id', 'other_id'], observed=True)\n    result = grouped.count()\n    assert result.index.levels[0].nunique() == df.cat.nunique()\n    assert result.index.levels[1].nunique() == df.int_id.nunique()\n    assert result.index.levels[2].nunique() == df.other_id.nunique()\n\n\ndef test_observed_groups(observed):\n    # gh-20583\n    # test that we have the appropriate groups\n\n    cat = Categorical(['a', 'c', 'a'], categories=['a', 'b', 'c'])\n    df = DataFrame({'cat': cat, 'vals': [1, 2, 3]})\n    g = df.groupby('cat', observed=observed)\n\n    result = g.groups\n    if observed:\n        expected = {'a': Index([0, 2], dtype='int64'),\n                    'c': Index([1], dtype='int64')}\n    else:\n        expected = {'a': Index([0, 2], dtype='int64'),\n                    'b': Index([], dtype='int64'),\n                    'c': Index([1], dtype='int64')}\n\n    tm.assert_dict_equal(result, expected)\n\n\ndef test_observed_groups_with_nan(observed):\n    # GH 24740\n    df = DataFrame({'cat': Categorical(['a', np.nan, 'a'],\n                    categories=['a', 'b', 'd']),\n                    'vals': [1, 2, 3]})\n    g = df.groupby('cat', observed=observed)\n    result = g.groups\n    if observed:\n        expected = {'a': Index([0, 2], dtype='int64')}\n    else:\n        expected = {'a': Index([0, 2], dtype='int64'),\n                    'b': Index([], dtype='int64'),\n                    'd': Index([], dtype='int64')}\n    tm.assert_dict_equal(result, expected)\n\n\ndef test_dataframe_categorical_with_nan(observed):\n    # GH 21151\n    s1 = Categorical([np.nan, 'a', np.nan, 'a'],\n                     categories=['a', 'b', 'c'])\n    s2 = Series([1, 2, 3, 4])\n    df = DataFrame({'s1': s1, 's2': s2})\n    result = df.groupby('s1', observed=observed).first().reset_index()\n    if observed:\n        expected = DataFrame({'s1': Categorical(['a'],\n                              categories=['a', 'b', 'c']), 's2': [2]})\n    else:\n        expected = DataFrame({'s1': Categorical(['a', 'b', 'c'],\n                              categories=['a', 'b', 'c']),\n                              's2': [2, np.nan, np.nan]})\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"ordered\", [True, False])\n@pytest.mark.parametrize(\"observed\", [True, False])\n@pytest.mark.parametrize(\"sort\", [True, False])\ndef test_dataframe_categorical_ordered_observed_sort(ordered, observed, sort):\n    # GH 25871: Fix groupby sorting on ordered Categoricals\n    # GH 25167: Groupby with observed=True doesn't sort\n\n    # Build a dataframe with cat having one unobserved category ('missing'),\n    # and a Series with identical values\n    label = Categorical(['d', 'a', 'b', 'a', 'd', 'b'],\n                        categories=['a', 'b', 'missing', 'd'],\n                        ordered=ordered)\n    val = Series(['d', 'a', 'b', 'a', 'd', 'b'])\n    df = DataFrame({'label': label, 'val': val})\n\n    # aggregate on the Categorical\n    result = (df.groupby('label', observed=observed, sort=sort)['val']\n                .aggregate('first'))\n\n    # If ordering works, we expect index labels equal to aggregation results,\n    # except for 'observed=False': label 'missing' has aggregation None\n    label = Series(result.index.array, dtype='object')\n    aggr = Series(result.array)\n    if not observed:\n        aggr[aggr.isna()] = 'missing'\n    if not all(label == aggr):\n        msg = ('Labels and aggregation results not consistently sorted\\n' +\n               'for (ordered={}, observed={}, sort={})\\n' +\n               'Result:\\n{}').format(ordered, observed, sort, result)\n        assert False, msg\n\n\ndef test_datetime():\n    # GH9049: ensure backward compatibility\n    levels = pd.date_range('2014-01-01', periods=4)\n    codes = np.random.randint(0, 4, size=100)\n\n    cats = Categorical.from_codes(codes, levels, ordered=True)\n\n    data = DataFrame(np.random.randn(100, 4))\n    result = data.groupby(cats, observed=False).mean()\n\n    expected = data.groupby(np.asarray(cats), observed=False).mean()\n    expected = expected.reindex(levels)\n    expected.index = CategoricalIndex(expected.index,\n                                      categories=expected.index,\n                                      ordered=True)\n\n    assert_frame_equal(result, expected)\n\n    grouped = data.groupby(cats, observed=False)\n    desc_result = grouped.describe()\n\n    idx = cats.codes.argsort()\n    ord_labels = cats.take_nd(idx)\n    ord_data = data.take(idx)\n    expected = ord_data.groupby(ord_labels, observed=False).describe()\n    assert_frame_equal(desc_result, expected)\n    tm.assert_index_equal(desc_result.index, expected.index)\n    tm.assert_index_equal(\n        desc_result.index.get_level_values(0),\n        expected.index.get_level_values(0))\n\n    # GH 10460\n    expc = Categorical.from_codes(\n        np.arange(4).repeat(8), levels, ordered=True)\n    exp = CategoricalIndex(expc)\n    tm.assert_index_equal((desc_result.stack().index\n                           .get_level_values(0)), exp)\n    exp = Index(['count', 'mean', 'std', 'min', '25%', '50%',\n                 '75%', 'max'] * 4)\n    tm.assert_index_equal((desc_result.stack().index\n                           .get_level_values(1)), exp)\n\n\ndef test_categorical_index():\n\n    s = np.random.RandomState(12345)\n    levels = ['foo', 'bar', 'baz', 'qux']\n    codes = s.randint(0, 4, size=20)\n    cats = Categorical.from_codes(codes, levels, ordered=True)\n    df = DataFrame(\n        np.repeat(\n            np.arange(20), 4).reshape(-1, 4), columns=list('abcd'))\n    df['cats'] = cats\n\n    # with a cat index\n    result = df.set_index('cats').groupby(level=0, observed=False).sum()\n    expected = df[list('abcd')].groupby(cats.codes, observed=False).sum()\n    expected.index = CategoricalIndex(\n        Categorical.from_codes(\n            [0, 1, 2, 3], levels, ordered=True), name='cats')\n    assert_frame_equal(result, expected)\n\n    # with a cat column, should produce a cat index\n    result = df.groupby('cats', observed=False).sum()\n    expected = df[list('abcd')].groupby(cats.codes, observed=False).sum()\n    expected.index = CategoricalIndex(\n        Categorical.from_codes(\n            [0, 1, 2, 3], levels, ordered=True), name='cats')\n    assert_frame_equal(result, expected)\n\n\ndef test_describe_categorical_columns():\n    # GH 11558\n    cats = CategoricalIndex(['qux', 'foo', 'baz', 'bar'],\n                            categories=['foo', 'bar', 'baz', 'qux'],\n                            ordered=True)\n    df = DataFrame(np.random.randn(20, 4), columns=cats)\n    result = df.groupby([1, 2, 3, 4] * 5).describe()\n\n    tm.assert_index_equal(result.stack().columns, cats)\n    tm.assert_categorical_equal(result.stack().columns.values, cats.values)\n\n\ndef test_unstack_categorical():\n    # GH11558 (example is taken from the original issue)\n    df = DataFrame({'a': range(10),\n                    'medium': ['A', 'B'] * 5,\n                    'artist': list('XYXXY') * 2})\n    df['medium'] = df['medium'].astype('category')\n\n    gcat = df.groupby(\n        ['artist', 'medium'], observed=False)['a'].count().unstack()\n    result = gcat.describe()\n\n    exp_columns = CategoricalIndex(['A', 'B'], ordered=False,\n                                   name='medium')\n    tm.assert_index_equal(result.columns, exp_columns)\n    tm.assert_categorical_equal(result.columns.values, exp_columns.values)\n\n    result = gcat['A'] + gcat['B']\n    expected = Series([6, 4], index=Index(['X', 'Y'], name='artist'))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_bins_unequal_len():\n    # GH3011\n    series = Series([np.nan, np.nan, 1, 1, 2, 2, 3, 3, 4, 4])\n    bins = pd.cut(series.dropna().values, 4)\n\n    # len(bins) != len(series) here\n    with pytest.raises(ValueError):\n        series.groupby(bins).mean()\n\n\ndef test_as_index():\n    # GH13204\n    df = DataFrame({'cat': Categorical([1, 2, 2], [1, 2, 3]),\n                    'A': [10, 11, 11],\n                    'B': [101, 102, 103]})\n    result = df.groupby(['cat', 'A'], as_index=False, observed=True).sum()\n    expected = DataFrame(\n        {'cat': Categorical([1, 2], categories=df.cat.cat.categories),\n         'A': [10, 11],\n         'B': [101, 205]},\n        columns=['cat', 'A', 'B'])\n    tm.assert_frame_equal(result, expected)\n\n    # function grouper\n    f = lambda r: df.loc[r, 'A']\n    result = df.groupby(['cat', f], as_index=False, observed=True).sum()\n    expected = DataFrame(\n        {'cat': Categorical([1, 2], categories=df.cat.cat.categories),\n         'A': [10, 22],\n         'B': [101, 205]},\n        columns=['cat', 'A', 'B'])\n    tm.assert_frame_equal(result, expected)\n\n    # another not in-axis grouper (conflicting names in index)\n    s = Series(['a', 'b', 'b'], name='cat')\n    result = df.groupby(['cat', s], as_index=False, observed=True).sum()\n    tm.assert_frame_equal(result, expected)\n\n    # is original index dropped?\n    group_columns = ['cat', 'A']\n    expected = DataFrame(\n        {'cat': Categorical([1, 2], categories=df.cat.cat.categories),\n         'A': [10, 11],\n         'B': [101, 205]},\n        columns=['cat', 'A', 'B'])\n\n    for name in [None, 'X', 'B']:\n        df.index = Index(list(\"abc\"), name=name)\n        result = df.groupby(group_columns, as_index=False, observed=True).sum()\n\n        tm.assert_frame_equal(result, expected)\n\n\ndef test_preserve_categories():\n    # GH-13179\n    categories = list('abc')\n\n    # ordered=True\n    df = DataFrame({'A': Categorical(list('ba'),\n                                     categories=categories,\n                                     ordered=True)})\n    index = CategoricalIndex(categories, categories, ordered=True)\n    tm.assert_index_equal(\n        df.groupby('A', sort=True, observed=False).first().index, index)\n    tm.assert_index_equal(\n        df.groupby('A', sort=False, observed=False).first().index, index)\n\n    # ordered=False\n    df = DataFrame({'A': Categorical(list('ba'),\n                                     categories=categories,\n                                     ordered=False)})\n    sort_index = CategoricalIndex(categories, categories, ordered=False)\n    nosort_index = CategoricalIndex(list('bac'), list('bac'),\n                                    ordered=False)\n    tm.assert_index_equal(\n        df.groupby('A', sort=True, observed=False).first().index,\n        sort_index)\n    tm.assert_index_equal(\n        df.groupby('A', sort=False, observed=False).first().index,\n        nosort_index)\n\n\ndef test_preserve_categorical_dtype():\n    # GH13743, GH13854\n    df = DataFrame({'A': [1, 2, 1, 1, 2],\n                    'B': [10, 16, 22, 28, 34],\n                    'C1': Categorical(list(\"abaab\"),\n                                      categories=list(\"bac\"),\n                                      ordered=False),\n                    'C2': Categorical(list(\"abaab\"),\n                                      categories=list(\"bac\"),\n                                      ordered=True)})\n    # single grouper\n    exp_full = DataFrame({'A': [2.0, 1.0, np.nan],\n                          'B': [25.0, 20.0, np.nan],\n                          'C1': Categorical(list(\"bac\"),\n                                            categories=list(\"bac\"),\n                                            ordered=False),\n                          'C2': Categorical(list(\"bac\"),\n                                            categories=list(\"bac\"),\n                                            ordered=True)})\n    for col in ['C1', 'C2']:\n        result1 = df.groupby(by=col, as_index=False, observed=False).mean()\n        result2 = df.groupby(\n            by=col, as_index=True, observed=False).mean().reset_index()\n        expected = exp_full.reindex(columns=result1.columns)\n        tm.assert_frame_equal(result1, expected)\n        tm.assert_frame_equal(result2, expected)\n\n\n@pytest.mark.parametrize(\n    'func, values',\n    [('first', ['second', 'first']),\n     ('last', ['fourth', 'third']),\n     ('min', ['fourth', 'first']),\n     ('max', ['second', 'third'])])\ndef test_preserve_on_ordered_ops(func, values):\n    # gh-18502\n    # preserve the categoricals on ops\n    c = pd.Categorical(['first', 'second', 'third', 'fourth'], ordered=True)\n    df = pd.DataFrame(\n        {'payload': [-1, -2, -1, -2],\n         'col': c})\n    g = df.groupby('payload')\n    result = getattr(g, func)()\n    expected = pd.DataFrame(\n        {'payload': [-2, -1],\n         'col': pd.Series(values, dtype=c.dtype)}).set_index('payload')\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_categorical_no_compress():\n    data = Series(np.random.randn(9))\n\n    codes = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])\n    cats = Categorical.from_codes(codes, [0, 1, 2], ordered=True)\n\n    result = data.groupby(cats, observed=False).mean()\n    exp = data.groupby(codes, observed=False).mean()\n\n    exp.index = CategoricalIndex(exp.index, categories=cats.categories,\n                                 ordered=cats.ordered)\n    assert_series_equal(result, exp)\n\n    codes = np.array([0, 0, 0, 1, 1, 1, 3, 3, 3])\n    cats = Categorical.from_codes(codes, [0, 1, 2, 3], ordered=True)\n\n    result = data.groupby(cats, observed=False).mean()\n    exp = data.groupby(codes, observed=False).mean().reindex(cats.categories)\n    exp.index = CategoricalIndex(exp.index, categories=cats.categories,\n                                 ordered=cats.ordered)\n    assert_series_equal(result, exp)\n\n    cats = Categorical([\"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\"],\n                       categories=[\"a\", \"b\", \"c\", \"d\"], ordered=True)\n    data = DataFrame({\"a\": [1, 1, 1, 2, 2, 2, 3, 4, 5], \"b\": cats})\n\n    result = data.groupby(\"b\", observed=False).mean()\n    result = result[\"a\"].values\n    exp = np.array([1, 2, 4, np.nan])\n    tm.assert_numpy_array_equal(result, exp)\n\n\ndef test_sort():\n\n    # http://stackoverflow.com/questions/23814368/sorting-pandas-categorical-labels-after-groupby  # noqa: flake8\n    # This should result in a properly sorted Series so that the plot\n    # has a sorted x axis\n    # self.cat.groupby(['value_group'])['value_group'].count().plot(kind='bar')\n\n    df = DataFrame({'value': np.random.randint(0, 10000, 100)})\n    labels = [\"{0} - {1}\".format(i, i + 499) for i in range(0, 10000, 500)]\n    cat_labels = Categorical(labels, labels)\n\n    df = df.sort_values(by=['value'], ascending=True)\n    df['value_group'] = pd.cut(df.value, range(0, 10500, 500),\n                               right=False, labels=cat_labels)\n\n    res = df.groupby(['value_group'], observed=False)['value_group'].count()\n    exp = res[sorted(res.index, key=lambda x: float(x.split()[0]))]\n    exp.index = CategoricalIndex(exp.index, name=exp.index.name)\n    tm.assert_series_equal(res, exp)\n\n\ndef test_sort2():\n    # dataframe groupby sort was being ignored # GH 8868\n    df = DataFrame([['(7.5, 10]', 10, 10],\n                    ['(7.5, 10]', 8, 20],\n                    ['(2.5, 5]', 5, 30],\n                    ['(5, 7.5]', 6, 40],\n                    ['(2.5, 5]', 4, 50],\n                    ['(0, 2.5]', 1, 60],\n                    ['(5, 7.5]', 7, 70]], columns=['range', 'foo', 'bar'])\n    df['range'] = Categorical(df['range'], ordered=True)\n    index = CategoricalIndex(['(0, 2.5]', '(2.5, 5]', '(5, 7.5]',\n                              '(7.5, 10]'], name='range', ordered=True)\n    expected_sort = DataFrame([[1, 60], [5, 30], [6, 40], [10, 10]],\n                              columns=['foo', 'bar'], index=index)\n\n    col = 'range'\n    result_sort = df.groupby(col, sort=True, observed=False).first()\n    assert_frame_equal(result_sort, expected_sort)\n\n    # when categories is ordered, group is ordered by category's order\n    expected_sort = result_sort\n    result_sort = df.groupby(col, sort=False, observed=False).first()\n    assert_frame_equal(result_sort, expected_sort)\n\n    df['range'] = Categorical(df['range'], ordered=False)\n    index = CategoricalIndex(['(0, 2.5]', '(2.5, 5]', '(5, 7.5]',\n                              '(7.5, 10]'], name='range')\n    expected_sort = DataFrame([[1, 60], [5, 30], [6, 40], [10, 10]],\n                              columns=['foo', 'bar'], index=index)\n\n    index = CategoricalIndex(['(7.5, 10]', '(2.5, 5]', '(5, 7.5]',\n                              '(0, 2.5]'],\n                             categories=['(7.5, 10]', '(2.5, 5]',\n                                         '(5, 7.5]', '(0, 2.5]'],\n                             name='range')\n    expected_nosort = DataFrame([[10, 10], [5, 30], [6, 40], [1, 60]],\n                                index=index, columns=['foo', 'bar'])\n\n    col = 'range'\n\n    # this is an unordered categorical, but we allow this ####\n    result_sort = df.groupby(col, sort=True, observed=False).first()\n    assert_frame_equal(result_sort, expected_sort)\n\n    result_nosort = df.groupby(col, sort=False, observed=False).first()\n    assert_frame_equal(result_nosort, expected_nosort)\n\n\ndef test_sort_datetimelike():\n    # GH10505\n\n    # use same data as test_groupby_sort_categorical, which category is\n    # corresponding to datetime.month\n    df = DataFrame({'dt': [datetime(2011, 7, 1), datetime(2011, 7, 1),\n                           datetime(2011, 2, 1), datetime(2011, 5, 1),\n                           datetime(2011, 2, 1), datetime(2011, 1, 1),\n                           datetime(2011, 5, 1)],\n                    'foo': [10, 8, 5, 6, 4, 1, 7],\n                    'bar': [10, 20, 30, 40, 50, 60, 70]},\n                   columns=['dt', 'foo', 'bar'])\n\n    # ordered=True\n    df['dt'] = Categorical(df['dt'], ordered=True)\n    index = [datetime(2011, 1, 1), datetime(2011, 2, 1),\n             datetime(2011, 5, 1), datetime(2011, 7, 1)]\n    result_sort = DataFrame(\n        [[1, 60], [5, 30], [6, 40], [10, 10]], columns=['foo', 'bar'])\n    result_sort.index = CategoricalIndex(index, name='dt', ordered=True)\n\n    index = [datetime(2011, 7, 1), datetime(2011, 2, 1),\n             datetime(2011, 5, 1), datetime(2011, 1, 1)]\n    result_nosort = DataFrame([[10, 10], [5, 30], [6, 40], [1, 60]],\n                              columns=['foo', 'bar'])\n    result_nosort.index = CategoricalIndex(index, categories=index,\n                                           name='dt', ordered=True)\n\n    col = 'dt'\n    assert_frame_equal(\n        result_sort, df.groupby(col, sort=True, observed=False).first())\n\n    # when categories is ordered, group is ordered by category's order\n    assert_frame_equal(\n        result_sort, df.groupby(col, sort=False, observed=False).first())\n\n    # ordered = False\n    df['dt'] = Categorical(df['dt'], ordered=False)\n    index = [datetime(2011, 1, 1), datetime(2011, 2, 1),\n             datetime(2011, 5, 1), datetime(2011, 7, 1)]\n    result_sort = DataFrame(\n        [[1, 60], [5, 30], [6, 40], [10, 10]], columns=['foo', 'bar'])\n    result_sort.index = CategoricalIndex(index, name='dt')\n\n    index = [datetime(2011, 7, 1), datetime(2011, 2, 1),\n             datetime(2011, 5, 1), datetime(2011, 1, 1)]\n    result_nosort = DataFrame([[10, 10], [5, 30], [6, 40], [1, 60]],\n                              columns=['foo', 'bar'])\n    result_nosort.index = CategoricalIndex(index, categories=index,\n                                           name='dt')\n\n    col = 'dt'\n    assert_frame_equal(\n        result_sort, df.groupby(col, sort=True, observed=False).first())\n    assert_frame_equal(\n        result_nosort, df.groupby(col, sort=False, observed=False).first())\n\n\ndef test_empty_sum():\n    # https://github.com/pandas-dev/pandas/issues/18678\n    df = DataFrame({\"A\": Categorical(['a', 'a', 'b'],\n                                     categories=['a', 'b', 'c']),\n                    'B': [1, 2, 1]})\n    expected_idx = CategoricalIndex(['a', 'b', 'c'], name='A')\n\n    # 0 by default\n    result = df.groupby(\"A\", observed=False).B.sum()\n    expected = Series([3, 1, 0], expected_idx, name='B')\n    tm.assert_series_equal(result, expected)\n\n    # min_count=0\n    result = df.groupby(\"A\", observed=False).B.sum(min_count=0)\n    expected = Series([3, 1, 0], expected_idx, name='B')\n    tm.assert_series_equal(result, expected)\n\n    # min_count=1\n    result = df.groupby(\"A\", observed=False).B.sum(min_count=1)\n    expected = Series([3, 1, np.nan], expected_idx, name='B')\n    tm.assert_series_equal(result, expected)\n\n    # min_count>1\n    result = df.groupby(\"A\", observed=False).B.sum(min_count=2)\n    expected = Series([3, np.nan, np.nan], expected_idx, name='B')\n    tm.assert_series_equal(result, expected)\n\n\ndef test_empty_prod():\n    # https://github.com/pandas-dev/pandas/issues/18678\n    df = DataFrame({\"A\": Categorical(['a', 'a', 'b'],\n                                     categories=['a', 'b', 'c']),\n                    'B': [1, 2, 1]})\n\n    expected_idx = CategoricalIndex(['a', 'b', 'c'], name='A')\n\n    # 1 by default\n    result = df.groupby(\"A\", observed=False).B.prod()\n    expected = Series([2, 1, 1], expected_idx, name='B')\n    tm.assert_series_equal(result, expected)\n\n    # min_count=0\n    result = df.groupby(\"A\", observed=False).B.prod(min_count=0)\n    expected = Series([2, 1, 1], expected_idx, name='B')\n    tm.assert_series_equal(result, expected)\n\n    # min_count=1\n    result = df.groupby(\"A\", observed=False).B.prod(min_count=1)\n    expected = Series([2, 1, np.nan], expected_idx, name='B')\n    tm.assert_series_equal(result, expected)\n\n\ndef test_groupby_multiindex_categorical_datetime():\n    # https://github.com/pandas-dev/pandas/issues/21390\n\n    df = DataFrame({\n        'key1': Categorical(list('abcbabcba')),\n        'key2': Categorical(\n            list(pd.date_range('2018-06-01 00', freq='1T', periods=3)) * 3),\n        'values': np.arange(9),\n    })\n    result = df.groupby(['key1', 'key2']).mean()\n\n    idx = MultiIndex.from_product(\n        [Categorical(['a', 'b', 'c']),\n         Categorical(pd.date_range('2018-06-01 00', freq='1T', periods=3))],\n        names=['key1', 'key2'])\n    expected = DataFrame(\n        {'values': [0, 4, 8, 3, 4, 5, 6, np.nan, 2]}, index=idx)\n    assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"as_index, expected\", [\n    (True, Series(\n        index=MultiIndex.from_arrays(\n            [Series([1, 1, 2], dtype='category'),\n             [1, 2, 2]], names=['a', 'b']\n        ),\n        data=[1, 2, 3], name='x'\n    )),\n    (False, DataFrame({\n        'a': Series([1, 1, 2], dtype='category'),\n        'b': [1, 2, 2],\n        'x': [1, 2, 3]\n    }))\n])\ndef test_groupby_agg_observed_true_single_column(as_index, expected):\n    # GH-23970\n    df = DataFrame({\n        'a': Series([1, 1, 2], dtype='category'),\n        'b': [1, 2, 2],\n        'x': [1, 2, 3]\n    })\n\n    result = df.groupby(\n        ['a', 'b'], as_index=as_index, observed=True)['x'].sum()\n\n    assert_equal(result, expected)\n\n\n@pytest.mark.parametrize('fill_value', [None, np.nan, pd.NaT])\ndef test_shift(fill_value):\n    ct = Categorical(['a', 'b', 'c', 'd'],\n                     categories=['a', 'b', 'c', 'd'], ordered=False)\n    expected = Categorical([None, 'a', 'b', 'c'],\n                           categories=['a', 'b', 'c', 'd'], ordered=False)\n    res = ct.shift(1, fill_value=fill_value)\n    assert_equal(res, expected)\n\n\n@pytest.fixture\ndef df_cat(df):\n    \"\"\"\n    DataFrame with multiple categorical columns and a column of integers.\n    Shortened so as not to contain all possible combinations of categories.\n    Useful for testing `observed` kwarg functionality on GroupBy objects.\n\n    Parameters\n    ----------\n    df: DataFrame\n        Non-categorical, longer DataFrame from another fixture, used to derive\n        this one\n\n    Returns\n    -------\n    df_cat: DataFrame\n    \"\"\"\n    df_cat = df.copy()[:4]  # leave out some groups\n    df_cat['A'] = df_cat['A'].astype('category')\n    df_cat['B'] = df_cat['B'].astype('category')\n    df_cat['C'] = Series([1, 2, 3, 4])\n    df_cat = df_cat.drop(['D'], axis=1)\n    return df_cat\n\n\n@pytest.mark.parametrize('operation, kwargs', [\n    ('agg', dict(dtype='category')),\n    ('apply', dict())])\ndef test_seriesgroupby_observed_true(df_cat, operation, kwargs):\n    # GH 24880\n    index = MultiIndex.from_frame(\n        DataFrame({'A': ['foo', 'foo', 'bar', 'bar'],\n                   'B': ['one', 'two', 'one', 'three']\n                   }, **kwargs))\n    expected = Series(data=[1, 3, 2, 4], index=index, name='C')\n    grouped = df_cat.groupby(['A', 'B'], observed=True)['C']\n    result = getattr(grouped, operation)(sum)\n    assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize('operation', ['agg', 'apply'])\n@pytest.mark.parametrize('observed', [False, None])\ndef test_seriesgroupby_observed_false_or_none(df_cat, observed, operation):\n    # GH 24880\n    index, _ = MultiIndex.from_product(\n        [CategoricalIndex(['bar', 'foo'], ordered=False),\n         CategoricalIndex(['one', 'three', 'two'], ordered=False)],\n        names=['A', 'B']).sortlevel()\n\n    expected = Series(data=[2, 4, np.nan, 1, np.nan, 3],\n                      index=index, name='C')\n    grouped = df_cat.groupby(['A', 'B'], observed=observed)['C']\n    result = getattr(grouped, operation)(sum)\n    assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"observed, index, data\", [\n    (True, MultiIndex.from_tuples(\n        [('foo', 'one', 'min'), ('foo', 'one', 'max'),\n         ('foo', 'two', 'min'), ('foo', 'two', 'max'),\n         ('bar', 'one', 'min'), ('bar', 'one', 'max'),\n         ('bar', 'three', 'min'), ('bar', 'three', 'max')],\n        names=['A', 'B', None]), [1, 1, 3, 3, 2, 2, 4, 4]),\n    (False, MultiIndex.from_product(\n        [CategoricalIndex(['bar', 'foo'], ordered=False),\n         CategoricalIndex(['one', 'three', 'two'], ordered=False),\n         Index(['min', 'max'])],\n        names=['A', 'B', None]),\n     [2, 2, 4, 4, np.nan, np.nan, 1, 1, np.nan, np.nan, 3, 3]),\n    (None, MultiIndex.from_product(\n        [CategoricalIndex(['bar', 'foo'], ordered=False),\n         CategoricalIndex(['one', 'three', 'two'], ordered=False),\n         Index(['min', 'max'])],\n        names=['A', 'B', None]),\n     [2, 2, 4, 4, np.nan, np.nan, 1, 1, np.nan, np.nan, 3, 3])])\ndef test_seriesgroupby_observed_apply_dict(df_cat, observed, index, data):\n    # GH 24880\n    expected = Series(data=data, index=index, name='C')\n    result = df_cat.groupby(['A', 'B'], observed=observed)['C'].apply(\n        lambda x: OrderedDict([('min', x.min()), ('max', x.max())]))\n    assert_series_equal(result, expected)\n"
    },
    {
      "filename": "pandas/tests/groupby/test_function.py",
      "content": "import builtins\nfrom io import StringIO\nfrom itertools import product\nfrom string import ascii_lowercase\n\nimport numpy as np\nimport pytest\n\nfrom pandas.errors import UnsupportedFunctionCall\n\nimport pandas as pd\nfrom pandas import (\n    DataFrame, Index, MultiIndex, Series, Timestamp, date_range, isna)\nimport pandas.core.nanops as nanops\nfrom pandas.util import _test_decorators as td, testing as tm\n\n\n@pytest.mark.parametrize(\"agg_func\", ['any', 'all'])\n@pytest.mark.parametrize(\"skipna\", [True, False])\n@pytest.mark.parametrize(\"vals\", [\n    ['foo', 'bar', 'baz'], ['foo', '', ''], ['', '', ''],\n    [1, 2, 3], [1, 0, 0], [0, 0, 0],\n    [1., 2., 3.], [1., 0., 0.], [0., 0., 0.],\n    [True, True, True], [True, False, False], [False, False, False],\n    [np.nan, np.nan, np.nan]\n])\ndef test_groupby_bool_aggs(agg_func, skipna, vals):\n    df = DataFrame({'key': ['a'] * 3 + ['b'] * 3, 'val': vals * 2})\n\n    # Figure out expectation using Python builtin\n    exp = getattr(builtins, agg_func)(vals)\n\n    # edge case for missing data with skipna and 'any'\n    if skipna and all(isna(vals)) and agg_func == 'any':\n        exp = False\n\n    exp_df = DataFrame([exp] * 2, columns=['val'], index=Index(\n        ['a', 'b'], name='key'))\n    result = getattr(df.groupby('key'), agg_func)(skipna=skipna)\n    tm.assert_frame_equal(result, exp_df)\n\n\ndef test_max_min_non_numeric():\n    # #2700\n    aa = DataFrame({'nn': [11, 11, 22, 22],\n                    'ii': [1, 2, 3, 4],\n                    'ss': 4 * ['mama']})\n\n    result = aa.groupby('nn').max()\n    assert 'ss' in result\n\n    result = aa.groupby('nn').max(numeric_only=False)\n    assert 'ss' in result\n\n    result = aa.groupby('nn').min()\n    assert 'ss' in result\n\n    result = aa.groupby('nn').min(numeric_only=False)\n    assert 'ss' in result\n\n\ndef test_intercept_builtin_sum():\n    s = Series([1., 2., np.nan, 3.])\n    grouped = s.groupby([0, 1, 2, 2])\n\n    result = grouped.agg(builtins.sum)\n    result2 = grouped.apply(builtins.sum)\n    expected = grouped.sum()\n    tm.assert_series_equal(result, expected)\n    tm.assert_series_equal(result2, expected)\n\n\n# @pytest.mark.parametrize(\"f\", [max, min, sum])\n# def test_builtins_apply(f):\n\n@pytest.mark.parametrize(\"f\", [max, min, sum])\n@pytest.mark.parametrize('keys', [\n    \"jim\",  # Single key\n    [\"jim\", \"joe\"]  # Multi-key\n])\ndef test_builtins_apply(keys, f):\n    # see gh-8155\n    df = pd.DataFrame(np.random.randint(1, 50, (1000, 2)),\n                      columns=[\"jim\", \"joe\"])\n    df[\"jolie\"] = np.random.randn(1000)\n\n    fname = f.__name__\n    result = df.groupby(keys).apply(f)\n    ngroups = len(df.drop_duplicates(subset=keys))\n\n    assert_msg = (\"invalid frame shape: {} \"\n                  \"(expected ({}, 3))\".format(result.shape, ngroups))\n    assert result.shape == (ngroups, 3), assert_msg\n\n    tm.assert_frame_equal(result,  # numpy's equivalent function\n                          df.groupby(keys).apply(getattr(np, fname)))\n\n    if f != sum:\n        expected = df.groupby(keys).agg(fname).reset_index()\n        expected.set_index(keys, inplace=True, drop=False)\n        tm.assert_frame_equal(result, expected, check_dtype=False)\n\n    tm.assert_series_equal(getattr(result, fname)(),\n                           getattr(df, fname)())\n\n\ndef test_arg_passthru():\n    # make sure that we are passing thru kwargs\n    # to our agg functions\n\n    # GH3668\n    # GH5724\n    df = pd.DataFrame(\n        {'group': [1, 1, 2],\n         'int': [1, 2, 3],\n         'float': [4., 5., 6.],\n         'string': list('abc'),\n         'category_string': pd.Series(list('abc')).astype('category'),\n         'category_int': [7, 8, 9],\n         'datetime': pd.date_range('20130101', periods=3),\n         'datetimetz': pd.date_range('20130101',\n                                     periods=3,\n                                     tz='US/Eastern'),\n         'timedelta': pd.timedelta_range('1 s', periods=3, freq='s')},\n        columns=['group', 'int', 'float', 'string',\n                 'category_string', 'category_int',\n                 'datetime', 'datetimetz',\n                 'timedelta'])\n\n    expected_columns_numeric = Index(['int', 'float', 'category_int'])\n\n    # mean / median\n    expected = pd.DataFrame(\n        {'category_int': [7.5, 9],\n         'float': [4.5, 6.],\n         'timedelta': [pd.Timedelta('1.5s'),\n                       pd.Timedelta('3s')],\n         'int': [1.5, 3],\n         'datetime': [pd.Timestamp('2013-01-01 12:00:00'),\n                      pd.Timestamp('2013-01-03 00:00:00')],\n         'datetimetz': [\n             pd.Timestamp('2013-01-01 12:00:00', tz='US/Eastern'),\n             pd.Timestamp('2013-01-03 00:00:00', tz='US/Eastern')]},\n        index=Index([1, 2], name='group'),\n        columns=['int', 'float', 'category_int',\n                 'datetime', 'datetimetz', 'timedelta'])\n\n    for attr in ['mean', 'median']:\n        f = getattr(df.groupby('group'), attr)\n        result = f()\n        tm.assert_index_equal(result.columns, expected_columns_numeric)\n\n        result = f(numeric_only=False)\n        tm.assert_frame_equal(result.reindex_like(expected), expected)\n\n    # TODO: min, max *should* handle\n    # categorical (ordered) dtype\n    expected_columns = Index(['int', 'float', 'string',\n                              'category_int',\n                              'datetime', 'datetimetz',\n                              'timedelta'])\n    for attr in ['min', 'max']:\n        f = getattr(df.groupby('group'), attr)\n        result = f()\n        tm.assert_index_equal(result.columns, expected_columns)\n\n        result = f(numeric_only=False)\n        tm.assert_index_equal(result.columns, expected_columns)\n\n    expected_columns = Index(['int', 'float', 'string',\n                              'category_string', 'category_int',\n                              'datetime', 'datetimetz',\n                              'timedelta'])\n    for attr in ['first', 'last']:\n        f = getattr(df.groupby('group'), attr)\n        result = f()\n        tm.assert_index_equal(result.columns, expected_columns)\n\n        result = f(numeric_only=False)\n        tm.assert_index_equal(result.columns, expected_columns)\n\n    expected_columns = Index(['int', 'float', 'string',\n                              'category_int', 'timedelta'])\n    for attr in ['sum']:\n        f = getattr(df.groupby('group'), attr)\n        result = f()\n        tm.assert_index_equal(result.columns, expected_columns_numeric)\n\n        result = f(numeric_only=False)\n        tm.assert_index_equal(result.columns, expected_columns)\n\n    expected_columns = Index(['int', 'float', 'category_int'])\n    for attr in ['prod', 'cumprod']:\n        f = getattr(df.groupby('group'), attr)\n        result = f()\n        tm.assert_index_equal(result.columns, expected_columns_numeric)\n\n        result = f(numeric_only=False)\n        tm.assert_index_equal(result.columns, expected_columns)\n\n    # like min, max, but don't include strings\n    expected_columns = Index(['int', 'float',\n                              'category_int',\n                              'datetime', 'datetimetz',\n                              'timedelta'])\n    for attr in ['cummin', 'cummax']:\n        f = getattr(df.groupby('group'), attr)\n        result = f()\n        # GH 15561: numeric_only=False set by default like min/max\n        tm.assert_index_equal(result.columns, expected_columns)\n\n        result = f(numeric_only=False)\n        tm.assert_index_equal(result.columns, expected_columns)\n\n    expected_columns = Index(['int', 'float', 'category_int',\n                              'timedelta'])\n    for attr in ['cumsum']:\n        f = getattr(df.groupby('group'), attr)\n        result = f()\n        tm.assert_index_equal(result.columns, expected_columns_numeric)\n\n        result = f(numeric_only=False)\n        tm.assert_index_equal(result.columns, expected_columns)\n\n\ndef test_non_cython_api():\n\n    # GH5610\n    # non-cython calls should not include the grouper\n\n    df = DataFrame(\n        [[1, 2, 'foo'],\n         [1, np.nan, 'bar'],\n         [3, np.nan, 'baz']],\n        columns=['A', 'B', 'C'])\n    g = df.groupby('A')\n    gni = df.groupby('A', as_index=False)\n\n    # mad\n    expected = DataFrame([[0], [np.nan]], columns=['B'], index=[1, 3])\n    expected.index.name = 'A'\n    result = g.mad()\n    tm.assert_frame_equal(result, expected)\n\n    expected = DataFrame([[0., 0.], [0, np.nan]], columns=['A', 'B'],\n                         index=[0, 1])\n    result = gni.mad()\n    tm.assert_frame_equal(result, expected)\n\n    # describe\n    expected_index = pd.Index([1, 3], name='A')\n    expected_col = pd.MultiIndex(levels=[['B'],\n                                         ['count', 'mean', 'std', 'min',\n                                          '25%', '50%', '75%', 'max']],\n                                 codes=[[0] * 8, list(range(8))])\n    expected = pd.DataFrame([[1.0, 2.0, np.nan, 2.0, 2.0, 2.0, 2.0, 2.0],\n                             [0.0, np.nan, np.nan, np.nan, np.nan, np.nan,\n                              np.nan, np.nan]],\n                            index=expected_index,\n                            columns=expected_col)\n    result = g.describe()\n    tm.assert_frame_equal(result, expected)\n\n    expected = pd.concat([df[df.A == 1].describe().unstack().to_frame().T,\n                          df[df.A == 3].describe().unstack().to_frame().T])\n    expected.index = pd.Index([0, 1])\n    result = gni.describe()\n    tm.assert_frame_equal(result, expected)\n\n    # any\n    expected = DataFrame([[True, True], [False, True]], columns=['B', 'C'],\n                         index=[1, 3])\n    expected.index.name = 'A'\n    result = g.any()\n    tm.assert_frame_equal(result, expected)\n\n    # idxmax\n    expected = DataFrame([[0.0], [np.nan]], columns=['B'], index=[1, 3])\n    expected.index.name = 'A'\n    result = g.idxmax()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_cython_api2():\n\n    # this takes the fast apply path\n\n    # cumsum (GH5614)\n    df = DataFrame(\n        [[1, 2, np.nan], [1, np.nan, 9], [3, 4, 9]\n         ], columns=['A', 'B', 'C'])\n    expected = DataFrame(\n        [[2, np.nan], [np.nan, 9], [4, 9]], columns=['B', 'C'])\n    result = df.groupby('A').cumsum()\n    tm.assert_frame_equal(result, expected)\n\n    # GH 5755 - cumsum is a transformer and should ignore as_index\n    result = df.groupby('A', as_index=False).cumsum()\n    tm.assert_frame_equal(result, expected)\n\n    # GH 13994\n    result = df.groupby('A').cumsum(axis=1)\n    expected = df.cumsum(axis=1)\n    tm.assert_frame_equal(result, expected)\n    result = df.groupby('A').cumprod(axis=1)\n    expected = df.cumprod(axis=1)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_cython_median():\n    df = DataFrame(np.random.randn(1000))\n    df.values[::2] = np.nan\n\n    labels = np.random.randint(0, 50, size=1000).astype(float)\n    labels[::17] = np.nan\n\n    result = df.groupby(labels).median()\n    exp = df.groupby(labels).agg(nanops.nanmedian)\n    tm.assert_frame_equal(result, exp)\n\n    df = DataFrame(np.random.randn(1000, 5))\n    rs = df.groupby(labels).agg(np.median)\n    xp = df.groupby(labels).median()\n    tm.assert_frame_equal(rs, xp)\n\n\ndef test_median_empty_bins(observed):\n    df = pd.DataFrame(np.random.randint(0, 44, 500))\n\n    grps = range(0, 55, 5)\n    bins = pd.cut(df[0], grps)\n\n    result = df.groupby(bins, observed=observed).median()\n    expected = df.groupby(bins, observed=observed).agg(lambda x: x.median())\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"dtype\", [\n    'int8', 'int16', 'int32', 'int64', 'float32', 'float64'])\n@pytest.mark.parametrize(\"method,data\", [\n    ('first', {'df': [{'a': 1, 'b': 1}, {'a': 2, 'b': 3}]}),\n    ('last', {'df': [{'a': 1, 'b': 2}, {'a': 2, 'b': 4}]}),\n    ('min', {'df': [{'a': 1, 'b': 1}, {'a': 2, 'b': 3}]}),\n    ('max', {'df': [{'a': 1, 'b': 2}, {'a': 2, 'b': 4}]}),\n    ('nth', {'df': [{'a': 1, 'b': 2}, {'a': 2, 'b': 4}],\n             'args': [1]}),\n    ('count', {'df': [{'a': 1, 'b': 2}, {'a': 2, 'b': 2}],\n               'out_type': 'int64'})\n])\ndef test_groupby_non_arithmetic_agg_types(dtype, method, data):\n    # GH9311, GH6620\n    df = pd.DataFrame(\n        [{'a': 1, 'b': 1},\n         {'a': 1, 'b': 2},\n         {'a': 2, 'b': 3},\n         {'a': 2, 'b': 4}])\n\n    df['b'] = df.b.astype(dtype)\n\n    if 'args' not in data:\n        data['args'] = []\n\n    if 'out_type' in data:\n        out_type = data['out_type']\n    else:\n        out_type = dtype\n\n    exp = data['df']\n    df_out = pd.DataFrame(exp)\n\n    df_out['b'] = df_out.b.astype(out_type)\n    df_out.set_index('a', inplace=True)\n\n    grpd = df.groupby('a')\n    t = getattr(grpd, method)(*data['args'])\n    tm.assert_frame_equal(t, df_out)\n\n\n@pytest.mark.parametrize(\"i\", [\n    (Timestamp(\"2011-01-15 12:50:28.502376\"),\n     Timestamp(\"2011-01-20 12:50:28.593448\")),\n    (24650000000000001, 24650000000000002)\n])\ndef test_groupby_non_arithmetic_agg_int_like_precision(i):\n    # see gh-6620, gh-9311\n    df = pd.DataFrame([{\"a\": 1, \"b\": i[0]}, {\"a\": 1, \"b\": i[1]}])\n\n    grp_exp = {\"first\": {\"expected\": i[0]},\n               \"last\": {\"expected\": i[1]},\n               \"min\": {\"expected\": i[0]},\n               \"max\": {\"expected\": i[1]},\n               \"nth\": {\"expected\": i[1],\n                       \"args\": [1]},\n               \"count\": {\"expected\": 2}}\n\n    for method, data in grp_exp.items():\n        if \"args\" not in data:\n            data[\"args\"] = []\n\n        grouped = df.groupby(\"a\")\n        res = getattr(grouped, method)(*data[\"args\"])\n\n        assert res.iloc[0].b == data[\"expected\"]\n\n\n@pytest.mark.parametrize(\"func, values\", [\n    (\"idxmin\", {'c_int': [0, 2], 'c_float': [1, 3], 'c_date': [1, 2]}),\n    (\"idxmax\", {'c_int': [1, 3], 'c_float': [0, 2], 'c_date': [0, 3]})\n])\ndef test_idxmin_idxmax_returns_int_types(func, values):\n    # GH 25444\n    df = pd.DataFrame({'name': ['A', 'A', 'B', 'B'],\n                       'c_int': [1, 2, 3, 4],\n                       'c_float': [4.02, 3.03, 2.04, 1.05],\n                       'c_date': ['2019', '2018', '2016', '2017']})\n    df['c_date'] = pd.to_datetime(df['c_date'])\n\n    result = getattr(df.groupby('name'), func)()\n\n    expected = pd.DataFrame(values, index=Index(['A', 'B'], name=\"name\"))\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_fill_consistency():\n\n    # GH9221\n    # pass thru keyword arguments to the generated wrapper\n    # are set if the passed kw is None (only)\n    df = DataFrame(index=pd.MultiIndex.from_product(\n        [['value1', 'value2'], date_range('2014-01-01', '2014-01-06')]),\n        columns=Index(\n        ['1', '2'], name='id'))\n    df['1'] = [np.nan, 1, np.nan, np.nan, 11, np.nan, np.nan, 2, np.nan,\n               np.nan, 22, np.nan]\n    df['2'] = [np.nan, 3, np.nan, np.nan, 33, np.nan, np.nan, 4, np.nan,\n               np.nan, 44, np.nan]\n\n    expected = df.groupby(level=0, axis=0).fillna(method='ffill')\n    result = df.T.groupby(level=0, axis=1).fillna(method='ffill').T\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_cumprod():\n    # GH 4095\n    df = pd.DataFrame({'key': ['b'] * 10, 'value': 2})\n\n    actual = df.groupby('key')['value'].cumprod()\n    expected = df.groupby('key')['value'].apply(lambda x: x.cumprod())\n    expected.name = 'value'\n    tm.assert_series_equal(actual, expected)\n\n    df = pd.DataFrame({'key': ['b'] * 100, 'value': 2})\n    actual = df.groupby('key')['value'].cumprod()\n    # if overflows, groupby product casts to float\n    # while numpy passes back invalid values\n    df['value'] = df['value'].astype(float)\n    expected = df.groupby('key')['value'].apply(lambda x: x.cumprod())\n    expected.name = 'value'\n    tm.assert_series_equal(actual, expected)\n\n\ndef scipy_sem(*args, **kwargs):\n    from scipy.stats import sem\n    return sem(*args, ddof=1, **kwargs)\n\n\n@pytest.mark.parametrize(\n    'op,targop',\n    [('mean', np.mean),\n     ('median', np.median),\n     ('std', np.std),\n     ('var', np.var),\n     ('sum', np.sum),\n     ('prod', np.prod),\n     ('min', np.min),\n     ('max', np.max),\n     ('first', lambda x: x.iloc[0]),\n     ('last', lambda x: x.iloc[-1]),\n     ('count', np.size),\n     pytest.param(\n         'sem', scipy_sem, marks=td.skip_if_no_scipy)])\ndef test_ops_general(op, targop):\n    df = DataFrame(np.random.randn(1000))\n    labels = np.random.randint(0, 50, size=1000).astype(float)\n\n    result = getattr(df.groupby(labels), op)().astype(float)\n    expected = df.groupby(labels).agg(targop)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_max_nan_bug():\n    raw = \"\"\",Date,app,File\n-04-23,2013-04-23 00:00:00,,log080001.log\n-05-06,2013-05-06 00:00:00,,log.log\n-05-07,2013-05-07 00:00:00,OE,xlsx\"\"\"\n\n    df = pd.read_csv(StringIO(raw), parse_dates=[0])\n    gb = df.groupby('Date')\n    r = gb[['File']].max()\n    e = gb['File'].max().to_frame()\n    tm.assert_frame_equal(r, e)\n    assert not r['File'].isna().any()\n\n\ndef test_nlargest():\n    a = Series([1, 3, 5, 7, 2, 9, 0, 4, 6, 10])\n    b = Series(list('a' * 5 + 'b' * 5))\n    gb = a.groupby(b)\n    r = gb.nlargest(3)\n    e = Series([\n        7, 5, 3, 10, 9, 6\n    ], index=MultiIndex.from_arrays([list('aaabbb'), [3, 2, 1, 9, 5, 8]]))\n    tm.assert_series_equal(r, e)\n\n    a = Series([1, 1, 3, 2, 0, 3, 3, 2, 1, 0])\n    gb = a.groupby(b)\n    e = Series([\n        3, 2, 1, 3, 3, 2\n    ], index=MultiIndex.from_arrays([list('aaabbb'), [2, 3, 1, 6, 5, 7]]))\n    tm.assert_series_equal(gb.nlargest(3, keep='last'), e)\n\n\ndef test_nsmallest():\n    a = Series([1, 3, 5, 7, 2, 9, 0, 4, 6, 10])\n    b = Series(list('a' * 5 + 'b' * 5))\n    gb = a.groupby(b)\n    r = gb.nsmallest(3)\n    e = Series([\n        1, 2, 3, 0, 4, 6\n    ], index=MultiIndex.from_arrays([list('aaabbb'), [0, 4, 1, 6, 7, 8]]))\n    tm.assert_series_equal(r, e)\n\n    a = Series([1, 1, 3, 2, 0, 3, 3, 2, 1, 0])\n    gb = a.groupby(b)\n    e = Series([\n        0, 1, 1, 0, 1, 2\n    ], index=MultiIndex.from_arrays([list('aaabbb'), [4, 1, 0, 9, 8, 7]]))\n    tm.assert_series_equal(gb.nsmallest(3, keep='last'), e)\n\n\n@pytest.mark.parametrize(\"func\", [\n    'mean', 'var', 'std', 'cumprod', 'cumsum'\n])\ndef test_numpy_compat(func):\n    # see gh-12811\n    df = pd.DataFrame({'A': [1, 2, 1], 'B': [1, 2, 3]})\n    g = df.groupby('A')\n\n    msg = \"numpy operations are not valid with groupby\"\n\n    with pytest.raises(UnsupportedFunctionCall, match=msg):\n        getattr(g, func)(1, 2, 3)\n    with pytest.raises(UnsupportedFunctionCall, match=msg):\n        getattr(g, func)(foo=1)\n\n\ndef test_cummin_cummax():\n    # GH 15048\n    num_types = [np.int32, np.int64, np.float32, np.float64]\n    num_mins = [np.iinfo(np.int32).min, np.iinfo(np.int64).min,\n                np.finfo(np.float32).min, np.finfo(np.float64).min]\n    num_max = [np.iinfo(np.int32).max, np.iinfo(np.int64).max,\n               np.finfo(np.float32).max, np.finfo(np.float64).max]\n    base_df = pd.DataFrame({'A': [1, 1, 1, 1, 2, 2, 2, 2],\n                            'B': [3, 4, 3, 2, 2, 3, 2, 1]})\n    expected_mins = [3, 3, 3, 2, 2, 2, 2, 1]\n    expected_maxs = [3, 4, 4, 4, 2, 3, 3, 3]\n\n    for dtype, min_val, max_val in zip(num_types, num_mins, num_max):\n        df = base_df.astype(dtype)\n\n        # cummin\n        expected = pd.DataFrame({'B': expected_mins}).astype(dtype)\n        result = df.groupby('A').cummin()\n        tm.assert_frame_equal(result, expected)\n        result = df.groupby('A').B.apply(lambda x: x.cummin()).to_frame()\n        tm.assert_frame_equal(result, expected)\n\n        # Test cummin w/ min value for dtype\n        df.loc[[2, 6], 'B'] = min_val\n        expected.loc[[2, 3, 6, 7], 'B'] = min_val\n        result = df.groupby('A').cummin()\n        tm.assert_frame_equal(result, expected)\n        expected = df.groupby('A').B.apply(lambda x: x.cummin()).to_frame()\n        tm.assert_frame_equal(result, expected)\n\n        # cummax\n        expected = pd.DataFrame({'B': expected_maxs}).astype(dtype)\n        result = df.groupby('A').cummax()\n        tm.assert_frame_equal(result, expected)\n        result = df.groupby('A').B.apply(lambda x: x.cummax()).to_frame()\n        tm.assert_frame_equal(result, expected)\n\n        # Test cummax w/ max value for dtype\n        df.loc[[2, 6], 'B'] = max_val\n        expected.loc[[2, 3, 6, 7], 'B'] = max_val\n        result = df.groupby('A').cummax()\n        tm.assert_frame_equal(result, expected)\n        expected = df.groupby('A').B.apply(lambda x: x.cummax()).to_frame()\n        tm.assert_frame_equal(result, expected)\n\n    # Test nan in some values\n    base_df.loc[[0, 2, 4, 6], 'B'] = np.nan\n    expected = pd.DataFrame({'B': [np.nan, 4, np.nan, 2,\n                                   np.nan, 3, np.nan, 1]})\n    result = base_df.groupby('A').cummin()\n    tm.assert_frame_equal(result, expected)\n    expected = (base_df.groupby('A')\n                       .B\n                       .apply(lambda x: x.cummin())\n                       .to_frame())\n    tm.assert_frame_equal(result, expected)\n\n    expected = pd.DataFrame({'B': [np.nan, 4, np.nan, 4,\n                                   np.nan, 3, np.nan, 3]})\n    result = base_df.groupby('A').cummax()\n    tm.assert_frame_equal(result, expected)\n    expected = (base_df.groupby('A')\n                       .B\n                       .apply(lambda x: x.cummax())\n                       .to_frame())\n    tm.assert_frame_equal(result, expected)\n\n    # Test nan in entire column\n    base_df['B'] = np.nan\n    expected = pd.DataFrame({'B': [np.nan] * 8})\n    result = base_df.groupby('A').cummin()\n    tm.assert_frame_equal(expected, result)\n    result = base_df.groupby('A').B.apply(lambda x: x.cummin()).to_frame()\n    tm.assert_frame_equal(expected, result)\n    result = base_df.groupby('A').cummax()\n    tm.assert_frame_equal(expected, result)\n    result = base_df.groupby('A').B.apply(lambda x: x.cummax()).to_frame()\n    tm.assert_frame_equal(expected, result)\n\n    # GH 15561\n    df = pd.DataFrame(dict(a=[1], b=pd.to_datetime(['2001'])))\n    expected = pd.Series(pd.to_datetime('2001'), index=[0], name='b')\n    for method in ['cummax', 'cummin']:\n        result = getattr(df.groupby('a')['b'], method)()\n        tm.assert_series_equal(expected, result)\n\n    # GH 15635\n    df = pd.DataFrame(dict(a=[1, 2, 1], b=[2, 1, 1]))\n    result = df.groupby('a').b.cummax()\n    expected = pd.Series([2, 1, 2], name='b')\n    tm.assert_series_equal(result, expected)\n\n    df = pd.DataFrame(dict(a=[1, 2, 1], b=[1, 2, 2]))\n    result = df.groupby('a').b.cummin()\n    expected = pd.Series([1, 2, 1], name='b')\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize('in_vals, out_vals', [\n\n    # Basics: strictly increasing (T), strictly decreasing (F),\n    # abs val increasing (F), non-strictly increasing (T)\n    ([1, 2, 5, 3, 2, 0, 4, 5, -6, 1, 1],\n     [True, False, False, True]),\n\n    # Test with inf vals\n    ([1, 2.1, np.inf, 3, 2, np.inf, -np.inf, 5, 11, 1, -np.inf],\n     [True, False, True, False]),\n\n    # Test with nan vals; should always be False\n    ([1, 2, np.nan, 3, 2, np.nan, np.nan, 5, -np.inf, 1, np.nan],\n     [False, False, False, False]),\n])\ndef test_is_monotonic_increasing(in_vals, out_vals):\n    # GH 17015\n    source_dict = {\n        'A': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11'],\n        'B': ['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c', 'd', 'd'],\n        'C': in_vals}\n    df = pd.DataFrame(source_dict)\n    result = df.groupby('B').C.is_monotonic_increasing\n    index = Index(list('abcd'), name='B')\n    expected = pd.Series(index=index, data=out_vals, name='C')\n    tm.assert_series_equal(result, expected)\n\n    # Also check result equal to manually taking x.is_monotonic_increasing.\n    expected = (\n        df.groupby(['B']).C.apply(lambda x: x.is_monotonic_increasing))\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize('in_vals, out_vals', [\n    # Basics: strictly decreasing (T), strictly increasing (F),\n    # abs val decreasing (F), non-strictly increasing (T)\n    ([10, 9, 7, 3, 4, 5, -3, 2, 0, 1, 1],\n     [True, False, False, True]),\n\n    # Test with inf vals\n    ([np.inf, 1, -np.inf, np.inf, 2, -3, -np.inf, 5, -3, -np.inf, -np.inf],\n     [True, True, False, True]),\n\n    # Test with nan vals; should always be False\n    ([1, 2, np.nan, 3, 2, np.nan, np.nan, 5, -np.inf, 1, np.nan],\n     [False, False, False, False]),\n])\ndef test_is_monotonic_decreasing(in_vals, out_vals):\n    # GH 17015\n    source_dict = {\n        'A': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11'],\n        'B': ['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c', 'd', 'd'],\n        'C': in_vals}\n\n    df = pd.DataFrame(source_dict)\n    result = df.groupby('B').C.is_monotonic_decreasing\n    index = Index(list('abcd'), name='B')\n    expected = pd.Series(index=index, data=out_vals, name='C')\n    tm.assert_series_equal(result, expected)\n\n\n# describe\n# --------------------------------\n\ndef test_apply_describe_bug(mframe):\n    grouped = mframe.groupby(level='first')\n    grouped.describe()  # it works!\n\n\ndef test_series_describe_multikey():\n    ts = tm.makeTimeSeries()\n    grouped = ts.groupby([lambda x: x.year, lambda x: x.month])\n    result = grouped.describe()\n    tm.assert_series_equal(result['mean'], grouped.mean(),\n                           check_names=False)\n    tm.assert_series_equal(result['std'], grouped.std(), check_names=False)\n    tm.assert_series_equal(result['min'], grouped.min(), check_names=False)\n\n\ndef test_series_describe_single():\n    ts = tm.makeTimeSeries()\n    grouped = ts.groupby(lambda x: x.month)\n    result = grouped.apply(lambda x: x.describe())\n    expected = grouped.describe().stack()\n    tm.assert_series_equal(result, expected)\n\n\ndef test_series_index_name(df):\n    grouped = df.loc[:, ['C']].groupby(df['A'])\n    result = grouped.agg(lambda x: x.mean())\n    assert result.index.name == 'A'\n\n\ndef test_frame_describe_multikey(tsframe):\n    grouped = tsframe.groupby([lambda x: x.year, lambda x: x.month])\n    result = grouped.describe()\n    desc_groups = []\n    for col in tsframe:\n        group = grouped[col].describe()\n        # GH 17464 - Remove duplicate MultiIndex levels\n        group_col = pd.MultiIndex(\n            levels=[[col], group.columns],\n            codes=[[0] * len(group.columns), range(len(group.columns))])\n        group = pd.DataFrame(group.values,\n                             columns=group_col,\n                             index=group.index)\n        desc_groups.append(group)\n    expected = pd.concat(desc_groups, axis=1)\n    tm.assert_frame_equal(result, expected)\n\n    groupedT = tsframe.groupby({'A': 0, 'B': 0,\n                                'C': 1, 'D': 1}, axis=1)\n    result = groupedT.describe()\n    expected = tsframe.describe().T\n    expected.index = pd.MultiIndex(\n        levels=[[0, 1], expected.index],\n        codes=[[0, 0, 1, 1], range(len(expected.index))])\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_frame_describe_tupleindex():\n\n    # GH 14848 - regression from 0.19.0 to 0.19.1\n    df1 = DataFrame({'x': [1, 2, 3, 4, 5] * 3,\n                     'y': [10, 20, 30, 40, 50] * 3,\n                     'z': [100, 200, 300, 400, 500] * 3})\n    df1['k'] = [(0, 0, 1), (0, 1, 0), (1, 0, 0)] * 5\n    df2 = df1.rename(columns={'k': 'key'})\n    msg = \"Names should be list-like for a MultiIndex\"\n    with pytest.raises(ValueError, match=msg):\n        df1.groupby('k').describe()\n    with pytest.raises(ValueError, match=msg):\n        df2.groupby('key').describe()\n\n\ndef test_frame_describe_unstacked_format():\n    # GH 4792\n    prices = {pd.Timestamp('2011-01-06 10:59:05', tz=None): 24990,\n              pd.Timestamp('2011-01-06 12:43:33', tz=None): 25499,\n              pd.Timestamp('2011-01-06 12:54:09', tz=None): 25499}\n    volumes = {pd.Timestamp('2011-01-06 10:59:05', tz=None): 1500000000,\n               pd.Timestamp('2011-01-06 12:43:33', tz=None): 5000000000,\n               pd.Timestamp('2011-01-06 12:54:09', tz=None): 100000000}\n    df = pd.DataFrame({'PRICE': prices,\n                       'VOLUME': volumes})\n    result = df.groupby('PRICE').VOLUME.describe()\n    data = [df[df.PRICE == 24990].VOLUME.describe().values.tolist(),\n            df[df.PRICE == 25499].VOLUME.describe().values.tolist()]\n    expected = pd.DataFrame(data,\n                            index=pd.Index([24990, 25499], name='PRICE'),\n                            columns=['count', 'mean', 'std', 'min',\n                                     '25%', '50%', '75%', 'max'])\n    tm.assert_frame_equal(result, expected)\n\n\n# nunique\n# --------------------------------\n\n@pytest.mark.parametrize('n', 10 ** np.arange(2, 6))\n@pytest.mark.parametrize('m', [10, 100, 1000])\n@pytest.mark.parametrize('sort', [False, True])\n@pytest.mark.parametrize('dropna', [False, True])\ndef test_series_groupby_nunique(n, m, sort, dropna):\n\n    def check_nunique(df, keys, as_index=True):\n        gr = df.groupby(keys, as_index=as_index, sort=sort)\n        left = gr['julie'].nunique(dropna=dropna)\n\n        gr = df.groupby(keys, as_index=as_index, sort=sort)\n        right = gr['julie'].apply(Series.nunique, dropna=dropna)\n        if not as_index:\n            right = right.reset_index(drop=True)\n\n        tm.assert_series_equal(left, right, check_names=False)\n\n    days = date_range('2015-08-23', periods=10)\n\n    frame = DataFrame({'jim': np.random.choice(list(ascii_lowercase), n),\n                       'joe': np.random.choice(days, n),\n                       'julie': np.random.randint(0, m, n)})\n\n    check_nunique(frame, ['jim'])\n    check_nunique(frame, ['jim', 'joe'])\n\n    frame.loc[1::17, 'jim'] = None\n    frame.loc[3::37, 'joe'] = None\n    frame.loc[7::19, 'julie'] = None\n    frame.loc[8::19, 'julie'] = None\n    frame.loc[9::19, 'julie'] = None\n\n    check_nunique(frame, ['jim'])\n    check_nunique(frame, ['jim', 'joe'])\n    check_nunique(frame, ['jim'], as_index=False)\n    check_nunique(frame, ['jim', 'joe'], as_index=False)\n\n\ndef test_nunique():\n    df = DataFrame({\n        'A': list('abbacc'),\n        'B': list('abxacc'),\n        'C': list('abbacx'),\n    })\n\n    expected = DataFrame({'A': [1] * 3, 'B': [1, 2, 1], 'C': [1, 1, 2]})\n    result = df.groupby('A', as_index=False).nunique()\n    tm.assert_frame_equal(result, expected)\n\n    # as_index\n    expected.index = list('abc')\n    expected.index.name = 'A'\n    result = df.groupby('A').nunique()\n    tm.assert_frame_equal(result, expected)\n\n    # with na\n    result = df.replace({'x': None}).groupby('A').nunique(dropna=False)\n    tm.assert_frame_equal(result, expected)\n\n    # dropna\n    expected = DataFrame({'A': [1] * 3, 'B': [1] * 3, 'C': [1] * 3},\n                         index=list('abc'))\n    expected.index.name = 'A'\n    result = df.replace({'x': None}).groupby('A').nunique()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_nunique_with_object():\n    # GH 11077\n    data = pd.DataFrame(\n        [[100, 1, 'Alice'],\n         [200, 2, 'Bob'],\n         [300, 3, 'Charlie'],\n         [-400, 4, 'Dan'],\n         [500, 5, 'Edith']],\n        columns=['amount', 'id', 'name']\n    )\n\n    result = data.groupby(['id', 'amount'])['name'].nunique()\n    index = MultiIndex.from_arrays([data.id, data.amount])\n    expected = pd.Series([1] * 5, name='name', index=index)\n    tm.assert_series_equal(result, expected)\n\n\ndef test_nunique_with_empty_series():\n    # GH 12553\n    data = pd.Series(name='name')\n    result = data.groupby(level=0).nunique()\n    expected = pd.Series(name='name', dtype='int64')\n    tm.assert_series_equal(result, expected)\n\n\ndef test_nunique_with_timegrouper():\n    # GH 13453\n    test = pd.DataFrame({\n        'time': [Timestamp('2016-06-28 09:35:35'),\n                 Timestamp('2016-06-28 16:09:30'),\n                 Timestamp('2016-06-28 16:46:28')],\n        'data': ['1', '2', '3']}).set_index('time')\n    result = test.groupby(pd.Grouper(freq='h'))['data'].nunique()\n    expected = test.groupby(\n        pd.Grouper(freq='h')\n    )['data'].apply(pd.Series.nunique)\n    tm.assert_series_equal(result, expected)\n\n\ndef test_nunique_preserves_column_level_names():\n    # GH 23222\n    test = pd.DataFrame([1, 2, 2],\n                        columns=pd.Index(['A'], name=\"level_0\"))\n    result = test.groupby([0, 0, 0]).nunique()\n    expected = pd.DataFrame([2], columns=test.columns)\n    tm.assert_frame_equal(result, expected)\n\n\n# count\n# --------------------------------\n\ndef test_groupby_timedelta_cython_count():\n    df = DataFrame({'g': list('ab' * 2),\n                    'delt': np.arange(4).astype('timedelta64[ns]')})\n    expected = Series([\n        2, 2\n    ], index=pd.Index(['a', 'b'], name='g'), name='delt')\n    result = df.groupby('g').delt.count()\n    tm.assert_series_equal(expected, result)\n\n\ndef test_count():\n    n = 1 << 15\n    dr = date_range('2015-08-30', periods=n // 10, freq='T')\n\n    df = DataFrame({\n        '1st': np.random.choice(\n            list(ascii_lowercase), n),\n        '2nd': np.random.randint(0, 5, n),\n        '3rd': np.random.randn(n).round(3),\n        '4th': np.random.randint(-10, 10, n),\n        '5th': np.random.choice(dr, n),\n        '6th': np.random.randn(n).round(3),\n        '7th': np.random.randn(n).round(3),\n        '8th': np.random.choice(dr, n) - np.random.choice(dr, 1),\n        '9th': np.random.choice(\n            list(ascii_lowercase), n)\n    })\n\n    for col in df.columns.drop(['1st', '2nd', '4th']):\n        df.loc[np.random.choice(n, n // 10), col] = np.nan\n\n    df['9th'] = df['9th'].astype('category')\n\n    for key in ['1st', '2nd', ['1st', '2nd']]:\n        left = df.groupby(key).count()\n        right = df.groupby(key).apply(DataFrame.count).drop(key, axis=1)\n        tm.assert_frame_equal(left, right)\n\n\ndef test_count_non_nulls():\n    # GH#5610\n    # count counts non-nulls\n    df = pd.DataFrame([[1, 2, 'foo'],\n                       [1, np.nan, 'bar'],\n                       [3, np.nan, np.nan]],\n                      columns=['A', 'B', 'C'])\n\n    count_as = df.groupby('A').count()\n    count_not_as = df.groupby('A', as_index=False).count()\n\n    expected = DataFrame([[1, 2], [0, 0]], columns=['B', 'C'],\n                         index=[1, 3])\n    expected.index.name = 'A'\n    tm.assert_frame_equal(count_not_as, expected.reset_index())\n    tm.assert_frame_equal(count_as, expected)\n\n    count_B = df.groupby('A')['B'].count()\n    tm.assert_series_equal(count_B, expected['B'])\n\n\ndef test_count_object():\n    df = pd.DataFrame({'a': ['a'] * 3 + ['b'] * 3, 'c': [2] * 3 + [3] * 3})\n    result = df.groupby('c').a.count()\n    expected = pd.Series([\n        3, 3\n    ], index=pd.Index([2, 3], name='c'), name='a')\n    tm.assert_series_equal(result, expected)\n\n    df = pd.DataFrame({'a': ['a', np.nan, np.nan] + ['b'] * 3,\n                       'c': [2] * 3 + [3] * 3})\n    result = df.groupby('c').a.count()\n    expected = pd.Series([\n        1, 3\n    ], index=pd.Index([2, 3], name='c'), name='a')\n    tm.assert_series_equal(result, expected)\n\n\ndef test_count_cross_type():\n    # GH8169\n    vals = np.hstack((np.random.randint(0, 5, (100, 2)), np.random.randint(\n        0, 2, (100, 2))))\n\n    df = pd.DataFrame(vals, columns=['a', 'b', 'c', 'd'])\n    df[df == 2] = np.nan\n    expected = df.groupby(['c', 'd']).count()\n\n    for t in ['float32', 'object']:\n        df['a'] = df['a'].astype(t)\n        df['b'] = df['b'].astype(t)\n        result = df.groupby(['c', 'd']).count()\n        tm.assert_frame_equal(result, expected)\n\n\ndef test_lower_int_prec_count():\n    df = DataFrame({'a': np.array(\n        [0, 1, 2, 100], np.int8),\n        'b': np.array(\n        [1, 2, 3, 6], np.uint32),\n        'c': np.array(\n        [4, 5, 6, 8], np.int16),\n        'grp': list('ab' * 2)})\n    result = df.groupby('grp').count()\n    expected = DataFrame({'a': [2, 2],\n                          'b': [2, 2],\n                          'c': [2, 2]}, index=pd.Index(list('ab'),\n                                                       name='grp'))\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_count_uses_size_on_exception():\n    class RaisingObjectException(Exception):\n        pass\n\n    class RaisingObject:\n\n        def __init__(self, msg='I will raise inside Cython'):\n            super().__init__()\n            self.msg = msg\n\n        def __eq__(self, other):\n            # gets called in Cython to check that raising calls the method\n            raise RaisingObjectException(self.msg)\n\n    df = DataFrame({'a': [RaisingObject() for _ in range(4)],\n                    'grp': list('ab' * 2)})\n    result = df.groupby('grp').count()\n    expected = DataFrame({'a': [2, 2]}, index=pd.Index(\n        list('ab'), name='grp'))\n    tm.assert_frame_equal(result, expected)\n\n\n# size\n# --------------------------------\n\ndef test_size(df):\n    grouped = df.groupby(['A', 'B'])\n    result = grouped.size()\n    for key, group in grouped:\n        assert result[key] == len(group)\n\n    grouped = df.groupby('A')\n    result = grouped.size()\n    for key, group in grouped:\n        assert result[key] == len(group)\n\n    grouped = df.groupby('B')\n    result = grouped.size()\n    for key, group in grouped:\n        assert result[key] == len(group)\n\n    df = DataFrame(np.random.choice(20, (1000, 3)), columns=list('abc'))\n    for sort, key in product((False, True), ('a', 'b', ['a', 'b'])):\n        left = df.groupby(key, sort=sort).size()\n        right = df.groupby(key, sort=sort)['c'].apply(lambda a: a.shape[0])\n        tm.assert_series_equal(left, right, check_names=False)\n\n    # GH11699\n    df = DataFrame(columns=['A', 'B'])\n    out = Series(dtype='int64', index=Index([], name='A'))\n    tm.assert_series_equal(df.groupby('A').size(), out)\n\n\ndef test_size_groupby_all_null():\n    # GH23050\n    # Assert no 'Value Error : Length of passed values is 2, index implies 0'\n    df = DataFrame({'A': [None, None]})  # all-null groups\n    result = df.groupby('A').size()\n    expected = Series(dtype='int64', index=Index([], name='A'))\n    tm.assert_series_equal(result, expected)\n\n\n# quantile\n# --------------------------------\n@pytest.mark.parametrize(\"interpolation\", [\n    \"linear\", \"lower\", \"higher\", \"nearest\", \"midpoint\"])\n@pytest.mark.parametrize(\"a_vals,b_vals\", [\n    # Ints\n    ([1, 2, 3, 4, 5], [5, 4, 3, 2, 1]),\n    ([1, 2, 3, 4], [4, 3, 2, 1]),\n    ([1, 2, 3, 4, 5], [4, 3, 2, 1]),\n    # Floats\n    ([1., 2., 3., 4., 5.], [5., 4., 3., 2., 1.]),\n    # Missing data\n    ([1., np.nan, 3., np.nan, 5.], [5., np.nan, 3., np.nan, 1.]),\n    ([np.nan, 4., np.nan, 2., np.nan], [np.nan, 4., np.nan, 2., np.nan]),\n    # Timestamps\n    ([x for x in pd.date_range('1/1/18', freq='D', periods=5)],\n     [x for x in pd.date_range('1/1/18', freq='D', periods=5)][::-1]),\n    # All NA\n    ([np.nan] * 5, [np.nan] * 5),\n])\n@pytest.mark.parametrize('q', [0, .25, .5, .75, 1])\ndef test_quantile(interpolation, a_vals, b_vals, q):\n    if interpolation == 'nearest' and q == 0.5 and b_vals == [4, 3, 2, 1]:\n        pytest.skip(\"Unclear numpy expectation for nearest result with \"\n                    \"equidistant data\")\n\n    a_expected = pd.Series(a_vals).quantile(q, interpolation=interpolation)\n    b_expected = pd.Series(b_vals).quantile(q, interpolation=interpolation)\n\n    df = DataFrame({\n        'key': ['a'] * len(a_vals) + ['b'] * len(b_vals),\n        'val': a_vals + b_vals})\n\n    expected = DataFrame([a_expected, b_expected], columns=['val'],\n                         index=Index(['a', 'b'], name='key'))\n    result = df.groupby('key').quantile(q, interpolation=interpolation)\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_quantile_raises():\n    df = pd.DataFrame([\n        ['foo', 'a'], ['foo', 'b'], ['foo', 'c']], columns=['key', 'val'])\n\n    with pytest.raises(TypeError, match=\"cannot be performed against \"\n                       \"'object' dtypes\"):\n        df.groupby('key').quantile()\n\n\n# pipe\n# --------------------------------\n\ndef test_pipe():\n    # Test the pipe method of DataFrameGroupBy.\n    # Issue #17871\n\n    random_state = np.random.RandomState(1234567890)\n\n    df = DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n                          'foo', 'bar', 'foo', 'foo'],\n                    'B': random_state.randn(8),\n                    'C': random_state.randn(8)})\n\n    def f(dfgb):\n        return dfgb.B.max() - dfgb.C.min().min()\n\n    def square(srs):\n        return srs ** 2\n\n    # Note that the transformations are\n    # GroupBy -> Series\n    # Series -> Series\n    # This then chains the GroupBy.pipe and the\n    # NDFrame.pipe methods\n    result = df.groupby('A').pipe(f).pipe(square)\n\n    index = Index(['bar', 'foo'], dtype='object', name='A')\n    expected = pd.Series([8.99110003361, 8.17516964785], name='B',\n                         index=index)\n\n    tm.assert_series_equal(expected, result)\n\n\ndef test_pipe_args():\n    # Test passing args to the pipe method of DataFrameGroupBy.\n    # Issue #17871\n\n    df = pd.DataFrame({'group': ['A', 'A', 'B', 'B', 'C'],\n                       'x': [1.0, 2.0, 3.0, 2.0, 5.0],\n                       'y': [10.0, 100.0, 1000.0, -100.0, -1000.0]})\n\n    def f(dfgb, arg1):\n        return (dfgb.filter(lambda grp: grp.y.mean() > arg1, dropna=False)\n                    .groupby(dfgb.grouper))\n\n    def g(dfgb, arg2):\n        return dfgb.sum() / dfgb.sum().sum() + arg2\n\n    def h(df, arg3):\n        return df.x + df.y - arg3\n\n    result = (df\n              .groupby('group')\n              .pipe(f, 0)\n              .pipe(g, 10)\n              .pipe(h, 100))\n\n    # Assert the results here\n    index = pd.Index(['A', 'B', 'C'], name='group')\n    expected = pd.Series([-79.5160891089, -78.4839108911, -80],\n                         index=index)\n\n    tm.assert_series_equal(expected, result)\n\n    # test SeriesGroupby.pipe\n    ser = pd.Series([1, 1, 2, 2, 3, 3])\n    result = ser.groupby(ser).pipe(lambda grp: grp.sum() * grp.count())\n\n    expected = pd.Series([4, 8, 12], index=pd.Int64Index([1, 2, 3]))\n\n    tm.assert_series_equal(result, expected)\n\n\ndef test_groupby_mean_no_overflow():\n    # Regression test for (#22487)\n    df = pd.DataFrame({\n        \"user\": [\"A\", \"A\", \"A\", \"A\", \"A\"],\n        \"connections\": [4970, 4749, 4719, 4704, 18446744073699999744]\n    })\n    assert df.groupby('user')['connections'].mean()['A'] == 3689348814740003840\n"
    },
    {
      "filename": "pandas/tests/groupby/test_nth.py",
      "content": "import numpy as np\nimport pytest\n\nimport pandas as pd\nfrom pandas import DataFrame, Index, MultiIndex, Series, Timestamp, isna\nfrom pandas.util.testing import (\n    assert_frame_equal, assert_produces_warning, assert_series_equal)\n\n\ndef test_first_last_nth(df):\n    # tests for first / last / nth\n    grouped = df.groupby('A')\n    first = grouped.first()\n    expected = df.loc[[1, 0], ['B', 'C', 'D']]\n    expected.index = Index(['bar', 'foo'], name='A')\n    expected = expected.sort_index()\n    assert_frame_equal(first, expected)\n\n    nth = grouped.nth(0)\n    assert_frame_equal(nth, expected)\n\n    last = grouped.last()\n    expected = df.loc[[5, 7], ['B', 'C', 'D']]\n    expected.index = Index(['bar', 'foo'], name='A')\n    assert_frame_equal(last, expected)\n\n    nth = grouped.nth(-1)\n    assert_frame_equal(nth, expected)\n\n    nth = grouped.nth(1)\n    expected = df.loc[[2, 3], ['B', 'C', 'D']].copy()\n    expected.index = Index(['foo', 'bar'], name='A')\n    expected = expected.sort_index()\n    assert_frame_equal(nth, expected)\n\n    # it works!\n    grouped['B'].first()\n    grouped['B'].last()\n    grouped['B'].nth(0)\n\n    df.loc[df['A'] == 'foo', 'B'] = np.nan\n    assert isna(grouped['B'].first()['foo'])\n    assert isna(grouped['B'].last()['foo'])\n    assert isna(grouped['B'].nth(0)['foo'])\n\n    # v0.14.0 whatsnew\n    df = DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=['A', 'B'])\n    g = df.groupby('A')\n    result = g.first()\n    expected = df.iloc[[1, 2]].set_index('A')\n    assert_frame_equal(result, expected)\n\n    expected = df.iloc[[1, 2]].set_index('A')\n    result = g.nth(0, dropna='any')\n    assert_frame_equal(result, expected)\n\n\ndef test_first_last_nth_dtypes(df_mixed_floats):\n\n    df = df_mixed_floats.copy()\n    df['E'] = True\n    df['F'] = 1\n\n    # tests for first / last / nth\n    grouped = df.groupby('A')\n    first = grouped.first()\n    expected = df.loc[[1, 0], ['B', 'C', 'D', 'E', 'F']]\n    expected.index = Index(['bar', 'foo'], name='A')\n    expected = expected.sort_index()\n    assert_frame_equal(first, expected)\n\n    last = grouped.last()\n    expected = df.loc[[5, 7], ['B', 'C', 'D', 'E', 'F']]\n    expected.index = Index(['bar', 'foo'], name='A')\n    expected = expected.sort_index()\n    assert_frame_equal(last, expected)\n\n    nth = grouped.nth(1)\n    expected = df.loc[[3, 2], ['B', 'C', 'D', 'E', 'F']]\n    expected.index = Index(['bar', 'foo'], name='A')\n    expected = expected.sort_index()\n    assert_frame_equal(nth, expected)\n\n    # GH 2763, first/last shifting dtypes\n    idx = list(range(10))\n    idx.append(9)\n    s = Series(data=range(11), index=idx, name='IntCol')\n    assert s.dtype == 'int64'\n    f = s.groupby(level=0).first()\n    assert f.dtype == 'int64'\n\n\ndef test_nth():\n    df = DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=['A', 'B'])\n    g = df.groupby('A')\n\n    assert_frame_equal(g.nth(0), df.iloc[[0, 2]].set_index('A'))\n    assert_frame_equal(g.nth(1), df.iloc[[1]].set_index('A'))\n    assert_frame_equal(g.nth(2), df.loc[[]].set_index('A'))\n    assert_frame_equal(g.nth(-1), df.iloc[[1, 2]].set_index('A'))\n    assert_frame_equal(g.nth(-2), df.iloc[[0]].set_index('A'))\n    assert_frame_equal(g.nth(-3), df.loc[[]].set_index('A'))\n    assert_series_equal(g.B.nth(0), df.set_index('A').B.iloc[[0, 2]])\n    assert_series_equal(g.B.nth(1), df.set_index('A').B.iloc[[1]])\n    assert_frame_equal(g[['B']].nth(0),\n                       df.loc[[0, 2], ['A', 'B']].set_index('A'))\n\n    exp = df.set_index('A')\n    assert_frame_equal(g.nth(0, dropna='any'), exp.iloc[[1, 2]])\n    assert_frame_equal(g.nth(-1, dropna='any'), exp.iloc[[1, 2]])\n\n    exp['B'] = np.nan\n    assert_frame_equal(g.nth(7, dropna='any'), exp.iloc[[1, 2]])\n    assert_frame_equal(g.nth(2, dropna='any'), exp.iloc[[1, 2]])\n\n    # out of bounds, regression from 0.13.1\n    # GH 6621\n    df = DataFrame({'color': {0: 'green',\n                              1: 'green',\n                              2: 'red',\n                              3: 'red',\n                              4: 'red'},\n                    'food': {0: 'ham',\n                             1: 'eggs',\n                             2: 'eggs',\n                             3: 'ham',\n                             4: 'pork'},\n                    'two': {0: 1.5456590000000001,\n                            1: -0.070345000000000005,\n                            2: -2.4004539999999999,\n                            3: 0.46206000000000003,\n                            4: 0.52350799999999997},\n                    'one': {0: 0.56573799999999996,\n                            1: -0.9742360000000001,\n                            2: 1.033801,\n                            3: -0.78543499999999999,\n                            4: 0.70422799999999997}}).set_index(['color',\n                                                                 'food'])\n\n    result = df.groupby(level=0, as_index=False).nth(2)\n    expected = df.iloc[[-1]]\n    assert_frame_equal(result, expected)\n\n    result = df.groupby(level=0, as_index=False).nth(3)\n    expected = df.loc[[]]\n    assert_frame_equal(result, expected)\n\n    # GH 7559\n    # from the vbench\n    df = DataFrame(np.random.randint(1, 10, (100, 2)), dtype='int64')\n    s = df[1]\n    g = df[0]\n    expected = s.groupby(g).first()\n    expected2 = s.groupby(g).apply(lambda x: x.iloc[0])\n    assert_series_equal(expected2, expected, check_names=False)\n    assert expected.name == 1\n    assert expected2.name == 1\n\n    # validate first\n    v = s[g == 1].iloc[0]\n    assert expected.iloc[0] == v\n    assert expected2.iloc[0] == v\n\n    # this is NOT the same as .first (as sorted is default!)\n    # as it keeps the order in the series (and not the group order)\n    # related GH 7287\n    expected = s.groupby(g, sort=False).first()\n    result = s.groupby(g, sort=False).nth(0, dropna='all')\n    assert_series_equal(result, expected)\n\n    # doc example\n    df = DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=['A', 'B'])\n    g = df.groupby('A')\n    # PR 17493, related to issue 11038\n    # test Series.nth with True for dropna produces FutureWarning\n    with assert_produces_warning(FutureWarning):\n        result = g.B.nth(0, dropna=True)\n    expected = g.B.first()\n    assert_series_equal(result, expected)\n\n    # test multiple nth values\n    df = DataFrame([[1, np.nan], [1, 3], [1, 4], [5, 6], [5, 7]],\n                   columns=['A', 'B'])\n    g = df.groupby('A')\n\n    assert_frame_equal(g.nth(0), df.iloc[[0, 3]].set_index('A'))\n    assert_frame_equal(g.nth([0]), df.iloc[[0, 3]].set_index('A'))\n    assert_frame_equal(g.nth([0, 1]), df.iloc[[0, 1, 3, 4]].set_index('A'))\n    assert_frame_equal(\n        g.nth([0, -1]), df.iloc[[0, 2, 3, 4]].set_index('A'))\n    assert_frame_equal(\n        g.nth([0, 1, 2]), df.iloc[[0, 1, 2, 3, 4]].set_index('A'))\n    assert_frame_equal(\n        g.nth([0, 1, -1]), df.iloc[[0, 1, 2, 3, 4]].set_index('A'))\n    assert_frame_equal(g.nth([2]), df.iloc[[2]].set_index('A'))\n    assert_frame_equal(g.nth([3, 4]), df.loc[[]].set_index('A'))\n\n    business_dates = pd.date_range(start='4/1/2014', end='6/30/2014',\n                                   freq='B')\n    df = DataFrame(1, index=business_dates, columns=['a', 'b'])\n    # get the first, fourth and last two business days for each month\n    key = [df.index.year, df.index.month]\n    result = df.groupby(key, as_index=False).nth([0, 3, -2, -1])\n    expected_dates = pd.to_datetime(\n        ['2014/4/1', '2014/4/4', '2014/4/29', '2014/4/30', '2014/5/1',\n         '2014/5/6', '2014/5/29', '2014/5/30', '2014/6/2', '2014/6/5',\n         '2014/6/27', '2014/6/30'])\n    expected = DataFrame(1, columns=['a', 'b'], index=expected_dates)\n    assert_frame_equal(result, expected)\n\n\ndef test_nth_multi_index(three_group):\n    # PR 9090, related to issue 8979\n    # test nth on MultiIndex, should match .first()\n    grouped = three_group.groupby(['A', 'B'])\n    result = grouped.nth(0)\n    expected = grouped.first()\n    assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize('data, expected_first, expected_last', [\n    ({'id': ['A'],\n      'time': Timestamp('2012-02-01 14:00:00',\n                        tz='US/Central'),\n      'foo': [1]},\n     {'id': ['A'],\n      'time': Timestamp('2012-02-01 14:00:00',\n                        tz='US/Central'),\n      'foo': [1]},\n     {'id': ['A'],\n      'time': Timestamp('2012-02-01 14:00:00',\n                        tz='US/Central'),\n      'foo': [1]}),\n    ({'id': ['A', 'B', 'A'],\n      'time': [Timestamp('2012-01-01 13:00:00',\n                         tz='America/New_York'),\n               Timestamp('2012-02-01 14:00:00',\n                         tz='US/Central'),\n               Timestamp('2012-03-01 12:00:00',\n                         tz='Europe/London')],\n      'foo': [1, 2, 3]},\n     {'id': ['A', 'B'],\n      'time': [Timestamp('2012-01-01 13:00:00',\n                         tz='America/New_York'),\n               Timestamp('2012-02-01 14:00:00',\n                         tz='US/Central')],\n      'foo': [1, 2]},\n     {'id': ['A', 'B'],\n      'time': [Timestamp('2012-03-01 12:00:00',\n                         tz='Europe/London'),\n               Timestamp('2012-02-01 14:00:00',\n                         tz='US/Central')],\n      'foo': [3, 2]})\n])\ndef test_first_last_tz(data, expected_first, expected_last):\n    # GH15884\n    # Test that the timezone is retained when calling first\n    # or last on groupby with as_index=False\n\n    df = DataFrame(data)\n\n    result = df.groupby('id', as_index=False).first()\n    expected = DataFrame(expected_first)\n    cols = ['id', 'time', 'foo']\n    assert_frame_equal(result[cols], expected[cols])\n\n    result = df.groupby('id', as_index=False)['time'].first()\n    assert_frame_equal(result, expected[['id', 'time']])\n\n    result = df.groupby('id', as_index=False).last()\n    expected = DataFrame(expected_last)\n    cols = ['id', 'time', 'foo']\n    assert_frame_equal(result[cols], expected[cols])\n\n    result = df.groupby('id', as_index=False)['time'].last()\n    assert_frame_equal(result, expected[['id', 'time']])\n\n\n@pytest.mark.parametrize('method, ts, alpha', [\n    ['first', Timestamp('2013-01-01', tz='US/Eastern'), 'a'],\n    ['last', Timestamp('2013-01-02', tz='US/Eastern'), 'b']\n])\ndef test_first_last_tz_multi_column(method, ts, alpha):\n    # GH 21603\n    category_string = pd.Series(list('abc')).astype(\n        'category')\n    df = pd.DataFrame({'group': [1, 1, 2],\n                       'category_string': category_string,\n                       'datetimetz': pd.date_range('20130101', periods=3,\n                                                   tz='US/Eastern')})\n    result = getattr(df.groupby('group'), method)()\n    expected = pd.DataFrame(\n        {'category_string': pd.Categorical(\n            [alpha, 'c'], dtype=category_string.dtype),\n         'datetimetz': [ts,\n                        Timestamp('2013-01-03',\n                                  tz='US/Eastern')]},\n        index=pd.Index([1, 2], name='group'))\n    assert_frame_equal(result, expected)\n\n\ndef test_nth_multi_index_as_expected():\n    # PR 9090, related to issue 8979\n    # test nth on MultiIndex\n    three_group = DataFrame(\n        {'A': ['foo', 'foo', 'foo', 'foo', 'bar', 'bar', 'bar', 'bar',\n               'foo', 'foo', 'foo'],\n         'B': ['one', 'one', 'one', 'two', 'one', 'one', 'one', 'two',\n               'two', 'two', 'one'],\n         'C': ['dull', 'dull', 'shiny', 'dull', 'dull', 'shiny', 'shiny',\n               'dull', 'shiny', 'shiny', 'shiny']})\n    grouped = three_group.groupby(['A', 'B'])\n    result = grouped.nth(0)\n    expected = DataFrame(\n        {'C': ['dull', 'dull', 'dull', 'dull']},\n        index=MultiIndex.from_arrays([['bar', 'bar', 'foo', 'foo'],\n                                      ['one', 'two', 'one', 'two']],\n                                     names=['A', 'B']))\n    assert_frame_equal(result, expected)\n\n\ndef test_groupby_head_tail():\n    df = DataFrame([[1, 2], [1, 4], [5, 6]], columns=['A', 'B'])\n    g_as = df.groupby('A', as_index=True)\n    g_not_as = df.groupby('A', as_index=False)\n\n    # as_index= False, much easier\n    assert_frame_equal(df.loc[[0, 2]], g_not_as.head(1))\n    assert_frame_equal(df.loc[[1, 2]], g_not_as.tail(1))\n\n    empty_not_as = DataFrame(columns=df.columns,\n                             index=pd.Index([], dtype=df.index.dtype))\n    empty_not_as['A'] = empty_not_as['A'].astype(df.A.dtype)\n    empty_not_as['B'] = empty_not_as['B'].astype(df.B.dtype)\n    assert_frame_equal(empty_not_as, g_not_as.head(0))\n    assert_frame_equal(empty_not_as, g_not_as.tail(0))\n    assert_frame_equal(empty_not_as, g_not_as.head(-1))\n    assert_frame_equal(empty_not_as, g_not_as.tail(-1))\n\n    assert_frame_equal(df, g_not_as.head(7))  # contains all\n    assert_frame_equal(df, g_not_as.tail(7))\n\n    # as_index=True, (used to be different)\n    df_as = df\n\n    assert_frame_equal(df_as.loc[[0, 2]], g_as.head(1))\n    assert_frame_equal(df_as.loc[[1, 2]], g_as.tail(1))\n\n    empty_as = DataFrame(index=df_as.index[:0], columns=df.columns)\n    empty_as['A'] = empty_not_as['A'].astype(df.A.dtype)\n    empty_as['B'] = empty_not_as['B'].astype(df.B.dtype)\n    assert_frame_equal(empty_as, g_as.head(0))\n    assert_frame_equal(empty_as, g_as.tail(0))\n    assert_frame_equal(empty_as, g_as.head(-1))\n    assert_frame_equal(empty_as, g_as.tail(-1))\n\n    assert_frame_equal(df_as, g_as.head(7))  # contains all\n    assert_frame_equal(df_as, g_as.tail(7))\n\n    # test with selection\n    assert_frame_equal(g_as[[]].head(1), df_as.loc[[0, 2], []])\n    assert_frame_equal(g_as[['A']].head(1), df_as.loc[[0, 2], ['A']])\n    assert_frame_equal(g_as[['B']].head(1), df_as.loc[[0, 2], ['B']])\n    assert_frame_equal(g_as[['A', 'B']].head(1), df_as.loc[[0, 2]])\n\n    assert_frame_equal(g_not_as[[]].head(1), df_as.loc[[0, 2], []])\n    assert_frame_equal(g_not_as[['A']].head(1), df_as.loc[[0, 2], ['A']])\n    assert_frame_equal(g_not_as[['B']].head(1), df_as.loc[[0, 2], ['B']])\n    assert_frame_equal(g_not_as[['A', 'B']].head(1), df_as.loc[[0, 2]])\n\n\ndef test_group_selection_cache():\n    # GH 12839 nth, head, and tail should return same result consistently\n    df = DataFrame([[1, 2], [1, 4], [5, 6]], columns=['A', 'B'])\n    expected = df.iloc[[0, 2]].set_index('A')\n\n    g = df.groupby('A')\n    result1 = g.head(n=2)\n    result2 = g.nth(0)\n    assert_frame_equal(result1, df)\n    assert_frame_equal(result2, expected)\n\n    g = df.groupby('A')\n    result1 = g.tail(n=2)\n    result2 = g.nth(0)\n    assert_frame_equal(result1, df)\n    assert_frame_equal(result2, expected)\n\n    g = df.groupby('A')\n    result1 = g.nth(0)\n    result2 = g.head(n=2)\n    assert_frame_equal(result1, expected)\n    assert_frame_equal(result2, df)\n\n    g = df.groupby('A')\n    result1 = g.nth(0)\n    result2 = g.tail(n=2)\n    assert_frame_equal(result1, expected)\n    assert_frame_equal(result2, df)\n\n\ndef test_nth_empty():\n    # GH 16064\n    df = DataFrame(index=[0], columns=['a', 'b', 'c'])\n    result = df.groupby('a').nth(10)\n    expected = DataFrame(index=Index([], name='a'), columns=['b', 'c'])\n    assert_frame_equal(result, expected)\n\n    result = df.groupby(['a', 'b']).nth(10)\n    expected = DataFrame(index=MultiIndex([[], []], [[], []],\n                                          names=['a', 'b']),\n                         columns=['c'])\n    assert_frame_equal(result, expected)\n\n\ndef test_nth_column_order():\n    # GH 20760\n    # Check that nth preserves column order\n    df = DataFrame([[1, 'b', 100],\n                    [1, 'a', 50],\n                    [1, 'a', np.nan],\n                    [2, 'c', 200],\n                    [2, 'd', 150]],\n                   columns=['A', 'C', 'B'])\n    result = df.groupby('A').nth(0)\n    expected = DataFrame([['b', 100.0],\n                          ['c', 200.0]],\n                         columns=['C', 'B'],\n                         index=Index([1, 2], name='A'))\n    assert_frame_equal(result, expected)\n\n    result = df.groupby('A').nth(-1, dropna='any')\n    expected = DataFrame([['a', 50.0],\n                          ['d', 150.0]],\n                         columns=['C', 'B'],\n                         index=Index([1, 2], name='A'))\n    assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"dropna\", [None, 'any', 'all'])\ndef test_nth_nan_in_grouper(dropna):\n    # GH 26011\n    df = DataFrame([\n        [np.nan, 0, 1],\n        ['abc', 2, 3],\n        [np.nan, 4, 5],\n        ['def', 6, 7],\n        [np.nan, 8, 9],\n    ], columns=list('abc'))\n    result = df.groupby('a').nth(0, dropna=dropna)\n    expected = pd.DataFrame([[2, 3], [6, 7]], columns=list('bc'),\n                            index=Index(['abc', 'def'], name='a'))\n\n    assert_frame_equal(result, expected)\n"
    },
    {
      "filename": "pandas/tests/resample/test_datetime_index.py",
      "content": "from datetime import datetime, timedelta\nfrom functools import partial\nfrom io import StringIO\n\nimport numpy as np\nimport pytest\nimport pytz\n\nfrom pandas.errors import UnsupportedFunctionCall\n\nimport pandas as pd\nfrom pandas import DataFrame, Series, Timedelta, Timestamp, isna, notna\nfrom pandas.core.groupby.grouper import Grouper\nfrom pandas.core.indexes.datetimes import date_range\nfrom pandas.core.indexes.period import Period, period_range\nfrom pandas.core.resample import DatetimeIndex, _get_timestamp_range_edges\nimport pandas.util.testing as tm\nfrom pandas.util.testing import (\n    assert_almost_equal, assert_frame_equal, assert_series_equal)\n\nimport pandas.tseries.offsets as offsets\nfrom pandas.tseries.offsets import BDay, Minute\n\n\n@pytest.fixture()\ndef _index_factory():\n    return date_range\n\n\n@pytest.fixture\ndef _index_freq():\n    return 'Min'\n\n\n@pytest.fixture\ndef _static_values(index):\n    return np.random.rand(len(index))\n\n\ndef test_custom_grouper(index):\n\n    dti = index\n    s = Series(np.array([1] * len(dti)), index=dti, dtype='int64')\n\n    b = Grouper(freq=Minute(5))\n    g = s.groupby(b)\n\n    # check all cython functions work\n    funcs = ['add', 'mean', 'prod', 'ohlc', 'min', 'max', 'var']\n    for f in funcs:\n        g._cython_agg_general(f)\n\n    b = Grouper(freq=Minute(5), closed='right', label='right')\n    g = s.groupby(b)\n    # check all cython functions work\n    funcs = ['add', 'mean', 'prod', 'ohlc', 'min', 'max', 'var']\n    for f in funcs:\n        g._cython_agg_general(f)\n\n    assert g.ngroups == 2593\n    assert notna(g.mean()).all()\n\n    # construct expected val\n    arr = [1] + [5] * 2592\n    idx = dti[0:-1:5]\n    idx = idx.append(dti[-1:])\n    expect = Series(arr, index=idx)\n\n    # GH2763 - return in put dtype if we can\n    result = g.agg(np.sum)\n    assert_series_equal(result, expect)\n\n    df = DataFrame(np.random.rand(len(dti), 10),\n                   index=dti, dtype='float64')\n    r = df.groupby(b).agg(np.sum)\n\n    assert len(r.columns) == 10\n    assert len(r.index) == 2593\n\n\n@pytest.mark.parametrize(\n    '_index_start,_index_end,_index_name',\n    [('1/1/2000 00:00:00', '1/1/2000 00:13:00', 'index')])\n@pytest.mark.parametrize('closed, expected', [\n    ('right',\n        lambda s: Series(\n            [s[0], s[1:6].mean(), s[6:11].mean(), s[11:].mean()],\n            index=date_range(\n                '1/1/2000', periods=4, freq='5min', name='index'))),\n    ('left',\n        lambda s: Series(\n            [s[:5].mean(), s[5:10].mean(), s[10:].mean()],\n            index=date_range(\n                '1/1/2000 00:05', periods=3, freq='5min', name='index'))\n     )\n])\ndef test_resample_basic(series, closed, expected):\n    s = series\n    expected = expected(s)\n    result = s.resample('5min', closed=closed, label='right').mean()\n    assert_series_equal(result, expected)\n\n\ndef test_resample_integerarray():\n    # GH 25580, resample on IntegerArray\n    ts = pd.Series(range(9),\n                   index=pd.date_range('1/1/2000', periods=9, freq='T'),\n                   dtype='Int64')\n    result = ts.resample('3T').sum()\n    expected = Series([3, 12, 21],\n                      index=pd.date_range('1/1/2000', periods=3, freq='3T'),\n                      dtype=\"Int64\")\n    assert_series_equal(result, expected)\n\n    result = ts.resample('3T').mean()\n    expected = Series([1, 4, 7],\n                      index=pd.date_range('1/1/2000', periods=3, freq='3T'),\n                      dtype='Int64')\n    assert_series_equal(result, expected)\n\n\ndef test_resample_basic_grouper(series):\n    s = series\n    result = s.resample('5Min').last()\n    grouper = Grouper(freq=Minute(5), closed='left', label='left')\n    expected = s.groupby(grouper).agg(lambda x: x[-1])\n    assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    '_index_start,_index_end,_index_name',\n    [('1/1/2000 00:00:00', '1/1/2000 00:13:00', 'index')])\n@pytest.mark.parametrize('keyword,value', [\n    ('label', 'righttt'),\n    ('closed', 'righttt'),\n    ('convention', 'starttt')\n])\ndef test_resample_string_kwargs(series, keyword, value):\n    # see gh-19303\n    # Check that wrong keyword argument strings raise an error\n    msg = \"Unsupported value {value} for `{keyword}`\".format(\n        value=value, keyword=keyword)\n    with pytest.raises(ValueError, match=msg):\n        series.resample('5min', **({keyword: value}))\n\n\n@pytest.mark.parametrize(\n    '_index_start,_index_end,_index_name',\n    [('1/1/2000 00:00:00', '1/1/2000 00:13:00', 'index')])\ndef test_resample_how(series, downsample_method):\n    if downsample_method == 'ohlc':\n        pytest.skip('covered by test_resample_how_ohlc')\n\n    s = series\n    grouplist = np.ones_like(s)\n    grouplist[0] = 0\n    grouplist[1:6] = 1\n    grouplist[6:11] = 2\n    grouplist[11:] = 3\n    expected = s.groupby(grouplist).agg(downsample_method)\n    expected.index = date_range(\n        '1/1/2000', periods=4, freq='5min', name='index')\n\n    result = getattr(s.resample(\n        '5min', closed='right', label='right'), downsample_method)()\n    assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    '_index_start,_index_end,_index_name',\n    [('1/1/2000 00:00:00', '1/1/2000 00:13:00', 'index')])\ndef test_resample_how_ohlc(series):\n    s = series\n    grouplist = np.ones_like(s)\n    grouplist[0] = 0\n    grouplist[1:6] = 1\n    grouplist[6:11] = 2\n    grouplist[11:] = 3\n\n    def _ohlc(group):\n        if isna(group).all():\n            return np.repeat(np.nan, 4)\n        return [group[0], group.max(), group.min(), group[-1]]\n\n    expected = DataFrame(\n        s.groupby(grouplist).agg(_ohlc).values.tolist(),\n        index=date_range('1/1/2000', periods=4, freq='5min', name='index'),\n        columns=['open', 'high', 'low', 'close'])\n\n    result = s.resample('5min', closed='right', label='right').ohlc()\n    assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    'func', ['min', 'max', 'sum', 'prod', 'mean', 'var', 'std'])\ndef test_numpy_compat(func):\n    # see gh-12811\n    s = Series([1, 2, 3, 4, 5], index=date_range(\n        '20130101', periods=5, freq='s'))\n    r = s.resample('2s')\n\n    msg = \"numpy operations are not valid with resample\"\n\n    with pytest.raises(UnsupportedFunctionCall, match=msg):\n        getattr(r, func)(func, 1, 2, 3)\n    with pytest.raises(UnsupportedFunctionCall, match=msg):\n        getattr(r, func)(axis=1)\n\n\ndef test_resample_how_callables():\n    # GH#7929\n    data = np.arange(5, dtype=np.int64)\n    ind = date_range(start='2014-01-01', periods=len(data), freq='d')\n    df = DataFrame({\"A\": data, \"B\": data}, index=ind)\n\n    def fn(x, a=1):\n        return str(type(x))\n\n    class FnClass:\n\n        def __call__(self, x):\n            return str(type(x))\n\n    df_standard = df.resample(\"M\").apply(fn)\n    df_lambda = df.resample(\"M\").apply(lambda x: str(type(x)))\n    df_partial = df.resample(\"M\").apply(partial(fn))\n    df_partial2 = df.resample(\"M\").apply(partial(fn, a=2))\n    df_class = df.resample(\"M\").apply(FnClass())\n\n    assert_frame_equal(df_standard, df_lambda)\n    assert_frame_equal(df_standard, df_partial)\n    assert_frame_equal(df_standard, df_partial2)\n    assert_frame_equal(df_standard, df_class)\n\n\ndef test_resample_rounding():\n    # GH 8371\n    # odd results when rounding is needed\n\n    data = \"\"\"date,time,value\n11-08-2014,00:00:01.093,1\n11-08-2014,00:00:02.159,1\n11-08-2014,00:00:02.667,1\n11-08-2014,00:00:03.175,1\n11-08-2014,00:00:07.058,1\n11-08-2014,00:00:07.362,1\n11-08-2014,00:00:08.324,1\n11-08-2014,00:00:08.830,1\n11-08-2014,00:00:08.982,1\n11-08-2014,00:00:09.815,1\n11-08-2014,00:00:10.540,1\n11-08-2014,00:00:11.061,1\n11-08-2014,00:00:11.617,1\n11-08-2014,00:00:13.607,1\n11-08-2014,00:00:14.535,1\n11-08-2014,00:00:15.525,1\n11-08-2014,00:00:17.960,1\n11-08-2014,00:00:20.674,1\n11-08-2014,00:00:21.191,1\"\"\"\n\n    df = pd.read_csv(StringIO(data), parse_dates={'timestamp': [\n        'date', 'time']}, index_col='timestamp')\n    df.index.name = None\n    result = df.resample('6s').sum()\n    expected = DataFrame({'value': [\n        4, 9, 4, 2\n    ]}, index=date_range('2014-11-08', freq='6s', periods=4))\n    assert_frame_equal(result, expected)\n\n    result = df.resample('7s').sum()\n    expected = DataFrame({'value': [\n        4, 10, 4, 1\n    ]}, index=date_range('2014-11-08', freq='7s', periods=4))\n    assert_frame_equal(result, expected)\n\n    result = df.resample('11s').sum()\n    expected = DataFrame({'value': [\n        11, 8\n    ]}, index=date_range('2014-11-08', freq='11s', periods=2))\n    assert_frame_equal(result, expected)\n\n    result = df.resample('13s').sum()\n    expected = DataFrame({'value': [\n        13, 6\n    ]}, index=date_range('2014-11-08', freq='13s', periods=2))\n    assert_frame_equal(result, expected)\n\n    result = df.resample('17s').sum()\n    expected = DataFrame({'value': [\n        16, 3\n    ]}, index=date_range('2014-11-08', freq='17s', periods=2))\n    assert_frame_equal(result, expected)\n\n\ndef test_resample_basic_from_daily():\n    # from daily\n    dti = date_range(start=datetime(2005, 1, 1),\n                     end=datetime(2005, 1, 10), freq='D', name='index')\n\n    s = Series(np.random.rand(len(dti)), dti)\n\n    # to weekly\n    result = s.resample('w-sun').last()\n\n    assert len(result) == 3\n    assert (result.index.dayofweek == [6, 6, 6]).all()\n    assert result.iloc[0] == s['1/2/2005']\n    assert result.iloc[1] == s['1/9/2005']\n    assert result.iloc[2] == s.iloc[-1]\n\n    result = s.resample('W-MON').last()\n    assert len(result) == 2\n    assert (result.index.dayofweek == [0, 0]).all()\n    assert result.iloc[0] == s['1/3/2005']\n    assert result.iloc[1] == s['1/10/2005']\n\n    result = s.resample('W-TUE').last()\n    assert len(result) == 2\n    assert (result.index.dayofweek == [1, 1]).all()\n    assert result.iloc[0] == s['1/4/2005']\n    assert result.iloc[1] == s['1/10/2005']\n\n    result = s.resample('W-WED').last()\n    assert len(result) == 2\n    assert (result.index.dayofweek == [2, 2]).all()\n    assert result.iloc[0] == s['1/5/2005']\n    assert result.iloc[1] == s['1/10/2005']\n\n    result = s.resample('W-THU').last()\n    assert len(result) == 2\n    assert (result.index.dayofweek == [3, 3]).all()\n    assert result.iloc[0] == s['1/6/2005']\n    assert result.iloc[1] == s['1/10/2005']\n\n    result = s.resample('W-FRI').last()\n    assert len(result) == 2\n    assert (result.index.dayofweek == [4, 4]).all()\n    assert result.iloc[0] == s['1/7/2005']\n    assert result.iloc[1] == s['1/10/2005']\n\n    # to biz day\n    result = s.resample('B').last()\n    assert len(result) == 7\n    assert (result.index.dayofweek == [4, 0, 1, 2, 3, 4, 0]).all()\n\n    assert result.iloc[0] == s['1/2/2005']\n    assert result.iloc[1] == s['1/3/2005']\n    assert result.iloc[5] == s['1/9/2005']\n    assert result.index.name == 'index'\n\n\ndef test_resample_upsampling_picked_but_not_correct():\n\n    # Test for issue #3020\n    dates = date_range('01-Jan-2014', '05-Jan-2014', freq='D')\n    series = Series(1, index=dates)\n\n    result = series.resample('D').mean()\n    assert result.index[0] == dates[0]\n\n    # GH 5955\n    # incorrect deciding to upsample when the axis frequency matches the\n    # resample frequency\n\n    s = Series(np.arange(1., 6), index=[datetime(\n        1975, 1, i, 12, 0) for i in range(1, 6)])\n    expected = Series(np.arange(1., 6), index=date_range(\n        '19750101', periods=5, freq='D'))\n\n    result = s.resample('D').count()\n    assert_series_equal(result, Series(1, index=expected.index))\n\n    result1 = s.resample('D').sum()\n    result2 = s.resample('D').mean()\n    assert_series_equal(result1, expected)\n    assert_series_equal(result2, expected)\n\n\ndef test_resample_frame_basic():\n    df = tm.makeTimeDataFrame()\n\n    b = Grouper(freq='M')\n    g = df.groupby(b)\n\n    # check all cython functions work\n    funcs = ['add', 'mean', 'prod', 'min', 'max', 'var']\n    for f in funcs:\n        g._cython_agg_general(f)\n\n    result = df.resample('A').mean()\n    assert_series_equal(result['A'], df['A'].resample('A').mean())\n\n    result = df.resample('M').mean()\n    assert_series_equal(result['A'], df['A'].resample('M').mean())\n\n    df.resample('M', kind='period').mean()\n    df.resample('W-WED', kind='period').mean()\n\n\n@pytest.mark.parametrize('loffset', [timedelta(minutes=1),\n                                     '1min', Minute(1),\n                                     np.timedelta64(1, 'm')])\ndef test_resample_loffset(loffset):\n    # GH 7687\n    rng = date_range('1/1/2000 00:00:00', '1/1/2000 00:13:00', freq='min')\n    s = Series(np.random.randn(14), index=rng)\n\n    result = s.resample('5min', closed='right', label='right',\n                        loffset=loffset).mean()\n    idx = date_range('1/1/2000', periods=4, freq='5min')\n    expected = Series([s[0], s[1:6].mean(), s[6:11].mean(), s[11:].mean()],\n                      index=idx + timedelta(minutes=1))\n    assert_series_equal(result, expected)\n    assert result.index.freq == Minute(5)\n\n    # from daily\n    dti = date_range(start=datetime(2005, 1, 1),\n                     end=datetime(2005, 1, 10), freq='D')\n    ser = Series(np.random.rand(len(dti)), dti)\n\n    # to weekly\n    result = ser.resample('w-sun').last()\n    business_day_offset = BDay()\n    expected = ser.resample('w-sun', loffset=-business_day_offset).last()\n    assert result.index[0] - business_day_offset == expected.index[0]\n\n\ndef test_resample_loffset_upsample():\n    # GH 20744\n    rng = date_range('1/1/2000 00:00:00', '1/1/2000 00:13:00', freq='min')\n    s = Series(np.random.randn(14), index=rng)\n\n    result = s.resample('5min', closed='right', label='right',\n                        loffset=timedelta(minutes=1)).ffill()\n    idx = date_range('1/1/2000', periods=4, freq='5min')\n    expected = Series([s[0], s[5], s[10], s[-1]],\n                      index=idx + timedelta(minutes=1))\n\n    assert_series_equal(result, expected)\n\n\ndef test_resample_loffset_count():\n    # GH 12725\n    start_time = '1/1/2000 00:00:00'\n    rng = date_range(start_time, periods=100, freq='S')\n    ts = Series(np.random.randn(len(rng)), index=rng)\n\n    result = ts.resample('10S', loffset='1s').count()\n\n    expected_index = (\n        date_range(start_time, periods=10, freq='10S') +\n        timedelta(seconds=1)\n    )\n    expected = Series(10, index=expected_index)\n\n    assert_series_equal(result, expected)\n\n    # Same issue should apply to .size() since it goes through\n    #   same code path\n    result = ts.resample('10S', loffset='1s').size()\n\n    assert_series_equal(result, expected)\n\n\ndef test_resample_upsample():\n    # from daily\n    dti = date_range(start=datetime(2005, 1, 1),\n                     end=datetime(2005, 1, 10), freq='D', name='index')\n\n    s = Series(np.random.rand(len(dti)), dti)\n\n    # to minutely, by padding\n    result = s.resample('Min').pad()\n    assert len(result) == 12961\n    assert result[0] == s[0]\n    assert result[-1] == s[-1]\n\n    assert result.index.name == 'index'\n\n\ndef test_resample_how_method():\n    # GH9915\n    s = Series([11, 22],\n               index=[Timestamp('2015-03-31 21:48:52.672000'),\n                      Timestamp('2015-03-31 21:49:52.739000')])\n    expected = Series([11, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, 22],\n                      index=[Timestamp('2015-03-31 21:48:50'),\n                             Timestamp('2015-03-31 21:49:00'),\n                             Timestamp('2015-03-31 21:49:10'),\n                             Timestamp('2015-03-31 21:49:20'),\n                             Timestamp('2015-03-31 21:49:30'),\n                             Timestamp('2015-03-31 21:49:40'),\n                             Timestamp('2015-03-31 21:49:50')])\n    assert_series_equal(s.resample(\"10S\").mean(), expected)\n\n\ndef test_resample_extra_index_point():\n    # GH#9756\n    index = date_range(start='20150101', end='20150331', freq='BM')\n    expected = DataFrame({'A': Series([21, 41, 63], index=index)})\n\n    index = date_range(start='20150101', end='20150331', freq='B')\n    df = DataFrame(\n        {'A': Series(range(len(index)), index=index)}, dtype='int64')\n    result = df.resample('BM').last()\n    assert_frame_equal(result, expected)\n\n\ndef test_upsample_with_limit():\n    rng = date_range('1/1/2000', periods=3, freq='5t')\n    ts = Series(np.random.randn(len(rng)), rng)\n\n    result = ts.resample('t').ffill(limit=2)\n    expected = ts.reindex(result.index, method='ffill', limit=2)\n    assert_series_equal(result, expected)\n\n\ndef test_nearest_upsample_with_limit():\n    rng = date_range('1/1/2000', periods=3, freq='5t')\n    ts = Series(np.random.randn(len(rng)), rng)\n\n    result = ts.resample('t').nearest(limit=2)\n    expected = ts.reindex(result.index, method='nearest', limit=2)\n    assert_series_equal(result, expected)\n\n\ndef test_resample_ohlc(series):\n    s = series\n\n    grouper = Grouper(freq=Minute(5))\n    expect = s.groupby(grouper).agg(lambda x: x[-1])\n    result = s.resample('5Min').ohlc()\n\n    assert len(result) == len(expect)\n    assert len(result.columns) == 4\n\n    xs = result.iloc[-2]\n    assert xs['open'] == s[-6]\n    assert xs['high'] == s[-6:-1].max()\n    assert xs['low'] == s[-6:-1].min()\n    assert xs['close'] == s[-2]\n\n    xs = result.iloc[0]\n    assert xs['open'] == s[0]\n    assert xs['high'] == s[:5].max()\n    assert xs['low'] == s[:5].min()\n    assert xs['close'] == s[4]\n\n\ndef test_resample_ohlc_result():\n\n    # GH 12332\n    index = pd.date_range('1-1-2000', '2-15-2000', freq='h')\n    index = index.union(pd.date_range('4-15-2000', '5-15-2000', freq='h'))\n    s = Series(range(len(index)), index=index)\n\n    a = s.loc[:'4-15-2000'].resample('30T').ohlc()\n    assert isinstance(a, DataFrame)\n\n    b = s.loc[:'4-14-2000'].resample('30T').ohlc()\n    assert isinstance(b, DataFrame)\n\n    # GH12348\n    # raising on odd period\n    rng = date_range('2013-12-30', '2014-01-07')\n    index = rng.drop([Timestamp('2014-01-01'),\n                      Timestamp('2013-12-31'),\n                      Timestamp('2014-01-04'),\n                      Timestamp('2014-01-05')])\n    df = DataFrame(data=np.arange(len(index)), index=index)\n    result = df.resample('B').mean()\n    expected = df.reindex(index=date_range(rng[0], rng[-1], freq='B'))\n    assert_frame_equal(result, expected)\n\n\ndef test_resample_ohlc_dataframe():\n    df = (\n        DataFrame({\n            'PRICE': {\n                Timestamp('2011-01-06 10:59:05', tz=None): 24990,\n                Timestamp('2011-01-06 12:43:33', tz=None): 25499,\n                Timestamp('2011-01-06 12:54:09', tz=None): 25499},\n            'VOLUME': {\n                Timestamp('2011-01-06 10:59:05', tz=None): 1500000000,\n                Timestamp('2011-01-06 12:43:33', tz=None): 5000000000,\n                Timestamp('2011-01-06 12:54:09', tz=None): 100000000}})\n    ).reindex(['VOLUME', 'PRICE'], axis=1)\n    res = df.resample('H').ohlc()\n    exp = pd.concat([df['VOLUME'].resample('H').ohlc(),\n                     df['PRICE'].resample('H').ohlc()],\n                    axis=1,\n                    keys=['VOLUME', 'PRICE'])\n    assert_frame_equal(exp, res)\n\n    df.columns = [['a', 'b'], ['c', 'd']]\n    res = df.resample('H').ohlc()\n    exp.columns = pd.MultiIndex.from_tuples([\n        ('a', 'c', 'open'), ('a', 'c', 'high'), ('a', 'c', 'low'),\n        ('a', 'c', 'close'), ('b', 'd', 'open'), ('b', 'd', 'high'),\n        ('b', 'd', 'low'), ('b', 'd', 'close')])\n    assert_frame_equal(exp, res)\n\n    # dupe columns fail atm\n    # df.columns = ['PRICE', 'PRICE']\n\n\ndef test_resample_dup_index():\n\n    # GH 4812\n    # dup columns with resample raising\n    df = DataFrame(np.random.randn(4, 12), index=[2000, 2000, 2000, 2000],\n                   columns=[Period(year=2000, month=i + 1, freq='M')\n                            for i in range(12)])\n    df.iloc[3, :] = np.nan\n    result = df.resample('Q', axis=1).mean()\n    expected = df.groupby(lambda x: int((x.month - 1) / 3), axis=1).mean()\n    expected.columns = [\n        Period(year=2000, quarter=i + 1, freq='Q') for i in range(4)]\n    assert_frame_equal(result, expected)\n\n\ndef test_resample_reresample():\n    dti = date_range(start=datetime(2005, 1, 1),\n                     end=datetime(2005, 1, 10), freq='D')\n    s = Series(np.random.rand(len(dti)), dti)\n    bs = s.resample('B', closed='right', label='right').mean()\n    result = bs.resample('8H').mean()\n    assert len(result) == 22\n    assert isinstance(result.index.freq, offsets.DateOffset)\n    assert result.index.freq == offsets.Hour(8)\n\n\ndef test_resample_timestamp_to_period(simple_date_range_series):\n    ts = simple_date_range_series('1/1/1990', '1/1/2000')\n\n    result = ts.resample('A-DEC', kind='period').mean()\n    expected = ts.resample('A-DEC').mean()\n    expected.index = period_range('1990', '2000', freq='a-dec')\n    assert_series_equal(result, expected)\n\n    result = ts.resample('A-JUN', kind='period').mean()\n    expected = ts.resample('A-JUN').mean()\n    expected.index = period_range('1990', '2000', freq='a-jun')\n    assert_series_equal(result, expected)\n\n    result = ts.resample('M', kind='period').mean()\n    expected = ts.resample('M').mean()\n    expected.index = period_range('1990-01', '2000-01', freq='M')\n    assert_series_equal(result, expected)\n\n    result = ts.resample('M', kind='period').mean()\n    expected = ts.resample('M').mean()\n    expected.index = period_range('1990-01', '2000-01', freq='M')\n    assert_series_equal(result, expected)\n\n\ndef test_ohlc_5min():\n    def _ohlc(group):\n        if isna(group).all():\n            return np.repeat(np.nan, 4)\n        return [group[0], group.max(), group.min(), group[-1]]\n\n    rng = date_range('1/1/2000 00:00:00', '1/1/2000 5:59:50', freq='10s')\n    ts = Series(np.random.randn(len(rng)), index=rng)\n\n    resampled = ts.resample('5min', closed='right',\n                            label='right').ohlc()\n\n    assert (resampled.loc['1/1/2000 00:00'] == ts[0]).all()\n\n    exp = _ohlc(ts[1:31])\n    assert (resampled.loc['1/1/2000 00:05'] == exp).all()\n\n    exp = _ohlc(ts['1/1/2000 5:55:01':])\n    assert (resampled.loc['1/1/2000 6:00:00'] == exp).all()\n\n\ndef test_downsample_non_unique():\n    rng = date_range('1/1/2000', '2/29/2000')\n    rng2 = rng.repeat(5).values\n    ts = Series(np.random.randn(len(rng2)), index=rng2)\n\n    result = ts.resample('M').mean()\n\n    expected = ts.groupby(lambda x: x.month).mean()\n    assert len(result) == 2\n    assert_almost_equal(result[0], expected[1])\n    assert_almost_equal(result[1], expected[2])\n\n\ndef test_asfreq_non_unique():\n    # GH #1077\n    rng = date_range('1/1/2000', '2/29/2000')\n    rng2 = rng.repeat(2).values\n    ts = Series(np.random.randn(len(rng2)), index=rng2)\n\n    msg = 'cannot reindex from a duplicate axis'\n    with pytest.raises(ValueError, match=msg):\n        ts.asfreq('B')\n\n\ndef test_resample_axis1():\n    rng = date_range('1/1/2000', '2/29/2000')\n    df = DataFrame(np.random.randn(3, len(rng)), columns=rng,\n                   index=['a', 'b', 'c'])\n\n    result = df.resample('M', axis=1).mean()\n    expected = df.T.resample('M').mean().T\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_resample_anchored_ticks():\n    # If a fixed delta (5 minute, 4 hour) evenly divides a day, we should\n    # \"anchor\" the origin at midnight so we get regular intervals rather\n    # than starting from the first timestamp which might start in the\n    # middle of a desired interval\n\n    rng = date_range('1/1/2000 04:00:00', periods=86400, freq='s')\n    ts = Series(np.random.randn(len(rng)), index=rng)\n    ts[:2] = np.nan  # so results are the same\n\n    freqs = ['t', '5t', '15t', '30t', '4h', '12h']\n    for freq in freqs:\n        result = ts[2:].resample(freq, closed='left', label='left').mean()\n        expected = ts.resample(freq, closed='left', label='left').mean()\n        assert_series_equal(result, expected)\n\n\ndef test_resample_single_group():\n    mysum = lambda x: x.sum()\n\n    rng = date_range('2000-1-1', '2000-2-10', freq='D')\n    ts = Series(np.random.randn(len(rng)), index=rng)\n    assert_series_equal(ts.resample('M').sum(),\n                        ts.resample('M').apply(mysum))\n\n    rng = date_range('2000-1-1', '2000-1-10', freq='D')\n    ts = Series(np.random.randn(len(rng)), index=rng)\n    assert_series_equal(ts.resample('M').sum(),\n                        ts.resample('M').apply(mysum))\n\n    # GH 3849\n    s = Series([30.1, 31.6], index=[Timestamp('20070915 15:30:00'),\n                                    Timestamp('20070915 15:40:00')])\n    expected = Series([0.75], index=[Timestamp('20070915')])\n    result = s.resample('D').apply(lambda x: np.std(x))\n    assert_series_equal(result, expected)\n\n\ndef test_resample_base():\n    rng = date_range('1/1/2000 00:00:00', '1/1/2000 02:00', freq='s')\n    ts = Series(np.random.randn(len(rng)), index=rng)\n\n    resampled = ts.resample('5min', base=2).mean()\n    exp_rng = date_range('12/31/1999 23:57:00', '1/1/2000 01:57',\n                         freq='5min')\n    tm.assert_index_equal(resampled.index, exp_rng)\n\n\ndef test_resample_float_base():\n    # GH25161\n    dt = pd.to_datetime([\"2018-11-26 16:17:43.51\",\n                         \"2018-11-26 16:17:44.51\",\n                         \"2018-11-26 16:17:45.51\"])\n    s = Series(np.arange(3), index=dt)\n\n    base = 17 + 43.51 / 60\n    result = s.resample(\"3min\", base=base).size()\n    expected = Series(3, index=pd.DatetimeIndex([\"2018-11-26 16:17:43.51\"]))\n    assert_series_equal(result, expected)\n\n\ndef test_resample_daily_anchored():\n    rng = date_range('1/1/2000 0:00:00', periods=10000, freq='T')\n    ts = Series(np.random.randn(len(rng)), index=rng)\n    ts[:2] = np.nan  # so results are the same\n\n    result = ts[2:].resample('D', closed='left', label='left').mean()\n    expected = ts.resample('D', closed='left', label='left').mean()\n    assert_series_equal(result, expected)\n\n\ndef test_resample_to_period_monthly_buglet():\n    # GH #1259\n\n    rng = date_range('1/1/2000', '12/31/2000')\n    ts = Series(np.random.randn(len(rng)), index=rng)\n\n    result = ts.resample('M', kind='period').mean()\n    exp_index = period_range('Jan-2000', 'Dec-2000', freq='M')\n    tm.assert_index_equal(result.index, exp_index)\n\n\ndef test_period_with_agg():\n\n    # aggregate a period resampler with a lambda\n    s2 = Series(np.random.randint(0, 5, 50),\n                index=pd.period_range('2012-01-01', freq='H', periods=50),\n                dtype='float64')\n\n    expected = s2.to_timestamp().resample('D').mean().to_period()\n    result = s2.resample('D').agg(lambda x: x.mean())\n    assert_series_equal(result, expected)\n\n\ndef test_resample_segfault():\n    # GH 8573\n    # segfaulting in older versions\n    all_wins_and_wagers = [\n        (1, datetime(2013, 10, 1, 16, 20), 1, 0),\n        (2, datetime(2013, 10, 1, 16, 10), 1, 0),\n        (2, datetime(2013, 10, 1, 18, 15), 1, 0),\n        (2, datetime(2013, 10, 1, 16, 10, 31), 1, 0)]\n\n    df = DataFrame.from_records(all_wins_and_wagers,\n                                columns=(\"ID\", \"timestamp\", \"A\", \"B\")\n                                ).set_index(\"timestamp\")\n    result = df.groupby(\"ID\").resample(\"5min\").sum()\n    expected = df.groupby(\"ID\").apply(lambda x: x.resample(\"5min\").sum())\n    assert_frame_equal(result, expected)\n\n\ndef test_resample_dtype_preservation():\n\n    # GH 12202\n    # validation tests for dtype preservation\n\n    df = DataFrame({'date': pd.date_range(start='2016-01-01',\n                                          periods=4, freq='W'),\n                    'group': [1, 1, 2, 2],\n                    'val': Series([5, 6, 7, 8],\n                                  dtype='int32')}\n                   ).set_index('date')\n\n    result = df.resample('1D').ffill()\n    assert result.val.dtype == np.int32\n\n    result = df.groupby('group').resample('1D').ffill()\n    assert result.val.dtype == np.int32\n\n\ndef test_resample_dtype_coerceion():\n\n    pytest.importorskip('scipy.interpolate')\n\n    # GH 16361\n    df = {\"a\": [1, 3, 1, 4]}\n    df = DataFrame(df, index=pd.date_range(\"2017-01-01\", \"2017-01-04\"))\n\n    expected = (df.astype(\"float64\")\n                .resample(\"H\")\n                .mean()\n                [\"a\"]\n                .interpolate(\"cubic\")\n                )\n\n    result = df.resample(\"H\")[\"a\"].mean().interpolate(\"cubic\")\n    tm.assert_series_equal(result, expected)\n\n    result = df.resample(\"H\").mean()[\"a\"].interpolate(\"cubic\")\n    tm.assert_series_equal(result, expected)\n\n\ndef test_weekly_resample_buglet():\n    # #1327\n    rng = date_range('1/1/2000', freq='B', periods=20)\n    ts = Series(np.random.randn(len(rng)), index=rng)\n\n    resampled = ts.resample('W').mean()\n    expected = ts.resample('W-SUN').mean()\n    assert_series_equal(resampled, expected)\n\n\ndef test_monthly_resample_error():\n    # #1451\n    dates = date_range('4/16/2012 20:00', periods=5000, freq='h')\n    ts = Series(np.random.randn(len(dates)), index=dates)\n    # it works!\n    ts.resample('M')\n\n\ndef test_nanosecond_resample_error():\n    # GH 12307 - Values falls after last bin when\n    # Resampling using pd.tseries.offsets.Nano as period\n    start = 1443707890427\n    exp_start = 1443707890400\n    indx = pd.date_range(\n        start=pd.to_datetime(start),\n        periods=10,\n        freq='100n'\n    )\n    ts = Series(range(len(indx)), index=indx)\n    r = ts.resample(pd.tseries.offsets.Nano(100))\n    result = r.agg('mean')\n\n    exp_indx = pd.date_range(\n        start=pd.to_datetime(exp_start),\n        periods=10,\n        freq='100n'\n    )\n    exp = Series(range(len(exp_indx)), index=exp_indx)\n\n    assert_series_equal(result, exp)\n\n\ndef test_resample_anchored_intraday(simple_date_range_series):\n    # #1471, #1458\n\n    rng = date_range('1/1/2012', '4/1/2012', freq='100min')\n    df = DataFrame(rng.month, index=rng)\n\n    result = df.resample('M').mean()\n    expected = df.resample(\n        'M', kind='period').mean().to_timestamp(how='end')\n    expected.index += Timedelta(1, 'ns') - Timedelta(1, 'D')\n    tm.assert_frame_equal(result, expected)\n\n    result = df.resample('M', closed='left').mean()\n    exp = df.tshift(1, freq='D').resample('M', kind='period').mean()\n    exp = exp.to_timestamp(how='end')\n\n    exp.index = exp.index + Timedelta(1, 'ns') - Timedelta(1, 'D')\n    tm.assert_frame_equal(result, exp)\n\n    rng = date_range('1/1/2012', '4/1/2012', freq='100min')\n    df = DataFrame(rng.month, index=rng)\n\n    result = df.resample('Q').mean()\n    expected = df.resample(\n        'Q', kind='period').mean().to_timestamp(how='end')\n    expected.index += Timedelta(1, 'ns') - Timedelta(1, 'D')\n    tm.assert_frame_equal(result, expected)\n\n    result = df.resample('Q', closed='left').mean()\n    expected = df.tshift(1, freq='D').resample('Q', kind='period',\n                                               closed='left').mean()\n    expected = expected.to_timestamp(how='end')\n    expected.index += Timedelta(1, 'ns') - Timedelta(1, 'D')\n    tm.assert_frame_equal(result, expected)\n\n    ts = simple_date_range_series('2012-04-29 23:00', '2012-04-30 5:00',\n                                  freq='h')\n    resampled = ts.resample('M').mean()\n    assert len(resampled) == 1\n\n\ndef test_resample_anchored_monthstart(simple_date_range_series):\n    ts = simple_date_range_series('1/1/2000', '12/31/2002')\n\n    freqs = ['MS', 'BMS', 'QS-MAR', 'AS-DEC', 'AS-JUN']\n\n    for freq in freqs:\n        ts.resample(freq).mean()\n\n\ndef test_resample_anchored_multiday():\n    # When resampling a range spanning multiple days, ensure that the\n    # start date gets used to determine the offset.  Fixes issue where\n    # a one day period is not a multiple of the frequency.\n    #\n    # See: https://github.com/pandas-dev/pandas/issues/8683\n\n    index = pd.date_range(\n        '2014-10-14 23:06:23.206', periods=3, freq='400L'\n    ) | pd.date_range(\n        '2014-10-15 23:00:00', periods=2, freq='2200L')\n\n    s = Series(np.random.randn(5), index=index)\n\n    # Ensure left closing works\n    result = s.resample('2200L').mean()\n    assert result.index[-1] == Timestamp('2014-10-15 23:00:02.000')\n\n    # Ensure right closing works\n    result = s.resample('2200L', label='right').mean()\n    assert result.index[-1] == Timestamp('2014-10-15 23:00:04.200')\n\n\ndef test_corner_cases(simple_period_range_series,\n                      simple_date_range_series):\n    # miscellaneous test coverage\n\n    rng = date_range('1/1/2000', periods=12, freq='t')\n    ts = Series(np.random.randn(len(rng)), index=rng)\n\n    result = ts.resample('5t', closed='right', label='left').mean()\n    ex_index = date_range('1999-12-31 23:55', periods=4, freq='5t')\n    tm.assert_index_equal(result.index, ex_index)\n\n    len0pts = simple_period_range_series(\n        '2007-01', '2010-05', freq='M')[:0]\n    # it works\n    result = len0pts.resample('A-DEC').mean()\n    assert len(result) == 0\n\n    # resample to periods\n    ts = simple_date_range_series(\n        '2000-04-28', '2000-04-30 11:00', freq='h')\n    result = ts.resample('M', kind='period').mean()\n    assert len(result) == 1\n    assert result.index[0] == Period('2000-04', freq='M')\n\n\ndef test_anchored_lowercase_buglet():\n    dates = date_range('4/16/2012 20:00', periods=50000, freq='s')\n    ts = Series(np.random.randn(len(dates)), index=dates)\n    # it works!\n    ts.resample('d').mean()\n\n\ndef test_upsample_apply_functions():\n    # #1596\n    rng = pd.date_range('2012-06-12', periods=4, freq='h')\n\n    ts = Series(np.random.randn(len(rng)), index=rng)\n\n    result = ts.resample('20min').aggregate(['mean', 'sum'])\n    assert isinstance(result, DataFrame)\n\n\ndef test_resample_not_monotonic():\n    rng = pd.date_range('2012-06-12', periods=200, freq='h')\n    ts = Series(np.random.randn(len(rng)), index=rng)\n\n    ts = ts.take(np.random.permutation(len(ts)))\n\n    result = ts.resample('D').sum()\n    exp = ts.sort_index().resample('D').sum()\n    assert_series_equal(result, exp)\n\n\ndef test_resample_median_bug_1688():\n\n    for dtype in ['int64', 'int32', 'float64', 'float32']:\n        df = DataFrame([1, 2], index=[datetime(2012, 1, 1, 0, 0, 0),\n                                      datetime(2012, 1, 1, 0, 5, 0)],\n                       dtype=dtype)\n\n        result = df.resample(\"T\").apply(lambda x: x.mean())\n        exp = df.asfreq('T')\n        tm.assert_frame_equal(result, exp)\n\n        result = df.resample(\"T\").median()\n        exp = df.asfreq('T')\n        tm.assert_frame_equal(result, exp)\n\n\ndef test_how_lambda_functions(simple_date_range_series):\n\n    ts = simple_date_range_series('1/1/2000', '4/1/2000')\n\n    result = ts.resample('M').apply(lambda x: x.mean())\n    exp = ts.resample('M').mean()\n    tm.assert_series_equal(result, exp)\n\n    foo_exp = ts.resample('M').mean()\n    foo_exp.name = 'foo'\n    bar_exp = ts.resample('M').std()\n    bar_exp.name = 'bar'\n\n    result = ts.resample('M').apply(\n        [lambda x: x.mean(), lambda x: x.std(ddof=1)])\n    result.columns = ['foo', 'bar']\n    tm.assert_series_equal(result['foo'], foo_exp)\n    tm.assert_series_equal(result['bar'], bar_exp)\n\n    # this is a MI Series, so comparing the names of the results\n    # doesn't make sense\n    result = ts.resample('M').aggregate({'foo': lambda x: x.mean(),\n                                         'bar': lambda x: x.std(ddof=1)})\n    tm.assert_series_equal(result['foo'], foo_exp, check_names=False)\n    tm.assert_series_equal(result['bar'], bar_exp, check_names=False)\n\n\ndef test_resample_unequal_times():\n    # #1772\n    start = datetime(1999, 3, 1, 5)\n    # end hour is less than start\n    end = datetime(2012, 7, 31, 4)\n    bad_ind = date_range(start, end, freq=\"30min\")\n    df = DataFrame({'close': 1}, index=bad_ind)\n\n    # it works!\n    df.resample('AS').sum()\n\n\ndef test_resample_consistency():\n\n    # GH 6418\n    # resample with bfill / limit / reindex consistency\n\n    i30 = pd.date_range('2002-02-02', periods=4, freq='30T')\n    s = Series(np.arange(4.), index=i30)\n    s[2] = np.NaN\n\n    # Upsample by factor 3 with reindex() and resample() methods:\n    i10 = pd.date_range(i30[0], i30[-1], freq='10T')\n\n    s10 = s.reindex(index=i10, method='bfill')\n    s10_2 = s.reindex(index=i10, method='bfill', limit=2)\n    rl = s.reindex_like(s10, method='bfill', limit=2)\n    r10_2 = s.resample('10Min').bfill(limit=2)\n    r10 = s.resample('10Min').bfill()\n\n    # s10_2, r10, r10_2, rl should all be equal\n    assert_series_equal(s10_2, r10)\n    assert_series_equal(s10_2, r10_2)\n    assert_series_equal(s10_2, rl)\n\n\ndef test_resample_timegrouper():\n    # GH 7227\n    dates1 = [datetime(2014, 10, 1), datetime(2014, 9, 3),\n              datetime(2014, 11, 5), datetime(2014, 9, 5),\n              datetime(2014, 10, 8), datetime(2014, 7, 15)]\n\n    dates2 = dates1[:2] + [pd.NaT] + dates1[2:4] + [pd.NaT] + dates1[4:]\n    dates3 = [pd.NaT] + dates1 + [pd.NaT]\n\n    for dates in [dates1, dates2, dates3]:\n        df = DataFrame(dict(A=dates, B=np.arange(len(dates))))\n        result = df.set_index('A').resample('M').count()\n        exp_idx = pd.DatetimeIndex(['2014-07-31', '2014-08-31',\n                                    '2014-09-30',\n                                    '2014-10-31', '2014-11-30'],\n                                   freq='M', name='A')\n        expected = DataFrame({'B': [1, 0, 2, 2, 1]}, index=exp_idx)\n        assert_frame_equal(result, expected)\n\n        result = df.groupby(pd.Grouper(freq='M', key='A')).count()\n        assert_frame_equal(result, expected)\n\n        df = DataFrame(dict(A=dates, B=np.arange(len(dates)), C=np.arange(\n            len(dates))))\n        result = df.set_index('A').resample('M').count()\n        expected = DataFrame({'B': [1, 0, 2, 2, 1], 'C': [1, 0, 2, 2, 1]},\n                             index=exp_idx, columns=['B', 'C'])\n        assert_frame_equal(result, expected)\n\n        result = df.groupby(pd.Grouper(freq='M', key='A')).count()\n        assert_frame_equal(result, expected)\n\n\ndef test_resample_nunique():\n\n    # GH 12352\n    df = DataFrame({\n        'ID': {Timestamp('2015-06-05 00:00:00'): '0010100903',\n               Timestamp('2015-06-08 00:00:00'): '0010150847'},\n        'DATE': {Timestamp('2015-06-05 00:00:00'): '2015-06-05',\n                 Timestamp('2015-06-08 00:00:00'): '2015-06-08'}})\n    r = df.resample('D')\n    g = df.groupby(pd.Grouper(freq='D'))\n    expected = df.groupby(pd.Grouper(freq='D')).ID.apply(lambda x:\n                                                         x.nunique())\n    assert expected.name == 'ID'\n\n    for t in [r, g]:\n        result = r.ID.nunique()\n        assert_series_equal(result, expected)\n\n    result = df.ID.resample('D').nunique()\n    assert_series_equal(result, expected)\n\n    result = df.ID.groupby(pd.Grouper(freq='D')).nunique()\n    assert_series_equal(result, expected)\n\n\ndef test_resample_nunique_preserves_column_level_names():\n    # see gh-23222\n    df = tm.makeTimeDataFrame(freq=\"1D\").abs()\n    df.columns = pd.MultiIndex.from_arrays([df.columns.tolist()] * 2,\n                                           names=[\"lev0\", \"lev1\"])\n    result = df.resample(\"1h\").nunique()\n    tm.assert_index_equal(df.columns, result.columns)\n\n\ndef test_resample_nunique_with_date_gap():\n    # GH 13453\n    index = pd.date_range('1-1-2000', '2-15-2000', freq='h')\n    index2 = pd.date_range('4-15-2000', '5-15-2000', freq='h')\n    index3 = index.append(index2)\n    s = Series(range(len(index3)), index=index3, dtype='int64')\n    r = s.resample('M')\n\n    # Since all elements are unique, these should all be the same\n    results = [\n        r.count(),\n        r.nunique(),\n        r.agg(Series.nunique),\n        r.agg('nunique')\n    ]\n\n    assert_series_equal(results[0], results[1])\n    assert_series_equal(results[0], results[2])\n    assert_series_equal(results[0], results[3])\n\n\n@pytest.mark.parametrize('n', [10000, 100000])\n@pytest.mark.parametrize('k', [10, 100, 1000])\ndef test_resample_group_info(n, k):\n    # GH10914\n\n    # use a fixed seed to always have the same uniques\n    prng = np.random.RandomState(1234)\n\n    dr = date_range(start='2015-08-27', periods=n // 10, freq='T')\n    ts = Series(prng.randint(0, n // k, n).astype('int64'),\n                index=prng.choice(dr, n))\n\n    left = ts.resample('30T').nunique()\n    ix = date_range(start=ts.index.min(), end=ts.index.max(),\n                    freq='30T')\n\n    vals = ts.values\n    bins = np.searchsorted(ix.values, ts.index, side='right')\n\n    sorter = np.lexsort((vals, bins))\n    vals, bins = vals[sorter], bins[sorter]\n\n    mask = np.r_[True, vals[1:] != vals[:-1]]\n    mask |= np.r_[True, bins[1:] != bins[:-1]]\n\n    arr = np.bincount(bins[mask] - 1,\n                      minlength=len(ix)).astype('int64', copy=False)\n    right = Series(arr, index=ix)\n\n    assert_series_equal(left, right)\n\n\ndef test_resample_size():\n    n = 10000\n    dr = date_range('2015-09-19', periods=n, freq='T')\n    ts = Series(np.random.randn(n), index=np.random.choice(dr, n))\n\n    left = ts.resample('7T').size()\n    ix = date_range(start=left.index.min(), end=ts.index.max(), freq='7T')\n\n    bins = np.searchsorted(ix.values, ts.index.values, side='right')\n    val = np.bincount(bins, minlength=len(ix) + 1)[1:].astype('int64',\n                                                              copy=False)\n\n    right = Series(val, index=ix)\n    assert_series_equal(left, right)\n\n\ndef test_resample_across_dst():\n    # The test resamples a DatetimeIndex with values before and after a\n    # DST change\n    # Issue: 14682\n\n    # The DatetimeIndex we will start with\n    # (note that DST happens at 03:00+02:00 -> 02:00+01:00)\n    # 2016-10-30 02:23:00+02:00, 2016-10-30 02:23:00+01:00\n    df1 = DataFrame([1477786980, 1477790580], columns=['ts'])\n    dti1 = DatetimeIndex(pd.to_datetime(df1.ts, unit='s')\n                         .dt.tz_localize('UTC')\n                            .dt.tz_convert('Europe/Madrid'))\n\n    # The expected DatetimeIndex after resampling.\n    # 2016-10-30 02:00:00+02:00, 2016-10-30 02:00:00+01:00\n    df2 = DataFrame([1477785600, 1477789200], columns=['ts'])\n    dti2 = DatetimeIndex(pd.to_datetime(df2.ts, unit='s')\n                         .dt.tz_localize('UTC')\n                            .dt.tz_convert('Europe/Madrid'))\n    df = DataFrame([5, 5], index=dti1)\n\n    result = df.resample(rule='H').sum()\n    expected = DataFrame([5, 5], index=dti2)\n\n    assert_frame_equal(result, expected)\n\n\ndef test_groupby_with_dst_time_change():\n    # GH 24972\n    index = pd.DatetimeIndex([1478064900001000000, 1480037118776792000],\n                             tz='UTC').tz_convert('America/Chicago')\n\n    df = pd.DataFrame([1, 2], index=index)\n    result = df.groupby(pd.Grouper(freq='1d')).last()\n    expected_index_values = pd.date_range('2016-11-02', '2016-11-24',\n                                          freq='d', tz='America/Chicago')\n\n    index = pd.DatetimeIndex(expected_index_values)\n    expected = pd.DataFrame([1.0] + ([np.nan] * 21) + [2.0], index=index)\n    assert_frame_equal(result, expected)\n\n\ndef test_resample_dst_anchor():\n    # 5172\n    dti = DatetimeIndex([datetime(2012, 11, 4, 23)], tz='US/Eastern')\n    df = DataFrame([5], index=dti)\n    assert_frame_equal(df.resample(rule='D').sum(),\n                       DataFrame([5], index=df.index.normalize()))\n    df.resample(rule='MS').sum()\n    assert_frame_equal(\n        df.resample(rule='MS').sum(),\n        DataFrame([5], index=DatetimeIndex([datetime(2012, 11, 1)],\n                                           tz='US/Eastern')))\n\n    dti = date_range('2013-09-30', '2013-11-02', freq='30Min',\n                     tz='Europe/Paris')\n    values = range(dti.size)\n    df = DataFrame({\"a\": values,\n                    \"b\": values,\n                    \"c\": values}, index=dti, dtype='int64')\n    how = {\"a\": \"min\", \"b\": \"max\", \"c\": \"count\"}\n\n    assert_frame_equal(\n        df.resample(\"W-MON\").agg(how)[[\"a\", \"b\", \"c\"]],\n        DataFrame({\"a\": [0, 48, 384, 720, 1056, 1394],\n                   \"b\": [47, 383, 719, 1055, 1393, 1586],\n                   \"c\": [48, 336, 336, 336, 338, 193]},\n                  index=date_range('9/30/2013', '11/4/2013',\n                                   freq='W-MON', tz='Europe/Paris')),\n        'W-MON Frequency')\n\n    assert_frame_equal(\n        df.resample(\"2W-MON\").agg(how)[[\"a\", \"b\", \"c\"]],\n        DataFrame({\"a\": [0, 48, 720, 1394],\n                   \"b\": [47, 719, 1393, 1586],\n                   \"c\": [48, 672, 674, 193]},\n                  index=date_range('9/30/2013', '11/11/2013',\n                                   freq='2W-MON', tz='Europe/Paris')),\n        '2W-MON Frequency')\n\n    assert_frame_equal(\n        df.resample(\"MS\").agg(how)[[\"a\", \"b\", \"c\"]],\n        DataFrame({\"a\": [0, 48, 1538],\n                   \"b\": [47, 1537, 1586],\n                   \"c\": [48, 1490, 49]},\n                  index=date_range('9/1/2013', '11/1/2013',\n                                   freq='MS', tz='Europe/Paris')),\n        'MS Frequency')\n\n    assert_frame_equal(\n        df.resample(\"2MS\").agg(how)[[\"a\", \"b\", \"c\"]],\n        DataFrame({\"a\": [0, 1538],\n                   \"b\": [1537, 1586],\n                   \"c\": [1538, 49]},\n                  index=date_range('9/1/2013', '11/1/2013',\n                                   freq='2MS', tz='Europe/Paris')),\n        '2MS Frequency')\n\n    df_daily = df['10/26/2013':'10/29/2013']\n    assert_frame_equal(\n        df_daily.resample(\"D\").agg({\"a\": \"min\", \"b\": \"max\", \"c\": \"count\"})\n        [[\"a\", \"b\", \"c\"]],\n        DataFrame({\"a\": [1248, 1296, 1346, 1394],\n                   \"b\": [1295, 1345, 1393, 1441],\n                   \"c\": [48, 50, 48, 48]},\n                  index=date_range('10/26/2013', '10/29/2013',\n                                   freq='D', tz='Europe/Paris')),\n        'D Frequency')\n\n\ndef test_downsample_across_dst():\n    # GH 8531\n    tz = pytz.timezone('Europe/Berlin')\n    dt = datetime(2014, 10, 26)\n    dates = date_range(tz.localize(dt), periods=4, freq='2H')\n    result = Series(5, index=dates).resample('H').mean()\n    expected = Series([5., np.nan] * 3 + [5.],\n                      index=date_range(tz.localize(dt), periods=7,\n                                       freq='H'))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_downsample_across_dst_weekly():\n    # GH 9119, GH 21459\n    df = DataFrame(index=DatetimeIndex([\n        '2017-03-25', '2017-03-26', '2017-03-27',\n        '2017-03-28', '2017-03-29'\n    ], tz='Europe/Amsterdam'),\n        data=[11, 12, 13, 14, 15])\n    result = df.resample('1W').sum()\n    expected = DataFrame([23, 42], index=pd.DatetimeIndex([\n        '2017-03-26', '2017-04-02'\n    ], tz='Europe/Amsterdam'))\n    tm.assert_frame_equal(result, expected)\n\n    idx = pd.date_range(\"2013-04-01\", \"2013-05-01\", tz='Europe/London',\n                        freq='H')\n    s = Series(index=idx)\n    result = s.resample('W').mean()\n    expected = Series(index=pd.date_range(\n        '2013-04-07', freq='W', periods=5, tz='Europe/London'\n    ))\n    tm.assert_series_equal(result, expected)\n\n\ndef test_resample_with_nat():\n    # GH 13020\n    index = DatetimeIndex([pd.NaT,\n                           '1970-01-01 00:00:00',\n                           pd.NaT,\n                           '1970-01-01 00:00:01',\n                           '1970-01-01 00:00:02'])\n    frame = DataFrame([2, 3, 5, 7, 11], index=index)\n\n    index_1s = DatetimeIndex(['1970-01-01 00:00:00',\n                              '1970-01-01 00:00:01',\n                              '1970-01-01 00:00:02'])\n    frame_1s = DataFrame([3, 7, 11], index=index_1s)\n    assert_frame_equal(frame.resample('1s').mean(), frame_1s)\n\n    index_2s = DatetimeIndex(['1970-01-01 00:00:00',\n                              '1970-01-01 00:00:02'])\n    frame_2s = DataFrame([5, 11], index=index_2s)\n    assert_frame_equal(frame.resample('2s').mean(), frame_2s)\n\n    index_3s = DatetimeIndex(['1970-01-01 00:00:00'])\n    frame_3s = DataFrame([7], index=index_3s)\n    assert_frame_equal(frame.resample('3s').mean(), frame_3s)\n\n    assert_frame_equal(frame.resample('60s').mean(), frame_3s)\n\n\ndef test_resample_datetime_values():\n    # GH 13119\n    # check that datetime dtype is preserved when NaT values are\n    # introduced by the resampling\n\n    dates = [datetime(2016, 1, 15), datetime(2016, 1, 19)]\n    df = DataFrame({'timestamp': dates}, index=dates)\n\n    exp = Series([datetime(2016, 1, 15), pd.NaT, datetime(2016, 1, 19)],\n                 index=date_range('2016-01-15', periods=3, freq='2D'),\n                 name='timestamp')\n\n    res = df.resample('2D').first()['timestamp']\n    tm.assert_series_equal(res, exp)\n    res = df['timestamp'].resample('2D').first()\n    tm.assert_series_equal(res, exp)\n\n\ndef test_resample_apply_with_additional_args(series):\n    # GH 14615\n    def f(data, add_arg):\n        return np.mean(data) * add_arg\n\n    multiplier = 10\n    result = series.resample('D').apply(f, multiplier)\n    expected = series.resample('D').mean().multiply(multiplier)\n    tm.assert_series_equal(result, expected)\n\n    # Testing as kwarg\n    result = series.resample('D').apply(f, add_arg=multiplier)\n    expected = series.resample('D').mean().multiply(multiplier)\n    tm.assert_series_equal(result, expected)\n\n    # Testing dataframe\n    df = pd.DataFrame({\"A\": 1, \"B\": 2},\n                      index=pd.date_range('2017', periods=10))\n    result = df.groupby(\"A\").resample(\"D\").agg(f, multiplier)\n    expected = df.groupby(\"A\").resample('D').mean().multiply(multiplier)\n    assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize('k', [1, 2, 3])\n@pytest.mark.parametrize('n1, freq1, n2, freq2', [\n    (30, 'S', 0.5, 'Min'),\n    (60, 'S', 1, 'Min'),\n    (3600, 'S', 1, 'H'),\n    (60, 'Min', 1, 'H'),\n    (21600, 'S', 0.25, 'D'),\n    (86400, 'S', 1, 'D'),\n    (43200, 'S', 0.5, 'D'),\n    (1440, 'Min', 1, 'D'),\n    (12, 'H', 0.5, 'D'),\n    (24, 'H', 1, 'D'),\n])\ndef test_resample_equivalent_offsets(n1, freq1, n2, freq2, k):\n    # GH 24127\n    n1_ = n1 * k\n    n2_ = n2 * k\n    s = pd.Series(0, index=pd.date_range('19910905 13:00',\n                                         '19911005 07:00',\n                                         freq=freq1))\n    s = s + range(len(s))\n\n    result1 = s.resample(str(n1_) + freq1).mean()\n    result2 = s.resample(str(n2_) + freq2).mean()\n    assert_series_equal(result1, result2)\n\n\n@pytest.mark.parametrize('first,last,offset,exp_first,exp_last', [\n    ('19910905', '19920406', 'D', '19910905', '19920407'),\n    ('19910905 00:00', '19920406 06:00', 'D', '19910905', '19920407'),\n    ('19910905 06:00', '19920406 06:00', 'H', '19910905 06:00',\n        '19920406 07:00'),\n    ('19910906', '19920406', 'M', '19910831', '19920430'),\n    ('19910831', '19920430', 'M', '19910831', '19920531'),\n    ('1991-08', '1992-04', 'M', '19910831', '19920531'),\n])\ndef test_get_timestamp_range_edges(first, last, offset,\n                                   exp_first, exp_last):\n    first = pd.Period(first)\n    first = first.to_timestamp(first.freq)\n    last = pd.Period(last)\n    last = last.to_timestamp(last.freq)\n\n    exp_first = pd.Timestamp(exp_first, freq=offset)\n    exp_last = pd.Timestamp(exp_last, freq=offset)\n\n    offset = pd.tseries.frequencies.to_offset(offset)\n    result = _get_timestamp_range_edges(first, last, offset)\n    expected = (exp_first, exp_last)\n    assert result == expected\n"
    },
    {
      "filename": "pandas/tests/sparse/test_groupby.py",
      "content": "import numpy as np\nimport pytest\n\nimport pandas as pd\nimport pandas.util.testing as tm\n\n\n@pytest.mark.filterwarnings(\"ignore:Sparse:FutureWarning\")\n@pytest.mark.filterwarnings(\"ignore:DataFrame.to_sparse:FutureWarning\")\nclass TestSparseGroupBy:\n\n    def setup_method(self, method):\n        self.dense = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n                                         'foo', 'bar', 'foo', 'foo'],\n                                   'B': ['one', 'one', 'two', 'three',\n                                         'two', 'two', 'one', 'three'],\n                                   'C': np.random.randn(8),\n                                   'D': np.random.randn(8),\n                                   'E': [np.nan, np.nan, 1, 2,\n                                         np.nan, 1, np.nan, np.nan]})\n        self.sparse = self.dense.to_sparse()\n\n    def test_first_last_nth(self):\n        # tests for first / last / nth\n        sparse_grouped = self.sparse.groupby('A')\n        dense_grouped = self.dense.groupby('A')\n\n        sparse_grouped_first = sparse_grouped.first()\n        sparse_grouped_last = sparse_grouped.last()\n        sparse_grouped_nth = sparse_grouped.nth(1)\n\n        dense_grouped_first = pd.DataFrame(dense_grouped.first().to_sparse())\n        dense_grouped_last = pd.DataFrame(dense_grouped.last().to_sparse())\n        dense_grouped_nth = pd.DataFrame(dense_grouped.nth(1).to_sparse())\n\n        tm.assert_frame_equal(sparse_grouped_first,\n                              dense_grouped_first)\n        tm.assert_frame_equal(sparse_grouped_last,\n                              dense_grouped_last)\n        tm.assert_frame_equal(sparse_grouped_nth,\n                              dense_grouped_nth)\n\n    def test_aggfuncs(self):\n        sparse_grouped = self.sparse.groupby('A')\n        dense_grouped = self.dense.groupby('A')\n\n        result = sparse_grouped.mean().to_sparse()\n        expected = dense_grouped.mean().to_sparse()\n\n        tm.assert_frame_equal(result, expected)\n\n        # ToDo: sparse sum includes str column\n        # tm.assert_frame_equal(sparse_grouped.sum(),\n        #                       dense_grouped.sum())\n\n        result = sparse_grouped.count().to_sparse()\n        expected = dense_grouped.count().to_sparse()\n\n        tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"fill_value\", [0, np.nan])\n@pytest.mark.filterwarnings(\"ignore:Sparse:FutureWarning\")\n@pytest.mark.filterwarnings(\"ignore:DataFrame.to_sparse:FutureWarning\")\ndef test_groupby_includes_fill_value(fill_value):\n    # https://github.com/pandas-dev/pandas/issues/5078\n    df = pd.DataFrame({'a': [fill_value, 1, fill_value, fill_value],\n                       'b': [fill_value, 1, fill_value, fill_value]})\n    sdf = df.to_sparse(fill_value=fill_value)\n    result = sdf.groupby('a').sum()\n    expected = pd.DataFrame(df.groupby('a').sum().to_sparse(\n        fill_value=fill_value))\n    tm.assert_frame_equal(result, expected, check_index_type=False)\n"
    },
    {
      "filename": "pandas/tests/sparse/test_pivot.py",
      "content": "import numpy as np\nimport pytest\n\nimport pandas as pd\nfrom pandas import _np_version_under1p17\nimport pandas.util.testing as tm\n\n\n@pytest.mark.filterwarnings(\"ignore:Sparse:FutureWarning\")\n@pytest.mark.filterwarnings(\"ignore:Series.to_sparse:FutureWarning\")\n@pytest.mark.filterwarnings(\"ignore:DataFrame.to_sparse:FutureWarning\")\nclass TestPivotTable:\n\n    def setup_method(self, method):\n        rs = np.random.RandomState(0)\n        self.dense = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n                                         'foo', 'bar', 'foo', 'foo'],\n                                   'B': ['one', 'one', 'two', 'three',\n                                         'two', 'two', 'one', 'three'],\n                                   'C': rs.randn(8),\n                                   'D': rs.randn(8),\n                                   'E': [np.nan, np.nan, 1, 2,\n                                         np.nan, 1, np.nan, np.nan]})\n        self.sparse = self.dense.to_sparse()\n\n    def test_pivot_table(self):\n        res_sparse = pd.pivot_table(self.sparse, index='A', columns='B',\n                                    values='C')\n        res_dense = pd.pivot_table(self.dense, index='A', columns='B',\n                                   values='C')\n        tm.assert_frame_equal(res_sparse, res_dense)\n\n        res_sparse = pd.pivot_table(self.sparse, index='A', columns='B',\n                                    values='E')\n        res_dense = pd.pivot_table(self.dense, index='A', columns='B',\n                                   values='E')\n        tm.assert_frame_equal(res_sparse, res_dense)\n\n        res_sparse = pd.pivot_table(self.sparse, index='A', columns='B',\n                                    values='E', aggfunc='mean')\n        res_dense = pd.pivot_table(self.dense, index='A', columns='B',\n                                   values='E', aggfunc='mean')\n        tm.assert_frame_equal(res_sparse, res_dense)\n\n    def test_pivot_table_with_nans(self):\n        res_sparse = pd.pivot_table(self.sparse, index='A', columns='B',\n                                    values='E', aggfunc='sum')\n        res_dense = pd.pivot_table(self.dense, index='A', columns='B',\n                                   values='E', aggfunc='sum')\n        tm.assert_frame_equal(res_sparse, res_dense)\n\n    @pytest.mark.xfail(not _np_version_under1p17,\n                       reason=\"failing occasionally on numpy > 1.17\",\n                       strict=False)\n    def test_pivot_table_multi(self):\n        res_sparse = pd.pivot_table(self.sparse, index='A', columns='B',\n                                    values=['D', 'E'])\n        res_dense = pd.pivot_table(self.dense, index='A', columns='B',\n                                   values=['D', 'E'])\n        res_dense = res_dense.apply(lambda x: x.astype(\"Sparse[float64]\"))\n        tm.assert_frame_equal(res_sparse, res_dense)\n"
    }
  ],
  "questions": [
    "I went the `git bisect` route. I wrote a single failing test for this condition \r\n```\r\ndf.groupby(\"cat\").agg({\"nr\": \"min\", \"cat_ord\": [\"min\", \"max\"]})\r\n```\r\nResult of `git bisect`\r\n```\r\nd0292fe1e12f1d460e52df4da4250bef32324579 is the first bad commit\r\ncommit d0292fe1e12f1d460e52df4da4250bef32324579\r\nAuthor: Jeff Reback <jeff@reback.net>\r\nDate:   Thu Jun 27 18:13:27 2019 -0500\r\n\r\n    BUG: preserve categorical & sparse types when grouping / pivot (#27071)\r\n```\r\n\r\n[link to that PR](https://github.com/pandas-dev/pandas/pull/27071/files) \r\n\r\nLooks like a lot of thinking went into that PR, maybe those of you (e.g. @jreback ) who worked on it want to look into this bug further?"
  ],
  "golden_answers": [
    "Thanks @ncernek. At least for this specific one, it's from how concat handles a mix of categorical & non-categorical indexes\r\n\r\n```pytb\r\nIn [2]: a = pd.DataFrame({\"A\": [1, 2]})\r\n\r\nIn [3]: b = pd.DataFrame({\"B\": [3, 4]}, index=pd.CategoricalIndex(['a', 'b']))\r\n\r\nIn [4]: pd.concat([a, b], axis=1)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-4-a98a78ec2995> in <module>\r\n----> 1 pd.concat([a, b], axis=1)\r\n\r\n~/sandbox/pandas/pandas/core/reshape/concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\r\n    256     )\r\n    257\r\n--> 258     return op.get_result()\r\n    259\r\n    260\r\n\r\n~/sandbox/pandas/pandas/core/reshape/concat.py in get_result(self)\r\n    466                     obj_labels = mgr.axes[ax]\r\n    467                     if not new_labels.equals(obj_labels):\r\n--> 468                         indexers[ax] = obj_labels.reindex(new_labels)[1]\r\n    469\r\n    470                 mgrs_indexers.append((obj._data, indexers))\r\n\r\n~/sandbox/pandas/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance)\r\n    615                 # coerce to a regular index here!\r\n    616                 result = Index(np.array(self), name=self.name)\r\n--> 617                 new_target, indexer, _ = result._reindex_non_unique(np.array(target))\r\n    618             else:\r\n    619\r\n\r\n~/sandbox/pandas/pandas/core/indexes/base.py in _reindex_non_unique(self, target)\r\n   3388\r\n   3389         target = ensure_index(target)\r\n-> 3390         indexer, missing = self.get_indexer_non_unique(target)\r\n   3391         check = indexer != -1\r\n   3392         new_labels = self.take(indexer[check])\r\n\r\n~/sandbox/pandas/pandas/core/indexes/base.py in get_indexer_non_unique(self, target)\r\n   4751             tgt_values = target._ndarray_values\r\n   4752\r\n-> 4753         indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\r\n   4754         return ensure_platform_int(indexer), missing\r\n   4755\r\n\r\n~/sandbox/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_indexer_non_unique()\r\n    305             # increasing, then use binary search for each starget\r\n    306             for starget in stargets:\r\n--> 307                 start = values.searchsorted(starget, side='left')\r\n    308                 end = values.searchsorted(starget, side='right')\r\n    309                 if start != end:\r\n\r\nTypeError: '<' not supported between instances of 'str' and 'int'\r\n\r\nIn [5]: a = pd.DataFrame({\"A\": [1, 2]})\r\n\r\nIn [6]: b = pd.DataFrame({\"B\": [3, 4]}, index=pd.CategoricalIndex(['a', 'b']))\r\n\r\nIn [7]: pd.concat([a, b], axis=1)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-a98a78ec2995> in <module>\r\n----> 1 pd.concat([a, b], axis=1)\r\n\r\n~/sandbox/pandas/pandas/core/reshape/concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\r\n    256     )\r\n    257\r\n--> 258     return op.get_result()\r\n    259\r\n    260\r\n\r\n~/sandbox/pandas/pandas/core/reshape/concat.py in get_result(self)\r\n    466                     obj_labels = mgr.axes[ax]\r\n    467                     if not new_labels.equals(obj_labels):\r\n--> 468                         indexers[ax] = obj_labels.reindex(new_labels)[1]\r\n    469\r\n    470                 mgrs_indexers.append((obj._data, indexers))\r\n\r\n~/sandbox/pandas/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance)\r\n    615                 # coerce to a regular index here!\r\n    616                 result = Index(np.array(self), name=self.name)\r\n--> 617                 new_target, indexer, _ = result._reindex_non_unique(np.array(target))\r\n    618             else:\r\n    619\r\n\r\n~/sandbox/pandas/pandas/core/indexes/base.py in _reindex_non_unique(self, target)\r\n   3388\r\n   3389         target = ensure_index(target)\r\n-> 3390         indexer, missing = self.get_indexer_non_unique(target)\r\n   3391         check = indexer != -1\r\n   3392         new_labels = self.take(indexer[check])\r\n\r\n~/sandbox/pandas/pandas/core/indexes/base.py in get_indexer_non_unique(self, target)\r\n   4751             tgt_values = target._ndarray_values\r\n   4752\r\n-> 4753         indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\r\n   4754         return ensure_platform_int(indexer), missing\r\n   4755\r\n\r\n~/sandbox/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_indexer_non_unique()\r\n    305             # increasing, then use binary search for each starget\r\n    306             for starget in stargets:\r\n--> 307                 start = values.searchsorted(starget, side='left')\r\n    308                 end = values.searchsorted(starget, side='right')\r\n    309                 if start != end:\r\n\r\n```\r\n\r\nI think that should coerce both to object-dtype index before going too far."
  ],
  "questions_generated": [
    "What is the main issue described in the pandas-dev/pandas repository regarding aggregation on ordered categorical columns?",
    "In the provided code context, what is the expected behavior when performing single aggregations on an ordered categorical column?",
    "Why does performing combined aggregations on numeric and ordered categorical columns result in a TypeError?",
    "How does the current implementation of the `aggregate` method in pandas handle different types of columns, and what is the limitation with ordered categorical columns?",
    "What changes could be proposed to the pandas library to address the issue of aggregation on ordered categorical columns?"
  ],
  "golden_answers_generated": [
    "The main issue is that aggregations on ordered categorical columns either drop the grouping index or result in a crash, depending on the context. This behavior is inconsistent with how aggregations work on non-categorical columns, making it difficult to perform combined aggregations over large datasets correctly and efficiently.",
    "The expected behavior when performing single aggregations on an ordered categorical column is that the aggregation should work similarly to non-categorical columns, retaining the grouping index. However, in the current context, the index is dropped, which is not the desired behavior.",
    "Performing combined aggregations on numeric and ordered categorical columns results in a TypeError because the pandas library attempts to concatenate the results of the aggregation, which fails due to a mismatch in data types. Specifically, the error occurs when comparing instances of 'str' and 'int', which are not compatible during reindexing operations within the library's internal methods.",
    "The current implementation of the `aggregate` method in pandas handles different types of columns by applying specified aggregation functions to each column. However, with ordered categorical columns, there is a limitation where the method either drops the grouping index or fails due to type incompatibilities during the concatenation and reindexing processes. This inconsistency arises because the method is not fully equipped to handle the unique properties of ordered categorical data types.",
    "To address the issue of aggregation on ordered categorical columns, proposed changes could include enhancing the `aggregate` method to properly handle the reindexing and concatenation of ordered categorical data, ensuring that the grouping index is preserved. Additionally, implementing type-checking mechanisms or conversion utilities to manage type mismatches during aggregation operations could prevent TypeErrors from occurring."
  ]
}