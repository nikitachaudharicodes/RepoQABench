{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "12412",
  "issue_description": "# Add type stub files to python's typeshed\n\nSame issue as xarray: https://github.com/pydata/xarray/issues/771, resolved with https://github.com/pydata/xarray/pull/773.\n\nOK to do the same to `NDFrame`?\n",
  "issue_comments": [
    {
      "id": 186976187,
      "user": "jreback",
      "body": "why? these ops already have been defined for quite some time\n\nthis is a downstream issue \n"
    },
    {
      "id": 186977409,
      "user": "max-sixty",
      "body": "They are monkey patched rather than defined explicitly (I think - tell me if I'm off), and so IDEs / static code checkers can't resolve them. \n\nCheck out the xarray PR...\n"
    },
    {
      "id": 186977965,
      "user": "jreback",
      "body": "as I said all of these are defined in line\n"
    },
    {
      "id": 186982651,
      "user": "max-sixty",
      "body": "Help me understand where I'm mistaken here... I see them being defined here for `Series`: https://github.com/pydata/pandas/blob/master/pandas/core/series.py#L3001\n\n...and there's no `__add__` method explicitly defined in line (i.e. with `def __add__` or `__add__ =` on the Series class or any of its parents.\n"
    },
    {
      "id": 186984267,
      "user": "jreback",
      "body": "these are all defined in ops\n\nI am unclear as to the problem\n\npy charm has used pandas for years\n"
    },
    {
      "id": 186988417,
      "user": "max-sixty",
      "body": "Ok but pause for a moment on what I'm trying to get across, or ask if you need more info.\n\nBecause the methods aren't defined _in line_, PyCharm can't evaluate that pandas objects have the methods. For example:\n![image](https://cloud.githubusercontent.com/assets/5635139/13208859/464a97b8-d8eb-11e5-99d8-5ea195287e4a.png)\n\nSo in xarray I [added them in line](https://github.com/pydata/xarray/pull/773/files#diff-aec89f8189374cf98061efad7425f561R484), and PyCharm then can resolve them, even though they are never hit at run time.\n"
    },
    {
      "id": 186991204,
      "user": "jreback",
      "body": "and is this new?\n\nthis has never been reported before\n\nI would say this is a bug in py charm \n"
    },
    {
      "id": 186991515,
      "user": "jreback",
      "body": "fixing a symptom is very rarely s good idea\n"
    },
    {
      "id": 186995970,
      "user": "max-sixty",
      "body": "It is not new. I imagine that it hasn't been raised before because it's only very mildly annoying - although for a lot of people. Either that, or my setup is somehow different. \n\nRe the symptom - generally I couldn't agree more - in this case I can also empathize with a static code checker trying to resolve whether a method exists. And I think of this as more like an interface definition than a hack. FWIW I submitted an issue to PyCharm here: https://youtrack.jetbrains.com/issue/PY-18095\n\nAnother option would be to define skeletons [here](https://github.com/JetBrains/python-skeletons) (I think this is how numpy does it), but I haven't gone into detail as to how that works.\n"
    },
    {
      "id": 186997846,
      "user": "jreback",
      "body": "ok if they ever get back to you then either post here or open a new issue\n\ndo you use PyCharm? no one I know ever used it / very heavyweight \n"
    },
    {
      "id": 187004274,
      "user": "max-sixty",
      "body": "We use PyCharm for dev, Jupyter for exploration. I find PyCharm great actually - navigation, some code checking, refactoring. \n\nYou're on vim / emacs?\n"
    },
    {
      "id": 187010401,
      "user": "jreback",
      "body": "emacs\n"
    },
    {
      "id": 191306421,
      "user": "max-sixty",
      "body": "@jreback I heard back from PyCharm https://youtrack.jetbrains.com/issue/PY-18095\n\nThey suggest putting together `.pyi` files on the standard python type repo: https://github.com/python/typeshed\n\nWhat are your views on this generally? Not sure it's the measured solution for the specific issue we were having, but a reasonable broader issue nonetheless. \n"
    },
    {
      "id": 191336853,
      "user": "jreback",
      "body": "yes we should probably do this as it's the way to hint to the ch checkers. no idea how long / tricky this is \n"
    },
    {
      "id": 191454889,
      "user": "max-sixty",
      "body": "OK @jreback, shall I start a new issue or do you want to reopen this and I'll rename it?\n"
    },
    {
      "id": 266476260,
      "user": "smontanaro",
      "body": "What's the status of this? It's not clear this ticket is still open. The last comment on March 2 indicated that it was reopened, but... what does the \"(!) Open\" button mean? Are there some stubs laying about I can test, or should I just punt and run mypy's stubgen over my installed Pandas code?"
    },
    {
      "id": 266479545,
      "user": "jreback",
      "body": "@smontanaro open means its open! nothing has been done on this, you are welcome to contribute fixes though!"
    },
    {
      "id": 266487987,
      "user": "smontanaro",
      "body": "Thanks... Eh, I was thinking that was a button (given its color matching the obvious \"Comment\" button, and didn't hover my mouse to check that it wasn't. I can't push anything out of my work account, but will at least try and add some stubgen-generated .pyi files from home. I will also try to figure out why stubgen fails on some modules. Watch this space."
    },
    {
      "id": 329020719,
      "user": "frankcarey",
      "body": "I'm looking into this now.. Looks like you can generate stubs with the following script that's installed with mypy.. https://github.com/python/mypy/blob/master/mypy/stubgen.py"
    },
    {
      "id": 630303988,
      "user": "OliverSieweke",
      "body": "See also [data-science-types](https://github.com/predictive-analytics-lab/data-science-types) which has started work in that direction."
    },
    {
      "id": 630318510,
      "user": "mroeschke",
      "body": "Since we have https://github.com/pandas-dev/pandas/issues/28142 going about the same issue. Going to close this issue in favor of that one."
    }
  ],
  "text_context": "# Add type stub files to python's typeshed\n\nSame issue as xarray: https://github.com/pydata/xarray/issues/771, resolved with https://github.com/pydata/xarray/pull/773.\n\nOK to do the same to `NDFrame`?\n\n\nwhy? these ops already have been defined for quite some time\n\nthis is a downstream issue \n\n\nThey are monkey patched rather than defined explicitly (I think - tell me if I'm off), and so IDEs / static code checkers can't resolve them. \n\nCheck out the xarray PR...\n\n\nas I said all of these are defined in line\n\n\nHelp me understand where I'm mistaken here... I see them being defined here for `Series`: https://github.com/pydata/pandas/blob/master/pandas/core/series.py#L3001\n\n...and there's no `__add__` method explicitly defined in line (i.e. with `def __add__` or `__add__ =` on the Series class or any of its parents.\n\n\nthese are all defined in ops\n\nI am unclear as to the problem\n\npy charm has used pandas for years\n\n\nOk but pause for a moment on what I'm trying to get across, or ask if you need more info.\n\nBecause the methods aren't defined _in line_, PyCharm can't evaluate that pandas objects have the methods. For example:\n![image](https://cloud.githubusercontent.com/assets/5635139/13208859/464a97b8-d8eb-11e5-99d8-5ea195287e4a.png)\n\nSo in xarray I [added them in line](https://github.com/pydata/xarray/pull/773/files#diff-aec89f8189374cf98061efad7425f561R484), and PyCharm then can resolve them, even though they are never hit at run time.\n\n\nand is this new?\n\nthis has never been reported before\n\nI would say this is a bug in py charm \n\n\nfixing a symptom is very rarely s good idea\n\n\nIt is not new. I imagine that it hasn't been raised before because it's only very mildly annoying - although for a lot of people. Either that, or my setup is somehow different. \n\nRe the symptom - generally I couldn't agree more - in this case I can also empathize with a static code checker trying to resolve whether a method exists. And I think of this as more like an interface definition than a hack. FWIW I submitted an issue to PyCharm here: https://youtrack.jetbrains.com/issue/PY-18095\n\nAnother option would be to define skeletons [here](https://github.com/JetBrains/python-skeletons) (I think this is how numpy does it), but I haven't gone into detail as to how that works.\n\n\nok if they ever get back to you then either post here or open a new issue\n\ndo you use PyCharm? no one I know ever used it / very heavyweight \n\n\nWe use PyCharm for dev, Jupyter for exploration. I find PyCharm great actually - navigation, some code checking, refactoring. \n\nYou're on vim / emacs?\n\n\nemacs\n\n\n@jreback I heard back from PyCharm https://youtrack.jetbrains.com/issue/PY-18095\n\nThey suggest putting together `.pyi` files on the standard python type repo: https://github.com/python/typeshed\n\nWhat are your views on this generally? Not sure it's the measured solution for the specific issue we were having, but a reasonable broader issue nonetheless. \n\n\nyes we should probably do this as it's the way to hint to the ch checkers. no idea how long / tricky this is \n\n\nOK @jreback, shall I start a new issue or do you want to reopen this and I'll rename it?\n\n\nWhat's the status of this? It's not clear this ticket is still open. The last comment on March 2 indicated that it was reopened, but... what does the \"(!) Open\" button mean? Are there some stubs laying about I can test, or should I just punt and run mypy's stubgen over my installed Pandas code?\n\n@smontanaro open means its open! nothing has been done on this, you are welcome to contribute fixes though!\n\nThanks... Eh, I was thinking that was a button (given its color matching the obvious \"Comment\" button, and didn't hover my mouse to check that it wasn't. I can't push anything out of my work account, but will at least try and add some stubgen-generated .pyi files from home. I will also try to figure out why stubgen fails on some modules. Watch this space.\n\nI'm looking into this now.. Looks like you can generate stubs with the following script that's installed with mypy.. https://github.com/python/mypy/blob/master/mypy/stubgen.py\n\nSee also [data-science-types](https://github.com/predictive-analytics-lab/data-science-types) which has started work in that direction.\n\nSince we have https://github.com/pandas-dev/pandas/issues/28142 going about the same issue. Going to close this issue in favor of that one.",
  "pr_link": "https://github.com/pydata/xarray/pull/773",
  "code_context": [
    {
      "filename": "xarray/core/common.py",
      "content": "import numpy as np\nimport pandas as pd\n\nfrom .pycompat import basestring, iteritems, suppress, dask_array_type\nfrom . import formatting\nfrom .utils import SortedKeysDict, not_implemented\n\n\nclass ImplementsArrayReduce(object):\n    @classmethod\n    def _reduce_method(cls, func, include_skipna, numeric_only):\n        if include_skipna:\n            def wrapped_func(self, dim=None, axis=None, skipna=None,\n                             keep_attrs=False, **kwargs):\n                return self.reduce(func, dim, axis, keep_attrs=keep_attrs,\n                                   skipna=skipna, allow_lazy=True, **kwargs)\n        else:\n            def wrapped_func(self, dim=None, axis=None, keep_attrs=False,\n                             **kwargs):\n                return self.reduce(func, dim, axis, keep_attrs=keep_attrs,\n                                   allow_lazy=True, **kwargs)\n        return wrapped_func\n\n    _reduce_extra_args_docstring = \\\n        \"\"\"dim : str or sequence of str, optional\n            Dimension(s) over which to apply `{name}`.\n        axis : int or sequence of int, optional\n            Axis(es) over which to apply `{name}`. Only one of the 'dim'\n            and 'axis' arguments can be supplied. If neither are supplied, then\n            `{name}` is calculated over axes.\"\"\"\n\n\nclass ImplementsDatasetReduce(object):\n    @classmethod\n    def _reduce_method(cls, func, include_skipna, numeric_only):\n        if include_skipna:\n            def wrapped_func(self, dim=None, keep_attrs=False, skipna=None,\n                             **kwargs):\n                return self.reduce(func, dim, keep_attrs, skipna=skipna,\n                                   numeric_only=numeric_only, allow_lazy=True,\n                                   **kwargs)\n        else:\n            def wrapped_func(self, dim=None, keep_attrs=False, **kwargs):\n                return self.reduce(func, dim, keep_attrs,\n                                   numeric_only=numeric_only, allow_lazy=True,\n                                   **kwargs)\n        return wrapped_func\n\n    _reduce_extra_args_docstring = \\\n        \"\"\"dim : str or sequence of str, optional\n            Dimension(s) over which to apply `func`.  By default `func` is\n            applied over all dimensions.\"\"\"\n\n\nclass ImplementsRollingArrayReduce(object):\n    @classmethod\n    def _reduce_method(cls, func):\n        def wrapped_func(self, **kwargs):\n            return self.reduce(func, **kwargs)\n        return wrapped_func\n\n    @classmethod\n    def _bottleneck_reduce(cls, func):\n        def wrapped_func(self, **kwargs):\n            from .dataarray import DataArray\n\n            if isinstance(self.obj.data, dask_array_type):\n                raise NotImplementedError(\n                    'Rolling window operation does not work with dask arrays')\n\n            # bottleneck doesn't allow min_count to be 0, although it should\n            # work the same as if min_count = 1\n            if self.min_periods is not None and self.min_periods == 0:\n                min_count = self.min_periods + 1\n            else:\n                min_count = self.min_periods\n\n            values = func(self.obj.data, window=self.window,\n                          min_count=min_count, axis=self._axis_num)\n\n            result = DataArray(values, self.obj.coords)\n\n            if self.center:\n                result = self._center_result(result)\n\n            return result\n        return wrapped_func\n\n    @classmethod\n    def _bottleneck_reduce_without_min_count(cls, func):\n        def wrapped_func(self, **kwargs):\n            from .dataarray import DataArray\n\n            if self.min_periods is not None:\n                raise ValueError('Rolling.median does not accept min_periods')\n\n            if isinstance(self.obj.data, dask_array_type):\n                raise NotImplementedError(\n                    'Rolling window operation does not work with dask arrays')\n\n            values = func(self.obj.data, window=self.window, axis=self._axis_num)\n\n            result = DataArray(values, self.obj.coords)\n\n            if self.center:\n                result = self._center_result(result)\n\n            return result\n        return wrapped_func\n\n\nclass AbstractArray(ImplementsArrayReduce):\n    def __bool__(self):\n        return bool(self.values)\n\n    # Python 3 uses __bool__, Python 2 uses __nonzero__\n    __nonzero__ = __bool__\n\n    def __float__(self):\n        return float(self.values)\n\n    def __int__(self):\n        return int(self.values)\n\n    def __complex__(self):\n        return complex(self.values)\n\n    def __long__(self):\n        return long(self.values)\n\n    def __array__(self, dtype=None):\n        return np.asarray(self.values, dtype=dtype)\n\n    def __repr__(self):\n        return formatting.array_repr(self)\n\n    def _iter(self):\n        for n in range(len(self)):\n            yield self[n]\n\n    def __iter__(self):\n        if self.ndim == 0:\n            raise TypeError('iteration over a 0-d array')\n        return self._iter()\n\n    @property\n    def T(self):\n        return self.transpose()\n\n    def get_axis_num(self, dim):\n        \"\"\"Return axis number(s) corresponding to dimension(s) in this array.\n\n        Parameters\n        ----------\n        dim : str or iterable of str\n            Dimension name(s) for which to lookup axes.\n\n        Returns\n        -------\n        int or tuple of int\n            Axis number or numbers corresponding to the given dimensions.\n        \"\"\"\n        if isinstance(dim, basestring):\n            return self._get_axis_num(dim)\n        else:\n            return tuple(self._get_axis_num(d) for d in dim)\n\n    def _get_axis_num(self, dim):\n        try:\n            return self.dims.index(dim)\n        except ValueError:\n            raise ValueError(\"%r not found in array dimensions %r\" %\n                             (dim, self.dims))\n\n\nclass AttrAccessMixin(object):\n    \"\"\"Mixin class that allows getting keys with attribute access\n    \"\"\"\n    _initialized = False\n\n    @property\n    def _attr_sources(self):\n        \"\"\"List of places to look-up items for attribute-style access\"\"\"\n        return [self, self.attrs]\n\n    def __getattr__(self, name):\n        if name != '__setstate__':\n            # this avoids an infinite loop when pickle looks for the\n            # __setstate__ attribute before the xarray object is initialized\n            for source in self._attr_sources:\n                with suppress(KeyError):\n                    return source[name]\n        raise AttributeError(\"%r object has no attribute %r\" %\n                             (type(self).__name__, name))\n\n    def __setattr__(self, name, value):\n        if self._initialized:\n            try:\n                # Allow setting instance variables if they already exist\n                # (e.g., _attrs). We use __getattribute__ instead of hasattr\n                # to avoid key lookups with attribute-style access.\n                self.__getattribute__(name)\n            except AttributeError:\n                raise AttributeError(\n                    \"cannot set attribute %r on a %r object. Use __setitem__ \"\n                    \"style assignment (e.g., `ds['name'] = ...`) instead to \"\n                    \"assign variables.\" % (name, type(self).__name__))\n        object.__setattr__(self, name, value)\n\n    def __dir__(self):\n        \"\"\"Provide method name lookup and completion. Only provide 'public'\n        methods.\n        \"\"\"\n        extra_attrs = [item for sublist in self._attr_sources\n                       for item in sublist]\n        return sorted(set(dir(type(self)) + extra_attrs))\n\n\nclass BaseDataObject(AttrAccessMixin):\n    def _calc_assign_results(self, kwargs):\n        results = SortedKeysDict()\n        for k, v in kwargs.items():\n            if callable(v):\n                results[k] = v(self)\n            else:\n                results[k] = v\n        return results\n\n    def assign_coords(self, **kwargs):\n        \"\"\"Assign new coordinates to this object, returning a new object\n        with all the original data in addition to the new coordinates.\n\n        Parameters\n        ----------\n        kwargs : keyword, value pairs\n            keywords are the variables names. If the values are callable, they\n            are computed on this object and assigned to new coordinate\n            variables. If the values are not callable, (e.g. a DataArray,\n            scalar, or array), they are simply assigned.\n\n        Returns\n        -------\n        assigned : same type as caller\n            A new object with the new coordinates in addition to the existing\n            data.\n\n        Notes\n        -----\n        Since ``kwargs`` is a dictionary, the order of your arguments may not\n        be preserved, and so the order of the new variables is not well\n        defined. Assigning multiple variables within the same ``assign_coords``\n        is possible, but you cannot reference other variables created within\n        the same ``assign_coords`` call.\n\n        See also\n        --------\n        Dataset.assign\n        \"\"\"\n        data = self.copy(deep=False)\n        results = self._calc_assign_results(kwargs)\n        data.coords.update(results)\n        return data\n\n    def pipe(self, func, *args, **kwargs):\n        \"\"\"\n        Apply func(self, *args, **kwargs)\n\n        This method replicates the pandas method of the same name.\n\n        Parameters\n        ----------\n        func : function\n            function to apply to this xarray object (Dataset/DataArray).\n            ``args``, and ``kwargs`` are passed into ``func``.\n            Alternatively a ``(callable, data_keyword)`` tuple where\n            ``data_keyword`` is a string indicating the keyword of\n            ``callable`` that expects the xarray object.\n        args : positional arguments passed into ``func``.\n        kwargs : a dictionary of keyword arguments passed into ``func``.\n\n        Returns\n        -------\n        object : the return type of ``func``.\n\n        Notes\n        -----\n\n        Use ``.pipe`` when chaining together functions that expect\n        xarray or pandas objects, e.g., instead of writing\n\n        >>> f(g(h(ds), arg1=a), arg2=b, arg3=c)\n\n        You can write\n\n        >>> (ds.pipe(h)\n        ...    .pipe(g, arg1=a)\n        ...    .pipe(f, arg2=b, arg3=c)\n        ... )\n\n        If you have a function that takes the data as (say) the second\n        argument, pass a tuple indicating which keyword expects the\n        data. For example, suppose ``f`` takes its data as ``arg2``:\n\n        >>> (ds.pipe(h)\n        ...    .pipe(g, arg1=a)\n        ...    .pipe((f, 'arg2'), arg1=a, arg3=c)\n        ...  )\n\n        See Also\n        --------\n        pandas.DataFrame.pipe\n        \"\"\"\n        if isinstance(func, tuple):\n            func, target = func\n            if target in kwargs:\n                msg = '%s is both the pipe target and a keyword argument' % target\n                raise ValueError(msg)\n            kwargs[target] = self\n            return func(*args, **kwargs)\n        else:\n            return func(self, *args, **kwargs)\n\n    def groupby(self, group, squeeze=True):\n        \"\"\"Returns a GroupBy object for performing grouped operations.\n\n        Parameters\n        ----------\n        group : str, DataArray or Coordinate\n            Array whose unique values should be used to group this array. If a\n            string, must be the name of a variable contained in this dataset.\n        squeeze : boolean, optional\n            If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n            controls whether the subarrays have a dimension of length 1 along\n            that dimension or if the dimension is squeezed out.\n\n        Returns\n        -------\n        grouped : GroupBy\n            A `GroupBy` object patterned after `pandas.GroupBy` that can be\n            iterated over in the form of `(unique_value, grouped_array)` pairs.\n        \"\"\"\n        if isinstance(group, basestring):\n            group = self[group]\n        return self.groupby_cls(self, group, squeeze=squeeze)\n\n    def rolling(self, min_periods=None, center=False, **kwarg):\n        \"\"\"Returns a Rolling object for performing moving window operations.\n\n        Parameters\n        ----------\n        min_periods : int, default None\n            Minimum number of observations in window required to have a value\n            (otherwise result is NA).\n        center : boolean, default False\n            Set the labels at the center of the window.\n        kwarg : dim=window\n            dim : str\n                Name of the dimension to create the rolling iterator\n                along (e.g., `time`).\n            window : int\n                Size of the moving window.\n\n        Returns\n        -------\n        rolling : type of input argument\n        \"\"\"\n\n        return self.rolling_cls(self, min_periods=min_periods,\n                                center=center, **kwarg)\n\n    def resample(self, freq, dim, how='mean', skipna=None, closed=None,\n                 label=None, base=0):\n        \"\"\"Resample this object to a new temporal resolution.\n\n        Handles both downsampling and upsampling. Upsampling with filling is\n        not yet supported; if any intervals contain no values in the original\n        object, they will be given the value ``NaN``.\n\n        Parameters\n        ----------\n        freq : str\n            String in the '#offset' to specify the step-size along the\n            resampled dimension, where '#' is an (optional) integer multipler\n            (default 1) and 'offset' is any pandas date offset alias. Examples\n            of valid offsets include:\n\n            * 'AS': year start\n            * 'QS-DEC': quarterly, starting on December 1\n            * 'MS': month start\n            * 'D': day\n            * 'H': hour\n            * 'Min': minute\n\n            The full list of these offset aliases is documented in pandas [1]_.\n        dim : str\n            Name of the dimension to resample along (e.g., 'time').\n        how : str or func, optional\n            Used for downsampling. If a string, ``how`` must be a valid\n            aggregation operation supported by xarray. Otherwise, ``how`` must be\n            a function that can be called like ``how(values, axis)`` to reduce\n            ndarray values along the given axis. Valid choices that can be\n            provided as a string include all the usual Dataset/DataArray\n            aggregations (``all``, ``any``, ``argmax``, ``argmin``, ``max``,\n            ``mean``, ``median``, ``min``, ``prod``, ``sum``, ``std`` and\n            ``var``), as well as ``first`` and ``last``.\n        skipna : bool, optional\n            Whether to skip missing values when aggregating in downsampling.\n        closed : 'left' or 'right', optional\n            Side of each interval to treat as closed.\n        label : 'left or 'right', optional\n            Side of each interval to use for labeling.\n        base : int, optionalt\n            For frequencies that evenly subdivide 1 day, the \"origin\" of the\n            aggregated intervals. For example, for '24H' frequency, base could\n            range from 0 through 23.\n\n        Returns\n        -------\n        resampled : same type as caller\n            This object resampled.\n\n        References\n        ----------\n\n        .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n        \"\"\"\n        from .dataarray import DataArray\n\n        RESAMPLE_DIM = '__resample_dim__'\n        if isinstance(dim, basestring):\n            dim = self[dim]\n        group = DataArray(dim, [(RESAMPLE_DIM, dim)], name=RESAMPLE_DIM)\n        time_grouper = pd.TimeGrouper(freq=freq, how=how, closed=closed,\n                                      label=label, base=base)\n        gb = self.groupby_cls(self, group, grouper=time_grouper)\n        if isinstance(how, basestring):\n            f = getattr(gb, how)\n            if how in ['first', 'last']:\n                result = f(skipna=skipna)\n            else:\n                result = f(dim=dim.name, skipna=skipna)\n        else:\n            result = gb.reduce(how, dim=dim.name)\n        result = result.rename({RESAMPLE_DIM: dim.name})\n        return result\n\n    def where(self, cond):\n        \"\"\"Return an object of the same shape with all entries where cond is\n        True and all other entries masked.\n\n        This operation follows the normal broadcasting and alignment rules that\n        xarray uses for binary arithmetic.\n\n        Parameters\n        ----------\n        cond : boolean DataArray or Dataset\n\n        Returns\n        -------\n        same type as caller\n\n        Examples\n        --------\n\n        >>> import numpy as np\n        >>> a = xr.DataArray(np.arange(25).reshape(5, 5), dims=('x', 'y'))\n        >>> a.where((a > 6) & (a < 18))\n        <xarray.DataArray (x: 5, y: 5)>\n        array([[ nan,  nan,  nan,  nan,  nan],\n               [ nan,  nan,   7.,   8.,   9.],\n               [ 10.,  11.,  12.,  13.,  14.],\n               [ 15.,  16.,  17.,  nan,  nan],\n               [ nan,  nan,  nan,  nan,  nan]])\n        Coordinates:\n          * y        (y) int64 0 1 2 3 4\n          * x        (x) int64 0 1 2 3 4\n        \"\"\"\n        return self._where(cond)\n\n    # this has no runtime function - these are listed so IDEs know these methods\n    # are defined and don't warn on these operations\n    __lt__ = __le__ =__ge__ = __gt__ = __add__ = __sub__ = __mul__ = \\\n    __truediv__ = __floordiv__ = __mod__ = __pow__ = __and__  = __xor__ = \\\n    __or__ = __div__ = __eq__ = __ne__ = not_implemented\n\n\ndef squeeze(xarray_obj, dims, dim=None):\n    \"\"\"Squeeze the dims of an xarray object.\"\"\"\n    if dim is None:\n        dim = [d for d, s in iteritems(dims) if s == 1]\n    else:\n        if isinstance(dim, basestring):\n            dim = [dim]\n        if any(dims[k] > 1 for k in dim):\n            raise ValueError('cannot select a dimension to squeeze out '\n                             'which has length greater than one')\n    return xarray_obj.isel(**dict((d, 0) for d in dim))\n\n\ndef _maybe_promote(dtype):\n    \"\"\"Simpler equivalent of pandas.core.common._maybe_promote\"\"\"\n    # N.B. these casting rules should match pandas\n    if np.issubdtype(dtype, float):\n        fill_value = np.nan\n    elif np.issubdtype(dtype, int):\n        # convert to floating point so NaN is valid\n        dtype = float\n        fill_value = np.nan\n    elif np.issubdtype(dtype, complex):\n        fill_value = np.nan + np.nan * 1j\n    elif np.issubdtype(dtype, np.datetime64):\n        fill_value = np.datetime64('NaT')\n    elif np.issubdtype(dtype, np.timedelta64):\n        fill_value = np.timedelta64('NaT')\n    else:\n        dtype = object\n        fill_value = np.nan\n    return np.dtype(dtype), fill_value\n\n\ndef _possibly_convert_objects(values):\n    \"\"\"Convert arrays of datetime.datetime and datetime.timedelta objects into\n    datetime64 and timedelta64, according to the pandas convention.\n    \"\"\"\n    return np.asarray(pd.Series(values.ravel())).reshape(values.shape)\n\n\ndef _get_fill_value(dtype):\n    \"\"\"Return a fill value that appropriately promotes types when used with\n    np.concatenate\n    \"\"\"\n    _, fill_value = _maybe_promote(dtype)\n    return fill_value\n\n\ndef _full_like_dataarray(arr, keep_attrs=False, fill_value=None):\n    \"\"\"empty DataArray\"\"\"\n    from .dataarray import DataArray\n\n    attrs = arr.attrs if keep_attrs else {}\n\n    if fill_value is None:\n        values = np.empty_like(arr)\n    elif fill_value is True:\n        dtype, fill_value = _maybe_promote(arr.dtype)\n        values = np.full_like(arr, fill_value=fill_value, dtype=dtype)\n    else:\n        dtype, _ = _maybe_promote(np.array(fill_value).dtype)\n        values = np.full_like(arr, fill_value=fill_value, dtype=dtype)\n\n    return DataArray(values, dims=arr.dims, coords=arr.coords, attrs=attrs)\n\n\ndef _full_like(xray_obj, keep_attrs=False, fill_value=None):\n    \"\"\"Return a new object with the same shape and type as a given object.\n\n    Parameters\n    ----------\n    xray_obj : DataArray or Dataset\n        Return a full object with the same shape/dims/coords/attrs.\n            `func` is calculated over all dimension for each group item.\n    keep_attrs : bool, optional\n        If True, the datasets's attributes (`attrs`) will be copied from\n        the original object to the new one.  If False (default), the new\n        object will be returned without attributes.\n    fill_value : scalar, optional\n        Value to fill DataArray(s) with before returning.\n\n    Returns\n    -------\n    out : same as xray_obj\n        New object with the same shape and type as a given object.\n    \"\"\"\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    if isinstance(xray_obj, Dataset):\n        attrs = xray_obj.attrs if keep_attrs else {}\n\n        return Dataset(dict((k, _full_like_dataarray(v, keep_attrs=keep_attrs,\n                                                     fill_value=fill_value))\n                            for k, v in iteritems(xray_obj.data_vars)),\n                       name=xray_obj.name, attrs=attrs)\n    elif isinstance(xray_obj, DataArray):\n        return _full_like_dataarray(xray_obj, keep_attrs=keep_attrs,\n                                    fill_value=fill_value)\n"
    },
    {
      "filename": "xarray/core/utils.py",
      "content": "\"\"\"Internal utilties; not for external use\n\"\"\"\nimport contextlib\nimport datetime\nimport functools\nimport itertools\nimport re\nimport warnings\nfrom collections import Mapping, MutableMapping\n\nimport numpy as np\nimport pandas as pd\n\nfrom . import ops\nfrom .pycompat import iteritems, OrderedDict\n\n\ndef alias_warning(old_name, new_name, stacklevel=3):  # pragma: no cover\n    warnings.warn('%s has been deprecated and renamed to %s'\n                  % (old_name, new_name),\n                  FutureWarning, stacklevel=stacklevel)\n\n\ndef function_alias(obj, old_name):  # pragma: no cover\n    @functools.wraps(obj)\n    def wrapper(*args, **kwargs):\n        alias_warning(old_name, obj.__name__)\n        return obj(*args, **kwargs)\n    return wrapper\n\n\ndef class_alias(obj, old_name):  # pragma: no cover\n    class Wrapper(obj):\n        def __new__(cls, *args, **kwargs):\n            alias_warning(old_name, obj.__name__)\n            return super(Wrapper, cls).__new__(cls, *args, **kwargs)\n    Wrapper.__name__ = obj.__name__\n    return Wrapper\n\n\ndef safe_cast_to_index(array):\n    \"\"\"Given an array, safely cast it to a pandas.Index.\n\n    If it is already a pandas.Index, return it unchanged.\n\n    Unlike pandas.Index, if the array has dtype=object or dtype=timedelta64,\n    this function will not attempt to do automatic type conversion but will\n    always return an index with dtype=object.\n    \"\"\"\n    if isinstance(array, pd.Index):\n        index = array\n    elif hasattr(array, 'to_index'):\n        index = array.to_index()\n    else:\n        kwargs = {}\n        if hasattr(array, 'dtype') and array.dtype.kind == 'O':\n            kwargs['dtype'] = object\n        index = pd.Index(np.asarray(array), **kwargs)\n    return index\n\n\ndef maybe_wrap_array(original, new_array):\n    \"\"\"Wrap a transformed array with __array_wrap__ is it can be done safely.\n\n    This lets us treat arbitrary functions that take and return ndarray objects\n    like ufuncs, as long as they return an array with the same shape.\n    \"\"\"\n    # in case func lost array's metadata\n    if isinstance(new_array, np.ndarray) and new_array.shape == original.shape:\n        return original.__array_wrap__(new_array)\n    else:\n        return new_array\n\n\ndef equivalent(first, second):\n    \"\"\"Compare two objects for equivalence (identity or equality), using\n    array_equiv if either object is an ndarray\n    \"\"\"\n    if isinstance(first, np.ndarray) or isinstance(second, np.ndarray):\n        return ops.array_equiv(first, second)\n    else:\n        return first is second or first == second\n\n\ndef peek_at(iterable):\n    \"\"\"Returns the first value from iterable, as well as a new iterable with\n    the same content as the original iterable\n    \"\"\"\n    gen = iter(iterable)\n    peek = next(gen)\n    return peek, itertools.chain([peek], gen)\n\n\ndef update_safety_check(first_dict, second_dict, compat=equivalent):\n    \"\"\"Check the safety of updating one dictionary with another.\n\n    Raises ValueError if dictionaries have non-compatible values for any key,\n    where compatibility is determined by identity (they are the same item) or\n    the `compat` function.\n\n    Parameters\n    ----------\n    first_dict, second_dict : dict-like\n        All items in the second dictionary are checked against for conflicts\n        against items in the first dictionary.\n    compat : function, optional\n        Binary operator to determine if two values are compatible. By default,\n        checks for equivalence.\n    \"\"\"\n    for k, v in iteritems(second_dict):\n        if k in first_dict and not compat(v, first_dict[k]):\n            raise ValueError('unsafe to merge dictionaries without '\n                             'overriding values; conflicting key %r' % k)\n\n\ndef remove_incompatible_items(first_dict, second_dict, compat=equivalent):\n    \"\"\"Remove incompatible items from the first dictionary in-place.\n\n    Items are retained if their keys are found in both dictionaries and the\n    values are compatible.\n\n    Parameters\n    ----------\n    first_dict, second_dict : dict-like\n        Mappings to merge.\n    compat : function, optional\n        Binary operator to determine if two values are compatible. By default,\n        checks for equivalence.\n    \"\"\"\n    for k in list(first_dict):\n        if (k not in second_dict or\n            (k in second_dict and\n                not compat(first_dict[k], second_dict[k]))):\n            del first_dict[k]\n\n\ndef is_dict_like(value):\n    return hasattr(value, '__getitem__') and hasattr(value, 'keys')\n\n\ndef is_full_slice(value):\n    return isinstance(value, slice) and value == slice(None)\n\n\ndef combine_pos_and_kw_args(pos_kwargs, kw_kwargs, func_name):\n    if pos_kwargs is not None:\n        if not is_dict_like(pos_kwargs):\n            raise ValueError('the first argument to .%s must be a dictionary'\n                             % func_name)\n        if kw_kwargs:\n            raise ValueError('cannot specify both keyword and positional '\n                             'arguments to .%s' % func_name)\n        return pos_kwargs\n    else:\n        return kw_kwargs\n\n\n_SCALAR_TYPES = (datetime.datetime, datetime.date, datetime.timedelta)\n\n\ndef is_scalar(value):\n    \"\"\"np.isscalar only works on primitive numeric types and (bizarrely)\n    excludes 0-d ndarrays; this version does more comprehensive checks\n    \"\"\"\n    if hasattr(value, 'ndim'):\n        return value.ndim == 0\n    return (np.isscalar(value) or\n            isinstance(value, _SCALAR_TYPES) or\n            value is None)\n\n\ndef is_valid_numpy_dtype(dtype):\n    try:\n        np.dtype(dtype)\n    except (TypeError, ValueError):\n        return False\n    else:\n        return True\n\n\ndef tuple_to_0darray(value):\n    result = np.empty((1,), dtype=object)\n    result[:] = [value]\n    result.shape = ()\n    return result\n\n\ndef dict_equiv(first, second, compat=equivalent):\n    \"\"\"Test equivalence of two dict-like objects. If any of the values are\n    numpy arrays, compare them correctly.\n\n    Parameters\n    ----------\n    first, second : dict-like\n        Dictionaries to compare for equality\n    compat : function, optional\n        Binary operator to determine if two values are compatible. By default,\n        checks for equivalence.\n\n    Returns\n    -------\n    equals : bool\n        True if the dictionaries are equal\n    \"\"\"\n    for k in first:\n        if k not in second or not compat(first[k], second[k]):\n            return False\n    for k in second:\n        if k not in first:\n            return False\n    return True\n\n\ndef ordered_dict_intersection(first_dict, second_dict, compat=equivalent):\n    \"\"\"Return the intersection of two dictionaries as a new OrderedDict.\n\n    Items are retained if their keys are found in both dictionaries and the\n    values are compatible.\n\n    Parameters\n    ----------\n    first_dict, second_dict : dict-like\n        Mappings to merge.\n    compat : function, optional\n        Binary operator to determine if two values are compatible. By default,\n        checks for equivalence.\n\n    Returns\n    -------\n    intersection : OrderedDict\n        Intersection of the contents.\n    \"\"\"\n    new_dict = OrderedDict(first_dict)\n    remove_incompatible_items(new_dict, second_dict, compat)\n    return new_dict\n\n\nclass SingleSlotPickleMixin(object):\n    \"\"\"Mixin class to add the ability to pickle objects whose state is defined\n    by a single __slots__ attribute. Only necessary under Python 2.\n    \"\"\"\n    def __getstate__(self):\n        return getattr(self, self.__slots__[0])\n\n    def __setstate__(self, state):\n        setattr(self, self.__slots__[0], state)\n\n\nclass Frozen(Mapping, SingleSlotPickleMixin):\n    \"\"\"Wrapper around an object implementing the mapping interface to make it\n    immutable. If you really want to modify the mapping, the mutable version is\n    saved under the `mapping` attribute.\n    \"\"\"\n    __slots__ = ['mapping']\n\n    def __init__(self, mapping):\n        self.mapping = mapping\n\n    def __getitem__(self, key):\n        return self.mapping[key]\n\n    def __iter__(self):\n        return iter(self.mapping)\n\n    def __len__(self):\n        return len(self.mapping)\n\n    def __contains__(self, key):\n        return key in self.mapping\n\n    def __repr__(self):\n        return '%s(%r)' % (type(self).__name__, self.mapping)\n\n\ndef FrozenOrderedDict(*args, **kwargs):\n    return Frozen(OrderedDict(*args, **kwargs))\n\n\nclass SortedKeysDict(MutableMapping, SingleSlotPickleMixin):\n    \"\"\"An wrapper for dictionary-like objects that always iterates over its\n    items in sorted order by key but is otherwise equivalent to the underlying\n    mapping.\n    \"\"\"\n    __slots__ = ['mapping']\n\n    def __init__(self, mapping=None):\n        self.mapping = {} if mapping is None else mapping\n\n    def __getitem__(self, key):\n        return self.mapping[key]\n\n    def __setitem__(self, key, value):\n        self.mapping[key] = value\n\n    def __delitem__(self, key):\n        del self.mapping[key]\n\n    def __iter__(self):\n        return iter(sorted(self.mapping))\n\n    def __len__(self):\n        return len(self.mapping)\n\n    def __contains__(self, key):\n        return key in self.mapping\n\n    def __repr__(self):\n        return '%s(%r)' % (type(self).__name__, self.mapping)\n\n    def copy(self):\n        return type(self)(self.mapping.copy())\n\n\nclass ChainMap(MutableMapping, SingleSlotPickleMixin):\n    \"\"\"Partial backport of collections.ChainMap from Python>=3.3\n\n    Don't return this from any public APIs, since some of the public methods\n    for a MutableMapping are missing (they will raise a NotImplementedError)\n    \"\"\"\n    __slots__ = ['maps']\n\n    def __init__(self, *maps):\n        self.maps = maps\n\n    def __getitem__(self, key):\n        for mapping in self.maps:\n            try:\n                return mapping[key]\n            except KeyError:\n                pass\n        raise KeyError(key)\n\n    def __setitem__(self, key, value):\n        self.maps[0][key] = value\n\n    def __delitem__(self, value):  # pragma: no cover\n        raise NotImplementedError\n\n    def __iter__(self):\n        seen = set()\n        for mapping in self.maps:\n            for item in mapping:\n                if item not in seen:\n                    yield item\n                    seen.add(item)\n\n    def __len__(self):\n        raise len(iter(self))\n\n\nclass NdimSizeLenMixin(object):\n    \"\"\"Mixin class that extends a class that defines a ``shape`` property to\n    one that also defines ``ndim``, ``size`` and ``__len__``.\n    \"\"\"\n    @property\n    def ndim(self):\n        return len(self.shape)\n\n    @property\n    def size(self):\n        # cast to int so that shape = () gives size = 1\n        return int(np.prod(self.shape))\n\n    def __len__(self):\n        try:\n            return self.shape[0]\n        except IndexError:\n            raise TypeError('len() of unsized object')\n\n\nclass NDArrayMixin(NdimSizeLenMixin):\n    \"\"\"Mixin class for making wrappers of N-dimensional arrays that conform to\n    the ndarray interface required for the data argument to Variable objects.\n\n    A subclass should set the `array` property and override one or more of\n    `dtype`, `shape` and `__getitem__`.\n    \"\"\"\n    @property\n    def dtype(self):\n        return self.array.dtype\n\n    @property\n    def shape(self):\n        return self.array.shape\n\n    def __array__(self, dtype=None):\n        return np.asarray(self[...], dtype=dtype)\n\n    def __getitem__(self, key):\n        return self.array[key]\n\n    def __repr__(self):\n        return '%s(array=%r)' % (type(self).__name__, self.array)\n\n\n@contextlib.contextmanager\ndef close_on_error(f):\n    \"\"\"Context manager to ensure that a file opened by xarray is closed if an\n    exception is raised before the user sees the file object.\n    \"\"\"\n    try:\n        yield\n    except Exception:\n        f.close()\n        raise\n\n\ndef is_remote_uri(path):\n    return bool(re.search('^https?\\://', path))\n\n\ndef is_uniform_spaced(arr, **kwargs):\n    \"\"\"Return True if values of an array are uniformly spaced and sorted.\n\n    >>> is_uniform_spaced(range(5))\n    True\n    >>> is_uniform_spaced([-4, 0, 100])\n    False\n\n    kwargs are additional arguments to ``np.isclose``\n    \"\"\"\n    arr = np.array(arr, dtype=float)\n    diffs = np.diff(arr)\n    return np.isclose(diffs.min(), diffs.max(), **kwargs)\n\n\ndef hashable(v):\n    \"\"\"Determine whether `v` can be hashed.\"\"\"\n    try:\n        hash(v)\n    except TypeError:\n        return False\n    return True\n\n\ndef not_implemented(*args, **kwargs):\n    return NotImplemented\n"
    }
  ]
}