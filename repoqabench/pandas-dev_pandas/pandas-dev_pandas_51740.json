{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "51740",
  "issue_description": "# STYLE enable ruff TCH\n\nThis code actually may make a difference to performance: \r\n\r\nTask here is:\r\n0. make sure you've followed the steps in https://pandas.pydata.org/docs/dev/development/contributing.html and have set up your development environment\r\n1. remove 3-5 of the lines in\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/d3854657e1c9842b8264b6cc90488ccfed0cd6a2/pyproject.toml#L293-L326\r\n\r\n2. run `pre-commit run ruff --all-files`, and fix up the issues it reports. You can see https://github.com/pandas-dev/pandas/pull/51687 for some examples of how to do this\r\n3. if `pre-commit run ruff --all-files` passes, then stage, commit, push, open pull request, celebrate\r\n\r\nPlease don't comment \"take\" on this issue (as multiple people can be assigned), and don't ask for permission to work on it. Instead, just leave a comment letting people know which ",
  "issue_comments": [
    {
      "id": 1453034075,
      "user": "t0uch9r455",
      "body": "I am working on pandas/core/arrays"
    },
    {
      "id": 1453928622,
      "user": "potap75",
      "body": "Doing pandas/core/io/*"
    },
    {
      "id": 1454085056,
      "user": "potap75",
      "body": "I got this when trying to do a pull request: \r\n<img width=\"1339\" alt=\"Screenshot 2023-03-04 at 12 22 52 AM\" src=\"https://user-images.githubusercontent.com/3213886/222823133-e21ab92a-2e2a-4c1e-9d53-4a6ab6e18eb4.png\">\r\n\r\n\r\n"
    },
    {
      "id": 1454346141,
      "user": "luke396",
      "body": "I am working on \r\n```\r\n\"pandas/core/indexers/*\" \r\n\"pandas/core/indexes/*\"\r\n\"pandas/core/internals/*\" \r\n\"pandas/core/groupby/*\"\r\n\"pandas/core/methods/*\"\r\n```"
    },
    {
      "id": 1454702299,
      "user": "luke396",
      "body": "Continue to working on\r\n```\r\n\"pandas/core/array_algos/*\"\r\n\"pandas/core/dtypes/*\"\r\n\"pandas/core/generic.py\"\r\n\"pandas/core/frame.py\"\r\n\"pandas/core/series.py\"\r\n```"
    },
    {
      "id": 1454940143,
      "user": "v-mcoutinho",
      "body": "I did \"pandas/_libs/*\""
    },
    {
      "id": 1455105338,
      "user": "DerTimonius",
      "body": "I will try `pandas/core/common.py`!"
    },
    {
      "id": 1455165146,
      "user": "Rylie-W",
      "body": "Trying pandas/core/resample.py"
    },
    {
      "id": 1455535440,
      "user": "DerTimonius",
      "body": "I'll go for `pandas/core/util/*` next. Also @MarcoGorelli it looks like my PR closed this issue..."
    },
    {
      "id": 1457282143,
      "user": "luke396",
      "body": "Working on \r\n```\r\n\"pandas/core/reshape/*\"\r\n\"pandas/core/strings/*\" \r\n\"pandas/core/tools/*\"\r\n\"pandas/core/window/*\"\r\n\"pandas/io/*\"\r\n```"
    },
    {
      "id": 1457788240,
      "user": "ErdiTk",
      "body": "Working on:\r\n```\r\n \"pandas/core/apply.py\" = [\"TCH\"] \r\n \"pandas/core/base.py\" = [\"TCH\"] \r\n \"pandas/core/algorithms.py\" = [\"TCH\"] \r\n \"pandas/core/ops/*\" = [\"TCH\"] \r\n \"pandas/core/sorting.py\" = [\"TCH\"] \r\n```"
    },
    {
      "id": 1459082166,
      "user": "luke396",
      "body": "Working on, and  @MarcoGorelli , maybe need to reopen this issue again, when #51827 merged.\r\n```\r\n\"pandas/core/construction.py\" = [\"TCH\"] \r\n\"pandas/core/missing.py\" = [\"TCH\"] \r\n\"pandas/tseries/*\" = [\"TCH\"] \r\n\"pandas/tests/*\" = [\"TCH\"] \r\n\"pandas/plotting/*\" = [\"TCH\"] \r\n```"
    },
    {
      "id": 1461278063,
      "user": "Ramyahkay",
      "body": "Working on : `\"pandas/core/nanops.py\" = [\"TCH\"] `"
    },
    {
      "id": 1464308117,
      "user": "ahmad-04",
      "body": "I am working on \"pandas/util/*\" = [\"TCH\"]\r\n\r\n"
    },
    {
      "id": 1465251139,
      "user": "ahmad-04",
      "body": "hey @Ramyahkay are you working on \"pandas/core/nanops.py\" = [\"TCH\"]  ??"
    },
    {
      "id": 1465257334,
      "user": "Ramyahkay",
      "body": "> hey @Ramyahkay are you working on \"pandas/core/nanops.py\" = [\"TCH\"] ??\r\n\r\nYes, I am still working on it. "
    },
    {
      "id": 1480922757,
      "user": "MarcoGorelli",
      "body": "@ahmad2901 (or anyone else) - feel free to submit a PR and close off this issue, at this point I presume they're no longer working on it"
    },
    {
      "id": 1480925466,
      "user": "ahmad-04",
      "body": "Sure I will do it.\n\n"
    }
  ],
  "text_context": "# STYLE enable ruff TCH\n\nThis code actually may make a difference to performance: \r\n\r\nTask here is:\r\n0. make sure you've followed the steps in https://pandas.pydata.org/docs/dev/development/contributing.html and have set up your development environment\r\n1. remove 3-5 of the lines in\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/d3854657e1c9842b8264b6cc90488ccfed0cd6a2/pyproject.toml#L293-L326\r\n\r\n2. run `pre-commit run ruff --all-files`, and fix up the issues it reports. You can see https://github.com/pandas-dev/pandas/pull/51687 for some examples of how to do this\r\n3. if `pre-commit run ruff --all-files` passes, then stage, commit, push, open pull request, celebrate\r\n\r\nPlease don't comment \"take\" on this issue (as multiple people can be assigned), and don't ask for permission to work on it. Instead, just leave a comment letting people know which \n\nI am working on pandas/core/arrays\n\nDoing pandas/core/io/*\n\nI got this when trying to do a pull request: \r\n<img width=\"1339\" alt=\"Screenshot 2023-03-04 at 12 22 52 AM\" src=\"https://user-images.githubusercontent.com/3213886/222823133-e21ab92a-2e2a-4c1e-9d53-4a6ab6e18eb4.png\">\r\n\r\n\r\n\n\nI am working on \r\n```\r\n\"pandas/core/indexers/*\" \r\n\"pandas/core/indexes/*\"\r\n\"pandas/core/internals/*\" \r\n\"pandas/core/groupby/*\"\r\n\"pandas/core/methods/*\"\r\n```\n\nContinue to working on\r\n```\r\n\"pandas/core/array_algos/*\"\r\n\"pandas/core/dtypes/*\"\r\n\"pandas/core/generic.py\"\r\n\"pandas/core/frame.py\"\r\n\"pandas/core/series.py\"\r\n```\n\nI did \"pandas/_libs/*\"\n\nI will try `pandas/core/common.py`!\n\nTrying pandas/core/resample.py\n\nI'll go for `pandas/core/util/*` next. Also @MarcoGorelli it looks like my PR closed this issue...\n\nWorking on \r\n```\r\n\"pandas/core/reshape/*\"\r\n\"pandas/core/strings/*\" \r\n\"pandas/core/tools/*\"\r\n\"pandas/core/window/*\"\r\n\"pandas/io/*\"\r\n```\n\nWorking on:\r\n```\r\n \"pandas/core/apply.py\" = [\"TCH\"] \r\n \"pandas/core/base.py\" = [\"TCH\"] \r\n \"pandas/core/algorithms.py\" = [\"TCH\"] \r\n \"pandas/core/ops/*\" = [\"TCH\"] \r\n \"pandas/core/sorting.py\" = [\"TCH\"] \r\n```\n\nWorking on, and  @MarcoGorelli , maybe need to reopen this issue again, when #51827 merged.\r\n```\r\n\"pandas/core/construction.py\" = [\"TCH\"] \r\n\"pandas/core/missing.py\" = [\"TCH\"] \r\n\"pandas/tseries/*\" = [\"TCH\"] \r\n\"pandas/tests/*\" = [\"TCH\"] \r\n\"pandas/plotting/*\" = [\"TCH\"] \r\n```\n\nWorking on : `\"pandas/core/nanops.py\" = [\"TCH\"] `\n\nI am working on \"pandas/util/*\" = [\"TCH\"]\r\n\r\n\n\nhey @Ramyahkay are you working on \"pandas/core/nanops.py\" = [\"TCH\"]  ??\n\n> hey @Ramyahkay are you working on \"pandas/core/nanops.py\" = [\"TCH\"] ??\r\n\r\nYes, I am still working on it. \n\n@ahmad2901 (or anyone else) - feel free to submit a PR and close off this issue, at this point I presume they're no longer working on it\n\nSure I will do it.\n\n",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/51687",
  "code_context": [
    {
      "filename": "pandas/_config/config.py",
      "content": "\"\"\"\nThe config module holds package-wide configurables and provides\na uniform API for working with them.\n\nOverview\n========\n\nThis module supports the following requirements:\n- options are referenced using keys in dot.notation, e.g. \"x.y.option - z\".\n- keys are case-insensitive.\n- functions should accept partial/regex keys, when unambiguous.\n- options can be registered by modules at import time.\n- options can be registered at init-time (via core.config_init)\n- options have a default value, and (optionally) a description and\n  validation function associated with them.\n- options can be deprecated, in which case referencing them\n  should produce a warning.\n- deprecated options can optionally be rerouted to a replacement\n  so that accessing a deprecated option reroutes to a differently\n  named option.\n- options can be reset to their default value.\n- all option can be reset to their default value at once.\n- all options in a certain sub - namespace can be reset at once.\n- the user can set / get / reset or ask for the description of an option.\n- a developer can register and mark an option as deprecated.\n- you can register a callback to be invoked when the option value\n  is set or reset. Changing the stored value is considered misuse, but\n  is not verboten.\n\nImplementation\n==============\n\n- Data is stored using nested dictionaries, and should be accessed\n  through the provided API.\n\n- \"Registered options\" and \"Deprecated options\" have metadata associated\n  with them, which are stored in auxiliary dictionaries keyed on the\n  fully-qualified key, e.g. \"x.y.z.option\".\n\n- the config_init module is imported by the package's __init__.py file.\n  placing any register_option() calls there will ensure those options\n  are available as soon as pandas is loaded. If you use register_option\n  in a module, it will only be available after that module is imported,\n  which you should be aware of.\n\n- `config_prefix` is a context_manager (for use with the `with` keyword)\n  which can save developers some typing, see the docstring.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom contextlib import (\n    ContextDecorator,\n    contextmanager,\n)\nimport re\nfrom typing import (\n    Any,\n    Callable,\n    Generator,\n    Generic,\n    Iterable,\n    NamedTuple,\n    cast,\n)\nimport warnings\n\nfrom pandas._typing import F  # noqa: TCH001\nfrom pandas._typing import T  # noqa: TCH001\nfrom pandas.util._exceptions import find_stack_level\n\n\nclass DeprecatedOption(NamedTuple):\n    key: str\n    msg: str | None\n    rkey: str | None\n    removal_ver: str | None\n\n\nclass RegisteredOption(NamedTuple):\n    key: str\n    defval: object\n    doc: str\n    validator: Callable[[object], Any] | None\n    cb: Callable[[str], Any] | None\n\n\n# holds deprecated option metadata\n_deprecated_options: dict[str, DeprecatedOption] = {}\n\n# holds registered option metadata\n_registered_options: dict[str, RegisteredOption] = {}\n\n# holds the current values for registered options\n_global_config: dict[str, Any] = {}\n\n# keys which have a special meaning\n_reserved_keys: list[str] = [\"all\"]\n\n\nclass OptionError(AttributeError, KeyError):\n    \"\"\"\n    Exception raised for pandas.options.\n\n    Backwards compatible with KeyError checks.\n    \"\"\"\n\n\n#\n# User API\n\n\ndef _get_single_key(pat: str, silent: bool) -> str:\n    keys = _select_options(pat)\n    if len(keys) == 0:\n        if not silent:\n            _warn_if_deprecated(pat)\n        raise OptionError(f\"No such keys(s): {repr(pat)}\")\n    if len(keys) > 1:\n        raise OptionError(\"Pattern matched multiple keys\")\n    key = keys[0]\n\n    if not silent:\n        _warn_if_deprecated(key)\n\n    key = _translate_key(key)\n\n    return key\n\n\ndef _get_option(pat: str, silent: bool = False) -> Any:\n    key = _get_single_key(pat, silent)\n\n    # walk the nested dict\n    root, k = _get_root(key)\n    return root[k]\n\n\ndef _set_option(*args, **kwargs) -> None:\n    # must at least 1 arg deal with constraints later\n    nargs = len(args)\n    if not nargs or nargs % 2 != 0:\n        raise ValueError(\"Must provide an even number of non-keyword arguments\")\n\n    # default to false\n    silent = kwargs.pop(\"silent\", False)\n\n    if kwargs:\n        kwarg = list(kwargs.keys())[0]\n        raise TypeError(f'_set_option() got an unexpected keyword argument \"{kwarg}\"')\n\n    for k, v in zip(args[::2], args[1::2]):\n        key = _get_single_key(k, silent)\n\n        o = _get_registered_option(key)\n        if o and o.validator:\n            o.validator(v)\n\n        # walk the nested dict\n        root, k = _get_root(key)\n        root[k] = v\n\n        if o.cb:\n            if silent:\n                with warnings.catch_warnings(record=True):\n                    o.cb(key)\n            else:\n                o.cb(key)\n\n\ndef _describe_option(pat: str = \"\", _print_desc: bool = True) -> str | None:\n    keys = _select_options(pat)\n    if len(keys) == 0:\n        raise OptionError(\"No such keys(s)\")\n\n    s = \"\\n\".join([_build_option_description(k) for k in keys])\n\n    if _print_desc:\n        print(s)\n        return None\n    return s\n\n\ndef _reset_option(pat: str, silent: bool = False) -> None:\n    keys = _select_options(pat)\n\n    if len(keys) == 0:\n        raise OptionError(\"No such keys(s)\")\n\n    if len(keys) > 1 and len(pat) < 4 and pat != \"all\":\n        raise ValueError(\n            \"You must specify at least 4 characters when \"\n            \"resetting multiple keys, use the special keyword \"\n            '\"all\" to reset all the options to their default value'\n        )\n\n    for k in keys:\n        _set_option(k, _registered_options[k].defval, silent=silent)\n\n\ndef get_default_val(pat: str):\n    key = _get_single_key(pat, silent=True)\n    return _get_registered_option(key).defval\n\n\nclass DictWrapper:\n    \"\"\"provide attribute-style access to a nested dict\"\"\"\n\n    def __init__(self, d: dict[str, Any], prefix: str = \"\") -> None:\n        object.__setattr__(self, \"d\", d)\n        object.__setattr__(self, \"prefix\", prefix)\n\n    def __setattr__(self, key: str, val: Any) -> None:\n        prefix = object.__getattribute__(self, \"prefix\")\n        if prefix:\n            prefix += \".\"\n        prefix += key\n        # you can't set new keys\n        # can you can't overwrite subtrees\n        if key in self.d and not isinstance(self.d[key], dict):\n            _set_option(prefix, val)\n        else:\n            raise OptionError(\"You can only set the value of existing options\")\n\n    def __getattr__(self, key: str):\n        prefix = object.__getattribute__(self, \"prefix\")\n        if prefix:\n            prefix += \".\"\n        prefix += key\n        try:\n            v = object.__getattribute__(self, \"d\")[key]\n        except KeyError as err:\n            raise OptionError(\"No such option\") from err\n        if isinstance(v, dict):\n            return DictWrapper(v, prefix)\n        else:\n            return _get_option(prefix)\n\n    def __dir__(self) -> Iterable[str]:\n        return list(self.d.keys())\n\n\n# For user convenience,  we'd like to have the available options described\n# in the docstring. For dev convenience we'd like to generate the docstrings\n# dynamically instead of maintaining them by hand. To this, we use the\n# class below which wraps functions inside a callable, and converts\n# __doc__ into a property function. The doctsrings below are templates\n# using the py2.6+ advanced formatting syntax to plug in a concise list\n# of options, and option descriptions.\n\n\nclass CallableDynamicDoc(Generic[T]):\n    def __init__(self, func: Callable[..., T], doc_tmpl: str) -> None:\n        self.__doc_tmpl__ = doc_tmpl\n        self.__func__ = func\n\n    def __call__(self, *args, **kwds) -> T:\n        return self.__func__(*args, **kwds)\n\n    # error: Signature of \"__doc__\" incompatible with supertype \"object\"\n    @property\n    def __doc__(self) -> str:  # type: ignore[override]\n        opts_desc = _describe_option(\"all\", _print_desc=False)\n        opts_list = pp_options_list(list(_registered_options.keys()))\n        return self.__doc_tmpl__.format(opts_desc=opts_desc, opts_list=opts_list)\n\n\n_get_option_tmpl = \"\"\"\nget_option(pat)\n\nRetrieves the value of the specified option.\n\nAvailable options:\n\n{opts_list}\n\nParameters\n----------\npat : str\n    Regexp which should match a single option.\n    Note: partial matches are supported for convenience, but unless you use the\n    full option name (e.g. x.y.z.option_name), your code may break in future\n    versions if new options with similar names are introduced.\n\nReturns\n-------\nresult : the value of the option\n\nRaises\n------\nOptionError : if no such option exists\n\nNotes\n-----\nPlease reference the :ref:`User Guide <options>` for more information.\n\nThe available options with its descriptions:\n\n{opts_desc}\n\"\"\"\n\n_set_option_tmpl = \"\"\"\nset_option(pat, value)\n\nSets the value of the specified option.\n\nAvailable options:\n\n{opts_list}\n\nParameters\n----------\npat : str\n    Regexp which should match a single option.\n    Note: partial matches are supported for convenience, but unless you use the\n    full option name (e.g. x.y.z.option_name), your code may break in future\n    versions if new options with similar names are introduced.\nvalue : object\n    New value of option.\n\nReturns\n-------\nNone\n\nRaises\n------\nOptionError if no such option exists\n\nNotes\n-----\nPlease reference the :ref:`User Guide <options>` for more information.\n\nThe available options with its descriptions:\n\n{opts_desc}\n\"\"\"\n\n_describe_option_tmpl = \"\"\"\ndescribe_option(pat, _print_desc=False)\n\nPrints the description for one or more registered options.\n\nCall with no arguments to get a listing for all registered options.\n\nAvailable options:\n\n{opts_list}\n\nParameters\n----------\npat : str\n    Regexp pattern. All matching keys will have their description displayed.\n_print_desc : bool, default True\n    If True (default) the description(s) will be printed to stdout.\n    Otherwise, the description(s) will be returned as a unicode string\n    (for testing).\n\nReturns\n-------\nNone by default, the description(s) as a unicode string if _print_desc\nis False\n\nNotes\n-----\nPlease reference the :ref:`User Guide <options>` for more information.\n\nThe available options with its descriptions:\n\n{opts_desc}\n\"\"\"\n\n_reset_option_tmpl = \"\"\"\nreset_option(pat)\n\nReset one or more options to their default value.\n\nPass \"all\" as argument to reset all options.\n\nAvailable options:\n\n{opts_list}\n\nParameters\n----------\npat : str/regex\n    If specified only options matching `prefix*` will be reset.\n    Note: partial matches are supported for convenience, but unless you\n    use the full option name (e.g. x.y.z.option_name), your code may break\n    in future versions if new options with similar names are introduced.\n\nReturns\n-------\nNone\n\nNotes\n-----\nPlease reference the :ref:`User Guide <options>` for more information.\n\nThe available options with its descriptions:\n\n{opts_desc}\n\"\"\"\n\n# bind the functions with their docstrings into a Callable\n# and use that as the functions exposed in pd.api\nget_option = CallableDynamicDoc(_get_option, _get_option_tmpl)\nset_option = CallableDynamicDoc(_set_option, _set_option_tmpl)\nreset_option = CallableDynamicDoc(_reset_option, _reset_option_tmpl)\ndescribe_option = CallableDynamicDoc(_describe_option, _describe_option_tmpl)\noptions = DictWrapper(_global_config)\n\n#\n# Functions for use by pandas developers, in addition to User - api\n\n\nclass option_context(ContextDecorator):\n    \"\"\"\n    Context manager to temporarily set options in the `with` statement context.\n\n    You need to invoke as ``option_context(pat, val, [(pat, val), ...])``.\n\n    Examples\n    --------\n    >>> from pandas import option_context\n    >>> with option_context('display.max_rows', 10, 'display.max_columns', 5):\n    ...     pass\n    \"\"\"\n\n    def __init__(self, *args) -> None:\n        if len(args) % 2 != 0 or len(args) < 2:\n            raise ValueError(\n                \"Need to invoke as option_context(pat, val, [(pat, val), ...]).\"\n            )\n\n        self.ops = list(zip(args[::2], args[1::2]))\n\n    def __enter__(self) -> None:\n        self.undo = [(pat, _get_option(pat, silent=True)) for pat, val in self.ops]\n\n        for pat, val in self.ops:\n            _set_option(pat, val, silent=True)\n\n    def __exit__(self, *args) -> None:\n        if self.undo:\n            for pat, val in self.undo:\n                _set_option(pat, val, silent=True)\n\n\ndef register_option(\n    key: str,\n    defval: object,\n    doc: str = \"\",\n    validator: Callable[[object], Any] | None = None,\n    cb: Callable[[str], Any] | None = None,\n) -> None:\n    \"\"\"\n    Register an option in the package-wide pandas config object\n\n    Parameters\n    ----------\n    key : str\n        Fully-qualified key, e.g. \"x.y.option - z\".\n    defval : object\n        Default value of the option.\n    doc : str\n        Description of the option.\n    validator : Callable, optional\n        Function of a single argument, should raise `ValueError` if\n        called with a value which is not a legal value for the option.\n    cb\n        a function of a single argument \"key\", which is called\n        immediately after an option value is set/reset. key is\n        the full name of the option.\n\n    Raises\n    ------\n    ValueError if `validator` is specified and `defval` is not a valid value.\n\n    \"\"\"\n    import keyword\n    import tokenize\n\n    key = key.lower()\n\n    if key in _registered_options:\n        raise OptionError(f\"Option '{key}' has already been registered\")\n    if key in _reserved_keys:\n        raise OptionError(f\"Option '{key}' is a reserved key\")\n\n    # the default value should be legal\n    if validator:\n        validator(defval)\n\n    # walk the nested dict, creating dicts as needed along the path\n    path = key.split(\".\")\n\n    for k in path:\n        if not re.match(\"^\" + tokenize.Name + \"$\", k):\n            raise ValueError(f\"{k} is not a valid identifier\")\n        if keyword.iskeyword(k):\n            raise ValueError(f\"{k} is a python keyword\")\n\n    cursor = _global_config\n    msg = \"Path prefix to option '{option}' is already an option\"\n\n    for i, p in enumerate(path[:-1]):\n        if not isinstance(cursor, dict):\n            raise OptionError(msg.format(option=\".\".join(path[:i])))\n        if p not in cursor:\n            cursor[p] = {}\n        cursor = cursor[p]\n\n    if not isinstance(cursor, dict):\n        raise OptionError(msg.format(option=\".\".join(path[:-1])))\n\n    cursor[path[-1]] = defval  # initialize\n\n    # save the option metadata\n    _registered_options[key] = RegisteredOption(\n        key=key, defval=defval, doc=doc, validator=validator, cb=cb\n    )\n\n\ndef deprecate_option(\n    key: str,\n    msg: str | None = None,\n    rkey: str | None = None,\n    removal_ver: str | None = None,\n) -> None:\n    \"\"\"\n    Mark option `key` as deprecated, if code attempts to access this option,\n    a warning will be produced, using `msg` if given, or a default message\n    if not.\n    if `rkey` is given, any access to the key will be re-routed to `rkey`.\n\n    Neither the existence of `key` nor that if `rkey` is checked. If they\n    do not exist, any subsequence access will fail as usual, after the\n    deprecation warning is given.\n\n    Parameters\n    ----------\n    key : str\n        Name of the option to be deprecated.\n        must be a fully-qualified option name (e.g \"x.y.z.rkey\").\n    msg : str, optional\n        Warning message to output when the key is referenced.\n        if no message is given a default message will be emitted.\n    rkey : str, optional\n        Name of an option to reroute access to.\n        If specified, any referenced `key` will be\n        re-routed to `rkey` including set/get/reset.\n        rkey must be a fully-qualified option name (e.g \"x.y.z.rkey\").\n        used by the default message if no `msg` is specified.\n    removal_ver : str, optional\n        Specifies the version in which this option will\n        be removed. used by the default message if no `msg` is specified.\n\n    Raises\n    ------\n    OptionError\n        If the specified key has already been deprecated.\n    \"\"\"\n    key = key.lower()\n\n    if key in _deprecated_options:\n        raise OptionError(f\"Option '{key}' has already been defined as deprecated.\")\n\n    _deprecated_options[key] = DeprecatedOption(key, msg, rkey, removal_ver)\n\n\n#\n# functions internal to the module\n\n\ndef _select_options(pat: str) -> list[str]:\n    \"\"\"\n    returns a list of keys matching `pat`\n\n    if pat==\"all\", returns all registered options\n    \"\"\"\n    # short-circuit for exact key\n    if pat in _registered_options:\n        return [pat]\n\n    # else look through all of them\n    keys = sorted(_registered_options.keys())\n    if pat == \"all\":  # reserved key\n        return keys\n\n    return [k for k in keys if re.search(pat, k, re.I)]\n\n\ndef _get_root(key: str) -> tuple[dict[str, Any], str]:\n    path = key.split(\".\")\n    cursor = _global_config\n    for p in path[:-1]:\n        cursor = cursor[p]\n    return cursor, path[-1]\n\n\ndef _is_deprecated(key: str) -> bool:\n    \"\"\"Returns True if the given option has been deprecated\"\"\"\n    key = key.lower()\n    return key in _deprecated_options\n\n\ndef _get_deprecated_option(key: str):\n    \"\"\"\n    Retrieves the metadata for a deprecated option, if `key` is deprecated.\n\n    Returns\n    -------\n    DeprecatedOption (namedtuple) if key is deprecated, None otherwise\n    \"\"\"\n    try:\n        d = _deprecated_options[key]\n    except KeyError:\n        return None\n    else:\n        return d\n\n\ndef _get_registered_option(key: str):\n    \"\"\"\n    Retrieves the option metadata if `key` is a registered option.\n\n    Returns\n    -------\n    RegisteredOption (namedtuple) if key is deprecated, None otherwise\n    \"\"\"\n    return _registered_options.get(key)\n\n\ndef _translate_key(key: str) -> str:\n    \"\"\"\n    if key id deprecated and a replacement key defined, will return the\n    replacement key, otherwise returns `key` as - is\n    \"\"\"\n    d = _get_deprecated_option(key)\n    if d:\n        return d.rkey or key\n    else:\n        return key\n\n\ndef _warn_if_deprecated(key: str) -> bool:\n    \"\"\"\n    Checks if `key` is a deprecated option and if so, prints a warning.\n\n    Returns\n    -------\n    bool - True if `key` is deprecated, False otherwise.\n    \"\"\"\n    d = _get_deprecated_option(key)\n    if d:\n        if d.msg:\n            warnings.warn(\n                d.msg,\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n        else:\n            msg = f\"'{key}' is deprecated\"\n            if d.removal_ver:\n                msg += f\" and will be removed in {d.removal_ver}\"\n            if d.rkey:\n                msg += f\", please use '{d.rkey}' instead.\"\n            else:\n                msg += \", please refrain from using it.\"\n\n            warnings.warn(msg, FutureWarning, stacklevel=find_stack_level())\n        return True\n    return False\n\n\ndef _build_option_description(k: str) -> str:\n    \"\"\"Builds a formatted description of a registered option and prints it\"\"\"\n    o = _get_registered_option(k)\n    d = _get_deprecated_option(k)\n\n    s = f\"{k} \"\n\n    if o.doc:\n        s += \"\\n\".join(o.doc.strip().split(\"\\n\"))\n    else:\n        s += \"No description available.\"\n\n    if o:\n        s += f\"\\n    [default: {o.defval}] [currently: {_get_option(k, True)}]\"\n\n    if d:\n        rkey = d.rkey or \"\"\n        s += \"\\n    (Deprecated\"\n        s += f\", use `{rkey}` instead.\"\n        s += \")\"\n\n    return s\n\n\ndef pp_options_list(keys: Iterable[str], width: int = 80, _print: bool = False):\n    \"\"\"Builds a concise listing of available options, grouped by prefix\"\"\"\n    from itertools import groupby\n    from textwrap import wrap\n\n    def pp(name: str, ks: Iterable[str]) -> list[str]:\n        pfx = \"- \" + name + \".[\" if name else \"\"\n        ls = wrap(\n            \", \".join(ks),\n            width,\n            initial_indent=pfx,\n            subsequent_indent=\"  \",\n            break_long_words=False,\n        )\n        if ls and ls[-1] and name:\n            ls[-1] = ls[-1] + \"]\"\n        return ls\n\n    ls: list[str] = []\n    singles = [x for x in sorted(keys) if x.find(\".\") < 0]\n    if singles:\n        ls += pp(\"\", singles)\n    keys = [x for x in keys if x.find(\".\") >= 0]\n\n    for k, g in groupby(sorted(keys), lambda x: x[: x.rfind(\".\")]):\n        ks = [x[len(k) + 1 :] for x in list(g)]\n        ls += pp(k, ks)\n    s = \"\\n\".join(ls)\n    if _print:\n        print(s)\n    else:\n        return s\n\n\n#\n# helpers\n\n\n@contextmanager\ndef config_prefix(prefix) -> Generator[None, None, None]:\n    \"\"\"\n    contextmanager for multiple invocations of API with a common prefix\n\n    supported API functions: (register / get / set )__option\n\n    Warning: This is not thread - safe, and won't work properly if you import\n    the API functions into your module using the \"from x import y\" construct.\n\n    Example\n    -------\n    import pandas._config.config as cf\n    with cf.config_prefix(\"display.font\"):\n        cf.register_option(\"color\", \"red\")\n        cf.register_option(\"size\", \" 5 pt\")\n        cf.set_option(size, \" 6 pt\")\n        cf.get_option(size)\n        ...\n\n        etc'\n\n    will register options \"display.font.color\", \"display.font.size\", set the\n    value of \"display.font.size\"... and so on.\n    \"\"\"\n    # Note: reset_option relies on set_option, and on key directly\n    # it does not fit in to this monkey-patching scheme\n\n    global register_option, get_option, set_option\n\n    def wrap(func: F) -> F:\n        def inner(key: str, *args, **kwds):\n            pkey = f\"{prefix}.{key}\"\n            return func(pkey, *args, **kwds)\n\n        return cast(F, inner)\n\n    _register_option = register_option\n    _get_option = get_option\n    _set_option = set_option\n    set_option = wrap(set_option)\n    get_option = wrap(get_option)\n    register_option = wrap(register_option)\n    try:\n        yield\n    finally:\n        set_option = _set_option\n        get_option = _get_option\n        register_option = _register_option\n\n\n# These factories and methods are handy for use as the validator\n# arg in register_option\n\n\ndef is_type_factory(_type: type[Any]) -> Callable[[Any], None]:\n    \"\"\"\n\n    Parameters\n    ----------\n    `_type` - a type to be compared against (e.g. type(x) == `_type`)\n\n    Returns\n    -------\n    validator - a function of a single argument x , which raises\n                ValueError if type(x) is not equal to `_type`\n\n    \"\"\"\n\n    def inner(x) -> None:\n        if type(x) != _type:\n            raise ValueError(f\"Value must have type '{_type}'\")\n\n    return inner\n\n\ndef is_instance_factory(_type) -> Callable[[Any], None]:\n    \"\"\"\n\n    Parameters\n    ----------\n    `_type` - the type to be checked against\n\n    Returns\n    -------\n    validator - a function of a single argument x , which raises\n                ValueError if x is not an instance of `_type`\n\n    \"\"\"\n    if isinstance(_type, (tuple, list)):\n        _type = tuple(_type)\n        type_repr = \"|\".join(map(str, _type))\n    else:\n        type_repr = f\"'{_type}'\"\n\n    def inner(x) -> None:\n        if not isinstance(x, _type):\n            raise ValueError(f\"Value must be an instance of {type_repr}\")\n\n    return inner\n\n\ndef is_one_of_factory(legal_values) -> Callable[[Any], None]:\n    callables = [c for c in legal_values if callable(c)]\n    legal_values = [c for c in legal_values if not callable(c)]\n\n    def inner(x) -> None:\n        if x not in legal_values:\n            if not any(c(x) for c in callables):\n                uvals = [str(lval) for lval in legal_values]\n                pp_values = \"|\".join(uvals)\n                msg = f\"Value must be one of {pp_values}\"\n                if len(callables):\n                    msg += \" or a callable\"\n                raise ValueError(msg)\n\n    return inner\n\n\ndef is_nonnegative_int(value: object) -> None:\n    \"\"\"\n    Verify that value is None or a positive int.\n\n    Parameters\n    ----------\n    value : None or int\n            The `value` to be checked.\n\n    Raises\n    ------\n    ValueError\n        When the value is not None or is a negative integer\n    \"\"\"\n    if value is None:\n        return\n\n    elif isinstance(value, int):\n        if value >= 0:\n            return\n\n    msg = \"Value must be a nonnegative integer or None\"\n    raise ValueError(msg)\n\n\n# common type validators, for convenience\n# usage: register_option(... , validator = is_int)\nis_int = is_type_factory(int)\nis_bool = is_type_factory(bool)\nis_float = is_type_factory(float)\nis_str = is_type_factory(str)\nis_text = is_instance_factory((str, bytes))\n\n\ndef is_callable(obj) -> bool:\n    \"\"\"\n\n    Parameters\n    ----------\n    `obj` - the object to be checked\n\n    Returns\n    -------\n    validator - returns True if object is callable\n        raises ValueError otherwise.\n\n    \"\"\"\n    if not callable(obj):\n        raise ValueError(\"Value must be a callable\")\n    return True\n"
    },
    {
      "filename": "pandas/_testing/__init__.py",
      "content": "from __future__ import annotations\n\nimport collections\nfrom datetime import datetime\nfrom decimal import Decimal\nimport operator\nimport os\nimport re\nimport string\nfrom sys import byteorder\nfrom typing import (\n    TYPE_CHECKING,\n    Callable,\n    ContextManager,\n    Counter,\n    Iterable,\n    cast,\n)\n\nimport numpy as np\n\nfrom pandas._config.localization import (\n    can_set_locale,\n    get_locales,\n    set_locale,\n)\n\nfrom pandas.compat import pa_version_under7p0\n\nfrom pandas.core.dtypes.common import (\n    is_float_dtype,\n    is_integer_dtype,\n    is_sequence,\n    is_signed_integer_dtype,\n    is_unsigned_integer_dtype,\n    pandas_dtype,\n)\n\nimport pandas as pd\nfrom pandas import (\n    ArrowDtype,\n    Categorical,\n    CategoricalIndex,\n    DataFrame,\n    DatetimeIndex,\n    Index,\n    IntervalIndex,\n    MultiIndex,\n    RangeIndex,\n    Series,\n    bdate_range,\n)\nfrom pandas._testing._io import (\n    close,\n    network,\n    round_trip_localpath,\n    round_trip_pathlib,\n    round_trip_pickle,\n    write_to_compressed,\n)\nfrom pandas._testing._random import (\n    rands,\n    rands_array,\n)\nfrom pandas._testing._warnings import (\n    assert_produces_warning,\n    maybe_produces_warning,\n)\nfrom pandas._testing.asserters import (\n    assert_almost_equal,\n    assert_attr_equal,\n    assert_categorical_equal,\n    assert_class_equal,\n    assert_contains_all,\n    assert_copy,\n    assert_datetime_array_equal,\n    assert_dict_equal,\n    assert_equal,\n    assert_extension_array_equal,\n    assert_frame_equal,\n    assert_index_equal,\n    assert_indexing_slices_equivalent,\n    assert_interval_array_equal,\n    assert_is_sorted,\n    assert_is_valid_plot_return_object,\n    assert_metadata_equivalent,\n    assert_numpy_array_equal,\n    assert_period_array_equal,\n    assert_series_equal,\n    assert_sp_array_equal,\n    assert_timedelta_array_equal,\n    raise_assert_detail,\n)\nfrom pandas._testing.compat import (\n    get_dtype,\n    get_obj,\n)\nfrom pandas._testing.contexts import (\n    decompress_file,\n    ensure_clean,\n    ensure_safe_environment_variables,\n    raises_chained_assignment_error,\n    set_timezone,\n    use_numexpr,\n    with_csv_dialect,\n)\nfrom pandas.core.arrays import (\n    BaseMaskedArray,\n    ExtensionArray,\n    PandasArray,\n)\nfrom pandas.core.arrays._mixins import NDArrayBackedExtensionArray\nfrom pandas.core.construction import extract_array\n\nif TYPE_CHECKING:\n    from pandas._typing import (\n        Dtype,\n        Frequency,\n        NpDtype,\n    )\n\n    from pandas import (\n        PeriodIndex,\n        TimedeltaIndex,\n    )\n    from pandas.core.arrays import ArrowExtensionArray\n\n_N = 30\n_K = 4\n\nUNSIGNED_INT_NUMPY_DTYPES: list[NpDtype] = [\"uint8\", \"uint16\", \"uint32\", \"uint64\"]\nUNSIGNED_INT_EA_DTYPES: list[Dtype] = [\"UInt8\", \"UInt16\", \"UInt32\", \"UInt64\"]\nSIGNED_INT_NUMPY_DTYPES: list[NpDtype] = [int, \"int8\", \"int16\", \"int32\", \"int64\"]\nSIGNED_INT_EA_DTYPES: list[Dtype] = [\"Int8\", \"Int16\", \"Int32\", \"Int64\"]\nALL_INT_NUMPY_DTYPES = UNSIGNED_INT_NUMPY_DTYPES + SIGNED_INT_NUMPY_DTYPES\nALL_INT_EA_DTYPES = UNSIGNED_INT_EA_DTYPES + SIGNED_INT_EA_DTYPES\nALL_INT_DTYPES: list[Dtype] = [*ALL_INT_NUMPY_DTYPES, *ALL_INT_EA_DTYPES]\n\nFLOAT_NUMPY_DTYPES: list[NpDtype] = [float, \"float32\", \"float64\"]\nFLOAT_EA_DTYPES: list[Dtype] = [\"Float32\", \"Float64\"]\nALL_FLOAT_DTYPES: list[Dtype] = [*FLOAT_NUMPY_DTYPES, *FLOAT_EA_DTYPES]\n\nCOMPLEX_DTYPES: list[Dtype] = [complex, \"complex64\", \"complex128\"]\nSTRING_DTYPES: list[Dtype] = [str, \"str\", \"U\"]\n\nDATETIME64_DTYPES: list[Dtype] = [\"datetime64[ns]\", \"M8[ns]\"]\nTIMEDELTA64_DTYPES: list[Dtype] = [\"timedelta64[ns]\", \"m8[ns]\"]\n\nBOOL_DTYPES: list[Dtype] = [bool, \"bool\"]\nBYTES_DTYPES: list[Dtype] = [bytes, \"bytes\"]\nOBJECT_DTYPES: list[Dtype] = [object, \"object\"]\n\nALL_REAL_NUMPY_DTYPES = FLOAT_NUMPY_DTYPES + ALL_INT_NUMPY_DTYPES\nALL_REAL_EXTENSION_DTYPES = FLOAT_EA_DTYPES + ALL_INT_EA_DTYPES\nALL_REAL_DTYPES: list[Dtype] = [*ALL_REAL_NUMPY_DTYPES, *ALL_REAL_EXTENSION_DTYPES]\nALL_NUMERIC_DTYPES: list[Dtype] = [*ALL_REAL_DTYPES, *COMPLEX_DTYPES]\n\nALL_NUMPY_DTYPES = (\n    ALL_REAL_NUMPY_DTYPES\n    + COMPLEX_DTYPES\n    + STRING_DTYPES\n    + DATETIME64_DTYPES\n    + TIMEDELTA64_DTYPES\n    + BOOL_DTYPES\n    + OBJECT_DTYPES\n    + BYTES_DTYPES\n)\n\nNARROW_NP_DTYPES = [\n    np.float16,\n    np.float32,\n    np.int8,\n    np.int16,\n    np.int32,\n    np.uint8,\n    np.uint16,\n    np.uint32,\n]\n\nENDIAN = {\"little\": \"<\", \"big\": \">\"}[byteorder]\n\nNULL_OBJECTS = [None, np.nan, pd.NaT, float(\"nan\"), pd.NA, Decimal(\"NaN\")]\nNP_NAT_OBJECTS = [\n    cls(\"NaT\", unit)\n    for cls in [np.datetime64, np.timedelta64]\n    for unit in [\n        \"Y\",\n        \"M\",\n        \"W\",\n        \"D\",\n        \"h\",\n        \"m\",\n        \"s\",\n        \"ms\",\n        \"us\",\n        \"ns\",\n        \"ps\",\n        \"fs\",\n        \"as\",\n    ]\n]\n\nif not pa_version_under7p0:\n    import pyarrow as pa\n\n    UNSIGNED_INT_PYARROW_DTYPES = [pa.uint8(), pa.uint16(), pa.uint32(), pa.uint64()]\n    SIGNED_INT_PYARROW_DTYPES = [pa.int8(), pa.int16(), pa.int32(), pa.int64()]\n    ALL_INT_PYARROW_DTYPES = UNSIGNED_INT_PYARROW_DTYPES + SIGNED_INT_PYARROW_DTYPES\n    ALL_INT_PYARROW_DTYPES_STR_REPR = [\n        str(ArrowDtype(typ)) for typ in ALL_INT_PYARROW_DTYPES\n    ]\n\n    # pa.float16 doesn't seem supported\n    # https://github.com/apache/arrow/blob/master/python/pyarrow/src/arrow/python/helpers.cc#L86\n    FLOAT_PYARROW_DTYPES = [pa.float32(), pa.float64()]\n    FLOAT_PYARROW_DTYPES_STR_REPR = [\n        str(ArrowDtype(typ)) for typ in FLOAT_PYARROW_DTYPES\n    ]\n    DECIMAL_PYARROW_DTYPES = [pa.decimal128(7, 3)]\n    STRING_PYARROW_DTYPES = [pa.string()]\n    BINARY_PYARROW_DTYPES = [pa.binary()]\n\n    TIME_PYARROW_DTYPES = [\n        pa.time32(\"s\"),\n        pa.time32(\"ms\"),\n        pa.time64(\"us\"),\n        pa.time64(\"ns\"),\n    ]\n    DATE_PYARROW_DTYPES = [pa.date32(), pa.date64()]\n    DATETIME_PYARROW_DTYPES = [\n        pa.timestamp(unit=unit, tz=tz)\n        for unit in [\"s\", \"ms\", \"us\", \"ns\"]\n        for tz in [None, \"UTC\", \"US/Pacific\", \"US/Eastern\"]\n    ]\n    TIMEDELTA_PYARROW_DTYPES = [pa.duration(unit) for unit in [\"s\", \"ms\", \"us\", \"ns\"]]\n\n    BOOL_PYARROW_DTYPES = [pa.bool_()]\n\n    # TODO: Add container like pyarrow types:\n    #  https://arrow.apache.org/docs/python/api/datatypes.html#factory-functions\n    ALL_PYARROW_DTYPES = (\n        ALL_INT_PYARROW_DTYPES\n        + FLOAT_PYARROW_DTYPES\n        + DECIMAL_PYARROW_DTYPES\n        + STRING_PYARROW_DTYPES\n        + BINARY_PYARROW_DTYPES\n        + TIME_PYARROW_DTYPES\n        + DATE_PYARROW_DTYPES\n        + DATETIME_PYARROW_DTYPES\n        + TIMEDELTA_PYARROW_DTYPES\n        + BOOL_PYARROW_DTYPES\n    )\nelse:\n    FLOAT_PYARROW_DTYPES_STR_REPR = []\n    ALL_INT_PYARROW_DTYPES_STR_REPR = []\n\n\nEMPTY_STRING_PATTERN = re.compile(\"^$\")\n\n\ndef reset_display_options() -> None:\n    \"\"\"\n    Reset the display options for printing and representing objects.\n    \"\"\"\n    pd.reset_option(\"^display.\", silent=True)\n\n\n# -----------------------------------------------------------------------------\n# Comparators\n\n\ndef equalContents(arr1, arr2) -> bool:\n    \"\"\"\n    Checks if the set of unique elements of arr1 and arr2 are equivalent.\n    \"\"\"\n    return frozenset(arr1) == frozenset(arr2)\n\n\ndef box_expected(expected, box_cls, transpose: bool = True):\n    \"\"\"\n    Helper function to wrap the expected output of a test in a given box_class.\n\n    Parameters\n    ----------\n    expected : np.ndarray, Index, Series\n    box_cls : {Index, Series, DataFrame}\n\n    Returns\n    -------\n    subclass of box_cls\n    \"\"\"\n    if box_cls is pd.array:\n        if isinstance(expected, RangeIndex):\n            # pd.array would return an IntegerArray\n            expected = PandasArray(np.asarray(expected._values))\n        else:\n            expected = pd.array(expected, copy=False)\n    elif box_cls is Index:\n        expected = Index(expected)\n    elif box_cls is Series:\n        expected = Series(expected)\n    elif box_cls is DataFrame:\n        expected = Series(expected).to_frame()\n        if transpose:\n            # for vector operations, we need a DataFrame to be a single-row,\n            #  not a single-column, in order to operate against non-DataFrame\n            #  vectors of the same length. But convert to two rows to avoid\n            #  single-row special cases in datetime arithmetic\n            expected = expected.T\n            expected = pd.concat([expected] * 2, ignore_index=True)\n    elif box_cls is np.ndarray or box_cls is np.array:\n        expected = np.array(expected)\n    elif box_cls is to_array:\n        expected = to_array(expected)\n    else:\n        raise NotImplementedError(box_cls)\n    return expected\n\n\ndef to_array(obj):\n    \"\"\"\n    Similar to pd.array, but does not cast numpy dtypes to nullable dtypes.\n    \"\"\"\n    # temporary implementation until we get pd.array in place\n    dtype = getattr(obj, \"dtype\", None)\n\n    if dtype is None:\n        return np.asarray(obj)\n\n    return extract_array(obj, extract_numpy=True)\n\n\n# -----------------------------------------------------------------------------\n# Others\n\n\ndef getCols(k) -> str:\n    return string.ascii_uppercase[:k]\n\n\n# make index\ndef makeStringIndex(k: int = 10, name=None) -> Index:\n    return Index(rands_array(nchars=10, size=k), name=name)\n\n\ndef makeCategoricalIndex(\n    k: int = 10, n: int = 3, name=None, **kwargs\n) -> CategoricalIndex:\n    \"\"\"make a length k index or n categories\"\"\"\n    x = rands_array(nchars=4, size=n, replace=False)\n    return CategoricalIndex(\n        Categorical.from_codes(np.arange(k) % n, categories=x), name=name, **kwargs\n    )\n\n\ndef makeIntervalIndex(k: int = 10, name=None, **kwargs) -> IntervalIndex:\n    \"\"\"make a length k IntervalIndex\"\"\"\n    x = np.linspace(0, 100, num=(k + 1))\n    return IntervalIndex.from_breaks(x, name=name, **kwargs)\n\n\ndef makeBoolIndex(k: int = 10, name=None) -> Index:\n    if k == 1:\n        return Index([True], name=name)\n    elif k == 2:\n        return Index([False, True], name=name)\n    return Index([False, True] + [False] * (k - 2), name=name)\n\n\ndef makeNumericIndex(k: int = 10, *, name=None, dtype: Dtype | None) -> Index:\n    dtype = pandas_dtype(dtype)\n    assert isinstance(dtype, np.dtype)\n\n    if is_integer_dtype(dtype):\n        values = np.arange(k, dtype=dtype)\n        if is_unsigned_integer_dtype(dtype):\n            values += 2 ** (dtype.itemsize * 8 - 1)\n    elif is_float_dtype(dtype):\n        values = np.random.random_sample(k) - np.random.random_sample(1)\n        values.sort()\n        values = values * (10 ** np.random.randint(0, 9))\n    else:\n        raise NotImplementedError(f\"wrong dtype {dtype}\")\n\n    return Index(values, dtype=dtype, name=name)\n\n\ndef makeIntIndex(k: int = 10, *, name=None, dtype: Dtype = \"int64\") -> Index:\n    dtype = pandas_dtype(dtype)\n    if not is_signed_integer_dtype(dtype):\n        raise TypeError(f\"Wrong dtype {dtype}\")\n    return makeNumericIndex(k, name=name, dtype=dtype)\n\n\ndef makeUIntIndex(k: int = 10, *, name=None, dtype: Dtype = \"uint64\") -> Index:\n    dtype = pandas_dtype(dtype)\n    if not is_unsigned_integer_dtype(dtype):\n        raise TypeError(f\"Wrong dtype {dtype}\")\n    return makeNumericIndex(k, name=name, dtype=dtype)\n\n\ndef makeRangeIndex(k: int = 10, name=None, **kwargs) -> RangeIndex:\n    return RangeIndex(0, k, 1, name=name, **kwargs)\n\n\ndef makeFloatIndex(k: int = 10, *, name=None, dtype: Dtype = \"float64\") -> Index:\n    dtype = pandas_dtype(dtype)\n    if not is_float_dtype(dtype):\n        raise TypeError(f\"Wrong dtype {dtype}\")\n    return makeNumericIndex(k, name=name, dtype=dtype)\n\n\ndef makeDateIndex(\n    k: int = 10, freq: Frequency = \"B\", name=None, **kwargs\n) -> DatetimeIndex:\n    dt = datetime(2000, 1, 1)\n    dr = bdate_range(dt, periods=k, freq=freq, name=name)\n    return DatetimeIndex(dr, name=name, **kwargs)\n\n\ndef makeTimedeltaIndex(\n    k: int = 10, freq: Frequency = \"D\", name=None, **kwargs\n) -> TimedeltaIndex:\n    return pd.timedelta_range(start=\"1 day\", periods=k, freq=freq, name=name, **kwargs)\n\n\ndef makePeriodIndex(k: int = 10, name=None, **kwargs) -> PeriodIndex:\n    dt = datetime(2000, 1, 1)\n    return pd.period_range(start=dt, periods=k, freq=\"B\", name=name, **kwargs)\n\n\ndef makeMultiIndex(k: int = 10, names=None, **kwargs):\n    N = (k // 2) + 1\n    rng = range(N)\n    mi = MultiIndex.from_product([(\"foo\", \"bar\"), rng], names=names, **kwargs)\n    assert len(mi) >= k  # GH#38795\n    return mi[:k]\n\n\ndef index_subclass_makers_generator():\n    make_index_funcs = [\n        makeDateIndex,\n        makePeriodIndex,\n        makeTimedeltaIndex,\n        makeRangeIndex,\n        makeIntervalIndex,\n        makeCategoricalIndex,\n        makeMultiIndex,\n    ]\n    yield from make_index_funcs\n\n\ndef all_timeseries_index_generator(k: int = 10) -> Iterable[Index]:\n    \"\"\"\n    Generator which can be iterated over to get instances of all the classes\n    which represent time-series.\n\n    Parameters\n    ----------\n    k: length of each of the index instances\n    \"\"\"\n    make_index_funcs: list[Callable[..., Index]] = [\n        makeDateIndex,\n        makePeriodIndex,\n        makeTimedeltaIndex,\n    ]\n    for make_index_func in make_index_funcs:\n        yield make_index_func(k=k)\n\n\n# make series\ndef make_rand_series(name=None, dtype=np.float64) -> Series:\n    index = makeStringIndex(_N)\n    data = np.random.randn(_N)\n    with np.errstate(invalid=\"ignore\"):\n        data = data.astype(dtype, copy=False)\n    return Series(data, index=index, name=name)\n\n\ndef makeFloatSeries(name=None) -> Series:\n    return make_rand_series(name=name)\n\n\ndef makeStringSeries(name=None) -> Series:\n    return make_rand_series(name=name)\n\n\ndef makeObjectSeries(name=None) -> Series:\n    data = makeStringIndex(_N)\n    data = Index(data, dtype=object)\n    index = makeStringIndex(_N)\n    return Series(data, index=index, name=name)\n\n\ndef getSeriesData() -> dict[str, Series]:\n    index = makeStringIndex(_N)\n    return {c: Series(np.random.randn(_N), index=index) for c in getCols(_K)}\n\n\ndef makeTimeSeries(nper=None, freq: Frequency = \"B\", name=None) -> Series:\n    if nper is None:\n        nper = _N\n    return Series(\n        np.random.randn(nper), index=makeDateIndex(nper, freq=freq), name=name\n    )\n\n\ndef makePeriodSeries(nper=None, name=None) -> Series:\n    if nper is None:\n        nper = _N\n    return Series(np.random.randn(nper), index=makePeriodIndex(nper), name=name)\n\n\ndef getTimeSeriesData(nper=None, freq: Frequency = \"B\") -> dict[str, Series]:\n    return {c: makeTimeSeries(nper, freq) for c in getCols(_K)}\n\n\ndef getPeriodData(nper=None) -> dict[str, Series]:\n    return {c: makePeriodSeries(nper) for c in getCols(_K)}\n\n\n# make frame\ndef makeTimeDataFrame(nper=None, freq: Frequency = \"B\") -> DataFrame:\n    data = getTimeSeriesData(nper, freq)\n    return DataFrame(data)\n\n\ndef makeDataFrame() -> DataFrame:\n    data = getSeriesData()\n    return DataFrame(data)\n\n\ndef getMixedTypeDict():\n    index = Index([\"a\", \"b\", \"c\", \"d\", \"e\"])\n\n    data = {\n        \"A\": [0.0, 1.0, 2.0, 3.0, 4.0],\n        \"B\": [0.0, 1.0, 0.0, 1.0, 0.0],\n        \"C\": [\"foo1\", \"foo2\", \"foo3\", \"foo4\", \"foo5\"],\n        \"D\": bdate_range(\"1/1/2009\", periods=5),\n    }\n\n    return index, data\n\n\ndef makeMixedDataFrame() -> DataFrame:\n    return DataFrame(getMixedTypeDict()[1])\n\n\ndef makePeriodFrame(nper=None) -> DataFrame:\n    data = getPeriodData(nper)\n    return DataFrame(data)\n\n\ndef makeCustomIndex(\n    nentries,\n    nlevels,\n    prefix: str = \"#\",\n    names: bool | str | list[str] | None = False,\n    ndupe_l=None,\n    idx_type=None,\n) -> Index:\n    \"\"\"\n    Create an index/multindex with given dimensions, levels, names, etc'\n\n    nentries - number of entries in index\n    nlevels - number of levels (> 1 produces multindex)\n    prefix - a string prefix for labels\n    names - (Optional), bool or list of strings. if True will use default\n       names, if false will use no names, if a list is given, the name of\n       each level in the index will be taken from the list.\n    ndupe_l - (Optional), list of ints, the number of rows for which the\n       label will repeated at the corresponding level, you can specify just\n       the first few, the rest will use the default ndupe_l of 1.\n       len(ndupe_l) <= nlevels.\n    idx_type - \"i\"/\"f\"/\"s\"/\"dt\"/\"p\"/\"td\".\n       If idx_type is not None, `idx_nlevels` must be 1.\n       \"i\"/\"f\" creates an integer/float index,\n       \"s\" creates a string\n       \"dt\" create a datetime index.\n       \"td\" create a datetime index.\n\n        if unspecified, string labels will be generated.\n    \"\"\"\n    if ndupe_l is None:\n        ndupe_l = [1] * nlevels\n    assert is_sequence(ndupe_l) and len(ndupe_l) <= nlevels\n    assert names is None or names is False or names is True or len(names) is nlevels\n    assert idx_type is None or (\n        idx_type in (\"i\", \"f\", \"s\", \"u\", \"dt\", \"p\", \"td\") and nlevels == 1\n    )\n\n    if names is True:\n        # build default names\n        names = [prefix + str(i) for i in range(nlevels)]\n    if names is False:\n        # pass None to index constructor for no name\n        names = None\n\n    # make singleton case uniform\n    if isinstance(names, str) and nlevels == 1:\n        names = [names]\n\n    # specific 1D index type requested?\n    idx_func_dict: dict[str, Callable[..., Index]] = {\n        \"i\": makeIntIndex,\n        \"f\": makeFloatIndex,\n        \"s\": makeStringIndex,\n        \"dt\": makeDateIndex,\n        \"td\": makeTimedeltaIndex,\n        \"p\": makePeriodIndex,\n    }\n    idx_func = idx_func_dict.get(idx_type)\n    if idx_func:\n        idx = idx_func(nentries)\n        # but we need to fill in the name\n        if names:\n            idx.name = names[0]\n        return idx\n    elif idx_type is not None:\n        raise ValueError(\n            f\"{repr(idx_type)} is not a legal value for `idx_type`, \"\n            \"use  'i'/'f'/'s'/'dt'/'p'/'td'.\"\n        )\n\n    if len(ndupe_l) < nlevels:\n        ndupe_l.extend([1] * (nlevels - len(ndupe_l)))\n    assert len(ndupe_l) == nlevels\n\n    assert all(x > 0 for x in ndupe_l)\n\n    list_of_lists = []\n    for i in range(nlevels):\n\n        def keyfunc(x):\n            numeric_tuple = re.sub(r\"[^\\d_]_?\", \"\", x).split(\"_\")\n            return [int(num) for num in numeric_tuple]\n\n        # build a list of lists to create the index from\n        div_factor = nentries // ndupe_l[i] + 1\n\n        # Deprecated since version 3.9: collections.Counter now supports []. See PEP 585\n        # and Generic Alias Type.\n        cnt: Counter[str] = collections.Counter()\n        for j in range(div_factor):\n            label = f\"{prefix}_l{i}_g{j}\"\n            cnt[label] = ndupe_l[i]\n        # cute Counter trick\n        result = sorted(cnt.elements(), key=keyfunc)[:nentries]\n        list_of_lists.append(result)\n\n    tuples = list(zip(*list_of_lists))\n\n    # convert tuples to index\n    if nentries == 1:\n        # we have a single level of tuples, i.e. a regular Index\n        name = None if names is None else names[0]\n        index = Index(tuples[0], name=name)\n    elif nlevels == 1:\n        name = None if names is None else names[0]\n        index = Index((x[0] for x in tuples), name=name)\n    else:\n        index = MultiIndex.from_tuples(tuples, names=names)\n    return index\n\n\ndef makeCustomDataframe(\n    nrows,\n    ncols,\n    c_idx_names: bool | list[str] = True,\n    r_idx_names: bool | list[str] = True,\n    c_idx_nlevels: int = 1,\n    r_idx_nlevels: int = 1,\n    data_gen_f=None,\n    c_ndupe_l=None,\n    r_ndupe_l=None,\n    dtype=None,\n    c_idx_type=None,\n    r_idx_type=None,\n) -> DataFrame:\n    \"\"\"\n    Create a DataFrame using supplied parameters.\n\n    Parameters\n    ----------\n    nrows,  ncols - number of data rows/cols\n    c_idx_names, r_idx_names  - False/True/list of strings,  yields No names ,\n            default names or uses the provided names for the levels of the\n            corresponding index. You can provide a single string when\n            c_idx_nlevels ==1.\n    c_idx_nlevels - number of levels in columns index. > 1 will yield MultiIndex\n    r_idx_nlevels - number of levels in rows index. > 1 will yield MultiIndex\n    data_gen_f - a function f(row,col) which return the data value\n            at that position, the default generator used yields values of the form\n            \"RxCy\" based on position.\n    c_ndupe_l, r_ndupe_l - list of integers, determines the number\n            of duplicates for each label at a given level of the corresponding\n            index. The default `None` value produces a multiplicity of 1 across\n            all levels, i.e. a unique index. Will accept a partial list of length\n            N < idx_nlevels, for just the first N levels. If ndupe doesn't divide\n            nrows/ncol, the last label might have lower multiplicity.\n    dtype - passed to the DataFrame constructor as is, in case you wish to\n            have more control in conjunction with a custom `data_gen_f`\n    r_idx_type, c_idx_type -  \"i\"/\"f\"/\"s\"/\"dt\"/\"td\".\n        If idx_type is not None, `idx_nlevels` must be 1.\n        \"i\"/\"f\" creates an integer/float index,\n        \"s\" creates a string index\n        \"dt\" create a datetime index.\n        \"td\" create a timedelta index.\n\n            if unspecified, string labels will be generated.\n\n    Examples\n    --------\n    # 5 row, 3 columns, default names on both, single index on both axis\n    >> makeCustomDataframe(5,3)\n\n    # make the data a random int between 1 and 100\n    >> mkdf(5,3,data_gen_f=lambda r,c:randint(1,100))\n\n    # 2-level multiindex on rows with each label duplicated\n    # twice on first level, default names on both axis, single\n    # index on both axis\n    >> a=makeCustomDataframe(5,3,r_idx_nlevels=2,r_ndupe_l=[2])\n\n    # DatetimeIndex on row, index with unicode labels on columns\n    # no names on either axis\n    >> a=makeCustomDataframe(5,3,c_idx_names=False,r_idx_names=False,\n                             r_idx_type=\"dt\",c_idx_type=\"u\")\n\n    # 4-level multindex on rows with names provided, 2-level multindex\n    # on columns with default labels and default names.\n    >> a=makeCustomDataframe(5,3,r_idx_nlevels=4,\n                             r_idx_names=[\"FEE\",\"FIH\",\"FOH\",\"FUM\"],\n                             c_idx_nlevels=2)\n\n    >> a=mkdf(5,3,r_idx_nlevels=2,c_idx_nlevels=4)\n    \"\"\"\n    assert c_idx_nlevels > 0\n    assert r_idx_nlevels > 0\n    assert r_idx_type is None or (\n        r_idx_type in (\"i\", \"f\", \"s\", \"dt\", \"p\", \"td\") and r_idx_nlevels == 1\n    )\n    assert c_idx_type is None or (\n        c_idx_type in (\"i\", \"f\", \"s\", \"dt\", \"p\", \"td\") and c_idx_nlevels == 1\n    )\n\n    columns = makeCustomIndex(\n        ncols,\n        nlevels=c_idx_nlevels,\n        prefix=\"C\",\n        names=c_idx_names,\n        ndupe_l=c_ndupe_l,\n        idx_type=c_idx_type,\n    )\n    index = makeCustomIndex(\n        nrows,\n        nlevels=r_idx_nlevels,\n        prefix=\"R\",\n        names=r_idx_names,\n        ndupe_l=r_ndupe_l,\n        idx_type=r_idx_type,\n    )\n\n    # by default, generate data based on location\n    if data_gen_f is None:\n        data_gen_f = lambda r, c: f\"R{r}C{c}\"\n\n    data = [[data_gen_f(r, c) for c in range(ncols)] for r in range(nrows)]\n\n    return DataFrame(data, index, columns, dtype=dtype)\n\n\ndef _create_missing_idx(nrows, ncols, density: float, random_state=None):\n    if random_state is None:\n        random_state = np.random\n    else:\n        random_state = np.random.RandomState(random_state)\n\n    # below is cribbed from scipy.sparse\n    size = round((1 - density) * nrows * ncols)\n    # generate a few more to ensure unique values\n    min_rows = 5\n    fac = 1.02\n    extra_size = min(size + min_rows, fac * size)\n\n    def _gen_unique_rand(rng, _extra_size):\n        ind = rng.rand(int(_extra_size))\n        return np.unique(np.floor(ind * nrows * ncols))[:size]\n\n    ind = _gen_unique_rand(random_state, extra_size)\n    while ind.size < size:\n        extra_size *= 1.05\n        ind = _gen_unique_rand(random_state, extra_size)\n\n    j = np.floor(ind * 1.0 / nrows).astype(int)\n    i = (ind - j * nrows).astype(int)\n    return i.tolist(), j.tolist()\n\n\ndef makeMissingDataframe(density: float = 0.9, random_state=None) -> DataFrame:\n    df = makeDataFrame()\n    i, j = _create_missing_idx(*df.shape, density=density, random_state=random_state)\n    df.iloc[i, j] = np.nan\n    return df\n\n\nclass SubclassedSeries(Series):\n    _metadata = [\"testattr\", \"name\"]\n\n    @property\n    def _constructor(self):\n        # For testing, those properties return a generic callable, and not\n        # the actual class. In this case that is equivalent, but it is to\n        # ensure we don't rely on the property returning a class\n        # See https://github.com/pandas-dev/pandas/pull/46018 and\n        # https://github.com/pandas-dev/pandas/issues/32638 and linked issues\n        return lambda *args, **kwargs: SubclassedSeries(*args, **kwargs)\n\n    @property\n    def _constructor_expanddim(self):\n        return lambda *args, **kwargs: SubclassedDataFrame(*args, **kwargs)\n\n\nclass SubclassedDataFrame(DataFrame):\n    _metadata = [\"testattr\"]\n\n    @property\n    def _constructor(self):\n        return lambda *args, **kwargs: SubclassedDataFrame(*args, **kwargs)\n\n    @property\n    def _constructor_sliced(self):\n        return lambda *args, **kwargs: SubclassedSeries(*args, **kwargs)\n\n\nclass SubclassedCategorical(Categorical):\n    @property\n    def _constructor(self):\n        return SubclassedCategorical\n\n\ndef _make_skipna_wrapper(alternative, skipna_alternative=None):\n    \"\"\"\n    Create a function for calling on an array.\n\n    Parameters\n    ----------\n    alternative : function\n        The function to be called on the array with no NaNs.\n        Only used when 'skipna_alternative' is None.\n    skipna_alternative : function\n        The function to be called on the original array\n\n    Returns\n    -------\n    function\n    \"\"\"\n    if skipna_alternative:\n\n        def skipna_wrapper(x):\n            return skipna_alternative(x.values)\n\n    else:\n\n        def skipna_wrapper(x):\n            nona = x.dropna()\n            if len(nona) == 0:\n                return np.nan\n            return alternative(nona)\n\n    return skipna_wrapper\n\n\ndef convert_rows_list_to_csv_str(rows_list: list[str]) -> str:\n    \"\"\"\n    Convert list of CSV rows to single CSV-formatted string for current OS.\n\n    This method is used for creating expected value of to_csv() method.\n\n    Parameters\n    ----------\n    rows_list : List[str]\n        Each element represents the row of csv.\n\n    Returns\n    -------\n    str\n        Expected output of to_csv() in current OS.\n    \"\"\"\n    sep = os.linesep\n    return sep.join(rows_list) + sep\n\n\ndef external_error_raised(expected_exception: type[Exception]) -> ContextManager:\n    \"\"\"\n    Helper function to mark pytest.raises that have an external error message.\n\n    Parameters\n    ----------\n    expected_exception : Exception\n        Expected error to raise.\n\n    Returns\n    -------\n    Callable\n        Regular `pytest.raises` function with `match` equal to `None`.\n    \"\"\"\n    import pytest\n\n    return pytest.raises(expected_exception, match=None)\n\n\ncython_table = pd.core.common._cython_table.items()\n\n\ndef get_cython_table_params(ndframe, func_names_and_expected):\n    \"\"\"\n    Combine frame, functions from com._cython_table\n    keys and expected result.\n\n    Parameters\n    ----------\n    ndframe : DataFrame or Series\n    func_names_and_expected : Sequence of two items\n        The first item is a name of a NDFrame method ('sum', 'prod') etc.\n        The second item is the expected return value.\n\n    Returns\n    -------\n    list\n        List of three items (DataFrame, function, expected result)\n    \"\"\"\n    results = []\n    for func_name, expected in func_names_and_expected:\n        results.append((ndframe, func_name, expected))\n        results += [\n            (ndframe, func, expected)\n            for func, name in cython_table\n            if name == func_name\n        ]\n    return results\n\n\ndef get_op_from_name(op_name: str) -> Callable:\n    \"\"\"\n    The operator function for a given op name.\n\n    Parameters\n    ----------\n    op_name : str\n        The op name, in form of \"add\" or \"__add__\".\n\n    Returns\n    -------\n    function\n        A function performing the operation.\n    \"\"\"\n    short_opname = op_name.strip(\"_\")\n    try:\n        op = getattr(operator, short_opname)\n    except AttributeError:\n        # Assume it is the reverse operator\n        rop = getattr(operator, short_opname[1:])\n        op = lambda x, y: rop(y, x)\n\n    return op\n\n\n# -----------------------------------------------------------------------------\n# Indexing test helpers\n\n\ndef getitem(x):\n    return x\n\n\ndef setitem(x):\n    return x\n\n\ndef loc(x):\n    return x.loc\n\n\ndef iloc(x):\n    return x.iloc\n\n\ndef at(x):\n    return x.at\n\n\ndef iat(x):\n    return x.iat\n\n\n# -----------------------------------------------------------------------------\n\n\ndef shares_memory(left, right) -> bool:\n    \"\"\"\n    Pandas-compat for np.shares_memory.\n    \"\"\"\n    if isinstance(left, np.ndarray) and isinstance(right, np.ndarray):\n        return np.shares_memory(left, right)\n    elif isinstance(left, np.ndarray):\n        # Call with reversed args to get to unpacking logic below.\n        return shares_memory(right, left)\n\n    if isinstance(left, RangeIndex):\n        return False\n    if isinstance(left, MultiIndex):\n        return shares_memory(left._codes, right)\n    if isinstance(left, (Index, Series)):\n        return shares_memory(left._values, right)\n\n    if isinstance(left, NDArrayBackedExtensionArray):\n        return shares_memory(left._ndarray, right)\n    if isinstance(left, pd.core.arrays.SparseArray):\n        return shares_memory(left.sp_values, right)\n    if isinstance(left, pd.core.arrays.IntervalArray):\n        return shares_memory(left._left, right) or shares_memory(left._right, right)\n\n    if isinstance(left, ExtensionArray) and left.dtype == \"string[pyarrow]\":\n        # https://github.com/pandas-dev/pandas/pull/43930#discussion_r736862669\n        left = cast(\"ArrowExtensionArray\", left)\n        if isinstance(right, ExtensionArray) and right.dtype == \"string[pyarrow]\":\n            right = cast(\"ArrowExtensionArray\", right)\n            left_pa_data = left._data\n            right_pa_data = right._data\n            left_buf1 = left_pa_data.chunk(0).buffers()[1]\n            right_buf1 = right_pa_data.chunk(0).buffers()[1]\n            return left_buf1 == right_buf1\n\n    if isinstance(left, BaseMaskedArray) and isinstance(right, BaseMaskedArray):\n        # By convention, we'll say these share memory if they share *either*\n        #  the _data or the _mask\n        return np.shares_memory(left._data, right._data) or np.shares_memory(\n            left._mask, right._mask\n        )\n\n    if isinstance(left, DataFrame) and len(left._mgr.arrays) == 1:\n        arr = left._mgr.arrays[0]\n        return shares_memory(arr, right)\n\n    raise NotImplementedError(type(left), type(right))\n\n\n__all__ = [\n    \"ALL_INT_EA_DTYPES\",\n    \"ALL_INT_NUMPY_DTYPES\",\n    \"ALL_NUMPY_DTYPES\",\n    \"ALL_REAL_NUMPY_DTYPES\",\n    \"all_timeseries_index_generator\",\n    \"assert_almost_equal\",\n    \"assert_attr_equal\",\n    \"assert_categorical_equal\",\n    \"assert_class_equal\",\n    \"assert_contains_all\",\n    \"assert_copy\",\n    \"assert_datetime_array_equal\",\n    \"assert_dict_equal\",\n    \"assert_equal\",\n    \"assert_extension_array_equal\",\n    \"assert_frame_equal\",\n    \"assert_index_equal\",\n    \"assert_indexing_slices_equivalent\",\n    \"assert_interval_array_equal\",\n    \"assert_is_sorted\",\n    \"assert_is_valid_plot_return_object\",\n    \"assert_metadata_equivalent\",\n    \"assert_numpy_array_equal\",\n    \"assert_period_array_equal\",\n    \"assert_produces_warning\",\n    \"assert_series_equal\",\n    \"assert_sp_array_equal\",\n    \"assert_timedelta_array_equal\",\n    \"at\",\n    \"BOOL_DTYPES\",\n    \"box_expected\",\n    \"BYTES_DTYPES\",\n    \"can_set_locale\",\n    \"close\",\n    \"COMPLEX_DTYPES\",\n    \"convert_rows_list_to_csv_str\",\n    \"DATETIME64_DTYPES\",\n    \"decompress_file\",\n    \"EMPTY_STRING_PATTERN\",\n    \"ENDIAN\",\n    \"ensure_clean\",\n    \"ensure_safe_environment_variables\",\n    \"equalContents\",\n    \"external_error_raised\",\n    \"FLOAT_EA_DTYPES\",\n    \"FLOAT_NUMPY_DTYPES\",\n    \"getCols\",\n    \"get_cython_table_params\",\n    \"get_dtype\",\n    \"getitem\",\n    \"get_locales\",\n    \"getMixedTypeDict\",\n    \"get_obj\",\n    \"get_op_from_name\",\n    \"getPeriodData\",\n    \"getSeriesData\",\n    \"getTimeSeriesData\",\n    \"iat\",\n    \"iloc\",\n    \"index_subclass_makers_generator\",\n    \"loc\",\n    \"makeBoolIndex\",\n    \"makeCategoricalIndex\",\n    \"makeCustomDataframe\",\n    \"makeCustomIndex\",\n    \"makeDataFrame\",\n    \"makeDateIndex\",\n    \"makeFloatIndex\",\n    \"makeFloatSeries\",\n    \"makeIntervalIndex\",\n    \"makeIntIndex\",\n    \"makeMissingDataframe\",\n    \"makeMixedDataFrame\",\n    \"makeMultiIndex\",\n    \"makeNumericIndex\",\n    \"makeObjectSeries\",\n    \"makePeriodFrame\",\n    \"makePeriodIndex\",\n    \"makePeriodSeries\",\n    \"make_rand_series\",\n    \"makeRangeIndex\",\n    \"makeStringIndex\",\n    \"makeStringSeries\",\n    \"makeTimeDataFrame\",\n    \"makeTimedeltaIndex\",\n    \"makeTimeSeries\",\n    \"makeUIntIndex\",\n    \"maybe_produces_warning\",\n    \"NARROW_NP_DTYPES\",\n    \"network\",\n    \"NP_NAT_OBJECTS\",\n    \"NULL_OBJECTS\",\n    \"OBJECT_DTYPES\",\n    \"raise_assert_detail\",\n    \"rands\",\n    \"reset_display_options\",\n    \"raises_chained_assignment_error\",\n    \"round_trip_localpath\",\n    \"round_trip_pathlib\",\n    \"round_trip_pickle\",\n    \"setitem\",\n    \"set_locale\",\n    \"set_timezone\",\n    \"shares_memory\",\n    \"SIGNED_INT_EA_DTYPES\",\n    \"SIGNED_INT_NUMPY_DTYPES\",\n    \"STRING_DTYPES\",\n    \"SubclassedCategorical\",\n    \"SubclassedDataFrame\",\n    \"SubclassedSeries\",\n    \"TIMEDELTA64_DTYPES\",\n    \"to_array\",\n    \"UNSIGNED_INT_EA_DTYPES\",\n    \"UNSIGNED_INT_NUMPY_DTYPES\",\n    \"use_numexpr\",\n    \"with_csv_dialect\",\n    \"write_to_compressed\",\n]\n"
    },
    {
      "filename": "pandas/_testing/_io.py",
      "content": "from __future__ import annotations\n\nimport bz2\nfrom functools import wraps\nimport gzip\nimport io\nimport socket\nimport tarfile\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n)\nimport zipfile\n\nfrom pandas.compat import get_lzma_file\nfrom pandas.compat._optional import import_optional_dependency\n\nimport pandas as pd\nfrom pandas._testing._random import rands\nfrom pandas._testing.contexts import ensure_clean\n\nfrom pandas.io.common import urlopen\n\nif TYPE_CHECKING:\n    from pandas._typing import (\n        FilePath,\n        ReadPickleBuffer,\n    )\n\n    from pandas import (\n        DataFrame,\n        Series,\n    )\n\n# skip tests on exceptions with these messages\n_network_error_messages = (\n    # 'urlopen error timed out',\n    # 'timeout: timed out',\n    # 'socket.timeout: timed out',\n    \"timed out\",\n    \"Server Hangup\",\n    \"HTTP Error 503: Service Unavailable\",\n    \"502: Proxy Error\",\n    \"HTTP Error 502: internal error\",\n    \"HTTP Error 502\",\n    \"HTTP Error 503\",\n    \"HTTP Error 403\",\n    \"HTTP Error 400\",\n    \"Temporary failure in name resolution\",\n    \"Name or service not known\",\n    \"Connection refused\",\n    \"certificate verify\",\n)\n\n# or this e.errno/e.reason.errno\n_network_errno_vals = (\n    101,  # Network is unreachable\n    111,  # Connection refused\n    110,  # Connection timed out\n    104,  # Connection reset Error\n    54,  # Connection reset by peer\n    60,  # urllib.error.URLError: [Errno 60] Connection timed out\n)\n\n# Both of the above shouldn't mask real issues such as 404's\n# or refused connections (changed DNS).\n# But some tests (test_data yahoo) contact incredibly flakey\n# servers.\n\n# and conditionally raise on exception types in _get_default_network_errors\n\n\ndef _get_default_network_errors():\n    # Lazy import for http.client & urllib.error\n    # because it imports many things from the stdlib\n    import http.client\n    import urllib.error\n\n    return (\n        OSError,\n        http.client.HTTPException,\n        TimeoutError,\n        urllib.error.URLError,\n        socket.timeout,\n    )\n\n\ndef optional_args(decorator):\n    \"\"\"\n    allows a decorator to take optional positional and keyword arguments.\n    Assumes that taking a single, callable, positional argument means that\n    it is decorating a function, i.e. something like this::\n\n        @my_decorator\n        def function(): pass\n\n    Calls decorator with decorator(f, *args, **kwargs)\n    \"\"\"\n\n    @wraps(decorator)\n    def wrapper(*args, **kwargs):\n        def dec(f):\n            return decorator(f, *args, **kwargs)\n\n        is_decorating = not kwargs and len(args) == 1 and callable(args[0])\n        if is_decorating:\n            f = args[0]\n            args = ()\n            return dec(f)\n        else:\n            return dec\n\n    return wrapper\n\n\n# error: Untyped decorator makes function \"network\" untyped\n@optional_args  # type: ignore[misc]\ndef network(\n    t,\n    url: str = \"https://www.google.com\",\n    raise_on_error: bool = False,\n    check_before_test: bool = False,\n    error_classes=None,\n    skip_errnos=_network_errno_vals,\n    _skip_on_messages=_network_error_messages,\n):\n    \"\"\"\n    Label a test as requiring network connection and, if an error is\n    encountered, only raise if it does not find a network connection.\n\n    In comparison to ``network``, this assumes an added contract to your test:\n    you must assert that, under normal conditions, your test will ONLY fail if\n    it does not have network connectivity.\n\n    You can call this in 3 ways: as a standard decorator, with keyword\n    arguments, or with a positional argument that is the url to check.\n\n    Parameters\n    ----------\n    t : callable\n        The test requiring network connectivity.\n    url : path\n        The url to test via ``pandas.io.common.urlopen`` to check\n        for connectivity. Defaults to 'https://www.google.com'.\n    raise_on_error : bool\n        If True, never catches errors.\n    check_before_test : bool\n        If True, checks connectivity before running the test case.\n    error_classes : tuple or Exception\n        error classes to ignore. If not in ``error_classes``, raises the error.\n        defaults to OSError. Be careful about changing the error classes here.\n    skip_errnos : iterable of int\n        Any exception that has .errno or .reason.erno set to one\n        of these values will be skipped with an appropriate\n        message.\n    _skip_on_messages: iterable of string\n        any exception e for which one of the strings is\n        a substring of str(e) will be skipped with an appropriate\n        message. Intended to suppress errors where an errno isn't available.\n\n    Notes\n    -----\n    * ``raise_on_error`` supersedes ``check_before_test``\n\n    Returns\n    -------\n    t : callable\n        The decorated test ``t``, with checks for connectivity errors.\n\n    Example\n    -------\n\n    Tests decorated with @network will fail if it's possible to make a network\n    connection to another URL (defaults to google.com)::\n\n      >>> from pandas import _testing as tm\n      >>> @tm.network\n      ... def test_network():\n      ...     with pd.io.common.urlopen(\"rabbit://bonanza.com\"):\n      ...         pass\n      >>> test_network()  # doctest: +SKIP\n      Traceback\n         ...\n      URLError: <urlopen error unknown url type: rabbit>\n\n      You can specify alternative URLs::\n\n        >>> @tm.network(\"https://www.yahoo.com\")\n        ... def test_something_with_yahoo():\n        ...    raise OSError(\"Failure Message\")\n        >>> test_something_with_yahoo()  # doctest: +SKIP\n        Traceback (most recent call last):\n            ...\n        OSError: Failure Message\n\n    If you set check_before_test, it will check the url first and not run the\n    test on failure::\n\n        >>> @tm.network(\"failing://url.blaher\", check_before_test=True)\n        ... def test_something():\n        ...     print(\"I ran!\")\n        ...     raise ValueError(\"Failure\")\n        >>> test_something()  # doctest: +SKIP\n        Traceback (most recent call last):\n            ...\n\n    Errors not related to networking will always be raised.\n    \"\"\"\n    import pytest\n\n    if error_classes is None:\n        error_classes = _get_default_network_errors()\n\n    t.network = True\n\n    @wraps(t)\n    def wrapper(*args, **kwargs):\n        if (\n            check_before_test\n            and not raise_on_error\n            and not can_connect(url, error_classes)\n        ):\n            pytest.skip(\n                f\"May not have network connectivity because cannot connect to {url}\"\n            )\n        try:\n            return t(*args, **kwargs)\n        except Exception as err:\n            errno = getattr(err, \"errno\", None)\n            if not errno and hasattr(errno, \"reason\"):\n                # error: \"Exception\" has no attribute \"reason\"\n                errno = getattr(err.reason, \"errno\", None)  # type: ignore[attr-defined]\n\n            if errno in skip_errnos:\n                pytest.skip(f\"Skipping test due to known errno and error {err}\")\n\n            e_str = str(err)\n\n            if any(m.lower() in e_str.lower() for m in _skip_on_messages):\n                pytest.skip(\n                    f\"Skipping test because exception message is known and error {err}\"\n                )\n\n            if not isinstance(err, error_classes) or raise_on_error:\n                raise\n            pytest.skip(f\"Skipping test due to lack of connectivity and error {err}\")\n\n    return wrapper\n\n\ndef can_connect(url, error_classes=None) -> bool:\n    \"\"\"\n    Try to connect to the given url. True if succeeds, False if OSError\n    raised\n\n    Parameters\n    ----------\n    url : basestring\n        The URL to try to connect to\n\n    Returns\n    -------\n    connectable : bool\n        Return True if no OSError (unable to connect) or URLError (bad url) was\n        raised\n    \"\"\"\n    if error_classes is None:\n        error_classes = _get_default_network_errors()\n\n    try:\n        with urlopen(url, timeout=20) as response:\n            # Timeout just in case rate-limiting is applied\n            if response.status != 200:\n                return False\n    except error_classes:\n        return False\n    else:\n        return True\n\n\n# ------------------------------------------------------------------\n# File-IO\n\n\ndef round_trip_pickle(\n    obj: Any, path: FilePath | ReadPickleBuffer | None = None\n) -> DataFrame | Series:\n    \"\"\"\n    Pickle an object and then read it again.\n\n    Parameters\n    ----------\n    obj : any object\n        The object to pickle and then re-read.\n    path : str, path object or file-like object, default None\n        The path where the pickled object is written and then read.\n\n    Returns\n    -------\n    pandas object\n        The original object that was pickled and then re-read.\n    \"\"\"\n    _path = path\n    if _path is None:\n        _path = f\"__{rands(10)}__.pickle\"\n    with ensure_clean(_path) as temp_path:\n        pd.to_pickle(obj, temp_path)\n        return pd.read_pickle(temp_path)\n\n\ndef round_trip_pathlib(writer, reader, path: str | None = None):\n    \"\"\"\n    Write an object to file specified by a pathlib.Path and read it back\n\n    Parameters\n    ----------\n    writer : callable bound to pandas object\n        IO writing function (e.g. DataFrame.to_csv )\n    reader : callable\n        IO reading function (e.g. pd.read_csv )\n    path : str, default None\n        The path where the object is written and then read.\n\n    Returns\n    -------\n    pandas object\n        The original object that was serialized and then re-read.\n    \"\"\"\n    import pytest\n\n    Path = pytest.importorskip(\"pathlib\").Path\n    if path is None:\n        path = \"___pathlib___\"\n    with ensure_clean(path) as path:\n        writer(Path(path))\n        obj = reader(Path(path))\n    return obj\n\n\ndef round_trip_localpath(writer, reader, path: str | None = None):\n    \"\"\"\n    Write an object to file specified by a py.path LocalPath and read it back.\n\n    Parameters\n    ----------\n    writer : callable bound to pandas object\n        IO writing function (e.g. DataFrame.to_csv )\n    reader : callable\n        IO reading function (e.g. pd.read_csv )\n    path : str, default None\n        The path where the object is written and then read.\n\n    Returns\n    -------\n    pandas object\n        The original object that was serialized and then re-read.\n    \"\"\"\n    import pytest\n\n    LocalPath = pytest.importorskip(\"py.path\").local\n    if path is None:\n        path = \"___localpath___\"\n    with ensure_clean(path) as path:\n        writer(LocalPath(path))\n        obj = reader(LocalPath(path))\n    return obj\n\n\ndef write_to_compressed(compression, path, data, dest: str = \"test\"):\n    \"\"\"\n    Write data to a compressed file.\n\n    Parameters\n    ----------\n    compression : {'gzip', 'bz2', 'zip', 'xz', 'zstd'}\n        The compression type to use.\n    path : str\n        The file path to write the data.\n    data : str\n        The data to write.\n    dest : str, default \"test\"\n        The destination file (for ZIP only)\n\n    Raises\n    ------\n    ValueError : An invalid compression value was passed in.\n    \"\"\"\n    args: tuple[Any, ...] = (data,)\n    mode = \"wb\"\n    method = \"write\"\n    compress_method: Callable\n\n    if compression == \"zip\":\n        compress_method = zipfile.ZipFile\n        mode = \"w\"\n        args = (dest, data)\n        method = \"writestr\"\n    elif compression == \"tar\":\n        compress_method = tarfile.TarFile\n        mode = \"w\"\n        file = tarfile.TarInfo(name=dest)\n        bytes = io.BytesIO(data)\n        file.size = len(data)\n        args = (file, bytes)\n        method = \"addfile\"\n    elif compression == \"gzip\":\n        compress_method = gzip.GzipFile\n    elif compression == \"bz2\":\n        compress_method = bz2.BZ2File\n    elif compression == \"zstd\":\n        compress_method = import_optional_dependency(\"zstandard\").open\n    elif compression == \"xz\":\n        compress_method = get_lzma_file()\n    else:\n        raise ValueError(f\"Unrecognized compression type: {compression}\")\n\n    with compress_method(path, mode=mode) as f:\n        getattr(f, method)(*args)\n\n\n# ------------------------------------------------------------------\n# Plotting\n\n\ndef close(fignum=None) -> None:\n    from matplotlib.pyplot import (\n        close as _close,\n        get_fignums,\n    )\n\n    if fignum is None:\n        for fignum in get_fignums():\n            _close(fignum)\n    else:\n        _close(fignum)\n"
    },
    {
      "filename": "pandas/_testing/_random.py",
      "content": "from __future__ import annotations\n\nimport string\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\n\nif TYPE_CHECKING:\n    from pandas._typing import NpDtype\nRANDS_CHARS = np.array(list(string.ascii_letters + string.digits), dtype=(np.str_, 1))\n\n\ndef rands_array(nchars, size, dtype: NpDtype = \"O\", replace: bool = True) -> np.ndarray:\n    \"\"\"\n    Generate an array of byte strings.\n    \"\"\"\n    retval = (\n        np.random.choice(RANDS_CHARS, size=nchars * np.prod(size), replace=replace)\n        .view((np.str_, nchars))\n        .reshape(size)\n    )\n    return retval.astype(dtype)\n\n\ndef rands(nchars) -> str:\n    \"\"\"\n    Generate one random byte string.\n\n    See `rands_array` if you want to create an array of random strings.\n\n    \"\"\"\n    return \"\".join(np.random.choice(RANDS_CHARS, nchars))\n"
    },
    {
      "filename": "pandas/_testing/compat.py",
      "content": "\"\"\"\nHelpers for sharing tests between DataFrame/Series\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom pandas import DataFrame\n\nif TYPE_CHECKING:\n    from pandas._typing import DtypeObj\n\n\ndef get_dtype(obj) -> DtypeObj:\n    if isinstance(obj, DataFrame):\n        # Note: we are assuming only one column\n        return obj.dtypes.iat[0]\n    else:\n        return obj.dtype\n\n\ndef get_obj(df: DataFrame, klass):\n    \"\"\"\n    For sharing tests using frame_or_series, either return the DataFrame\n    unchanged or return it's first column as a Series.\n    \"\"\"\n    if klass is DataFrame:\n        return df\n    return df._ixs(0, axis=1)\n"
    },
    {
      "filename": "pandas/_testing/contexts.py",
      "content": "from __future__ import annotations\n\nfrom contextlib import contextmanager\nimport os\nfrom pathlib import Path\nimport tempfile\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Generator,\n)\nimport uuid\n\nfrom pandas.compat import PYPY\nfrom pandas.errors import ChainedAssignmentError\n\nfrom pandas import set_option\n\nfrom pandas.io.common import get_handle\n\nif TYPE_CHECKING:\n    from pandas._typing import (\n        BaseBuffer,\n        CompressionOptions,\n        FilePath,\n    )\n\n\n@contextmanager\ndef decompress_file(\n    path: FilePath | BaseBuffer, compression: CompressionOptions\n) -> Generator[IO[bytes], None, None]:\n    \"\"\"\n    Open a compressed file and return a file object.\n\n    Parameters\n    ----------\n    path : str\n        The path where the file is read from.\n\n    compression : {'gzip', 'bz2', 'zip', 'xz', 'zstd', None}\n        Name of the decompression to use\n\n    Returns\n    -------\n    file object\n    \"\"\"\n    with get_handle(path, \"rb\", compression=compression, is_text=False) as handle:\n        yield handle.handle\n\n\n@contextmanager\ndef set_timezone(tz: str) -> Generator[None, None, None]:\n    \"\"\"\n    Context manager for temporarily setting a timezone.\n\n    Parameters\n    ----------\n    tz : str\n        A string representing a valid timezone.\n\n    Examples\n    --------\n    >>> from datetime import datetime\n    >>> from dateutil.tz import tzlocal\n    >>> tzlocal().tzname(datetime(2021, 1, 1))  # doctest: +SKIP\n    'IST'\n\n    >>> with set_timezone('US/Eastern'):\n    ...     tzlocal().tzname(datetime(2021, 1, 1))\n    ...\n    'EST'\n    \"\"\"\n    import time\n\n    def setTZ(tz) -> None:\n        if tz is None:\n            try:\n                del os.environ[\"TZ\"]\n            except KeyError:\n                pass\n        else:\n            os.environ[\"TZ\"] = tz\n            time.tzset()\n\n    orig_tz = os.environ.get(\"TZ\")\n    setTZ(tz)\n    try:\n        yield\n    finally:\n        setTZ(orig_tz)\n\n\n@contextmanager\ndef ensure_clean(\n    filename=None, return_filelike: bool = False, **kwargs: Any\n) -> Generator[Any, None, None]:\n    \"\"\"\n    Gets a temporary path and agrees to remove on close.\n\n    This implementation does not use tempfile.mkstemp to avoid having a file handle.\n    If the code using the returned path wants to delete the file itself, windows\n    requires that no program has a file handle to it.\n\n    Parameters\n    ----------\n    filename : str (optional)\n        suffix of the created file.\n    return_filelike : bool (default False)\n        if True, returns a file-like which is *always* cleaned. Necessary for\n        savefig and other functions which want to append extensions.\n    **kwargs\n        Additional keywords are passed to open().\n\n    \"\"\"\n    folder = Path(tempfile.gettempdir())\n\n    if filename is None:\n        filename = \"\"\n    filename = str(uuid.uuid4()) + filename\n    path = folder / filename\n\n    path.touch()\n\n    handle_or_str: str | IO = str(path)\n    if return_filelike:\n        kwargs.setdefault(\"mode\", \"w+b\")\n        handle_or_str = open(path, **kwargs)\n\n    try:\n        yield handle_or_str\n    finally:\n        if not isinstance(handle_or_str, str):\n            handle_or_str.close()\n        if path.is_file():\n            path.unlink()\n\n\n@contextmanager\ndef ensure_safe_environment_variables() -> Generator[None, None, None]:\n    \"\"\"\n    Get a context manager to safely set environment variables\n\n    All changes will be undone on close, hence environment variables set\n    within this contextmanager will neither persist nor change global state.\n    \"\"\"\n    saved_environ = dict(os.environ)\n    try:\n        yield\n    finally:\n        os.environ.clear()\n        os.environ.update(saved_environ)\n\n\n@contextmanager\ndef with_csv_dialect(name, **kwargs) -> Generator[None, None, None]:\n    \"\"\"\n    Context manager to temporarily register a CSV dialect for parsing CSV.\n\n    Parameters\n    ----------\n    name : str\n        The name of the dialect.\n    kwargs : mapping\n        The parameters for the dialect.\n\n    Raises\n    ------\n    ValueError : the name of the dialect conflicts with a builtin one.\n\n    See Also\n    --------\n    csv : Python's CSV library.\n    \"\"\"\n    import csv\n\n    _BUILTIN_DIALECTS = {\"excel\", \"excel-tab\", \"unix\"}\n\n    if name in _BUILTIN_DIALECTS:\n        raise ValueError(\"Cannot override builtin dialect.\")\n\n    csv.register_dialect(name, **kwargs)\n    try:\n        yield\n    finally:\n        csv.unregister_dialect(name)\n\n\n@contextmanager\ndef use_numexpr(use, min_elements=None) -> Generator[None, None, None]:\n    from pandas.core.computation import expressions as expr\n\n    if min_elements is None:\n        min_elements = expr._MIN_ELEMENTS\n\n    olduse = expr.USE_NUMEXPR\n    oldmin = expr._MIN_ELEMENTS\n    set_option(\"compute.use_numexpr\", use)\n    expr._MIN_ELEMENTS = min_elements\n    try:\n        yield\n    finally:\n        expr._MIN_ELEMENTS = oldmin\n        set_option(\"compute.use_numexpr\", olduse)\n\n\ndef raises_chained_assignment_error():\n    if PYPY:\n        from contextlib import nullcontext\n\n        return nullcontext()\n    else:\n        import pytest\n\n        return pytest.raises(\n            ChainedAssignmentError,\n            match=(\n                \"A value is trying to be set on a copy of a DataFrame or Series \"\n                \"through chained assignment\"\n            ),\n        )\n"
    },
    {
      "filename": "pandas/compat/__init__.py",
      "content": "\"\"\"\ncompat\n======\n\nCross-compatible functions for different versions of Python.\n\nOther items:\n* platform checker\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport platform\nimport sys\nfrom typing import TYPE_CHECKING\n\nfrom pandas.compat._constants import (\n    IS64,\n    PY39,\n    PY310,\n    PY311,\n    PYPY,\n)\nimport pandas.compat.compressors\nfrom pandas.compat.numpy import (\n    is_numpy_dev,\n    np_version_under1p21,\n)\nfrom pandas.compat.pyarrow import (\n    pa_version_under7p0,\n    pa_version_under8p0,\n    pa_version_under9p0,\n    pa_version_under11p0,\n)\n\nif TYPE_CHECKING:\n    from pandas._typing import F\n\n\ndef set_function_name(f: F, name: str, cls) -> F:\n    \"\"\"\n    Bind the name/qualname attributes of the function.\n    \"\"\"\n    f.__name__ = name\n    f.__qualname__ = f\"{cls.__name__}.{name}\"\n    f.__module__ = cls.__module__\n    return f\n\n\ndef is_platform_little_endian() -> bool:\n    \"\"\"\n    Checking if the running platform is little endian.\n\n    Returns\n    -------\n    bool\n        True if the running platform is little endian.\n    \"\"\"\n    return sys.byteorder == \"little\"\n\n\ndef is_platform_windows() -> bool:\n    \"\"\"\n    Checking if the running platform is windows.\n\n    Returns\n    -------\n    bool\n        True if the running platform is windows.\n    \"\"\"\n    return sys.platform in [\"win32\", \"cygwin\"]\n\n\ndef is_platform_linux() -> bool:\n    \"\"\"\n    Checking if the running platform is linux.\n\n    Returns\n    -------\n    bool\n        True if the running platform is linux.\n    \"\"\"\n    return sys.platform == \"linux\"\n\n\ndef is_platform_mac() -> bool:\n    \"\"\"\n    Checking if the running platform is mac.\n\n    Returns\n    -------\n    bool\n        True if the running platform is mac.\n    \"\"\"\n    return sys.platform == \"darwin\"\n\n\ndef is_platform_arm() -> bool:\n    \"\"\"\n    Checking if the running platform use ARM architecture.\n\n    Returns\n    -------\n    bool\n        True if the running platform uses ARM architecture.\n    \"\"\"\n    return platform.machine() in (\"arm64\", \"aarch64\") or platform.machine().startswith(\n        \"armv\"\n    )\n\n\ndef is_platform_power() -> bool:\n    \"\"\"\n    Checking if the running platform use Power architecture.\n\n    Returns\n    -------\n    bool\n        True if the running platform uses ARM architecture.\n    \"\"\"\n    return platform.machine() in (\"ppc64\", \"ppc64le\")\n\n\ndef is_ci_environment() -> bool:\n    \"\"\"\n    Checking if running in a continuous integration environment by checking\n    the PANDAS_CI environment variable.\n\n    Returns\n    -------\n    bool\n        True if the running in a continuous integration environment.\n    \"\"\"\n    return os.environ.get(\"PANDAS_CI\", \"0\") == \"1\"\n\n\ndef get_lzma_file() -> type[pandas.compat.compressors.LZMAFile]:\n    \"\"\"\n    Importing the `LZMAFile` class from the `lzma` module.\n\n    Returns\n    -------\n    class\n        The `LZMAFile` class from the `lzma` module.\n\n    Raises\n    ------\n    RuntimeError\n        If the `lzma` module was not imported correctly, or didn't exist.\n    \"\"\"\n    if not pandas.compat.compressors.has_lzma:\n        raise RuntimeError(\n            \"lzma module not available. \"\n            \"A Python re-install with the proper dependencies, \"\n            \"might be required to solve this issue.\"\n        )\n    return pandas.compat.compressors.LZMAFile\n\n\n__all__ = [\n    \"is_numpy_dev\",\n    \"np_version_under1p21\",\n    \"pa_version_under7p0\",\n    \"pa_version_under8p0\",\n    \"pa_version_under9p0\",\n    \"pa_version_under11p0\",\n    \"IS64\",\n    \"PY39\",\n    \"PY310\",\n    \"PY311\",\n    \"PYPY\",\n]\n"
    },
    {
      "filename": "pandas/compat/_optional.py",
      "content": "from __future__ import annotations\n\nimport importlib\nimport sys\nfrom typing import TYPE_CHECKING\nimport warnings\n\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.util.version import Version\n\nif TYPE_CHECKING:\n    import types\n\n# Update install.rst & setup.cfg when updating versions!\n\nVERSIONS = {\n    \"bs4\": \"4.9.3\",\n    \"blosc\": \"1.21.0\",\n    \"bottleneck\": \"1.3.2\",\n    \"brotli\": \"0.7.0\",\n    \"fastparquet\": \"0.6.3\",\n    \"fsspec\": \"2021.07.0\",\n    \"html5lib\": \"1.1\",\n    \"hypothesis\": \"6.34.2\",\n    \"gcsfs\": \"2021.07.0\",\n    \"jinja2\": \"3.0.0\",\n    \"lxml.etree\": \"4.6.3\",\n    \"matplotlib\": \"3.6.1\",\n    \"numba\": \"0.53.1\",\n    \"numexpr\": \"2.7.3\",\n    \"odfpy\": \"1.4.1\",\n    \"openpyxl\": \"3.0.7\",\n    \"pandas_gbq\": \"0.15.0\",\n    \"psycopg2\": \"2.8.6\",  # (dt dec pq3 ext lo64)\n    \"pymysql\": \"1.0.2\",\n    \"pyarrow\": \"7.0.0\",\n    \"pyreadstat\": \"1.1.2\",\n    \"pytest\": \"7.0.0\",\n    \"pyxlsb\": \"1.0.8\",\n    \"s3fs\": \"2021.08.0\",\n    \"scipy\": \"1.7.1\",\n    \"snappy\": \"0.6.0\",\n    \"sqlalchemy\": \"1.4.16\",\n    \"tables\": \"3.6.1\",\n    \"tabulate\": \"0.8.9\",\n    \"xarray\": \"0.21.0\",\n    \"xlrd\": \"2.0.1\",\n    \"xlsxwriter\": \"1.4.3\",\n    \"zstandard\": \"0.15.2\",\n    \"tzdata\": \"2022.1\",\n    \"qtpy\": \"2.2.0\",\n    \"pyqt5\": \"5.15.1\",\n}\n\n# A mapping from import name to package name (on PyPI) for packages where\n# these two names are different.\n\nINSTALL_MAPPING = {\n    \"bs4\": \"beautifulsoup4\",\n    \"bottleneck\": \"Bottleneck\",\n    \"brotli\": \"brotlipy\",\n    \"jinja2\": \"Jinja2\",\n    \"lxml.etree\": \"lxml\",\n    \"odf\": \"odfpy\",\n    \"pandas_gbq\": \"pandas-gbq\",\n    \"snappy\": \"python-snappy\",\n    \"sqlalchemy\": \"SQLAlchemy\",\n    \"tables\": \"pytables\",\n}\n\n\ndef get_version(module: types.ModuleType) -> str:\n    version = getattr(module, \"__version__\", None)\n    if version is None:\n        # xlrd uses a capitalized attribute name\n        version = getattr(module, \"__VERSION__\", None)\n\n    if version is None:\n        if module.__name__ == \"brotli\":\n            # brotli doesn't contain attributes to confirm it's version\n            return \"\"\n        if module.__name__ == \"snappy\":\n            # snappy doesn't contain attributes to confirm it's version\n            # See https://github.com/andrix/python-snappy/pull/119\n            return \"\"\n        raise ImportError(f\"Can't determine version for {module.__name__}\")\n    if module.__name__ == \"psycopg2\":\n        # psycopg2 appends \" (dt dec pq3 ext lo64)\" to it's version\n        version = version.split()[0]\n    return version\n\n\ndef import_optional_dependency(\n    name: str,\n    extra: str = \"\",\n    errors: str = \"raise\",\n    min_version: str | None = None,\n):\n    \"\"\"\n    Import an optional dependency.\n\n    By default, if a dependency is missing an ImportError with a nice\n    message will be raised. If a dependency is present, but too old,\n    we raise.\n\n    Parameters\n    ----------\n    name : str\n        The module name.\n    extra : str\n        Additional text to include in the ImportError message.\n    errors : str {'raise', 'warn', 'ignore'}\n        What to do when a dependency is not found or its version is too old.\n\n        * raise : Raise an ImportError\n        * warn : Only applicable when a module's version is to old.\n          Warns that the version is too old and returns None\n        * ignore: If the module is not installed, return None, otherwise,\n          return the module, even if the version is too old.\n          It's expected that users validate the version locally when\n          using ``errors=\"ignore\"`` (see. ``io/html.py``)\n    min_version : str, default None\n        Specify a minimum version that is different from the global pandas\n        minimum version required.\n    Returns\n    -------\n    maybe_module : Optional[ModuleType]\n        The imported module, when found and the version is correct.\n        None is returned when the package is not found and `errors`\n        is False, or when the package's version is too old and `errors`\n        is ``'warn'``.\n    \"\"\"\n\n    assert errors in {\"warn\", \"raise\", \"ignore\"}\n\n    package_name = INSTALL_MAPPING.get(name)\n    install_name = package_name if package_name is not None else name\n\n    msg = (\n        f\"Missing optional dependency '{install_name}'. {extra} \"\n        f\"Use pip or conda to install {install_name}.\"\n    )\n    try:\n        module = importlib.import_module(name)\n    except ImportError:\n        if errors == \"raise\":\n            raise ImportError(msg)\n        return None\n\n    # Handle submodules: if we have submodule, grab parent module from sys.modules\n    parent = name.split(\".\")[0]\n    if parent != name:\n        install_name = parent\n        module_to_get = sys.modules[install_name]\n    else:\n        module_to_get = module\n    minimum_version = min_version if min_version is not None else VERSIONS.get(parent)\n    if minimum_version:\n        version = get_version(module_to_get)\n        if version and Version(version) < Version(minimum_version):\n            msg = (\n                f\"Pandas requires version '{minimum_version}' or newer of '{parent}' \"\n                f\"(version '{version}' currently installed).\"\n            )\n            if errors == \"warn\":\n                warnings.warn(\n                    msg,\n                    UserWarning,\n                    stacklevel=find_stack_level(),\n                )\n                return None\n            elif errors == \"raise\":\n                raise ImportError(msg)\n\n    return module\n"
    },
    {
      "filename": "pandas/compat/numpy/function.py",
      "content": "\"\"\"\nFor compatibility with numpy libraries, pandas functions or methods have to\naccept '*args' and '**kwargs' parameters to accommodate numpy arguments that\nare not actually used or respected in the pandas implementation.\n\nTo ensure that users do not abuse these parameters, validation is performed in\n'validators.py' to make sure that any extra parameters passed correspond ONLY\nto those in the numpy signature. Part of that validation includes whether or\nnot the user attempted to pass in non-default values for these extraneous\nparameters. As we want to discourage users from relying on these parameters\nwhen calling the pandas implementation, we want them only to pass in the\ndefault values for these parameters.\n\nThis module provides a set of commonly used default arguments for functions and\nmethods that are spread throughout the codebase. This module will make it\neasier to adjust to future upstream changes in the analogous numpy signatures.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    TypeVar,\n    cast,\n    overload,\n)\n\nfrom numpy import ndarray\n\nfrom pandas._libs.lib import (\n    is_bool,\n    is_integer,\n)\nfrom pandas.errors import UnsupportedFunctionCall\nfrom pandas.util._validators import (\n    validate_args,\n    validate_args_and_kwargs,\n    validate_kwargs,\n)\n\nif TYPE_CHECKING:\n    from pandas._typing import (\n        Axis,\n        AxisInt,\n    )\n\n    AxisNoneT = TypeVar(\"AxisNoneT\", Axis, None)\n\n\nclass CompatValidator:\n    def __init__(\n        self,\n        defaults,\n        fname=None,\n        method: str | None = None,\n        max_fname_arg_count=None,\n    ) -> None:\n        self.fname = fname\n        self.method = method\n        self.defaults = defaults\n        self.max_fname_arg_count = max_fname_arg_count\n\n    def __call__(\n        self,\n        args,\n        kwargs,\n        fname=None,\n        max_fname_arg_count=None,\n        method: str | None = None,\n    ) -> None:\n        if args or kwargs:\n            fname = self.fname if fname is None else fname\n            max_fname_arg_count = (\n                self.max_fname_arg_count\n                if max_fname_arg_count is None\n                else max_fname_arg_count\n            )\n            method = self.method if method is None else method\n\n            if method == \"args\":\n                validate_args(fname, args, max_fname_arg_count, self.defaults)\n            elif method == \"kwargs\":\n                validate_kwargs(fname, kwargs, self.defaults)\n            elif method == \"both\":\n                validate_args_and_kwargs(\n                    fname, args, kwargs, max_fname_arg_count, self.defaults\n                )\n            else:\n                raise ValueError(f\"invalid validation method '{method}'\")\n\n\nARGMINMAX_DEFAULTS = {\"out\": None}\nvalidate_argmin = CompatValidator(\n    ARGMINMAX_DEFAULTS, fname=\"argmin\", method=\"both\", max_fname_arg_count=1\n)\nvalidate_argmax = CompatValidator(\n    ARGMINMAX_DEFAULTS, fname=\"argmax\", method=\"both\", max_fname_arg_count=1\n)\n\n\ndef process_skipna(skipna: bool | ndarray | None, args) -> tuple[bool, Any]:\n    if isinstance(skipna, ndarray) or skipna is None:\n        args = (skipna,) + args\n        skipna = True\n\n    return skipna, args\n\n\ndef validate_argmin_with_skipna(skipna: bool | ndarray | None, args, kwargs) -> bool:\n    \"\"\"\n    If 'Series.argmin' is called via the 'numpy' library, the third parameter\n    in its signature is 'out', which takes either an ndarray or 'None', so\n    check if the 'skipna' parameter is either an instance of ndarray or is\n    None, since 'skipna' itself should be a boolean\n    \"\"\"\n    skipna, args = process_skipna(skipna, args)\n    validate_argmin(args, kwargs)\n    return skipna\n\n\ndef validate_argmax_with_skipna(skipna: bool | ndarray | None, args, kwargs) -> bool:\n    \"\"\"\n    If 'Series.argmax' is called via the 'numpy' library, the third parameter\n    in its signature is 'out', which takes either an ndarray or 'None', so\n    check if the 'skipna' parameter is either an instance of ndarray or is\n    None, since 'skipna' itself should be a boolean\n    \"\"\"\n    skipna, args = process_skipna(skipna, args)\n    validate_argmax(args, kwargs)\n    return skipna\n\n\nARGSORT_DEFAULTS: dict[str, int | str | None] = {}\nARGSORT_DEFAULTS[\"axis\"] = -1\nARGSORT_DEFAULTS[\"kind\"] = \"quicksort\"\nARGSORT_DEFAULTS[\"order\"] = None\nARGSORT_DEFAULTS[\"kind\"] = None\n\n\nvalidate_argsort = CompatValidator(\n    ARGSORT_DEFAULTS, fname=\"argsort\", max_fname_arg_count=0, method=\"both\"\n)\n\n# two different signatures of argsort, this second validation for when the\n# `kind` param is supported\nARGSORT_DEFAULTS_KIND: dict[str, int | None] = {}\nARGSORT_DEFAULTS_KIND[\"axis\"] = -1\nARGSORT_DEFAULTS_KIND[\"order\"] = None\nvalidate_argsort_kind = CompatValidator(\n    ARGSORT_DEFAULTS_KIND, fname=\"argsort\", max_fname_arg_count=0, method=\"both\"\n)\n\n\ndef validate_argsort_with_ascending(ascending: bool | int | None, args, kwargs) -> bool:\n    \"\"\"\n    If 'Categorical.argsort' is called via the 'numpy' library, the first\n    parameter in its signature is 'axis', which takes either an integer or\n    'None', so check if the 'ascending' parameter has either integer type or is\n    None, since 'ascending' itself should be a boolean\n    \"\"\"\n    if is_integer(ascending) or ascending is None:\n        args = (ascending,) + args\n        ascending = True\n\n    validate_argsort_kind(args, kwargs, max_fname_arg_count=3)\n    ascending = cast(bool, ascending)\n    return ascending\n\n\nCLIP_DEFAULTS: dict[str, Any] = {\"out\": None}\nvalidate_clip = CompatValidator(\n    CLIP_DEFAULTS, fname=\"clip\", method=\"both\", max_fname_arg_count=3\n)\n\n\n@overload\ndef validate_clip_with_axis(axis: ndarray, args, kwargs) -> None:\n    ...\n\n\n@overload\ndef validate_clip_with_axis(axis: AxisNoneT, args, kwargs) -> AxisNoneT:\n    ...\n\n\ndef validate_clip_with_axis(\n    axis: ndarray | AxisNoneT, args, kwargs\n) -> AxisNoneT | None:\n    \"\"\"\n    If 'NDFrame.clip' is called via the numpy library, the third parameter in\n    its signature is 'out', which can takes an ndarray, so check if the 'axis'\n    parameter is an instance of ndarray, since 'axis' itself should either be\n    an integer or None\n    \"\"\"\n    if isinstance(axis, ndarray):\n        args = (axis,) + args\n        # error: Incompatible types in assignment (expression has type \"None\",\n        # variable has type \"Union[ndarray[Any, Any], str, int]\")\n        axis = None  # type: ignore[assignment]\n\n    validate_clip(args, kwargs)\n    # error: Incompatible return value type (got \"Union[ndarray[Any, Any],\n    # str, int]\", expected \"Union[str, int, None]\")\n    return axis  # type: ignore[return-value]\n\n\nCUM_FUNC_DEFAULTS: dict[str, Any] = {}\nCUM_FUNC_DEFAULTS[\"dtype\"] = None\nCUM_FUNC_DEFAULTS[\"out\"] = None\nvalidate_cum_func = CompatValidator(\n    CUM_FUNC_DEFAULTS, method=\"both\", max_fname_arg_count=1\n)\nvalidate_cumsum = CompatValidator(\n    CUM_FUNC_DEFAULTS, fname=\"cumsum\", method=\"both\", max_fname_arg_count=1\n)\n\n\ndef validate_cum_func_with_skipna(skipna, args, kwargs, name) -> bool:\n    \"\"\"\n    If this function is called via the 'numpy' library, the third parameter in\n    its signature is 'dtype', which takes either a 'numpy' dtype or 'None', so\n    check if the 'skipna' parameter is a boolean or not\n    \"\"\"\n    if not is_bool(skipna):\n        args = (skipna,) + args\n        skipna = True\n\n    validate_cum_func(args, kwargs, fname=name)\n    return skipna\n\n\nALLANY_DEFAULTS: dict[str, bool | None] = {}\nALLANY_DEFAULTS[\"dtype\"] = None\nALLANY_DEFAULTS[\"out\"] = None\nALLANY_DEFAULTS[\"keepdims\"] = False\nALLANY_DEFAULTS[\"axis\"] = None\nvalidate_all = CompatValidator(\n    ALLANY_DEFAULTS, fname=\"all\", method=\"both\", max_fname_arg_count=1\n)\nvalidate_any = CompatValidator(\n    ALLANY_DEFAULTS, fname=\"any\", method=\"both\", max_fname_arg_count=1\n)\n\nLOGICAL_FUNC_DEFAULTS = {\"out\": None, \"keepdims\": False}\nvalidate_logical_func = CompatValidator(LOGICAL_FUNC_DEFAULTS, method=\"kwargs\")\n\nMINMAX_DEFAULTS = {\"axis\": None, \"out\": None, \"keepdims\": False}\nvalidate_min = CompatValidator(\n    MINMAX_DEFAULTS, fname=\"min\", method=\"both\", max_fname_arg_count=1\n)\nvalidate_max = CompatValidator(\n    MINMAX_DEFAULTS, fname=\"max\", method=\"both\", max_fname_arg_count=1\n)\n\nRESHAPE_DEFAULTS: dict[str, str] = {\"order\": \"C\"}\nvalidate_reshape = CompatValidator(\n    RESHAPE_DEFAULTS, fname=\"reshape\", method=\"both\", max_fname_arg_count=1\n)\n\nREPEAT_DEFAULTS: dict[str, Any] = {\"axis\": None}\nvalidate_repeat = CompatValidator(\n    REPEAT_DEFAULTS, fname=\"repeat\", method=\"both\", max_fname_arg_count=1\n)\n\nROUND_DEFAULTS: dict[str, Any] = {\"out\": None}\nvalidate_round = CompatValidator(\n    ROUND_DEFAULTS, fname=\"round\", method=\"both\", max_fname_arg_count=1\n)\n\nSORT_DEFAULTS: dict[str, int | str | None] = {}\nSORT_DEFAULTS[\"axis\"] = -1\nSORT_DEFAULTS[\"kind\"] = \"quicksort\"\nSORT_DEFAULTS[\"order\"] = None\nvalidate_sort = CompatValidator(SORT_DEFAULTS, fname=\"sort\", method=\"kwargs\")\n\nSTAT_FUNC_DEFAULTS: dict[str, Any | None] = {}\nSTAT_FUNC_DEFAULTS[\"dtype\"] = None\nSTAT_FUNC_DEFAULTS[\"out\"] = None\n\nSUM_DEFAULTS = STAT_FUNC_DEFAULTS.copy()\nSUM_DEFAULTS[\"axis\"] = None\nSUM_DEFAULTS[\"keepdims\"] = False\nSUM_DEFAULTS[\"initial\"] = None\n\nPROD_DEFAULTS = STAT_FUNC_DEFAULTS.copy()\nPROD_DEFAULTS[\"axis\"] = None\nPROD_DEFAULTS[\"keepdims\"] = False\nPROD_DEFAULTS[\"initial\"] = None\n\nMEDIAN_DEFAULTS = STAT_FUNC_DEFAULTS.copy()\nMEDIAN_DEFAULTS[\"overwrite_input\"] = False\nMEDIAN_DEFAULTS[\"keepdims\"] = False\n\nSTAT_FUNC_DEFAULTS[\"keepdims\"] = False\n\nvalidate_stat_func = CompatValidator(STAT_FUNC_DEFAULTS, method=\"kwargs\")\nvalidate_sum = CompatValidator(\n    SUM_DEFAULTS, fname=\"sum\", method=\"both\", max_fname_arg_count=1\n)\nvalidate_prod = CompatValidator(\n    PROD_DEFAULTS, fname=\"prod\", method=\"both\", max_fname_arg_count=1\n)\nvalidate_mean = CompatValidator(\n    STAT_FUNC_DEFAULTS, fname=\"mean\", method=\"both\", max_fname_arg_count=1\n)\nvalidate_median = CompatValidator(\n    MEDIAN_DEFAULTS, fname=\"median\", method=\"both\", max_fname_arg_count=1\n)\n\nSTAT_DDOF_FUNC_DEFAULTS: dict[str, bool | None] = {}\nSTAT_DDOF_FUNC_DEFAULTS[\"dtype\"] = None\nSTAT_DDOF_FUNC_DEFAULTS[\"out\"] = None\nSTAT_DDOF_FUNC_DEFAULTS[\"keepdims\"] = False\nvalidate_stat_ddof_func = CompatValidator(STAT_DDOF_FUNC_DEFAULTS, method=\"kwargs\")\n\nTAKE_DEFAULTS: dict[str, str | None] = {}\nTAKE_DEFAULTS[\"out\"] = None\nTAKE_DEFAULTS[\"mode\"] = \"raise\"\nvalidate_take = CompatValidator(TAKE_DEFAULTS, fname=\"take\", method=\"kwargs\")\n\n\ndef validate_take_with_convert(convert: ndarray | bool | None, args, kwargs) -> bool:\n    \"\"\"\n    If this function is called via the 'numpy' library, the third parameter in\n    its signature is 'axis', which takes either an ndarray or 'None', so check\n    if the 'convert' parameter is either an instance of ndarray or is None\n    \"\"\"\n    if isinstance(convert, ndarray) or convert is None:\n        args = (convert,) + args\n        convert = True\n\n    validate_take(args, kwargs, max_fname_arg_count=3, method=\"both\")\n    return convert\n\n\nTRANSPOSE_DEFAULTS = {\"axes\": None}\nvalidate_transpose = CompatValidator(\n    TRANSPOSE_DEFAULTS, fname=\"transpose\", method=\"both\", max_fname_arg_count=0\n)\n\n\ndef validate_groupby_func(name, args, kwargs, allowed=None) -> None:\n    \"\"\"\n    'args' and 'kwargs' should be empty, except for allowed kwargs because all\n    of their necessary parameters are explicitly listed in the function\n    signature\n    \"\"\"\n    if allowed is None:\n        allowed = []\n\n    kwargs = set(kwargs) - set(allowed)\n\n    if len(args) + len(kwargs) > 0:\n        raise UnsupportedFunctionCall(\n            \"numpy operations are not valid with groupby. \"\n            f\"Use .groupby(...).{name}() instead\"\n        )\n\n\nRESAMPLER_NUMPY_OPS = (\"min\", \"max\", \"sum\", \"prod\", \"mean\", \"std\", \"var\")\n\n\ndef validate_resampler_func(method: str, args, kwargs) -> None:\n    \"\"\"\n    'args' and 'kwargs' should be empty because all of their necessary\n    parameters are explicitly listed in the function signature\n    \"\"\"\n    if len(args) + len(kwargs) > 0:\n        if method in RESAMPLER_NUMPY_OPS:\n            raise UnsupportedFunctionCall(\n                \"numpy operations are not valid with resample. \"\n                f\"Use .resample(...).{method}() instead\"\n            )\n        raise TypeError(\"too many arguments passed in\")\n\n\ndef validate_minmax_axis(axis: AxisInt | None, ndim: int = 1) -> None:\n    \"\"\"\n    Ensure that the axis argument passed to min, max, argmin, or argmax is zero\n    or None, as otherwise it will be incorrectly ignored.\n\n    Parameters\n    ----------\n    axis : int or None\n    ndim : int, default 1\n\n    Raises\n    ------\n    ValueError\n    \"\"\"\n    if axis is None:\n        return\n    if axis >= ndim or (axis < 0 and ndim + axis < 0):\n        raise ValueError(f\"`axis` must be fewer than the number of dimensions ({ndim})\")\n"
    },
    {
      "filename": "pandas/core/_numba/executor.py",
      "content": "from __future__ import annotations\n\nimport functools\nfrom typing import (\n    TYPE_CHECKING,\n    Callable,\n)\n\nif TYPE_CHECKING:\n    from pandas._typing import Scalar\n\nimport numpy as np\n\nfrom pandas.compat._optional import import_optional_dependency\n\n\n@functools.lru_cache(maxsize=None)\ndef generate_shared_aggregator(\n    func: Callable[..., Scalar],\n    nopython: bool,\n    nogil: bool,\n    parallel: bool,\n):\n    \"\"\"\n    Generate a Numba function that loops over the columns 2D object and applies\n    a 1D numba kernel over each column.\n\n    Parameters\n    ----------\n    func : function\n        aggregation function to be applied to each column\n    nopython : bool\n        nopython to be passed into numba.jit\n    nogil : bool\n        nogil to be passed into numba.jit\n    parallel : bool\n        parallel to be passed into numba.jit\n\n    Returns\n    -------\n    Numba function\n    \"\"\"\n    if TYPE_CHECKING:\n        import numba\n    else:\n        numba = import_optional_dependency(\"numba\")\n\n    @numba.jit(nopython=nopython, nogil=nogil, parallel=parallel)\n    def column_looper(\n        values: np.ndarray,\n        start: np.ndarray,\n        end: np.ndarray,\n        min_periods: int,\n        *args,\n    ):\n        result = np.empty((len(start), values.shape[1]), dtype=np.float64)\n        for i in numba.prange(values.shape[1]):\n            result[:, i] = func(values[:, i], start, end, min_periods, *args)\n        return result\n\n    return column_looper\n"
    },
    {
      "filename": "pandas/core/_numba/kernels/shared.py",
      "content": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nimport numba\n\nif TYPE_CHECKING:\n    import numpy as np\n\n\n@numba.jit(\n    # error: Any? not callable\n    numba.boolean(numba.int64[:]),  # type: ignore[misc]\n    nopython=True,\n    nogil=True,\n    parallel=False,\n)\ndef is_monotonic_increasing(bounds: np.ndarray) -> bool:\n    \"\"\"Check if int64 values are monotonically increasing.\"\"\"\n    n = len(bounds)\n    if n < 2:\n        return True\n    prev = bounds[0]\n    for i in range(1, n):\n        cur = bounds[i]\n        if cur < prev:\n            return False\n        prev = cur\n    return True\n"
    },
    {
      "filename": "pandas/core/computation/expressions.py",
      "content": "\"\"\"\nExpressions\n-----------\n\nOffer fast expression evaluation through numexpr\n\n\"\"\"\nfrom __future__ import annotations\n\nimport operator\nfrom typing import TYPE_CHECKING\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.core.computation.check import NUMEXPR_INSTALLED\nfrom pandas.core.ops import roperator\n\nif NUMEXPR_INSTALLED:\n    import numexpr as ne\n\nif TYPE_CHECKING:\n    from pandas._typing import FuncType\n\n_TEST_MODE: bool | None = None\n_TEST_RESULT: list[bool] = []\nUSE_NUMEXPR = NUMEXPR_INSTALLED\n_evaluate: FuncType | None = None\n_where: FuncType | None = None\n\n# the set of dtypes that we will allow pass to numexpr\n_ALLOWED_DTYPES = {\n    \"evaluate\": {\"int64\", \"int32\", \"float64\", \"float32\", \"bool\"},\n    \"where\": {\"int64\", \"float64\", \"bool\"},\n}\n\n# the minimum prod shape that we will use numexpr\n_MIN_ELEMENTS = 1_000_000\n\n\ndef set_use_numexpr(v: bool = True) -> None:\n    # set/unset to use numexpr\n    global USE_NUMEXPR\n    if NUMEXPR_INSTALLED:\n        USE_NUMEXPR = v\n\n    # choose what we are going to do\n    global _evaluate, _where\n\n    _evaluate = _evaluate_numexpr if USE_NUMEXPR else _evaluate_standard\n    _where = _where_numexpr if USE_NUMEXPR else _where_standard\n\n\ndef set_numexpr_threads(n=None) -> None:\n    # if we are using numexpr, set the threads to n\n    # otherwise reset\n    if NUMEXPR_INSTALLED and USE_NUMEXPR:\n        if n is None:\n            n = ne.detect_number_of_cores()\n        ne.set_num_threads(n)\n\n\ndef _evaluate_standard(op, op_str, a, b):\n    \"\"\"\n    Standard evaluation.\n    \"\"\"\n    if _TEST_MODE:\n        _store_test_result(False)\n    return op(a, b)\n\n\ndef _can_use_numexpr(op, op_str, a, b, dtype_check) -> bool:\n    \"\"\"return a boolean if we WILL be using numexpr\"\"\"\n    if op_str is not None:\n        # required min elements (otherwise we are adding overhead)\n        if a.size > _MIN_ELEMENTS:\n            # check for dtype compatibility\n            dtypes: set[str] = set()\n            for o in [a, b]:\n                # ndarray and Series Case\n                if hasattr(o, \"dtype\"):\n                    dtypes |= {o.dtype.name}\n\n            # allowed are a superset\n            if not len(dtypes) or _ALLOWED_DTYPES[dtype_check] >= dtypes:\n                return True\n\n    return False\n\n\ndef _evaluate_numexpr(op, op_str, a, b):\n    result = None\n\n    if _can_use_numexpr(op, op_str, a, b, \"evaluate\"):\n        is_reversed = op.__name__.strip(\"_\").startswith(\"r\")\n        if is_reversed:\n            # we were originally called by a reversed op method\n            a, b = b, a\n\n        a_value = a\n        b_value = b\n\n        try:\n            result = ne.evaluate(\n                f\"a_value {op_str} b_value\",\n                local_dict={\"a_value\": a_value, \"b_value\": b_value},\n                casting=\"safe\",\n            )\n        except TypeError:\n            # numexpr raises eg for array ** array with integers\n            # (https://github.com/pydata/numexpr/issues/379)\n            pass\n        except NotImplementedError:\n            if _bool_arith_fallback(op_str, a, b):\n                pass\n            else:\n                raise\n\n        if is_reversed:\n            # reverse order to original for fallback\n            a, b = b, a\n\n    if _TEST_MODE:\n        _store_test_result(result is not None)\n\n    if result is None:\n        result = _evaluate_standard(op, op_str, a, b)\n\n    return result\n\n\n_op_str_mapping = {\n    operator.add: \"+\",\n    roperator.radd: \"+\",\n    operator.mul: \"*\",\n    roperator.rmul: \"*\",\n    operator.sub: \"-\",\n    roperator.rsub: \"-\",\n    operator.truediv: \"/\",\n    roperator.rtruediv: \"/\",\n    # floordiv not supported by numexpr 2.x\n    operator.floordiv: None,\n    roperator.rfloordiv: None,\n    # we require Python semantics for mod of negative for backwards compatibility\n    # see https://github.com/pydata/numexpr/issues/365\n    # so sticking with unaccelerated for now GH#36552\n    operator.mod: None,\n    roperator.rmod: None,\n    operator.pow: \"**\",\n    roperator.rpow: \"**\",\n    operator.eq: \"==\",\n    operator.ne: \"!=\",\n    operator.le: \"<=\",\n    operator.lt: \"<\",\n    operator.ge: \">=\",\n    operator.gt: \">\",\n    operator.and_: \"&\",\n    roperator.rand_: \"&\",\n    operator.or_: \"|\",\n    roperator.ror_: \"|\",\n    operator.xor: \"^\",\n    roperator.rxor: \"^\",\n    divmod: None,\n    roperator.rdivmod: None,\n}\n\n\ndef _where_standard(cond, a, b):\n    # Caller is responsible for extracting ndarray if necessary\n    return np.where(cond, a, b)\n\n\ndef _where_numexpr(cond, a, b):\n    # Caller is responsible for extracting ndarray if necessary\n    result = None\n\n    if _can_use_numexpr(None, \"where\", a, b, \"where\"):\n        result = ne.evaluate(\n            \"where(cond_value, a_value, b_value)\",\n            local_dict={\"cond_value\": cond, \"a_value\": a, \"b_value\": b},\n            casting=\"safe\",\n        )\n\n    if result is None:\n        result = _where_standard(cond, a, b)\n\n    return result\n\n\n# turn myself on\nset_use_numexpr(get_option(\"compute.use_numexpr\"))\n\n\ndef _has_bool_dtype(x):\n    try:\n        return x.dtype == bool\n    except AttributeError:\n        return isinstance(x, (bool, np.bool_))\n\n\n_BOOL_OP_UNSUPPORTED = {\"+\": \"|\", \"*\": \"&\", \"-\": \"^\"}\n\n\ndef _bool_arith_fallback(op_str, a, b) -> bool:\n    \"\"\"\n    Check if we should fallback to the python `_evaluate_standard` in case\n    of an unsupported operation by numexpr, which is the case for some\n    boolean ops.\n    \"\"\"\n    if _has_bool_dtype(a) and _has_bool_dtype(b):\n        if op_str in _BOOL_OP_UNSUPPORTED:\n            warnings.warn(\n                f\"evaluating in Python space because the {repr(op_str)} \"\n                \"operator is not supported by numexpr for the bool dtype, \"\n                f\"use {repr(_BOOL_OP_UNSUPPORTED[op_str])} instead.\",\n                stacklevel=find_stack_level(),\n            )\n            return True\n    return False\n\n\ndef evaluate(op, a, b, use_numexpr: bool = True):\n    \"\"\"\n    Evaluate and return the expression of the op on a and b.\n\n    Parameters\n    ----------\n    op : the actual operand\n    a : left operand\n    b : right operand\n    use_numexpr : bool, default True\n        Whether to try to use numexpr.\n    \"\"\"\n    op_str = _op_str_mapping[op]\n    if op_str is not None:\n        if use_numexpr:\n            # error: \"None\" not callable\n            return _evaluate(op, op_str, a, b)  # type: ignore[misc]\n    return _evaluate_standard(op, op_str, a, b)\n\n\ndef where(cond, a, b, use_numexpr: bool = True):\n    \"\"\"\n    Evaluate the where condition cond on a and b.\n\n    Parameters\n    ----------\n    cond : np.ndarray[bool]\n    a : return if cond is True\n    b : return if cond is False\n    use_numexpr : bool, default True\n        Whether to try to use numexpr.\n    \"\"\"\n    assert _where is not None\n    return _where(cond, a, b) if use_numexpr else _where_standard(cond, a, b)\n\n\ndef set_test_mode(v: bool = True) -> None:\n    \"\"\"\n    Keeps track of whether numexpr was used.\n\n    Stores an additional ``True`` for every successful use of evaluate with\n    numexpr since the last ``get_test_result``.\n    \"\"\"\n    global _TEST_MODE, _TEST_RESULT\n    _TEST_MODE = v\n    _TEST_RESULT = []\n\n\ndef _store_test_result(used_numexpr: bool) -> None:\n    if used_numexpr:\n        _TEST_RESULT.append(used_numexpr)\n\n\ndef get_test_result() -> list[bool]:\n    \"\"\"\n    Get test result and reset test_results.\n    \"\"\"\n    global _TEST_RESULT\n    res = _TEST_RESULT\n    _TEST_RESULT = []\n    return res\n"
    },
    {
      "filename": "pandas/core/computation/pytables.py",
      "content": "\"\"\" manage PyTables query interface via Expressions \"\"\"\nfrom __future__ import annotations\n\nimport ast\nfrom functools import partial\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n)\n\nimport numpy as np\n\nfrom pandas._libs.tslibs import (\n    Timedelta,\n    Timestamp,\n)\nfrom pandas.errors import UndefinedVariableError\n\nfrom pandas.core.dtypes.common import is_list_like\n\nimport pandas.core.common as com\nfrom pandas.core.computation import (\n    expr,\n    ops,\n    scope as _scope,\n)\nfrom pandas.core.computation.common import ensure_decoded\nfrom pandas.core.computation.expr import BaseExprVisitor\nfrom pandas.core.computation.ops import is_term\nfrom pandas.core.construction import extract_array\nfrom pandas.core.indexes.base import Index\n\nfrom pandas.io.formats.printing import (\n    pprint_thing,\n    pprint_thing_encoded,\n)\n\nif TYPE_CHECKING:\n    from pandas._typing import npt\n\n\nclass PyTablesScope(_scope.Scope):\n    __slots__ = (\"queryables\",)\n\n    queryables: dict[str, Any]\n\n    def __init__(\n        self,\n        level: int,\n        global_dict=None,\n        local_dict=None,\n        queryables: dict[str, Any] | None = None,\n    ) -> None:\n        super().__init__(level + 1, global_dict=global_dict, local_dict=local_dict)\n        self.queryables = queryables or {}\n\n\nclass Term(ops.Term):\n    env: PyTablesScope\n\n    def __new__(cls, name, env, side=None, encoding=None):\n        if isinstance(name, str):\n            klass = cls\n        else:\n            klass = Constant\n        return object.__new__(klass)\n\n    def __init__(self, name, env: PyTablesScope, side=None, encoding=None) -> None:\n        super().__init__(name, env, side=side, encoding=encoding)\n\n    def _resolve_name(self):\n        # must be a queryables\n        if self.side == \"left\":\n            # Note: The behavior of __new__ ensures that self.name is a str here\n            if self.name not in self.env.queryables:\n                raise NameError(f\"name {repr(self.name)} is not defined\")\n            return self.name\n\n        # resolve the rhs (and allow it to be None)\n        try:\n            return self.env.resolve(self.name, is_local=False)\n        except UndefinedVariableError:\n            return self.name\n\n    # read-only property overwriting read/write property\n    @property  # type: ignore[misc]\n    def value(self):\n        return self._value\n\n\nclass Constant(Term):\n    def __init__(self, value, env: PyTablesScope, side=None, encoding=None) -> None:\n        assert isinstance(env, PyTablesScope), type(env)\n        super().__init__(value, env, side=side, encoding=encoding)\n\n    def _resolve_name(self):\n        return self._name\n\n\nclass BinOp(ops.BinOp):\n    _max_selectors = 31\n\n    op: str\n    queryables: dict[str, Any]\n    condition: str | None\n\n    def __init__(self, op: str, lhs, rhs, queryables: dict[str, Any], encoding) -> None:\n        super().__init__(op, lhs, rhs)\n        self.queryables = queryables\n        self.encoding = encoding\n        self.condition = None\n\n    def _disallow_scalar_only_bool_ops(self) -> None:\n        pass\n\n    def prune(self, klass):\n        def pr(left, right):\n            \"\"\"create and return a new specialized BinOp from myself\"\"\"\n            if left is None:\n                return right\n            elif right is None:\n                return left\n\n            k = klass\n            if isinstance(left, ConditionBinOp):\n                if isinstance(right, ConditionBinOp):\n                    k = JointConditionBinOp\n                elif isinstance(left, k):\n                    return left\n                elif isinstance(right, k):\n                    return right\n\n            elif isinstance(left, FilterBinOp):\n                if isinstance(right, FilterBinOp):\n                    k = JointFilterBinOp\n                elif isinstance(left, k):\n                    return left\n                elif isinstance(right, k):\n                    return right\n\n            return k(\n                self.op, left, right, queryables=self.queryables, encoding=self.encoding\n            ).evaluate()\n\n        left, right = self.lhs, self.rhs\n\n        if is_term(left) and is_term(right):\n            res = pr(left.value, right.value)\n        elif not is_term(left) and is_term(right):\n            res = pr(left.prune(klass), right.value)\n        elif is_term(left) and not is_term(right):\n            res = pr(left.value, right.prune(klass))\n        elif not (is_term(left) or is_term(right)):\n            res = pr(left.prune(klass), right.prune(klass))\n\n        return res\n\n    def conform(self, rhs):\n        \"\"\"inplace conform rhs\"\"\"\n        if not is_list_like(rhs):\n            rhs = [rhs]\n        if isinstance(rhs, np.ndarray):\n            rhs = rhs.ravel()\n        return rhs\n\n    @property\n    def is_valid(self) -> bool:\n        \"\"\"return True if this is a valid field\"\"\"\n        return self.lhs in self.queryables\n\n    @property\n    def is_in_table(self) -> bool:\n        \"\"\"\n        return True if this is a valid column name for generation (e.g. an\n        actual column in the table)\n        \"\"\"\n        return self.queryables.get(self.lhs) is not None\n\n    @property\n    def kind(self):\n        \"\"\"the kind of my field\"\"\"\n        return getattr(self.queryables.get(self.lhs), \"kind\", None)\n\n    @property\n    def meta(self):\n        \"\"\"the meta of my field\"\"\"\n        return getattr(self.queryables.get(self.lhs), \"meta\", None)\n\n    @property\n    def metadata(self):\n        \"\"\"the metadata of my field\"\"\"\n        return getattr(self.queryables.get(self.lhs), \"metadata\", None)\n\n    def generate(self, v) -> str:\n        \"\"\"create and return the op string for this TermValue\"\"\"\n        val = v.tostring(self.encoding)\n        return f\"({self.lhs} {self.op} {val})\"\n\n    def convert_value(self, v) -> TermValue:\n        \"\"\"\n        convert the expression that is in the term to something that is\n        accepted by pytables\n        \"\"\"\n\n        def stringify(value):\n            if self.encoding is not None:\n                return pprint_thing_encoded(value, encoding=self.encoding)\n            return pprint_thing(value)\n\n        kind = ensure_decoded(self.kind)\n        meta = ensure_decoded(self.meta)\n        if kind in (\"datetime64\", \"datetime\"):\n            if isinstance(v, (int, float)):\n                v = stringify(v)\n            v = ensure_decoded(v)\n            v = Timestamp(v).as_unit(\"ns\")\n            if v.tz is not None:\n                v = v.tz_convert(\"UTC\")\n            return TermValue(v, v._value, kind)\n        elif kind in (\"timedelta64\", \"timedelta\"):\n            if isinstance(v, str):\n                v = Timedelta(v)\n            else:\n                v = Timedelta(v, unit=\"s\")\n            v = v.as_unit(\"ns\")._value\n            return TermValue(int(v), v, kind)\n        elif meta == \"category\":\n            metadata = extract_array(self.metadata, extract_numpy=True)\n            result: npt.NDArray[np.intp] | np.intp | int\n            if v not in metadata:\n                result = -1\n            else:\n                result = metadata.searchsorted(v, side=\"left\")\n            return TermValue(result, result, \"integer\")\n        elif kind == \"integer\":\n            v = int(float(v))\n            return TermValue(v, v, kind)\n        elif kind == \"float\":\n            v = float(v)\n            return TermValue(v, v, kind)\n        elif kind == \"bool\":\n            if isinstance(v, str):\n                v = v.strip().lower() not in [\n                    \"false\",\n                    \"f\",\n                    \"no\",\n                    \"n\",\n                    \"none\",\n                    \"0\",\n                    \"[]\",\n                    \"{}\",\n                    \"\",\n                ]\n            else:\n                v = bool(v)\n            return TermValue(v, v, kind)\n        elif isinstance(v, str):\n            # string quoting\n            return TermValue(v, stringify(v), \"string\")\n        else:\n            raise TypeError(f\"Cannot compare {v} of type {type(v)} to {kind} column\")\n\n    def convert_values(self) -> None:\n        pass\n\n\nclass FilterBinOp(BinOp):\n    filter: tuple[Any, Any, Index] | None = None\n\n    def __repr__(self) -> str:\n        if self.filter is None:\n            return \"Filter: Not Initialized\"\n        return pprint_thing(f\"[Filter : [{self.filter[0]}] -> [{self.filter[1]}]\")\n\n    def invert(self):\n        \"\"\"invert the filter\"\"\"\n        if self.filter is not None:\n            self.filter = (\n                self.filter[0],\n                self.generate_filter_op(invert=True),\n                self.filter[2],\n            )\n        return self\n\n    def format(self):\n        \"\"\"return the actual filter format\"\"\"\n        return [self.filter]\n\n    def evaluate(self):\n        if not self.is_valid:\n            raise ValueError(f\"query term is not valid [{self}]\")\n\n        rhs = self.conform(self.rhs)\n        values = list(rhs)\n\n        if self.is_in_table:\n            # if too many values to create the expression, use a filter instead\n            if self.op in [\"==\", \"!=\"] and len(values) > self._max_selectors:\n                filter_op = self.generate_filter_op()\n                self.filter = (self.lhs, filter_op, Index(values))\n\n                return self\n            return None\n\n        # equality conditions\n        if self.op in [\"==\", \"!=\"]:\n            filter_op = self.generate_filter_op()\n            self.filter = (self.lhs, filter_op, Index(values))\n\n        else:\n            raise TypeError(\n                f\"passing a filterable condition to a non-table indexer [{self}]\"\n            )\n\n        return self\n\n    def generate_filter_op(self, invert: bool = False):\n        if (self.op == \"!=\" and not invert) or (self.op == \"==\" and invert):\n            return lambda axis, vals: ~axis.isin(vals)\n        else:\n            return lambda axis, vals: axis.isin(vals)\n\n\nclass JointFilterBinOp(FilterBinOp):\n    def format(self):\n        raise NotImplementedError(\"unable to collapse Joint Filters\")\n\n    def evaluate(self):\n        return self\n\n\nclass ConditionBinOp(BinOp):\n    def __repr__(self) -> str:\n        return pprint_thing(f\"[Condition : [{self.condition}]]\")\n\n    def invert(self):\n        \"\"\"invert the condition\"\"\"\n        # if self.condition is not None:\n        #    self.condition = \"~(%s)\" % self.condition\n        # return self\n        raise NotImplementedError(\n            \"cannot use an invert condition when passing to numexpr\"\n        )\n\n    def format(self):\n        \"\"\"return the actual ne format\"\"\"\n        return self.condition\n\n    def evaluate(self):\n        if not self.is_valid:\n            raise ValueError(f\"query term is not valid [{self}]\")\n\n        # convert values if we are in the table\n        if not self.is_in_table:\n            return None\n\n        rhs = self.conform(self.rhs)\n        values = [self.convert_value(v) for v in rhs]\n\n        # equality conditions\n        if self.op in [\"==\", \"!=\"]:\n            # too many values to create the expression?\n            if len(values) <= self._max_selectors:\n                vs = [self.generate(v) for v in values]\n                self.condition = f\"({' | '.join(vs)})\"\n\n            # use a filter after reading\n            else:\n                return None\n        else:\n            self.condition = self.generate(values[0])\n\n        return self\n\n\nclass JointConditionBinOp(ConditionBinOp):\n    def evaluate(self):\n        self.condition = f\"({self.lhs.condition} {self.op} {self.rhs.condition})\"\n        return self\n\n\nclass UnaryOp(ops.UnaryOp):\n    def prune(self, klass):\n        if self.op != \"~\":\n            raise NotImplementedError(\"UnaryOp only support invert type ops\")\n\n        operand = self.operand\n        operand = operand.prune(klass)\n\n        if operand is not None and (\n            issubclass(klass, ConditionBinOp)\n            and operand.condition is not None\n            or not issubclass(klass, ConditionBinOp)\n            and issubclass(klass, FilterBinOp)\n            and operand.filter is not None\n        ):\n            return operand.invert()\n        return None\n\n\nclass PyTablesExprVisitor(BaseExprVisitor):\n    const_type = Constant\n    term_type = Term\n\n    def __init__(self, env, engine, parser, **kwargs) -> None:\n        super().__init__(env, engine, parser)\n        for bin_op in self.binary_ops:\n            bin_node = self.binary_op_nodes_map[bin_op]\n            setattr(\n                self,\n                f\"visit_{bin_node}\",\n                lambda node, bin_op=bin_op: partial(BinOp, bin_op, **kwargs),\n            )\n\n    def visit_UnaryOp(self, node, **kwargs):\n        if isinstance(node.op, (ast.Not, ast.Invert)):\n            return UnaryOp(\"~\", self.visit(node.operand))\n        elif isinstance(node.op, ast.USub):\n            return self.const_type(-self.visit(node.operand).value, self.env)\n        elif isinstance(node.op, ast.UAdd):\n            raise NotImplementedError(\"Unary addition not supported\")\n\n    def visit_Index(self, node, **kwargs):\n        return self.visit(node.value).value\n\n    def visit_Assign(self, node, **kwargs):\n        cmpr = ast.Compare(\n            ops=[ast.Eq()], left=node.targets[0], comparators=[node.value]\n        )\n        return self.visit(cmpr)\n\n    def visit_Subscript(self, node, **kwargs):\n        # only allow simple subscripts\n\n        value = self.visit(node.value)\n        slobj = self.visit(node.slice)\n        try:\n            value = value.value\n        except AttributeError:\n            pass\n\n        if isinstance(slobj, Term):\n            # In py39 np.ndarray lookups with Term containing int raise\n            slobj = slobj.value\n\n        try:\n            return self.const_type(value[slobj], self.env)\n        except TypeError as err:\n            raise ValueError(\n                f\"cannot subscript {repr(value)} with {repr(slobj)}\"\n            ) from err\n\n    def visit_Attribute(self, node, **kwargs):\n        attr = node.attr\n        value = node.value\n\n        ctx = type(node.ctx)\n        if ctx == ast.Load:\n            # resolve the value\n            resolved = self.visit(value)\n\n            # try to get the value to see if we are another expression\n            try:\n                resolved = resolved.value\n            except AttributeError:\n                pass\n\n            try:\n                return self.term_type(getattr(resolved, attr), self.env)\n            except AttributeError:\n                # something like datetime.datetime where scope is overridden\n                if isinstance(value, ast.Name) and value.id == attr:\n                    return resolved\n\n        raise ValueError(f\"Invalid Attribute context {ctx.__name__}\")\n\n    def translate_In(self, op):\n        return ast.Eq() if isinstance(op, ast.In) else op\n\n    def _rewrite_membership_op(self, node, left, right):\n        return self.visit(node.op), node.op, left, right\n\n\ndef _validate_where(w):\n    \"\"\"\n    Validate that the where statement is of the right type.\n\n    The type may either be String, Expr, or list-like of Exprs.\n\n    Parameters\n    ----------\n    w : String term expression, Expr, or list-like of Exprs.\n\n    Returns\n    -------\n    where : The original where clause if the check was successful.\n\n    Raises\n    ------\n    TypeError : An invalid data type was passed in for w (e.g. dict).\n    \"\"\"\n    if not (isinstance(w, (PyTablesExpr, str)) or is_list_like(w)):\n        raise TypeError(\n            \"where must be passed as a string, PyTablesExpr, \"\n            \"or list-like of PyTablesExpr\"\n        )\n\n    return w\n\n\nclass PyTablesExpr(expr.Expr):\n    \"\"\"\n    Hold a pytables-like expression, comprised of possibly multiple 'terms'.\n\n    Parameters\n    ----------\n    where : string term expression, PyTablesExpr, or list-like of PyTablesExprs\n    queryables : a \"kinds\" map (dict of column name -> kind), or None if column\n        is non-indexable\n    encoding : an encoding that will encode the query terms\n\n    Returns\n    -------\n    a PyTablesExpr object\n\n    Examples\n    --------\n    'index>=date'\n    \"columns=['A', 'D']\"\n    'columns=A'\n    'columns==A'\n    \"~(columns=['A','B'])\"\n    'index>df.index[3] & string=\"bar\"'\n    '(index>df.index[3] & index<=df.index[6]) | string=\"bar\"'\n    \"ts>=Timestamp('2012-02-01')\"\n    \"major_axis>=20130101\"\n    \"\"\"\n\n    _visitor: PyTablesExprVisitor | None\n    env: PyTablesScope\n    expr: str\n\n    def __init__(\n        self,\n        where,\n        queryables: dict[str, Any] | None = None,\n        encoding=None,\n        scope_level: int = 0,\n    ) -> None:\n        where = _validate_where(where)\n\n        self.encoding = encoding\n        self.condition = None\n        self.filter = None\n        self.terms = None\n        self._visitor = None\n\n        # capture the environment if needed\n        local_dict: _scope.DeepChainMap[Any, Any] | None = None\n\n        if isinstance(where, PyTablesExpr):\n            local_dict = where.env.scope\n            _where = where.expr\n\n        elif is_list_like(where):\n            where = list(where)\n            for idx, w in enumerate(where):\n                if isinstance(w, PyTablesExpr):\n                    local_dict = w.env.scope\n                else:\n                    w = _validate_where(w)\n                    where[idx] = w\n            _where = \" & \".join([f\"({w})\" for w in com.flatten(where)])\n        else:\n            # _validate_where ensures we otherwise have a string\n            _where = where\n\n        self.expr = _where\n        self.env = PyTablesScope(scope_level + 1, local_dict=local_dict)\n\n        if queryables is not None and isinstance(self.expr, str):\n            self.env.queryables.update(queryables)\n            self._visitor = PyTablesExprVisitor(\n                self.env,\n                queryables=queryables,\n                parser=\"pytables\",\n                engine=\"pytables\",\n                encoding=encoding,\n            )\n            self.terms = self.parse()\n\n    def __repr__(self) -> str:\n        if self.terms is not None:\n            return pprint_thing(self.terms)\n        return pprint_thing(self.expr)\n\n    def evaluate(self):\n        \"\"\"create and return the numexpr condition and filter\"\"\"\n        try:\n            self.condition = self.terms.prune(ConditionBinOp)\n        except AttributeError as err:\n            raise ValueError(\n                f\"cannot process expression [{self.expr}], [{self}] \"\n                \"is not a valid condition\"\n            ) from err\n        try:\n            self.filter = self.terms.prune(FilterBinOp)\n        except AttributeError as err:\n            raise ValueError(\n                f\"cannot process expression [{self.expr}], [{self}] \"\n                \"is not a valid filter\"\n            ) from err\n\n        return self.condition, self.filter\n\n\nclass TermValue:\n    \"\"\"hold a term value the we use to construct a condition/filter\"\"\"\n\n    def __init__(self, value, converted, kind: str) -> None:\n        assert isinstance(kind, str), kind\n        self.value = value\n        self.converted = converted\n        self.kind = kind\n\n    def tostring(self, encoding) -> str:\n        \"\"\"quote the string if not encoded else encode and return\"\"\"\n        if self.kind == \"string\":\n            if encoding is not None:\n                return str(self.converted)\n            return f'\"{self.converted}\"'\n        elif self.kind == \"float\":\n            # python 2 str(float) is not always\n            # round-trippable so use repr()\n            return repr(self.converted)\n        return str(self.converted)\n\n\ndef maybe_expression(s) -> bool:\n    \"\"\"loose checking if s is a pytables-acceptable expression\"\"\"\n    if not isinstance(s, str):\n        return False\n    operations = PyTablesExprVisitor.binary_ops + PyTablesExprVisitor.unary_ops + (\"=\",)\n\n    # make sure we have an op at least\n    return any(op in s for op in operations)\n"
    },
    {
      "filename": "pandas/core/indexing.py",
      "content": "from __future__ import annotations\n\nfrom contextlib import suppress\nimport sys\nfrom typing import (\n    TYPE_CHECKING,\n    Hashable,\n    Sequence,\n    TypeVar,\n    cast,\n    final,\n)\n\nimport numpy as np\n\nfrom pandas._config import using_copy_on_write\n\nfrom pandas._libs.indexing import NDFrameIndexerBase\nfrom pandas._libs.lib import item_from_zerodim\nfrom pandas.compat import PYPY\nfrom pandas.errors import (\n    AbstractMethodError,\n    ChainedAssignmentError,\n    IndexingError,\n    InvalidIndexError,\n    LossySetitemError,\n    _chained_assignment_msg,\n)\nfrom pandas.util._decorators import doc\n\nfrom pandas.core.dtypes.cast import (\n    can_hold_element,\n    maybe_promote,\n)\nfrom pandas.core.dtypes.common import (\n    is_array_like,\n    is_bool_dtype,\n    is_extension_array_dtype,\n    is_hashable,\n    is_integer,\n    is_iterator,\n    is_list_like,\n    is_numeric_dtype,\n    is_object_dtype,\n    is_scalar,\n    is_sequence,\n)\nfrom pandas.core.dtypes.concat import concat_compat\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.missing import (\n    infer_fill_value,\n    is_valid_na_for_dtype,\n    isna,\n    na_value_for_dtype,\n)\n\nfrom pandas.core import algorithms as algos\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    array as pd_array,\n    extract_array,\n)\nfrom pandas.core.indexers import (\n    check_array_indexer,\n    is_list_like_indexer,\n    is_scalar_indexer,\n    length_of_indexer,\n)\nfrom pandas.core.indexes.api import (\n    Index,\n    MultiIndex,\n)\n\nif TYPE_CHECKING:\n    from pandas._typing import (\n        Axis,\n        AxisInt,\n    )\n\n    from pandas import (\n        DataFrame,\n        Series,\n    )\n\n_LocationIndexerT = TypeVar(\"_LocationIndexerT\", bound=\"_LocationIndexer\")\n\n# \"null slice\"\n_NS = slice(None, None)\n_one_ellipsis_message = \"indexer may only contain one '...' entry\"\n\n\n# the public IndexSlicerMaker\nclass _IndexSlice:\n    \"\"\"\n    Create an object to more easily perform multi-index slicing.\n\n    See Also\n    --------\n    MultiIndex.remove_unused_levels : New MultiIndex with no unused levels.\n\n    Notes\n    -----\n    See :ref:`Defined Levels <advanced.shown_levels>`\n    for further info on slicing a MultiIndex.\n\n    Examples\n    --------\n    >>> midx = pd.MultiIndex.from_product([['A0','A1'], ['B0','B1','B2','B3']])\n    >>> columns = ['foo', 'bar']\n    >>> dfmi = pd.DataFrame(np.arange(16).reshape((len(midx), len(columns))),\n    ...                     index=midx, columns=columns)\n\n    Using the default slice command:\n\n    >>> dfmi.loc[(slice(None), slice('B0', 'B1')), :]\n               foo  bar\n        A0 B0    0    1\n           B1    2    3\n        A1 B0    8    9\n           B1   10   11\n\n    Using the IndexSlice class for a more intuitive command:\n\n    >>> idx = pd.IndexSlice\n    >>> dfmi.loc[idx[:, 'B0':'B1'], :]\n               foo  bar\n        A0 B0    0    1\n           B1    2    3\n        A1 B0    8    9\n           B1   10   11\n    \"\"\"\n\n    def __getitem__(self, arg):\n        return arg\n\n\nIndexSlice = _IndexSlice()\n\n\nclass IndexingMixin:\n    \"\"\"\n    Mixin for adding .loc/.iloc/.at/.iat to Dataframes and Series.\n    \"\"\"\n\n    @property\n    def iloc(self) -> _iLocIndexer:\n        \"\"\"\n        Purely integer-location based indexing for selection by position.\n\n        ``.iloc[]`` is primarily integer position based (from ``0`` to\n        ``length-1`` of the axis), but may also be used with a boolean\n        array.\n\n        Allowed inputs are:\n\n        - An integer, e.g. ``5``.\n        - A list or array of integers, e.g. ``[4, 3, 0]``.\n        - A slice object with ints, e.g. ``1:7``.\n        - A boolean array.\n        - A ``callable`` function with one argument (the calling Series or\n          DataFrame) and that returns valid output for indexing (one of the above).\n          This is useful in method chains, when you don't have a reference to the\n          calling object, but would like to base your selection on some value.\n        - A tuple of row and column indexes. The tuple elements consist of one of the\n          above inputs, e.g. ``(0, 1)``.\n\n        ``.iloc`` will raise ``IndexError`` if a requested indexer is\n        out-of-bounds, except *slice* indexers which allow out-of-bounds\n        indexing (this conforms with python/numpy *slice* semantics).\n\n        See more at :ref:`Selection by Position <indexing.integer>`.\n\n        See Also\n        --------\n        DataFrame.iat : Fast integer location scalar accessor.\n        DataFrame.loc : Purely label-location based indexer for selection by label.\n        Series.iloc : Purely integer-location based indexing for\n                       selection by position.\n\n        Examples\n        --------\n        >>> mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n        ...           {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n        ...           {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }]\n        >>> df = pd.DataFrame(mydict)\n        >>> df\n              a     b     c     d\n        0     1     2     3     4\n        1   100   200   300   400\n        2  1000  2000  3000  4000\n\n        **Indexing just the rows**\n\n        With a scalar integer.\n\n        >>> type(df.iloc[0])\n        <class 'pandas.core.series.Series'>\n        >>> df.iloc[0]\n        a    1\n        b    2\n        c    3\n        d    4\n        Name: 0, dtype: int64\n\n        With a list of integers.\n\n        >>> df.iloc[[0]]\n           a  b  c  d\n        0  1  2  3  4\n        >>> type(df.iloc[[0]])\n        <class 'pandas.core.frame.DataFrame'>\n\n        >>> df.iloc[[0, 1]]\n             a    b    c    d\n        0    1    2    3    4\n        1  100  200  300  400\n\n        With a `slice` object.\n\n        >>> df.iloc[:3]\n              a     b     c     d\n        0     1     2     3     4\n        1   100   200   300   400\n        2  1000  2000  3000  4000\n\n        With a boolean mask the same length as the index.\n\n        >>> df.iloc[[True, False, True]]\n              a     b     c     d\n        0     1     2     3     4\n        2  1000  2000  3000  4000\n\n        With a callable, useful in method chains. The `x` passed\n        to the ``lambda`` is the DataFrame being sliced. This selects\n        the rows whose index label even.\n\n        >>> df.iloc[lambda x: x.index % 2 == 0]\n              a     b     c     d\n        0     1     2     3     4\n        2  1000  2000  3000  4000\n\n        **Indexing both axes**\n\n        You can mix the indexer types for the index and columns. Use ``:`` to\n        select the entire axis.\n\n        With scalar integers.\n\n        >>> df.iloc[0, 1]\n        2\n\n        With lists of integers.\n\n        >>> df.iloc[[0, 2], [1, 3]]\n              b     d\n        0     2     4\n        2  2000  4000\n\n        With `slice` objects.\n\n        >>> df.iloc[1:3, 0:3]\n              a     b     c\n        1   100   200   300\n        2  1000  2000  3000\n\n        With a boolean array whose length matches the columns.\n\n        >>> df.iloc[:, [True, False, True, False]]\n              a     c\n        0     1     3\n        1   100   300\n        2  1000  3000\n\n        With a callable function that expects the Series or DataFrame.\n\n        >>> df.iloc[:, lambda df: [0, 2]]\n              a     c\n        0     1     3\n        1   100   300\n        2  1000  3000\n        \"\"\"\n        return _iLocIndexer(\"iloc\", self)\n\n    @property\n    def loc(self) -> _LocIndexer:\n        \"\"\"\n        Access a group of rows and columns by label(s) or a boolean array.\n\n        ``.loc[]`` is primarily label based, but may also be used with a\n        boolean array.\n\n        Allowed inputs are:\n\n        - A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is\n          interpreted as a *label* of the index, and **never** as an\n          integer position along the index).\n        - A list or array of labels, e.g. ``['a', 'b', 'c']``.\n        - A slice object with labels, e.g. ``'a':'f'``.\n\n          .. warning:: Note that contrary to usual python slices, **both** the\n              start and the stop are included\n\n        - A boolean array of the same length as the axis being sliced,\n          e.g. ``[True, False, True]``.\n        - An alignable boolean Series. The index of the key will be aligned before\n          masking.\n        - An alignable Index. The Index of the returned selection will be the input.\n        - A ``callable`` function with one argument (the calling Series or\n          DataFrame) and that returns valid output for indexing (one of the above)\n\n        See more at :ref:`Selection by Label <indexing.label>`.\n\n        Raises\n        ------\n        KeyError\n            If any items are not found.\n        IndexingError\n            If an indexed key is passed and its index is unalignable to the frame index.\n\n        See Also\n        --------\n        DataFrame.at : Access a single value for a row/column label pair.\n        DataFrame.iloc : Access group of rows and columns by integer position(s).\n        DataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\n            Series/DataFrame.\n        Series.loc : Access group of values using labels.\n\n        Examples\n        --------\n        **Getting values**\n\n        >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n        ...      index=['cobra', 'viper', 'sidewinder'],\n        ...      columns=['max_speed', 'shield'])\n        >>> df\n                    max_speed  shield\n        cobra               1       2\n        viper               4       5\n        sidewinder          7       8\n\n        Single label. Note this returns the row as a Series.\n\n        >>> df.loc['viper']\n        max_speed    4\n        shield       5\n        Name: viper, dtype: int64\n\n        List of labels. Note using ``[[]]`` returns a DataFrame.\n\n        >>> df.loc[['viper', 'sidewinder']]\n                    max_speed  shield\n        viper               4       5\n        sidewinder          7       8\n\n        Single label for row and column\n\n        >>> df.loc['cobra', 'shield']\n        2\n\n        Slice with labels for row and single label for column. As mentioned\n        above, note that both the start and stop of the slice are included.\n\n        >>> df.loc['cobra':'viper', 'max_speed']\n        cobra    1\n        viper    4\n        Name: max_speed, dtype: int64\n\n        Boolean list with the same length as the row axis\n\n        >>> df.loc[[False, False, True]]\n                    max_speed  shield\n        sidewinder          7       8\n\n        Alignable boolean Series:\n\n        >>> df.loc[pd.Series([False, True, False],\n        ...        index=['viper', 'sidewinder', 'cobra'])]\n                    max_speed  shield\n        sidewinder          7       8\n\n        Index (same behavior as ``df.reindex``)\n\n        >>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\n               max_speed  shield\n        foo\n        cobra          1       2\n        viper          4       5\n\n        Conditional that returns a boolean Series\n\n        >>> df.loc[df['shield'] > 6]\n                    max_speed  shield\n        sidewinder          7       8\n\n        Conditional that returns a boolean Series with column labels specified\n\n        >>> df.loc[df['shield'] > 6, ['max_speed']]\n                    max_speed\n        sidewinder          7\n\n        Callable that returns a boolean Series\n\n        >>> df.loc[lambda df: df['shield'] == 8]\n                    max_speed  shield\n        sidewinder          7       8\n\n        **Setting values**\n\n        Set value for all items matching the list of labels\n\n        >>> df.loc[['viper', 'sidewinder'], ['shield']] = 50\n        >>> df\n                    max_speed  shield\n        cobra               1       2\n        viper               4      50\n        sidewinder          7      50\n\n        Set value for an entire row\n\n        >>> df.loc['cobra'] = 10\n        >>> df\n                    max_speed  shield\n        cobra              10      10\n        viper               4      50\n        sidewinder          7      50\n\n        Set value for an entire column\n\n        >>> df.loc[:, 'max_speed'] = 30\n        >>> df\n                    max_speed  shield\n        cobra              30      10\n        viper              30      50\n        sidewinder         30      50\n\n        Set value for rows matching callable condition\n\n        >>> df.loc[df['shield'] > 35] = 0\n        >>> df\n                    max_speed  shield\n        cobra              30      10\n        viper               0       0\n        sidewinder          0       0\n\n        **Getting values on a DataFrame with an index that has integer labels**\n\n        Another example using integers for the index\n\n        >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n        ...      index=[7, 8, 9], columns=['max_speed', 'shield'])\n        >>> df\n           max_speed  shield\n        7          1       2\n        8          4       5\n        9          7       8\n\n        Slice with integer labels for rows. As mentioned above, note that both\n        the start and stop of the slice are included.\n\n        >>> df.loc[7:9]\n           max_speed  shield\n        7          1       2\n        8          4       5\n        9          7       8\n\n        **Getting values with a MultiIndex**\n\n        A number of examples using a DataFrame with a MultiIndex\n\n        >>> tuples = [\n        ...    ('cobra', 'mark i'), ('cobra', 'mark ii'),\n        ...    ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n        ...    ('viper', 'mark ii'), ('viper', 'mark iii')\n        ... ]\n        >>> index = pd.MultiIndex.from_tuples(tuples)\n        >>> values = [[12, 2], [0, 4], [10, 20],\n        ...         [1, 4], [7, 1], [16, 36]]\n        >>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n        >>> df\n                             max_speed  shield\n        cobra      mark i           12       2\n                   mark ii           0       4\n        sidewinder mark i           10      20\n                   mark ii           1       4\n        viper      mark ii           7       1\n                   mark iii         16      36\n\n        Single label. Note this returns a DataFrame with a single index.\n\n        >>> df.loc['cobra']\n                 max_speed  shield\n        mark i          12       2\n        mark ii          0       4\n\n        Single index tuple. Note this returns a Series.\n\n        >>> df.loc[('cobra', 'mark ii')]\n        max_speed    0\n        shield       4\n        Name: (cobra, mark ii), dtype: int64\n\n        Single label for row and column. Similar to passing in a tuple, this\n        returns a Series.\n\n        >>> df.loc['cobra', 'mark i']\n        max_speed    12\n        shield        2\n        Name: (cobra, mark i), dtype: int64\n\n        Single tuple. Note using ``[[]]`` returns a DataFrame.\n\n        >>> df.loc[[('cobra', 'mark ii')]]\n                       max_speed  shield\n        cobra mark ii          0       4\n\n        Single tuple for the index with a single label for the column\n\n        >>> df.loc[('cobra', 'mark i'), 'shield']\n        2\n\n        Slice from index tuple to single label\n\n        >>> df.loc[('cobra', 'mark i'):'viper']\n                             max_speed  shield\n        cobra      mark i           12       2\n                   mark ii           0       4\n        sidewinder mark i           10      20\n                   mark ii           1       4\n        viper      mark ii           7       1\n                   mark iii         16      36\n\n        Slice from index tuple to index tuple\n\n        >>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n                            max_speed  shield\n        cobra      mark i          12       2\n                   mark ii          0       4\n        sidewinder mark i          10      20\n                   mark ii          1       4\n        viper      mark ii          7       1\n\n        Please see the :ref:`user guide<advanced.advanced_hierarchical>`\n        for more details and explanations of advanced indexing.\n        \"\"\"\n        return _LocIndexer(\"loc\", self)\n\n    @property\n    def at(self) -> _AtIndexer:\n        \"\"\"\n        Access a single value for a row/column label pair.\n\n        Similar to ``loc``, in that both provide label-based lookups. Use\n        ``at`` if you only need to get or set a single value in a DataFrame\n        or Series.\n\n        Raises\n        ------\n        KeyError\n            * If getting a value and 'label' does not exist in a DataFrame or\n                Series.\n        ValueError\n            * If row/column label pair is not a tuple or if any label from\n                the pair is not a scalar for DataFrame.\n            * If label is list-like (*excluding* NamedTuple) for Series.\n\n        See Also\n        --------\n        DataFrame.at : Access a single value for a row/column pair by label.\n        DataFrame.iat : Access a single value for a row/column pair by integer\n            position.\n        DataFrame.loc : Access a group of rows and columns by label(s).\n        DataFrame.iloc : Access a group of rows and columns by integer\n            position(s).\n        Series.at : Access a single value by label.\n        Series.iat : Access a single value by integer position.\n        Series.loc : Access a group of rows by label(s).\n        Series.iloc : Access a group of rows by integer position(s).\n\n        Notes\n        -----\n        See :ref:`Fast scalar value getting and setting <indexing.basics.get_value>`\n        for more details.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n        ...                   index=[4, 5, 6], columns=['A', 'B', 'C'])\n        >>> df\n            A   B   C\n        4   0   2   3\n        5   0   4   1\n        6  10  20  30\n\n        Get value at specified row/column pair\n\n        >>> df.at[4, 'B']\n        2\n\n        Set value at specified row/column pair\n\n        >>> df.at[4, 'B'] = 10\n        >>> df.at[4, 'B']\n        10\n\n        Get value within a Series\n\n        >>> df.loc[5].at['B']\n        4\n        \"\"\"\n        return _AtIndexer(\"at\", self)\n\n    @property\n    def iat(self) -> _iAtIndexer:\n        \"\"\"\n        Access a single value for a row/column pair by integer position.\n\n        Similar to ``iloc``, in that both provide integer-based lookups. Use\n        ``iat`` if you only need to get or set a single value in a DataFrame\n        or Series.\n\n        Raises\n        ------\n        IndexError\n            When integer position is out of bounds.\n\n        See Also\n        --------\n        DataFrame.at : Access a single value for a row/column label pair.\n        DataFrame.loc : Access a group of rows and columns by label(s).\n        DataFrame.iloc : Access a group of rows and columns by integer position(s).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n        ...                   columns=['A', 'B', 'C'])\n        >>> df\n            A   B   C\n        0   0   2   3\n        1   0   4   1\n        2  10  20  30\n\n        Get value at specified row/column pair\n\n        >>> df.iat[1, 2]\n        1\n\n        Set value at specified row/column pair\n\n        >>> df.iat[1, 2] = 10\n        >>> df.iat[1, 2]\n        10\n\n        Get value within a series\n\n        >>> df.loc[0].iat[1]\n        2\n        \"\"\"\n        return _iAtIndexer(\"iat\", self)\n\n\nclass _LocationIndexer(NDFrameIndexerBase):\n    _valid_types: str\n    axis: AxisInt | None = None\n\n    # sub-classes need to set _takeable\n    _takeable: bool\n\n    @final\n    def __call__(\n        self: _LocationIndexerT, axis: Axis | None = None\n    ) -> _LocationIndexerT:\n        # we need to return a copy of ourselves\n        new_self = type(self)(self.name, self.obj)\n\n        if axis is not None:\n            axis_int_none = self.obj._get_axis_number(axis)\n        else:\n            axis_int_none = axis\n        new_self.axis = axis_int_none\n        return new_self\n\n    def _get_setitem_indexer(self, key):\n        \"\"\"\n        Convert a potentially-label-based key into a positional indexer.\n        \"\"\"\n        if self.name == \"loc\":\n            # always holds here bc iloc overrides _get_setitem_indexer\n            self._ensure_listlike_indexer(key)\n\n        if isinstance(key, tuple):\n            for x in key:\n                check_dict_or_set_indexers(x)\n\n        if self.axis is not None:\n            key = _tupleize_axis_indexer(self.ndim, self.axis, key)\n\n        ax = self.obj._get_axis(0)\n\n        if isinstance(ax, MultiIndex) and self.name != \"iloc\" and is_hashable(key):\n            with suppress(KeyError, InvalidIndexError):\n                # TypeError e.g. passed a bool\n                return ax.get_loc(key)\n\n        if isinstance(key, tuple):\n            with suppress(IndexingError):\n                # suppress \"Too many indexers\"\n                return self._convert_tuple(key)\n\n        if isinstance(key, range):\n            # GH#45479 test_loc_setitem_range_key\n            key = list(key)\n\n        return self._convert_to_indexer(key, axis=0)\n\n    @final\n    def _maybe_mask_setitem_value(self, indexer, value):\n        \"\"\"\n        If we have obj.iloc[mask] = series_or_frame and series_or_frame has the\n        same length as obj, we treat this as obj.iloc[mask] = series_or_frame[mask],\n        similar to Series.__setitem__.\n\n        Note this is only for loc, not iloc.\n        \"\"\"\n\n        if (\n            isinstance(indexer, tuple)\n            and len(indexer) == 2\n            and isinstance(value, (ABCSeries, ABCDataFrame))\n        ):\n            pi, icols = indexer\n            ndim = value.ndim\n            if com.is_bool_indexer(pi) and len(value) == len(pi):\n                newkey = pi.nonzero()[0]\n\n                if is_scalar_indexer(icols, self.ndim - 1) and ndim == 1:\n                    # e.g. test_loc_setitem_boolean_mask_allfalse\n                    if len(newkey) == 0:\n                        # FIXME: kludge for test_loc_setitem_boolean_mask_allfalse\n                        # TODO(GH#45333): may be fixed when deprecation is enforced\n\n                        value = value.iloc[:0]\n                    else:\n                        # test_loc_setitem_ndframe_values_alignment\n                        value = self.obj.iloc._align_series(indexer, value)\n                    indexer = (newkey, icols)\n\n                elif (\n                    isinstance(icols, np.ndarray)\n                    and icols.dtype.kind == \"i\"\n                    and len(icols) == 1\n                ):\n                    if ndim == 1:\n                        # We implicitly broadcast, though numpy does not, see\n                        # github.com/pandas-dev/pandas/pull/45501#discussion_r789071825\n                        # test_loc_setitem_ndframe_values_alignment\n                        value = self.obj.iloc._align_series(indexer, value)\n                        indexer = (newkey, icols)\n\n                    elif ndim == 2 and value.shape[1] == 1:\n                        if len(newkey) == 0:\n                            # FIXME: kludge for\n                            #  test_loc_setitem_all_false_boolean_two_blocks\n                            #  TODO(GH#45333): may be fixed when deprecation is enforced\n                            value = value.iloc[:0]\n                        else:\n                            # test_loc_setitem_ndframe_values_alignment\n                            value = self.obj.iloc._align_frame(indexer, value)\n                        indexer = (newkey, icols)\n        elif com.is_bool_indexer(indexer):\n            indexer = indexer.nonzero()[0]\n\n        return indexer, value\n\n    @final\n    def _ensure_listlike_indexer(self, key, axis=None, value=None) -> None:\n        \"\"\"\n        Ensure that a list-like of column labels are all present by adding them if\n        they do not already exist.\n\n        Parameters\n        ----------\n        key : list-like of column labels\n            Target labels.\n        axis : key axis if known\n        \"\"\"\n        column_axis = 1\n\n        # column only exists in 2-dimensional DataFrame\n        if self.ndim != 2:\n            return\n\n        orig_key = key\n        if isinstance(key, tuple) and len(key) > 1:\n            # key may be a tuple if we are .loc\n            # if length of key is > 1 set key to column part\n            key = key[column_axis]\n            axis = column_axis\n\n        if (\n            axis == column_axis\n            and not isinstance(self.obj.columns, MultiIndex)\n            and is_list_like_indexer(key)\n            and not com.is_bool_indexer(key)\n            and all(is_hashable(k) for k in key)\n        ):\n            # GH#38148\n            keys = self.obj.columns.union(key, sort=False)\n            diff = Index(key).difference(self.obj.columns, sort=False)\n\n            if len(diff) and com.is_null_slice(orig_key[0]):\n                # e.g. if we are doing df.loc[:, [\"A\", \"B\"]] = 7 and \"B\"\n                #  is a new column, add the new columns with dtype=np.void\n                #  so that later when we go through setitem_single_column\n                #  we will use isetitem. Without this, the reindex_axis\n                #  below would create float64 columns in this example, which\n                #  would successfully hold 7, so we would end up with the wrong\n                #  dtype.\n                indexer = np.arange(len(keys), dtype=np.intp)\n                indexer[len(self.obj.columns) :] = -1\n                new_mgr = self.obj._mgr.reindex_indexer(\n                    keys, indexer=indexer, axis=0, only_slice=True, use_na_proxy=True\n                )\n                self.obj._mgr = new_mgr\n                return\n\n            self.obj._mgr = self.obj._mgr.reindex_axis(keys, axis=0, only_slice=True)\n\n    @final\n    def __setitem__(self, key, value) -> None:\n        if not PYPY and using_copy_on_write():\n            if sys.getrefcount(self.obj) <= 2:\n                raise ChainedAssignmentError(_chained_assignment_msg)\n\n        check_dict_or_set_indexers(key)\n        if isinstance(key, tuple):\n            key = tuple(list(x) if is_iterator(x) else x for x in key)\n            key = tuple(com.apply_if_callable(x, self.obj) for x in key)\n        else:\n            key = com.apply_if_callable(key, self.obj)\n        indexer = self._get_setitem_indexer(key)\n        self._has_valid_setitem_indexer(key)\n\n        iloc = self if self.name == \"iloc\" else self.obj.iloc\n        iloc._setitem_with_indexer(indexer, value, self.name)\n\n    def _validate_key(self, key, axis: AxisInt):\n        \"\"\"\n        Ensure that key is valid for current indexer.\n\n        Parameters\n        ----------\n        key : scalar, slice or list-like\n            Key requested.\n        axis : int\n            Dimension on which the indexing is being made.\n\n        Raises\n        ------\n        TypeError\n            If the key (or some element of it) has wrong type.\n        IndexError\n            If the key (or some element of it) is out of bounds.\n        KeyError\n            If the key was not found.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @final\n    def _expand_ellipsis(self, tup: tuple) -> tuple:\n        \"\"\"\n        If a tuple key includes an Ellipsis, replace it with an appropriate\n        number of null slices.\n        \"\"\"\n        if any(x is Ellipsis for x in tup):\n            if tup.count(Ellipsis) > 1:\n                raise IndexingError(_one_ellipsis_message)\n\n            if len(tup) == self.ndim:\n                # It is unambiguous what axis this Ellipsis is indexing,\n                #  treat as a single null slice.\n                i = tup.index(Ellipsis)\n                # FIXME: this assumes only one Ellipsis\n                new_key = tup[:i] + (_NS,) + tup[i + 1 :]\n                return new_key\n\n            # TODO: other cases?  only one test gets here, and that is covered\n            #  by _validate_key_length\n        return tup\n\n    @final\n    def _validate_tuple_indexer(self, key: tuple) -> tuple:\n        \"\"\"\n        Check the key for valid keys across my indexer.\n        \"\"\"\n        key = self._validate_key_length(key)\n        key = self._expand_ellipsis(key)\n        for i, k in enumerate(key):\n            try:\n                self._validate_key(k, i)\n            except ValueError as err:\n                raise ValueError(\n                    \"Location based indexing can only have \"\n                    f\"[{self._valid_types}] types\"\n                ) from err\n        return key\n\n    @final\n    def _is_nested_tuple_indexer(self, tup: tuple) -> bool:\n        \"\"\"\n        Returns\n        -------\n        bool\n        \"\"\"\n        if any(isinstance(ax, MultiIndex) for ax in self.obj.axes):\n            return any(is_nested_tuple(tup, ax) for ax in self.obj.axes)\n        return False\n\n    @final\n    def _convert_tuple(self, key: tuple) -> tuple:\n        # Note: we assume _tupleize_axis_indexer has been called, if necessary.\n        self._validate_key_length(key)\n        keyidx = [self._convert_to_indexer(k, axis=i) for i, k in enumerate(key)]\n        return tuple(keyidx)\n\n    @final\n    def _validate_key_length(self, key: tuple) -> tuple:\n        if len(key) > self.ndim:\n            if key[0] is Ellipsis:\n                # e.g. Series.iloc[..., 3] reduces to just Series.iloc[3]\n                key = key[1:]\n                if Ellipsis in key:\n                    raise IndexingError(_one_ellipsis_message)\n                return self._validate_key_length(key)\n            raise IndexingError(\"Too many indexers\")\n        return key\n\n    @final\n    def _getitem_tuple_same_dim(self, tup: tuple):\n        \"\"\"\n        Index with indexers that should return an object of the same dimension\n        as self.obj.\n\n        This is only called after a failed call to _getitem_lowerdim.\n        \"\"\"\n        retval = self.obj\n        for i, key in enumerate(tup):\n            if com.is_null_slice(key):\n                continue\n\n            retval = getattr(retval, self.name)._getitem_axis(key, axis=i)\n            # We should never have retval.ndim < self.ndim, as that should\n            #  be handled by the _getitem_lowerdim call above.\n            assert retval.ndim == self.ndim\n\n        if retval is self.obj:\n            # if all axes were a null slice (`df.loc[:, :]`), ensure we still\n            # return a new object (https://github.com/pandas-dev/pandas/pull/49469)\n            retval = retval.copy(deep=False)\n\n        return retval\n\n    @final\n    def _getitem_lowerdim(self, tup: tuple):\n        # we can directly get the axis result since the axis is specified\n        if self.axis is not None:\n            axis = self.obj._get_axis_number(self.axis)\n            return self._getitem_axis(tup, axis=axis)\n\n        # we may have a nested tuples indexer here\n        if self._is_nested_tuple_indexer(tup):\n            return self._getitem_nested_tuple(tup)\n\n        # we maybe be using a tuple to represent multiple dimensions here\n        ax0 = self.obj._get_axis(0)\n        # ...but iloc should handle the tuple as simple integer-location\n        # instead of checking it as multiindex representation (GH 13797)\n        if (\n            isinstance(ax0, MultiIndex)\n            and self.name != \"iloc\"\n            and not any(isinstance(x, slice) for x in tup)\n        ):\n            # Note: in all extant test cases, replacing the slice condition with\n            #  `all(is_hashable(x) or com.is_null_slice(x) for x in tup)`\n            #  is equivalent.\n            #  (see the other place where we call _handle_lowerdim_multi_index_axis0)\n            with suppress(IndexingError):\n                return cast(_LocIndexer, self)._handle_lowerdim_multi_index_axis0(tup)\n\n        tup = self._validate_key_length(tup)\n\n        for i, key in enumerate(tup):\n            if is_label_like(key):\n                # We don't need to check for tuples here because those are\n                #  caught by the _is_nested_tuple_indexer check above.\n                section = self._getitem_axis(key, axis=i)\n\n                # We should never have a scalar section here, because\n                #  _getitem_lowerdim is only called after a check for\n                #  is_scalar_access, which that would be.\n                if section.ndim == self.ndim:\n                    # we're in the middle of slicing through a MultiIndex\n                    # revise the key wrt to `section` by inserting an _NS\n                    new_key = tup[:i] + (_NS,) + tup[i + 1 :]\n\n                else:\n                    # Note: the section.ndim == self.ndim check above\n                    #  rules out having DataFrame here, so we dont need to worry\n                    #  about transposing.\n                    new_key = tup[:i] + tup[i + 1 :]\n\n                    if len(new_key) == 1:\n                        new_key = new_key[0]\n\n                # Slices should return views, but calling iloc/loc with a null\n                # slice returns a new object.\n                if com.is_null_slice(new_key):\n                    return section\n                # This is an elided recursive call to iloc/loc\n                return getattr(section, self.name)[new_key]\n\n        raise IndexingError(\"not applicable\")\n\n    @final\n    def _getitem_nested_tuple(self, tup: tuple):\n        # we have a nested tuple so have at least 1 multi-index level\n        # we should be able to match up the dimensionality here\n\n        for key in tup:\n            check_dict_or_set_indexers(key)\n\n        # we have too many indexers for our dim, but have at least 1\n        # multi-index dimension, try to see if we have something like\n        # a tuple passed to a series with a multi-index\n        if len(tup) > self.ndim:\n            if self.name != \"loc\":\n                # This should never be reached, but let's be explicit about it\n                raise ValueError(\"Too many indices\")  # pragma: no cover\n            if all(is_hashable(x) or com.is_null_slice(x) for x in tup):\n                # GH#10521 Series should reduce MultiIndex dimensions instead of\n                #  DataFrame, IndexingError is not raised when slice(None,None,None)\n                #  with one row.\n                with suppress(IndexingError):\n                    return cast(_LocIndexer, self)._handle_lowerdim_multi_index_axis0(\n                        tup\n                    )\n            elif isinstance(self.obj, ABCSeries) and any(\n                isinstance(k, tuple) for k in tup\n            ):\n                # GH#35349 Raise if tuple in tuple for series\n                # Do this after the all-hashable-or-null-slice check so that\n                #  we are only getting non-hashable tuples, in particular ones\n                #  that themselves contain a slice entry\n                # See test_loc_series_getitem_too_many_dimensions\n                raise IndexingError(\"Too many indexers\")\n\n            # this is a series with a multi-index specified a tuple of\n            # selectors\n            axis = self.axis or 0\n            return self._getitem_axis(tup, axis=axis)\n\n        # handle the multi-axis by taking sections and reducing\n        # this is iterative\n        obj = self.obj\n        # GH#41369 Loop in reverse order ensures indexing along columns before rows\n        # which selects only necessary blocks which avoids dtype conversion if possible\n        axis = len(tup) - 1\n        for key in tup[::-1]:\n            if com.is_null_slice(key):\n                axis -= 1\n                continue\n\n            obj = getattr(obj, self.name)._getitem_axis(key, axis=axis)\n            axis -= 1\n\n            # if we have a scalar, we are done\n            if is_scalar(obj) or not hasattr(obj, \"ndim\"):\n                break\n\n        return obj\n\n    def _convert_to_indexer(self, key, axis: AxisInt):\n        raise AbstractMethodError(self)\n\n    @final\n    def __getitem__(self, key):\n        check_dict_or_set_indexers(key)\n        if type(key) is tuple:\n            key = tuple(list(x) if is_iterator(x) else x for x in key)\n            key = tuple(com.apply_if_callable(x, self.obj) for x in key)\n            if self._is_scalar_access(key):\n                return self.obj._get_value(*key, takeable=self._takeable)\n            return self._getitem_tuple(key)\n        else:\n            # we by definition only have the 0th axis\n            axis = self.axis or 0\n\n            maybe_callable = com.apply_if_callable(key, self.obj)\n            return self._getitem_axis(maybe_callable, axis=axis)\n\n    def _is_scalar_access(self, key: tuple):\n        raise NotImplementedError()\n\n    def _getitem_tuple(self, tup: tuple):\n        raise AbstractMethodError(self)\n\n    def _getitem_axis(self, key, axis: AxisInt):\n        raise NotImplementedError()\n\n    def _has_valid_setitem_indexer(self, indexer) -> bool:\n        raise AbstractMethodError(self)\n\n    @final\n    def _getbool_axis(self, key, axis: AxisInt):\n        # caller is responsible for ensuring non-None axis\n        labels = self.obj._get_axis(axis)\n        key = check_bool_indexer(labels, key)\n        inds = key.nonzero()[0]\n        return self.obj._take_with_is_copy(inds, axis=axis)\n\n\n@doc(IndexingMixin.loc)\nclass _LocIndexer(_LocationIndexer):\n    _takeable: bool = False\n    _valid_types = (\n        \"labels (MUST BE IN THE INDEX), slices of labels (BOTH \"\n        \"endpoints included! Can be slices of integers if the \"\n        \"index is integers), listlike of labels, boolean\"\n    )\n\n    # -------------------------------------------------------------------\n    # Key Checks\n\n    @doc(_LocationIndexer._validate_key)\n    def _validate_key(self, key, axis: Axis):\n        # valid for a collection of labels (we check their presence later)\n        # slice of labels (where start-end in labels)\n        # slice of integers (only if in the labels)\n        # boolean not in slice and with boolean index\n        ax = self.obj._get_axis(axis)\n        if isinstance(key, bool) and not (\n            is_bool_dtype(ax)\n            or ax.dtype.name == \"boolean\"\n            or isinstance(ax, MultiIndex)\n            and is_bool_dtype(ax.get_level_values(0))\n        ):\n            raise KeyError(\n                f\"{key}: boolean label can not be used without a boolean index\"\n            )\n\n        if isinstance(key, slice) and (\n            isinstance(key.start, bool) or isinstance(key.stop, bool)\n        ):\n            raise TypeError(f\"{key}: boolean values can not be used in a slice\")\n\n    def _has_valid_setitem_indexer(self, indexer) -> bool:\n        return True\n\n    def _is_scalar_access(self, key: tuple) -> bool:\n        \"\"\"\n        Returns\n        -------\n        bool\n        \"\"\"\n        # this is a shortcut accessor to both .loc and .iloc\n        # that provide the equivalent access of .at and .iat\n        # a) avoid getting things via sections and (to minimize dtype changes)\n        # b) provide a performant path\n        if len(key) != self.ndim:\n            return False\n\n        for i, k in enumerate(key):\n            if not is_scalar(k):\n                return False\n\n            ax = self.obj.axes[i]\n            if isinstance(ax, MultiIndex):\n                return False\n\n            if isinstance(k, str) and ax._supports_partial_string_indexing:\n                # partial string indexing, df.loc['2000', 'A']\n                # should not be considered scalar\n                return False\n\n            if not ax._index_as_unique:\n                return False\n\n        return True\n\n    # -------------------------------------------------------------------\n    # MultiIndex Handling\n\n    def _multi_take_opportunity(self, tup: tuple) -> bool:\n        \"\"\"\n        Check whether there is the possibility to use ``_multi_take``.\n\n        Currently the limit is that all axes being indexed, must be indexed with\n        list-likes.\n\n        Parameters\n        ----------\n        tup : tuple\n            Tuple of indexers, one per axis.\n\n        Returns\n        -------\n        bool\n            Whether the current indexing,\n            can be passed through `_multi_take`.\n        \"\"\"\n        if not all(is_list_like_indexer(x) for x in tup):\n            return False\n\n        # just too complicated\n        return not any(com.is_bool_indexer(x) for x in tup)\n\n    def _multi_take(self, tup: tuple):\n        \"\"\"\n        Create the indexers for the passed tuple of keys, and\n        executes the take operation. This allows the take operation to be\n        executed all at once, rather than once for each dimension.\n        Improving efficiency.\n\n        Parameters\n        ----------\n        tup : tuple\n            Tuple of indexers, one per axis.\n\n        Returns\n        -------\n        values: same type as the object being indexed\n        \"\"\"\n        # GH 836\n        d = {\n            axis: self._get_listlike_indexer(key, axis)\n            for (key, axis) in zip(tup, self.obj._AXIS_ORDERS)\n        }\n        return self.obj._reindex_with_indexers(d, copy=True, allow_dups=True)\n\n    # -------------------------------------------------------------------\n\n    def _getitem_iterable(self, key, axis: AxisInt):\n        \"\"\"\n        Index current object with an iterable collection of keys.\n\n        Parameters\n        ----------\n        key : iterable\n            Targeted labels.\n        axis : int\n            Dimension on which the indexing is being made.\n\n        Raises\n        ------\n        KeyError\n            If no key was found. Will change in the future to raise if not all\n            keys were found.\n\n        Returns\n        -------\n        scalar, DataFrame, or Series: indexed value(s).\n        \"\"\"\n        # we assume that not com.is_bool_indexer(key), as that is\n        #  handled before we get here.\n        self._validate_key(key, axis)\n\n        # A collection of keys\n        keyarr, indexer = self._get_listlike_indexer(key, axis)\n        return self.obj._reindex_with_indexers(\n            {axis: [keyarr, indexer]}, copy=True, allow_dups=True\n        )\n\n    def _getitem_tuple(self, tup: tuple):\n        with suppress(IndexingError):\n            tup = self._expand_ellipsis(tup)\n            return self._getitem_lowerdim(tup)\n\n        # no multi-index, so validate all of the indexers\n        tup = self._validate_tuple_indexer(tup)\n\n        # ugly hack for GH #836\n        if self._multi_take_opportunity(tup):\n            return self._multi_take(tup)\n\n        return self._getitem_tuple_same_dim(tup)\n\n    def _get_label(self, label, axis: AxisInt):\n        # GH#5567 this will fail if the label is not present in the axis.\n        return self.obj.xs(label, axis=axis)\n\n    def _handle_lowerdim_multi_index_axis0(self, tup: tuple):\n        # we have an axis0 multi-index, handle or raise\n        axis = self.axis or 0\n        try:\n            # fast path for series or for tup devoid of slices\n            return self._get_label(tup, axis=axis)\n\n        except KeyError as ek:\n            # raise KeyError if number of indexers match\n            # else IndexingError will be raised\n            if self.ndim < len(tup) <= self.obj.index.nlevels:\n                raise ek\n            raise IndexingError(\"No label returned\") from ek\n\n    def _getitem_axis(self, key, axis: AxisInt):\n        key = item_from_zerodim(key)\n        if is_iterator(key):\n            key = list(key)\n        if key is Ellipsis:\n            key = slice(None)\n\n        labels = self.obj._get_axis(axis)\n\n        if isinstance(key, tuple) and isinstance(labels, MultiIndex):\n            key = tuple(key)\n\n        if isinstance(key, slice):\n            self._validate_key(key, axis)\n            return self._get_slice_axis(key, axis=axis)\n        elif com.is_bool_indexer(key):\n            return self._getbool_axis(key, axis=axis)\n        elif is_list_like_indexer(key):\n            # an iterable multi-selection\n            if not (isinstance(key, tuple) and isinstance(labels, MultiIndex)):\n                if hasattr(key, \"ndim\") and key.ndim > 1:\n                    raise ValueError(\"Cannot index with multidimensional key\")\n\n                return self._getitem_iterable(key, axis=axis)\n\n            # nested tuple slicing\n            if is_nested_tuple(key, labels):\n                locs = labels.get_locs(key)\n                indexer = [slice(None)] * self.ndim\n                indexer[axis] = locs\n                return self.obj.iloc[tuple(indexer)]\n\n        # fall thru to straight lookup\n        self._validate_key(key, axis)\n        return self._get_label(key, axis=axis)\n\n    def _get_slice_axis(self, slice_obj: slice, axis: AxisInt):\n        \"\"\"\n        This is pretty simple as we just have to deal with labels.\n        \"\"\"\n        # caller is responsible for ensuring non-None axis\n        obj = self.obj\n        if not need_slice(slice_obj):\n            return obj.copy(deep=False)\n\n        labels = obj._get_axis(axis)\n        indexer = labels.slice_indexer(slice_obj.start, slice_obj.stop, slice_obj.step)\n\n        if isinstance(indexer, slice):\n            return self.obj._slice(indexer, axis=axis)\n        else:\n            # DatetimeIndex overrides Index.slice_indexer and may\n            #  return a DatetimeIndex instead of a slice object.\n            return self.obj.take(indexer, axis=axis)\n\n    def _convert_to_indexer(self, key, axis: AxisInt):\n        \"\"\"\n        Convert indexing key into something we can use to do actual fancy\n        indexing on a ndarray.\n\n        Examples\n        ix[:5] -> slice(0, 5)\n        ix[[1,2,3]] -> [1,2,3]\n        ix[['foo', 'bar', 'baz']] -> [i, j, k] (indices of foo, bar, baz)\n\n        Going by Zen of Python?\n        'In the face of ambiguity, refuse the temptation to guess.'\n        raise AmbiguousIndexError with integer labels?\n        - No, prefer label-based indexing\n        \"\"\"\n        labels = self.obj._get_axis(axis)\n\n        if isinstance(key, slice):\n            return labels._convert_slice_indexer(key, kind=\"loc\")\n\n        if (\n            isinstance(key, tuple)\n            and not isinstance(labels, MultiIndex)\n            and self.ndim < 2\n            and len(key) > 1\n        ):\n            raise IndexingError(\"Too many indexers\")\n\n        if is_scalar(key) or (isinstance(labels, MultiIndex) and is_hashable(key)):\n            # Otherwise get_loc will raise InvalidIndexError\n\n            # if we are a label return me\n            try:\n                return labels.get_loc(key)\n            except LookupError:\n                if isinstance(key, tuple) and isinstance(labels, MultiIndex):\n                    if len(key) == labels.nlevels:\n                        return {\"key\": key}\n                    raise\n            except InvalidIndexError:\n                # GH35015, using datetime as column indices raises exception\n                if not isinstance(labels, MultiIndex):\n                    raise\n            except ValueError:\n                if not is_integer(key):\n                    raise\n                return {\"key\": key}\n\n        if is_nested_tuple(key, labels):\n            if self.ndim == 1 and any(isinstance(k, tuple) for k in key):\n                # GH#35349 Raise if tuple in tuple for series\n                raise IndexingError(\"Too many indexers\")\n            return labels.get_locs(key)\n\n        elif is_list_like_indexer(key):\n            if is_iterator(key):\n                key = list(key)\n\n            if com.is_bool_indexer(key):\n                key = check_bool_indexer(labels, key)\n                return key\n            else:\n                return self._get_listlike_indexer(key, axis)[1]\n        else:\n            try:\n                return labels.get_loc(key)\n            except LookupError:\n                # allow a not found key only if we are a setter\n                if not is_list_like_indexer(key):\n                    return {\"key\": key}\n                raise\n\n    def _get_listlike_indexer(self, key, axis: AxisInt):\n        \"\"\"\n        Transform a list-like of keys into a new index and an indexer.\n\n        Parameters\n        ----------\n        key : list-like\n            Targeted labels.\n        axis:  int\n            Dimension on which the indexing is being made.\n\n        Raises\n        ------\n        KeyError\n            If at least one key was requested but none was found.\n\n        Returns\n        -------\n        keyarr: Index\n            New index (coinciding with 'key' if the axis is unique).\n        values : array-like\n            Indexer for the return object, -1 denotes keys not found.\n        \"\"\"\n        ax = self.obj._get_axis(axis)\n        axis_name = self.obj._get_axis_name(axis)\n\n        keyarr, indexer = ax._get_indexer_strict(key, axis_name)\n\n        return keyarr, indexer\n\n\n@doc(IndexingMixin.iloc)\nclass _iLocIndexer(_LocationIndexer):\n    _valid_types = (\n        \"integer, integer slice (START point is INCLUDED, END \"\n        \"point is EXCLUDED), listlike of integers, boolean array\"\n    )\n    _takeable = True\n\n    # -------------------------------------------------------------------\n    # Key Checks\n\n    def _validate_key(self, key, axis: AxisInt):\n        if com.is_bool_indexer(key):\n            if hasattr(key, \"index\") and isinstance(key.index, Index):\n                if key.index.inferred_type == \"integer\":\n                    raise NotImplementedError(\n                        \"iLocation based boolean \"\n                        \"indexing on an integer type \"\n                        \"is not available\"\n                    )\n                raise ValueError(\n                    \"iLocation based boolean indexing cannot use \"\n                    \"an indexable as a mask\"\n                )\n            return\n\n        if isinstance(key, slice):\n            return\n        elif is_integer(key):\n            self._validate_integer(key, axis)\n        elif isinstance(key, tuple):\n            # a tuple should already have been caught by this point\n            # so don't treat a tuple as a valid indexer\n            raise IndexingError(\"Too many indexers\")\n        elif is_list_like_indexer(key):\n            if isinstance(key, ABCSeries):\n                arr = key._values\n            elif is_array_like(key):\n                arr = key\n            else:\n                arr = np.array(key)\n            len_axis = len(self.obj._get_axis(axis))\n\n            # check that the key has a numeric dtype\n            if not is_numeric_dtype(arr.dtype):\n                raise IndexError(f\".iloc requires numeric indexers, got {arr}\")\n\n            # check that the key does not exceed the maximum size of the index\n            if len(arr) and (arr.max() >= len_axis or arr.min() < -len_axis):\n                raise IndexError(\"positional indexers are out-of-bounds\")\n        else:\n            raise ValueError(f\"Can only index by location with a [{self._valid_types}]\")\n\n    def _has_valid_setitem_indexer(self, indexer) -> bool:\n        \"\"\"\n        Validate that a positional indexer cannot enlarge its target\n        will raise if needed, does not modify the indexer externally.\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        if isinstance(indexer, dict):\n            raise IndexError(\"iloc cannot enlarge its target object\")\n\n        if isinstance(indexer, ABCDataFrame):\n            raise TypeError(\n                \"DataFrame indexer for .iloc is not supported. \"\n                \"Consider using .loc with a DataFrame indexer for automatic alignment.\",\n            )\n\n        if not isinstance(indexer, tuple):\n            indexer = _tuplify(self.ndim, indexer)\n\n        for ax, i in zip(self.obj.axes, indexer):\n            if isinstance(i, slice):\n                # should check the stop slice?\n                pass\n            elif is_list_like_indexer(i):\n                # should check the elements?\n                pass\n            elif is_integer(i):\n                if i >= len(ax):\n                    raise IndexError(\"iloc cannot enlarge its target object\")\n            elif isinstance(i, dict):\n                raise IndexError(\"iloc cannot enlarge its target object\")\n\n        return True\n\n    def _is_scalar_access(self, key: tuple) -> bool:\n        \"\"\"\n        Returns\n        -------\n        bool\n        \"\"\"\n        # this is a shortcut accessor to both .loc and .iloc\n        # that provide the equivalent access of .at and .iat\n        # a) avoid getting things via sections and (to minimize dtype changes)\n        # b) provide a performant path\n        if len(key) != self.ndim:\n            return False\n\n        return all(is_integer(k) for k in key)\n\n    def _validate_integer(self, key: int, axis: AxisInt) -> None:\n        \"\"\"\n        Check that 'key' is a valid position in the desired axis.\n\n        Parameters\n        ----------\n        key : int\n            Requested position.\n        axis : int\n            Desired axis.\n\n        Raises\n        ------\n        IndexError\n            If 'key' is not a valid position in axis 'axis'.\n        \"\"\"\n        len_axis = len(self.obj._get_axis(axis))\n        if key >= len_axis or key < -len_axis:\n            raise IndexError(\"single positional indexer is out-of-bounds\")\n\n    # -------------------------------------------------------------------\n\n    def _getitem_tuple(self, tup: tuple):\n        tup = self._validate_tuple_indexer(tup)\n        with suppress(IndexingError):\n            return self._getitem_lowerdim(tup)\n\n        return self._getitem_tuple_same_dim(tup)\n\n    def _get_list_axis(self, key, axis: AxisInt):\n        \"\"\"\n        Return Series values by list or array of integers.\n\n        Parameters\n        ----------\n        key : list-like positional indexer\n        axis : int\n\n        Returns\n        -------\n        Series object\n\n        Notes\n        -----\n        `axis` can only be zero.\n        \"\"\"\n        try:\n            return self.obj._take_with_is_copy(key, axis=axis)\n        except IndexError as err:\n            # re-raise with different error message\n            raise IndexError(\"positional indexers are out-of-bounds\") from err\n\n    def _getitem_axis(self, key, axis: AxisInt):\n        if key is Ellipsis:\n            key = slice(None)\n        elif isinstance(key, ABCDataFrame):\n            raise IndexError(\n                \"DataFrame indexer is not allowed for .iloc\\n\"\n                \"Consider using .loc for automatic alignment.\"\n            )\n\n        if isinstance(key, slice):\n            return self._get_slice_axis(key, axis=axis)\n\n        if is_iterator(key):\n            key = list(key)\n\n        if isinstance(key, list):\n            key = np.asarray(key)\n\n        if com.is_bool_indexer(key):\n            self._validate_key(key, axis)\n            return self._getbool_axis(key, axis=axis)\n\n        # a list of integers\n        elif is_list_like_indexer(key):\n            return self._get_list_axis(key, axis=axis)\n\n        # a single integer\n        else:\n            key = item_from_zerodim(key)\n            if not is_integer(key):\n                raise TypeError(\"Cannot index by location index with a non-integer key\")\n\n            # validate the location\n            self._validate_integer(key, axis)\n\n            return self.obj._ixs(key, axis=axis)\n\n    def _get_slice_axis(self, slice_obj: slice, axis: AxisInt):\n        # caller is responsible for ensuring non-None axis\n        obj = self.obj\n\n        if not need_slice(slice_obj):\n            return obj.copy(deep=False)\n\n        labels = obj._get_axis(axis)\n        labels._validate_positional_slice(slice_obj)\n        return self.obj._slice(slice_obj, axis=axis)\n\n    def _convert_to_indexer(self, key, axis: AxisInt):\n        \"\"\"\n        Much simpler as we only have to deal with our valid types.\n        \"\"\"\n        return key\n\n    def _get_setitem_indexer(self, key):\n        # GH#32257 Fall through to let numpy do validation\n        if is_iterator(key):\n            key = list(key)\n\n        if self.axis is not None:\n            key = _tupleize_axis_indexer(self.ndim, self.axis, key)\n\n        return key\n\n    # -------------------------------------------------------------------\n\n    def _setitem_with_indexer(self, indexer, value, name: str = \"iloc\"):\n        \"\"\"\n        _setitem_with_indexer is for setting values on a Series/DataFrame\n        using positional indexers.\n\n        If the relevant keys are not present, the Series/DataFrame may be\n        expanded.\n\n        This method is currently broken when dealing with non-unique Indexes,\n        since it goes from positional indexers back to labels when calling\n        BlockManager methods, see GH#12991, GH#22046, GH#15686.\n        \"\"\"\n        info_axis = self.obj._info_axis_number\n\n        # maybe partial set\n        take_split_path = not self.obj._mgr.is_single_block\n\n        if not take_split_path and isinstance(value, ABCDataFrame):\n            # Avoid cast of values\n            take_split_path = not value._mgr.is_single_block\n\n        # if there is only one block/type, still have to take split path\n        # unless the block is one-dimensional or it can hold the value\n        if not take_split_path and len(self.obj._mgr.arrays) and self.ndim > 1:\n            # in case of dict, keys are indices\n            val = list(value.values()) if isinstance(value, dict) else value\n            arr = self.obj._mgr.arrays[0]\n            take_split_path = not can_hold_element(\n                arr, extract_array(val, extract_numpy=True)\n            )\n\n        # if we have any multi-indexes that have non-trivial slices\n        # (not null slices) then we must take the split path, xref\n        # GH 10360, GH 27841\n        if isinstance(indexer, tuple) and len(indexer) == len(self.obj.axes):\n            for i, ax in zip(indexer, self.obj.axes):\n                if isinstance(ax, MultiIndex) and not (\n                    is_integer(i) or com.is_null_slice(i)\n                ):\n                    take_split_path = True\n                    break\n\n        if isinstance(indexer, tuple):\n            nindexer = []\n            for i, idx in enumerate(indexer):\n                if isinstance(idx, dict):\n                    # reindex the axis to the new value\n                    # and set inplace\n                    key, _ = convert_missing_indexer(idx)\n\n                    # if this is the items axes, then take the main missing\n                    # path first\n                    # this correctly sets the dtype and avoids cache issues\n                    # essentially this separates out the block that is needed\n                    # to possibly be modified\n                    if self.ndim > 1 and i == info_axis:\n                        # add the new item, and set the value\n                        # must have all defined axes if we have a scalar\n                        # or a list-like on the non-info axes if we have a\n                        # list-like\n                        if not len(self.obj):\n                            if not is_list_like_indexer(value):\n                                raise ValueError(\n                                    \"cannot set a frame with no \"\n                                    \"defined index and a scalar\"\n                                )\n                            self.obj[key] = value\n                            return\n\n                        # add a new item with the dtype setup\n                        if com.is_null_slice(indexer[0]):\n                            # We are setting an entire column\n                            self.obj[key] = value\n                            return\n                        elif is_array_like(value):\n                            # GH#42099\n                            arr = extract_array(value, extract_numpy=True)\n                            taker = -1 * np.ones(len(self.obj), dtype=np.intp)\n                            empty_value = algos.take_nd(arr, taker)\n                            if not isinstance(value, ABCSeries):\n                                # if not Series (in which case we need to align),\n                                #  we can short-circuit\n                                empty_value[indexer[0]] = arr\n                                self.obj[key] = empty_value\n                                return\n\n                            self.obj[key] = empty_value\n\n                        else:\n                            # FIXME: GH#42099#issuecomment-864326014\n                            self.obj[key] = infer_fill_value(value)\n\n                        new_indexer = convert_from_missing_indexer_tuple(\n                            indexer, self.obj.axes\n                        )\n                        self._setitem_with_indexer(new_indexer, value, name)\n\n                        return\n\n                    # reindex the axis\n                    # make sure to clear the cache because we are\n                    # just replacing the block manager here\n                    # so the object is the same\n                    index = self.obj._get_axis(i)\n                    labels = index.insert(len(index), key)\n\n                    # We are expanding the Series/DataFrame values to match\n                    #  the length of thenew index `labels`.  GH#40096 ensure\n                    #  this is valid even if the index has duplicates.\n                    taker = np.arange(len(index) + 1, dtype=np.intp)\n                    taker[-1] = -1\n                    reindexers = {i: (labels, taker)}\n                    new_obj = self.obj._reindex_with_indexers(\n                        reindexers, allow_dups=True\n                    )\n                    self.obj._mgr = new_obj._mgr\n                    self.obj._maybe_update_cacher(clear=True)\n                    self.obj._is_copy = None\n\n                    nindexer.append(labels.get_loc(key))\n\n                else:\n                    nindexer.append(idx)\n\n            indexer = tuple(nindexer)\n        else:\n            indexer, missing = convert_missing_indexer(indexer)\n\n            if missing:\n                self._setitem_with_indexer_missing(indexer, value)\n                return\n\n        if name == \"loc\":\n            # must come after setting of missing\n            indexer, value = self._maybe_mask_setitem_value(indexer, value)\n\n        # align and set the values\n        if take_split_path:\n            # We have to operate column-wise\n            self._setitem_with_indexer_split_path(indexer, value, name)\n        else:\n            self._setitem_single_block(indexer, value, name)\n\n    def _setitem_with_indexer_split_path(self, indexer, value, name: str):\n        \"\"\"\n        Setitem column-wise.\n        \"\"\"\n        # Above we only set take_split_path to True for 2D cases\n        assert self.ndim == 2\n\n        if not isinstance(indexer, tuple):\n            indexer = _tuplify(self.ndim, indexer)\n        if len(indexer) > self.ndim:\n            raise IndexError(\"too many indices for array\")\n        if isinstance(indexer[0], np.ndarray) and indexer[0].ndim > 2:\n            raise ValueError(r\"Cannot set values with ndim > 2\")\n\n        if (isinstance(value, ABCSeries) and name != \"iloc\") or isinstance(value, dict):\n            from pandas import Series\n\n            value = self._align_series(indexer, Series(value))\n\n        # Ensure we have something we can iterate over\n        info_axis = indexer[1]\n        ilocs = self._ensure_iterable_column_indexer(info_axis)\n\n        pi = indexer[0]\n        lplane_indexer = length_of_indexer(pi, self.obj.index)\n        # lplane_indexer gives the expected length of obj[indexer[0]]\n\n        # we need an iterable, with a ndim of at least 1\n        # eg. don't pass through np.array(0)\n        if is_list_like_indexer(value) and getattr(value, \"ndim\", 1) > 0:\n            if isinstance(value, ABCDataFrame):\n                self._setitem_with_indexer_frame_value(indexer, value, name)\n\n            elif np.ndim(value) == 2:\n                # TODO: avoid np.ndim call in case it isn't an ndarray, since\n                #  that will construct an ndarray, which will be wasteful\n                self._setitem_with_indexer_2d_value(indexer, value)\n\n            elif len(ilocs) == 1 and lplane_indexer == len(value) and not is_scalar(pi):\n                # We are setting multiple rows in a single column.\n                self._setitem_single_column(ilocs[0], value, pi)\n\n            elif len(ilocs) == 1 and 0 != lplane_indexer != len(value):\n                # We are trying to set N values into M entries of a single\n                #  column, which is invalid for N != M\n                # Exclude zero-len for e.g. boolean masking that is all-false\n\n                if len(value) == 1 and not is_integer(info_axis):\n                    # This is a case like df.iloc[:3, [1]] = [0]\n                    #  where we treat as df.iloc[:3, 1] = 0\n                    return self._setitem_with_indexer((pi, info_axis[0]), value[0])\n\n                raise ValueError(\n                    \"Must have equal len keys and value \"\n                    \"when setting with an iterable\"\n                )\n\n            elif lplane_indexer == 0 and len(value) == len(self.obj.index):\n                # We get here in one case via .loc with a all-False mask\n                pass\n\n            elif self._is_scalar_access(indexer) and is_object_dtype(\n                self.obj.dtypes[ilocs[0]]\n            ):\n                # We are setting nested data, only possible for object dtype data\n                self._setitem_single_column(indexer[1], value, pi)\n\n            elif len(ilocs) == len(value):\n                # We are setting multiple columns in a single row.\n                for loc, v in zip(ilocs, value):\n                    self._setitem_single_column(loc, v, pi)\n\n            elif len(ilocs) == 1 and com.is_null_slice(pi) and len(self.obj) == 0:\n                # This is a setitem-with-expansion, see\n                #  test_loc_setitem_empty_append_expands_rows_mixed_dtype\n                # e.g. df = DataFrame(columns=[\"x\", \"y\"])\n                #  df[\"x\"] = df[\"x\"].astype(np.int64)\n                #  df.loc[:, \"x\"] = [1, 2, 3]\n                self._setitem_single_column(ilocs[0], value, pi)\n\n            else:\n                raise ValueError(\n                    \"Must have equal len keys and value \"\n                    \"when setting with an iterable\"\n                )\n\n        else:\n            # scalar value\n            for loc in ilocs:\n                self._setitem_single_column(loc, value, pi)\n\n    def _setitem_with_indexer_2d_value(self, indexer, value):\n        # We get here with np.ndim(value) == 2, excluding DataFrame,\n        #  which goes through _setitem_with_indexer_frame_value\n        pi = indexer[0]\n\n        ilocs = self._ensure_iterable_column_indexer(indexer[1])\n\n        if not is_array_like(value):\n            # cast lists to array\n            value = np.array(value, dtype=object)\n        if len(ilocs) != value.shape[1]:\n            raise ValueError(\n                \"Must have equal len keys and value when setting with an ndarray\"\n            )\n\n        for i, loc in enumerate(ilocs):\n            value_col = value[:, i]\n            if is_object_dtype(value_col.dtype):\n                # casting to list so that we do type inference in setitem_single_column\n                value_col = value_col.tolist()\n            self._setitem_single_column(loc, value_col, pi)\n\n    def _setitem_with_indexer_frame_value(self, indexer, value: DataFrame, name: str):\n        ilocs = self._ensure_iterable_column_indexer(indexer[1])\n\n        sub_indexer = list(indexer)\n        pi = indexer[0]\n\n        multiindex_indexer = isinstance(self.obj.columns, MultiIndex)\n\n        unique_cols = value.columns.is_unique\n\n        # We do not want to align the value in case of iloc GH#37728\n        if name == \"iloc\":\n            for i, loc in enumerate(ilocs):\n                val = value.iloc[:, i]\n                self._setitem_single_column(loc, val, pi)\n\n        elif not unique_cols and value.columns.equals(self.obj.columns):\n            # We assume we are already aligned, see\n            # test_iloc_setitem_frame_duplicate_columns_multiple_blocks\n            for loc in ilocs:\n                item = self.obj.columns[loc]\n                if item in value:\n                    sub_indexer[1] = item\n                    val = self._align_series(\n                        tuple(sub_indexer),\n                        value.iloc[:, loc],\n                        multiindex_indexer,\n                    )\n                else:\n                    val = np.nan\n\n                self._setitem_single_column(loc, val, pi)\n\n        elif not unique_cols:\n            raise ValueError(\"Setting with non-unique columns is not allowed.\")\n\n        else:\n            for loc in ilocs:\n                item = self.obj.columns[loc]\n                if item in value:\n                    sub_indexer[1] = item\n                    val = self._align_series(\n                        tuple(sub_indexer), value[item], multiindex_indexer\n                    )\n                else:\n                    val = np.nan\n\n                self._setitem_single_column(loc, val, pi)\n\n    def _setitem_single_column(self, loc: int, value, plane_indexer) -> None:\n        \"\"\"\n\n        Parameters\n        ----------\n        loc : int\n            Indexer for column position\n        plane_indexer : int, slice, listlike[int]\n            The indexer we use for setitem along axis=0.\n        \"\"\"\n        pi = plane_indexer\n\n        is_full_setter = com.is_null_slice(pi) or com.is_full_slice(pi, len(self.obj))\n\n        is_null_setter = com.is_empty_slice(pi) or is_array_like(pi) and len(pi) == 0\n\n        if is_null_setter:\n            # no-op, don't cast dtype later\n            return\n\n        elif is_full_setter:\n            try:\n                self.obj._mgr.column_setitem(\n                    loc, plane_indexer, value, inplace_only=True\n                )\n            except (ValueError, TypeError, LossySetitemError):\n                # If we're setting an entire column and we can't do it inplace,\n                #  then we can use value's dtype (or inferred dtype)\n                #  instead of object\n                self.obj.isetitem(loc, value)\n        else:\n            # set value into the column (first attempting to operate inplace, then\n            #  falling back to casting if necessary)\n            self.obj._mgr.column_setitem(loc, plane_indexer, value)\n\n        self.obj._clear_item_cache()\n\n    def _setitem_single_block(self, indexer, value, name: str) -> None:\n        \"\"\"\n        _setitem_with_indexer for the case when we have a single Block.\n        \"\"\"\n        from pandas import Series\n\n        info_axis = self.obj._info_axis_number\n        item_labels = self.obj._get_axis(info_axis)\n        if isinstance(indexer, tuple):\n            # if we are setting on the info axis ONLY\n            # set using those methods to avoid block-splitting\n            # logic here\n            if (\n                self.ndim == len(indexer) == 2\n                and is_integer(indexer[1])\n                and com.is_null_slice(indexer[0])\n            ):\n                col = item_labels[indexer[info_axis]]\n                if len(item_labels.get_indexer_for([col])) == 1:\n                    # e.g. test_loc_setitem_empty_append_expands_rows\n                    loc = item_labels.get_loc(col)\n                    self._setitem_single_column(loc, value, indexer[0])\n                    return\n\n            indexer = maybe_convert_ix(*indexer)  # e.g. test_setitem_frame_align\n\n        if (isinstance(value, ABCSeries) and name != \"iloc\") or isinstance(value, dict):\n            # TODO(EA): ExtensionBlock.setitem this causes issues with\n            # setting for extensionarrays that store dicts. Need to decide\n            # if it's worth supporting that.\n            value = self._align_series(indexer, Series(value))\n\n        elif isinstance(value, ABCDataFrame) and name != \"iloc\":\n            value = self._align_frame(indexer, value)._values\n\n        # check for chained assignment\n        self.obj._check_is_chained_assignment_possible()\n\n        # actually do the set\n        self.obj._mgr = self.obj._mgr.setitem(indexer=indexer, value=value)\n        self.obj._maybe_update_cacher(clear=True, inplace=True)\n\n    def _setitem_with_indexer_missing(self, indexer, value):\n        \"\"\"\n        Insert new row(s) or column(s) into the Series or DataFrame.\n        \"\"\"\n        from pandas import Series\n\n        # reindex the axis to the new value\n        # and set inplace\n        if self.ndim == 1:\n            index = self.obj.index\n            new_index = index.insert(len(index), indexer)\n\n            # we have a coerced indexer, e.g. a float\n            # that matches in an int64 Index, so\n            # we will not create a duplicate index, rather\n            # index to that element\n            # e.g. 0.0 -> 0\n            # GH#12246\n            if index.is_unique:\n                # pass new_index[-1:] instead if [new_index[-1]]\n                #  so that we retain dtype\n                new_indexer = index.get_indexer(new_index[-1:])\n                if (new_indexer != -1).any():\n                    # We get only here with loc, so can hard code\n                    return self._setitem_with_indexer(new_indexer, value, \"loc\")\n\n            # this preserves dtype of the value and of the object\n            if not is_scalar(value):\n                new_dtype = None\n\n            elif is_valid_na_for_dtype(value, self.obj.dtype):\n                if not is_object_dtype(self.obj.dtype):\n                    # Every NA value is suitable for object, no conversion needed\n                    value = na_value_for_dtype(self.obj.dtype, compat=False)\n\n                new_dtype = maybe_promote(self.obj.dtype, value)[0]\n\n            elif isna(value):\n                new_dtype = None\n            elif not self.obj.empty and not is_object_dtype(self.obj.dtype):\n                # We should not cast, if we have object dtype because we can\n                # set timedeltas into object series\n                curr_dtype = self.obj.dtype\n                curr_dtype = getattr(curr_dtype, \"numpy_dtype\", curr_dtype)\n                new_dtype = maybe_promote(curr_dtype, value)[0]\n            else:\n                new_dtype = None\n\n            new_values = Series([value], dtype=new_dtype)._values\n\n            if len(self.obj._values):\n                # GH#22717 handle casting compatibility that np.concatenate\n                #  does incorrectly\n                new_values = concat_compat([self.obj._values, new_values])\n            self.obj._mgr = self.obj._constructor(\n                new_values, index=new_index, name=self.obj.name\n            )._mgr\n            self.obj._maybe_update_cacher(clear=True)\n\n        elif self.ndim == 2:\n            if not len(self.obj.columns):\n                # no columns and scalar\n                raise ValueError(\"cannot set a frame with no defined columns\")\n\n            has_dtype = hasattr(value, \"dtype\")\n            if isinstance(value, ABCSeries):\n                # append a Series\n                value = value.reindex(index=self.obj.columns, copy=True)\n                value.name = indexer\n            elif isinstance(value, dict):\n                value = Series(\n                    value, index=self.obj.columns, name=indexer, dtype=object\n                )\n            else:\n                # a list-list\n                if is_list_like_indexer(value):\n                    # must have conforming columns\n                    if len(value) != len(self.obj.columns):\n                        raise ValueError(\"cannot set a row with mismatched columns\")\n\n                value = Series(value, index=self.obj.columns, name=indexer)\n\n            if not len(self.obj):\n                # We will ignore the existing dtypes instead of using\n                #  internals.concat logic\n                df = value.to_frame().T\n\n                idx = self.obj.index\n                if isinstance(idx, MultiIndex):\n                    name = idx.names\n                else:\n                    name = idx.name\n\n                df.index = Index([indexer], name=name)\n                if not has_dtype:\n                    # i.e. if we already had a Series or ndarray, keep that\n                    #  dtype.  But if we had a list or dict, then do inference\n                    df = df.infer_objects(copy=False)\n                self.obj._mgr = df._mgr\n            else:\n                self.obj._mgr = self.obj._append(value)._mgr\n            self.obj._maybe_update_cacher(clear=True)\n\n    def _ensure_iterable_column_indexer(self, column_indexer):\n        \"\"\"\n        Ensure that our column indexer is something that can be iterated over.\n        \"\"\"\n        ilocs: Sequence[int] | np.ndarray\n        if is_integer(column_indexer):\n            ilocs = [column_indexer]\n        elif isinstance(column_indexer, slice):\n            ilocs = np.arange(len(self.obj.columns))[column_indexer]\n        elif isinstance(column_indexer, np.ndarray) and is_bool_dtype(\n            column_indexer.dtype\n        ):\n            ilocs = np.arange(len(column_indexer))[column_indexer]\n        else:\n            ilocs = column_indexer\n        return ilocs\n\n    def _align_series(self, indexer, ser: Series, multiindex_indexer: bool = False):\n        \"\"\"\n        Parameters\n        ----------\n        indexer : tuple, slice, scalar\n            Indexer used to get the locations that will be set to `ser`.\n        ser : pd.Series\n            Values to assign to the locations specified by `indexer`.\n        multiindex_indexer : bool, optional\n            Defaults to False. Should be set to True if `indexer` was from\n            a `pd.MultiIndex`, to avoid unnecessary broadcasting.\n\n        Returns\n        -------\n        `np.array` of `ser` broadcast to the appropriate shape for assignment\n        to the locations selected by `indexer`\n        \"\"\"\n        if isinstance(indexer, (slice, np.ndarray, list, Index)):\n            indexer = (indexer,)\n\n        if isinstance(indexer, tuple):\n            # flatten np.ndarray indexers\n            def ravel(i):\n                return i.ravel() if isinstance(i, np.ndarray) else i\n\n            indexer = tuple(map(ravel, indexer))\n\n            aligners = [not com.is_null_slice(idx) for idx in indexer]\n            sum_aligners = sum(aligners)\n            single_aligner = sum_aligners == 1\n            is_frame = self.ndim == 2\n            obj = self.obj\n\n            # are we a single alignable value on a non-primary\n            # dim (e.g. panel: 1,2, or frame: 0) ?\n            # hence need to align to a single axis dimension\n            # rather that find all valid dims\n\n            # frame\n            if is_frame:\n                single_aligner = single_aligner and aligners[0]\n\n            # we have a frame, with multiple indexers on both axes; and a\n            # series, so need to broadcast (see GH5206)\n            if sum_aligners == self.ndim and all(is_sequence(_) for _ in indexer):\n                ser_values = ser.reindex(obj.axes[0][indexer[0]], copy=True)._values\n\n                # single indexer\n                if len(indexer) > 1 and not multiindex_indexer:\n                    len_indexer = len(indexer[1])\n                    ser_values = (\n                        np.tile(ser_values, len_indexer).reshape(len_indexer, -1).T\n                    )\n\n                return ser_values\n\n            for i, idx in enumerate(indexer):\n                ax = obj.axes[i]\n\n                # multiple aligners (or null slices)\n                if is_sequence(idx) or isinstance(idx, slice):\n                    if single_aligner and com.is_null_slice(idx):\n                        continue\n                    new_ix = ax[idx]\n                    if not is_list_like_indexer(new_ix):\n                        new_ix = Index([new_ix])\n                    else:\n                        new_ix = Index(new_ix)\n                    if ser.index.equals(new_ix) or not len(new_ix):\n                        return ser._values.copy()\n\n                    return ser.reindex(new_ix)._values\n\n                # 2 dims\n                elif single_aligner:\n                    # reindex along index\n                    ax = self.obj.axes[1]\n                    if ser.index.equals(ax) or not len(ax):\n                        return ser._values.copy()\n                    return ser.reindex(ax)._values\n\n        elif is_integer(indexer) and self.ndim == 1:\n            if is_object_dtype(self.obj):\n                return ser\n            ax = self.obj._get_axis(0)\n\n            if ser.index.equals(ax):\n                return ser._values.copy()\n\n            return ser.reindex(ax)._values[indexer]\n\n        elif is_integer(indexer):\n            ax = self.obj._get_axis(1)\n\n            if ser.index.equals(ax):\n                return ser._values.copy()\n\n            return ser.reindex(ax)._values\n\n        raise ValueError(\"Incompatible indexer with Series\")\n\n    def _align_frame(self, indexer, df: DataFrame) -> DataFrame:\n        is_frame = self.ndim == 2\n\n        if isinstance(indexer, tuple):\n            idx, cols = None, None\n            sindexers = []\n            for i, ix in enumerate(indexer):\n                ax = self.obj.axes[i]\n                if is_sequence(ix) or isinstance(ix, slice):\n                    if isinstance(ix, np.ndarray):\n                        ix = ix.ravel()\n                    if idx is None:\n                        idx = ax[ix]\n                    elif cols is None:\n                        cols = ax[ix]\n                    else:\n                        break\n                else:\n                    sindexers.append(i)\n\n            if idx is not None and cols is not None:\n                if df.index.equals(idx) and df.columns.equals(cols):\n                    val = df.copy()\n                else:\n                    val = df.reindex(idx, columns=cols)\n                return val\n\n        elif (isinstance(indexer, slice) or is_list_like_indexer(indexer)) and is_frame:\n            ax = self.obj.index[indexer]\n            if df.index.equals(ax):\n                val = df.copy()\n            else:\n                # we have a multi-index and are trying to align\n                # with a particular, level GH3738\n                if (\n                    isinstance(ax, MultiIndex)\n                    and isinstance(df.index, MultiIndex)\n                    and ax.nlevels != df.index.nlevels\n                ):\n                    raise TypeError(\n                        \"cannot align on a multi-index with out \"\n                        \"specifying the join levels\"\n                    )\n\n                val = df.reindex(index=ax)\n            return val\n\n        raise ValueError(\"Incompatible indexer with DataFrame\")\n\n\nclass _ScalarAccessIndexer(NDFrameIndexerBase):\n    \"\"\"\n    Access scalars quickly.\n    \"\"\"\n\n    # sub-classes need to set _takeable\n    _takeable: bool\n\n    def _convert_key(self, key):\n        raise AbstractMethodError(self)\n\n    def __getitem__(self, key):\n        if not isinstance(key, tuple):\n            # we could have a convertible item here (e.g. Timestamp)\n            if not is_list_like_indexer(key):\n                key = (key,)\n            else:\n                raise ValueError(\"Invalid call for scalar access (getting)!\")\n\n        key = self._convert_key(key)\n        return self.obj._get_value(*key, takeable=self._takeable)\n\n    def __setitem__(self, key, value) -> None:\n        if isinstance(key, tuple):\n            key = tuple(com.apply_if_callable(x, self.obj) for x in key)\n        else:\n            # scalar callable may return tuple\n            key = com.apply_if_callable(key, self.obj)\n\n        if not isinstance(key, tuple):\n            key = _tuplify(self.ndim, key)\n        key = list(self._convert_key(key))\n        if len(key) != self.ndim:\n            raise ValueError(\"Not enough indexers for scalar access (setting)!\")\n\n        self.obj._set_value(*key, value=value, takeable=self._takeable)\n\n\n@doc(IndexingMixin.at)\nclass _AtIndexer(_ScalarAccessIndexer):\n    _takeable = False\n\n    def _convert_key(self, key):\n        \"\"\"\n        Require they keys to be the same type as the index. (so we don't\n        fallback)\n        \"\"\"\n        # GH 26989\n        # For series, unpacking key needs to result in the label.\n        # This is already the case for len(key) == 1; e.g. (1,)\n        if self.ndim == 1 and len(key) > 1:\n            key = (key,)\n\n        return key\n\n    @property\n    def _axes_are_unique(self) -> bool:\n        # Only relevant for self.ndim == 2\n        assert self.ndim == 2\n        return self.obj.index.is_unique and self.obj.columns.is_unique\n\n    def __getitem__(self, key):\n        if self.ndim == 2 and not self._axes_are_unique:\n            # GH#33041 fall back to .loc\n            if not isinstance(key, tuple) or not all(is_scalar(x) for x in key):\n                raise ValueError(\"Invalid call for scalar access (getting)!\")\n            return self.obj.loc[key]\n\n        return super().__getitem__(key)\n\n    def __setitem__(self, key, value):\n        if self.ndim == 2 and not self._axes_are_unique:\n            # GH#33041 fall back to .loc\n            if not isinstance(key, tuple) or not all(is_scalar(x) for x in key):\n                raise ValueError(\"Invalid call for scalar access (setting)!\")\n\n            self.obj.loc[key] = value\n            return\n\n        return super().__setitem__(key, value)\n\n\n@doc(IndexingMixin.iat)\nclass _iAtIndexer(_ScalarAccessIndexer):\n    _takeable = True\n\n    def _convert_key(self, key):\n        \"\"\"\n        Require integer args. (and convert to label arguments)\n        \"\"\"\n        for i in key:\n            if not is_integer(i):\n                raise ValueError(\"iAt based indexing can only have integer indexers\")\n        return key\n\n\ndef _tuplify(ndim: int, loc: Hashable) -> tuple[Hashable | slice, ...]:\n    \"\"\"\n    Given an indexer for the first dimension, create an equivalent tuple\n    for indexing over all dimensions.\n\n    Parameters\n    ----------\n    ndim : int\n    loc : object\n\n    Returns\n    -------\n    tuple\n    \"\"\"\n    _tup: list[Hashable | slice]\n    _tup = [slice(None, None) for _ in range(ndim)]\n    _tup[0] = loc\n    return tuple(_tup)\n\n\ndef _tupleize_axis_indexer(ndim: int, axis: AxisInt, key) -> tuple:\n    \"\"\"\n    If we have an axis, adapt the given key to be axis-independent.\n    \"\"\"\n    new_key = [slice(None)] * ndim\n    new_key[axis] = key\n    return tuple(new_key)\n\n\ndef check_bool_indexer(index: Index, key) -> np.ndarray:\n    \"\"\"\n    Check if key is a valid boolean indexer for an object with such index and\n    perform reindexing or conversion if needed.\n\n    This function assumes that is_bool_indexer(key) == True.\n\n    Parameters\n    ----------\n    index : Index\n        Index of the object on which the indexing is done.\n    key : list-like\n        Boolean indexer to check.\n\n    Returns\n    -------\n    np.array\n        Resulting key.\n\n    Raises\n    ------\n    IndexError\n        If the key does not have the same length as index.\n    IndexingError\n        If the index of the key is unalignable to index.\n    \"\"\"\n    result = key\n    if isinstance(key, ABCSeries) and not key.index.equals(index):\n        indexer = result.index.get_indexer_for(index)\n        if -1 in indexer:\n            raise IndexingError(\n                \"Unalignable boolean Series provided as \"\n                \"indexer (index of the boolean Series and of \"\n                \"the indexed object do not match).\"\n            )\n\n        result = result.take(indexer)\n\n        # fall through for boolean\n        if not is_extension_array_dtype(result.dtype):\n            return result.astype(bool)._values\n\n    if is_object_dtype(key):\n        # key might be object-dtype bool, check_array_indexer needs bool array\n        result = np.asarray(result, dtype=bool)\n    elif not is_array_like(result):\n        # GH 33924\n        # key may contain nan elements, check_array_indexer needs bool array\n        result = pd_array(result, dtype=bool)\n    return check_array_indexer(index, result)\n\n\ndef convert_missing_indexer(indexer):\n    \"\"\"\n    Reverse convert a missing indexer, which is a dict\n    return the scalar indexer and a boolean indicating if we converted\n    \"\"\"\n    if isinstance(indexer, dict):\n        # a missing key (but not a tuple indexer)\n        indexer = indexer[\"key\"]\n\n        if isinstance(indexer, bool):\n            raise KeyError(\"cannot use a single bool to index into setitem\")\n        return indexer, True\n\n    return indexer, False\n\n\ndef convert_from_missing_indexer_tuple(indexer, axes):\n    \"\"\"\n    Create a filtered indexer that doesn't have any missing indexers.\n    \"\"\"\n\n    def get_indexer(_i, _idx):\n        return axes[_i].get_loc(_idx[\"key\"]) if isinstance(_idx, dict) else _idx\n\n    return tuple(get_indexer(_i, _idx) for _i, _idx in enumerate(indexer))\n\n\ndef maybe_convert_ix(*args):\n    \"\"\"\n    We likely want to take the cross-product.\n    \"\"\"\n    for arg in args:\n        if not isinstance(arg, (np.ndarray, list, ABCSeries, Index)):\n            return args\n    return np.ix_(*args)\n\n\ndef is_nested_tuple(tup, labels) -> bool:\n    \"\"\"\n    Returns\n    -------\n    bool\n    \"\"\"\n    # check for a compatible nested tuple and multiindexes among the axes\n    if not isinstance(tup, tuple):\n        return False\n\n    for k in tup:\n        if is_list_like(k) or isinstance(k, slice):\n            return isinstance(labels, MultiIndex)\n\n    return False\n\n\ndef is_label_like(key) -> bool:\n    \"\"\"\n    Returns\n    -------\n    bool\n    \"\"\"\n    # select a label or row\n    return (\n        not isinstance(key, slice)\n        and not is_list_like_indexer(key)\n        and key is not Ellipsis\n    )\n\n\ndef need_slice(obj: slice) -> bool:\n    \"\"\"\n    Returns\n    -------\n    bool\n    \"\"\"\n    return (\n        obj.start is not None\n        or obj.stop is not None\n        or (obj.step is not None and obj.step != 1)\n    )\n\n\ndef check_dict_or_set_indexers(key) -> None:\n    \"\"\"\n    Check if the indexer is or contains a dict or set, which is no longer allowed.\n    \"\"\"\n    if (\n        isinstance(key, set)\n        or isinstance(key, tuple)\n        and any(isinstance(x, set) for x in key)\n    ):\n        raise TypeError(\n            \"Passing a set as an indexer is not supported. Use a list instead.\"\n        )\n\n    if (\n        isinstance(key, dict)\n        or isinstance(key, tuple)\n        and any(isinstance(x, dict) for x in key)\n    ):\n        raise TypeError(\n            \"Passing a dict as an indexer is not supported. Use a list instead.\"\n        )\n"
    },
    {
      "filename": "pandas/core/interchange/utils.py",
      "content": "\"\"\"\nUtility functions and objects for implementing the interchange API.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nimport typing\n\nimport numpy as np\n\nimport pandas as pd\nfrom pandas.api.types import is_datetime64_dtype\n\nif typing.TYPE_CHECKING:\n    from pandas._typing import DtypeObj\n\n\nclass ArrowCTypes:\n    \"\"\"\n    Enum for Apache Arrow C type format strings.\n\n    The Arrow C data interface:\n    https://arrow.apache.org/docs/format/CDataInterface.html#data-type-description-format-strings\n    \"\"\"\n\n    NULL = \"n\"\n    BOOL = \"b\"\n    INT8 = \"c\"\n    UINT8 = \"C\"\n    INT16 = \"s\"\n    UINT16 = \"S\"\n    INT32 = \"i\"\n    UINT32 = \"I\"\n    INT64 = \"l\"\n    UINT64 = \"L\"\n    FLOAT16 = \"e\"\n    FLOAT32 = \"f\"\n    FLOAT64 = \"g\"\n    STRING = \"u\"  # utf-8\n    DATE32 = \"tdD\"\n    DATE64 = \"tdm\"\n    # Resoulution:\n    #   - seconds -> 's'\n    #   - milliseconds -> 'm'\n    #   - microseconds -> 'u'\n    #   - nanoseconds -> 'n'\n    TIMESTAMP = \"ts{resolution}:{tz}\"\n    TIME = \"tt{resolution}\"\n\n\nclass Endianness:\n    \"\"\"Enum indicating the byte-order of a data-type.\"\"\"\n\n    LITTLE = \"<\"\n    BIG = \">\"\n    NATIVE = \"=\"\n    NA = \"|\"\n\n\ndef dtype_to_arrow_c_fmt(dtype: DtypeObj) -> str:\n    \"\"\"\n    Represent pandas `dtype` as a format string in Apache Arrow C notation.\n\n    Parameters\n    ----------\n    dtype : np.dtype\n        Datatype of pandas DataFrame to represent.\n\n    Returns\n    -------\n    str\n        Format string in Apache Arrow C notation of the given `dtype`.\n    \"\"\"\n    if isinstance(dtype, pd.CategoricalDtype):\n        return ArrowCTypes.INT64\n    elif dtype == np.dtype(\"O\"):\n        return ArrowCTypes.STRING\n\n    format_str = getattr(ArrowCTypes, dtype.name.upper(), None)\n    if format_str is not None:\n        return format_str\n\n    if is_datetime64_dtype(dtype):\n        # Selecting the first char of resolution string:\n        # dtype.str -> '<M8[ns]'\n        resolution = re.findall(r\"\\[(.*)\\]\", typing.cast(np.dtype, dtype).str)[0][:1]\n        return ArrowCTypes.TIMESTAMP.format(resolution=resolution, tz=\"\")\n\n    raise NotImplementedError(\n        f\"Conversion of {dtype} to Arrow C format string is not implemented.\"\n    )\n"
    },
    {
      "filename": "pandas/core/sample.py",
      "content": "\"\"\"\nModule containing utilities for NDFrame.sample() and .GroupBy.sample()\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\n\nfrom pandas._libs import lib\n\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame,\n    ABCSeries,\n)\n\nif TYPE_CHECKING:\n    from pandas._typing import AxisInt\n\n    from pandas.core.generic import NDFrame\n\n\ndef preprocess_weights(obj: NDFrame, weights, axis: AxisInt) -> np.ndarray:\n    \"\"\"\n    Process and validate the `weights` argument to `NDFrame.sample` and\n    `.GroupBy.sample`.\n\n    Returns `weights` as an ndarray[np.float64], validated except for normalizing\n    weights (because that must be done groupwise in groupby sampling).\n    \"\"\"\n    # If a series, align with frame\n    if isinstance(weights, ABCSeries):\n        weights = weights.reindex(obj.axes[axis])\n\n    # Strings acceptable if a dataframe and axis = 0\n    if isinstance(weights, str):\n        if isinstance(obj, ABCDataFrame):\n            if axis == 0:\n                try:\n                    weights = obj[weights]\n                except KeyError as err:\n                    raise KeyError(\n                        \"String passed to weights not a valid column\"\n                    ) from err\n            else:\n                raise ValueError(\n                    \"Strings can only be passed to \"\n                    \"weights when sampling from rows on \"\n                    \"a DataFrame\"\n                )\n        else:\n            raise ValueError(\n                \"Strings cannot be passed as weights when sampling from a Series.\"\n            )\n\n    if isinstance(obj, ABCSeries):\n        func = obj._constructor\n    else:\n        func = obj._constructor_sliced\n\n    weights = func(weights, dtype=\"float64\")._values\n\n    if len(weights) != obj.shape[axis]:\n        raise ValueError(\"Weights and axis to be sampled must be of same length\")\n\n    if lib.has_infs(weights):\n        raise ValueError(\"weight vector may not include `inf` values\")\n\n    if (weights < 0).any():\n        raise ValueError(\"weight vector many not include negative values\")\n\n    missing = np.isnan(weights)\n    if missing.any():\n        # Don't modify weights in place\n        weights = weights.copy()\n        weights[missing] = 0\n    return weights\n\n\ndef process_sampling_size(\n    n: int | None, frac: float | None, replace: bool\n) -> int | None:\n    \"\"\"\n    Process and validate the `n` and `frac` arguments to `NDFrame.sample` and\n    `.GroupBy.sample`.\n\n    Returns None if `frac` should be used (variable sampling sizes), otherwise returns\n    the constant sampling size.\n    \"\"\"\n    # If no frac or n, default to n=1.\n    if n is None and frac is None:\n        n = 1\n    elif n is not None and frac is not None:\n        raise ValueError(\"Please enter a value for `frac` OR `n`, not both\")\n    elif n is not None:\n        if n < 0:\n            raise ValueError(\n                \"A negative number of rows requested. Please provide `n` >= 0.\"\n            )\n        if n % 1 != 0:\n            raise ValueError(\"Only integers accepted as `n` values\")\n    else:\n        assert frac is not None  # for mypy\n        if frac > 1 and not replace:\n            raise ValueError(\n                \"Replace has to be set to `True` when \"\n                \"upsampling the population `frac` > 1.\"\n            )\n        if frac < 0:\n            raise ValueError(\n                \"A negative number of rows requested. Please provide `frac` >= 0.\"\n            )\n\n    return n\n\n\ndef sample(\n    obj_len: int,\n    size: int,\n    replace: bool,\n    weights: np.ndarray | None,\n    random_state: np.random.RandomState | np.random.Generator,\n) -> np.ndarray:\n    \"\"\"\n    Randomly sample `size` indices in `np.arange(obj_len)`\n\n    Parameters\n    ----------\n    obj_len : int\n        The length of the indices being considered\n    size : int\n        The number of values to choose\n    replace : bool\n        Allow or disallow sampling of the same row more than once.\n    weights : np.ndarray[np.float64] or None\n        If None, equal probability weighting, otherwise weights according\n        to the vector normalized\n    random_state: np.random.RandomState or np.random.Generator\n        State used for the random sampling\n\n    Returns\n    -------\n    np.ndarray[np.intp]\n    \"\"\"\n    if weights is not None:\n        weight_sum = weights.sum()\n        if weight_sum != 0:\n            weights = weights / weight_sum\n        else:\n            raise ValueError(\"Invalid weights: weights sum to zero\")\n\n    return random_state.choice(obj_len, size=size, replace=replace, p=weights).astype(\n        np.intp, copy=False\n    )\n"
    }
  ]
}