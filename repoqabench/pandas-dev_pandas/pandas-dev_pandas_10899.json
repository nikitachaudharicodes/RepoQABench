{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "10899",
  "issue_description": "# read_sql should accept a sql_params parameter\n\nHello,\n\nI wonder if current use of `read_sql` couldn't lead to SQL injection.\n\nI read in https://docs.python.org/2/library/sqlite3.html\n\n```\n# Never do this -- insecure!\nsymbol = 'RHAT'\nc.execute(\"SELECT * FROM stocks WHERE symbol = '%s'\" % symbol)\n\n# Do this instead\nt = ('RHAT',)\nc.execute('SELECT * FROM stocks WHERE symbol=?', t)\nprint c.fetchone()\n\n# Larger example that inserts many records at a time\npurchases = [('2006-03-28', 'BUY', 'IBM', 1000, 45.00),\n            ('2006-04-05', 'BUY', 'MSFT', 1000, 72.00),\n            ('2006-04-06', 'SELL', 'IBM', 500, 53.00),\n           ]\nc.executemany('INSERT INTO stocks VALUES (?,?,?,?,?)', purchases)\n```\n\nMost of people will use\n\n```\n\"SELECT * FROM stocks WHERE symbol = '%s'\" % symbol\n```\n\n(or `.format(...)`)\n\nwith `read_sql`\n\nif `symbol` is an unsafe input it could lead some problems\n\nhttp://xkcd.com/327/\n\nIs it safe to do it here ?\n\nKind regards\n",
  "issue_comments": [
    {
      "id": 134370199,
      "user": "jorisvandenbossche",
      "body": "`read_sql` has a `params` keyword: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html#pandas.read_sql\n\nIs it that what you are looking for?\n"
    },
    {
      "id": 134376709,
      "user": "s-celles",
      "body": "That's probably the parameter to use\n\nbut I think we should warm more in doc\n\n```\nimport pandas as pd\nfrom sqlalchemy import create_engine\ndb_uri = 'mysql+mysqlconnector://root:root@localhost:3306/meteo'\nengine = create_engine(db_uri)\n#query = \"SELECT * FROM data where Ta>35\" # no problem\n# you need to create a 'test_table' table which will be dropped\ntemp = \"35; DROP TABLE test_table;\" \nquery = \"SELECT * FROM data where Ta>%s\" % temp\npd.read_sql(query, engine)\n```\n\nraises\n\n```\nInterfaceError: (mysql.connector.errors.InterfaceError) Use multi=True when executing multiple statements [SQL: 'SELECT * FROM data where Ta>35; DROP TABLE test_table;']\n```\n\nbut `test_table` is dropped\n\n```\npd.read_sql(query, engine)\n```\n\nagain raises\n\n```\nProgrammingError: (mysql.connector.errors.ProgrammingError) 1051 (42S02): Unknown table 'meteo.test'\n```\n"
    },
    {
      "id": 134377340,
      "user": "s-celles",
      "body": "I think doc should (at least) provide a SQL query with a `SELECT` example and parameters.\n\nMaybe after http://pandas-docs.github.io/pandas-docs-travis/io.html#querying\n\n```\nOf course, you can specify a more “complex” query.\n\nIn [425]: pd.read_sql_query(\"SELECT id, Col_1, Col_2 FROM data WHERE id = 42;\", engine)\nOut[425]: \n   id Col_1  Col_2\n0  42     Y  -12.5\n```\n\nand before chunks\n\nI wonder if there isn't a way to disable multiple statements.\n"
    },
    {
      "id": 137177153,
      "user": "s-celles",
      "body": "I try this:\n\n```\nimport sqlalchemy\nimport pandas as pd\nimport numpy as np\ndb_uri = 'sqlite:///test.db'\nengine = sqlalchemy.create_engine(db_uri)\ndf = pd.DataFrame(np.random.randn(4,3), columns=['a','b','c'])\ndf.to_sql(\"df\", engine)\ndf.to_sql(\"test_table\", engine)\n#pd.read_sql(\"SELECT * from df where a>0;\", engine)\npd.read_sql(\"SELECT * from df where a>0; DROP TABLE test_table\", engine)\n```\n\nit raises\n\n```\nWarning: You can only execute one statement at a time.\n```\n\nand `test_table` is fortunately not dropped.\n\nSo I think the problem is on (mysqlconnector) driver side.\nhttp://bugs.mysql.com/bug.php?id=78308\n"
    },
    {
      "id": 137420684,
      "user": "jorisvandenbossche",
      "body": "@scls19fr Improvement to the docs are certainly welcome!\n\nThe warning you see above is actually a warning (feature) from sqlite3 itself (the have `executescript` to execute multiple statements).\n\nIt is always possible to misuse `read_sql`, just as you can misuse a plain `conn.execute`. This is a general issue with sql querying, so I don't think pandas should directly do anything about that. But of course, warning for that in the docs is easy to do!\n"
    },
    {
      "id": 137449811,
      "user": "s-celles",
      "body": "In fact the problem is on driver side because Pandas seems not to allow by default several statements.\nmysql-connector-python seems to execute several statements even if `multi` is not set to `True`\n\nI think that doc should be improve with the use of `sqlalchemy.text` and bind variables\n\nsee https://github.com/pydata/pandas/issues/10846\n"
    },
    {
      "id": 143406516,
      "user": "s-celles",
      "body": "This PR https://github.com/pydata/pandas/pull/10983/ shows that\nit is possible to do\n\n```\nname_text = sqlalchemy.text('select * from iris where name=:name')\niris_df = sql.read_sql(name_text, self.conn, params={'name': 'Iris-versicolor'})\n```\n\nmaybe doc http://pandas-docs.github.io/pandas-docs-travis/io.html#id4 should be improved accordingly\n\nAfter\n\n\"\nOf course, you can specify a more “complex” query.\n\n```\nIn [437]: pd.read_sql_query(\"SELECT id, Col_1, Col_2 FROM data WHERE id = 42;\", engine)\nOut[437]: \n   id Col_1  Col_2\n0  42     Y  -12.5\n```\n\n\"\n\nmaybe a query with parameters should be shown\n\n```\nfrom sqlalchemy import text\nmy_id = 42\nquery = text(\"SELECT id, Col_1, Col_2 FROM data WHERE id = :my_id;\")\npd.read_sql_query(query, engine, params={'my_id': my_id})\n```\n"
    },
    {
      "id": 143407808,
      "user": "s-celles",
      "body": "But what is odd, is that I can't do \n\n```\nmy_id = 42\ntable = 'data'\nquery = text(\"SELECT id, Col_1, Col_2 FROM :table WHERE id = :my_id;\")\npd.read_sql_query(query, engine, params={'my_id': my_id, 'table': table})\n```\n\ntable name can't be a parameter. Why ?\n\nMaybe @stephenpascoe can help\n"
    },
    {
      "id": 143677391,
      "user": "stephenpascoe",
      "body": "AFAIK this would depend on the SQL backend.  SQLAlchemy passes the unsubstituted SQL expression and the parameter dictionary to the underlying DB API to interpret.  It would be reasonable for a DB API not to support substitution of the table parameter as it could be an SQL injection vulnerability.  Also it wouldn't necessarily be supported by the DB's stored procedure system.\n\nPersonally, I've never tried this but a quick test with the raw sqlite3 db api shows a \"?\" in the table position gives a syntax error.\n"
    },
    {
      "id": 143686778,
      "user": "jorisvandenbossche",
      "body": "Parameter substitution is not possible for the table name AFAIK.\n\nThe thing is, in sql there is often a difference between string quoting, and variable quoting (see eg https://sqlite.org/lang_keywords.html the difference in quoting between string and identifier). So you are filling in a _string_, which is for sql something else as a variable name (in this case a table name).\n\n@scls19fr if you want to add that example to the docs, always welcome!\n"
    },
    {
      "id": 1859636794,
      "user": "erichxchen",
      "body": "Hello, I find this issue is quite interesting. I've created a PR to add some examples (including the bad examples and good examples) to the docs. Comments are appreciated."
    },
    {
      "id": 1859636973,
      "user": "erichxchen",
      "body": "take"
    },
    {
      "id": 2190410508,
      "user": "tdy",
      "body": "Is this issue fully resolved by #56546? Or are there any remaining tasks?"
    }
  ],
  "text_context": "# read_sql should accept a sql_params parameter\n\nHello,\n\nI wonder if current use of `read_sql` couldn't lead to SQL injection.\n\nI read in https://docs.python.org/2/library/sqlite3.html\n\n```\n# Never do this -- insecure!\nsymbol = 'RHAT'\nc.execute(\"SELECT * FROM stocks WHERE symbol = '%s'\" % symbol)\n\n# Do this instead\nt = ('RHAT',)\nc.execute('SELECT * FROM stocks WHERE symbol=?', t)\nprint c.fetchone()\n\n# Larger example that inserts many records at a time\npurchases = [('2006-03-28', 'BUY', 'IBM', 1000, 45.00),\n            ('2006-04-05', 'BUY', 'MSFT', 1000, 72.00),\n            ('2006-04-06', 'SELL', 'IBM', 500, 53.00),\n           ]\nc.executemany('INSERT INTO stocks VALUES (?,?,?,?,?)', purchases)\n```\n\nMost of people will use\n\n```\n\"SELECT * FROM stocks WHERE symbol = '%s'\" % symbol\n```\n\n(or `.format(...)`)\n\nwith `read_sql`\n\nif `symbol` is an unsafe input it could lead some problems\n\nhttp://xkcd.com/327/\n\nIs it safe to do it here ?\n\nKind regards\n\n\n`read_sql` has a `params` keyword: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html#pandas.read_sql\n\nIs it that what you are looking for?\n\n\nThat's probably the parameter to use\n\nbut I think we should warm more in doc\n\n```\nimport pandas as pd\nfrom sqlalchemy import create_engine\ndb_uri = 'mysql+mysqlconnector://root:root@localhost:3306/meteo'\nengine = create_engine(db_uri)\n#query = \"SELECT * FROM data where Ta>35\" # no problem\n# you need to create a 'test_table' table which will be dropped\ntemp = \"35; DROP TABLE test_table;\" \nquery = \"SELECT * FROM data where Ta>%s\" % temp\npd.read_sql(query, engine)\n```\n\nraises\n\n```\nInterfaceError: (mysql.connector.errors.InterfaceError) Use multi=True when executing multiple statements [SQL: 'SELECT * FROM data where Ta>35; DROP TABLE test_table;']\n```\n\nbut `test_table` is dropped\n\n```\npd.read_sql(query, engine)\n```\n\nagain raises\n\n```\nProgrammingError: (mysql.connector.errors.ProgrammingError) 1051 (42S02): Unknown table 'meteo.test'\n```\n\n\nI think doc should (at least) provide a SQL query with a `SELECT` example and parameters.\n\nMaybe after http://pandas-docs.github.io/pandas-docs-travis/io.html#querying\n\n```\nOf course, you can specify a more “complex” query.\n\nIn [425]: pd.read_sql_query(\"SELECT id, Col_1, Col_2 FROM data WHERE id = 42;\", engine)\nOut[425]: \n   id Col_1  Col_2\n0  42     Y  -12.5\n```\n\nand before chunks\n\nI wonder if there isn't a way to disable multiple statements.\n\n\nI try this:\n\n```\nimport sqlalchemy\nimport pandas as pd\nimport numpy as np\ndb_uri = 'sqlite:///test.db'\nengine = sqlalchemy.create_engine(db_uri)\ndf = pd.DataFrame(np.random.randn(4,3), columns=['a','b','c'])\ndf.to_sql(\"df\", engine)\ndf.to_sql(\"test_table\", engine)\n#pd.read_sql(\"SELECT * from df where a>0;\", engine)\npd.read_sql(\"SELECT * from df where a>0; DROP TABLE test_table\", engine)\n```\n\nit raises\n\n```\nWarning: You can only execute one statement at a time.\n```\n\nand `test_table` is fortunately not dropped.\n\nSo I think the problem is on (mysqlconnector) driver side.\nhttp://bugs.mysql.com/bug.php?id=78308\n\n\n@scls19fr Improvement to the docs are certainly welcome!\n\nThe warning you see above is actually a warning (feature) from sqlite3 itself (the have `executescript` to execute multiple statements).\n\nIt is always possible to misuse `read_sql`, just as you can misuse a plain `conn.execute`. This is a general issue with sql querying, so I don't think pandas should directly do anything about that. But of course, warning for that in the docs is easy to do!\n\n\nIn fact the problem is on driver side because Pandas seems not to allow by default several statements.\nmysql-connector-python seems to execute several statements even if `multi` is not set to `True`\n\nI think that doc should be improve with the use of `sqlalchemy.text` and bind variables\n\nsee https://github.com/pydata/pandas/issues/10846\n\n\nThis PR https://github.com/pydata/pandas/pull/10983/ shows that\nit is possible to do\n\n```\nname_text = sqlalchemy.text('select * from iris where name=:name')\niris_df = sql.read_sql(name_text, self.conn, params={'name': 'Iris-versicolor'})\n```\n\nmaybe doc http://pandas-docs.github.io/pandas-docs-travis/io.html#id4 should be improved accordingly\n\nAfter\n\n\"\nOf course, you can specify a more “complex” query.\n\n```\nIn [437]: pd.read_sql_query(\"SELECT id, Col_1, Col_2 FROM data WHERE id = 42;\", engine)\nOut[437]: \n   id Col_1  Col_2\n0  42     Y  -12.5\n```\n\n\"\n\nmaybe a query with parameters should be shown\n\n```\nfrom sqlalchemy import text\nmy_id = 42\nquery = text(\"SELECT id, Col_1, Col_2 FROM data WHERE id = :my_id;\")\npd.read_sql_query(query, engine, params={'my_id': my_id})\n```\n\n\nBut what is odd, is that I can't do \n\n```\nmy_id = 42\ntable = 'data'\nquery = text(\"SELECT id, Col_1, Col_2 FROM :table WHERE id = :my_id;\")\npd.read_sql_query(query, engine, params={'my_id': my_id, 'table': table})\n```\n\ntable name can't be a parameter. Why ?\n\nMaybe @stephenpascoe can help\n\n\nAFAIK this would depend on the SQL backend.  SQLAlchemy passes the unsubstituted SQL expression and the parameter dictionary to the underlying DB API to interpret.  It would be reasonable for a DB API not to support substitution of the table parameter as it could be an SQL injection vulnerability.  Also it wouldn't necessarily be supported by the DB's stored procedure system.\n\nPersonally, I've never tried this but a quick test with the raw sqlite3 db api shows a \"?\" in the table position gives a syntax error.\n\n\nParameter substitution is not possible for the table name AFAIK.\n\nThe thing is, in sql there is often a difference between string quoting, and variable quoting (see eg https://sqlite.org/lang_keywords.html the difference in quoting between string and identifier). So you are filling in a _string_, which is for sql something else as a variable name (in this case a table name).\n\n@scls19fr if you want to add that example to the docs, always welcome!\n\n\nHello, I find this issue is quite interesting. I've created a PR to add some examples (including the bad examples and good examples) to the docs. Comments are appreciated.\n\ntake\n\nIs this issue fully resolved by #56546? Or are there any remaining tasks?",
  "pr_link": "https://github.com/pydata/pandas/pull/10983",
  "code_context": [
    {
      "filename": "pandas/io/sql.py",
      "content": "# -*- coding: utf-8 -*-\n\"\"\"\nCollection of query wrappers / abstractions to both facilitate data\nretrieval and to reduce dependency on DB-specific API.\n\"\"\"\n\nfrom __future__ import print_function, division\nfrom datetime import datetime, date\n\nimport warnings\nimport traceback\nimport re\nimport numpy as np\n\nimport pandas.lib as lib\nimport pandas.core.common as com\nfrom pandas.compat import lzip, map, zip, raise_with_traceback, string_types\nfrom pandas.core.api import DataFrame, Series\nfrom pandas.core.common import isnull\nfrom pandas.core.base import PandasObject\nfrom pandas.tseries.tools import to_datetime\nfrom pandas.util.decorators import Appender\n\nfrom contextlib import contextmanager\n\n\nclass SQLAlchemyRequired(ImportError):\n    pass\n\n\nclass DatabaseError(IOError):\n    pass\n\n\n#------------------------------------------------------------------------------\n#--- Helper functions\n\n_SQLALCHEMY_INSTALLED = None\n\n\ndef _is_sqlalchemy_connectable(con):\n    global _SQLALCHEMY_INSTALLED\n    if _SQLALCHEMY_INSTALLED is None:\n        try:\n            import sqlalchemy\n            _SQLALCHEMY_INSTALLED = True\n\n            from distutils.version import LooseVersion\n            ver = LooseVersion(sqlalchemy.__version__)\n            # For sqlalchemy versions < 0.8.2, the BIGINT type is recognized\n            # for a sqlite engine, which results in a warning when trying to\n            # read/write a DataFrame with int64 values. (GH7433)\n            if ver < '0.8.2':\n                from sqlalchemy import BigInteger\n                from sqlalchemy.ext.compiler import compiles\n\n                @compiles(BigInteger, 'sqlite')\n                def compile_big_int_sqlite(type_, compiler, **kw):\n                    return 'INTEGER'\n        except ImportError:\n            _SQLALCHEMY_INSTALLED = False\n\n    if _SQLALCHEMY_INSTALLED:\n        import sqlalchemy\n        return isinstance(con, sqlalchemy.engine.Connectable)\n    else:\n        return False\n\n\ndef _convert_params(sql, params):\n    \"\"\"convert sql and params args to DBAPI2.0 compliant format\"\"\"\n    args = [sql]\n    if params is not None:\n        if hasattr(params, 'keys'):  # test if params is a mapping\n            args += [params]\n        else:\n            args += [list(params)]\n    return args\n\n\ndef _handle_date_column(col, format=None):\n    if isinstance(format, dict):\n        return to_datetime(col, errors='ignore', **format)\n    else:\n        if format in ['D', 's', 'ms', 'us', 'ns']:\n            return to_datetime(col, errors='coerce', unit=format, utc=True)\n        elif (issubclass(col.dtype.type, np.floating)\n                or issubclass(col.dtype.type, np.integer)):\n            # parse dates as timestamp\n            format = 's' if format is None else format\n            return to_datetime(col, errors='coerce', unit=format, utc=True)\n        else:\n            return to_datetime(col, errors='coerce', format=format, utc=True)\n\n\ndef _parse_date_columns(data_frame, parse_dates):\n    \"\"\"\n    Force non-datetime columns to be read as such.\n    Supports both string formatted and integer timestamp columns\n    \"\"\"\n    # handle non-list entries for parse_dates gracefully\n    if parse_dates is True or parse_dates is None or parse_dates is False:\n        parse_dates = []\n\n    if not hasattr(parse_dates, '__iter__'):\n        parse_dates = [parse_dates]\n\n    for col_name in parse_dates:\n        df_col = data_frame[col_name]\n        try:\n            fmt = parse_dates[col_name]\n        except TypeError:\n            fmt = None\n        data_frame[col_name] = _handle_date_column(df_col, format=fmt)\n\n    return data_frame\n\n\ndef _wrap_result(data, columns, index_col=None, coerce_float=True,\n                 parse_dates=None):\n    \"\"\"Wrap result set of query in a DataFrame \"\"\"\n\n    frame = DataFrame.from_records(data, columns=columns,\n                                   coerce_float=coerce_float)\n\n    _parse_date_columns(frame, parse_dates)\n\n    if index_col is not None:\n        frame.set_index(index_col, inplace=True)\n\n    return frame\n\n\ndef execute(sql, con, cur=None, params=None):\n    \"\"\"\n    Execute the given SQL query using the provided connection object.\n\n    Parameters\n    ----------\n    sql : string\n        Query to be executed\n    con : SQLAlchemy connectable(engine/connection) or sqlite3 DBAPI2 connection\n        Using SQLAlchemy makes it possible to use any DB supported by that\n        library.\n        If a DBAPI2 object, only sqlite3 is supported.\n    cur : deprecated, cursor is obtained from connection, default: None\n    params : list or tuple, optional, default: None\n        List of parameters to pass to execute method.\n\n    Returns\n    -------\n    Results Iterable\n    \"\"\"\n    if cur is None:\n        pandas_sql = pandasSQL_builder(con)\n    else:\n        pandas_sql = pandasSQL_builder(cur, is_cursor=True)\n    args = _convert_params(sql, params)\n    return pandas_sql.execute(*args)\n\n\n#------------------------------------------------------------------------------\n#--- Deprecated tquery and uquery\n\ndef _safe_fetch(cur):\n    try:\n        result = cur.fetchall()\n        if not isinstance(result, list):\n            result = list(result)\n        return result\n    except Exception as e:  # pragma: no cover\n        excName = e.__class__.__name__\n        if excName == 'OperationalError':\n            return []\n\n\ndef tquery(sql, con=None, cur=None, retry=True):\n    \"\"\"\n    DEPRECATED. Returns list of tuples corresponding to each row in given sql\n    query.\n\n    If only one column selected, then plain list is returned.\n\n    To obtain the same result in the future, you can use the following:\n\n    >>> execute(sql, con, params).fetchall()\n\n    Parameters\n    ----------\n    sql: string\n        SQL query to be executed\n    con: DBAPI2 connection, default: None\n    cur: deprecated, cursor is obtained from connection, default: None\n    retry: boolean value to specify whether to retry after failure, default: True\n\n    Returns\n    -------\n    Results Iterable\n\n    \"\"\"\n    warnings.warn(\n        \"tquery is deprecated, and will be removed in future versions. \"\n        \"You can use ``execute(...).fetchall()`` instead.\",\n        FutureWarning, stacklevel=2)\n\n    cur = execute(sql, con, cur=cur)\n    result = _safe_fetch(cur)\n\n    if con is not None:\n        try:\n            cur.close()\n            con.commit()\n        except Exception as e:\n            excName = e.__class__.__name__\n            if excName == 'OperationalError':  # pragma: no cover\n                print('Failed to commit, may need to restart interpreter')\n            else:\n                raise\n\n            traceback.print_exc()\n            if retry:\n                return tquery(sql, con=con, retry=False)\n\n    if result and len(result[0]) == 1:\n        # python 3 compat\n        result = list(lzip(*result)[0])\n    elif result is None:  # pragma: no cover\n        result = []\n\n    return result\n\n\ndef uquery(sql, con=None, cur=None, retry=True, params=None):\n    \"\"\"\n    DEPRECATED. Does the same thing as tquery, but instead of returning\n    results, it returns the number of rows affected.  Good for update queries.\n\n    To obtain the same result in the future, you can use the following:\n\n    >>> execute(sql, con).rowcount\n\n    Parameters\n    ----------\n    sql: string\n        SQL query to be executed\n    con: DBAPI2 connection, default: None\n    cur: deprecated, cursor is obtained from connection, default: None\n    retry: boolean value to specify whether to retry after failure, default: True\n    params: list or tuple, optional, default: None\n        List of parameters to pass to execute method.\n\n    Returns\n    -------\n    Number of affected rows\n\n    \"\"\"\n    warnings.warn(\n        \"uquery is deprecated, and will be removed in future versions. \"\n        \"You can use ``execute(...).rowcount`` instead.\",\n        FutureWarning, stacklevel=2)\n\n    cur = execute(sql, con, cur=cur, params=params)\n\n    result = cur.rowcount\n    try:\n        con.commit()\n    except Exception as e:\n        excName = e.__class__.__name__\n        if excName != 'OperationalError':\n            raise\n\n        traceback.print_exc()\n        if retry:\n            print('Looks like your connection failed, reconnecting...')\n            return uquery(sql, con, retry=False)\n    return result\n\n\n#------------------------------------------------------------------------------\n#--- Read and write to DataFrames\n\ndef read_sql_table(table_name, con, schema=None, index_col=None,\n                   coerce_float=True, parse_dates=None, columns=None,\n                   chunksize=None):\n    \"\"\"Read SQL database table into a DataFrame.\n\n    Given a table name and an SQLAlchemy connectable, returns a DataFrame.\n    This function does not support DBAPI connections.\n\n    Parameters\n    ----------\n    table_name : string\n        Name of SQL table in database\n    con : SQLAlchemy connectable (or database string URI)\n        Sqlite DBAPI connection mode not supported\n    schema : string, default None\n        Name of SQL schema in database to query (if database flavor\n        supports this). If None, use default schema (default).\n    index_col : string, optional, default: None\n        Column to set as index\n    coerce_float : boolean, default True\n        Attempt to convert values to non-string, non-numeric objects (like\n        decimal.Decimal) to floating point. Can result in loss of Precision.\n    parse_dates : list or dict, default: None\n        - List of column names to parse as dates\n        - Dict of ``{column_name: format string}`` where format string is\n          strftime compatible in case of parsing string times or is one of\n          (D, s, ns, ms, us) in case of parsing integer timestamps\n        - Dict of ``{column_name: arg dict}``, where the arg dict corresponds\n          to the keyword arguments of :func:`pandas.to_datetime`\n          Especially useful with databases without native Datetime support,\n          such as SQLite\n    columns : list, default: None\n        List of column names to select from sql table\n    chunksize : int, default None\n        If specified, return an iterator where `chunksize` is the number of\n        rows to include in each chunk.\n\n    Returns\n    -------\n    DataFrame\n\n    Notes\n    -----\n    Any datetime values with time zone information will be converted to UTC\n\n    See also\n    --------\n    read_sql_query : Read SQL query into a DataFrame.\n    read_sql\n\n    \"\"\"\n\n    con = _engine_builder(con)\n    if not _is_sqlalchemy_connectable(con):\n        raise NotImplementedError(\"read_sql_table only supported for \"\n                                  \"SQLAlchemy connectable.\")\n    import sqlalchemy\n    from sqlalchemy.schema import MetaData\n    meta = MetaData(con, schema=schema)\n    try:\n        meta.reflect(only=[table_name], views=True)\n    except sqlalchemy.exc.InvalidRequestError:\n        raise ValueError(\"Table %s not found\" % table_name)\n\n    pandas_sql = SQLDatabase(con, meta=meta)\n    table = pandas_sql.read_table(\n        table_name, index_col=index_col, coerce_float=coerce_float,\n        parse_dates=parse_dates, columns=columns, chunksize=chunksize)\n\n    if table is not None:\n        return table\n    else:\n        raise ValueError(\"Table %s not found\" % table_name, con)\n\n\ndef read_sql_query(sql, con, index_col=None, coerce_float=True, params=None,\n                   parse_dates=None, chunksize=None):\n    \"\"\"Read SQL query into a DataFrame.\n\n    Returns a DataFrame corresponding to the result set of the query\n    string. Optionally provide an `index_col` parameter to use one of the\n    columns as the index, otherwise default integer index will be used.\n\n    Parameters\n    ----------\n    sql : string SQL query or SQLAlchemy Selectable (select or text object)\n        to be executed.\n    con : SQLAlchemy connectable(engine/connection) or database string URI \n        or sqlite3 DBAPI2 connection\n        Using SQLAlchemy makes it possible to use any DB supported by that\n        library.\n        If a DBAPI2 object, only sqlite3 is supported.\n    index_col : string, optional, default: None\n        Column name to use as index for the returned DataFrame object.\n    coerce_float : boolean, default True\n        Attempt to convert values to non-string, non-numeric objects (like\n        decimal.Decimal) to floating point, useful for SQL result sets\n    params : list, tuple or dict, optional, default: None\n        List of parameters to pass to execute method.  The syntax used\n        to pass parameters is database driver dependent. Check your\n        database driver documentation for which of the five syntax styles,\n        described in PEP 249's paramstyle, is supported.\n        Eg. for psycopg2, uses %(name)s so use params={'name' : 'value'}\n    parse_dates : list or dict, default: None\n        - List of column names to parse as dates\n        - Dict of ``{column_name: format string}`` where format string is\n          strftime compatible in case of parsing string times or is one of\n          (D, s, ns, ms, us) in case of parsing integer timestamps\n        - Dict of ``{column_name: arg dict}``, where the arg dict corresponds\n          to the keyword arguments of :func:`pandas.to_datetime`\n          Especially useful with databases without native Datetime support,\n          such as SQLite\n    chunksize : int, default None\n        If specified, return an iterator where `chunksize` is the number of\n        rows to include in each chunk.\n\n    Returns\n    -------\n    DataFrame\n\n    Notes\n    -----\n    Any datetime values with time zone information parsed via the `parse_dates`\n    parameter will be converted to UTC\n\n    See also\n    --------\n    read_sql_table : Read SQL database table into a DataFrame\n    read_sql\n\n    \"\"\"\n    pandas_sql = pandasSQL_builder(con)\n    return pandas_sql.read_query(\n        sql, index_col=index_col, params=params, coerce_float=coerce_float,\n        parse_dates=parse_dates, chunksize=chunksize)\n\n\ndef read_sql(sql, con, index_col=None, coerce_float=True, params=None,\n             parse_dates=None, columns=None, chunksize=None):\n    \"\"\"\n    Read SQL query or database table into a DataFrame.\n\n    Parameters\n    ----------\n    sql : string SQL query or SQLAlchemy Selectable (select or text object)\n        to be executed, or database table name.\n    con : SQLAlchemy connectable(engine/connection) or database string URI\n        or DBAPI2 connection (fallback mode)\n        Using SQLAlchemy makes it possible to use any DB supported by that\n        library.\n        If a DBAPI2 object, only sqlite3 is supported.\n    index_col : string, optional, default: None\n        column name to use as index for the returned DataFrame object.\n    coerce_float : boolean, default True\n        Attempt to convert values to non-string, non-numeric objects (like\n        decimal.Decimal) to floating point, useful for SQL result sets\n    params : list, tuple or dict, optional, default: None\n        List of parameters to pass to execute method.  The syntax used\n        to pass parameters is database driver dependent. Check your\n        database driver documentation for which of the five syntax styles,\n        described in PEP 249's paramstyle, is supported.\n        Eg. for psycopg2, uses %(name)s so use params={'name' : 'value'}\n    parse_dates : list or dict, default: None\n        - List of column names to parse as dates\n        - Dict of ``{column_name: format string}`` where format string is\n          strftime compatible in case of parsing string times or is one of\n          (D, s, ns, ms, us) in case of parsing integer timestamps\n        - Dict of ``{column_name: arg dict}``, where the arg dict corresponds\n          to the keyword arguments of :func:`pandas.to_datetime`\n          Especially useful with databases without native Datetime support,\n          such as SQLite\n    columns : list, default: None\n        List of column names to select from sql table (only used when reading\n        a table).\n    chunksize : int, default None\n        If specified, return an iterator where `chunksize` is the\n        number of rows to include in each chunk.\n\n    Returns\n    -------\n    DataFrame\n\n    Notes\n    -----\n    This function is a convenience wrapper around ``read_sql_table`` and\n    ``read_sql_query`` (and for backward compatibility) and will delegate\n    to the specific function depending on the provided input (database\n    table name or sql query).  The delegated function might have more specific\n    notes about their functionality not listed here.\n\n    See also\n    --------\n    read_sql_table : Read SQL database table into a DataFrame\n    read_sql_query : Read SQL query into a DataFrame\n\n    \"\"\"\n    pandas_sql = pandasSQL_builder(con)\n\n    if isinstance(pandas_sql, SQLiteDatabase):\n        return pandas_sql.read_query(\n            sql, index_col=index_col, params=params,\n            coerce_float=coerce_float, parse_dates=parse_dates,\n            chunksize=chunksize)\n\n    try:\n        _is_table_name = pandas_sql.has_table(sql)\n    except:\n        _is_table_name = False\n\n    if _is_table_name:\n        pandas_sql.meta.reflect(only=[sql])\n        return pandas_sql.read_table(\n            sql, index_col=index_col, coerce_float=coerce_float,\n            parse_dates=parse_dates, columns=columns, chunksize=chunksize)\n    else:\n        return pandas_sql.read_query(\n            sql, index_col=index_col, params=params,\n            coerce_float=coerce_float, parse_dates=parse_dates,\n            chunksize=chunksize)\n\n\ndef to_sql(frame, name, con, flavor='sqlite', schema=None, if_exists='fail',\n           index=True, index_label=None, chunksize=None, dtype=None):\n    \"\"\"\n    Write records stored in a DataFrame to a SQL database.\n\n    Parameters\n    ----------\n    frame : DataFrame\n    name : string\n        Name of SQL table\n    con : SQLAlchemy connectable(engine/connection) or database string URI\n        or sqlite3 DBAPI2 connection\n        Using SQLAlchemy makes it possible to use any DB supported by that\n        library.\n        If a DBAPI2 object, only sqlite3 is supported.\n    flavor : {'sqlite', 'mysql'}, default 'sqlite'\n        The flavor of SQL to use. Ignored when using SQLAlchemy connectable.\n        'mysql' is deprecated and will be removed in future versions, but it\n        will be further supported through SQLAlchemy connectables.\n    schema : string, default None\n        Name of SQL schema in database to write to (if database flavor\n        supports this). If None, use default schema (default).\n    if_exists : {'fail', 'replace', 'append'}, default 'fail'\n        - fail: If table exists, do nothing.\n        - replace: If table exists, drop it, recreate it, and insert data.\n        - append: If table exists, insert data. Create if does not exist.\n    index : boolean, default True\n        Write DataFrame index as a column\n    index_label : string or sequence, default None\n        Column label for index column(s). If None is given (default) and\n        `index` is True, then the index names are used.\n        A sequence should be given if the DataFrame uses MultiIndex.\n    chunksize : int, default None\n        If not None, then rows will be written in batches of this size at a\n        time.  If None, all rows will be written at once.\n    dtype : dict of column name to SQL type, default None\n        Optional specifying the datatype for columns. The SQL type should\n        be a SQLAlchemy type, or a string for sqlite3 fallback connection.\n\n    \"\"\"\n    if if_exists not in ('fail', 'replace', 'append'):\n        raise ValueError(\"'{0}' is not valid for if_exists\".format(if_exists))\n\n    pandas_sql = pandasSQL_builder(con, schema=schema, flavor=flavor)\n\n    if isinstance(frame, Series):\n        frame = frame.to_frame()\n    elif not isinstance(frame, DataFrame):\n        raise NotImplementedError(\"'frame' argument should be either a \"\n                                  \"Series or a DataFrame\")\n\n    pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index,\n                      index_label=index_label, schema=schema,\n                      chunksize=chunksize, dtype=dtype)\n\n\ndef has_table(table_name, con, flavor='sqlite', schema=None):\n    \"\"\"\n    Check if DataBase has named table.\n\n    Parameters\n    ----------\n    table_name: string\n        Name of SQL table\n    con: SQLAlchemy connectable(engine/connection) or sqlite3 DBAPI2 connection\n        Using SQLAlchemy makes it possible to use any DB supported by that\n        library.\n        If a DBAPI2 object, only sqlite3 is supported.\n    flavor: {'sqlite', 'mysql'}, default 'sqlite'\n        The flavor of SQL to use. Ignored when using SQLAlchemy connectable.\n        'mysql' is deprecated and will be removed in future versions, but it\n        will be further supported through SQLAlchemy connectables.\n    schema : string, default None\n        Name of SQL schema in database to write to (if database flavor supports\n        this). If None, use default schema (default).\n\n    Returns\n    -------\n    boolean\n    \"\"\"\n    pandas_sql = pandasSQL_builder(con, flavor=flavor, schema=schema)\n    return pandas_sql.has_table(table_name)\n\ntable_exists = has_table\n\n\n_MYSQL_WARNING = (\"The 'mysql' flavor with DBAPI connection is deprecated \"\n                  \"and will be removed in future versions. \"\n                  \"MySQL will be further supported with SQLAlchemy connectables.\")\n\n\ndef _engine_builder(con):\n    \"\"\"\n    Returns a SQLAlchemy engine from a URI (if con is a string)\n    else it just return con without modifying it\n    \"\"\"\n    if isinstance(con, string_types):\n        try:\n            import sqlalchemy\n            con = sqlalchemy.create_engine(con)\n            return con\n\n        except ImportError:\n            _SQLALCHEMY_INSTALLED = False\n\n    return con\n\ndef pandasSQL_builder(con, flavor=None, schema=None, meta=None,\n                      is_cursor=False):\n    \"\"\"\n    Convenience function to return the correct PandasSQL subclass based on the\n    provided parameters\n    \"\"\"\n    # When support for DBAPI connections is removed,\n    # is_cursor should not be necessary.\n    con = _engine_builder(con)\n    if _is_sqlalchemy_connectable(con):\n        return SQLDatabase(con, schema=schema, meta=meta)\n    else:\n        if flavor == 'mysql':\n            warnings.warn(_MYSQL_WARNING, FutureWarning, stacklevel=3)\n        return SQLiteDatabase(con, flavor, is_cursor=is_cursor)\n\n\nclass SQLTable(PandasObject):\n    \"\"\"\n    For mapping Pandas tables to SQL tables.\n    Uses fact that table is reflected by SQLAlchemy to\n    do better type convertions.\n    Also holds various flags needed to avoid having to\n    pass them between functions all the time.\n    \"\"\"\n    # TODO: support for multiIndex\n    def __init__(self, name, pandas_sql_engine, frame=None, index=True,\n                 if_exists='fail', prefix='pandas', index_label=None,\n                 schema=None, keys=None, dtype=None):\n        self.name = name\n        self.pd_sql = pandas_sql_engine\n        self.prefix = prefix\n        self.frame = frame\n        self.index = self._index_name(index, index_label)\n        self.schema = schema\n        self.if_exists = if_exists\n        self.keys = keys\n        self.dtype = dtype\n\n        if frame is not None:\n            # We want to initialize based on a dataframe\n            self.table = self._create_table_setup()\n        else:\n            # no data provided, read-only mode\n            self.table = self.pd_sql.get_table(self.name, self.schema)\n\n        if self.table is None:\n            raise ValueError(\"Could not init table '%s'\" % name)\n\n    def exists(self):\n        return self.pd_sql.has_table(self.name, self.schema)\n\n    def sql_schema(self):\n        from sqlalchemy.schema import CreateTable\n        return str(CreateTable(self.table).compile(self.pd_sql.connectable))\n\n    def _execute_create(self):\n        # Inserting table into database, add to MetaData object\n        self.table = self.table.tometadata(self.pd_sql.meta)\n        self.table.create()\n\n    def create(self):\n        if self.exists():\n            if self.if_exists == 'fail':\n                raise ValueError(\"Table '%s' already exists.\" % self.name)\n            elif self.if_exists == 'replace':\n                self.pd_sql.drop_table(self.name, self.schema)\n                self._execute_create()\n            elif self.if_exists == 'append':\n                pass\n            else:\n                raise ValueError(\n                    \"'{0}' is not valid for if_exists\".format(self.if_exists))\n        else:\n            self._execute_create()\n\n    def insert_statement(self):\n        return self.table.insert()\n\n    def insert_data(self):\n        if self.index is not None:\n            temp = self.frame.copy()\n            temp.index.names = self.index\n            try:\n                temp.reset_index(inplace=True)\n            except ValueError as err:\n                raise ValueError(\n                    \"duplicate name in index/columns: {0}\".format(err))\n        else:\n            temp = self.frame\n\n        column_names = list(map(str, temp.columns))\n        ncols = len(column_names)\n        data_list = [None] * ncols\n        blocks = temp._data.blocks\n\n        for i in range(len(blocks)):\n            b = blocks[i]\n            if b.is_datetime:\n                # convert to microsecond resolution so this yields\n                # datetime.datetime\n                d = b.values.astype('M8[us]').astype(object)\n            else:\n                d = np.array(b.get_values(), dtype=object)\n\n            # replace NaN with None\n            if b._can_hold_na:\n                mask = isnull(d)\n                d[mask] = None\n\n            for col_loc, col in zip(b.mgr_locs, d):\n                data_list[col_loc] = col\n\n        return column_names, data_list\n\n    def _execute_insert(self, conn, keys, data_iter):\n        data = [dict((k, v) for k, v in zip(keys, row)) for row in data_iter]\n        conn.execute(self.insert_statement(), data)\n\n    def insert(self, chunksize=None):\n        keys, data_list = self.insert_data()\n\n        nrows = len(self.frame)\n\n        if nrows == 0:\n            return\n\n        if chunksize is None:\n            chunksize = nrows\n        elif chunksize == 0:\n            raise ValueError('chunksize argument should be non-zero')\n\n        chunks = int(nrows / chunksize) + 1\n\n        with self.pd_sql.run_transaction() as conn:\n            for i in range(chunks):\n                start_i = i * chunksize\n                end_i = min((i + 1) * chunksize, nrows)\n                if start_i >= end_i:\n                    break\n\n                chunk_iter = zip(*[arr[start_i:end_i] for arr in data_list])\n                self._execute_insert(conn, keys, chunk_iter)\n\n    def _query_iterator(self, result, chunksize, columns, coerce_float=True,\n                        parse_dates=None):\n        \"\"\"Return generator through chunked result set\"\"\"\n\n        while True:\n            data = result.fetchmany(chunksize)\n            if not data:\n                break\n            else:\n                self.frame = DataFrame.from_records(\n                    data, columns=columns, coerce_float=coerce_float)\n\n                self._harmonize_columns(parse_dates=parse_dates)\n\n                if self.index is not None:\n                    self.frame.set_index(self.index, inplace=True)\n\n                yield self.frame\n\n    def read(self, coerce_float=True, parse_dates=None, columns=None,\n             chunksize=None):\n\n        if columns is not None and len(columns) > 0:\n            from sqlalchemy import select\n            cols = [self.table.c[n] for n in columns]\n            if self.index is not None:\n                [cols.insert(0, self.table.c[idx]) for idx in self.index[::-1]]\n            sql_select = select(cols)\n        else:\n            sql_select = self.table.select()\n\n        result = self.pd_sql.execute(sql_select)\n        column_names = result.keys()\n\n        if chunksize is not None:\n            return self._query_iterator(result, chunksize, column_names,\n                                        coerce_float=coerce_float,\n                                        parse_dates=parse_dates)\n        else:\n            data = result.fetchall()\n            self.frame = DataFrame.from_records(\n                data, columns=column_names, coerce_float=coerce_float)\n\n            self._harmonize_columns(parse_dates=parse_dates)\n\n            if self.index is not None:\n                self.frame.set_index(self.index, inplace=True)\n\n            return self.frame\n\n    def _index_name(self, index, index_label):\n        # for writing: index=True to include index in sql table\n        if index is True:\n            nlevels = self.frame.index.nlevels\n            # if index_label is specified, set this as index name(s)\n            if index_label is not None:\n                if not isinstance(index_label, list):\n                    index_label = [index_label]\n                if len(index_label) != nlevels:\n                    raise ValueError(\n                        \"Length of 'index_label' should match number of \"\n                        \"levels, which is {0}\".format(nlevels))\n                else:\n                    return index_label\n            # return the used column labels for the index columns\n            if (nlevels == 1 and 'index' not in self.frame.columns\n                    and self.frame.index.name is None):\n                return ['index']\n            else:\n                return [l if l is not None else \"level_{0}\".format(i)\n                        for i, l in enumerate(self.frame.index.names)]\n\n        # for reading: index=(list of) string to specify column to set as index\n        elif isinstance(index, string_types):\n            return [index]\n        elif isinstance(index, list):\n            return index\n        else:\n            return None\n\n    def _get_column_names_and_types(self, dtype_mapper):\n        column_names_and_types = []\n        if self.index is not None:\n            for i, idx_label in enumerate(self.index):\n                idx_type = dtype_mapper(\n                    self.frame.index.get_level_values(i))\n                column_names_and_types.append((idx_label, idx_type, True))\n\n        column_names_and_types += [\n            (str(self.frame.columns[i]),\n             dtype_mapper(self.frame.iloc[:, i]),\n             False)\n            for i in range(len(self.frame.columns))\n            ]\n\n        return column_names_and_types\n\n    def _create_table_setup(self):\n        from sqlalchemy import Table, Column, PrimaryKeyConstraint\n\n        column_names_and_types = \\\n            self._get_column_names_and_types(self._sqlalchemy_type)\n\n        columns = [Column(name, typ, index=is_index)\n                   for name, typ, is_index in column_names_and_types]\n\n        if self.keys is not None:\n            if not com.is_list_like(self.keys):\n                keys = [self.keys]\n            else:\n                keys = self.keys\n            pkc = PrimaryKeyConstraint(*keys, name=self.name + '_pk')\n            columns.append(pkc)\n\n        schema = self.schema or self.pd_sql.meta.schema\n\n        # At this point, attach to new metadata, only attach to self.meta\n        # once table is created.\n        from sqlalchemy.schema import MetaData\n        meta = MetaData(self.pd_sql, schema=schema)\n\n        return Table(self.name, meta, *columns, schema=schema)\n\n    def _harmonize_columns(self, parse_dates=None):\n        \"\"\"\n        Make the DataFrame's column types align with the SQL table\n        column types.\n        Need to work around limited NA value support. Floats are always\n        fine, ints must always be floats if there are Null values.\n        Booleans are hard because converting bool column with None replaces\n        all Nones with false. Therefore only convert bool if there are no\n        NA values.\n        Datetimes should already be converted to np.datetime64 if supported,\n        but here we also force conversion if required\n        \"\"\"\n        # handle non-list entries for parse_dates gracefully\n        if parse_dates is True or parse_dates is None or parse_dates is False:\n            parse_dates = []\n\n        if not hasattr(parse_dates, '__iter__'):\n            parse_dates = [parse_dates]\n\n        for sql_col in self.table.columns:\n            col_name = sql_col.name\n            try:\n                df_col = self.frame[col_name]\n                # the type the dataframe column should have\n                col_type = self._numpy_type(sql_col.type)\n\n                if col_type is datetime or col_type is date:\n                    if not issubclass(df_col.dtype.type, np.datetime64):\n                        self.frame[col_name] = _handle_date_column(df_col)\n\n                elif col_type is float:\n                    # floats support NA, can always convert!\n                    self.frame[col_name] = df_col.astype(col_type, copy=False)\n\n                elif len(df_col) == df_col.count():\n                    # No NA values, can convert ints and bools\n                    if col_type is np.dtype('int64') or col_type is bool:\n                        self.frame[col_name] = df_col.astype(col_type, copy=False)\n\n                # Handle date parsing\n                if col_name in parse_dates:\n                    try:\n                        fmt = parse_dates[col_name]\n                    except TypeError:\n                        fmt = None\n                    self.frame[col_name] = _handle_date_column(\n                        df_col, format=fmt)\n\n            except KeyError:\n                pass  # this column not in results\n\n    def _get_notnull_col_dtype(self, col):\n        \"\"\"\n        Infer datatype of the Series col.  In case the dtype of col is 'object'\n        and it contains NA values, this infers the datatype of the not-NA\n        values.  Needed for inserting typed data containing NULLs, GH8778.\n        \"\"\"\n        col_for_inference = col\n        if col.dtype == 'object':\n            notnulldata = col[~isnull(col)]\n            if len(notnulldata):\n                col_for_inference = notnulldata\n\n        return lib.infer_dtype(col_for_inference)\n\n    def _sqlalchemy_type(self, col):\n\n        dtype = self.dtype or {}\n        if col.name in dtype:\n            return self.dtype[col.name]\n\n        col_type = self._get_notnull_col_dtype(col)\n\n        from sqlalchemy.types import (BigInteger, Integer, Float, Text, Boolean,\n            DateTime, Date, Time)\n\n        if col_type == 'datetime64' or col_type == 'datetime':\n            try:\n                tz = col.tzinfo\n                return DateTime(timezone=True)\n            except:\n                return DateTime\n        if col_type == 'timedelta64':\n            warnings.warn(\"the 'timedelta' type is not supported, and will be \"\n                          \"written as integer values (ns frequency) to the \"\n                          \"database.\", UserWarning, stacklevel=8)\n            return BigInteger\n        elif col_type == 'floating':\n            if col.dtype == 'float32':\n                return Float(precision=23)\n            else:\n                return Float(precision=53)\n        elif col_type == 'integer':\n            if col.dtype == 'int32':\n                return Integer\n            else:\n                return BigInteger\n        elif col_type == 'boolean':\n            return Boolean\n        elif col_type == 'date':\n            return Date\n        elif col_type == 'time':\n            return Time\n        elif col_type == 'complex':\n            raise ValueError('Complex datatypes not supported')\n\n        return Text\n\n    def _numpy_type(self, sqltype):\n        from sqlalchemy.types import Integer, Float, Boolean, DateTime, Date\n\n        if isinstance(sqltype, Float):\n            return float\n        if isinstance(sqltype, Integer):\n            # TODO: Refine integer size.\n            return np.dtype('int64')\n        if isinstance(sqltype, DateTime):\n            # Caution: np.datetime64 is also a subclass of np.number.\n            return datetime\n        if isinstance(sqltype, Date):\n            return date\n        if isinstance(sqltype, Boolean):\n            return bool\n        return object\n\n\nclass PandasSQL(PandasObject):\n    \"\"\"\n    Subclasses Should define read_sql and to_sql\n    \"\"\"\n\n    def read_sql(self, *args, **kwargs):\n        raise ValueError(\"PandasSQL must be created with an SQLAlchemy connectable\"\n                         \" or connection+sql flavor\")\n\n    def to_sql(self, *args, **kwargs):\n        raise ValueError(\"PandasSQL must be created with an SQLAlchemy connectable\"\n                         \" or connection+sql flavor\")\n\n\nclass SQLDatabase(PandasSQL):\n    \"\"\"\n    This class enables convertion between DataFrame and SQL databases\n    using SQLAlchemy to handle DataBase abstraction\n\n    Parameters\n    ----------\n    engine : SQLAlchemy connectable\n        Connectable to connect with the database. Using SQLAlchemy makes it\n        possible to use any DB supported by that library.\n    schema : string, default None\n        Name of SQL schema in database to write to (if database flavor\n        supports this). If None, use default schema (default).\n    meta : SQLAlchemy MetaData object, default None\n        If provided, this MetaData object is used instead of a newly\n        created. This allows to specify database flavor specific\n        arguments in the MetaData object.\n\n    \"\"\"\n\n    def __init__(self, engine, schema=None, meta=None):\n        self.connectable = engine\n        if not meta:\n            from sqlalchemy.schema import MetaData\n            meta = MetaData(self.connectable, schema=schema)\n\n        self.meta = meta\n\n    @contextmanager\n    def run_transaction(self):\n        with self.connectable.begin() as tx:\n            if hasattr(tx, 'execute'):\n                yield tx\n            else:\n                yield self.connectable\n\n    def execute(self, *args, **kwargs):\n        \"\"\"Simple passthrough to SQLAlchemy connectable\"\"\"\n        return self.connectable.execute(*args, **kwargs)\n\n    def read_table(self, table_name, index_col=None, coerce_float=True,\n                   parse_dates=None, columns=None, schema=None,\n                   chunksize=None):\n        \"\"\"Read SQL database table into a DataFrame.\n\n        Parameters\n        ----------\n        table_name : string\n            Name of SQL table in database\n        index_col : string, optional, default: None\n            Column to set as index\n        coerce_float : boolean, default True\n            Attempt to convert values to non-string, non-numeric objects\n            (like decimal.Decimal) to floating point. This can result in\n            loss of precision.\n        parse_dates : list or dict, default: None\n            - List of column names to parse as dates\n            - Dict of ``{column_name: format string}`` where format string is\n              strftime compatible in case of parsing string times or is one of\n              (D, s, ns, ms, us) in case of parsing integer timestamps\n            - Dict of ``{column_name: arg}``, where the arg corresponds\n              to the keyword arguments of :func:`pandas.to_datetime`.\n              Especially useful with databases without native Datetime support,\n              such as SQLite\n        columns : list, default: None\n            List of column names to select from sql table\n        schema : string, default None\n            Name of SQL schema in database to query (if database flavor\n            supports this).  If specified, this overwrites the default\n            schema of the SQLDatabase object.\n        chunksize : int, default None\n            If specified, return an iterator where `chunksize` is the number\n            of rows to include in each chunk.\n\n        Returns\n        -------\n        DataFrame\n\n        See also\n        --------\n        pandas.read_sql_table\n        SQLDatabase.read_query\n\n        \"\"\"\n        table = SQLTable(table_name, self, index=index_col, schema=schema)\n        return table.read(coerce_float=coerce_float,\n                          parse_dates=parse_dates, columns=columns,\n                          chunksize=chunksize)\n\n    @staticmethod\n    def _query_iterator(result, chunksize, columns, index_col=None,\n                        coerce_float=True, parse_dates=None):\n        \"\"\"Return generator through chunked result set\"\"\"\n\n        while True:\n            data = result.fetchmany(chunksize)\n            if not data:\n                break\n            else:\n                yield _wrap_result(data, columns, index_col=index_col,\n                                   coerce_float=coerce_float,\n                                   parse_dates=parse_dates)\n\n    def read_query(self, sql, index_col=None, coerce_float=True,\n                   parse_dates=None, params=None, chunksize=None):\n        \"\"\"Read SQL query into a DataFrame.\n\n        Parameters\n        ----------\n        sql : string\n            SQL query to be executed\n        index_col : string, optional, default: None\n            Column name to use as index for the returned DataFrame object.\n        coerce_float : boolean, default True\n            Attempt to convert values to non-string, non-numeric objects (like\n            decimal.Decimal) to floating point, useful for SQL result sets\n        params : list, tuple or dict, optional, default: None\n            List of parameters to pass to execute method.  The syntax used\n            to pass parameters is database driver dependent. Check your\n            database driver documentation for which of the five syntax styles,\n            described in PEP 249's paramstyle, is supported.\n            Eg. for psycopg2, uses %(name)s so use params={'name' : 'value'}\n        parse_dates : list or dict, default: None\n            - List of column names to parse as dates\n            - Dict of ``{column_name: format string}`` where format string is\n              strftime compatible in case of parsing string times or is one of\n              (D, s, ns, ms, us) in case of parsing integer timestamps\n            - Dict of ``{column_name: arg dict}``, where the arg dict corresponds\n              to the keyword arguments of :func:`pandas.to_datetime`\n              Especially useful with databases without native Datetime support,\n              such as SQLite\n        chunksize : int, default None\n            If specified, return an iterator where `chunksize` is the number\n            of rows to include in each chunk.\n\n        Returns\n        -------\n        DataFrame\n\n        See also\n        --------\n        read_sql_table : Read SQL database table into a DataFrame\n        read_sql\n\n        \"\"\"\n        args = _convert_params(sql, params)\n\n        result = self.execute(*args)\n        columns = result.keys()\n\n        if chunksize is not None:\n            return self._query_iterator(result, chunksize, columns,\n                                        index_col=index_col,\n                                        coerce_float=coerce_float,\n                                        parse_dates=parse_dates)\n        else:\n            data = result.fetchall()\n            frame = _wrap_result(data, columns, index_col=index_col,\n                                 coerce_float=coerce_float,\n                                 parse_dates=parse_dates)\n            return frame\n\n    read_sql = read_query\n\n    def to_sql(self, frame, name, if_exists='fail', index=True,\n               index_label=None, schema=None, chunksize=None, dtype=None):\n        \"\"\"\n        Write records stored in a DataFrame to a SQL database.\n\n        Parameters\n        ----------\n        frame : DataFrame\n        name : string\n            Name of SQL table\n        if_exists : {'fail', 'replace', 'append'}, default 'fail'\n            - fail: If table exists, do nothing.\n            - replace: If table exists, drop it, recreate it, and insert data.\n            - append: If table exists, insert data. Create if does not exist.\n        index : boolean, default True\n            Write DataFrame index as a column\n        index_label : string or sequence, default None\n            Column label for index column(s). If None is given (default) and\n            `index` is True, then the index names are used.\n            A sequence should be given if the DataFrame uses MultiIndex.\n        schema : string, default None\n            Name of SQL schema in database to write to (if database flavor\n            supports this). If specified, this overwrites the default\n            schema of the SQLDatabase object.\n        chunksize : int, default None\n            If not None, then rows will be written in batches of this size at a\n            time.  If None, all rows will be written at once.\n        dtype : dict of column name to SQL type, default None\n            Optional specifying the datatype for columns. The SQL type should\n            be a SQLAlchemy type.\n\n        \"\"\"\n        if dtype is not None:\n            from sqlalchemy.types import to_instance, TypeEngine\n            for col, my_type in dtype.items():\n                if not isinstance(to_instance(my_type), TypeEngine):\n                    raise ValueError('The type of %s is not a SQLAlchemy '\n                                     'type ' % col)\n\n        table = SQLTable(name, self, frame=frame, index=index,\n                         if_exists=if_exists, index_label=index_label,\n                         schema=schema, dtype=dtype)\n        table.create()\n        table.insert(chunksize)\n        # check for potentially case sensitivity issues (GH7815)\n        engine = self.connectable.engine\n        with self.connectable.connect() as conn:\n            table_names = engine.table_names(\n                schema=schema or self.meta.schema,\n                connection=conn,\n            )\n        if name not in table_names:\n            warnings.warn(\"The provided table name '{0}' is not found exactly \"\n                          \"as such in the database after writing the table, \"\n                          \"possibly due to case sensitivity issues. Consider \"\n                          \"using lower case table names.\".format(name), UserWarning)\n\n    @property\n    def tables(self):\n        return self.meta.tables\n\n    def has_table(self, name, schema=None):\n        return self.connectable.run_callable(\n            self.connectable.dialect.has_table,\n            name,\n            schema or self.meta.schema,\n        )\n\n    def get_table(self, table_name, schema=None):\n        schema = schema or self.meta.schema\n        if schema:\n            tbl = self.meta.tables.get('.'.join([schema, table_name]))\n        else:\n            tbl = self.meta.tables.get(table_name)\n\n        # Avoid casting double-precision floats into decimals\n        from sqlalchemy import Numeric\n        for column in tbl.columns:\n            if isinstance(column.type, Numeric):\n                column.type.asdecimal = False\n\n        return tbl\n\n    def drop_table(self, table_name, schema=None):\n        schema = schema or self.meta.schema\n        if self.has_table(table_name, schema):\n            self.meta.reflect(only=[table_name], schema=schema)\n            self.get_table(table_name, schema).drop()\n            self.meta.clear()\n\n    def _create_sql_schema(self, frame, table_name, keys=None, dtype=None):\n        table = SQLTable(table_name, self, frame=frame, index=False, keys=keys,\n                         dtype=dtype)\n        return str(table.sql_schema())\n\n\n# ---- SQL without SQLAlchemy ---\n# Flavour specific sql strings and handler class for access to DBs without\n# SQLAlchemy installed\n# SQL type convertions for each DB\n_SQL_TYPES = {\n    'string': {\n        'mysql': 'VARCHAR (63)',\n        'sqlite': 'TEXT',\n    },\n    'floating': {\n        'mysql': 'DOUBLE',\n        'sqlite': 'REAL',\n    },\n    'integer': {\n        'mysql': 'BIGINT',\n        'sqlite': 'INTEGER',\n    },\n    'datetime': {\n        'mysql': 'DATETIME',\n        'sqlite': 'TIMESTAMP',\n    },\n    'date': {\n        'mysql': 'DATE',\n        'sqlite': 'DATE',\n    },\n    'time': {\n        'mysql': 'TIME',\n        'sqlite': 'TIME',\n    },\n    'boolean': {\n        'mysql': 'BOOLEAN',\n        'sqlite': 'INTEGER',\n    }\n}\n\n\ndef _get_unicode_name(name):\n    try:\n        uname = name.encode(\"utf-8\", \"strict\").decode(\"utf-8\")\n    except UnicodeError:\n        raise ValueError(\"Cannot convert identifier to UTF-8: '%s'\" % name)\n    return uname\n\ndef _get_valid_mysql_name(name):\n    # Filter for unquoted identifiers\n    # See http://dev.mysql.com/doc/refman/5.0/en/identifiers.html\n    uname = _get_unicode_name(name)\n    if not len(uname):\n        raise ValueError(\"Empty table or column name specified\")\n\n    basere = r'[0-9,a-z,A-Z$_]'\n    for c in uname:\n        if not re.match(basere, c):\n            if not (0x80 < ord(c) < 0xFFFF):\n                raise ValueError(\"Invalid MySQL identifier '%s'\" % uname)\n\n    return '`' + uname + '`'\n\n\ndef _get_valid_sqlite_name(name):\n    # See http://stackoverflow.com/questions/6514274/how-do-you-escape-strings-for-sqlite-table-column-names-in-python\n    # Ensure the string can be encoded as UTF-8.\n    # Ensure the string does not include any NUL characters.\n    # Replace all \" with \"\".\n    # Wrap the entire thing in double quotes.\n\n    uname = _get_unicode_name(name)\n    if not len(uname):\n        raise ValueError(\"Empty table or column name specified\")\n\n    nul_index = uname.find(\"\\x00\")\n    if nul_index >= 0:\n        raise ValueError('SQLite identifier cannot contain NULs')\n    return '\"' + uname.replace('\"', '\"\"') + '\"'\n\n\n# SQL enquote and wildcard symbols\n_SQL_WILDCARD = {\n    'mysql': '%s',\n    'sqlite': '?'\n}\n\n# Validate and return escaped identifier\n_SQL_GET_IDENTIFIER = {\n    'mysql': _get_valid_mysql_name,\n    'sqlite': _get_valid_sqlite_name,\n}\n\n\n_SAFE_NAMES_WARNING = (\"The spaces in these column names will not be changed. \"\n                       \"In pandas versions < 0.14, spaces were converted to \"\n                       \"underscores.\")\n\n\nclass SQLiteTable(SQLTable):\n    \"\"\"\n    Patch the SQLTable for fallback support.\n    Instead of a table variable just use the Create Table statement.\n    \"\"\"\n\n    def sql_schema(self):\n        return str(\";\\n\".join(self.table))\n\n    def _execute_create(self):\n        with self.pd_sql.run_transaction() as conn:\n            for stmt in self.table:\n                conn.execute(stmt)\n\n    def insert_statement(self):\n        names = list(map(str, self.frame.columns))\n        flv = self.pd_sql.flavor\n        wld = _SQL_WILDCARD[flv]  # wildcard char\n        escape = _SQL_GET_IDENTIFIER[flv]\n\n        if self.index is not None:\n            [names.insert(0, idx) for idx in self.index[::-1]]\n\n        bracketed_names = [escape(column) for column in names]\n        col_names = ','.join(bracketed_names)\n        wildcards = ','.join([wld] * len(names))\n        insert_statement = 'INSERT INTO %s (%s) VALUES (%s)' % (\n            escape(self.name), col_names, wildcards)\n        return insert_statement\n\n    def _execute_insert(self, conn, keys, data_iter):\n        data_list = list(data_iter)\n        conn.executemany(self.insert_statement(), data_list)\n\n    def _create_table_setup(self):\n        \"\"\"\n        Return a list of SQL statement that create a table reflecting the\n        structure of a DataFrame.  The first entry will be a CREATE TABLE\n        statement while the rest will be CREATE INDEX statements\n        \"\"\"\n        column_names_and_types = \\\n            self._get_column_names_and_types(self._sql_type_name)\n\n        pat = re.compile('\\s+')\n        column_names = [col_name for col_name, _, _ in column_names_and_types]\n        if any(map(pat.search, column_names)):\n            warnings.warn(_SAFE_NAMES_WARNING, stacklevel=6)\n\n        flv = self.pd_sql.flavor\n        escape = _SQL_GET_IDENTIFIER[flv]\n\n        create_tbl_stmts = [escape(cname) + ' ' + ctype\n                            for cname, ctype, _ in column_names_and_types]\n\n        if self.keys is not None and len(self.keys):\n            if not com.is_list_like(self.keys):\n                keys = [self.keys]\n            else:\n                keys = self.keys\n            cnames_br = \", \".join([escape(c) for c in keys])\n            create_tbl_stmts.append(\n                \"CONSTRAINT {tbl}_pk PRIMARY KEY ({cnames_br})\".format(\n                tbl=self.name, cnames_br=cnames_br))\n\n        create_stmts = [\"CREATE TABLE \" + escape(self.name) + \" (\\n\" +\n                        ',\\n  '.join(create_tbl_stmts) + \"\\n)\"]\n\n        ix_cols = [cname for cname, _, is_index in column_names_and_types\n                   if is_index]\n        if len(ix_cols):\n            cnames = \"_\".join(ix_cols)\n            cnames_br = \",\".join([escape(c) for c in ix_cols])\n            create_stmts.append(\n                \"CREATE INDEX \" + escape(\"ix_\"+self.name+\"_\"+cnames) +\n                \"ON \" + escape(self.name) + \" (\" + cnames_br + \")\")\n\n        return create_stmts\n\n    def _sql_type_name(self, col):\n        dtype = self.dtype or {}\n        if col.name in dtype:\n            return dtype[col.name]\n\n        col_type = self._get_notnull_col_dtype(col)\n        if col_type == 'timedelta64':\n            warnings.warn(\"the 'timedelta' type is not supported, and will be \"\n                          \"written as integer values (ns frequency) to the \"\n                          \"database.\", UserWarning, stacklevel=8)\n            col_type = \"integer\"\n\n        elif col_type == \"datetime64\":\n            col_type = \"datetime\"\n\n        elif col_type == \"empty\":\n            col_type = \"string\"\n\n        elif col_type == \"complex\":\n            raise ValueError('Complex datatypes not supported')\n\n        if col_type not in _SQL_TYPES:\n            col_type = \"string\"\n\n        return _SQL_TYPES[col_type][self.pd_sql.flavor]\n\n\nclass SQLiteDatabase(PandasSQL):\n    \"\"\"\n    Version of SQLDatabase to support sqlite connections (fallback without\n    sqlalchemy). This should only be used internally.\n\n    For now still supports `flavor` argument to deal with 'mysql' database\n    for backwards compatibility, but this will be removed in future versions.\n\n    Parameters\n    ----------\n    con : sqlite connection object\n\n    \"\"\"\n\n    def __init__(self, con, flavor, is_cursor=False):\n        self.is_cursor = is_cursor\n        self.con = con\n        if flavor is None:\n            flavor = 'sqlite'\n        if flavor not in ['sqlite', 'mysql']:\n            raise NotImplementedError(\"flavors other than SQLite and MySQL \"\n                                      \"are not supported\")\n        else:\n            self.flavor = flavor\n\n    @contextmanager\n    def run_transaction(self):\n        cur = self.con.cursor()\n        try:\n            yield cur\n            self.con.commit()\n        except:\n            self.con.rollback()\n            raise\n        finally:\n            cur.close()\n\n    def execute(self, *args, **kwargs):\n        if self.is_cursor:\n            cur = self.con\n        else:\n            cur = self.con.cursor()\n        try:\n            if kwargs:\n                cur.execute(*args, **kwargs)\n            else:\n                cur.execute(*args)\n            return cur\n        except Exception as exc:\n            try:\n                self.con.rollback()\n            except Exception:  # pragma: no cover\n                ex = DatabaseError(\"Execution failed on sql: %s\\n%s\\nunable\"\n                                   \" to rollback\" % (args[0], exc))\n                raise_with_traceback(ex)\n\n            ex = DatabaseError(\"Execution failed on sql '%s': %s\" % (args[0], exc))\n            raise_with_traceback(ex)\n\n    @staticmethod\n    def _query_iterator(cursor, chunksize, columns, index_col=None,\n                        coerce_float=True, parse_dates=None):\n        \"\"\"Return generator through chunked result set\"\"\"\n\n        while True:\n            data = cursor.fetchmany(chunksize)\n            if not data:\n                cursor.close()\n                break\n            else:\n                yield _wrap_result(data, columns, index_col=index_col,\n                                   coerce_float=coerce_float,\n                                   parse_dates=parse_dates)\n\n    def read_query(self, sql, index_col=None, coerce_float=True, params=None,\n                   parse_dates=None, chunksize=None):\n\n        args = _convert_params(sql, params)\n        cursor = self.execute(*args)\n        columns = [col_desc[0] for col_desc in cursor.description]\n\n        if chunksize is not None:\n            return self._query_iterator(cursor, chunksize, columns,\n                                        index_col=index_col,\n                                        coerce_float=coerce_float,\n                                        parse_dates=parse_dates)\n        else:\n            data = self._fetchall_as_list(cursor)\n            cursor.close()\n\n            frame = _wrap_result(data, columns, index_col=index_col,\n                                 coerce_float=coerce_float,\n                                 parse_dates=parse_dates)\n            return frame\n\n    def _fetchall_as_list(self, cur):\n        result = cur.fetchall()\n        if not isinstance(result, list):\n            result = list(result)\n        return result\n\n    def to_sql(self, frame, name, if_exists='fail', index=True,\n               index_label=None, schema=None, chunksize=None, dtype=None):\n        \"\"\"\n        Write records stored in a DataFrame to a SQL database.\n\n        Parameters\n        ----------\n        frame: DataFrame\n        name: name of SQL table\n        if_exists: {'fail', 'replace', 'append'}, default 'fail'\n            fail: If table exists, do nothing.\n            replace: If table exists, drop it, recreate it, and insert data.\n            append: If table exists, insert data. Create if does not exist.\n        index : boolean, default True\n            Write DataFrame index as a column\n        index_label : string or sequence, default None\n            Column label for index column(s). If None is given (default) and\n            `index` is True, then the index names are used.\n            A sequence should be given if the DataFrame uses MultiIndex.\n        schema : string, default None\n            Ignored parameter included for compatability with SQLAlchemy\n            version of ``to_sql``.\n        chunksize : int, default None\n            If not None, then rows will be written in batches of this\n            size at a time. If None, all rows will be written at once.\n        dtype : dict of column name to SQL type, default None\n            Optional specifying the datatype for columns. The SQL type should\n            be a string.\n\n        \"\"\"\n        if dtype is not None:\n            for col, my_type in dtype.items():\n                if not isinstance(my_type, str):\n                    raise ValueError('%s (%s) not a string' % (\n                        col, str(my_type)))\n\n        table = SQLiteTable(name, self, frame=frame, index=index,\n                            if_exists=if_exists, index_label=index_label,\n                            dtype=dtype)\n        table.create()\n        table.insert(chunksize)\n\n    def has_table(self, name, schema=None):\n        escape = _SQL_GET_IDENTIFIER[self.flavor]\n        esc_name = escape(name)\n        wld = _SQL_WILDCARD[self.flavor]\n        flavor_map = {\n            'sqlite': (\"SELECT name FROM sqlite_master \"\n                       \"WHERE type='table' AND name=%s;\") % wld,\n            'mysql': \"SHOW TABLES LIKE %s\" % wld}\n        query = flavor_map.get(self.flavor)\n\n        return len(self.execute(query, [name,]).fetchall()) > 0\n\n    def get_table(self, table_name, schema=None):\n        return None  # not supported in fallback mode\n\n    def drop_table(self, name, schema=None):\n        escape = _SQL_GET_IDENTIFIER[self.flavor]\n        drop_sql = \"DROP TABLE %s\" % escape(name)\n        self.execute(drop_sql)\n\n    def _create_sql_schema(self, frame, table_name, keys=None, dtype=None):\n        table = SQLiteTable(table_name, self, frame=frame, index=False,\n                            keys=keys, dtype=dtype)\n        return str(table.sql_schema())\n\n\ndef get_schema(frame, name, flavor='sqlite', keys=None, con=None, dtype=None):\n    \"\"\"\n    Get the SQL db table schema for the given frame.\n\n    Parameters\n    ----------\n    frame : DataFrame\n    name : string\n        name of SQL table\n    flavor : {'sqlite', 'mysql'}, default 'sqlite'\n        The flavor of SQL to use. Ignored when using SQLAlchemy connectable.\n        'mysql' is deprecated and will be removed in future versions, but it\n        will be further supported through SQLAlchemy engines.\n    keys : string or sequence, default: None\n        columns to use a primary key\n    con: an open SQL database connection object or a SQLAlchemy connectable\n        Using SQLAlchemy makes it possible to use any DB supported by that\n        library, default: None\n        If a DBAPI2 object, only sqlite3 is supported.\n    dtype : dict of column name to SQL type, default None\n        Optional specifying the datatype for columns. The SQL type should\n        be a SQLAlchemy type, or a string for sqlite3 fallback connection.\n\n    \"\"\"\n\n    pandas_sql = pandasSQL_builder(con=con, flavor=flavor)\n    return pandas_sql._create_sql_schema(frame, name, keys=keys, dtype=dtype)\n\n\n# legacy names, with depreciation warnings and copied docs\n\n@Appender(read_sql.__doc__, join='\\n')\ndef read_frame(*args, **kwargs):\n    \"\"\"DEPRECATED - use read_sql\n    \"\"\"\n    warnings.warn(\"read_frame is deprecated, use read_sql\", FutureWarning,\n                  stacklevel=2)\n    return read_sql(*args, **kwargs)\n\n\n@Appender(read_sql.__doc__, join='\\n')\ndef frame_query(*args, **kwargs):\n    \"\"\"DEPRECATED - use read_sql\n    \"\"\"\n    warnings.warn(\"frame_query is deprecated, use read_sql\", FutureWarning,\n                  stacklevel=2)\n    return read_sql(*args, **kwargs)\n\n\ndef write_frame(frame, name, con, flavor='sqlite', if_exists='fail', **kwargs):\n    \"\"\"DEPRECATED - use to_sql\n\n    Write records stored in a DataFrame to a SQL database.\n\n    Parameters\n    ----------\n    frame : DataFrame\n    name : string\n    con : DBAPI2 connection\n    flavor : {'sqlite', 'mysql'}, default 'sqlite'\n        The flavor of SQL to use.\n    if_exists : {'fail', 'replace', 'append'}, default 'fail'\n        - fail: If table exists, do nothing.\n        - replace: If table exists, drop it, recreate it, and insert data.\n        - append: If table exists, insert data. Create if does not exist.\n    index : boolean, default False\n        Write DataFrame index as a column\n\n    Notes\n    -----\n    This function is deprecated in favor of ``to_sql``. There are however\n    two differences:\n\n    - With ``to_sql`` the index is written to the sql database by default. To\n      keep the behaviour this function you need to specify ``index=False``.\n    - The new ``to_sql`` function supports sqlalchemy connectables to work\n      with different sql flavors.\n\n    See also\n    --------\n    pandas.DataFrame.to_sql\n\n    \"\"\"\n    warnings.warn(\"write_frame is deprecated, use to_sql\", FutureWarning,\n                  stacklevel=2)\n\n    # for backwards compatibility, set index=False when not specified\n    index = kwargs.pop('index', False)\n    return to_sql(frame, name, con, flavor=flavor, if_exists=if_exists,\n                  index=index, **kwargs)\n"
    },
    {
      "filename": "pandas/io/tests/test_sql.py",
      "content": "\"\"\"SQL io tests\n\nThe SQL tests are broken down in different classes:\n\n- `PandasSQLTest`: base class with common methods for all test classes\n- Tests for the public API (only tests with sqlite3)\n    - `_TestSQLApi` base class\n    - `TestSQLApi`: test the public API with sqlalchemy engine\n    - `TestSQLiteFallbackApi`: test the public API with a sqlite DBAPI connection\n- Tests for the different SQL flavors (flavor specific type conversions)\n    - Tests for the sqlalchemy mode: `_TestSQLAlchemy` is the base class with\n      common methods, `_TestSQLAlchemyConn` tests the API with a SQLAlchemy\n      Connection object. The different tested flavors (sqlite3, MySQL, PostgreSQL)\n      derive from the base class\n    - Tests for the fallback mode (`TestSQLiteFallback` and `TestMySQLLegacy`)\n\n\"\"\"\n\nfrom __future__ import print_function\nimport unittest\nimport sqlite3\nimport csv\nimport os\nimport sys\n\nimport nose\nimport warnings\nimport numpy as np\n\nfrom datetime import datetime, date, time\n\nfrom pandas import DataFrame, Series, Index, MultiIndex, isnull, concat\nfrom pandas import date_range, to_datetime, to_timedelta, Timestamp\nimport pandas.compat as compat\nfrom pandas.compat import StringIO, range, lrange, string_types\nfrom pandas.core.datetools import format as date_format\n\nimport pandas.io.sql as sql\nfrom pandas.io.sql import read_sql_table, read_sql_query\nimport pandas.util.testing as tm\n\n\ntry:\n    import sqlalchemy\n    import sqlalchemy.schema\n    import sqlalchemy.sql.sqltypes as sqltypes\n    from sqlalchemy.ext import declarative\n    from sqlalchemy.orm import session as sa_session\n    SQLALCHEMY_INSTALLED = True\nexcept ImportError:\n    SQLALCHEMY_INSTALLED = False\n\nSQL_STRINGS = {\n    'create_iris': {\n        'sqlite': \"\"\"CREATE TABLE iris (\n                \"SepalLength\" REAL,\n                \"SepalWidth\" REAL,\n                \"PetalLength\" REAL,\n                \"PetalWidth\" REAL,\n                \"Name\" TEXT\n            )\"\"\",\n        'mysql': \"\"\"CREATE TABLE iris (\n                `SepalLength` DOUBLE,\n                `SepalWidth` DOUBLE,\n                `PetalLength` DOUBLE,\n                `PetalWidth` DOUBLE,\n                `Name` VARCHAR(200)\n            )\"\"\",\n        'postgresql': \"\"\"CREATE TABLE iris (\n                \"SepalLength\" DOUBLE PRECISION,\n                \"SepalWidth\" DOUBLE PRECISION,\n                \"PetalLength\" DOUBLE PRECISION,\n                \"PetalWidth\" DOUBLE PRECISION,\n                \"Name\" VARCHAR(200)\n            )\"\"\"\n    },\n    'insert_iris': {\n        'sqlite': \"\"\"INSERT INTO iris VALUES(?, ?, ?, ?, ?)\"\"\",\n        'mysql': \"\"\"INSERT INTO iris VALUES(%s, %s, %s, %s, \"%s\");\"\"\",\n        'postgresql': \"\"\"INSERT INTO iris VALUES(%s, %s, %s, %s, %s);\"\"\"\n    },\n    'create_test_types': {\n        'sqlite': \"\"\"CREATE TABLE types_test_data (\n                    \"TextCol\" TEXT,\n                    \"DateCol\" TEXT,\n                    \"IntDateCol\" INTEGER,\n                    \"FloatCol\" REAL,\n                    \"IntCol\" INTEGER,\n                    \"BoolCol\" INTEGER,\n                    \"IntColWithNull\" INTEGER,\n                    \"BoolColWithNull\" INTEGER\n                )\"\"\",\n        'mysql': \"\"\"CREATE TABLE types_test_data (\n                    `TextCol` TEXT,\n                    `DateCol` DATETIME,\n                    `IntDateCol` INTEGER,\n                    `FloatCol` DOUBLE,\n                    `IntCol` INTEGER,\n                    `BoolCol` BOOLEAN,\n                    `IntColWithNull` INTEGER,\n                    `BoolColWithNull` BOOLEAN\n                )\"\"\",\n        'postgresql': \"\"\"CREATE TABLE types_test_data (\n                    \"TextCol\" TEXT,\n                    \"DateCol\" TIMESTAMP,\n                    \"DateColWithTz\" TIMESTAMP WITH TIME ZONE,\n                    \"IntDateCol\" INTEGER,\n                    \"FloatCol\" DOUBLE PRECISION,\n                    \"IntCol\" INTEGER,\n                    \"BoolCol\" BOOLEAN,\n                    \"IntColWithNull\" INTEGER,\n                    \"BoolColWithNull\" BOOLEAN\n                )\"\"\"\n    },\n    'insert_test_types': {\n        'sqlite': {\n            'query': \"\"\"\n                INSERT INTO types_test_data\n                VALUES(?, ?, ?, ?, ?, ?, ?, ?)\n                \"\"\",\n            'fields': (\n                'TextCol', 'DateCol', 'IntDateCol', 'FloatCol',\n                'IntCol', 'BoolCol', 'IntColWithNull', 'BoolColWithNull'\n            )\n        },\n        'mysql': {\n            'query': \"\"\"\n                INSERT INTO types_test_data\n                VALUES(\"%s\", %s, %s, %s, %s, %s, %s, %s)\n                \"\"\",\n            'fields': (\n                'TextCol', 'DateCol', 'IntDateCol', 'FloatCol',\n                'IntCol', 'BoolCol', 'IntColWithNull', 'BoolColWithNull'\n            )\n        },\n        'postgresql': {\n            'query': \"\"\"\n                INSERT INTO types_test_data\n                VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s)\n                \"\"\",\n            'fields': (\n                'TextCol', 'DateCol', 'DateColWithTz', 'IntDateCol', 'FloatCol',\n                'IntCol', 'BoolCol', 'IntColWithNull', 'BoolColWithNull'\n            )\n        },\n    },\n    'read_parameters': {\n        'sqlite': \"SELECT * FROM iris WHERE Name=? AND SepalLength=?\",\n        'mysql': 'SELECT * FROM iris WHERE `Name`=\"%s\" AND `SepalLength`=%s',\n        'postgresql': 'SELECT * FROM iris WHERE \"Name\"=%s AND \"SepalLength\"=%s'\n    },\n    'read_named_parameters': {\n        'sqlite': \"\"\"\n                SELECT * FROM iris WHERE Name=:name AND SepalLength=:length\n                \"\"\",\n        'mysql': \"\"\"\n                SELECT * FROM iris WHERE\n                `Name`=\"%(name)s\" AND `SepalLength`=%(length)s\n                \"\"\",\n        'postgresql': \"\"\"\n                SELECT * FROM iris WHERE\n                \"Name\"=%(name)s AND \"SepalLength\"=%(length)s\n                \"\"\"\n    },\n    'create_view': {\n        'sqlite': \"\"\"\n                CREATE VIEW iris_view AS\n                SELECT * FROM iris\n                \"\"\"\n    }\n}\n\n\nclass MixInBase(object):\n    def tearDown(self):\n        for tbl in self._get_all_tables():\n            self.drop_table(tbl)\n        self._close_conn()\n\n\nclass MySQLMixIn(MixInBase):\n    def drop_table(self, table_name):\n        cur = self.conn.cursor()\n        cur.execute(\"DROP TABLE IF EXISTS %s\" % sql._get_valid_mysql_name(table_name))\n        self.conn.commit()\n\n    def _get_all_tables(self):\n        cur = self.conn.cursor()\n        cur.execute('SHOW TABLES')\n        return [table[0] for table in cur.fetchall()]\n\n    def _close_conn(self):\n        from pymysql.err import Error\n        try:\n            self.conn.close()\n        except Error:\n            pass\n\n\nclass SQLiteMixIn(MixInBase):\n    def drop_table(self, table_name):\n        self.conn.execute(\"DROP TABLE IF EXISTS %s\" % sql._get_valid_sqlite_name(table_name))\n        self.conn.commit()\n\n    def _get_all_tables(self):\n        c = self.conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n        return [table[0] for table in c.fetchall()]\n\n    def _close_conn(self):\n        self.conn.close()\n\n\nclass SQLAlchemyMixIn(MixInBase):\n    def drop_table(self, table_name):\n        sql.SQLDatabase(self.conn).drop_table(table_name)\n\n    def _get_all_tables(self):\n        meta = sqlalchemy.schema.MetaData(bind=self.conn)\n        meta.reflect()\n        table_list = meta.tables.keys()\n        return table_list\n\n    def _close_conn(self):\n        pass\n\nclass PandasSQLTest(unittest.TestCase):\n    \"\"\"\n    Base class with common private methods for SQLAlchemy and fallback cases.\n\n    \"\"\"\n\n    def _get_exec(self):\n        if hasattr(self.conn, 'execute'):\n            return self.conn\n        else:\n            return self.conn.cursor()\n\n    def _load_iris_data(self):\n        import io\n        iris_csv_file = os.path.join(tm.get_data_path(), 'iris.csv')\n\n        self.drop_table('iris')\n        self._get_exec().execute(SQL_STRINGS['create_iris'][self.flavor])\n\n        with io.open(iris_csv_file, mode='r', newline=None) as iris_csv:\n            r = csv.reader(iris_csv)\n            next(r)  # skip header row\n            ins = SQL_STRINGS['insert_iris'][self.flavor]\n\n            for row in r:\n                self._get_exec().execute(ins, row)\n\n    def _load_iris_view(self):\n        self.drop_table('iris_view')\n        self._get_exec().execute(SQL_STRINGS['create_view'][self.flavor])\n\n    def _check_iris_loaded_frame(self, iris_frame):\n        pytype = iris_frame.dtypes[0].type\n        row = iris_frame.iloc[0]\n\n        self.assertTrue(\n            issubclass(pytype, np.floating), 'Loaded frame has incorrect type')\n        tm.equalContents(row.values, [5.1, 3.5, 1.4, 0.2, 'Iris-setosa'])\n\n    def _load_test1_data(self):\n        columns = ['index', 'A', 'B', 'C', 'D']\n        data = [(\n            '2000-01-03 00:00:00', 0.980268513777, 3.68573087906, -0.364216805298, -1.15973806169),\n            ('2000-01-04 00:00:00', 1.04791624281, -\n             0.0412318367011, -0.16181208307, 0.212549316967),\n            ('2000-01-05 00:00:00', 0.498580885705,\n             0.731167677815, -0.537677223318, 1.34627041952),\n            ('2000-01-06 00:00:00', 1.12020151869, 1.56762092543, 0.00364077397681, 0.67525259227)]\n\n        self.test_frame1 = DataFrame(data, columns=columns)\n\n    def _load_test2_data(self):\n        df = DataFrame(dict(A=[4, 1, 3, 6],\n                            B=['asd', 'gsq', 'ylt', 'jkl'],\n                            C=[1.1, 3.1, 6.9, 5.3],\n                            D=[False, True, True, False],\n                            E=['1990-11-22', '1991-10-26', '1993-11-26', '1995-12-12']))\n        df['E'] = to_datetime(df['E'])\n\n        self.test_frame2 = df\n\n    def _load_test3_data(self):\n        columns = ['index', 'A', 'B']\n        data = [(\n            '2000-01-03 00:00:00', 2 ** 31 - 1, -1.987670),\n            ('2000-01-04 00:00:00', -29, -0.0412318367011),\n            ('2000-01-05 00:00:00', 20000, 0.731167677815),\n            ('2000-01-06 00:00:00', -290867, 1.56762092543)]\n\n        self.test_frame3 = DataFrame(data, columns=columns)\n\n    def _load_raw_sql(self):\n        self.drop_table('types_test_data')\n        self._get_exec().execute(SQL_STRINGS['create_test_types'][self.flavor])\n        ins = SQL_STRINGS['insert_test_types'][self.flavor]\n\n        data = [\n            {\n                'TextCol': 'first',\n                'DateCol': '2000-01-03 00:00:00',\n                'DateColWithTz': '2000-01-01 00:00:00-08:00',\n                'IntDateCol': 535852800,\n                'FloatCol': 10.10,\n                'IntCol': 1,\n                'BoolCol': False,\n                'IntColWithNull': 1,\n                'BoolColWithNull': False,\n            },\n            {\n                'TextCol': 'first',\n                'DateCol': '2000-01-04 00:00:00',\n                'DateColWithTz': '2000-06-01 00:00:00-07:00',\n                'IntDateCol': 1356998400,\n                'FloatCol': 10.10,\n                'IntCol': 1,\n                'BoolCol': False,\n                'IntColWithNull': None,\n                'BoolColWithNull': None,\n            },\n        ]\n\n        for d in data:\n            self._get_exec().execute(\n                ins['query'],\n                [d[field] for field in ins['fields']]\n            )\n\n    def _count_rows(self, table_name):\n        result = self._get_exec().execute(\n            \"SELECT count(*) AS count_1 FROM %s\" % table_name).fetchone()\n        return result[0]\n\n    def _read_sql_iris(self):\n        iris_frame = self.pandasSQL.read_query(\"SELECT * FROM iris\")\n        self._check_iris_loaded_frame(iris_frame)\n\n    def _read_sql_iris_parameter(self):\n        query = SQL_STRINGS['read_parameters'][self.flavor]\n        params = ['Iris-setosa', 5.1]\n        iris_frame = self.pandasSQL.read_query(query, params=params)\n        self._check_iris_loaded_frame(iris_frame)\n\n    def _read_sql_iris_named_parameter(self):\n        query = SQL_STRINGS['read_named_parameters'][self.flavor]\n        params = {'name': 'Iris-setosa', 'length': 5.1}\n        iris_frame = self.pandasSQL.read_query(query, params=params)\n        self._check_iris_loaded_frame(iris_frame)\n\n    def _to_sql(self):\n        self.drop_table('test_frame1')\n\n        self.pandasSQL.to_sql(self.test_frame1, 'test_frame1')\n        self.assertTrue(self.pandasSQL.has_table(\n            'test_frame1'), 'Table not written to DB')\n\n        # Nuke table\n        self.drop_table('test_frame1')\n\n    def _to_sql_empty(self):\n        self.drop_table('test_frame1')\n        self.pandasSQL.to_sql(self.test_frame1.iloc[:0], 'test_frame1')\n\n    def _to_sql_fail(self):\n        self.drop_table('test_frame1')\n\n        self.pandasSQL.to_sql(\n            self.test_frame1, 'test_frame1', if_exists='fail')\n        self.assertTrue(self.pandasSQL.has_table(\n            'test_frame1'), 'Table not written to DB')\n\n        self.assertRaises(ValueError, self.pandasSQL.to_sql,\n                          self.test_frame1, 'test_frame1', if_exists='fail')\n\n        self.drop_table('test_frame1')\n\n    def _to_sql_replace(self):\n        self.drop_table('test_frame1')\n\n        self.pandasSQL.to_sql(\n            self.test_frame1, 'test_frame1', if_exists='fail')\n        # Add to table again\n        self.pandasSQL.to_sql(\n            self.test_frame1, 'test_frame1', if_exists='replace')\n        self.assertTrue(self.pandasSQL.has_table(\n            'test_frame1'), 'Table not written to DB')\n\n        num_entries = len(self.test_frame1)\n        num_rows = self._count_rows('test_frame1')\n\n        self.assertEqual(\n            num_rows, num_entries, \"not the same number of rows as entries\")\n\n        self.drop_table('test_frame1')\n\n    def _to_sql_append(self):\n        # Nuke table just in case\n        self.drop_table('test_frame1')\n\n        self.pandasSQL.to_sql(\n            self.test_frame1, 'test_frame1', if_exists='fail')\n\n        # Add to table again\n        self.pandasSQL.to_sql(\n            self.test_frame1, 'test_frame1', if_exists='append')\n        self.assertTrue(self.pandasSQL.has_table(\n            'test_frame1'), 'Table not written to DB')\n\n        num_entries = 2 * len(self.test_frame1)\n        num_rows = self._count_rows('test_frame1')\n\n        self.assertEqual(\n            num_rows, num_entries, \"not the same number of rows as entries\")\n\n        self.drop_table('test_frame1')\n\n    def _roundtrip(self):\n        self.drop_table('test_frame_roundtrip')\n        self.pandasSQL.to_sql(self.test_frame1, 'test_frame_roundtrip')\n        result = self.pandasSQL.read_query('SELECT * FROM test_frame_roundtrip')\n\n        result.set_index('level_0', inplace=True)\n        # result.index.astype(int)\n\n        result.index.name = None\n\n        tm.assert_frame_equal(result, self.test_frame1)\n\n    def _execute_sql(self):\n        # drop_sql = \"DROP TABLE IF EXISTS test\"  # should already be done\n        iris_results = self.pandasSQL.execute(\"SELECT * FROM iris\")\n        row = iris_results.fetchone()\n        tm.equalContents(row, [5.1, 3.5, 1.4, 0.2, 'Iris-setosa'])\n\n    def _to_sql_save_index(self):\n        df = DataFrame.from_records([(1,2.1,'line1'), (2,1.5,'line2')],\n                                    columns=['A','B','C'], index=['A'])\n        self.pandasSQL.to_sql(df, 'test_to_sql_saves_index')\n        ix_cols = self._get_index_columns('test_to_sql_saves_index')\n        self.assertEqual(ix_cols, [['A',],])\n\n    def _transaction_test(self):\n        self.pandasSQL.execute(\"CREATE TABLE test_trans (A INT, B TEXT)\")\n\n        ins_sql = \"INSERT INTO test_trans (A,B) VALUES (1, 'blah')\"\n\n        # Make sure when transaction is rolled back, no rows get inserted\n        try:\n            with self.pandasSQL.run_transaction() as trans:\n                trans.execute(ins_sql)\n                raise Exception('error')\n        except:\n            # ignore raised exception\n            pass\n        res = self.pandasSQL.read_query('SELECT * FROM test_trans')\n        self.assertEqual(len(res), 0)\n\n        # Make sure when transaction is committed, rows do get inserted\n        with self.pandasSQL.run_transaction() as trans:\n            trans.execute(ins_sql)\n        res2 = self.pandasSQL.read_query('SELECT * FROM test_trans')\n        self.assertEqual(len(res2), 1)\n\n\n#------------------------------------------------------------------------------\n#--- Testing the public API\n\nclass _TestSQLApi(PandasSQLTest):\n\n    \"\"\"\n    Base class to test the public API.\n\n    From this two classes are derived to run these tests for both the\n    sqlalchemy mode (`TestSQLApi`) and the fallback mode (`TestSQLiteFallbackApi`).\n    These tests are run with sqlite3. Specific tests for the different\n    sql flavours are included in `_TestSQLAlchemy`.\n\n    Notes:\n    flavor can always be passed even in SQLAlchemy mode,\n    should be correctly ignored.\n\n    we don't use drop_table because that isn't part of the public api\n\n    \"\"\"\n    flavor = 'sqlite'\n    mode = None\n\n    def setUp(self):\n        self.conn = self.connect()\n        self._load_iris_data()\n        self._load_iris_view()\n        self._load_test1_data()\n        self._load_test2_data()\n        self._load_test3_data()\n        self._load_raw_sql()\n\n    def test_read_sql_iris(self):\n        iris_frame = sql.read_sql_query(\n            \"SELECT * FROM iris\", self.conn)\n        self._check_iris_loaded_frame(iris_frame)\n\n    def test_read_sql_view(self):\n        iris_frame = sql.read_sql_query(\n            \"SELECT * FROM iris_view\", self.conn)\n        self._check_iris_loaded_frame(iris_frame)\n\n    def test_legacy_read_frame(self):\n        with tm.assert_produces_warning(FutureWarning):\n            iris_frame = sql.read_frame(\n                \"SELECT * FROM iris\", self.conn)\n        self._check_iris_loaded_frame(iris_frame)\n\n    def test_to_sql(self):\n        sql.to_sql(self.test_frame1, 'test_frame1', self.conn, flavor='sqlite')\n        self.assertTrue(\n            sql.has_table('test_frame1', self.conn, flavor='sqlite'), 'Table not written to DB')\n\n    def test_to_sql_fail(self):\n        sql.to_sql(self.test_frame1, 'test_frame2',\n                   self.conn, flavor='sqlite', if_exists='fail')\n        self.assertTrue(\n            sql.has_table('test_frame2', self.conn, flavor='sqlite'), 'Table not written to DB')\n\n        self.assertRaises(ValueError, sql.to_sql, self.test_frame1,\n                          'test_frame2', self.conn, flavor='sqlite', if_exists='fail')\n\n    def test_to_sql_replace(self):\n        sql.to_sql(self.test_frame1, 'test_frame3',\n                   self.conn, flavor='sqlite', if_exists='fail')\n        # Add to table again\n        sql.to_sql(self.test_frame1, 'test_frame3',\n                   self.conn, flavor='sqlite', if_exists='replace')\n        self.assertTrue(\n            sql.has_table('test_frame3', self.conn, flavor='sqlite'),\n            'Table not written to DB')\n\n        num_entries = len(self.test_frame1)\n        num_rows = self._count_rows('test_frame3')\n\n        self.assertEqual(\n            num_rows, num_entries, \"not the same number of rows as entries\")\n\n    def test_to_sql_append(self):\n        sql.to_sql(self.test_frame1, 'test_frame4',\n                   self.conn, flavor='sqlite', if_exists='fail')\n\n        # Add to table again\n        sql.to_sql(self.test_frame1, 'test_frame4',\n                   self.conn, flavor='sqlite', if_exists='append')\n        self.assertTrue(\n            sql.has_table('test_frame4', self.conn, flavor='sqlite'),\n            'Table not written to DB')\n\n        num_entries = 2 * len(self.test_frame1)\n        num_rows = self._count_rows('test_frame4')\n\n        self.assertEqual(\n            num_rows, num_entries, \"not the same number of rows as entries\")\n\n    def test_to_sql_type_mapping(self):\n        sql.to_sql(self.test_frame3, 'test_frame5',\n                   self.conn, flavor='sqlite', index=False)\n        result = sql.read_sql(\"SELECT * FROM test_frame5\", self.conn)\n\n        tm.assert_frame_equal(self.test_frame3, result)\n\n    def test_to_sql_series(self):\n        s = Series(np.arange(5, dtype='int64'), name='series')\n        sql.to_sql(s, \"test_series\", self.conn, flavor='sqlite', index=False)\n        s2 = sql.read_sql_query(\"SELECT * FROM test_series\", self.conn)\n        tm.assert_frame_equal(s.to_frame(), s2)\n\n    def test_to_sql_panel(self):\n        panel = tm.makePanel()\n        self.assertRaises(NotImplementedError, sql.to_sql, panel,\n                          'test_panel', self.conn, flavor='sqlite')\n\n    def test_legacy_write_frame(self):\n        # Assume that functionality is already tested above so just do\n        # quick check that it basically works\n        with tm.assert_produces_warning(FutureWarning):\n            sql.write_frame(self.test_frame1, 'test_frame_legacy', self.conn,\n                            flavor='sqlite')\n\n        self.assertTrue(\n            sql.has_table('test_frame_legacy', self.conn, flavor='sqlite'),\n            'Table not written to DB')\n\n    def test_roundtrip(self):\n        sql.to_sql(self.test_frame1, 'test_frame_roundtrip',\n                   con=self.conn, flavor='sqlite')\n        result = sql.read_sql_query(\n            'SELECT * FROM test_frame_roundtrip',\n            con=self.conn)\n\n        # HACK!\n        result.index = self.test_frame1.index\n        result.set_index('level_0', inplace=True)\n        result.index.astype(int)\n        result.index.name = None\n        tm.assert_frame_equal(result, self.test_frame1)\n\n    def test_roundtrip_chunksize(self):\n        sql.to_sql(self.test_frame1, 'test_frame_roundtrip', con=self.conn,\n            index=False, flavor='sqlite', chunksize=2)\n        result = sql.read_sql_query(\n            'SELECT * FROM test_frame_roundtrip',\n            con=self.conn)\n        tm.assert_frame_equal(result, self.test_frame1)\n\n    def test_execute_sql(self):\n        # drop_sql = \"DROP TABLE IF EXISTS test\"  # should already be done\n        iris_results = sql.execute(\"SELECT * FROM iris\", con=self.conn)\n        row = iris_results.fetchone()\n        tm.equalContents(row, [5.1, 3.5, 1.4, 0.2, 'Iris-setosa'])\n\n    def test_date_parsing(self):\n        # Test date parsing in read_sq\n        # No Parsing\n        df = sql.read_sql_query(\"SELECT * FROM types_test_data\", self.conn)\n        self.assertFalse(\n            issubclass(df.DateCol.dtype.type, np.datetime64),\n            \"DateCol loaded with incorrect type\")\n\n        df = sql.read_sql_query(\"SELECT * FROM types_test_data\", self.conn,\n                                parse_dates=['DateCol'])\n        self.assertTrue(\n            issubclass(df.DateCol.dtype.type, np.datetime64),\n            \"DateCol loaded with incorrect type\")\n\n        df = sql.read_sql_query(\"SELECT * FROM types_test_data\", self.conn,\n                                parse_dates={'DateCol': '%Y-%m-%d %H:%M:%S'})\n        self.assertTrue(\n            issubclass(df.DateCol.dtype.type, np.datetime64),\n            \"DateCol loaded with incorrect type\")\n\n        df = sql.read_sql_query(\"SELECT * FROM types_test_data\", self.conn,\n                                parse_dates=['IntDateCol'])\n\n        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64),\n                        \"IntDateCol loaded with incorrect type\")\n\n        df = sql.read_sql_query(\"SELECT * FROM types_test_data\", self.conn,\n                                parse_dates={'IntDateCol': 's'})\n\n        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64),\n                        \"IntDateCol loaded with incorrect type\")\n\n    def test_date_and_index(self):\n        # Test case where same column appears in parse_date and index_col\n\n        df = sql.read_sql_query(\"SELECT * FROM types_test_data\", self.conn,\n                                index_col='DateCol',\n                                parse_dates=['DateCol', 'IntDateCol'])\n\n        self.assertTrue(issubclass(df.index.dtype.type, np.datetime64),\n                        \"DateCol loaded with incorrect type\")\n\n        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64),\n                        \"IntDateCol loaded with incorrect type\")\n\n    def test_timedelta(self):\n\n        # see #6921\n        df = to_timedelta(Series(['00:00:01', '00:00:03'], name='foo')).to_frame()\n        with tm.assert_produces_warning(UserWarning):\n            df.to_sql('test_timedelta', self.conn)\n        result = sql.read_sql_query('SELECT * FROM test_timedelta', self.conn)\n        tm.assert_series_equal(result['foo'], df['foo'].astype('int64'))\n\n    def test_complex(self):\n        df = DataFrame({'a':[1+1j, 2j]})\n        # Complex data type should raise error\n        self.assertRaises(ValueError, df.to_sql, 'test_complex', self.conn)\n\n    def test_to_sql_index_label(self):\n        temp_frame = DataFrame({'col1': range(4)})\n\n        # no index name, defaults to 'index'\n        sql.to_sql(temp_frame, 'test_index_label', self.conn)\n        frame = sql.read_sql_query('SELECT * FROM test_index_label', self.conn)\n        self.assertEqual(frame.columns[0], 'index')\n\n        # specifying index_label\n        sql.to_sql(temp_frame, 'test_index_label', self.conn,\n                   if_exists='replace', index_label='other_label')\n        frame = sql.read_sql_query('SELECT * FROM test_index_label', self.conn)\n        self.assertEqual(frame.columns[0], 'other_label',\n                         \"Specified index_label not written to database\")\n\n        # using the index name\n        temp_frame.index.name = 'index_name'\n        sql.to_sql(temp_frame, 'test_index_label', self.conn,\n                   if_exists='replace')\n        frame = sql.read_sql_query('SELECT * FROM test_index_label', self.conn)\n        self.assertEqual(frame.columns[0], 'index_name',\n                         \"Index name not written to database\")\n\n        # has index name, but specifying index_label\n        sql.to_sql(temp_frame, 'test_index_label', self.conn,\n                   if_exists='replace', index_label='other_label')\n        frame = sql.read_sql_query('SELECT * FROM test_index_label', self.conn)\n        self.assertEqual(frame.columns[0], 'other_label',\n                         \"Specified index_label not written to database\")\n\n    def test_to_sql_index_label_multiindex(self):\n        temp_frame = DataFrame({'col1': range(4)},\n            index=MultiIndex.from_product([('A0', 'A1'), ('B0', 'B1')]))\n\n        # no index name, defaults to 'level_0' and 'level_1'\n        sql.to_sql(temp_frame, 'test_index_label', self.conn)\n        frame = sql.read_sql_query('SELECT * FROM test_index_label', self.conn)\n        self.assertEqual(frame.columns[0], 'level_0')\n        self.assertEqual(frame.columns[1], 'level_1')\n\n        # specifying index_label\n        sql.to_sql(temp_frame, 'test_index_label', self.conn,\n                   if_exists='replace', index_label=['A', 'B'])\n        frame = sql.read_sql_query('SELECT * FROM test_index_label', self.conn)\n        self.assertEqual(frame.columns[:2].tolist(), ['A', 'B'],\n                         \"Specified index_labels not written to database\")\n\n        # using the index name\n        temp_frame.index.names = ['A', 'B']\n        sql.to_sql(temp_frame, 'test_index_label', self.conn,\n                   if_exists='replace')\n        frame = sql.read_sql_query('SELECT * FROM test_index_label', self.conn)\n        self.assertEqual(frame.columns[:2].tolist(), ['A', 'B'],\n                         \"Index names not written to database\")\n\n        # has index name, but specifying index_label\n        sql.to_sql(temp_frame, 'test_index_label', self.conn,\n                   if_exists='replace', index_label=['C', 'D'])\n        frame = sql.read_sql_query('SELECT * FROM test_index_label', self.conn)\n        self.assertEqual(frame.columns[:2].tolist(), ['C', 'D'],\n                         \"Specified index_labels not written to database\")\n\n        # wrong length of index_label\n        self.assertRaises(ValueError, sql.to_sql, temp_frame,\n                          'test_index_label', self.conn, if_exists='replace',\n                          index_label='C')\n\n    def test_multiindex_roundtrip(self):\n        df = DataFrame.from_records([(1,2.1,'line1'), (2,1.5,'line2')],\n                                    columns=['A','B','C'], index=['A','B'])\n\n        df.to_sql('test_multiindex_roundtrip', self.conn)\n        result = sql.read_sql_query('SELECT * FROM test_multiindex_roundtrip',\n                                    self.conn, index_col=['A','B'])\n        tm.assert_frame_equal(df, result, check_index_type=True)\n\n    def test_integer_col_names(self):\n        df = DataFrame([[1, 2], [3, 4]], columns=[0, 1])\n        sql.to_sql(df, \"test_frame_integer_col_names\", self.conn,\n                   if_exists='replace')\n\n    def test_get_schema(self):\n        create_sql = sql.get_schema(self.test_frame1, 'test', 'sqlite',\n                                    con=self.conn)\n        self.assertTrue('CREATE' in create_sql)\n\n    def test_get_schema_dtypes(self):\n        float_frame = DataFrame({'a':[1.1,1.2], 'b':[2.1,2.2]})\n        dtype = sqlalchemy.Integer if self.mode == 'sqlalchemy' else 'INTEGER'\n        create_sql = sql.get_schema(float_frame, 'test', 'sqlite',\n                                    con=self.conn, dtype={'b':dtype})\n        self.assertTrue('CREATE' in create_sql)\n        self.assertTrue('INTEGER' in create_sql)\n\n    def test_get_schema_keys(self):\n        frame = DataFrame({'Col1':[1.1,1.2], 'Col2':[2.1,2.2]})\n        create_sql = sql.get_schema(frame, 'test', 'sqlite',\n                                    con=self.conn, keys='Col1')\n        constraint_sentence = 'CONSTRAINT test_pk PRIMARY KEY (\"Col1\")'\n        self.assertTrue(constraint_sentence in create_sql)\n\n        # multiple columns as key (GH10385)\n        create_sql = sql.get_schema(self.test_frame1, 'test', 'sqlite',\n                                    con=self.conn, keys=['A', 'B'])\n        constraint_sentence = 'CONSTRAINT test_pk PRIMARY KEY (\"A\", \"B\")'\n        self.assertTrue(constraint_sentence in create_sql)\n\n    def test_chunksize_read(self):\n        df = DataFrame(np.random.randn(22, 5), columns=list('abcde'))\n        df.to_sql('test_chunksize', self.conn, index=False)\n\n        # reading the query in one time\n        res1 = sql.read_sql_query(\"select * from test_chunksize\", self.conn)\n\n        # reading the query in chunks with read_sql_query\n        res2 = DataFrame()\n        i = 0\n        sizes = [5, 5, 5, 5, 2]\n\n        for chunk in sql.read_sql_query(\"select * from test_chunksize\",\n                                        self.conn, chunksize=5):\n            res2 = concat([res2, chunk], ignore_index=True)\n            self.assertEqual(len(chunk), sizes[i])\n            i += 1\n\n        tm.assert_frame_equal(res1, res2)\n\n        # reading the query in chunks with read_sql_query\n        if self.mode == 'sqlalchemy':\n            res3 = DataFrame()\n            i = 0\n            sizes = [5, 5, 5, 5, 2]\n\n            for chunk in sql.read_sql_table(\"test_chunksize\", self.conn,\n                                            chunksize=5):\n                res3 = concat([res3, chunk], ignore_index=True)\n                self.assertEqual(len(chunk), sizes[i])\n                i += 1\n\n            tm.assert_frame_equal(res1, res3)\n\n    def test_categorical(self):\n        # GH8624\n        # test that categorical gets written correctly as dense column\n        df = DataFrame(\n            {'person_id': [1, 2, 3],\n             'person_name': ['John P. Doe', 'Jane Dove', 'John P. Doe']})\n        df2 = df.copy()\n        df2['person_name'] = df2['person_name'].astype('category')\n\n        df2.to_sql('test_categorical', self.conn, index=False)\n        res = sql.read_sql_query('SELECT * FROM test_categorical', self.conn)\n\n        tm.assert_frame_equal(res, df)\n\n\nclass TestSQLApi(SQLAlchemyMixIn, _TestSQLApi):\n    \"\"\"\n    Test the public API as it would be used directly\n\n    Tests for `read_sql_table` are included here, as this is specific for the\n    sqlalchemy mode.\n\n    \"\"\"\n    flavor = 'sqlite'\n    mode = 'sqlalchemy'\n\n    def connect(self):\n        if SQLALCHEMY_INSTALLED:\n            return sqlalchemy.create_engine('sqlite:///:memory:')\n        else:\n            raise nose.SkipTest('SQLAlchemy not installed')\n\n    def test_read_table_columns(self):\n        # test columns argument in read_table\n        sql.to_sql(self.test_frame1, 'test_frame', self.conn)\n\n        cols = ['A', 'B']\n        result = sql.read_sql_table('test_frame', self.conn, columns=cols)\n        self.assertEqual(result.columns.tolist(), cols,\n                         \"Columns not correctly selected\")\n\n    def test_read_table_index_col(self):\n        # test columns argument in read_table\n        sql.to_sql(self.test_frame1, 'test_frame', self.conn)\n\n        result = sql.read_sql_table('test_frame', self.conn, index_col=\"index\")\n        self.assertEqual(result.index.names, [\"index\"],\n                         \"index_col not correctly set\")\n\n        result = sql.read_sql_table('test_frame', self.conn, index_col=[\"A\", \"B\"])\n        self.assertEqual(result.index.names, [\"A\", \"B\"],\n                         \"index_col not correctly set\")\n\n        result = sql.read_sql_table('test_frame', self.conn, index_col=[\"A\", \"B\"],\n                                columns=[\"C\", \"D\"])\n        self.assertEqual(result.index.names, [\"A\", \"B\"],\n                         \"index_col not correctly set\")\n        self.assertEqual(result.columns.tolist(), [\"C\", \"D\"],\n                         \"columns not set correctly whith index_col\")\n\n    def test_read_sql_delegate(self):\n        iris_frame1 = sql.read_sql_query(\n            \"SELECT * FROM iris\", self.conn)\n        iris_frame2 = sql.read_sql(\n            \"SELECT * FROM iris\", self.conn)\n        tm.assert_frame_equal(iris_frame1, iris_frame2)\n\n        iris_frame1 = sql.read_sql_table('iris', self.conn)\n        iris_frame2 = sql.read_sql('iris', self.conn)\n        tm.assert_frame_equal(iris_frame1, iris_frame2)\n\n    def test_not_reflect_all_tables(self):\n        # create invalid table\n        qry = \"\"\"CREATE TABLE invalid (x INTEGER, y UNKNOWN);\"\"\"\n        self.conn.execute(qry)\n        qry = \"\"\"CREATE TABLE other_table (x INTEGER, y INTEGER);\"\"\"\n        self.conn.execute(qry)\n\n        with warnings.catch_warnings(record=True) as w:\n            # Cause all warnings to always be triggered.\n            warnings.simplefilter(\"always\")\n            # Trigger a warning.\n            sql.read_sql_table('other_table', self.conn)\n            sql.read_sql_query('SELECT * FROM other_table', self.conn)\n            # Verify some things\n            self.assertEqual(len(w), 0, \"Warning triggered for other table\")\n\n    def test_warning_case_insensitive_table_name(self):\n        # see GH7815.\n        # We can't test that this warning is triggered, a the database\n        # configuration would have to be altered. But here we test that\n        # the warning is certainly NOT triggered in a normal case.\n        with warnings.catch_warnings(record=True) as w:\n            # Cause all warnings to always be triggered.\n            warnings.simplefilter(\"always\")\n            # This should not trigger a Warning\n            self.test_frame1.to_sql('CaseSensitive', self.conn)\n            # Verify some things\n            self.assertEqual(len(w), 0, \"Warning triggered for writing a table\")\n\n    def _get_index_columns(self, tbl_name):\n        from sqlalchemy.engine import reflection\n        insp = reflection.Inspector.from_engine(self.conn)\n        ixs = insp.get_indexes('test_index_saved')\n        ixs = [i['column_names'] for i in ixs]\n        return ixs\n\n    def test_sqlalchemy_type_mapping(self):\n\n        # Test Timestamp objects (no datetime64 because of timezone) (GH9085)\n        df = DataFrame({'time': to_datetime(['201412120154', '201412110254'],\n                                            utc=True)})\n        db = sql.SQLDatabase(self.conn)\n        table = sql.SQLTable(\"test_type\", db, frame=df)\n        self.assertTrue(isinstance(table.table.c['time'].type, sqltypes.DateTime))\n\n    def test_to_sql_read_sql_with_database_uri(self):\n\n        # Test read_sql and .to_sql method with a database URI (GH10654)\n        test_frame1 = self.test_frame1\n        #db_uri = 'sqlite:///:memory:' # raises sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) near \"iris\": syntax error [SQL: 'iris']\n        with tm.ensure_clean() as name:\n            db_uri = 'sqlite:///' + name\n            table = 'iris'\n            test_frame1.to_sql(table, db_uri, if_exists='replace', index=False)\n            test_frame2 = sql.read_sql(table, db_uri)\n            test_frame3 = sql.read_sql_table(table, db_uri)\n            query = 'SELECT * FROM iris'\n            test_frame4 = sql.read_sql_query(query, db_uri)\n        tm.assert_frame_equal(test_frame1, test_frame2)\n        tm.assert_frame_equal(test_frame1, test_frame3)\n        tm.assert_frame_equal(test_frame1, test_frame4)\n\n    def _make_iris_table_metadata(self):\n        sa = sqlalchemy\n        metadata = sa.MetaData()\n        iris = sa.Table('iris', metadata,\n            sa.Column('SepalLength', sa.REAL),\n            sa.Column('SepalWidth', sa.REAL),\n            sa.Column('PetalLength', sa.REAL),\n            sa.Column('PetalWidth', sa.REAL),\n            sa.Column('Name', sa.TEXT)\n        )\n\n        return iris\n\n    def test_query_by_text_obj(self):\n        # WIP : GH10846\n        name_text = sqlalchemy.text('select * from iris where name=:name')\n        iris_df = sql.read_sql(name_text, self.conn, params={'name': 'Iris-versicolor'})\n        all_names = set(iris_df['Name'])\n        self.assertEqual(all_names, set(['Iris-versicolor']))\n\n    def test_query_by_select_obj(self):\n        # WIP : GH10846\n        iris = self._make_iris_table_metadata()\n\n        name_select = sqlalchemy.select([iris]).where(iris.c.Name == sqlalchemy.bindparam('name'))\n        iris_df = sql.read_sql(name_select, self.conn, params={'name': 'Iris-setosa'})\n        all_names = set(iris_df['Name'])\n        self.assertEqual(all_names, set(['Iris-setosa']))\n\n\nclass _EngineToConnMixin(object):\n    \"\"\"\n    A mixin that causes setup_connect to create a conn rather than an engine.\n    \"\"\"\n\n    def setUp(self):\n        super(_EngineToConnMixin, self).setUp()\n        engine = self.conn\n        conn = engine.connect()\n        self.__tx = conn.begin()\n        self.pandasSQL = sql.SQLDatabase(conn)\n        self.__engine = engine\n        self.conn = conn\n\n    def tearDown(self):\n        self.__tx.rollback()\n        self.conn.close()\n        self.conn = self.__engine\n        self.pandasSQL = sql.SQLDatabase(self.__engine)\n        super(_EngineToConnMixin, self).tearDown()\n\n\nclass TestSQLApiConn(_EngineToConnMixin, TestSQLApi):\n    pass\n\n\nclass TestSQLiteFallbackApi(SQLiteMixIn, _TestSQLApi):\n    \"\"\"\n    Test the public sqlite connection fallback API\n\n    \"\"\"\n    flavor = 'sqlite'\n    mode = 'fallback'\n\n    def connect(self, database=\":memory:\"):\n        return sqlite3.connect(database)\n\n    def test_sql_open_close(self):\n        # Test if the IO in the database still work if the connection closed\n        # between the writing and reading (as in many real situations).\n\n        with tm.ensure_clean() as name:\n\n            conn = self.connect(name)\n            sql.to_sql(self.test_frame3, \"test_frame3_legacy\", conn,\n                       flavor=\"sqlite\", index=False)\n            conn.close()\n\n            conn = self.connect(name)\n            result = sql.read_sql_query(\"SELECT * FROM test_frame3_legacy;\",\n                                        conn)\n            conn.close()\n\n        tm.assert_frame_equal(self.test_frame3, result)\n\n    def test_read_sql_delegate(self):\n        iris_frame1 = sql.read_sql_query(\"SELECT * FROM iris\", self.conn)\n        iris_frame2 = sql.read_sql(\"SELECT * FROM iris\", self.conn)\n        tm.assert_frame_equal(iris_frame1, iris_frame2)\n\n        self.assertRaises(sql.DatabaseError, sql.read_sql, 'iris', self.conn)\n\n    def test_safe_names_warning(self):\n        # GH 6798\n        df = DataFrame([[1, 2], [3, 4]], columns=['a', 'b '])  # has a space\n        # warns on create table with spaces in names\n        with tm.assert_produces_warning():\n            sql.to_sql(df, \"test_frame3_legacy\", self.conn,\n                       flavor=\"sqlite\", index=False)\n\n    def test_get_schema2(self):\n        # without providing a connection object (available for backwards comp)\n        create_sql = sql.get_schema(self.test_frame1, 'test', 'sqlite')\n        self.assertTrue('CREATE' in create_sql)\n\n    def test_tquery(self):\n        with tm.assert_produces_warning(FutureWarning):\n            iris_results = sql.tquery(\"SELECT * FROM iris\", con=self.conn)\n        row = iris_results[0]\n        tm.equalContents(row, [5.1, 3.5, 1.4, 0.2, 'Iris-setosa'])\n\n    def test_uquery(self):\n        with tm.assert_produces_warning(FutureWarning):\n            rows = sql.uquery(\"SELECT * FROM iris LIMIT 1\", con=self.conn)\n        self.assertEqual(rows, -1)\n\n    def _get_sqlite_column_type(self, schema, column):\n\n        for col in schema.split('\\n'):\n            if col.split()[0].strip('\"\"') == column:\n                return col.split()[1]\n        raise ValueError('Column %s not found' % (column))\n\n    def test_sqlite_type_mapping(self):\n\n        # Test Timestamp objects (no datetime64 because of timezone) (GH9085)\n        df = DataFrame({'time': to_datetime(['201412120154', '201412110254'],\n                                            utc=True)})\n        db = sql.SQLiteDatabase(self.conn, self.flavor)\n        table = sql.SQLiteTable(\"test_type\", db, frame=df)\n        schema = table.sql_schema()\n        self.assertEqual(self._get_sqlite_column_type(schema, 'time'),\n                         \"TIMESTAMP\")\n\n\n#------------------------------------------------------------------------------\n#--- Database flavor specific tests\n\n\nclass _TestSQLAlchemy(SQLAlchemyMixIn, PandasSQLTest):\n    \"\"\"\n    Base class for testing the sqlalchemy backend.\n\n    Subclasses for specific database types are created below. Tests that\n    deviate for each flavor are overwritten there.\n\n    \"\"\"\n    flavor = None\n\n    @classmethod\n    def setUpClass(cls):\n        cls.setup_import()\n        cls.setup_driver()\n\n        # test connection\n        try:\n            conn = cls.connect()\n            conn.connect()\n        except sqlalchemy.exc.OperationalError:\n            msg = \"{0} - can't connect to {1} server\".format(cls, cls.flavor)\n            raise nose.SkipTest(msg)\n\n    def setUp(self):\n        self.setup_connect()\n\n        self._load_iris_data()\n        self._load_raw_sql()\n        self._load_test1_data()\n\n    @classmethod\n    def setup_import(cls):\n        # Skip this test if SQLAlchemy not available\n        if not SQLALCHEMY_INSTALLED:\n            raise nose.SkipTest('SQLAlchemy not installed')\n\n    @classmethod\n    def setup_driver(cls):\n        raise NotImplementedError()\n\n    @classmethod\n    def connect(cls):\n        raise NotImplementedError()\n\n    def setup_connect(self):\n        try:\n            self.conn = self.connect()\n            self.pandasSQL = sql.SQLDatabase(self.conn)\n            # to test if connection can be made:\n            self.conn.connect()\n        except sqlalchemy.exc.OperationalError:\n            raise nose.SkipTest(\"Can't connect to {0} server\".format(self.flavor))\n\n    def test_aread_sql(self):\n        self._read_sql_iris()\n\n    def test_read_sql_parameter(self):\n        self._read_sql_iris_parameter()\n\n    def test_read_sql_named_parameter(self):\n        self._read_sql_iris_named_parameter()\n\n    def test_to_sql(self):\n        self._to_sql()\n\n    def test_to_sql_empty(self):\n        self._to_sql_empty()\n\n    def test_to_sql_fail(self):\n        self._to_sql_fail()\n\n    def test_to_sql_replace(self):\n        self._to_sql_replace()\n\n    def test_to_sql_append(self):\n        self._to_sql_append()\n\n    def test_create_table(self):\n        temp_conn = self.connect()\n        temp_frame = DataFrame(\n            {'one': [1., 2., 3., 4.], 'two': [4., 3., 2., 1.]})\n\n        pandasSQL = sql.SQLDatabase(temp_conn)\n        pandasSQL.to_sql(temp_frame, 'temp_frame')\n\n        self.assertTrue(\n            temp_conn.has_table('temp_frame'), 'Table not written to DB')\n\n    def test_drop_table(self):\n        temp_conn = self.connect()\n\n        temp_frame = DataFrame(\n            {'one': [1., 2., 3., 4.], 'two': [4., 3., 2., 1.]})\n\n        pandasSQL = sql.SQLDatabase(temp_conn)\n        pandasSQL.to_sql(temp_frame, 'temp_frame')\n\n        self.assertTrue(\n            temp_conn.has_table('temp_frame'), 'Table not written to DB')\n\n        pandasSQL.drop_table('temp_frame')\n\n        self.assertFalse(\n            temp_conn.has_table('temp_frame'), 'Table not deleted from DB')\n\n    def test_roundtrip(self):\n        self._roundtrip()\n\n    def test_execute_sql(self):\n        self._execute_sql()\n\n    def test_read_table(self):\n        iris_frame = sql.read_sql_table(\"iris\", con=self.conn)\n        self._check_iris_loaded_frame(iris_frame)\n\n    def test_read_table_columns(self):\n        iris_frame = sql.read_sql_table(\n            \"iris\", con=self.conn, columns=['SepalLength', 'SepalLength'])\n        tm.equalContents(\n            iris_frame.columns.values, ['SepalLength', 'SepalLength'])\n\n    def test_read_table_absent(self):\n        self.assertRaises(\n            ValueError, sql.read_sql_table, \"this_doesnt_exist\", con=self.conn)\n\n    def test_default_type_conversion(self):\n        df = sql.read_sql_table(\"types_test_data\", self.conn)\n\n        self.assertTrue(issubclass(df.FloatCol.dtype.type, np.floating),\n                        \"FloatCol loaded with incorrect type\")\n        self.assertTrue(issubclass(df.IntCol.dtype.type, np.integer),\n                        \"IntCol loaded with incorrect type\")\n        self.assertTrue(issubclass(df.BoolCol.dtype.type, np.bool_),\n                        \"BoolCol loaded with incorrect type\")\n\n        # Int column with NA values stays as float\n        self.assertTrue(issubclass(df.IntColWithNull.dtype.type, np.floating),\n                        \"IntColWithNull loaded with incorrect type\")\n        # Bool column with NA values becomes object\n        self.assertTrue(issubclass(df.BoolColWithNull.dtype.type, np.object),\n                        \"BoolColWithNull loaded with incorrect type\")\n\n    def test_bigint(self):\n        # int64 should be converted to BigInteger, GH7433\n        df = DataFrame(data={'i64':[2**62]})\n        df.to_sql('test_bigint', self.conn, index=False)\n        result = sql.read_sql_table('test_bigint', self.conn)\n\n        tm.assert_frame_equal(df, result)\n\n    def test_default_date_load(self):\n        df = sql.read_sql_table(\"types_test_data\", self.conn)\n\n        # IMPORTANT - sqlite has no native date type, so shouldn't parse, but\n        # MySQL SHOULD be converted.\n        self.assertTrue(issubclass(df.DateCol.dtype.type, np.datetime64),\n                        \"DateCol loaded with incorrect type\")\n\n    def test_date_parsing(self):\n        # No Parsing\n        df = sql.read_sql_table(\"types_test_data\", self.conn)\n\n        df = sql.read_sql_table(\"types_test_data\", self.conn,\n                                parse_dates=['DateCol'])\n        self.assertTrue(issubclass(df.DateCol.dtype.type, np.datetime64),\n                        \"DateCol loaded with incorrect type\")\n\n        df = sql.read_sql_table(\"types_test_data\", self.conn,\n                                parse_dates={'DateCol': '%Y-%m-%d %H:%M:%S'})\n        self.assertTrue(issubclass(df.DateCol.dtype.type, np.datetime64),\n                        \"DateCol loaded with incorrect type\")\n\n        df = sql.read_sql_table(\"types_test_data\", self.conn, parse_dates={\n                            'DateCol': {'format': '%Y-%m-%d %H:%M:%S'}})\n        self.assertTrue(issubclass(df.DateCol.dtype.type, np.datetime64),\n                        \"IntDateCol loaded with incorrect type\")\n\n        df = sql.read_sql_table(\n            \"types_test_data\", self.conn, parse_dates=['IntDateCol'])\n        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64),\n                        \"IntDateCol loaded with incorrect type\")\n\n        df = sql.read_sql_table(\n            \"types_test_data\", self.conn, parse_dates={'IntDateCol': 's'})\n        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64),\n                        \"IntDateCol loaded with incorrect type\")\n\n        df = sql.read_sql_table(\n            \"types_test_data\", self.conn, parse_dates={'IntDateCol': {'unit': 's'}})\n        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64),\n                        \"IntDateCol loaded with incorrect type\")\n\n    def test_datetime(self):\n        df = DataFrame({'A': date_range('2013-01-01 09:00:00', periods=3),\n                        'B': np.arange(3.0)})\n        df.to_sql('test_datetime', self.conn)\n\n        # with read_table -> type information from schema used\n        result = sql.read_sql_table('test_datetime', self.conn)\n        result = result.drop('index', axis=1)\n        tm.assert_frame_equal(result, df)\n\n        # with read_sql -> no type information -> sqlite has no native\n        result = sql.read_sql_query('SELECT * FROM test_datetime', self.conn)\n        result = result.drop('index', axis=1)\n        if self.flavor == 'sqlite':\n            self.assertTrue(isinstance(result.loc[0, 'A'], string_types))\n            result['A'] = to_datetime(result['A'])\n            tm.assert_frame_equal(result, df)\n        else:\n            tm.assert_frame_equal(result, df)\n\n    def test_datetime_NaT(self):\n        df = DataFrame({'A': date_range('2013-01-01 09:00:00', periods=3),\n                        'B': np.arange(3.0)})\n        df.loc[1, 'A'] = np.nan\n        df.to_sql('test_datetime', self.conn, index=False)\n\n        # with read_table -> type information from schema used\n        result = sql.read_sql_table('test_datetime', self.conn)\n        tm.assert_frame_equal(result, df)\n\n        # with read_sql -> no type information -> sqlite has no native\n        result = sql.read_sql_query('SELECT * FROM test_datetime', self.conn)\n        if self.flavor == 'sqlite':\n            self.assertTrue(isinstance(result.loc[0, 'A'], string_types))\n            result['A'] = to_datetime(result['A'], errors='coerce')\n            tm.assert_frame_equal(result, df)\n        else:\n            tm.assert_frame_equal(result, df)\n\n    def test_datetime_date(self):\n        # test support for datetime.date\n        df = DataFrame([date(2014, 1, 1), date(2014, 1, 2)], columns=[\"a\"])\n        df.to_sql('test_date', self.conn, index=False)\n        res = read_sql_table('test_date', self.conn)\n        # comes back as datetime64\n        tm.assert_series_equal(res['a'], to_datetime(df['a']))\n\n    def test_datetime_time(self):\n        # test support for datetime.time\n        df = DataFrame([time(9, 0, 0), time(9, 1, 30)], columns=[\"a\"])\n        df.to_sql('test_time', self.conn, index=False)\n        res = read_sql_table('test_time', self.conn)\n        tm.assert_frame_equal(res, df)\n\n    def test_mixed_dtype_insert(self):\n        # see GH6509\n        s1 = Series(2**25 + 1,dtype=np.int32)\n        s2 = Series(0.0,dtype=np.float32)\n        df = DataFrame({'s1': s1, 's2': s2})\n\n        # write and read again\n        df.to_sql(\"test_read_write\", self.conn, index=False)\n        df2 = sql.read_sql_table(\"test_read_write\", self.conn)\n\n        tm.assert_frame_equal(df, df2, check_dtype=False, check_exact=True)\n\n    def test_nan_numeric(self):\n        # NaNs in numeric float column\n        df = DataFrame({'A':[0, 1, 2], 'B':[0.2, np.nan, 5.6]})\n        df.to_sql('test_nan', self.conn, index=False)\n\n        # with read_table\n        result = sql.read_sql_table('test_nan', self.conn)\n        tm.assert_frame_equal(result, df)\n\n        # with read_sql\n        result = sql.read_sql_query('SELECT * FROM test_nan', self.conn)\n        tm.assert_frame_equal(result, df)\n\n    def test_nan_fullcolumn(self):\n        # full NaN column (numeric float column)\n        df = DataFrame({'A':[0, 1, 2], 'B':[np.nan, np.nan, np.nan]})\n        df.to_sql('test_nan', self.conn, index=False)\n\n        # with read_table\n        result = sql.read_sql_table('test_nan', self.conn)\n        tm.assert_frame_equal(result, df)\n\n        # with read_sql -> not type info from table -> stays None\n        df['B'] = df['B'].astype('object')\n        df['B'] = None\n        result = sql.read_sql_query('SELECT * FROM test_nan', self.conn)\n        tm.assert_frame_equal(result, df)\n\n    def test_nan_string(self):\n        # NaNs in string column\n        df = DataFrame({'A':[0, 1, 2], 'B':['a', 'b', np.nan]})\n        df.to_sql('test_nan', self.conn, index=False)\n\n        # NaNs are coming back as None\n        df.loc[2, 'B'] = None\n\n        # with read_table\n        result = sql.read_sql_table('test_nan', self.conn)\n        tm.assert_frame_equal(result, df)\n\n        # with read_sql\n        result = sql.read_sql_query('SELECT * FROM test_nan', self.conn)\n        tm.assert_frame_equal(result, df)\n\n    def _get_index_columns(self, tbl_name):\n        from sqlalchemy.engine import reflection\n        insp = reflection.Inspector.from_engine(self.conn)\n        ixs = insp.get_indexes(tbl_name)\n        ixs = [i['column_names'] for i in ixs]\n        return ixs\n\n    def test_to_sql_save_index(self):\n        self._to_sql_save_index()\n\n    def test_transactions(self):\n        self._transaction_test()\n\n    def test_get_schema_create_table(self):\n        # Use a dataframe without a bool column, since MySQL converts bool to\n        # TINYINT (which read_sql_table returns as an int and causes a dtype\n        # mismatch)\n\n        self._load_test3_data()\n        tbl = 'test_get_schema_create_table'\n        create_sql = sql.get_schema(self.test_frame3, tbl, con=self.conn)\n        blank_test_df = self.test_frame3.iloc[:0]\n\n        self.drop_table(tbl)\n        self.conn.execute(create_sql)\n        returned_df = sql.read_sql_table(tbl, self.conn)\n        tm.assert_frame_equal(returned_df, blank_test_df)\n        self.drop_table(tbl)\n\n    def test_dtype(self):\n        cols = ['A', 'B']\n        data = [(0.8, True),\n                (0.9, None)]\n        df = DataFrame(data, columns=cols)\n        df.to_sql('dtype_test', self.conn)\n        df.to_sql('dtype_test2', self.conn, dtype={'B': sqlalchemy.TEXT})\n        meta = sqlalchemy.schema.MetaData(bind=self.conn)\n        meta.reflect()\n        sqltype = meta.tables['dtype_test2'].columns['B'].type\n        self.assertTrue(isinstance(sqltype, sqlalchemy.TEXT))\n        self.assertRaises(ValueError, df.to_sql,\n                          'error', self.conn, dtype={'B': str})\n\n        # GH9083\n        df.to_sql('dtype_test3', self.conn, dtype={'B': sqlalchemy.String(10)})\n        meta.reflect()\n        sqltype = meta.tables['dtype_test3'].columns['B'].type\n        self.assertTrue(isinstance(sqltype, sqlalchemy.String))\n        self.assertEqual(sqltype.length, 10)\n\n    def test_notnull_dtype(self):\n        cols = {'Bool': Series([True,None]),\n                'Date': Series([datetime(2012, 5, 1), None]),\n                'Int' : Series([1, None], dtype='object'),\n                'Float': Series([1.1, None])\n               }\n        df = DataFrame(cols)\n\n        tbl = 'notnull_dtype_test'\n        df.to_sql(tbl, self.conn)\n        returned_df = sql.read_sql_table(tbl, self.conn)\n        meta = sqlalchemy.schema.MetaData(bind=self.conn)\n        meta.reflect()\n        if self.flavor == 'mysql':\n            my_type = sqltypes.Integer\n        else:\n            my_type = sqltypes.Boolean\n\n        col_dict = meta.tables[tbl].columns\n\n        self.assertTrue(isinstance(col_dict['Bool'].type, my_type))\n        self.assertTrue(isinstance(col_dict['Date'].type, sqltypes.DateTime))\n        self.assertTrue(isinstance(col_dict['Int'].type, sqltypes.Integer))\n        self.assertTrue(isinstance(col_dict['Float'].type, sqltypes.Float))\n\n    def test_double_precision(self):\n        V = 1.23456789101112131415\n\n        df = DataFrame({'f32':Series([V,], dtype='float32'),\n                        'f64':Series([V,], dtype='float64'),\n                        'f64_as_f32':Series([V,], dtype='float64'),\n                        'i32':Series([5,], dtype='int32'),\n                        'i64':Series([5,], dtype='int64'),\n                        })\n\n        df.to_sql('test_dtypes', self.conn, index=False, if_exists='replace',\n            dtype={'f64_as_f32':sqlalchemy.Float(precision=23)})\n        res = sql.read_sql_table('test_dtypes', self.conn)\n\n        # check precision of float64\n        self.assertEqual(np.round(df['f64'].iloc[0],14),\n                         np.round(res['f64'].iloc[0],14))\n\n        # check sql types\n        meta = sqlalchemy.schema.MetaData(bind=self.conn)\n        meta.reflect()\n        col_dict = meta.tables['test_dtypes'].columns\n        self.assertEqual(str(col_dict['f32'].type),\n                         str(col_dict['f64_as_f32'].type))\n        self.assertTrue(isinstance(col_dict['f32'].type, sqltypes.Float))\n        self.assertTrue(isinstance(col_dict['f64'].type, sqltypes.Float))\n        self.assertTrue(isinstance(col_dict['i32'].type, sqltypes.Integer))\n        self.assertTrue(isinstance(col_dict['i64'].type, sqltypes.BigInteger))\n\n    def test_connectable_issue_example(self):\n        # This tests the example raised in issue\n        # https://github.com/pydata/pandas/issues/10104\n\n        def foo(connection):\n            query = 'SELECT test_foo_data FROM test_foo_data'\n            return sql.read_sql_query(query, con=connection)\n\n        def bar(connection, data):\n            data.to_sql(name='test_foo_data', con=connection, if_exists='append')\n\n        def main(connectable):\n            with connectable.connect() as conn:\n                with conn.begin():\n                    foo_data = conn.run_callable(foo)\n                    conn.run_callable(bar, foo_data)\n\n        DataFrame({'test_foo_data': [0, 1, 2]}).to_sql('test_foo_data', self.conn)\n        main(self.conn)\n\n    def test_temporary_table(self):\n        test_data = u'Hello, World!'\n        expected = DataFrame({'spam': [test_data]})\n        Base = declarative.declarative_base()\n\n        class Temporary(Base):\n            __tablename__ = 'temp_test'\n            __table_args__ = {'prefixes': ['TEMPORARY']}\n            id = sqlalchemy.Column(sqlalchemy.Integer, primary_key=True)\n            spam = sqlalchemy.Column(sqlalchemy.Unicode(30), nullable=False)\n\n        Session = sa_session.sessionmaker(bind=self.conn)\n        session = Session()\n        with session.transaction:\n            conn = session.connection()\n            Temporary.__table__.create(conn)\n            session.add(Temporary(spam=test_data))\n            session.flush()\n            df = sql.read_sql_query(\n                sql=sqlalchemy.select([Temporary.spam]),\n                con=conn,\n            )\n\n        tm.assert_frame_equal(df, expected)\n\n\nclass _TestSQLAlchemyConn(_EngineToConnMixin, _TestSQLAlchemy):\n    def test_transactions(self):\n        raise nose.SkipTest(\"Nested transactions rollbacks don't work with Pandas\")\n\n\nclass _TestSQLiteAlchemy(object):\n    \"\"\"\n    Test the sqlalchemy backend against an in-memory sqlite database.\n\n    \"\"\"\n    flavor = 'sqlite'\n\n    @classmethod\n    def connect(cls):\n        return sqlalchemy.create_engine('sqlite:///:memory:')\n\n    @classmethod\n    def setup_driver(cls):\n        # sqlite3 is built-in\n        cls.driver = None\n\n    def test_default_type_conversion(self):\n        df = sql.read_sql_table(\"types_test_data\", self.conn)\n\n        self.assertTrue(issubclass(df.FloatCol.dtype.type, np.floating),\n                        \"FloatCol loaded with incorrect type\")\n        self.assertTrue(issubclass(df.IntCol.dtype.type, np.integer),\n                        \"IntCol loaded with incorrect type\")\n        # sqlite has no boolean type, so integer type is returned\n        self.assertTrue(issubclass(df.BoolCol.dtype.type, np.integer),\n                        \"BoolCol loaded with incorrect type\")\n\n        # Int column with NA values stays as float\n        self.assertTrue(issubclass(df.IntColWithNull.dtype.type, np.floating),\n                        \"IntColWithNull loaded with incorrect type\")\n        # Non-native Bool column with NA values stays as float\n        self.assertTrue(issubclass(df.BoolColWithNull.dtype.type, np.floating),\n                        \"BoolColWithNull loaded with incorrect type\")\n\n    def test_default_date_load(self):\n        df = sql.read_sql_table(\"types_test_data\", self.conn)\n\n        # IMPORTANT - sqlite has no native date type, so shouldn't parse, but\n        self.assertFalse(issubclass(df.DateCol.dtype.type, np.datetime64),\n                         \"DateCol loaded with incorrect type\")\n\n    def test_bigint_warning(self):\n        # test no warning for BIGINT (to support int64) is raised (GH7433)\n        df = DataFrame({'a':[1,2]}, dtype='int64')\n        df.to_sql('test_bigintwarning', self.conn, index=False)\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            sql.read_sql_table('test_bigintwarning', self.conn)\n            self.assertEqual(len(w), 0, \"Warning triggered for other table\")\n\n\nclass _TestMySQLAlchemy(object):\n    \"\"\"\n    Test the sqlalchemy backend against an MySQL database.\n\n    \"\"\"\n    flavor = 'mysql'\n\n    @classmethod\n    def connect(cls):\n        url = 'mysql+{driver}://root@localhost/pandas_nosetest'\n        return sqlalchemy.create_engine(url.format(driver=cls.driver))\n\n    @classmethod\n    def setup_driver(cls):\n        try:\n            import pymysql\n            cls.driver = 'pymysql'\n        except ImportError:\n            raise nose.SkipTest('pymysql not installed')\n\n    def test_default_type_conversion(self):\n        df = sql.read_sql_table(\"types_test_data\", self.conn)\n\n        self.assertTrue(issubclass(df.FloatCol.dtype.type, np.floating),\n                        \"FloatCol loaded with incorrect type\")\n        self.assertTrue(issubclass(df.IntCol.dtype.type, np.integer),\n                        \"IntCol loaded with incorrect type\")\n        # MySQL has no real BOOL type (it's an alias for TINYINT)\n        self.assertTrue(issubclass(df.BoolCol.dtype.type, np.integer),\n                        \"BoolCol loaded with incorrect type\")\n\n        # Int column with NA values stays as float\n        self.assertTrue(issubclass(df.IntColWithNull.dtype.type, np.floating),\n                        \"IntColWithNull loaded with incorrect type\")\n        # Bool column with NA = int column with NA values => becomes float\n        self.assertTrue(issubclass(df.BoolColWithNull.dtype.type, np.floating),\n                        \"BoolColWithNull loaded with incorrect type\")\n\n    def test_read_procedure(self):\n        # see GH7324. Although it is more an api test, it is added to the\n        # mysql tests as sqlite does not have stored procedures\n        df = DataFrame({'a': [1, 2, 3], 'b':[0.1, 0.2, 0.3]})\n        df.to_sql('test_procedure', self.conn, index=False)\n\n        proc = \"\"\"DROP PROCEDURE IF EXISTS get_testdb;\n\n        CREATE PROCEDURE get_testdb ()\n\n        BEGIN\n            SELECT * FROM test_procedure;\n        END\"\"\"\n\n        connection = self.conn.connect()\n        trans = connection.begin()\n        try:\n            r1 = connection.execute(proc)\n            trans.commit()\n        except:\n            trans.rollback()\n            raise\n\n        res1 = sql.read_sql_query(\"CALL get_testdb();\", self.conn)\n        tm.assert_frame_equal(df, res1)\n\n        # test delegation to read_sql_query\n        res2 = sql.read_sql(\"CALL get_testdb();\", self.conn)\n        tm.assert_frame_equal(df, res2)\n\n\nclass _TestPostgreSQLAlchemy(object):\n    \"\"\"\n    Test the sqlalchemy backend against an PostgreSQL database.\n\n    \"\"\"\n    flavor = 'postgresql'\n\n    @classmethod\n    def connect(cls):\n        url = 'postgresql+{driver}://postgres@localhost/pandas_nosetest'\n        return sqlalchemy.create_engine(url.format(driver=cls.driver))\n\n    @classmethod\n    def setup_driver(cls):\n        try:\n            import psycopg2\n            cls.driver = 'psycopg2'\n        except ImportError:\n            raise nose.SkipTest('psycopg2 not installed')\n\n    def test_schema_support(self):\n        # only test this for postgresql (schema's not supported in mysql/sqlite)\n        df = DataFrame({'col1':[1, 2], 'col2':[0.1, 0.2], 'col3':['a', 'n']})\n\n        # create a schema\n        self.conn.execute(\"DROP SCHEMA IF EXISTS other CASCADE;\")\n        self.conn.execute(\"CREATE SCHEMA other;\")\n\n        # write dataframe to different schema's\n        df.to_sql('test_schema_public', self.conn, index=False)\n        df.to_sql('test_schema_public_explicit', self.conn, index=False,\n                  schema='public')\n        df.to_sql('test_schema_other', self.conn, index=False, schema='other')\n\n        # read dataframes back in\n        res1 = sql.read_sql_table('test_schema_public', self.conn)\n        tm.assert_frame_equal(df, res1)\n        res2 = sql.read_sql_table('test_schema_public_explicit', self.conn)\n        tm.assert_frame_equal(df, res2)\n        res3 = sql.read_sql_table('test_schema_public_explicit', self.conn,\n                                  schema='public')\n        tm.assert_frame_equal(df, res3)\n        res4 = sql.read_sql_table('test_schema_other', self.conn,\n                                  schema='other')\n        tm.assert_frame_equal(df, res4)\n        self.assertRaises(ValueError, sql.read_sql_table, 'test_schema_other',\n                          self.conn, schema='public')\n\n        ## different if_exists options\n\n        # create a schema\n        self.conn.execute(\"DROP SCHEMA IF EXISTS other CASCADE;\")\n        self.conn.execute(\"CREATE SCHEMA other;\")\n\n        # write dataframe with different if_exists options\n        df.to_sql('test_schema_other', self.conn, schema='other', index=False)\n        df.to_sql('test_schema_other', self.conn, schema='other', index=False,\n                  if_exists='replace')\n        df.to_sql('test_schema_other', self.conn, schema='other', index=False,\n                  if_exists='append')\n        res = sql.read_sql_table('test_schema_other', self.conn, schema='other')\n        tm.assert_frame_equal(concat([df, df], ignore_index=True), res)\n\n        ## specifying schema in user-provided meta\n\n        # The schema won't be applied on another Connection\n        # because of transactional schemas\n        if isinstance(self.conn, sqlalchemy.engine.Engine):\n            engine2 = self.connect()\n            meta = sqlalchemy.MetaData(engine2, schema='other')\n            pdsql = sql.SQLDatabase(engine2, meta=meta)\n            pdsql.to_sql(df, 'test_schema_other2', index=False)\n            pdsql.to_sql(df, 'test_schema_other2', index=False, if_exists='replace')\n            pdsql.to_sql(df, 'test_schema_other2', index=False, if_exists='append')\n            res1 = sql.read_sql_table('test_schema_other2', self.conn, schema='other')\n            res2 = pdsql.read_table('test_schema_other2')\n            tm.assert_frame_equal(res1, res2)\n\n    def test_datetime_with_time_zone(self):\n\n        # Test to see if we read the date column with timezones that\n        # the timezone information is converted to utc and into a\n        # np.datetime64 (GH #7139)\n\n        df = sql.read_sql_table(\"types_test_data\", self.conn)\n        self.assertTrue(issubclass(df.DateColWithTz.dtype.type, np.datetime64),\n                        \"DateColWithTz loaded with incorrect type -> {0}\".format(df.DateColWithTz.dtype))\n\n        # \"2000-01-01 00:00:00-08:00\" should convert to \"2000-01-01 08:00:00\"\n        self.assertEqual(df.DateColWithTz[0], Timestamp('2000-01-01 08:00:00'))\n\n        # \"2000-06-01 00:00:00-07:00\" should convert to \"2000-06-01 07:00:00\"\n        self.assertEqual(df.DateColWithTz[1], Timestamp('2000-06-01 07:00:00'))\n\n\nclass TestMySQLAlchemy(_TestMySQLAlchemy, _TestSQLAlchemy):\n    pass\n\n\nclass TestMySQLAlchemyConn(_TestMySQLAlchemy, _TestSQLAlchemyConn):\n    pass\n\n\nclass TestPostgreSQLAlchemy(_TestPostgreSQLAlchemy, _TestSQLAlchemy):\n    pass\n\n\nclass TestPostgreSQLAlchemyConn(_TestPostgreSQLAlchemy, _TestSQLAlchemyConn):\n    pass\n\n\nclass TestSQLiteAlchemy(_TestSQLiteAlchemy, _TestSQLAlchemy):\n    pass\n\n\nclass TestSQLiteAlchemyConn(_TestSQLiteAlchemy, _TestSQLAlchemyConn):\n    pass\n\n\n#------------------------------------------------------------------------------\n#--- Test Sqlite / MySQL fallback\n\nclass TestSQLiteFallback(SQLiteMixIn, PandasSQLTest):\n    \"\"\"\n    Test the fallback mode against an in-memory sqlite database.\n\n    \"\"\"\n    flavor = 'sqlite'\n\n    @classmethod\n    def connect(cls):\n        return sqlite3.connect(':memory:')\n\n    def setUp(self):\n        self.conn = self.connect()\n        self.pandasSQL = sql.SQLiteDatabase(self.conn, 'sqlite')\n\n        self._load_iris_data()\n\n        self._load_test1_data()\n\n    def test_invalid_flavor(self):\n        self.assertRaises(\n            NotImplementedError, sql.SQLiteDatabase, self.conn, 'oracle')\n\n    def test_read_sql(self):\n        self._read_sql_iris()\n\n    def test_read_sql_parameter(self):\n        self._read_sql_iris_parameter()\n\n    def test_read_sql_named_parameter(self):\n        self._read_sql_iris_named_parameter()\n\n    def test_to_sql(self):\n        self._to_sql()\n\n    def test_to_sql_empty(self):\n        self._to_sql_empty()\n\n    def test_to_sql_fail(self):\n        self._to_sql_fail()\n\n    def test_to_sql_replace(self):\n        self._to_sql_replace()\n\n    def test_to_sql_append(self):\n        self._to_sql_append()\n\n    def test_create_and_drop_table(self):\n        temp_frame = DataFrame(\n            {'one': [1., 2., 3., 4.], 'two': [4., 3., 2., 1.]})\n\n        self.pandasSQL.to_sql(temp_frame, 'drop_test_frame')\n\n        self.assertTrue(self.pandasSQL.has_table('drop_test_frame'),\n                        'Table not written to DB')\n\n        self.pandasSQL.drop_table('drop_test_frame')\n\n        self.assertFalse(self.pandasSQL.has_table('drop_test_frame'),\n                         'Table not deleted from DB')\n\n    def test_roundtrip(self):\n        self._roundtrip()\n\n    def test_execute_sql(self):\n        self._execute_sql()\n\n    def test_datetime_date(self):\n        # test support for datetime.date\n        df = DataFrame([date(2014, 1, 1), date(2014, 1, 2)], columns=[\"a\"])\n        df.to_sql('test_date', self.conn, index=False, flavor=self.flavor)\n        res = read_sql_query('SELECT * FROM test_date', self.conn)\n        if self.flavor == 'sqlite':\n            # comes back as strings\n            tm.assert_frame_equal(res, df.astype(str))\n        elif self.flavor == 'mysql':\n            tm.assert_frame_equal(res, df)\n\n    def test_datetime_time(self):\n        # test support for datetime.time\n        df = DataFrame([time(9, 0, 0), time(9, 1, 30)], columns=[\"a\"])\n        # test it raises an error and not fails silently (GH8341)\n        if self.flavor == 'sqlite':\n            self.assertRaises(sqlite3.InterfaceError, sql.to_sql, df,\n                              'test_time', self.conn)\n\n    def _get_index_columns(self, tbl_name):\n        ixs = sql.read_sql_query(\n            \"SELECT * FROM sqlite_master WHERE type = 'index' \" +\n            \"AND tbl_name = '%s'\" % tbl_name, self.conn)\n        ix_cols = []\n        for ix_name in ixs.name:\n            ix_info = sql.read_sql_query(\n                \"PRAGMA index_info(%s)\" % ix_name, self.conn)\n            ix_cols.append(ix_info.name.tolist())\n        return ix_cols\n\n    def test_to_sql_save_index(self):\n        self._to_sql_save_index()\n\n    def test_transactions(self):\n        self._transaction_test()\n\n    def _get_sqlite_column_type(self, table, column):\n        recs = self.conn.execute('PRAGMA table_info(%s)' % table)\n        for cid, name, ctype, not_null, default, pk in recs:\n            if name == column:\n                return ctype\n        raise ValueError('Table %s, column %s not found' % (table, column))\n\n    def test_dtype(self):\n        if self.flavor == 'mysql':\n            raise nose.SkipTest('Not applicable to MySQL legacy')\n        cols = ['A', 'B']\n        data = [(0.8, True),\n                (0.9, None)]\n        df = DataFrame(data, columns=cols)\n        df.to_sql('dtype_test', self.conn)\n        df.to_sql('dtype_test2', self.conn, dtype={'B': 'STRING'})\n\n        # sqlite stores Boolean values as INTEGER\n        self.assertEqual(self._get_sqlite_column_type('dtype_test', 'B'), 'INTEGER')\n\n        self.assertEqual(self._get_sqlite_column_type('dtype_test2', 'B'), 'STRING')\n        self.assertRaises(ValueError, df.to_sql,\n                          'error', self.conn, dtype={'B': bool})\n\n    def test_notnull_dtype(self):\n        if self.flavor == 'mysql':\n            raise nose.SkipTest('Not applicable to MySQL legacy')\n\n        cols = {'Bool': Series([True,None]),\n                'Date': Series([datetime(2012, 5, 1), None]),\n                'Int' : Series([1, None], dtype='object'),\n                'Float': Series([1.1, None])\n               }\n        df = DataFrame(cols)\n\n        tbl = 'notnull_dtype_test'\n        df.to_sql(tbl, self.conn)\n\n        self.assertEqual(self._get_sqlite_column_type(tbl, 'Bool'), 'INTEGER')\n        self.assertEqual(self._get_sqlite_column_type(tbl, 'Date'), 'TIMESTAMP')\n        self.assertEqual(self._get_sqlite_column_type(tbl, 'Int'), 'INTEGER')\n        self.assertEqual(self._get_sqlite_column_type(tbl, 'Float'), 'REAL')\n\n    def test_illegal_names(self):\n        # For sqlite, these should work fine\n        df = DataFrame([[1, 2], [3, 4]], columns=['a', 'b'])\n\n        # Raise error on blank\n        self.assertRaises(ValueError, df.to_sql, \"\", self.conn,\n            flavor=self.flavor)\n\n        for ndx, weird_name in enumerate(['test_weird_name]','test_weird_name[',\n            'test_weird_name`','test_weird_name\"', 'test_weird_name\\'',\n            '_b.test_weird_name_01-30', '\"_b.test_weird_name_01-30\"',\n            '99beginswithnumber', '12345']):\n            df.to_sql(weird_name, self.conn, flavor=self.flavor)\n            sql.table_exists(weird_name, self.conn)\n\n            df2 = DataFrame([[1, 2], [3, 4]], columns=['a', weird_name])\n            c_tbl = 'test_weird_col_name%d'%ndx\n            df2.to_sql(c_tbl, self.conn, flavor=self.flavor)\n            sql.table_exists(c_tbl, self.conn)\n\n\nclass TestMySQLLegacy(MySQLMixIn, TestSQLiteFallback):\n    \"\"\"\n    Test the legacy mode against a MySQL database.\n\n    \"\"\"\n    flavor = 'mysql'\n\n    @classmethod\n    def setUpClass(cls):\n        cls.setup_driver()\n\n        # test connection\n        try:\n            cls.connect()\n        except cls.driver.err.OperationalError:\n            raise nose.SkipTest(\"{0} - can't connect to MySQL server\".format(cls))\n\n    @classmethod\n    def setup_driver(cls):\n        try:\n            import pymysql\n            cls.driver = pymysql\n        except ImportError:\n            raise nose.SkipTest('pymysql not installed')\n\n    @classmethod\n    def connect(cls):\n        return cls.driver.connect(host='127.0.0.1', user='root', passwd='', db='pandas_nosetest')\n\n    def _count_rows(self, table_name):\n        cur = self._get_exec()\n        cur.execute(\n            \"SELECT count(*) AS count_1 FROM %s\" % table_name)\n        rows = cur.fetchall()\n        return rows[0][0]\n\n    def setUp(self):\n        try:\n            self.conn = self.connect()\n        except self.driver.err.OperationalError:\n            raise nose.SkipTest(\"Can't connect to MySQL server\")\n\n        self.pandasSQL = sql.SQLiteDatabase(self.conn, 'mysql')\n\n        self._load_iris_data()\n        self._load_test1_data()\n\n    def test_a_deprecation(self):\n        with tm.assert_produces_warning(FutureWarning):\n            sql.to_sql(self.test_frame1, 'test_frame1', self.conn,\n                       flavor='mysql')\n        self.assertTrue(\n            sql.has_table('test_frame1', self.conn, flavor='mysql'),\n            'Table not written to DB')\n\n    def _get_index_columns(self, tbl_name):\n        ixs = sql.read_sql_query(\n            \"SHOW INDEX IN %s\" % tbl_name, self.conn)\n        ix_cols = {}\n        for ix_name, ix_col in zip(ixs.Key_name, ixs.Column_name):\n            if ix_name not in ix_cols:\n                ix_cols[ix_name] = []\n            ix_cols[ix_name].append(ix_col)\n        return list(ix_cols.values())\n\n    def test_to_sql_save_index(self):\n        self._to_sql_save_index()\n\n        for ix_name, ix_col in zip(ixs.Key_name, ixs.Column_name):\n            if ix_name not in ix_cols:\n                ix_cols[ix_name] = []\n            ix_cols[ix_name].append(ix_col)\n        return ix_cols.values()\n\n    def test_to_sql_save_index(self):\n        self._to_sql_save_index()\n\n    def test_illegal_names(self):\n        df = DataFrame([[1, 2], [3, 4]], columns=['a', 'b'])\n\n        # These tables and columns should be ok\n        for ndx, ok_name in enumerate(['99beginswithnumber','12345']):\n            df.to_sql(ok_name, self.conn, flavor=self.flavor, index=False,\n                      if_exists='replace')\n            df2 = DataFrame([[1, 2], [3, 4]], columns=['a', ok_name])\n\n            df2.to_sql('test_ok_col_name', self.conn, flavor=self.flavor, index=False,\n                      if_exists='replace')\n\n        # For MySQL, these should raise ValueError\n        for ndx, illegal_name in enumerate(['test_illegal_name]','test_illegal_name[',\n            'test_illegal_name`','test_illegal_name\"', 'test_illegal_name\\'', '']):\n            self.assertRaises(ValueError, df.to_sql, illegal_name, self.conn,\n                flavor=self.flavor, index=False)\n\n            df2 = DataFrame([[1, 2], [3, 4]], columns=['a', illegal_name])\n            self.assertRaises(ValueError, df2.to_sql, 'test_illegal_col_name%d'%ndx,\n                self.conn, flavor=self.flavor, index=False)\n\n\n#------------------------------------------------------------------------------\n#--- Old tests from 0.13.1 (before refactor using sqlalchemy)\n\n\n_formatters = {\n    datetime: lambda dt: \"'%s'\" % date_format(dt),\n    str: lambda x: \"'%s'\" % x,\n    np.str_: lambda x: \"'%s'\" % x,\n    compat.text_type: lambda x: \"'%s'\" % x,\n    compat.binary_type: lambda x: \"'%s'\" % x,\n    float: lambda x: \"%.8f\" % x,\n    int: lambda x: \"%s\" % x,\n    type(None): lambda x: \"NULL\",\n    np.float64: lambda x: \"%.10f\" % x,\n    bool: lambda x: \"'%s'\" % x,\n}\n\ndef format_query(sql, *args):\n    \"\"\"\n\n    \"\"\"\n    processed_args = []\n    for arg in args:\n        if isinstance(arg, float) and isnull(arg):\n            arg = None\n\n        formatter = _formatters[type(arg)]\n        processed_args.append(formatter(arg))\n\n    return sql % tuple(processed_args)\n\ndef _skip_if_no_pymysql():\n    try:\n        import pymysql\n    except ImportError:\n        raise nose.SkipTest('pymysql not installed, skipping')\n\n\nclass TestXSQLite(SQLiteMixIn, tm.TestCase):\n\n    def setUp(self):\n        self.conn = sqlite3.connect(':memory:')\n\n    def test_basic(self):\n        frame = tm.makeTimeDataFrame()\n        self._check_roundtrip(frame)\n\n    def test_write_row_by_row(self):\n\n        frame = tm.makeTimeDataFrame()\n        frame.ix[0, 0] = np.nan\n        create_sql = sql.get_schema(frame, 'test', 'sqlite')\n        cur = self.conn.cursor()\n        cur.execute(create_sql)\n\n        cur = self.conn.cursor()\n\n        ins = \"INSERT INTO test VALUES (%s, %s, %s, %s)\"\n        for idx, row in frame.iterrows():\n            fmt_sql = format_query(ins, *row)\n            sql.tquery(fmt_sql, cur=cur)\n\n        self.conn.commit()\n\n        result = sql.read_frame(\"select * from test\", con=self.conn)\n        result.index = frame.index\n        tm.assert_frame_equal(result, frame)\n\n    def test_execute(self):\n        frame = tm.makeTimeDataFrame()\n        create_sql = sql.get_schema(frame, 'test', 'sqlite')\n        cur = self.conn.cursor()\n        cur.execute(create_sql)\n        ins = \"INSERT INTO test VALUES (?, ?, ?, ?)\"\n\n        row = frame.ix[0]\n        sql.execute(ins, self.conn, params=tuple(row))\n        self.conn.commit()\n\n        result = sql.read_frame(\"select * from test\", self.conn)\n        result.index = frame.index[:1]\n        tm.assert_frame_equal(result, frame[:1])\n\n    def test_schema(self):\n        frame = tm.makeTimeDataFrame()\n        create_sql = sql.get_schema(frame, 'test', 'sqlite')\n        lines = create_sql.splitlines()\n        for l in lines:\n            tokens = l.split(' ')\n            if len(tokens) == 2 and tokens[0] == 'A':\n                self.assertTrue(tokens[1] == 'DATETIME')\n\n        frame = tm.makeTimeDataFrame()\n        create_sql = sql.get_schema(frame, 'test', 'sqlite', keys=['A', 'B'],)\n        lines = create_sql.splitlines()\n        self.assertTrue('PRIMARY KEY (\"A\", \"B\")' in create_sql)\n        cur = self.conn.cursor()\n        cur.execute(create_sql)\n\n    def test_execute_fail(self):\n        create_sql = \"\"\"\n        CREATE TABLE test\n        (\n        a TEXT,\n        b TEXT,\n        c REAL,\n        PRIMARY KEY (a, b)\n        );\n        \"\"\"\n        cur = self.conn.cursor()\n        cur.execute(create_sql)\n\n        sql.execute('INSERT INTO test VALUES(\"foo\", \"bar\", 1.234)', self.conn)\n        sql.execute('INSERT INTO test VALUES(\"foo\", \"baz\", 2.567)', self.conn)\n\n        try:\n            sys.stdout = StringIO()\n            self.assertRaises(Exception, sql.execute,\n                              'INSERT INTO test VALUES(\"foo\", \"bar\", 7)',\n                              self.conn)\n        finally:\n            sys.stdout = sys.__stdout__\n\n    def test_execute_closed_connection(self):\n        create_sql = \"\"\"\n        CREATE TABLE test\n        (\n        a TEXT,\n        b TEXT,\n        c REAL,\n        PRIMARY KEY (a, b)\n        );\n        \"\"\"\n        cur = self.conn.cursor()\n        cur.execute(create_sql)\n\n        sql.execute('INSERT INTO test VALUES(\"foo\", \"bar\", 1.234)', self.conn)\n        self.conn.close()\n        try:\n            sys.stdout = StringIO()\n            self.assertRaises(Exception, sql.tquery, \"select * from test\",\n                              con=self.conn)\n        finally:\n            sys.stdout = sys.__stdout__\n\n        # Initialize connection again (needed for tearDown)\n        self.setUp()\n\n    def test_na_roundtrip(self):\n        pass\n\n    def _check_roundtrip(self, frame):\n        sql.write_frame(frame, name='test_table', con=self.conn)\n        result = sql.read_frame(\"select * from test_table\", self.conn)\n\n        # HACK! Change this once indexes are handled properly.\n        result.index = frame.index\n\n        expected = frame\n        tm.assert_frame_equal(result, expected)\n\n        frame['txt'] = ['a'] * len(frame)\n        frame2 = frame.copy()\n        frame2['Idx'] = Index(lrange(len(frame2))) + 10\n        sql.write_frame(frame2, name='test_table2', con=self.conn)\n        result = sql.read_frame(\"select * from test_table2\", self.conn,\n                                index_col='Idx')\n        expected = frame.copy()\n        expected.index = Index(lrange(len(frame2))) + 10\n        expected.index.name = 'Idx'\n        tm.assert_frame_equal(expected, result)\n\n    def test_tquery(self):\n        frame = tm.makeTimeDataFrame()\n        sql.write_frame(frame, name='test_table', con=self.conn)\n        result = sql.tquery(\"select A from test_table\", self.conn)\n        expected = Series(frame.A.values, frame.index) # not to have name\n        result = Series(result, frame.index)\n        tm.assert_series_equal(result, expected)\n\n        try:\n            sys.stdout = StringIO()\n            self.assertRaises(sql.DatabaseError, sql.tquery,\n                              'select * from blah', con=self.conn)\n\n            self.assertRaises(sql.DatabaseError, sql.tquery,\n                              'select * from blah', con=self.conn, retry=True)\n        finally:\n            sys.stdout = sys.__stdout__\n\n    def test_uquery(self):\n        frame = tm.makeTimeDataFrame()\n        sql.write_frame(frame, name='test_table', con=self.conn)\n        stmt = 'INSERT INTO test_table VALUES(2.314, -123.1, 1.234, 2.3)'\n        self.assertEqual(sql.uquery(stmt, con=self.conn), 1)\n\n        try:\n            sys.stdout = StringIO()\n\n            self.assertRaises(sql.DatabaseError, sql.tquery,\n                              'insert into blah values (1)', con=self.conn)\n\n            self.assertRaises(sql.DatabaseError, sql.tquery,\n                              'insert into blah values (1)', con=self.conn,\n                              retry=True)\n        finally:\n            sys.stdout = sys.__stdout__\n\n    def test_keyword_as_column_names(self):\n        '''\n        '''\n        df = DataFrame({'From':np.ones(5)})\n        sql.write_frame(df, con = self.conn, name = 'testkeywords')\n\n    def test_onecolumn_of_integer(self):\n        # GH 3628\n        # a column_of_integers dataframe should transfer well to sql\n\n        mono_df=DataFrame([1 , 2], columns=['c0'])\n        sql.write_frame(mono_df, con = self.conn, name = 'mono_df')\n        # computing the sum via sql\n        con_x=self.conn\n        the_sum=sum([my_c0[0] for  my_c0 in con_x.execute(\"select * from mono_df\")])\n        # it should not fail, and gives 3 ( Issue #3628 )\n        self.assertEqual(the_sum , 3)\n\n        result = sql.read_frame(\"select * from mono_df\",con_x)\n        tm.assert_frame_equal(result,mono_df)\n\n    def test_if_exists(self):\n        df_if_exists_1 = DataFrame({'col1': [1, 2], 'col2': ['A', 'B']})\n        df_if_exists_2 = DataFrame({'col1': [3, 4, 5], 'col2': ['C', 'D', 'E']})\n        table_name = 'table_if_exists'\n        sql_select = \"SELECT * FROM %s\" % table_name\n\n        def clean_up(test_table_to_drop):\n            \"\"\"\n            Drops tables created from individual tests\n            so no dependencies arise from sequential tests\n            \"\"\"\n            self.drop_table(test_table_to_drop)\n\n        # test if invalid value for if_exists raises appropriate error\n        self.assertRaises(ValueError,\n                          sql.write_frame,\n                          frame=df_if_exists_1,\n                          con=self.conn,\n                          name=table_name,\n                          flavor='sqlite',\n                          if_exists='notvalidvalue')\n        clean_up(table_name)\n\n        # test if_exists='fail'\n        sql.write_frame(frame=df_if_exists_1, con=self.conn, name=table_name,\n                        flavor='sqlite', if_exists='fail')\n        self.assertRaises(ValueError,\n                          sql.write_frame,\n                          frame=df_if_exists_1,\n                          con=self.conn,\n                          name=table_name,\n                          flavor='sqlite',\n                          if_exists='fail')\n\n        # test if_exists='replace'\n        sql.write_frame(frame=df_if_exists_1, con=self.conn, name=table_name,\n                        flavor='sqlite', if_exists='replace')\n        self.assertEqual(sql.tquery(sql_select, con=self.conn),\n                         [(1, 'A'), (2, 'B')])\n        sql.write_frame(frame=df_if_exists_2, con=self.conn, name=table_name,\n                        flavor='sqlite', if_exists='replace')\n        self.assertEqual(sql.tquery(sql_select, con=self.conn),\n                         [(3, 'C'), (4, 'D'), (5, 'E')])\n        clean_up(table_name)\n\n        # test if_exists='append'\n        sql.write_frame(frame=df_if_exists_1, con=self.conn, name=table_name,\n                        flavor='sqlite', if_exists='fail')\n        self.assertEqual(sql.tquery(sql_select, con=self.conn),\n                         [(1, 'A'), (2, 'B')])\n        sql.write_frame(frame=df_if_exists_2, con=self.conn, name=table_name,\n                        flavor='sqlite', if_exists='append')\n        self.assertEqual(sql.tquery(sql_select, con=self.conn),\n                         [(1, 'A'), (2, 'B'), (3, 'C'), (4, 'D'), (5, 'E')])\n        clean_up(table_name)\n\n\nclass TestXMySQL(MySQLMixIn, tm.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        _skip_if_no_pymysql()\n\n        # test connection\n        import pymysql\n        try:\n            # Try Travis defaults.\n            # No real user should allow root access with a blank password.\n            pymysql.connect(host='localhost', user='root', passwd='',\n                            db='pandas_nosetest')\n        except:\n            pass\n        else:\n            return\n        try:\n            pymysql.connect(read_default_group='pandas')\n        except pymysql.ProgrammingError as e:\n            raise nose.SkipTest(\n                \"Create a group of connection parameters under the heading \"\n                \"[pandas] in your system's mysql default file, \"\n                \"typically located at ~/.my.cnf or /etc/.my.cnf. \")\n        except pymysql.Error as e:\n            raise nose.SkipTest(\n                \"Cannot connect to database. \"\n                \"Create a group of connection parameters under the heading \"\n                \"[pandas] in your system's mysql default file, \"\n                \"typically located at ~/.my.cnf or /etc/.my.cnf. \")\n\n    def setUp(self):\n        _skip_if_no_pymysql()\n        import pymysql\n        try:\n            # Try Travis defaults.\n            # No real user should allow root access with a blank password.\n            self.conn = pymysql.connect(host='localhost', user='root', passwd='',\n                                    db='pandas_nosetest')\n        except:\n            pass\n        else:\n            return\n        try:\n            self.conn = pymysql.connect(read_default_group='pandas')\n        except pymysql.ProgrammingError as e:\n            raise nose.SkipTest(\n                \"Create a group of connection parameters under the heading \"\n                \"[pandas] in your system's mysql default file, \"\n                \"typically located at ~/.my.cnf or /etc/.my.cnf. \")\n        except pymysql.Error as e:\n            raise nose.SkipTest(\n                \"Cannot connect to database. \"\n                \"Create a group of connection parameters under the heading \"\n                \"[pandas] in your system's mysql default file, \"\n                \"typically located at ~/.my.cnf or /etc/.my.cnf. \")\n\n\n    def test_basic(self):\n        _skip_if_no_pymysql()\n        frame = tm.makeTimeDataFrame()\n        self._check_roundtrip(frame)\n\n    def test_write_row_by_row(self):\n\n        _skip_if_no_pymysql()\n        frame = tm.makeTimeDataFrame()\n        frame.ix[0, 0] = np.nan\n        drop_sql = \"DROP TABLE IF EXISTS test\"\n        create_sql = sql.get_schema(frame, 'test', 'mysql')\n        cur = self.conn.cursor()\n        cur.execute(drop_sql)\n        cur.execute(create_sql)\n        ins = \"INSERT INTO test VALUES (%s, %s, %s, %s)\"\n        for idx, row in frame.iterrows():\n            fmt_sql = format_query(ins, *row)\n            sql.tquery(fmt_sql, cur=cur)\n\n        self.conn.commit()\n\n        result = sql.read_frame(\"select * from test\", con=self.conn)\n        result.index = frame.index\n        tm.assert_frame_equal(result, frame)\n\n    def test_execute(self):\n        _skip_if_no_pymysql()\n        frame = tm.makeTimeDataFrame()\n        drop_sql = \"DROP TABLE IF EXISTS test\"\n        create_sql = sql.get_schema(frame, 'test', 'mysql')\n        cur = self.conn.cursor()\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"Unknown table.*\")\n            cur.execute(drop_sql)\n        cur.execute(create_sql)\n        ins = \"INSERT INTO test VALUES (%s, %s, %s, %s)\"\n\n        row = frame.ix[0].values.tolist()\n        sql.execute(ins, self.conn, params=tuple(row))\n        self.conn.commit()\n\n        result = sql.read_frame(\"select * from test\", self.conn)\n        result.index = frame.index[:1]\n        tm.assert_frame_equal(result, frame[:1])\n\n    def test_schema(self):\n        _skip_if_no_pymysql()\n        frame = tm.makeTimeDataFrame()\n        create_sql = sql.get_schema(frame, 'test', 'mysql')\n        lines = create_sql.splitlines()\n        for l in lines:\n            tokens = l.split(' ')\n            if len(tokens) == 2 and tokens[0] == 'A':\n                self.assertTrue(tokens[1] == 'DATETIME')\n\n        frame = tm.makeTimeDataFrame()\n        drop_sql = \"DROP TABLE IF EXISTS test\"\n        create_sql = sql.get_schema(frame, 'test', 'mysql', keys=['A', 'B'],)\n        lines = create_sql.splitlines()\n        self.assertTrue('PRIMARY KEY (`A`, `B`)' in create_sql)\n        cur = self.conn.cursor()\n        cur.execute(drop_sql)\n        cur.execute(create_sql)\n\n    def test_execute_fail(self):\n        _skip_if_no_pymysql()\n        drop_sql = \"DROP TABLE IF EXISTS test\"\n        create_sql = \"\"\"\n        CREATE TABLE test\n        (\n        a TEXT,\n        b TEXT,\n        c REAL,\n        PRIMARY KEY (a(5), b(5))\n        );\n        \"\"\"\n        cur = self.conn.cursor()\n        cur.execute(drop_sql)\n        cur.execute(create_sql)\n\n        sql.execute('INSERT INTO test VALUES(\"foo\", \"bar\", 1.234)', self.conn)\n        sql.execute('INSERT INTO test VALUES(\"foo\", \"baz\", 2.567)', self.conn)\n\n        try:\n            sys.stdout = StringIO()\n            self.assertRaises(Exception, sql.execute,\n                              'INSERT INTO test VALUES(\"foo\", \"bar\", 7)',\n                              self.conn)\n        finally:\n            sys.stdout = sys.__stdout__\n\n    def test_execute_closed_connection(self):\n        _skip_if_no_pymysql()\n        drop_sql = \"DROP TABLE IF EXISTS test\"\n        create_sql = \"\"\"\n        CREATE TABLE test\n        (\n        a TEXT,\n        b TEXT,\n        c REAL,\n        PRIMARY KEY (a(5), b(5))\n        );\n        \"\"\"\n        cur = self.conn.cursor()\n        cur.execute(drop_sql)\n        cur.execute(create_sql)\n\n        sql.execute('INSERT INTO test VALUES(\"foo\", \"bar\", 1.234)', self.conn)\n        self.conn.close()\n        try:\n            sys.stdout = StringIO()\n            self.assertRaises(Exception, sql.tquery, \"select * from test\",\n                              con=self.conn)\n        finally:\n            sys.stdout = sys.__stdout__\n\n        # Initialize connection again (needed for tearDown)\n        self.setUp()\n\n\n    def test_na_roundtrip(self):\n        _skip_if_no_pymysql()\n        pass\n\n    def _check_roundtrip(self, frame):\n        _skip_if_no_pymysql()\n        drop_sql = \"DROP TABLE IF EXISTS test_table\"\n        cur = self.conn.cursor()\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"Unknown table.*\")\n            cur.execute(drop_sql)\n        sql.write_frame(frame, name='test_table', con=self.conn, flavor='mysql')\n        result = sql.read_frame(\"select * from test_table\", self.conn)\n\n        # HACK! Change this once indexes are handled properly.\n        result.index = frame.index\n        result.index.name = frame.index.name\n\n        expected = frame\n        tm.assert_frame_equal(result, expected)\n\n        frame['txt'] = ['a'] * len(frame)\n        frame2 = frame.copy()\n        index = Index(lrange(len(frame2))) + 10\n        frame2['Idx'] = index\n        drop_sql = \"DROP TABLE IF EXISTS test_table2\"\n        cur = self.conn.cursor()\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"Unknown table.*\")\n            cur.execute(drop_sql)\n        sql.write_frame(frame2, name='test_table2', con=self.conn, flavor='mysql')\n        result = sql.read_frame(\"select * from test_table2\", self.conn,\n                                index_col='Idx')\n        expected = frame.copy()\n\n        # HACK! Change this once indexes are handled properly.\n        expected.index = index\n        expected.index.names = result.index.names\n        tm.assert_frame_equal(expected, result)\n\n    def test_tquery(self):\n        try:\n            import pymysql\n        except ImportError:\n            raise nose.SkipTest(\"no pymysql\")\n        frame = tm.makeTimeDataFrame()\n        drop_sql = \"DROP TABLE IF EXISTS test_table\"\n        cur = self.conn.cursor()\n        cur.execute(drop_sql)\n        sql.write_frame(frame, name='test_table', con=self.conn, flavor='mysql')\n        result = sql.tquery(\"select A from test_table\", self.conn)\n        expected = Series(frame.A.values, frame.index) # not to have name\n        result = Series(result, frame.index)\n        tm.assert_series_equal(result, expected)\n\n        try:\n            sys.stdout = StringIO()\n            self.assertRaises(sql.DatabaseError, sql.tquery,\n                              'select * from blah', con=self.conn)\n\n            self.assertRaises(sql.DatabaseError, sql.tquery,\n                              'select * from blah', con=self.conn, retry=True)\n        finally:\n            sys.stdout = sys.__stdout__\n\n    def test_uquery(self):\n        try:\n            import pymysql\n        except ImportError:\n            raise nose.SkipTest(\"no pymysql\")\n        frame = tm.makeTimeDataFrame()\n        drop_sql = \"DROP TABLE IF EXISTS test_table\"\n        cur = self.conn.cursor()\n        cur.execute(drop_sql)\n        sql.write_frame(frame, name='test_table', con=self.conn, flavor='mysql')\n        stmt = 'INSERT INTO test_table VALUES(2.314, -123.1, 1.234, 2.3)'\n        self.assertEqual(sql.uquery(stmt, con=self.conn), 1)\n\n        try:\n            sys.stdout = StringIO()\n\n            self.assertRaises(sql.DatabaseError, sql.tquery,\n                              'insert into blah values (1)', con=self.conn)\n\n            self.assertRaises(sql.DatabaseError, sql.tquery,\n                              'insert into blah values (1)', con=self.conn,\n                              retry=True)\n        finally:\n            sys.stdout = sys.__stdout__\n\n    def test_keyword_as_column_names(self):\n        '''\n        '''\n        _skip_if_no_pymysql()\n        df = DataFrame({'From':np.ones(5)})\n        sql.write_frame(df, con = self.conn, name = 'testkeywords',\n                        if_exists='replace', flavor='mysql')\n\n    def test_if_exists(self):\n        _skip_if_no_pymysql()\n        df_if_exists_1 = DataFrame({'col1': [1, 2], 'col2': ['A', 'B']})\n        df_if_exists_2 = DataFrame({'col1': [3, 4, 5], 'col2': ['C', 'D', 'E']})\n        table_name = 'table_if_exists'\n        sql_select = \"SELECT * FROM %s\" % table_name\n\n        def clean_up(test_table_to_drop):\n            \"\"\"\n            Drops tables created from individual tests\n            so no dependencies arise from sequential tests\n            \"\"\"\n            self.drop_table(test_table_to_drop)\n\n        # test if invalid value for if_exists raises appropriate error\n        self.assertRaises(ValueError,\n                          sql.write_frame,\n                          frame=df_if_exists_1,\n                          con=self.conn,\n                          name=table_name,\n                          flavor='mysql',\n                          if_exists='notvalidvalue')\n        clean_up(table_name)\n\n        # test if_exists='fail'\n        sql.write_frame(frame=df_if_exists_1, con=self.conn, name=table_name,\n                        flavor='mysql', if_exists='fail')\n        self.assertRaises(ValueError,\n                          sql.write_frame,\n                          frame=df_if_exists_1,\n                          con=self.conn,\n                          name=table_name,\n                          flavor='mysql',\n                          if_exists='fail')\n\n        # test if_exists='replace'\n        sql.write_frame(frame=df_if_exists_1, con=self.conn, name=table_name,\n                        flavor='mysql', if_exists='replace')\n        self.assertEqual(sql.tquery(sql_select, con=self.conn),\n                         [(1, 'A'), (2, 'B')])\n        sql.write_frame(frame=df_if_exists_2, con=self.conn, name=table_name,\n                        flavor='mysql', if_exists='replace')\n        self.assertEqual(sql.tquery(sql_select, con=self.conn),\n                         [(3, 'C'), (4, 'D'), (5, 'E')])\n        clean_up(table_name)\n\n        # test if_exists='append'\n        sql.write_frame(frame=df_if_exists_1, con=self.conn, name=table_name,\n                        flavor='mysql', if_exists='fail')\n        self.assertEqual(sql.tquery(sql_select, con=self.conn),\n                         [(1, 'A'), (2, 'B')])\n        sql.write_frame(frame=df_if_exists_2, con=self.conn, name=table_name,\n                        flavor='mysql', if_exists='append')\n        self.assertEqual(sql.tquery(sql_select, con=self.conn),\n                         [(1, 'A'), (2, 'B'), (3, 'C'), (4, 'D'), (5, 'E')])\n        clean_up(table_name)\n\n\nif __name__ == '__main__':\n    nose.runmodule(argv=[__file__, '-vvs', '-x', '--pdb', '--pdb-failure'],\n                   exit=False)\n"
    }
  ],
  "questions": [
    "`read_sql` has a `params` keyword: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html#pandas.read_sql\n\nIs it that what you are looking for?",
    "I try this:\n\n```\nimport sqlalchemy\nimport pandas as pd\nimport numpy as np\ndb_uri = 'sqlite:///test.db'\nengine = sqlalchemy.create_engine(db_uri)\ndf = pd.DataFrame(np.random.randn(4,3), columns=['a','b','c'])\ndf.to_sql(\"df\", engine)\ndf.to_sql(\"test_table\", engine)\n#pd.read_sql(\"SELECT * from df where a>0;\", engine)\npd.read_sql(\"SELECT * from df where a>0; DROP TABLE test_table\", engine)\n```\n\nit raises\n\n```\nWarning: You can only execute one statement at a time.\n```\n\nand `test_table` is fortunately not dropped.\n\nSo I think the problem is on (mysqlconnector) driver side.\nhttp://bugs.mysql.com/bug.php?id=78308"
  ],
  "golden_answers": [
    "That's probably the parameter to use\n\nbut I think we should warm more in doc\n\n```\nimport pandas as pd\nfrom sqlalchemy import create_engine\ndb_uri = 'mysql+mysqlconnector://root:root@localhost:3306/meteo'\nengine = create_engine(db_uri)\n#query = \"SELECT * FROM data where Ta>35\" # no problem\n# you need to create a 'test_table' table which will be dropped\ntemp = \"35; DROP TABLE test_table;\" \nquery = \"SELECT * FROM data where Ta>%s\" % temp\npd.read_sql(query, engine)\n```\n\nraises\n\n```\nInterfaceError: (mysql.connector.errors.InterfaceError) Use multi=True when executing multiple statements [SQL: 'SELECT * FROM data where Ta>35; DROP TABLE test_table;']\n```\n\nbut `test_table` is dropped\n\n```\npd.read_sql(query, engine)\n```\n\nagain raises\n\n```\nProgrammingError: (mysql.connector.errors.ProgrammingError) 1051 (42S02): Unknown table 'meteo.test'\n```",
    "@scls19fr Improvement to the docs are certainly welcome!\n\nThe warning you see above is actually a warning (feature) from sqlite3 itself (the have `executescript` to execute multiple statements).\n\nIt is always possible to misuse `read_sql`, just as you can misuse a plain `conn.execute`. This is a general issue with sql querying, so I don't think pandas should directly do anything about that. But of course, warning for that in the docs is easy to do!"
  ],
  "questions_generated": [
    "How does the current implementation of `read_sql` in pandas potentially expose users to SQL injection vulnerabilities?",
    "What is the role of the `params` keyword in the `read_sql` function, and how does it help mitigate SQL injection risks?",
    "Why might the `read_sql` function not prevent SQL injection when using the MySQL Connector, and what alternative approach is suggested in the issue discussion?",
    "In the given code snippets, what is the significance of using placeholders like '?' in SQL statements, and how does this relate to the `read_sql` function's usage?",
    "What are the potential consequences of not addressing the SQL injection vulnerability in the pandas `read_sql` function, as discussed in the issue?"
  ],
  "golden_answers_generated": [
    "The current use of `read_sql` in pandas can expose users to SQL injection vulnerabilities if they concatenate SQL queries using unsafe string formatting methods, such as the `%` operator or `.format()`. This is because these methods can include untrusted inputs directly into the SQL statement, which could be manipulated by an attacker. The recommended practice is to use parameterized queries, which `read_sql` supports through the `params` keyword argument. This ensures that inputs are treated as data values rather than executable SQL code.",
    "The `params` keyword in the `read_sql` function allows users to pass parameters separately from the SQL query string. It is used to safely insert user inputs into SQL queries by binding them as parameters, rather than directly including them in the query string. This approach prevents potentially malicious inputs from being executed as part of the SQL statement, thereby mitigating SQL injection risks.",
    "The `read_sql` function might not prevent SQL injection when using the MySQL Connector if users do not utilize the `params` keyword and instead construct queries using unsafe string interpolation techniques. The issue discussion suggests using parameterized queries with the `params` keyword to prevent SQL injection. Furthermore, ensuring that the database driver or connector enforces single-statement execution can also help prevent SQL injection attacks that rely on executing multiple statements in one go.",
    "Using placeholders like '?' in SQL statements is significant because it allows the use of parameterized queries. Placeholders act as markers in the SQL statement where actual values will be inserted safely at runtime. This prevents direct insertion of user input into the SQL code, thus avoiding SQL injection vulnerabilities. In the context of the `read_sql` function, using such placeholders in combination with the `params` keyword enables safe execution of queries that incorporate user-provided data.",
    "Not addressing the SQL injection vulnerability in the pandas `read_sql` function could lead to severe security risks. If an attacker is able to inject malicious SQL code, they could manipulate the database, execute unauthorized commands, extract sensitive data, or even delete database tables. This could compromise the integrity and confidentiality of the data, cause data loss, and potentially lead to financial and reputational damage for the affected organization."
  ]
}