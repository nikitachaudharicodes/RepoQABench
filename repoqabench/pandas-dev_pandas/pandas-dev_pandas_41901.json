{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "41901",
  "issue_description": "# STYLE add check for future annotations\n\nxref https://github.com/pandas-dev/pandas/pull/41892#issuecomment-857834651",
  "issue_comments": [
    {
      "id": 858321225,
      "user": "aacosta13",
      "body": "Hey all, this is my first time helping contribute to an open-source project so forgive any of my ignorance here. I'd be willing to work on this issue, just a couple of questions if that's fine. Based on the referenced comment, should I just add that `import` command to every file (except the ones mentioned)? And I'll be sure to look at the contributions guide before doing so, of course. Thanks."
    },
    {
      "id": 858437593,
      "user": "MarcoGorelli",
      "body": "Hey - the issue is to add a pre-commit check to make sure that `from __future__ import annotations` is always used, see https://github.com/pymc-devs/pymc-examples/blob/01233b6b32f731c83a11689e14a662dfc574b760/.pre-commit-config.yaml#L22-L30 for an example of how to write a pre-commit check which ensures that a pattern is always present. also see the pre-commit docs"
    },
    {
      "id": 859225070,
      "user": "ForestMars",
      "body": "Since this is needed for 1.3 and is a quick fix: [PR #41941 ]\r\n\r\n"
    },
    {
      "id": 859225333,
      "user": "ForestMars",
      "body": "@aacosta13 Just noticed your earlier comment you were working on this? Sorry I didn't realize, this was just a super quick one "
    },
    {
      "id": 859247351,
      "user": "aacosta13",
      "body": "@ForestMars no worries, was going to work on it and upload my PR tonight once I got home but it’s cool! And like you said it was an easy fix haha"
    },
    {
      "id": 859492262,
      "user": "simonjayhawkins",
      "body": "> Since this is needed for 1.3 and is a quick fix: [PR #41941 ]\r\n\r\nThe reason we did the changes for 1.3 is we are going to branch soon and wanted to make sure the new style is on the backport branch to reduce the chances of autobackports failing.\r\n\r\nI think that task is done and adding it to the other files is just to avoid new future code additions/modifications using Optional, Union, Type, Tuple, List etc in type annotations.\r\n\r\nSo just adding `from __future__ import annotations` to the modules that don't yet need them is unlikely to create issues with the autobackports.\r\n\r\nThis task does not therefore need to be done in time for the rc. (and if the PR was just a few additions and changes to the CI with no behavioral changes... which it should be, could be merged during the rc anyway)"
    },
    {
      "id": 859726342,
      "user": "ForestMars",
      "body": "@simonjayhawkins - There are no behavioral changes, however it does seem that we also need to add that import to 120 files that do no have it (excluding tests, etc.), waiting on Marco to confirm that. \r\n\r\n"
    },
    {
      "id": 859738136,
      "user": "aacosta13",
      "body": "If the import is needed on all files I’ve actually typed up a script that adds the line to all files from when I originally thought that was the scope of this issue, would just need to verify the files that should be excluded. I could submit the PR if needed."
    },
    {
      "id": 859768827,
      "user": "ForestMars",
      "body": "@aacosta13 that's a +1 for me. Can you share the script in a gist? \r\n\r\nI was actually going to suggest doing that in a separate PR from the pre-commit hook. "
    },
    {
      "id": 859996468,
      "user": "aacosta13",
      "body": "@ForestMars Sorry for the delayed response, hadn't had time to get to my desk till now. But sure, [here](https://gist.github.com/aacosta13/eb1cb4b22ece354c60f0e7aab90137b0) is the link to the gist of the script.\r\n\r\nA few things to note: the script is a bit scrappy, I just typed up something quick as I'm sure it's to be discarded after use. It also ignores any styling placements of the import line, as the only check done is to ensure that it avoids pre-pending the import line above all lines in the case of a .py file being a bash script (leading it to be awkwardly placed between other import lines).\r\n\r\nAlso, I did not exclude any specific files, and simply added the import to all .py files as I was hoping to get more clarification as to which files would not need it added. I did, however, ensure that all the .py files it is being added to do not already have the import line within them.\r\n\r\nI'll be happy to make any corrections if needed."
    },
    {
      "id": 860064719,
      "user": "ForestMars",
      "body": "@aacosta13 Hmm, this is going to put the import statement in the first empty line (which may be part of the docstring for the file.) I don't think that's what you intended...? (Eg. isort pre-commit hook won't recognize it, in such cases.) \r\n\r\n"
    },
    {
      "id": 860067780,
      "user": "MarcoGorelli",
      "body": "you can use isort to add imports https://pycqa.github.io/isort/docs/configuration/add_or_remove_imports/"
    },
    {
      "id": 860068832,
      "user": "aacosta13",
      "body": "Nice, that makes things a whole lot easier haha. Then I suppose we just do an `isort` on the list of files that don’t have the import line already which should be fairly easy. Were there any other specific files to look out for to exclude from having the line added?"
    },
    {
      "id": 860083279,
      "user": "ForestMars",
      "body": "yes, you can see the list of exclusions in the code on my PR, which is: \r\n\r\n- __ init __.py\r\n- api.py\r\n- setup.py\r\n- asv_bench/\r\n- tests/\r\n- web/"
    },
    {
      "id": 860118437,
      "user": "aacosta13",
      "body": "Alright, here's the updated [gist](https://gist.github.com/aacosta13/eb1cb4b22ece354c60f0e7aab90137b0). Excluded the file types listed and used `isort` to add the import to the rest which does a much cleaner job than my previous script. Let me know if there are any other issues, though."
    },
    {
      "id": 861552410,
      "user": "ForestMars",
      "body": "@aacosta13 - please see the updated list of file exclusions in my PR #41941. The number of files needing the import statement is reduced from ~120 to 55. "
    },
    {
      "id": 861561270,
      "user": "MarcoGorelli",
      "body": "@aacosta13 could you send a PR to @ForestMars 's branch, so this is all done in the same PR to pandas?"
    },
    {
      "id": 863359330,
      "user": "ForestMars",
      "body": "@aacosta13 - LMK if you'd like me to send the PR adding import to those files, to my branch with the pre-commit hook.  Happy to do that if you've don't have time. "
    },
    {
      "id": 869873817,
      "user": "ForestMars",
      "body": "Just checking in here, as I haven't seen any activity on this. Originally I had the impression this was something that needed to go right in. "
    },
    {
      "id": 869882644,
      "user": "aacosta13",
      "body": "@ForestMars sorry, have been tied up with some personal business over the past few weeks. Sorry about not getting back sooner, I can go ahead and jump back in and fix up the issues on my end today"
    },
    {
      "id": 881296838,
      "user": "MarcoGorelli",
      "body": "Feel free to add the imports in your PR @ForestMars - I think you could also add `files: ^pandas/` in the pre-commit config file"
    }
  ],
  "text_context": "# STYLE add check for future annotations\n\nxref https://github.com/pandas-dev/pandas/pull/41892#issuecomment-857834651\n\nHey all, this is my first time helping contribute to an open-source project so forgive any of my ignorance here. I'd be willing to work on this issue, just a couple of questions if that's fine. Based on the referenced comment, should I just add that `import` command to every file (except the ones mentioned)? And I'll be sure to look at the contributions guide before doing so, of course. Thanks.\n\nHey - the issue is to add a pre-commit check to make sure that `from __future__ import annotations` is always used, see https://github.com/pymc-devs/pymc-examples/blob/01233b6b32f731c83a11689e14a662dfc574b760/.pre-commit-config.yaml#L22-L30 for an example of how to write a pre-commit check which ensures that a pattern is always present. also see the pre-commit docs\n\nSince this is needed for 1.3 and is a quick fix: [PR #41941 ]\r\n\r\n\n\n@aacosta13 Just noticed your earlier comment you were working on this? Sorry I didn't realize, this was just a super quick one \n\n@ForestMars no worries, was going to work on it and upload my PR tonight once I got home but it’s cool! And like you said it was an easy fix haha\n\n> Since this is needed for 1.3 and is a quick fix: [PR #41941 ]\r\n\r\nThe reason we did the changes for 1.3 is we are going to branch soon and wanted to make sure the new style is on the backport branch to reduce the chances of autobackports failing.\r\n\r\nI think that task is done and adding it to the other files is just to avoid new future code additions/modifications using Optional, Union, Type, Tuple, List etc in type annotations.\r\n\r\nSo just adding `from __future__ import annotations` to the modules that don't yet need them is unlikely to create issues with the autobackports.\r\n\r\nThis task does not therefore need to be done in time for the rc. (and if the PR was just a few additions and changes to the CI with no behavioral changes... which it should be, could be merged during the rc anyway)\n\n@simonjayhawkins - There are no behavioral changes, however it does seem that we also need to add that import to 120 files that do no have it (excluding tests, etc.), waiting on Marco to confirm that. \r\n\r\n\n\nIf the import is needed on all files I’ve actually typed up a script that adds the line to all files from when I originally thought that was the scope of this issue, would just need to verify the files that should be excluded. I could submit the PR if needed.\n\n@aacosta13 that's a +1 for me. Can you share the script in a gist? \r\n\r\nI was actually going to suggest doing that in a separate PR from the pre-commit hook. \n\n@ForestMars Sorry for the delayed response, hadn't had time to get to my desk till now. But sure, [here](https://gist.github.com/aacosta13/eb1cb4b22ece354c60f0e7aab90137b0) is the link to the gist of the script.\r\n\r\nA few things to note: the script is a bit scrappy, I just typed up something quick as I'm sure it's to be discarded after use. It also ignores any styling placements of the import line, as the only check done is to ensure that it avoids pre-pending the import line above all lines in the case of a .py file being a bash script (leading it to be awkwardly placed between other import lines).\r\n\r\nAlso, I did not exclude any specific files, and simply added the import to all .py files as I was hoping to get more clarification as to which files would not need it added. I did, however, ensure that all the .py files it is being added to do not already have the import line within them.\r\n\r\nI'll be happy to make any corrections if needed.\n\n@aacosta13 Hmm, this is going to put the import statement in the first empty line (which may be part of the docstring for the file.) I don't think that's what you intended...? (Eg. isort pre-commit hook won't recognize it, in such cases.) \r\n\r\n\n\nyou can use isort to add imports https://pycqa.github.io/isort/docs/configuration/add_or_remove_imports/\n\nNice, that makes things a whole lot easier haha. Then I suppose we just do an `isort` on the list of files that don’t have the import line already which should be fairly easy. Were there any other specific files to look out for to exclude from having the line added?\n\nyes, you can see the list of exclusions in the code on my PR, which is: \r\n\r\n- __ init __.py\r\n- api.py\r\n- setup.py\r\n- asv_bench/\r\n- tests/\r\n- web/\n\nAlright, here's the updated [gist](https://gist.github.com/aacosta13/eb1cb4b22ece354c60f0e7aab90137b0). Excluded the file types listed and used `isort` to add the import to the rest which does a much cleaner job than my previous script. Let me know if there are any other issues, though.\n\n@aacosta13 - please see the updated list of file exclusions in my PR #41941. The number of files needing the import statement is reduced from ~120 to 55. \n\n@aacosta13 could you send a PR to @ForestMars 's branch, so this is all done in the same PR to pandas?\n\n@aacosta13 - LMK if you'd like me to send the PR adding import to those files, to my branch with the pre-commit hook.  Happy to do that if you've don't have time. \n\nJust checking in here, as I haven't seen any activity on this. Originally I had the impression this was something that needed to go right in. \n\n@ForestMars sorry, have been tied up with some personal business over the past few weeks. Sorry about not getting back sooner, I can go ahead and jump back in and fix up the issues on my end today\n\nFeel free to add the imports in your PR @ForestMars - I think you could also add `files: ^pandas/` in the pre-commit config file",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/41892",
  "code_context": [
    {
      "filename": "pandas/io/json/_json.py",
      "content": "from __future__ import annotations\n\nfrom abc import (\n    ABC,\n    abstractmethod,\n)\nfrom collections import abc\nimport functools\nfrom io import StringIO\nfrom itertools import islice\nfrom typing import (\n    Any,\n    Callable,\n    Mapping,\n)\n\nimport numpy as np\n\nimport pandas._libs.json as json\nfrom pandas._libs.tslibs import iNaT\nfrom pandas._typing import (\n    CompressionOptions,\n    DtypeArg,\n    FrameOrSeriesUnion,\n    IndexLabel,\n    JSONSerializable,\n    StorageOptions,\n)\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import (\n    deprecate_kwarg,\n    deprecate_nonkeyword_arguments,\n    doc,\n)\n\nfrom pandas.core.dtypes.common import (\n    ensure_str,\n    is_period_dtype,\n)\n\nfrom pandas import (\n    DataFrame,\n    MultiIndex,\n    Series,\n    isna,\n    notna,\n    to_datetime,\n)\nfrom pandas.core import generic\nfrom pandas.core.construction import create_series_with_explicit_dtype\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.reshape.concat import concat\n\nfrom pandas.io.common import (\n    IOHandles,\n    file_exists,\n    get_handle,\n    is_fsspec_url,\n    is_url,\n    stringify_path,\n)\nfrom pandas.io.json._normalize import convert_to_line_delimits\nfrom pandas.io.json._table_schema import (\n    build_table_schema,\n    parse_table_schema,\n)\nfrom pandas.io.parsers.readers import validate_integer\n\nloads = json.loads\ndumps = json.dumps\n\nTABLE_SCHEMA_VERSION = \"0.20.0\"\n\n\n# interface to/from\ndef to_json(\n    path_or_buf,\n    obj: NDFrame,\n    orient: str | None = None,\n    date_format: str = \"epoch\",\n    double_precision: int = 10,\n    force_ascii: bool = True,\n    date_unit: str = \"ms\",\n    default_handler: Callable[[Any], JSONSerializable] | None = None,\n    lines: bool = False,\n    compression: CompressionOptions = \"infer\",\n    index: bool = True,\n    indent: int = 0,\n    storage_options: StorageOptions = None,\n):\n\n    if not index and orient not in [\"split\", \"table\"]:\n        raise ValueError(\n            \"'index=False' is only valid when 'orient' is 'split' or 'table'\"\n        )\n\n    if lines and orient != \"records\":\n        raise ValueError(\"'lines' keyword only valid when 'orient' is records\")\n\n    if orient == \"table\" and isinstance(obj, Series):\n        obj = obj.to_frame(name=obj.name or \"values\")\n\n    writer: type[Writer]\n    if orient == \"table\" and isinstance(obj, DataFrame):\n        writer = JSONTableWriter\n    elif isinstance(obj, Series):\n        writer = SeriesWriter\n    elif isinstance(obj, DataFrame):\n        writer = FrameWriter\n    else:\n        raise NotImplementedError(\"'obj' should be a Series or a DataFrame\")\n\n    s = writer(\n        obj,\n        orient=orient,\n        date_format=date_format,\n        double_precision=double_precision,\n        ensure_ascii=force_ascii,\n        date_unit=date_unit,\n        default_handler=default_handler,\n        index=index,\n        indent=indent,\n    ).write()\n\n    if lines:\n        s = convert_to_line_delimits(s)\n\n    if path_or_buf is not None:\n        # apply compression and byte/text conversion\n        with get_handle(\n            path_or_buf, \"w\", compression=compression, storage_options=storage_options\n        ) as handles:\n            handles.handle.write(s)\n    else:\n        return s\n\n\nclass Writer(ABC):\n    _default_orient: str\n\n    def __init__(\n        self,\n        obj,\n        orient: str | None,\n        date_format: str,\n        double_precision: int,\n        ensure_ascii: bool,\n        date_unit: str,\n        index: bool,\n        default_handler: Callable[[Any], JSONSerializable] | None = None,\n        indent: int = 0,\n    ):\n        self.obj = obj\n\n        if orient is None:\n            orient = self._default_orient\n\n        self.orient = orient\n        self.date_format = date_format\n        self.double_precision = double_precision\n        self.ensure_ascii = ensure_ascii\n        self.date_unit = date_unit\n        self.default_handler = default_handler\n        self.index = index\n        self.indent = indent\n\n        self.is_copy = None\n        self._format_axes()\n\n    def _format_axes(self):\n        raise AbstractMethodError(self)\n\n    def write(self):\n        iso_dates = self.date_format == \"iso\"\n        return dumps(\n            self.obj_to_write,\n            orient=self.orient,\n            double_precision=self.double_precision,\n            ensure_ascii=self.ensure_ascii,\n            date_unit=self.date_unit,\n            iso_dates=iso_dates,\n            default_handler=self.default_handler,\n            indent=self.indent,\n        )\n\n    @property\n    @abstractmethod\n    def obj_to_write(self) -> NDFrame | Mapping[IndexLabel, Any]:\n        \"\"\"Object to write in JSON format.\"\"\"\n        pass\n\n\nclass SeriesWriter(Writer):\n    _default_orient = \"index\"\n\n    @property\n    def obj_to_write(self) -> NDFrame | Mapping[IndexLabel, Any]:\n        if not self.index and self.orient == \"split\":\n            return {\"name\": self.obj.name, \"data\": self.obj.values}\n        else:\n            return self.obj\n\n    def _format_axes(self):\n        if not self.obj.index.is_unique and self.orient == \"index\":\n            raise ValueError(f\"Series index must be unique for orient='{self.orient}'\")\n\n\nclass FrameWriter(Writer):\n    _default_orient = \"columns\"\n\n    @property\n    def obj_to_write(self) -> NDFrame | Mapping[IndexLabel, Any]:\n        if not self.index and self.orient == \"split\":\n            obj_to_write = self.obj.to_dict(orient=\"split\")\n            del obj_to_write[\"index\"]\n        else:\n            obj_to_write = self.obj\n        return obj_to_write\n\n    def _format_axes(self):\n        \"\"\"\n        Try to format axes if they are datelike.\n        \"\"\"\n        if not self.obj.index.is_unique and self.orient in (\"index\", \"columns\"):\n            raise ValueError(\n                f\"DataFrame index must be unique for orient='{self.orient}'.\"\n            )\n        if not self.obj.columns.is_unique and self.orient in (\n            \"index\",\n            \"columns\",\n            \"records\",\n        ):\n            raise ValueError(\n                f\"DataFrame columns must be unique for orient='{self.orient}'.\"\n            )\n\n\nclass JSONTableWriter(FrameWriter):\n    _default_orient = \"records\"\n\n    def __init__(\n        self,\n        obj,\n        orient: str | None,\n        date_format: str,\n        double_precision: int,\n        ensure_ascii: bool,\n        date_unit: str,\n        index: bool,\n        default_handler: Callable[[Any], JSONSerializable] | None = None,\n        indent: int = 0,\n    ):\n        \"\"\"\n        Adds a `schema` attribute with the Table Schema, resets\n        the index (can't do in caller, because the schema inference needs\n        to know what the index is, forces orient to records, and forces\n        date_format to 'iso'.\n        \"\"\"\n        super().__init__(\n            obj,\n            orient,\n            date_format,\n            double_precision,\n            ensure_ascii,\n            date_unit,\n            index,\n            default_handler=default_handler,\n            indent=indent,\n        )\n\n        if date_format != \"iso\":\n            msg = (\n                \"Trying to write with `orient='table'` and \"\n                f\"`date_format='{date_format}'`. Table Schema requires dates \"\n                \"to be formatted with `date_format='iso'`\"\n            )\n            raise ValueError(msg)\n\n        self.schema = build_table_schema(obj, index=self.index)\n\n        # NotImplemented on a column MultiIndex\n        if obj.ndim == 2 and isinstance(obj.columns, MultiIndex):\n            raise NotImplementedError(\n                \"orient='table' is not supported for MultiIndex columns\"\n            )\n\n        # TODO: Do this timedelta properly in objToJSON.c See GH #15137\n        if (\n            (obj.ndim == 1)\n            and (obj.name in set(obj.index.names))\n            or len(obj.columns.intersection(obj.index.names))\n        ):\n            msg = \"Overlapping names between the index and columns\"\n            raise ValueError(msg)\n\n        obj = obj.copy()\n        timedeltas = obj.select_dtypes(include=[\"timedelta\"]).columns\n        if len(timedeltas):\n            obj[timedeltas] = obj[timedeltas].applymap(lambda x: x.isoformat())\n        # Convert PeriodIndex to datetimes before serializing\n        if is_period_dtype(obj.index.dtype):\n            obj.index = obj.index.to_timestamp()\n\n        # exclude index from obj if index=False\n        if not self.index:\n            self.obj = obj.reset_index(drop=True)\n        else:\n            self.obj = obj.reset_index(drop=False)\n        self.date_format = \"iso\"\n        self.orient = \"records\"\n        self.index = index\n\n    @property\n    def obj_to_write(self) -> NDFrame | Mapping[IndexLabel, Any]:\n        return {\"schema\": self.schema, \"data\": self.obj}\n\n\n@doc(storage_options=generic._shared_docs[\"storage_options\"])\n@deprecate_kwarg(old_arg_name=\"numpy\", new_arg_name=None)\n@deprecate_nonkeyword_arguments(\n    version=\"2.0\", allowed_args=[\"path_or_buf\"], stacklevel=3\n)\ndef read_json(\n    path_or_buf=None,\n    orient=None,\n    typ=\"frame\",\n    dtype: DtypeArg | None = None,\n    convert_axes=None,\n    convert_dates=True,\n    keep_default_dates: bool = True,\n    numpy: bool = False,\n    precise_float: bool = False,\n    date_unit=None,\n    encoding=None,\n    encoding_errors: str | None = \"strict\",\n    lines: bool = False,\n    chunksize: int | None = None,\n    compression: CompressionOptions = \"infer\",\n    nrows: int | None = None,\n    storage_options: StorageOptions = None,\n):\n    \"\"\"\n    Convert a JSON string to pandas object.\n\n    Parameters\n    ----------\n    path_or_buf : a valid JSON str, path object or file-like object\n        Any valid string path is acceptable. The string could be a URL. Valid\n        URL schemes include http, ftp, s3, and file. For file URLs, a host is\n        expected. A local file could be:\n        ``file://localhost/path/to/table.json``.\n\n        If you want to pass in a path object, pandas accepts any\n        ``os.PathLike``.\n\n        By file-like object, we refer to objects with a ``read()`` method,\n        such as a file handle (e.g. via builtin ``open`` function)\n        or ``StringIO``.\n    orient : str\n        Indication of expected JSON string format.\n        Compatible JSON strings can be produced by ``to_json()`` with a\n        corresponding orient value.\n        The set of possible orients is:\n\n        - ``'split'`` : dict like\n          ``{{index -> [index], columns -> [columns], data -> [values]}}``\n        - ``'records'`` : list like\n          ``[{{column -> value}}, ... , {{column -> value}}]``\n        - ``'index'`` : dict like ``{{index -> {{column -> value}}}}``\n        - ``'columns'`` : dict like ``{{column -> {{index -> value}}}}``\n        - ``'values'`` : just the values array\n\n        The allowed and default values depend on the value\n        of the `typ` parameter.\n\n        * when ``typ == 'series'``,\n\n          - allowed orients are ``{{'split','records','index'}}``\n          - default is ``'index'``\n          - The Series index must be unique for orient ``'index'``.\n\n        * when ``typ == 'frame'``,\n\n          - allowed orients are ``{{'split','records','index',\n            'columns','values', 'table'}}``\n          - default is ``'columns'``\n          - The DataFrame index must be unique for orients ``'index'`` and\n            ``'columns'``.\n          - The DataFrame columns must be unique for orients ``'index'``,\n            ``'columns'``, and ``'records'``.\n\n    typ : {{'frame', 'series'}}, default 'frame'\n        The type of object to recover.\n\n    dtype : bool or dict, default None\n        If True, infer dtypes; if a dict of column to dtype, then use those;\n        if False, then don't infer dtypes at all, applies only to the data.\n\n        For all ``orient`` values except ``'table'``, default is True.\n\n        .. versionchanged:: 0.25.0\n\n           Not applicable for ``orient='table'``.\n\n    convert_axes : bool, default None\n        Try to convert the axes to the proper dtypes.\n\n        For all ``orient`` values except ``'table'``, default is True.\n\n        .. versionchanged:: 0.25.0\n\n           Not applicable for ``orient='table'``.\n\n    convert_dates : bool or list of str, default True\n        If True then default datelike columns may be converted (depending on\n        keep_default_dates).\n        If False, no dates will be converted.\n        If a list of column names, then those columns will be converted and\n        default datelike columns may also be converted (depending on\n        keep_default_dates).\n\n    keep_default_dates : bool, default True\n        If parsing dates (convert_dates is not False), then try to parse the\n        default datelike columns.\n        A column label is datelike if\n\n        * it ends with ``'_at'``,\n\n        * it ends with ``'_time'``,\n\n        * it begins with ``'timestamp'``,\n\n        * it is ``'modified'``, or\n\n        * it is ``'date'``.\n\n    numpy : bool, default False\n        Direct decoding to numpy arrays. Supports numeric data only, but\n        non-numeric column and index labels are supported. Note also that the\n        JSON ordering MUST be the same for each term if numpy=True.\n\n        .. deprecated:: 1.0.0\n\n    precise_float : bool, default False\n        Set to enable usage of higher precision (strtod) function when\n        decoding string to double values. Default (False) is to use fast but\n        less precise builtin functionality.\n\n    date_unit : str, default None\n        The timestamp unit to detect if converting dates. The default behaviour\n        is to try and detect the correct precision, but if this is not desired\n        then pass one of 's', 'ms', 'us' or 'ns' to force parsing only seconds,\n        milliseconds, microseconds or nanoseconds respectively.\n\n    encoding : str, default is 'utf-8'\n        The encoding to use to decode py3 bytes.\n\n    encoding_errors : str, optional, default \"strict\"\n        How encoding errors are treated. `List of possible values\n        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n        .. versionadded:: 1.3.0\n\n    lines : bool, default False\n        Read the file as a json object per line.\n\n    chunksize : int, optional\n        Return JsonReader object for iteration.\n        See the `line-delimited json docs\n        <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#line-delimited-json>`_\n        for more information on ``chunksize``.\n        This can only be passed if `lines=True`.\n        If this is None, the file will be read into memory all at once.\n\n        .. versionchanged:: 1.2\n\n           ``JsonReader`` is a context manager.\n\n    compression : {{'infer', 'gzip', 'bz2', 'zip', 'xz', None}}, default 'infer'\n        For on-the-fly decompression of on-disk data. If 'infer', then use\n        gzip, bz2, zip or xz if path_or_buf is a string ending in\n        '.gz', '.bz2', '.zip', or 'xz', respectively, and no decompression\n        otherwise. If using 'zip', the ZIP file must contain only one data\n        file to be read in. Set to None for no decompression.\n\n    nrows : int, optional\n        The number of lines from the line-delimited jsonfile that has to be read.\n        This can only be passed if `lines=True`.\n        If this is None, all the rows will be returned.\n\n        .. versionadded:: 1.1\n\n    {storage_options}\n\n        .. versionadded:: 1.2.0\n\n    Returns\n    -------\n    Series or DataFrame\n        The type returned depends on the value of `typ`.\n\n    See Also\n    --------\n    DataFrame.to_json : Convert a DataFrame to a JSON string.\n    Series.to_json : Convert a Series to a JSON string.\n\n    Notes\n    -----\n    Specific to ``orient='table'``, if a :class:`DataFrame` with a literal\n    :class:`Index` name of `index` gets written with :func:`to_json`, the\n    subsequent read operation will incorrectly set the :class:`Index` name to\n    ``None``. This is because `index` is also used by :func:`DataFrame.to_json`\n    to denote a missing :class:`Index` name, and the subsequent\n    :func:`read_json` operation cannot distinguish between the two. The same\n    limitation is encountered with a :class:`MultiIndex` and any names\n    beginning with ``'level_'``.\n\n    Examples\n    --------\n    >>> df = pd.DataFrame([['a', 'b'], ['c', 'd']],\n    ...                   index=['row 1', 'row 2'],\n    ...                   columns=['col 1', 'col 2'])\n\n    Encoding/decoding a Dataframe using ``'split'`` formatted JSON:\n\n    >>> df.to_json(orient='split')\n        '\\\n{{\\\n\"columns\":[\"col 1\",\"col 2\"],\\\n\"index\":[\"row 1\",\"row 2\"],\\\n\"data\":[[\"a\",\"b\"],[\"c\",\"d\"]]\\\n}}\\\n'\n    >>> pd.read_json(_, orient='split')\n          col 1 col 2\n    row 1     a     b\n    row 2     c     d\n\n    Encoding/decoding a Dataframe using ``'index'`` formatted JSON:\n\n    >>> df.to_json(orient='index')\n    '{{\"row 1\":{{\"col 1\":\"a\",\"col 2\":\"b\"}},\"row 2\":{{\"col 1\":\"c\",\"col 2\":\"d\"}}}}'\n\n    >>> pd.read_json(_, orient='index')\n          col 1 col 2\n    row 1     a     b\n    row 2     c     d\n\n    Encoding/decoding a Dataframe using ``'records'`` formatted JSON.\n    Note that index labels are not preserved with this encoding.\n\n    >>> df.to_json(orient='records')\n    '[{{\"col 1\":\"a\",\"col 2\":\"b\"}},{{\"col 1\":\"c\",\"col 2\":\"d\"}}]'\n    >>> pd.read_json(_, orient='records')\n      col 1 col 2\n    0     a     b\n    1     c     d\n\n    Encoding with Table Schema\n\n    >>> df.to_json(orient='table')\n        '\\\n{{\"schema\":{{\"fields\":[\\\n{{\"name\":\"index\",\"type\":\"string\"}},\\\n{{\"name\":\"col 1\",\"type\":\"string\"}},\\\n{{\"name\":\"col 2\",\"type\":\"string\"}}],\\\n\"primaryKey\":[\"index\"],\\\n\"pandas_version\":\"0.20.0\"}},\\\n\"data\":[\\\n{{\"index\":\"row 1\",\"col 1\":\"a\",\"col 2\":\"b\"}},\\\n{{\"index\":\"row 2\",\"col 1\":\"c\",\"col 2\":\"d\"}}]\\\n}}\\\n'\n    \"\"\"\n    if orient == \"table\" and dtype:\n        raise ValueError(\"cannot pass both dtype and orient='table'\")\n    if orient == \"table\" and convert_axes:\n        raise ValueError(\"cannot pass both convert_axes and orient='table'\")\n\n    if dtype is None and orient != \"table\":\n        # error: Incompatible types in assignment (expression has type \"bool\", variable\n        # has type \"Union[ExtensionDtype, str, dtype[Any], Type[str], Type[float],\n        # Type[int], Type[complex], Type[bool], Type[object], Dict[Hashable,\n        # Union[ExtensionDtype, Union[str, dtype[Any]], Type[str], Type[float],\n        # Type[int], Type[complex], Type[bool], Type[object]]], None]\")\n        dtype = True  # type: ignore[assignment]\n    if convert_axes is None and orient != \"table\":\n        convert_axes = True\n\n    json_reader = JsonReader(\n        path_or_buf,\n        orient=orient,\n        typ=typ,\n        dtype=dtype,\n        convert_axes=convert_axes,\n        convert_dates=convert_dates,\n        keep_default_dates=keep_default_dates,\n        numpy=numpy,\n        precise_float=precise_float,\n        date_unit=date_unit,\n        encoding=encoding,\n        lines=lines,\n        chunksize=chunksize,\n        compression=compression,\n        nrows=nrows,\n        storage_options=storage_options,\n        encoding_errors=encoding_errors,\n    )\n\n    if chunksize:\n        return json_reader\n\n    with json_reader:\n        return json_reader.read()\n\n\nclass JsonReader(abc.Iterator):\n    \"\"\"\n    JsonReader provides an interface for reading in a JSON file.\n\n    If initialized with ``lines=True`` and ``chunksize``, can be iterated over\n    ``chunksize`` lines at a time. Otherwise, calling ``read`` reads in the\n    whole document.\n    \"\"\"\n\n    def __init__(\n        self,\n        filepath_or_buffer,\n        orient,\n        typ,\n        dtype,\n        convert_axes,\n        convert_dates,\n        keep_default_dates: bool,\n        numpy: bool,\n        precise_float: bool,\n        date_unit,\n        encoding,\n        lines: bool,\n        chunksize: int | None,\n        compression: CompressionOptions,\n        nrows: int | None,\n        storage_options: StorageOptions = None,\n        encoding_errors: str | None = \"strict\",\n    ):\n\n        self.orient = orient\n        self.typ = typ\n        self.dtype = dtype\n        self.convert_axes = convert_axes\n        self.convert_dates = convert_dates\n        self.keep_default_dates = keep_default_dates\n        self.numpy = numpy\n        self.precise_float = precise_float\n        self.date_unit = date_unit\n        self.encoding = encoding\n        self.compression = compression\n        self.storage_options = storage_options\n        self.lines = lines\n        self.chunksize = chunksize\n        self.nrows_seen = 0\n        self.nrows = nrows\n        self.encoding_errors = encoding_errors\n        self.handles: IOHandles | None = None\n\n        if self.chunksize is not None:\n            self.chunksize = validate_integer(\"chunksize\", self.chunksize, 1)\n            if not self.lines:\n                raise ValueError(\"chunksize can only be passed if lines=True\")\n        if self.nrows is not None:\n            self.nrows = validate_integer(\"nrows\", self.nrows, 0)\n            if not self.lines:\n                raise ValueError(\"nrows can only be passed if lines=True\")\n\n        data = self._get_data_from_filepath(filepath_or_buffer)\n        self.data = self._preprocess_data(data)\n\n    def _preprocess_data(self, data):\n        \"\"\"\n        At this point, the data either has a `read` attribute (e.g. a file\n        object or a StringIO) or is a string that is a JSON document.\n\n        If self.chunksize, we prepare the data for the `__next__` method.\n        Otherwise, we read it into memory for the `read` method.\n        \"\"\"\n        if hasattr(data, \"read\") and not (self.chunksize or self.nrows):\n            with self:\n                data = data.read()\n        if not hasattr(data, \"read\") and (self.chunksize or self.nrows):\n            data = StringIO(data)\n\n        return data\n\n    def _get_data_from_filepath(self, filepath_or_buffer):\n        \"\"\"\n        The function read_json accepts three input types:\n            1. filepath (string-like)\n            2. file-like object (e.g. open file object, StringIO)\n            3. JSON string\n\n        This method turns (1) into (2) to simplify the rest of the processing.\n        It returns input types (2) and (3) unchanged.\n        \"\"\"\n        # if it is a string but the file does not exist, it might be a JSON string\n        filepath_or_buffer = stringify_path(filepath_or_buffer)\n        if (\n            not isinstance(filepath_or_buffer, str)\n            or is_url(filepath_or_buffer)\n            or is_fsspec_url(filepath_or_buffer)\n            or file_exists(filepath_or_buffer)\n        ):\n            self.handles = get_handle(\n                filepath_or_buffer,\n                \"r\",\n                encoding=self.encoding,\n                compression=self.compression,\n                storage_options=self.storage_options,\n                errors=self.encoding_errors,\n            )\n            filepath_or_buffer = self.handles.handle\n\n        return filepath_or_buffer\n\n    def _combine_lines(self, lines) -> str:\n        \"\"\"\n        Combines a list of JSON objects into one JSON object.\n        \"\"\"\n        return (\n            f'[{\",\".join((line for line in (line.strip() for line in lines) if line))}]'\n        )\n\n    def read(self):\n        \"\"\"\n        Read the whole JSON input into a pandas object.\n        \"\"\"\n        if self.lines:\n            if self.chunksize:\n                obj = concat(self)\n            elif self.nrows:\n                lines = list(islice(self.data, self.nrows))\n                lines_json = self._combine_lines(lines)\n                obj = self._get_object_parser(lines_json)\n            else:\n                data = ensure_str(self.data)\n                data_lines = data.split(\"\\n\")\n                obj = self._get_object_parser(self._combine_lines(data_lines))\n        else:\n            obj = self._get_object_parser(self.data)\n        self.close()\n        return obj\n\n    def _get_object_parser(self, json):\n        \"\"\"\n        Parses a json document into a pandas object.\n        \"\"\"\n        typ = self.typ\n        dtype = self.dtype\n        kwargs = {\n            \"orient\": self.orient,\n            \"dtype\": self.dtype,\n            \"convert_axes\": self.convert_axes,\n            \"convert_dates\": self.convert_dates,\n            \"keep_default_dates\": self.keep_default_dates,\n            \"numpy\": self.numpy,\n            \"precise_float\": self.precise_float,\n            \"date_unit\": self.date_unit,\n        }\n        obj = None\n        if typ == \"frame\":\n            obj = FrameParser(json, **kwargs).parse()\n\n        if typ == \"series\" or obj is None:\n            if not isinstance(dtype, bool):\n                kwargs[\"dtype\"] = dtype\n            obj = SeriesParser(json, **kwargs).parse()\n\n        return obj\n\n    def close(self):\n        \"\"\"\n        If we opened a stream earlier, in _get_data_from_filepath, we should\n        close it.\n\n        If an open stream or file was passed, we leave it open.\n        \"\"\"\n        if self.handles is not None:\n            self.handles.close()\n\n    def __next__(self):\n        if self.nrows:\n            if self.nrows_seen >= self.nrows:\n                self.close()\n                raise StopIteration\n\n        lines = list(islice(self.data, self.chunksize))\n        if lines:\n            lines_json = self._combine_lines(lines)\n            obj = self._get_object_parser(lines_json)\n\n            # Make sure that the returned objects have the right index.\n            obj.index = range(self.nrows_seen, self.nrows_seen + len(obj))\n            self.nrows_seen += len(obj)\n\n            return obj\n\n        self.close()\n        raise StopIteration\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n\nclass Parser:\n    _split_keys: tuple[str, ...]\n    _default_orient: str\n\n    _STAMP_UNITS = (\"s\", \"ms\", \"us\", \"ns\")\n    _MIN_STAMPS = {\n        \"s\": 31536000,\n        \"ms\": 31536000000,\n        \"us\": 31536000000000,\n        \"ns\": 31536000000000000,\n    }\n\n    def __init__(\n        self,\n        json,\n        orient,\n        dtype: DtypeArg | None = None,\n        convert_axes=True,\n        convert_dates=True,\n        keep_default_dates=False,\n        numpy=False,\n        precise_float=False,\n        date_unit=None,\n    ):\n        self.json = json\n\n        if orient is None:\n            orient = self._default_orient\n\n        self.orient = orient\n\n        self.dtype = dtype\n\n        if orient == \"split\":\n            numpy = False\n\n        if date_unit is not None:\n            date_unit = date_unit.lower()\n            if date_unit not in self._STAMP_UNITS:\n                raise ValueError(f\"date_unit must be one of {self._STAMP_UNITS}\")\n            self.min_stamp = self._MIN_STAMPS[date_unit]\n        else:\n            self.min_stamp = self._MIN_STAMPS[\"s\"]\n\n        self.numpy = numpy\n        self.precise_float = precise_float\n        self.convert_axes = convert_axes\n        self.convert_dates = convert_dates\n        self.date_unit = date_unit\n        self.keep_default_dates = keep_default_dates\n        self.obj: FrameOrSeriesUnion | None = None\n\n    def check_keys_split(self, decoded):\n        \"\"\"\n        Checks that dict has only the appropriate keys for orient='split'.\n        \"\"\"\n        bad_keys = set(decoded.keys()).difference(set(self._split_keys))\n        if bad_keys:\n            bad_keys_joined = \", \".join(bad_keys)\n            raise ValueError(f\"JSON data had unexpected key(s): {bad_keys_joined}\")\n\n    def parse(self):\n\n        # try numpy\n        numpy = self.numpy\n        if numpy:\n            self._parse_numpy()\n\n        else:\n            self._parse_no_numpy()\n\n        if self.obj is None:\n            return None\n        if self.convert_axes:\n            self._convert_axes()\n        self._try_convert_types()\n        return self.obj\n\n    def _parse_numpy(self):\n        raise AbstractMethodError(self)\n\n    def _parse_no_numpy(self):\n        raise AbstractMethodError(self)\n\n    def _convert_axes(self):\n        \"\"\"\n        Try to convert axes.\n        \"\"\"\n        obj = self.obj\n        assert obj is not None  # for mypy\n        for axis_name in obj._AXIS_ORDERS:\n            new_axis, result = self._try_convert_data(\n                name=axis_name,\n                data=obj._get_axis(axis_name),\n                use_dtypes=False,\n                convert_dates=True,\n            )\n            if result:\n                setattr(self.obj, axis_name, new_axis)\n\n    def _try_convert_types(self):\n        raise AbstractMethodError(self)\n\n    def _try_convert_data(self, name, data, use_dtypes=True, convert_dates=True):\n        \"\"\"\n        Try to parse a ndarray like into a column by inferring dtype.\n        \"\"\"\n        # don't try to coerce, unless a force conversion\n        if use_dtypes:\n            if not self.dtype:\n                if all(notna(data)):\n                    return data, False\n                return data.fillna(np.nan), True\n\n            # error: Non-overlapping identity check (left operand type:\n            # \"Union[ExtensionDtype, str, dtype[Any], Type[object],\n            # Dict[Hashable, Union[ExtensionDtype, Union[str, dtype[Any]],\n            # Type[str], Type[float], Type[int], Type[complex], Type[bool],\n            # Type[object]]]]\", right operand type: \"Literal[True]\")\n            elif self.dtype is True:  # type: ignore[comparison-overlap]\n                pass\n            else:\n                # dtype to force\n                dtype = (\n                    self.dtype.get(name) if isinstance(self.dtype, dict) else self.dtype\n                )\n                if dtype is not None:\n                    try:\n                        # error: Argument 1 to \"dtype\" has incompatible type\n                        # \"Union[ExtensionDtype, str, dtype[Any], Type[object]]\";\n                        # expected \"Type[Any]\"\n                        dtype = np.dtype(dtype)  # type: ignore[arg-type]\n                        return data.astype(dtype), True\n                    except (TypeError, ValueError):\n                        return data, False\n\n        if convert_dates:\n            new_data, result = self._try_convert_to_date(data)\n            if result:\n                return new_data, True\n\n        if data.dtype == \"object\":\n\n            # try float\n            try:\n                data = data.astype(\"float64\")\n            except (TypeError, ValueError):\n                pass\n\n        if data.dtype.kind == \"f\":\n\n            if data.dtype != \"float64\":\n\n                # coerce floats to 64\n                try:\n                    data = data.astype(\"float64\")\n                except (TypeError, ValueError):\n                    pass\n\n        # don't coerce 0-len data\n        if len(data) and (data.dtype == \"float\" or data.dtype == \"object\"):\n\n            # coerce ints if we can\n            try:\n                new_data = data.astype(\"int64\")\n                if (new_data == data).all():\n                    data = new_data\n            except (TypeError, ValueError, OverflowError):\n                pass\n\n        # coerce ints to 64\n        if data.dtype == \"int\":\n\n            # coerce floats to 64\n            try:\n                data = data.astype(\"int64\")\n            except (TypeError, ValueError):\n                pass\n\n        # if we have an index, we want to preserve dtypes\n        if name == \"index\" and len(data):\n            if self.orient == \"split\":\n                return data, False\n\n        return data, True\n\n    def _try_convert_to_date(self, data):\n        \"\"\"\n        Try to parse a ndarray like into a date column.\n\n        Try to coerce object in epoch/iso formats and integer/float in epoch\n        formats. Return a boolean if parsing was successful.\n        \"\"\"\n        # no conversion on empty\n        if not len(data):\n            return data, False\n\n        new_data = data\n        if new_data.dtype == \"object\":\n            try:\n                new_data = data.astype(\"int64\")\n            except (TypeError, ValueError, OverflowError):\n                pass\n\n        # ignore numbers that are out of range\n        if issubclass(new_data.dtype.type, np.number):\n            in_range = (\n                isna(new_data._values)\n                | (new_data > self.min_stamp)\n                | (new_data._values == iNaT)\n            )\n            if not in_range.all():\n                return data, False\n\n        date_units = (self.date_unit,) if self.date_unit else self._STAMP_UNITS\n        for date_unit in date_units:\n            try:\n                new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError, TypeError):\n                continue\n            return new_data, True\n        return data, False\n\n    def _try_convert_dates(self):\n        raise AbstractMethodError(self)\n\n\nclass SeriesParser(Parser):\n    _default_orient = \"index\"\n    _split_keys = (\"name\", \"index\", \"data\")\n\n    def _parse_no_numpy(self):\n        data = loads(self.json, precise_float=self.precise_float)\n\n        if self.orient == \"split\":\n            decoded = {str(k): v for k, v in data.items()}\n            self.check_keys_split(decoded)\n            self.obj = create_series_with_explicit_dtype(**decoded)\n        else:\n            self.obj = create_series_with_explicit_dtype(data, dtype_if_empty=object)\n\n    def _parse_numpy(self):\n        load_kwargs = {\n            \"dtype\": None,\n            \"numpy\": True,\n            \"precise_float\": self.precise_float,\n        }\n        if self.orient in [\"columns\", \"index\"]:\n            load_kwargs[\"labelled\"] = True\n        loads_ = functools.partial(loads, **load_kwargs)\n        data = loads_(self.json)\n\n        if self.orient == \"split\":\n            decoded = {str(k): v for k, v in data.items()}\n            self.check_keys_split(decoded)\n            self.obj = create_series_with_explicit_dtype(**decoded)\n        elif self.orient in [\"columns\", \"index\"]:\n            # error: \"create_series_with_explicit_dtype\"\n            #  gets multiple values for keyword argument \"dtype_if_empty\n            self.obj = create_series_with_explicit_dtype(\n                *data, dtype_if_empty=object\n            )  # type:ignore[misc]\n        else:\n            self.obj = create_series_with_explicit_dtype(data, dtype_if_empty=object)\n\n    def _try_convert_types(self):\n        if self.obj is None:\n            return\n        obj, result = self._try_convert_data(\n            \"data\", self.obj, convert_dates=self.convert_dates\n        )\n        if result:\n            self.obj = obj\n\n\nclass FrameParser(Parser):\n    _default_orient = \"columns\"\n    _split_keys = (\"columns\", \"index\", \"data\")\n\n    def _parse_numpy(self):\n\n        json = self.json\n        orient = self.orient\n\n        if orient == \"columns\":\n            args = loads(\n                json,\n                dtype=None,\n                numpy=True,\n                labelled=True,\n                precise_float=self.precise_float,\n            )\n            if len(args):\n                args = (args[0].T, args[2], args[1])\n            self.obj = DataFrame(*args)\n        elif orient == \"split\":\n            decoded = loads(\n                json, dtype=None, numpy=True, precise_float=self.precise_float\n            )\n            decoded = {str(k): v for k, v in decoded.items()}\n            self.check_keys_split(decoded)\n            self.obj = DataFrame(**decoded)\n        elif orient == \"values\":\n            self.obj = DataFrame(\n                loads(json, dtype=None, numpy=True, precise_float=self.precise_float)\n            )\n        else:\n            self.obj = DataFrame(\n                *loads(\n                    json,\n                    dtype=None,\n                    numpy=True,\n                    labelled=True,\n                    precise_float=self.precise_float,\n                )\n            )\n\n    def _parse_no_numpy(self):\n\n        json = self.json\n        orient = self.orient\n\n        if orient == \"columns\":\n            self.obj = DataFrame(\n                loads(json, precise_float=self.precise_float), dtype=None\n            )\n        elif orient == \"split\":\n            decoded = {\n                str(k): v\n                for k, v in loads(json, precise_float=self.precise_float).items()\n            }\n            self.check_keys_split(decoded)\n            self.obj = DataFrame(dtype=None, **decoded)\n        elif orient == \"index\":\n            self.obj = DataFrame.from_dict(\n                loads(json, precise_float=self.precise_float),\n                dtype=None,\n                orient=\"index\",\n            )\n        elif orient == \"table\":\n            self.obj = parse_table_schema(json, precise_float=self.precise_float)\n        else:\n            self.obj = DataFrame(\n                loads(json, precise_float=self.precise_float), dtype=None\n            )\n\n    def _process_converter(self, f, filt=None):\n        \"\"\"\n        Take a conversion function and possibly recreate the frame.\n        \"\"\"\n        if filt is None:\n            filt = lambda col, c: True\n\n        obj = self.obj\n        assert obj is not None  # for mypy\n\n        needs_new_obj = False\n        new_obj = {}\n        for i, (col, c) in enumerate(obj.items()):\n            if filt(col, c):\n                new_data, result = f(col, c)\n                if result:\n                    c = new_data\n                    needs_new_obj = True\n            new_obj[i] = c\n\n        if needs_new_obj:\n\n            # possibly handle dup columns\n            new_frame = DataFrame(new_obj, index=obj.index)\n            new_frame.columns = obj.columns\n            self.obj = new_frame\n\n    def _try_convert_types(self):\n        if self.obj is None:\n            return\n        if self.convert_dates:\n            self._try_convert_dates()\n\n        self._process_converter(\n            lambda col, c: self._try_convert_data(col, c, convert_dates=False)\n        )\n\n    def _try_convert_dates(self):\n        if self.obj is None:\n            return\n\n        # our columns to parse\n        convert_dates = self.convert_dates\n        if convert_dates is True:\n            convert_dates = []\n        convert_dates = set(convert_dates)\n\n        def is_ok(col) -> bool:\n            \"\"\"\n            Return if this col is ok to try for a date parse.\n            \"\"\"\n            if not isinstance(col, str):\n                return False\n\n            col_lower = col.lower()\n            if (\n                col_lower.endswith(\"_at\")\n                or col_lower.endswith(\"_time\")\n                or col_lower == \"modified\"\n                or col_lower == \"date\"\n                or col_lower == \"datetime\"\n                or col_lower.startswith(\"timestamp\")\n            ):\n                return True\n            return False\n\n        self._process_converter(\n            lambda col, c: self._try_convert_to_date(c),\n            lambda col, c: (\n                (self.keep_default_dates and is_ok(col)) or col in convert_dates\n            ),\n        )\n"
    },
    {
      "filename": "pandas/plotting/_matplotlib/converter.py",
      "content": "from __future__ import annotations\n\nimport contextlib\nimport datetime as pydt\nfrom datetime import (\n    datetime,\n    timedelta,\n    tzinfo,\n)\nimport functools\nfrom typing import Any\n\nfrom dateutil.relativedelta import relativedelta\nimport matplotlib.dates as dates\nfrom matplotlib.ticker import (\n    AutoLocator,\n    Formatter,\n    Locator,\n)\nfrom matplotlib.transforms import nonsingular\nimport matplotlib.units as units\nimport numpy as np\n\nfrom pandas._libs import lib\nfrom pandas._libs.tslibs import (\n    Timestamp,\n    to_offset,\n)\nfrom pandas._libs.tslibs.dtypes import FreqGroup\nfrom pandas._libs.tslibs.offsets import BaseOffset\n\nfrom pandas.core.dtypes.common import (\n    is_float,\n    is_float_dtype,\n    is_integer,\n    is_integer_dtype,\n    is_nested_list_like,\n)\n\nfrom pandas import (\n    Index,\n    Series,\n    get_option,\n)\nimport pandas.core.common as com\nfrom pandas.core.indexes.datetimes import date_range\nfrom pandas.core.indexes.period import (\n    Period,\n    PeriodIndex,\n    period_range,\n)\nimport pandas.core.tools.datetimes as tools\n\n# constants\nHOURS_PER_DAY = 24.0\nMIN_PER_HOUR = 60.0\nSEC_PER_MIN = 60.0\n\nSEC_PER_HOUR = SEC_PER_MIN * MIN_PER_HOUR\nSEC_PER_DAY = SEC_PER_HOUR * HOURS_PER_DAY\n\nMUSEC_PER_DAY = 10 ** 6 * SEC_PER_DAY\n\n_mpl_units = {}  # Cache for units overwritten by us\n\n\ndef get_pairs():\n    pairs = [\n        (Timestamp, DatetimeConverter),\n        (Period, PeriodConverter),\n        (pydt.datetime, DatetimeConverter),\n        (pydt.date, DatetimeConverter),\n        (pydt.time, TimeConverter),\n        (np.datetime64, DatetimeConverter),\n    ]\n    return pairs\n\n\ndef register_pandas_matplotlib_converters(func):\n    \"\"\"\n    Decorator applying pandas_converters.\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        with pandas_converters():\n            return func(*args, **kwargs)\n\n    return wrapper\n\n\n@contextlib.contextmanager\ndef pandas_converters():\n    \"\"\"\n    Context manager registering pandas' converters for a plot.\n\n    See Also\n    --------\n    register_pandas_matplotlib_converters : Decorator that applies this.\n    \"\"\"\n    value = get_option(\"plotting.matplotlib.register_converters\")\n\n    if value:\n        # register for True or \"auto\"\n        register()\n    try:\n        yield\n    finally:\n        if value == \"auto\":\n            # only deregister for \"auto\"\n            deregister()\n\n\ndef register():\n    pairs = get_pairs()\n    for type_, cls in pairs:\n        # Cache previous converter if present\n        if type_ in units.registry and not isinstance(units.registry[type_], cls):\n            previous = units.registry[type_]\n            _mpl_units[type_] = previous\n        # Replace with pandas converter\n        units.registry[type_] = cls()\n\n\ndef deregister():\n    # Renamed in pandas.plotting.__init__\n    for type_, cls in get_pairs():\n        # We use type to catch our classes directly, no inheritance\n        if type(units.registry.get(type_)) is cls:\n            units.registry.pop(type_)\n\n    # restore the old keys\n    for unit, formatter in _mpl_units.items():\n        if type(formatter) not in {DatetimeConverter, PeriodConverter, TimeConverter}:\n            # make it idempotent by excluding ours.\n            units.registry[unit] = formatter\n\n\ndef _to_ordinalf(tm: pydt.time) -> float:\n    tot_sec = tm.hour * 3600 + tm.minute * 60 + tm.second + tm.microsecond / 10 ** 6\n    return tot_sec\n\n\ndef time2num(d):\n    if isinstance(d, str):\n        parsed = tools.to_datetime(d)\n        if not isinstance(parsed, datetime):\n            raise ValueError(f\"Could not parse time {d}\")\n        return _to_ordinalf(parsed.time())\n    if isinstance(d, pydt.time):\n        return _to_ordinalf(d)\n    return d\n\n\nclass TimeConverter(units.ConversionInterface):\n    @staticmethod\n    def convert(value, unit, axis):\n        valid_types = (str, pydt.time)\n        if isinstance(value, valid_types) or is_integer(value) or is_float(value):\n            return time2num(value)\n        if isinstance(value, Index):\n            return value.map(time2num)\n        if isinstance(value, (list, tuple, np.ndarray, Index)):\n            return [time2num(x) for x in value]\n        return value\n\n    @staticmethod\n    def axisinfo(unit, axis) -> units.AxisInfo | None:\n        if unit != \"time\":\n            return None\n\n        majloc = AutoLocator()\n        majfmt = TimeFormatter(majloc)\n        return units.AxisInfo(majloc=majloc, majfmt=majfmt, label=\"time\")\n\n    @staticmethod\n    def default_units(x, axis) -> str:\n        return \"time\"\n\n\n# time formatter\nclass TimeFormatter(Formatter):\n    def __init__(self, locs):\n        self.locs = locs\n\n    def __call__(self, x, pos=0) -> str:\n        \"\"\"\n        Return the time of day as a formatted string.\n\n        Parameters\n        ----------\n        x : float\n            The time of day specified as seconds since 00:00 (midnight),\n            with up to microsecond precision.\n        pos\n            Unused\n\n        Returns\n        -------\n        str\n            A string in HH:MM:SS.mmmuuu format. Microseconds,\n            milliseconds and seconds are only displayed if non-zero.\n        \"\"\"\n        fmt = \"%H:%M:%S.%f\"\n        s = int(x)\n        msus = round((x - s) * 10 ** 6)\n        ms = msus // 1000\n        us = msus % 1000\n        m, s = divmod(s, 60)\n        h, m = divmod(m, 60)\n        _, h = divmod(h, 24)\n        if us != 0:\n            return pydt.time(h, m, s, msus).strftime(fmt)\n        elif ms != 0:\n            return pydt.time(h, m, s, msus).strftime(fmt)[:-3]\n        elif s != 0:\n            return pydt.time(h, m, s).strftime(\"%H:%M:%S\")\n\n        return pydt.time(h, m).strftime(\"%H:%M\")\n\n\n# Period Conversion\n\n\nclass PeriodConverter(dates.DateConverter):\n    @staticmethod\n    def convert(values, units, axis):\n        if is_nested_list_like(values):\n            values = [PeriodConverter._convert_1d(v, units, axis) for v in values]\n        else:\n            values = PeriodConverter._convert_1d(values, units, axis)\n        return values\n\n    @staticmethod\n    def _convert_1d(values, units, axis):\n        if not hasattr(axis, \"freq\"):\n            raise TypeError(\"Axis must have `freq` set to convert to Periods\")\n        valid_types = (str, datetime, Period, pydt.date, pydt.time, np.datetime64)\n        if isinstance(values, valid_types) or is_integer(values) or is_float(values):\n            return get_datevalue(values, axis.freq)\n        elif isinstance(values, PeriodIndex):\n            return values.asfreq(axis.freq).asi8\n        elif isinstance(values, Index):\n            return values.map(lambda x: get_datevalue(x, axis.freq))\n        elif lib.infer_dtype(values, skipna=False) == \"period\":\n            # https://github.com/pandas-dev/pandas/issues/24304\n            # convert ndarray[period] -> PeriodIndex\n            return PeriodIndex(values, freq=axis.freq).asi8\n        elif isinstance(values, (list, tuple, np.ndarray, Index)):\n            return [get_datevalue(x, axis.freq) for x in values]\n        return values\n\n\ndef get_datevalue(date, freq):\n    if isinstance(date, Period):\n        return date.asfreq(freq).ordinal\n    elif isinstance(date, (str, datetime, pydt.date, pydt.time, np.datetime64)):\n        return Period(date, freq).ordinal\n    elif (\n        is_integer(date)\n        or is_float(date)\n        or (isinstance(date, (np.ndarray, Index)) and (date.size == 1))\n    ):\n        return date\n    elif date is None:\n        return None\n    raise ValueError(f\"Unrecognizable date '{date}'\")\n\n\n# Datetime Conversion\nclass DatetimeConverter(dates.DateConverter):\n    @staticmethod\n    def convert(values, unit, axis):\n        # values might be a 1-d array, or a list-like of arrays.\n        if is_nested_list_like(values):\n            values = [DatetimeConverter._convert_1d(v, unit, axis) for v in values]\n        else:\n            values = DatetimeConverter._convert_1d(values, unit, axis)\n        return values\n\n    @staticmethod\n    def _convert_1d(values, unit, axis):\n        def try_parse(values):\n            try:\n                return dates.date2num(tools.to_datetime(values))\n            except Exception:\n                return values\n\n        if isinstance(values, (datetime, pydt.date, np.datetime64, pydt.time)):\n            return dates.date2num(values)\n        elif is_integer(values) or is_float(values):\n            return values\n        elif isinstance(values, str):\n            return try_parse(values)\n        elif isinstance(values, (list, tuple, np.ndarray, Index, Series)):\n            if isinstance(values, Series):\n                # https://github.com/matplotlib/matplotlib/issues/11391\n                # Series was skipped. Convert to DatetimeIndex to get asi8\n                values = Index(values)\n            if isinstance(values, Index):\n                values = values.values\n            if not isinstance(values, np.ndarray):\n                values = com.asarray_tuplesafe(values)\n\n            if is_integer_dtype(values) or is_float_dtype(values):\n                return values\n\n            try:\n                values = tools.to_datetime(values)\n            except Exception:\n                pass\n\n            values = dates.date2num(values)\n\n        return values\n\n    @staticmethod\n    def axisinfo(unit: tzinfo | None, axis) -> units.AxisInfo:\n        \"\"\"\n        Return the :class:`~matplotlib.units.AxisInfo` for *unit*.\n\n        *unit* is a tzinfo instance or None.\n        The *axis* argument is required but not used.\n        \"\"\"\n        tz = unit\n\n        majloc = PandasAutoDateLocator(tz=tz)\n        majfmt = PandasAutoDateFormatter(majloc, tz=tz)\n        datemin = pydt.date(2000, 1, 1)\n        datemax = pydt.date(2010, 1, 1)\n\n        return units.AxisInfo(\n            majloc=majloc, majfmt=majfmt, label=\"\", default_limits=(datemin, datemax)\n        )\n\n\nclass PandasAutoDateFormatter(dates.AutoDateFormatter):\n    def __init__(self, locator, tz=None, defaultfmt=\"%Y-%m-%d\"):\n        dates.AutoDateFormatter.__init__(self, locator, tz, defaultfmt)\n\n\nclass PandasAutoDateLocator(dates.AutoDateLocator):\n    def get_locator(self, dmin, dmax):\n        \"\"\"Pick the best locator based on a distance.\"\"\"\n        delta = relativedelta(dmax, dmin)\n\n        num_days = (delta.years * 12.0 + delta.months) * 31.0 + delta.days\n        num_sec = (delta.hours * 60.0 + delta.minutes) * 60.0 + delta.seconds\n        tot_sec = num_days * 86400.0 + num_sec\n\n        if abs(tot_sec) < self.minticks:\n            self._freq = -1\n            locator = MilliSecondLocator(self.tz)\n            locator.set_axis(self.axis)\n\n            locator.set_view_interval(*self.axis.get_view_interval())\n            locator.set_data_interval(*self.axis.get_data_interval())\n            return locator\n\n        return dates.AutoDateLocator.get_locator(self, dmin, dmax)\n\n    def _get_unit(self):\n        return MilliSecondLocator.get_unit_generic(self._freq)\n\n\nclass MilliSecondLocator(dates.DateLocator):\n\n    UNIT = 1.0 / (24 * 3600 * 1000)\n\n    def __init__(self, tz):\n        dates.DateLocator.__init__(self, tz)\n        self._interval = 1.0\n\n    def _get_unit(self):\n        return self.get_unit_generic(-1)\n\n    @staticmethod\n    def get_unit_generic(freq):\n        unit = dates.RRuleLocator.get_unit_generic(freq)\n        if unit < 0:\n            return MilliSecondLocator.UNIT\n        return unit\n\n    def __call__(self):\n        # if no data have been set, this will tank with a ValueError\n        try:\n            dmin, dmax = self.viewlim_to_dt()\n        except ValueError:\n            return []\n\n        # We need to cap at the endpoints of valid datetime\n        nmax, nmin = dates.date2num((dmax, dmin))\n\n        num = (nmax - nmin) * 86400 * 1000\n        max_millis_ticks = 6\n        for interval in [1, 10, 50, 100, 200, 500]:\n            if num <= interval * (max_millis_ticks - 1):\n                self._interval = interval\n                break\n            else:\n                # We went through the whole loop without breaking, default to 1\n                self._interval = 1000.0\n\n        estimate = (nmax - nmin) / (self._get_unit() * self._get_interval())\n\n        if estimate > self.MAXTICKS * 2:\n            raise RuntimeError(\n                \"MillisecondLocator estimated to generate \"\n                f\"{estimate:d} ticks from {dmin} to {dmax}: exceeds Locator.MAXTICKS\"\n                f\"* 2 ({self.MAXTICKS * 2:d}) \"\n            )\n\n        interval = self._get_interval()\n        freq = f\"{interval}L\"\n        tz = self.tz.tzname(None)\n        st = dmin.replace(tzinfo=None)\n        ed = dmin.replace(tzinfo=None)\n        all_dates = date_range(start=st, end=ed, freq=freq, tz=tz).astype(object)\n\n        try:\n            if len(all_dates) > 0:\n                locs = self.raise_if_exceeds(dates.date2num(all_dates))\n                return locs\n        except Exception:  # pragma: no cover\n            pass\n\n        lims = dates.date2num([dmin, dmax])\n        return lims\n\n    def _get_interval(self):\n        return self._interval\n\n    def autoscale(self):\n        \"\"\"\n        Set the view limits to include the data range.\n        \"\"\"\n        # We need to cap at the endpoints of valid datetime\n        dmin, dmax = self.datalim_to_dt()\n\n        vmin = dates.date2num(dmin)\n        vmax = dates.date2num(dmax)\n\n        return self.nonsingular(vmin, vmax)\n\n\ndef _from_ordinal(x, tz: tzinfo | None = None) -> datetime:\n    ix = int(x)\n    dt = datetime.fromordinal(ix)\n    remainder = float(x) - ix\n    hour, remainder = divmod(24 * remainder, 1)\n    minute, remainder = divmod(60 * remainder, 1)\n    second, remainder = divmod(60 * remainder, 1)\n    microsecond = int(1_000_000 * remainder)\n    if microsecond < 10:\n        microsecond = 0  # compensate for rounding errors\n    dt = datetime(\n        dt.year, dt.month, dt.day, int(hour), int(minute), int(second), microsecond\n    )\n    if tz is not None:\n        dt = dt.astimezone(tz)\n\n    if microsecond > 999990:  # compensate for rounding errors\n        dt += timedelta(microseconds=1_000_000 - microsecond)\n\n    return dt\n\n\n# Fixed frequency dynamic tick locators and formatters\n\n# -------------------------------------------------------------------------\n# --- Locators ---\n# -------------------------------------------------------------------------\n\n\ndef _get_default_annual_spacing(nyears) -> tuple[int, int]:\n    \"\"\"\n    Returns a default spacing between consecutive ticks for annual data.\n    \"\"\"\n    if nyears < 11:\n        (min_spacing, maj_spacing) = (1, 1)\n    elif nyears < 20:\n        (min_spacing, maj_spacing) = (1, 2)\n    elif nyears < 50:\n        (min_spacing, maj_spacing) = (1, 5)\n    elif nyears < 100:\n        (min_spacing, maj_spacing) = (5, 10)\n    elif nyears < 200:\n        (min_spacing, maj_spacing) = (5, 25)\n    elif nyears < 600:\n        (min_spacing, maj_spacing) = (10, 50)\n    else:\n        factor = nyears // 1000 + 1\n        (min_spacing, maj_spacing) = (factor * 20, factor * 100)\n    return (min_spacing, maj_spacing)\n\n\ndef period_break(dates: PeriodIndex, period: str) -> np.ndarray:\n    \"\"\"\n    Returns the indices where the given period changes.\n\n    Parameters\n    ----------\n    dates : PeriodIndex\n        Array of intervals to monitor.\n    period : str\n        Name of the period to monitor.\n    \"\"\"\n    current = getattr(dates, period)\n    previous = getattr(dates - 1 * dates.freq, period)\n    return np.nonzero(current - previous)[0]\n\n\ndef has_level_label(label_flags: np.ndarray, vmin: float) -> bool:\n    \"\"\"\n    Returns true if the ``label_flags`` indicate there is at least one label\n    for this level.\n\n    if the minimum view limit is not an exact integer, then the first tick\n    label won't be shown, so we must adjust for that.\n    \"\"\"\n    if label_flags.size == 0 or (\n        label_flags.size == 1 and label_flags[0] == 0 and vmin % 1 > 0.0\n    ):\n        return False\n    else:\n        return True\n\n\ndef _daily_finder(vmin, vmax, freq: BaseOffset):\n    dtype_code = freq._period_dtype_code\n\n    periodsperday = -1\n\n    if dtype_code >= FreqGroup.FR_HR.value:\n        if dtype_code == FreqGroup.FR_NS.value:\n            periodsperday = 24 * 60 * 60 * 1000000000\n        elif dtype_code == FreqGroup.FR_US.value:\n            periodsperday = 24 * 60 * 60 * 1000000\n        elif dtype_code == FreqGroup.FR_MS.value:\n            periodsperday = 24 * 60 * 60 * 1000\n        elif dtype_code == FreqGroup.FR_SEC.value:\n            periodsperday = 24 * 60 * 60\n        elif dtype_code == FreqGroup.FR_MIN.value:\n            periodsperday = 24 * 60\n        elif dtype_code == FreqGroup.FR_HR.value:\n            periodsperday = 24\n        else:  # pragma: no cover\n            raise ValueError(f\"unexpected frequency: {dtype_code}\")\n        periodsperyear = 365 * periodsperday\n        periodspermonth = 28 * periodsperday\n\n    elif dtype_code == FreqGroup.FR_BUS.value:\n        periodsperyear = 261\n        periodspermonth = 19\n    elif dtype_code == FreqGroup.FR_DAY.value:\n        periodsperyear = 365\n        periodspermonth = 28\n    elif FreqGroup.get_freq_group(dtype_code) == FreqGroup.FR_WK:\n        periodsperyear = 52\n        periodspermonth = 3\n    else:  # pragma: no cover\n        raise ValueError(\"unexpected frequency\")\n\n    # save this for later usage\n    vmin_orig = vmin\n\n    (vmin, vmax) = (\n        Period(ordinal=int(vmin), freq=freq),\n        Period(ordinal=int(vmax), freq=freq),\n    )\n    span = vmax.ordinal - vmin.ordinal + 1\n    dates_ = period_range(start=vmin, end=vmax, freq=freq)\n    # Initialize the output\n    info = np.zeros(\n        span, dtype=[(\"val\", np.int64), (\"maj\", bool), (\"min\", bool), (\"fmt\", \"|S20\")]\n    )\n    info[\"val\"][:] = dates_.asi8\n    info[\"fmt\"][:] = \"\"\n    info[\"maj\"][[0, -1]] = True\n    # .. and set some shortcuts\n    info_maj = info[\"maj\"]\n    info_min = info[\"min\"]\n    info_fmt = info[\"fmt\"]\n\n    def first_label(label_flags):\n        if (label_flags[0] == 0) and (label_flags.size > 1) and ((vmin_orig % 1) > 0.0):\n            return label_flags[1]\n        else:\n            return label_flags[0]\n\n    # Case 1. Less than a month\n    if span <= periodspermonth:\n        day_start = period_break(dates_, \"day\")\n        month_start = period_break(dates_, \"month\")\n\n        def _hour_finder(label_interval, force_year_start):\n            _hour = dates_.hour\n            _prev_hour = (dates_ - 1 * dates_.freq).hour\n            hour_start = (_hour - _prev_hour) != 0\n            info_maj[day_start] = True\n            info_min[hour_start & (_hour % label_interval == 0)] = True\n            year_start = period_break(dates_, \"year\")\n            info_fmt[hour_start & (_hour % label_interval == 0)] = \"%H:%M\"\n            info_fmt[day_start] = \"%H:%M\\n%d-%b\"\n            info_fmt[year_start] = \"%H:%M\\n%d-%b\\n%Y\"\n            if force_year_start and not has_level_label(year_start, vmin_orig):\n                info_fmt[first_label(day_start)] = \"%H:%M\\n%d-%b\\n%Y\"\n\n        def _minute_finder(label_interval):\n            hour_start = period_break(dates_, \"hour\")\n            _minute = dates_.minute\n            _prev_minute = (dates_ - 1 * dates_.freq).minute\n            minute_start = (_minute - _prev_minute) != 0\n            info_maj[hour_start] = True\n            info_min[minute_start & (_minute % label_interval == 0)] = True\n            year_start = period_break(dates_, \"year\")\n            info_fmt = info[\"fmt\"]\n            info_fmt[minute_start & (_minute % label_interval == 0)] = \"%H:%M\"\n            info_fmt[day_start] = \"%H:%M\\n%d-%b\"\n            info_fmt[year_start] = \"%H:%M\\n%d-%b\\n%Y\"\n\n        def _second_finder(label_interval):\n            minute_start = period_break(dates_, \"minute\")\n            _second = dates_.second\n            _prev_second = (dates_ - 1 * dates_.freq).second\n            second_start = (_second - _prev_second) != 0\n            info[\"maj\"][minute_start] = True\n            info[\"min\"][second_start & (_second % label_interval == 0)] = True\n            year_start = period_break(dates_, \"year\")\n            info_fmt = info[\"fmt\"]\n            info_fmt[second_start & (_second % label_interval == 0)] = \"%H:%M:%S\"\n            info_fmt[day_start] = \"%H:%M:%S\\n%d-%b\"\n            info_fmt[year_start] = \"%H:%M:%S\\n%d-%b\\n%Y\"\n\n        if span < periodsperday / 12000:\n            _second_finder(1)\n        elif span < periodsperday / 6000:\n            _second_finder(2)\n        elif span < periodsperday / 2400:\n            _second_finder(5)\n        elif span < periodsperday / 1200:\n            _second_finder(10)\n        elif span < periodsperday / 800:\n            _second_finder(15)\n        elif span < periodsperday / 400:\n            _second_finder(30)\n        elif span < periodsperday / 150:\n            _minute_finder(1)\n        elif span < periodsperday / 70:\n            _minute_finder(2)\n        elif span < periodsperday / 24:\n            _minute_finder(5)\n        elif span < periodsperday / 12:\n            _minute_finder(15)\n        elif span < periodsperday / 6:\n            _minute_finder(30)\n        elif span < periodsperday / 2.5:\n            _hour_finder(1, False)\n        elif span < periodsperday / 1.5:\n            _hour_finder(2, False)\n        elif span < periodsperday * 1.25:\n            _hour_finder(3, False)\n        elif span < periodsperday * 2.5:\n            _hour_finder(6, True)\n        elif span < periodsperday * 4:\n            _hour_finder(12, True)\n        else:\n            info_maj[month_start] = True\n            info_min[day_start] = True\n            year_start = period_break(dates_, \"year\")\n            info_fmt = info[\"fmt\"]\n            info_fmt[day_start] = \"%d\"\n            info_fmt[month_start] = \"%d\\n%b\"\n            info_fmt[year_start] = \"%d\\n%b\\n%Y\"\n            if not has_level_label(year_start, vmin_orig):\n                if not has_level_label(month_start, vmin_orig):\n                    info_fmt[first_label(day_start)] = \"%d\\n%b\\n%Y\"\n                else:\n                    info_fmt[first_label(month_start)] = \"%d\\n%b\\n%Y\"\n\n    # Case 2. Less than three months\n    elif span <= periodsperyear // 4:\n        month_start = period_break(dates_, \"month\")\n        info_maj[month_start] = True\n        if dtype_code < FreqGroup.FR_HR.value:\n            info[\"min\"] = True\n        else:\n            day_start = period_break(dates_, \"day\")\n            info[\"min\"][day_start] = True\n        week_start = period_break(dates_, \"week\")\n        year_start = period_break(dates_, \"year\")\n        info_fmt[week_start] = \"%d\"\n        info_fmt[month_start] = \"\\n\\n%b\"\n        info_fmt[year_start] = \"\\n\\n%b\\n%Y\"\n        if not has_level_label(year_start, vmin_orig):\n            if not has_level_label(month_start, vmin_orig):\n                info_fmt[first_label(week_start)] = \"\\n\\n%b\\n%Y\"\n            else:\n                info_fmt[first_label(month_start)] = \"\\n\\n%b\\n%Y\"\n    # Case 3. Less than 14 months ...............\n    elif span <= 1.15 * periodsperyear:\n        year_start = period_break(dates_, \"year\")\n        month_start = period_break(dates_, \"month\")\n        week_start = period_break(dates_, \"week\")\n        info_maj[month_start] = True\n        info_min[week_start] = True\n        info_min[year_start] = False\n        info_min[month_start] = False\n        info_fmt[month_start] = \"%b\"\n        info_fmt[year_start] = \"%b\\n%Y\"\n        if not has_level_label(year_start, vmin_orig):\n            info_fmt[first_label(month_start)] = \"%b\\n%Y\"\n    # Case 4. Less than 2.5 years ...............\n    elif span <= 2.5 * periodsperyear:\n        year_start = period_break(dates_, \"year\")\n        quarter_start = period_break(dates_, \"quarter\")\n        month_start = period_break(dates_, \"month\")\n        info_maj[quarter_start] = True\n        info_min[month_start] = True\n        info_fmt[quarter_start] = \"%b\"\n        info_fmt[year_start] = \"%b\\n%Y\"\n    # Case 4. Less than 4 years .................\n    elif span <= 4 * periodsperyear:\n        year_start = period_break(dates_, \"year\")\n        month_start = period_break(dates_, \"month\")\n        info_maj[year_start] = True\n        info_min[month_start] = True\n        info_min[year_start] = False\n\n        month_break = dates_[month_start].month\n        jan_or_jul = month_start[(month_break == 1) | (month_break == 7)]\n        info_fmt[jan_or_jul] = \"%b\"\n        info_fmt[year_start] = \"%b\\n%Y\"\n    # Case 5. Less than 11 years ................\n    elif span <= 11 * periodsperyear:\n        year_start = period_break(dates_, \"year\")\n        quarter_start = period_break(dates_, \"quarter\")\n        info_maj[year_start] = True\n        info_min[quarter_start] = True\n        info_min[year_start] = False\n        info_fmt[year_start] = \"%Y\"\n    # Case 6. More than 12 years ................\n    else:\n        year_start = period_break(dates_, \"year\")\n        year_break = dates_[year_start].year\n        nyears = span / periodsperyear\n        (min_anndef, maj_anndef) = _get_default_annual_spacing(nyears)\n        major_idx = year_start[(year_break % maj_anndef == 0)]\n        info_maj[major_idx] = True\n        minor_idx = year_start[(year_break % min_anndef == 0)]\n        info_min[minor_idx] = True\n        info_fmt[major_idx] = \"%Y\"\n\n    return info\n\n\ndef _monthly_finder(vmin, vmax, freq):\n    periodsperyear = 12\n\n    vmin_orig = vmin\n    (vmin, vmax) = (int(vmin), int(vmax))\n    span = vmax - vmin + 1\n\n    # Initialize the output\n    info = np.zeros(\n        span, dtype=[(\"val\", int), (\"maj\", bool), (\"min\", bool), (\"fmt\", \"|S8\")]\n    )\n    info[\"val\"] = np.arange(vmin, vmax + 1)\n    dates_ = info[\"val\"]\n    info[\"fmt\"] = \"\"\n    year_start = (dates_ % 12 == 0).nonzero()[0]\n    info_maj = info[\"maj\"]\n    info_fmt = info[\"fmt\"]\n\n    if span <= 1.15 * periodsperyear:\n        info_maj[year_start] = True\n        info[\"min\"] = True\n\n        info_fmt[:] = \"%b\"\n        info_fmt[year_start] = \"%b\\n%Y\"\n\n        if not has_level_label(year_start, vmin_orig):\n            if dates_.size > 1:\n                idx = 1\n            else:\n                idx = 0\n            info_fmt[idx] = \"%b\\n%Y\"\n\n    elif span <= 2.5 * periodsperyear:\n        quarter_start = (dates_ % 3 == 0).nonzero()\n        info_maj[year_start] = True\n        # TODO: Check the following : is it really info['fmt'] ?\n        info[\"fmt\"][quarter_start] = True\n        info[\"min\"] = True\n\n        info_fmt[quarter_start] = \"%b\"\n        info_fmt[year_start] = \"%b\\n%Y\"\n\n    elif span <= 4 * periodsperyear:\n        info_maj[year_start] = True\n        info[\"min\"] = True\n\n        jan_or_jul = (dates_ % 12 == 0) | (dates_ % 12 == 6)\n        info_fmt[jan_or_jul] = \"%b\"\n        info_fmt[year_start] = \"%b\\n%Y\"\n\n    elif span <= 11 * periodsperyear:\n        quarter_start = (dates_ % 3 == 0).nonzero()\n        info_maj[year_start] = True\n        info[\"min\"][quarter_start] = True\n\n        info_fmt[year_start] = \"%Y\"\n\n    else:\n        nyears = span / periodsperyear\n        (min_anndef, maj_anndef) = _get_default_annual_spacing(nyears)\n        years = dates_[year_start] // 12 + 1\n        major_idx = year_start[(years % maj_anndef == 0)]\n        info_maj[major_idx] = True\n        info[\"min\"][year_start[(years % min_anndef == 0)]] = True\n\n        info_fmt[major_idx] = \"%Y\"\n\n    return info\n\n\ndef _quarterly_finder(vmin, vmax, freq):\n    periodsperyear = 4\n    vmin_orig = vmin\n    (vmin, vmax) = (int(vmin), int(vmax))\n    span = vmax - vmin + 1\n\n    info = np.zeros(\n        span, dtype=[(\"val\", int), (\"maj\", bool), (\"min\", bool), (\"fmt\", \"|S8\")]\n    )\n    info[\"val\"] = np.arange(vmin, vmax + 1)\n    info[\"fmt\"] = \"\"\n    dates_ = info[\"val\"]\n    info_maj = info[\"maj\"]\n    info_fmt = info[\"fmt\"]\n    year_start = (dates_ % 4 == 0).nonzero()[0]\n\n    if span <= 3.5 * periodsperyear:\n        info_maj[year_start] = True\n        info[\"min\"] = True\n\n        info_fmt[:] = \"Q%q\"\n        info_fmt[year_start] = \"Q%q\\n%F\"\n        if not has_level_label(year_start, vmin_orig):\n            if dates_.size > 1:\n                idx = 1\n            else:\n                idx = 0\n            info_fmt[idx] = \"Q%q\\n%F\"\n\n    elif span <= 11 * periodsperyear:\n        info_maj[year_start] = True\n        info[\"min\"] = True\n        info_fmt[year_start] = \"%F\"\n\n    else:\n        years = dates_[year_start] // 4 + 1\n        nyears = span / periodsperyear\n        (min_anndef, maj_anndef) = _get_default_annual_spacing(nyears)\n        major_idx = year_start[(years % maj_anndef == 0)]\n        info_maj[major_idx] = True\n        info[\"min\"][year_start[(years % min_anndef == 0)]] = True\n        info_fmt[major_idx] = \"%F\"\n\n    return info\n\n\ndef _annual_finder(vmin, vmax, freq):\n    (vmin, vmax) = (int(vmin), int(vmax + 1))\n    span = vmax - vmin + 1\n\n    info = np.zeros(\n        span, dtype=[(\"val\", int), (\"maj\", bool), (\"min\", bool), (\"fmt\", \"|S8\")]\n    )\n    info[\"val\"] = np.arange(vmin, vmax + 1)\n    info[\"fmt\"] = \"\"\n    dates_ = info[\"val\"]\n\n    (min_anndef, maj_anndef) = _get_default_annual_spacing(span)\n    major_idx = dates_ % maj_anndef == 0\n    info[\"maj\"][major_idx] = True\n    info[\"min\"][(dates_ % min_anndef == 0)] = True\n    info[\"fmt\"][major_idx] = \"%Y\"\n\n    return info\n\n\ndef get_finder(freq: BaseOffset):\n    dtype_code = freq._period_dtype_code\n    fgroup = (dtype_code // 1000) * 1000\n    fgroup = FreqGroup(fgroup)\n\n    if fgroup == FreqGroup.FR_ANN:\n        return _annual_finder\n    elif fgroup == FreqGroup.FR_QTR:\n        return _quarterly_finder\n    elif dtype_code == FreqGroup.FR_MTH.value:\n        return _monthly_finder\n    elif (dtype_code >= FreqGroup.FR_BUS.value) or fgroup == FreqGroup.FR_WK:\n        return _daily_finder\n    else:  # pragma: no cover\n        raise NotImplementedError(f\"Unsupported frequency: {dtype_code}\")\n\n\nclass TimeSeries_DateLocator(Locator):\n    \"\"\"\n    Locates the ticks along an axis controlled by a :class:`Series`.\n\n    Parameters\n    ----------\n    freq : {var}\n        Valid frequency specifier.\n    minor_locator : {False, True}, optional\n        Whether the locator is for minor ticks (True) or not.\n    dynamic_mode : {True, False}, optional\n        Whether the locator should work in dynamic mode.\n    base : {int}, optional\n    quarter : {int}, optional\n    month : {int}, optional\n    day : {int}, optional\n    \"\"\"\n\n    def __init__(\n        self,\n        freq,\n        minor_locator=False,\n        dynamic_mode=True,\n        base=1,\n        quarter=1,\n        month=1,\n        day=1,\n        plot_obj=None,\n    ):\n        freq = to_offset(freq)\n        self.freq = freq\n        self.base = base\n        (self.quarter, self.month, self.day) = (quarter, month, day)\n        self.isminor = minor_locator\n        self.isdynamic = dynamic_mode\n        self.offset = 0\n        self.plot_obj = plot_obj\n        self.finder = get_finder(freq)\n\n    def _get_default_locs(self, vmin, vmax):\n        \"\"\"Returns the default locations of ticks.\"\"\"\n        if self.plot_obj.date_axis_info is None:\n            self.plot_obj.date_axis_info = self.finder(vmin, vmax, self.freq)\n\n        locator = self.plot_obj.date_axis_info\n\n        if self.isminor:\n            return np.compress(locator[\"min\"], locator[\"val\"])\n        return np.compress(locator[\"maj\"], locator[\"val\"])\n\n    def __call__(self):\n        \"\"\"Return the locations of the ticks.\"\"\"\n        # axis calls Locator.set_axis inside set_m<xxxx>_formatter\n\n        vi = tuple(self.axis.get_view_interval())\n        if vi != self.plot_obj.view_interval:\n            self.plot_obj.date_axis_info = None\n        self.plot_obj.view_interval = vi\n        vmin, vmax = vi\n        if vmax < vmin:\n            vmin, vmax = vmax, vmin\n        if self.isdynamic:\n            locs = self._get_default_locs(vmin, vmax)\n        else:  # pragma: no cover\n            base = self.base\n            (d, m) = divmod(vmin, base)\n            vmin = (d + 1) * base\n            locs = list(range(vmin, vmax + 1, base))\n        return locs\n\n    def autoscale(self):\n        \"\"\"\n        Sets the view limits to the nearest multiples of base that contain the\n        data.\n        \"\"\"\n        # requires matplotlib >= 0.98.0\n        (vmin, vmax) = self.axis.get_data_interval()\n\n        locs = self._get_default_locs(vmin, vmax)\n        (vmin, vmax) = locs[[0, -1]]\n        if vmin == vmax:\n            vmin -= 1\n            vmax += 1\n        return nonsingular(vmin, vmax)\n\n\n# -------------------------------------------------------------------------\n# --- Formatter ---\n# -------------------------------------------------------------------------\n\n\nclass TimeSeries_DateFormatter(Formatter):\n    \"\"\"\n    Formats the ticks along an axis controlled by a :class:`PeriodIndex`.\n\n    Parameters\n    ----------\n    freq : {int, string}\n        Valid frequency specifier.\n    minor_locator : bool, default False\n        Whether the current formatter should apply to minor ticks (True) or\n        major ticks (False).\n    dynamic_mode : bool, default True\n        Whether the formatter works in dynamic mode or not.\n    \"\"\"\n\n    def __init__(\n        self,\n        freq,\n        minor_locator: bool = False,\n        dynamic_mode: bool = True,\n        plot_obj=None,\n    ):\n        freq = to_offset(freq)\n        self.format = None\n        self.freq = freq\n        self.locs: list[Any] = []  # unused, for matplotlib compat\n        self.formatdict: dict[Any, Any] | None = None\n        self.isminor = minor_locator\n        self.isdynamic = dynamic_mode\n        self.offset = 0\n        self.plot_obj = plot_obj\n        self.finder = get_finder(freq)\n\n    def _set_default_format(self, vmin, vmax):\n        \"\"\"Returns the default ticks spacing.\"\"\"\n        if self.plot_obj.date_axis_info is None:\n            self.plot_obj.date_axis_info = self.finder(vmin, vmax, self.freq)\n        info = self.plot_obj.date_axis_info\n\n        if self.isminor:\n            format = np.compress(info[\"min\"] & np.logical_not(info[\"maj\"]), info)\n        else:\n            format = np.compress(info[\"maj\"], info)\n        self.formatdict = {x: f for (x, _, _, f) in format}\n        return self.formatdict\n\n    def set_locs(self, locs):\n        \"\"\"Sets the locations of the ticks\"\"\"\n        # don't actually use the locs. This is just needed to work with\n        # matplotlib. Force to use vmin, vmax\n\n        self.locs = locs\n\n        (vmin, vmax) = vi = tuple(self.axis.get_view_interval())\n        if vi != self.plot_obj.view_interval:\n            self.plot_obj.date_axis_info = None\n        self.plot_obj.view_interval = vi\n        if vmax < vmin:\n            (vmin, vmax) = (vmax, vmin)\n        self._set_default_format(vmin, vmax)\n\n    def __call__(self, x, pos=0) -> str:\n\n        if self.formatdict is None:\n            return \"\"\n        else:\n            fmt = self.formatdict.pop(x, \"\")\n            if isinstance(fmt, np.bytes_):\n                fmt = fmt.decode(\"utf-8\")\n            return Period(ordinal=int(x), freq=self.freq).strftime(fmt)\n\n\nclass TimeSeries_TimedeltaFormatter(Formatter):\n    \"\"\"\n    Formats the ticks along an axis controlled by a :class:`TimedeltaIndex`.\n    \"\"\"\n\n    @staticmethod\n    def format_timedelta_ticks(x, pos, n_decimals: int) -> str:\n        \"\"\"\n        Convert seconds to 'D days HH:MM:SS.F'\n        \"\"\"\n        s, ns = divmod(x, 10 ** 9)\n        m, s = divmod(s, 60)\n        h, m = divmod(m, 60)\n        d, h = divmod(h, 24)\n        decimals = int(ns * 10 ** (n_decimals - 9))\n        s = f\"{int(h):02d}:{int(m):02d}:{int(s):02d}\"\n        if n_decimals > 0:\n            s += f\".{decimals:0{n_decimals}d}\"\n        if d != 0:\n            s = f\"{int(d):d} days {s}\"\n        return s\n\n    def __call__(self, x, pos=0) -> str:\n        (vmin, vmax) = tuple(self.axis.get_view_interval())\n        n_decimals = int(np.ceil(np.log10(100 * 10 ** 9 / abs(vmax - vmin))))\n        if n_decimals > 9:\n            n_decimals = 9\n        return self.format_timedelta_ticks(x, pos, n_decimals)\n"
    },
    {
      "filename": "pandas/tests/extension/base/ops.py",
      "content": "from __future__ import annotations\n\nimport pytest\n\nimport pandas as pd\nimport pandas._testing as tm\nfrom pandas.core import ops\nfrom pandas.tests.extension.base.base import BaseExtensionTests\n\n\nclass BaseOpsUtil(BaseExtensionTests):\n    def get_op_from_name(self, op_name):\n        return tm.get_op_from_name(op_name)\n\n    def check_opname(self, s, op_name, other, exc=Exception):\n        op = self.get_op_from_name(op_name)\n\n        self._check_op(s, op, other, op_name, exc)\n\n    def _combine(self, obj, other, op):\n        if isinstance(obj, pd.DataFrame):\n            if len(obj.columns) != 1:\n                raise NotImplementedError\n            expected = obj.iloc[:, 0].combine(other, op).to_frame()\n        else:\n            expected = obj.combine(other, op)\n        return expected\n\n    def _check_op(self, s, op, other, op_name, exc=NotImplementedError):\n        if exc is None:\n            result = op(s, other)\n            expected = self._combine(s, other, op)\n            assert isinstance(result, type(s))\n            self.assert_equal(result, expected)\n        else:\n            with pytest.raises(exc):\n                op(s, other)\n\n    def _check_divmod_op(self, s, op, other, exc=Exception):\n        # divmod has multiple return values, so check separately\n        if exc is None:\n            result_div, result_mod = op(s, other)\n            if op is divmod:\n                expected_div, expected_mod = s // other, s % other\n            else:\n                expected_div, expected_mod = other // s, other % s\n            self.assert_series_equal(result_div, expected_div)\n            self.assert_series_equal(result_mod, expected_mod)\n        else:\n            with pytest.raises(exc):\n                divmod(s, other)\n\n\nclass BaseArithmeticOpsTests(BaseOpsUtil):\n    \"\"\"\n    Various Series and DataFrame arithmetic ops methods.\n\n    Subclasses supporting various ops should set the class variables\n    to indicate that they support ops of that kind\n\n    * series_scalar_exc = TypeError\n    * frame_scalar_exc = TypeError\n    * series_array_exc = TypeError\n    * divmod_exc = TypeError\n    \"\"\"\n\n    series_scalar_exc: type[TypeError] | None = TypeError\n    frame_scalar_exc: type[TypeError] | None = TypeError\n    series_array_exc: type[TypeError] | None = TypeError\n    divmod_exc: type[TypeError] | None = TypeError\n\n    def test_arith_series_with_scalar(self, data, all_arithmetic_operators):\n        # series & scalar\n        op_name = all_arithmetic_operators\n        s = pd.Series(data)\n        self.check_opname(s, op_name, s.iloc[0], exc=self.series_scalar_exc)\n\n    def test_arith_frame_with_scalar(self, data, all_arithmetic_operators):\n        # frame & scalar\n        op_name = all_arithmetic_operators\n        df = pd.DataFrame({\"A\": data})\n        self.check_opname(df, op_name, data[0], exc=self.frame_scalar_exc)\n\n    def test_arith_series_with_array(self, data, all_arithmetic_operators):\n        # ndarray & other series\n        op_name = all_arithmetic_operators\n        s = pd.Series(data)\n        self.check_opname(\n            s, op_name, pd.Series([s.iloc[0]] * len(s)), exc=self.series_array_exc\n        )\n\n    def test_divmod(self, data):\n        s = pd.Series(data)\n        self._check_divmod_op(s, divmod, 1, exc=self.divmod_exc)\n        self._check_divmod_op(1, ops.rdivmod, s, exc=self.divmod_exc)\n\n    def test_divmod_series_array(self, data, data_for_twos):\n        s = pd.Series(data)\n        self._check_divmod_op(s, divmod, data)\n\n        other = data_for_twos\n        self._check_divmod_op(other, ops.rdivmod, s)\n\n        other = pd.Series(other)\n        self._check_divmod_op(other, ops.rdivmod, s)\n\n    def test_add_series_with_extension_array(self, data):\n        s = pd.Series(data)\n        result = s + data\n        expected = pd.Series(data + data)\n        self.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"box\", [pd.Series, pd.DataFrame])\n    def test_direct_arith_with_ndframe_returns_not_implemented(self, data, box):\n        # EAs should return NotImplemented for ops with Series/DataFrame\n        # Pandas takes care of unboxing the series and calling the EA's op.\n        other = pd.Series(data)\n        if box is pd.DataFrame:\n            other = other.to_frame()\n        if hasattr(data, \"__add__\"):\n            result = data.__add__(other)\n            assert result is NotImplemented\n        else:\n            raise pytest.skip(f\"{type(data).__name__} does not implement add\")\n\n\nclass BaseComparisonOpsTests(BaseOpsUtil):\n    \"\"\"Various Series and DataFrame comparison ops methods.\"\"\"\n\n    def _compare_other(self, s, data, op_name, other):\n        op = self.get_op_from_name(op_name)\n        if op_name == \"__eq__\":\n            assert not op(s, other).all()\n        elif op_name == \"__ne__\":\n            assert op(s, other).all()\n\n        else:\n\n            # array\n            assert getattr(data, op_name)(other) is NotImplemented\n\n            # series\n            s = pd.Series(data)\n            with pytest.raises(TypeError):\n                op(s, other)\n\n    def test_compare_scalar(self, data, all_compare_operators):\n        op_name = all_compare_operators\n        s = pd.Series(data)\n        self._compare_other(s, data, op_name, 0)\n\n    def test_compare_array(self, data, all_compare_operators):\n        op_name = all_compare_operators\n        s = pd.Series(data)\n        other = pd.Series([data[0]] * len(data))\n        self._compare_other(s, data, op_name, other)\n\n    @pytest.mark.parametrize(\"box\", [pd.Series, pd.DataFrame])\n    def test_direct_arith_with_ndframe_returns_not_implemented(self, data, box):\n        # EAs should return NotImplemented for ops with Series/DataFrame\n        # Pandas takes care of unboxing the series and calling the EA's op.\n        other = pd.Series(data)\n        if box is pd.DataFrame:\n            other = other.to_frame()\n\n        if hasattr(data, \"__eq__\"):\n            result = data.__eq__(other)\n            assert result is NotImplemented\n        else:\n            raise pytest.skip(f\"{type(data).__name__} does not implement __eq__\")\n\n        if hasattr(data, \"__ne__\"):\n            result = data.__ne__(other)\n            assert result is NotImplemented\n        else:\n            raise pytest.skip(f\"{type(data).__name__} does not implement __ne__\")\n\n\nclass BaseUnaryOpsTests(BaseOpsUtil):\n    def test_invert(self, data):\n        s = pd.Series(data, name=\"name\")\n        result = ~s\n        expected = pd.Series(~data, name=\"name\")\n        self.assert_series_equal(result, expected)\n"
    },
    {
      "filename": "pandas/tests/io/parser/conftest.py",
      "content": "from __future__ import annotations\n\nimport os\n\nimport pytest\n\nfrom pandas import (\n    read_csv,\n    read_table,\n)\n\n\nclass BaseParser:\n    engine: str | None = None\n    low_memory = True\n    float_precision_choices: list[str | None] = []\n\n    def update_kwargs(self, kwargs):\n        kwargs = kwargs.copy()\n        kwargs.update({\"engine\": self.engine, \"low_memory\": self.low_memory})\n\n        return kwargs\n\n    def read_csv(self, *args, **kwargs):\n        kwargs = self.update_kwargs(kwargs)\n        return read_csv(*args, **kwargs)\n\n    def read_table(self, *args, **kwargs):\n        kwargs = self.update_kwargs(kwargs)\n        return read_table(*args, **kwargs)\n\n\nclass CParser(BaseParser):\n    engine = \"c\"\n    float_precision_choices = [None, \"high\", \"round_trip\"]\n\n\nclass CParserHighMemory(CParser):\n    low_memory = False\n\n\nclass CParserLowMemory(CParser):\n    low_memory = True\n\n\nclass PythonParser(BaseParser):\n    engine = \"python\"\n    float_precision_choices = [None]\n\n\n@pytest.fixture\ndef csv_dir_path(datapath):\n    \"\"\"\n    The directory path to the data files needed for parser tests.\n    \"\"\"\n    return datapath(\"io\", \"parser\", \"data\")\n\n\n@pytest.fixture\ndef csv1(datapath):\n    \"\"\"\n    The path to the data file \"test1.csv\" needed for parser tests.\n    \"\"\"\n    return os.path.join(datapath(\"io\", \"data\", \"csv\"), \"test1.csv\")\n\n\n_cParserHighMemory = CParserHighMemory()\n_cParserLowMemory = CParserLowMemory()\n_pythonParser = PythonParser()\n\n_py_parsers_only = [_pythonParser]\n_c_parsers_only = [_cParserHighMemory, _cParserLowMemory]\n_all_parsers = [*_c_parsers_only, *_py_parsers_only]\n\n_py_parser_ids = [\"python\"]\n_c_parser_ids = [\"c_high\", \"c_low\"]\n_all_parser_ids = [*_c_parser_ids, *_py_parser_ids]\n\n\n@pytest.fixture(params=_all_parsers, ids=_all_parser_ids)\ndef all_parsers(request):\n    \"\"\"\n    Fixture all of the CSV parsers.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=_c_parsers_only, ids=_c_parser_ids)\ndef c_parser_only(request):\n    \"\"\"\n    Fixture all of the CSV parsers using the C engine.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=_py_parsers_only, ids=_py_parser_ids)\ndef python_parser_only(request):\n    \"\"\"\n    Fixture all of the CSV parsers using the Python engine.\n    \"\"\"\n    return request.param\n\n\ndef _get_all_parser_float_precision_combinations():\n    \"\"\"\n    Return all allowable parser and float precision\n    combinations and corresponding ids.\n    \"\"\"\n    params = []\n    ids = []\n    for parser, parser_id in zip(_all_parsers, _all_parser_ids):\n        for precision in parser.float_precision_choices:\n            params.append((parser, precision))\n            ids.append(f\"{parser_id}-{precision}\")\n\n    return {\"params\": params, \"ids\": ids}\n\n\n@pytest.fixture(\n    params=_get_all_parser_float_precision_combinations()[\"params\"],\n    ids=_get_all_parser_float_precision_combinations()[\"ids\"],\n)\ndef all_parsers_all_precisions(request):\n    \"\"\"\n    Fixture for all allowable combinations of parser\n    and float precision\n    \"\"\"\n    return request.param\n\n\n_utf_values = [8, 16, 32]\n\n_encoding_seps = [\"\", \"-\", \"_\"]\n_encoding_prefixes = [\"utf\", \"UTF\"]\n\n_encoding_fmts = [\n    f\"{prefix}{sep}\" + \"{0}\" for sep in _encoding_seps for prefix in _encoding_prefixes\n]\n\n\n@pytest.fixture(params=_utf_values)\ndef utf_value(request):\n    \"\"\"\n    Fixture for all possible integer values for a UTF encoding.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=_encoding_fmts)\ndef encoding_fmt(request):\n    \"\"\"\n    Fixture for all possible string formats of a UTF encoding.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(\n    params=[\n        (\"-1,0\", -1.0),\n        (\"-1,2e0\", -1.2),\n        (\"-1e0\", -1.0),\n        (\"+1e0\", 1.0),\n        (\"+1e+0\", 1.0),\n        (\"+1e-1\", 0.1),\n        (\"+,1e1\", 1.0),\n        (\"+1,e0\", 1.0),\n        (\"-,1e1\", -1.0),\n        (\"-1,e0\", -1.0),\n        (\"0,1\", 0.1),\n        (\"1,\", 1.0),\n        (\",1\", 0.1),\n        (\"-,1\", -0.1),\n        (\"1_,\", 1.0),\n        (\"1_234,56\", 1234.56),\n        (\"1_234,56e0\", 1234.56),\n        # negative cases; must not parse as float\n        (\"_\", \"_\"),\n        (\"-_\", \"-_\"),\n        (\"-_1\", \"-_1\"),\n        (\"-_1e0\", \"-_1e0\"),\n        (\"_1\", \"_1\"),\n        (\"_1,\", \"_1,\"),\n        (\"_1,_\", \"_1,_\"),\n        (\"_1e0\", \"_1e0\"),\n        (\"1,2e_1\", \"1,2e_1\"),\n        (\"1,2e1_0\", \"1,2e1_0\"),\n        (\"1,_2\", \"1,_2\"),\n        (\",1__2\", \",1__2\"),\n        (\",1e\", \",1e\"),\n        (\"-,1e\", \"-,1e\"),\n        (\"1_000,000_000\", \"1_000,000_000\"),\n        (\"1,e1_2\", \"1,e1_2\"),\n        (\"e11,2\", \"e11,2\"),\n        (\"1e11,2\", \"1e11,2\"),\n        (\"1,2,2\", \"1,2,2\"),\n        (\"1,2_1\", \"1,2_1\"),\n        (\"1,2e-10e1\", \"1,2e-10e1\"),\n        (\"--1,2\", \"--1,2\"),\n        (\"1a_2,1\", \"1a_2,1\"),\n        (\"1,2E-1\", 0.12),\n        (\"1,2E1\", 12.0),\n    ]\n)\ndef numeric_decimal(request):\n    \"\"\"\n    Fixture for all numeric formats which should get recognized. The first entry\n    represents the value to read while the second represents the expected result.\n    \"\"\"\n    return request.param\n"
    },
    {
      "filename": "pandas/tests/tseries/offsets/common.py",
      "content": "\"\"\"\nAssertion helpers and base class for offsets tests\n\"\"\"\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom dateutil.tz.tz import tzlocal\nimport pytest\n\nfrom pandas._libs.tslibs import (\n    OutOfBoundsDatetime,\n    Timestamp,\n)\nfrom pandas._libs.tslibs.offsets import (\n    FY5253,\n    BusinessHour,\n    CustomBusinessHour,\n    DateOffset,\n    FY5253Quarter,\n    LastWeekOfMonth,\n    Week,\n    WeekOfMonth,\n)\nfrom pandas.compat import IS64\n\n\ndef assert_offset_equal(offset, base, expected):\n    actual = offset + base\n    actual_swapped = base + offset\n    actual_apply = offset.apply(base)\n    try:\n        assert actual == expected\n        assert actual_swapped == expected\n        assert actual_apply == expected\n    except AssertionError as err:\n        raise AssertionError(\n            f\"\\nExpected: {expected}\\nActual: {actual}\\nFor Offset: {offset})\"\n            f\"\\nAt Date: {base}\"\n        ) from err\n\n\ndef assert_is_on_offset(offset, date, expected):\n    actual = offset.is_on_offset(date)\n    assert actual == expected, (\n        f\"\\nExpected: {expected}\\nActual: {actual}\\nFor Offset: {offset})\"\n        f\"\\nAt Date: {date}\"\n    )\n\n\nclass WeekDay:\n    MON = 0\n    TUE = 1\n    WED = 2\n    THU = 3\n    FRI = 4\n    SAT = 5\n    SUN = 6\n\n\nclass Base:\n    _offset: type[DateOffset] | None = None\n    d = Timestamp(datetime(2008, 1, 2))\n\n    timezones = [\n        None,\n        \"UTC\",\n        \"Asia/Tokyo\",\n        \"US/Eastern\",\n        \"dateutil/Asia/Tokyo\",\n        \"dateutil/US/Pacific\",\n    ]\n\n    def _get_offset(self, klass, value=1, normalize=False):\n        # create instance from offset class\n        if klass is FY5253:\n            klass = klass(\n                n=value,\n                startingMonth=1,\n                weekday=1,\n                variation=\"last\",\n                normalize=normalize,\n            )\n        elif klass is FY5253Quarter:\n            klass = klass(\n                n=value,\n                startingMonth=1,\n                weekday=1,\n                qtr_with_extra_week=1,\n                variation=\"last\",\n                normalize=normalize,\n            )\n        elif klass is LastWeekOfMonth:\n            klass = klass(n=value, weekday=5, normalize=normalize)\n        elif klass is WeekOfMonth:\n            klass = klass(n=value, week=1, weekday=5, normalize=normalize)\n        elif klass is Week:\n            klass = klass(n=value, weekday=5, normalize=normalize)\n        elif klass is DateOffset:\n            klass = klass(days=value, normalize=normalize)\n        else:\n            klass = klass(value, normalize=normalize)\n        return klass\n\n    def test_apply_out_of_range(self, request, tz_naive_fixture):\n        tz = tz_naive_fixture\n        if self._offset is None:\n            return\n\n        # try to create an out-of-bounds result timestamp; if we can't create\n        # the offset skip\n        try:\n            if self._offset in (BusinessHour, CustomBusinessHour):\n                # Using 10000 in BusinessHour fails in tz check because of DST\n                # difference\n                offset = self._get_offset(self._offset, value=100000)\n            else:\n                offset = self._get_offset(self._offset, value=10000)\n\n            result = Timestamp(\"20080101\") + offset\n            assert isinstance(result, datetime)\n            assert result.tzinfo is None\n\n            # Check tz is preserved\n            t = Timestamp(\"20080101\", tz=tz)\n            result = t + offset\n            assert isinstance(result, datetime)\n\n            if isinstance(tz, tzlocal) and not IS64:\n                # If we hit OutOfBoundsDatetime on non-64 bit machines\n                # we'll drop out of the try clause before the next test\n                request.node.add_marker(\n                    pytest.mark.xfail(reason=\"OverflowError inside tzlocal past 2038\")\n                )\n            assert t.tzinfo == result.tzinfo\n\n        except OutOfBoundsDatetime:\n            pass\n        except (ValueError, KeyError):\n            # we are creating an invalid offset\n            # so ignore\n            pass\n\n    def test_offsets_compare_equal(self):\n        # root cause of GH#456: __ne__ was not implemented\n        if self._offset is None:\n            return\n        offset1 = self._offset()\n        offset2 = self._offset()\n        assert not offset1 != offset2\n        assert offset1 == offset2\n\n    def test_rsub(self):\n        if self._offset is None or not hasattr(self, \"offset2\"):\n            # i.e. skip for TestCommon and YQM subclasses that do not have\n            # offset2 attr\n            return\n        assert self.d - self.offset2 == (-self.offset2).apply(self.d)\n\n    def test_radd(self):\n        if self._offset is None or not hasattr(self, \"offset2\"):\n            # i.e. skip for TestCommon and YQM subclasses that do not have\n            # offset2 attr\n            return\n        assert self.d + self.offset2 == self.offset2 + self.d\n\n    def test_sub(self):\n        if self._offset is None or not hasattr(self, \"offset2\"):\n            # i.e. skip for TestCommon and YQM subclasses that do not have\n            # offset2 attr\n            return\n        off = self.offset2\n        msg = \"Cannot subtract datetime from offset\"\n        with pytest.raises(TypeError, match=msg):\n            off - self.d\n\n        assert 2 * off - off == off\n        assert self.d - self.offset2 == self.d + self._offset(-2)\n        assert self.d - self.offset2 == self.d - (2 * off - off)\n\n    def testMult1(self):\n        if self._offset is None or not hasattr(self, \"offset1\"):\n            # i.e. skip for TestCommon and YQM subclasses that do not have\n            # offset1 attr\n            return\n        assert self.d + 10 * self.offset1 == self.d + self._offset(10)\n        assert self.d + 5 * self.offset1 == self.d + self._offset(5)\n\n    def testMult2(self):\n        if self._offset is None:\n            return\n        assert self.d + (-5 * self._offset(-10)) == self.d + self._offset(50)\n        assert self.d + (-3 * self._offset(-2)) == self.d + self._offset(6)\n\n    def test_compare_str(self):\n        # GH#23524\n        # comparing to strings that cannot be cast to DateOffsets should\n        #  not raise for __eq__ or __ne__\n        if self._offset is None:\n            return\n        off = self._get_offset(self._offset)\n\n        assert not off == \"infer\"\n        assert off != \"foo\"\n        # Note: inequalities are only implemented for Tick subclasses;\n        #  tests for this are in test_ticks\n"
    },
    {
      "filename": "pandas/util/version/__init__.py",
      "content": "# Vendored from https://github.com/pypa/packaging/blob/main/packaging/_structures.py\n# and https://github.com/pypa/packaging/blob/main/packaging/_structures.py\n# changeset ae891fd74d6dd4c6063bb04f2faeadaac6fc6313\n# 04/30/2021\n\n# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import annotations\n\nimport collections\nimport itertools\nimport re\nfrom typing import (\n    Callable,\n    Iterator,\n    SupportsInt,\n    Tuple,\n    Union,\n)\nimport warnings\n\n__all__ = [\"parse\", \"Version\", \"LegacyVersion\", \"InvalidVersion\", \"VERSION_PATTERN\"]\n\n\nclass InfinityType:\n    def __repr__(self) -> str:\n        return \"Infinity\"\n\n    def __hash__(self) -> int:\n        return hash(repr(self))\n\n    def __lt__(self, other: object) -> bool:\n        return False\n\n    def __le__(self, other: object) -> bool:\n        return False\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, type(self))\n\n    def __ne__(self, other: object) -> bool:\n        return not isinstance(other, type(self))\n\n    def __gt__(self, other: object) -> bool:\n        return True\n\n    def __ge__(self, other: object) -> bool:\n        return True\n\n    def __neg__(self: object) -> NegativeInfinityType:\n        return NegativeInfinity\n\n\nInfinity = InfinityType()\n\n\nclass NegativeInfinityType:\n    def __repr__(self) -> str:\n        return \"-Infinity\"\n\n    def __hash__(self) -> int:\n        return hash(repr(self))\n\n    def __lt__(self, other: object) -> bool:\n        return True\n\n    def __le__(self, other: object) -> bool:\n        return True\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, type(self))\n\n    def __ne__(self, other: object) -> bool:\n        return not isinstance(other, type(self))\n\n    def __gt__(self, other: object) -> bool:\n        return False\n\n    def __ge__(self, other: object) -> bool:\n        return False\n\n    def __neg__(self: object) -> InfinityType:\n        return Infinity\n\n\nNegativeInfinity = NegativeInfinityType()\n\n\nInfiniteTypes = Union[InfinityType, NegativeInfinityType]\nPrePostDevType = Union[InfiniteTypes, Tuple[str, int]]\nSubLocalType = Union[InfiniteTypes, int, str]\nLocalType = Union[\n    NegativeInfinityType,\n    Tuple[\n        Union[\n            SubLocalType,\n            Tuple[SubLocalType, str],\n            Tuple[NegativeInfinityType, SubLocalType],\n        ],\n        ...,\n    ],\n]\nCmpKey = Tuple[\n    int, Tuple[int, ...], PrePostDevType, PrePostDevType, PrePostDevType, LocalType\n]\nLegacyCmpKey = Tuple[int, Tuple[str, ...]]\nVersionComparisonMethod = Callable[\n    [Union[CmpKey, LegacyCmpKey], Union[CmpKey, LegacyCmpKey]], bool\n]\n\n_Version = collections.namedtuple(\n    \"_Version\", [\"epoch\", \"release\", \"dev\", \"pre\", \"post\", \"local\"]\n)\n\n\ndef parse(version: str) -> LegacyVersion | Version:\n    \"\"\"\n    Parse the given version string and return either a :class:`Version` object\n    or a :class:`LegacyVersion` object depending on if the given version is\n    a valid PEP 440 version or a legacy version.\n    \"\"\"\n    try:\n        return Version(version)\n    except InvalidVersion:\n        return LegacyVersion(version)\n\n\nclass InvalidVersion(ValueError):\n    \"\"\"\n    An invalid version was found, users should refer to PEP 440.\n    \"\"\"\n\n\nclass _BaseVersion:\n    _key: CmpKey | LegacyCmpKey\n\n    def __hash__(self) -> int:\n        return hash(self._key)\n\n    # Please keep the duplicated `isinstance` check\n    # in the six comparisons hereunder\n    # unless you find a way to avoid adding overhead function calls.\n    def __lt__(self, other: _BaseVersion) -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key < other._key\n\n    def __le__(self, other: _BaseVersion) -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key <= other._key\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key == other._key\n\n    def __ge__(self, other: _BaseVersion) -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key >= other._key\n\n    def __gt__(self, other: _BaseVersion) -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key > other._key\n\n    def __ne__(self, other: object) -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key != other._key\n\n\nclass LegacyVersion(_BaseVersion):\n    def __init__(self, version: str) -> None:\n        self._version = str(version)\n        self._key = _legacy_cmpkey(self._version)\n\n        warnings.warn(\n            \"Creating a LegacyVersion has been deprecated and will be \"\n            \"removed in the next major release\",\n            DeprecationWarning,\n        )\n\n    def __str__(self) -> str:\n        return self._version\n\n    def __repr__(self) -> str:\n        return f\"<LegacyVersion('{self}')>\"\n\n    @property\n    def public(self) -> str:\n        return self._version\n\n    @property\n    def base_version(self) -> str:\n        return self._version\n\n    @property\n    def epoch(self) -> int:\n        return -1\n\n    @property\n    def release(self) -> None:\n        return None\n\n    @property\n    def pre(self) -> None:\n        return None\n\n    @property\n    def post(self) -> None:\n        return None\n\n    @property\n    def dev(self) -> None:\n        return None\n\n    @property\n    def local(self) -> None:\n        return None\n\n    @property\n    def is_prerelease(self) -> bool:\n        return False\n\n    @property\n    def is_postrelease(self) -> bool:\n        return False\n\n    @property\n    def is_devrelease(self) -> bool:\n        return False\n\n\n_legacy_version_component_re = re.compile(r\"(\\d+ | [a-z]+ | \\.| -)\", re.VERBOSE)\n\n_legacy_version_replacement_map = {\n    \"pre\": \"c\",\n    \"preview\": \"c\",\n    \"-\": \"final-\",\n    \"rc\": \"c\",\n    \"dev\": \"@\",\n}\n\n\ndef _parse_version_parts(s: str) -> Iterator[str]:\n    for part in _legacy_version_component_re.split(s):\n        part = _legacy_version_replacement_map.get(part, part)\n\n        if not part or part == \".\":\n            continue\n\n        if part[:1] in \"0123456789\":\n            # pad for numeric comparison\n            yield part.zfill(8)\n        else:\n            yield \"*\" + part\n\n    # ensure that alpha/beta/candidate are before final\n    yield \"*final\"\n\n\ndef _legacy_cmpkey(version: str) -> LegacyCmpKey:\n\n    # We hardcode an epoch of -1 here. A PEP 440 version can only have a epoch\n    # greater than or equal to 0. This will effectively put the LegacyVersion,\n    # which uses the defacto standard originally implemented by setuptools,\n    # as before all PEP 440 versions.\n    epoch = -1\n\n    # This scheme is taken from pkg_resources.parse_version setuptools prior to\n    # it's adoption of the packaging library.\n    parts: list[str] = []\n    for part in _parse_version_parts(version.lower()):\n        if part.startswith(\"*\"):\n            # remove \"-\" before a prerelease tag\n            if part < \"*final\":\n                while parts and parts[-1] == \"*final-\":\n                    parts.pop()\n\n            # remove trailing zeros from each series of numeric parts\n            while parts and parts[-1] == \"00000000\":\n                parts.pop()\n\n        parts.append(part)\n\n    return epoch, tuple(parts)\n\n\n# Deliberately not anchored to the start and end of the string, to make it\n# easier for 3rd party code to reuse\nVERSION_PATTERN = r\"\"\"\n    v?\n    (?:\n        (?:(?P<epoch>[0-9]+)!)?                           # epoch\n        (?P<release>[0-9]+(?:\\.[0-9]+)*)                  # release segment\n        (?P<pre>                                          # pre-release\n            [-_\\.]?\n            (?P<pre_l>(a|b|c|rc|alpha|beta|pre|preview))\n            [-_\\.]?\n            (?P<pre_n>[0-9]+)?\n        )?\n        (?P<post>                                         # post release\n            (?:-(?P<post_n1>[0-9]+))\n            |\n            (?:\n                [-_\\.]?\n                (?P<post_l>post|rev|r)\n                [-_\\.]?\n                (?P<post_n2>[0-9]+)?\n            )\n        )?\n        (?P<dev>                                          # dev release\n            [-_\\.]?\n            (?P<dev_l>dev)\n            [-_\\.]?\n            (?P<dev_n>[0-9]+)?\n        )?\n    )\n    (?:\\+(?P<local>[a-z0-9]+(?:[-_\\.][a-z0-9]+)*))?       # local version\n\"\"\"\n\n\nclass Version(_BaseVersion):\n\n    _regex = re.compile(r\"^\\s*\" + VERSION_PATTERN + r\"\\s*$\", re.VERBOSE | re.IGNORECASE)\n\n    def __init__(self, version: str) -> None:\n\n        # Validate the version and parse it into pieces\n        match = self._regex.search(version)\n        if not match:\n            raise InvalidVersion(f\"Invalid version: '{version}'\")\n\n        # Store the parsed out pieces of the version\n        self._version = _Version(\n            epoch=int(match.group(\"epoch\")) if match.group(\"epoch\") else 0,\n            release=tuple(int(i) for i in match.group(\"release\").split(\".\")),\n            pre=_parse_letter_version(match.group(\"pre_l\"), match.group(\"pre_n\")),\n            post=_parse_letter_version(\n                match.group(\"post_l\"), match.group(\"post_n1\") or match.group(\"post_n2\")\n            ),\n            dev=_parse_letter_version(match.group(\"dev_l\"), match.group(\"dev_n\")),\n            local=_parse_local_version(match.group(\"local\")),\n        )\n\n        # Generate a key which will be used for sorting\n        self._key = _cmpkey(\n            self._version.epoch,\n            self._version.release,\n            self._version.pre,\n            self._version.post,\n            self._version.dev,\n            self._version.local,\n        )\n\n    def __repr__(self) -> str:\n        return f\"<Version('{self}')>\"\n\n    def __str__(self) -> str:\n        parts = []\n\n        # Epoch\n        if self.epoch != 0:\n            parts.append(f\"{self.epoch}!\")\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self.release))\n\n        # Pre-release\n        if self.pre is not None:\n            parts.append(\"\".join(str(x) for x in self.pre))\n\n        # Post-release\n        if self.post is not None:\n            parts.append(f\".post{self.post}\")\n\n        # Development release\n        if self.dev is not None:\n            parts.append(f\".dev{self.dev}\")\n\n        # Local version segment\n        if self.local is not None:\n            parts.append(f\"+{self.local}\")\n\n        return \"\".join(parts)\n\n    @property\n    def epoch(self) -> int:\n        _epoch: int = self._version.epoch\n        return _epoch\n\n    @property\n    def release(self) -> tuple[int, ...]:\n        _release: tuple[int, ...] = self._version.release\n        return _release\n\n    @property\n    def pre(self) -> tuple[str, int] | None:\n        _pre: tuple[str, int] | None = self._version.pre\n        return _pre\n\n    @property\n    def post(self) -> int | None:\n        return self._version.post[1] if self._version.post else None\n\n    @property\n    def dev(self) -> int | None:\n        return self._version.dev[1] if self._version.dev else None\n\n    @property\n    def local(self) -> str | None:\n        if self._version.local:\n            return \".\".join(str(x) for x in self._version.local)\n        else:\n            return None\n\n    @property\n    def public(self) -> str:\n        return str(self).split(\"+\", 1)[0]\n\n    @property\n    def base_version(self) -> str:\n        parts = []\n\n        # Epoch\n        if self.epoch != 0:\n            parts.append(f\"{self.epoch}!\")\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self.release))\n\n        return \"\".join(parts)\n\n    @property\n    def is_prerelease(self) -> bool:\n        return self.dev is not None or self.pre is not None\n\n    @property\n    def is_postrelease(self) -> bool:\n        return self.post is not None\n\n    @property\n    def is_devrelease(self) -> bool:\n        return self.dev is not None\n\n    @property\n    def major(self) -> int:\n        return self.release[0] if len(self.release) >= 1 else 0\n\n    @property\n    def minor(self) -> int:\n        return self.release[1] if len(self.release) >= 2 else 0\n\n    @property\n    def micro(self) -> int:\n        return self.release[2] if len(self.release) >= 3 else 0\n\n\ndef _parse_letter_version(\n    letter: str, number: str | bytes | SupportsInt\n) -> tuple[str, int] | None:\n\n    if letter:\n        # We consider there to be an implicit 0 in a pre-release if there is\n        # not a numeral associated with it.\n        if number is None:\n            number = 0\n\n        # We normalize any letters to their lower case form\n        letter = letter.lower()\n\n        # We consider some words to be alternate spellings of other words and\n        # in those cases we want to normalize the spellings to our preferred\n        # spelling.\n        if letter == \"alpha\":\n            letter = \"a\"\n        elif letter == \"beta\":\n            letter = \"b\"\n        elif letter in [\"c\", \"pre\", \"preview\"]:\n            letter = \"rc\"\n        elif letter in [\"rev\", \"r\"]:\n            letter = \"post\"\n\n        return letter, int(number)\n    if not letter and number:\n        # We assume if we are given a number, but we are not given a letter\n        # then this is using the implicit post release syntax (e.g. 1.0-1)\n        letter = \"post\"\n\n        return letter, int(number)\n\n    return None\n\n\n_local_version_separators = re.compile(r\"[\\._-]\")\n\n\ndef _parse_local_version(local: str) -> LocalType | None:\n    \"\"\"\n    Takes a string like abc.1.twelve and turns it into (\"abc\", 1, \"twelve\").\n    \"\"\"\n    if local is not None:\n        return tuple(\n            part.lower() if not part.isdigit() else int(part)\n            for part in _local_version_separators.split(local)\n        )\n    return None\n\n\ndef _cmpkey(\n    epoch: int,\n    release: tuple[int, ...],\n    pre: tuple[str, int] | None,\n    post: tuple[str, int] | None,\n    dev: tuple[str, int] | None,\n    local: tuple[SubLocalType] | None,\n) -> CmpKey:\n\n    # When we compare a release version, we want to compare it with all of the\n    # trailing zeros removed. So we'll use a reverse the list, drop all the now\n    # leading zeros until we come to something non zero, then take the rest\n    # re-reverse it back into the correct order and make it a tuple and use\n    # that for our sorting key.\n    _release = tuple(\n        reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release))))\n    )\n\n    # We need to \"trick\" the sorting algorithm to put 1.0.dev0 before 1.0a0.\n    # We'll do this by abusing the pre segment, but we _only_ want to do this\n    # if there is not a pre or a post segment. If we have one of those then\n    # the normal sorting rules will handle this case correctly.\n    if pre is None and post is None and dev is not None:\n        _pre: PrePostDevType = NegativeInfinity\n    # Versions without a pre-release (except as noted above) should sort after\n    # those with one.\n    elif pre is None:\n        _pre = Infinity\n    else:\n        _pre = pre\n\n    # Versions without a post segment should sort before those with one.\n    if post is None:\n        _post: PrePostDevType = NegativeInfinity\n\n    else:\n        _post = post\n\n    # Versions without a development segment should sort after those with one.\n    if dev is None:\n        _dev: PrePostDevType = Infinity\n\n    else:\n        _dev = dev\n\n    if local is None:\n        # Versions without a local segment should sort before those with one.\n        _local: LocalType = NegativeInfinity\n    else:\n        # Versions with a local segment need that segment parsed to implement\n        # the sorting rules in PEP440.\n        # - Alpha numeric segments sort before numeric segments\n        # - Alpha numeric segments sort lexicographically\n        # - Numeric segments sort numerically\n        # - Shorter versions sort before longer versions when the prefixes\n        #   match exactly\n        _local = tuple(\n            (i, \"\") if isinstance(i, int) else (NegativeInfinity, i) for i in local\n        )\n\n    return epoch, _release, _pre, _post, _dev, _local\n"
    },
    {
      "filename": "scripts/no_bool_in_generic.py",
      "content": "\"\"\"\nCheck that pandas/core/generic.py doesn't use bool as a type annotation.\n\nThere is already the method `bool`, so the alias `bool_t` should be used instead.\n\nThis is meant to be run as a pre-commit hook - to run it manually, you can do:\n\n    pre-commit run no-bool-in-core-generic --all-files\n\nThe function `visit` is adapted from a function by the same name in pyupgrade:\nhttps://github.com/asottile/pyupgrade/blob/5495a248f2165941c5d3b82ac3226ba7ad1fa59d/pyupgrade/_data.py#L70-L113\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport ast\nimport collections\nfrom typing import Sequence\n\n\ndef visit(tree: ast.Module) -> dict[int, list[int]]:\n    \"Step through tree, recording when nodes are in annotations.\"\n    in_annotation = False\n    nodes: list[tuple[bool, ast.AST]] = [(in_annotation, tree)]\n    to_replace = collections.defaultdict(list)\n\n    while nodes:\n        in_annotation, node = nodes.pop()\n\n        if isinstance(node, ast.Name) and in_annotation and node.id == \"bool\":\n            to_replace[node.lineno].append(node.col_offset)\n\n        for name in reversed(node._fields):\n            value = getattr(node, name)\n            if name in {\"annotation\", \"returns\"}:\n                next_in_annotation = True\n            else:\n                next_in_annotation = in_annotation\n            if isinstance(value, ast.AST):\n                nodes.append((next_in_annotation, value))\n            elif isinstance(value, list):\n                for value in reversed(value):\n                    if isinstance(value, ast.AST):\n                        nodes.append((next_in_annotation, value))\n\n    return to_replace\n\n\ndef replace_bool_with_bool_t(to_replace, content: str) -> str:\n    new_lines = []\n\n    for n, line in enumerate(content.splitlines(), start=1):\n        if n in to_replace:\n            for col_offset in reversed(to_replace[n]):\n                line = line[:col_offset] + \"bool_t\" + line[col_offset + 4 :]\n        new_lines.append(line)\n    return \"\\n\".join(new_lines)\n\n\ndef check_for_bool_in_generic(content: str) -> tuple[bool, str]:\n    tree = ast.parse(content)\n    to_replace = visit(tree)\n\n    if not to_replace:\n        mutated = False\n        return mutated, content\n\n    mutated = True\n    return mutated, replace_bool_with_bool_t(to_replace, content)\n\n\ndef main(argv: Sequence[str] | None = None) -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"paths\", nargs=\"*\")\n    args = parser.parse_args(argv)\n\n    for path in args.paths:\n        with open(path, encoding=\"utf-8\") as fd:\n            content = fd.read()\n        mutated, new_content = check_for_bool_in_generic(content)\n        if mutated:\n            with open(path, \"w\", encoding=\"utf-8\") as fd:\n                fd.write(new_content)\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "filename": "scripts/use_pd_array_in_core.py",
      "content": "\"\"\"\nCheck that pandas/core imports pandas.array as pd_array.\n\nThis makes it easier to grep for usage of pandas array.\n\nThis is meant to be run as a pre-commit hook - to run it manually, you can do:\n\n    pre-commit run use-pd_array-in-core --all-files\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport ast\nimport sys\nfrom typing import Sequence\n\nERROR_MESSAGE = (\n    \"{path}:{lineno}:{col_offset}: \"\n    \"Don't use pd.array in core, import array as pd_array instead\\n\"\n)\n\n\nclass Visitor(ast.NodeVisitor):\n    def __init__(self, path: str) -> None:\n        self.path = path\n\n    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:\n        # If array has been imported from somewhere in pandas,\n        # check it's aliased as pd_array.\n        if (\n            node.module is not None\n            and node.module.startswith(\"pandas\")\n            and any(i.name == \"array\" and i.asname != \"pd_array\" for i in node.names)\n        ):\n            msg = ERROR_MESSAGE.format(\n                path=self.path, lineno=node.lineno, col_offset=node.col_offset\n            )\n            sys.stdout.write(msg)\n            sys.exit(1)\n        super().generic_visit(node)\n\n    def visit_Attribute(self, node: ast.Attribute) -> None:\n        if (\n            isinstance(node.value, ast.Name)\n            and node.value.id == \"pd\"\n            and node.attr == \"array\"\n        ):\n            msg = ERROR_MESSAGE.format(\n                path=self.path, lineno=node.lineno, col_offset=node.col_offset\n            )\n            sys.stdout.write(msg)\n            sys.exit(1)\n        super().generic_visit(node)\n\n\ndef use_pd_array(content: str, path: str) -> None:\n    tree = ast.parse(content)\n    visitor = Visitor(path)\n    visitor.visit(tree)\n\n\ndef main(argv: Sequence[str] | None = None) -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"paths\", nargs=\"*\")\n    args = parser.parse_args(argv)\n\n    for path in args.paths:\n        with open(path, encoding=\"utf-8\") as fd:\n            content = fd.read()\n        use_pd_array(content, path)\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "filename": "scripts/validate_docstrings.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nAnalyze docstrings to detect errors.\n\nIf no argument is provided, it does a quick check of docstrings and returns\na csv with all API functions and results of basic checks.\n\nIf a function or method is provided in the form \"pandas.function\",\n\"pandas.module.class.method\", etc. a list of all errors in the docstring for\nthe specified function or method.\n\nUsage::\n    $ ./validate_docstrings.py\n    $ ./validate_docstrings.py pandas.DataFrame.head\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport doctest\nimport glob\nimport importlib\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\n\ntry:\n    from io import StringIO\nexcept ImportError:\n    from cStringIO import StringIO\n\n# Template backend makes matplotlib to not plot anything. This is useful\n# to avoid that plot windows are open from the doctests while running the\n# script. Setting here before matplotlib is loaded.\n# We don't warn for the number of open plots, as none is actually being opened\nos.environ[\"MPLBACKEND\"] = \"Template\"\nimport matplotlib  # isort:skip\n\nmatplotlib.rc(\"figure\", max_open_warning=10000)\n\nimport numpy  # isort:skip\n\nBASE_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\nsys.path.insert(0, os.path.join(BASE_PATH))\nimport pandas  # isort:skip\n\nsys.path.insert(1, os.path.join(BASE_PATH, \"doc\", \"sphinxext\"))\nfrom numpydoc.validate import validate, Docstring  # isort:skip\n\n\nPRIVATE_CLASSES = [\"NDFrame\", \"IndexOpsMixin\"]\nERROR_MSGS = {\n    \"GL04\": \"Private classes ({mentioned_private_classes}) should not be \"\n    \"mentioned in public docstrings\",\n    \"SA05\": \"{reference_name} in `See Also` section does not need `pandas` \"\n    \"prefix, use {right_reference} instead.\",\n    \"EX02\": \"Examples do not pass tests:\\n{doctest_log}\",\n    \"EX03\": \"flake8 error: {error_code} {error_message}{times_happening}\",\n    \"EX04\": \"Do not import {imported_library}, as it is imported \"\n    \"automatically for the examples (numpy as np, pandas as pd)\",\n}\n\n\ndef pandas_error(code, **kwargs):\n    \"\"\"\n    Copy of the numpydoc error function, since ERROR_MSGS can't be updated\n    with our custom errors yet.\n    \"\"\"\n    return (code, ERROR_MSGS[code].format(**kwargs))\n\n\ndef get_api_items(api_doc_fd):\n    \"\"\"\n    Yield information about all public API items.\n\n    Parse api.rst file from the documentation, and extract all the functions,\n    methods, classes, attributes... This should include all pandas public API.\n\n    Parameters\n    ----------\n    api_doc_fd : file descriptor\n        A file descriptor of the API documentation page, containing the table\n        of contents with all the public API.\n\n    Yields\n    ------\n    name : str\n        The name of the object (e.g. 'pandas.Series.str.upper).\n    func : function\n        The object itself. In most cases this will be a function or method,\n        but it can also be classes, properties, cython objects...\n    section : str\n        The name of the section in the API page where the object item is\n        located.\n    subsection : str\n        The name of the subsection in the API page where the object item is\n        located.\n    \"\"\"\n    current_module = \"pandas\"\n    previous_line = current_section = current_subsection = \"\"\n    position = None\n    for line in api_doc_fd:\n        line = line.strip()\n        if len(line) == len(previous_line):\n            if set(line) == set(\"-\"):\n                current_section = previous_line\n                continue\n            if set(line) == set(\"~\"):\n                current_subsection = previous_line\n                continue\n\n        if line.startswith(\".. currentmodule::\"):\n            current_module = line.replace(\".. currentmodule::\", \"\").strip()\n            continue\n\n        if line == \".. autosummary::\":\n            position = \"autosummary\"\n            continue\n\n        if position == \"autosummary\":\n            if line == \"\":\n                position = \"items\"\n                continue\n\n        if position == \"items\":\n            if line == \"\":\n                position = None\n                continue\n            item = line.strip()\n            func = importlib.import_module(current_module)\n            for part in item.split(\".\"):\n                func = getattr(func, part)\n\n            yield (\n                \".\".join([current_module, item]),\n                func,\n                current_section,\n                current_subsection,\n            )\n\n        previous_line = line\n\n\nclass PandasDocstring(Docstring):\n    @property\n    def mentioned_private_classes(self):\n        return [klass for klass in PRIVATE_CLASSES if klass in self.raw_doc]\n\n    @property\n    def examples_errors(self):\n        flags = doctest.NORMALIZE_WHITESPACE | doctest.IGNORE_EXCEPTION_DETAIL\n        finder = doctest.DocTestFinder()\n        runner = doctest.DocTestRunner(optionflags=flags)\n        context = {\"np\": numpy, \"pd\": pandas}\n        error_msgs = \"\"\n        for test in finder.find(self.raw_doc, self.name, globs=context):\n            f = StringIO()\n            runner.run(test, out=f.write)\n            error_msgs += f.getvalue()\n        return error_msgs\n\n    @property\n    def examples_source_code(self):\n        lines = doctest.DocTestParser().get_examples(self.raw_doc)\n        return [line.source for line in lines]\n\n    def validate_pep8(self):\n        if not self.examples:\n            return\n\n        # F401 is needed to not generate flake8 errors in examples\n        # that do not user numpy or pandas\n        content = \"\".join(\n            (\n                \"import numpy as np  # noqa: F401\\n\",\n                \"import pandas as pd  # noqa: F401\\n\",\n                *self.examples_source_code,\n            )\n        )\n\n        error_messages = []\n        with tempfile.NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\") as file:\n            file.write(content)\n            file.flush()\n            cmd = [\"python\", \"-m\", \"flake8\", \"--quiet\", \"--statistics\", file.name]\n            response = subprocess.run(cmd, capture_output=True, text=True)\n            stdout = response.stdout\n            stdout = stdout.replace(file.name, \"\")\n            messages = stdout.strip(\"\\n\")\n            if messages:\n                error_messages.append(messages)\n\n        for error_message in error_messages:\n            error_count, error_code, message = error_message.split(maxsplit=2)\n            yield error_code, message, int(error_count)\n\n\ndef pandas_validate(func_name: str):\n    \"\"\"\n    Call the numpydoc validation, and add the errors specific to pandas.\n\n    Parameters\n    ----------\n    func_name : str\n        Name of the object of the docstring to validate.\n\n    Returns\n    -------\n    dict\n        Information about the docstring and the errors found.\n    \"\"\"\n    doc = PandasDocstring(func_name)\n    result = validate(func_name)\n\n    mentioned_errs = doc.mentioned_private_classes\n    if mentioned_errs:\n        result[\"errors\"].append(\n            pandas_error(\"GL04\", mentioned_private_classes=\", \".join(mentioned_errs))\n        )\n\n    if doc.see_also:\n        for rel_name in doc.see_also:\n            if rel_name.startswith(\"pandas.\"):\n                result[\"errors\"].append(\n                    pandas_error(\n                        \"SA05\",\n                        reference_name=rel_name,\n                        right_reference=rel_name[len(\"pandas.\") :],\n                    )\n                )\n\n    result[\"examples_errs\"] = \"\"\n    if doc.examples:\n        result[\"examples_errs\"] = doc.examples_errors\n        if result[\"examples_errs\"]:\n            result[\"errors\"].append(\n                pandas_error(\"EX02\", doctest_log=result[\"examples_errs\"])\n            )\n\n        for error_code, error_message, error_count in doc.validate_pep8():\n            times_happening = f\" ({error_count} times)\" if error_count > 1 else \"\"\n            result[\"errors\"].append(\n                pandas_error(\n                    \"EX03\",\n                    error_code=error_code,\n                    error_message=error_message,\n                    times_happening=times_happening,\n                )\n            )\n        examples_source_code = \"\".join(doc.examples_source_code)\n        for wrong_import in (\"numpy\", \"pandas\"):\n            if f\"import {wrong_import}\" in examples_source_code:\n                result[\"errors\"].append(\n                    pandas_error(\"EX04\", imported_library=wrong_import)\n                )\n\n    return result\n\n\ndef validate_all(prefix, ignore_deprecated=False):\n    \"\"\"\n    Execute the validation of all docstrings, and return a dict with the\n    results.\n\n    Parameters\n    ----------\n    prefix : str or None\n        If provided, only the docstrings that start with this pattern will be\n        validated. If None, all docstrings will be validated.\n    ignore_deprecated: bool, default False\n        If True, deprecated objects are ignored when validating docstrings.\n\n    Returns\n    -------\n    dict\n        A dictionary with an item for every function/method... containing\n        all the validation information.\n    \"\"\"\n    result = {}\n    seen = {}\n\n    api_doc_fnames = os.path.join(BASE_PATH, \"doc\", \"source\", \"reference\", \"*.rst\")\n    api_items = []\n    for api_doc_fname in glob.glob(api_doc_fnames):\n        with open(api_doc_fname) as f:\n            api_items += list(get_api_items(f))\n\n    for func_name, func_obj, section, subsection in api_items:\n        if prefix and not func_name.startswith(prefix):\n            continue\n        doc_info = pandas_validate(func_name)\n        if ignore_deprecated and doc_info[\"deprecated\"]:\n            continue\n        result[func_name] = doc_info\n\n        shared_code_key = doc_info[\"file\"], doc_info[\"file_line\"]\n        shared_code = seen.get(shared_code_key, \"\")\n        result[func_name].update(\n            {\n                \"in_api\": True,\n                \"section\": section,\n                \"subsection\": subsection,\n                \"shared_code_with\": shared_code,\n            }\n        )\n\n        seen[shared_code_key] = func_name\n\n    return result\n\n\ndef print_validate_all_results(\n    prefix: str,\n    errors: list[str] | None,\n    output_format: str,\n    ignore_deprecated: bool,\n):\n    if output_format not in (\"default\", \"json\", \"actions\"):\n        raise ValueError(f'Unknown output_format \"{output_format}\"')\n\n    result = validate_all(prefix, ignore_deprecated)\n\n    if output_format == \"json\":\n        sys.stdout.write(json.dumps(result))\n        return 0\n\n    prefix = \"##[error]\" if output_format == \"actions\" else \"\"\n    exit_status = 0\n    for name, res in result.items():\n        for err_code, err_desc in res[\"errors\"]:\n            if errors and err_code not in errors:\n                continue\n            sys.stdout.write(\n                f'{prefix}{res[\"file\"]}:{res[\"file_line\"]}:'\n                f\"{err_code}:{name}:{err_desc}\\n\"\n            )\n            exit_status += 1\n\n    return exit_status\n\n\ndef print_validate_one_results(func_name: str):\n    def header(title, width=80, char=\"#\"):\n        full_line = char * width\n        side_len = (width - len(title) - 2) // 2\n        adj = \"\" if len(title) % 2 == 0 else \" \"\n        title_line = f\"{char * side_len} {title}{adj} {char * side_len}\"\n\n        return f\"\\n{full_line}\\n{title_line}\\n{full_line}\\n\\n\"\n\n    result = pandas_validate(func_name)\n\n    sys.stderr.write(header(f\"Docstring ({func_name})\"))\n    sys.stderr.write(f\"{result['docstring']}\\n\")\n\n    sys.stderr.write(header(\"Validation\"))\n    if result[\"errors\"]:\n        sys.stderr.write(f'{len(result[\"errors\"])} Errors found:\\n')\n        for err_code, err_desc in result[\"errors\"]:\n            if err_code == \"EX02\":  # Failing examples are printed at the end\n                sys.stderr.write(\"\\tExamples do not pass tests\\n\")\n                continue\n            sys.stderr.write(f\"\\t{err_desc}\\n\")\n    else:\n        sys.stderr.write(f'Docstring for \"{func_name}\" correct. :)\\n')\n\n    if result[\"examples_errs\"]:\n        sys.stderr.write(header(\"Doctests\"))\n        sys.stderr.write(result[\"examples_errs\"])\n\n\ndef main(func_name, prefix, errors, output_format, ignore_deprecated):\n    \"\"\"\n    Main entry point. Call the validation for one or for all docstrings.\n    \"\"\"\n    if func_name is None:\n        return print_validate_all_results(\n            prefix, errors, output_format, ignore_deprecated\n        )\n    else:\n        print_validate_one_results(func_name)\n        return 0\n\n\nif __name__ == \"__main__\":\n    format_opts = \"default\", \"json\", \"actions\"\n    func_help = (\n        \"function or method to validate (e.g. pandas.DataFrame.head) \"\n        \"if not provided, all docstrings are validated and returned \"\n        \"as JSON\"\n    )\n    argparser = argparse.ArgumentParser(description=\"validate pandas docstrings\")\n    argparser.add_argument(\"function\", nargs=\"?\", default=None, help=func_help)\n    argparser.add_argument(\n        \"--format\",\n        default=\"default\",\n        choices=format_opts,\n        help=\"format of the output when validating \"\n        \"multiple docstrings (ignored when validating one). \"\n        \"It can be {str(format_opts)[1:-1]}\",\n    )\n    argparser.add_argument(\n        \"--prefix\",\n        default=None,\n        help=\"pattern for the \"\n        \"docstring names, in order to decide which ones \"\n        'will be validated. A prefix \"pandas.Series.str.\"'\n        \"will make the script validate all the docstrings \"\n        \"of methods starting by this pattern. It is \"\n        \"ignored if parameter function is provided\",\n    )\n    argparser.add_argument(\n        \"--errors\",\n        default=None,\n        help=\"comma separated \"\n        \"list of error codes to validate. By default it \"\n        \"validates all errors (ignored when validating \"\n        \"a single docstring)\",\n    )\n    argparser.add_argument(\n        \"--ignore_deprecated\",\n        default=False,\n        action=\"store_true\",\n        help=\"if this flag is set, \"\n        \"deprecated objects are ignored when validating \"\n        \"all docstrings\",\n    )\n\n    args = argparser.parse_args()\n    sys.exit(\n        main(\n            args.function,\n            args.prefix,\n            args.errors.split(\",\") if args.errors else None,\n            args.format,\n            args.ignore_deprecated,\n        )\n    )\n"
    }
  ]
}